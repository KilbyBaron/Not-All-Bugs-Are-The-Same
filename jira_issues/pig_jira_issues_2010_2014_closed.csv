Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Labels,Labels,Labels,Labels,Labels,Labels,Labels,Description,Environment,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Cloners),Outward issue link (Container),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Regression),Outward issue link (Regression),Outward issue link (Regression),Outward issue link (Regression),Outward issue link (Required),Outward issue link (Supercedes),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (Hadoop Flags),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Patch Info),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Severity),Custom field (Severity),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Tags),Custom field (Test and Documentation Plan),Custom field (Testcase included),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Pig creates wrong schema after dereferencing nested tuple fields with sorts,PIG-3807,12700853,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,ddreyfus,ddreyfus,11/Mar/14 23:24,07/Jul/14 18:07,14/Mar/19 03:06,25/May/14 19:46,0.12.0,0.13.0,,,,,,0.13.0,,,,parser,,,0,,,,,,,,,,,,,"The following script fails:

d1 = load 'test_data.txt' USING PigStorage() AS (f1: int, f2: int, f3: int, f4: int);
d2 = load 'test_data.txt' USING PigStorage() AS (f1: int, f2: int, f3: int, f4: int);
-- the sorting causes the error
n1 = foreach (group d1 by f1) {
	sorted = ORDER d1 by f2;
	generate group, flatten(d1.f3) as x3;
};
n2 = foreach (group d2 by f1) {
	sorted = ORDER d2 by f2;
	generate group, flatten(d2.f3) as q3;
};
describe n1;
describe n2;

joined = join n1 by (x3), n2 by (q3);
describe joined;
final = foreach joined generate n1::x3;
dump final;

-- selected output below
n1: {group: int,x3: int}
n2: {group: int,q3: int}
joined: {n1::group: int,n1::x3: int,n2::group: int,n2::q3: int}
2014-03-11 19:16:35 ERROR Grunt:125 - ERROR 1025: 
<file , line 17, column 32> Invalid field projection. Projected field [n1::x3] does not exist in schema: n1::f1:int,n1::f2:int,n1::f3:int,n1::f4:int,n2::f1:int,n2::f2:int,n2::f3:int,n2::f4:int.
Details at logfile: ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Mar/14 03:34;daijy;PIG-3807-1.patch;https://issues.apache.org/jira/secure/attachment/12635025/PIG-3807-1.patch,11/Mar/14 23:26;ddreyfus;test_data.txt;https://issues.apache.org/jira/secure/attachment/12634049/test_data.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-03-17 03:34:45.986,,,no_permission,,,,,,,,,,,,379196,Reviewed,,,Mon Apr 14 20:02:42 UTC 2014,,,,,,,0|i1tc73:,379488,,,,,,,,,,11/Mar/14 23:26;ddreyfus;Test data used in script. Same as PIG-2767,17/Mar/14 03:34;daijy;Another DanglingNestedNode issue. Thanks for reporting!,19/Mar/14 18:48;rohini;+1,"19/Mar/14 21:18;daijy;Turns out the change is a subset of [~knoguchi]'s patch in PIG-2970. Credit to Koji!

Patch committed to trunk. Thanks Rohini for review!","19/Mar/14 21:47;ddreyfus;Hi Daniel,
How can I locate jar files with these patches implemented?

Thank you

Sent from my iPhone


","19/Mar/14 21:53;daijy;You can build from trunk:
svn co http://svn.apache.org/repos/asf/pig/trunk
cd trunk
ant (or ant -Dhadoopversion=23 if you are using Hadoop 2)","09/Apr/14 19:10;ddreyfus;When I test using pig 0.13.0 the problem remains.
Can someone double-check that the problem has been resolved?
```
n1: {group: int,x3: int}
n2: {group: int,q3: int}
joined: {n1::group: int,n1::x3: int,n2::group: int,n2::q3: int}
2014-04-09 14:58:55 ERROR Grunt:125 - ERROR 1025: 
<file C:/Users/dreyfusd/Dropbox/workspace_Pig/PIG-2767-bug/pig-2767-bug.pig, line 18, column 32> Invalid field projection. Projected field [n1::x3] does not exist in schema: n1::f1:int,n1::f2:int,n1::f3:int,n1::f4:int,n2::f1:int,n2::f2:int,n2::f3:int,n2::f4:int.
","11/Apr/14 01:01;daijy;Seems it is fine for me:
d1 = load 'test_data.txt' USING PigStorage() AS (f1: int, f2: int, f3: int, f4: int);
d2 = load 'test_data.txt' USING PigStorage() AS (f1: int, f2: int, f3: int, f4: int);
n1 = foreach (group d1 by f1) {sorted = ORDER d1 by f2; generate group, flatten(d1.f3) as x3; };
n2 = foreach (group d2 by f1) {sorted = ORDER d2 by f2; generate group, flatten(d2.f3) as q3; };
joined = join n1 by x3, n2 by q3;
final = foreach joined generate n1::x3;
describe final;

final: {n1::x3: int}

What is the exact script are you running?","12/Apr/14 17:28;ddreyfus;I run the script that I posted originally.
I test running against 0.13.0 without the attached patch, based on my understanding.
Do I need to apply patch-3807-1, too?",14/Apr/14 04:56;daijy;The patch is already committed to trunk. What do you mean 0.13.0? Do you check out trunk and build Pig by yourself?,"14/Apr/14 11:19;ddreyfus;1) Yes, I built the trunk.
2) After rebuilding the trunk doing a -clean- first, it worked better.
3) However, testing on Windows under Eclipse I notice that Pig can't delete its temp files. Do you notice any such problem?",14/Apr/14 12:29;ddreyfus;It works as expected on Linux after a better build of the trunk.,"14/Apr/14 19:11;ddreyfus;GruntParser.processDump() doesn't close any files on Windows after completing the dump.
Not sure where to look, but the iterator allocated in processDump() doesn't close the file.
ReadToEndLoader.initializeReader() doesn't close the file when the curSplitIndex > inpSplits.size()-1. Perhaps it should close the reader if it's open and ALSO return false.",14/Apr/14 19:46;daijy;This seems to be a separate issue and we shall open a new Jira for it. Do you see any exception? How can I reproduce it?,"14/Apr/14 20:02;ddreyfus;Yes. Close the 3807 and open a new one case.
No exceptions.
Use the same script as provided for pig-3807, except run it from Eclipse on Windows.
You'll notice two errors at the end of execution corresponding to an attempt to delete the two files that are still open.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigServer constructor throws NPE after PIG-3765,PIG-3806,12700851,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,aniket486,aniket486,11/Mar/14 23:14,07/Jul/14 18:08,14/Mar/19 03:07,12/Mar/14 05:52,0.13.0,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"PigServer constructor throws NPE because filter is not initialized at the right place in PIG-3765.
{noformat}
java.lang.NullPointerException
 at org.apache.pig.PigServer.registerJar(PigServer.java:540)
 at org.apache.pig.PigServer.addJarsFromProperties(PigServer.java:261)
 at org.apache.pig.PigServer.<init>(PigServer.java:237)
 at org.apache.pig.PigServer.<init>(PigServer.java:219)
 at org.apache.pig.tools.grunt.Grunt.<init>(Grunt.java:46)
 at org.apache.pig.Main.run(Main.java:600)
 at org.apache.pig.Main.main(Main.java:156)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
 at java.lang.reflect.Method.invoke(Method.java:597)
 at org.apache.hadoop.util.RunJar.main(RunJar.java:197)
=========================================================
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11/Mar/14 23:16;aniket486;PIG-3806.patch;https://issues.apache.org/jira/secure/attachment/12634042/PIG-3806.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-11 23:43:42.577,,,no_permission,,,,,,,,,,,,379194,,,,Wed Mar 12 05:52:02 UTC 2014,,,,,,,0|i1tc6n:,379486,,,,,,,,,,"11/Mar/14 23:43;prkommireddi;+1

Thanks for the fix [~aniket486]",12/Mar/14 05:52;aniket486;Committed to trunk! Thanks [~prkommireddi].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ToString(datetime [, format string]) doesn't work without the second argument",PIG-3805,12700612,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,jennythompson,jennythompson,jennythompson,11/Mar/14 00:36,07/Jul/14 18:07,14/Mar/19 03:07,16/Mar/14 04:07,0.12.0,,,,,,,0.13.0,,,,internal-udfs,,,0,easyfix,,,,,,,,,,,,"The exec function of ToString is written to handle 1 or 2 arguments (it defaults to ISO, which is consistent with ToDate).

However, the getArgToFuncMapping function returns only one FuncSpec, requiring the formatString argument.

To fix: just return add another FuncSpec to getArcToFuncMapping.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Mar/14 18:47;jennythompson;PIG-3805.patch;https://issues.apache.org/jira/secure/attachment/12634786/PIG-3805.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-13 18:17:15.799,,,no_permission,,,,,,,,,,,,378329,Reviewed,,,Sun Mar 16 04:07:31 UTC 2014,,,,,,,0|i1t6uv:,378621,,,,,,,,,,"13/Mar/14 18:17;smit;I am new to Apache Pig and tried to work on this issue as a start.

From my basic investigation it seems testConversionBetweenDateTimeAndString() function in test/org/apache/pig/test/TestBuiltin.java has some tested ToString method without second argument.

Am I missing something? I downloaded code from the ""git clone https://github.com/apache/pig.git"".
","14/Mar/14 17:47;jennythompson;That only tests the exec function of ToString, not the getArgToFuncMapping functions. This means that when using the Pig script ""ToString(ToDate('2009-01-07T01:07:01.000Z'))"" I get the error:

Caused by: org.apache.pig.impl.logicalLayer.validators.TypeCheckerException: ERROR 1045: 
<line 2, column 144> Could not infer the matching function for org.apache.pig.builtin.ToString as multiple or none of them fit. Please use an explicit cast.
	at org.apache.pig.newplan.logical.visitor.TypeCheckingExpVisitor.visit(TypeCheckingExpVisitor.java:784)

though
ToDate('2009-01-07T01:07:01.000Z') works fine.
",14/Mar/14 18:11;jennythompson;This adds another FuncSpec to the list returned by ToString.getArgToFuncMapping().,16/Mar/14 04:07;daijy;Makes sense. Patch committed to trunk. Thanks Jenny!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix TestBlackAndWhitelistValidator failures,PIG-3802,12699666,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,prkommireddi,prkommireddi,prkommireddi,09/Mar/14 02:19,07/Jul/14 18:08,14/Mar/19 03:07,10/Mar/14 02:01,0.13.0,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"From [~cheolsoo]

Prashant Kommireddi, I am reopening this jira. Your new tests fail with the following error-
Testcase: testWhitelist2 took 0.046 sec 
        Caused an ERROR
org.hamcrest.Matcher.describeMismatch(Ljava/lang/Object;Lorg/hamcrest/Description;)V
java.lang.NoSuchMethodError: org.hamcrest.Matcher.describeMismatch(Ljava/lang/Object;Lorg/hamcrest/Description;)V
        at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)
I think we're hitting this problem-
https://tedvinke.wordpress.com/2013/12/17/mixing-junit-hamcrest-and-mockito-explaining-nosuchmethoderror/
",,,,,,,,,,,,,,,,,,,,,,,,,PIG-3765,,,,,,,09/Mar/14 02:24;prkommireddi;PIG-3802.patch;https://issues.apache.org/jira/secure/attachment/12633574/PIG-3802.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-09 05:01:14.519,,,no_permission,,,,,,,,,,,,378013,,,,Mon Mar 10 02:01:39 UTC 2014,,,Patch Available,,,,0|i1t4wv:,378305,,,,,,,,,,"09/Mar/14 02:24;prkommireddi;Fixing errors from PIG-3765 check-in
[~cheolsoo]",09/Mar/14 05:01;cheolsoo;+1. Thank you sir!,10/Mar/14 02:01;prkommireddi;Thanks for the review [~cheolsoo]. Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Auto local mode does not call storeSchema,PIG-3801,12699555,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,aniket486,aniket486,08/Mar/14 00:58,07/Jul/14 18:08,14/Mar/19 03:07,23/Mar/14 06:47,,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"https://github.com/apache/pig/blob/trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java#L481

Pig code explicitly runs PigOutputCommitter.storeCleanup for local jobs. We also need to add this for auto-local jobs.

To repro this problem, run-
>  a = load '2.txt' as (a0:chararray, a1:int);
>  store a into 'a' using PigStorage(',','-schema');

This creates .pig_schema file in pig -x local mode, but does not create .pig_schema file in auto-local mode.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Mar/14 00:59;aniket486;PIG-3801.patch;https://issues.apache.org/jira/secure/attachment/12633503/PIG-3801.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-10 22:28:25.792,,,no_permission,,,,,,,,,,,,377902,,,,Sun Mar 23 06:47:43 UTC 2014,,,,,,,0|i1t487:,378194,,,,,,,,,,"10/Mar/14 22:28;julienledem;I would use properties.getProperty(MAPREDUCE_FRAMEWORK_NAME).equals(LOCAL) to decide if it's running locally, but otherwise this looks good to me.",10/Mar/14 22:28;julienledem;+1,23/Mar/14 06:47;aniket486;Committed to trunk. Thanks [~julienledem] for the review.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
registered jar in pig script are appended to the classpath multiple times,PIG-3798,12699094,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,dotanp,dotanp,06/Mar/14 08:33,07/Jul/14 18:07,14/Mar/19 03:07,02/Apr/14 11:50,,,,,,,,0.13.0,,,,,,,0,newbie,,,,,,,,,,,,"when running several pig scripts one after another using java class PigServer, the classpath in taskjvm.sh gets longer and longer, eventually execution breaks on having a too long classpath. 

The jar registered at the pig script are appended to the classpath on every execution. It seems that PigContext's skipJars member is the root cause for this as it is added jars that already exists in the list multiple times.", Apache Pig version 0.11.0-cdh4.4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30/Mar/14 22:33;cheolsoo;PIG-3798-1.patch;https://issues.apache.org/jira/secure/attachment/12637746/PIG-3798-1.patch,02/Apr/14 11:49;cheolsoo;PIG-3798-2.patch;https://issues.apache.org/jira/secure/attachment/12638238/PIG-3798-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-03-30 22:33:27.186,,,no_permission,,,,,,,,,,,,377441,,,,Wed Apr 02 11:50:29 UTC 2014,,,,,,,0|i1t1en:,377735,,,,,,,,,,"30/Mar/14 22:33;cheolsoo;The attached fixes the issue by adding jars only if they're not already present in the lists - extraJars, scriptJars, skipJars, and predeployedJars.",02/Apr/14 00:19;daijy;+1,02/Apr/14 11:49;cheolsoo;Uploading the final patch with minor clean-ups for the record. I'll commit it shortly.,"02/Apr/14 11:50;cheolsoo;Committed to trunk.

Thank you Daniel for reviewing the patch!
Thank you Dotan for reporting the issue!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigStats output bytes written not collected for relative paths,PIG-3796,12698748,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,04/Mar/14 22:09,07/Jul/14 18:07,14/Mar/19 03:07,30/Apr/14 14:01,,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"PIG-2924 added support for custom stats reader. But the FileBasedOutputSizeReader only checks for 

{code}
public static boolean isHDFSFileOrLocalOrS3N(String uri){
        if(uri == null)
            return false;
        if(uri.startsWith(""/"") || uri.matches(""[A-Za-z]:.*"") || uri.startsWith(""hdfs:"")
                || uri.startsWith(""viewfs:"") || uri.startsWith(""file:"") || uri.startsWith(""s3n:"")) {
            return true;
        }
        return false;
    }
{code}
Better to change this to UriUtil.hasFileSystemImpl which will automatically filter out hbase://.  This would still not solve cases like HCatStorer which does not have a scheme. Will also write a default stats reader that checks for known StoreFuncInterface implementations that are not file based like HCatStorer. More standard ones can be added later. AccumuloStorage should not be a problem as it has scheme accumulo://.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,377096,,,,Wed Apr 30 14:01:36 UTC 2014,,,,,,,0|i1szan:,377391,,,,,,,,,,24/Apr/14 19:02;rohini;Addressing this as part of PIG-3672,30/Apr/14 14:01;rohini;Fixed as part of PIG-3672.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig -useHCatalog fails using pig command line interface on HDInsight,PIG-3794,12698551,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ehans,ehans,ehans,04/Mar/14 01:51,15/Apr/14 20:45,14/Mar/19 03:07,25/Mar/14 00:24,0.11,0.12.0,0.13.0,,,,,0.12.1,0.13.0,,,,,,0,,,,,,,,,,,,,"When you connect to an HDP 1.3 version HDINSIGHT cluster with remote desktop, if you try this:

c:\apps\dist\pig-0.11.0.1.3.2.0-05>bin\pig -useHCatalog

you get this:

""HCAT_HOME should be defined""

but you should not get an error.

It appears that pig.cmd should use ""HCATALOG_HOME"" instead of ""HCAT_HOME"".

The same problem exists on the 1.3 one-box installation for Windows. A quick look at the source code indicates it is still a problem on trunk.

In addition, if you set hive.metastore.uris to '', this is supposed to create an embedded metastore instead of going to the metastore service. This fails on Azure HDINSIGHT and Windows because of missing datanucleus and sqljdbc4 jars. 

Finally, if you submit a pig job from WebHCat, due to argument quoting for windows, -useHCatalog comes in as ""-useHCatalog"" into pig.cmd. This causes -useHCatalog to never work on Windows from WebHCat.","Windows Azure HDINSIGHT
c:\apps\dist\pig-0.11.0.1.3.2.0-05",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Mar/14 02:20;ehans;PIG-3794.01.patch;https://issues.apache.org/jira/secure/attachment/12632425/PIG-3794.01.patch,05/Mar/14 21:44;ehans;PIG-3794.02.patch;https://issues.apache.org/jira/secure/attachment/12632920/PIG-3794.02.patch,05/Mar/14 21:55;ehans;PIG-3794.03.patch;https://issues.apache.org/jira/secure/attachment/12632922/PIG-3794.03.patch,12/Mar/14 00:30;ehans;PIG-3794.04.patch;https://issues.apache.org/jira/secure/attachment/12634063/PIG-3794.04.patch,24/Mar/14 21:32;ehans;PIG-3794.05.patch;https://issues.apache.org/jira/secure/attachment/12636437/PIG-3794.05.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2014-03-09 20:24:48.285,,,no_permission,,,,,,,,,,,,376909,Reviewed,,,Tue Mar 25 00:28:22 UTC 2014,,,,,,,0|i1sy53:,377204,,,,,,,,,,04/Mar/14 02:20;ehans;Fix for both issues (Monarch PIG-125 and PIG-126),04/Mar/14 19:27;ehans;Code review available at: https://reviews.apache.org/r/18744/,"05/Mar/14 21:44;ehans;Updating to included necessary jars for embedded metastore case, plus check for quoted -useHCatalog argument that can come from WebHCat.",05/Mar/14 21:55;ehans;fix typo in comment,"09/Mar/14 20:24;daijy;[~ehans], do you know why in Windows pig.cmd should use HCATALOG_HOME instead? It should follow the same convention as Unix for better maintenance. ",10/Mar/14 20:18;ehans;I just know that HCAT_HOME is not defined on HDINSIGHT or our windows one-box setup. ,"11/Mar/14 07:28;daijy;Sounds like HDINSIGHT should define HCAT_HOME. I see it defined in other Azure installation, not sure how it is different than the one-box setup.",11/Mar/14 23:36;ehans;What version of Azure HDINSIGHT installation were you looking at when you saw HCAT_HOME defined?,12/Mar/14 00:30;ehans;Updated patch to use HCAT_HOME but conditionally set it from HCATALOG_HOME. This will allow us to phase out use of HCATALOG_HOME and use HCAT_HOME. Chasing down all uses of HCATALOG_HOME is beyond the scope of this patch.,24/Mar/14 21:32;ehans;Added comment to say use of HCATALOG_HOME is deprecated.,25/Mar/14 00:24;daijy;+1. Patch committed to trunk. We can remove HCATALOG_HOME later. Thanks Eric!,25/Mar/14 00:28;daijy;Also commit to 0.12 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PushDownForEachFlatten + ColumnMapKeyPrune with user defined schema failing due to incorrect UID assignment,PIG-3782,12697736,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,knoguchi,knoguchi,27/Feb/14 16:50,15/Apr/14 20:44,14/Mar/19 03:07,16/Mar/14 03:56,0.10.1,0.11.1,0.12.0,,,,,0.12.1,0.13.0,,,,,,0,,,,,,,,,,,,,"{noformat}
a = load '1.txt' as (a0:int, a1, a2:bag{});
b = load '2.txt' as (b0:int, b1);
c = foreach a generate a0, flatten(a2) as (q1, q2);
d = join c by a0, b by b0;
e = foreach d generate a0, q1, q2;
f = foreach e generate a0, (int)q1, (int)q2;
store f into 'output';
{noformat}

This pig script fails with 
2014-02-27 11:49:45,657 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2229: Couldn't find matching uid -1 for project (Name: Project Type: bytearray Uid: 13 Input: 0 Column: 1)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Mar/14 07:37;daijy;PIG-3782-2.patch;https://issues.apache.org/jira/secure/attachment/12633584/PIG-3782-2.patch,27/Feb/14 17:50;knoguchi;pig-3782-v01.patch;https://issues.apache.org/jira/secure/attachment/12631566/pig-3782-v01.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-03-09 07:37:35.816,,,no_permission,,,,,,,,,,,,376210,Reviewed,,,Sun Mar 16 03:57:19 UTC 2014,,,,,,,0|i1sttz:,376506,,,,,,,,,,"27/Feb/14 17:14;knoguchi;This error is happening when PushDownForEachFlatten inserts a FOREACH after 'd=join' to move the flatten after the join for optimization.  Somehow, this new foreach is containing completely new UIDs for q1 and q2.
You can see below that new foreach has q1#25 and q2#26 instead of q1#13 and q2#14 that are later used.
This breaks the linage tracking of ColumnMapKeyPrune.

BEFORE PushDownForEachFlatten
{noformat}
    |---e: (Name: LOForEach Schema: c::a0#1:int,c::q1#13:bytearray,c::q2#14:bytearray)
        |   |
        |   (Name: LOGenerate[false,false,false] Schema: c::a0#1:int,c::q1#13:bytearray,c::q2#14:bytearray)
        |   |   |
        |   |   c::a0:(Name: Project Type: int Uid: 1 Input: 0 Column: (*))
        |   |   |
        |   |   c::q1:(Name: Project Type: bytearray Uid: 13 Input: 1 Column: (*))
        |   |   |
        |   |   c::q2:(Name: Project Type: bytearray Uid: 14 Input: 2 Column: (*))
        |   |
        |   |---(Name: LOInnerLoad[0] Schema: c::a0#1:int)
        |   |
        |   |---(Name: LOInnerLoad[1] Schema: c::q1#13:bytearray)
        |   |
        |   |---(Name: LOInnerLoad[2] Schema: c::q2#14:bytearray)
        |
        |---d: (Name: LOJoin(HASH) Schema: c::a0#1:int,c::q1#13:bytearray,c::q2#14:bytearray,b::b0#7:int,b::b1#8:bytearray)
            |   |
            |   a0:(Name: Project Type: int Uid: 1 Input: 0 Column: 0)
            |   |
            |   b0:(Name: Project Type: int Uid: 7 Input: 1 Column: 0)
            |
            |---c: (Name: LOForEach Schema: a0#1:int,q1#13:bytearray,q2#14:bytearray)
{noformat}

After PushDownForEachFlatten
{noformat}
    |---e: (Name: LOForEach Schema: c::a0#1:int,c::q1#13:bytearray,c::q2#14:bytearray)
        |   |
        |   (Name: LOGenerate[false,false,false] Schema: c::a0#1:int,c::q1#13:bytearray,c::q2#14:bytearray)
        |   |   |
        |   |   c::a0:(Name: Project Type: int Uid: 1 Input: 0 Column: (*))
        |   |   |
        |   |   c::q1:(Name: Project Type: bytearray Uid: 13 Input: 1 Column: (*))
        |   |   |
        |   |   c::q2:(Name: Project Type: bytearray Uid: 14 Input: 2 Column: (*))
        |   |
        |   |---(Name: LOInnerLoad[0] Schema: c::a0#1:int)
        |   |
        |   |---(Name: LOInnerLoad[1] Schema: c::q1#13:bytearray)
        |   |
        |   |---(Name: LOInnerLoad[2] Schema: c::q2#14:bytearray)
        |
        |---d: (Name: LOForEach Schema: c::a0#1:int,q1#25:bytearray,q2#26:bytearray,b::b0#7:int,b::b1#8:bytearray)
            |   |
            |   (Name: LOGenerate[false,true,false,false] Schema: c::a0#1:int,q1#25:bytearray,q2#26:bytearray,b::b0#7:int,b::b1#8:bytearray)
            |   |   |
            |   |   c::a0:(Name: Project Type: int Uid: 1 Input: 0 Column: (*))
            |   |   |
            |   |   c::a2:(Name: Project Type: bag Uid: 3 Input: 1 Column: (*))
            |   |   |
            |   |   b::b0:(Name: Project Type: int Uid: 7 Input: 2 Column: (*))
            |   |   |
            |   |   b::b1:(Name: Project Type: bytearray Uid: 8 Input: 3 Column: (*))
            |   |
            |   |---(Name: LOInnerLoad[0] Schema: c::a0#1:int)
            |   |
            |   |---c::a2: (Name: LOInnerLoad[1] Schema: null)
            |   |
            |   |---(Name: LOInnerLoad[2] Schema: b::b0#7:int)
            |   |
            |   |---(Name: LOInnerLoad[3] Schema: b::b1#8:bytearray)
            |
            |---d: (Name: LOJoin(HASH) Schema: c::a0#1:int,c::a2#3:bag{#4:tuple()},b::b0#7:int,b::b1#8:bytearray)
                |   |
                |   a0:(Name: Project Type: int Uid: 1 Input: 0 Column: 0)
                |   |
                |   b0:(Name: Project Type: int Uid: 7 Input: 1 Column: 0)
                |
                |---c: (Name: LOForEach Schema: a0#1:int,a2#3:bag{#4:tuple()})
{noformat}","27/Feb/14 17:35;knoguchi;PushDownForEachFlatten is properly copying the UserDefinedSchema from the original LOGenerate to the new LOGenerate (that is added after the join).

{code:title=PushDownForEachFlatten.java|borderStyle=solid}
295                 if (mUserDefinedSchema!=null)
296                     gen.setUserDefinedSchema(mUserDefinedSchema);
{code}

I'm thinking the problem is rather on LOGenerate.mUserDefinedSchema such that they never contain an actual UID since all the updates are done on the copy.

{code:title=LOGenerate.java|borderStyle=solid}
 76             LogicalSchema mUserDefinedSchemaCopy = null;
 77             if (mUserDefinedSchema!=null && mUserDefinedSchema.get(i)!=null) {
 78                 mUserDefinedSchemaCopy = new LogicalSchema();
 79                 for (LogicalSchema.LogicalFieldSchema fs : mUserDefinedSchema.get(i).getFields()) {
 80                     mUserDefinedSchemaCopy.addField(fs.deepCopy());
 81                 }
 82             }
...
143                     for (LogicalFieldSchema fs : mUserDefinedSchemaCopy.getFields()) {
144                         fs.stampFieldSchema();  //new UID is assigned
{code}","27/Feb/14 17:50;knoguchi;I lack the understanding of why we need the copying of userDefinedSchema. 
This patch takes out the copying step so that UID of the userDefinedSchema would be saved.

[~daijy], can you check if I'm not messing up with this change?","09/Mar/14 07:37;daijy;Hi, Koji,
I attach another patch only fix PushDownForEachFlatten. I reuse your test case though. Can you check if it looks better?","10/Mar/14 17:52;knoguchi;bq. I attach another patch only fix PushDownForEachFlatten. I reuse your test case though. Can you check if it looks better?

Thanks [~daijy].  I can see how your patch fixes the issue, but I'm not sure if it's better. 
Maybe it'll help me understand if you can teach me why we want to avoid my approach of getting rid of the extra copy inside the LOGenerate.
To me, fixing the issue at the source(producer) by storing UIDs inside mUserDefinedSchema is better than fixing on the receiver side with a workaround. 
","11/Mar/14 07:26;daijy;[~knoguchi], mUserDefinedSchema is considered the original input from the user. That's why we make a copy every time and avoid changing it directly. The uid field of mUserDefinedSchema is considered dummy. The reason I upload a different patch is I am not sure if there is any side effect for caching uid in mUserDefinedSchema. It might be possible we need mUserDefinedSchema but don't want uid field. I'd like to take a safer approach.","12/Mar/14 14:37;knoguchi;bq. The uid field of mUserDefinedSchema is considered dummy. The reason I upload a different patch is I am not sure if there is any side effect for caching uid in mUserDefinedSchema.

Thanks [~daijy] for the explanation.  I'm +1 on your patch.  (unit&e2e tests were fine.)
But can you add some comment in LOGenerate.java so that it's clear mUserDefinedSchema shouldn't store any UIDs and the reasons behind it? ",13/Mar/14 02:56;daijy;Added the comments for mUserDefinedSchema. Patch committed to 0.12 branch and trunk. Thanks Koji for reviewing!,"14/Mar/14 16:48;cheolsoo;[~daijy] and [~knoguchi], sorry for reopening the jira. But {{testForeachJoinWithUserDefinedSchemaAndPruning}} fails in branch-0.12 although it passes in trunk. Can you take a look?

Here is the stack trace-
{code}
Testcase: testForeachJoinWithUserDefinedSchemaAndPruning took 0.019 sec 
        Caused an ERROR
null
java.lang.reflect.InvocationTargetException
        at org.apache.pig.test.Util.buildLp(Util.java:1057)
        at org.apache.pig.test.TestNewPlanPushDownForeachFlatten.migrateAndOptimizePlanWithPruning(TestNewPlanPushDownForeachFlatten.java:1165)
        at org.apache.pig.test.TestNewPlanPushDownForeachFlatten.testForeachJoinWithUserDefinedSchemaAndPruning(TestNewPlanPushDownForeachFlatten.java:1126)
Caused by: org.apache.pig.impl.logicalLayer.validators.TypeCheckerException: ERROR 1059: 
<line 1, column 191> Problem while reconciling output schema of ForEach
        at org.apache.pig.newplan.logical.visitor.TypeCheckingRelVisitor.throwTypeCheckerException(TypeCheckingRelVisitor.java:142)
        at org.apache.pig.newplan.logical.visitor.TypeCheckingRelVisitor.visit(TypeCheckingRelVisitor.java:182)
        at org.apache.pig.newplan.logical.relational.LOForEach.accept(LOForEach.java:76)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:52)
        at org.apache.pig.PigServer$Graph.compile(PigServer.java:1733)
        at org.apache.pig.PigServer$Graph.compile(PigServer.java:1710)
        at org.apache.pig.PigServer$Graph.access$200(PigServer.java:1411)
        at org.apache.pig.PigServer.buildLp(PigServer.java:1376)
Caused by: org.apache.pig.impl.logicalLayer.validators.TypeCheckerException: ERROR 1052: 
<line 1, column 215> Cannot cast bytearray to int 
        at org.apache.pig.newplan.logical.visitor.TypeCheckingExpVisitor.visit(TypeCheckingExpVisitor.java:516)
        at org.apache.pig.newplan.logical.expression.CastExpression.accept(CastExpression.java:44)
        at org.apache.pig.newplan.ReverseDependencyOrderWalker.walk(ReverseDependencyOrderWalker.java:70)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:52)
        at org.apache.pig.newplan.logical.visitor.TypeCheckingRelVisitor.visitExpressionPlan(TypeCheckingRelVisitor.java:191)
        at org.apache.pig.newplan.logical.visitor.TypeCheckingRelVisitor.visit(TypeCheckingRelVisitor.java:157)
        at org.apache.pig.newplan.logical.relational.LOGenerate.accept(LOGenerate.java:244)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:52)
        at org.apache.pig.newplan.logical.visitor.TypeCheckingRelVisitor.visit(TypeCheckingRelVisitor.java:174)
{code}",14/Mar/14 19:56;daijy;Let me take a look.,16/Mar/14 03:42;daijy;This is because of PIG-3753. Mind if I commit PIG-3753 to 0.12 branch?,"16/Mar/14 03:48;cheolsoo;[~daijy], thank you for looking into it. I'm +1 to backporing PIG-3753.",16/Mar/14 03:57;daijy;PIG-3753 committed to 0.12 branch. This fix the failed testcase. Thanks Cheolsoo bring this out!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assert constructs ConstantExpression with null when no comment is given,PIG-3779,12697214,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thedatachef,cheolsoo,cheolsoo,25/Feb/14 21:03,15/Apr/14 20:45,14/Mar/19 03:07,26/Feb/14 20:45,,,,,,,,0.12.1,0.13.0,,,,,,0,,,,,,,,,,,,,"I ran into this problem with Lipstick after deploying ASSERT. When ASSERT is not given any comment, LogicalPlanBuilder constructs ConstantExpression with null. Later tools like Lipstick try to convert it to a String resulting in NPE.

Although it's possible to handle this case in Lipstick, I feel ConstantExpression shouldn't be constructed with null in the first place.

Here is the code in LogicalPlanBuilder-
{code}
ConstantExpression constExpr = new ConstantExpression(exprPlan, comment);
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Feb/14 22:05;thedatachef;assert.diff;https://issues.apache.org/jira/secure/attachment/12631064/assert.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,375688,,,,Wed Feb 26 20:45:30 UTC 2014,,,,,,,0|i1sqmf:,375984,,,,,,,,,,"25/Feb/14 22:46;cheolsoo;+1.

I will commit it tomorrow.",26/Feb/14 20:45;cheolsoo;Committed to 0.12.1 and trunk. Thank you Jacob!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Conflicting versions of jline is present in trunk ,PIG-3776,12696826,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,23/Feb/14 23:14,07/Jul/14 18:07,14/Mar/19 03:07,24/Feb/14 02:10,,,,,,,,0.13.0,,,,build,,,0,,,,,,,,,,,,,"This is a regression of PIG-3573.

In ivy/libraries.properties, the version of jline is specified as 0.9.94-
{code}
jline.version=0.9.94
{code}
But since jline is also a dependency of accumulo, jline-1.0.jar is pulled down at compile time-
{code}
$ ls build/ivy/lib/Pig/jline-*
build/ivy/lib/Pig/jline-1.0.jar
{code}

Now when pig-withouthadoop.jar is built, jline-1.0.jar is not packaged because ant looks for jline-0.9.94.jar-
{code}
<include name=""jline-${jline.version}.jar""/>
{code}

This causes a ClassNotFount exception at runtime.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Feb/14 23:18;cheolsoo;PIG-3776-1.patch;https://issues.apache.org/jira/secure/attachment/12630587/PIG-3776-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-24 00:44:09.498,,,no_permission,,,,,,,,,,,,375301,,,,Mon Feb 24 02:10:53 UTC 2014,,,,,,,0|i1so8n:,375597,,,,,,,,,,"23/Feb/14 23:18;cheolsoo;In the attached patch, I am doing two things-
# Bump jline.version to 1.0 in libraries.properties.
# Exclude jline from dependencies of accumulo so the version of jline is explicitly controlled by libraries.properties.

","24/Feb/14 00:44;elserj;Oops, sorry for not catching that [~cheolsoo]. Verified that jline is not included in the -withouthadoop.jar and that it is included after applying the patch.",24/Feb/14 00:58;daijy;+1,24/Feb/14 02:10;cheolsoo;Committed to trunk. Thank you Josh and Daniel for reviewing the patch!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Piggybank Over UDF get wrong result,PIG-3774,12696418,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,20/Feb/14 23:20,15/Apr/14 20:45,14/Mar/19 03:07,21/Feb/14 00:19,,,,,,,,0.12.1,0.13.0,,,piggybank,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Feb/14 23:21;daijy;PIG-3774-1.patch;https://issues.apache.org/jira/secure/attachment/12630188/PIG-3774-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-21 00:16:12.968,,,no_permission,,,,,,,,,,,,374894,Reviewed,,,Fri Feb 21 00:19:55 UTC 2014,,,,,,,0|i1slqv:,375193,,,,,,,,,,21/Feb/14 00:16;alangates;+1.  ,21/Feb/14 00:19;daijy;Patch committed to trunk and 0.12 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Syntax error when casting an inner schema of a bag and line break involved,PIG-3772,12696140,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ssvinarchukhorton,ahoyleo,ahoyleo,20/Feb/14 01:58,08/Jun/16 20:48,14/Mar/19 03:07,12/May/16 16:37,0.11.1,,,,,,,0.16.0,,,,,,,1,,,,,,,,,,,,,"Hi,

The following script fails with syntax error

{code}
A = load 'data' as (a:{(x:chararray, y:float)}, b:chararray);
B = foreach A generate
    b,
    (bag{tuple(long)}) a.x as ax:{(x:long)};
{code}

where the cast statement is on its own line.

The script fails with the following exception:
{code}
19-02-2014 17:30:22 PST bug_script ERROR - org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1000: Error during parsing.   Syntax error, unexpected symbol at or near 'bag'
19-02-2014 17:30:22 PST bug_script ERROR - 	at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1607)
19-02-2014 17:30:22 PST bug_script ERROR - 	at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1546)
19-02-2014 17:30:22 PST bug_script ERROR - 	at org.apache.pig.PigServer.registerQuery(PigServer.java:516)
19-02-2014 17:30:22 PST bug_script ERROR - 	at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:988)
19-02-2014 17:30:22 PST bug_script ERROR - 	at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:412)
19-02-2014 17:30:22 PST bug_script ERROR - 	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:194)
19-02-2014 17:30:22 PST bug_script ERROR - 	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:170)
19-02-2014 17:30:22 PST bug_script ERROR - 	at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:84)
19-02-2014 17:30:22 PST bug_script ERROR - 	at org.apache.pig.Main.run(Main.java:604)
19-02-2014 17:30:22 PST bug_script ERROR - 	at org.apache.pig.PigRunner.run(PigRunner.java:49)
19-02-2014 17:30:22 PST bug_script ERROR - 	at azkaban.jobtype.HadoopSecurePigWrapper.runPigJob(HadoopSecurePigWrapper.java:116)
19-02-2014 17:30:22 PST bug_script ERROR - 	at azkaban.jobtype.HadoopSecurePigWrapper$1.run(HadoopSecurePigWrapper.java:106)
19-02-2014 17:30:22 PST bug_script ERROR - 	at azkaban.jobtype.HadoopSecurePigWrapper$1.run(HadoopSecurePigWrapper.java:103)
19-02-2014 17:30:22 PST bug_script ERROR - 	at java.security.AccessController.doPrivileged(Native Method)
19-02-2014 17:30:22 PST bug_script ERROR - 	at javax.security.auth.Subject.doAs(Subject.java:396)
19-02-2014 17:30:22 PST bug_script ERROR - 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
19-02-2014 17:30:22 PST bug_script ERROR - 	at azkaban.jobtype.HadoopSecurePigWrapper.main(HadoopSecurePigWrapper.java:103)
19-02-2014 17:30:22 PST bug_script ERROR - Caused by: Failed to parse:   Syntax error, unexpected symbol at or near 'bag'
19-02-2014 17:30:22 PST bug_script ERROR - 	at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:235)
19-02-2014 17:30:22 PST bug_script ERROR - 	at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:177)
19-02-2014 17:30:22 PST bug_script ERROR - 	at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1599)
19-02-2014 17:30:22 PST bug_script ERROR - 	... 16 more
{code}

The script succeeds if the foreach statement is written in one line:
{code}
B = foreach A generate b, (bag{tuple(long)}) a.x as ax:{(x:long)};
{code}

This problem happens only in batch mode.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PIG-2597,16/Apr/14 08:53;ssvinarchukhorton;PIG-3772-2.patch;https://issues.apache.org/jira/secure/attachment/12640404/PIG-3772-2.patch,17/Apr/14 12:03;ssvinarchukhorton;PIG-3772-3.patch;https://issues.apache.org/jira/secure/attachment/12640619/PIG-3772-3.patch,12/May/16 16:33;knoguchi;PIG-3772-4.patch;https://issues.apache.org/jira/secure/attachment/12803699/PIG-3772-4.patch,15/Apr/14 13:11;ssvinarchukhorton;PIG-3772.patch;https://issues.apache.org/jira/secure/attachment/12640262/PIG-3772.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2014-04-14 11:42:08.883,,,no_permission,,,,,,,,,,,,374617,Reviewed,,,Thu May 12 16:37:16 UTC 2016,,,,,,,0|i1sk1j:,374917,,,,,,,,,,"14/Apr/14 11:42;ssvinarchukhorton;I tested this your pig script on trunk and this issue not reproduce when batch mode on and off.
{noformat}
A = load 'testdata' as (a:{(x:chararray, y:float)}, b:chararray);
B = foreach A generate
      b,
      bag{tuple(long)}) a.x as ax:{(x:long)};
{noformat}
",14/Apr/14 11:52;ssvinarchukhorton;Issue not reproduce on trunk,"14/Apr/14 19:37;daijy;[~ssvinarchuk], I can reproduce the issue on trunk. If you break the line as follows, it does not work:
{code}
A = load 'data' as (a:{(x:chararray, y:float)}, b:chararray);
B = foreach A generate
    b,
    (bag{tuple(long)}) a.x as ax:{(x:long)};
{code}

Can you double check?","15/Apr/14 11:31;ssvinarchukhorton;I reproduced this issue. But I had next exception: 
2014-04-15 13:16:19,047 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1200: <line 4, column 21>  mismatched input ';' expecting RIGHT_PAREN
And this issue reproduce in batch mode and interactive mode. 
But it not reproduce if create test with this script. 
This is problem in read and parse pig scripts. Because in GruntParser.processPig(String cmd) input string for second command will be 
{noformat}
B = foreach A generate
    b,    
    (bag{tuple(long)}
{noformat}",15/Apr/14 13:12;ssvinarchukhorton;I attached patch with fix this issue. Please review it.,"15/Apr/14 18:19;daijy;Thanks [~ssvinarchukhorton], the patch works for me. There is another occurrence of the same pattern in ""<IN_BLOCK> MORE"", shall we change it as well?","16/Apr/14 08:53;ssvinarchukhorton;Yes, I forgot added this changes to first patch.
I attached new patch.","16/Apr/14 20:07;daijy;Looks good, would you mind adding a testcase? TestGruntParser sounds like a good place to add.",17/Apr/14 12:04;ssvinarchukhorton;I added testcase for this issue. ,21/Apr/14 18:30;daijy;Patch committed to trunk. Thanks Sergey!,"15/May/14 17:32;rohini;This patch makes some of the existing scripts fail. For eg: 
a = FOREACH a GENERATE
        *,
--      0 AS f:int;
        ((f1 IS NOT NULL AND ((int)f2#'k1' == 0 OR (int)f2#'k1' == 1)) ? MyUDF(f2#'k3', f1#'k3', f1#'k4') : 0) AS f:int;

fails with ""Syntax error, unexpected symbol at or near '<EOF>'""  for the line after the comment",22/May/14 01:52;daijy;Rollback the patch. Look forward to solve it in PIG-2597 (Move grunt from javacc to ANTRL).,"13/Nov/14 22:24;knoguchi;We had a user wasting hours of his time due to this bug.  

I have no understanding of how the parsing works, but PIG-3772.patch and PIG-3772-3.patch has a typo in putting ""r"" instead of ""\r"".   PIG-3772-2.patch does have the correct ""\r"". 

Can this cause the error Rohini faced?  ","13/Nov/14 23:34;daijy;I tried fixing the typo, still see the same exception.","05/May/16 19:55;knoguchi;[~rohini], can you reproduce the error with the parser on trunk?  I couldn't.
Appreciate if you can attach the full script.

Yes, it's best to throw away javacc code in PIG-2597, but until then it would be nice to get this silly error fixed.  It's pretty confusing.","10/May/16 21:35;knoguchi;Got to reproduce Rohini's issue of 
{{ ""Syntax error, unexpected symbol at or near '<EOF>'""}} 
on pig 0.11.  
This was coming from PIG-4818 which is already fixed in trunk.
Basically comment within <GENERATE> state was not being ignored.","10/May/16 23:27;daijy;Great, let's commit PIG-3772-3.patch then.","12/May/16 16:33;knoguchi;{code}
-|   <("" "" | ""\t"")+[""G"",""g""][""E"",""e""][""N"",""n""][""E"",""e""][""R"",""r""][""A"",""a""][""T"",""t""][""E"",""e""]("" "" | ""\t"")+ > {prevState = PIG_START;} : GENERATE
+|   <("" "" | ""\t"")+[""G"",""g""][""E"",""e""][""N"",""n""][""E"",""e""][""R"",""r""][""A"",""a""][""T"",""t""][""E"",""e""]("" "" | ""\t"" | ""r"" | ""\n"")+ > {prevState = PIG_START;} : GENERATE
{code}
Uploading a patch that corrects the typo of {{""r""}} to {{""\r""}}.",12/May/16 16:37;knoguchi;Committed {{PIG-3772-4.patch}} to trunk.  Thanks Sergey! ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Piggybank Avrostorage makes a lot of namenode calls in the backend,PIG-3771,12696100,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,19/Feb/14 22:52,07/Jul/14 18:07,14/Mar/19 03:07,16/Apr/14 17:28,0.11.1,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,  The amount of list status calls it makes in setLocation if combined with wildcards can really slow down the namenode. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Feb/14 08:47;rohini;PIG-3771-1.patch;https://issues.apache.org/jira/secure/attachment/12630632/PIG-3771-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-04-15 21:52:13.119,,,no_permission,,,,,,,,,,,,374577,Reviewed,,,Wed Apr 16 17:28:49 UTC 2014,,,,,,,0|i1sjsn:,374877,,,,,,,,,,15/Apr/14 19:14;rohini;Can someone review this?,"15/Apr/14 21:52;cheolsoo;I was wondering whether it's possible to change the key type of schemaToMergedSchemaMap from Path to URI to \[de\]serialize it directly, but it seems to require quite a few changes.

+1.",16/Apr/14 17:28;rohini;Committed to trunk. Thanks Cheolsoo for the review.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
auto local mode selection does not check lower bound for size,PIG-3755,12694048,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,aniket486,aniket486,aniket486,08/Feb/14 00:21,07/Jul/14 18:07,14/Mar/19 03:07,11/Feb/14 00:17,,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"Size base decision is made from following -
{code}
long totalInputFileSize = InputSizeReducerEstimator.getTotalInputFileSize(conf, lds, job);
{code}
We should add a check in case InputSizeReducerEstimator.getTotalInputFileSize returns -1. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Feb/14 00:51;aniket486;PIG-3755.patch;https://issues.apache.org/jira/secure/attachment/12627756/PIG-3755.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-08 02:18:09.82,,,no_permission,,,,,,,,,,,,372557,,,,Mon Feb 10 22:59:10 UTC 2014,,,,,,,0|i1s7ef:,372861,,,,,,,,,,08/Feb/14 02:18;cheolsoo;+1,10/Feb/14 22:59;aniket486;Committed to trunk! Thanks [~cheolsoo] for the review.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InputSizeReducerEstimator.getTotalInputFileSize reports incorrect size,PIG-3754,12694047,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,aniket486,aniket486,aniket486,08/Feb/14 00:16,16/Sep/15 20:56,14/Mar/19 03:07,07/Mar/14 07:03,0.13.0,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"If you have more than one input, InputSizeReducerEstimator.getTotalInputFileSize can return incorrect value if one of the loader returns \-1 and is not file based (eg- hbase). This causes incorrect reducer estimation and problems in auto.local mode.

If size of input is not found in for any of the inputs, we should bail out with return value of -1.",,,,,,,,,,,,,,,,,,,,,,,,,PIG-4679,,,,,,,06/Mar/14 21:34;aniket486;PIG-3754-1.patch;https://issues.apache.org/jira/secure/attachment/12633228/PIG-3754-1.patch,07/Mar/14 07:00;aniket486;PIG-3754-2.patch;https://issues.apache.org/jira/secure/attachment/12633324/PIG-3754-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-03-06 22:15:37.687,,,no_permission,,,,,,,,,,,,372556,,,,Fri Mar 07 07:03:34 UTC 2014,,,,,,,0|i1s7e7:,372860,,,,,,,,,,06/Mar/14 22:15;cheolsoo;Looks good. Would you mind adding a test case for this corner case to TestInputSizeReducerEstimator?,06/Mar/14 22:20;aniket486;Will do.,07/Mar/14 01:23;julienledem;LGTM too,07/Mar/14 07:01;aniket486;Attached PIG-3754-2.patch with tests. Will commit to trunk.,07/Mar/14 07:03;aniket486;Committed to trunk. Thanks [~cheolsoo] and [~julienledem] for the review.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LOGenerate generates null schema,PIG-3753,12693947,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,07/Feb/14 17:45,15/Apr/14 20:45,14/Mar/19 03:07,08/Feb/14 18:24,,,,,,,,0.12.1,0.13.0,,,impl,,,0,,,,,,,,,,,,,"In PIG-3627, we discover flatten(STRSPLIT) generate null schema. 
Here is the sample:
{code}
a = load 'a' as (line:chararray);
b = foreach a generate flatten(STRSPLIT(line)) as (i0, i1, i2);
describe b;

b: {i0: NULL,i1: NULL,i2: NULL}
{code}
PIG-3627 fixed the JsonStorage to deal with null schema. In this ticket, we will fix the null schema generation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Feb/14 17:47;daijy;PIG-3753-1.patch;https://issues.apache.org/jira/secure/attachment/12627657/PIG-3753-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-08 03:05:36.157,,,no_permission,,,,,,,,,,,,372456,Reviewed,,,Sat Feb 08 18:24:55 UTC 2014,,,,,,,0|i1s6s7:,372760,,,,,,,,,,"08/Feb/14 03:05;cheolsoo;+1.

Thank you Daniel for taking care of this.",08/Feb/14 18:24;daijy;Patch committed to trunk. Thanks Cheolsoo for review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix e2e Parallel test for Windows,PIG-3752,12693839,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,07/Feb/14 07:31,07/Jul/14 18:07,14/Mar/19 03:07,28/May/14 18:23,,,,,,,,0.13.0,,,,e2e harness,,,0,,,,,,,,,,,,,"Parallel e2e test does not work for Windows (either activePerl or strawberryPerl). This is because IPC::Run module has limitation under Win32 (http://search.cpan.org/~toddr/IPC-Run-0.92/lib/IPC/Run.pm#Win32_LIMITATIONS). It will run into crash or deadlock. Fortunately cygwin perl does not use Win32 subsystem, and it does not have this issue. So we want to solve this by using cygwin perl.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Feb/14 07:36;daijy;PIG-3752-1.patch;https://issues.apache.org/jira/secure/attachment/12627569/PIG-3752-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-05-28 15:40:01.452,,,no_permission,,,,,,,,,,,,372348,Reviewed,,,Wed May 28 18:23:33 UTC 2014,,,,,,,0|i1s647:,372652,,,,,,,,,,"07/Feb/14 07:36;daijy;Instruction for test environment:
1. Install cygwin 1.7.28+ (earlier version has a bug), minimum core + perl, dev module
2. Install perl module (Parallel::ForkManager, IPC::Run)
To install it, open a cygwin terminal, use ""cpan Parallel::ForkManager"" etc to install.
3. Need to put $CYGWIN_HOME/bin in PATH, after GnuWin32. Test will pick perl from cygwin, every other utilities still from GnuWin32.",28/May/14 15:40;arpitgupta;+1 we have been running tests in parallel on windows with this patch and have found no regressions in the framework.,28/May/14 16:58;rohini;+1,"28/May/14 18:23;daijy;Patch committed to both 0.13 branch and trunk. Thanks Arpit, Rohini for review!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update skewed join documentation,PIG-3747,12693491,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,06/Feb/14 03:44,07/Jul/14 18:08,14/Mar/19 03:07,18/Feb/14 19:26,,,,,,,,0.13.0,,,,documentation,,,0,,,,,,,,,,,,,"While working on Tez skewed join, I found two problems with the skewed join documentation-
# The following is misleading because skewed join also works with left, right, and full outer joins.
{quote}
Skewed join works with two-table inner join. 
{quote}
# In two-way join, the 1st table must be the skewed one because Pig samples on that. But this is not mentioned anywhere.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Feb/14 14:18;cheolsoo;PIG-3747-1.patch;https://issues.apache.org/jira/secure/attachment/12629537/PIG-3747-1.patch,18/Feb/14 18:37;cheolsoo;PIG-3747-2.patch;https://issues.apache.org/jira/secure/attachment/12629590/PIG-3747-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-02-18 17:04:23.688,,,no_permission,,,,,,,,,,,,372076,,,,Tue Feb 18 19:26:52 UTC 2014,,,,,,,0|i1s4gf:,372381,,,,,,,,,,18/Feb/14 14:18;cheolsoo;Uploading a patch.,"18/Feb/14 17:04;daijy;I think the right table name ""massive"" should also be changed. The right table is not necessarily to be massive.","18/Feb/14 18:37;cheolsoo;Thanks Daniel. I changed the table names in the example query-
{code}
A = LOAD 'skewed_data' AS (a1,a2,a3);
B = LOAD 'data' AS (b1,b2,b3);
C = JOIN A BY a1, B BY b1 USING 'skewed';
{code}",18/Feb/14 18:43;daijy;+1,18/Feb/14 19:26;cheolsoo;Committed to trunk. Thank you Daniel!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE is thrown if Pig fails before PigStats is intialized,PIG-3746,12693488,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,06/Feb/14 03:20,07/Jul/14 18:07,14/Mar/19 03:07,18/Feb/14 19:28,,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"This is a regression of PIG-3525 where I moved the initialization of PigStats into the PigServer constructor. The problem is that if Pig fails even before PigServer is created, PigStats remains null resulting in NPE in PigStatsUtil.
{code:title=PigStatsUtil}
    public static void setErrorMessage(String msg) {
        PigStats.get().setErrorMessage(msg); //---> PigStats.get() returns null
    }
    public static void setErrorCode(int code) {
        PigStats.get().setErrorCode(code); //---> PigStats.get() returns null
    }
{code}
To reproduce the issue, run the following command-
{code}
pig -x -f script.pig  #---> Note -x is missing the argument
{code}
The stack trace looks like this-
{code}
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.pig.tools.pigstats.PigStatsUtil.setErrorMessage(PigStatsUtil.java:145)
	at org.apache.pig.Main.run(Main.java:618)
	at org.apache.pig.Main.main(Main.java:156)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Feb/14 14:08;cheolsoo;PIG-3746-1.patch;https://issues.apache.org/jira/secure/attachment/12629534/PIG-3746-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-18 17:14:27.911,,,no_permission,,,,,,,,,,,,372073,,,,Tue Feb 18 19:28:15 UTC 2014,,,,,,,0|i1s4fr:,372378,,,,,,,,,,"18/Feb/14 14:08;cheolsoo;PigStats is not initialized means PigServer hasn't started yet. So setError\[Messages|Code|Throwable\]() can simply exit in that case.

Attached is a patch that adds null checks to setError\[Messages|Code|Throwable\]().",18/Feb/14 17:14;daijy;+1,18/Feb/14 19:28;cheolsoo;Committed to trunk. Thanks Daniel!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SequenceFileLoader does not support BytesWritable,PIG-3744,12693198,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,04/Feb/14 20:04,07/Jul/14 18:08,14/Mar/19 03:07,04/Feb/14 20:39,0.11.1,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"SequenceFileLoader should be referring to BytesWritable for bytearray type, but it refers to pig's DataByteArray which does not even implement Writable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Feb/14 20:20;rohini;PIG-3744-1.patch;https://issues.apache.org/jira/secure/attachment/12626963/PIG-3744-1.patch,04/Feb/14 20:39;rohini;PIG-3744-2.patch;https://issues.apache.org/jira/secure/attachment/12626965/PIG-3744-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-02-04 20:26:41.813,,,no_permission,,,,,,,,,,,,371784,,,,Tue Feb 04 20:39:44 UTC 2014,,,,,,,0|i1s2nb:,372083,,,,,,,,,,04/Feb/14 20:26;daijy;+1,04/Feb/14 20:39;rohini;Test failed while running before commit as a single quote in the LOAD statement got accidentally deleted before generating the patch. Fixed that in PIG-3744-2.patch,04/Feb/14 20:39;rohini;Committed to trunk. Thanks Daniel for the review.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Utils.setTmpFileCompressionOnConf can cause side effect for SequenceFileInterStorage,PIG-3741,12692791,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,aniket486,aniket486,03/Feb/14 07:30,15/Apr/14 20:44,14/Mar/19 03:07,05/Feb/14 23:08,,,,,,,,0.12.1,0.13.0,,,,,,0,,,,,,,,,,,,,"Currently, Utils.setTmpFileCompressionOnConf(pigContext, conf); is invoked for every job. In case of Seqfile, this api sets mapreduce params on conf to assist SequenceFileInterStorage. However, as a side effect, this might change the behavior of other storers due to these mapred properties. This api should only be called for jobs with intermediate storage.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Feb/14 08:36;aniket486;PIG-3741.patch;https://issues.apache.org/jira/secure/attachment/12626619/PIG-3741.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-04 19:02:22.015,,,no_permission,,,,,,,,,,,,371377,,,,Wed Feb 05 23:08:39 UTC 2014,,,,,,,0|i1s05z:,371680,,,,,,,,,,"04/Feb/14 19:02;julienledem;Ideally each store would get its own config object, but that would be a major refactoring.
In the meantime, this looks like a good improvement to me.
+1",05/Feb/14 23:08;aniket486;Committed to 0.12.1 branch and trunk. Thanks [~julienledem] for the review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The Warning_4 e2e test is broken in trunk,PIG-3739,12692696,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,cheolsoo,cheolsoo,02/Feb/14 05:15,07/Jul/14 18:08,14/Mar/19 03:07,31/May/14 17:17,,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"This is a regression of PIG-2207.

The Warning_4 e2e test fails because expected warning messages are not printed to stderr. I confirmed that the following lines are present w/o PIG-2207, whereas they are not w/ PIG-2207.
{code}
2014-02-02 04:48:11,211 [main] WARN  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Encountered Warning UDF_WARNING_3 10989 time(s).
2014-02-02 04:48:11,211 [main] WARN  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Encountered Warning UDF_WARNING_4 22 time(s).
{code}

Here is the test query-
{code}
register ./lib/java/testudf.jar;
a = load '/user/pig/tests/data/singlefile/studentnulltab10k' as (name, age: int, gpa: float);
b = foreach a generate org.apache.pig.test.udf.evalfunc.TestWarningFunc(name, age, gpa);
store b into '/user/pig/out/cheolsoop-1391202001-cmdline.conf-Warning/Warning_4.out';
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,31/May/14 00:19;aniket486;PIG-3739-1.patch;https://issues.apache.org/jira/secure/attachment/12647732/PIG-3739-1.patch,31/May/14 00:13;aniket486;PIG-3739-4.patch;https://issues.apache.org/jira/secure/attachment/12647730/PIG-3739-4.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-02-02 05:43:42.299,,,no_permission,,,,,,,,,,,,371291,,,,Sat May 31 17:17:29 UTC 2014,,,,,,,0|i1rzmv:,371594,,,,,,,,,,02/Feb/14 05:43;aniket486;This is expected due to backwards incompatibility of PIG-2207. I will submit a patch to fix the test.,"13/May/14 21:50;aniket486;New counters
|| Group || Name || Map || Reduce || Total ||
|org.​apache.​pig.​PigWarning|SKIP_UDF_CALL_FOR_NULL	| 3|	 0|	 3|
|org.​apache.​pig.​test.​udf.​evalfunc.​TestWarningFunc | UDF_WARNING_3|	 10980	| 0	| 10980|
|org.​apache.​pig.​test.​udf.​evalfunc.​TestWarningFunc | UDF_WARNING_4	| 19|	 0	| 19|

Old counters (with pig11)
|| Group || Name || Map || Reduce || Total ||
|org.​apache.​pig.​PigWarning	|UDF_WARNING_3	| 10989|	 0|	 10989|
|org.​apache.​pig.​PigWarning	|UDF_WARNING_4	 |22	 |0|	 22|

PIG-2207 caused the grouping to change. What's causing counter numbers to change? [~cheolsoo], [~daijy] comments?","14/May/14 04:05;cheolsoo;I think it has to do with PIG-3679 that made POUserFunc skip calling UDF for null tuples (i.e. all the fields are null).

# From the counter, there were 3 null tuples.
{code}
  SKIP_UDF_CALL_FOR_NULL = 3
+ UDF_WARNING_4          = 19
-----------------------------
  UDF_WARNING_4          = 22
{code}
# Tuples in the test have 3 fields ({{name, age: int, gpa: float}}). If you look at TestWarningFunc.java, UDF_WARNING_3 is incremented per field.
{code}
  UDF_WARNING_3 = 10989
- UDF_WARNING_3 = 10980
-----------------------------
                = 9
{code}
The difference is 9, so 9 / 3 = 3 tuples are skipped. This matches #1.",14/May/14 06:29;daijy;All null tuple is passed to UDF before PIG-3679. Seems only a single item tuple with null value is skipped.,"14/May/14 06:47;aniket486;Thanks [~cheolsoo] for looking into this. This makes sense, so, we need to fix two things - fix the test and fix computeWarningAggregate to print the new counters on stdout.","14/May/14 18:54;daijy;The goal of PIG-3679 is to bring backward compatibility, we shall not fix the test but need to bring old behavior. Hope this will do:
Change:
{code}
boolean allNulls = true;
for (int i = 0; i < t.size(); i++) {
    if (!t.isNull(i)) {
        allNulls = false;
        break;
    }
}
if (allNulls) {
  // skip
  ...
}
{code}
to something like:
{code}
if (t.size()==1 && t.isNull(i)) {
  //skip
  ...
} else {
  pass to UDF
}
{code}","29/May/14 20:28;aniket486;Thanks [~daijy], I have attached the suggested fix at PIG-3679.",30/May/14 05:41;aniket486;PIG-2207 changed group of WARNINGs from PigWarning(enum) to class of udf (string). This breaks computeWarningAggregate and hence no warning messages are displayed on stdout. We can get list of udfs for a job from jcc.getJobMroMap().get(job).UDFs (private->public or getter) and then iterate over udfs and get all the counters (not ideal). Comments?,30/May/14 07:12;daijy;That's fine as long as we skip the standard Hadoop counters.,"31/May/14 00:16;aniket486;I tried PIG-3739-4.patch, but it turned out tricky. By the time, we query for counters, we lose mapping between jobs and MROs (and hence the udfs). Quickest way to fix this would be get the backwards compatible behavior from reverting a part of PIG-2207. Testing it.",31/May/14 00:20;aniket486;Please review PIG-3739-1.patch that reverts a part of the backwards incompatible behavior in PIG-2207.,"31/May/14 00:29;cheolsoo;+1 to {{PIG-3739-1.patch}}. Let's get 0.13 out.

Btw, I am curious whether it's okay to break backward compatibility of counters. I don't think it's necessary to keep counters backward compatible, but that's a separate discussion.

",31/May/14 17:17;aniket486;Committed to trunk and branch 0.13! Thanks [~cheolsoo] and [~daijy].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bundle dependent jars in distribution in %PIG_HOME%/lib folder,PIG-3737,12692556,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,shuainie,shuainie,31/Jan/14 18:20,07/Jul/14 18:08,14/Mar/19 03:07,24/Apr/14 22:49,,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,Pig should bundle with dependencies like avro.jar and json-simple.jar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Apr/14 00:07;daijy;PIG-3737-2.patch;https://issues.apache.org/jira/secure/attachment/12641619/PIG-3737-2.patch,31/Jan/14 19:05;shuainie;PIG-3737.1.patch;https://issues.apache.org/jira/secure/attachment/12626351/PIG-3737.1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-01-31 19:10:54.595,,,no_permission,,,,,,,,,,,,371151,Reviewed,,,Thu Apr 24 22:49:10 UTC 2014,,,,,,,0|i1rys7:,371455,,,,,,,,,,"31/Jan/14 19:10;daijy;Actually not just avro/json jars. We don't ship dependent jars (except jython.jar) in Pig releases, user need to find all those jars themselves. I would suggest to bundle more jars in Pig distribution. I am thinking of two ways:
1. bundle all build dependent jars in distribution
2. hand pick several jars in distribution (hbase*.jar, json*.jar, avro*.jar, etc)

#1 is apparently more maintainable, however, more jars means more chance user get a jar conflict. Thoughts?","14/Apr/14 16:41;cheolsoo;I am in favor of #2 to be safe (i.e. avoiding jar conflicts).

Can we add groovy jars to the distribution? Someone was complaining about a ClassNotFoundException while using a groovy UDF.","14/Apr/14 19:02;daijy;Thanks for input. Let's compile a list:
hbase.jar (HBaseStorage)
protobuf.jar (HbaseStorage)
json-simple.jar (JsonLoader/JsonStorage)
avro*.jar (AvroStorage)
accumulo*.jar (AccumuloStorage)
groovy.jar(Groovy)
jython.jar (Jython)
jruby.jar (JRuby)
js*.jar (JavaScript)",15/Apr/14 17:40;cheolsoo;Thank you Daniel. That looks like a good list to me.,24/Apr/14 00:07;daijy;Extend [~shuainie]'s patch to other jars.,24/Apr/14 16:34;cheolsoo;+1,24/Apr/14 22:49;daijy;Patch committed to trunk. Thanks Cheolsoo for review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance issue in SelfSpillBag,PIG-3730,12692067,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rajesh.balamohan,rajesh.balamohan,rajesh.balamohan,29/Jan/14 16:06,07/Jul/14 18:08,14/Mar/19 03:07,30/Jan/14 16:39,0.11,,,,,,,0.13.0,,,,impl,,,0,,,,,,,,,,,,,"We have bunch of joins in our pig scripts (joining 5 to 15 datasets together).  Pig creates a bunch of REPLICATED, HASH_JOINs and we observed heavy performance degradation in one of the launched M/R job.  This was specifically on the reducer side.  Taking multiple threaddumps revealed the following

""main"" prio=10 tid=0x00007fbaa801c000 nid=0x1464 runnable [0x00007fbaaee76000]
   java.lang.Thread.State: RUNNABLE
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1781)
	- locked <0x00000000b5316370> (a org.apache.hadoop.mapred.JobConf)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:712)
	at org.apache.pig.data.SelfSpillBag$MemoryLimits.init(SelfSpillBag.java:73)
	at org.apache.pig.data.SelfSpillBag$MemoryLimits.<init>(SelfSpillBag.java:65)
	at org.apache.pig.data.SelfSpillBag.<init>(SelfSpillBag.java:39)
	at org.apache.pig.data.InternalCachedBag.<init>(InternalCachedBag.java:63)
	at org.apache.pig.data.InternalCachedBag.<init>(InternalCachedBag.java:59)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POJoinPackage.getNext(POJoinPackage.java:146)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:422)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:405)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:257)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:164)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:610)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:444)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:268)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
	at org.apache.hadoop.mapred.Child.main(Child.java:262)

at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1781)
	- locked <0x00000000b5316388> (a org.apache.hadoop.mapred.JobConf)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:712)
	at org.apache.pig.data.SelfSpillBag$MemoryLimits.init(SelfSpillBag.java:73)
	at org.apache.pig.data.SelfSpillBag$MemoryLimits.<init>(SelfSpillBag.java:65)
	at org.apache.pig.data.SelfSpillBag.<init>(SelfSpillBag.java:39)
	at org.apache.pig.data.InternalCachedBag.<init>(InternalCachedBag.java:63)
	at org.apache.pig.data.InternalCachedBag.<init>(InternalCachedBag.java:59)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POJoinPackage.getNext(POJoinPackage.java:146)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:422)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:405)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:257)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:164)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:610)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:444)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:268)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
	at org.apache.hadoop.mapred.Child.main(Child.java:262)

In certain corner cases (where pig.cachedbag.type is not ""default""), InternalCachedBag is initialized in POJoinPackage.  

InternalCachedBag internally calls SelfSpillBag--> MemoryLimits --> PigMapReduce.sJobConfInternal.get().get(
                        PigConfiguration.PROP_CACHEDBAG_MEMUSAGE);

Since this is happening very frequently, the cost of Configuration.get()  itself is increasing causing the degradation.  Here is the counters snippet from one of the reducer.

E.g : counter snippet from a reducer
        FILE: Number of bytes read 	57,762,717
	FILE: Number of bytes written 	25,256,417
        HDFS: Number of bytes read 	0
	HDFS: Number of bytes written 	2,521,311
	HDFS: Number of read operations 	0
	HDFS: Number of large read operations 	0
	HDFS: Number of write operations 	1

 	Reduce input groups 	4,282,722
	Reduce shuffle bytes 	26,858,192
	Reduce input records 	4,912,881
	Reduce output records 	630,159
	Spilled Records 	4,912,881
",Pig 0.11 with MR-V1,,,,,,,,,,,,,,,,,,PIG-3456,,,,,,,,,,,,,29/Jan/14 16:12;rajesh.balamohan;PIG-3730-trunk-v1.patch;https://issues.apache.org/jira/secure/attachment/12625890/PIG-3730-trunk-v1.patch,29/Jan/14 22:26;rajesh.balamohan;PIG-3730-trunk-v2.patch;https://issues.apache.org/jira/secure/attachment/12625994/PIG-3730-trunk-v2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-01-29 17:33:22.149,,,no_permission,,,,,,,,,,,,370657,,,,Thu Jan 30 16:39:14 UTC 2014,,,,,,,0|i1rvsf:,370967,,,,,,,,,,29/Jan/14 16:12;rajesh.balamohan;v1 patch for trunk,29/Jan/14 17:33;rohini;Even long max = Runtime.getRuntime().maxMemory(); can be moved to the static block.,29/Jan/14 22:26;rajesh.balamohan;Thanks Rohini.  Uploading v2 patch for trunk.,"29/Jan/14 23:25;rohini;+1. Will commit after running tests. Also reminded me that I haven't put up the patch for PIG-3456, where I fixed same kind of Threadlocal configuration access in lot of other places.",30/Jan/14 00:13;rajesh.balamohan;Thanks a lot Rohini.,30/Jan/14 16:39;rohini;Committed to trunk. Thanks Rajesh.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ranking empty records leads to NullPointerException,PIG-3726,12691390,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jarcec,mtelizhyn,mtelizhyn,27/Jan/14 15:05,07/Jul/14 18:08,14/Mar/19 03:07,31/Jan/14 18:27,0.12.0,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"steps to reproduce:

1) create an empty file on hdfs (e.g. /user/root/test/empty.txt)
2) execute the following pig script:

records = LOAD '/user/root/test/empty.txt' AS (Amount: DOUBLE);
ranked = RANK records BY Amount DESC;
DUMP ranked;

3) on console you will get "" ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2043: Unexpected error during execution."" and in pig's logs you will see:

Pig Stack Trace
{code}
---------------
ERROR 2043: Unexpected error during execution.

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias ranked
at org.apache.pig.PigServer.openIterator(PigServer.java:880)
at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:774)
at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:372)
at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:198)
at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:173)
at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:84)
at org.apache.pig.Main.run(Main.java:607)
at org.apache.pig.Main.main(Main.java:156)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
Caused by: org.apache.pig.PigException: ERROR 1002: Unable to store alias ranked
at org.apache.pig.PigServer.storeEx(PigServer.java:982)
at org.apache.pig.PigServer.store(PigServer.java:942)
at org.apache.pig.PigServer.openIterator(PigServer.java:855)
... 12 more
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 2043: Unexpected error during execution.
at org.apache.pig.PigServer.launchPlan(PigServer.java:1333)
at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1307)
at org.apache.pig.PigServer.storeEx(PigServer.java:978)
... 14 more
Caused by: java.lang.RuntimeException: Error to read counters into Rank operation counterSize 0
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.saveCounters(JobControlCompiler.java:386)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.updateMROpPlan(JobControlCompiler.java:332)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:387)
at org.apache.pig.PigServer.launchPlan(PigServer.java:1322)
... 16 more
Caused by: java.lang.NullPointerException
at org.apache.hadoop.mapreduce.counters.Limits.filterName(Limits.java:44)
at org.apache.hadoop.mapreduce.counters.Limits.filterGroupName(Limits.java:52)
at org.apache.hadoop.mapreduce.counters.AbstractCounters.getGroup(AbstractCounters.java:220)
at org.apache.hadoop.mapred.Counters.getGroup(Counters.java:113)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.saveCounters(JobControlCompiler.java:360)
... 19 more
================================================================================
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Jan/14 03:19;jarcec;PIG-3726.patch;https://issues.apache.org/jira/secure/attachment/12625779/PIG-3726.patch,28/Jan/14 05:17;jarcec;PIG-3726.patch;https://issues.apache.org/jira/secure/attachment/12625523/PIG-3726.patch,27/Jan/14 15:08;mtelizhyn;emptyRecordsNPE.patch;https://issues.apache.org/jira/secure/attachment/12625366/emptyRecordsNPE.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-01-27 15:36:55.322,,,no_permission,,,,,,,,,,,,370135,Reviewed,,,Fri Jan 31 18:54:29 UTC 2014,,,Patch Available,,,,0|i1rsl3:,370437,,,,,,,,,,27/Jan/14 15:36;jarcec;I've noticed the same problem as well and have very similar patch to this one :-),27/Jan/14 15:49;mtelizhyn;Hi Jarek! You are welcome to submit you patch here :).  ,"28/Jan/14 05:17;jarcec;Sure [~mtelizhyn], here is patch that I was working on just yesterday!  As you can see, it's very similar to yours :-)

The reason why I did not submitted it yet is that I wasn't sure whether my (our) approach is entirely correct. Method {{saveCounters}} is written in a way to throw an exception if the RANK counters are not present. I'm assuming that the counters might be missing from many reasons and the only valid case that we want to cover is when the input relation is empty. I wanted poke around a bit to see if that is indeed the case. Will be happy to hear others thoughts.","28/Jan/14 08:30;mtelizhyn;Sure [~jarcec], I think your approach is covering better the case when the input relation is empty. So please free to commit your patch.",29/Jan/14 03:19;jarcec;Thank you for your feedback [~mtelizhyn]. I went ahead and added the check to ensure that the only acceptable option for missing RANK counters is when the relation is empty.,31/Jan/14 18:27;daijy;+1. Patch committed to trunk. Thanks Jarek!,31/Jan/14 18:54;jarcec;Thank you [~daijy] and [~mtelizhyn]!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Udf deserialization for registered classes fails in local_mode,PIG-3722,12691160,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,aniket486,aniket486,24/Jan/14 23:40,07/Jul/14 18:07,14/Mar/19 03:07,30/Jan/14 20:13,0.13.0,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"Similar to https://issues.apache.org/jira/browse/PIG-2532, registered classes are not available if jobs are converted to local_mode.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Jan/14 01:31;aniket486;PIG-3722.patch;https://issues.apache.org/jira/secure/attachment/12625162/PIG-3722.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-30 19:53:16.452,,,no_permission,,,,,,,,,,,,369903,,,,Thu Jan 30 20:13:07 UTC 2014,,,,,,,0|i1rr5r:,370205,,,,,,,,,,"25/Jan/14 01:29;aniket486;This happens because ObjectInputStream doesn't take into consideration Thread ContextClassLoader when deserializing hence we get following stack trace in local-mode backend-
{noformat}
2014-01-24 08:30:33,260 WARN org.apache.hadoop.mapred.LocalJobRunner: job_local_0002
java.io.IOException: Deserialization error: org.apache.hcatalog.data.schema.HCatSchema
 at org.apache.pig.impl.util.ObjectSerializer.deserialize(ObjectSerializer.java:59)
 at org.apache.pig.impl.util.UDFContext.deserialize(UDFContext.java:192)
 at org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil.setupUDFContext(MapRedUtil.java:173)
 at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.setupUdfEnvAndStores(PigOutputFormat.java:229)
 at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.getOutputCommitter(PigOutputFormat.java:275)
 at org.apache.hadoop.mapred.Task.initialize(Task.java:511)
 at org.apache.hadoop.mapred.MapTask.run(MapTask.java:306)
 at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:210)
{noformat}

To fix this, we can use [ClassLoaderObjectInputStream|http://commons.apache.org/proper/commons-io/apidocs/org/apache/commons/io/input/ClassLoaderObjectInputStream.html].","27/Jan/14 07:22;aniket486;I tested this on a production job, it works well.",30/Jan/14 19:53;dvryaboy;+1,30/Jan/14 20:13;aniket486;Committed to trunk. Thanks [~dvryaboy] for the review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroStorage is not filtering hidden files on directory recursion,PIG-3717,12690788,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jarcec,jarcec,jarcec,23/Jan/14 15:44,07/Jul/14 18:07,14/Mar/19 03:07,24/Jan/14 22:25,0.12.0,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"We've noticed that when we generate couple of sub-directories with Oozie and then use AvroStorage (the older version from PiggyBank) to load those directories, hidden files such as {{_SUCCESS}} won't get ignored as they should be and in turn will cause failures in the Pig job. This seems to be regression caused by PIG-3223.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Jan/14 17:24;jarcec;PIG-3717.patch;https://issues.apache.org/jira/secure/attachment/12624831/PIG-3717.patch,23/Jan/14 15:51;jarcec;PIG-3717.patch;https://issues.apache.org/jira/secure/attachment/12624813/PIG-3717.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-01-23 18:14:30.94,,,no_permission,,,,,,,,,,,,369526,,,,Fri Jan 24 22:25:43 UTC 2014,,,,,,,0|i1rovb:,369831,,,,,,,,,,23/Jan/14 18:14;xuefuz;+1,23/Jan/14 20:50;rohini;I actually fixed this and couple of other issues with AvroStorage as part of PIG-3661. Also did some cleanup w.r.t to getAllSubDirs() method. Had the patch done for 0.11. Will rebase and put up the patch soon. ,23/Jan/14 20:57;jarcec;Do you think that we can go ahead and commit this one separately or would you prefer to cover it by your patch [~rohini]?,23/Jan/14 21:09;rohini;Will cover in my patch as I have got rid of the getAllSubDirs() method itself.,23/Jan/14 21:31;jarcec;Thank you for the feedback [~rohini]!,24/Jan/14 22:25;rohini;Fix committed as part of PIG-3661.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mvn-inst target does not install pig-h2.jar into local .m2,PIG-3682,12690409,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,raluri,raja@cmbasics.com,raja@cmbasics.com,22/Jan/14 21:22,07/Jul/14 18:08,14/Mar/19 03:07,30/Jan/14 01:10,0.12.0,,,,,,,0.13.0,,,,build,,,0,,,,,,,,,,,,,"ant target 'mvn-inst' does not install pig-h2.jar into local .m2, where as deploy does.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Jan/14 21:26;raja@cmbasics.com;0001-PIG-3682-mvn-inst-target-does-not-install-pig-h2.jar.patch;https://issues.apache.org/jira/secure/attachment/12624434/0001-PIG-3682-mvn-inst-target-does-not-install-pig-h2.jar.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-23 05:55:32.798,,,no_permission,,,,,,,,,,,,369361,,,,Thu Jan 30 01:10:40 UTC 2014,,,,,,,0|i1rnu7:,369664,,,,,,,,,,23/Jan/14 05:55;aniket486;+1,30/Jan/14 01:10;aniket486;Committed to trunk! Thanks [~raluri].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e2e StreamingPythonUDFs_10 fails in trunk,PIG-3679,12690085,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,21/Jan/14 19:05,07/Jul/14 18:08,14/Mar/19 03:07,30/May/14 00:48,,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"The e2e test StreamingPythonUDFs_10 fails in trunk with NPE-
{code}
Caused by: java.lang.NullPointerException
        at org.apache.pig.builtin.DoubleRound.exec(DoubleRound.java:45)
{code}
The test query is as follows-
{code}
a = load '/user/pig/tests/data/singlefile/allscalar10k' using PigStorage() as (name:chararray, age:int, gpa:double, instate:chararray);
b = foreach a generate name, ((double)ROUND((instate=='true'?gpa:gpa+1)*10000)) / 10000.0;
store b into '/user/pig/out/cheolsoop-1390330024-nightly.conf-StreamingPythonUDFs/StreamingPythonUDFs_10_benchmark.out';
{code}",,,,,,,,,,,,,,,,,,,,,,,,,PIG-3739,,,,,,,21/Jan/14 19:38;cheolsoo;PIG-3679-1.patch;https://issues.apache.org/jira/secure/attachment/12624182/PIG-3679-1.patch,03/Feb/14 06:10;cheolsoo;PIG-3679-2.patch;https://issues.apache.org/jira/secure/attachment/12626609/PIG-3679-2.patch,03/Feb/14 18:37;daijy;PIG-3679-3.patch;https://issues.apache.org/jira/secure/attachment/12626708/PIG-3679-3.patch,06/Feb/14 03:08;cheolsoo;PIG-3679-4.patch;https://issues.apache.org/jira/secure/attachment/12627282/PIG-3679-4.patch,24/Feb/14 08:25;cheolsoo;PIG-3679-5.patch;https://issues.apache.org/jira/secure/attachment/12630627/PIG-3679-5.patch,25/Feb/14 21:35;cheolsoo;PIG-3679-6.patch;https://issues.apache.org/jira/secure/attachment/12631047/PIG-3679-6.patch,27/Feb/14 06:41;cheolsoo;PIG-3679-7.patch;https://issues.apache.org/jira/secure/attachment/12631464/PIG-3679-7.patch,29/May/14 20:13;aniket486;PIG-3679-backward-compatibility.patch;https://issues.apache.org/jira/secure/attachment/12647434/PIG-3679-backward-compatibility.patch,,,,8.0,,,,,,,,,,,,,,,,,,,2014-01-21 22:51:01.982,,,no_permission,,,,,,,,,,,,369042,,,,Fri May 30 00:48:53 UTC 2014,,,,,,,0|i1rlw7:,369347,,,,,,,,,,"21/Jan/14 19:38;cheolsoo;The problem is that random-generated allscalar10k may contain null values for instate and gpa columns. For eg,
{code}
(fred hernandez,44,0.28,)
(calvin brown,22,1.73,)
(priscilla carson,71,0.0,)
{code}
That causes ""instate=='true'?gpa:gpa+1"" to become null.

Attaching a fix that adds a filter-by that filters out null values on those two columns. Confirmed the test passes with the fix.",21/Jan/14 22:51;daijy;A little suprised why we not see it before. We should always has some null in instate and gpa.,21/Jan/14 22:55;cheolsoo;Good question. I don't know.,"25/Jan/14 00:36;rohini;In 0.11, null or non-numbers are ignored.

ROUND.java
{code}
try{
            Double d =  DataType.toDouble(input.get(0));
		    return Math.round(d);
        } catch (NumberFormatException nfe){
            System.err.println(""Failed to process input; error - "" + nfe.getMessage());
            return null;
        } catch (Exception e){
            throw new IOException(""Caught exception processing input row "", e);
        }
{code}","25/Jan/14 00:43;cheolsoo;But ROUND.java hasn't changed in trunk, has it?","03/Feb/14 06:10;cheolsoo;OK, I found this is a regression of PIG-3568 (Define the semantics of POStatus.STATUS_NULL). If I drop PIG-3568 in trunk, the test passes. In shorts, the exec() in DoubleRound was never called with nulls in 0.12, whereas it is in trunk resulting in NPE.

In terms of the fix, I think we should explicitly handle NullPointerException in addition to NumberFormatException in ROUND, DoubleRound, and FloatRound. Currently, if input.get(0) returns null, Math.round() will throw NPE.

I am attaching a new patch that adds a null check before calling Math.round(). I also copied the handler for NumberFormatException from ROUND to DoubleRound and FloadRound since it was missing in those functions.","03/Feb/14 06:42;daijy;Thanks for pursuing this.

Did a quick check, seems I cannot find another UDF like this. +1 for the patch.","03/Feb/14 07:40;rohini; If the previous behavior was to not execute in case of null and if we are executing now, isn't that a big behavior change? There might be other user written UDFs which might encounter issues even if there are no builtin ones. ","03/Feb/14 17:37;daijy;Yes, agree this might break some UDF. We should bring the old behavior.",03/Feb/14 18:37;daijy;Put up a patch to restore old behavior.,03/Feb/14 19:01;rohini;Wouldn't it be better for us to check for POStatus.STATUS_NULL instead of temp.result == null ?,"03/Feb/14 19:06;daijy;With Mark's change, POStatus.STATUS_NULL is changed into POStatus.STATUS_OK with result.result == null. This is actually more natural, however, we need to bring backward compatibility for POUserFunc.","05/Feb/14 21:58;cheolsoo;[~daijy], unfortunately, you patch introduces another issue. The e2e test still fails but for a different reason. Here is the stack trace-
{code}
org.apache.pig.backend.executionengine.ExecException: ERROR 0: Failed adding input to inputQueue
    at org.apache.pig.impl.builtin.StreamingUDF.getOutput(StreamingUDF.java:328)
    at org.apache.pig.impl.builtin.StreamingUDF.exec(StreamingUDF.java:150)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:328)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNextDouble(POUserFunc.java:394)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:318)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:378)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:298)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:282)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:277)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
    at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:771)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:375)
    at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1132)
    at org.apache.hadoop.mapred.Child.main(Child.java:249)
Caused by: java.lang.NullPointerException
    at java.util.concurrent.ArrayBlockingQueue.checkNotNull(ArrayBlockingQueue.java:145)
    at java.util.concurrent.ArrayBlockingQueue.put(ArrayBlockingQueue.java:319)
    at org.apache.pig.impl.builtin.StreamingUDF.getOutput(StreamingUDF.java:326)
    ... 17 more
{code}
To be clear, I don't think POUserFunc used to filter out nulls before PIG-3568 because it didn't modify POUserFunc. I believe\(?\) it's one of expression operators that filtered out nulls in the following plan-
{code}
    |   POUserFunc(org.apache.pig.builtin.DoubleRound)[long] - scope-25
    |   |   
    |   |---Multiply[double] - scope-24
    |       |   
    |       |---POBinCond[double] - scope-21
    |       |   |   
    |       |   |---Equal To[boolean] - scope-15
    |       |   |   |   
    |       |   |   |---Project[chararray][2] - scope-13
    |       |   |   |   
    |       |   |   |---Constant(true) - scope-14
    |       |   |   
    |       |   |---Project[double][1] - scope-16
    |       |   |   
    |       |   |---Add[double] - scope-20
    |       |       |   
    |       |       |---Project[double][1] - scope-17
    |       |       |   
    |       |       |---Cast[double] - scope-19
    |       |           |   
    |       |           |---Constant(1) - scope-18
    |       |   
    |       |---Cast[double] - scope-23
    |           |   
    |           |---Constant(10000) - scope-22
{code}
I am not entirely sure whether we can bring the old behavior in every case where PIG-3568 breaks backward compatibility. We discovered this particular one, but we don't know how many cases like this exist.","06/Feb/14 03:08;cheolsoo;I am uploading yet another patch that fixes the e2e test. This time, I took Daniel's patch and added a null check to the exec() of StreamingUDF.

On a second thought about backward incompatibility of PIG-3568, looks like we're safe with changing POUserFunc to filter out nulls for the following reasons-
# The changes in all the other PO operators except POUserFunc should be transparent to users because in the end, the result remains unchanged. The only difference is whether nulls travel the full or part of pipeline.
# Since POUserFunc now filters out nulls, some UDFs that used to throw NPE will no longer throw NPE (e.g. DoubleRound). But this is more of an improvement than an backward incompatible change.

So I will commit PIG-3679-4.patch if there is no objection. Thanks!
",06/Feb/14 06:12;daijy;+1,"06/Feb/14 21:18;cheolsoo;Actually, the latest patch is still no good. I see a dozen of e2e test cases fail with NPE. To name a few, EvalFunc_5, Types_3, 4, and more. Apparently, short-circuiting nulls in POUserFunc has a side effect.

I will need to debug them more carefully once I get back to work. Hard to debug them in a hotel room. Canceling patch for now.","24/Feb/14 08:25;cheolsoo;Here is yet another attempt to fix the issue. I confirmed that all the affected test cases pass now.

This time I made the logic more explicitly such that the invocation of UDF will be skipped only if all the fields in the input tuple are null. I am running full e2e test suites.","25/Feb/14 21:35;cheolsoo;I fixed one more minor issue in the latest patch (PIG-3679-6.patch).

If the output schema of UDF is tuple, POUserFunc should return a tuple whose fields are null instead of a null. I ran into this issue with ""Scripting_5"", and I fixed it in the new patch.

All e2e test cases pass except a known failure (Warning_4). Ready for review.","26/Feb/14 18:15;cheolsoo;RB link-
https://reviews.apache.org/r/18525/",27/Feb/14 06:14;rohini;+1,27/Feb/14 06:41;cheolsoo;Committed to trunk. Thank you Rohini for the review!,"29/May/14 18:43;aniket486;During investigation of PIG-3739, we identified that this still does not bring back the full backwards compatibility. ","29/May/14 20:27;aniket486;I tried fix as per Daniel's suggestion (PIG-3739), I have attached PIG-3679-backward-compatibility.patch with fix. [~daijy] can you check e2e after this fix?

Results of manual testing-

Old counters (with pig11)
|| Group || Name || Map || Reduce || Total ||
|org.​apache.​pig.​PigWarning	|UDF_WARNING_3	| 10989|	 0|	 10989|
|org.​apache.​pig.​PigWarning	|UDF_WARNING_4	 |22	 |0|	 22|

New counters (with the fix)
New counters
|| Group || Name || Map || Reduce || Total ||
|org.​apache.​pig.​test.​udf.​evalfunc.​TestWarningFunc | UDF_WARNING_3|	 10989	| 0	| 10989|
|org.​apache.​pig.​test.​udf.​evalfunc.​TestWarningFunc | UDF_WARNING_4	| 22|	 0	| 22|
","29/May/14 20:31;cheolsoo;+1 to {{PIG-3679-backward-compatibility.patch}}. Thank you Aniket for fixing it.

",29/May/14 20:32;cheolsoo;I just ran StreamingPythonUDFs_10. It passes since the issue was with a tuple with a single null field.,29/May/14 20:36;daijy;+1,30/May/14 00:48;cheolsoo;Committed the addendum patch to both trunk and branch-0.13.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConfigurationUtil.getLocalFSProperties can return an inconsistent property set,PIG-3677,12689600,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,17/Jan/14 23:42,15/Apr/14 20:44,14/Mar/19 03:07,23/Jan/14 05:57,0.11.1,,,,,,,0.12.1,0.13.0,,,,,,0,,,,,,,,,,,,,"From  [~jlowe]:

Our integration tests with a recent version of Hadoop 2.2 and pig were failing with these kinds of stacktraces for replicated join:

2014-01-17 17:20:41,811 [main] ERROR
org.apache.pig.tools.pigstats.SimplePigStats - ERROR 2997: Unable to recreate
exception from backed error: AttemptID:attempt_1389973251957_0127_m_000000_3
Info:Error: org.apache.pig.backend.executionengine.ExecException: ERROR 2081:
Unable to setup the load function.
    at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad.getNext(POLoad.java:125)
    at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:308)
    at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNext(POLocalRearrange.java:263)
    at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin.setUpHashMap(POFRJoin.java:398)
    at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin.getNext(POFRJoin.java:231)
    at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:286)
    at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:281)
    at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
    at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:165)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1562)
    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:160)
Caused by: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input
path does not exist:
hdfs://cluster-nn1.yahoo.com:8020/user/hitusr_1/pigrepl_scope-24_21617961_1389979200720_1
    at
org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:285)
    at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigFileInputFormat.listStatus(PigFileInputFormat.java:37)
    at
org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:340)
    at org.apache.pig.impl.io.ReadToEndLoader.init(ReadToEndLoader.java:190)
    at org.apache.pig.impl.io.ReadToEndLoader.<init>(ReadToEndLoader.java:146)
    at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad.setUp(POLoad.java:93)
    at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad.getNext(POLoad.java:121)
    ... 15 more


Debugged this to find that ConfigurationUtil.getLocalFSProperties can return a
property set where fs.default.name=file:/// but fs.defaultFS was
hdfs://cluster-nn1.yahoo.com:8020.  Pig is bypassing
Configuration.set(), so deprecated names aren't being handled properly.  Later
when we try to take these properties and poke them into a Configuration object,
if we try to set fs.defaultFS *after* fs.default.name then both will end up as
hdfs://... and it breaks.

We could have pig also set fs.defaultFS.  Problem is that if fs.default.name
gets yet another alias or is itself deprecated later then pig will have to
change or we can break again.  In that sense, another fix is to assert that
Configuration.isDeprecated(""fs.default.name"") is false and then filter out any
property where Configuration.isDeprecated() is true.  Then we're left with
settings that won't clobber each other when we go to build the configuration
later.

Thanks [~jlowe] for helping with debugging this. We had a hard time figuring this out as it failed with only 2.2 version of hadoop(+ few patches) and we don't have an answer to how we did not encounter this issue long ago with hadoop 0.23 and earlier versions of 2.x.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Jan/14 19:19;rohini;PIG-3677-1.patch;https://issues.apache.org/jira/secure/attachment/12624401/PIG-3677-1.patch,22/Jan/14 19:30;rohini;PIG-3677-2.patch;https://issues.apache.org/jira/secure/attachment/12624403/PIG-3677-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-01-23 05:49:47.674,,,no_permission,,,,,,,,,,,,368567,,,,Mon Jan 27 08:02:36 UTC 2014,,,,,,,0|i1riyf:,368871,,,,,,,,,,22/Jan/14 19:19;rohini;Doing a unset which takes care of removing all aliased configuration properties.,22/Jan/14 19:30;rohini;Changed to use set instead of unset which is more simple.,"22/Jan/14 19:33;rohini;Note: If HADOOP-9725 gets fixed and if ""fs.default.name"" is a final parameter, this will break. There are other places in code like WeightedRangePartitioner, SkewedPartitioner, etc also that do conf.set(""fs.default.name"") directly and will break. ",23/Jan/14 05:49;aniket486;Good catch! +1.,23/Jan/14 05:57;rohini;Committed to trunk. Thanks Aniket for the review.,27/Jan/14 07:35;daijy;Can we port it to 12.1?,27/Jan/14 08:02;rohini;Committed to branch-0.12 as well.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation for AccumuloStorage,PIG-3675,12689440,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,elserj,daijy,daijy,17/Jan/14 06:41,07/Jul/14 18:07,14/Mar/19 03:07,20/Feb/14 19:53,,,,,,,,0.13.0,,,,documentation,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PIG-3573,19/Feb/14 23:00;elserj;0001-PIG-3675-Initial-documentation-for-AccumuloStorage.patch;https://issues.apache.org/jira/secure/attachment/12629923/0001-PIG-3675-Initial-documentation-for-AccumuloStorage.patch,19/Feb/14 23:27;elserj;0001-PIG-3675-Initial-documentation-for-AccumuloStorage.patch.2;https://issues.apache.org/jira/secure/attachment/12629925/0001-PIG-3675-Initial-documentation-for-AccumuloStorage.patch.2,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-01-20 17:58:58.075,,,no_permission,,,,,,,,,,,,368407,Reviewed,,,Thu Feb 20 19:53:20 UTC 2014,,,,,,,0|i1rhyv:,368711,,,,,,,,,,"20/Jan/14 17:58;elserj;I'd be happy to write up some docs; what did you have in mind? Something on http://pig.apache.org/docs/?

If you can point me to where the doc source is, I'd be happy to supply at patch.","14/Feb/14 04:59;elserj;Bump, [~daijy]?","14/Feb/14 06:23;daijy;[~elserj], you can refer to HBaseStorage at http://pig.apache.org/docs/r0.12.0/func.html.",14/Feb/14 06:24;daijy;The source is in http://svn.apache.org/viewvc/pig/trunk/src/docs/src/documentation/content/xdocs/func.xml?view=markup,"14/Feb/14 18:40;elserj;Sweet -- thanks, [~daijy].",19/Feb/14 23:00;elserj;A first stab at AccumuloStorage documentation. Adds relevant section to func.xml. Did some contrived local tests using the example source blocks in the documentation.,19/Feb/14 23:27;elserj;Forgot that I wanted to add a note about adding Accumulo jars to PIG_CLASSPATH.,20/Feb/14 01:33;daijy;Is the patch ready?,"20/Feb/14 02:15;elserj;Yup, 0001-PIG-3675-Initial-documentation-for-AccumuloStorage.patch.2 is good to go.

",20/Feb/14 19:53;daijy;Patch committed to trunk. Thanks Josh!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix TestAccumuloPigCluster on Hadoop 2,PIG-3674,12689439,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,elserj,daijy,daijy,17/Jan/14 06:38,07/Jul/14 18:08,14/Mar/19 03:07,20/Feb/14 01:28,,,,,,,,0.13.0,,,,internal-udfs,,,0,,,,,,,,,,,,,Currently TestAccumuloPigCluster is disabled for Hadoop 2. We need to fix it and enable it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Feb/14 00:17;elserj;0001-PIG-3674-Remove-commented-timeout-and-update-exclude.patch;https://issues.apache.org/jira/secure/attachment/12629937/0001-PIG-3674-Remove-commented-timeout-and-update-exclude.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-19 23:57:45.14,,,no_permission,,,,,,,,,,,,368406,Reviewed,,,Thu Feb 20 01:28:29 UTC 2014,,,,,,,0|i1rhyn:,368710,,,,,,,,,,"19/Feb/14 23:57;elserj;Working my way back around to this: it seems to be a spurious failure. I haven't been able to track down what exactly is happening that's causing this. The exception I'm getting is

{code}
java.lang.NullPointerException
	at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:177)
	at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1678)
	at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1625)
	at org.apache.pig.PigServer.registerQuery(PigServer.java:615)
	at org.apache.pig.PigServer.registerQuery(PigServer.java:628)
	at org.apache.pig.backend.hadoop.accumulo.TestAccumuloPigCluster.test(TestAccumuloPigCluster.java:187)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
{code}","20/Feb/14 00:17;elserj;Both hadoop1 and hadoop2 fail when I try to put the timeout on the JUnit Test annotation, and both work when the timeout is not present. I'm guessing that the ScriptState ThreadLocal is getting initialized differently when JUnit is running the test with the timeout. Patch removes the test exclusion and removes the commented out timeout.",20/Feb/14 01:28;daijy;Verified it works. Patch committed to trunk. Thanks Josh!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Divide by zero error in runpigmix.pl script,PIG-3673,12689415,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,suhassatish,suhassatish,suhassatish,17/Jan/14 01:58,07/Jul/14 18:07,14/Mar/19 03:07,17/Mar/14 04:23,0.11.1,0.12.0,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"runpigmix.pl script has the following line which does not check against divide by zero. This could fail the launch script unexpectedly. 

my $multiplier = $pig_times/$mr_times;
",,,,,,,,,,,,,,,,,,,,,,,,,PIG-3895,,,,,,,17/Jan/14 01:59;suhassatish;PIG-3673.patch;https://issues.apache.org/jira/secure/attachment/12623558/PIG-3673.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-17 04:23:22.303,,,no_permission,,,,,,,,,,,,368382,Reviewed,,,Mon Mar 17 04:23:22 UTC 2014,,,Patch Available,,,,0|i1rhtb:,368686,,,,,,,,,,17/Mar/14 04:23;daijy;Patch committed to trunk. Thanks Suhas!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig should not check for hardcoded file system implementations,PIG-3672,12689398,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,suhassatish,suhassatish,16/Jan/14 23:30,07/Jul/14 18:07,14/Mar/19 03:07,30/Apr/14 14:01,0.10.0,0.11.1,0.12.0,,,,,0.13.0,,,,data,parser,,0,,,,,,,,,,,,,"QueryParserUtils.java has the code - 
result.add(""hdfs://""+thisHost+"":""+uri.getPort());

I propose to make it generic like - 
result.add(uri.getScheme() + ""://""+thisHost+"":""+uri.getPort());

Similarly jobControlCompiler.java has - 
if (!outputPathString.contains(""://"") || outputPathString.startsWith(""hdfs://"")) {

 I have a patch version which I ran passing unit tests on. Will be uploading it shortly.  ",,,,,,,,,,,,,,,,,PIG-3796,,,,,,,,,,,,,,,17/Jan/14 01:44;suhassatish;PIG-3672-1.patch;https://issues.apache.org/jira/secure/attachment/12623557/PIG-3672-1.patch,18/Jan/14 01:41;suhassatish;PIG-3672-2.patch;https://issues.apache.org/jira/secure/attachment/12623764/PIG-3672-2.patch,22/Apr/14 17:26;rohini;PIG-3672-3.patch;https://issues.apache.org/jira/secure/attachment/12641286/PIG-3672-3.patch,17/Jan/14 00:57;suhassatish;PIG-3672.patch;https://issues.apache.org/jira/secure/attachment/12623552/PIG-3672.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2014-01-17 15:32:21.136,,,no_permission,,,,,,,,,,,,368365,Reviewed,,,Wed Apr 30 14:01:01 UTC 2014,,,,,,,0|i00em7:,293,,,,,,,,,,17/Jan/14 00:58;suhassatish;This patch addresses the hdfs:// hardcoding and calls generic uri.getScheme() to get the file system implementation. ,"17/Jan/14 15:32;rohini;Few comments:
- WeightedRangePartitioner.java and MapredUtil.java
     Please do not add custom file formats. You can iterate through the configuration and if the key starts with ""fs."" and ends with "".impl"" set that in the new configuration so that it is generic. Wondering how this was working before for s3. 
- QueryParserUtils.java
    a) Can you add the Utils.hasFileSystemImpl check for the scheme before adding to remote host. hbase:// currently is skipped because there is no host. But this would future proof for other schemes that pig might support in future which has host as well but no filesystem implementation. 
    b) If har is the scheme, then you need to return hdfs:// in the remote host
- JobControlCompiler.java 
   !fileuri.getScheme().equals(""file:///"") condition is wrong. Even if file:// we should write to output directory. You can just change 
if (!outputPathString.contains(""://"") || outputPathString.startsWith(""hdfs://"")) { 
to 
if (Utils.hasFileSystemImpl(new Path(outputPathString), conf)) {
   That should take care of fixing PIG-1424 as well.","18/Jan/14 01:32;suhassatish;* Removed custom file formats from WeightedRangePartitioner.java and MapredUtil.java as I realized this was not really required and was not being used anywhere. 
* QueryParserUtils.java
** a) Do you mean to add the check at the code block shown in the diff below  ? The problem with this is that there is no access to Configuration object in QueryParserUtils.java. Do you have recommendations on how and where to instantiate a conf object to pass into Utils.hasFileSystemImpl as an argument?

{quote}
                  String scheme = uri.getScheme();
-                 if (scheme!=null && scheme.toLowerCase().equals(""hdfs"")||sc
+                 if (scheme!=null && Utils.hasFileSystemImpl(p,conf)) {
{quote}
** b) Is this currently being returned correctly for ""har"" in trunk? If so,  this change would handle it right?
{quote}
                 if (scheme!=null && Utils.hasFileSystemImpl(p,conf)) {
                     if (uri.getHost()==null)
                         continue;
                     String thisHost = uri.getHost().toLowerCase();
                     if (scheme.toLowerCase().equals(""har"")) {
                         if (thisHost.startsWith(HAR_PREFIX)) {
                             thisHost = thisHost.substring(HAR_PREFIX.length());
                         }
			 scheme=""hdfs"";
                     }
                     if (!uri.getHost().isEmpty() && 
                             !thisHost.equals(defaultHost)) {
                         if (uri.getPort()!=-1)
                             result.add(scheme + ""://""+thisHost+"":""+uri.getPort());
                         else
                             result.add(scheme + ""://""+thisHost);
                     }
                 }
{quote}
* JobControlCompiler.java
This has been taken care of as -
{quote}
                String outputPathString = st.getSFile().getFileName();
 		if (Utils.hasFileSystemImpl(new Path(outputPathString), conf)) {
                    conf.set(""pig.streaming.log.dir"",
                            new Path(outputPathString, LOG_DIR).toString());
                } else {
{quote}",27/Jan/14 04:15;rohini;Logic looks fine. PIG-3672-2.patch does not compile as it does not include imports for Utils class and conf is a unknown variable (Create configuration from pigContext.getProperties()). Please fix them. Also please format the code. ,30/Jan/14 21:43;dvryaboy;cancelling patch available status given Rohini's comments -- please make patch available again when a new patch is submitted,12/Mar/14 18:44;rohini;We hit some issues with webhdfs and I have a patch. Also Utils.hasFileSystemImpl() logic needs to change for hadoop-2. Assigning to myself. ,"22/Apr/14 17:25;rohini;This patch

   -  handles recent change in hadoop (HADOOP-7549) w.r.t to getting filesystem implementations.
   - handles configuring mapreduce.job.hdfs-servers correctly for other schemes like webhdfs, viewfs, etc. 
   - Fixes PIG-3796","25/Apr/14 01:30;daijy;+1 for PIG-3672-3.patch

",30/Apr/14 14:01;rohini;Committed to trunk. Thanks Daniel for the reivew.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix assert in Pig script,PIG-3670,12689328,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,lbendig,daijy,daijy,16/Jan/14 19:09,01/Dec/14 06:22,14/Mar/19 03:07,08/Feb/14 18:19,,,,,,,,0.12.1,0.13.0,,,impl,,,0,,,,,,,,,,,,,"PIG-3367 introduce ""assert"" keyword. However, it can only be used in Java embedding, when use registerQuery. In Pig script or Grunt shell, GruntParser will complain.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Jan/14 19:23;daijy;PIG-3670-1.patch;https://issues.apache.org/jira/secure/attachment/12623449/PIG-3670-1.patch,08/Feb/14 04:01;cheolsoo;PIG-3670-2.patch;https://issues.apache.org/jira/secure/attachment/12627778/PIG-3670-2.patch,09/Feb/14 09:29;lbendig;PIG-3670-2_addition.patch;https://issues.apache.org/jira/secure/attachment/12627856/PIG-3670-2_addition.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-02-08 04:01:39.387,,,no_permission,,,,,,,,,,,,368295,Reviewed,,,Mon Dec 01 06:22:12 UTC 2014,,,,,,,0|i1rha7:,368600,,,,,,,,,,"08/Feb/14 04:01;cheolsoo;+1. Thank you for fixing this.

Your patch didn't apply cleanly to trunk, so I rebased it. In addition, I replaced the temporary file in the test with ByteArrayInputStream. Since PigServer.registerScript() accepts InputStream, it seems better to avoid to create a temporary file. Let me know if you think otherwise.","08/Feb/14 18:19;daijy;Patch committed to trunk. I would also like to commit to 0.12 branch since we claim assert work in 0.12.

Thanks Cheolsoo for review and improve!","09/Feb/14 09:29;lbendig;Thanks for fixing it! Now ASSERT works fine, however, grammar allows to assign alias to it which may lead to the following issue:
{code}
A = load 'data.txt' AS (a0:int,a1:int,a2:int);
B = ASSERT A by a0 >= 5, 'a0 should be >= 5'; 

describe B;                                                                                      
14/02/09 10:21:04 ERROR grunt.Grunt: ERROR 1003: Unable to find an operator for alias B

C = foreach B generate $0;
14/02/09 10:22:08 ERROR grunt.Grunt: ERROR 1200: Pig script failed to parse: 
<line 3, column 4> pig script failed to validate: Unrecognized alias B
{code}

I'd address this issue by changing the grammar so that assigning alias will not be possible (similar to SPLIT). ","10/Feb/14 06:40;daijy;Thank Lorand, that is right, there is no alias for assert in document. I committed your patch to trunk and branch 0.12.",11/Feb/14 00:20;aniket486;JFYI - b = store a into 'crap2'; - is also a correct syntax in pig trunk. [~daijy] does this deserve a jira?,"11/Feb/14 01:54;daijy;Ok, it is better to have an aliased version so that user can weave assert statement into pipeline. Can you open a new ticket for it?","11/Feb/14 05:32;aniket486;bq. Ok, it is better to have an aliased version so that user can weave assert statement into pipeline.

Do you mean we should support B = ASSERT A by a0 >= 5, 'a0 should be >= 5'; ?","11/Feb/14 06:18;daijy;I think we should allow the following script:
{code}
A = load 'data.txt' AS (a0:int,a1:int,a2:int);
B = ASSERT A by a0 >= 5, 'a0 should be >= 5'; 
C = foreach B generate xxxx;
{code}
How do you think?","11/Feb/14 11:03;lbendig;I think [~aniket486] is right, it shouldn't be allowed to be able to assign an alias to a STORE, similarly to SPLIT.  However, I also think that an aliased ASSERT makes the interpretation of the pipeline clearer. Then the assigned alias, which initially didn't have any meaning, could be interpreted in case of a dump/store as 'foreach A generate * which fails if a0 > 5' .

",29/Nov/14 00:25;russell.jurney;Can someone please chime in and tell us what the syntax for ASSERT ended up being?,"01/Dec/14 06:22;russell.jurney;The syntax ended up being:

ASSERT my_relation BY field1 IS NOT NULL, 'Oh no, field1 is null';",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
build.xml jar-all target does not include jython*.jar in lib/ directory ,PIG-3667,12688897,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,suhassatish,suhassatish,suhassatish,15/Jan/14 00:35,31/Jan/14 00:18,14/Mar/19 03:07,15/Jan/14 18:56,0.12.0,,,,,,,,,,,build,,,0,build,,,,,,,,,,,,"Pig package does not include the jython jar within lib/ directory  with the jar-all ant target but includes it in the ""ant package"" target. It should be including it in both targets as often, the build/ directory is excluded from packaging which is where ivy puts all the dependency jars while building under build/ivy/lib/Pig  

To reproduce:
ant jar-all 
rm -rf build/ 
bin/pig
grunt> register '/tmp/test.py' using jython as myfunction;

If done prior to installing jython, here's the error one gets:
2013-12-27 18:22:31,145 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR
2998: Unhandled internal error. org/python/core/PyObject
Details at logfile: pig_*.log

Within the pig_*.log => 

<log>

Pig Stack Trace
---------------
ERROR 2998: Unhandled internal error. org/python/core/PyObject

java.lang.NoClassDefFoundError: org/python/core/PyObject
        at
org.apache.pig.scripting.jython.JythonScriptEngine.registerFunctions(JythonScriptEngine.java:304)
        at org.apache.pig.PigServer.registerCode(PigServer.java:501)
        at
org.apache.pig.tools.grunt.GruntParser.processRegister(GruntParser.java:436)
        at
org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:445)
        at
org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:194)
        at
org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:170)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:69)
        at org.apache.pig.Main.run(Main.java:538)
        at org.apache.pig.Main.main(Main.java:157)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:197)
Caused by: java.lang.ClassNotFoundException: org.python.core.PyObject
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
        ... 14 more


Fix: Including jython*.jar within the lib/ directory gets rid of this issue and the UDF can be loaded- 
grunt>  register '/tmp/test.py' using jython as myfuncs;

2013-12-27 18:37:02,402 [main] INFO 
org.apache.pig.scripting.jython.JythonScriptEngine - created tmp
python.cachedir=/tmp/pig_jython_4887743829482443898
2013-12-27 18:37:03,448 [main] WARN 
org.apache.pig.scripting.jython.JythonScriptEngine - pig.cmd.args.remainders is
empty. This is not expected unless on testing.
2013-12-27 18:37:03,724 [main] INFO 
org.apache.pig.scripting.jython.JythonScriptEngine - Register scripting UDF:
myfuncs.helloworld
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Jan/14 01:08;suhassatish;PIG-3667.patch;https://issues.apache.org/jira/secure/attachment/12623037/PIG-3667.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-16 02:38:05.967,,,no_permission,,,,,,,,,,,,367864,Reviewed,,,Fri Jan 31 00:18:34 UTC 2014,,,Patch Available,,,,0|i1reo7:,368171,,,,,,,,,,"16/Jan/14 02:38;cheolsoo;This patch is not committed but marked as resolved.

I was trying to edit the status to ""won't fix"" and accidentally changed to ""closed""...",25/Jan/14 00:19;suhassatish;Any reason why this wont be fixed? ,"25/Jan/14 00:39;cheolsoo;[~suhassatish], you marked the jira as ""fixed"" by yourself even though the patch was not committed. Why did you do you that? Wasn't that because you don't need to fix it anymore?","30/Jan/14 22:15;suhassatish;[~cheolsoo]- Sorry, I accidentally marked it as ""fixed"". The intention was to fix it and get the patch committed to pig trunk since the build/ directory where the jython standalone jar currently resides is not packaged in production environments. It is desirable to have it in $PIG_HOME/lib/ directory in the package. ","30/Jan/14 23:38;daijy;Since it is ""closed"", there is no way to reopen, can you open a new Jira?","31/Jan/14 00:18;suhassatish;I have created a clone here 
https://issues.apache.org/jira/browse/PIG-3734",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Piggy Bank XPath UDF can't be called,PIG-3664,12688653,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,nezihyigitbasi,nezihyigitbasi,nezihyigitbasi,14/Jan/14 01:09,07/Jul/14 18:08,14/Mar/19 03:07,16/Jan/14 00:46,0.12.0,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"When I try to call XPath UDF to process a very simple XML with Pig 0.12 I get the problem:
2014-01-13 16:14:19,530 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1045: 
<line 3, column 25> Could not infer the matching function for org.apache.pig.piggybank.evaluation.xml.XPath as multiple or none of them fit. Please use an explicit cast. I guess the XPath UDF overrides the getArgToFuncMapping() in an incorrect way. A fixed is attached.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Jan/14 00:10;nezihyigitbasi;PIG-3664.1.patch;https://issues.apache.org/jira/secure/attachment/12623022/PIG-3664.1.patch,14/Jan/14 01:10;nezihyigitbasi;PIG-3664.patch;https://issues.apache.org/jira/secure/attachment/12622757/PIG-3664.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-01-14 23:41:52.318,,,no_permission,,,,,,,,,,,,367672,Reviewed,,,Thu Jan 16 00:46:01 UTC 2014,,,Patch Available,,,,0|i1rdi7:,367979,,,,,,,,,,14/Jan/14 01:10;nezihyigitbasi;I just removed the getArgToFuncMapping() implementation as I guess this is not necessary; this UDF doesn't need to implement multiple UDF versions for different inputs. ,14/Jan/14 23:41;daijy;getArgToFuncMapping still better cuz we can capture schema mismatch in the frontend. I would prefer fix it rather than get rid of it.,15/Jan/14 00:10;nezihyigitbasi;Attached a new patch that implements the getArgToFuncMapping method.,16/Jan/14 00:46;daijy;Patch committed to trunk. Thanks Nezih!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Static loadcaster in BinStorage can cause exception,PIG-3662,12688343,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,lbendig,lbendig,lbendig,10/Jan/14 22:18,07/Jul/14 18:07,14/Mar/19 03:07,11/Jan/14 00:06,,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"I came a cross this issue when testing PIG-3642. Consider the following two testcases from {{TestEvalPipeline2}} executed in local mode:
{{testBinStorageByteCast}}:
{code}
A = load 'table_testBinStorageByteCast' as (a0, a1, a2);
store A into 'table_testBinStorageByteCast.temp' using BinStorage();
A = load 'table_testBinStorageByteCast.temp' using BinStorage() as (a0, a1, a2);
B = foreach A generate (long)a0;
dump B;
{code}

{{testBinStorageByteArrayCastsSimple}}:
{code}
A = load 'table_bs_ac';
store A into 'TestEvalPipeline2_BinStorageByteArrayCasts' using
  org.apache.pig.builtin.BinStorage();
B = load 'TestEvalPipeline2_BinStorageByteArrayCasts'  using 
  BinStorage('Utf8StorageConverter') as (name: int, age: int, gpa: float, 
lage: long, dgpa: double); 
dump B;
{code}

The first testcase should fail (same example at: http://pig.apache.org/docs/r0.12.0/func.html#binstorage) while the second one should pass.
When I run *only* _testBinStorageByteArrayCastsSimple_ there's no problem, but when it runs *after* _testBinStorageByteCast_, it fails with the same exception as _testBinStorageByteArrayCastsSimple_ :
{{java.lang.Exception: org.apache.pig.backend.executionengine.ExecException: ERROR 1118: Cannot cast bytes loaded from BinStorage. Please provide a custom converter.}}

Reason:
When 'table_testBinStorageByteCast.temp' is loaded, _BinStorage#getLoadCaster()_ sets _UnImplementedLoadCaster_ since casterString is not defined. This causes the exception when _BinStorage#bytesToLong()_ is called during the cast of a0 which is correct. Now move on to the next testcase where _'TestEvalPipeline2_BinStorageByteArrayCasts'_ is loaded. We expect BinStorage to use _Utf8StorageConverter_ as a loadcaster but it will use _UnImplementedLoadCaster_ instead, which results in an exception. It's because caster and casterString are *static* variables in BinStorage, however these are set and accessed like instance variables. Therefore, when BinStorage('Utf8StorageConverter') gets instantiated, it will contain the already initialized caster from the previous run. _BinStorage#getLoadCaster()_ will just return this instead of instantiating Utf8StorageConverter from the provided constructor parameter.

Are caster and casterString just by accident static? If so, I'd address this issue with the patch attached.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Jan/14 22:19;lbendig;PIG-3662.patch;https://issues.apache.org/jira/secure/attachment/12622450/PIG-3662.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-11 00:06:10.919,,,no_permission,,,,,,,,,,,,367362,,,,Sat Jan 11 00:14:33 UTC 2014,,,,,,,0|i1rblz:,367671,,,,,,,,,,"11/Jan/14 00:06;rohini;+1. Commited to trunk. Thanks Lorand.

Please do click on ""Submit Patch"" next time for it to show up in the Patch Available notifications.","11/Jan/14 00:14;lbendig;OK, sure. Thank you for committing it!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Piggybank AvroStorage fails if used in more than one load or store statement,PIG-3661,12688111,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,09/Jan/14 18:03,15/Apr/14 20:44,14/Mar/19 03:07,24/Jan/14 22:24,0.11.1,,,,,,,0.12.1,0.13.0,,,,,,0,,,,,,,,,,,,,"To reproduce:
A =load '/tmp/data' as (a1:int, a2:int, a3:int);
B = load '/tmp/data1' as (b1:chararray, b2:chararray, b3:chararray);
store A into '/tmp/out/a' using org.apache.pig.piggybank.storage.avro.AvroStorage();
store B into '/tmp/out2/b' using org.apache.pig.piggybank.storage.avro.AvroStorage();

It either fails in the map job if schema is incompatible, or B gets schema of A and B merged leading to incorrect results.

Reason is schema is stored and accessed from a property of UDFContext without using a context signature.

UDFContext context = UDFContext.getUDFContext();
        Properties property = context.getUDFProperties(ResourceSchema.class);
        String prevSchemaStr = property.getProperty(AVRO_OUTPUT_SCHEMA_PROPERTY);",,,,,,,,,,,,,,,,,PIG-3717,,,,,,,,,,,,,,,23/Jan/14 21:23;rohini;PIG-3661-1.patch;https://issues.apache.org/jira/secure/attachment/12624915/PIG-3661-1.patch,24/Jan/14 20:28;rohini;PIG-3661-2.patch;https://issues.apache.org/jira/secure/attachment/12625102/PIG-3661-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-04-04 21:22:55.026,,,no_permission,,,,,,,,,,,,367117,Reviewed,,,Fri Apr 04 21:22:55 UTC 2014,,,,,,,0|i1ra3z:,367427,,,,,,,,,,"23/Jan/14 21:22;rohini;This patch fixes other issues with AvroStorage as well apart from fixing multiple load and store
   - Hidden files were not excluded (PIG-3717)
   - mapred.input.dir was getting populated with all files instead of the top level directory making the conf very big
   - Default value was not set for a Union

https://reviews.apache.org/r/17266/",24/Jan/14 22:24;rohini;Committed to trunk. Thanks Cheolsoo for the review.,04/Apr/14 21:22;cheolsoo;We discovered piggybank TestAvroStorage fails in branch 0.12 w/o this patch due to .svn files. Backported to 0.12.1.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New partition filter extractor fails with NPE,PIG-3657,12687899,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,08/Jan/14 17:18,15/Apr/14 20:44,14/Mar/19 03:07,09/Jan/14 18:30,,,,,,,,0.12.1,0.13.0,,,impl,,,0,,,,,,,,,,,,,"To reproduce the issue, try a filter expression as follows-
{code}
b = FILTER a BY partition_column < x OR (partition_column == y AND non-partition_column == z); 
{code}
Pig fails with NPE, and the stack trace is something like this-
{code}
Caused by: java.lang.NullPointerException
    at org.apache.pig.newplan.FilterExtractor.checkPushDown(FilterExtractor.java:252)
    at org.apache.pig.newplan.FilterExtractor.visit(FilterExtractor.java:115)
    at org.apache.pig.newplan.logical.rules.PartitionFilterOptimizer$PartitionFilterPushDownTransformer.transform(PartitionFilterOptimizer.java:150)
    at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:110)
    ... 18 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Jan/14 17:35;cheolsoo;PIG-3657-1.patch;https://issues.apache.org/jira/secure/attachment/12622002/PIG-3657-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-08 18:19:17.575,,,no_permission,,,,,,,,,,,,366906,,,,Tue Feb 18 19:58:00 UTC 2014,,,,,,,0|i1r8tb:,367217,,,,,,,,,,"08/Jan/14 17:35;cheolsoo;The patch includes a unit test that demonstrates the issue. Without the fix, the test throws a NPE.

Regarding the cause, the filtered plan returned by FilterExtrator is not always translatable to physical plan because newly added OR/AND expressions do not copy successors.

For the aforementioned example, FilterExtractor returns the following filtered plan-
{code}
(Name: Or Type: null Uid: null)
|
|---(Name: LessThan Type: boolean Uid: 112)
|
|---(Name: Equal Type: null Uid: null)
    |
    |---(Name: Project Type: null Uid: null Input: 0 Column: 42)
    |
    |---(Name: Constant Type: null Uid: null)
{code}
As can be seen, LessThan has no children resulting in NPE while converting it to physical plan.",08/Jan/14 18:19;aniket486;Good catch! +1.,09/Jan/14 00:11;cheolsoo;Thank you Aniket for the review. I will commit it after running unit tests.,09/Jan/14 18:30;cheolsoo;Committed to trunk.,"18/Feb/14 19:36;cheolsoo;I think we should also commit this into 0.12.1 to make NewFilterExtractor more useable.

I will commit the patch if none has objections. Thanks!","18/Feb/14 19:43;aniket486;Yes, please commit on 0.12.1. +1.",18/Feb/14 19:58;cheolsoo;Done. It's committed to 0.12.1 too. Thanks Aniket for the quick response.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pigmix parser (PigPerformanceLoader) deletes chars during parsing ,PIG-3652,12687585,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kereno,keren3000,keren3000,07/Jan/14 03:34,07/Jul/14 18:07,14/Mar/19 03:07,30/Jan/14 23:31,0.12.0,,,,,,,0.13.0,,,,parser,,,0,,,,,,,,,,,,,"When importing data generated by Pigmix using pigper.jar, the first char of the value of a map are missing like in the following example:

DATA GENERATED:
f^DGvds_NL //^D is the delimiter

DATA LOADED:
[f#vds_NL]

The letter G is missing.
This issue reproduces to the key of the map when the number of bytes >1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Jan/14 02:13;daijy;PIG-3652-1.patch;https://issues.apache.org/jira/secure/attachment/12623295/PIG-3652-1.patch,20/Jan/14 23:14;daijy;PIG-3652-2.patch;https://issues.apache.org/jira/secure/attachment/12624028/PIG-3652-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-01-16 02:13:49.281,,,no_permission,,,,,,,,,,,,366586,Reviewed,,,Thu Jan 30 23:31:17 UTC 2014,,,,,,,0|i1r6tz:,366897,,,,,,,,,,16/Jan/14 02:13;daijy;I think you are right. Can you try attached patch?,"20/Jan/14 23:13;keren3000;+1. 

PigPerformanceLoader still misses the page_info from the output. ",20/Jan/14 23:14;daijy;Fix another issue in PigPerformanceLoader.java.,"20/Jan/14 23:29;keren3000;I tried the patch, it works for me.
+1",30/Jan/14 07:04;rohini;+1,30/Jan/14 23:31;daijy;Patch committed to trunk. Thanks Keren!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix for PIG-3100 breaks column pruning,PIG-3650,12687573,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,tmwoodruff,tmwoodruff,tmwoodruff,07/Jan/14 01:52,07/Jul/14 18:07,14/Mar/19 03:07,07/Jan/14 21:54,0.12.0,0.12.1,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"The fix for PIG-3100 does {{Math.min(fieldSchemas.length, tup.size())}} in {{PigStorage.applySchema()}}. This means that if column pruning is in use, all schema columns will not be iterated over (since tup.size() will be less than fieldSchemas.length) and the schema will not be applied to all columns,",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Jan/14 02:02;tmwoodruff;PIG-3650.patch;https://issues.apache.org/jira/secure/attachment/12621738/PIG-3650.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-07 21:54:33.5,,,no_permission,,,,,,,,,,,,366574,Reviewed,,,Tue Jan 07 21:54:33 UTC 2014,,,,,,,0|i1r6rb:,366885,,,,,,,,,,"07/Jan/14 02:02;tmwoodruff;Adding patch to fix. Iterates over all fieldSchemas (removes the Math.min) and adds padding to the tuple (as requested PIG-3100) as needed within that loop.

Added new test to reproduce this issue.

Updated imcomplete data test to verify that null padding still works in this case,",07/Jan/14 21:54;daijy;+1. Patch committed to trunk. Thanks Travis!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
POPartialAgg incorrectly calculates size reduction when multiple values aggregated,PIG-3649,12687526,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,tmwoodruff,tmwoodruff,tmwoodruff,06/Jan/14 21:19,07/Jul/14 18:08,14/Mar/19 03:07,07/Jan/14 22:31,0.11,0.11.1,0.12.0,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"{{POPartialAgg.aggregate()}} counts the number of output columns ({{valueTuple.size() - 1}}), but {{checkSizeReduction()}} compares this to the number of input tuples. 

When multiple columns are aggregated, this causes the reduction factor to be calculated as too high by a factor of the number of columns being aggregated, which causes in-memory aggregation to be disabled when it should not be, adversely affecting performance,",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Jan/14 21:25;tmwoodruff;PIG-3469.patch;https://issues.apache.org/jira/secure/attachment/12621680/PIG-3469.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-07 22:31:29.051,,,no_permission,,,,,,,,,,,,366527,Reviewed,,,Tue Jan 07 22:31:29 UTC 2014,,,,,,,0|i1r6gv:,366838,,,,,,,,,,"06/Jan/14 21:25;tmwoodruff;Attaching path that updates {{aggregate()}} to count number of result tuples.

Includes two new tests:
- One that shows that basic aggregation of multiple columns works
- Another that reproduces the issue reported here. This requires aggregating > 10,000 rows, so it is a bit slow. Suggestions for alternative approaches welcome.

",07/Jan/14 22:31;daijy;+1. Patch committed to trunk. Thanks Travis!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nested Foreach with UDF and bincond is broken,PIG-3643,12686533,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,rohini,rohini,27/Dec/13 16:27,07/Jul/14 18:08,14/Mar/19 03:07,29/Dec/13 22:53,0.13.0,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"Was checking out PIG-3000. 

A = load 'data' as (a:chararray);
B = foreach A { c = UPPER(a); generate ((c eq 'TEST') ? 1 : 0), ((c eq 'DEV') ? 1 : 0); }

This now throws ""Invalid field projection. Projected field [c] does not exist in schema"".  Works fine in 0.11. Broken in trunk. Haven't checked 0.12. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Dec/13 01:13;cheolsoo;PIG-3643-1.patch;https://issues.apache.org/jira/secure/attachment/12620697/PIG-3643-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-27 18:11:33.811,,,no_permission,,,,,,,,,,,,365524,,,,Mon Dec 30 23:29:08 UTC 2013,,,,,,,0|i1r03z:,365825,,,,,,,,,,"27/Dec/13 18:11;cheolsoo;This is a regression of PIG-3581. If I drop the PIG-3581 commit, the example works in trunk too.

[~aniket486] and I were discussion this in PIG-3581 too.

",27/Dec/13 21:43;cheolsoo;TestColumnAliasConversion is broken in trunk for this as well.,27/Dec/13 22:47;cheolsoo;TestLogicalPlanGenerator is broken by this too. Let me fix this.,"28/Dec/13 01:13;cheolsoo;Uploading a patch that fixes the regression.

To summarize the issue-
# Before PIG-3581:
{code}
if X, do a
else if Y, do b
else do c
{code}
# With PIG-3581:
{code}
If Y && N, do b
else if X, do a
else do c
{code}
As can be seen, if Y && !N, it falls into c now while we used to fall into b.
# With my patch:
{code}
If Y && N, do b
else if X, do a
else if Y, do b
else do c
{code}

I verified-
* TestColumnAliasConversion passes.
* TestLogicalPlanGenerator passes.
* The new test cases added by PIG-3581 pass.
* The example in this jira wrks.",29/Dec/13 19:50;rohini;+1. This patch just adds back code that was removed by PIG-3581. ,29/Dec/13 22:53;cheolsoo;Committed to trunk. Thank you Rohini for the review!,"30/Dec/13 23:29;lbendig;Thanks for this patch. It seems to me that it also fixed:

TestEvalPipelineLocal#testArithmeticCloning
TestEvalPipelineLocal#testExpressionReUse
TestPlanGeneration#testDanglingNestedNode
TestEvalPipeline2#testUdfInputOrder",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Split ""otherwise"" producing incorrect output when combined with ColumnPruning",PIG-3641,12686225,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,23/Dec/13 22:46,15/Apr/14 20:44,14/Mar/19 03:07,30/Jan/14 17:03,0.10.0,0.11.1,0.12.0,0.13.0,,,,0.12.1,,,,,,,0,,,,,,,,,,,,,"Our user was observing incorrect outputs depending on if the query had intermediate output or not.  Below is a simplified testcase I came up with.

{noformat}
knoguchi pig> cat test.txt
9,1,ignored
9,1,ignored
9,1,ignored
knoguchi pig> cat bz-6590644/test.pig
A = load 'test.txt' using PigStorage(',') as (a1:int, a2:int, a3:chararray);
B = foreach A generate a1,a2;
SPLIT B into C1 if a2 == 1, D1 otherwise;
C2 = foreach C1 generate a2;
store C2 into '/tmp/testC';
store D1 into '/tmp/testD';
knoguchi@nameother-lm pig>
{noformat}

Incorrect output shown below.  /tmp/testD should be empty but somehow has data in it.

{noformat}
knoguchi@nameother-lm pig> cat /tmp/testC/part-m-00000
1
1
1
knoguchi pig> cat /tmp/testD/part-m-00000
9       1
9       1
9       1
knoguchi pig>
{noformat}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Dec/13 22:52;knoguchi;pig-3641_v01.patch;https://issues.apache.org/jira/secure/attachment/12620279/pig-3641_v01.patch,28/Jan/14 22:49;knoguchi;pig-3641_v02_withe2etest.patch;https://issues.apache.org/jira/secure/attachment/12625696/pig-3641_v02_withe2etest.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-01-30 16:40:43.456,,,no_permission,,,,,,,,,,,,365198,,,,Thu Jan 30 17:03:46 UTC 2014,,,,,,,0|i1qy4n:,365503,,,,,,,,,,"23/Dec/13 22:52;knoguchi;This issue is similar to PIG-3051 where projection expression was pointing to a different operator.  In Pig-3051, it was about copied LOSort.  Here, it's about LOSplitOutput for 'otherwise'. 

Attaching a preliminary patch.  Need to add testing.","28/Jan/14 22:49;knoguchi;Added an e2e testcase that shows how the output differs between 
""SPLIT B into C1 if age > 50, C2 otherwise;"" 
and
""SPLIT B into C1 if age > 50, C2 if age <= 50;"" 
due to this bug.","30/Jan/14 16:40;rohini;+1. I ran the full suite of unit tests and they are fine. TestAutoLocalMode fails, but that is unrelated.",30/Jan/14 16:55;rohini;TestScriptLanguage.runParallelTest2 also failed. That is also unrelated. Will file a jira to fix both.,"30/Jan/14 17:03;knoguchi;Thanks Rohini for the review!

Committed to 0.12.1 and trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Retain intermediate files for debugging purpose in batch mode,PIG-3640,12686086,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,23/Dec/13 00:43,07/Jul/14 18:08,14/Mar/19 03:07,24/Dec/13 05:02,,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"PIG-3117 make it configurable whether to keep intermediate files between MR jobs during the execution. So if we run queries in Grunt shell, we can keep intermediate files.

However, intermediate files are still deleted when Pig exits. It would be nice if we could retain intermediate files even in batch mode for debugging purpose.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Dec/13 00:45;cheolsoo;PIG-3640-1.patch;https://issues.apache.org/jira/secure/attachment/12620108/PIG-3640-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-24 00:28:39.719,,,no_permission,,,,,,,,,,,,365043,,,,Tue Dec 24 05:02:34 UTC 2013,,,,,,,0|i1qx6v:,365351,,,,,,,,,,23/Dec/13 00:45;cheolsoo;Attaching a patch.,24/Dec/13 00:28;aniket486;+1,"24/Dec/13 05:02;cheolsoo;Committed to trunk. Thank you Aniket for the review.

p.s. Main.java had funny indentations. (Several hundreds of lines were left shifted by a tab.) I fixed it while touching the file.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestRegisteredJarVisibility is broken in trunk,PIG-3639,12686030,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,21/Dec/13 23:32,07/Jul/14 18:07,14/Mar/19 03:07,27/Dec/13 22:17,,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"{code}
    [junit] Running org.apache.pig.test.TestRegisteredJarVisibility
    [junit] Tests run: 3, Failures: 1, Errors: 0, Time elapsed: 44.127 sec
    [junit] Test org.apache.pig.test.TestRegisteredJarVisibility FAILED
{code}
This is a side-effect of PIG-3584 that bumped avro version to 1.7.5.

The problem is that avro 1.7.5 pulls down jackson 1.9.9 jars as dependencies, and that makes TestRegisteredJarVisibility.testRegisterJarOverridePigJarPackages fail because the test case assumes that jackson 1.9.9 jars are not present in classpath.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Dec/13 23:38;cheolsoo;PIG-3639-1.patch;https://issues.apache.org/jira/secure/attachment/12620060/PIG-3639-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-23 07:53:27.042,,,no_permission,,,,,,,,,,,,364907,,,,Fri Dec 27 22:17:44 UTC 2013,,,,,,,0|i1qwav:,365207,,,,,,,,,,"21/Dec/13 23:38;cheolsoo;The patch excludes the jackson jars from avro dependencies.

I confirmed TestRegisteredJarVisibility and TestAvroStorage both pass.","23/Dec/13 07:53;jarcec;Thank you for filling the JIRA and providing patch [~cheolsoo]! I was experiencing the same failure in our environment when upgraded Avro to version 1.7.5. Sadly I did not had time to investigate it myself. 

+1 (non-binding)",27/Dec/13 19:54;rohini;+1,27/Dec/13 22:17;cheolsoo;Committed to trunk. Thank you Jarcec and Rohini for the review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigCombiner creating log spam,PIG-3637,12685925,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,rohini,rohini,rohini,20/Dec/13 23:43,07/Jul/14 18:07,14/Mar/19 03:07,27/Dec/13 19:55,0.11.1,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"""Aliases being processed per job phase"" is logged a lot of times even though it is in setup because Combiners can be called multiple times in both map and reduce by MR framework (HADOOP-3226). With Hadoop 2.0 since logs go to hdfs this is inefficient.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Dec/13 00:12;rohini;PIG-3637-1.patch;https://issues.apache.org/jira/secure/attachment/12619938/PIG-3637-1.patch,27/Dec/13 19:54;rohini;PIG-3637-2-whitespacechanges.patch;https://issues.apache.org/jira/secure/attachment/12620642/PIG-3637-2-whitespacechanges.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-12-27 18:15:29.501,,,no_permission,,,,,,,,,,,,364924,Reviewed,,,Fri Dec 27 19:55:15 UTC 2013,,,,,,,0|i1qwen:,365224,,,,,,,,,,"21/Dec/13 00:18;rohini;No new test added. But ran TestCombiner with -Dhadoopversion=23

find target | xargs grep -H ""Aliases being processed"" | grep Combiner | wc -l

Before the patch it was 34 and after the patch it was 5. 

As for the usefulness of cutting down on this log statement - in the case reported by one of our users, one map task alone had the same log line repeated 1001 times and the job had 32K maps.",27/Dec/13 18:15;cheolsoo;+1. LGTM.,27/Dec/13 19:55;rohini;Committed to trunk (0.13). Thanks for the review Cheolsoo.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroStorage tests are failing when running against Avro 1.7.5,PIG-3633,12685346,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,jarcec,jarcec,jarcec,18/Dec/13 09:30,07/Jul/14 18:08,14/Mar/19 03:07,18/Dec/13 18:21,0.12.0,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"{{AvroStorage}} tests executed against latest Avro release 1.7.5 are failing:

{code}
ant clean test -Dtestcase=TestAvroStorage -Davro.version=1.7.5
...
   [junit] Running org.apache.pig.builtin.TestAvroStorage
   [junit] Tests run: 33, Failures: 15, Errors: 0, Time elapsed: 14.211 sec
{code}

With following exception:

{code}
Failed to parse: Pig script failed to parse: 
<line 1, column 5> pig script failed to validate: java.lang.RuntimeException: could not instantiate 'AvroStorage' with arguments '[, -r]'
	at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:196)
	at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1676)
	at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1623)
	at org.apache.pig.PigServer.registerQuery(PigServer.java:575)
	at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:1093)
	at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:501)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:198)
	at org.apache.pig.PigServer.registerScript(PigServer.java:649)
	at org.apache.pig.PigServer.registerScript(PigServer.java:726)
	at org.apache.pig.PigServer.registerScript(PigServer.java:699)
	at org.apache.pig.builtin.TestAvroStorage.testAvroStorage(TestAvroStorage.java:775)
	at org.apache.pig.builtin.TestAvroStorage.testLoadRecursiveRecordsOptionOn(TestAvroStorage.java:588)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:518)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1052)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:906)
Caused by: 
<line 1, column 5> pig script failed to validate: java.lang.RuntimeException: could not instantiate 'AvroStorage' with arguments '[, -r]'
	at org.apache.pig.parser.LogicalPlanBuilder.buildLoadOp(LogicalPlanBuilder.java:849)
	at org.apache.pig.parser.LogicalPlanGenerator.load_clause(LogicalPlanGenerator.java:3479)
	at org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1536)
	at org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:1013)
	at org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:553)
	at org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:421)
	at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:188)
	... 34 more
Caused by: java.lang.RuntimeException: could not instantiate 'AvroStorage' with arguments '[, -r]'
	at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:748)
	at org.apache.pig.parser.LogicalPlanBuilder.buildLoadOp(LogicalPlanBuilder.java:837)
	... 40 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:716)
	... 41 more
Caused by: java.lang.NullPointerException
	at org.apache.avro.Schema.parse(Schema.java:1072)
	at org.apache.avro.Schema$Parser.parse(Schema.java:950)
	at org.apache.avro.Schema$Parser.parse(Schema.java:940)
	at org.apache.pig.builtin.AvroStorage.<init>(AvroStorage.java:136)
	... 46 more
3701 [main] WARN  org.apache.pig.tools.parameters.PreprocessorContext  - Warning : Multiple values found for INFILE. Using value test/org/apache/pig/builtin/avro/data/trevni/uncompressed/simpleRecordsTrevni.trevni
3701 [main] WARN  org.apache.pig.tools.parameters.PreprocessorContext  - Warning : Multiple values found for AVROSTORAGE_OUT_2. Using value -f test/org/apache/pig/builtin/avro/schema/simpleRecordsTrevni.avsc
3702 [main] WARN  org.apache.pig.tools.parameters.PreprocessorContext  - Warning : Multiple values found for OUTFILE. Using value /home/jarcec/cloudera/repos/pig/build/test/TestAvroStorage/testLoadTrevniRecords
3732 [main] ERROR org.apache.pig.PigServer  - exception during parsing: Error during parsing. Pig script failed to parse: 
<line 4, column 0> pig script failed to validate: java.lang.RuntimeException: could not instantiate 'AvroStorage' with arguments '[, -f test/org/apache/pig/builtin/avro/schema/simpleRecordsTrevni.avsc]'
Failed to parse: Pig script failed to parse: 
<line 4, column 0> pig script failed to validate: java.lang.RuntimeException: could not instantiate 'AvroStorage' with arguments '[, -f test/org/apache/pig/builtin/avro/schema/simpleRecordsTrevni.avsc]'
	at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:196)
	at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1676)
	at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1623)
	at org.apache.pig.PigServer.registerQuery(PigServer.java:575)
	at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:1093)
	at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:501)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:198)
	at org.apache.pig.PigServer.registerScript(PigServer.java:649)
	at org.apache.pig.PigServer.registerScript(PigServer.java:726)
	at org.apache.pig.PigServer.registerScript(PigServer.java:699)
	at org.apache.pig.builtin.TestAvroStorage.testAvroStorage(TestAvroStorage.java:775)
	at org.apache.pig.builtin.TestAvroStorage.testLoadTrevniRecords(TestAvroStorage.java:709)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:518)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1052)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:906)
Caused by: 
<line 4, column 0> pig script failed to validate: java.lang.RuntimeException: could not instantiate 'AvroStorage' with arguments '[, -f test/org/apache/pig/builtin/avro/schema/simpleRecordsTrevni.avsc]'
	at org.apache.pig.parser.LogicalPlanBuilder.buildStoreOp(LogicalPlanBuilder.java:936)
	at org.apache.pig.parser.LogicalPlanGenerator.store_clause(LogicalPlanGenerator.java:7691)
	at org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1580)
	at org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:1013)
	at org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:553)
	at org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:421)
	at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:188)
	... 34 more
Caused by: java.lang.RuntimeException: could not instantiate 'AvroStorage' with arguments '[, -f test/org/apache/pig/builtin/avro/schema/simpleRecordsTrevni.avsc]'
	at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:748)
	at org.apache.pig.parser.LogicalPlanBuilder.buildStoreOp(LogicalPlanBuilder.java:915)
	... 40 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:716)
	... 41 more
Caused by: java.lang.NullPointerException
	at org.apache.avro.Schema.parse(Schema.java:1072)
	at org.apache.avro.Schema$Parser.parse(Schema.java:950)
	at org.apache.avro.Schema$Parser.parse(Schema.java:940)
	at org.apache.pig.builtin.AvroStorage.<init>(AvroStorage.java:136)
	... 46 more
{code}

[~tomwhite] has looked into the problem and found out that Avro  in version 1.7.5 has updated the JSON parser library Jackson from 1.8.8 to 1.9.13. The new version is changing the way empty strings are processed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Dec/13 09:33;jarcec;PIG-3633.patch;https://issues.apache.org/jira/secure/attachment/12619283/PIG-3633.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-18 18:19:23.663,,,no_permission,,,,,,,,,,,,364423,,,,Thu Dec 19 08:00:39 UTC 2013,,,,,,,0|i1qtbr:,364723,,,,,,,,,,18/Dec/13 09:33;jarcec;Attaching patch that will skip schema parsing if the given string is empty. This seems to resolve the failing tests.,18/Dec/13 18:19;cheolsoo;+1. I can reproduce failures. I will commit it shortly.,18/Dec/13 18:21;cheolsoo;Committed to trunk. Thank you Jarcec!,"19/Dec/13 08:00;jarcec;Thank you for the review [~cheolsoo], appreciated!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add option to configure cacheBlocks in HBaseStorage,PIG-3632,12685247,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,17/Dec/13 20:54,07/Jul/14 18:07,14/Mar/19 03:07,27/Dec/13 19:24,,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"By default it should be set to false as we don't wan't users trashing the cache unintentionally.  But there are some cases, a user wants to work on the same data potentially and it would be good to warm up the cache.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Dec/13 23:35;rohini;PIG-3632-1.patch;https://issues.apache.org/jira/secure/attachment/12619438/PIG-3632-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-26 08:56:30.324,,,no_permission,,,,,,,,,,,,364324,Reviewed,,,Fri Dec 27 19:24:13 UTC 2013,,,,,,,0|i1qspz:,364624,,,,,,,,,,"26/Dec/13 08:56;prkommireddi;LGTM +1

Important to note users repeatedly accessing data using similar scans could notice a difference here as the default hbase config for this property is set to true. For MR in general, completely makes sense to set it to false.","27/Dec/13 19:24;rohini; We want it to be false by default in MR (Refer PIG-2619). Our hbase team also has recommended the same as default, but for few users they wanted to have this true. So made it configurable.

Committed to trunk (0.13). Thanks Prashant for the review.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Json storage : Doesn't work in cases , where other Store Functions (like PigStorage / AvroStorage) do work. ",PIG-3627,12684949,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ssvinarchukhorton,jayunit100,jayunit100,16/Dec/13 13:26,07/Jul/14 18:07,14/Mar/19 03:07,07/Feb/14 17:37,,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"The following query 
{code:title=Bar.java|borderStyle=solid}
        pigServer.registerQuery(
                ""uniqcnt  = foreach transactionsG {""+
                               ""sym = transactions.product ;""+
                               ""dsym = distinct sym  ;""+
                               ""generate flatten(dsym.product) as product, COUNT(dsym) as count ;"" +
                               ""};"");
{code} 

Results in the schema:

{code} 
   Schema : {product: NULL,count: long}
{code}

This schema, is storable using AvroStorage or PigStorage, but it fails if stored using JsonStorage: 

{code}
Failed to parse: <line 1, column 8>  Syntax error, unexpected symbol at or near ','
	at org.apache.pig.parser.QueryParserDriver.parseSchema(QueryParserDriver.java:94)
	at org.apache.pig.parser.QueryParserDriver.parseSchema(QueryParserDriver.java:108)
	at org.apache.pig.impl.util.Utils.parseSchema(Utils.java:208)
	at org.apache.pig.impl.util.Utils.getSchemaFromString(Utils.java:182)
	at org.apache.pig.builtin.JsonStorage.prepareToWrite(JsonStorage.java:140)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.<init>(PigOutputFormat.java:125)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.getRecordWriter(PigOutputFormat.java:86)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:553)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216)

{code}

It appears that JsonStorage is thus less robust than the other storage formats.  Can we confirm or deny if some types of data structures do/ do not work with JsonStorage? 

So,I suggest:

1) Ideally, I would think JsonStorage should support the same data that other Storage functions support.   

the next best thing: 

2) Maybe a wiki page of examples that can / cannot work with JsonStorage and/or a better error message would be sufficient to solve this ""bug"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Feb/14 17:56;ssvinarchukhorton;PIG-3627-2.patch;https://issues.apache.org/jira/secure/attachment/12627383/PIG-3627-2.patch,07/Feb/14 10:22;ssvinarchukhorton;PIG-3627-3.patch;https://issues.apache.org/jira/secure/attachment/12627584/PIG-3627-3.patch,06/Feb/14 08:09;daijy;PIG-3627-schemageneration.patch;https://issues.apache.org/jira/secure/attachment/12627313/PIG-3627-schemageneration.patch,05/Feb/14 18:24;ssvinarchukhorton;PIG-3627.patch;https://issues.apache.org/jira/secure/attachment/12627173/PIG-3627.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-12-16 19:06:30.226,,,no_permission,,,,,,,,,,,,364026,Reviewed,,,Fri Feb 07 17:37:26 UTC 2014,,,,,,,0|i1qqvr:,364326,,,,,,,,,,"16/Dec/13 19:06;cheolsoo;The problem is 'NULL' in your schema. 'NULL' is not recognized by the schema parser.

Explicitly defining the type of {{product}} in this line probably will fix your error-
{code}
flatten(dsym.product) as product:chararray,
{code}

However, I think the better way is to define the type of every field  in your query from the beginning so that you won't end up with NULL type after flattening {{dsym.product}} in the first place. Pig is type-sensitive, so I strongly recommend to define type for every field if possible.

To answer your questions, I agree that JsonStorage is not robust. For this case, it passes around the schema info as string and parse it using Utils.parseSchema() function. This is not robust at all.

Ideally, what JsonStorage should do is to cast any field with no type info to bytearray. Contribution is welcome. :-)
","16/Dec/13 22:36;jayunit100;Well, so I tried many permutations of casting , and it didnt work.  

In the end, I had to update my load function to pre package the originator of the product field with chararray type.

Then, the flatten statements at the bottom of my script didnt error anymore. 

I guess the schema  types cant be casted in on the fly.
","16/Dec/13 22:47;cheolsoo;Thanks [~jayunit100] for sharing it.
{quote}
I guess the schema types cant be casted in on the fly.
{quote}
Yes, it usually can be by adding a cast expression. But in your case, it was probably not possible because the schema was NULL to begin with.

What I don't understand is how you end up with NULL in the schema because if any schema is not specified in load, it should be loaded as bytearray.","16/Dec/13 22:56;jayunit100;here are some more details.  I've written it up here

http://jayunit100.blogspot.com/2013/12/pig-if-using-jsonstorage-beware-of.html

hope this helps with the JIRA or to other people with the same problem.






-- 
Jay Vyas
http://jayunit100.blogspot.com
","03/Feb/14 00:50;nezihyigitbasi;Cheolsoo, I checked the link Jay posted in his previous comment and after some digging it seems like the NULL schemas come from the STRSPLIT function, not from the LOAD function. Jay uses FLATTEN(STRSPLIT(...)) in his script and STRSPLIT returns a tuple without any internal schema (the schema of the internal elements are NULL).",05/Feb/14 18:24;ssvinarchukhorton;I attached file which fix this problem. It change type to chararray if field schema include null.,"06/Feb/14 08:09;daijy;Thanks Sergey. I am fine to check the schema for JsonStorage, but we need to treat Null schema as bytearray instead of chararray.

And the real problem here is we shall not get NULL schema in the first place. I attach a patch to fix the schema generation.",06/Feb/14 11:57;ssvinarchukhorton;Thanks Daniel. Your patch works fine.,"06/Feb/14 17:37;daijy;Sergey, would you like to update your JsonStorage patch for Null schema handling? That should also applicable.",06/Feb/14 17:57;ssvinarchukhorton;I updated my patch,"06/Feb/14 18:17;daijy;Hi, Sergey, I think your previous patch (PIG-3627.patch) is good, just change the default type to bytearray, plus not use string search for null schema, instead, iterate the field schema to replace null type to bytearray type (verifySchema(s.toString()) -> fixSchema(s).toString()).","07/Feb/14 10:23;ssvinarchukhorton;Daniel, I updated patch with your recommendation.","07/Feb/14 17:37;daijy;Patch committed to trunk. Thanks Sergey!

For the rest part (don't generate null schema in LOGenerate), I feel more proper to open a new ticket. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBaseStorage: setting loadKey and noWAL to false doesn't have any affect,PIG-3623,12684473,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nezihyigitbasi,mstefaniak,mstefaniak,13/Dec/13 14:53,07/Jul/14 18:08,14/Mar/19 03:07,05/Feb/14 03:25,0.12.0,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"The documentation for HBaseStorage (http://pig.apache.org/docs/r0.12.0/func.html#HBaseStorage)

says -loadKey=(true|false) Load the row key as the first value in every tuple returned from HBase (default=false)

However, looking at the source (http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java)

it is just doing a check for the existence of this option

loadRowKey_ = configuredOptions_.hasOption(""loadKey"");

So setting -loadKey=false in the options string, still results in a true value",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30/Jan/14 00:51;nezihyigitbasi;PIG-3623.1.patch;https://issues.apache.org/jira/secure/attachment/12626029/PIG-3623.1.patch,31/Jan/14 22:08;nezihyigitbasi;PIG-3623.2.patch;https://issues.apache.org/jira/secure/attachment/12626388/PIG-3623.2.patch,02/Feb/14 11:58;nezihyigitbasi;PIG-3623.3.patch;https://issues.apache.org/jira/secure/attachment/12626527/PIG-3623.3.patch,29/Jan/14 22:54;nezihyigitbasi;PIG-3623.patch;https://issues.apache.org/jira/secure/attachment/12626003/PIG-3623.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-12-26 09:13:00.403,,,no_permission,,,,,,,,,,,,363545,Reviewed,,,Wed Feb 05 03:25:19 UTC 2014,,,Patch Available,,,,0|i1qnyn:,363851,,,,,,,,,,"26/Dec/13 09:13;prkommireddi;Thanks [~mstefaniak]. This actually sounds like a bug, the following would fix it.

{code}
if(configuredOptions_.hasOption(""loadKey"")) {
    loadRowKey_ = configuredOptions_.getOptionValue(""loadKey"");
}
{code}

[~rohini] [~billgraham] would this be backward incompatible? Its the right thing to do w.r.t docs, but I am not sure if this will throw users off because of current behavior?","26/Dec/13 15:19;rohini;We should fix the code and not the documentation as the expected behavior should that be of the documentation. Since default is false, people don't usually specify -loadKey false and if they specified they would immediately realize as in this jira that it is not working correctly because the fields returned are not expected, so backward compatibility is not a issue. Saw this with -noWAL as well when fixing PIG-3632. Both options need to be fixed.",27/Dec/13 04:47;billgraham;+1 for fixing behavior to match the docs here. Great find [~mstefaniak].,"29/Jan/14 22:54;nezihyigitbasi;Guys, Seems like nobody has fixed this issue, so I fixed it. Any feedback is appreciated.",30/Jan/14 00:43;rohini;Can you please fix -noWAL as well?,30/Jan/14 00:51;nezihyigitbasi;Added a fix for noWAL,"31/Jan/14 22:08;nezihyigitbasi;This patch fixes the backward compatibility issues I found during my tests. The solution is to use OptionBuilder to create options that take optional arguments since addOption doesn't let me do that. And if we don't make loadKey and noWal arguments optional (that is if we make 2nd argument of addOption true) then old code that doesn't specify true|false breaks. Rohini, can you please review? 

Btw I have run my own unit tests and couldn't get TestHBaseStorage running. I get exceptions like :
  [junit] 533071 [Thread-4871] WARN  org.apache.hadoop.mapred.TaskTracker  - Error initializing att                                                           empt_20140131132522179_0022_m_000001_1:
    [junit] java.io.FileNotFoundException: File does not exist: hdfs://localhost:44996/tmp/hadoop-nyi                                                           gitba/mapred/system/job_20140131132522179_0022/jobToken
    [junit]     at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.j                                                           ava:517)
    [junit]     at org.apache.hadoop.mapred.TaskTracker.localizeJobTokenFile(TaskTracker.java:4250)
    [junit]     at org.apache.hadoop.mapred.TaskTracker.initializeJob(TaskTracker.java:1158)
    [junit]     at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:1099)
    [junit]     at org.apache.hadoop.mapred.TaskTracker$5.run(TaskTracker.java:2382)
    [junit]     at java.lang.Thread.run(Thread.java:722)
    [junit]
    [junit] 533071 [Thread-4871] ERROR org.apache.hadoop.mapred.TaskStatus  - Trying to set finish ti                                                           me for task attempt_20140131132522179_0022_m_000001_1 when no start time is set, stackTrace is : java                                                           .lang.Exception
    [junit]     at org.apache.hadoop.mapred.TaskStatus.setFinishTime(TaskStatus.java:145)
    [junit]     at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.kill(TaskTracker.java:3094)
    [junit]     at org.apache.hadoop.mapred.TaskTracker$5.run(TaskTracker.java:2392)
    [junit]     at java.lang.Thread.run(Thread.java:722)
    [junit]
","02/Feb/14 09:05;rohini;bq. And if we don't make loadKey and noWal arguments optional (that is if we make 2nd argument of addOption true) then old code that doesn't specify true|false breaks 
   validOptions_.addOption(""loadKey"", false, ""Load Key"");
   if (""true"".equalsIgnoreCase(value) || value == null) {//the null check is for backward compat.

Just realized that -loadKey and -noWAL were command line switches/flags instead of options which take arguments even though the document said otherwise. Seeing the code for ignoreWhitespace which takes argument value, I had just assumed that -loadKey and -noWAL also took argument  but was not honoring those argument values. Wish all of them were consistent. Changing documentation now to say that they are only flags will still make people looking at older release documentation make mistakes or will have them confuse with -ignoreWhiteSpace and -cacheBlocks which always take arguments. Your patch takes care of it both ways (either as flag or as an option taking argument). So I guess we are good. Patch looks good as well. 

TestHBaseStorage fails for me as well. When doing svn up -r 1553724 which is the last checkin in http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java, the tests pass. Initially thought it was due to PIG-3573 changing zookeeper version to 3.4.5, but even reverting it to 3.4.4 the tests fail.  Will commit once I figure out the cause of the failure.","02/Feb/14 09:07;rohini;[~nezihyigitbasi],
   There are 6 failures and 2 errors with this patch when applying it after svn up -r 1553724. Can you take a look at those failures?","02/Feb/14 11:58;nezihyigitbasi;Rohini, thanks for running the TestHBaseStorage for me. Finally I also got it working (needed to switch to JDK 6, found out that this is a bug ZOOKEEPER-1550). Anyways, fixed the issue & all unit tests pass now.","05/Feb/14 03:25;rohini;+1. Committed to trunk. Thanks Nezih

The TestHBaseStorage still fails in the trunk. Passes fine when reverting to the old revision with this patch. Will address that in a separate jira. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python Avro library can't read Avros made with builtin AvroStorage,PIG-3621,12684414,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,russell.jurney,russell.jurney,russell.jurney,13/Dec/13 05:07,15/Apr/14 20:44,14/Mar/19 03:07,19/Dec/13 03:03,0.12.0,,,,,,,0.12.1,0.13.0,,,internal-udfs,,,0,,,,,,,,,,,,,"Using this script:

from avro import schema, datafile, io
import pprint
import sys
import json

field_id = None
# Optional key to print
if (len(sys.argv) > 2):
  field_id = sys.argv[2]

# Test reading avros
rec_reader = io.DatumReader()

# Create a 'data file' (avro file) reader
df_reader = datafile.DataFileReader(
  open(sys.argv[1]),
  rec_reader
)

the last line fails with:

Traceback (most recent call last):
  File ""/Users/rjurney/bin/cat_avro"", line 22, in <module>
    rec_reader
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/avro/datafile.py"", line 247, in __init__
    self.datum_reader.writers_schema = schema.parse(self.get_meta(SCHEMA_KEY))
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/avro/schema.py"", line 784, in parse
    return make_avsc_object(json_data, names)
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/avro/schema.py"", line 740, in make_avsc_object
    return RecordSchema(name, namespace, fields, names, type, doc, other_props)
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/avro/schema.py"", line 653, in __init__
    other_props)
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/avro/schema.py"", line 294, in __init__
    new_name = names.add_name(name, namespace, self)
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/avro/schema.py"", line 268, in add_name
    raise SchemaParseException(fail_msg)
avro.schema.SchemaParseException: record is a reserved type name.",,,,,,,,,,,,,,,,,,,AVRO-1412,,,,,,,,,,,,,19/Dec/13 02:11;cheolsoo;PIG-3621-3.patch;https://issues.apache.org/jira/secure/attachment/12619454/PIG-3621-3.patch,18/Dec/13 23:34;cheolsoo;PIG-3631-2.patch;https://issues.apache.org/jira/secure/attachment/12619437/PIG-3631-2.patch,18/Dec/13 21:47;russell.jurney;PIG-3631.patch;https://issues.apache.org/jira/secure/attachment/12619413/PIG-3631.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-12-18 23:34:38.587,,,no_permission,,,,,,,,,,,,363486,,,,Thu Dec 19 03:12:56 UTC 2013,,,,,,,0|i1qnlj:,363792,,,,,,,,,,"18/Dec/13 21:45;russell.jurney;I discovered the bug. Line 106 of AvroStorage.java uses the word ""record"" as the schema name. This is a reserved word in the Python implementation. Changing this name makes records from AvroStorage read fine by Python Avro.

Patch coming.",18/Dec/13 21:47;russell.jurney;One-line patch that changes the name of the schema to a non-reserved word.,"18/Dec/13 23:34;cheolsoo;Thank you for identifying the bug.

I think using a more pig specific name is better. Looking at the code, it seems that ""pig_output"" is supposed to be used when no schema name is specified. I am attaching a new patch that sets the default schema name to ""pig_output"".

I will commit this if there is no objection.

","19/Dec/13 01:23;dvryaboy;Uh, no thanks :)","19/Dec/13 01:25;dvryaboy;Sorry, that was a ""no"" to the assignment. Cheolsoo, does that var get set elsewhere? Why remove the logic for checking empty string, etc, and using a default?","19/Dec/13 02:11;cheolsoo;I thought that logic is redundant since 1) it is not possible to set schemaName to null, and 2) an empty schemaName is fine. But clearly, I am wrong about #2. It can result in an error. Here is an example-
{code}
a = LOAD 'foo' AS (i:int);
STORE a INTO 'bar' USING AvroStorage('');
{code}
I am putting the logic back in a new patch.

Thank you Dmitriy for catching that!",19/Dec/13 02:23;dvryaboy;+1,19/Dec/13 03:03;cheolsoo;Committed to trunk. Thank you Russell and Dmitriy.,19/Dec/13 03:12;cheolsoo;Committed to 0.12.1 too.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
problem with temp file deletion in MAPREDUCE operator,PIG-3617,12683914,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nezihyigitbasi,nezihyigitbasi,nezihyigitbasi,10/Dec/13 20:26,07/Jul/14 18:07,14/Mar/19 03:07,09/Jan/14 00:40,0.12.0,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"Hi all,
When I run a native MR job with the MAPREDUCE keyword and store the intermediate data in HBase with:
    stored = MAPREDUCE 'my.jar'
              STORE x INTO 'hbase://temp_table'
              USING org.apache.pig.backend.hadoop.hbase.HBaseStorage('hbase_schema')
              .... and the rest ....;

Pig tries to delete the temp files, which in this case has an HBase path, and fails with the exception:

Caused by: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:hbase:/temp_table
        at org.apache.hadoop.fs.Path.initialize(Path.java:148)
        at org.apache.hadoop.fs.Path.<init>(Path.java:126)
        at org.apache.pig.backend.hadoop.datastorage.HDataStorage.isContainer(HDataStorage.java:197)
        at org.apache.pig.backend.hadoop.datastorage.HDataStorage.asElement(HDataStorage.java:128)
        at org.apache.pig.impl.io.FileLocalizer.delete(FileLocalizer.java:415)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:419)
        at org.apache.pig.PigServer.launchPlan(PigServer.java:1322)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Jan/14 23:23;cheolsoo;PIG-3617-2.patch;https://issues.apache.org/jira/secure/attachment/12622061/PIG-3617-2.patch,06/Jan/14 17:55;nezihyigitbasi;PIG-3617.patch;https://issues.apache.org/jira/secure/attachment/12621630/PIG-3617.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-12-29 07:15:39.639,,,no_permission,,,,,,,,,,,,362986,,,,Wed Mar 12 18:42:25 UTC 2014,,,Patch Available,,,,0|i1qkj3:,363292,,,,,,,,,,"12/Dec/13 23:16;nezihyigitbasi;Guys, I plan to implement a solution that checks for an hbase prefix (""hbase://"") in the fileSpec variable (see FileLocalizer.delete), but this method currently uses the DataStorage interfaces (either local or distributed) kept in PigContext to access the backend storage and as far as I can see there is no HBase specific implementation for the DataStorage interface. So what do you guys think is the right way to fix this issue? Is it implementing a DataStorage interface for HBase or simply use HBaseAdmin.deleteTable to delete the temp table?


","29/Dec/13 07:15;cheolsoo;[~nezihyigitbasi], sorry for the late reply.

I see why you propose implementing a DataStorage interface for HBase, but that sounds like an overkill for this problem. Do you think that will be useful in the future for other things?

In addition, how about handling an hbase prefix in MapReduceLauncher rather than in FileLocalizer? PIG-3592 fixes a similar issue, and it does in MapReducerLauncher. I think it is better to do storage-specific things in Launcher than in FileLocalizer because the latter currently assumes file system.

Feel free to disagree with me. Thanks!

","06/Jan/14 17:55;nezihyigitbasi;Cheolsoo, thanks for the reply. I also think that implementing a DataStorage interface for HBase is an overkill. So I implemented a simple fix to skip HBase paths during temporary file deletions in MapReduceLauncher like you proposed. Please review.
","08/Jan/14 23:23;cheolsoo;[~nezihyigitbasi], thank you very much for the patch. It looks good, but can I make a minor suggestion?

How about using Utils.hasFileSystemImpl() instead of checking whether a path starts with ""hbase""? I think this approach is better because then other non-filesystem storages such as accumulo will also be protected from the same problem.

PIG-3617-2.patch implements what I describe here. I can commit it if you agree. Thanks!","08/Jan/14 23:47;nezihyigitbasi;Yes, that's even better! Thanks.",09/Jan/14 00:40;cheolsoo;Committed to trunk. Thank you Nezih!,"12/Mar/14 17:08;rohini;Just found out that with HADOOP-7549 in hadoop-2 branch, Utils.hasFileSystemImpl() is broken. Will open a jira to fix it.","12/Mar/14 17:58;nezihyigitbasi;Aha, good catch.",12/Mar/14 18:42;rohini;Will fix that as part of PIG-3672,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestBuiltIn.testURIwithCurlyBrace() silently fails,PIG-3616,12683851,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,lbendig,lbendig,lbendig,10/Dec/13 15:09,07/Jul/14 18:07,14/Mar/19 03:07,29/Dec/13 22:50,0.12.0,,,,,,,0.13.0,,,,,,,0,test,,,,,,,,,,,,"This test runs against MiniCluster but takes the input from the local path.
The empty catch block swallows the exception (""input path does not exist"") thus making a false negative result.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Dec/13 06:40;cheolsoo;PIG-3616-2.patch;https://issues.apache.org/jira/secure/attachment/12620754/PIG-3616-2.patch,10/Dec/13 15:11;lbendig;PIG-3616.patch;https://issues.apache.org/jira/secure/attachment/12618047/PIG-3616.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-12-29 06:22:57.646,,,no_permission,,,,,,,,,,,,362923,,,,Sun Dec 29 22:58:08 UTC 2013,,,,,,,0|i1qk5j:,363229,,,,,,,,,,"29/Dec/13 06:22;cheolsoo;[~lbendig], thank you for finding this. I see the problem.

I have two minor comments on your patch-
# Can we generate the input file to local instead of making the test run in mr mode? I prefer local mode to mr mode if possible since the latter is slower.
# Can we move FileLocalizer.reset(null) to setup()? That will let not only this test but also every test run w/o a side effect in the future.

I am uploading a patch that addresses my comments and will commit this if you have no objection. Let me know.
",29/Dec/13 19:35;rohini;+1,29/Dec/13 22:50;cheolsoo;Committed to trunk. Thank you Lorand and Rohini!,"29/Dec/13 22:58;lbendig;Sure, I had no objections. Thank you for the update!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Storing schema does not work cross cluster with PigStorage and JsonStorage,PIG-3612,12682927,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,05/Dec/13 15:46,07/Jul/14 18:07,14/Mar/19 03:07,06/Dec/13 01:04,,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"java.lang.IllegalArgumentException: Wrong FS:
hdfs://cluster2-nn:8020/tmp/output/.pig_schema,
expected: hdfs://cluster1-nn1
    at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:582)
    at
org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:155)
    at
org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:734)
    at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1272)
    at
org.apache.pig.backend.hadoop.datastorage.HDataStorage.isContainer(HDataStorage.java:200)
    at
org.apache.pig.backend.hadoop.datastorage.HDataStorage.asElement(HDataStorage.java:128)
    at
org.apache.pig.backend.hadoop.datastorage.HDataStorage.asElement(HDataStorage.java:144)
    at org.apache.pig.builtin.JsonMetadata.storeSchema(JsonMetadata.java:294)
    at org.apache.pig.builtin.JsonStorage.storeSchema(JsonStorage.java:274)
    at",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Dec/13 15:51;rohini;PIG-3612-1.patch;https://issues.apache.org/jira/secure/attachment/12617178/PIG-3612-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-05 23:31:51.302,,,no_permission,,,,,,,,,,,,362184,Reviewed,,,Fri Dec 06 01:04:12 UTC 2013,,,,,,,0|i1qfk7:,362479,,,,,,,,,,"05/Dec/13 15:51;rohini; Just a minor constructor change to get the correct HDataStorage based on URI. Reading from schema file, was already getting correct HDataStorage based on URI. Only store had issue.",05/Dec/13 23:31;aniket486;+1,06/Dec/13 01:04;rohini;Committed to trunk (0.13). Thanks for the review Aniket.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassCastException when calling compareTo method on AvroBagWrapper ,PIG-3609,12682761,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,rding,rding,rding,04/Dec/13 20:03,07/Jul/14 18:07,14/Mar/19 03:07,24/Dec/13 04:53,0.12.0,,,,,,,0.13.0,,,,impl,,,0,,,,,,,,,,,,,"One got the following exception when calling compareTo method on AvroBagWrapper with an AvroBagWrapper object:

{code}
java.lang.ClassCastException: org.apache.pig.impl.util.avro.AvroBagWrapper incompatible with java.util.Collection
        at org.apache.avro.generic.GenericData.compare(GenericData.java:786)
        at org.apache.avro.generic.GenericData.compare(GenericData.java:760)
        at org.apache.pig.impl.util.avro.AvroBagWrapper.compareTo(AvroBagWrapper.java:78)
{code}

Looking at the code, it compares objects with different types:

{code}
return GenericData.get().compare(theArray, o, theArray.getSchema());
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Dec/13 00:59;rding;PIG-3609.patch;https://issues.apache.org/jira/secure/attachment/12617283/PIG-3609.patch,06/Dec/13 22:42;rding;PIG-3609_2.patch;https://issues.apache.org/jira/secure/attachment/12617491/PIG-3609_2.patch,18/Dec/13 22:11;cheolsoo;PIG-3609_3.patch;https://issues.apache.org/jira/secure/attachment/12619420/PIG-3609_3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-12-18 22:08:38.678,,,no_permission,,,,,,,,,,,,362018,,,,Tue Dec 24 04:53:07 UTC 2013,,,,,,,0|i1qejb:,362313,,,,,,,,,,06/Dec/13 00:59;rding;Attaching a patch.,06/Dec/13 22:42;rding;New patch with a test case.,"18/Dec/13 22:08;cheolsoo;[~rding], I don't think the following is correct-
{code}
+      AvroBagWrapper bOther = (AvroBagWrapper) o;
+      if (this.size() != bOther.size()) {
+          if (this.size() > bOther.size()) return 1;
+          else return -1;
+      }
+      return GenericData.get().compare(theArray, bOther.theArray, theArray.getSchema());
{code}
When comparing arrays, we should compare entries one by one before comparing the size of arrays, shouldn't we? At least, that's how GenericData#compare() works for arrays-
{code}
    case ARRAY:
      Collection a1 = (Collection)o1;
      Collection a2 = (Collection)o2;
      Iterator e1 = a1.iterator();
      Iterator e2 = a2.iterator();
      Schema elementType = s.getElementType();
      while(e1.hasNext() && e2.hasNext()) {
        int compare = compare(e1.next(), e2.next(), elementType, equals);
        if (compare != 0) return compare;
      }
      return e1.hasNext() ? 1 : (e2.hasNext() ? -1 : 0);
{code}
I think we should just call GenericData#compare() w/o comparing the size.

In addition, I found your test case a bit confusing. Since getExpect() returns a set which doesn't guarantee any ordering, it's not clear how arrays are mapped to bw0, bw1, and bw2. That makes it hard to reason about the assertions.",18/Dec/13 22:11;cheolsoo;I am attaching a patch that addresses my comments. I'll commit this if you agree. Thank!,"24/Dec/13 00:14;rding;[~cheolsoo], checking size is an optimization, this is also what DefaultAbstractBag implements. 

+1 on the patch.",24/Dec/13 04:53;cheolsoo;Committed to trunk. Thank you Richard!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassCastException when looking up a value from AvroMapWrapper using a Utf8 key,PIG-3608,12682759,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,rding,rding,rding,04/Dec/13 19:57,07/Jul/14 18:08,14/Mar/19 03:07,28/Dec/13 04:54,0.12.0,,,,,,,0.13.0,,,,impl,,,0,,,,,,,,,,,,,"One got the following exception:

{code}
java.lang.ClassCastException: org.apache.avro.util.Utf8 incompatible with java.lang.String 
    at org.apache.pig.impl.util.avro.AvroMapWrapper.get(AvroMapWrapper.java:80)
{code}

This is related to the change by PIG-3420.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Dec/13 00:04;rding;PIG-3608.patch;https://issues.apache.org/jira/secure/attachment/12617275/PIG-3608.patch,06/Dec/13 18:17;rding;PIG-3608_2.patch;https://issues.apache.org/jira/secure/attachment/12617431/PIG-3608_2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-12-06 00:28:16.725,,,no_permission,,,,,,,,,,,,362016,Reviewed,,,Sun Dec 29 19:16:06 UTC 2013,,,,,,,0|i1qeiv:,362311,"Committed to trunk.
",,,,,,,,,06/Dec/13 00:04;rding;Attach a simple patch.,06/Dec/13 00:28;rohini;Code looks good. Can we just have a unit testcase added or modified to have this case so that some other change does not break it?,"06/Dec/13 01:22;rding;Actually I have a question: should it be

{code}
    if (isUtf8key) {
      v = innerMap.get(key);
    } else {
      v = innerMap.get(new Utf8((String) key));
    }
{code}

since isUft8key == true means the key is already Utf8?","06/Dec/13 04:00;rohini;With the testcase in PIG-3420, the key in AvroMapper was utf8 (i.e isUtf8key=true), but the pig map had the key passed as String.  I don't know in what case you are getting ClassCastException. The full stack trace and pig script would help to know what is happening.",06/Dec/13 18:17;rding;You are right. Update the patch with a test case.,"06/Dec/13 23:28;rohini; - Can we change ""if (isUtf8key && !(key instanceof Utf8))""  to ""if (isUtf8key && (key instanceof String))"". Previous code was assuming it was always a String key. This would be a better check. 
 - Also can we have a pig script as test case instead of just testing the method. Because I still don't get in which case the key is being Utf8 when pig tries to access the map. Having that test might cover other code paths as well. Or are you hitting this error when calling the class standalone outside of pig?","24/Dec/13 00:30;rding;Thanks for reviewing the patch.

Right now I don't have a Pig script to demonstrate this use case. I'm getting this problem while trying to iterate an instance of AvroMapWrapper and find out that I can't look up the value from the map using the key just retrieved from the map. I think this breaks the basic contract of a map implementation.

I think the check

{code}
if (isUtf8key && !(key instanceof Utf8))
{code}

is more general. But I'm ok if it is restricted to String.
",27/Dec/13 06:09;rohini;+1,29/Dec/13 05:12;cheolsoo;CHANGES.txt was not updated at the commit. I fixed it.,29/Dec/13 19:16;rding;Thanks [~cheolsoo].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigRecordReader should report progress for each inputsplit processed,PIG-3607,12682727,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,04/Dec/13 17:18,07/Jul/14 18:08,14/Mar/19 03:07,06/Dec/13 00:55,0.11.1,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,," Currently progress() is called only when records are processed. In a case where there were lot of empty input files, the task timed out and was killed because no progress was reported.  Too many empty input files are bad, but we still don't want to fail. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Dec/13 15:53;rohini;PIG-3607-1.patch;https://issues.apache.org/jira/secure/attachment/12617179/PIG-3607-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-05 23:52:45.575,,,no_permission,,,,,,,,,,,,361984,,,,Fri Dec 06 00:55:58 UTC 2013,,,,,,,0|i1qebr:,362279,,,,,,,,,,05/Dec/13 23:52;aniket486;+1,06/Dec/13 00:55;rohini;Committed to trunk (0.13). Thanks Aniket for the review.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig script throws error when searching for hcatalog jars in latest hive,PIG-3606,12682567,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,deepesh,deepesh,deepesh,03/Dec/13 23:28,07/Jul/14 18:08,14/Mar/19 03:07,07/Feb/14 07:22,0.12.0,,,,,,,0.13.0,,,,tools,,,0,,,,,,,,,,,,,"When pig CLI is run with -useHCatalog flag against latest hive binaries it complains about the following:
{noformat}
ls: cannot access /usr/lib/hcatalog/share/hcatalog/hcatalog-core-*.jar: No such file or directory
ls: cannot access /usr/lib/hcatalog/share/hcatalog/hcatalog-*.jar: No such file or directory
ls: cannot access /usr/lib/hcatalog/share/hcatalog/hcatalog-pig-adapter-*.jar: No such file or directory
{noformat}
Basically it searches for hcatalog jars as ""hcatalog\-{core,pig-adapter}\-*.jar"" whereas in the latest hive 0.13 these jars are prefixed by ""hive\-"" (hive\-hcatalog\-{core,pig-adapter}\-*.jar).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Dec/13 23:30;deepesh;PIG-3606.patch;https://issues.apache.org/jira/secure/attachment/12616878/PIG-3606.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-07 07:22:41.996,,,no_permission,,,,,,,,,,,,361824,Reviewed,,,Fri Feb 07 07:22:41 UTC 2014,,,,,,,0|i1qdcn:,362121,,,,,,,,,,03/Dec/13 23:30;deepesh;Attached a patch with the fix.,07/Feb/14 07:22;daijy;+1. Patch committed to trunk. Thanks Deepesh!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Import jython standard module fail on cluster,PIG-3593,12681656,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,27/Nov/13 20:56,15/Apr/14 20:45,14/Mar/19 03:07,28/Nov/13 22:53,,,,,,,,0.12.1,,,,impl,,,0,,,,,,,,,,,,,"The following script fail on cluster:
{code}
import urllib

@outputSchema(""url:chararray"")
def urlDecode(str):
    return urllib.unquote_plus( str )
{code}

{code}
register '126.py' using jython as myfuncs;

a = load 'studenttab10k' using PigStorage() as (name:chararray, age:int, gpa:double);
b = foreach a generate myfuncs.urlDecode(name);
dump b;
{code}

Error stack:

java.io.IOException: Deserialization error: could not instantiate 'org.apache.pig.scripting.jython.JythonFunction' with arguments '[127.py, resplit]'
	at org.apache.pig.impl.util.ObjectSerializer.deserialize(ObjectSerializer.java:59)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.setup(PigGenericMapBase.java:180)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:394)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)
Caused by: java.lang.RuntimeException: could not instantiate 'org.apache.pig.scripting.jython.JythonFunction' with arguments '[127.py, resplit]'
	at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:727)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.instantiateFunc(POUserFunc.java:126)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.readObject(POUserFunc.java:567)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:979)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1873)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:349)
	at java.util.ArrayList.readObject(ArrayList.java:593)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:979)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1873)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:349)
	at java.util.HashMap.readObject(HashMap.java:1030)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:979)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1873)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1970)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1895)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1970)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1895)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:349)
	at java.util.ArrayList.readObject(ArrayList.java:593)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:979)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1873)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1970)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1895)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:349)
	at java.util.HashMap.readObject(HashMap.java:1030)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:979)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1873)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1970)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1895)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:349)
	at org.apache.pig.impl.util.ObjectSerializer.deserialize(ObjectSerializer.java:57)
	... 9 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:695)
	... 75 more
Caused by: java.lang.IllegalStateException: Could not initialize: 127.py
	at org.apache.pig.scripting.jython.JythonFunction.<init>(JythonFunction.java:92)
	... 80 more
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 1121: Python Error. Traceback (most recent call last):
  File ""127.py"", line 2, in <module>
    import re
  File ""__pyclasspath__/re$py.class"", line 279, in <module>
java.lang.ArrayIndexOutOfBoundsException: 10
	at org.python.objectweb.asm.ClassReader.a(Unknown Source)
	at org.python.objectweb.asm.ClassReader.accept(Unknown Source)
	at org.python.objectweb.asm.ClassReader.accept(Unknown Source)
	at org.python.core.AnnotationReader.<init>(AnnotationReader.java:44)
	at org.python.core.imp.readCode(imp.java:219)
	at org.python.core.util.importer.getModuleCode(importer.java:202)
	at org.python.core.util.importer.importer_load_module(importer.java:95)
	at org.python.core.ClasspathPyImporter.ClasspathPyImporter_load_module(ClasspathPyImporter.java:63)
	at org.python.core.ClasspathPyImporter$ClasspathPyImporter_load_module_exposer.__call__(Unknown Source)
	at org.python.core.PyBuiltinMethodNarrow.__call__(PyBuiltinMethodNarrow.java:47)
	at org.python.core.imp.loadFromLoader(imp.java:518)
	at org.python.core.imp.find_module(imp.java:472)
	at org.python.core.imp.import_next(imp.java:718)
	at org.python.core.imp.import_module_level(imp.java:827)
	at org.python.core.imp.importName(imp.java:917)
	at org.python.core.ImportFunction.__call__(__builtin__.java:1220)
	at org.python.core.PyObject.__call__(PyObject.java:357)
	at org.python.core.__builtin__.__import__(__builtin__.java:1173)
	at org.python.core.imp.importOne(imp.java:936)
	at re$py.f$0(/home/frank/hg/jython/jython/dist/Lib/re.py:289)
	at re$py.call_function(/home/frank/hg/jython/jython/dist/Lib/re.py)
	at org.python.core.PyTableCode.call(PyTableCode.java:165)
	at org.python.core.PyCode.call(PyCode.java:18)
	at org.python.core.imp.createFromCode(imp.java:391)
	at org.python.core.util.importer.importer_load_module(importer.java:109)
	at org.python.core.ClasspathPyImporter.ClasspathPyImporter_load_module(ClasspathPyImporter.java:63)
	at org.python.core.ClasspathPyImporter$ClasspathPyImporter_load_module_exposer.__call__(Unknown Source)
	at org.python.core.PyBuiltinMethodNarrow.__call__(PyBuiltinMethodNarrow.java:47)
	at org.python.core.imp.loadFromLoader(imp.java:518)
	at org.python.core.imp.find_module(imp.java:472)
	at org.python.core.imp.import_next(imp.java:718)
	at org.python.core.imp.import_module_level(imp.java:827)
	at org.python.core.imp.importName(imp.java:917)
	at org.python.core.ImportFunction.__call__(__builtin__.java:1220)
	at org.python.core.PyObject.__call__(PyObject.java:357)
	at org.python.core.__builtin__.__import__(__builtin__.java:1173)
	at org.python.core.imp.importOne(imp.java:936)
	at org.python.pycode._pyx3.f$0(127.py:3)
	at org.python.pycode._pyx3.call_function(127.py)
	at org.python.core.PyTableCode.call(PyTableCode.java:165)
	at org.python.core.PyCode.call(PyCode.java:18)
	at org.python.core.Py.runCode(Py.java:1275)
	at org.python.util.PythonInterpreter.execfile(PythonInterpreter.java:235)
	at org.apache.pig.scripting.jython.JythonScriptEngine$Interpreter.execfile(JythonScriptEngine.java:217)
	at org.apache.pig.scripting.jython.JythonScriptEngine$Interpreter.init(JythonScriptEngine.java:163)
	at org.apache.pig.scripting.jython.JythonScriptEngine.getFunction(JythonScriptEngine.java:388)
	at org.apache.pig.scripting.jython.JythonFunction.<init>(JythonFunction.java:55)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:695)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.instantiateFunc(POUserFunc.java:126)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.readObject(POUserFunc.java:567)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:979)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1873)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:349)
	at java.util.ArrayList.readObject(ArrayList.java:593)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:979)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1873)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:349)
	at java.util.HashMap.readObject(HashMap.java:1030)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:979)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1873)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1970)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1895)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1970)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1895)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:349)
	at java.util.ArrayList.readObject(ArrayList.java:593)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:979)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1873)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1970)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1895)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:349)
	at java.util.HashMap.readObject(HashMap.java:1030)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:979)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1873)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1970)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1895)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:349)
	at org.apache.pig.impl.util.ObjectSerializer.deserialize(ObjectSerializer.java:57)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.setup(PigGenericMapBase.java:180)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:394)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)

Seems objectweb asm does not like repackaged jython libraries. 

We have similar tests in TestScriptUDF with MiniCluster but those are running fine. Those are added by PIG-1824 and that ticket seems solves the issue. But I check all released versions since 0.10.0, none is working on cluster. I am not sure whether we fixed issue once in a while then break it, or we didn't solve the issue on the cluster.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Nov/13 20:59;daijy;PIG-3593-1.patch;https://issues.apache.org/jira/secure/attachment/12616116/PIG-3593-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-27 21:09:03.722,,,no_permission,,,,,,,,,,,,360920,Reviewed,,,Thu Nov 28 22:53:09 UTC 2013,,,,,,,0|i1q7sv:,361219,,,,,,,,,,"27/Nov/13 20:59;daijy;asm does not complain if we ship jython.jar as a single unit and put in distributed cache. Attach patch. Since this issue only manifest on cluster, and similar tests on MiniCluster pass, I don't include a test case here.",27/Nov/13 21:09;rohini;+1. Just a minor comment. You don't have to do scriptJar.toString() as it is already a string,"27/Nov/13 21:21;rohini;bq. I am not sure whether we fixed issue once in a while then break it, or we didn't solve the issue on the cluster.
  This issue does not happen always. Only happens when there are imports within the module that you are importing and happens in special cases. We also hit a similar error, but that went away when we did a register of the jython jar in the script. This would help get rid of the extra register we did to workaround the problem. ","28/Nov/13 22:53;daijy;In my case, even register fail. job.jar always take precedence.

Patch committed to 0.12 branch and trunk. Thanks Rohini for quick review!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should not try to create success file for non-fs schemes like hbase,PIG-3592,12681625,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,27/Nov/13 18:15,15/Apr/14 20:45,14/Mar/19 03:07,18/Dec/13 23:28,0.11.1,,,,,,,0.12.1,0.13.0,,,,,,0,,,,,,,,,,,,,"Having mapreduce.fileoutputcommitter.marksuccessfuljobs set to true and having both fs path (where you want _SUCCESS file created) and hbase in STORE statements results in 

Caused by: java.io.IOException: No FileSystem for scheme: hbase
    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2157)
    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:85)
    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2194)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2176)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:306)
    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:194)
    at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.createSuccessFile(MapReduceLauncher.java:651)
    at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:434)
    at org.apache.pig.PigServer.launchPlan(PigServer.java:1283)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Nov/13 21:44;rohini;PIG-3592-1.patch;https://issues.apache.org/jira/secure/attachment/12616132/PIG-3592-1.patch,27/Nov/13 21:44;rohini;PIG-3592-branch12-1.patch;https://issues.apache.org/jira/secure/attachment/12616133/PIG-3592-branch12-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-11-27 21:56:46.8,,,no_permission,,,,,,,,,,,,360889,Reviewed,,,Wed Dec 18 23:28:21 UTC 2013,,,,,,,0|i1q7lz:,361188,,,,,,,,,,27/Nov/13 21:44;rohini;Attached patches for trunk and branch-12,27/Nov/13 21:56;aniket486;+1,15/Dec/13 20:24;rohini;Saw some existing test failures with TestHBaseStorage when I was running tests before commit. Will get back to this shortly when I get time to investigate them. Cancelling patch for now.,"18/Dec/13 23:28;rohini;The test failures were due to my recent Mac upgrade. Running on Linux they were fine.

Committed to branch-0.12 and trunk (0.13). Thanks Aniket for the review.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor POPackage to separate MR specific code from packaging,PIG-3591,12681618,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,mwagner,mwagner,mwagner,27/Nov/13 17:53,06/Jan/17 21:35,14/Mar/19 03:07,06/Apr/14 10:59,,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,POPackage is currently closely associated with the MR shuffle semantics. This makes it difficult to adapt the variety of subclasses of POPackage to other execution engines without duplicating code.,,,,,,,,,,,,,,,,,,,PIG-4683,PIG-3595,,,,,PIG-5083,,,,PIG-3600,,,27/Nov/13 18:07;mwagner;PIG-3591.1.patch;https://issues.apache.org/jira/secure/attachment/12616085/PIG-3591.1.patch,28/Nov/13 00:44;mwagner;PIG-3591.2.patch;https://issues.apache.org/jira/secure/attachment/12616166/PIG-3591.2.patch,05/Apr/14 02:27;mwagner;PIG-3591.5.patch;https://issues.apache.org/jira/secure/attachment/12638829/PIG-3591.5.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-04-05 02:45:10.569,,,no_permission,,,,,,,,,,,,360882,,,,Sun Apr 06 10:59:47 UTC 2014,,,,,,,0|i1q7kn:,361181,,,,,,,,,,"27/Nov/13 18:07;mwagner;Separate ""packaging"" logic from ""shuffle handling"" logic. This moves the packaging logic to a new class ""Packager"", which is extended by CombinePackager, LitePackager, MultiQueryPackager, and JoinPackager.

This is not finished. Known problems are illustrate and streaming the last input are not implemented.",05/Apr/14 02:27;mwagner;Here's the latest patch.,"05/Apr/14 02:45;cheolsoo;+1. Thank you so much for the great work! I have one minor comment below.

When you commit, do you mind fixing the following test case? Shouldn't ""distinct A"" to be ""order A by x""? I see another testDistinct() in the test suite that tests distinct, so it looks like this test case is supposed to test order by. Please correct me if I am wrong.
{code:title=TestExampleGenerator.java}
+    @Test
+    public void testOrderBy() throws Exception {
+        PigServer pigServer = new PigServer(pigContext);
+        pigServer.registerQuery(""A = load "" + A.toString() + "" as (x, y);"");
+        pigServer.registerQuery(""B = distinct A;"");
+        Map<Operator, DataBag> derivedData = pigServer.getExamples(""B"");
+
         assertNotNull(derivedData);
     }
{code}","06/Apr/14 10:59;cheolsoo;I confirmed all unit tests pass myself, so I went ahead committed the final patch (+ my comment above) to trunk. I will merge it down to tez branch later today.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove PartitionFilterOptimizer from trunk,PIG-3590,12681421,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,aniket486,aniket486,26/Nov/13 21:42,07/Jul/14 18:07,14/Mar/19 03:07,28/Nov/13 01:32,,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"Since we deprecated it in pig-0.12, we can remove it from the trunk and use only NewPartitionFilterOptimizer in pig.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Nov/13 00:56;aniket486;PIG-3590.patch;https://issues.apache.org/jira/secure/attachment/12616169/PIG-3590.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-28 00:35:17.479,,,no_permission,,,,,,,,,,,,360686,,,,Thu Nov 28 01:31:56 UTC 2013,,,,,,,0|i1q6d3:,360985,,,,,,,,,,"28/Nov/13 00:35;cheolsoo;[~aniket486], do you mind removing the comments in pig.properties too?
{code}
# Set this option to true if you need to use the old partition filter optimizer. 
# Note: Old filter optimizer PColFilterOptimizer will be deprecated in the future.
# pig.exec.useOldPartitionFilterOptimizer=true
{code}
Otherwise, LGTM!","28/Nov/13 00:38;aniket486;Yes, I missed that one. Will remove it. Thanks!",28/Nov/13 01:31;aniket486;Committed to trunk. Thanks for reviewing [~cheolsoo]! :),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroStorage does not correctly translate arrays of strings ,PIG-3584,12680018,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jadler,jadler,jadler,19/Nov/13 19:55,13/Nov/14 21:14,14/Mar/19 03:07,19/Dec/13 18:09,0.12.0,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"I found an issue this morning with AvroStorage and arrays of strings. Suppose that you have an Avro file with an array of strings like this:

{""name"" : ""example"",
 ""namespace"" : ""org.apache.pig.avro"",
 ""type"" : ""record"",
 ""fields"" : [
 {""name"" : ""arrayOfStrings"",
  ""type"" : {{""type"" : ""array"", ""items"" : ""string""}}}}

The current version of AvroStorage would translate this schema into a bag of tuples, each containing a single chararray field. 

The current version of AvroStorage will naively place each value in the array into a tuple inside a bag of values. Unfortunately, each value is a Utf8 item (not a Java String type). This can cause problems in later processing steps, because Pig does not know how to deal with Utf8 objects. 

I've written a patch for AvroStorage that will correctly translate objects in arrays into a form that Pig can process. (While I was at it, I made sure that we correctly translated other Avro types, including fixed, enums, and unions.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Nov/13 19:56;jadler;PIG-3584.patch;https://issues.apache.org/jira/secure/attachment/12614683/PIG-3584.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-18 22:47:47.586,,,no_permission,,,,,,,,,,,,359375,,,,Thu Dec 19 18:08:57 UTC 2013,,,Patch Available,,,,0|i1pybb:,359674,,,,,,,,,,19/Nov/13 19:56;jadler;Patch for this issue,18/Dec/13 22:47;cheolsoo;+1. I will commit this.,19/Dec/13 18:08;cheolsoo;Committed to trunk. Thanks Joe!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Document SUM, MIN, MAX, and AVG functions for BigInteger and BigDecimal",PIG-3582,12679788,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,harichinnan,cheolsoo,cheolsoo,18/Nov/13 18:24,07/Jul/14 18:08,14/Mar/19 03:07,20/Nov/13 18:34,0.12.0,,,,,,,0.13.0,,,,documentation,,,0,,,,,,,,,,,,,"PIG-3569 and PIG-3580 added support for BigDecimal and BigInteger to SUM, MIN, MAX, and AVG. The documentation needs to be updated. For eg,
{code}
An expression with data types int, long, float, double, or bytearray cast as double.
{code}
http://pig.apache.org/docs/r0.12.0/func.html#sum",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Nov/13 05:34;harichinnan;PIG-3582.patch;https://issues.apache.org/jira/secure/attachment/12614553/PIG-3582.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-19 05:34:41.158,,,no_permission,,,,,,,,,,,,359146,,,,Wed Nov 20 18:34:11 UTC 2013,,,,,,,0|i1pwwn:,359445,"documentation updated to reflect addition of bigdecimal and biginteger data types to SUM, AVG, MAX and MIN functions. ",,,,,,,,,19/Nov/13 05:34;harichinnan;Documentation updated. ,20/Nov/13 17:38;cheolsoo;+1. Looks great! Thank you [~harichinnan]!,20/Nov/13 18:34;cheolsoo;Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect scope resolution with nested foreach,PIG-3581,12679678,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,venusatuluri,venusatuluri,18/Nov/13 06:12,07/Jul/14 18:08,14/Mar/19 03:07,26/Nov/13 19:23,,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"Consider the following script:
{code}
A = LOAD 'test_data' AS (a: int, b: int);
C = FOREACH A GENERATE *;
B = FOREACH (GROUP A BY a) {
	C = FILTER A BY b % 2 == 0;
	D = FILTER A BY b % 2 == 1;
	GENERATE group AS a, A.b AS every, C.b AS even, D.b AS odd;
};
DESCRIBE B;
{code}
Notice that C is defined both inside the nested foreach as well as outside. I would expect that in the GENERATE inside the nested FOREACH, the C that is used will be the one that is defined inside. If that is not so, I think at least a warning is due.

However, currently Pig silently assumes that the C you mean one is the one that is defined *outside* the nested FOREACH.

Hence, the result of ""DESCRIBE B"" looks as follows:
{code}
B: {
    a: int,
    every: {
        (
            b: int
        )
    },
    even: int,
    odd: {
        (
            b: int
        )
    }
}
{code}

If I remove the definition of C that is outside the foreach, then I get the following for ""DESCRIBE B"":
{code}
B: {
    a: int,
    every: {
        (
            b: int
        )
    },
    even: {
        (
            b: int
        )
    },
    odd: {
        (
            b: int
        )
    }
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,PIG-3643,,,,,,,23/Nov/13 01:58;aniket486;PIG-3581-1.patch;https://issues.apache.org/jira/secure/attachment/12615434/PIG-3581-1.patch,26/Nov/13 02:22;aniket486;PIG-3581-2.patch;https://issues.apache.org/jira/secure/attachment/12615770/PIG-3581-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-11-25 18:02:49.318,,,no_permission,,,,,,,,,,,,359038,,,,Sat Dec 28 01:24:20 UTC 2013,,,,,,,0|i1pw5j:,359328,,,,,,,,,,"25/Nov/13 18:02;xuefuz;[~aniket] Could you please put a review board entry for this? Also, do you think if a test case would be helpful? Thanks.","25/Nov/13 18:43;aniket486;[~xuefuz], Thanks for looking into this. I will submit a review shortly.

It's a minor change, I'm just changing the order of name resolution to give priority to nested variable over scalar. Essentially following code -
{code}
// From --
if(1) doScalar
else if(2) doNested
else if(3) other

// To--
if(2) doNested
else if(1) doScalar
else if(3) other
{code}
If you insist, I can add a test for this, but I think we won't need it.","25/Nov/13 19:28;xuefuz;[~aniket] Thanks for your explanation. I agree that not every patch needs a test. While the change might be minor, it changed or corrected the behaviour. Unless we find it very difficult to construct a test, I'd think a test case is helpful in the sense that it not only provides a way to verify the fix but also prevents future break of this. As a fyi, there are some example of test cases in TestPigServer which might be followed.",26/Nov/13 02:23;aniket486;Thanks [~xuefuz]. Attached new patch with tests. RB: https://reviews.apache.org/r/15852/,26/Nov/13 15:33;xuefuz;[~aniket] Thanks for spending time on the test case. I posted a couple of comments on rb.,"26/Nov/13 18:51;aniket486;Thanks [~xuefuz], I do not understand your concern, scalars are allowed anywhere in the script and that change doesn't seem to affect that anyway.

[JFYI, you are looping in the wrong Aniket above.]","26/Nov/13 19:05;xuefuz;Okay. Then I was mistaken indeed. Patch looks good. +1.

My apology to another Aniket also. :)","26/Nov/13 19:22;aniket486;Thanks for the review, [~xuefuz]! :)
Committed to trunk..

","23/Dec/13 21:16;cheolsoo;[~aniket486], I think this patch introduced a regression. Consider the following query-
{code}
a = LOAD 'foo' AS (x:int, y:chararray);
b = GROUP a BY x;
c = FOREACH b {
    expr = 'bar';
    filtered = FILTER a BY y == expr;
    GENERATE COUNT(filtered);
}
DESCRIBE c;
{code}
This used to work in 0.11 but no longer works in trunk. It looks like 'expr' used to be resolved to a scalar expression ('bar'), but it's not the case anymore. 

My question are,
1. Is it supported to define a local scalar expression inside a nested foreach? e.g. expr = 'bar';
2. If so, can you fix the regression?","23/Dec/13 21:38;aniket486;Let me look into this in detail.

In general, expr = 'bar'; filtered = FILTER a BY y == expr; - using scalar is very bad (crazy). It is going to store the word 'bar' in a file on HDFS and read it back.","23/Dec/13 21:44;cheolsoo;I am not saying it's a good thing to do. I was answering a user questions on the mailing list and curious whether it's supported or not. For the context, see here-
http://www.mail-archive.com/user@pig.apache.org/msg08977.html","23/Dec/13 21:53;cheolsoo;{quote}
It is going to store the word 'bar' in a file on HDFS and read it back.
{quote}
Plus, I don't think this is true. See the explain output in 0.11. There is no hdfs reading for 'bar'. It's a constant-
{code}
c: Store(fakefile:org.apache.pig.builtin.PigStorage) - scope-20
|
|---c: New For Each(false)[bag] - scope-19
    |   |
    |   POUserFunc(org.apache.pig.builtin.COUNT)[long] - scope-13
    |   |
    |   |---RelationToExpressionProject[bag][*] - scope-12
    |       |
    |       |---filtered: Filter[bag] - scope-15
    |           |   |
    |           |   Equal To[boolean] - scope-18
    |           |   |
    |           |   |---Project[chararray][1] - scope-16
    |           |   |
    |           |   |---Constant(bar) - scope-17
    |           |
    |           |---Project[bag][1] - scope-14
    |
    |---b: Package[tuple]{int} - scope-9--------
{code}","23/Dec/13 22:01;aniket486;Understood. So, its a problem with nested_command resolution and not scalar resolution. I will submit a patch for this.","23/Dec/13 22:02;aniket486;bq. Is it supported to define a local scalar expression inside a nested foreach? e.g. expr = 'bar';
Yes. https://github.com/apache/pig/blob/branch-0.12/src/org/apache/pig/parser/QueryParser.g#L923
bq. If so, can you fix the regression?
Yes","28/Dec/13 01:24;cheolsoo;[~aniket486], I put up a fix in PIG-3643.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE due to PIG-3549 when job never gets submitted,PIG-3576,12679510,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,lbendig,aniket486,aniket486,15/Nov/13 22:30,15/Apr/14 20:45,14/Mar/19 03:07,18/Nov/13 18:30,,,,,,,,0.12.1,0.13.0,,,,,,0,,,,,,,,,,,,,"NPE can happen if job never gets submitted due to errors like - input path does not exists. Following line (toString method) throws npe-
{code}
msg.append(""JobID: "" + j.getAssignedJobID().toString() + "" Reason: "" + j.getMessage());
{code}

Apparently, a.toString can throw NPE, but String.valueOf(a) handles null well(writes ""null"" for null values).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Nov/13 21:46;aniket486;PIG-3576-1.patch;https://issues.apache.org/jira/secure/attachment/12616134/PIG-3576-1.patch,18/Nov/13 10:47;lbendig;PIG-3576.patch;https://issues.apache.org/jira/secure/attachment/12614368/PIG-3576.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-11-18 17:14:58.039,,,no_permission,,,,,,,,,,,,358870,,,,Wed Nov 27 21:47:23 UTC 2013,,,,,,,0|i1pv47:,359160,,,,,,,,,,18/Nov/13 17:14;cheolsoo;+1.,18/Nov/13 18:30;cheolsoo;Committed to trunk. Thank you Lorand!,18/Nov/13 18:35;aniket486; FYI- String.valueOf is implicit with + operator in java.,"27/Nov/13 20:19;aniket486;[~cheolsoo], we need this on pig-12 branch. Any objections?","27/Nov/13 21:11;cheolsoo;[~aniket486], no sir. Go ahead.","27/Nov/13 21:47;aniket486;Thanks [~cheolsoo], I've attached the new patch. Will commit it to branch-0.12 and trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rollback PIG-3060,PIG-3570,12679072,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,13/Nov/13 18:48,15/Apr/14 20:44,14/Mar/19 03:07,01/Apr/14 21:15,,,,,,,,0.12.1,,,,impl,,,0,,,,,,,,,,,,,"Will result missing records in some cases. One case is when we have two flatten in a single pipeline, when the first flatten still hold some records, the second flatten cannot return EOP just because an empty bag. Here is the test script:

{code}
a = load '1.txt' as (bag1:bag{(t:int)});
b = foreach a generate flatten(bag1) as field1;
c = foreach b generate flatten(GenBag(field1));
dump c;
{code}
GenBag:
{code}
public class GenBag extends EvalFunc<DataBag> {
        @Override
        public DataBag exec(Tuple input) throws IOException {
            Integer content = (Integer)input.get(0);
            DataBag bag = BagFactory.getInstance().newDefaultBag();

            if (content > 10) {
                Tuple t = TupleFactory.getInstance().newTuple();
                t.append(content);
                bag.add(t);
            }
            return bag;
        }
    }
{code}
Input:
{code}
{(1),(12),(9)}
{(15),(2)}
{code}

The test case in PIG-3060 fails if rollback, need to fix it when rollback.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Nov/13 18:49;daijy;PIG-3570-1.patch;https://issues.apache.org/jira/secure/attachment/12613651/PIG-3570-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-13 19:29:43.61,,,no_permission,,,,,,,,,,,,358437,,,,Tue Apr 01 21:14:58 UTC 2014,,,,,,,0|i1psfz:,358727,,,,,,,,,,13/Nov/13 19:29;cheolsoo;+1. Confirmed that TestEvalPipelineLocal passes.,"13/Nov/13 19:41;daijy;Thanks for quick review, Cheolsoo!

Committed to trunk and 0.12 branch. Shall I commit to tez branch, or wait for the merge?",13/Nov/13 19:45;cheolsoo;Let's merge trunk into tez branch. Let me do it now to unblock Mark. ,"13/Nov/13 19:50;cheolsoo;[~mwagner], [~daijy], done. This fix is in tez branch now.",13/Nov/13 22:02;daijy;Thanks!,01/Apr/14 21:04;prkommireddi;[~cheolsoo] can we mark this resolved?,"01/Apr/14 21:14;cheolsoo;Yes, we can. Closing it now. Thank you for reminding me of it!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Define the semantics of POStatus.STATUS_NULL,PIG-3568,12678923,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,mwagner,mwagner,mwagner,13/Nov/13 01:38,07/Jul/14 18:08,14/Mar/19 03:07,16/Nov/13 00:27,,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"The meaning of POStatus.STATUS_NULL is not well documented and there are conflicting view points on its interpretation. The two interpretations are:

1. POStatus.STATUS_NULL indicates that the pulled output IS null. This is mostly found in expression operators, particularly comparison operators.

2. POStatus.STATUS_NULL indicates that the pull did not produce any output. This is backed up by its usage in POPackage (not JoinPackage) for flattening an empty bag, and PigGenericMapBase where pulls on the operator pipeline that result in STATUS_NULL are discarded.

I propose that 2 should be the official definition going forward. The first meaning is easily indicated by (null, STATUS_OK) and all the relational operators already seem to follow 2. I'd like to hear others' opinions as well though.",,,,,,,,,,,,,,,,,,,,,,,,,PIG-3679,,,,,,,14/Nov/13 18:32;mwagner;PIG-3568.1.patch;https://issues.apache.org/jira/secure/attachment/12613891/PIG-3568.1.patch,15/Nov/13 23:21;mwagner;PIG-3568.2.patch;https://issues.apache.org/jira/secure/attachment/12614159/PIG-3568.2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-11-13 02:01:14.22,,,no_permission,,,,,,,,,,,,358289,,,,Sat Nov 16 00:27:51 UTC 2013,,,,,,,0|i1prj3:,358579,,,,,,,,,,"13/Nov/13 02:01;daijy;Agree with #1. For #2, seems that's not the intention for the current code. Though we can see it in POPackage, but the code path is not in use. But since we plan to clean up STATUS_NULL -> (null, STATUS_OK), we are free to redefine the meaning for STATUS_NULL (a different name might be better).","13/Nov/13 22:10;daijy;I think #2 is also fine. We do use it to signal a tuple need to be skipped in POForEach (which wrongly changed by PIG-3060, PIG-3570 roll it back). 

However, when we use STATUS_NULL to signal a skip tuple, STATUS_NULL need to go through the rest of pipeline. We might consider to contain STATUS_NULL in the operator in the future.",14/Nov/13 18:32;mwagner;Here's a patch for option #2. I've also created a RB: https://reviews.apache.org/r/15524/,"15/Nov/13 06:07;cheolsoo;Thank you for cleaning up this mess!

The patch looks good to me. I am running unit and e2e tests now.",15/Nov/13 23:21;mwagner;Updated in response to Daniel's comments on RB.,16/Nov/13 00:11;cheolsoo;+1. Will commit to trunk and merge down to tez branch.,16/Nov/13 00:27;cheolsoo;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogicalPlanPrinter throws OOM for large scripts,PIG-3567,12678715,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,aniket486,aniket486,12/Nov/13 01:33,12/Nov/14 07:55,14/Mar/19 03:07,26/Nov/13 20:20,,,,,,,,0.12.1,0.13.0,,,,,,0,,,,,,,,,,,,,"As mentioned in PIG-3455, LogicalPlanPrinter throws OOM for large scripts. Problem is LogicalPlanPrinter's visit method generates a large string before its written to the PrintStream.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Nov/13 03:08;aniket486;PIG-3567.patch;https://issues.apache.org/jira/secure/attachment/12613774/PIG-3567.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-11-08 07:02:07.529,,,no_permission,,,,,,,,,,,,358082,,,,Wed Nov 12 07:55:26 UTC 2014,,,,,,,0|i1pq93:,358372,,,,,,,,,,14/Nov/13 03:10;aniket486;Fixed it by making LogicalPlanPrinter write to stream recursively (patch attached). There are no existing tests for verifying printing for logical plan. Do we need to add tests for this(changes are non-trivial)? I verified with a few scripts that logical plan remains the same.,"26/Nov/13 20:20;aniket486;Committed to trunk and branch-0.12.

Thanks [~daijy] for the review.","08/Nov/14 07:02;skanda83;I'm now getting the following exception after applying the fix:

Pig Stack Trace
---------------
ERROR 2998: Unhandled internal error. Java heap space

java.lang.OutOfMemoryError: Java heap space
at java.util.Arrays.copyOf(Arrays.java:2271)
at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113)
at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:140)
at java.io.PrintStream.write(PrintStream.java:480)
at java.io.FilterOutputStream.write(FilterOutputStream.java:97)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.writeWithTabs(LogicalPlanPrinter.java:103)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:125)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
rg.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.writeWithTabs
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirstLP(LogicalPlanPrinter.java:97)
",12/Nov/14 07:55;skanda83;Opened PIG-4317,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot set useMatches of REGEX_EXTRACT_ALL and REGEX_EXTRACT,PIG-3566,12678114,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nezihyigitbasi,nezihyigitbasi,nezihyigitbasi,07/Nov/13 23:55,07/Jul/14 18:08,14/Mar/19 03:07,28/Nov/13 00:19,0.12.0,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"In the javadoc of REGEX_EXTRACT_ALL and REGEX_EXTRACT it says we can set the 'useMatches' property of these UDFs with DEFINE statements:
DEFINE GREEDY_EXTRACT REGEX_EXTRACT_ALL(false);
DEFINE NON_GREEDY_EXTRACT REGEX_EXTRACT(false);

However, both of these do not work (2013-11-07 15:35:15,107 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1200: <line 7, column 40>  mismatched input 'false' expecting RIGHT_PAREN)

These UDFs should take string arguments to their constructors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Nov/13 20:33;nezihyigitbasi;PIG-3566.1.patch;https://issues.apache.org/jira/secure/attachment/12616112/PIG-3566.1.patch,08/Nov/13 00:08;nezihyigitbasi;PIG-3566.patch;https://issues.apache.org/jira/secure/attachment/12612742/PIG-3566.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-11-26 18:50:53.925,,,no_permission,,,,,,,,,,,,357489,,,,Thu Nov 28 00:19:04 UTC 2013,,,Patch Available,,,,0|i1pmlr:,357779,,,,,,,,,,"26/Nov/13 18:50;cheolsoo;Thank you Nezih for the patch.

Your fix looks correct to me. Please update TestBuiltin as you suggested, and then, I will commit it.",27/Nov/13 20:33;nezihyigitbasi;Fixed the TestBuiltin test class.,"27/Nov/13 20:34;nezihyigitbasi;Cheolsoo, I have also updated the TestBuiltin class and made sure that unit tests succeed. Please review, thanks.",28/Nov/13 00:19;cheolsoo;Committed to trunk. Thank you Nazih!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up PigStats and JobStats after PIG-3419,PIG-3561,12677454,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,04/Nov/13 21:33,07/Jul/14 18:07,14/Mar/19 03:07,11/Nov/13 21:51,,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,This is a clean up that I did as part of PIG-3514 and PIG-3541. I'd like to commit this in trunk so tez branch won't diverge from trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Nov/13 21:55;cheolsoo;PIG-3561-1.patch;https://issues.apache.org/jira/secure/attachment/12612041/PIG-3561-1.patch,11/Nov/13 18:55;cheolsoo;PIG-3561-2.patch;https://issues.apache.org/jira/secure/attachment/12613195/PIG-3561-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-11-11 19:37:32.659,,,no_permission,,,,,,,,,,,,356829,,,,Mon Nov 11 21:51:10 UTC 2013,,,,,,,0|i1pijj:,357119,,,,,,,,,,"04/Nov/13 21:55;cheolsoo;The attached patch factors out a few methods/fields that can be used by MR and non-MR backends into PigStats and JobStats. PIG-3419 used to move them to SimplePigStats and MRJobStats, but I found these are not MR-specific and applicable to non-MR backends such as Tez backend.

ant test-commit passes.",10/Nov/13 04:41;cheolsoo;https://reviews.apache.org/r/15390/,11/Nov/13 18:55;cheolsoo;All unit tests pass.,11/Nov/13 19:37;daijy;+1 for trunk.,11/Nov/13 21:51;cheolsoo;Thank you Daniel for the review. Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trunk is broken by PIG-3522,PIG-3559,12677231,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,cheolsoo,cheolsoo,cheolsoo,02/Nov/13 16:02,07/Jul/14 18:07,14/Mar/19 03:07,02/Nov/13 18:47,,,,,,,,0.13.0,,,,build,,,0,,,,,,,,,,,,,Shock was not deleted from the ant script.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Nov/13 16:06;cheolsoo;PIG-3559-1.patch;https://issues.apache.org/jira/secure/attachment/12611771/PIG-3559-1.patch,02/Nov/13 17:39;cheolsoo;PIG-3559-2.patch;https://issues.apache.org/jira/secure/attachment/12611775/PIG-3559-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-11-02 16:40:37.472,,,no_permission,,,,,,,,,,,,356606,,,,Sat Nov 02 18:47:30 UTC 2013,,,,,,,0|i1ph67:,356896,,,,,,,,,,02/Nov/13 16:06;cheolsoo;Attaching a fix.,02/Nov/13 16:40;jarcec;+1 (non-binding),"02/Nov/13 17:39;cheolsoo;Actually, there is one more function (Utils.setSSHFactory) that needs to be removed. Attaching a new patch.",02/Nov/13 18:42;daijy;+1,02/Nov/13 18:47;cheolsoo;Committed to trunk. Thank you Daniel for the review.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HadoopJobHistoryLoader fails to load job history on hadoop v 1.2,PIG-3553,12676733,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,lgiri,lgiri,lgiri,30/Oct/13 22:45,07/Jul/14 18:07,14/Mar/19 03:07,10/Nov/13 06:21,0.10.0,,,,,,,0.13.0,,,,piggybank,,,0,,,,,,,,,,,,,"while following the example on pig wiki - 

a = load '/mapred/history/done' using HadoopJobHistoryLoader() as (j:map[], m:map[], r:map[]);
b = foreach a generate (Chararray) j#'STATUS' as status, j#'PIG_SCRIPT_ID' as id, j#'USER' as user, j#'JOBNAME' as script_name, j#'JOBID' as job;
c = filter b by status != 'SUCCESS';
dump c;

when tried to parse /mapred/history logs, it fails to parse the job conf logs because in hadoop 1 the logs are stored as 

job_201310301419_0001_conf.xml

rathar than as per the code says line number 203 in hadoopjobhistoryloader.java 

Path parent = full.getParent();
                String jobXml = jobDetails[0] + ""_"" + jobDetails[1] + ""_""
                        + jobDetails[2] + ""_"" + jobDetails[3] + ""_""
                        + jobDetails[4] + ""_conf.xml"";
                Path p = new Path(parent, jobXml);  

attaching the patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Nov/13 14:40;jeremykarn;PIG-3553-test-fix.patch;https://issues.apache.org/jira/secure/attachment/12614895/PIG-3553-test-fix.patch,30/Oct/13 23:15;lgiri;PIG-3553_lgiri1.patch;https://issues.apache.org/jira/secure/attachment/12611199/PIG-3553_lgiri1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-10-31 05:35:01.964,,,no_permission,,,,,,,,,,,,356165,,,,Wed Nov 20 17:34:15 UTC 2013,,,,,,,0|i1pefr:,356453,,,,,,,,,,30/Oct/13 22:48;lgiri;Attaching the patch that fixed my problem,30/Oct/13 23:15;lgiri;Attaching latest patch,31/Oct/13 05:35;aniket486;Dup of PIG-2894. Closing other one.,"31/Oct/13 05:36;aniket486;Now files get moved to done directory after job is done, would your patch work with that?","31/Oct/13 20:14;lgiri;yes, i tested it our cluster running hadoop 1.2. the pig job history loader worked fine there. is there any tests i need to include here? or some examples?",10/Nov/13 06:10;cheolsoo;+1.,10/Nov/13 06:21;cheolsoo;Committed to trunk. Thank you Lohit!,20/Nov/13 14:40;jeremykarn;This patch fixes the HadoopJobHistoryLoader piggybank test.  TestHadoopJobHistoryLoader is looking for the new file name format but only the old file name is in the data directory.  I just copied that file and named it in the new format.,"20/Nov/13 17:34;cheolsoo;Thank you [~jeremykarn] for fixing the test. I overlooked the test failure.

Committed to trunk.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UriUtil used by reducer estimator should support viewfs,PIG-3552,12676731,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,amatsukawa,amatsukawa,amatsukawa,30/Oct/13 22:25,15/Apr/14 20:45,14/Mar/19 03:07,31/Oct/13 22:02,,,,,,,,0.12.1,,,,impl,,,0,,,,,,,,,,,,,"On Hadoop2 running viewfs throws error 
{code}
2013-10-30 02:12:46,916 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Using reducer estimator: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator
2013-10-30 02:12:46,917 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator - BytesPerReducer=1610612736 maxReducers=999 totalInputFileSize=-1
2013-10-30 02:12:46,917 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Could not estimate number of reducers and no requested or default parallelism set. Defaulting to 1 reducer.
{code}

This is caused by the fact that UriUtil.isHDFSFileOrLocalOrS3N only supports files that start with ""/"", ""hdfs:"", ""file:"", ""s3n:"" but not ""viewfs:"".

Trivial fix, patch attached. 

Testing: ant test-commit passes.",Pig 0.11+,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,31/Oct/13 21:50;amatsukawa;UriUtil_enable_viewfs.patch;https://issues.apache.org/jira/secure/attachment/12611490/UriUtil_enable_viewfs.patch,31/Oct/13 21:50;amatsukawa;UriUtil_tests.patch;https://issues.apache.org/jira/secure/attachment/12611492/UriUtil_tests.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-10-31 20:14:51.398,,,no_permission,,,,,,,,,,,,356163,,,,Thu Oct 31 22:02:25 UTC 2013,,,Patch Available,,,,0|i1pefb:,356451,,,,,,,,,,"31/Oct/13 20:14;aniket486;+1. Can you add a test before we commit this?

[~rohini], [~knoguchi], I'm curious, why didn't you hit this one when running with hadoop 2?","31/Oct/13 20:44;knoguchi;bq. Rohini Palaniswamy, Koji Noguchi, I'm curious, why didn't you hit this one when running with hadoop 2? 
We haven't started using viewfs on production yet.","31/Oct/13 21:23;aniket486;[~amatsukawa], patch doesn't apply cleanly to trunk. Can you take a look?",31/Oct/13 21:51;amatsukawa;Sorry was on branch-0.11 rather than trunk. Patches updated.,"31/Oct/13 21:57;aniket486;+1. New change is trivial (no refactor), so will not commit a new test.",31/Oct/13 22:02;aniket486;committed to trunk and branch-0.12. Thanks [~amatsukawa]!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Legal expression causing ERROR 2229: Couldn't find matching uid -1 for project,PIG-3550,12676318,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,adcohen,adcohen,29/Oct/13 11:41,15/Apr/14 20:44,14/Mar/19 03:07,29/Nov/13 08:00,0.11.1,,,,,,,0.12.1,,,,,,,0,,,,,,,,,,,,,"--Here is the script the create the problem

test = load 'test' as (a:long,b:long,c:long);
testGroup = group test by (a,b);
bug = FOREACH testGroup {
	temp = filter test by c==0;
	GENERATE MAX(test.c) as c;
};

--First issuse: bad schema!
describe bug;
--RESULT: bug: {a: long,b: long,c: long}
--SHOULD BE bug:{c:long}

evenWorst = JOIN bug BY c, test BY c;
STORE evenWorst into 'veryStrangeMessage';
-- Result:
--[main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2229: Couldn't find matching uid -1 for project (Name: Project Type: bytearray Uid: -1 Input: 0 Column: 2)

------------LOG
sh-3.2$ cat pig_1383046750271.log
Pig Stack Trace
---------------
ERROR 2229: Couldn't find matching uid -1 for project (Name: Project Type: bytearray Uid: -1 Input: 0 Column: 2)

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1002: Unable to store alias evenWorst
	at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1558)
	at org.apache.pig.PigServer.registerQuery(PigServer.java:516)
	at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:991)
	at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:412)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:194)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:170)
	at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:69)
	at org.apache.pig.Main.run(Main.java:538)
	at org.apache.pig.Main.main(Main.java:157)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:160)
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2000: Error processing rule LoadTypeCastInserter
	at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:122)
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:277)
	at org.apache.pig.PigServer.compilePp(PigServer.java:1322)
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1247)
	at org.apache.pig.PigServer.execute(PigServer.java:1239)
	at org.apache.pig.PigServer.access$400(PigServer.java:121)
	at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1553)
	... 13 more
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2229: Couldn't find matching uid -1 for project (Name: Project Type: bytearray Uid: -1 Input: 0 Column: 2)
	at org.apache.pig.newplan.logical.optimizer.ProjectionPatcher$ProjectionRewriter.visit(ProjectionPatcher.java:91)
	at org.apache.pig.newplan.logical.expression.ProjectExpression.accept(ProjectExpression.java:207)
	at org.apache.pig.newplan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:64)
	at org.apache.pig.newplan.DepthFirstWalker.walk(DepthFirstWalker.java:53)
	at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:52)
	at org.apache.pig.newplan.logical.optimizer.AllExpressionVisitor.visitAll(AllExpressionVisitor.java:72)
	at org.apache.pig.newplan.logical.optimizer.AllExpressionVisitor.visit(AllExpressionVisitor.java:95)
	at org.apache.pig.newplan.logical.relational.LOJoin.accept(LOJoin.java:174)
	at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
	at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:52)
	at org.apache.pig.newplan.logical.optimizer.ProjectionPatcher.transformed(ProjectionPatcher.java:48)
	at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:113)
	... 19 more
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-11-29 08:00:01.335,,,no_permission,,,,,,,,,,,,355815,Reviewed,,,Fri Nov 29 08:00:01 UTC 2013,,,,,,,0|i1pcaf:,356103,,,,,,,,,,29/Nov/13 08:00;daijy;Verified fixed on trunk and 0.12 branch (0.12.1 to come).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Print hadoop jobids for failed, killed job",PIG-3549,12676237,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,aniket486,aniket486,28/Oct/13 22:53,15/Apr/14 20:45,14/Mar/19 03:07,29/Oct/13 18:26,0.12.0,,,,,,,0.12.1,,,,,,,0,,,,,,,,,,,,,"It would be better if we dump the hadoop job ids for failed, killed jobs in pig log. Right now, log looks like following-
{noformat}
ERROR org.apache.pig.tools.grunt.Grunt: ERROR 6017: Job failed! Error - NA
INFO org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher: Job job_pigexec_1 killed
{noformat}
From that its hard to say which hadoop job failed if there are multiple jobs running in parallel.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Oct/13 22:54;aniket486;PIG-3549.patch;https://issues.apache.org/jira/secure/attachment/12610709/PIG-3549.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-29 14:10:48.978,,,no_permission,,,,,,,,,,,,355734,,,,Tue Oct 29 18:26:17 UTC 2013,,,,,,,0|i1pbsf:,356022,,,,,,,,,,28/Oct/13 22:54;aniket486;Initial attempt to solve this.,"29/Oct/13 14:10;dvryaboy;OMG. Thanks.
+1.","29/Oct/13 14:50;billgraham;Wow, fix of the year.

+1","29/Oct/13 16:28;rangadi;awesome! Job url would be even better. Especially with Hadoop2 and Yarn, it is not as simple as before to go the job history.",29/Oct/13 17:51;jcoveney;you are a prince,29/Oct/13 18:26;aniket486;Committed to trunk and branch-0.12. Thanks everyone!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Seperate validation rules from optimizer,PIG-3545,12675556,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,24/Oct/13 20:28,07/Jul/14 18:08,14/Mar/19 03:07,16/May/14 18:14,,,,,,,,0.13.0,,,,impl,,,0,,,,,,,,,,,,,"A continuation of PIG-3492. We realize separate validation rules (such as DuplicateForEachColumnRewriteVisitor, ImplicitSplitInsertVisitor) from optimizer would make code clear. The reason is:
1. validation rule only need to run once
2. In contrast to optimization rule, validation rule works on an inconsistent plan, and should be careful about precondition. Also some global operation such as resetSchema is not possible on inconsistent plan
3. Some operations should work with unoptimized but validated plan, such as describe

Also want to address PIG-3508 in the cleanup. We should call LogicalPlan.optimize explicitly rather than rely on HExecutionEngine.compile to do the logical plan optimization.",,,,,,,,,,,PIG-3508,,,,,,PIG-3508,,,,,,,,,,,,,,,31/Oct/13 17:27;daijy;PIG-3545-1.patch;https://issues.apache.org/jira/secure/attachment/12611431/PIG-3545-1.patch,16/May/14 06:59;daijy;PIG-3545-2.patch;https://issues.apache.org/jira/secure/attachment/12645188/PIG-3545-2.patch,16/May/14 18:06;daijy;PIG-3545-3.patch;https://issues.apache.org/jira/secure/attachment/12645281/PIG-3545-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-04-29 05:39:11.411,,,no_permission,,,,,,,,,,,,355133,Reviewed,,,Fri May 16 18:14:27 UTC 2014,,,,,,,0|i1p833:,355421,,,,,,,,,,28/Apr/14 20:22;daijy;Can someone help review this? It is a must for 0.13.0.,29/Apr/14 05:39;aniket486;+1. LGTM.,16/May/14 06:59;daijy;Resync with trunk.,16/May/14 18:06;daijy;Fix a unit test failure.,16/May/14 18:14;daijy;Patch committed to trunk. Thanks Aniket for review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tweak build to support HBase with Hadoop 23 profile,PIG-3537,12675124,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jarcec,jarcec,jarcec,22/Oct/13 20:40,21/Nov/14 05:58,14/Mar/19 03:07,10/Oct/14 20:31,0.12.0,,,,,,,0.14.0,,,,,,,0,,,,,,,,,,,,,We should tweak the ant build to allow building (and running tests) HBaseStorage on Hadoop profile 23 (Hadoop 2).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-10-10 20:31:57.528,,,no_permission,,,,,,,,,,,,354744,,,,Fri Oct 10 20:31:57 UTC 2014,,,,,,,0|i1p5ov:,355033,,,,,,,,,,10/Oct/14 20:31;daijy;This issue is fixed as part of PIG-4005.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some e2e tests is broken due to PIG-3480,PIG-3530,12674869,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,21/Oct/13 18:55,15/Apr/14 20:44,14/Mar/19 03:07,23/Oct/13 18:28,,,,,,,,0.12.1,0.13.0,,,impl,,,0,,,,,,,,,,,,,"Eg: MultiQuery_9
Error message:
FATAL org.apache.hadoop.mapred.Child: Error running child : java.lang.NoClassDefFoundError: org/xerial/snappy/SnappyCodec
	at org.apache.pig.impl.util.Utils$TEMPFILE_CODEC.<clinit>(Utils.java:253)
	at org.apache.pig.impl.util.Utils$TEMPFILE_STORAGE.<clinit>(Utils.java:274)
	at org.apache.pig.impl.util.Utils.getTmpFileStorage(Utils.java:352)
	at org.apache.pig.impl.util.Utils.getTmpFileStorageObject(Utils.java:339)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.setConf(WeightedRangePartitioner.java:129)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.<init>(MapTask.java:677)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:756)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)
Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.SnappyCodec
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
	... 15 more

Need to ship snappy-java.jar to backend.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Oct/13 19:08;daijy;PIG-3530-1.patch;https://issues.apache.org/jira/secure/attachment/12609506/PIG-3530-1.patch,21/Oct/13 21:54;daijy;PIG-3530-2.patch;https://issues.apache.org/jira/secure/attachment/12609538/PIG-3530-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-10-21 22:25:13.055,,,no_permission,,,,,,,,,,,,354491,Reviewed,,,Wed Oct 23 18:28:34 UTC 2013,,,,,,,0|i1p44v:,354781,,,,,,,,,,"21/Oct/13 21:53;daijy;Forget PIG-3530-1.patch, we don't need to ship the jar to add another 700k.",21/Oct/13 22:25;rohini;+1,23/Oct/13 18:28;daijy;Patch committed to both trunk and 0.12 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unions with Enums do not work with AvroStorage,PIG-3526,12674395,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jadler,jadler,jadler,17/Oct/13 20:29,07/Jul/14 18:07,14/Mar/19 03:07,27/Oct/13 01:51,0.12.0,,,,,,,0.13.0,,,,internal-udfs,,,0,,,,,,,,,,,,,"If you have an input schema with unions of enum types and nulls, AvroStorage can't read the data correctly. This patch will translate the enums to strings so that Pig can process them.

(Sorry for the short description and lack of a unit test; ran into this issue while working on a deadline for another project.) 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Oct/13 20:30;jadler;PIG-3526.patch;https://issues.apache.org/jira/secure/attachment/12609017/PIG-3526.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-27 01:51:01.98,,,no_permission,,,,,,,,,,,,354017,,,,Sun Oct 27 01:51:01 UTC 2013,,,Patch Available,,,,0|i1p193:,354309,,,,,,,,,,17/Oct/13 20:30;jadler;Patch for this issue.,27/Oct/13 01:51;cheolsoo;+1. Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigStats.get() and ScriptState.get() shouldn't return MR-specific objects ,PIG-3525,12674261,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,17/Oct/13 06:52,07/Jul/14 18:08,14/Mar/19 03:07,18/Nov/13 15:25,0.13.0,,,,,,,0.13.0,,,,impl,,,0,,,,,,,,,,,,,"After PIG-3419, PigStats and ScriptState are no longer MR-specific classes. But PigStats.get() and ScriptState.get() methods still return MR-specific objects (SimplePigStats and MRScriptState respectively). We should deprecate these method and provide a new way to create appropriate objects based on the ExecType.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Nov/13 23:12;cheolsoo;PIG-3525-1.patch;https://issues.apache.org/jira/secure/attachment/12614256/PIG-3525-1.patch,16/Nov/13 23:56;cheolsoo;PIG-3525-2.patch;https://issues.apache.org/jira/secure/attachment/12614258/PIG-3525-2.patch,18/Nov/13 01:25;cheolsoo;PIG-3525-3.patch;https://issues.apache.org/jira/secure/attachment/12614320/PIG-3525-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-11-18 02:03:17.696,,,no_permission,,,,,,,,,,,,353883,,,,Mon Nov 18 15:25:12 UTC 2013,,,,,,,0|i1p0fb:,354175,,,,,,,,,,16/Nov/13 23:12;cheolsoo;Attached is a patch that make these get() functions return proper type of objects based on the exec type.,17/Nov/13 01:50;cheolsoo;Canceling the patch since it doesn't fix the issue.,"18/Nov/13 01:25;cheolsoo;Updating a proper fix. The RB link is here-
https://reviews.apache.org/r/15634/",18/Nov/13 02:03;rohini;+1. Looks good to me,18/Nov/13 15:25;cheolsoo;Committed to trunk. Thank you Rohini for the review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up Launcher and MapReduceLauncher after PIG-3419,PIG-3524,12674257,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,17/Oct/13 06:33,04/Jun/16 21:30,14/Mar/19 03:07,19/Oct/13 04:36,0.13.0,,,,,,,0.13.0,,,,impl,,,0,,,,,,,,,,,,,"This is clean-up that I did for Launcher and MapReduceLauncher as part of PIG-3502. I'd like to merge applicable changes from tez branch to trunk so that they will not diverge too much over time.

The changes include:
# Rename ExecutionEngineKiller to HangingJobKiller and move it from PigContext to Launcher. This brings back HangingJobKiller to its original place before PIG-3419.
# Remove kill() method from the ExecutionEngine interface. It was needed only by ExecutionEngineKiller, but after #1, it's no longer needed.
# Refactor Launcher and MapReduceLauncer classes. Basically, move MR-specific stuff to MapReduceLauncher and common stuff to Launcher.
",,,,,,,,,,,,,,,,,,,,,,,,,PIG-4921,,,,,,,17/Oct/13 06:36;cheolsoo;PIG-3524-1.patch;https://issues.apache.org/jira/secure/attachment/12608888/PIG-3524-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-19 03:19:40.487,,,no_permission,,,,,,,,,,,,353879,,,,Sat Oct 19 04:36:20 UTC 2013,,,Patch Available,,,,0|i1p0ef:,354171,,,,,,,,,,17/Oct/13 06:36;cheolsoo;ant test-commit passes.,19/Oct/13 03:19;aniket486;+1,19/Oct/13 04:36;cheolsoo;Committed to trunk. Thank you Aniket for the review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide backward compatibility for PigRunner and PPNL after PIG-3419,PIG-3520,12674176,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,16/Oct/13 18:51,07/Jul/14 18:08,14/Mar/19 03:07,17/Oct/13 05:08,,,,,,,,0.13.0,,,,impl,,,0,,,,,,,,,,,,,"PIG-3419 did backend refactory to support multiple backend. However, it break backward compatibility of two widely used API: PigRunner and PPNL. And it does not provide a deprecate path. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Oct/13 19:32;daijy;PIG-3520-1.patch;https://issues.apache.org/jira/secure/attachment/12608783/PIG-3520-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-16 21:55:25.909,,,no_permission,,,,,,,,,,,,353798,,,,Thu Oct 17 05:08:10 UTC 2013,,,,,,,0|i1ozwv:,354090,,,,,,,,,,"16/Oct/13 19:32;daijy;Provide one option in the patch:
1. Retain all old methods in JobStats, MRJobStats provide implementation, other inheritance (eg, TezJobStats) will throw NotImplmented exception
2. Do something similar to PigStatsUtil/MRPigStatsUtil
3. In the future, user need to cast JobStats to backend-specific class to use backend-specific methods

The good:
1. Keep backward compatibility
2. Change is very small
3. Retain the spirit of multi-backend
4. Provide deprecate path

The ugly:
JobStats still contains MR specific methods initially
","16/Oct/13 21:55;cheolsoo;[~daijy], thank you very much for the patch! Certainly, this doesn't conflict to what I am doing with TezStats and is the cleanest solution so far.

One remaining problem is PigStats.get() returns MR-specific SimplePigStats.
{code}
    public static PigStats get() {
        if (tps.get() == null) {
            LOG.info(""PigStats has not been set. Defaulting to SimplePigStats"");
            tps.set(new SimplePigStats());
        }
        return tps.get();
    }
{code}
I am adding PigStatsFactory class that creates a PigStats depending on the ExecType and marking PigStats.get() as deprecated for backward compatibility. This requires a few more changes in PigStatsUtil. I also see a similar problem with ScriptState.get(), so I will introduce a factory for ScriptState as well.

Nevertheless, I think I can build TezStats upon your patch. I will post a patch once I have it ready. Thanks!","16/Oct/13 22:27;daijy;Sure, we can move PigStats.get to a new location and provide a deprecate path. ","17/Oct/13 05:04;cheolsoo;+1.

This patch alone fixes backward compatibility. I will fix PigStats.get() and ScriptState.get() in a separate jira.",17/Oct/13 05:08;cheolsoo;Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to ship jruby.jar in the release,PIG-3518,12673750,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,14/Oct/13 19:03,07/Jul/14 18:08,14/Mar/19 03:07,23/Oct/13 18:35,,,,,,,,0.13.0,,,,impl,,,0,,,,,,,,,,,,,Need to put jruby*.jar in lib in release same as jython*.jar.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Oct/13 19:04;daijy;PIG-3518-1.patch;https://issues.apache.org/jira/secure/attachment/12608324/PIG-3518-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-15 22:48:40.021,,,no_permission,,,,,,,,,,,,353373,Reviewed,,,Wed Oct 23 18:35:50 UTC 2013,,,,,,,0|i1oxb3:,353665,,,,,,,,,,15/Oct/13 22:48;rohini;+1,23/Oct/13 18:35;daijy;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig does not bring in joda-time as dependency in its pig-template.xml,PIG-3516,12673748,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,14/Oct/13 18:52,15/Apr/14 20:44,14/Mar/19 03:07,23/Oct/13 18:34,,,,,,,,0.12.1,0.13.0,,,build,,,0,,,,,,,,,,,,,joda-time is missing from pig-template.xml. Downstream projects such as HCatalog/Oozie are impacted.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Oct/13 18:52;daijy;PIG-3516-1.patch;https://issues.apache.org/jira/secure/attachment/12608319/PIG-3516-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-15 22:50:13.577,,,no_permission,,,,,,,,,,,,353371,Reviewed,,,Wed Mar 19 18:51:18 UTC 2014,,,,,,,0|i1oxan:,353663,,,,,,,,,,15/Oct/13 22:50;rohini;+1,23/Oct/13 18:34;daijy;Patch committed to trunk.,19/Mar/14 18:51;daijy;Request by [~ekoifman] to backport it to 0.12 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shell commands are limited from OS buffer,PIG-3515,12673676,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,andronat,andronat,andronat,14/Oct/13 12:38,07/Jul/14 18:08,14/Mar/19 03:07,19/Oct/13 03:59,0.11.1,0.12.0,,,,,,0.13.0,,,,parser,,,0,command-line,parser,,,,,,,,,,,"Executing shell commands in pig scripts, may stuck due to Java buffer limitations.

Example:
%declare VAR   `cat 100kbytes.txt`

Produce:
2013-10-09 15:25:56,825 [main] INFO  org.apache.pig.tools.parameters.PreprocessorContext - Executing command : cat 100kbytes.txt

The execution hungs, so you have to interrupt the program.

^C
2013-10-09 15:26:10,066 [main] ERROR org.apache.pig.Main - ERROR 2999: Unexpected internal error. Error executing shell command: cat 100kbytes.txt. Command exit with exit code of 130

Explanation:
The problem lies in org.apache.pig.tools.parameters.PreprocessorContext#executeShellCommand method, line: 191 (trunk, at revision 1531874).

We wait for the process to complete before we get all the output, but the process waits for an indefinite amount of time, due to the fact that there is no space left in the buffer and the output can't be delivered.

References:
[1] http://www.javaworld.com/javaworld/jw-12-2000/jw-1229-traps.html?page=2
[2] http://vyvaks.wordpress.com/2006/05/27/does-runtimeexec-hangs-in-java/

Solution:
I attach a test case that illustrates the problem and a patch as a solution. I would really like some comments.","Centos 6.5, Mac OS X 10.8.5",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Oct/13 12:41;andronat;PIG-3515-test.patch;https://issues.apache.org/jira/secure/attachment/12608273/PIG-3515-test.patch,14/Oct/13 12:41;andronat;PIG-3515.patch;https://issues.apache.org/jira/secure/attachment/12608274/PIG-3515.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-10-18 05:38:03.522,,,no_permission,,,,,,,,,,,,353299,,,,Sat Oct 19 12:52:52 UTC 2013,,,,,,,0|i1owuv:,353592,,,,,,,,,,14/Oct/13 12:41;andronat;Test and solution in different files.,18/Oct/13 05:38;aniket486;Looks good to me. Will commit this to trunk.,"18/Oct/13 05:39;aniket486;Anastasis Andronidis, can you please send a note on pig-dev@ mailing-list to add yourself to contributors list?",18/Oct/13 16:40;cheolsoo;Assigning to Anastasis.,"18/Oct/13 16:54;cheolsoo;I noticed that the Apache header is missing in the test file, and 2 spaces is used instead of 4. [~aniket486], do you mind fixing them when you commit this? Thanks!","18/Oct/13 18:31;aniket486;[~cheolsoo], thanks for the comments. I will do that for sure. If you have more context on PreprocessorContext, can you also take a quick look at the patch?","19/Oct/13 03:59;cheolsoo;Hi Aniket, sorry for the late reply. It looks good to me too. [~andronat], thank you for the good catch! I didn't know this until now. lol

Since I was already running the test, I went ahead and committed the patch to trunk. I didn't commit the test case because I thought this bug doesn't need a unit test. Please let me know if you want me to commit the test case.","19/Oct/13 12:52;andronat;I don't mind. I am just thinking that every test case is illustrating something, so maybe someone else can benefit from reading this code.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reducer estimater is broken by PIG-3497,PIG-3512,12673482,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,11/Oct/13 19:18,15/Apr/14 20:44,14/Mar/19 03:07,11/Oct/13 21:54,,,,,,,,0.12.1,,,,impl,,,0,,,,,,,,,,,,,PIG-3497 moves adjustNumReducers to a much later stage than should. We need to revert it and do it the right way.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11/Oct/13 19:19;daijy;PIG-3512-1.patch;https://issues.apache.org/jira/secure/attachment/12608060/PIG-3512-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-11 20:21:48.993,,,no_permission,,,,,,,,,,,,353105,Reviewed,,,Fri Oct 11 21:54:45 UTC 2013,,,,,,,0|i1ovmf:,353392,,,,,,,,,,11/Oct/13 20:21;aniket486;+1,11/Oct/13 21:54;daijy;Patch committed to both trunk and 0.12 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Security: Pig temporary directories might have world readable permissions,PIG-3511,12673307,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,aniket486,aniket486,10/Oct/13 20:58,07/Jul/14 18:08,14/Mar/19 03:07,23/Jan/14 05:58,,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"Currently, udf jars are copied to FileLocalizer.getTemporaryPath which is a unsecured location. We need to make sure the directory that we copy these jars to have 700 permission settings (similar behavior as JobClient).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Jan/14 19:14;rohini;PIG-3511-1.patch;https://issues.apache.org/jira/secure/attachment/12624400/PIG-3511-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-10 17:25:54.67,,,no_permission,,,,,,,,,,,,352930,,,,Thu Jan 23 05:58:22 UTC 2014,,,,,,,0|i1oujj:,353217,,,,,,,,,,"10/Jan/14 17:25;rohini;If the user umask for the output directories are world readable then the temporary directories created by pig also has same permissions. And these temporary directory has job.jar, other registered jars and scripts, replicated table, quantile file for orderby, etc which are localized for jobs. Hadoop localizes them as public when the permissions are readable by all. The heavy requests for public localization from pig caused YARN-1575. The hadoop team would like to get this fixed in pig as well as having the data readable by all in /tmp directory is not a good thing. Also publicly localized things are localized directly by the NM process via a thread pool. Privately localized things are localized by a container localizer process run as the user. Keeping it private puts less load on NM and avoids localization being slowed down by the NM thread pool limit.","10/Jan/14 20:22;knoguchi;Hadoop/Yarn should do a better job in handling of the public distributed cache.  
I'm more worried for the case when we have a pig job that takes sensitive input (permission 700),  filter out those sensitive portion and produces an output with permission 755.  

Users may feel that they're still protecting the sensitive info but pig may be making them available through world readable intermediate outputs.

Of course, the original issue of udf jars being world writable (when output permission is 777) is most critical.","21/Jan/14 03:07;aniket486;In context of hadoop 1, rolling hadoop with the proper fixes for this distributed cache problem could be hard. Currently, pig is writing jars to pig.temp.dir (/tmp), a quick fix would be to make sure pig uses JobSubmissionFiles.getStagingDir or similar api to write its jars. Does that sound like a reasonable approach?",21/Jan/14 04:35;rohini;I am just setting permissions to 700 on the relativeRoot in FileLocalizer and that works fine. Will upload the patch tomorrow. ,22/Jan/14 19:14;rohini;We cannot use job staging dir as the temporary location as usually that is in user home directory and quotas are set limiting the size and number of files. The pig.temp.dir (/tmp) location usually has a higher limit set. ,23/Jan/14 05:39;aniket486;+1. Thanks [~rohini]!,23/Jan/14 05:58;rohini;Committed to trunk. Thanks Aniket for the review.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New filter extractor fails with more than one filter statement,PIG-3510,12673140,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,cheolsoo,cheolsoo,09/Oct/13 23:02,15/Apr/14 20:44,14/Mar/19 03:07,10/Oct/13 20:14,0.12.0,,,,,,,0.12.1,,,,impl,,,0,,,,,,,,,,,,,"This is a regression from PIG-3461 - rewrite of partition filter optimizer. Here is an example that demonstrates the problem:
{code:title=two filters}
b = FILTER a BY (dateint >= 20130901 AND dateint <= 20131001);
c = FILTER b BY (event_id == 419 OR event_id == 418);
{code}
{code:title=one filter}
b = FILTER a BY (dateint >= 20130901 AND dateint <= 20131001) AND (event_id == 419 OR event_id == 418);
{code}
Both dateint and event_id are partition columns. For the 1 filter case, the whole expression is pushed down whereas for the 2 filter case, only (event_id == 419 OR event_id == 418) is pushed down.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Oct/13 12:25;cheolsoo;PIG-3510-2.patch;https://issues.apache.org/jira/secure/attachment/12607792/PIG-3510-2.patch,10/Oct/13 04:31;aniket486;PIG-3510.patch;https://issues.apache.org/jira/secure/attachment/12607737/PIG-3510.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-10-10 04:32:27.801,,,no_permission,,,,,,,,,,,,352763,,,,Thu Oct 10 20:37:59 UTC 2013,,,,,,,0|i1otiv:,353050,,,,,,,,,,"10/Oct/13 00:43;cheolsoo;After debugging further, I found that the real problem is that the order of optimization rules being applied has changed by PIG-3461. In particular, MergeFilter is applied *after* NewPartitionFilterOptimizer resulting that only partial expressions are pushed down.

Old order:
# MergeFilter
# PartitionFilterOptimizer

New order:
# NewPartitionFilterOptimizer
# MergeFilter
","10/Oct/13 00:51;cheolsoo;Here is what I see when printing out optimization rules with new and old partition filter optimizers:
{code:title=new}
ImplicitSplitInserter
DuplicateForEachColumnRewrite
--> NewPartitionFilterOptimizer
--> NewPartitionFilterOptimizer
--> NewPartitionFilterOptimizer
StreamTypeCastInserter
LoadTypeCastInserter
SplitFilter
PushUpFilter
PushUpFilter
PushUpFilter
--> MergeFilter
PushDownForEachFlatten
ColumnMapKeyPrune
AddForEach
MergeForEach
GroupByConstParallelSetter
LimitOptimizer
{code}
{code:title=old}
ImplicitSplitInserter
DuplicateForEachColumnRewrite
LoadTypeCastInserter
StreamTypeCastInserter
SplitFilter
SplitFilter
PushUpFilter
PushUpFilter
--> MergeFilter
PushUpFilter
--> MergeFilter
PushUpFilter
--> MergeFilter
PushUpFilter
--> PartitionFilterOptimizer
--> PartitionFilterOptimizer
PushDownForEachFlatten
ColumnMapKeyPrune
AddForEach
MergeForEach
GroupByConstParallelSetter
LimitOptimizer
{code}","10/Oct/13 04:32;aniket486;[~cheolsoo], lets commit the patch to trunk. We can also apply it to 0.12 later.","10/Oct/13 12:25;cheolsoo;+1

Thank you [~aniket486]! I confirmed that fixes the issue. In a new patch, I added some typo corrections in pig.properties and FilterExtractor.java as follows:
{code}
-# pig.exec.useOldPartitionFilterOptimize=true
+# pig.exec.useOldPartitionFilterOptimizer=true

-            // AND (rightState.pushdownExpr AND leftState.filterExpr)
+            // AND (rightState.pushdownExpr AND rightState.filterExpr)

-            // OR (rightState.pushdownExpr AND leftState.filterExpr)
+            // OR (rightState.pushdownExpr AND rightState.filterExpr)
{code}",10/Oct/13 20:14;cheolsoo;Committed to trunk and branch 0.12.,10/Oct/13 20:37;aniket486;Thanks a lot [~cheolsoo]!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception swallowing in TOP,PIG-3509,12672920,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,vrajaram,vrajaram,vrajaram,08/Oct/13 22:53,07/Jul/14 15:58,14/Mar/19 03:07,09/Oct/13 01:38,0.11.1,,,,,,,0.12.1,0.13.0,,,,,,0,,,,,,,,,,,,,Related to https://issues.apache.org/jira/browse/PIG-2827 but that does not fix all the places in TOP where this exception swallowing occurs.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Oct/13 22:54;vrajaram;TOP_exception.patch;https://issues.apache.org/jira/secure/attachment/12607463/TOP_exception.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-08 22:57:44.291,,,no_permission,,,,,,,,,,,,352543,,,,Wed Oct 09 01:37:40 UTC 2013,,,Patch Available,,,,0|i1os67:,352830,,,,,,,,,,"08/Oct/13 22:54;vrajaram;This was generated with:
git diff trunk --no-prefix","08/Oct/13 22:57;jcoveney;Thanks for this, Vijay. +1. Just a note: generally, we like to post patches in the form of PIG-####_<patch version>.patch. But this is small and it's not a huge issue.",08/Oct/13 23:22;vrajaram;Oh got it. Thanks for the heads up!,09/Oct/13 01:37;aniket486;Committed to trunk. Thanks [~vrajaram] and [~jcoveney]!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"'explain' now showing logical plan BEFORE the necessary optimization (ImplicitSplitInserter, DuplicateForEachColumnRewrite,etc)",PIG-3508,12672882,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,,knoguchi,knoguchi,08/Oct/13 19:56,07/Jul/14 18:07,14/Mar/19 03:07,16/May/14 18:15,0.13.0,tez-branch,,,,,,0.13.0,,,,parser,,,0,,,,,,,,,,,,,"Probably PIG-3419 changed the order of the call such that now 'explain' is  showing the logicalPlan before the LogicalPlanOptimizer.optimize() is being called.

Before we had 
{noformat}
1030             PhysicalPlan pp = compilePp();
1031             currDAG.lp.explain(lps, format, verbose);
{noformat}
where  LogicalPlanOptimizer.optimize() was called from compilePp().",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-10-08 20:21:59.966,,,no_permission,,,,,,,,,,,,352505,,,,Fri May 16 18:15:13 UTC 2014,,,,,,,0|i1orxr:,352792,,,,,,,,,,"08/Oct/13 20:21;cheolsoo;I am adding tez-branch to affected versions. If we deiced to revert PIG-3419 in trunk, the fix should go into tez-branch.",08/Oct/13 20:39;daijy;We shall change it anyway even rollback PIG-3419. Depends on compilePp() to optimize the logical plan is very weird.,16/May/14 18:15;daijy; PIG-3545 committed to trunk. Mark the ticket to fix.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig fails to run in local mode on a Kerberos enabled Hadoop cluster,PIG-3507,12672740,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kellyzly,chiyang,chiyang,08/Oct/13 02:53,21/Nov/14 05:58,14/Mar/19 03:07,18/Sep/14 14:43,0.10.0,0.11,,,,,,0.14.0,,,,,,,0,,,,,,,,,,,,,"It fails to run pig in local mode on a Kerberos enabled Hadoop cluster

*Command*
pig -x local <pig script>
		
*Pig script*
A = load '/etc/passwd';
dump A;
	
*Root cause*
When running pig in local mode, jobConf in HExecutionEngine is initiated with core-default.xml (hadoop.security.authentication = simple), mapred-default.xml, and yarn-default.xml. However, the settings are not passed to UserGroupInformation. That's why obtainTokensForNamenodesInternal() is called from obtainTokensForNamenodes(), and causes the exception to happen.

{noformat}
public static void obtainTokensForNamenodes(Credentials credentials, Path[] ps, Configuration conf) throws IOException {
    if (!UserGroupInformation.isSecurityEnabled()) {
        return;
    }
    obtainTokensForNamenodesInternal(credentials, ps, conf);
}	
{noformat}

*Error*
Pig Stack Trace
---------------
ERROR 6000: Output Location Validation Failed for: 'file:/tmp/temp-308998488/tmp-2025176494 More info to follow:
Can't get JT Kerberos principal for use as renewer

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias A
	at org.apache.pig.PigServer.openIterator(PigServer.java:841)
	at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:696)
	at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:320)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:194)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:170)
	at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:84)
	at org.apache.pig.Main.run(Main.java:604)
	at org.apache.pig.Main.main(Main.java:157)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:208)
Caused by: org.apache.pig.PigException: ERROR 1002: Unable to store alias A
	at org.apache.pig.PigServer.storeEx(PigServer.java:940)
	at org.apache.pig.PigServer.store(PigServer.java:903)
	at org.apache.pig.PigServer.openIterator(PigServer.java:816)
	... 12 more
Caused by: org.apache.pig.impl.plan.VisitorException: ERROR 6000: Output Location Validation Failed for: 'file:/tmp/temp-308998488/tmp-2025176494 More info to follow:
Can't get JT Kerberos principal for use as renewer
	at org.apache.pig.newplan.logical.rules.InputOutputFileValidator$InputOutputFileVisitor.visit(InputOutputFileValidator.java:95)
	at org.apache.pig.newplan.logical.relational.LOStore.accept(LOStore.java:66)
	at org.apache.pig.newplan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:64)
	at org.apache.pig.newplan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:66)
	at org.apache.pig.newplan.DepthFirstWalker.walk(DepthFirstWalker.java:53)
	at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:52)
	at org.apache.pig.newplan.logical.rules.InputOutputFileValidator.validate(InputOutputFileValidator.java:45)
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:288)
	at org.apache.pig.PigServer.compilePp(PigServer.java:1327)
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1252)
	at org.apache.pig.PigServer.storeEx(PigServer.java:936)
	... 14 more
Caused by: java.io.IOException: Can't get JT Kerberos principal for use as renewer
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:129)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:111)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:85)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:127)
	at org.apache.pig.newplan.logical.rules.InputOutputFileValidator$InputOutputFileVisitor.visit(InputOutputFileValidator.java:80)
	... 24 more
================================================================================
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Sep/14 02:54;kellyzly;PIG-3507-2.patch;https://issues.apache.org/jira/secure/attachment/12669615/PIG-3507-2.patch,08/Oct/13 04:13;chiyang;PIG-3507.patch;https://issues.apache.org/jira/secure/attachment/12607306/PIG-3507.patch,28/Aug/14 11:30;kellyzly;PIG_3507_1.patch;https://issues.apache.org/jira/secure/attachment/12664872/PIG_3507_1.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-11-04 09:58:45.029,,,no_permission,,,,,,,,,,,,352363,Reviewed,,,Thu Sep 18 14:43:13 UTC 2014,,,,,,,0|i1or2f:,352651,,,,,,,,,,"04/Nov/13 09:58;takeshi.miao;This patch works for our env with kerberos enabled cluster as well. I think it is valuable if someone suffer the same issue, can anyone help to review this patch ?","10/Nov/13 06:47;cheolsoo;Sorry for the late review.

Since I don't have kerberos set up, I can't really reproduce the issue. But the patch itself looks reasonable to me. If I don't hear any objections, I will commit the patch on Monday. Please chime in if anyone has concerns.",12/Nov/13 01:59;cheolsoo;Committed to trunk. Thank you Chiyang!,"12/Nov/13 20:04;rohini;This patch needs to be reverted if we are still supporting hadoop 0.23 as it will break compatibility. 

http://svn.apache.org/viewvc/hadoop/common/branches/branch-0.20/src/core/org/apache/hadoop/security/UserGroupInformation.java?view=markup does not have setConfiguration method. 

Also I don't think the fix is correct. The problem that is happening is their core-site.xml contains hadoop.security.authentication = kerberos and that is getting picked up as it is loaded as a default resource in UserGroupInformation's configuration. We should fix the bin/pig script to not load HADOOP_CONF_DIR in local mode. ","12/Nov/13 20:17;cheolsoo;[~rohini], my bad. Feel free to revert it.",12/Nov/13 20:22;rohini;Thanks [~cheolsoo]. Reverting for now and reopening this jira till we find the actual root cause and decide on the appropriate fix. I am suspecting that the culprit in their case is that core-site.xml is being picked up in UserGroupInformation and that causes problem.,"28/Aug/14 11:23;kellyzly;The problem that pig can not work in local mode with kerberos, only exists in hadoop2.
{noformat}
As the root cause of the bug description said:
When running pig in local mode, jobConf in HExecutionEngine is initiated with core-default.xml (hadoop.security.authentication = simple), mapred-default.xml, and yarn-default.xml. However, the settings are not passed to UserGroupInformation. That's why obtainTokensForNamenodesInternal() is called from obtainTokensForNamenodes(), and causes the exception
to happen. 
 org.apache.hadoop.mapreduce.security.TokenCache#obtainTokensForNamenodes
 public static void obtainTokensForNamenodes(Credentials credentials, Path[] ps, Configuration conf) throws IOException {
    if (!UserGroupInformation.isSecurityEnabled()) {
        return;
    }
    obtainTokensForNamenodesInternal(credentials, ps, conf);
}	
{noformat}
in hadoop1.2.1:
In function ""obtainTokensForNamenodesInternal"",if path is in local mode, the fsName will be null. In this case, checks about whether the kerberos credentials exist or not will not be executed.
{{org.apache.hadoop.mapreduce.security.TokenCache#obtainTokensForNamenodesInternal}}
{code:java}
 static void obtainTokensForNamenodesInternal(Credentials credentials,
                                               Path [] ps, 
                                               Configuration conf
                                               ) throws IOException {
    // get jobtracker principal id (for the renewer)
    KerberosName jtKrbName = new KerberosName(conf.get(JobTracker.JT_USER_NAME, """"));
    String delegTokenRenewer = jtKrbName.getShortName();
    boolean readFile = true;
    for(Path p: ps) {
      FileSystem fs = FileSystem.get(p.toUri(), conf);
      String fsName = fs.getCanonicalServiceName();
      if (fsName == null) { 
        continue;
      }
      if (TokenCache.getDelegationToken(credentials, fsName) == null) {
        //TODO: Need to come up with a better place to put
        //this block of code to do with reading the file
        if (readFile) {
          readFile = false;
          String binaryTokenFilename =
            conf.get(MAPREDUCE_JOB_CREDENTIALS_BINARY);
          if (binaryTokenFilename != null) {
            Credentials binary;
            try {
              binary = Credentials.readTokenStorageFile(new Path(""file:///"" +  
                  binaryTokenFilename), conf);
            } catch (IOException e) {
              throw new RuntimeException(e);
            }
            credentials.addAll(binary);
          }
          if (TokenCache.getDelegationToken(credentials, fsName) != null) {
            LOG.debug(""DT for "" + fsName  + "" is already present"");
            continue;
          }
        }
        Token<?> token = fs.getDelegationToken(delegTokenRenewer);
        if (token != null) {
          Text fsNameText = new Text(fsName);
          credentials.addToken(fsNameText, token);
          LOG.info(""Got dt for "" + p + "";uri=""+ fsName + 
                   "";t.service=""+token.getService());
        }
      }
    }
  }
  {code}

  in hadoop2.3:
 In function ""obtainTokensForNamenodesInternal"", whether the path is in local mode or not, obtainTokensForNamenodesInternal always is executed. At that time, kerberos check fails in local mode.
{{org.apache.hadoop.mapreduce.security.TokenCache#obtainTokensForNamenodesInternal}}
{code:java}
  static void obtainTokensForNamenodesInternal(Credentials credentials,
      Path[] ps, Configuration conf) throws IOException {
    Set<FileSystem> fsSet = new HashSet<FileSystem>();
    for(Path p: ps) {
      fsSet.add(p.getFileSystem(conf)); 
    }
    for (FileSystem fs : fsSet) {
      obtainTokensForNamenodesInternal(fs, credentials, conf);
    }
  }
  
  static void obtainTokensForNamenodesInternal(FileSystem fs, 
      Credentials credentials, Configuration conf) throws IOException {
    String delegTokenRenewer = Master.getMasterPrincipal(conf);
    if (delegTokenRenewer == null || delegTokenRenewer.length() == 0) {
      throw new IOException(
          ""Can't get Master Kerberos principal for use as renewer"");
    }
    mergeBinaryTokens(credentials, conf);

    final Token<?> tokens[] = fs.addDelegationTokens(delegTokenRenewer,
                                                     credentials);
    if (tokens != null) {
      for (Token<?> token : tokens) {
        LOG.info(""Got dt for "" + fs.getUri() + ""; ""+token);
      }
    }
  }
{code}
As [~rohini]  said:{quote}  We should fix the bin/pig script to not load HADOOP_CONF_DIR in local mode.{quote} What patch did is not load the configurations in  HADOOP_CONF_DIR in local mode.","16/Sep/14 13:43;xuefuz;Hi [~rohini], [~cheolsoo], I'm wondering if you have any additional thoughts on patch _1? Could we move this forward? Thanks.","17/Sep/14 04:28;rohini; Does not sound right to me. UserGroupInformation.java uses new Configuration(true) to initialize configuration.

{code}
String value = conf.get(HADOOP_SECURITY_AUTHENTICATION);
    if (value == null || ""simple"".equals(value)) {
      useKerberos = false;
    }
{code}

So unless there is some other configuration setting it to kerberos you should not be seeing the issue. Can you find out where it is being set to kerberos? Most likely it is being picked up from *-site.xml.","17/Sep/14 07:39;kellyzly;Hi [~rohini]
Thanks for your comment.  Keberos security is widely used in hadoop. This bug was also found by other endusers(see http://www.ghostar.org/2014/05/pig-local-mode-fails-kerberos-auth-enabled/)
detail steps to reproduce the bug:
1. build kerbreos env
2. build hadoop1 env
    which hadoop
    /home/zly/prj/oss/hadoop-1.2.1/bin/hadoop

    grep -C2 kerberos  /home/zly/prj/oss/hadoop-1.2.1/conf/core-site.xml
    <name>hadoop.security.authentication</name>
    <value>kerberos</value>
  </property>

3. build pig and run in local mode
   cd $PIG_HOME/bin
   ./pig -x local id.pig
   ps -ef|grep pig
root     12126 10072  0 14:42 pts/4    00:00:01 /usr/java/jdk1.7.0_51//bin/java -Dproc_jar -Xmx1000m -Xmx1000m -Dpig.log.dir=/home/zly/prj/oss/pig/logs -Dpig.log.file=pig.log -Dpig.home.dir=/home/zly/prj/oss/pig -Xmx1000m -Dpig.log.dir=/home/zly/prj/oss/pig/logs -Dpig.log.file=pig.log -Dpig.home.dir=/home/zly/prj/oss/pig -Dhadoop.log.dir=/home/zly/prj/oss/hadoop-1.2.1/libexec/../logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/zly/prj/oss/hadoop-1.2.1/libexec/.. -Dhadoop.id.str= -Dhadoop.root.logger=INFO,console -Dhadoop.security.logger=INFO,NullAppender -Djava.library.path=/home/zly/prj/oss/hadoop-1.2.1/libexec/../lib/native/Linux-amd64-64 -Dhadoop.policy.file=hadoop-policy.xml -Xrunjdwp:transport=dt_socket,server=y,address=9999 -classpath /home/zly/prj/oss/hadoop-1.2.1/conf:/home/zly/prj/oss/pig/conf:/usr/java/jdk1.7.0_51/lib/tools.jar:/home/zly/prj/oss/pig/build/ivy/lib/Pig/*:/home/zly/prj/oss/hadoop-1.2.1/conf:/home/zly/prj/oss/hadoop-1.2.1/conf:/home/zly/prj/oss/pig/lib/ST4-4.0.4.jar:/home/zly/prj/oss/pig/lib/accumulo-core-1.5.0.jar:......
*/home/zly/prj/oss/hadoop-1.2.1/conf:/home/zly/prj/oss/hadoop-1.2.1/conf is in the classpath*

*Why UserGroupInformation set ""hadoop.security.authentication"" as ""kerberos""*
{code}
   UserGroupInformation#ensureInitialized
 private static synchronized void ensureInitialized() {
    if (!isInitialized) {
      initialize(new Configuration());
    }
  }
{code}
{code}
UserGroupInformation#initialize
private static synchronized void initialize(Configuration conf) {
    String value = conf.get(HADOOP_SECURITY_AUTHENTICATION);
    if (value == null || ""simple"".equals(value)) {
      useKerberos = false;
    } else if (""kerberos"".equals(value)) {
      useKerberos = true;
    } else {
      throw new IllegalArgumentException(""Invalid attribute value for "" +
                                         HADOOP_SECURITY_AUTHENTICATION + 
                                         "" of "" + value);
    }
{code}
   The value of conf.get(HADOOP_SECURITY_AUTHENTICATION)  decides ""simple""  or ""kerberos"".  

  Referred https://hadoop.apache.org/docs/r1.2.1/api/org/apache/hadoop/conf/Configuration.html
   Configurations are specified by resources. A resource contains a set of name/value pairs as XML data. Each resource is named by either a String or by a Path. If named by a String, then the classpath is examined for a file with that name. If named by a Path, then the local filesystem is examined directly, without referring to the classpath.
Unless explicitly turned off, Hadoop by default specifies two resources, loaded in-order from the classpath:
core-default.xml : Read-only defaults for hadoop.
core-site.xml: Site-specific configuration for a given hadoop installation.
*core-default.xml in the jar and core-site.xml in the $HADOOP_HOME/conf/  are loaded.* So the value of ""hadoop.security.authentication"" is ""kerberos"".


If you don't build hadoop env and only deploy kerberos.
1. build kerberos env
2. build pig and run in local mode:
    cd $PIG_HOME/bin; ./pig -x local id.pig
    Error messages are found:
    exec /usr/java/jdk1.7.0_51/bin/java -Xmx1000m -Dpig.log.dir=/home/zly/prj/oss/pig/bin/../logs -Dpig.log.file=pig.log -Dpig.home.dir=/home/zly/prj/oss/pig/bin/.. -Xrunjdwp:transport=dt_socket,server=y,address=9999 -classpath /home/zly/prj/oss/pig/bin/../conf:/usr/java/jdk1.7.0_51/lib/tools.jar:/home/zly/prj/oss/pig/bin/../lib/ST4-4.0.4.jar:/home/zly/prj/oss/pig/bin/../lib/accumulo-core-1.5.0.jar:/home/zly/prj/oss/pig/bin/../lib/accumulo-fate-1.5.0.jar:/home/zly/prj/oss/pig/bin/../lib/accumulo-server-1.5.0.jar:/home/zly/prj/oss/pig/bin/../lib/accumulo-start-1.5.0.jar:/home/zly/prj/oss/pig/bin/../lib/accumulo-trace-1.5.0.jar:/home/zly/prj/oss/pig/bin/../lib/antlr-runtime-3.4.jar:/home/zly/prj/oss/pig/bin/../lib/asm-4.0.jar:/home/zly/prj/oss/pig/bin/../lib/asm-commons-4.0.jar:/home/zly/prj/oss/pig/bin/../lib/asm-tree-4.0.jar:/home/zly/prj/oss/pig/bin/../lib/automaton-1.11-8.jar:/home/zly/prj/oss/pig/bin/../lib/avro-1.7.5.jar:/home/zly/prj/oss/pig/bin/../lib/avro-tools-1.7.5-nodeps.jar:/home/zly/prj/oss/pig/bin/../lib/groovy-all-1.8.6.jar:/home/zly/prj/oss/pig/bin/../lib/guava-14.0.1.jar:/home/zly/prj/oss/pig/bin/../lib/hive-common-0.14.0-SNAPSHOT.jar:/home/zly/prj/oss/pig/bin/../lib/hive-exec-0.14.0-SNAPSHOT-core.jar:/home/zly/prj/oss/pig/bin/../lib/hive-serde-0.14.0-SNAPSHOT.jar:/home/zly/prj/oss/pig/bin/../lib/hive-shims-common-0.14.0-SNAPSHOT.jar:/home/zly/prj/oss/pig/bin/../lib/hive-shims-common-secure-0.14.0-SNAPSHOT.jar:/home/zly/prj/oss/pig/bin/../lib/jackson-core-asl-1.8.8.jar:/home/zly/prj/oss/pig/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/zly/prj/oss/pig/bin/../lib/jansi-1.9.jar:/home/zly/prj/oss/pig/bin/../lib/jline-1.0.jar:/home/zly/prj/oss/pig/bin/../lib/joda-time-2.1.jar:/home/zly/prj/oss/pig/bin/../lib/jruby-complete-1.6.7.jar:/home/zly/prj/oss/pig/bin/../lib/js-1.7R2.jar:/home/zly/prj/oss/pig/bin/../lib/json-simple-1.1.jar:/home/zly/prj/oss/pig/bin/../lib/jython-standalone-2.5.3.jar:/home/zly/prj/oss/pig/bin/../lib/protobuf-java-2.4.1-shaded.jar:/home/zly/prj/oss/pig/bin/../lib/protobuf-java-2.4.1.jar:/home/zly/prj/oss/pig/bin/../lib/protobuf-java-2.5.0.jar:/home/zly/prj/oss/pig/bin/../lib/snappy-java-1.0.5.jar:/home/zly/prj/oss/pig/bin/../lib/trevni-avro-1.7.5.jar:/home/zly/prj/oss/pig/bin/../lib/trevni-core-1.7.5.jar:/home/zly/prj/oss/pig/bin/../lib/zookeeper-3.4.5.jar:/home/zly/prj/oss/pig/bin/../pig-0.14.0-SNAPSHOT-core-h1.jar:/home/zly/prj/oss/pig/bin/../lib/h1/avro-mapred-1.7.5.jar:/home/zly/prj/oss/pig/bin/../lib/h1/hbase-client-0.96.0-hadoop1.jar:/home/zly/prj/oss/pig/bin/../lib/h1/hbase-common-0.96.0-hadoop1.jar:/home/zly/prj/oss/pig/bin/../lib/h1/hbase-hadoop-compat-0.96.0-hadoop1.jar:/home/zly/prj/oss/pig/bin/../lib/h1/hbase-hadoop1-compat-0.96.0-hadoop1.jar:/home/zly/prj/oss/pig/bin/../lib/h1/hbase-protocol-0.96.0-hadoop1.jar:/home/zly/prj/oss/pig/bin/../lib/h1/hbase-server-0.96.0-hadoop1.jar:/home/zly/prj/oss/pig/bin/../lib/h1/hive-shims-0.20S-0.14.0-SNAPSHOT.jar org.apache.pig.Main -x local id.pig
Listening for transport dt_socket at address: 9999
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/commons/logging/LogFactory
        at org.apache.pig.Main.<clinit>(Main.java:100)
Caused by: java.lang.ClassNotFoundException: org.apache.commons.logging.LogFactory
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        ... 1 more
Exception in thread ""Thread-0"" java.lang.NoClassDefFoundError: org/apache/hadoop/fs/LocalFileSystem
        at org.apache.pig.Main$1.run(Main.java:95)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.fs.LocalFileSystem
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        ... 1 more","17/Sep/14 20:05;rohini;Sorry. My bad. Forgot that core-site.xml is also loaded as part of defaults. We use a perl script instead of the bash bin/pig and what we do there is not add HADOOP_CONF_DIR to classpath if it is local mode. 

{code}
if [ ""$HADOOP_CONF_DIR"" != """" ]; then
    CLASSPATH=${CLASSPATH}:${HADOOP_CONF_DIR}
fi
{code}

should not be done if local mode. Either you can do that or even the current patch is ok.  But you can simplify the code as below as you don't have to initialize UserGroupInformation in case of non local mode.

{code}
                    if (opts.getValStr().toLowerCase().contains(""local"") {
                        UserGroupInformation.setConfiguration(new Configuration(false));
                   }
{code}

Above check also ensures kerberos is not enabled for tez_local mode. With 0.14 we are dropping support for Hadoop 0.20 so using UserGroupInformation class directly is ok at last ...


                    ","18/Sep/14 02:55;kellyzly;Hi [~rohini]:
  Thanks for your comment.  Patch has been updated and submitted. Can you help to review again?",18/Sep/14 14:43;rohini;+1. Committed to trunk (0.14).  Thanks Liyun Zhang for the patch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLOOR documentation references CEIL function instead of FLOOR,PIG-3506,12672735,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,seshness,seshness,seshness,08/Oct/13 01:41,07/Jul/14 18:07,14/Mar/19 03:07,08/Oct/13 04:19,,,,,,,,0.13.0,,,,documentation,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Oct/13 01:43;seshness;PIG-3506.patch;https://issues.apache.org/jira/secure/attachment/12607292/PIG-3506.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-08 04:19:19.839,,,no_permission,,,,,,,,,,,,352358,Reviewed,,,Tue Oct 08 04:19:19 UTC 2013,,,,,,,0|i1or1b:,352646,,,,,,,,,,08/Oct/13 04:19;daijy;Patch committed to trunk. Thanks Seshadri!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix e2e Describe_cmdline_12,PIG-3504,12672553,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,06/Oct/13 21:23,14/Oct/13 16:46,14/Mar/19 03:07,06/Oct/13 23:47,0.12.0,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"The test case fails with the following error:
{code}
./test_harness.pl::TestDriverPig::compareScript INFO Checking test stderr for regular expression <No plan for X to describe>
./test_harness.pl::TestDriverPig::compareScript INFO Check failed: regex match of <No plan for X to describe> expected in stderr: ./out/pigtest/cheolsoop/cheolsoop-1381090887-cmdline.conf-Describe_cmdline/Describe_cmdline_12.out/stderr
INFO: TestDriver::runTestGroup() at 706:Test Describe_cmdline_12 FAILED at 1381091044
Results so far,    PASSED: 10   FAILED: 1    SKIPPED: 0    ABORTED: 0    FAILED DEPENDENCY: 0    [cmdline.conf-Describe_cmdline]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Oct/13 21:25;cheolsoo;PIG-3504-1.patch;https://issues.apache.org/jira/secure/attachment/12607106/PIG-3504-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-06 23:47:37.058,,,no_permission,,,,,,,,,,,,352176,Reviewed,,,Sun Oct 06 23:47:37 UTC 2013,,,,,,,0|i1opwv:,352464,,,,,,,,,,06/Oct/13 21:25;cheolsoo;Error message has changed in 0.12. See PIG-3491 for details.,06/Oct/13 23:47;daijy;Patch committed to trunk and branch 0.12. Thanks Cheolsoo!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobControlCompiler should only do reducer estimation when the job has a reduce phase,PIG-3497,12672226,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,amatsukawa,amatsukawa,amatsukawa,03/Oct/13 22:04,14/Oct/13 16:46,14/Mar/19 03:07,04/Oct/13 22:44,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"Currently, JobControlCompiler makes an estimation for the number of reducers required (by default based on input size into mappers) regardless of whether there is a reduce phase in the job. This is unnecessary, especially when running more complex custom reducer estimators. 

Change to only estimate reducers when necessary.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Oct/13 22:06;amatsukawa;reducer_estimation.patch;https://issues.apache.org/jira/secure/attachment/12606657/reducer_estimation.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-04 03:44:44.124,,,no_permission,,,,,,,,,,,,351852,,,,Fri Oct 11 19:18:49 UTC 2013,,,,,,,0|i1onxj:,352140,,,,,,,,,,"04/Oct/13 03:44;aniket486;+1. Committed to trunk.
[~daijy], should we also commit this to 0.12?",04/Oct/13 06:53;daijy;+1 for 0.12. The change is simple enough and not likely to break anything.,04/Oct/13 22:44;aniket486;Thanks. I committed this to pig-0.12. (will move the trunk's changes in CHANGES.txt in the right section).,"11/Oct/13 19:16;daijy;Actually I overlooked the patch. It also moves adjustNumReducers to a much later stage. And at that time, all POLoad has been removed from plan, so reduceAdjuster will stop working. Will create another ticket for that.","11/Oct/13 19:18;aniket486;Thanks Daniel!
I found some related test failures-

junit.framework.AssertionFailedError: Unexpected value found in configs for pig.info.reducers.estimated.parallel expected:<6> but was:<-1>
	at org.apache.pig.test.Util.assertConfLong(Util.java:1223)
	at org.apache.pig.test.Util.assertParallelValues(Util.java:1218)
	at org.apache.pig.test.TestJobSubmission.testReducerNumEstimation(TestJobSubmission.java:570)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Propagate HBase 0.95 jars to the backend,PIG-3496,12672221,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,jarcec,jarcec,jarcec,03/Oct/13 21:39,07/Jul/14 18:07,14/Mar/19 03:07,07/Oct/13 17:35,0.11,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"In PIG-3390 we've introduced support for HBase 0.95 that introduced a lot of significant changes to HBase. One of the biggest user facing changes was splitting one uber jar file into multiple independent jars (such as {{hbase-common}}, {{hbase-client}}, ...).  

{{HBaseStore}} have [special code|https://github.com/apache/pig/blob/trunk/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java#L724] for propagating HBase jar files and important dependencies to the backend. This logic has not been altered to take into account the different HBase jars after the split and as a result the HBase integration with 0.95 is not working in fully distributed mode (it is work in local mode thought).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Oct/13 21:59;jarcec;PIG-3496.patch;https://issues.apache.org/jira/secure/attachment/12606656/PIG-3496.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-04 18:33:00.873,,,no_permission,,,,,,,,,,,,351847,,,,Mon Oct 07 17:35:07 UTC 2013,,,,,,,0|i1onwf:,352135,,,,,,,,,,04/Oct/13 18:33;xuefuz;Review comments on rb.,04/Oct/13 21:54;xuefuz;+1. Will commit once tests pass.,"07/Oct/13 17:35;xuefuz;Patch committed to trunk. Thanks, Jarcec.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming udf e2e tests failures on Windows,PIG-3495,12672031,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,02/Oct/13 21:40,14/Oct/13 16:46,14/Mar/19 03:07,03/Oct/13 18:06,,,,,,,,0.12.0,,,,impl,,,0,,,,,,,,,,,,,"Register a jython script with an absolute path fail. For Example:
{code}
register 'D:\scriptingudf.py' using jython as myfuncs;
a = load 'studenttab10k' using PigStorage() as (name, age:int, gpa:double);
b = foreach a generate myfuncs.square(age);
dump b;
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Oct/13 21:41;daijy;PIG-3495-1.patch;https://issues.apache.org/jira/secure/attachment/12606450/PIG-3495-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-02 22:35:15.234,,,no_permission,,,,,,,,,,,,351657,Reviewed,,,Thu Oct 03 18:06:35 UTC 2013,,,,,,,0|i1omqf:,351945,,,,,,,,,,"02/Oct/13 22:35;rohini;if (Shell.WINDOWS && scriptPath.charAt(1)==':') {
+                scriptPath = scriptPath.charAt(0) + scriptPath.substring(2);
+            }

Can you tell what exactly is the error and how this change will make it work?  Doesn't this piece of code look for files inside jar. How does removing : after drive letter help here?
","03/Oct/13 05:34;daijy;Sure. When we register a script with absolute path, such as D:\scriptingudf.py, it ends up with D\scriptingudf.py in job.jar. The store part is done with PigContext.addScriptFile:

        aliasedScriptFiles.put(name.replaceFirst(""^/"", """").replaceAll("":"", """"), new File(path));

The PigContext code is part of PIG-3333.

When we retrieve it back in the backend, we need to do the symmetric operation.",03/Oct/13 13:50;rohini;Thanks. +1,03/Oct/13 18:06;daijy;Patch committed to branch 0.12 and trunk. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Several fixes for e2e tests,PIG-3494,12672019,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,02/Oct/13 20:36,25/Feb/14 23:26,14/Mar/19 03:07,03/Oct/13 04:29,,,,,,,,0.12.0,,,,e2e harness,,,0,,,,,,,,,,,,,"Address several issues in e2e tests:
1. Adding the capacity to test Pig installed by rpm (also involves configurable piggybank.jar)
2. Remove hadoop23.res since it is no longer needed
3. Remove hadoop2 specific error message ""UdfException_[1-4]"" since they are fixed by PIG-3360",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Oct/13 20:37;daijy;PIG-3494-1.patch;https://issues.apache.org/jira/secure/attachment/12606438/PIG-3494-1.patch,25/Feb/14 06:09;rohini;PIG-3494-Fix-localmode.patch;https://issues.apache.org/jira/secure/attachment/12630890/PIG-3494-Fix-localmode.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-10-02 22:39:41.445,,,no_permission,,,,,,,,,,,,351645,Reviewed,,,Tue Feb 25 23:26:34 UTC 2014,,,,,,,0|i1omnr:,351933,,,,,,,,,,02/Oct/13 22:39;rohini;+1,02/Oct/13 22:46;rohini;Actually noticed that PIG-3360 is partially reverted in branch-0.12 ( src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/Launcher.java) when reverting the execution engine changes. You will have to recommit that .,"03/Oct/13 04:29;daijy;Thanks Rohini, nice catch. Commit the reverted part back to 0.12 branch.

PIG-3494-1.patch committed to both branch 0.12 and trunk. ","03/Oct/13 05:04;hudson;FAILURE: Integrated in Hive-branch-0.12-hadoop1 #6 (See [https://builds.apache.org/job/Hive-branch-0.12-hadoop1/6/])
PIG-3494: Several fixes for e2e tests (daijy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1528712)
* /pig/trunk/test/e2e/harness/test_harness.pl
* /pig/trunk/test/e2e/pig/conf/default.conf
* /pig/trunk/test/e2e/pig/conf/rpm.conf
* /pig/trunk/test/e2e/pig/drivers/TestDriverPig.pm
* /pig/trunk/test/e2e/pig/drivers/TestDriverScript.pm
* /pig/trunk/test/e2e/pig/tests/negative.conf
* /pig/trunk/test/e2e/pig/tests/nightly.conf
","03/Oct/13 16:54;hudson;FAILURE: Integrated in Hive-trunk-h0.21 #2376 (See [https://builds.apache.org/job/Hive-trunk-h0.21/2376/])
PIG-3494: Several fixes for e2e tests (daijy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1528712)
* /pig/trunk/test/e2e/harness/test_harness.pl
* /pig/trunk/test/e2e/pig/conf/default.conf
* /pig/trunk/test/e2e/pig/conf/rpm.conf
* /pig/trunk/test/e2e/pig/drivers/TestDriverPig.pm
* /pig/trunk/test/e2e/pig/drivers/TestDriverScript.pm
* /pig/trunk/test/e2e/pig/tests/negative.conf
* /pig/trunk/test/e2e/pig/tests/nightly.conf
","03/Oct/13 19:30;hudson;ABORTED: Integrated in Hive-trunk-hadoop2 #472 (See [https://builds.apache.org/job/Hive-trunk-hadoop2/472/])
PIG-3494: Several fixes for e2e tests (daijy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1528712)
* /pig/trunk/test/e2e/harness/test_harness.pl
* /pig/trunk/test/e2e/pig/conf/default.conf
* /pig/trunk/test/e2e/pig/conf/rpm.conf
* /pig/trunk/test/e2e/pig/drivers/TestDriverPig.pm
* /pig/trunk/test/e2e/pig/drivers/TestDriverScript.pm
* /pig/trunk/test/e2e/pig/tests/negative.conf
* /pig/trunk/test/e2e/pig/tests/nightly.conf
","03/Oct/13 23:08;hudson;FAILURE: Integrated in Hive-trunk-hadoop2-ptest #123 (See [https://builds.apache.org/job/Hive-trunk-hadoop2-ptest/123/])
PIG-3494: Several fixes for e2e tests (daijy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1528712)
* /pig/trunk/test/e2e/harness/test_harness.pl
* /pig/trunk/test/e2e/pig/conf/default.conf
* /pig/trunk/test/e2e/pig/conf/rpm.conf
* /pig/trunk/test/e2e/pig/drivers/TestDriverPig.pm
* /pig/trunk/test/e2e/pig/drivers/TestDriverScript.pm
* /pig/trunk/test/e2e/pig/tests/negative.conf
* /pig/trunk/test/e2e/pig/tests/nightly.conf
","04/Oct/13 02:17;hudson;SUCCESS: Integrated in Hive-trunk-hadoop1-ptest #189 (See [https://builds.apache.org/job/Hive-trunk-hadoop1-ptest/189/])
PIG-3494: Several fixes for e2e tests (daijy: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1528712)
* /pig/trunk/test/e2e/harness/test_harness.pl
* /pig/trunk/test/e2e/pig/conf/default.conf
* /pig/trunk/test/e2e/pig/conf/rpm.conf
* /pig/trunk/test/e2e/pig/drivers/TestDriverPig.pm
* /pig/trunk/test/e2e/pig/drivers/TestDriverScript.pm
* /pig/trunk/test/e2e/pig/tests/negative.conf
* /pig/trunk/test/e2e/pig/tests/nightly.conf
","25/Feb/14 06:09;rohini;[~daijy],
PIG-3494-Fix-localmode.patch is a very minor patch that fixes local mode. Not opening another jira as it just a one line change. Can you review it?",25/Feb/14 07:41;daijy;+1,25/Feb/14 23:26;rohini;Thanks Daniel. Committed to branch-0.12 and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnPrune dropping used column due to LogicalRelationalOperator.fixDuplicateUids changes not propagating,PIG-3492,12671642,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,01/Oct/13 18:57,15/Apr/14 20:45,14/Mar/19 03:07,23/Oct/13 18:15,0.11.1,0.12.1,0.13.0,,,,,0.12.1,,,,,,,0,,,,,,,,,,,,,"I don't have a testcase I can upload at the moment, but here's my observation.

SplitFilter -> schemaResetter -> LOGenerate.getSchema -> LogicalRelationalOperator.fixDuplicateUids() creating a new UID but that UID is not propagated to the entire plan (since SplitFilter.reportChanges only returns subplan).

As a result, I am seeing ColumnPruning cutting off those used columns.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Oct/13 18:06;daijy;PIG-3492-trunk-delta.patch;https://issues.apache.org/jira/secure/attachment/12609890/PIG-3492-trunk-delta.patch,17/Oct/13 15:13;knoguchi;pig-3492-trunk_04.patch;https://issues.apache.org/jira/secure/attachment/12608947/pig-3492-trunk_04.patch,09/Oct/13 16:55;knoguchi;pig-3492-v0.12_01.patch;https://issues.apache.org/jira/secure/attachment/12607600/pig-3492-v0.12_01.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-10-01 19:05:02.467,,,no_permission,,,,,,,,,,,,351352,Reviewed,,,Wed Oct 23 18:15:18 UTC 2013,,,,,,,0|i1okvj:,351644,,,,,,,,,,"01/Oct/13 19:05;daijy;Yes, I also see couple of places fixDuplicateUids is get misused. uid play a vital role in ColumnPruner. So every time we reassign uid, we need to make sure operator has the knowledge how does that uid get generated, and convey it to ColumnPruner.","04/Oct/13 18:06;knoguchi;We're only seeing this issue on complicate scripts with hundreds of lines.
This is the shortest I got.  This test needs to be called with '-t PushUpFilter'.
{noformat}
pig> cat test.pig
A = load './test.txt' as (a:int, b:chararray);
B = FOREACH A generate a;
C = GROUP B by a;
D = filter C by group > 0 and group < 100;
E = FOREACH D {
         F = LIMIT B 1 ;
         GENERATE B.a as mya, FLATTEN(F.a) as setting;
    }
G = FOREACH E GENERATE mya, setting as setting;
dump G;
{noformat}
Relation G should contain two columns, 'mya' and 'setting'.  But result only contains 1 column.

{noformat}
pig> cat test.txt
3       i
3       i
1       i
2       i
2       i
3       i
pig> pig -x local  -t PushUpFilter ./test.pig
({(1)})
({(2),(2)})
({(3),(3),(3)})
{noformat}

By skipping ColumnMapKeyPrune or SplitFilter, you get a correct result of 
{noformat}
pig> pig -x local  -t PushUpFilter -t ColumnMapKeyPrune ./test.pig
or
pig> pig -x local  -t PushUpFilter -t SplitFilter  ./test.pig
({(1)},1)
({(2),(2)},2)
({(3),(3),(3)},3)
{noformat}

Explain would show that second column was cut off.
{noformat}
Incorrect case (-t PushUpFilter)
G: (Name: LOStore Schema: mya#60:bag{#59:tuple(a#23:int)})ColumnPrune:InputUids=[63, 60]ColumnPrune:OutputUids=[63, 60]
Correct case (-t PushUpFilter -t SplitFilter)
G: (Name: LOStore Schema: mya#60:bag{#59:tuple(a#23:int)},setting#63:int)ColumnPrune:InputUids=[63, 60]ColumnPrune:OutputUids=[63, 60]
{noformat}
","08/Oct/13 21:28;knoguchi;I see three Jiras that added LogicalRelationalOperator.fixDuplicateUids.

* PIG-3020 (LOJoin) ""Duplicate uid in schema"" error when joining two relations derived from the same load statement""
* PIG-3144 (LOGenerate)  ""Erroneous map entry alias resolution leading to ""Duplicate schema alias"" errors""
* PIG-3292 (LOCross)   ""Logical plan invalid state: duplicate uid in schema during self-join to get cross product""

I'm skipping PIG-3292 since Daniel reviewed with the comment
  ""Interplay with ColumnPruner is fine here since nested plan will include entire required plan branch""


PIG-3020 (LOJoin) actually talks about two separate problems.
(i-1) PigParser failing with 'Duplicate schema alias: age'. Only happened in 0.11.
   This was actually about ImplicitSplitInserter's new uid not propagating to the top foreach.
I believe this issue was fixed later by PIG-3310 (""ImplicitSplitInserter does not generate new uids for nested schema fields, leading to miscomputations"" fixed only in 0.12).  Confirmed by running a simple test without LogicalRelationalOperator.fixDuplicateUids.

(i-2) 'describe' showing incorrect schema due to duplicate UID.  Happened on 0.10 and 0.11.
   This was due to 'describe' being called without LogicalPlanOptimizer.optimize() which includes some important rules like ImplicitSplitInserter and  DuplicateForEachColumnRewrite.

(ii)  PIG-3144(LOGenerate) issue seems to have started after a completely unrelated Jira,

    PIG-2710 ""Implement Naive CUBE operator"" in 0.11.

{noformat}
src/org/apache/pig/parser/LogicalPlanBuilder.java

+ 406     private void expandAndResetVisitor(SourceLocation loc,
+ 407       LogicalRelationalOperator lrop) throws ParserValidationException {
+ 408           try {
+ 409               (new ProjectStarExpander(lrop.getPlan())).visit();
+ 410               (new ProjStarInUdfExpander(lrop.getPlan())).visit();
+ 411               new SchemaResetter(lrop.getPlan(), true).visit();
+ 412           } catch (FrontendException e) {
+ 413               throw new ParserValidationException(intStream, loc, e);
+ 414           }
+ 415     }

 934     String buildForeachOp(SourceLocation loc, LOForEach op, String alias, String inputAlias, LogicalPlan innerPlan)
 935     throws ParserValidationException {
 936         op.setInnerPlan( innerPlan );
 937         alias = buildOp( loc, op, alias, inputAlias, null );

-             (new ProjectStarExpander(op.getPlan())).visit(op);
-             (new ProjStarInUdfExpander(op.getPlan())).visit(op);
-             new SchemaResetter(op.getPlan(), true).visit(op);

+938         expandAndResetVisitor(loc, op);
 939         return alias;
 940     }
{noformat}

So basically we started traversing the entire plan (visit()) for every operator builds instead of just the operator it's working on (visit(op)).
This has caused the 'alias' to get updated before LogicalPlanOptimizer.optimize() -> DuplicateForEachColumnRewrite and causing the
""Duplicate schema alias"" error.  Rolling back this change seems to bring back the pre-0.11 behavior.


Uploading an intial patch. Goal is to take out the LogicalRelationalOperator.fixDuplicateUids. from both PIG-3020(LOJoin) and PIG-3144(LOGenerate).

(i-1) For release-0.12: No-op.  For release-0.11: Backport pig-3310.
(i-2) We can either fix it by forcing compilePp() before describe or moving ImplicitSplitInserter/DuplicateForEachColumnRewrite to PigServer.compile().
      There is a comment that says

{noformat}
./src/org/apache/pig/PigServer.java
1692         private void compile(LogicalPlan lp) throws FrontendException  {
....
1699
1700             // TODO: move optimizer here from HExecuteEngine.
1701             // TODO: input/output validation visitor
1702
{noformat}

       For now, I'm taking an easy approach of calling compilePp() for describe.

(ii) I'm rolling back small section of PIG-2710 in src/org/apache/pig/parser/LogicalPlanBuilder.java that was hopefully only for shortening the code and the change in behavior was unintended.

For now, patch only applies to release 0.12 since it seems like location of LogicalPlanOptimizer.optimize() may change in the near future (PIG-3508).","09/Oct/13 16:55;knoguchi;I carelessly uploaded a patch with a typo (and would fail on compile error).
Running unittest,  but appreciate your feedback.",11/Oct/13 17:34;rohini;compilePp in dumpSchema makes couple of tests fail returning empty schema.,"17/Oct/13 15:13;knoguchi;bq. compilePp in dumpSchema makes couple of tests fail returning empty schema.

Thanks Rohini.  I guess my shortcut didn't work for some tests...  For 0.11, we might want to take out compilePp hack and keep the rest.  That would at least make 0.11 as good(bad) as 0.10 in terms of the bugs we're seeing. ('describe' bug would remain but LOGenerate/LOJoin bug fixed)

Now, for the 0.12 and long term,  Taking the latter approach from my previous comment.

bq. (i-2) We can either fix it by forcing compilePp() before describe or moving ImplicitSplitInserter/DuplicateForEachColumnRewrite to PigServer.compile().

ImplicitSplitInserter/DuplicateForEachColumnRewrite seem to be an essential part of compilation for correctness and they aren't really an optimization.   With that assumption, I rewrote them as Visitors and moved them from LogicalPlanOptimizer to PigServer.compile.

With this change, 38 unit tests started failing.
* 5 tests failing with NullPointerException in Illustrate.  I believe this was due to a bug in pig/pen/LineageTrimmingVisitor.java.  Added entry of LOSplitOutput for the fix.
* 1 TestOptimizeLimit failing with typecasting LOLimit to LOForeach.  This was due to change in logical plan having  ImplicitSplitInserter/DuplicateForEachColumnRewrite as default irrespective of the optimizer the test picks.
* 32 failures at TestMultiQueryCompiler/TestMultiQueryLocal since the tests were counting the number of Logical Operators.  Updated the tests after making sure increase is only coming from LOSplit and LOSplitOutput.

Trying to get e2e running with this patch.
","18/Oct/13 19:17;knoguchi;Daniel asked me to summarize the changes.  Here you go.

(1) Move DuplicateForEachColumnRewrite and ImplicitSplitInserter from LogicalPlanOptimizer to PigServer.compile().
    Reason: Next visitor, TypeCheckingRelVisitor, was calling resetSchema/getSchema and fields with duplicate UIDs were getting incorrect aliases due to that.  (Execution was fine since LogicalPlanOptimizer.optimize() was eventually called, but not for 'describe'.  Also, even temporary, it's not good to have incorrect aliases assigned to LogicalOperators.

(2) Fix the test cases that started failing due to (1).
    (2-1) LineageTrimmingVisitor (used in illustrate) was hitting with NullPointerException since LOSplitOutput was missing in the code.
    (2-2) TestOptimizeLimit failed due to changed logicalplan causing typecast error.  Fixed.
    (2-3) Bunch of TestOptimizeLimit tests failed due to new logicalplan including LOSplit and LOSplitOUtput.

(3) Rolling back changes in LogicalPlanBuilder from pig-2710 since ProjectStarExpander/ProjStarInUdfExpander/ProjStarInUdfExpander used to be called only for the corresponding LogicalOperator but the change started calling them for the entire plan each time.  This change itself fixed (5) and brought back the 0.10 behavior.

(4) Revert PIG-3020. Take out LogicalRelationalOperator.fixDuplicateUids from LOJoin.  Fixed in PIG-3310.

(5) Revert PIG-3144. Take out LogicalRelationalOperator.fixDuplicateUids from LOGenerate.  Fixed by (1) and (3).","18/Oct/13 19:38;knoguchi;Forgot to add. 

For 0.11, asking if we can consider (3), (4) and (5).  Leaving the 'describe' bug.
For 0.12 and later, asking (1) to (5).","19/Oct/13 01:49;daijy;Thanks [~horaguchi] for detailed explanation. This patch fixed several issues:
1. Remove fixDuplicateUids from LOJoin/LOGenerate
That's for sure need to be removed. We cannot arbitrary reassign uid. Every operator should have lineage knowledge and be able to trace to input uids which generate a particular output uid. Sometimes we do need to reassign the uid, but if this happens, we need to remember the mapping inside the operator, and update the logic ColumnDependencyVisitor of tracing every output uid to its input uid. 

Take LOJoin for example, each output column is derived from one input column without transformation, so we keep uid unchanged. If input#1 of join has the uid (1, 2, 3), input#2 has the uid (4, 5), the join output will have the uid (1, 2, 3, 4, 5). When ColumnDependencyVisitor try to prune uid 5, we know that is from second column of input#2. In the case we do want to regenerate output uid to (21, 22, 23, 24, 25), join operator need to remember the mapping (1->21, 2->22, 3->23, 4->24, 5->25), so that when ColumnDependencyVisitor try to prune uid 25, we know it is from second column of input#2 as well. One example of uid regeneration is LOSplit, which has a field uidMapping to remember the mapping. ColumnDependencyVisitor will make use of this information to trace the lineage. 

As we already mentioned, uid regeneration is already done in LOSplitOutput, so after ImplicitSplitInserter, we shall not have duplicate uids and should not have uid conflict. The issue seen in PIG-3020 is due to a bug in LOSplitOutput uid mapping process as Koji mentioned.

2. LogicalPlanBuilder.expandAndResetVisitor change
Not sure what drive the original change, seems there is no reason to visit the whole plan after building one operator. The logical plan is not in a consistent state at this moment, global SchemaResetter might not work

3. move DuplicateForEachColumnRewrite and ImplicitSplitInserter
When we design optimizer, we try to include every plan transformation into it. But now I think we may separate it into two part. One is to bring the plan into a valid state, the other is the real optimizer rule. The reason is at validate stage, the plan is still inconsistent and many optimizer rule cannot handle, and the global SchemaReset/UidResetter does not work. We may want to do some refactory and make the separation clear in another Jira. ""describe"" should happen after validation, but might not need to go through optimizer. Move DuplicateForEachColumnRewrite/ImplicitSplitInserter into compile for now seems to be ok.

Overall, the patch looks pretty good. It fix above mentioned issue cleanly. We might want to preceed with clear validator/optmizer separation in anther Jira.

Will commit if tests pass.","20/Oct/13 00:06;knoguchi;Thanks for the review Daniel.

bq. Thanks [~horaguchi] for detailed explanation. This patch fixed several issues:

Different Koji :)   It's [~knoguchi].","23/Oct/13 18:06;daijy;Fix some e2e test failures (eg: udf_TOBAGandTOTUPLE_3), also include Koji's test case in PIG-3492-trunk-delta.patch.","23/Oct/13 18:15;daijy;pig-3492-trunk_04.patch and pig-3492-trunk-delta.patch are committed to both trunk and 0.12 branch. I don't think we will have additional 0.11 release, so skip 0.11 branch.

I will open additional Jira to formalize the validator. 

Thanks Koji, that's a really important fix.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix e2e failure Jython_Diagnostics_4,PIG-3491,12671414,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,30/Sep/13 18:43,14/Oct/13 16:45,14/Mar/19 03:07,30/Sep/13 23:34,,,,,,,,0.12.0,,,,e2e harness,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30/Sep/13 18:44;daijy;PIG-3491-1.patch;https://issues.apache.org/jira/secure/attachment/12605948/PIG-3491-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-30 21:27:31.379,,,no_permission,,,,,,,,,,,,351120,Reviewed,,,Mon Sep 30 23:34:49 UTC 2013,,,,,,,0|i1ojfz:,351412,,,,,,,,,,30/Sep/13 19:28;daijy;Simple error message change due to PIG-2970.,30/Sep/13 21:27;rohini;+1,30/Sep/13 23:34;daijy;Patch committed to both branch 0.12 and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix syntax errors in nightly.conf,PIG-3487,12670747,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,arpitgupta,arpitgupta,arpitgupta,26/Sep/13 15:26,14/Oct/13 16:46,14/Mar/19 03:07,26/Sep/13 16:05,0.12.1,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"{code}
FATAL ERROR ./test_harness.pl at 150 : Error reading config file </path/pig/test/e2e/pig/testdist/tests/nightly.conf>, <Bad name after pig' at (eval 19) line 3690.>
BUILD FAILED
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Sep/13 15:59;arpitgupta;PIG-3487.patch;https://issues.apache.org/jira/secure/attachment/12605267/PIG-3487.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-26 16:05:21.075,,,no_permission,,,,,,,,,,,,350576,Reviewed,,,Thu Sep 26 16:05:21 UTC 2013,,,,,,,0|i1og3r:,350869,,,,,,,,,,26/Sep/13 15:59;arpitgupta;Attached a patch with this applied the build passes.,26/Sep/13 16:05;daijy;+1. Patch committed to both 0.12 branch and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TFile-based tmpfile compression crashes in some cases,PIG-3480,12670332,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,dvryaboy,dvryaboy,24/Sep/13 18:32,15/Apr/14 20:45,14/Mar/19 03:07,14/Oct/13 03:23,,,,,,,,0.12.1,,,,,,,1,,,,,,,,,,,,,"When pig tmpfile compression is on, some jobs fail inside core hadoop internals.
Suspect TFile is the problem, because an experiment in replacing TFile with SequenceFile succeeded.",,,,,,,,,,,,,,,,,,,,,,,,,PIG-3530,,,,,,,09/Oct/13 00:09;aniket486;PIG-3480-2.patch;https://issues.apache.org/jira/secure/attachment/12607474/PIG-3480-2.patch,09/Oct/13 01:01;aniket486;PIG-3480-3.patch;https://issues.apache.org/jira/secure/attachment/12607480/PIG-3480-3.patch,09/Oct/13 01:33;aniket486;PIG-3480-4.patch;https://issues.apache.org/jira/secure/attachment/12607489/PIG-3480-4.patch,11/Oct/13 20:44;aniket486;PIG-3480-5.patch;https://issues.apache.org/jira/secure/attachment/12608073/PIG-3480-5.patch,12/Oct/13 00:45;aniket486;PIG-3480-6.patch;https://issues.apache.org/jira/secure/attachment/12608120/PIG-3480-6.patch,14/Oct/13 03:21;aniket486;PIG-3480-7.patch;https://issues.apache.org/jira/secure/attachment/12608235/PIG-3480-7.patch,24/Sep/13 18:46;dvryaboy;PIG-3480.patch;https://issues.apache.org/jira/secure/attachment/12604855/PIG-3480.patch,,,,,7.0,,,,,,,,,,,,,,,,,,,2013-09-24 19:15:14.284,,,no_permission,,,,,,,,,,,,350161,,,,Mon Oct 14 03:23:20 UTC 2013,,,,,,,0|i1odjz:,350455,,,,,,,,,,"24/Sep/13 18:35;dvryaboy;For most of the tasks that fail, no stack trace is available on Hadoop 1 (they just die with ""nonzero status 134"").

I did catch one task with a stack trace:
{code}
java.io.IOException: Error while reading compressed data at
org.apache.hadoop.io.IOUtils.wrappedReadForCompressedData(IOUtils.java:205) at 
org.apache.hadoop.mapred.IFile$Reader.readData(IFile.java:342) at 
org.apache.hadoop.mapred.IFile$Reader.rejigData(IFile.java:373) at 
org.apache.hadoop.mapred.IFile$Reader.readNextBlock(IFile.java:357) at 
org.apache.hadoop.mapred.IFile$Reader.next(IFile.java:389) at 
org.apache.hadoop.mapred.Merger$Segment.next(Merger.java:220) at 
org.apache.hadoop.mapred.Merger$MergeQueue.merge(Merger.java:420) at 
org.apache.hadoop.mapred.Merger$MergeQueue.merge(Merger.java:381) at 
org.apache.hadoop.mapred.Merger.merge(Merger.java:77) at 
org.apache.hadoop.mapred.MapTask$MapOutputBuffer.mergeParts(MapTask.java:1548) at 
org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1180) at 
org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:582) at 
org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:649) at org.apache.hadoop.mapred.MapTask.run(Map
{code}

No idea if this is relevant.

This problem does happen consistently -- 100% of the time on my script that shows this problem. Anecdotally, about 1/10 of our production scripts encounter this; I have not been able to establish a pattern yet.","24/Sep/13 18:46;dvryaboy;Attaching a rough patch which replaces use of TFile with SequenceFile.

Next steps:
- evaluate effect on size of compressed data for TFile vs SeqFile when TFile does work
- add tests, make TFile tests pass (in this file they fail, because of course TFile is not being used)
- make SeqFile the default method, since it doesn't break
- allow TFile use by a switch, since current users may want to keep it. I would prefer to not do that, but might if the first step shows significant differences.

Thoughts?
Especially from folks using TFile-based compression in production ([~rohini]?)","24/Sep/13 19:15;knoguchi;Dmitriy, isn't your stacktrace failing at mapred.IFile and not TFile? 

> This problem does happen consistently – 100% of the time on my script that shows this problem. 
>
And this problem goes away once tmpcompression is turned off? 
(pig.tmpfilecompression=false)","24/Sep/13 19:42;dvryaboy;[~knoguchi] yeah, I'm not sure the stack trace is relevant -- it's the only part that's not consistent about this.

The problem goes away when I set pig.tmpfilecompression to false, or when I replace TFile with SequenceFile.
I've also seen stack traces that were inside TFile, and had to do with some LZO decoding issues.. the actual error is really hard to capture, other than the fact that mappers fail consistently.","24/Sep/13 19:59;rohini;[~dvryaboy],
   We have been running it with pig.tmpfilecompression=true and pig.tmpfilecompression.codec=lzo as defaults from 2010 and have not encountered any issues so far. The problem might be elsewhere and TFile might not be the issue.","24/Sep/13 20:47;dvryaboy;Rohini I suspect this might be something about complex data types, which afaik are pretty rare at Y! and extremely common at Twitter.","24/Sep/13 20:48;dvryaboy;Rohini, do you guys use lzo or gz compression? Maybe it's just lzo that's breaking. I can test gz. That never actually occurred to me, I just assumed this is completely busted because I could never get it to work (since 2010..)",24/Sep/13 21:06;olgan;Could this be related to Hadoop version? ,"24/Sep/13 21:24;rohini;We do have complex types like bag of maps and bag of bags and one or two levels of nesting. But I assume you have way more nesting than we do. Does that matter though as what is written to TFile is just bytes for both key and value?

We use lzo. It would be good to try gz and see if the problem is with lzo for you. 

2013-09-24 21:10:21,289 INFO [main] com.hadoop.compression.lzo.GPLNativeCodeLoader: Loaded native gpl library
2013-09-24 21:10:21,291 INFO [main] com.hadoop.compression.lzo.LzoCodec: Successfully loaded & initialized native-lzo library
2013-09-24 21:10:21,293 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.lzo_deflate]

I don't think hadoop version should matter as we had hadoop 1.x till mid 2012. ","29/Sep/13 04:11;aniket486;bq. evaluate effect on size of compressed data for TFile vs SeqFile when TFile does work
https://issues.apache.org/jira/browse/HADOOP-3315?focusedCommentId=12631905&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12631905 has some benchmark details for SequenceFile vs TFile.
bq. add tests, make TFile tests pass (in this file they fail, because of course TFile is not being used)
I will submit a patch for this.
bq. make SeqFile the default method, since it doesn't break
+1 for this as the effect is not substantially worse.
bq. allow TFile use by a switch, since current users may want to keep it. I would prefer to not do that, but might if the first step shows significant differences.
[~rohini], what are your thoughts on this?","29/Sep/13 04:24;rohini;[~aniket486], 
   Would prefer having TFile as the default and Sequence file as an option. We have not had any issues with it for years and may be few others too it has been the default. Also the performance numbers are better for it by 10-40% with compression according to HADOOP-3315 that you have referred.

  And it would also be good to have the actual cause of failure with TFile investigated if possible to see if something is not being done right in Pig as TFile just writes byte[] for keys and values.","30/Sep/13 15:59;olgan;Agree with Rohini. Changing default just because we found a bug does not seem like a sound approach,","30/Sep/13 19:09;dvryaboy;That is fine with me, lets make sequence file optional. It will let people avoid the bug I am encountering, an also do things like use snappy compression. 
",11/Oct/13 21:02;aniket486;https://reviews.apache.org/r/14552/,"14/Oct/13 03:23;aniket486;Committed to trunk and 0.12 branch.
Thanks [~dvryaboy] and [~julienledem]!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix BigInt, BigDec, Date serialization. Improve perf of PigNullableWritable deserilization",PIG-3479,12670207,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,dvryaboy,dvryaboy,24/Sep/13 05:27,14/Oct/13 16:46,14/Mar/19 03:07,24/Sep/13 22:55,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"While working on something unrelated I discovered some serialization errors with recently added data types, and a heavy use of reflection slowing down PigNullableWritable deserialization.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Sep/13 05:28;dvryaboy;PIG-3479.patch;https://issues.apache.org/jira/secure/attachment/12604740/PIG-3479.patch,24/Sep/13 22:42;dvryaboy;PIG-3479.whitespace.patch;https://issues.apache.org/jira/secure/attachment/12604895/PIG-3479.whitespace.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-09-24 21:09:57.349,,,no_permission,,,,,,,,,,,,350036,,,,Tue Sep 24 22:55:57 UTC 2013,,,,,,,0|i1ocs7:,350330,Skewed join internals improved to get 10% or better improvement on reducers by eliminating unnecessary reflection.,,,,,,,,,"24/Sep/13 05:28;dvryaboy;Attaching a patch.

I extended an existing test to test the serialziation.. it's the only place we test Nullables at all :(.",24/Sep/13 21:09;jcoveney;+1,"24/Sep/13 22:42;dvryaboy;Same patch, but with whitespace changes. Committing this.",24/Sep/13 22:55;dvryaboy;Committed to trunk and 0.12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make StreamingUDF work for Hadoop 2,PIG-3478,12670086,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,lbendig,daijy,daijy,23/Sep/13 15:47,21/Nov/14 05:59,14/Mar/19 03:07,22/Jun/14 23:36,,,,,,,,0.14.0,,,,impl,,,0,,,,,,,,,,,,,"PIG-2417 introduced Streaming UDF. However, it does not work under Hadoop 2. Both unit tests/e2e tests under Haodop 2 fails. We need to fix it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Jun/14 15:49;lbendig;PIG-3478-2.patch;https://issues.apache.org/jira/secure/attachment/12647916/PIG-3478-2.patch,03/Jun/14 00:33;lbendig;PIG-3478-3.patch;https://issues.apache.org/jira/secure/attachment/12648038/PIG-3478-3.patch,09/Oct/13 16:28;jeremykarn;PIG-3478.patch;https://issues.apache.org/jira/secure/attachment/12607597/PIG-3478.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-10-09 16:29:17.918,,,no_permission,,,,,,,,,,,,349916,Reviewed,,,Sun Jun 22 23:36:32 UTC 2014,,,,,,,0|i1oc2f:,350214,,,,,,,,,,09/Oct/13 16:29;jeremykarn;Here's a patch I tested on a Hadoop 2.1.0 cluster.  I was able to run an example script and get the unit tests to pass but I wasn't able to run the e2e tests on the cluster to confirm that those are also fixed.,"10/Nov/13 23:16;cheolsoo;[~jeremykarn], I ran e2e tests (StreamingPythonUDFs) on an EMR Hadoop 2.2 cluster and saw two issues as follows:
# NPE in StreamingUDF.java
{code}
2013-11-10 22:32:19,694 FATAL [IPC Server handler 11 on 33809] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1383086282107_1892_m_000000_3 - exited : org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing [POUserFunc (Name: POUserFunc(org.apache.pig.impl.builtin.StreamingUDF)[int] - scope-3 Operator Key: scope-3) children: null at []]: java.lang.NullPointerException
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:338)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:378)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:298)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:282)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:277)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:775)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:162)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)
Caused by: java.lang.NullPointerException
	at org.apache.pig.impl.builtin.StreamingUDF.ensureUserFileAvailable(StreamingUDF.java:249)
	at org.apache.pig.impl.builtin.StreamingUDF.constructCommand(StreamingUDF.java:218)
	at org.apache.pig.impl.builtin.StreamingUDF.startUdfController(StreamingUDF.java:163)
	at org.apache.pig.impl.builtin.StreamingUDF.initialize(StreamingUDF.java:156)
	at org.apache.pig.impl.builtin.StreamingUDF.exec(StreamingUDF.java:146)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:330)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNextInteger(POUserFunc.java:379)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:321)
	... 13 more
{code}
NPE is thrown from {{udfFileStream.close();}} where udfFileStream is null.
# After fixing #1 by adding a null check, I ran into this error:
{code}
2013-11-10 23:00:51,402 FATAL [IPC Server handler 11 on 40139] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1383086282107_1905_m_000000_3 - exited : org.apache.pig.backend.executionengine.ExecException: ERROR 2078: Caught error from UDF: StreamingUDF [Could not create directory: /home/hadoop/.versions/2.2.0/logs/udfOutput]at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:358)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNextInteger(POUserFunc.java:379)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:321)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:378)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:298)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:282)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:277)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:775)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:162)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)
Caused by: java.io.IOException: Could not create directory: /home/hadoop/.versions/2.2.0/logs/udfOutput
at org.apache.pig.scripting.ScriptingOutputCapturer.getTaskLogDir(ScriptingOutputCapturer.java:104)
at org.apache.pig.scripting.ScriptingOutputCapturer.getStandardOutputRootWriteLocation(ScriptingOutputCapturer.java:86)
at org.apache.pig.impl.builtin.StreamingUDF.constructCommand(StreamingUDF.java:187)
at org.apache.pig.impl.builtin.StreamingUDF.startUdfController(StreamingUDF.java:163)
at org.apache.pig.impl.builtin.StreamingUDF.initialize(StreamingUDF.java:156)at org.apache.pig.impl.builtin.StreamingUDF.exec(StreamingUDF.java:146)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:330)... 15 more
{code}

Can you look into these failures? We should also enable {{StreamingPythonUDFs}} tests in nightly.conf once they're fixed.",12/Nov/13 19:55;rohini;Cancelling patch till Cheolsoo's comments are addressed.,10/Feb/14 22:16;russell.jurney;Looking into this myself.,01/Apr/14 21:05;prkommireddi;[~russell.jurney] [~jeremykarn] anyone looking into this?,02/Apr/14 16:13;jeremykarn;I probably won't get a chance to look at this in the next couple of weeks.  ,"02/Apr/14 20:14;prkommireddi;Thanks for getting back. I've moved this to 0.13, we could possibly get it in by then.","02/Jun/14 15:49;lbendig;Adjustment of the initial patch: {{StreamingUDF#ensureUserFileAvailable}} couldn't get the inputStream of the registered python script due to an additional slash in the file's path. Now I managed to run unit and e2e tests (local and cluster mode (pseudo-distributed)) successfully on Hadoop 2.2.0 and 2.4.0. 
","02/Jun/14 16:55;cheolsoo;This is great! I am running the tests to verify your fix.

[~aniket486], [~daijy], can we commit this into branch-0.13 if all the tests pass? I think this is an important bug fix.","02/Jun/14 19:38;cheolsoo;[~lbendig], unfortunately, I am getting the following error while running the e2e tests-
{code}
Caused by: java.io.IOException: Could not create directory: /apps/hadoop/2.4.0/logs/udfOutput
        at org.apache.pig.scripting.ScriptingOutputCapturer.getTaskLogDir(ScriptingOutputCapturer.java:103)
        at org.apache.pig.scripting.ScriptingOutputCapturer.getStandardOutputRootWriteLocation(ScriptingOutputCapturer.java:85)
        at org.apache.pig.impl.builtin.StreamingUDF.constructCommand(StreamingUDF.java:186)
        at org.apache.pig.impl.builtin.StreamingUDF.startUdfController(StreamingUDF.java:163)
        at org.apache.pig.impl.builtin.StreamingUDF.initialize(StreamingUDF.java:156)
        at org.apache.pig.impl.builtin.StreamingUDF.exec(StreamingUDF.java:146)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:344)
{code}
The problem is that users don't always have write permissions to hadoop.log.dir {{/apps/hadoop/2.4.0/logs}}. Shouldn't we write to the temp dir instead?","02/Jun/14 20:17;jeremykarn;Thanks [~lbendig]!  

[~cheolsoo], we originally chose the log directory because we wanted users to be able to view their udf output from the Task Tracker in the same place as the rest of the logs for that task.  Maybe it makes sense to fall back to temp dir if the user doesn't have permission to write to the log dir?  ","02/Jun/14 21:12;cheolsoo;[~jeremykarn], that sounds reasonable to me. Thank you for the clarification.","03/Jun/14 00:33;lbendig;[~cheolsoo], [~jeremykarn] thanks for looking at this issue. I updated the patch so that hadoop.tmp.dir will be used if hadoop.log.dir is not writable.","22/Jun/14 22:30;daijy;+1, PIG-3478-3.patch works for me. [~lbendig], would you like to commit it?","22/Jun/14 23:19;lbendig;[~daijy], thanks for checking it. Sure I will, but currently I'm experiencing permission issues and can't commit. ","22/Jun/14 23:36;daijy;Ok, let me commit it to trunk. Thanks Lorand!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skewed join can cause unrecoverable NullPointerException when one of its inputs is missing.,PIG-3469,12669315,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jarcec,xton,xton,18/Sep/13 19:26,07/Jul/14 18:07,14/Mar/19 03:07,03/Oct/13 21:35,0.11,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"Run this script in the local execution environment (affects cluster mode too):
{noformat}
%declare DATA_EXISTS /tmp/test_data_exists.tsv
%declare DATA_MISSING /tmp/test_data_missing.tsv
%declare DUMMY `bash -c '(for (( i=0; \$i < 10; i++ )); do echo \$i; done) > /tmp/test_data_exists.tsv; true'`

exists = LOAD '$DATA_EXISTS' AS (a:long);
missing = LOAD '$DATA_MISSING' AS (a:long);

missing = FOREACH ( GROUP missing BY a ) GENERATE $0 AS a, COUNT_STAR($1);

joined = JOIN exists BY a, missing BY a USING 'skewed';

STORE joined INTO '/tmp/test_out.tsv';
{noformat}

Results in NullPointerException which halts entire pig execution, including unrelated jobs. Expected: only dependencies of the error'd LOAD statement should fail. 

Error:
{noformat}
2013-09-18 11:42:31,518 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2017: Internal error creating job configuration.
2013-09-18 11:42:31,518 [main] ERROR org.apache.pig.tools.grunt.Grunt - org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobCreationException: ERROR 2017: Internal error creating job configuration.
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJob(JobControlCompiler.java:848)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.compile(JobControlCompiler.java:294)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:177)
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1266)
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1251)
	at org.apache.pig.PigServer.execute(PigServer.java:1241)
	at org.apache.pig.PigServer.executeBatch(PigServer.java:335)
	at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:137)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:198)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:170)
	at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:84)
	at org.apache.pig.Main.run(Main.java:604)
	at org.apache.pig.Main.main(Main.java:157)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:208)
Caused by: java.lang.NullPointerException
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.adjustNumReducers(JobControlCompiler.java:868)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJob(JobControlCompiler.java:480)
	... 17 more
{noformat}

Script above is as small as I can make it while still reproducing the issue. Removing the group-foreach causes the join to fail harmlessly (not stopping pig execution), as does using the default join. Did not occur on 0.10.1.","Apache Pig version 0.11.0-cdh4.4.0
Happens in both local execution environment (os x) and cluster environment (linux)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Oct/13 19:08;jarcec;PIG-3469.patch;https://issues.apache.org/jira/secure/attachment/12606164/PIG-3469.patch,01/Oct/13 19:05;jarcec;PIG-3469.patch;https://issues.apache.org/jira/secure/attachment/12606162/PIG-3469.patch,26/Sep/13 15:02;jarcec;PIG-3469.patch;https://issues.apache.org/jira/secure/attachment/12605262/PIG-3469.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-09-25 23:28:33.345,,,no_permission,,,,,,,,,,,,349247,,,,Thu Oct 03 21:35:44 UTC 2013,,,,,,,0|i1o7xr:,349545,,,,,,,,,,18/Sep/13 20:13;xton;...and still occurs with Pig 0.11.1 downloaded just now.,"25/Sep/13 23:28;jarcec;I believe that I do have understanding of this issue, will upload patch after running all tests.","26/Sep/13 15:02;jarcec;The NPE is thrown in {{JobControlCompiler.adjustNumReducers}} when getting successor nodes from plan. This method is called from {{JobControlCompiler.getJob()}} that is called from {{MapReduceLauncher.launchPig()}} through {{JobControlCompiler.compile()}}. There is a while loop in {{MapReduceLauncher.launchPig()}}, that is calling the {{compile()}} on every iteration. The loop itself is however changing the plan, by for example removing failed failed jobs with all their dependencies. As a result the call {{getSuccessors()}} can return {{NULL}} in case of a job failure as the nodes has been removed from the plan.

I'm attaching patch that will verify that the {{getSuccessors()}} is indeed not {{NULL}} prior using it. I've run all the tests and they seem to be passing.","26/Sep/13 15:04;jarcec;[~daijy], the patch is quite simple, so I'm wondering if there is any chance to get it into 0.12?",01/Oct/13 19:05;jarcec;Attached patch with test case to ensure that PigServer won't die on the incorrect input.,02/Oct/13 22:43;xuefuz;+1 Patch looks good. Will commit after running tests.,"03/Oct/13 21:35;xuefuz;Patch committed to trunk. Thanks, Jarces!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PIG-3123 breaks e2e test Jython_Diagnostics_2,PIG-3468,12669293,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,18/Sep/13 17:22,14/Oct/13 16:45,14/Mar/19 03:07,24/Sep/13 23:49,,,,,,,,0.12.0,,,,impl,,,0,,,,,,,,,,,,,"PIG-3123 optimized TypeCastInserter by adding a castInserted flag for LOLoad which do not need a LOForEach just to do the pruning. However, this flag is also used in illustrate to visualize the output from the loader (DisplayExamples:110). That's why Jython_Diagnostics_2 is broken.",,,,,,,,,,,,,,,PIG-3428,,,,,,,,,,,,,,,,,18/Sep/13 18:36;daijy;PIG-3468-1.patch;https://issues.apache.org/jira/secure/attachment/12603880/PIG-3468-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-18 18:41:04.064,,,no_permission,,,,,,,,,,,,349225,Reviewed,,,Tue Sep 24 23:49:03 UTC 2013,,,,,,,0|i1o7sv:,349523,,,,,,,,,,18/Sep/13 18:36;daijy;The attached patch distinguish scenario where LOForEach is inserted or ommitted.,18/Sep/13 18:41;cheolsoo;I opened a jira a while ago for e2e test failures. I will close it out as duplicate.,24/Sep/13 23:32;alangates;+1,24/Sep/13 23:49;daijy;Patch committed to both 0.12 branch and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race Conditions in InternalDistinctBag during proactive spill,PIG-3466,12669257,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,18/Sep/13 14:39,14/Oct/13 16:46,14/Mar/19 03:07,18/Sep/13 15:43,0.11.1,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"I have several jobs that use the following pattern:
{code}
b = group a by x;
c = foreach b {
            dist_y = DISTINCT a.y;
            generate
            group,
            COUNT(dist_y) as y_cnt;

};
{code}
These job fail intermittently during  proactive spill when the data set is large:
{code}
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
        at java.util.HashMap$KeyIterator.next(HashMap.java:828)
        at java.util.AbstractCollection.toArray(AbstractCollection.java:171)
        at org.apache.pig.data.SortedSpillBag.proactive_spill(SortedSpillBag.java:77)
        at org.apache.pig.data.InternalDistinctBag.spill(InternalDistinctBag.java:464)
        at org.apache.pig.impl.util.SpillableMemoryManager.handleNotification(SpillableMemoryManager.java:274)
        at sun.management.NotificationEmitterSupport.sendNotification(NotificationEmitterSupport.java:138)
        at sun.management.MemoryImpl.createNotification(MemoryImpl.java:171)
        at sun.management.MemoryPoolImpl$PoolSensor.triggerAction(MemoryPoolImpl.java:272)
        at sun.management.Sensor.trigger(Sensor.java:120)
{code}
PIG-3212 fixed the same issue for *InternalSortedBag* by synchronizing accesses to the content of bag. But *InternalDistinctBag* wasn't fixed, so the issue remains for nested DISTINCT.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Sep/13 14:58;cheolsoo;PIG-3466-1.patch;https://issues.apache.org/jira/secure/attachment/12603840/PIG-3466-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-18 15:14:54.384,,,no_permission,,,,,,,,,,,,349189,,,,Wed Sep 18 15:43:49 UTC 2013,,,,,,,0|i1o7kv:,349487,,,,,,,,,,"18/Sep/13 14:58;cheolsoo;Attached is a patch that applies the same fix to InternalDistinctBag as what PIG-3212 did to InternalSortedBag.

All unit tests pass.","18/Sep/13 15:10;cheolsoo;The patch includes several whitespace changes. I uploaded it to the RB for your convenience:
https://reviews.apache.org/r/14206/",18/Sep/13 15:14;rohini;+1,18/Sep/13 15:43;cheolsoo;Thank you Ronihi for the review. Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mark ExecType and ExecutionEngine interfaces as evolving,PIG-3464,12669166,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,18/Sep/13 01:27,07/Jul/14 18:07,14/Mar/19 03:07,18/Sep/13 15:40,0.12.0,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Sep/13 01:32;cheolsoo;PIG-3464-1.patch;https://issues.apache.org/jira/secure/attachment/12603740/PIG-3464-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-18 15:07:28.772,,,no_permission,,,,,,,,,,,,349098,,,,Tue Oct 01 04:31:26 UTC 2013,,,,,,,0|i1o70n:,349396,,,,,,,,,,18/Sep/13 01:32;cheolsoo;The attached adds the evolving annotation to the new interfaces: ExecType and ExecutionEngine.,18/Sep/13 15:07;rohini;+1,18/Sep/13 15:40;cheolsoo;Thank you Ronihi for the review!,01/Oct/13 04:31;cheolsoo;Reverted in 0.12.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rewrite PartitionFilterOptimizer to make it work for all the cases,PIG-3461,12668882,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,aniket486,aniket486,16/Sep/13 20:38,21/May/14 03:16,14/Mar/19 03:07,24/Sep/13 19:05,0.11.1,,,,,,,0.12.0,,,,impl,,,0,,,,,,,,,,,,,Current algorithm for Partition Filter pushdown identification fails in several corner cases. We need to rewrite its logic so that it works in all cases and does the maximum possible filter pushdown.,,,,,,,,,,,,,,,,,,,PIG-3300,,,,,,PIG-3510,,,,,,,17/Sep/13 20:51;aniket486;PIG-3461-2.patch;https://issues.apache.org/jira/secure/attachment/12603664/PIG-3461-2.patch,18/Sep/13 05:54;aniket486;PIG-3461-4.patch;https://issues.apache.org/jira/secure/attachment/12603759/PIG-3461-4.patch,23/Sep/13 20:58;aniket486;PIG-3461-6.patch;https://issues.apache.org/jira/secure/attachment/12604667/PIG-3461-6.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-09-22 22:42:27.109,,,no_permission,,,,,,,,,,,,348814,,,,Thu Oct 10 23:48:48 UTC 2013,,,,,,,0|i1o59r:,349112,,,,,,,,,,17/Sep/13 21:13;aniket486;Some extra code (EvalFunc) got added to the patch mistakenly. I will resubmit a refactored patch soon. Canceling the patch in meantime.,18/Sep/13 05:55;aniket486;RB: https://reviews.apache.org/r/14196/,"22/Sep/13 22:42;cheolsoo;(Apache RB is not responsive now, so I am leaving comments here.)

Excellent work, [~aniket486]! Your code is a lot more readable and maintainable! I just have few minor comments as follows:
# Can you make the old visitor default and the new one optional in 0.12? I think that's safer given we haven't tested the new code enough. Do you agree?
# Can you annotate the old visitor class (and its unit test) as deprecated so that we can delete them perhaps in 0.13?
# Can you make FilterExtractor extend PlanVisitor? It doesn't seem necessary, but I find it helpful to view it in the type hierarchy of PlanVisitor in IDE.
# Please remove trailing whites paces.
# Lastly, I have a question about backward incompatibility.
{code}
* Flag to use old PartitionFilterOptimizer in case NewPartitionFilterOptimizer is not backwards compatible
* (A known case is ""filter a by 1 == 0"").
{code}
As far as I understand, your visitor is simply superior to the old one. (i.e. more filter expressions can be pushed down.) Is there any filter expression that used to be pushed down is no longer pushed down? The ""1 == 0"" case in your comment is pushed down now while it wasn't before, so I see it more as an improvement than a backward incompatibility. I am trying to understand whether there are real backward incompatibilities before I deploy your code in production.

Thank you very much!",22/Sep/13 22:49;cheolsoo;Forgot to mention that your patch no longer applies nicely because I committed PIG-3471. Please rebase it to trunk. (I am sorry :-)),"23/Sep/13 05:44;aniket486;bq. Can you make the old visitor default and the new one optional in 0.12? I think that's safer given we haven't tested the new code enough. Do you agree?
I prefer having the new optimizer as default. That way we can make sure everyone migrates to the new code instead hiding that against a flag. Reverting to old behavior should only happen if there bugs/corner cases.
bq. Can you annotate the old visitor class (and its unit test) as deprecated so that we can delete them perhaps in 0.13?
Definitely.
bq. Can you make FilterExtractor extend PlanVisitor? It doesn't seem necessary, but I find it helpful to view it in the type hierarchy of PlanVisitor in IDE.
FilterExtractor is not a PlanVisitor actually. It can have applications even outside of the optimizer framework and hence I do not want to tie it to visitor framework. The code needs to be strictly recursive (aka depth first) instead of the visitor pattern. PColFilterExtractor also extends PlanVisitor for namesake and does not use any of its features.
bq. Please remove trailing whites paces.
Will do.
bq. Is there any filter expression that used to be pushed down is no longer pushed down.
That should not be the case.
bq. The ""1 == 0"" case in your comment is pushed down now while it wasn't before, so I see it more as an improvement than a backward incompatibility.
True. I agree with you. However, I have seen people use filter a by 1==0 as a no-op load-and-drop data operation. That won't work anymore with Loader's that implement LoadMetadata. There are no known incompatibilities with this fix. But, since more stuff is getting pushed to the loaders, you might need to make sure your loaders have the correct logic for all cases (eg- 1==0, long expressions etc.)

",23/Sep/13 05:45;aniket486;I will rebase and submit a new patch (with CR changes) shortly.,"23/Sep/13 06:07;cheolsoo;{quote}
I prefer having the new optimizer as default. That way we can make sure everyone migrates to the new code instead hiding that against a flag. Reverting to old behavior should only happen if there bugs/corner cases.
{quote}
Sure. This doesn't make any difference to me because I am going to explicitly enable the new optimizer in the pig.properties file regardlessly. The reason why I asked is I thought it would be nicer if early adopters (Twitter, Yahoo, Netflix, etc) could stabilize it first before opening it to the community. Some conner cases/bugs might surprise users who do not follow JIRAs. Does this make sense?

","23/Sep/13 06:19;aniket486;That makes sense. However, wider adoption would help us stabilize this code path faster. I see more risk with the Loaders that implement LoadPushDown interface compared to having corner cases/bugs in this code itself. So, its better to get more coverage sooner.","23/Sep/13 06:24;cheolsoo;Alright, sir. I do not object.",23/Sep/13 21:00;aniket486;Thanks [~cheolsoo]! I have uploaded the new patch and updated RB accordingly.,"24/Sep/13 04:46;cheolsoo;+1. LGTM!

When you commit, can you please add the new property to ""conf/pig.properties"" with some comment? For eg,
{code}
# Set this option to true if you need to use the old partition filter optimizer. But note it will be deprecated in the future.
# pig.exec.useOldPartitionFilterOptimize=true
{code}
I think we should document it somewhere even though we'll remove it in the next release.",24/Sep/13 04:58;aniket486;That makes sense. I will add the documentation to pig.properties and I will commit to trunk + pig-0.12 branch.,24/Sep/13 19:05;aniket486;Committed to trunk and 0.12 branch. Thanks again [~cheolsoo] for reviewing!,"10/Oct/13 01:17;cheolsoo;[~aniket486], can you take a look at PIG-3510? I discovered it after deploying this new partition filter optimizer to production today. It seems that optimization rules are applied in a different order than before. Particularly in my case, MergeFilter used to come *before* PartionFilterPushDown, but now it comes *after* NewPartitionFilterPushDown. I didn't expect this side effect, but it seems like a serious regression.","10/Oct/13 04:31;aniket486;Yes. Good catch. I moved the optimizer before FilterLogicExpressionSimplifier to leverage its features. Didn't realize that it would regress in this scenario. We need to move it down where PartitionFilterOptimizer is applied. Attaching a patch right now. [~daijy], how should we apply it to 0.12 branch?",10/Oct/13 19:43;daijy;Please commit to 0.12 branch. Seems some filter condition cannot push to the loader. Do we think this serious enough to reroll the 0.12.0 RC?,"10/Oct/13 20:07;cheolsoo;I just committed PIG-3510 to 0.12 branch. 

Regarding seriousness, this doesn't make jobs fail, but it definitely has production impact since it makes jobs run longer. I would vote for another RC, but if we can release 0.12.1 soon enough after 0.12.0 is out, that'll be fine too.","10/Oct/13 20:37;aniket486;I think its serious enough to reroll the rc. Otherwise, we'll have to document that anyone using 0.12.0 should use the pig.exec.useOldPartitionFilterOptimizer flag.","10/Oct/13 23:48;daijy;That should only affect user who write the filter condition separately in two filter statement. And there is no correctness issue and user has a clear workaround. So I don't feel it is that critical. Plus, I don't think 0.12.1 will take too long, since we need to include PIG-3492, PIG-3480 soon. I would say let's put it into 0.12.1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ScalarExpression lost with multiquery optimization,PIG-3458,12668373,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,12/Sep/13 22:27,14/Oct/13 16:46,14/Mar/19 03:07,25/Sep/13 18:27,,,,,,,,0.12.0,,,,parser,,,0,,,,,,,,,,,,,"Our user reported an issue where their scalar results goes missing when having two store statements.

{noformat}
A = load 'test1.txt' using PigStorage('\t') as (a:chararray, count:long);
B = group A all;
C = foreach B generate SUM(A.count) as total ;
store C into 'deleteme6_C' using PigStorage(',');

Z = load 'test2.txt' using PigStorage('\t') as (a:chararray, id:chararray );
Y = group Z by id;
X = foreach Y generate group, C.total;
store X into 'deleteme6_X' using PigStorage(',');

====Inputs
 pig> cat test1.txt
a       1
b       2
c       8
d       9
 pig> cat test2.txt
a       z
b       y
c       x
 pig>
{noformat}

Result X should contain the total count of '20' but instead it's empty.

{noformat}
 pig> cat deleteme6_C/part-r-00000
20
 pig> cat deleteme6_X/part-r-00000
x,
y,
z,
 pig>
{noformat}

This works if we take out first ""store C"" statement.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Sep/13 20:57;knoguchi;pig-3458-v01.patch;https://issues.apache.org/jira/secure/attachment/12603433/pig-3458-v01.patch,23/Sep/13 18:49;knoguchi;pig-3458-v02.patch;https://issues.apache.org/jira/secure/attachment/12604649/pig-3458-v02.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-09-13 18:46:20.053,,,no_permission,,,,,,,,,,,,348307,Reviewed,,,Wed Sep 25 18:27:50 UTC 2013,,,,,,,0|i1o24v:,348603,,,,,,,,,,"12/Sep/13 22:35;knoguchi;Reason it gets lost is, we store C using PigStorage but ReadScalars tries to read it by a hardcoded InterStorage. 

{noformat}
...
[First mapreduce job]
Reduce Plan
C: Store(/.../deleteme6_C:PigStorage(',')) - scope-17
|
...

[Second mapreduce job]
    |   POUserFunc(org.apache.pig.impl.builtin.ReadScalars)[long] - scope-31
    |   |
    |   |---Constant(0) - scope-29
    |   |
    |   |---Constant(/.../deleteme6_C) - scope-30
{noformat}

Trying to understand what the fix should be.
1. Make ReadScalars use the corresponding Loader.
2. Split relation 'C' so that we store them in both PigStorage AND InterStorage.

I'm guessing latter, but appreciate your feedback.
",13/Sep/13 18:46;mwagner;I'd be in favor of 2 as well. There's no guarantee that the StoreFunc will also be a LoadFunc and the scalar will be a small file so writing an extra copy isn't costly.,16/Sep/13 20:21;daijy;#2 has a simpler implementation as well. Just add one more check in ScalarVisitor.,"16/Sep/13 20:30;knoguchi;bq. #2 has a simpler implementation as well. Just add one more check in ScalarVisitor.

Yes, uploading a patch shortly.  Thanks Mark, Daniel.","16/Sep/13 20:57;knoguchi;Uploading a patch that would make sure that ScalarVisitor is using the storefunc that is 
(a) tmpStore 
and
(b) using InterStorage

Not sure if both checks are needed.

Test case to follow.",23/Sep/13 18:49;knoguchi;Sorry for the delay.  Adding a test case.,24/Sep/13 21:45;daijy;+1. Please commit to trunk and 0.12 branch.,"25/Sep/13 18:27;knoguchi;Patch committed to branch-0.12 and trunk.  
Thanks Mark and Daniel for your feedback!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig 0.11.1 OutOfMemory error,PIG-3455,12667630,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,rohini,shubhamc,shubhamc,09/Sep/13 18:01,08/Nov/14 07:00,14/Mar/19 03:07,18/Sep/13 15:55,0.11.1,,,,,,,0.11.2,0.12.0,,,,,,0,,,,,,,,,,,,,"When running Pig on a relatively large script (around 1.5k lines, 85 assignments), Pig fails with the following error even before any jobs are fired:
Pig Stack Trace
---------------
ERROR 2998: Unhandled internal error. Java heap space

java.lang.OutOfMemoryError: Java heap space
        at java.util.Arrays.copyOf(Arrays.java:2882)
        at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:100)
        at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:390)
        at java.lang.StringBuilder.append(StringBuilder.java:119)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirstLP(LogicalPlanPrinter.java:83)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.visit(LogicalPlanPrinter.java:69)
        at org.apache.pig.newplan.logical.relational.LogicalPlan.getSignature(LogicalPlan.java:122)
        at org.apache.pig.PigServer.execute(PigServer.java:1237)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:333)
        at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:137)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:198)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:170)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:84)
        at org.apache.pig.Main.run(Main.java:604)
        at org.apache.pig.Main.main(Main.java:157)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:160)

The same script works fine with Pig-0.10.1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Sep/13 00:32;rohini;PIG-3455-1.patch;https://issues.apache.org/jira/secure/attachment/12603724/PIG-3455-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-10 17:12:38.432,,,no_permission,,,,,,,,,,,,347567,,,,Sat Nov 08 07:00:32 UTC 2014,,,,,,,0|i1nxlj:,347866,,,,,,,,,,10/Sep/13 17:12;rohini;We can probably improve the logic to getSignature to do create a signature using MessageDigest and DigestOutputStream instead of constructing a huge String and getting hashcode. Thoughts?,10/Sep/13 17:36;rohini;Basically a MessageDigest implementation that mimics String.hashCode() behavior so it still matches older signatures and previously collected stats don't go waste.,"10/Sep/13 17:42;rohini;[~billgraham],
   Do you want backward compatibility so that your previous stats don't get wasted or if there is a better and faster way can the new signatures be different? There are some streaming hash functions in guava also which can be tried.","11/Sep/13 03:21;billgraham;Thanks [~rohini.u] for kicking this off. Yes, a streaming based hash function would be a much better approach. No need for backward compatibility. The signature contract is that it could change between Pig releases.","18/Sep/13 00:54;billgraham;+1, much better.",18/Sep/13 15:55;rohini;Committed to 0.11.2 and trunk (0.12). Thanks Bill.,"11/Nov/13 21:54;aniket486;We still see exceptions with this in pig-0.12

{noformat}
java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:2882)
	at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:100)
	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:390)
	at java.lang.StringBuilder.append(StringBuilder.java:119)
	at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.shiftStringByTabs(LogicalPlanPrinter.java:223)
	at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:108)
	at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:102)
	at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:102)
	at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirstLP(LogicalPlanPrinter.java:83)
	at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.visit(LogicalPlanPrinter.java:69)
	at org.apache.pig.newplan.logical.relational.LogicalPlan.getSignature(LogicalPlan.java:135)
	at org.apache.pig.PigServer.execute(PigServer.java:1297)
	at org.apache.pig.PigServer.executeBatch(PigServer.java:377)
	at org.apache.pig.PigServer.executeBatch(PigServer.java:355)
	at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:140)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:202)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:173)
	at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:84)
	at org.apache.pig.Main.run(Main.java:607)
	at org.apache.pig.Main.main(Main.java:156)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:197)
{noformat}","11/Nov/13 22:00;rohini;[~aniket486],
    Do you know what is taking space from the heap dump?","11/Nov/13 23:08;aniket486;It's the logical plan's string representation.
Actually, using a streaming based hash function doesn't help as the LogicalPlanPrinter's visit method gets the string representation first before it writes to the stream. We need to change that in order to fix this problem.

{code}
public void visit() throws FrontendException {
        try {
            if (plan instanceof LogicalPlan) {
                mStream.write(depthFirstLP().getBytes());
            }
            else {
                mStream.write(reverseDepthFirstLP().getBytes());
            }
        } catch (IOException e) {
            throw new FrontendException(e);
        }
    }
{code}",12/Nov/13 01:33;aniket486;Opened: https://issues.apache.org/jira/browse/PIG-3567,12/Nov/13 18:21;rohini;My bad. Thanks Aniket for catching that. PIG-3455 is still required though. ,"08/Nov/14 07:00;skanda83;I'm now getting the following exception after applying the fix:

Pig Stack Trace
---------------
ERROR 2998: Unhandled internal error. Java heap space

java.lang.OutOfMemoryError: Java heap space
        at java.util.Arrays.copyOf(Arrays.java:2271)
        at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113)
        at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
        at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:140)
        at java.io.PrintStream.write(PrintStream.java:480)
        at java.io.FilterOutputStream.write(FilterOutputStream.java:97)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.writeWithTabs(LogicalPlanPrinter.java:103)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:125)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
rg.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.writeWithTabs
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirst(LogicalPlanPrinter.java:133)
        at org.apache.pig.newplan.logical.optimizer.LogicalPlanPrinter.depthFirstLP(LogicalPlanPrinter.java:97)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EvalFunc<T> ctor reflection to determine value of type param T is brittle,PIG-3451,12667156,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,hazen,hazen,hazen,05/Sep/13 18:33,07/Jul/14 18:08,14/Mar/19 03:07,18/Oct/13 05:02,0.11.1,,,,,,,0.13.0,,,,impl,,,0,,,,,,,,,,,,,"The {{EvalFunc<T>}} base class has logic in its default ctor to attempt to determine the runtime type of its type parameter {{T}}. This logic is brittle when the derived class has type parameters of its own. For instance:

{code}
public static abstract EvalFunc1<T> extends EvalFunc<T> {}
public static abstract EvalFunc2<X, T> extends EvalFunc1<T> {}
public static EvalFunc3<X> extends EvalFunc1<X, DataBag> { ... }
{code}

Here, {{EvalFunc3<X>}} does specify concrete type {{DataBag}} for {{T}} of {{EvalFunc<T>}}, but the existing logic in the default ctor fails to identify it.

Here's a unit test which reproduces this failure:

https://github.com/sagemintblue/pig/compare/apache:trunk...hazen/repro_eval_func_reflection_bug

Here's the test with an update to {{EvalFunc}}'s logic which fixes the issue:

https://github.com/sagemintblue/pig/compare/apache:trunk...hazen/fix_eval_func_reflection",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Sep/13 20:07;aniket486;PIG-3451-3.patch;https://issues.apache.org/jira/secure/attachment/12601667/PIG-3451-3.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-05 20:07:26.087,,,no_permission,,,,,,,,,,,,347093,,,,Fri Oct 11 18:33:35 UTC 2013,,,,,,,0|i1nuof:,347392,,,,,,,,,,05/Sep/13 20:07;aniket486;ant test-commit pass.,"10/Sep/13 00:42;hazen;[~aniket486], did this hit trunk? Thanks for shepherding!","10/Sep/13 01:22;aniket486;[~hazen], there were a few failures in full test suite (ant test). I'm still investigating those.","23/Sep/13 01:25;hazen;Aniket, I'd be curious to see which failures you ran into. If you have time, could you paste here the stack traces of the failed tests? Thanks!","11/Oct/13 18:00;aniket486;+1
I found out that the test failures are not related to this change. 
I will fix some formatting changes and commit this patch to trunk.",11/Oct/13 18:33;aniket486;committed to trunk. Thanks [~hazen] for your contribution!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compiler warning message dropped for CastLineageSetter and others with no enum kind,PIG-3447,12666723,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,knoguchi,knoguchi,knoguchi,03/Sep/13 18:14,07/Jul/14 18:08,14/Mar/19 03:07,08/Feb/14 04:39,,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"Following compiler warning was never shown to users for two reasons.
{noformat}
//./src/org/apache/pig/newplan/logical/visitor/CastLineageSetter.java
  106  if(inLoadFunc == null){
  107      String msg = ""Cannot resolve load function to use for casting from "" +
  108                  DataType.findTypeName(inType) + "" to "" +
  109                  DataType.findTypeName(outType) + "". "";
  110      msgCollector.collect(msg, MessageType.Warning);
  111  }
{noformat}

# CompilationMessageCollector.logMessages or logAllMessages not being called after CastLineageSetter.visit.
# CompilationMessageCollector.collect with no KIND don't print out any messages when aggregate.warning=true (default)

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Sep/13 18:26;knoguchi;pig-3447-v01.txt;https://issues.apache.org/jira/secure/attachment/12601197/pig-3447-v01.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-19 01:42:48.933,,,no_permission,,,,,,,,,,,,346661,,,,Sat Feb 08 04:39:50 UTC 2014,,,,,,,0|i1ns0v:,346962,,,,,,,,,,"03/Sep/13 18:26;knoguchi;With the patch, it'll print out 

{noformat}
2013-09-03 13:58:20,625 [main] WARN  org.apache.pig.PigServer - Encountered Warning NO_LOAD_FUNCTION_FOR_CASTING_BYTEARRAY 1 time(s).
{noformat}


If anyone still calls CompilationMessageCollector.collect without enum KIND, then it'll at least print out 

{noformat}
2013-08-30 22:25:20,940 [main] WARN  org.apache.pig.PigServer - Encountered Warning Aggregated unknown kind messages.  Please set -Daggregate.warning=false to retrieve these messages 1 time(s).
{noformat}

Before, it wasn't printing out anything.

With -Daggregate.warning=false, it'll print out the following (even without this patch).

{noformat}
2013-09-03 14:24:48,275 [main] WARN  org.apache.pig.PigServer - Cannot resolve load function to use for casting from bytearray to chararray.
{noformat}",19/Jan/14 01:42;aniket486;+1,08/Feb/14 04:39;cheolsoo;Committed to trunk. Thank you Koji.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CONCAT with 2+ input parameters fail,PIG-3444,12666023,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,lbendig,daijy,daijy,28/Aug/13 22:46,07/Jul/14 18:08,14/Mar/19 03:07,25/Oct/13 05:16,,,,,,,,0.13.0,,,,impl,,,0,,,,,,,,,,,,,"This is a continuation work of PIG-1420 since PIG-1420 is closed. Currently CONCAT fails with more than 2 parameters. The error message is:
Could not infer the matching function for org.apache.pig.builtin.CONCAT as multiple or none of them fit",,,,,,,,,,,,,,,,,,,PIG-1577,,,,,,,,,,,PIG-1420,,24/Oct/13 15:14;lbendig;PIG-3444-2.patch;https://issues.apache.org/jira/secure/attachment/12610095/PIG-3444-2.patch,25/Oct/13 05:14;daijy;PIG-3444-3.patch;https://issues.apache.org/jira/secure/attachment/12610272/PIG-3444-3.patch,30/Sep/13 14:38;lbendig;PIG-3444.patch;https://issues.apache.org/jira/secure/attachment/12605901/PIG-3444.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-09-30 14:38:55.732,,,no_permission,,,,,,,,,,,,345962,Reviewed,,,Fri Oct 25 08:15:15 UTC 2013,,,,,,,0|i1nnqf:,346263,,,,,,,,,,"30/Sep/13 14:38;lbendig;An attempt to address this issue. 
In the EvalFunc::getArgToFuncMapping the last field of the schema can marked as a vararg field. (schema.setLastFieldSchemaVarArg(true)) .
The idea is that when the funcspec and input schema fields are compared for exact/best fit match, the last funcspec field is taken repeatedly (if it's of type vararg) when comparing with the next input schema field.
","23/Oct/13 22:33;daijy;Thanks Lorand, this is a long desired feature.

Wonder if we can put the flag in EvalFunc instead of Schema? Schema change will probably propagate to other part of Pig code, and seems nowhere else in Pig would need variable schema.","24/Oct/13 15:14;lbendig;Daniel, I modified the patch according to your comment.","24/Oct/13 18:51;daijy;Thanks, looks pretty neat now. Will commit once test pass. Also we need to update document.","24/Oct/13 19:04;daijy;One comment, why not use getSchemaType on StringConcat?","25/Oct/13 05:14;daijy;Attach another patch which fixed unit test failure TestBestFitCast.testByteArrayCast11, also add getSchemaType to StringConcat. ",25/Oct/13 05:16;daijy;Patch committed to trunk. Thanks Lorand!,25/Oct/13 08:15;lbendig;Thank you for fixing those issues in the meanwhile (that time zone difference...) .,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Custom Partitioner not working with MultiQueryOptimizer,PIG-3435,12664924,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,21/Aug/13 21:53,14/Oct/13 16:46,14/Mar/19 03:07,28/Aug/13 00:35,,,,,,,,0.11.2,0.12.0,,,impl,,,0,,,,,,,,,,,,,"When looking at PIG-3385, noticed some issues in handling of custom partitioner with multi-query optimization.

{noformat}
C1 = group B1 by col1 PARTITION BY
       org.apache.pig.test.utils.SimpleCustomPartitioner parallel 2;
C2 = group B2 by col1 PARTITION BY
       org.apache.pig.test.utils.SimpleCustomPartitioner parallel 2;
{noformat}
This seems to be merged to one mapreduce job correctly but custom partitioner information was lost.

{noformat}
C1 = group B1 by col1 PARTITION BY org.apache.pig.test.utils.SimpleCustomPartitioner parallel 2;
C2 = group B2 by col1 parallel 2;
{noformat}
This seems to be merged even though they should run on two different partitioner.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Aug/13 22:34;knoguchi;pig-3435-v01.patch;https://issues.apache.org/jira/secure/attachment/12599525/pig-3435-v01.patch,23/Aug/13 18:26;knoguchi;pig-3435-v02_skipcustompatitioner_for_merge.patch;https://issues.apache.org/jira/secure/attachment/12599679/pig-3435-v02_skipcustompatitioner_for_merge.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-08-28 00:31:30.893,,,no_permission,,,,,,,,,,,,344867,Reviewed,,,Wed Aug 28 20:41:46 UTC 2013,,,,,,,0|i1ngzj:,345168,,,,,,,,,,"22/Aug/13 22:34;knoguchi;Looking at the multi-query optimization code and documents.  I chickened out. 

Taking the same approach as PIG-1108 and simply skipping the MR jobs with custom partitioner.

Attaching the test case soon.","23/Aug/13 18:26;knoguchi;While looking at the testcase, found PIG-2627 where it fixed one of the issues with custom-partitioner and multiquery optimization (but not all).

Specific case mentioned on that ticket is handled on that jira and it works, but my patch here simply skips multiquery optimization for ALL custom partitioner jobs.

Since it's sort of a correctness issue, I want this fix to be back-ported to 0.11.  And for that, I kept the change to be simple.

Can we create a separate jira for reviving custom-partitioner + multiquery optimization for later releases?
","28/Aug/13 00:31;daijy;custom-partitioner + multiquery optimization should be a valid use case, we shall open a Jira ticket for it.

But correctness first, +1 for the patch.",28/Aug/13 00:35;daijy;Patch committed to trunk. Opened PIG-3440 to track the custom partitioner + multiquery optimization. Thanks Koji!,"28/Aug/13 01:32;knoguchi;Thanks Daniel!  
Can we back-port this patch to 0.11? 
(That was one of the motivation for me to keep the patch simple.)

I'll work on PIG-3440.
",28/Aug/13 20:41;daijy;Committed to 0.11 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null subexpression in bincond nullifies outer tuple (or bag),PIG-3434,12664831,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,mwagner,fedyakov,fedyakov,21/Aug/13 13:05,21/Nov/14 05:59,14/Mar/19 03:07,11/Oct/14 07:07,0.11,,,,,,,0.14.0,,,,,,,0,,,,,,,,,,,,,"According to docs, for bincond operator ""If a Boolean subexpression results in null value, the resulting expression is null"" (http://pig.apache.org/docs/r0.11.0/basic.html#nulls).

It works as described in plain foreach..generate expression:

{noformat}
in = load 'in';
out = FOREACH in GENERATE 1, ($0 > 0 ? 2 : 3);
dump out;
{noformat}

in (3 lines, 2nd is empty):
{noformat}
0

1
{noformat}

out:
{noformat}
(1,3)
(1,)
(1,2)
{noformat}

But if we wrap generated variables in tuple (or bag), we lose the whole 2nd line in output:

{noformat}
out = FOREACH in GENERATE (1, ($0 > 0 ? 2 : 3));
{noformat}

out:
{noformat}
((1,3))
()
((1,2))
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Aug/13 01:37;mwagner;PIG-3434.1.patch;https://issues.apache.org/jira/secure/attachment/12599560/PIG-3434.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-22 19:25:11.921,,,no_permission,,,,,,,,,,,,344774,Reviewed,,,Sat Oct 11 07:07:37 UTC 2014,,,,,,,0|i1ngen:,345074,,,,,,,,,,22/Aug/13 19:25;mwagner;I was able to reproduce this. It looks like POUserFunc isn't handling STATUS_NULL correctly. I'll make a patch.,"23/Aug/13 01:37;mwagner;Fixed null handling in POUserFunc. It seems like POStatus.STATUS_NULL isn't being set everywhere e.g. NULL constants have STATUS_OK. That's the work of another JIRA though, I think.",03/Sep/13 20:49;mwagner;Some of my other work sneaked in here. I'll upload a clean patch.,"03/Sep/13 20:54;mwagner;Nevermind, I was confused. This is ready to be reviewed.",30/Sep/13 20:46;daijy;Looks good. +1.,30/Sep/13 20:49;daijy;Patch committed to both 0.12 branch and trunk. Thanks Mark!,"04/Oct/13 21:36;daijy;Need to rollback this patch. The following script generate unexpected results:
in = load 'in';
out = FOREACH in GENERATE ROUND(($0 > 0 ? 2 : 3));
dump out;

out:
(3)
(())
(2)

The second line should be ().

The problem is:
1. Another fix in POUserFunc:
{code}
-            res.returnStatus = temp.returnStatus;
+            res.returnStatus = POStatus.STATUS_OK;
{code}
The status of the UDF input should be STATUS_OK instead of the status of the last input.

2. All UDF should be able to handle null input. Even with above mentioned change, the script file with NPE: FloatRound.java:47
FloatRound is not able to deal with null inputs. Adding null check for all UDF might be too much for 0.12.0. ",11/Oct/14 07:07;daijy;This issue is fixed as part of PIG-3568.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
typo in log message in SchemaTupleFrontend,PIG-3432,12664471,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,epishkin,epishkin,epishkin,19/Aug/13 22:55,14/Oct/13 16:45,14/Mar/19 03:07,20/Aug/13 19:23,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,https://github.com/apache/pig/pull/11.patch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Aug/13 17:37;epishkin;PIG-3432.patch;https://issues.apache.org/jira/secure/attachment/12598988/PIG-3432.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-20 16:44:18.708,,,no_permission,,,,,,,,,,,,344472,,,,Tue Aug 20 19:23:00 UTC 2013,,,Patch Available,,,,0|i1nejz:,344772,,,,,,,,,,"20/Aug/13 16:44;cheolsoo;[~epishkin], thank you for the contribution. Unfortunately, we cannot pull your request on github since it's just a read-only mirror. Do you mind uploading your patch to this jira?

Please see here:
https://cwiki.apache.org/confluence/display/PIG/HowToContribute


","20/Aug/13 17:41;epishkin;I've attached the patch to this ticket.

Just in case here is how the patch has been created:
{code}
git clone git@github.com:apache/pig.git
cd pig
git checkout trunk

# merge pull request
curl https://github.com/apache/pig/pull/11.patch | git am

#create patch file for apache (I wish it was easier for a simple typo fix)
git reset HEAD~
git diff --no-prefix > PIG-3432.patch
{code}",20/Aug/13 19:23;cheolsoo;Committed to trunk. Thank you Oleksii!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive jdo api jar referenced in pig script throws error ,PIG-3425,12663947,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,deepesh,deepesh,deepesh,15/Aug/13 23:08,14/Oct/13 16:46,14/Mar/19 03:07,16/Aug/13 19:00,0.11.1,,,,,,,0.12.0,,,,tools,,,0,,,,,,,,,,,,,"When pig CLI is run with -useHCatalog flag it complains about the following:
{code}
ls: cannot access /usr/lib/hive/lib/jdo2-api-*-ec.jar: No such file or directory
{code}
Basically it relies on a jdo-api library that used to be called jdo2-api-2.3-ec.jar which now in the latest hive is jdo-api-3.0.1.jar.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Aug/13 23:10;deepesh;PIG-3425.patch;https://issues.apache.org/jira/secure/attachment/12598318/PIG-3425.patch,16/Aug/13 03:50;deepesh;PIG-3425.patch.1;https://issues.apache.org/jira/secure/attachment/12598356/PIG-3425.patch.1,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-08-16 01:54:50.653,,,no_permission,,,,,,,,,,,,343948,,,,Sat Aug 17 05:43:05 UTC 2013,,,,,,,0|i1nbc7:,344250,,,,,,,,,,15/Aug/13 23:10;deepesh;Attached is a patch with the fix.,16/Aug/13 01:54;cheolsoo;+1.,"16/Aug/13 03:50;deepesh;Left the ""ec"" at the end of the filename. Wildcard'd that.",16/Aug/13 19:00;cheolsoo;Committed PIG-3425.patch.1 to trunk. Thank you Deepesh!,17/Aug/13 05:43;deepesh;Thanks for the review and commit!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Package import list should consider class name as is first even if -Dudf.import.list is passed,PIG-3424,12663860,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,15/Aug/13 14:56,14/Oct/13 16:46,14/Mar/19 03:07,19/Aug/13 00:56,0.11.1,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"Currently if -Dudf.import.list is passed it adds them to the beginning of the list and """" (class name as is) which is defined by default always is pushed to end of list. In cases where the pig deployment itself contains predefined -Dudf.import.list, class resolution tries all of that before trying the fully qualified class name defined in LOAD as is. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Aug/13 15:07;rohini;PIG-3424-1.patch;https://issues.apache.org/jira/secure/attachment/12598220/PIG-3424-1.patch,22/Aug/13 15:38;rohini;PIG-3424-fixtest.patch;https://issues.apache.org/jira/secure/attachment/12599446/PIG-3424-fixtest.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-08-16 01:51:11.981,,,no_permission,,,,,,,,,,,,343861,,,,Thu Aug 22 15:42:29 UTC 2013,,,,,,,0|i1nasv:,344163,,,,,,,,,,16/Aug/13 01:51;cheolsoo;+1.,19/Aug/13 00:56;rohini;Committed to trunk (0.12). Thanks Cheolsoo.,"22/Aug/13 15:38;rohini;When running the full test suite, encountered a test failure. Attaching patch that fixes the testcase. ",22/Aug/13 15:42;cheolsoo;Thanks for fixing it. +1.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Script jars should be added to extra jars instead of pig's job.jar,PIG-3421,12663511,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,aniket486,aniket486,13/Aug/13 21:03,21/Nov/14 05:58,14/Mar/19 03:07,10/Oct/14 20:27,0.11.1,,,,,,,0.14.0,,,,,,,0,,,,,,,,,,,,,"Currently, for all the script engines, pig adds script jars to pig's job jar even without consulting the skipJars list. Ideally, we should add these to extraJars so that they can benefit from distributed cache.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-10-10 20:27:42.898,,,no_permission,,,,,,,,,,,,343512,,,,Fri Oct 10 20:27:42 UTC 2014,,,,,,,0|i1n8nr:,343816,,,,,,,,,,10/Oct/14 20:27;daijy;This issue is solved as part of PIG-4054.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to retrieve map values from data loaded by AvroStorage,PIG-3420,12663320,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yuanlid,explosion7,explosion7,12/Aug/13 23:37,14/Oct/13 16:46,14/Mar/19 03:07,15/Aug/13 22:51,0.12.0,,,,,,,0.12.0,,,,impl,,,0,,,,,,,,,,,,,"Running the following script:
a = load './newavro/data/avro/EmployeeMapF.ser' USING AvroStorage();
dump a;
c = foreach a generate name, office, 'Toyota', cars#'Toyota' as toyota, 'Mazda', cars#'Mazda', 'Nissan', cars#'Nissan' as nissan;
Although object a has all the data loaded, c cannot retrieve the map values, column 4,6,8 are empty in the result.
The map keys is of class Utf8, but the keys used to retrieve data is String, that is the reason why we cannot retrieve the values. The patch fix this problem. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Aug/13 22:05;explosion7;PIG-3420_08152013.patch;https://issues.apache.org/jira/secure/attachment/12598307/PIG-3420_08152013.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-13 21:29:13.09,,,no_permission,,,,,,,,,,,,343321,,,,Thu Aug 15 22:51:35 UTC 2013,,,Patch Available,,,,0|i1n7hb:,343625,,,,,,,,,,12/Aug/13 23:42;explosion7;In patch08092013.patch,"13/Aug/13 21:29;rohini;Few comments on the patch:

1) You can skip the type cast in 
v = innerMap.get((String) key);
2) Keep the braces for if else block.
{code}
+    if (v instanceof Utf8)
       return v.toString();
-    } else {
+    else
       return v;
-    }
{code}
3) Can't we fix newSchemaFromRequiredFieldList to handle maps and bgas instead of catching AvroRunTimeException ?","13/Aug/13 21:46;explosion7;For 1),2) refleted in the new patch.
For 3), you can look at the code in class Schema in avro-1.7.4, the MapSchema type do not have an implementation of getField method, only RecordSchema has this function overridden. Then what we need is judging whether the oldSchema is a instance of RecordSchema, if it's not, we need to directly return it. Unfortunately, the class is a inner class in Schema class, and it is private, so we simply cannot achieve this.",13/Aug/13 22:51;rohini;And the patch is missing unit test. Can you please add one?,13/Aug/13 23:00;explosion7;I'll do that later,15/Aug/13 22:05;explosion7;unit test added in the latest patch,15/Aug/13 22:51;rohini;+1. Committed to trunk (0.12). Thanks Yuanli. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job fails when skewed join is done on tuple key,PIG-3417,12663119,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,nkollar,njw45,njw45,12/Aug/13 00:41,21/Jun/17 09:15,14/Mar/19 03:07,19/Dec/16 20:18,0.11.1,,,,,,,0.16.1,0.17.0,,,impl,,,0,,,,,,,,,,,,,"I've attached a test case that fails, but should pass. The test case groups two relations separately, then full-outer joins them on the grouped columns. The test case passes if ""using 'skewed'"" is removed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Dec/16 20:17;rohini;PIG-3417-3.patch;https://issues.apache.org/jira/secure/attachment/12843943/PIG-3417-3.patch,12/Dec/16 10:47;nkollar;PIG-3417.patch;https://issues.apache.org/jira/secure/attachment/12842779/PIG-3417.patch,18/Dec/16 21:22;nkollar;PIG-3417_2.patch;https://issues.apache.org/jira/secure/attachment/12843765/PIG-3417_2.patch,12/Aug/13 00:43;njw45;TestSkewJoinWithTuples.java;https://issues.apache.org/jira/secure/attachment/12597391/TestSkewJoinWithTuples.java,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2016-12-12 10:46:52.974,,,no_permission,,,,,,,,,,,,343120,Reviewed,,,Mon Dec 19 20:17:51 UTC 2016,,,,,,,0|i1n68n:,343424,,,,,,,,,,"12/Dec/16 10:46;nkollar;Investigated this issue, and it seems, that the problem is with the optimization of the sampling job. When the join key is a composite key, in the sampling job it is getting flattened, but since the secondary key optimizer expects composite keys to be wrapped into tuples, we get classcast exception (this also explains why the query didn't fail when secondary key optimizer is switched off). I think not flattening the tuples in the sampling job would solve the problem: PartitionSkewedKeys would work on ((key1, key2, ...), (tuple mem size, key count)) format for composite keys, and on (key, (tuple mem size, key count)) format for non-composite key. This way we can apply secondary key optimizer on the sampling job too. Attached a patch, tests in TestSkewedJoin (including Nick's test case) passed both on MR and on Tez mode, waiting for the result of the entire test suite to make sure it doesn't break other test cases.","15/Dec/16 09:13;nkollar;Looks like the patch didn't break any test, neither in Tez nor in MR mode. [~rohini], [~daijy] [~knoguchi] what do you think? Could either of you please take a look at my patch? I am interested in your thoughts.","16/Dec/16 23:23;rohini;Comments:
   1) 
bq. PartitionSkewedKeys would work on ((key1, key2, ...), (tuple mem size, key count)) format for composite keys, and on (key, (tuple mem size, key count)) format for non-composite key.
Shouldn't it be ((key1, key2, ...), tuple mem size, key count) and (key, tuple mem size, key count). Don't see why we need to have tuple mem size and key count in a tuple. i.e Instead of going from New For Each(true,true)\[tuple\] to New For Each(false,false)\[tuple\], you can do New For Each(false,true)\[tuple\] so that the key is not flattened, but stats is flattened. This will avoid unnecessary increase in size of the sampling data. This will also reduce the number of changes needed in your patch.
 2) TestTezCompiler/TestMRCompiler which compare plans generated should be failing as the plan has changed. Golden files will have to be changed. You can modify generate = true in test class to easily change them. 
  3) testSkewJoinWithTuples - Please assert the actual output and not just the size. Would be good to have a e2e test added as well for this case.","17/Dec/16 13:34;nkollar;Thanks Rohini for your comments, I'll upload a new patch soon.","18/Dec/16 21:28;nkollar;Attached PIG-3417_2.patch with these changes:
- key tuple is not flattened, but statistics are
- golden files changed accordingly. TestTezCompiler test passed with these changes, and no test failed in TestMRCompiler
- added an assert for the output to testSkewJoinWithTuples and added an e2e test too

We might not even need to have testSkewJoinWithTuples, since the e2e tests now cover this case. [~rohini] could you please have have a look at the second version of my patch? ","19/Dec/16 19:03;rohini;+1. Thanks for the changes. The patch is lot simpler with this. 

bq. We might not even need to have testSkewJoinWithTuples, since the e2e tests now cover this case. 
  Yes. That should be fine as it is kind of doing exactly what the e2e is doing even for verification. If it was local mode+ mock.Storage combo  it would have taken less time, but this one would take more than a minute. Considering that, will skip the unit test while checking in. 

[~njw45] / [~daijy],
   Do you want this to go into 0.16.1 as well? ","19/Dec/16 19:18;nkollar;Thanks Rohini, I agree, let's leave the unit test out, and commit just the e2e test. Also, PIG-5069 seems to be the same issue, at least based on the provided information: it fails with classcastexception, the script does a skew join on tuple keys, and Carlos told that without skew join it was fine. Given this, do you think that we can close that item as duplicate of this Jira?",19/Dec/16 19:21;rohini;Done. I will also commit this into 0.16.1 considering that there are more users facing it.,"19/Dec/16 19:29;daijy;Yes, bug fix is ok to backport.","19/Dec/16 20:17;rohini;+1. Committed to both branch-0.16 and trunk. Thanks Nandor for fixing this.

Attached is the final patch that was committed. It has the unit test removed and also tabs changed to spaces in nightly.conf from the previous patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QueryParserDriver.parseSchema(String) silently returns a wrong result when a comma is missing in the schema definition,PIG-3414,12662753,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,09/Aug/13 00:24,14/Oct/13 16:46,14/Mar/19 03:07,14/Aug/13 17:08,,,,,,,,0.12.0,,,,parser,,,0,,,,,,,,,,,,,"QueryParserDriver provides a convenient method to parse from string to LogicalSchema. But if a comma is missing between two fields in the schema definition, it silently returns a wrong result. For example,
{code}
a:int b:long
{code}
This string will be parsed up to ""a:int"", and ""b:long"" will be silently discarded. This should rather fail with a parser exception.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Aug/13 21:28;cheolsoo;PIG-3414-2.patch;https://issues.apache.org/jira/secure/attachment/12597175/PIG-3414-2.patch,09/Aug/13 22:14;cheolsoo;PIG-3414-3.patch;https://issues.apache.org/jira/secure/attachment/12597188/PIG-3414-3.patch,09/Aug/13 23:30;cheolsoo;PIG-3414-4.patch;https://issues.apache.org/jira/secure/attachment/12597210/PIG-3414-4.patch,09/Aug/13 23:45;cheolsoo;PIG-3414-5.patch;https://issues.apache.org/jira/secure/attachment/12597217/PIG-3414-5.patch,14/Aug/13 05:19;cheolsoo;PIG-3414-6.patch;https://issues.apache.org/jira/secure/attachment/12597889/PIG-3414-6.patch,09/Aug/13 00:34;cheolsoo;PIG-3414.patch;https://issues.apache.org/jira/secure/attachment/12596998/PIG-3414.patch,,,,,,6.0,,,,,,,,,,,,,,,,,,,2013-08-09 13:44:47.978,,,no_permission,,,,,,,,,,,,342755,,,,Wed Aug 14 17:08:27 UTC 2013,,,,,,,0|i1n3zj:,343059,,,,,,,,,,"09/Aug/13 00:34;cheolsoo;Attached is a patch that throws an exception when a comma is missing in the schema definition.

I also added new test cases.","09/Aug/13 13:44;xuefuz;I think using custom code to detect grammar error might not be the best approach. I can image that there can be more cases like this. Ideally, we should fix this by the grammar or the use of antlr. ","09/Aug/13 19:24;cheolsoo;[~xuefuz], thank you for your comment sir!

I agree with you. In fact, this issue doesn't exist when the schema definition is used inside the AS clause in a LOAD statement because the grammar looks for a right parenthesis and throws a parser exception if it doesn't find one. Unfortunately, I didn't have a good idea to detect this in the grammar itself when the schema definition is used alone.

For the context, I have a load func that takes the schema string as an option, and I use this helper function to parse it. One user accidentally omitted a comma, and that resulted in some surprising side-effects in downstream ETL processes. So I put some quick fix to prevent such unfortunate event again.

But yes, I agree that this is probably not the best approach. I will cancel the patch for now. :-)","09/Aug/13 21:27;cheolsoo;OK, here is a new patch that fixes the grammar. I added a new rule as follows:
{code}
standalone_field_def_list: field_def_list EOF;
{code}
As can be seen, this rule will ensure that all the tokens are consumed. Let me know if you have a better suggestion.","09/Aug/13 21:53;xuefuz;[~cheolsoo] Thanks for reworking on the patch. The patch looks very good, except for a minor point: instead of ""standalone_field_def_list"", would you consider naming it just as ""schema"" or some sort, paring with its counterpart ""query""? It's now a top-level root and cannot be used in other grammar rules. This might make it clear to other developers.","09/Aug/13 22:14;cheolsoo;Thank you for the suggestion! I renamed the rule to ""schema"" and moved it next to ""query"". So it should be clear that it's a top level rule.",09/Aug/13 22:34;xuefuz;+1,09/Aug/13 22:56;cheolsoo;Committed to trunk. Thank you Xuefu for the review!,"09/Aug/13 23:27;cheolsoo;I realized that I broke TestSchema so reverted my commit. In fact, it's the test case that has a bug:
{code}
datetime(int,long,float,double,boolean,datetime) -- A comma is missing, so it should fail!
{code}
I am going to post a new patch that fixes this test case.","09/Aug/13 23:30;cheolsoo;A new patch includes the following changes:
* Moved my test case to TestSchema.java since that seems like a better place than TestQueryParser.java.
* Fixed the bug in TestSchema.testGetStringFromSchema(), so now TestSchema passes.","10/Aug/13 00:39;cheolsoo;I am discovering several buggy unit tests that were accidentally passing. Now I am adding this new check, they start failing.

I will fix all the buggy unit tests in a new patch.","14/Aug/13 05:28;cheolsoo;All the unit tests are fixed. I uploaded the new patch to RB:
https://reviews.apache.org/r/13551/

Please review one more time. Thanks!",14/Aug/13 14:23;xuefuz;+1. I assume all test pass now.,"14/Aug/13 17:08;cheolsoo;Thank you Xuefu! Yes, all the unit tests pass now. Sorry for all the unexpected inconvenience.

Committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JsonLoader fails the pig job in case of malformed json input,PIG-3413,12662651,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,eyal,sztanko,sztanko,08/Aug/13 16:48,07/Jun/15 03:48,14/Mar/19 03:07,10/Nov/14 07:22,0.11.1,,,,,,,0.15.0,,,,,,,0,json,loader,pig,,,,,,,,,,"The following pig script: 
b = load 'bad.input' using JsonLoader('a0: chararray');
dump b;

runs well for the input:
{""a"": ""good""}

and fails the whole job for the following input (mallformed json)
{""a"", bad}


I was expecting that it will just skip the line and go further.

Getting this error:
org.codehaus.jackson.JsonParseException: Unexpected character ('g' (code 103)): was expecting comma to separate OBJECT entries
 at [Source: java.io.ByteArrayInputStream@4610c772; line: 1, column: 4100]
	at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:1433)
	at org.codehaus.jackson.impl.JsonParserMinimalBase._reportError(JsonParserMinimalBase.java:521)
	at org.codehaus.jackson.impl.JsonParserMinimalBase._reportUnexpectedChar(JsonParserMinimalBase.java:442)
	at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:482)
	at org.apache.pig.builtin.JsonLoader.readField(JsonLoader.java:173)
	at org.apache.pig.builtin.JsonLoader.getNext(JsonLoader.java:157)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:211)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:540)
	at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:771)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:375)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1132)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)",,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,,,,,09/Nov/14 14:33;eyal;PIG-3413.patch;https://issues.apache.org/jira/secure/attachment/12680483/PIG-3413.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-27 01:34:23.832,,,no_permission,,,,,,,,,,,,342654,Reviewed,,,Mon Nov 10 07:22:49 UTC 2014,,,,,,,0|i1n3d3:,342958,,,,,,,,,,08/Aug/13 16:49;sztanko;It is fairly trivial to fix it (just catch the JsonParseException and return null) and I am going to submit a patch soon.,27/Oct/13 01:34;cheolsoo;Please mark as patch available _after_ uploading a patch.,"03/Nov/14 13:18;eyal;Still interested in a patch? It does look pretty simple. If so, for what branch?",03/Nov/14 18:29;daijy;You can base your patch with trunk.,"04/Nov/14 09:51;eyal;I'm having trouble building trunk - probably because of [PIG-3399|https://issues.apache.org/jira/browse/PIG-3399]. I tested it out on Pig 0.11 (which I can build) because that's what I'm using, and because it builds successfully for me in Eclipse. Should I put a patch for that, for 11.2? Or do you have any pointers for dealing with PIG-3399?","09/Nov/14 14:33;eyal;This is a patch based on trunk which includes the fix for ignoring malformed jsons (a warning is issued) and a new unit test (within TestJsonLoaderStorage).

This bug affects the newer Pig versions, too, not just 0.11.1","10/Nov/14 07:22;daijy;+1.

Patch committed to trunk. Thanks Eyal!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jsonstorage breaks when tuple does not have as many columns as schema,PIG-3412,12662182,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aesilberstein,aesilberstein,aesilberstein,06/Aug/13 18:18,14/Oct/13 16:46,14/Mar/19 03:07,14/Aug/13 02:39,0.11,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"Noticed this error when doing something like 
A = flatten(STRSPLIT($0, ',', 3)) AS (col1:chararray, col2:chararray, col3:chararray);
STORE A INTO 'foo' USING JsonStorage();

If the string being split doesn't generate 3 columns, then JsonStorage errors out with an index exception.  This is because it tries to read the fields of the tuple passed to it or not.  See JsonStorage, line 148.

MY patch checks the length of the tuple.  If any schema column positions are past the length of the tuple, it fills in null.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Aug/13 18:20;aesilberstein;jsonStoragePatch.patch;https://issues.apache.org/jira/secure/attachment/12596384/jsonStoragePatch.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-14 02:32:16.435,,,no_permission,,,,,,,,,,,,342186,,,,Wed Aug 14 02:39:00 UTC 2013,,,Patch Available,,,,0|i1n0hz:,342491,,,,,,,,,,14/Aug/13 02:32;cheolsoo;+1.,14/Aug/13 02:39;cheolsoo;Committed to trunk. Thank you Adam!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LimitOptimizer is applied before PartitionFilterOptimizer,PIG-3410,12661491,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,aniket486,aniket486,02/Aug/13 21:27,14/Oct/13 16:46,14/Mar/19 03:07,16/Aug/13 21:52,0.11.1,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"Consider following script-
{code}
hcat_load = LOAD 'X' using org.apache.hcatalog.pig.HCatLoader();
hcat_filter = FILTER hcat_load BY (part='Y');
hcat_limited = limit hcat_filter 5;
dump hcat_limited;                     
{code}
This script is not benefited from LimitOptimizer (pushing limit to loadfunc) because LimitOptimizer is applied before PartitionFilterOptimizer. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Aug/13 21:28;aniket486;PIG-3410.patch;https://issues.apache.org/jira/secure/attachment/12596950/PIG-3410.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-14 02:17:31.514,,,no_permission,,,,,,,,,,,,341680,,,,Fri Aug 16 23:18:05 UTC 2013,,,,,,,0|i1mxdr:,341986,,,,,,,,,,"14/Aug/13 02:17;cheolsoo;[~aniket486], did you run the full unit test suite? I just want to make sure we don't break anything.","15/Aug/13 17:34;aniket486;[~cheolsoo], yes, I did run full test suite. None of failure are related to this (there were a few due to pig not cleaning up files in our environment).
I haven't tried e2e, do you have a setup for that?","16/Aug/13 00:56;cheolsoo;Let me run e2e test.

I played with your patch a bit, and it works well for me. Basically, I tested the following cases:
{code:title=LIMIT is not pushed down}
a = LOAD 'prodhive.cheolsoop.foo' USING MyStorage();
b = FILTER a BY part == 1 and nonpart == 'a';
c = LIMIT b 1;
DUMP c;
{code}
{code:title=LIMIT is pushed down}
a = LOAD 'prodhive.cheolsoop.foo' USING MyStorage();
b = FILTER a BY part == 1;
c = LIMIT b 1;
DUMP c;
{code}","16/Aug/13 01:15;aniket486;I'm planning to play on my side what happens when we do For n times (run all optimizers one after other). Right now, we have run all optimizers m times one after the other, but that's not helpful.

Let's do this as first step.","16/Aug/13 01:18;cheolsoo;Ok, are you saying you want to do more testing? In that case, I will wait for now.",16/Aug/13 01:24;cheolsoo;I misn-uderstood Aniket's comment. I will continue to test/review.,"16/Aug/13 01:24;aniket486;Sorry, I wanted to say that we should solve the real problem (that order of optimizers matter) in another jira. Please review if this looks good.","16/Aug/13 21:27;cheolsoo;+1.

I ran both unit test and e2e test. I saw 2 failures in e2e test, but they're not related and will open jiras for them.",16/Aug/13 21:52;aniket486;Committed to trunk. Thanks Cheolsoo for the review!,16/Aug/13 23:18;cheolsoo;I filed PIG-3428 for the failing e2e tests.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Top UDF documentation indicates improper use,PIG-3405,12661167,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,aniket486,keith.wyss,keith.wyss,01/Aug/13 16:42,14/Oct/13 16:46,14/Mar/19 03:07,16/Aug/13 19:07,0.10.0,,,,,,,0.12.0,,,,documentation,,,0,,,,,,,,,,,,,"The documentation in both the source code javadoc: http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/builtin/TOP.java and the Apache Pig Reference manual: http://pig.apache.org/docs/r0.8.1/piglatin_ref2.html#TOP indicates that the index that is passed to the TOP UDF should be 1-indexed. 

Reading the code and testing the results, this is false. Zero-indexing gives the correct behavior. Users should not be betrayed by the official documentation.",Javadoc and pig tutorial,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,04/Aug/13 08:19;aniket486;PIG-3405.patch;https://issues.apache.org/jira/secure/attachment/12595793/PIG-3405.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-04 08:21:06.098,,,no_permission,,,,,,,,,,,,341356,,,,Fri Aug 16 19:07:53 UTC 2013,,,,,,,0|i1mvgf:,341674,,,,,,,,,,"04/Aug/13 08:21;aniket486;[~keith.wyss], Thanks for opening this jira. I have seen people often getting confused because of this. Can you review the patch?","05/Aug/13 17:41;keith.wyss;Thanks for making these changes Aniket. Hopefully some mistakes in the future can be averted due to this patch.
",05/Aug/13 17:47;cheolsoo;+1.,16/Aug/13 19:07;cheolsoo;Committed to trunk. Thank you Aniket!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Large filter expression makes Pig hang,PIG-3395,12659856,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,25/Jul/13 17:56,14/Oct/13 16:46,14/Mar/19 03:07,01/Aug/13 22:20,,,,,,,,0.12.0,,,,impl,,,0,,,,,,,,,,,,,"Currently, partition filter push down is quite costly. For example, if you have many nested or/and expressions, Pig hangs:
{code}
base = load '<partitioned table>' using MyStorage();
filt = filter base by
(dateint == 20130719 and batchid == 'merged_1' and hour IN (19,20,21,22,23))
or
(dateint == 20130720 and batchid == 'merged_1' and hour IN (0,1,2,3,4,5,6,7,8))
or
(dateint == 20130720 and batchid == 'merged_2' and hour == 7)
or
(dateint == 20130720 and batchid == 'merged_1' and hour IN (9,10,11,12,13,14,15,16,17,18,19,20,21,22,23))
or
(dateint == 20130721 and batchid == 'merged_1' and hour IN (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23))
or
(dateint == 20130722 and batchid == 'merged_1' and hour IN (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16));
dump filt;
{code}
Note that IN operator is converted to nested OR's by Pig parser.

Looking at the thread dump, I found it creates almost 60 stack frames and makes JVM suffer. (I will attach full stack trace.)
{code}
<repeated ...>
at org.apache.pig.newplan.PColFilterExtractor.visit(PColFilterExtractor.java:504)
at org.apache.pig.newplan.PColFilterExtractor.visit(PColFilterExtractor.java:237)
at org.apache.pig.newplan.PColFilterExtractor.visit(PColFilterExtractor.java:504)
at org.apache.pig.newplan.PColFilterExtractor.visit(PColFilterExtractor.java:214)
at org.apache.pig.newplan.PColFilterExtractor.visit(PColFilterExtractor.java:504)
at org.apache.pig.newplan.PColFilterExtractor.visit(PColFilterExtractor.java:211)
at org.apache.pig.newplan.PColFilterExtractor.visit(PColFilterExtractor.java:108)
{code}
Although the filter expression can be simplified, it seems possible to make PColFilterExtractor more efficient.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Aug/13 04:15;cheolsoo;PIG-3395-2.patch;https://issues.apache.org/jira/secure/attachment/12595340/PIG-3395-2.patch,25/Jul/13 21:03;cheolsoo;PIG-3395.patch;https://issues.apache.org/jira/secure/attachment/12594268/PIG-3395.patch,25/Jul/13 17:59;cheolsoo;thread_dump.txt;https://issues.apache.org/jira/secure/attachment/12594214/thread_dump.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-07-29 19:42:32.355,,,no_permission,,,,,,,,,,,,340048,,,,Thu Aug 01 22:20:58 UTC 2013,,,,,,,0|i1mnef:,340366,,,,,,,,,,25/Jul/13 17:59;cheolsoo;Attaching the tread dump. I will shortly upload a patch that refactors PColFilterExtractor that makes it work.,"25/Jul/13 21:03;cheolsoo;I am attaching a patch that refactors the logic of detecting the pattern of ""(pcond and cond) or (pcond and cond)"" that was added by PIG-3173 into a separate function.

I also added a unit test case that demonstrates the issue. If you run it without the fix, it infinitely hangs. With the fix, it runs within seconds.

In terms of testing,
* ant test-commit passes.
* ant test -Dtestcase=TestPartitionFilterPushDown passes.",26/Jul/13 10:10;cheolsoo;All unit tests pass.,"29/Jul/13 19:42;rohini;Additional cases that visit(ProjectExpression project) checks like userfunc, cast, null, bincond will not be done right? It might lead to script failing if they are present. ","29/Jul/13 19:54;cheolsoo;[~rohini], thank you for the comment. Please correct me if I am wrong, but I am not removing all the recursive calls from visit() but taking out one particular check (i.e. (pcond and cond) or (pcond and cond)). So I think it still checks other cases that you mentioned, no?
","29/Jul/13 20:05;rohini;Before the whole and/or tree will not be pushed down. The visit of lhs and rhs is still there, but I am not sure how the replace will behave because it does not have full context and something partial might get pushed. Can you just modify your testcase to include one of those conditions to test the behaviour if we have cast or null check? ","29/Jul/13 20:13;cheolsoo;[~rohini], I can certainly add more test cases to verify that. Let me do it.","30/Jul/13 17:09;cheolsoo;I have been testing my patch as per Rohini's suggestion, and it works correctly. Here are what I tested:
* (cast) or (pcond and pcond) or (pcond and pcond)
* (pcond and pcond) or (cast) or (pcond and pcond)
* (pcond and pcond) or (pcond and pcond) or (cast)

In all these cases, the entire filter is rejected due to the cast expression, which is the same as before.

Adding test cases is a bit more involving because the test helper function isn't written for such conditions. But I will add a few test cases.","01/Aug/13 04:15;cheolsoo;Added new test cases to confirm that the filter doesn't get pushed down if udf/cast/null expressions are mixed with or/and expressions.

ReviewBoard:
https://reviews.apache.org/r/13186/",01/Aug/13 21:42;rohini;+1. There are few white spaces. Can you just remove them before the commit?,"01/Aug/13 22:20;cheolsoo;Thank you Rohini for the review! Committed it to trunk.

I fixed all the white space problems in PColFilterExtractor.java (trailing white spaces and tabs), so it should be easy on the eyes now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
STARTSWITH udf doesn't override outputSchema method,PIG-3393,12659611,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,skrishnan,cheolsoo,cheolsoo,24/Jul/13 16:22,14/Oct/13 16:46,14/Mar/19 03:07,25/Jul/13 23:56,,,,,,,,0.12.0,,,,internal-udfs,,,0,,,,,,,,,,,,,"It appears that a wrong patch was committed in PIG-2879. Looking at the code in trunk, the comments in the jira are not addressed yet committed:
# outputSchema() method should be overridden.
# exceptions should be handled better.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Jul/13 23:41;skrishnan;PIG-3393.patch;https://issues.apache.org/jira/secure/attachment/12594294/PIG-3393.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-25 23:41:35.308,,,no_permission,,,,,,,,,,,,339804,,,,Thu Jul 25 23:56:04 UTC 2013,,,,,,,0|i1mlwf:,340123,,,,,,,,,,24/Jul/13 16:23;cheolsoo;Assigning this jira to Sriram since he originally discovered the issues. Thanks!,25/Jul/13 23:41;skrishnan;Fix for outputSchema and exception handling,25/Jul/13 23:41;skrishnan;Patch attached.,25/Jul/13 23:49;cheolsoo;+1. I will commit it after running test.,25/Jul/13 23:56;cheolsoo;Committed to trunk. Thanks Sriram!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Set job.name"" does not work with dump command",PIG-3389,12659367,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,cheolsoo,cheolsoo,cheolsoo,23/Jul/13 16:19,14/Oct/13 16:46,14/Mar/19 03:07,25/Jul/13 21:59,,,,,,,,0.12.0,,,,grunt,,,0,,,,,,,,,,,,,"The ""job.name"" property can be used to overwrite the default job name in Pig, but the dump command does not honor it.

To reproduce the issue, run the following commands in Grunt shell in MR mode:
{code}
SET job.name 'FOO';
a = LOAD '/foo';
DUMP a;
{code}
You will see the job name is not 'FOO' in the JT UI. However, using store instead of dump sets the job name correctly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Jul/13 16:21;cheolsoo;PIG-3389.patch;https://issues.apache.org/jira/secure/attachment/12593725/PIG-3389.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-24 22:03:27.048,,,no_permission,,,,,,,,,,,,339560,,,,Thu Jul 25 21:59:45 UTC 2013,,,,,,,0|i1mken:,339880,,,,,,,,,,"23/Jul/13 16:21;cheolsoo;The problem is that the job.name property is not set in PigContext when executing PigServer#openIterator().

Attached is a patch that fixes the problem.",24/Jul/13 22:03;alangates;+1,25/Jul/13 21:59;cheolsoo;Committed to trunk. Thank you Alan for the review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DISTINCT no longer uses custom partitioner,PIG-3385,12658642,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,knoguchi,oberman,oberman,18/Jul/13 21:15,14/Oct/13 16:46,14/Mar/19 03:07,28/Aug/13 00:23,,,,,,,,0.11.2,0.12.0,,,impl,,,0,,,,,,,,,,,,,From user@pig.apache.org:  It looks like an optimization was put in to make distinct use a special partitioner which prevents the user from setting the partitioner.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Aug/13 22:02;knoguchi;pig-3385-v01.patch;https://issues.apache.org/jira/secure/attachment/12599042/pig-3385-v01.patch,21/Aug/13 19:28;knoguchi;pig-3385-v02.patch;https://issues.apache.org/jira/secure/attachment/12599253/pig-3385-v02.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-07-19 15:03:12.682,,,no_permission,,,,,,,,,,,,338836,Reviewed,,,Wed Aug 28 20:39:22 UTC 2013,,,,,,,0|i1mfxr:,339156,,,,,,,,,,19/Jul/13 15:03;sbilstein;I observed this issue as well with DISTINCT clauses.,19/Jul/13 17:54;sbilstein;here is the SO question I made documenting the issue: http://stackoverflow.com/questions/17554593/custom-partitioner-in-hadoop/17747335?noredirect=1#17747335,"20/Aug/13 22:02;knoguchi;Wondering if custom partitioner ever worked for distinct.  

Looks like partitioner info is passed through POGlobalRearrange but ""distinct"" doesn't use it. 

Uploading an initial patch that just passes that info through PODistinct. 

It's the first time for me to touch the backend code. Appreciate if someone can take a look.  I'll upload a testcase next.",21/Aug/13 19:28;knoguchi;Uploading a patch with test.  Noticed that original test for custom partitioners didn't give different partition results than the default so added one silly partitioner that always return 1 (second reducer).,"21/Aug/13 22:45;knoguchi;While looking at this jira, noticed custom partitioner being dropped when run with multi query optimization.  Created PIG-3435.",28/Aug/13 00:21;daijy;+1. Verified distinct does not work with custom partition even in early releases.,28/Aug/13 00:23;daijy;Patch committed to trunk. Thanks Koji!,"28/Aug/13 01:33;knoguchi;Thanks Daniel!  Can we back-port this patch and PIG-3435 to 0.11?  Without them, custom partitioner is almost unusable.
",28/Aug/13 20:39;daijy;Committed to 0.11 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing negation in UDF doc sample code,PIG-3384,12658440,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,ddamours,ddamours,ddamours,18/Jul/13 03:29,14/Oct/13 16:46,14/Mar/19 03:07,18/Jul/13 23:55,0.11.1,,,,,,,0.12.0,,,,documentation,,,0,documentation,,,,,,,,,,,,"Code sample for UDF has source code for the TOKENIZE function but it is missing a negation compared to the real souce code in the trunk.

if ((o instanceof String)) {
  throw new IOException(""Expected input to be chararray, but  got "" + o.getClass().getName());
}

should be

if (!(o instanceof String)) {
  throw new IOException(""Expected input to be chararray, but  got "" + o.getClass().getName());
}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Jul/13 03:34;ddamours;PIG-3384.patch;https://issues.apache.org/jira/secure/attachment/12592906/PIG-3384.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-18 23:49:58.401,,,no_permission,,,,,,,,,,,,338634,,,,Thu Jul 18 23:55:40 UTC 2013,,,Patch Available,,,,0|i1meov:,338954,,,,,,,,,,18/Jul/13 03:34;ddamours;Patch to fix documentation PIG-3384,18/Jul/13 23:49;cheolsoo;+1. I will commit it soon.,18/Jul/13 23:55;cheolsoo;Committed to trunk. Thank you Danny!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix e2e test float precision related test failures when run with -Dpig.exec.mapPartAgg=true,PIG-3380,12658155,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,rohini,anlilin,anlilin,17/Jul/13 01:04,14/Oct/13 16:45,14/Mar/19 03:07,17/Jul/13 17:25,0.11.2,,,,,,,0.11.2,0.12.0,,,e2e harness,,,0,,,,,,,,,,,,,"attached is the patch created from
http://svn.apache.org/repos/asf/pig/branches/branch-0.11/

two conf files are modified:
  nightly.conf
  turing_jython.conf

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Jul/13 17:27;anlilin;PIG-3380.patch;https://issues.apache.org/jira/secure/attachment/12592805/PIG-3380.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-17 17:25:26.991,,,no_permission,,,,,,,,,,,,338349,,,,Wed Jul 17 17:27:55 UTC 2013,,,Patch Available,,,,0|i1mcxr:,338669,,,,,,,,,,17/Jul/13 01:08;anlilin;patch to fix existing e2e failures due to test related problem.,17/Jul/13 17:25;rohini;+1. Committed to branch-11 and trunk. Thanks Annie.,17/Jul/13 17:27;anlilin;patch to fix failures caused by test related issue.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Alias reuse in nested foreach causes PIG script to fail,PIG-3379,12658087,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,16/Jul/13 18:37,14/Oct/13 16:46,14/Mar/19 03:07,23/Aug/13 21:07,0.11.1,,,,,,,0.12.0,,,,impl,,,0,,,,,,,,,,,,,"The following script fails:
{code:title=temp.pig}
Events = LOAD 'x' AS (eventTime:long, deviceId:chararray, eventName:chararray);
Events = FOREACH Events GENERATE eventTime, deviceId, eventName;
EventsPerMinute = GROUP Events BY (eventTime / 60000);
EventsPerMinute = FOREACH EventsPerMinute {
  DistinctDevices = DISTINCT Events.deviceId;
  nbDevices = SIZE(DistinctDevices);

  DistinctDevices = FILTER Events BY eventName == 'xuaHeartBeat';
  nbDevicesWatching = SIZE(DistinctDevices);

  GENERATE $0*60000 as timeStamp, nbDevices as nbDevices, nbDevicesWatching as nbDevicesWatching;
}
EventsPerMinute = FILTER EventsPerMinute BY timeStamp >= 0  AND timeStamp < 100000;
A = FOREACH EventsPerMinute GENERATE timeStamp;
describe A;
{code}
With the error:
{code}
2013-07-16 11:31:20,450 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1025: 
<file /home/xzhang/Documents/temp.pig, line 14, column 37> Invalid field projection. Projected field [timeStamp] does not exist in schema: deviceId:chararray.
{code}
Using distinct alias name for the 2nd ""DistinctDevices"" fixes the problem. As an observation, removing the last filter statement also fixes the problem.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Aug/13 19:37;daijy;PIG-3379-draft.patch;https://issues.apache.org/jira/secure/attachment/12596916/PIG-3379-draft.patch,05/Aug/13 20:08;xuefuz;PIG-3379.patch;https://issues.apache.org/jira/secure/attachment/12596204/PIG-3379.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-08-08 19:37:25.874,,,no_permission,,,,,,,,,,,,338281,Reviewed,,,Fri Aug 23 21:07:12 UTC 2013,,,,,,,0|i1mcin:,338601,,,,,,,,,,"16/Jul/13 18:59;xuefuz;It seems related to PIG-1271 and PIG-2530, but both were marked as fixed.","01/Aug/13 16:37;xuefuz;Correction, I meant PIG-1721 instead in above comment.","08/Aug/13 19:37;daijy;[~xuefuz], seems we can have a simpler fix. Attach PIG-3379-draft.patch. 

How do you think?","08/Aug/13 20:15;xuefuz;[~daijy] Thanks for your suggestion. While your patch does make ""describe A"" work, it generates the wrong result with the new test case in my patch. Further, the following is shown in the logical plan for ""EventsPerMinute"", in which we only have one ""DistinctDevices"" operator, which is incorrect. My original patch was to fix this, making sure that the projected impression is pointing to the right operator. Please let me know your further thoughts.

    |---EventsPerMinute: (Name: LOForEach Schema: timeStamp#141:long,nbDevices#142:long,nbDevicesWatching#143:long)
        |   |
        |   (Name: LOGenerate[false,false,false] Schema: timeStamp#141:long,nbDevices#142:long,nbDevicesWatching#143:long)ColumnPrune:InputUids=[135, 134]ColumnPrune:OutputUids=[141, 143, 142]
        |   |   |
        |   |   (Name: Multiply Type: long Uid: 141)
        |   |   |
        |   |   |---group:(Name: Project Type: long Uid: 134 Input: 0 Column: (*))
        |   |   |
        |   |   |---(Name: Cast Type: long Uid: 139)
        |   |       |
        |   |       |---(Name: Constant Type: int Uid: 139)
        |   |   |
        |   |   (Name: UserFunc(org.apache.pig.builtin.BagSize) Type: long Uid: 142)
        |   |   |
        |   |   |---DistinctDevices:(Name: Project Type: bag Uid: 135 Input: 1 Column: (*))
        |   |   |
        |   |   (Name: UserFunc(org.apache.pig.builtin.BagSize) Type: long Uid: 143)
        |   |   |
        |   |   |---DistinctDevices:(Name: Project Type: bag Uid: 135 Input: 1 Column: (*))
        |   |
        |   |---(Name: LOInnerLoad[0] Schema: group#134:long)
        |   |
        |   |---DistinctDevices: (Name: LOFilter Schema: eventTime#106:long,deviceId#107:chararray,eventName#108:chararray)
        |       |   |
        |       |   (Name: Equal Type: boolean Uid: 138)
        |       |   |
        |       |   |---eventName:(Name: Project Type: chararray Uid: 108 Input: 0 Column: 2)
        |       |   |
        |       |   |---(Name: Constant Type: chararray Uid: 137)
        |       |
        |       |---Events: (Name: LOInnerLoad[1] Schema: eventTime#106:long,deviceId#107:chararray,eventName#108:chararray)
","08/Aug/13 20:16;xuefuz;Repost the logical plan snippet.

{code}    |---EventsPerMinute: (Name: LOForEach Schema: timeStamp#141:long,nbDevices#142:long,nbDevicesWatching#143:long)
        |   |
        |   (Name: LOGenerate[false,false,false] Schema: timeStamp#141:long,nbDevices#142:long,nbDevicesWatching#143:long)ColumnPrune:InputUids=[135, 134]ColumnPrune:OutputUids=[141, 143, 142]
        |   |   |
        |   |   (Name: Multiply Type: long Uid: 141)
        |   |   |
        |   |   |---group:(Name: Project Type: long Uid: 134 Input: 0 Column: (*))
        |   |   |
        |   |   |---(Name: Cast Type: long Uid: 139)
        |   |       |
        |   |       |---(Name: Constant Type: int Uid: 139)
        |   |   |
        |   |   (Name: UserFunc(org.apache.pig.builtin.BagSize) Type: long Uid: 142)
        |   |   |
        |   |   |---DistinctDevices:(Name: Project Type: bag Uid: 135 Input: 1 Column: (*))
        |   |   |
        |   |   (Name: UserFunc(org.apache.pig.builtin.BagSize) Type: long Uid: 143)
        |   |   |
        |   |   |---DistinctDevices:(Name: Project Type: bag Uid: 135 Input: 1 Column: (*))
        |   |
        |   |---(Name: LOInnerLoad[0] Schema: group#134:long)
        |   |
        |   |---DistinctDevices: (Name: LOFilter Schema: eventTime#106:long,deviceId#107:chararray,eventName#108:chararray)
        |       |   |
        |       |   (Name: Equal Type: boolean Uid: 138)
        |       |   |
        |       |   |---eventName:(Name: Project Type: chararray Uid: 108 Input: 0 Column: 2)
        |       |   |
        |       |   |---(Name: Constant Type: chararray Uid: 137)
        |       |
        |       |---Events: (Name: LOInnerLoad[1] Schema: eventTime#106:long,deviceId#107:chararray,eventName#108:chararray)
        |


{code}","09/Aug/13 00:24;daijy;Yes, you are right, it's not the dangling branch, it's the incorrect inner plan. Let me take a look again.","23/Aug/13 01:32;daijy;Missing LODistinct in the posted logical plan. Should be:
{code}
    |---EventsPerMinute: (Name: LOForEach Schema: timeStamp#56:long,nbDevices#57:long,nbDevicesWatching#58:long)
        |   |
        |   (Name: LOGenerate[false,false,false] Schema: timeStamp#56:long,nbDevices#57:long,nbDevicesWatching#58:long)ColumnPrune:InputUids=[50, 49]ColumnPrune:OutputUids=[58, 57, 56]
        |   |   |
        |   |   (Name: Multiply Type: long Uid: 56)
        |   |   |
        |   |   |---group:(Name: Project Type: long Uid: 49 Input: 0 Column: (*))
        |   |   |
        |   |   |---(Name: Cast Type: long Uid: 54)
        |   |       |
        |   |       |---(Name: Constant Type: int Uid: 54)
        |   |   |
        |   |   (Name: UserFunc(org.apache.pig.builtin.BagSize) Type: long Uid: 57)
        |   |   |
        |   |   |---DistinctDevices:(Name: Project Type: bag Uid: 50 Input: 1 Column: (*))
        |   |   |
        |   |   (Name: UserFunc(org.apache.pig.builtin.BagSize) Type: long Uid: 58)
        |   |   |
        |   |   |---DistinctDevices:(Name: Project Type: bag Uid: 50 Input: 2 Column: (*))
        |   |
        |   |---(Name: LOInnerLoad[0] Schema: group#49:long)
        |   |
        |   |---DistinctDevices: (Name: LODistinct Schema: deviceId#22:chararray)
        |   |   |
        |   |   |---1-3: (Name: LOForEach Schema: deviceId#22:chararray)
        |   |       |   |
        |   |       |   (Name: LOGenerate[false] Schema: deviceId#22:chararray)
        |   |       |   |   |
        |   |       |   |   deviceId:(Name: Project Type: chararray Uid: 22 Input: 0 Column: (*))
        |   |       |   |
        |   |       |   |---(Name: LOInnerLoad[1] Schema: deviceId#22:chararray)
        |   |       |
        |   |       |---Events: (Name: LOInnerLoad[1] Schema: eventTime#21:long,deviceId#22:chararray,eventName#23:chararray)
{code}

The plan looks right.

Talked with [~xuefuz], the idea is to use projectedOperator instead of alias at the time we convert alias to position. The newly introduced projectedOperator is only used in alias translation. After that, input# and col# will be use as the coordinates of ProjectExpression. Patch looks good. I will commit it once tests pass.",23/Aug/13 21:07;daijy;Patch committed to trunk. Thanks Xuefu!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New AvroStorage throws NPE when storing untyped map/array/bag,PIG-3377,12657551,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jadler,cheolsoo,cheolsoo,13/Jul/13 00:35,07/Jul/14 18:08,14/Mar/19 03:07,27/Oct/13 01:43,,,,,,,,0.13.0,,,,internal-udfs,,,0,,,,,,,,,,,,,"The following example demonstrates the issue:
{code}
a = LOAD 'foo' AS (m:map[]);
STORE a INTO 'bar' USING AvroStorage();
{code}
This fails with the following error:
{code}
java.lang.NullPointerException
    at org.apache.pig.impl.util.avro.AvroStorageSchemaConversionUtilities.resourceFieldSchemaToAvroSchema(AvroStorageSchemaConversionUtilities.java:462)
    at org.apache.pig.impl.util.avro.AvroStorageSchemaConversionUtilities.resourceSchemaToAvroSchema(AvroStorageSchemaConversionUtilities.java:335)
    at org.apache.pig.builtin.AvroStorage.checkSchema(AvroStorage.java:472)
{code}
Similarly, untyped bag causes the following error:
{code}
Caused by: java.lang.NullPointerException
    at org.apache.avro.Schema$ArraySchema.toJson(Schema.java:722)
    ...
    at org.apache.avro.Schema.getElementType(Schema.java:256)
    at org.apache.pig.builtin.AvroStorage.setOutputAvroSchema(AvroStorage.java:491)
{code}
The problem is that AvroStorage cannot derive the output schema from untyped map/bag/tuple. When type is not defined, it should be assumed as bytearray.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Oct/13 13:11;cheolsoo;PIG-3377-2.patch;https://issues.apache.org/jira/secure/attachment/12607799/PIG-3377-2.patch,07/Oct/13 22:52;jadler;PIG-3377.patch;https://issues.apache.org/jira/secure/attachment/12607260/PIG-3377.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-07-17 18:06:04.927,,,no_permission,,,,,,,,,,,,337772,,,,Sun Oct 27 01:43:28 UTC 2013,,,,,,,0|i1m9dz:,338094,,,,,,,,,,17/Jul/13 18:06;jadler;Want to assign this to me? I can take a look at this and submit a patch.,"17/Jul/13 18:20;cheolsoo;[~jadler], sure! That would be awesome!","25/Sep/13 22:27;daijy;[~jadler], are you still working on it?",07/Oct/13 22:37;jadler;Working on this now...,07/Oct/13 22:52;jadler;Patch for this issue (provides a meaningful error message),"10/Oct/13 13:11;cheolsoo;[~jadler], thank you for the patch! It looks good to me.

I think we should add the null checks for untyped bag and tuple too. I am uploading a patch that includes them. Please let me know if this looks okay to you.


",27/Oct/13 01:43;cheolsoo;Committed to trunk. Thank you Joe!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CASE does not preserve the order of when branches,PIG-3375,12656410,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,05/Jul/13 19:37,14/Oct/13 16:45,14/Mar/19 03:07,09/Jul/13 19:17,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"Currently CASE expression builds nested BinCond expressions in the following order: from top to bottom when branches => from in to out bin conds.

This can be confusing if non-mutually exclusive conditions are used in when branches. For example,
{code}
CASE 
    WHEN x > 100 THEN 'a'
    WHEN x > 50  THEN 'b'
    ELSE 'c'
END
{code}
is converted to
{code}
x > 50 ? 'b' : ( x > 100 ? 'a' : 'c' )
{code}
Now if x is 1000, the result is 'b', which is not quite expected.

Instead, we should construct nested BinCond expressions in the same order as that of when branches, i.e.
{code}
x > 100 ? 'a' : ( x > 50 ? 'b' : 'c' )
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Jul/13 01:31;cheolsoo;PIG-3375.patch;https://issues.apache.org/jira/secure/attachment/12591063/PIG-3375.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-08 23:35:40.818,,,no_permission,,,,,,,,,,,,336633,,,,Tue Jul 09 19:17:13 UTC 2013,,,,,,,0|i1m2dj:,336956,,,,,,,,,,"06/Jul/13 01:31;cheolsoo;Attached is a patch that fixes the problem. Added a new test case to TestCase and verified ""ant test -Dtestcase=TestCase"" passes.",08/Jul/13 23:35;rohini;+1,09/Jul/13 19:17;cheolsoo;Committed to trunk. Thanks Rohini for the review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CASE and IN fail when expression includes dereferencing operator,PIG-3374,12656408,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,05/Jul/13 19:27,14/Oct/13 16:46,14/Mar/19 03:07,30/Aug/13 22:42,,,,,,,,0.12.0,,,,parser,,,0,,,,,,,,,,,,,"This is another bug that I discovered after deploying CASE/IN expressions internally.

The current implementation of CASE/IN expression assumes that the 1st operand is a single expression. But this is not true, for example, if it contains a dereferencing operator. The following example demonstrates the problem:
{code}
A = LOAD 'foo' AS (k1:chararray, k2:chararray, v:int);
B = GROUP A BY (k1, k2);
C = FILTER B BY group.k1 IN ('a', 'b');
DUMP C;
{code}
This fails with the following error:
{code}
Caused by: java.lang.IndexOutOfBoundsException: Index: 5, Size: 5
    at java.util.ArrayList.RangeCheck(ArrayList.java:547)
    at java.util.ArrayList.get(ArrayList.java:322)
    at org.apache.pig.parser.LogicalPlanGenerator.in_eval(LogicalPlanGenerator.java:8624)
    at org.apache.pig.parser.LogicalPlanGenerator.cond(LogicalPlanGenerator.java:8405)
    at org.apache.pig.parser.LogicalPlanGenerator.filter_clause(LogicalPlanGenerator.java:7564)
    at org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1403)
    at org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:821)
    at org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:539)
    at org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:414)
    at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:181)
{code}
Here is the relavant code that causes trouble:
{code:title=QueryParser.g}
if(tree.getType() == IN) {
  Tree lhs = tree.getChild(0); // lhs is not a single node!
  for(int i = 2; i < tree.getChildCount(); i = i + 2) {
    tree.insertChild(i, deepCopy(lhs));
  }
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Jul/13 19:55;cheolsoo;PIG-3374-2.patch;https://issues.apache.org/jira/secure/attachment/12591140/PIG-3374-2.patch,09/Jul/13 22:47;cheolsoo;PIG-3374-3.patch;https://issues.apache.org/jira/secure/attachment/12591542/PIG-3374-3.patch,17/Jul/13 05:48;cheolsoo;PIG-3374-4.patch;https://issues.apache.org/jira/secure/attachment/12592722/PIG-3374-4.patch,06/Jul/13 01:04;cheolsoo;PIG-3374.patch;https://issues.apache.org/jira/secure/attachment/12591062/PIG-3374.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,336631,,,,Fri Aug 30 22:42:32 UTC 2013,,,,,,,0|i1m2d3:,336954,,,,,,,,,,"06/Jul/13 01:04;cheolsoo;ReviewBoard:
https://reviews.apache.org/r/12290/",07/Jul/13 19:55;cheolsoo;Corrected few typos in a new patch.,"09/Jul/13 22:47;cheolsoo;Rebased the patch to trunk. I also verified that all unit tests pass.

In terms of the fix, here is the summary of what I did.

When constructing antlr trees from case/in syntax, I used to collapse expressions into a flat list and assume that the first element is the lhs.

i.e. expr IN ( expr, expr* ) -> ^( IN expr+ )

But now I construct a list of sub-trees instead.

i.e. lhs IN ( rhs, rhs* ) -> ^( IN ^( LHS lhs ) ^( RHS rhs )+  )


This way it is clear where the boundaries between expressions are, so I no longer need to assume anything.","17/Jul/13 05:48;cheolsoo;I forgot to update AliasMasker.g in the previous patch, so I included the update in a new patch.

I also discovered that PIG-3342 didn't update AliasMasker.g and included the change in the new patch.","02/Aug/13 01:50;cheolsoo;ReviewBoard:
https://reviews.apache.org/r/13210/","30/Aug/13 22:42;cheolsoo;Got +1 from Daniel in RB. Committed to trunk.

Thank you Daniel for the review!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XMLLoader returns non-matching nodes when a tag name spans through the block boundary,PIG-3373,12656103,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aseldawy,aseldawy,aseldawy,03/Jul/13 19:46,07/Jul/14 18:07,14/Mar/19 03:07,04/May/14 17:08,site,,,,,,,0.13.0,,,,piggybank,,,0,patch,,,,,,,,,,,,"When node start tag spans two blocks this tag is returned even if it is not of the type.
Example: For the following input file

<event id=""3423"">
<ev
-------- BLOCK BOUNDARY
entually id=""dfasd"">

XMLoader with tag type 'event' should return only the first one but it actually returns both of them",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Jul/13 20:10;aseldawy;PIG3373.patch;https://issues.apache.org/jira/secure/attachment/12590708/PIG3373.patch,29/Jan/14 17:30;aseldawy;PIG3373_1.patch;https://issues.apache.org/jira/secure/attachment/12625902/PIG3373_1.patch,31/Jan/14 17:33;aseldawy;PIG3373_2.patch;https://issues.apache.org/jira/secure/attachment/12626331/PIG3373_2.patch,31/Jan/14 22:27;aseldawy;PIG3373_3.patch;https://issues.apache.org/jira/secure/attachment/12626394/PIG3373_3.patch,29/Jan/14 17:31;aseldawy;bad-file.xml.bz2;https://issues.apache.org/jira/secure/attachment/12625903/bad-file.xml.bz2,31/Jan/14 22:24;aseldawy;test-file-2.xml.bz2;https://issues.apache.org/jira/secure/attachment/12626392/test-file-2.xml.bz2,,,,,,6.0,,,,,,,,,,,,,,,,,,,2013-07-15 13:33:22.141,,,no_permission,,,,,,,,,,,,336378,,,,Sun May 04 12:14:37 UTC 2014,,,Patch Available,,,,0|i1m0t3:,336702,"I added a new patch that fixes this bug. It turned out that this bug happens only when the input file is .bz2 compressed and the non-matching tag spans two file splits in the compressed file. Since it's almost impossible to tailor an example that has this bug since the compression is virtually non-deterministic, I included a random generator that generates this test case.
I don't like the idea of discovering a bug using this randomly generated file since, by definition, it's non-deterministic, I attached the test file for reference.
The fix is still the same as the previous patch, but this time, the test fails without this fix.",,,,,,,,,03/Jul/13 19:53;aseldawy;I fixed this bug by allowing the reader to continue reading from the file after block boundary until the start tag is completely read and tested,"03/Jul/13 20:09;aseldawy;diff --git java/src/main/java/org/apache/pig/piggybank/storage/XMLLoader.java java/src/main/java/org/apache/pig/piggybank/storage/XMLLoader.java
index 7b1a75c..0909795 100644
--- java/src/main/java/org/apache/pig/piggybank/storage/XMLLoader.java
+++ java/src/main/java/org/apache/pig/piggybank/storage/XMLLoader.java
@@ -336,7 +336,7 @@ class XMLLoaderBufferedPositionedInputStream extends BufferedPositionedInputStre
           if (state == S_MATCH_TAG && (b == '>' || Character.isWhitespace(b))) {
             break;
           }
-          if (state != S_MATCH_TAG && this.getPosition() > limit) {
+          if (matchBuf.size() == 0 && this.getPosition() > limit) {
             // need to break, no record in this block
             break;
           }
diff --git java/src/test/java/org/apache/pig/piggybank/test/storage/TestXMLLoader.java java/src/test/java/org/apache/pig/piggybank/test/storage/TestXMLLoader.java
index e9abbc0..d908cde 100644
--- java/src/test/java/org/apache/pig/piggybank/test/storage/TestXMLLoader.java
+++ java/src/test/java/org/apache/pig/piggybank/test/storage/TestXMLLoader.java
@@ -17,6 +17,7 @@ import static org.apache.pig.ExecType.LOCAL;
 
 import java.io.ByteArrayInputStream;
 import java.io.File;
+import java.io.PrintStream;
 import java.util.ArrayList;
 import java.util.Iterator;
 
@@ -25,6 +26,8 @@ import javax.xml.parsers.DocumentBuilderFactory;
 
 import junit.framework.TestCase;
 
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
 import org.apache.pig.ExecType;
 import org.apache.pig.PigServer;
 import org.apache.pig.data.Tuple;
@@ -391,4 +394,54 @@ public class TestXMLLoader extends TestCase {
        }
      }
    }
+   
+   
+   public void testXMLLoaderShouldNotReturnLastNonMatchedTag() throws Exception {
+     long blockSize = FileSystem.get(new Configuration()).getDefaultBlockSize();
+     File tempFile = File.createTempFile(""bad-file"", "".xml"");
+     String one_k_line = ""<event>xxx</event>"";
+     int content_length = 1024 - one_k_line.length() + 3;
+     String content = String.format(""%""+content_length+""s"", ""content"");
+     one_k_line = one_k_line.replace(""xxx"", content);
+     
+     PrintStream ps = new PrintStream(tempFile);
+     long total_size = 0;
+     int number_of_correct_tags = 0;
+     while (total_size + one_k_line.length() < blockSize) {
+       ps.print(one_k_line);
+       total_size += one_k_line.length();
+       number_of_correct_tags++;
+     }
+     String last_tag = one_k_line.replace(""content"", ""cont"");
+     ps.print(last_tag);
+     number_of_correct_tags++;
+     total_size += last_tag.length();
+     String bad_content = ""<event-content-should-not-return-this/>"";
+     ps.print(bad_content);
+     total_size += bad_content.length();
+     while (total_size % blockSize < (blockSize / 2)) {
+       ps.print(bad_content);
+       total_size += bad_content.length();
+     }
+     ps.close();
+     
+     PigServer pig = new PigServer(LOCAL);
+     String tempFileName = tempFile.getAbsolutePath().replace(""\\"", ""\\\\"");
+     patternString = patternString.replace(""\\"", ""\\\\"");
+     String query = ""A = LOAD '"" + tempFileName + ""' USING org.apache.pig.piggybank.storage.XMLLoader('event') as (doc:chararray);"";
+     pig.registerQuery(query);
+     Iterator<?> it = pig.openIterator(""A"");
+     int tupleCount = 0;
+     while (it.hasNext()) {
+       Tuple tuple = (Tuple) it.next();
+       if (tuple == null)
+         break;
+       else {
+         if (tuple.size() > 0) {
+             tupleCount++;
+         }
+       }
+     }
+     assertEquals(number_of_correct_tags, tupleCount);  
+   }
 }","15/Jul/13 13:33;cheolsoo;[~aseldawy], thank you for the patch! Sorry for the late review.

I expect your test case to fail without your fix, but it passes. I ran ""ant clean test -Dtestcase=TestXMLLoader"".
{code}
    [junit] 2013-07-15 06:22:50.753 java[1499:1903] Unable to load realm mapping info from SCDynamicStore
    [junit] Tests run: 12, Failures: 0, Errors: 0, Time elapsed: 49.01 sec
{code}
What I am missing?
",17/Jul/13 04:56;cheolsoo;Canceling the patch waiting for response.,29/Jan/14 17:30;aseldawy;This new patch includes a test case that fails. The fix is still the same as the previous patch.,"29/Jan/14 17:31;aseldawy;This is an example of a file that fails. Currently, this file is generated in the patch. However, in the future, the way files are compressed might change causing this test to be invalid. This sample file is attached for reference.","31/Jan/14 01:55;daijy;[~aseldawy], seems the test still pass without your fix. Which version of Pig are you using?","31/Jan/14 17:33;aseldawy;This new patch uses a specific file rather than generating its own file. It turns out that generating a random file makes the test very fragile and sensitive to system and JRE.
For this test to run, please place the attached file (bad-file.xml.bz2) in piggybank/java and run the tests from there.","31/Jan/14 17:36;aseldawy;By the way, the attached file bad-file.xml.bz2 is the smallest file that can reveal this bug. To find this bug, we need a compressed XML file of at least two BZ2 blocks. The minimum block size of BZ2 is 100KB (by design).","31/Jan/14 20:15;daijy;I am fine to checkin a test data file. However, you will need to put it into a proper directory (eg contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/evaluation/xml/data) and make your test work with it.","31/Jan/14 22:24;aseldawy;Another, slightly smaller, test file that reveals this bug.",31/Jan/14 22:27;aseldawy;This patch uses test files in a test directory.,"02/May/14 19:29;alangates;Sorry, but the patch no longer applies and I couldn't figure out how apply it manually.","02/May/14 19:42;cheolsoo;Isn't this addressed as part of PIG-3865 (rewrite of XMLLoader)? If so, can we close this jira? ",04/May/14 12:14;aseldawy;Yes. This issue is already covered by PIG-3865. We can safely close it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unit test TestImplicitSplitOnTuple.testImplicitSplitterOnTuple failed when using hadoopversion=23,PIG-3369,12655666,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dreambird,dreambird,dreambird,01/Jul/13 22:32,14/Oct/13 16:46,14/Mar/19 03:07,13/Jul/13 01:46,0.12.0,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"in trunk, this test pass with hadoopversion 20, but when using -Dhadoopversion=23, it fail. The error message is
{noformat}
expected:<([1,1],1002)> but was:<([3,3],1002)>
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Jul/13 23:56;dreambird;PIG-3369.patch.txt;https://issues.apache.org/jira/secure/attachment/12592110/PIG-3369.patch.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-12 23:36:35.83,,,no_permission,,,,,,,,,,,,335941,,,,Sat Jul 13 01:46:47 UTC 2013,,,,,,,0|i1ly47:,336265,,,,,,,,,,"12/Jul/13 22:19;dreambird;turn out in hadoop23 profile, the test expects
{noformat}
assertEquals(""(1,1,1001)"", list.get(0).toString());
        assertEquals(""(3,3,1002)"", list.get(1).toString());
        assertEquals(""(3,2,1002)"", list.get(2).toString());
        assertEquals(""(3,1,1002)"", list.get(3).toString());
        assertEquals(""(2,3,1002)"", list.get(4).toString());
        assertEquals(""(2,2,1002)"", list.get(5).toString());
        assertEquals(""(2,1,1002)"", list.get(6).toString());
        assertEquals(""(1,3,1002)"", list.get(7).toString());
        assertEquals(""(1,2,1002)"", list.get(8).toString());
        assertEquals(""(1,1,1002)"", list.get(9).toString());
        assertEquals(""(2,2,1003)"", list.get(10).toString());
        assertEquals(""(2,1,1003)"", list.get(11).toString());
        assertEquals(""(1,2,1003)"", list.get(12).toString());
        assertEquals(""(1,1,1003)"", list.get(13).toString());
        assertEquals(""(1,1,1004)"", list.get(14).toString());
        assertEquals(""(3,3,1005)"", list.get(15).toString());
        assertEquals(""(3,2,1005)"", list.get(16).toString());
        assertEquals(""(2,3,1005)"", list.get(17).toString());
        assertEquals(""(2,2,1005)"", list.get(18).toString());
        assertEquals(""(3,3,1042)"", list.get(19).toString());
{noformat}

instead of
{noformat}
assertEquals(""(1,1,1001)"", list.get(0).toString());
        assertEquals(""(1,1,1002)"", list.get(1).toString());
        assertEquals(""(1,2,1002)"", list.get(2).toString());
        assertEquals(""(1,3,1002)"", list.get(3).toString());
        assertEquals(""(2,1,1002)"", list.get(4).toString());
        assertEquals(""(2,2,1002)"", list.get(5).toString());
        assertEquals(""(2,3,1002)"", list.get(6).toString());
        assertEquals(""(3,1,1002)"", list.get(7).toString());
        assertEquals(""(3,2,1002)"", list.get(8).toString());
        assertEquals(""(3,3,1002)"", list.get(9).toString());
        assertEquals(""(1,1,1003)"", list.get(10).toString());
        assertEquals(""(1,2,1003)"", list.get(11).toString());
        assertEquals(""(2,1,1003)"", list.get(12).toString());
        assertEquals(""(2,2,1003)"", list.get(13).toString());
        assertEquals(""(1,1,1004)"", list.get(14).toString());
        assertEquals(""(2,2,1005)"", list.get(15).toString());
        assertEquals(""(2,3,1005)"", list.get(16).toString());
        assertEquals(""(3,2,1005)"", list.get(17).toString());
        assertEquals(""(3,3,1005)"", list.get(18).toString());
        assertEquals(""(3,3,1042)"", list.get(19).toString());
{noformat}

the order is kind of reversed. The reason is because for same input (D1 and D2), below command generate different results
{code}
J = JOIN D1 By shopId, D2 by shopId;
{code}

still looking for the root cause","12/Jul/13 23:36;cheolsoo;[~dreambird], JOIN doesn't guarantee any kind of ordering, so this test is simply not well-written.

Looking at PIG-3310 which added this test, the intention is to verify unique schema ids are generated for every field of J. Can you add an ORDER BY before STORE so that the results are always consistent?

Thanks! ","12/Jul/13 23:56;dreambird;Thanks for the comments, Cheolsoo! I was looking at why JOIN get different result, but looks not necessary :)

I have verify the patch works for both hadoopversion.","13/Jul/13 00:31;cheolsoo;+1. I will commit it.
",13/Jul/13 01:46;cheolsoo;Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Case expression fails with an even number of when branches,PIG-3364,12654711,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,25/Jun/13 12:06,14/Oct/13 16:46,14/Mar/19 03:07,26/Jun/13 15:27,,,,,,,,0.12.0,,,,parser,,,0,,,,,,,,,,,,,"This is a bug introduced by PIG-3342 - Allow conditions in case statement.

If 1) there are an even number of when branches, and 2) there is the else branch, case statement with conditions fails with the following error:
{code}
Caused by: org.apache.pig.impl.logicalLayer.validators.TypeCheckerException: ERROR 1057: 
<line 2, column 14> LOGenerate expression plan can only have one output
{code}
This is because the logic of determining whether case statement has the else branch is incorrect. This doesn't happen when conditions are not used in case statement.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Jun/13 12:18;cheolsoo;PIG-3364.patch;https://issues.apache.org/jira/secure/attachment/12589594/PIG-3364.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-26 07:31:38.819,,,no_permission,,,,,,,,,,,,334988,,,,Wed Jun 26 15:27:58 UTC 2013,,,,,,,0|i1ls8v:,335312,,,,,,,,,,"25/Jun/13 12:18;cheolsoo;This is really one-line change:

{code}
-        boolean hasElse = exprs.size() \% 2 == 1;
+        boolean hasElse = exprs.size() != conds.size();
{code}
The exprs and conds are lists of LogicalExpressions in case statement (i.e. WHEN cond THEN expr ELSE expr).

The reason why this was not caught by the unit tests was because I only tested an odd number of when branches. So I also update the unit tests. Now they test 1) an even number of when branches w/ else, and 2) an odd number of when branches w/o else.",26/Jun/13 07:31;rohini;+!,26/Jun/13 15:27;cheolsoo;Committed to trunk. Thanks Rohini for reviewing it!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve Hadoop version detection logic for Pig unit test,PIG-3361,12654068,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,21/Jun/13 01:47,14/Oct/13 16:46,14/Mar/19 03:07,24/Jun/13 22:27,,,,,,,,0.12.0,,,,impl,,,0,,,,,,,,,,,,,Several unit test fail on Hadoop 2.1. This is the original Hadoop version detection logic is wrong.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Jun/13 01:49;daijy;PIG-3361-1.patch;https://issues.apache.org/jira/secure/attachment/12588986/PIG-3361-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-21 19:05:33.495,,,no_permission,,,,,,,,,,,,334345,Reviewed,,,Mon Jun 24 22:27:07 UTC 2013,,,,,,,0|i1lobr:,334671,,,,,,,,,,21/Jun/13 19:05;rohini;+1,24/Jun/13 22:27;daijy;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some intermittent negative e2e tests fail on hadoop 2,PIG-3360,12654067,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,21/Jun/13 01:43,14/Oct/13 16:45,14/Mar/19 03:07,24/Sep/13 23:51,,,,,,,,0.12.0,,,,impl,,,0,,,,,,,,,,,,,"One example is StreamingErrors_2. Here is the stack we get:
Backend error message
---------------------
Error: org.apache.pig.backend.executionengine.ExecException: ERROR 2055: Received Error while processing the map plan: 'perl PigStreamingBad.pl middle ' failed with exit status: 2
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:311)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:278)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:763)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:339)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:162)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1477)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)

Pig Stack Trace
---------------
ERROR 2244: Job failed, hadoop does not return any error message

org.apache.pig.backend.executionengine.ExecException: ERROR 2244: Job failed, hadoop does not return any error message
	at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:145)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:198)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:170)
	at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:84)
	at org.apache.pig.Main.run(Main.java:604)
	at org.apache.pig.Main.main(Main.java:157)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
================================================================================
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Jun/13 01:45;daijy;PIG-3360-1.patch;https://issues.apache.org/jira/secure/attachment/12588985/PIG-3360-1.patch,24/Sep/13 20:26;daijy;PIG-3360-2.patch;https://issues.apache.org/jira/secure/attachment/12604870/PIG-3360-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-09-24 23:46:35.173,,,no_permission,,,,,,,,,,,,334344,Reviewed,,,Tue Sep 24 23:51:24 UTC 2013,,,,,,,0|i1lobj:,334670,,,,,,,,,,21/Jun/13 01:45;daijy;Use report.getCurrentStatus() instead of report.getProgress() to get job status. I haven't check if report.getCurrentStatus() is supported by old Hadoop yet. So please hold on.,"24/Sep/13 20:26;daijy;The new patch put the logic in shims layer. The API TaskReport.getCurrentStatus is introduced in 0.21. However, we only see issues with the getProgress trick in Hadoop 2. So putting in the shims should be fine and avoid reflection which makes code ugly.","24/Sep/13 23:46;thejas;Looks good. +1
",24/Sep/13 23:51;daijy;Patch committed to both 0.12 branch and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnMapKeyPrune bug with distinct operator,PIG-3355,12652946,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jeremykarn,jeremykarn,jeremykarn,14/Jun/13 19:16,14/Oct/13 16:46,14/Mar/19 03:07,18/Jun/13 17:38,0.10.1,0.11.1,0.9.2,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,We came across a bug that happens when you have a distinct operator immediately followed by a union where the result of the union has at least one column that will be pruned by ColumnMapKeyPrune.  There's a test showing an example script in the submitted patch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Jun/13 19:16;jeremykarn;PIG-3355.patch;https://issues.apache.org/jira/secure/attachment/12587866/PIG-3355.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-18 04:48:19.805,,,no_permission,,,,,,,,,,,,333269,,,,Tue Aug 06 17:24:50 UTC 2013,,,,,,,0|i1lhp3:,333597,,,,,,,,,,"14/Jun/13 20:01;jeremykarn;I should also mention that this bug manifests itself in a couple of different ways.  The job generally crashes at some point
where the schema doesn't match the data tuple.  The most common exceptions we've seen are like:


java.lang.IndexOutOfBoundsException: Index: 2, Size: 2
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.pig.data.DefaultTuple.get(DefaultTuple.java:159)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage.getValueTuple(POPackage.java:341)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage.getNext(POPackage.java:264)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:416)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:407)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:261)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:566)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216)



2013-06-13 15:28:14,188   java.io.IOException: Type mismatch in key from map: expected org.apache.pig.impl.io.NullableText, recieved org.apache.pig.impl.io.NullableBytesWritable
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:845)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:541)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map.collect(PigGenericMapReduce.java:127)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:273)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:266)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.Child.main(Child.java:170)",18/Jun/13 04:48;aniket486;+1. Will commit if ant test-commit passes.,18/Jun/13 05:44;aniket486;Committed to trunk. Thanks Jeremy!,"18/Jun/13 15:11;knoguchi;bq. Committed to trunk. Thanks Jeremy!

[~aniket486], status is still ""Patch Available""?  
Also, can we patch 0.11 as well so that it'll be included if we release another 0.11.* ?","18/Jun/13 17:37;aniket486;[~knoguchi], Feel free to rebase and submit a patch for 0.11.",06/Aug/13 17:24;cheolsoo;Updating the fix version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDF example does not handle nulls,PIG-3354,12652574,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,patc888,patc888,patc888,13/Jun/13 06:33,14/Oct/13 16:46,14/Mar/19 03:07,24/Jun/13 21:50,,,,,,,,0.12.0,,,,documentation,,,0,,,,,,,,,,,,,"This bug is causing people to contribute built-in UDFs that don't properly handle null. 

In the file: src/docs/src/documentation/content/xdocs/udf.xml

9        if (input == null || input.size() == 0)

should be

9        if (input == null || input.size() == 0 || input.get(0) == null)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Jun/13 06:34;patc888;PIG-3354.patch;https://issues.apache.org/jira/secure/attachment/12587567/PIG-3354.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-24 21:50:44.272,,,no_permission,,,,,,,,,,,,332898,Reviewed,,,Mon Jun 24 21:50:44 UTC 2013,,,Patch Available,,,,0|i1lfen:,333226,,,,,,,,,,24/Jun/13 21:50;daijy;Patch committed to 0.12. Thanks Pat!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Document ToString(Datetime, String) UDF",PIG-3349,12651225,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,cheolsoo,patc888,patc888,05/Jun/13 22:33,14/Oct/13 16:46,14/Mar/19 03:07,30/Aug/13 23:01,0.11.1,,,,,,,0.12.0,,,,documentation,,,0,,,,,,,,,,,,,"Currently you can't cast a datetimeobject into a chararray:

grunt> B = foreach A generate (chararray)a; dump B;
2013-06-05 15:29:01,372 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1052: 
<line 8, column 24> Cannot cast datetime to chararray
Details at logfile: /Users/patc/projects/pig-0.11.1/pig_1370471270879.log

Was this an oversight? The documented casting matrix does not show the datetime object so I'm not sure if the current behavior is correct or not.

My recommendation would be to support casting to and from strings. Casting from a string would behave exactly like loading a datetime. Casting to a string would be exactly the format you get when you dump a datetime.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Aug/13 20:35;cheolsoo;PIG-3349.patch;https://issues.apache.org/jira/secure/attachment/12597165/PIG-3349.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-09 20:19:22.298,,,no_permission,,,,,,,,,,,,331551,,,,Fri Aug 30 23:01:29 UTC 2013,,,,,,,0|i1l75b:,331883,,,,,,,,,,"09/Aug/13 20:19;cheolsoo;In fact, there is a built-in UDF ToString(datetime, format_string), but it's not documented. I also noticed ToUnixTime(datetime) is not documented.",09/Aug/13 20:35;cheolsoo;Added ToString() to the built-in UDF section. ToUnixTime() is already documented in trunk.,29/Aug/13 22:36;daijy;+1. We also need to complete type conversion table later.,30/Aug/13 23:01;cheolsoo;Committed to trunk. Thank you Daniel for the review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Store invocation brings side effect,PIG-3347,12651130,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,daijy,serega_sheypak,serega_sheypak,05/Jun/13 15:05,15/Apr/14 20:44,14/Mar/19 03:07,08/Feb/14 18:28,0.11,,,,,,,0.12.1,,,,grunt,,,0,,,,,,,,,,,,,"The problem is that intermediate 'store' invocation ""changes"" the final store output. Looks like it brings some kind of side effect. We did use 'local' mode to run script
here is the input data:
1
1
Here is the script:
{code}
a = load 'test';

a_group = group a by $0;
b = foreach a_group {
  a_distinct = distinct a.$0;
  generate group, a_distinct;
}
--store b into 'b';
c = filter b by SIZE(a_distinct) == 1;
store c into 'out';
{code}
We expect output to be:
1 1
The output is empty file.

Uncomment {code}--store b into 'b';{code} line and see the diffrence.
Yuo would get expected output.
",local mode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30/Sep/13 23:33;daijy;PIG-3347-1.patch;https://issues.apache.org/jira/secure/attachment/12605999/PIG-3347-1.patch,31/Jan/14 23:31;knoguchi;PIG-3347-2-testonly.patch;https://issues.apache.org/jira/secure/attachment/12626420/PIG-3347-2-testonly.patch,03/Feb/14 21:32;daijy;PIG-3347-3.patch;https://issues.apache.org/jira/secure/attachment/12626740/PIG-3347-3.patch,04/Feb/14 17:46;knoguchi;PIG-3347-4-testonly.patch;https://issues.apache.org/jira/secure/attachment/12626909/PIG-3347-4-testonly.patch,04/Feb/14 18:53;daijy;PIG-3347-5.patch;https://issues.apache.org/jira/secure/attachment/12626934/PIG-3347-5.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2013-06-24 21:04:46.077,,,no_permission,,,,,,,,,,,,331456,Reviewed,,,Sat Feb 08 18:28:37 UTC 2014,,,,,,,0|i1l6k7:,331788,,,,,,,,,,"24/Jun/13 21:04;daijy;That's the incorrect PushUpFilter. Can be solved by disable PushUpFilter rule:
pig -t PushUpFilter -x local xxx.pig

Look at the logical plan:
{code}
c: (Name: LOStore Schema: group#28:bytearray,a_distinct#29:bag{#30:tuple(#31:bytearray)})
|
|---b: (Name: LOForEach Schema: group#28:bytearray,a_distinct#29:bag{#30:tuple(#31:bytearray)})
    |   |
    |   (Name: LOGenerate[false,false] Schema: group#28:bytearray,a_distinct#29:bag{#30:tuple(#31:bytearray)})ColumnPrune:InputUids=[29, 28]ColumnPrune:OutputUids=[29, 28]
    |   |   |
    |   |   group:(Name: Project Type: bytearray Uid: 28 Input: 0 Column: (*))
    |   |   |
    |   |   a_distinct:(Name: Project Type: bag Uid: 29 Input: 1 Column: (*))
    |   |
    |   |---(Name: LOInnerLoad[0] Schema: group#28:bytearray)
    |   |
    |   |---a_distinct: (Name: LODistinct Schema: #31:bytearray)
    |       |
    |       |---1-7: (Name: LOForEach Schema: #31:bytearray)
    |           |   |
    |           |   (Name: LOGenerate[false] Schema: #31:bytearray)
    |           |   |   |
    |           |   |   (Name: Project Type: bytearray Uid: 31 Input: 0 Column: (*))
    |           |   |
    |           |   |---(Name: LOInnerLoad[0] Schema: #31:bytearray)
    |           |
    |           |---a: (Name: LOInnerLoad[1] Schema: null)
    |
    |---c: (Name: LOFilter Schema: group#28:bytearray,a#29:bag{#36:tuple()})
        |   |
        |   (Name: Equal Type: boolean Uid: 35)
        |   |
        |   |---(Name: UserFunc(org.apache.pig.builtin.BagSize) Type: long Uid: 32)
        |   |   |
        |   |   |---a:(Name: Project Type: bag Uid: 29 Input: 0 Column: 1)
        |   |
        |   |---(Name: Cast Type: long Uid: 33)
        |       |
        |       |---(Name: Constant Type: int Uid: 33)
        |
        |---a_group: (Name: LOCogroup Schema: group#28:bytearray,a#29:bag{#36:tuple()})
            |   |
            |   (Name: Project Type: bytearray Uid: 28 Input: 0 Column: 0)
            |
            |---a: (Name: LOLoad Schema: null)RequiredFields:null
{code}

Filter is pushed in front of foreach, which is wrong.","08/Nov/13 15:03;knoguchi;This is a bad bug producing incorrect outputs.  Taking out ""in local mode"" from summary, since this happens on mapreduce mode too.","08/Nov/13 16:05;knoguchi;I wish there was a wiki/document describing how UID should be assigned.  Even after going through PIG-3492, I'm still lost on when exactly we should assign new UIDs.  
My current understanding(or guess) is, UID represents an uniqueness within a record.  
Just by looking at the UIDs from the two separate relations(bags), we can tell if the fields were altered or not.  (Although we cannot tell if the tuples were filtered or not.)

FilterAboveForeach(PushUpFilter) is using this property to determine if FILTER can be moved before the foreach.  Bug here is, nested distinct is not assigning a new UID for the bag it creates so FilterAboveForeach mistakenly thinks that no fields were altered within the foreach and decides to move this filter upfront.
Following show the schema BEFORE calling PushUpFilter/FilterAboveForeach without and with Daniel's patch.  We can see that after applying the patch, relation 'c' and 'a_group' contain different UIDs for the bag.
{noformat}
(without the patch)
|---c: (Name: LOFilter Schema: group#11:bytearray,a_distinct#12:bag{#13:tuple(#14:bytearray)})
    |---b: (Name: LOForEach Schema: group#11:bytearray,a_distinct#12:bag{#13:tuple(#14:bytearray)})
        |---a_group: (Name: LOCogroup Schema: group#11:bytearray,a#12:bag{#13:tuple()})
(with the patch)
|---c: (Name: LOFilter Schema: group#15:bytearray,a_distinct#20:bag{#19:tuple(#18:bytearray)})
    |---b: (Name: LOForEach Schema: group#15:bytearray,a_distinct#20:bag{#19:tuple(#18:bytearray)})
        |---a_group: (Name: LOCogroup Schema: group#15:bytearray,a#16:bag{#17:tuple()})
{noformat}

So I think the patch fixes the bug described on the jira nicely.  However, question remains for other nested operations. I believe the same bug can appear for nested LIMIT and nested FILTER.  For example,

{noformat}
a = load 'test.txt';    
a_group = group a by $0;
b = foreach a_group {
  a_limit = limit a.$0 5;
  generate group, a_limit;
}
c = filter b by SIZE(a_limit) == 5;
store c into 'out';
{noformat}

{noformat}
a = load 'test3.txt' as (a0, a1);     
a_group = group a by a0;
b = foreach a_group {
  newA = filter a by a1 == 2;
  generate group, newA;
}
c = filter b by SIZE(newA) == 5;
store c into 'out';
{noformat}

I confirmed these two examples also mistakenly push the filter before foreach and produce empty results.  Former case, nested LIMIT, is actually covered with the current patch since nested LIMIT uses LOLIMIT+LOForeach.  So the patch 
{noformat}
+  98      // If it is nested foreach or nested distinct, generate new uid
+  99      if (op instanceof LOForEach || op instanceof LODistinct) {
+ 100         needNewUid = true;
+ 101     }
{noformat} 
takes care of nested limit although comment doesn't mention it.   Nested filter is not the case here and the bug still exists after the current patch.  Can we cover this case as well? 
","30/Jan/14 21:49;dvryaboy;Yikes.

[~aniket486] & [~julienledem] this seems like a critical bug to look at. Julien, you investigated this UID situation before, right?","30/Jan/14 22:44;julienledem;I thought that the field UIDs were used to track lineage across the plan.
[~aniket486] correct me if I'm wrong but it is used to determine which fields are reads for projection push down.
In the case of self join (directly or indirectly) we end up with duplicate ids in the same relation because the same field is derived to 2 different fields.
Otherwise I'm as lost as [~knoguchi] regarding the actual mechanisms around the UID.
I tried to fix some of these in the past (PIG-3020) but it appears they created more problems (PIG-3492)
[~daijy] maybe you can enlighten us?","30/Jan/14 23:22;daijy;UID is to track column lineage so in logical optimizer, so that we can freely move operate up and down, ProjectionPatcher will reposition the column according to uid, even if the column get reordered. A new source of data should have a new UID, that's the case for nested LOForEach/LODistinct, since they are not directly derived from the previous operator, instead, it is a new field generated by the foreach.","31/Jan/14 23:31;knoguchi;[~daijy], I think your testcase in the patch succeeds even without your patch since it's missing FilterAboveForeach optimization.

Uploading a test-only patch with this change.  Added 2 more test cases for nested limit and nested filter. 

Nested distinct and nested limit are fixed with your current patch.  Nested filter still fails.","03/Feb/14 21:32;daijy;Thanks [~knoguchi]. I attached another patch include your test case, plus the fix for LOFilter/LOLimit, etc. Will run unit tests.",04/Feb/14 17:45;daijy;All unit tests pass with the patch.,"04/Feb/14 17:46;knoguchi;Thanks [~daijy].  
Adding one more testcase that I believe should push the filter before foreach.
This one succeeds without the patch but fails with the patch.","04/Feb/14 18:07;knoguchi;bq. UID is to track column lineage so in logical optimizer, so that we can freely move operate up and down,  ProjectionPatcher will reposition the column according to uid

I think part of my confusion comes from these two.  UID is used for (1) tracking column lineage.  (2) UID is also used for ProjectionPatcher to reposition therefore requiring UID to be unique within each relation.

Because of (2), we're seeing new uid being created whenever column is referenced multiple times.
Like 
A = load 'a.txt' as (a:int);
B = foreach A generate a as col1, a as col2; 

This would create a schema like 
{noformat}
1-2: (Name: LOStore Schema: col1#1:int,col2#2:int)
...
    |---A: (Name: LOLoad Schema: a#1:int)RequiredFields:null
{noformat}

So without traversing the lineage, I cannot connect 'col2' to original 'a'.
However, optimizer like PushUpFilter&FilterAboveForeach seems to be using just UID to determine the field usages...

But this is outside of this jira.  I need to spend more time learning how the pig compiler works.",04/Feb/14 18:53;daijy;Attach another patch which also address Koji's new case.,"04/Feb/14 19:02;daijy;[~knoguchi], in the ""B = foreach A generate a as col1, a as col2; "", we will need to generate a new uid for col2 to avoid uid conflict (using a UDF IdentityColumn). The downside is this will break the lineage chain. The uid is mostly used in optimizer, there several holes when we use it for pure lineage. Optimizer rules is expected to live with these holes by skip optimize (eg, PushUpFilter is skip the foreach with UDF, which include IdentityColumn aiming to fix the uid conflict)","04/Feb/14 20:06;knoguchi;bq. we will need to generate a new uid for col2 to avoid uid conflict (using a UDF IdentityColumn)

Daniel, I think I understand how it is being used, but my confusion is: for the pure purpose of tracking column lineage, shouldn't the redundant uid inside the relation be allowed?  Isn't the requirement of no-conflict-uid coming from using the same uid for ProjectionPatcher which serves a different purpose than the lineage tracking?","04/Feb/14 20:42;daijy;It could but introduce a lot of complications. Currently only LOForEach/LOSplitOutput is dealing with dup-uid, otherwise it will sprawl to all operators and all optimizer rules.","07/Feb/14 22:27;knoguchi;bq. It could but introduce a lot of complications. Currently only LOForEach/LOSplitOutput is dealing with dup-uid, otherwise it will sprawl to all operators and all optimizer rules.

Thanks Daniel.  This helps me understand why I always get confused on this.  Maybe someday I can separate the two.

As for your latest patch(PIG-3347-5.patch), it passed the unit tests(including mine) and e2e was fine also.  I'm +1.",08/Feb/14 18:28;daijy;Patch committed to trunk and 0.12 branch. Thanks Koji for review and test!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle null in DateTime functions,PIG-3345,12650749,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,03/Jun/13 21:56,14/Oct/13 16:46,14/Mar/19 03:07,04/Jun/13 13:29,0.11.1,,,,,,,0.11.2,0.12.0,,,,,,0,,,,,,,,,,,,, NPE is thrown in date time functions when a null value is passed. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Jun/13 22:02;rohini;PIG-3345-1.patch;https://issues.apache.org/jira/secure/attachment/12585965/PIG-3345-1.patch,04/Jun/13 04:25;rohini;PIG-3345-2.patch;https://issues.apache.org/jira/secure/attachment/12586041/PIG-3345-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-06-03 23:18:21.951,,,no_permission,,,,,,,,,,,,331076,,,,Wed Jun 05 17:13:17 UTC 2013,,,,,,,0|i1l47z:,331409,,,,,,,,,,"03/Jun/13 23:18;prkommireddi;Hi [~rohini], patch looks good. Would you like to add tests for ToDate* functions too (under testConversionBetweenDateTimeAndString())? ",04/Jun/13 04:25;rohini;Thanks Prashant. Added for all the udfs in testConversionBetweenDTAndString,"04/Jun/13 05:18;prkommireddi;LGTM +1

Thanks Rohini!",04/Jun/13 13:29;rohini;Committed to trunk (0.12). Thanks Prashant.,05/Jun/13 16:54;rohini;I started with this fix because we saw GetYear throw NPE. But found that TODate() returns current time when null value is passed before this fix which is incorrect. So will be porting this to 0.11.2 also. ,05/Jun/13 17:13;rohini;Committed to branch-0.11 (0.11.2) also,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestErrorHandling.tesNegative7 fails on MR2,PIG-3335,12648737,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,21/May/13 21:13,14/Oct/13 16:46,14/Mar/19 03:07,24/May/13 22:14,0.11,,,,,,,0.12.0,,,,grunt,,,0,,,,,,,,,,,,,"This test case fails when being tested with MR2:

junit.framework.AssertionFailedError
at org.apache.pig.parser.TestErrorHandling.tesNegative7(TestErrorHandling.java:138)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/May/13 21:45;xuefuz;PIG-3335.patch;https://issues.apache.org/jira/secure/attachment/12584177/PIG-3335.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-24 21:04:17.837,,,no_permission,,,,,,,,,,,,329067,,,,Wed May 29 18:34:03 UTC 2013,,,,,,,0|i1krwf:,329409,,,,,,,,,,"21/May/13 21:20;xuefuz;Problem found is the change in the implementation of FileOutputFormat.setOutputPath() in MR2, where a runtime exception is thrown, which gives back a different error message:

2013-05-21 14:19:50,855 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1002: Unable to store alias A
",21/May/13 21:45;xuefuz;Change exception handling so that the runtime exception is caught. Now the test case passes on both MR1 and MR2.,24/May/13 21:04;daijy;+1,"24/May/13 22:13;xuefuz;Thanks, Daniel.

Patch is committed to trunk.","29/May/13 14:37;rohini;[~xuefuz],
   This patch also seems to have gone into trunk (0.12) but marked as 0.11.2. CHANGES.txt also does not have the information. Can you please take a look at all the jira's that you have fixed and update the Fix Version and also update the CHANGES.txt.  ",29/May/13 18:34;xuefuz;CHANGES.txt is updated. Thanks to Rohini for pointing it out.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default values not stored in avro file when using specific schemas during store in AvroStorage,PIG-3331,12648511,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,viraj,viraj,viraj,20/May/13 21:45,14/Oct/13 16:46,14/Mar/19 03:07,05/Jun/13 14:50,0.11.1,,,,,,,0.12.0,,,,piggybank,,,0,,,,,,,,,,,,,"Script which stores Avro using a predefined schema does not store the default values in the file
{code}
a = LOAD 'numbers.txt' USING PigStorage (':') as (intnum1000: int,id:
int,intnum5: int,intnum100: int,intnum: int,longnum: long,floatnum:
float,doublenum: double);

b2 = foreach a generate id, intnum5, intnum100;

c2 = filter b2 by 110 <= id and id < 120;

STORE c2 INTO '/tmp/TestAvroStorage/testDefaultValueWrite' USING
org.apache.pig.piggybank.storage.avro.AvroStorage (' { ""debug"" : 5, ""schema"" :
{  ""name"" : ""rmyrecord"", ""type"" : ""record"",  ""fields"" : [ { ""name"" : ""id"",
""type"" : ""int"" , ""default"" : 0 }, {  ""name"" : ""intnum5"",  ""type"" : ""int"",
""default"" : 0 }, { ""name"" : ""intnum100"", ""type"" : ""int"", ""default"" : 0 } ] } }
');
{code}

Opening the file shows the following schema
{noformat} 
avro.schema
{""type"":""record"",""name"":""rmyrecord"",""fields"":[{""name"":""id"",""type"":""int""},{""name"":""intnum5"",""type"":""int""},{""name"":""intnum100"",""type"":""int""}]}
{noformat} 

There seems to be a problem storing the schema.
Viraj",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Jun/13 23:24;viraj;PIG-3331_1.patch;https://issues.apache.org/jira/secure/attachment/12586221/PIG-3331_1.patch,04/Jun/13 21:35;viraj;expected_DefaultSchemaWrite.avro;https://issues.apache.org/jira/secure/attachment/12586182/expected_DefaultSchemaWrite.avro,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-05-21 04:29:15.571,,,no_permission,,,,,,,,,,,,328866,,,,Wed Jun 05 14:50:20 UTC 2013,,,,,,,0|i1kqnz:,329208,,,,,,,,,,"21/May/13 04:29;scott_carey;It is possible that the problem is in reading the schema or in other places in between the Load and Store.  

Ideally this process would preserve defaults as well as ""doc"" and arbitrary user properties.",23/May/13 21:28;viraj;ExpectedAvro file with Default Schema,23/May/13 21:29;viraj;Input text file with numbers,"23/May/13 21:43;viraj;Patch posted on the review board.
https://reviews.apache.org/r/11355/
Viraj",02/Jun/13 20:58;cheolsoo;I provided some comments in the RB.,04/Jun/13 21:35;viraj;Expected Avro file,04/Jun/13 21:36;viraj;Updated Pig patch,04/Jun/13 21:53;viraj;Latest patch,04/Jun/13 23:24;viraj;Updated patch,05/Jun/13 14:50;rohini;Committed to trunk (0.12). Thanks Viraj and Cheolsoo.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RANK operator failed when working with SPLIT ,PIG-3329,12648108,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,xalan,redisliu,redisliu,17/May/13 03:44,14/Oct/13 16:46,14/Mar/19 03:07,09/Jun/13 22:13,0.11.1,,,,,,,0.11.2,0.12.0,,,,,,0,,,,,,,,,,,,,"input.txt:
1 2 3
4 5 6
7 8 9

script:
a = load 'input.txt' using PigStorage(' ') as (a:int, b:int, c:int);
SPLIT a into b if a > 0, c if a > 5;
d = RANK b;
dump d;

job will fail with error message:
java.lang.RuntimeException: Unable to read counter pig.counters.counter_4929375455335572575_-1
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PORank.addRank(PORank.java:161)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PORank.getNext(PORank.java:134)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:308)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.getNext(POSplit.java:214)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:283)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:278)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:157)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:673)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:324)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:275)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1340)
	at org.apache.hadoop.mapred.Child.main(Child.java:269)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Jun/13 22:12;cheolsoo;PIG-3329-branch-11.patch;https://issues.apache.org/jira/secure/attachment/12586993/PIG-3329-branch-11.patch,09/Jun/13 22:12;cheolsoo;PIG-3329-trunk.patch;https://issues.apache.org/jira/secure/attachment/12586994/PIG-3329-trunk.patch,05/Jun/13 15:11;xalan;patch01;https://issues.apache.org/jira/secure/attachment/12586331/patch01,09/Jun/13 18:15;xalan;patch02;https://issues.apache.org/jira/secure/attachment/12586960/patch02,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-05-23 17:41:53.012,,,no_permission,,,,,,,,,,,,328464,,,,Sun Jun 09 22:12:43 UTC 2013,,,,,,,0|i1ko73:,328808,,,,,,,,,,23/May/13 11:06;redisliu;Anybody working on this?,"23/May/13 17:41;dreambird;Redis, I actually can reproduce the issue with queries
{noformat}
d = load 'input1' as (a:int, b:int, c:int);
SPLIT d into e if a > 0, f if a < 0;
g = RANK e;
{noformat}

the error I got is from JobControlCompiler, it complains missing OperationID in MROperPlan. I guess the reason is when compile from physical plan to MROperPlan, the operators order is wrong, but not sure. I don't have a fix yet....

The workaround for now is using FILTER BY
{noformat}
d = load 'input1' as (a:int, b:int, c:int);
e = filter d by a > 0;
g = RANK e;
{noformat}

I post comment when I have new find. Thanks.","23/May/13 17:43;dreambird;The issue doesn't happen when using RANK BY, so another workaround is using ""g = RANK e BY a;"". ","23/May/13 18:44;xalan;Hi to everybody,

I'm working on this, Redis. In the meanwhile, a workaround is through a FILTER BY and then a RANK operator. 

In fact, Johnny, the RANK BY provides a ranking by considering the values of column a. Instead, RANK assigns a sequential number to each tuple. ","23/May/13 19:35;mwagner;Allan/Johnny: I took a look at this and got an exception using a filter also:
{noformat}
Caused by: java.lang.RuntimeException: Error to read counters into Rank operation counterSize 0
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.saveCounters(JobControlCompiler.java:386)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.updateMROpPlan(JobControlCompiler.java:332)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:380)
        at org.apache.pig.PigServer.launchPlan(PigServer.java:1322)
        ... 11 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.mapred.Counters.getShortName(Counters.java:669)
        at org.apache.hadoop.mapred.Counters.getGroup(Counters.java:405)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.saveCounters(JobControlCompiler.java:360)
        ... 14 more
{noformat}","23/May/13 20:21;xalan;Hi Mark,

Could you post your script?

I guess, the output of FILTER is empty. 
","23/May/13 20:40;mwagner;Ah, my load was broken. The workaround seems right in that case. Thanks, Allan","29/May/13 07:22;dreambird;[~xalan], thanks for your comments.
""the RANK BY provides a ranking by considering the values of column a. Instead, RANK assigns a sequential number to each tuple.""
does this refers to LogToPhyTranslationVisitor.visit(LORank loRank) function ?","04/Jun/13 01:54;dreambird;[~xalan], are you working this right now? I got the similar exception when I was working on another patch, so it will be very nice if I can understand how will you resolve this issue. Thanks a lot!","05/Jun/13 09:46;xalan;Hi Johnny, 

As I understood of your question: ""does this refers to LogToPhyTranslationVisitor.visit(LORank loRank) function ?""
Yes, at LogToPhyTranslationVisitor is differentiated the functionality of RANK BY and RANK. On simple words, this is how it works:
RANK: Counter + Rank
RANK BY: Group + ForEach + Sort + Counter + Rank + ForEach 

And yes, I'm working on it. I guess this weekend I'll publish a patch for it. ","05/Jun/13 15:11;xalan;According to my local tests, it's working fine. I'll proceed with nightly tests on this week. ",08/Jun/13 05:52;redisliu;Thanks Allan ! the patch works fine for me too.,"08/Jun/13 07:00;xalan;Yesterday, it passed e2e tests on a local machine, I hadn't tried it on a cluster, yet.","09/Jun/13 02:43;cheolsoo;Hi Allan, thank you for the patch. I can run e2e tests on cluster. Let me also run unit tests.",09/Jun/13 06:13;xalan;Thanks Cheolsoo for testing it! I tested it before with unit tests and it passed them.,"09/Jun/13 18:03;cheolsoo;All the Rank-related e2e test cases (Rank_1..10) pass on cluster too. I will commit it soon.

+1. ","09/Jun/13 18:15;xalan;Thanks again Cheolsoo! Yesterday, I created an specific test for it, included on this patch. ","09/Jun/13 22:12;cheolsoo;I committed the patch to both trunk and branch-0.11.

I fixed CHANGES.txt while doing so, so I am attaching the diff files for the record.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataBags created with an initial list of tuples don't get registered as spillable,PIG-3328,12647851,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,mwagner,mwagner,mwagner,15/May/13 22:53,14/Oct/13 16:46,14/Mar/19 03:07,24/May/13 21:41,0.11.1,0.11.2,0.12.0,,,,,0.11.2,0.12.0,,,,,,0,,,,,,,,,,,,,"DefaultDataBag has a constructor to take ownership of an existing list of tuples as its own contents, but registration for spilling only occurs when adding elements. If a bag starts out big enough to consider spilling, but no new tuples are added to it, it will never be spilled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/May/13 22:57;mwagner;PIG-3328.1.patch;https://issues.apache.org/jira/secure/attachment/12583394/PIG-3328.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-24 21:41:00.858,,,no_permission,,,,,,,,,,,,328207,Reviewed,,,Fri May 24 21:41:00 UTC 2013,,,,,,,0|i1kmlz:,328551,,,,,,,,,,15/May/13 22:57;mwagner;Small patch. 'ant test-commit' has passed.,24/May/13 21:41;daijy;+1. Patch committed to trunk. Thanks Mark!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig hits OOM when fetching task Reports,PIG-3327,12647830,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,15/May/13 21:22,14/Oct/13 16:45,14/Mar/19 03:07,03/Jun/13 14:51,0.10.1,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,java.lang.OutOfMemoryError: GC overhead limit exceeded is hit with hadoop 23 by the pig script when a launched job has 80K+ maps. The TaskReport[] array is causing OOM. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Jun/13 19:09;rohini;PIG-3327-1.patch;https://issues.apache.org/jira/secure/attachment/12585776/PIG-3327-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-02 19:37:02.911,,,no_permission,,,,,,,,,,,,328186,,,,Mon Jun 03 14:51:20 UTC 2013,,,,,,,0|i1kmhb:,328530,,,,,,,,,,02/Jun/13 19:37;cheolsoo;+1.,03/Jun/13 14:51;rohini;Committed to trunk (0.12). Thanks Cheolsoo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding a tuple to a bag is slow,PIG-3325,12647641,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,dvryaboy,mwagner,mwagner,15/May/13 00:07,15/Apr/14 20:45,14/Mar/19 03:07,22/Oct/13 22:45,0.11,0.11.1,0.11.2,0.12.0,,,,0.12.1,,,,,,,0,,,,,,,,,,,,,"The time it takes to add a tuple to a bag has increased significantly, causing some jobs to take about 50x longer compared to 0.10.1. I've tracked this down to PIG-2923, which has made adding a tuple heavier weight (it now includes some memory estimation).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Oct/13 06:37;aniket486;PIG-3325-benchmark.patch;https://issues.apache.org/jira/secure/attachment/12609298/PIG-3325-benchmark.patch,30/Jun/13 00:26;dvryaboy;PIG-3325.2.patch;https://issues.apache.org/jira/secure/attachment/12590190/PIG-3325.2.patch,30/Jun/13 00:42;dvryaboy;PIG-3325.3.patch;https://issues.apache.org/jira/secure/attachment/12590191/PIG-3325.3.patch,20/Oct/13 06:43;aniket486;PIG-3325.4.patch;https://issues.apache.org/jira/secure/attachment/12609299/PIG-3325.4.patch,15/May/13 00:10;mwagner;PIG-3325.demo.patch;https://issues.apache.org/jira/secure/attachment/12583250/PIG-3325.demo.patch,15/May/13 00:39;mwagner;PIG-3325.optimize.1.patch;https://issues.apache.org/jira/secure/attachment/12583257/PIG-3325.optimize.1.patch,,,,,,6.0,,,,,,,,,,,,,,,,,,,2013-05-24 22:18:48.483,,,no_permission,,,,,,,,,,,,327997,,,,Tue Oct 22 22:45:46 UTC 2013,,,,,,,0|i1klbb:,328341,,,,,,,,,,"15/May/13 00:10;mwagner;This patch demonstrates the issue. On version 0.10.1, the average time per add(Tuple) call is 360 ns. On trunk it typically takes around 40000 ns.","15/May/13 00:39;mwagner;The core issue is that getMemorySize() is O(N) if a new element has been added since the last call. I've made that case O(1). However, this patch only brings the call time for adding tuples to ~4500 ns (and job time is still 2x that of 0.10.1).

[~dvryaboy], can you share some of your experience with the issue you saw for PIG-2923? I have a nice test job for this issue but I don't have any benchmark really for bag spilling performance, so I'm not sure how big of an issue small bags were for spilling, or what a good tradeoff between add() speed and spill speed would be.
",24/May/13 22:18;daijy;Looks good. We can further do memory calculation after N tuple instead every tuple. ,"10/Jun/13 20:22;dvryaboy;[~mwagner] thanks for catching this perf regression.
I only had time for a cursory look today -- why is the existing code O(N)? Seems like it sampled up to 100 elements and no more, so it's constant (once n>=100). Seems to me like all that materially changed was that you added the sampling bit to add(). Unfortunately, a number of Bags override add() (see my notes in PIG-2923), which makes doing this in the default add() of the abstract function unreliable.

Seems to me like a better approach would be to tackle the fact that for every time that getMemorySize() is called while there are fewer than 100 elements, we iterate over the whole bag (which is what you mean by O(N)?). We can do this by jumping directly to the mLastContentsSize'th element in the Bag, if we know the structure, or at least iterate to it without calling getMemorySize(), and then add to our running avg, rather than recomputing it. So, no resetting aggSampleTupleSize in your version, or avgTupleSize in mine, to 0 when sampling, just ignoring the first mLastContentsSize in the iterator.

Thoughts?

","13/Jun/13 18:49;rohini;Dmitriy,

bq. at least iterate to it without calling getMemorySize(), and then add to our running avg, rather than recomputing it.
   Still does not help. It is around 5-6000 ns. However we try, I don't think it is going to come back to ~400ns unless we revert back to relying on the SpillableManager doing the memory size computation. Looking at the SpillableManager code, if GC has happened normally clearSpillables(); would take care of removing smaller bags. 

{noformat}
       if (toBeFreed < spillFileSizeThreshold) {
                    log.debug(""spilling small files - getting out of memory handler"");
                    break ;
                }
{noformat}
  With the default spillFileSizeThreshold at 5MB, we don't attempt spill at all of smaller objects. So going back to Mark's question, how big of an issue small bags were for spilling and do we need the markSpillableIfNecessary() at all?

 One thing I can see that can speed up spills is moving the getMemorySize call out of the compare in Collections.sort and having a composite Spillable that has the memory size reset in the beginning and calculated only once during the run. ","17/Jun/13 19:26;mwagner;Thanks for taking a look, Dmitriy. I agree that doing work in add() is the wrong way to go. I don't think there's a way to get the time back down to 400 ns while still having lazy registration, but that may be okay if it prevents bad behavior elsewhere.

I'll try out caching the memory sizes during sorting and see how things improve. That should improve performance no matter how 'spillables' gets populated.","17/Jun/13 22:31;dvryaboy;The previous behavior (having SMM check all bags) was pretty bad, it caused significant sudden delays if the data you were loading had bags in it. We observed pretty good speed gains for those use cases once we got rid of mandatory bag registration. Also got rid of a few memory leaks while we were in there, and the linked list maintenance overhead in SMM.","17/Jun/13 22:43;rohini;Mark,
  I already have a patch that does initialization of memory sizes only once and removes the markSpillableIfNecessary during addTuple. Will put it up by tomorrow after running some e2e tests for bag spilling. 

Dmitriy,
  Moving the getMemorySize out of the compare method should give a significant gain for the case that you were seeing. I will post some numbers after running some tests.","17/Jun/13 23:06;mwagner;Cool. Thanks, Rohini!

Dmitriy, what was your LoadFunc? I found that if the LoadFunc creates bags with an initial list of elements, then the slowness doesn't show up until later when new bags are created (during a DISTINCT in my case).","20/Jun/13 06:16;dvryaboy;[~mwagner] I was loading complex thrift structures that had bags in them. With old code (all bags register with SMM) this led to tons of weak references that needed to be cleaned out by the SMM; new code fixed that, but apparently created this other problem (which in practice on our workloads is not significant.. but your workloads may be different). Looking forward to Rohini's patch.","20/Jun/13 17:01;rohini;{noformat} 

if (avgTupleSize != 0 && (mLastContentsSize == numInMem ||
                    mLastContentsSize > 100 && numInMem > 100))
                return totalSizeFromAvgTupleSize(avgTupleSize, numInMem);

{noformat}

  Actually I was wrong. Initializing memory size only once does not help that much. It only saves on the call to totalSizeFromAvgTupleSize(avgTupleSize, numInMem).  When getMemorySize() is called multiple times from the Comparator, the second time it hits mLastContentsSize == numInMem and returns totalSizeFromAvgTupleSize() directly without iterating through tuples again. 

Still trying to figure out a solution to optimize spilling. Wondering if splitting into two lists one for bigger sizes and one for < spillFileSizeThreshold after the first spill pass and sorting/iterating through them separately will help in future invocations.  ","20/Jun/13 18:07;dvryaboy;What if instead of figuring out size based on the first 100 elements, we sampled first, 11th, 21st, etc until we get 100 samples? Would help with small bags (where accuracy of estimate doesn't matter as much).","29/Jun/13 22:34;dvryaboy;Ok I started looking at this, will update with a patch shortly. In the meantime -- my benchmark shows Mark's patch improves perf on small bags of 20-100 elements, but causes extremely poor performance for large bags.

I created a benchmark that does 100 rounds of creating a bag of N elements, for values of N in [1,20,100,1000]. These sets of 100 rounds are run 15 times each, performance of the first 5 is thrown out to account for system warmup / jit optimizations.

Results:
||Num Tuples in Bag || Trunk avg || Patch 1 avg ||
| 1 | round: 0.00 | round: 0.00 |
| 20 | round: 0.01 | round: 0.00 |
| 100 | round: 0.13 | round: 0.00 |
| 1000 | round: 0.19 | round: 1.20 |
","30/Jun/13 00:26;dvryaboy;Updating with a patch.

Results:
||Num Tuples in Bag || Trunk avg || Patch 1 avg || Patch 2 avg ||
| 1 | round: 0.00 | round: 0.00 | round: 0.00 |
| 20 | round: 0.01 | round: 0.00 | round: 0.00 |
| 100 | round: 0.13 | round: 0.00 | round: 0.00
| 1000 | round: 0.19 | round: 1.20 | round: 0.03 |

I also ran Mark's bench test in a loop 10 times (again, to account for jit effects).

Results are as follows:

My Patch, Mark's test
 7050 ns
 450 ns
 440 ns
 550 ns
 440 ns
 440 ns
 440 ns
 440 ns
 440 ns
 540 ns
 410 ns
 440 ns
 440 ns
 430 ns
 460 ns
 
 
 Trunk, Mark's test
 243240 ns
 156640 ns
 25440 ns
 23470 ns
 18930 ns
 20710 ns
 16890 ns
 20210 ns
 17630 ns
 17900 ns
 21420 ns
 22550 ns
 22900 ns
 19800 ns
 16770 ns
 
 Mark's patch, Mark's Test
 8480 ns
 2750 ns
 2690 ns
 2760 ns
 3270 ns
 3590 ns
 6530 ns
 5900 ns
 6340 ns
 5410 ns
 5400 ns
 5420 ns
 5670 ns
 5410 ns
 5420 ns","30/Jun/13 00:42;dvryaboy;Slight update -- resetting all counters on clear(), and getting rid of an unnecessarily long 10K tuple test.",26/Jul/13 19:54;dvryaboy;marking as patch available. please review.,"29/Jul/13 17:29;cheolsoo;[~dvryaboy], I think your sampling code is incorrect.
{code}
/**
 * Sample every 10th tuple until we reach a max of SPILL_SAMPLE_SIZE
 * to get an estimate of the tuple sizes.
 */
protected void sampleContents() {
    synchronized (mContents) {
        ...
        for (int i = sampled; iter.hasNext() && sampled < SPILL_SAMPLE_SIZE; i++) {
            if (i % SPILL_SAMPLE_FREQUENCY == 0) {
                aggSampleTupleSize += iter.next().getMemorySize();
                sampled += 1;
            }
        }
    }
}
{code}
The iterator doesn't get incremented every iteration, so you're sampling sequential tuples instead of every 10th. Don't you need to add an else block and increment the iterator always?","29/Jul/13 22:30;dvryaboy;Urgh, you are right of course. I can move the .next() call into the for loop... but I wonder if that will slow us down again. Will check.","19/Oct/13 05:50;aniket486;I added else clause and reran the benchmark.

Results:
||Num Tuples in Bag || Trunk avg || Patch avg ||
| 1 | round: 0.00 | round: 0.00 |
| 20 | round: 0.01 | round: 0.00 | 
| 100 | round: 0.11 | round: 0.00 |
| 1000 | round: 0.12 | round: 0.02 |

testDefaultSpeed:
|| trunk || patch ||
|282760 ns|3950 ns|
|30450 ns|510 ns|
|13670 ns|490 ns|
|18930 ns|500 ns|
|22680 ns|490 ns|
|16230 ns|540 ns|
|20370 ns|490 ns|
|16830 ns|490 ns|
|13810 ns|530 ns|
|13610 ns|490 ns|
|14010 ns|660 ns|
|15340 ns|490 ns|
|13840 ns|490 ns|
|14420 ns|490 ns|
|15510 ns|520 ns|",20/Oct/13 16:46;aniket486;RB: https://reviews.apache.org/r/14775/,22/Oct/13 05:49;aniket486;Thanks [~dvryaboy] and [~mwagner]! I have committed this to trunk.,"22/Oct/13 22:45;aniket486;Also, committed to 0.12.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AVRO: AvroStorage give NPE on reading file with union as top level schema,PIG-3322,12647407,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,viraj,esoren,esoren,13/May/13 23:44,14/Oct/13 16:46,14/Mar/19 03:07,04/Jun/13 01:03,0.11.2,,,,,,,0.12.0,,,,piggybank,,,0,patch,,,,,,,,,,,,"I am getting NPE when loading a file with AvroStorage a file that has schema like:

{code}
[""null"",{""type"":""record"",""name"":""TUPLE_0"",""fields"":[{""name"":""name"",""type"":[""null"",""string""],""doc"":""autogenerated from Pig Field Schema""},{""name"":""age"",""type"":[""null"",""int""],""doc"":""autogenerated from Pig Field Schema""},{""name"":""gpa"",""type"":[""null"",""double""],""doc"":""autogenerated from Pig Field Schema""}]}]
{code}


E.g. see the e2e style test, which fails on this:

{code}
                        {
                        'num' => 4,
                        # storing file with Pig type tuple relying on conversion to record
                        # loading using stored schemas 
                        'notmq' => 1,
                        'pig' => q\
a = load ':INPATH:/singlefile/studentcomplextab10k' using PigStorage() as (m:[], t:(name:chararray, age:int, gpa:double), b:{t:(name:chararray, age:int, gpa:double)});
b = foreach a generate t;
describe b;
store b into ':OUTPATH:.intermediate' USING org.apache.pig.piggybank.storage.avro.AvroStorage();

exec;

-- Read back what was stored with Avro
u = load ':OUTPATH:.intermediate' USING org.apache.pig.piggybank.storage.avro.AvroStorage();
describe u;
store u into ':OUTPATH:';
\,
                        'verify_pig_script' => q\
a = load ':INPATH:/singlefile/studentcomplextab10k' using PigStorage() as (m:[], t:(name:chararray, age:int, gpa:double), b:{t:(name:chararray, age:int, gpa:double)});
b = foreach a generate t;
describe b;
store b into ':OUTPATH:';
\,
                        },
{code}



",,,,,,,,,,,,,,,PIG-2330,,,,,,,,,,,,,,,,,04/Jun/13 00:19;viraj;PIG-3322_3.patch;https://issues.apache.org/jira/secure/attachment/12585999/PIG-3322_3.patch,04/Jun/13 00:20;viraj;test_loadavrowithnulls.avro;https://issues.apache.org/jira/secure/attachment/12586000/test_loadavrowithnulls.avro,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-05-15 01:13:33.793,,,no_permission,,,,,,,,,,,,327763,,,,Tue Jun 04 01:03:59 UTC 2013,,,,,,,0|i1kjvb:,328107,,,,,,,AvroStorage,,,"15/May/13 01:13;viraj;It seems that the schema specified during load time is stored in ""outputAvroSchema"" but is not used when reading the underlying data. It will be used when writing out the data. 
PIG-3321 will enable to use this schema when reading the data but will need to investigate if it fixes the above problem. ","15/May/13 06:10;viraj;Sorry the above comment was intended for PIG-3220
Viraj",15/May/13 06:11;viraj;I meant PIG-3320 ..,"17/May/13 01:20;esoren;The test was only storing one field, and as such seems to duplicate PIG-2330.","17/May/13 03:19;viraj;Hi Egil,
 The issue here is that the field ""t"" from the original data ""studentcomplextab10k"" set contains nulls. 
(fred hernandez,73,1.87)
(fred hernandez,20,2.11)

(calvin allen,60,2.49)
(yuri zipper,76,2.05)


So when this is stored via the AvroStorage, nulls are stored for the record.

When you read it out the written avro from the previous store, it fails with a null pointer exception.

The following snippet below works without any problems.
{code}
a = load 'studentcomplextab10k' using PigStorage() as (m:[], t:(name:chararray, age:int, gpa:double), b:{t:(name:chararray, age:int, gpa:double)});
b = foreach a generate t;
c = filter b by t is not null;
store c into 'singltupleavronotnull' USING org.apache.pig.piggybank.storage.avro.AvroStorage();
exec;
b = load 'singltupleavronotnull' USING org.apache.pig.piggybank.storage.avro.AvroStorage();
describe b;
dump b;
{code}

Kindly note: This issue is different from PIG-2330 
",22/May/13 21:09;viraj;Avro file used for the TestAvroStorage.java,22/May/13 21:10;viraj;Expected File generated from the testcase,"22/May/13 21:21;viraj;Review board
https://reviews.apache.org/r/11333/",29/May/13 23:10;viraj;Test Input Avro file,29/May/13 23:11;viraj;Golden test file generated,29/May/13 23:11;viraj;Patch for PIG-3322,04/Jun/13 01:03;rohini;Committed to trunk (0.12). Thanks Viraj and Cheolsoo.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AVRO: 'default value' not honored when merging schemas on load with AvroStorage,PIG-3318,12646937,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,viraj,viraj,viraj,10/May/13 01:41,14/Oct/13 16:45,14/Mar/19 03:07,14/Jun/13 13:35,0.11.2,,,,,,,0.12.0,,,,piggybank,,,0,patch,,,,,,,,,,,,"Piggybank - AvroStorage. When merging multiple schemas where default values have been specified in the avro schema; 
The AvroStorage puts nulls in the merged data set. 

==> Employee3.avro <==
{
""type"" : ""record"",
""name"" : ""employee"",
""fields"":[
        {""name"" : ""name"", ""type"" : ""string"", ""default"" : ""NU""},
        {""name"" : ""age"", ""type"" : ""int"", ""default"" : 0 },
        {""name"" : ""dept"", ""type"": ""string"", ""default"" : ""DU""} ] }

==> Employee4.avro <==
{
""type"" : ""record"",
""name"" : ""employee"",
""fields"":[
        {""name"" : ""name"", ""type"" : ""string"", ""default"" : ""NU""},
        {""name"" : ""age"", ""type"" : ""int"", ""default"" : 0},
        {""name"" : ""dept"", ""type"": ""string"", ""default"" : ""DU""},
        {""name"" : ""office"", ""type"": ""string"", ""default"" : ""OU""} ] }

==> Employee6.avro <==
{
""type"" : ""record"",
""name"" : ""employee"",
""fields"":[
        {""name"" : ""name"", ""type"" : ""string"", ""default"" : ""NU""},
        {""name"" : ""lastname"", ""type"": ""string"", ""default"" : ""LNU""},
        {""name"" : ""age"", ""type"" : ""int"",""default"" : 0},
        {""name"" : ""salary"", ""type"": ""int"", ""default"" : 0},
        {""name"" : ""dept"", ""type"": ""string"",""default"" : ""DU""},
        {""name"" : ""office"", ""type"": ""string"",""default"" : ""OU""} ] }

The pig script:
employee = load 'employee{3,4,6}.ser' using org.apache.pig.piggybank.storage.avro.AvroStorage('multiple_schemas');
describe employee;
dump employee;

Output Schema:
employee: {name: chararray,age: int,dept: chararray,lastname: chararray,salary: int,office: chararray}

(Milo,30,DH,,,)
(Asmya,34,PQ,,,)
(Baljit,23,RS,,,)
(Pune,60,Astrophysics,Warriors,5466,UTA)
(Rajsathan,20,Biochemistry,Royals,1378,Stanford)
(Chennai,50,Microbiology,Superkings,7338,Hopkins)
(Mumbai,20,Applied Math,Indians,4468,UAH)
(Praj,54,RMX,,,Champaign)
(Buba,767,HD,,,Sunnyvale)
(Manku,375,MS,,,New York)


Regards
Viraj",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Jun/13 00:16;viraj;Employee3.avro;https://issues.apache.org/jira/secure/attachment/12587735/Employee3.avro,14/Jun/13 00:17;viraj;Employee4.avro;https://issues.apache.org/jira/secure/attachment/12587736/Employee4.avro,14/Jun/13 00:17;viraj;Employee6.avro;https://issues.apache.org/jira/secure/attachment/12587737/Employee6.avro,14/Jun/13 00:17;viraj;PIG-3318_5.patch;https://issues.apache.org/jira/secure/attachment/12587738/PIG-3318_5.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-05-14 23:49:53.383,,,no_permission,,,,,,,,,,,,327294,,,,Fri Jun 14 13:35:28 UTC 2013,,,,,,,0|i1kgz3:,327638,,,,,,,AvroStorage,,,"12/May/13 02:36;viraj;Submitting a patch in both Pig 0.11 and Pig 0.12/trunk, which will fix this issue with the relevant test cases and files.",12/May/13 02:37;viraj;Trunk patch which contains the fix,12/May/13 02:38;viraj;Patch for branch 0.11.2,12/May/13 02:39;viraj;Avro file,12/May/13 02:39;viraj;avro test file,12/May/13 02:39;viraj;Avro test file,12/May/13 02:40;viraj;Expected resulting avro file,12/May/13 02:43;viraj;Patch for adding default values for merged schemas.,14/May/13 23:49;rohini;Review board request from Virag for the patch - https://reviews.apache.org/r/11135/,"12/Jun/13 14:24;rohini;Ran TestAvroStorage before committing. Encountered


Testcase: testMultipleSchemasWithDefaultValue took 3.543 sec
        Caused an ERROR
Not a data file.
java.io.IOException: Not a data file.
        at org.apache.avro.file.DataFileStream.initialize(DataFileStream.java:105)
        at org.apache.avro.file.DataFileStream.<init>(DataFileStream.java:84)
        at org.apache.pig.piggybank.test.storage.avro.TestAvroStorage.verifyResults(TestAvroStorage.java:1292)
        at org.apache.pig.piggybank.test.storage.avro.TestAvroStorage.verifyResults(TestAvroStorage.java:1262)
        at org.apache.pig.piggybank.test.storage.avro.TestAvroStorage.testMultipleSchemasWithDefaultValue(TestAvroStorage.java:704)

The problem is that the testcase is storing output using PigStorage and so output is not a avro file. I tried changing it to AvroStorage but still failed as it did not match with the expected_testMultipleSchemasWithDefaultValue.avro. Can you fix the testcase? Also can you rename Employee*.ser to Employee*.avro to be consistent with naming. ","13/Jun/13 02:04;viraj;Sorry for attaching the wrong patch, which makes the test case write to an Avro file. I have modified the test to use mock.Storage(), will reattach the correct patch.
Viraj",14/Jun/13 13:35;rohini;Committed to trunk(0.12). Thanks Viraj.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig failed to interpret DateTime values in some special cases,PIG-3316,12646675,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,08/May/13 19:01,14/Oct/13 16:46,14/Mar/19 03:07,10/May/13 20:46,0.11,,,,,,,0.12.0,,,,data,impl,,0,,,,,,,,,,,,,"For the query

A = load 'date.txt' as ( f1:int, f2:datetime );
dump A;

with input data
1,1970-01-01
2,1970-01

pig generates the following output
(1,1970-01-01T00:00:00.000-01:00)
(2,1970-01-01T00:00:00.000-01:00)

which seemingly incorrectly interprets the day or month part as time zone.

",1970-01-01,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/May/13 23:34;xuefuz;PIG-3316.patch;https://issues.apache.org/jira/secure/attachment/12582547/PIG-3316.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-09 23:46:30.199,,,no_permission,,,,,,,,,,,,327033,,,,Wed May 29 18:16:16 UTC 2013,,,,,,,0|i1kf33:,327377,,,,,,,,,,09/May/13 23:33;xuefuz;Fix for the regex in ToData.java is in the patch with test cases.,"09/May/13 23:46;dreambird;Xuefu, thanks for submit the patch, it looks good to me. I will run trunk unit tests on top of it.","10/May/13 02:20;sms;+1

I ran the commit tests and the test case for Hadoop20 and Hadoop23","10/May/13 20:46;xuefuz;Thanks, Santhosh.

Patch committed to trunk.",11/May/13 00:26;dreambird;it is a little bit late :) but the unit tests with patch pass for me,"28/May/13 23:54;rohini;[~xuefuz],
  I was looking for this fix and found that the CHANGES.txt file has not been updated. Can you update the same? 
  Fix is gone only into trunk and not 0.11. Updated the Fixed version accordingly. Did you intend to commit this to 0.11 also?",29/May/13 18:16;xuefuz;@Rohini Thanks for pointing that out. I was quite clear about the versions. I will update CHANGES.txt as suggested for this (and other JIRAs).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig job hang if the job tracker is bounced during execution,PIG-3313,12646342,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yu.chenjie,yu.chenjie,yu.chenjie,07/May/13 02:00,14/Oct/13 16:46,14/Mar/19 03:07,24/May/13 21:28,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"When running a pig job through PigRunner, after the mapreduce job is submitted, if there is a job tracker bounce which doesn't get back up very soon, the pig job will hang there.
The reason is pig is keeping all the JobControl objects, which are non-deamon threads, that keeps connecting to jobtracker. If the job tracker is down, pig will fail, but those jobcontrol threads keep running and there is no one who can stop them.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/May/13 17:53;yu.chenjie;PIG-3313;https://issues.apache.org/jira/secure/attachment/12582134/PIG-3313,24/May/13 21:27;daijy;PIG-3313-1.patch;https://issues.apache.org/jira/secure/attachment/12584762/PIG-3313-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-05-24 21:27:38.464,,,no_permission,,,,,,,,,,,,326700,Reviewed,,,Fri May 24 21:28:02 UTC 2013,,,Patch Available,,,,0|i1kd1b:,327045,,,,,,,,,,07/May/13 17:53;yu.chenjie;The patch that should fix this problem. Tested ok.,24/May/13 21:27;daijy;Patch looks good. Only change the format and reattach patch.,24/May/13 21:28;daijy;Patch committed to trunk. Thanks Chenjie!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ImplicitSplitInserter does not generate new uids for nested schema fields, leading to miscomputations",PIG-3310,12645931,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cstenac,cstenac,cstenac,03/May/13 07:59,14/Oct/13 16:46,14/Mar/19 03:07,30/May/13 18:57,0.11.1,,,,,,,0.12.0,,,,impl,,,0,,,,,,,,,,,,,"Hi,

Consider the following example

{code}
inp = LOAD '$INPUT' AS (memberId:long, shopId:long, score:int);

tuplified = FOREACH inp GENERATE (memberId, shopId) AS tuplify, score;

D1 = FOREACH tuplified GENERATE tuplify.memberId as memberId, tuplify.shopId as shopId, score AS score;
D2 = FOREACH tuplified GENERATE tuplify.memberId as memberId, tuplify.shopId as shopId, score AS score;

J = JOIN D1 By shopId, D2 by shopId;
K = FOREACH J GENERATE D1::memberId AS member_id1, D2::memberId AS member_id2, D1::shopId as shop;

EXPLAIN K;
DUMP K;
{code}

It is a bit weird written like that, but it provides a minimal reproduction case (in the real case, the ""tuplified"" phase came from a multi-key grouping).

On input data:
{code}
1       1001    101
1       1002    103
1       1003    102
1       1004    102
2       1005    101
2       1003    101
2       1002    123
3       1042    101
3       1005    101
3       1002    133
{code}

This will give a wrongful output like ..
{code}
(1,1001,1001)
(1,1002,1002)
(1,1002,1002)
(1,1002,1002)
{code}
The second column should be a member id so (1,2,3,4,5).

In the initial case, there was a FILTER (member_id1 < member_id2) after K, and computation failed because of PushUpFilter optimization mistakenly moving the LOFilter operation before the join, at a place where it tried to work on a tuple and failed.

My understanding of the issue is that when the ImplicitSplitInserter creates the LOSplitOutputs, it will correctly reset the schema, and the LOSplitOutput will regenerate uids for the fields of D1 and D2 ... but will not do that on the tuple members.

The logical plan after the ImplicitSplitINserter will look like (simplified)

{code}
   |---D1: (Name: LOForEach Schema: memberId#124:long,shopId#125:long)ColumnPrune:InputUids=[127]ColumnPrune:OutputUids=[125, 124]
        |---tuplified: (Name: LOSplitOutput Schema: tuplify#127:tuple(memberId#124:long,shopId#125:long))ColumnPrune:InputUids=[123]ColumnPrune:OutputUids=[127]
           |---tuplified: (Name: LOSplit Schema: tuplify#123:tuple(memberId#124:long,shopId#125:long))ColumnPrune:InputUids=[123]ColumnPrune:OutputUids=[123]
    |---D2: (Name: LOForEach Schema: memberId#124:long,shopId#125:long)ColumnPrune:InputUids=[130]ColumnPrune:OutputUids=[125, 124]
        |---tuplified: (Name: LOSplitOutput Schema: tuplify#130:tuple(memberId#124:long,shopId#125:long))ColumnPrune:InputUids=[123]ColumnPrune:OutputUids=[130]
           |---tuplified: (Name: LOSplit Schema: tuplify#123:tuple(memberId#124:long,shopId#125:long))ColumnPrune:InputUids=[123]ColumnPrune:OutputUids=[123]
{code}

tuplified correctly gets a new uid (127 and 130) but the members of the tuple don't. When they get reprojected, both branches have the same uid and the join looks like:
{code}
|---J: (Name: LOJoin(HASH) Schema: D1::memberId#124:long,D1::shopId#125:long,D2::memberId#139:long,D2::shopId#132:long)ColumnPrune:InputUids=[125, 124, 132]ColumnPrune:OutputUids=[125, 124, 132]
        |   |
        |   shopId:(Name: Project Type: long Uid: 125 Input: 0 Column: 1)
        |   |
        |   shopId:(Name: Project Type: long Uid: 125 Input: 1 Column: 1)
{code}

If for example instead of reprojecting ""memberId"", we project ""memberId+0"", a new node is created, and ultimately the two branches of the join will correctly get separate uids.

My understanding is that LOSplitOutput.getSchema() should recurse on nested schema fields. However, I only have a light understanding of all of the logical plan handling, so I may be completely wrong.

Attached is a draft of patch and a test reproducing the issue. Unfortunately, I haven't been able to run all unit tests with the ""fix"" (I have some weird hangs)

I'd be happy if you could indicate if that looks like completely the wrong way to fix the issue.","Reproduced on 0.10.1, 0.11.1 and trunk",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/May/13 08:01;cstenac;generate-uid-for-nested-fields.patch;https://issues.apache.org/jira/secure/attachment/12581672/generate-uid-for-nested-fields.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-23 17:53:04.213,,,no_permission,,,,,,,,,,,,326290,Reviewed,,,Thu May 30 18:57:12 UTC 2013,,,,,,,0|i1kaif:,326635,,,,,,,,,,"23/May/13 17:53;knoguchi;I also don't have a good understanding on these, but the change looks reasonable to me.  [~daijy], original uid reassignment was added in PIG-1705 for the self-join.  Can you take a look?",24/May/13 20:55;daijy;Looks good to me. Will commit once tests pass.,"30/May/13 18:57;daijy;Patch committed to trunk. Thanks Clément, Koji!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Infinite loop when input path contains empty partition directory ,PIG-3305,12645630,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,maczech,maczech,maczech,01/May/13 13:33,14/Oct/13 16:46,14/Mar/19 03:07,02/May/13 17:13,0.10.1,,,,,,,0.12.0,,,,piggybank,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/May/13 13:36;maczech;PIG-3305.patch;https://issues.apache.org/jira/secure/attachment/12581356/PIG-3305.patch,02/May/13 10:57;maczech;PIG-3305_trunk.patch;https://issues.apache.org/jira/secure/attachment/12581504/PIG-3305_trunk.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-05-02 00:33:55.229,,,no_permission,,,,,,,,,,,,325991,Reviewed,,,Thu May 02 17:13:10 UTC 2013,,,,,,,0|i1k8nz:,326336,,,,,,,,,,02/May/13 00:33;daijy;[~maczech]Can you make a patch for trunk?,02/May/13 17:13;daijy;Patch committed to trunk. Thanks Marcin!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XMLLoader in piggybank does not work with inline closed tags,PIG-3304,12645562,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aseldawy,aseldawy,aseldawy,01/May/13 00:20,14/Oct/13 16:46,14/Mar/19 03:07,02/May/13 00:33,0.11.1,,,,,,,0.12.0,,,,piggybank,,,0,patch,,,,,,,,,,,,"The XMLLoader fails to return elements when tags are closed inline such as
<event id=""342""/>
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/May/13 00:25;aseldawy;xmlloader_inline_close_tag.patch;https://issues.apache.org/jira/secure/attachment/12581290/xmlloader_inline_close_tag.patch,01/May/13 01:59;aseldawy;xmlloader_inline_close_tag_1.patch;https://issues.apache.org/jira/secure/attachment/12581321/xmlloader_inline_close_tag_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-05-02 00:33:04.8,,,no_permission,,,,,,,,,,,,325923,Reviewed,,,Thu May 02 00:33:04 UTC 2013,,,Patch Available,,,,0|i1k88v:,326268,,,,,,,xml,,,"01/May/13 00:24;aseldawy;diff --git java/src/main/java/org/apache/pig/piggybank/storage/XMLLoader.java java/src/main/java/org/apache/pig/piggybank/storage/XMLLoader.java
index 589a545..9daa1a4 100644
--- java/src/main/java/org/apache/pig/piggybank/storage/XMLLoader.java
+++ java/src/main/java/org/apache/pig/piggybank/storage/XMLLoader.java
@@ -212,10 +212,16 @@ class XMLLoaderBufferedPositionedInputStream extends BufferedPositionedInputStre
       //startTag[tmp.length+1] = (byte)'>';
       
       
+      // Used to detect tags that are closed inline
+      byte[] inlineCloseTag = {'/', '>'};
 
       ByteArrayOutputStream collectBuf = new ByteArrayOutputStream(1024);
       int idxTagChar = 0;
       int idxStartTagChar = 0;
+      int idxInlineCloseTagChar = 0;
+      // A flag to indicate that we are currently inside the tag to be matched
+      // Initially set to true as skipToTag has been called earlier
+      boolean insideMatchTag = true;
       boolean startTagMatched = false;
       /*
        * Read till an end tag is found.It need not check for any condition since it 
@@ -247,10 +253,18 @@ class XMLLoaderBufferedPositionedInputStream extends BufferedPositionedInputStre
           
           if (b == startTag[idxStartTagChar]){
              ++idxStartTagChar;
-             if(idxStartTagChar == startTag.length)
-                startTagMatched = true ; // Set the flag as true if start tag matches
-          }else
-             idxStartTagChar = 0;
+             if(idxStartTagChar == startTag.length) {
+               startTagMatched = true ; // Set the flag as true if start tag matches
+               // We are currently inside the tag to be matched
+               insideMatchTag = true;               
+             }
+          } else {
+            idxStartTagChar = 0;
+            if (idxStartTagChar > 1) {
+              // Matched only a part of the start tag of some element
+              insideMatchTag = false;
+            }
+          }
             
           
           
@@ -268,6 +282,23 @@ class XMLLoaderBufferedPositionedInputStream extends BufferedPositionedInputStre
           } else 
             idxTagChar = 0; 
           
+          if (b == inlineCloseTag[idxInlineCloseTagChar]) {
+            idxInlineCloseTagChar++;
+            if (idxInlineCloseTagChar == inlineCloseTag.length) {
+              idxInlineCloseTagChar = 0;
+              if (insideMatchTag) {
+                if(nestedTags==0) // Break the loop if there were no nested tags
+                  break;
+               else{
+                  --nestedTags; // Else decrement the count
+                  idxInlineCloseTagChar = 0; // Reset the index
+               }
+              }
+            }
+          } else {
+            idxInlineCloseTagChar = 0;
+          }
+          
         }
         catch (IOException e) {
           this.setReadable(false);
@@ -339,7 +370,7 @@ class XMLLoaderBufferedPositionedInputStream extends BufferedPositionedInputStre
               break;
             case S_MATCH_PREFIX:
               // tag match iff next character is whitespaces or close tag mark
-              if (b == ' ' || b == '\t' || b == '>') {
+              if (Character.isWhitespace(b) || b == '/' || b == '>') {
                 matchBuf.write((byte)(b));
                 state = S_MATCH_TAG;
               } else {
@@ -355,7 +386,7 @@ class XMLLoaderBufferedPositionedInputStream extends BufferedPositionedInputStre
             default:
               throw new IllegalArgumentException(""Invalid state: "" + state);
           }
-          if (state == S_MATCH_TAG && b == '>') {
+          if (state == S_MATCH_TAG && (b == '>' || Character.isWhitespace(b))) {
             break;
           }
           if (state != S_MATCH_TAG && this.getPosition() > limit) {
@@ -406,6 +437,12 @@ class XMLLoaderBufferedPositionedInputStream extends BufferedPositionedInputStre
     byte[] collectTag(String tagName, long limit) throws IOException {
        ByteArrayOutputStream collectBuf = new ByteArrayOutputStream(1024);
        byte[] beginTag = skipToTag(tagName, limit);
+       
+       // Check if the tag is closed inline
+       if (beginTag.length > 2 && beginTag[beginTag.length - 2] == '/' &&
+           beginTag[beginTag.length-1] == '>') {
+         return beginTag;
+       }
 
        // No need to search for the end tag if the start tag is not found
        if(beginTag.length > 0 ){ 
diff --git java/src/test/java/org/apache/pig/piggybank/test/storage/TestXMLLoader.java java/src/test/java/org/apache/pig/piggybank/test/storage/TestXMLLoader.java
index 4adc9cd..f83f0d9 100644
--- java/src/test/java/org/apache/pig/piggybank/test/storage/TestXMLLoader.java
+++ java/src/test/java/org/apache/pig/piggybank/test/storage/TestXMLLoader.java
@@ -75,6 +75,15 @@ public class TestXMLLoader extends TestCase {
      nestedTags.add(new String[] { ""</events>""});
   }
   
+  public static ArrayList<String[]> inlineClosedTags = new ArrayList<String[]>();
+  static {
+    inlineClosedTags.add(new String[] { ""<events>""});
+    inlineClosedTags.add(new String[] { ""<event id='3423'/>""});
+    inlineClosedTags.add(new String[] { ""<event/>""});
+    inlineClosedTags.add(new String[] { ""<event><event/></event>""});
+    inlineClosedTags.add(new String[] { ""</events>""});
+  }
+  
   public void testShouldReturn0TupleCountIfSearchTagIsNotFound () throws Exception
   {
     String filename = TestHelper.createTempFile(data, """");
@@ -333,5 +342,26 @@ public class TestXMLLoader extends TestCase {
       assertEquals(3, tupleCount);  
    }
    
-   
+
+   public void testXMLLoaderShouldWorkWithInlineClosedTags() throws Exception {
+     String filename = TestHelper.createTempFile(inlineClosedTags, """");
+     PigServer pig = new PigServer(LOCAL);
+     filename = filename.replace(""\\"", ""\\\\"");
+     patternString = patternString.replace(""\\"", ""\\\\"");
+     String query = ""A = LOAD 'file:"" + filename + ""' USING org.apache.pig.piggybank.storage.XMLLoader('event') as (doc:chararray);"";
+     pig.registerQuery(query);
+     Iterator<?> it = pig.openIterator(""A"");
+     int tupleCount = 0;
+     while (it.hasNext()) {
+       Tuple tuple = (Tuple) it.next();
+       if (tuple == null)
+         break;
+       else {
+         if (tuple.size() > 0) {
+             tupleCount++;
+         }
+       }
+     }
+     assertEquals(3, tupleCount);  
+   }
 }",01/May/13 00:25;aseldawy;A patch that solves the bug,01/May/13 00:55;aseldawy;Found a bug with nested entities that are closed inline,01/May/13 01:59;aseldawy;An updated path that handles nested elements with inline close tag,02/May/13 00:33;daijy;Piggybank tests pass. Patch commmitted to trunk. Thanks Ahmed!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add hadoop h2 artifact to publications in ivy.xml,PIG-3303,12645552,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,julienledem,julienledem,julienledem,30/Apr/13 23:36,14/Oct/13 16:46,14/Mar/19 03:07,01/May/13 20:44,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30/Apr/13 23:46;julienledem;PIG-3303.patch;https://issues.apache.org/jira/secure/attachment/12581273/PIG-3303.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-01 17:01:07.425,,,no_permission,,,,,,,,,,,,325913,,,,Wed May 01 20:44:34 UTC 2013,,,Patch Available,,,,0|i1k86n:,326258,,,,,,,,,,01/May/13 17:01;rohini;+1,"01/May/13 17:12;billgraham;+1

Created PIG-3306 for publishing the h2 artifact to maven. ",01/May/13 20:44;julienledem;Merged in trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JSONStorage throws NPE if map has null values,PIG-3302,12645327,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,29/Apr/13 22:45,14/Oct/13 16:46,14/Mar/19 03:07,13/May/13 23:28,0.11.1,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"JsonStorage.java:
case DataType.MAP:
            json.writeFieldName(field.getName());
            json.writeStartObject();
            for (Map.Entry<String, Object> e : ((Map<String,
Object>)d).entrySet()) {
                json.writeStringField(e.getKey(), e.getValue().toString());
            }
            json.writeEndObject();
            return;

If e.getValue() is null, e.getValue().toString() will throw NPE. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/May/13 02:22;rohini;PIG-3302-1.patch;https://issues.apache.org/jira/secure/attachment/12582883/PIG-3302-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-13 07:44:36.06,,,no_permission,,,,,,,,,,,,325689,,,,Mon May 13 23:28:24 UTC 2013,,,,,,,0|i1k6t3:,326034,,,,,,,,,,"13/May/13 07:44;prkommireddi;Looks good to me, TestJsonLoaderStorage and ant test-commit all pass.

+1",13/May/13 23:28;rohini;Committed to trunk (0.12). Thanks Prashanth.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avro files with stringType set to String cannot be read by the AvroStorage LoadFunc,PIG-3297,12644554,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nielsbasjes,nielsbasjes,nielsbasjes,25/Apr/13 08:42,14/Oct/13 16:46,14/Mar/19 03:07,21/May/13 22:11,0.11.1,,,,,,,0.12.0,,,,piggybank,,,0,,,,,,,,,,,,,"When an Avro file is created there exists the option to set the ""String Type"" to a different class than the default Utf8.
A very common situation is that the ""String Type"" is set to the default String class.

When trying to read such an Avro file in Pig using the AvroStorage LoadFunc from the included piggybank this gives the following Exception:

Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.avro.util.Utf8
        at org.apache.pig.piggybank.storage.avro.PigAvroDatumReader.readString(PigAvroDatumReader.java:154)
        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:150)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Apr/13 20:56;nielsbasjes;PIG-3297-1.patch;https://issues.apache.org/jira/secure/attachment/12580585/PIG-3297-1.patch,20/May/13 22:24;nielsbasjes;PIG-3297-v2-20130521-The-fix.patch;https://issues.apache.org/jira/secure/attachment/12583902/PIG-3297-v2-20130521-The-fix.patch,20/May/13 22:24;nielsbasjes;PIG-3297-v2-20130521-Unittest.patch;https://issues.apache.org/jira/secure/attachment/12583903/PIG-3297-v2-20130521-Unittest.patch,08/May/13 14:35;nielsbasjes;test_record.avro;https://issues.apache.org/jira/secure/attachment/12582298/test_record.avro,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-05-10 18:30:43.303,,,no_permission,,,,,,,,,,,,324919,,,,Tue May 21 22:11:29 UTC 2013,,,,,,,0|i1k227:,325264,"Read Avro files that have string fields that were written with avro.java.string = String
",,,,,,,,,25/Apr/13 15:25;nielsbasjes;I have a working fix that I'll submit shortly.,25/Apr/13 20:56;nielsbasjes;The patch I created.,08/May/13 11:38;nielsbasjes;I found that although the fix correct is that the test I created not right is.,08/May/13 14:35;nielsbasjes;The simplest test file that demonstrates the bug.,"10/May/13 18:30;michaelmoss;Niels, I've run into this also (and a similar issue with Hive), and it seems that it might be brought on not by the code you patched, but perhaps in the avro-1.x.y.jar files itself.

We are serializing strings as avro.java.string and everything was working fine on our HDP1.2 (Hortonworks) cluster, but when I upgraded the avro jar that pig uses to avro-1.7.4 from avro-1.5.3, I get this exception.

I'm also have this issue on the latest version of CDH4.2 (with Impala1.0) in both pig and hive and the culprit there seems to be the avro-1.7.x.jar that they use.

I'm just starting to dig into finding out why, but was hoping you or someone here might have some insight.

Thanks.","18/May/13 12:36;nielsbasjes;In reply to the question Michael Moss posted to the Avro mailing list the reponse was that the proposed fix is the correct way to solve the issue.
See: http://mail-archives.apache.org/mod_mbox/avro-user/201305.mbox/%3CCDB6AEC8.EE942%25scott@richrelevance.com%3E

Quote:{Quote}
The change in the Pig loader in PIG-3297 seems correct  they must use
CharSequence, not Utf8.
{Quote}
",20/May/13 22:21;nielsbasjes;The only way to have a unit test for this bug is to bump the version of Avro that is used while doing the junit tests to version 1.7.4,"20/May/13 22:24;nielsbasjes;The fix for this bug is very simple.
In order to have a unit test that actually fails without this fix we create a dependency with upgrading the dependency with avro to version 1.7.4

I leave it to the committers to decide to ""just do the fix"" or to ""do the fix, upgrade to Avro, and add the unit test"".",20/May/13 22:25;nielsbasjes;Is blocked iff we want to use the provided unit test.,"20/May/13 22:27;nielsbasjes;The fix for this bug is very simple.
In order to have a unit test that actually fails without this fix we create a dependency with upgrading the dependency with avro to version 1.7.4
I leave it to the committers to decide to ""just do the fix"" or to ""do the fix, upgrade to Avro, and add the unit test"".
For this reason the fix and the unit test are placed in separate files.",21/May/13 03:47;cheolsoo;+1.,"21/May/13 22:11;cheolsoo;I only committed the fix to trunk. Please let me know if you'd like to get the unit test committed as well.

Thanks Niels!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Casting from bytearray failing after Union (even when each field is from a single Loader),PIG-3295,12644449,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,knoguchi,knoguchi,knoguchi,24/Apr/13 21:30,07/Jul/14 18:07,14/Mar/19 03:07,30/Oct/13 15:28,,,,,,,,0.13.0,,,,parser,,,0,,,,,,,,,,,,,"One example
{noformat}
A = load 'data1.txt' as line:bytearray;
B = load 'c1.txt' using TextLoader() as cookie1;
C = load 'c2.txt' using TextLoader() as cookie2;
B2 = join A by line, B by cookie1;
C2 = join A by line, C by cookie2;
D = union onschema B2,C2; -- D: {A::line: bytearray,B::cookie1: bytearray,C::cookie2: bytearray}
E = foreach D generate (chararray) line, (chararray) cookie1, (chararray) cookie2;
dump E;
{noformat}

This script fails at runtime with 
""Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 1075: Received a bytearray from the UDF. Cannot determine how to convert the bytearray to string.""

This is different from PIG-3293 such that each field in 'D' belongs to a single loader whereas on PIG-3293, it came from multiple loader.



",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Apr/13 20:20;knoguchi;pig-3295-v01.patch;https://issues.apache.org/jira/secure/attachment/12580573/pig-3295-v01.patch,03/Sep/13 16:51;knoguchi;pig-3295-v02.patch;https://issues.apache.org/jira/secure/attachment/12601186/pig-3295-v02.patch,10/Sep/13 22:24;knoguchi;pig-3295-v03.patch;https://issues.apache.org/jira/secure/attachment/12602441/pig-3295-v03.patch,16/Sep/13 20:32;knoguchi;pig-3295-v04.patch;https://issues.apache.org/jira/secure/attachment/12603421/pig-3295-v04.patch,23/Sep/13 16:12;knoguchi;pig-3295-v05.patch;https://issues.apache.org/jira/secure/attachment/12604616/pig-3295-v05.patch,23/Sep/13 22:46;knoguchi;pig-3295-v06.patch;https://issues.apache.org/jira/secure/attachment/12604686/pig-3295-v06.patch,29/Oct/13 17:49;knoguchi;pig-3295-v07.patch;https://issues.apache.org/jira/secure/attachment/12610878/pig-3295-v07.patch,,,,,7.0,,,,,,,,,,,,,,,,,,,2013-09-04 20:24:18.663,,,no_permission,,,,,,,,,,,,324816,Reviewed,,,Thu Oct 31 15:46:36 UTC 2013,,,,,,,0|i1k1fj:,325162,"Before, LoadCaster after Union was only available when two union-ed relations were from the same Loader with same parameters.  Now, LoadCaster would work for cases when Loaders return the same LoadCaster that only have a default constructor.",,,,,,,,,"25/Apr/13 20:20;knoguchi;Attaching an initial patch.
Instead of having one FuncSpec per LOUnion (PIG-2493), checking each field and setting different FuncSpec when possible.","25/Apr/13 20:22;knoguchi;Forgot to mention, I didn't fix PIG-3293 case but updated the error message to indicate it could be from Union with multiple loaders.  ",03/Sep/13 16:51;knoguchi;Just noticed my previous patch wasn't created with '--no-prefix' option. Reattaching.,04/Sep/13 20:24;daijy;How about doing more aggressively by checking LoadCaster?,"05/Sep/13 16:08;knoguchi;bq. How about doing more aggressively by checking LoadCaster?
That was my first approach, but as I also wrote in PIG-3293, 
""Figuring out if the loaders were same was easy with calling 'equals' for the FuncSpec. I don't know how to achieve this easily for comparing casters.""","05/Sep/13 16:55;daijy;Oh, sorry not notice that. Can we instantiate the LoadFunc (with parameters) and then compare?","05/Sep/13 17:06;knoguchi;bq. Can we instantiate the LoadFunc (with parameters) and then compare?

Possible, but I can only compare the classnames?  For the funcspec comparisons, we're comparing the classname as well as the parameters passed to the constructors.
","05/Sep/13 17:31;daijy;Your concern is valid, comparing classname is not sufficient in theory. But I really want to make Utf8StorageConverter compatible, which might solve most problem. How about we make one exception in the case LoadCaster has only default construct?","06/Sep/13 16:00;knoguchi;bq. How about we make one exception in the case LoadCaster has only default construct?

If you mean, make an exception only for Utf8StorageConverter, that makes sense since we have full control over the class and we know that classname check is sufficient for equality.  Let me try coming up with a new patch.
","06/Sep/13 18:43;daijy;To make it clearer, I mean if the class name for LoadCaster are the same, and LoadCaster only has default construct, then two LoadCasters are equal. This covers Utf8StorageConverter.","10/Sep/13 22:24;knoguchi;Attaching a patch which includes Daniel's suggestion on comparing the LoadCaster (and limiting to ones with default constructors only).

Haven't run full test yet.",11/Sep/13 21:40;daijy;+1. Will commit if tests pass.,"13/Sep/13 22:05;knoguchi;Thanks Daniel.  

I just bumped into PIG-2699 where [~julienledem] reduced the number of Loader&Storer instantiations.  I'm afraid he won't be happy with my current patch.  Let me see if I can reuse the Loader instance from somewhere.","16/Sep/13 05:35;daijy;Or unless a tie, we don't instantiate a LoadFunc. I also see several NPE in LineageFindRelVisitor.java:115 in the unit tests.","16/Sep/13 20:32;knoguchi;bq. Or unless a tie, we don't instantiate a LoadFunc. I also see several NPE in LineageFindRelVisitor.java:115 in the unit tests.

Uploading a patch that delays the instantiation of LoadFunc.  Also added extra check to avoid NPE.  Running unit tests.",16/Sep/13 20:33;knoguchi;Waiting for my unit tests to pass on the new patch.,23/Sep/13 16:12;knoguchi;Testcase in my last patch (v04) wasn't able to find the inner test class. Resolving that and also added a check to make sure Loader is not instantiated more than necessary.,"23/Sep/13 17:49;knoguchi;On my environment, somehow seeing Unittest failures even before my patch.  I'll look at them later but now I do see that my last patch added many failures in  TestTypeCheckingValidatorNewLP.  Since we relaxed the Lineage condition,  loader info can propagate further.

Just to clarify, before 
* Loader classname and parameter had to match

With the patch, it'll be 
* Loader classname and parameter match 
OR 
* classname of the caster instantiated from the loader matches and it has only a default constructor.

","23/Sep/13 22:46;knoguchi;bq. but now I do see that my last patch added many failures in TestTypeCheckingValidatorNewLP. 

Looking at the test, I see that it's checking how the Loader info propagate or won't propagate.  It's using 

* PigStorage('a')
and
* PigStorage('b') 

to represent two different Loader.  However, with my patch, these are now considered equal since they both use the Utf8StorageConverter.

In most of the testcases, I replaced PigStorage('b') to org.apache.pig.test.PigStorageWithDifferentCaster('b') so that testcase would have two distinct loadcasters.

For testcase testCogroupStarLineageFail and testCogroupStarLineageNoSchemaFail, I kept the PigStorage since the exception is thrown even with a single loader.

For testCogroupStreamingLineageNoSchema, I changed the expectedResult since now PigStorage and PigStreaming both uses the same loadcaster, Utf8StorageConverter.

",25/Sep/13 00:53;daijy;+1.,"29/Oct/13 17:49;knoguchi;Finally got most of the e2e running on my environment.  My last patch introduced one failure in LineageErrors where it was grepping for the error message.  Updated the expected error message.

Since rest of the patch is reviewed with +1,  I'll commit this patch to trunk tomorrow.","30/Oct/13 15:28;knoguchi;Thanks [~daijy] for your helpful feedback!
Committed to trunk. ","31/Oct/13 15:46;knoguchi;bq. Committed to trunk.
FYI, my initial commit was missing 2 files I added as part of the patch.  This resulted in about 30 failing unit tests in TestTypeCheckingValidatorNewLP.  Added them now.  Sorry for the trouble.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Logical plan invalid state: duplicate uid in schema during self-join to get cross product,PIG-3292,12644306,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,serega_sheypak,serega_sheypak,24/Apr/13 08:06,14/Oct/13 16:46,14/Mar/19 03:07,01/Oct/13 17:53,0.10.0,,,,,,,0.12.0,,,,parser,,,0,newbie,,,,,,,,,,,,"Hi.
Looks like PIG-3020
but works in a different way.
Our pig version is: 
Apache Pig version 0.10.0-cdh4.2.0 (rexported) 
compiled Feb 15 2013, 12:20:54

Accoring to release note, PIG-3020 is included into CDH 4.2 dist
http://archive.cloudera.com/cdh4/cdh/4/pig-0.10.0-cdh4.2.0.CHANGES.txt

The problem:
We want to do self join to get cross-product
{code}
a = load '/input' as (key, x);

a_group = group a by key;
b = foreach a_group {
  y = a.x;
  pair = cross a.x, y;
  generate flatten(pair);
}

dump b;
{code}

And an error:
{code}
ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2270: Logical plan invalid state: duplicate uid in schema : 1-7::x#16:bytearray,y::x#16:bytearray
{code}

Here is workaround :)
{code}
a = load '/input' as (key, x:int);

a_group = group a by key;
b = foreach a_group {
  y = foreach a generate -(-x);
  pair = cross a.x, y;
  generate flatten(pair);
}

dump b;
{code}
",CDH 4.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Oct/13 17:51;daijy;PIG-3292-1.patch;https://issues.apache.org/jira/secure/attachment/12606141/PIG-3292-1.patch,09/Jun/13 23:46;cheolsoo;PIG-3292.patch;https://issues.apache.org/jira/secure/attachment/12587001/PIG-3292.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-06-07 23:43:42.296,,,no_permission,,,,,,,,,,,,324673,Reviewed,,,Wed Oct 02 17:59:56 UTC 2013,,,,,,,0|i1k0jj:,325018,,,,,,,,,,"07/Jun/13 23:43;dreambird;thanks for opening jira Sergey, took a look, not sure this is by design or not. Take a simpler case, we can do self-cross outside of foreach
{noformat}
a = load 'input' as (key, x);
b = cross a, a;
dump b;
{noformat}

but do it in a foreach will fail
{noformat}
a = load 'input' as (key, x);
a_group = group a by key;
b = foreach a_group { y = cross a, a; generate y; }
dump b;
{noformat}

although they fail with different error (Duplicate schema alias), but overall I think it is important to figure out why we have this kind of restriction within foreach ?","09/Jun/13 23:46;cheolsoo;The same fix as in PIG-3020 needs to be made to LOCross. In fact, PIG-3144 added a static helper function that fixes duplicate field uids to LogicalRelationalOperator. So we can simply call that function to fix field schemas in the getSchema() method of LOCross.","11/Jun/13 02:26;dreambird;thanks for the patch, works for me locally. +1 (non-committer)

I am running unit tests with it.",12/Jun/13 16:09;dreambird;unit tests all pass.,"01/Oct/13 17:51;daijy;Looks good. Note the issue only occurs in nested cross. Self-cross in top level cross is not a problem, since LOSplit will take care of the uid reassign. Interplay with ColumnPruner is fine here since nested plan will include entire required plan branch. So no need to track the lineage of LOCross in nested cross. To be more specific, check ""nested"" flag around ""fixDuplicateUids"". Also add a test case.",01/Oct/13 17:53;daijy;Patch committed to branch 0.12 and trunk. ,02/Oct/13 17:59;cheolsoo;Thank you Daniel for the detailed explanation. That's very helpful!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestLogicalPlanBuilder.testQuery85 fail in trunk,PIG-3290,12644160,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,dreambird,dreambird,23/Apr/13 17:28,14/Oct/13 16:46,14/Mar/19 03:07,24/Apr/13 07:15,0.11.2,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"I can reproduce it locally as well, the exception is
{noformat}
junit.framework.AssertionFailedError: org.apache.pig.impl.plan.PlanValidationException: ERROR 1108: 
<line 1, column 79> Duplicate schema alias: group
	at org.apache.pig.test.TestLogicalPlanBuilder.buildPlan(TestLogicalPlanBuilder.java:2211)
	at org.apache.pig.test.TestLogicalPlanBuilder.testQuery85(TestLogicalPlanBuilder.java:1011)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Apr/13 00:56;daijy;PIG-3290-1.patch;https://issues.apache.org/jira/secure/attachment/12580203/PIG-3290-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-23 17:39:10.194,,,no_permission,,,,,,,,,,,,324527,Reviewed,,,Wed Apr 24 07:15:41 UTC 2013,,,,,,,0|i1jzn3:,324872,,,,,,,,,,"23/Apr/13 17:39;cheolsoo;Johnny, thank you so much for reporting the test failure. I am glad to see you are running unit tests. :-)

I can verify that this test case is broken by PIG-2767.","24/Apr/13 00:56;daijy;Ops, my mistake. Attach patch.","24/Apr/13 01:39;cheolsoo;Thanks for fixing it, Daniel! Overall looks good to me.

Have a minor question regarding the following code:
{code}
if (subAlias==null) {
    subAlias = """";
}
alias = alias + ""_"" + subAlias;
{code}
Is it ever possible for subAlias to be null in the dereference expression? I actually tried something like the following:
{code}
a = load '1.txt';
b = group a by ($0, $1);
c = foreach b generate group.$0, group.$1, COUNT(a.gpa);
{code}
But this gives me a NPE before I hit these lines of code. So I suppose subAlias doesn't need to be checked whether it's null.

However, if there is a case where subAlias is null, won't your code cause an alias conflict because multiple columns will be named as ""alias_""?

So my questions is:
* Can we get rid of these lines of code at all?
* If not, shouldn't we append something unique per column when subAlias is null?

Please correct me if I am wrong.","24/Apr/13 01:51;cheolsoo;ACtually, I was totally wrong. I should have run the following script:
{code}
a = load '1.txt';
b = group a by ($0, $1);
c = foreach b generate group.($0, $1);
{code}
In this case, ""group.($0, $1)"" is named as ""group__"", which seems fine. Sorry that I misunderstood your patch.

+1.",24/Apr/13 07:15;daijy;Thanks for quick review Cheolsoo! Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestPigContext.testImportList fails in trunk,PIG-3286,12643796,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,20/Apr/13 20:38,14/Oct/13 16:46,14/Mar/19 03:07,02/May/13 03:43,0.12.0,,,,,,,0.12.0,,,,build,,,0,test,,,,,,,,,,,,"To reproduce, run ant clean test -Dtestcase=TestPigContext. It fails with the following error:
{code}
junit.framework.AssertionFailedError: expected:<5> but was:<6>
	at org.apache.pig.test.TestPigContext.testImportList(TestPigContext.java:157)
{code}
This is a regression from PIG-3198 that added ""java.lang."" to the default import list. Here is relevant code:
{code}
@@ -739,6 +739,7 @@ public class PigContext implements Serializable {
         if (packageImportList.get() == null) {
             ArrayList<String> importlist = new ArrayList<String>();
             importlist.add("""");
+            importlist.add(""java.lang."");
             importlist.add(""org.apache.pig.builtin."");
             importlist.add(""org.apache.pig.impl.builtin."");
             packageImportList.set(importlist);
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Apr/13 23:04;cheolsoo;PIG-3286-2.patch;https://issues.apache.org/jira/secure/attachment/12579694/PIG-3286-2.patch,20/Apr/13 20:41;cheolsoo;PIG-3286.patch;https://issues.apache.org/jira/secure/attachment/12579688/PIG-3286.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-04-20 22:48:06.444,,,no_permission,,,,,,,,,,,,324163,,,,Thu May 02 03:43:10 UTC 2013,,,,,,,0|i1jxen:,324508,,,,,,,,,,20/Apr/13 20:41;cheolsoo;Attached is a patch that updates the test case accordingly.,"20/Apr/13 22:48;prkommireddi;Looks good. +1 (non-binding)

Cheolsoo, would you like to clean up the unused import and Logger?

{code}
import static org.junit.Assert.fail;
{code}

{code}
    private static final Logger LOG = Logger.getLogger(TestPigContext.class);
{code}",20/Apr/13 23:04;cheolsoo;Removed unused code.,02/May/13 00:38;daijy;+1,02/May/13 03:43;cheolsoo;Committed to trunk. Thank you Daniel and Prashant for reviewing it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jobs using HBaseStorage fail to ship dependency jars,PIG-3285,12643728,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ndimiduk,ndimiduk,ndimiduk,19/Apr/13 23:30,07/Jul/14 18:07,14/Mar/19 03:07,24/Nov/13 23:45,,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"Launching a job consuming {{HBaseStorage}} fails out of the box. The user must specify {{-Dpig.additional.jars}} for HBase and all of its dependencies. Exceptions look something like this:

{noformat}
2013-04-19 18:58:39,360 FATAL org.apache.hadoop.mapred.Child: Error running child : java.lang.NoClassDefFoundError: com/google/protobuf/Message
	at org.apache.hadoop.hbase.io.HbaseObjectWritable.<clinit>(HbaseObjectWritable.java:266)
	at org.apache.hadoop.hbase.ipc.Invocation.write(Invocation.java:139)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.sendParam(HBaseClient.java:612)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:975)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:84)
	at $Proxy7.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine.getProxy(WritableRpcEngine.java:136)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:208)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Nov/13 23:05;ndimiduk;0001-PIG-3285-Add-HBase-dependency-jars.patch;https://issues.apache.org/jira/secure/attachment/12615020/0001-PIG-3285-Add-HBase-dependency-jars.patch,19/Nov/13 22:23;ndimiduk;0001-PIG-3285-Add-HBase-dependency-jars.patch;https://issues.apache.org/jira/secure/attachment/12614703/0001-PIG-3285-Add-HBase-dependency-jars.patch,18/Nov/13 23:15;ndimiduk;0001-PIG-3285-Add-HBase-dependency-jars.patch;https://issues.apache.org/jira/secure/attachment/12614508/0001-PIG-3285-Add-HBase-dependency-jars.patch,15/Nov/13 22:17;ndimiduk;0001-PIG-3285-Add-HBase-dependency-jars.patch;https://issues.apache.org/jira/secure/attachment/12614148/0001-PIG-3285-Add-HBase-dependency-jars.patch,23/Apr/13 20:15;ndimiduk;0001-PIG-3285-Add-HBase-dependency-jars.patch;https://issues.apache.org/jira/secure/attachment/12580121/0001-PIG-3285-Add-HBase-dependency-jars.patch,22/Apr/13 19:11;ndimiduk;0001-PIG-3285-Add-HBase-dependency-jars.patch;https://issues.apache.org/jira/secure/attachment/12579883/0001-PIG-3285-Add-HBase-dependency-jars.patch,19/Apr/13 23:32;ndimiduk;1.pig;https://issues.apache.org/jira/secure/attachment/12579634/1.pig,19/Apr/13 23:32;ndimiduk;1.txt;https://issues.apache.org/jira/secure/attachment/12579635/1.txt,19/Apr/13 23:32;ndimiduk;2.pig;https://issues.apache.org/jira/secure/attachment/12579636/2.pig,,,9.0,,,,,,,,,,,,,,,,,,,2013-04-20 17:50:43.695,,,no_permission,,,,,,,,,,,,324095,,,,Mon Nov 25 16:36:50 UTC 2013,,,,,,,0|i1jwzj:,324440,,,,,,,,,,"19/Apr/13 23:32;ndimiduk;Attaching example scripts to demonstrate the issue. Repro steps look something like:
# hadoop -put 1.txt to your home directory.
# Make sure HBase is running.
# Execute 1.pig.","20/Apr/13 17:50;daijy;The problem is we don't ship protobuf.jar, here is the code:
{code}
TableMapReduceUtil.addDependencyJars(job.getConfiguration(),
            org.apache.hadoop.hbase.client.HTable.class,
            com.google.common.collect.Lists.class,
            org.apache.zookeeper.ZooKeeper.class)
{code}

One other issue is we don't have protobuf.jar in Pig codebase. If we add a protobuf class, Pig cannot compile. We would need to add a compilation dependency for protobuf.","22/Apr/13 19:11;ndimiduk;This patch invokes the correct incantation of {{TableMapReduceUtils#addDependencyJars}}. Specifically, it allows HBase to add the jars to the job on Pig's behalf. Note that the HBase jars must be on the Pig classpath for job launching to succeed. This is provided via PIG-2786 and can be invoked by defining environment variables for {{HBASE_HOME}} and {{HBASE_CONF_DIR}}.",22/Apr/13 19:13;ndimiduk;The reason this patch works is that protobuf et al are shipped with HBase and are available on the launch script's classpath via {{$HBASE_HOME}}.,"23/Apr/13 18:07;daijy;TableMapReduce.addDependencyJars(job) works for HBase 0.95+. For HBase 0.94, there is a bug which does not add hbase.jar. Also a side effect for addDependencyJars(job) is it adds hadoop.jar and pig.jar into tmpjars. Both of which are already taken care of by Pig. I am not sure if we double ship those jars if we doing this. Actually I would prefer a TableMapReduce.addDependencyJars version which only adds hbase.jar/guava.jar/protobuf.jar and additional dependencies when hbase evolves (but no hadoop.jar/pig.jar)","23/Apr/13 20:05;ndimiduk;Ah, right. This bug was fixed in HBASE-8146, which will go out with hbase-0.94.7. I'll update the patch to smooth over both versions.",23/Apr/13 20:15;ndimiduk;Updated patch smoothes over hbase-0.94 versions.,30/Apr/13 16:49;ndimiduk;bump.,"30/Apr/13 17:49;rohini;bq. I am not sure if we double ship those jars if we doing this. Actually I would prefer a TableMapReduce.addDependencyJars version which only adds hbase.jar/guava.jar/protobuf.jar and additional dependencies when hbase evolves (but no hadoop.jar/pig.jar)

Nick, Looking at the code I am sure we will end up double shipping jars which is very inefficient. It would be good to write a separate function instead of TableMapReduce.addDependencyJars(job) that filters out pig and hadoop jars (classes starting with org.apache.pig and org.apache.hadoop) and those in pigContext.extraJars from the list of classes in TableMapReduce.addDependencyJars(job) and then set them on tmpjars. You can reuse JarManager.findContainingJar to find the jar for a class file.","30/Apr/13 19:04;ndimiduk;Good point, [~rohini]. Would it be instead relevant for Hadoop to remove duplicates in the ""tmpjars"" list?

Also tangentially relevant, have a look at HBASE-8438 for building a minimal set of classpath additions. This will minimize the number of places jars can be pulled from.","30/Apr/13 20:24;rohini;Nick,
 You can raise a jira in Hadoop to handle duplicates. But since we support all older versions, we can't rely on it. Also it will not help with the current problem anyways. 

  The problem here is that hbase code is setting some jars in tmpjars which copies the jar to hdfs to /user/[username]/.staging and adds that hdfs file to DistributedCache.addArchiveToClassPath when JobClient.submitJob() is done. Pig already puts the pig.jar as job.jar and it ships the other registered jar to a tmp location in hdfs (/tmp/...) and then does a DistributedCache.addFileToClassPath before submitting the job. In this case, all the three settings are different and since pig does not use tmpfiles or tmpjars and does the work by itself the hdfs path is also different. So duplicates have to be resolved at the pig level. ","30/Apr/13 20:46;ndimiduk;Gross. Too many code-paths. How then to proceed? Can we consolidate on a single, approved method of shipping job dependencies?

(cc [~apurtell] since you were interested in the related HIVE-2055)","30/Apr/13 21:59;rohini;Not too many code paths. For hbase storage, need to write a method that
   * Filters out pig and hadoop classes from the list of classes so that pig and hadoop jar are not included. 
   * Find the jars for the other classes and filter out any jars already present in PigContext.extrajars and add only the rest to tmpjars.
 
  Also ensure that you add HTable.class apart from Zookeeper, inputformat, input/output key/value, partitioner and combiner classes. ","01/May/13 01:44;ndimiduk;bq. Not too many code paths.

Sure there are. Both Pig and HBase are replicating the behavior of {{ToolRunner}}'s libjars argument for including jars with a job. They do so in slightly different ways, but thus we have 3 different code-paths. I'd prefer consolidation on a single code-path.

bq. Filters out pig and hadoop classes from the list of classes so that pig and hadoop jar are not included.

We can add a method, something like {{addHBaseDependencyJars(Job)}} which will add only HBase and it's dependency jars (currently: zookeeper, protobuf, guava), nothing else. That way, we're not including any redundant Pig or Hadoop jars and HBase is managing it's own dependencies (meaning Pig won't have to change every time we change something). This is effectively the same doing what you say above, ""Also ensure that you add HTable.class apart from Zookeeper, inputformat, input/output key/value, partitioner and combiner classes,"" that is, omitting inputformat, keys, values, partitioner, combiner. Does that sound like it'll accomplish what this filter intends?

bq. Find the jars for the other classes and filter out any jars already present in PigContext.extrajars and add only the rest to tmpjars.

How do we access the PigContext? Is it in the jobConf or some such? I'd rather not put Pig-specific code in the bowels of HBase mapreduce code; my preference is to build generic APIs that can be used across the board.

HBase APIs are designed to assist people writing raw MR jobs against HBase (ie, including key/value classes, input/output format classes, &c). The slightly different requirements of Pig and Hive need to be addressed as well.","01/May/13 17:10;rohini;Actually I got a little confused with the patch as TableMapReduce.addDependencyJars(job) was adding all those and focused just on Daniel's comment. If your intention is to only add protobuf jar, you can do a Class.forName(some protobuf class name) and if that does not throw a CNFE (meaning older hbase versions) you can pass that class also to the TableMapreduceUtil.addDependencyJars(Configuration conf, Class... classes)","01/May/13 22:11;daijy;Agree with Rohini, that should be a simple fix to add protobuf.jar. We don't need to double ship jars and make things complicated.","06/May/13 17:07;ndimiduk;I don't want this to become a game of dependency tracking across projects. That said, HBase doesn't add dependencies all that often, so maybe it doesn't matter in practice.","06/May/13 18:32;daijy;If HBase adding another method only adding hbase.jar/guava.jar/protobuf.jar etc(but not inputformat/outputformat jar), Pig can switch to use that. But for now, TableMapReduce.addDependencyJars(job) is not what we want.",03/Jun/13 22:09;rohini;Canceling patch for now so that it does not show in Patch Available list. ,09/Aug/13 00:38;ndimiduk;Hi [~daijy]. Have a look at the patch on HBASE-9165. It include a new method for this use-case: TableMapReduceUtil#addHBaseDependencyJars(Configuration).,29/Aug/13 23:13;daijy;That sounds good. We can switch to it for newer version of hbase once HBASE-9165 committed. ,30/Aug/13 00:36;ndimiduk;[~daijy] would you mind commenting positively on the HBase ticket as well? Thanks.,"15/Nov/13 18:29;ndimiduk;FYI HBASE-9165 is committed to 0.96 and trunk. It will be available with HBase 0.96.1. I hope to also backport it to 0.94, available on 0.94.14 and later. The new method is [TableMapReduceUtil#addHBaseDependencyJars(Configuration)|https://github.com/apache/hbase/commit/8d844e61a55c93bf23cf9427e02f9814b66d68a8#diff-7d445ba2b99d6f251da23153a1cc82f4R560]. I'm now using it in a new hbase bin script command {{mapredcp}}, as of HBASE-8438. Its output is as follows:

{noformat}
$ ./bin/hbase mapredcp | tr ':' '\n'
/private/tmp/hbase-0.97.0-SNAPSHOT/lib/netty-3.6.6.Final.jar
/private/tmp/hbase-0.97.0-SNAPSHOT/lib/hbase-hadoop-compat-0.97.0-SNAPSHOT.jar
/private/tmp/hbase-0.97.0-SNAPSHOT/lib/protobuf-java-2.5.0.jar
/private/tmp/hbase-0.97.0-SNAPSHOT/lib/guava-12.0.1.jar
/private/tmp/hbase-0.97.0-SNAPSHOT/lib/htrace-core-2.01.jar
/private/tmp/hbase-0.97.0-SNAPSHOT/lib/hbase-protocol-0.97.0-SNAPSHOT.jar
/private/tmp/hbase-0.97.0-SNAPSHOT/lib/hbase-client-0.97.0-SNAPSHOT.jar
/private/tmp/hbase-0.97.0-SNAPSHOT/lib/zookeeper-3.4.5.jar
/private/tmp/hbase-0.97.0-SNAPSHOT/lib/hbase-server-0.97.0-SNAPSHOT.jar
/private/tmp/hbase-0.97.0-SNAPSHOT/lib/hbase-common-0.97.0-SNAPSHOT.jar
{noformat}","15/Nov/13 20:45;hudson;SUCCESS: Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #838 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/838/])
HBASE-9165 [mapreduce] Modularize building dependency jars

 - Separate adding HBase and dependencies from adding other job
   dependencies, and expose it as a separate method that other
   projects can use (for PIG-3285).
 - Explicitly add hbase-server to the list of dependencies we ship
   with the job, for users who extend the classes we provide (see
   HBASE-9112).
 - Add integration test for addDependencyJars.
 - Code reuse for TestTableMapReduce. (ndimiduk: rev 1542341)
* /hbase/trunk/hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestTableMapReduceUtil.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/mapred/TableMapReduceUtil.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/IdentityTableMapper.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableMapReduce.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableMapReduceBase.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableMapReduceUtil.java
",15/Nov/13 22:08;ndimiduk;Backport to HBase 0.94.14 is complete.,"15/Nov/13 22:17;ndimiduk;Here's an updated patch. It would be nice to test and confirm end-to-end. Testing in my local checkout results in an NPE; I assume I'm doing something wrong.

{noformat}
$ ant clean && ant && ant -Dtestcase=TestHBaseStorage test
...
Testcase: org.apache.pig.test.TestHBaseStorage took 0 sec
        Caused an ERROR
null not an instance of org.apache.hadoop.hbase.MiniHBaseCluster
java.lang.RuntimeException: null not an instance of org.apache.hadoop.hbase.MiniHBaseCluster
        at org.apache.hadoop.hbase.HBaseTestingUtility.getMiniHBaseCluster(HBaseTestingUtility.java:701)
        at org.apache.hadoop.hbase.HBaseTestingUtility.getHBaseCluster(HBaseTestingUtility.java:1591)
        at org.apache.pig.test.TestHBaseStorage.oneTimeTearDown(TestHBaseStorage.java:92)
{noformat}","15/Nov/13 22:31;jarcec;I really do like idea behind this patch, thank you for driving the effort on both sides  [~ndimiduk]! Do you think that we can leave the older code laying around for couple of Pig releases and switch between those two conditionally based on the HBase version? I'm concerned a bit about backward compatibility here as this patch will make pig not working on a HBase releases where it is currently working.",15/Nov/13 22:38;ndimiduk;More compatibility is better. I think that can be done with a little reflection magic. That logic will be pretty hairy as it spans at least 4 HBase releases. I'm not familiar with Pig internals so I'll leave a more sophisticated patch up to a maintainer (unless someone can illustrate how I can test my patches).,"15/Nov/13 22:55;jarcec;Yeah I agree, more compatibility is definitely better. Considering that the class {{TableMapReduceUtil}} have ""always"" been there, it might become fairly straightforward. Do you think that following snippet might work?

{code}
Method m = TableMapReduceUtil.class.getMethod(""addHBaseDependencyJars"", Configuration.class);
if(m != null) {
  m.invoke(null, job.getConfiguration());
} else {
  // Old code
}
{code}

And then the basic sanity testing would be with changing the HBase version on the command line, for example:

{code}
ant clean test -Dtestcase=TestHBaseStorage -Dhbase.version=0.94.1
ant clean test -Dtestcase=TestHBaseStorage -Dhbase.version=0.94.14
{code}","15/Nov/13 23:02;ndimiduk;I suspect that would work, but I've been bitten enough by the nuances of indirect dependency resolution at runtime that I'll withhold judgement until running a job on a real cluster ;)

You'll also need to add permutations for 0.96.0 and 0.96.1. At least with this single API, we should be able to iterate on our side without repercussions for Pig.","16/Nov/13 04:05;hudson;SUCCESS: Integrated in HBase-TRUNK #4681 (See [https://builds.apache.org/job/HBase-TRUNK/4681/])
HBASE-9165 [mapreduce] Modularize building dependency jars

 - Separate adding HBase and dependencies from adding other job
   dependencies, and expose it as a separate method that other
   projects can use (for PIG-3285).
 - Explicitly add hbase-server to the list of dependencies we ship
   with the job, for users who extend the classes we provide (see
   HBASE-9112).
 - Add integration test for addDependencyJars.
 - Code reuse for TestTableMapReduce. (ndimiduk: rev 1542341)
* /hbase/trunk/hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestTableMapReduceUtil.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/mapred/TableMapReduceUtil.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/IdentityTableMapper.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableMapReduce.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableMapReduceBase.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableMapReduceUtil.java
","16/Nov/13 15:49;hudson;FAILURE: Integrated in hbase-0.96 #190 (See [https://builds.apache.org/job/hbase-0.96/190/])
HBASE-9165 [mapreduce] Modularize building dependency jars

 - Separate adding HBase and dependencies from adding other job
   dependencies, and expose it as a separate method that other
   projects can use (for PIG-3285).
 - Explicitly add hbase-server to the list of dependencies we ship
   with the job, for users who extend the classes we provide (see
   HBASE-9112).
 - Add integration test for addDependencyJars.
 - Code reuse for TestTableMapReduce.

Note this patch differs from the one applied to TRUNK/0.98 in that it omits
changes to TestTableMapReduceUtil which have not yet been backported from
HBASE-8534. This can be addressed when that patch is backported (HBASE-9484). (ndimiduk: rev 1542342)
* /hbase/branches/0.96/hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestTableMapReduceUtil.java
* /hbase/branches/0.96/hbase-server/src/main/java/org/apache/hadoop/hbase/mapred/TableMapReduceUtil.java
* /hbase/branches/0.96/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/IdentityTableMapper.java
* /hbase/branches/0.96/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java
* /hbase/branches/0.96/hbase-server/src/test/java/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java
* /hbase/branches/0.96/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableMapReduce.java
* /hbase/branches/0.96/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableMapReduceBase.java
","16/Nov/13 18:21;hudson;FAILURE: Integrated in HBase-0.94 #1203 (See [https://builds.apache.org/job/HBase-0.94/1203/])
HBASE-9165 [mapreduce] Modularize building dependency jars

Separate adding HBase and dependencies from adding other job dependencies, and
expose it as a separate method that other projects can use (for PIG-3285,
HIVE-2055). (ndimiduk: rev 1542414)
* /hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/mapred/TableMapReduceUtil.java
* /hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java
* /hbase/branches/0.94/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableMapReduce.java
","16/Nov/13 19:08;hudson;SUCCESS: Integrated in HBase-0.94-security #337 (See [https://builds.apache.org/job/HBase-0.94-security/337/])
HBASE-9165 [mapreduce] Modularize building dependency jars

Separate adding HBase and dependencies from adding other job dependencies, and
expose it as a separate method that other projects can use (for PIG-3285,
HIVE-2055). (ndimiduk: rev 1542414)
* /hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/mapred/TableMapReduceUtil.java
* /hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java
* /hbase/branches/0.94/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableMapReduce.java
","16/Nov/13 19:57;hudson;FAILURE: Integrated in hbase-0.96-hadoop2 #120 (See [https://builds.apache.org/job/hbase-0.96-hadoop2/120/])
HBASE-9165 [mapreduce] Modularize building dependency jars

 - Separate adding HBase and dependencies from adding other job
   dependencies, and expose it as a separate method that other
   projects can use (for PIG-3285).
 - Explicitly add hbase-server to the list of dependencies we ship
   with the job, for users who extend the classes we provide (see
   HBASE-9112).
 - Add integration test for addDependencyJars.
 - Code reuse for TestTableMapReduce.

Note this patch differs from the one applied to TRUNK/0.98 in that it omits
changes to TestTableMapReduceUtil which have not yet been backported from
HBASE-8534. This can be addressed when that patch is backported (HBASE-9484). (ndimiduk: rev 1542342)
* /hbase/branches/0.96/hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestTableMapReduceUtil.java
* /hbase/branches/0.96/hbase-server/src/main/java/org/apache/hadoop/hbase/mapred/TableMapReduceUtil.java
* /hbase/branches/0.96/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/IdentityTableMapper.java
* /hbase/branches/0.96/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java
* /hbase/branches/0.96/hbase-server/src/test/java/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java
* /hbase/branches/0.96/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableMapReduce.java
* /hbase/branches/0.96/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableMapReduceBase.java
","18/Nov/13 01:43;jarcec;Yeah, I hear your concern [~ndimiduk]. Nevertheless I'll be more than happy to work with you to ensure that all the required combinations will work. I have to say though that I'm not a Pig committer and thus we will need someone else to do the commit.","18/Nov/13 18:41;cheolsoo;[~jarcec], please upload a new patch that includes what you suggest. Committing it is not a problem at all.","18/Nov/13 22:14;jarcec;Thank you for your support [~cheolsoo]!

 Do you feel comfortable with making the suggested changes to the patch [~ndimiduk] or do you want me to jump in and do them?",18/Nov/13 23:15;ndimiduk;Here's an updated patch that does as you suggest.,"19/Nov/13 21:52;jarcec;Thank you [~ndimiduk]! Couple of high level notes:

{code}
-      TableMapReduceUtil.addDependencyJars(job.getConfiguration(), klass);
{code}

It seems that we've accidentally removed the call {{TableMapReduceUtil.addDependencyJars()}}  from method {{addClassToJobIfExists()}}. As a result not all required jars are propagated to the {{DistributedCache}}  when running on HBase 0.95+.

{code}
+            // method doesn't exist or invocation failed. moving on.
{code}

Can we at least do a debug log message stating that {{TableMapReduceUtil.addHBaseDependencyJars}} is not available and thus we will revert to our own logic for propagating jars? It think that it will help to know what logic we used for propagating jars while investigating failing pig scripts.","19/Nov/13 22:23;ndimiduk;Thanks for the review, [~jarcec]. Good catch on the unintended change to addClassToJobIfExists.

Attaching an updated patch addressing both comments.","19/Nov/13 23:23;jarcec;Thank you for the quick turnaround [~ndimiduk]!

I was able to run {{TestHBaseStorage}} unit tests for all the combinations using following commands:

{code}
# Old path:
ant clean test -Dhbaseversion=94 -Dtestcase=TestHBaseStorage
ant clean test -Dhbaseversion=95 -Dtestcase=TestHBaseStorage  -Dprotobuf-java.version=2.5.0

# New path:

# Compile HBase 0.94 branch and install it to local maven cache (mvn clean install -DskipTests)
ant clean test -Dhbaseversion=94 -Dtestcase=TestHBaseStorage -Dhbase94.version=0.94.14
# Compile HBase 0.96 branch and install it to local maven cache (mvn clean install -DskipTests)
ant clean test -Dhbaseversion=95 -Dtestcase=TestHBaseStorage  -Dprotobuf-java.version=2.5.0 -Dhbase95.version=0.96.1-SNAPSHOT
{code}

The only problem that I've encounter so far is the exception {{ReflectiveOperationException}} ([javadoc|http://docs.oracle.com/javase/7/docs/api/java/lang/ReflectiveOperationException.html]) that has been added in JDK7, while Pig still requires JDK6 compatibility. I've substituted the exception with {{Exception}}  for the purpose of above tests.

I'll try to give it a spin on a real clusters to see if everything will work as expected.
","20/Nov/13 01:12;ndimiduk;Excellent. I didn't realize ReflectiveOperationException was introduced in JDK7, nice catch. Thanks for taking this the last mile!","20/Nov/13 22:47;jarcec;I've tested the change on a real clusters running HBase 0.94 with and without the HBASE-9165 and everything seems to be working just fine. It would be great to also test 0.96 with and without the change, but I do not feel that it's entirely necessary. I'm +1 on the patch, provided that we will remove the JDK7 only class {{ReflectiveOperationException}}.","20/Nov/13 23:05;ndimiduk;Updating patch to not use ReflectiveOperationException.

Thanks for testing! Would you mind taking it for a spin with hbase-0.94.14RC1? It was just posted and includes all necessary patches required to test the new logic.

http://people.apache.org/~larsh/hbase-0.94.14-rc1/","23/Nov/13 17:44;jarcec;My apologies for the delay [~ndimiduk]. I've tried the latest patch against linked HBase RC and everything seems to be working.

+1 (non-binding)

Feel free to take a look [~cheolsoo]!","24/Nov/13 23:35;cheolsoo;+1. I ran TestHBaseStorage myself, and I trust Jarcec's test.",24/Nov/13 23:45;cheolsoo;Committed to trunk. Thank you Nick and Jarcec!,25/Nov/13 16:36;jarcec;Thank you [~cheolsoo]!,,,,,,,,,,,,,,,,,,,,,,,,,,,,
POSplit ignoring error from input processing giving empty results ,PIG-3271,12641706,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,knoguchi,knoguchi,knoguchi,09/Apr/13 19:50,14/Oct/13 16:46,14/Mar/19 03:07,11/Apr/13 19:04,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"Script below fails at union onschema due to PIG-3270 but pig ignores its error and creates empty outputs with return code 0 (SUCCESS).
{noformat}
t1 = LOAD 'file1.txt' USING PigStorage() AS (a: chararray, b: chararray);
t2 = LOAD 'file2.txt' USING PigStorage() AS (a: chararray, b: float);
tout = UNION ONSCHEMA t1, t2;
STORE tout INTO './out1' USING PigStorage();
STORE tout INTO './out2' USING PigStorage();
{noformat}

Is POSplit ignoring the error from input processing?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Apr/13 19:57;knoguchi;pig-3271-v01.patch;https://issues.apache.org/jira/secure/attachment/12577873/pig-3271-v01.patch,10/Apr/13 19:12;knoguchi;pig-3271-v02.patch;https://issues.apache.org/jira/secure/attachment/12578064/pig-3271-v02.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-04-09 20:19:32.058,,,no_permission,,,,,,,,,,,,322121,Reviewed,,,Thu Apr 11 19:04:58 UTC 2013,,,,,,,0|i1jkt3:,322466,,,,,,,,,,09/Apr/13 19:57;knoguchi;I'm having hard time tracking the code but this seems to catch the error.,09/Apr/13 20:19;daijy;Ready to go with a testcase.,"10/Apr/13 19:12;knoguchi;bq. Ready to go with a testcase.

I'm lost in this.
Original example pasted on the jira failed due to PIG-3270 and should be fixed thus cannot be used for the testcase for this PIG-3271.

Just to show how lost I am, created a test case that connects couple of operators to force the input processing to fail. (I need a testcase that doesn't throw an exception but returns POStatus.STATUS_ERR.)
",11/Apr/13 19:04;daijy;I am fine with the testcase. Patch committed to trunk. Thanks Koji!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Union onschema failing at runtime when merging incompatible types,PIG-3270,12641653,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,09/Apr/13 15:42,14/Oct/13 16:46,14/Mar/19 03:07,11/Apr/13 19:52,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"{noformat}
t1 = LOAD 'file1.txt' USING PigStorage() AS (a: chararray, b: chararray);
t2 = LOAD 'file2.txt' USING PigStorage() AS (a: chararray, b: float);
tout = UNION ONSCHEMA t1, t2;
dump tout;
{noformat}

Job fails with 
2013-04-09 11:37:37,817 [Thread-12] WARN  org.apache.hadoop.mapred.LocalJobRunner - job_local_0001
java.lang.Exception: org.apache.pig.backend.executionengine.ExecException: ERROR 2055: Received Error while processing the map plan.
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:399)
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 2055: Received Error while processing the map plan.
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:311)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:278)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:726)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:333)
        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:231)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
        at java.lang.Thread.run(Thread.java:680)

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Apr/13 21:16;knoguchi;pig-3270-v01.patch;https://issues.apache.org/jira/secure/attachment/12578080/pig-3270-v01.patch,10/Apr/13 22:46;knoguchi;pig-3270-v02.patch;https://issues.apache.org/jira/secure/attachment/12578100/pig-3270-v02.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-04-09 18:46:58.68,,,no_permission,,,,,,,,,,,,322069,Reviewed,,,Thu Apr 11 19:52:14 UTC 2013,,,,,,,0|i1jkhj:,322414,,,,,,,,,,"09/Apr/13 15:49;knoguchi;Before PIG-2071, this job would have dumped field 'b' as chararray instead of failing at the middle at runtime.  Reading that jira, I'm thinking this example should have failed at compile time with better error messages.  Am I understanding it correctly? ","09/Apr/13 18:46;daijy;This should not fail. When we merge chararray and numerical field, the result datatype is unknown (bytearray). Do an explain without MergeForEach (-t MergeForEach):
{code}
tout: (Name: LOStore Schema: a#11:chararray,b#12:bytearray)
|
|---tout: (Name: LOUnion Schema: a#11:chararray,b#12:bytearray)
    |
    |---tout: (Name: LOForEach Schema: a#1:chararray,b#2:bytearray)
    |   |   |
    |   |   (Name: LOGenerate[false,false] Schema: a#1:chararray,b#2:bytearray)ColumnPrune:InputUids=[1, 2]ColumnPrune:OutputUids=[1, 2]
    |   |   |   |
    |   |   |   a:(Name: Project Type: chararray Uid: 1 Input: 0 Column: 0)
    |   |   |   |
    |   |   |   (Name: Cast Type: bytearray Uid: 2)  <------- Wrong case
    |   |   |   |
    |   |   |   |---b:(Name: Project Type: chararray Uid: 2 Input: 1 Column: 0)
    |   |   |
    |   |   |---(Name: LOInnerLoad[0] Schema: a#1:chararray)
    |   |   |
    |   |   |---(Name: LOInnerLoad[1] Schema: b#2:chararray)
    |   |
    |   |---t1: (Name: LOForEach Schema: a#1:chararray,b#2:chararray)
    |       |   |
    |       |   (Name: LOGenerate[false,false] Schema: a#1:chararray,b#2:chararray)ColumnPrune:InputUids=[1, 2]ColumnPrune:OutputUids=[1, 2]
    |       |   |   |
    |       |   |   (Name: Cast Type: chararray Uid: 1)
    |       |   |   |
    |       |   |   |---a:(Name: Project Type: bytearray Uid: 1 Input: 0 Column: (*))
    |       |   |   |
    |       |   |   (Name: Cast Type: chararray Uid: 2)
    |       |   |   |
    |       |   |   |---b:(Name: Project Type: bytearray Uid: 2 Input: 1 Column: (*))
    |       |   |
    |       |   |---(Name: LOInnerLoad[0] Schema: a#1:bytearray)
    |       |   |
    |       |   |---(Name: LOInnerLoad[1] Schema: b#2:bytearray)
    |       |
    |       |---t1: (Name: LOLoad Schema: a#1:bytearray,b#2:bytearray)RequiredFields:null
    |
    |---tout: (Name: LOForEach Schema: a#3:chararray,b#4:bytearray)
        |   |
        |   (Name: LOGenerate[false,false] Schema: a#3:chararray,b#4:bytearray)ColumnPrune:InputUids=[3, 4]ColumnPrune:OutputUids=[3, 4]
        |   |   |
        |   |   a:(Name: Project Type: chararray Uid: 3 Input: 0 Column: 0)
        |   |   |
        |   |   (Name: Cast Type: bytearray Uid: 4)  <------- Wrong case
        |   |   |
        |   |   |---b:(Name: Project Type: float Uid: 4 Input: 1 Column: 0)
        |   |
        |   |---(Name: LOInnerLoad[0] Schema: a#3:chararray)
        |   |
        |   |---(Name: LOInnerLoad[1] Schema: b#4:float)
        |
        |---t2: (Name: LOForEach Schema: a#3:chararray,b#4:float)
            |   |
            |   (Name: LOGenerate[false,false] Schema: a#3:chararray,b#4:float)ColumnPrune:InputUids=[3, 4]ColumnPrune:OutputUids=[3, 4]
            |   |   |
            |   |   (Name: Cast Type: chararray Uid: 3)
            |   |   |
            |   |   |---a:(Name: Project Type: bytearray Uid: 3 Input: 0 Column: (*))
            |   |   |
            |   |   (Name: Cast Type: float Uid: 4)
            |   |   |
            |   |   |---b:(Name: Project Type: bytearray Uid: 4 Input: 1 Column: (*))
            |   |
            |   |---(Name: LOInnerLoad[0] Schema: a#3:bytearray)
            |   |
            |   |---(Name: LOInnerLoad[1] Schema: b#4:bytearray)
            |
            |---t2: (Name: LOLoad Schema: a#3:bytearray,b#4:bytearray)RequiredFields:null
{code}
We should not insert cast to bytes operation. It's probably in UnionOnSchemaSetter","10/Apr/13 21:16;knoguchi;bq. We should not insert cast to bytes operation. It's probably in UnionOnSchemaSetter

Ah, I see.  I saw that job was failing at POCast(DataByteArray) but didn't know that it would work without this cast. 

Writing a test.","10/Apr/13 22:46;knoguchi;bq. Writing a test.

Since the original job was failing at runtime due to invalid bytearray casting, I added a e2e test.",11/Apr/13 19:52;daijy;Patch committed to trunk. Thanks Koji!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HCatStorer fail in limit query,PIG-3267,12640609,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,03/Apr/13 21:14,14/Oct/13 16:46,14/Mar/19 03:07,12/Apr/13 00:30,0.10.1,0.11.1,0.9.2,,,,,0.11.2,0.12.0,,,,,,0,,,,,,,,,,,,,"The following query fail:
{code}
data = LOAD 'student.txt' as (name:chararray, age:int, gpa:double);
data_limited = limit data 10;
samples = foreach data_limited generate age as number;
store samples into 'samples' using org.apache.hcatalog.pig.HCatStorer('part_dt=20130101T010000T36');
{code}

Error happens before launching the second job. Error message:
{code}
Message: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://localhost:8020/user/hive/warehouse/samples/part_dt=20130101T010000T36 already exists
	at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:121)
	at org.apache.hcatalog.mapreduce.FileOutputFormatContainer.checkOutputSpecs(FileOutputFormatContainer.java:135)
	at org.apache.hcatalog.mapreduce.HCatBaseOutputFormat.checkOutputSpecs(HCatBaseOutputFormat.java:72)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.checkOutputSpecsHelper(PigOutputFormat.java:207)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.checkOutputSpecs(PigOutputFormat.java:188)
	at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:887)
	at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:850)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:850)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:824)
	at org.apache.hadoop.mapred.jobcontrol.Job.submit(Job.java:378)
	at org.apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs(JobControl.java:247)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.pig.backend.hadoop20.PigJobControl.mainLoopAction(PigJobControl.java:157)
	at org.apache.pig.backend.hadoop20.PigJobControl.run(PigJobControl.java:134)
	at java.lang.Thread.run(Thread.java:680)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:257)
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Apr/13 21:17;daijy;PIG-3267-1.patch;https://issues.apache.org/jira/secure/attachment/12576852/PIG-3267-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-12 00:01:27.524,,,no_permission,,,,,,,,,,,,321068,Reviewed,,,Fri Apr 12 00:30:56 UTC 2013,,,Patch Available,,,,0|i1jean:,321409,,,,,,,,,,"03/Apr/13 21:17;daijy;This is due to when we add a Limit job and convert the intermediate storer to temporary storer, we did not nullify the cached storer in POStore. End up we are still using HCatStorer in the intermediate job. Attach patch.",12/Apr/13 00:01;dvryaboy;Should we apply this to 0.11 too?,12/Apr/13 00:01;dvryaboy;(+1),12/Apr/13 00:30;daijy;Patch committed to both 0.11 and trunk as Dmitriy requested.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"mvn signanddeploy target broken for pigunit, pigsmoke and piggybank",PIG-3264,12639886,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,billgraham,billgraham,billgraham,29/Mar/13 21:32,14/Oct/13 16:45,14/Mar/19 03:07,29/Mar/13 21:49,,,,,,,,0.11.2,0.12.0,,,,,,0,,,,,,,,,,,,,"Build fails with:

{noformat}
[artifact:deploy] Invalid reference: 'pigunit'
{noformat}

Patch on the way.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Mar/13 21:42;billgraham;PIG_3264.1.patch;https://issues.apache.org/jira/secure/attachment/12576171/PIG_3264.1.patch,29/Mar/13 21:42;billgraham;PIG_3264_branch11.1.patch;https://issues.apache.org/jira/secure/attachment/12576170/PIG_3264_branch11.1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-04-01 19:30:19.784,,,no_permission,,,,,,,,,,,,320355,,,,Mon Apr 01 19:30:19 UTC 2013,,,,,,,0|i1j9w7:,320696,,,,,,,,,,29/Mar/13 21:42;billgraham;Attaching trunk and branch 11 patches.,01/Apr/13 19:30;daijy;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig contrib 0.11 doesn't compile on certain rpm systems,PIG-3262,12639690,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,mgrover,mgrover,mgrover,28/Mar/13 21:56,14/Oct/13 16:46,14/Mar/19 03:07,29/Mar/13 00:04,0.11,,,,,,,0.11.2,0.12.0,,,build,,,0,,,,,,,,,,,,,"While working on BIGTOP-870, I found out that Pig doesn't compile on certain rpm based systems and fails with an error like:
{code}
04:27:41  compile:
04:27:41       [echo]  *** Compiling Pig UDFs ***
04:27:41      [javac] /mnt/jenkins/workspace/Bigtop-trunk-Pig/label/centos6/build/pig/rpm/BUILD/pig-0.11.0/contrib/piggybank/java/build.xml:93: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds
04:27:41      [javac] Compiling 158 source files to /mnt/jenkins/workspace/Bigtop-trunk-Pig/label/centos6/build/pig/rpm/BUILD/pig-0.11.0/contrib/piggybank/java/build/classes
04:27:41      [javac] /mnt/jenkins/workspace/Bigtop-trunk-Pig/label/centos6/build/pig/rpm/BUILD/pig-0.11.0/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/IsInt.java:31: unmappable character for encoding ASCII
04:27:41      [javac]  * Note this function checks for Integer range ???2,147,483,648 to 2,147,483,647.
04:27:41      [javac]                                                ^
04:27:41      [javac] /mnt/jenkins/workspace/Bigtop-trunk-Pig/label/centos6/build/pig/rpm/BUILD/pig-0.11.0/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/IsInt.java:31: unmappable character for encoding ASCII
04:27:41      [javac]  * Note this function checks for Integer range ???2,147,483,648 to 2,147,483,647.
04:27:41      [javac]                                                 ^
04:27:41      [javac] /mnt/jenkins/workspace/Bigtop-trunk-Pig/label/centos6/build/pig/rpm/BUILD/pig-0.11.0/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/IsInt.java:31: unmappable character for encoding ASCII
04:27:41      [javac]  * Note this function checks for Integer range ???2,147,483,648 to 2,147,483,647.
04:27:41      [javac]                                                  ^
04:27:42      [javac] 3 errors
{code}

This seems to be related to encoding not being set for Javac in the build.xml file.",,,,,,,,,,,,,,,,,,,BIGTOP-894,,,,,,,,,,,,,28/Mar/13 22:20;mgrover;PIG-3262.1.patch;https://issues.apache.org/jira/secure/attachment/12575961/PIG-3262.1.patch,28/Mar/13 22:22;mgrover;PIG-3262.2.patch;https://issues.apache.org/jira/secure/attachment/12575962/PIG-3262.2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-03-28 23:49:47.438,,,no_permission,,,,,,,,,,,,320159,,,,Tue Apr 09 16:29:22 UTC 2013,,,,,,,0|i1j8on:,320500,,,,,,,,,,28/Mar/13 22:22;mgrover;Had an extra whitespace in the previous patch,28/Mar/13 23:11;mgrover;FYI: The patch applies cleanly on both {{trunk}} as well as {{branch-0.11}},28/Mar/13 23:49;cheolsoo;+1. I ran into this in the past by myself. The minus sign makes packaging fail on certain OSes.,"29/Mar/13 00:04;cheolsoo;Committed to trunk. I didn't commit to 0.11.1 because the RC is already on vote, and I don't think we want to respin another RC for this.","29/Mar/13 16:42;mgrover;[~cheolsoo] It's ok to commit to just trunk if the vote is going on. And, thanks for committing this!","29/Mar/13 20:53;rvs;Cheolsoo, it would be *extremely* appreciated if this can be made part of 0.11.1 since then it would make Bigtop use one less workaround. Any chance this could happen?","29/Mar/13 21:37;mgrover;Actually, that would be really nice. In general, Apache Bigtop can help out with testing of Pig release candidates so we can find issues like this before a release goes out. In particular, I would be happy to help out with integration testing of the new release candidate against Hadoop 2 via Apache Bigtop, if it does get released.","29/Mar/13 22:48;cheolsoo;[~rvs], the vote is passed now.

http://search-hadoop.com/m/p847u1NntBe/%255BVOTE%255D+Release+Pig+0.11.1+%2528candidate+0%2529&subj=+VOTE+Release+Pig+0+11+1+candidate+0+

Can you please express your concern to the pig-dev mailing list? I believe that Bill is going to proceed the release otherwise. ","29/Mar/13 23:01;mgrover;OK, let me do that.","02/Apr/13 00:39;mgrover;If someone runs into this same issue, please try doing the following before the build:
{code}
export LC_ALL=en_US.UTF-8
{code}

The credit for the above workaround goes to Bruno Mahé.",09/Apr/13 16:28;cheolsoo;I committed the patch to 0.11.2.,09/Apr/13 16:29;rvs;Thanks a million!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"User set PIG_CLASSPATH entries must be prepended to the CLASSPATH, not appended",PIG-3261,12639024,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,qwertymaniac,qwertymaniac,qwertymaniac,26/Mar/13 01:47,14/Oct/13 16:45,14/Mar/19 03:07,07/Apr/13 06:59,0.10.0,,,,,,,0.12.0,,,,grunt,,,0,,,,,,,,,,,,,"Currently we are doing this wrong:

{code}
if [ ""$PIG_CLASSPATH"" != """" ]; then
    CLASSPATH=${CLASSPATH}:${PIG_CLASSPATH}
{code}

This means that anything added to CLASSPATH until that point will never be able to get overridden by a user set environment, which is wrong behavior. Hadoop libs for example are added to CLASSPATH, before this extension is called in bin/pig.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Apr/13 09:37;qwertymaniac;PIG-3261.patch;https://issues.apache.org/jira/secure/attachment/12577365/PIG-3261.patch,04/Apr/13 06:03;qwertymaniac;PIG-3261.patch;https://issues.apache.org/jira/secure/attachment/12576933/PIG-3261.patch,04/Apr/13 05:59;qwertymaniac;PIG-3261.patch;https://issues.apache.org/jira/secure/attachment/12576931/PIG-3261.patch,27/Mar/13 04:31;qwertymaniac;PIG-3261.patch;https://issues.apache.org/jira/secure/attachment/12575652/PIG-3261.patch,26/Mar/13 01:48;qwertymaniac;PIG-3261.patch;https://issues.apache.org/jira/secure/attachment/12575450/PIG-3261.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2013-03-26 06:29:56.785,,,no_permission,,,,,,,,,,,,319494,Reviewed,,,Sun Apr 07 06:59:20 UTC 2013,,,,,,,0|i1j4kn:,319835,,,,,,,,,,26/Mar/13 06:29;prkommireddi;Thanks [~qwertymaniac]. Do you think user set classpath should always be added at the beginning? Or would it make sense to have a property similar to HADOOP_USER_CLASSPATH_FIRST ?,"26/Mar/13 08:21;qwertymaniac;We can make it configurable and document that I guess; but its kinda odd to have to do two toggles to get an override done. In most cases of an override requirement, users are aware of the overriding so the secondary toggle seems a tad unnecessary.

If you prefer that strongly, I'll send in another patch - let me know :)

An alternate fix would be to simply do the PIG_CLASSPATH addition before anything else is added to CLASSPATH, but this kinda position-in-code fix is harder to maintain over time.","26/Mar/13 08:31;prkommireddi;I am actually happy with this patch. Looking through hadoop JIRAs, documentation and comments in bin/hadoop script I could not clearly comprehend the reason for existence of the prop HADOOP_USER_CLASSPATH_FIRST. Making sure we don't miss it here if there's a legit reason, otherwise PIG_CLASSPATH is set generally when a user has certain custom jar/classpath requirements. Like you said, I don't think a user would set PIG_CLASSPATH but want default CLASSPATH to have a precedence.","26/Mar/13 11:33;qwertymaniac;IIRC the reason was to not have them step over the shipped library jars unintentionally with a simple HADOOP_CLASSPATH being set. I guess we can add a toggle instead of changing the behavior, would be safer. I'll update the patch.","27/Mar/13 04:31;qwertymaniac;Patch revised. Added an env-opt toggler PIG_USER_CLASSPATH_FIRST that preserves today's behavior if unset (default).

Testing:

Export:
{{export PIG_CLASSPATH=Foo}}

Default behavior:
{code}
bash -x bin/pig
…
CLASSPATH=/Users/harshchouraria/Work/installs/pig/conf:/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home/lib/tools.jar:Foo
…
{code}

Set toggle:

{{export PIG_USER_CLASSPATH_FIRST=true}}

{code}
bash -x bin/pig
…
CLASSPATH=Foo:/Users/harshchouraria/Work/installs/pig/conf:/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home/lib/tools.jar
…
{code}

Disable toggle:

{{export PIG_USER_CLASSPATH_FIRST=}}

{code}
bash -x bin/pig
…
CLASSPATH=/Users/harshchouraria/Work/installs/pig/conf:/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home/lib/tools.jar:Foo
…
{code}

Unset toggle:

{{unset PIG_USER_CLASSPATH_FIRST}}

{code}
bash -x bin/pig
…
CLASSPATH=/Users/harshchouraria/Work/installs/pig/conf:/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home/lib/tools.jar:Foo
…
{code}","27/Mar/13 05:02;prkommireddi;{quote}
IIRC the reason was to not have them step over the shipped library jars unintentionally with a simple HADOOP_CLASSPATH being set
{quote}

Based on that, I feel like keeping it simple and not having a toggle is better for following reasons:

# Pig does not have a env file like hadoop does for specifying CLASSPATH. Most likely this would be set by the user, would be intentional and not be picked up from any of pig's env files.
# Having a toggle for this seems like an additional step towards the same purpose. 

What do you think [~qwertymaniac]? It would be nice to have some others weight in on this. I am leaning more towards your initial patch, though I am not opposed to the latest patch either.
","27/Mar/13 05:55;qwertymaniac;I agree on both points from my own experience, but others have probably seen even more users than I.

I'm not so very active on the user lists either, but have been a long time subscriber and searching shows PIG_CLASSPATH's only ever used for UDF and library additives, and hence the other intention's users (i.e. those who want to override what Pig auto discovers) wouldn't mind this behavior change either. Of course, this data set does not represent all of the users :)","03/Apr/13 05:43;prkommireddi;Hey guys, can we have more people weigh in on this JIRA. [~qwertymaniac] has a couple of patch' uploaded and it would be good to see what approach makes more sense.","03/Apr/13 18:34;knoguchi;I prefer with PIG_USER_CLASSPATH_FIRST.  I've seen too many random users including old pig jar in their custom UDFs...

In our environment, we perform QE on set of frameworks. (hadoop, pig, oozie, etc)
And we tell our users, whenever they set HADOOP_USER_CLASSPATH_FIRST they are running outside of the QA-ed environment.  I want the same to apply within pig with PIG_USER_CLASSPATH_FIRST.

","04/Apr/13 04:02;qwertymaniac;Thanks for the comment from experience Koji! I think we can use the most latest patch then, if it looks good :)

Re: Docs, I didn't find PIG_CLASSPATH documented anywhere and wasn't sure which doc page to add it to either, but if someone has a clue I'll doc both PIG_CLASSPATH and this on the pointed page and update the patch.","04/Apr/13 04:09;prkommireddi;Thanks Koji, that's a good explanation for needing PIG_USER_CLASSPATH_FIRST.

Harsh, you can find reference to PIG_CLASSPATH here http://pig.apache.org/docs/r0.11.1/start.html#Running+the+Pig+Scripts+in+Mapreduce+Mode

I think you can add a note there (suggesting its optional) regarding PIG_USER_CLASSPATH_FIRST.","04/Apr/13 05:59;qwertymaniac;Thanks, here's docs added along where pointed. It also explains PIG_CLASSPATH's other uses.","04/Apr/13 06:03;qwertymaniac;Missed moving the list element tag, which I've fixed in this patch.","05/Apr/13 01:20;prkommireddi;Hi [~qwertymaniac], thanks for updating the patch. Documentation looks good, I tested by building the same. 

Minor nitpick, I would prefer nested-if in this case as ""$PIG_CLASSPATH"" is tested twice. But that's probably just me :)
{code}
if [ ""$PIG_CLASSPATH"" != """" ] && [ ""$PIG_USER_CLASSPATH_FIRST"" == """" ]; then
    CLASSPATH=${CLASSPATH}:${PIG_CLASSPATH}
elif [ ""$PIG_CLASSPATH"" != """" ] && [ ""$PIG_USER_CLASSPATH_FIRST"" != """" ]; then
    CLASSPATH=${PIG_CLASSPATH}:${CLASSPATH}
fi
{code}

Otherwise, +1 (non-binding)","06/Apr/13 09:37;qwertymaniac;Certainly and anything for you!

That'd look better too; so done.

Thanks for the reviews and comments :)",07/Apr/13 06:59;daijy;Patch committed to trunk. Thanks Harsh!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Number of languages which support UDF is listed 3 instead of 5,PIG-3260,12638731,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,daijy,spawgi,spawgi,24/Mar/13 19:04,02/Apr/13 15:54,14/Mar/19 03:07,25/Mar/13 18:56,0.11,,,,,,,0.11.1,,,,documentation,,,0,,,,,,,,,,,,,"On the Pig UDF page - http://pig.apache.org/docs/r0.11.0/udf.html#udfs, it says that - ""Pig UDFs can currently be implemented in three languages: Java, Python, JavaScript, Ruby and Groovy."" However, these are 5 languages. Very minor probably typing mistake. But thought of reporting it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-03-25 18:56:22.747,,,no_permission,,,,,,,,,,,,319206,,,,Mon Mar 25 18:56:22 UTC 2013,,,,,,,0|i1j2sv:,319547,,,,,,,,,,25/Mar/13 18:56;daijy;Fixed in 0.11 branch. trunk is already fixed. Thanks for reporting!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade jython to 2.5.3 (legal concern),PIG-3256,12638091,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,daijy,daijy,daijy,20/Mar/13 21:28,02/Apr/13 15:54,14/Mar/19 03:07,21/Mar/13 01:00,,,,,,,,0.11.1,0.12.0,,,,,,0,,,,,,,,,,,,,"When we review the legal documents with Microsoft for Windows work, here is the recommend from lawyer:

Jython 2.5.2 redistributes an external LGPL component (JNA) in a manner that puts Jython out of compliance with the LGPL. As such dependent components like Pig are also arguably out of compliance with the LGPL.

It appears that this has been quietly found and fixed by the Jython guys, as version 2.5.3 does not include the JNA component. However the status of 2.5.2 with respect to the LGPL and Apache legal is still unclear.

The easiest way to remediate this whole problem is to simply move Pig to Jython 2.5.3 and remove the question.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Mar/13 21:30;daijy;PIG-3256-1.patch;https://issues.apache.org/jira/secure/attachment/12574644/PIG-3256-1.patch,20/Mar/13 23:12;daijy;PIG-3256-2.patch;https://issues.apache.org/jira/secure/attachment/12574668/PIG-3256-2.patch,21/Mar/13 00:20;daijy;PIG-3256-3.patch;https://issues.apache.org/jira/secure/attachment/12574687/PIG-3256-3.patch,21/Mar/13 00:42;daijy;PIG-3256-4.patch;https://issues.apache.org/jira/secure/attachment/12574691/PIG-3256-4.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-03-20 22:51:13.655,,,no_permission,,,,,,,,,,,,318569,Reviewed,,,Thu Mar 21 01:00:19 UTC 2013,,,,,,,0|i1iyvb:,318910,,,,,,,,,,20/Mar/13 22:51;rohini;+1. Should we put it in 0.11 branch also?,"20/Mar/13 22:52;rohini;build.xml also needs to be modified with the new version. 

{code}
<!-- Remove jython jar from mrapp-generated-classpath -->
        <script language=""javascript"">
          project.setProperty('mr-apps-classpath', project.getProperty('mr-apps-classpath').
           replace("":"" + project.getProperty('ivy.default.ivy.user.dir') + ""/cache/org.python/jython-standalone/jars/jython-standalone-2.5.2.jar"", """"));
        </script>
{code}","20/Mar/13 23:12;daijy;Thanks Rohini. Updated the patch to include your suggestion.

I would like to commit to 0.11 as well. The legal concern should be treated as critical.","21/Mar/13 00:13;rohini;<loadproperties srcfile=""${ivy.dir}/libraries.properties""/> is already there in build.xml. So the newly introduced line <loadproperties srcfile=""${basedir}/ivy/libraries.properties""/> can be removed. ","21/Mar/13 00:20;daijy;Yes, miss that. Upload again.","21/Mar/13 00:31;rohini;Sorry. One last comment. Should have caught this earlier. Should be project.getProperty('jython.version') instead of ${jython.version} as this is javascript. And thanks for fixing this. Its a miss on my part to hardcode the version.

{code}
replace("":"" + project.getProperty('ivy.default.ivy.user.dir') + ""/cache/org.python/jython-standalone/jars/jython-standalone-"" + project.getProperty('jython.version') + "".jar"", """"));
{code}",21/Mar/13 00:42;daijy;Attached new patch. Thanks again!,21/Mar/13 00:43;rohini;+1,21/Mar/13 01:00;daijy;Patch committed to 0.11 branch and trunk. Thanks Rohini!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid extra byte array copies in streaming,PIG-3255,12638067,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,20/Mar/13 19:56,14/Oct/13 16:45,14/Mar/19 03:07,17/Sep/13 19:15,0.11,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"PigStreaming.java:

 public Tuple deserialize(byte[] bytes) throws IOException {
        Text val = new Text(bytes);  
        return StorageUtil.textToTuple(val, fieldDel);
    }

Should remove new Text(bytes) copy and construct the tuple directly from the bytes",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Mar/13 20:51;rohini;PIG-3255-1.patch;https://issues.apache.org/jira/secure/attachment/12574632/PIG-3255-1.patch,28/Aug/13 15:40;rohini;PIG-3255-2.patch;https://issues.apache.org/jira/secure/attachment/12600406/PIG-3255-2.patch,09/Sep/13 02:37;rohini;PIG-3255-3.patch;https://issues.apache.org/jira/secure/attachment/12602079/PIG-3255-3.patch,13/Sep/13 20:14;rohini;PIG-3255-4.patch;https://issues.apache.org/jira/secure/attachment/12603090/PIG-3255-4.patch,16/Sep/13 18:40;rohini;PIG-3255-5.patch;https://issues.apache.org/jira/secure/attachment/12603391/PIG-3255-5.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2013-03-20 21:14:58.329,,,no_permission,,,,,,,,,,,,318545,,,,Tue Sep 17 19:15:29 UTC 2013,,,,,,,0|i1iypz:,318886,,,,,,,,,,"20/Mar/13 20:08;rohini;OutputHandler.java:
{code}
Text value = new Text();
        int num = in.readLine(value);
        if (num <= 0) {
            return null;
        }
        
        byte[] newBytes = new byte[value.getLength()];
        System.arraycopy(value.getBytes(), 0, newBytes, 0, value.getLength());
        return deserializer.deserialize(newBytes);
{code}

  We can cut down another copy here if value.getLength() == value.getBytes().length as that would be the case mostly. ","20/Mar/13 21:14;knoguchi;+1 Looks good to me. 
Probably another Jira, but I wonder if we really need to create new Text for every streaming outputs.  Can we reuse it with value.clear() ?
(But if we do this, then in most cases value.getBytes().length <> value.getLength().)","20/Mar/13 21:48;rohini;Had a chat with Koji. He pointed out HADOOP-6109 which doubles the size of byte[] in Text every time a append happens. 

Text.java

Hadoop 1.x
{code}
private void setCapacity(int len, boolean keepData) {
    if (bytes == null || bytes.length < len) {
      byte[] newBytes = new byte[len];
      if (bytes != null && keepData) {
        System.arraycopy(bytes, 0, newBytes, 0, length);
      }
      bytes = newBytes;
    }
  }
{code}

Hadoop 0.23/2.x:
{code}
private void setCapacity(int len, boolean keepData) {
    if (bytes == null || bytes.length < len) {
      if (bytes != null && keepData) {
        bytes = Arrays.copyOf(bytes, Math.max(len,length << 1));
      } else {
        bytes = new byte[len];
      }
    }
  }
{code}

So value.getBytes().length == value.getLength() will be true only when the size of the line is < io.file.buffer.size. Since a copy of the byte[] needs to be created with the right size in any case, we can go with reusing the Text() for every getNext() in OutputHandler. It will be more beneficial when the record sizes are greater than io.file.buffer.size and value.getBytes().length is almost never equal to value.getLength() because of the doubling of the size.

I will modify the patch to reuse Text object.","28/Aug/13 15:40;rohini;Attaching a patch, that gets rid of the byte array copy totally. But this requires a interface change and will cause backward incompatibility. If it was an abstract class could have added a default implementation retaining the old method. But since it is an interface, adding a new method will anyways give a runtime error. So removed the deserialize(bytes[]) method.  
 
I don't see any documentation on implementing StreamToPig. Not sure how many would actually be implementing this interface. Is this change acceptable for the sake of performance? Thoughts?","29/Aug/13 19:50;daijy;I personally does not realize anyone using StreamToPig, but need to check with [~alangates], since he marked it as public stable. Other part of the patch looks good. Avoiding 2 byte array copy and reuse Text object would save memory and enhance performance.","05/Sep/13 23:11;rohini;[~alangates],
   Comments ?",07/Sep/13 00:44;daijy;PIG-2417 stream udf use this interface. Need to get it solved before check in.,07/Sep/13 14:10;jeremykarn;I think this change makes sense and it should be easy for me to update the patch in PIG-2417 once this is merged in.,"09/Sep/13 01:54;rohini;If the interface change is ok, then thinking of changing even the PigToStream.java interface 

public byte[] serialize(Tuple t) throws IOException;

to 

public DataBuffer serialize(Tuple t) throws IOException;
 
 where DataBuffer will be same as 
http://svn.apache.org/viewvc/hadoop/common/branches/branch-2/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/DataOutputBuffer.java?revision=1306187&view=markup

Don't want to use DataOutputBuffer itself as it is marked 
@InterfaceAudience.LimitedPrivate({""HDFS"", ""MapReduce""})
@InterfaceStability.Unstable

This will get rid of one more byte array copy. Thoughts ? 
",09/Sep/13 02:37;rohini;https://reviews.apache.org/r/14030/,"11/Sep/13 23:56;alangates;I don't know if anyone is using StreamToPig either, but marking an interface as stable and then changing it without deprecation or anything isn't cool.  So no, I don't think this change is ok.

We could add the proposed function ""public Tuple deserialize(byte[] bytes, int offset, int length) throws IOException;"" to the interface and change Pig to call it if it's present or use the old one if not.  ","12/Sep/13 01:12;rohini;Alan,
   If you add a method to the interface, won't it break existing compiled code at runtime anyways ?",12/Sep/13 01:28;jeremykarn;If the performance gain is significant enough you could deprecate the existing interface and create a new interface that extends it adding just the new deserialize method.  It's ugly though.,"12/Sep/13 03:05;alangates;At compile time, but not at runtime.  At runtime Pig would need to reflect the class implementing StreamToPig and see if it contained a deserialize method that matches your new signature.  You could then pick which method to call based on that.  As Jeremy suggests, you could instead do that with a new interface (PigToStreamV2) and then at compile time determine which interface is being implemented and act accordingly.  This is actually better than what I initially suggested as the determination can be made at compile time.  If you choose this route you should also change PIgToStreamV2 to an abstract class so that in the future we can add methods without going through this dance.",12/Sep/13 14:15;rohini;Thanks Jeremy and Alan. I will go with a new v2 abstract class approach and deprecate the old one.,"12/Sep/13 14:32;rohini;Came across a good article - http://haacked.com/archive/2008/02/20/versioning-issues-with-abstract-base-classes-and-interfaces.aspx. Similar approach but cleaner to code. Idea is to create StreamToPigBase and PigToStreamBase abstract classes that implement StreamToPig and PigToStream interfaces respectively and add the new method there. In the InputHandler and OutputHandler check if it is an instanceof StreamToPigBase, then call new method else call the old interface one. With this don't have to check if v1 or v2 interface is implemented during reflection and change Input/OutputHandler and its implementations to set two different serializer/de-serializers. Will still go ahead and deprecate the interface so that it can be removed in the next release. ",12/Sep/13 18:21;alangates;+1,"13/Sep/13 20:14;rohini;Updated review -https://reviews.apache.org/r/14030/

Wish we had this in jdk 6 instead of 8 - http://blog.hartveld.com/2013/03/jdk-8-13-interface-default-method.html

One thing I did was create only abstract class PigStreamingBase instead of one each for the serializing and deserializing. Hope that is ok. ","16/Sep/13 18:40;rohini;Patch with the missing @Override for Daniel's comment in https://reviews.apache.org/r/14030/. 

[~alangates],
   Do you want me to wait for your code review on this patch or can I go ahead and commit with Daniel's +1?","17/Sep/13 18:29;alangates;I gave my +1 above, so we're good from my viewpoint.","17/Sep/13 19:15;rohini;Committed to trunk. Thanks Alan, Daniel and Jeremy",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Misleading comment w.r.t getSplitIndex() method in PigSplit.java,PIG-3253,12637822,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,cheolsoo,cheolsoo,cheolsoo,19/Mar/13 21:04,14/Oct/13 16:46,14/Mar/19 03:07,20/Mar/13 21:11,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"While reviewing the patch for PIG-3141, I noticed that the following comment is out-of-date:
{code:title=PigSplit.java}
// package level access because we don't want LoadFunc implementations
// to get this information - this is to be used only from
// MergeJoinIndexer
public int getSplitIndex() {
    return splitIndex;
}
{code}
Looking at the commit history, the public qualifier was added by PIG-1309, but the comment wasn't updated accordingly.

Provided that more and more LoadFunc implementations use this method (e.g. PIG-3141), we should remove this misleading comment to avoid any confusion.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Mar/13 21:09;cheolsoo;PIG-3253.patch;https://issues.apache.org/jira/secure/attachment/12574406/PIG-3253.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-19 23:53:25.788,,,no_permission,,,,,,,,,,,,318302,,,,Wed Mar 20 21:11:42 UTC 2013,,,,,,,0|i1ix7z:,318643,,,,,,,,,,19/Mar/13 21:09;cheolsoo;The patch removes the comment.,19/Mar/13 23:53;daijy;+1,20/Mar/13 21:11;cheolsoo;Committed to trunk. Thank you Daniel for the review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroStorage gives wrong schema for schemas with named records,PIG-3252,12637695,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,mwagner,mwagner,mwagner,19/Mar/13 01:44,02/Apr/13 15:54,14/Mar/19 03:07,19/Mar/13 18:29,0.11,,,,,,,0.11.1,0.12.0,,,piggybank,,,0,,,,,,,,,,,,,"Given the Avro schema:

{code}
{""type"":""record"",
 ""name"":""toplevel"",
 ""fields"":[{""name"":""a"",""
            ""type"":{""type"":""record"",
                    ""name"":""x"",
                    ""fields"":[{""name"":""key"",""type"":""int""},
                              {""name"":""value"",""type"":""string""}]}},
           {""name"":""b"",""type"":""x""}]}
{code}

we should get back the Pig schema

{code} {a: (key: int,value: string),b:(key: int,value: string)} {code}
but instead it is

{code} {a: (key: int,value: string),b: bytearray} {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Mar/13 01:50;mwagner;PIG-3252.1.patch;https://issues.apache.org/jira/secure/attachment/12574294/PIG-3252.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-19 18:11:44.792,,,no_permission,,,,,,,,,,,,318175,,,,Tue Mar 19 18:29:12 UTC 2013,,,,,,,0|i1iwfr:,318516,,,,,,,,,,19/Mar/13 01:50;mwagner;This was introduced with the added support for recursive schemas in [PIG-2875]. I've attached a patch to fix.,"19/Mar/13 18:11;cheolsoo;+1.

Thank you Mark for the fix!",19/Mar/13 18:29;cheolsoo;Committed to trunk and 0.11.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig dryrun generates wrong output in .expanded file for 'SPLIT....OTHERWISE...' command,PIG-3250,12637127,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dreambird,dreambird,dreambird,15/Mar/13 01:54,14/Oct/13 16:46,14/Mar/19 03:07,10/Jun/13 00:31,0.12.0,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"step to reproduce it:
1. input files 'users'
{noformat}
1
2
3
4
5
{noformat}

2. pig script split.pig
{noformat}
define group_and_count (A,key) returns B {
    SPLIT $A INTO $B IF $key<7, Y IF $key==5, Z OTHERWISE;
}
alpha = load '/var/lib/jenkins/users' as (f1:int);
gamma = group_and_count (alpha, f1);
store gamma into '/var/lib/jenkins/byuser';
{noformat}

3. run command
{noformat}
pig -x local -r split.pig
{noformat}

4. the content of split.pig.expanded
{noformat}
alpha = load '/var/lib/jenkins/users' as f1:int;
SPLIT alpha INTO gamma IF f1 < 7, macro_group_and_count_Y_0 IF f1 == 5OTHERWISE macro_group_and_count_Z_0;
store gamma INTO '/var/lib/jenkins/byuser';
{noformat}

the line ""....f1 == 5OTHERWISE macro_group_and_count_Z_0;"" is wrong, it should be ""f1 == 5, macro_group_and_count_Z_0 OTHERWISE""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Jun/13 20:21;dreambird;PIG-3250.patch.txt;https://issues.apache.org/jira/secure/attachment/12586167/PIG-3250.patch.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-07 05:17:36.682,,,no_permission,,,,,,,,,,,,317619,,,,Mon Jun 10 00:31:29 UTC 2013,,,,,,,0|i1it07:,317960,,,,,,,,,,"04/Jun/13 20:21;dreambird;with this patch, macro including SPLIT...OTHERWISE is expanded as described above. I added a test case for it as well.","07/Jun/13 05:17;xuefuz;+1

Thanks for the patch, Johnny.",10/Jun/13 00:31;cheolsoo;Committed to trunk. Thanks Johnny!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig startup script prints out a wrong version of hadoop when using fat jar,PIG-3249,12637100,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,prkommireddi,prkommireddi,prkommireddi,14/Mar/13 22:41,14/Oct/13 16:46,14/Mar/19 03:07,21/Mar/13 00:28,0.11,,,,,,,0.12.0,,,,,,,0,newbie,,,,,,,,,,,,"Script suggests 0.20.2 is used with the bundled jar but we are using 1.0 at the moment.

{code}
    # fall back to use fat pig.jar
    if [ ""$debug"" == ""true"" ]; then
        echo ""Cannot find local hadoop installation, using bundled hadoop 20.2""
    fi
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Mar/13 23:41;prkommireddi;PIG-3249.patch;https://issues.apache.org/jira/secure/attachment/12573978/PIG-3249.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-15 17:17:43.808,,,no_permission,,,,,,,,,,,,317592,Reviewed,,,Thu Mar 21 00:28:17 UTC 2013,,,Patch Available,,,,0|i1isu7:,317933,,,,,,,,,,"15/Mar/13 17:17;daijy;We can change to use ""java -cp pig.jar org.apache.hadoop.util.VersionInfo"" to get Hadoop version.","15/Mar/13 22:47;prkommireddi;Thanks Daniel, that's a nice approach. 

{code}
    # fall back to use fat pig.jar
    if [ -f $PIG_HOME/pig.jar ]; then
        PIG_JAR=$PIG_HOME/pig.jar
    else
        PIG_JAR=`echo $PIG_HOME/pig-?.!(*withouthadoop).jar`
    fi

    if [ -n ""$PIG_JAR"" ]; then
        CLASSPATH=""${CLASSPATH}:$PIG_JAR""
    else
        echo ""Cannot locate pig.jar. do 'ant jar', and try again""
        exit 1
    fi

    if [ ""$debug"" == ""true"" ]; then
        echo ""Cannot find local hadoop installation, using bundled `java -cp $PIG_JAR org.apache.hadoop.util.VersionInfo | head -1`""
    fi
{code}

Please note I have placed the debug statement below the code that looks for pig jar. It makes sense that the debug statements execute only after pig jar is found. Do you agree? I will upload the patch shortly.","16/Mar/13 01:04;daijy;Sure, can't agree more.","21/Mar/13 00:28;daijy;+1, patch committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation error,PIG-3243,12636158,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,sarutak,tolgakonik,tolgakonik,09/Mar/13 01:35,14/Oct/13 16:45,14/Mar/19 03:07,05/Aug/13 17:50,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"Error in documentation on web related to python udf usage:
The document mentions JYTHON_PATH but it should be JYTHONPATH. Seasoned jython users will easily figure this out but for cpython users who are new to jython, 
this error can easily be a show stopper.

I observed this in 11.0 but it may be occuring in earlier versions.


REFERENCE:

http://pig.apache.org/docs/r0.11.0/udf.html#python-advanced

Advanced Topics
Importing Modules
You can import Python modules in your Python script. Pig resolves Python dependencies recursively, which means Pig will automatically ship all dependent Python modules to the backend. Python modules should be found in the jython search path: JYTHON_HOME, JYTHON_PATH, or current directory.",,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,30/Jul/13 14:51;sarutak;PIG-3243.patch;https://issues.apache.org/jira/secure/attachment/12594967/PIG-3243.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-16 01:09:02.719,,,no_permission,,,,,,,,,,,,316650,,,,Mon Aug 05 17:50:05 UTC 2013,,,,,,,0|i1in1b:,316992,,,,,,,,,,16/Mar/13 01:09;daijy;Would you like to create a patch? Change the src/docs/src/documentation/content/xdocs/udf.xml will do it.,"30/Jul/13 14:51;sarutak;Hi, 
I also found ""JYTHON_PATH"" in TestScriptUDF.java.
So, I've modified TestScriptUDF.java and udf.xml.",05/Aug/13 17:12;cheolsoo;+1.,05/Aug/13 17:50;cheolsoo;Committed to trunk. Thank you Kousuke!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException in POPartialAgg,PIG-3241,12635859,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,dvryaboy,lohit,lohit,07/Mar/13 18:35,02/Apr/13 15:54,14/Mar/19 03:07,13/Mar/13 00:53,0.11,,,,,,,0.11.1,0.12.0,,,,,,0,,,,,,,,,,,,,"While running few PIG scripts against Hadoop 2.0, I see consistently see ConcurrentModificationException 

{noformat}
at java.util.HashMap$HashIterator.remove(HashMap.java:811)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPartialAgg.aggregate(POPartialAgg.java:365)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPartialAgg.aggregateSecondLevel(POPartialAgg.java:379)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPartialAgg.getNext(POPartialAgg.java:203)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:308)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNext(POLocalRearrange.java:263)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:283)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:278)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:729)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:158)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1441)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:153)
{noformat}

It looks like there is rawInputMap is being modified while elements are removed from it. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Mar/13 00:03;dvryaboy;PIG-3241.patch;https://issues.apache.org/jira/secure/attachment/12573444/PIG-3241.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-07 20:03:39.503,,,no_permission,,,,,,,,,,,,316351,,,,Wed Mar 13 00:53:04 UTC 2013,,,,,,,0|i1il73:,316694,,,,,,,,,,"07/Mar/13 20:03;dvryaboy;Lohit, can you post the svn revision you compiled from?

[~rohini] did you guys get Pig running on Hadoop 2.0 in production? Seeing anything like this?","07/Mar/13 21:58;lohit;Compiled release 0.11 version against Hadoop 2.0.
I also see ConcurrentModificationExceptions with processedInputMap in same class
{noformat}
at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
	at java.util.HashMap$EntryIterator.next(HashMap.java:834)
	at java.util.HashMap$EntryIterator.next(HashMap.java:832)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPartialAgg.spillResult(POPartialAgg.java:339)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPartialAgg.getNext(POPartialAgg.java:156)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:308)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNext(POLocalRearrange.java:263)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:283)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:278)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:729)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:158)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1441)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:153)
{noformat}
","11/Mar/13 16:40;rohini;[~dvryaboy],
  No. We are currently running on Pig 0.10 only. Pig 0.11 testing is blocked due to lack of QE resources to certify new features. 

{code}
private Map<Object, List<Tuple>> rawInputMap = Maps.newHashMap();
    private Map<Object, List<Tuple>> processedInputMap = Maps.newHashMap();
{code}

Don't think it has to do with Hadoop 2.0. A ConcurrentHashMap should have been used in the code. Exception not visible in unit tests as spill does not happen there.","11/Mar/13 17:15;dvryaboy;Thing is, we are running this in production on 1.0 and don't observe this error.. 
","11/Mar/13 18:30;rohini;I was wrong about iterator.remove() of HashMap throwing ConcurrentModificationException. Happens only while iterating directly over the entrySet()

{code}
for (Entry<String, String> entry : map.entrySet()) {
            map.remove(entry.getKey());
        }
{code}
throws ConcurrentModificationException

{code}
Iterator<Entry<String, String>> spillingIterator = map.entrySet().iterator();
Entry<String, String> entry = spillingIterator.next();
spillingIterator.remove();
{code}
does not throw ConcurrentModificationException

  So theoretically it should be fine if the map was accessed within a single thread. Need to investigate if there is more than one thread accessing POPartialAgg. Possible that it could be a 2.0 issue. Simple fix would be to change that to ConcurrentHashMap, but need to find the underlying cause to assess further impact. I am blocked with few deadlines before going on vacation and don't have time to spare for the investigation. I am fine if anyone else can take a look at this. Cheolsoo?

[~lohit],
   Can you change it to ConcurrentHashMap and compile and see that fixes the problem for you?
 ",12/Mar/13 17:56;lohit;[~rohini] I already tried by changing those maps to ConcurrentHashMap. But I still end up with similar errors. ,"12/Mar/13 21:54;rohini;[~lohit],
    Can you give some script to reproduce this problem?","12/Mar/13 22:09;cheolsoo;I seem to be able to reproduce ConcurrentModificationException in my 3-node cluster (both MR1 and MR2). The only difference is that it happens a lot more frequently in MR2 than in MR1.

Here is the script that I am using with 100M integers:
{code}
a = LOAD '1.txt' AS (i:int);
b = GROUP a ALL;
c = FOREACH b GENERATE TOP(1000, 0, a);
STORE c INTO 'out';
{code}
This doesn't give me exactly the same stack trace, but almost every mapper fails with ConcurrentModificationException constantly in MR2. I am setting mapPartAgg properties as follows:
{code}
pig.exec.mapPartAgg=true
pig.exec.mapPartAgg.minReduction=3
{code}","12/Mar/13 23:18;dvryaboy;I think I have a clean fix, Lohit and I are testing.","13/Mar/13 00:03;dvryaboy;Attaching patch.

Rather than synchronize all memory access, I decided to simply avoid concurrent access all together. spill(), called by Spillable Memory Manager, used to set up the iterator used for spilling - that involved looking at the primary and secondary maps, applying the combiner to them, doing all kinds of things -- all in the SMM thread.

Instead, we now only set the doSpill flag in spill(), and do the work in the main thread, which now is the only thread that can modify iterators and hashmaps.

Most of this patch is just whitespace changes :).","13/Mar/13 00:38;cheolsoo;+1.

The fix looks good. I also verified that my test script runs fine with the patch.","13/Mar/13 00:53;dvryaboy;committed to 0.11 branch and trunk.

thanks all.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to return multiple values from a macro using SPLIT,PIG-3239,12635564,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,dreambird,luibelgo,luibelgo,06/Mar/13 13:06,14/Oct/13 16:45,14/Mar/19 03:07,15/Mar/13 16:58,0.10.0,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"Hi, I'm unable to return multiple values from a macro when values come from a SPLIT. Here is an small example:

{code}
DEFINE my_macro(seq) RETURNS valid, invalid {
    added = FOREACH $seq GENERATE $0 * 2, $1;
    SPLIT added INTO $valid IF $1 == true, $invalid OTHERWISE;
}

data = LOAD 'case.csv' USING PigStorage(',') AS (value: int, valid: boolean);
P, Q = my_macro(data);
DUMP P;
DUMP Q;
{code}

Pig is unable to recognize the {{OTHERWISE}} side. Error is: {{ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1200: <at case.pig, line 3> Invalid macro definition: . Reason: Macro 'my_macro' missing return alias: invalid}}

Simple workaround is to force {{$invalid}} to be returned as {{FOREACH}} result:

{code}
SPLIT added INTO $valid IF $1 == true, tmp_invalid OTHERWISE;
$invalid = FOREACH tmp_invalid GENERATE *;
{code}

Samples and logs attached to the issue.

","Apache Pig version 0.10.0-cdh4.2.0 (rexported) 
compiled Feb 15 2013, 12:19:17

Linux 3.2.0-38-generic #61-Ubuntu SMP Tue Feb 19 12:18:21 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Mar/13 23:03;dreambird;PIG-3239.patch.txt;https://issues.apache.org/jira/secure/attachment/12573790/PIG-3239.patch.txt,13/Mar/13 00:31;dreambird;PIG-3239.patch.txt;https://issues.apache.org/jira/secure/attachment/12573448/PIG-3239.patch.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-03-12 01:15:54.468,,,no_permission,,,,,,,,,,,,316057,,,,Fri Mar 15 16:58:38 UTC 2013,,,,,,,0|i1ijdr:,316400,,,,,,,,,,06/Mar/13 13:08;luibelgo;Files: https://gist.github.com/luisbelloch/5099190,"12/Mar/13 01:15;dreambird;[~luibelgo], please correct me if I am wrong, I think 'SPLIT' doesn't work with 'OTHERWISE'. 'SPLIT' only work with 'IF' http://pig.apache.org/docs/r0.7.0/piglatin_ref2.html#SPLIT

actually below script works with me very well on your example
{noforamt}
DEFINE my_macro(seq) RETURNS valid, invalid {                 
    added = FOREACH $seq GENERATE $0 * 2, $1;                 
    SPLIT added INTO $valid IF $1 == true, $invalid IF $1 ==false;
}
data = LOAD 'case.csv' USING PigStorage(',') AS (value: int, valid: boolean);
P, Q = my_macro(data);
DUMP P;
DUMP Q;
{noformat}","12/Mar/13 01:16;dreambird;sorry for the typo, I mean
{noformat}
DEFINE my_macro(seq) RETURNS valid, invalid
{ added = FOREACH $seq GENERATE $0 * 2, $1; SPLIT added INTO $valid IF $1 == true, $invalid IF $1 ==false; } data = LOAD 'case.csv' USING PigStorage(',') AS (value: int, valid: boolean); P, Q = my_macro(data); DUMP P; DUMP Q; 
{noformat}","12/Mar/13 08:25;luibelgo;Hi Johnny Zhang, if you have a look at version 0.10, OTHERWISE keyword exists.
http://pig.apache.org/docs/r0.10.0/basic.html#SPLIT","12/Mar/13 18:14;dreambird;[~luibelgo], you are right, sorry I was looking at the old release doc.",13/Mar/13 00:31;dreambird;this fix works for me. I will run all unit tests see if any regression it brings.,"13/Mar/13 08:39;luibelgo;Thanks! We'll test it internally.

","13/Mar/13 21:08;cheolsoo;[~dreambird], thank you for the fix. I think your fix is correct. Can you please add a unit test case for this?

TestMacroExpansion.java has splitTest, but that doesn't cover OTHERWISE. You might want to expand that test case, or add a new test case.

Thanks!","14/Mar/13 18:24;cheolsoo;Chatted with Johnny, and it looks like TestMacroExpansion isn't a good place to put a new test case for this. It might be better to add one to TestSplit.","14/Mar/13 23:04;dreambird;[~cheolsoo], thanks a lot for review my patch. I have add one test case into TestSplit.",15/Mar/13 05:37;cheolsoo;+1. Let me run unit tests.,15/Mar/13 16:58;cheolsoo;Committed to trunk. Thanks Johnny!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SearchEngineExtractor does not work for bing,PIG-3227,12634949,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,dannyant,dannyant,dannyant,02/Mar/13 01:44,08/Jun/16 20:48,14/Mar/19 03:07,31/May/16 06:36,0.11,,,,,,,0.16.0,0.17.0,,,piggybank,,,0,,,,,,,,,,,,,"org.apache.pig.piggybank.evaluation.util.apachelogparser.SearchEngineExtractor
Extracts a search engine from a URL, but it does not work for Bing",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Mar/13 01:46;dannyant;SearchEngineExtractor_Bing.patch;https://issues.apache.org/jira/secure/attachment/12571705/SearchEngineExtractor_Bing.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-05-31 06:36:07.141,,,no_permission,,,,,,,,,,,,315442,Reviewed,,,Tue May 31 06:36:07 UTC 2016,,,Patch Available,,,,0|i1iflj:,315786,,,,,,,,,,02/Mar/13 01:46;dannyant;Patch for supporting bing in the SearchEngineExtractor UDF,31/May/16 06:36;daijy;Patch committed to 0.16 branch and trunk. Thanks Danny!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroStorage does not handle comma separated input paths,PIG-3223,12634533,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dreambird,mkramer,mkramer,28/Feb/13 00:55,14/Oct/13 16:46,14/Mar/19 03:07,06/May/13 02:56,0.10.0,0.11,,,,,,0.11.2,0.12.0,,,piggybank,,,0,,,,,,,,,,,,,"In pig 0.11, a patch was issued to AvroStorage to support globs and comma separated input paths (PIG-2492).  While this function works fine for glob-formatted input paths, it fails when issued a standard comma separated list of paths.  fs.globStatus does not seem to be able to parse out such a list, and a java.net.URISyntaxException is thrown when toURI is called on the path.  

I have a working fix for this, but it's extremely ugly (basically checking if the string of input paths is globbed, otherwise splitting on "","").  I'm sure there's a more elegant solution.  I'd be happy to post the relevant methods and ""fixes"" if necessary.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Mar/13 21:21;mkramer;AvroStorage.patch;https://issues.apache.org/jira/secure/attachment/12571647/AvroStorage.patch,06/Mar/13 22:02;mkramer;AvroStorage.patch-2;https://issues.apache.org/jira/secure/attachment/12572406/AvroStorage.patch-2,01/Mar/13 21:21;mkramer;AvroStorageUtils.patch;https://issues.apache.org/jira/secure/attachment/12571648/AvroStorageUtils.patch,06/Mar/13 22:02;mkramer;AvroStorageUtils.patch-2;https://issues.apache.org/jira/secure/attachment/12572407/AvroStorageUtils.patch-2,03/May/13 22:40;dreambird;PIG-3223.branch-0.11.patch.txt;https://issues.apache.org/jira/secure/attachment/12581758/PIG-3223.branch-0.11.patch.txt,03/May/13 19:31;dreambird;PIG-3223.patch.txt;https://issues.apache.org/jira/secure/attachment/12581735/PIG-3223.patch.txt,03/May/13 00:32;dreambird;PIG-3223.patch.txt;https://issues.apache.org/jira/secure/attachment/12581645/PIG-3223.patch.txt,03/May/13 00:25;dreambird;PIG-3223.patch.txt;https://issues.apache.org/jira/secure/attachment/12581644/PIG-3223.patch.txt,22/Mar/13 18:14;dreambird;PIG-3223.patch.txt;https://issues.apache.org/jira/secure/attachment/12575060/PIG-3223.patch.txt,11/Mar/13 23:43;dreambird;PIG-3223.patch.txt;https://issues.apache.org/jira/secure/attachment/12573212/PIG-3223.patch.txt,,10.0,,,,,,,,,,,,,,,,,,,2013-03-01 01:20:46.917,,,no_permission,,,,,,,,,,,,315026,,,,Mon May 06 02:56:37 UTC 2013,,,,,,,0|i1id13:,315370,,,,,,,,,,"01/Mar/13 01:20;dreambird;[~mkramer], I am willing to continue working on this item. Seems very interesting. Do you mind post your current patch?",01/Mar/13 21:21;mkramer;[~dreambird]That's great Johnny.  I'm attaching the patch now.  I've modified the getSubDirs method call in AvroStorageUtils and added a rather ugly isGlob helper function.  I imagine there's a better filesystem api call that already takes care of these issues. ,"06/Mar/13 22:00;mkramer;It appears by returning the boolean status of the recursed ""getSubDirs"" method always results in ""false"".  I'm attaching a patch that reverts back to the original logic of discarding the recursive boolean statuses and just returning true if the first entrance into the method succeeds.","11/Mar/13 23:42;dreambird;[~mkramer], my apologize for late reply. Actually I found you properly can load comma separated avro file when using globe wrap it. like

in = LOAD 'test_dir1/{test_glob1.avro,test_glob2.avro,test_glob3.avro}' USING org.apache.pig.piggybank.storage.avro.AvroStorage ();

Does this satisfy your requirement? If this is the case, there is no code change required. 
There is no test cover it right now, I just added one more test case to TestAvroStorage to make sure it won't break in the future.
 ","13/Mar/13 00:53;cheolsoo;Johnny is correct. Comma-separated lists must be surrounded by ""{ }"".

[~dreambird], in fact, this is covered by the following:
{code}
test_dir1/test_glob{1,2,3}.avro
test_dir1/test_glob{3,2,1}.avro
{test_dir1,test_dir2}/test_glob*.avro
{test_dir2,test_dir1}/test_glob*.avro
test_dir{1,2}/file_that_does_not_exist*.avro
{code}
These test cases go through the same code path as your test case does (test_dir1/\{test_glob1.avro,test_glob2.avro,test_glob3.avro\}).","13/Mar/13 01:17;dreambird;thanks for the comments, [~cheolsoo], I think if this is the case, we can close the jira. [~mkramer], please let us know if you have any other concern.","15/Mar/13 00:15;mkramer;The problem is when we're dealing with comma separated paths that aren't enclosed by {}.  Globs put restrictions on fully qualified hdfs paths. If we're dealing with non-globbed input paths, which PigStorage does handle, this function breaks, i.e if I passed testdir1/,testdir2/ as input, it would fail. It also fails with something like {hdfs://namenode:8020/testdir1/,hdfs://namenode:8020/testdir2}/* ","15/Mar/13 00:17;mkramer;If the goal here is to mirror the functionality of PigStorage, then I think we'd want to support the same variations in input path formatting.","15/Mar/13 00:34;mkramer;Furthermore, the driving force for us comes from how Oozie constructs input paths.  If comma separated paths aren't supported in this way, AvroStorage as is can't be used with Oozie. ","15/Mar/13 01:36;cheolsoo;[~mkramer], thank you for the clarification.

You're right:
# PigStorage supports commma-separated input paths.
# The fully qualified paths such as \{hdfs://namenode:8020/testdir1/,hdfs://namenode:8020/testdir2\} don't work in AvroStorage.

I am not against adding the support for comma-separated list like you suggest.

That said, I don't understand your use case:
{quote}
The driving force for us comes from how Oozie constructs input paths. If comma separated paths aren't supported in this way, AvroStorage as is can't be used with Oozie.
{quote}
Can you please provide your Oozie workflow and Pig script? Why do input paths need to be fully qualified paths?","15/Mar/13 18:31;mkramer;[~cheolsoo], thanks for getting back to me so quickly!  

We're using variable substitution and input path generation via Oozie Coordinator.  We include the hdfs://namenode:8020 at the beginning of our path templates, which I think is pretty standard (e.g. something like <uri-template>$\{nameNode\}/data/</uri-template> )  When Oozie constructs input paths to be passed to the pig script or map reduce job, it enumerates the paths via a comma separated list, something like  hdfs://namenode:8020/data/1,hdfs://namenode:8020/data/2.  This is how we figured out AvroStorage was breaking in the first place.  

A good coordinator/workflow example that is indicative of the types of workflows we're running can be found in the Oozie source examples: https://github.com/apache/oozie/blob/trunk/examples/src/main/apps/aggregator/coordinator.xml","19/Mar/13 18:46;cheolsoo;[~mkramer], thanks for the explanation.

I actually asked an Oozie developer about this, and you're right that Oozie recommends to include the ""hdfs://namenode:8020"" in the path since the same Oozie server might be used for multiple Hadoop clusters. I agree that comma-separated input paths should be supported by AvroStorage.
","20/Mar/13 19:06;dreambird;Thanks for clarification, I will post patch soon.","22/Mar/13 18:20;dreambird;the latest patch makes AvroStorage working for comma separated input. The patch also adds two test cases for below inputs
{code}
final private String testCommaSeparated1 = getInputFile(""test_dir1/test_glob1.avro,test_dir1/test_glob2.avro,test_dir1/test_glob3.avro"");
final private String testCommaSeparated2 = getInputFile(""test_dir1/*, test_dir2/test_glob4.avro, test_dir2/test_glob5.avro"");
{code}","23/Mar/13 01:13;prkommireddi;Thanks [~dreambird]. 

I have a question regarding the current approach - why isn't the globbing implemented in PigAvroInputFormat? Overriding listStatus(JobContext job) should be cleaner, unless I am missing something very specific to Avro?","28/Mar/13 05:27;dreambird;[~prkommireddi], sorry I don't know why globbing is implemented in AvroStorage this way. Maybe a good idea to follow PIG-3015 to polish AvroStorage implementation. ;)",08/Apr/13 22:05;dreambird;this is the review board link,"02/May/13 18:23;viraj;Hi Johnny are you planning to work on this patch. Meanwhile I will post an updated patch, which I got working on Pig 0.11 and trunk.
Viraj","02/May/13 18:29;dreambird;[~viraj], thanks for your comments & patch, my previous patch got some comments from Rohini on the review board, I am going to update a revised on later today. I will review your patch as well. Thanks!","02/May/13 19:32;rohini;I think we should get this patch into 0.11 also to benefit current users as trunk has a rewrite of AvroStorage. Without comma separated paths, it is not possible to use it with oozie if one is consuming data from more than one directory. ","02/May/13 21:50;dreambird;@Rohini and [~viraj], thanks for your comments. I appreciate. Is that OK to still keep this jira in my queue (I notice the sssignee changed) so that I can deliver my contribution completely? I am actually working on revising the patch at this moment :) Thanks. ","02/May/13 21:54;viraj;Sorry please assign this Jira to yourself and continue. Since I already had a patch and I thought of uploading it.. Somehow could not get an option to attach the patch hence I changed the owner.
Viraj",02/May/13 22:48;viraj;Updated patch from Viraj,"03/May/13 00:25;dreambird;post the revised patch based on Rohini's comments, I also run TestAvroStorage and it pass for me. Thanks.","03/May/13 01:00;dreambird;[~rohini], could you please review the latest patch https://issues.apache.org/jira/secure/attachment/12581645/PIG-3223.patch.txt ?
new added test cases in TestAvroStorage is also clean. Please let me know any concern regarding to the implementation, I will revised it as soon as possible!
Let me know if you want me post another patch for 0.11 branch too!

I also tried Viraj's patch 'PIG-3223.viraj.txt', it not clean on trunk
{noformat}
patching file contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/avro/AvroStorage.java
patching file contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/avro/TestAvroStorage.java
Hunk #2 succeeded at 45 (offset 1 line).
Hunk #3 succeeded at 72 (offset 1 line).
Hunk #4 succeeded at 91 with fuzz 1 (offset 1 line).
Hunk #5 succeeded at 1005 (offset 19 lines).
{noformat}

also the TestAvroStorage test failed
{noformat}
    <error message=""Error during parsing. java.net.URISyntaxException: Illegal character in scheme name at index 4: test_glob1.avro,file:"" type=""org.apache.pig.impl.logicalLayer.FrontendException"">org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1000: Error during parsing. java.net.URISyntaxException: Illegal character in scheme name at index 4: test_glob1.avro,file:
        at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1670)
        at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1608)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:565)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:578)
        at org.apache.pig.piggybank.test.storage.avro.TestAvroStorage.testAvroStorage(TestAvroStorage.java:1058)
        at org.apache.pig.piggybank.test.storage.avro.TestAvroStorage.testAvroStorage(TestAvroStorage.java:1051)
        at org.apache.pig.piggybank.test.storage.avro.TestAvroStorage.testComma1(TestAvroStorage.java:1020)
Caused by: Failed to parse: java.net.URISyntaxException: Illegal character in scheme name at index 4: test_glob1.avro,file:
        at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:191)
        at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1661)
Caused by: java.lang.IllegalArgumentException: java.net.URISyntaxException: Illegal character in scheme name at index 4: test_glob1.avro,file:
        at org.apache.hadoop.fs.Path.initialize(Path.java:148)
        at org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:126)
        at org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:50)
        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:1084)
        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:1087)
        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:1087)
        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:1087)
        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:1087)
        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:1087)
        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:1087)
        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:1087)
        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:1087)
        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:1087)
        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:1087)
        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:1087)
        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:1087)
        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:1087)
        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:1087)
        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:1087)
        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:1087)
        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:1087)
        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:1087)
        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:1087)
        at org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:1023)
        at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:987)
        at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.getAllSubDirs(AvroStorageUtils.java:120)
        at org.apache.pig.piggybank.storage.avro.AvroStorage.getSchema(AvroStorage.java:387)
        at org.apache.pig.newplan.logical.relational.LOLoad.getSchemaFromMetaData(LOLoad.java:174)
        at org.apache.pig.newplan.logical.relational.LOLoad.&lt;init&gt;(LOLoad.java:88)
        at org.apache.pig.parser.LogicalPlanBuilder.buildLoadOp(LogicalPlanBuilder.java:856)
        at org.apache.pig.parser.LogicalPlanGenerator.load_clause(LogicalPlanGenerator.java:3256)
        at org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1335)
        at org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:819)
        at org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:537)
        at org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:412)
        at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:181)
Caused by: java.net.URISyntaxException: Illegal character in scheme name at index 4: test_glob1.avro,file:
        at java.net.URI$Parser.fail(URI.java:2809)
        at java.net.URI$Parser.checkChars(URI.java:2982)
        at java.net.URI$Parser.parse(URI.java:3009)
        at java.net.URI.&lt;init&gt;(URI.java:736)
        at org.apache.hadoop.fs.Path.initialize(Path.java:145)
{noformat}","03/May/13 01:16;viraj;Johnny, Thanks let me check as to why it is failing in trunk. I never tested on trunk, as I thought this was only going to be committed in Pig 0.11. ",03/May/13 19:31;dreambird;latest patch addressed Rohini's comments in RB,"03/May/13 22:40;dreambird;Rohini, thanks for your +1 on RB. Here is the patch for branch 0.11. Could please help commit the patch (to trunk and 0.11)? Appreciate.",06/May/13 02:56;rohini;+1. Committed to 0.11.2 and trunk. Thanks Johnny,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race Conditions in POSort and (Internal)SortedBag during Proactive Spill.,PIG-3212,12633618,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,kadeng,kadeng,kadeng,22/Feb/13 13:54,18/Sep/13 14:45,14/Mar/19 03:07,28/Feb/13 19:22,0.11,,,,,,,0.11.1,0.12.0,,,,,,0,,,,,,,,,,,,,"The following bug exists in the latest release of Pig 0.11.0

While running some large jobs involving groups and sorts like these:

{code}

events_by_user = GROUP events BY user_id;

sorted_events_by_user = FOREACH events_by_user {
	A = ORDER events BY ts, split_idx, line_num;
	GENERATE group, A;
}

{code}

I got a pretty strange behaviour: While this worked on small datasets, if I ran it on large datasets, the results were sometimes not sorted perfectly. 

So after a long debugging session, I tracked it down to at least one race condition:

The following partial stack trace shows how a proactive spill gets triggered on an InternalSortedBag. A spill in turn triggers a sort of that InternalSortedBag.

{code}
	at org.apache.pig.data.SortedSpillBag.proactive_spill(SortedSpillBag.java:83)
	at org.apache.pig.data.InternalSortedBag.spill(InternalSortedBag.java:455)
	at org.apache.pig.impl.util.SpillableMemoryManager.handleNotification(SpillableMemoryManager.java:243)
	at sun.management.NotificationEmitterSupport.sendNotification(NotificationEmitterSupport.java:138)
	at sun.management.MemoryImpl.createNotification(MemoryImpl.java:171)
	at sun.management.MemoryPoolImpl$PoolSensor.triggerAction(MemoryPoolImpl.java:272)
	at sun.management.Sensor.trigger(Sensor.java:120)
{code}

At the same time, the same InternalSortedBag might be sorted or accessed within a POSort Operation. For example using the following Code path (line numbers might be off, I had to add debug statements to diagnose this)

{code}
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSort.getNext(POSort.java:346)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:492)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.processInputBag(POProject.java:582)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PORelationToExprProject.getNext(PORelationToExprProject.java:107)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:394)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:372)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:297)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:368)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.getNext(POSplit.java:214)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:465)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:433)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:413)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:257)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:566)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)
	at org.apache.hadoop.mapred.Child.main(Child.java:170)
{code}

The key here is: Both operations try to compare and modify elements of the SortedBag simultaneously. This leads to all kinds of problems, most notably incorrectly sorted data.

POSort.SortComparator that's passed as a Comparison function to (Internal)SortedBag is not thread safe, since it works by attaching single input tuples to PhysicalOperator's - these Operators in turn are part of the POSort.sortPlans and are re-used among each thread accessing the (Internal)SortedBag.

 




",,,,,,,,,,,,,,,,,,,PIG-3466,,,,,,,,,,,,,26/Feb/13 01:54;kadeng;PIG-3212-p1.patch;https://issues.apache.org/jira/secure/attachment/12570907/PIG-3212-p1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-22 14:19:08.238,,,no_permission,,,,,,,,,,,,314113,,,,Thu Feb 28 19:22:57 UTC 2013,,,Patch Available,,,,0|i1i7ef:,314458,,,,,,,,,,22/Feb/13 14:19;jcoveney;Good job tracking down these threading issues. They definitely need to be addressed. Keep it up!,"26/Feb/13 01:54;kadeng;I have attached a patch which should fix this problem. 

To clarify a bit more, what caused the problems. 

The root cause of all problems here is that POSort.SortComparator is not thread safe - which is very hard to fix in a performant way. So I didn't do that in this fix.

Another problem is that the SpillableMemoryManager may call the spill method in it's own thread at any time, and that InternalSortedBag itself is not thread safe enough itself.

The approach I took in this patch is simple: First - synchronize modifications to InternalSortedBag correctly. Second - prevent all modifications (including spills), once a read iterator has been started.

The last point is extremely important.  POSort may create multiple InternalSortedBags after each other, and each of these is registered with the SpillableMemoryManager. Since the WeakReferences held by the SpillableMemoryManager are not cleared immediately if all other references to the old InternalSortedBag have been removed, it's possible that the ""spill"" method gets called on an otherwise dead instance of an InternalSortedBag. That way, it would be possible that two different InternalSortedBag instances (one dead, one alive) are trying to use the same POSort.SortComparator instance at the same time.
",26/Feb/13 01:58;kadeng;I would really recommend to review the code for other threading issues arising out of the SpillableMemoryManager. I would look for similar race conditions in all Spillable implementations.,"26/Feb/13 02:02;kadeng;I could reliably reproduce this bug on a large job. It is neccessary to have enough data and to create some memory pressure to trigger this bug (i.e. the SpillableMemoryManager has to kick in a lot). Using that same job, I could verify that the patch really fixed the problem.","26/Feb/13 02:52;dvryaboy;Wow. Thanks for finding this. That would be my fault :(.
Will review as soon as I get home.","26/Feb/13 18:31;dvryaboy;Ok so I think the fix works, but it seems like this bug has been there for a long time -- the synchronization issue with SMM would've been there before 0.11, as well. Is it just more visible now because SMM is faster (fewer bags to go through)? 

It seems unlikely that we ever get to spill an internal sorted bag, given this patch.. seems like it almost always has an iterator open.  If the concern is using the same comparator -- could we not solve this by initializing a new comparator for every bag?","27/Feb/13 10:10;kadeng;A new Comparator for every bag would mean you need a new deep clone of the sort plan of POSort. This means a lot of small object fodder for the garbage collector and probably increase the running time.

When it comes to spilling an internal sorted bag: Do you really want such an operation to happen *while you are iterating* over the bag contents ? If you allow that, there are not just synchronization issues. The state of the iterator (pointer to the current element) needs to adapt accordingly. 

And what would you gain ? The memory required for the bag won't increase while the iterator is running (since it's closed for modifications), so it can't cause out of memory exceptions itself. After the iterator is finished, the bag in it's entirety is usually fodder for the garbage collector (at least in the context of POSort, this is always the case). If you allow the SMM to call spill on the bag while the iterator is running, all you'll gain is the possibility that some memory (that's actually actively being used) gets freed a little bit earlier than it would usually be. 

It's better to let the SMM do it's work only while the InternalBag is being filled. That's exactly the point when it is likely that the SMM kicks in, since the memory usage is increasing. The fix allows for that.",27/Feb/13 19:28;dvryaboy;Ok I'm convinced. Will run tests and commit if all is good there.,"28/Feb/13 19:22;dvryaboy;Committed to 0.11.1 branch and trunk.
Thanks a ton Kai!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig fails to start when it cannot write log to log files,PIG-3210,12633537,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,mengsungwu,mengsungwu,mengsungwu,22/Feb/13 02:31,14/Oct/13 16:45,14/Mar/19 03:07,26/Jul/13 09:41,,,,,,,,0.12.0,,,,impl,,,0,,,,,,,,,,,,,"Pig will check whether logFileName is null or not before setting to pig.logfile property in some places. But forget to check in other places.


{code:xml} 
381         pigContext.getProperties().setProperty(""pig.logfile"", (logFileName == null? """": logFileName));
...
451         pigContext.getProperties().setProperty(""pig.logfile"", logFileName);
{code} 


{code:xml} 
12/12/25 16:38:00 WARN pig.Main: Need write permission in the directory: /opt/trend/hadooppet/sanity-tm-6/result to create log file.
 14 2012-12-25 16:38:00,372 [main] INFO org.apache.pig.Main - Apache Pig version 0.10.1.tm6 (rexported) compiled Oct 22 2012, 11:11:02
 15 2012-12-25 16:38:01,712 [main] WARN org.apache.pig.Main - Cannot write to log file: /opt/trend/hadooppet/sanity-tm-6/result//akamai.pig1356453481712.log
 16 2012-12-25 16:38:01,727 [main] ERROR org.apache.pig.Main - ERROR 2999: Unexpected internal error. null
 17 2012-12-25 16:38:01,727 [main] WARN org.apache.pig.Main - There is no log file to write to.
 18 2012-12-25 16:38:01,727 [main] ERROR org.apache.pig.Main - java.lang.NullPointerException
 19 at java.util.Hashtable.put(Hashtable.java:394)
 20 at java.util.Properties.setProperty(Properties.java:143)
 21 at org.apache.pig.Main.run(Main.java:542)
 22 at org.apache.pig.Main.main(Main.java:115)
 23 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 24 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
 25 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
 26 at java.lang.reflect.Method.invoke(Method.java:597)
 27 at org.apache.hadoop.util.RunJar.main(RunJar.java:208)
{code} ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Feb/13 02:34;mengsungwu;PIG-3210.patch;https://issues.apache.org/jira/secure/attachment/12570407/PIG-3210.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-25 18:40:26.822,,,no_permission,,,,,,,,,,,,314032,,,,Fri Jul 26 09:41:36 UTC 2013,,,Patch Available,,,,0|i1i6wf:,314377,,,,,,,,,,25/Feb/13 18:40;dvryaboy;what happens down the line if pig.logfile is empty?,26/Feb/13 01:54;mengsungwu;NullPointerException will occur since it will put a null value into Hashtable.,"26/Feb/13 02:49;dvryaboy;I should've phrased my question better -- what happens if the value of pig.logfile is an empty string? Do we know nothing bad will happen in that case (like trying to open """" for writing)?","26/Jul/13 09:36;cheolsoo;+1.

To answer Dmitriy's question, the value of pig.logfile is an empty string, nothing is logged. Here is the relevant code in LogUtils.java:
{code:title=LogUtils.java}
if(logFileName == null || logFileName.equals("""")) {
    //if exec is invoked programmatically then logFileName will be null
    log.warn(""There is no log file to write to."");
    log.error(bs.toString());
    return;
}
{code}",26/Jul/13 09:41;cheolsoo;Committed to trunk. Thank you Meng-Sung!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[zebra] TFile should not set io.compression.codec.lzo.buffersize,PIG-3208,12633535,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ekoontz,ekoontz,ekoontz,22/Feb/13 02:30,14/Oct/13 16:46,14/Mar/19 03:07,19/Mar/13 23:34,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"In contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/Compression.java, the following occurs:

{code}
conf.setInt(""io.compression.codec.lzo.buffersize"", 64 * 1024);
{code}

This can cause the LZO decompressor, if called within the context of reading TFiles, to return with an error code when trying to uncompress LZO-compressed data, if the data's compressed size is too large to fit in 64 * 1024 bytes.

For example, the Hadoop-LZO code uses a different default value (256 * 1024):

https://github.com/twitter/hadoop-lzo/blob/master/src/java/com/hadoop/compression/lzo/LzoCodec.java#L185

This can lead to a case where, if data is compressed with a cluster where the default {{io.compression.codec.lzo.buffersize}} = 256*1024 is used, then code that tries to read this data by using Pig's zebra, the Mapper will exit with code 134 because the LZO compressor returns a -4 (which encodes the LZO C library error LZO_E_INPUT_OVERRUN) when trying to uncompress the data. The stack trace of such a case is shown below:

{code}
2013-02-17 14:47:50,709 INFO com.hadoop.compression.lzo.LzoCodec: Creating stream for compressor: com.hadoop.compression.lzo.LzoCompressor@6818c458 with bufferSize: 262144
2013-02-17 14:47:50,849 INFO org.apache.hadoop.io.compress.CodecPool: Paying back codec: com.hadoop.compression.lzo.LzoCompressor@6818c458
2013-02-17 14:47:50,849 INFO org.apache.hadoop.mapred.MapTask: Finished spill 3
2013-02-17 14:47:50,857 INFO org.apache.hadoop.io.compress.CodecPool: Borrowing codec: com.hadoop.compression.lzo.LzoCompressor@6818c458
2013-02-17 14:47:50,866 INFO com.hadoop.compression.lzo.LzoCodec: Creating stream for compressor: com.hadoop.compression.lzo.LzoCompressor@6818c458 with bufferSize: 262144
2013-02-17 14:47:50,879 INFO org.apache.hadoop.io.compress.CodecPool: Paying back codec: com.hadoop.compression.lzo.LzoCompressor@6818c458
2013-02-17 14:47:50,879 INFO org.apache.hadoop.mapred.MapTask: Finished spill 4
2013-02-17 14:47:50,887 INFO org.apache.hadoop.mapred.Merger: Merging 5 sorted segments
2013-02-17 14:47:50,890 INFO org.apache.hadoop.io.compress.CodecPool: Borrowing codec: com.hadoop.compression.lzo.LzoDecompressor@66a23610
2013-02-17 14:47:50,891 INFO com.hadoop.compression.lzo.LzoDecompressor: calling decompressBytesDirect with buffer with: position: 0 and limit: 262144
2013-02-17 14:47:50,891 INFO com.hadoop.compression.lzo.LzoDecompressor: read: 245688 bytes from decompressor.
2013-02-17 14:47:50,891 INFO org.apache.hadoop.io.compress.CodecPool: Borrowing codec: com.hadoop.compression.lzo.LzoDecompressor@43684706
2013-02-17 14:47:50,892 INFO com.hadoop.compression.lzo.LzoDecompressor: calling decompressBytesDirect with buffer with: position: 0 and limit: 65536
2013-02-17 14:47:50,895 INFO org.apache.hadoop.mapred.TaskLogsTruncater: Initializing logs' truncater with mapRetainSize=-1 and reduceRetainSize=-1
2013-02-17 14:47:50,897 FATAL org.apache.hadoop.mapred.Child: Error running child : java.lang.InternalError: lzo1x_decompress returned: -4
        at com.hadoop.compression.lzo.LzoDecompressor.decompressBytesDirect(Native Method)
        at com.hadoop.compression.lzo.LzoDecompressor.decompress(LzoDecompressor.java:307)
        at org.apache.hadoop.io.compress.BlockDecompressorStream.decompress(BlockDecompressorStream.java:82)
        at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:75)
        at org.apache.hadoop.mapred.IFile$Reader.readData(IFile.java:341)
        at org.apache.hadoop.mapred.IFile$Reader.rejigData(IFile.java:371)
        at org.apache.hadoop.mapred.IFile$Reader.readNextBlock(IFile.java:355)
        at org.apache.hadoop.mapred.IFile$Reader.next(IFile.java:387)
        at org.apache.hadoop.mapred.Merger$Segment.next(Merger.java:220)
        at org.apache.hadoop.mapred.Merger$MergeQueue.merge(Merger.java:420)
        at org.apache.hadoop.mapred.Merger$MergeQueue.merge(Merger.java:381)
        at org.apache.hadoop.mapred.Merger.merge(Merger.java:77)
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.mergeParts(MapTask.java:1548)
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1180)
        at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:582)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:649)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:323)
        at org.apache.hadoop.mapred.Child$4.run(Child.java:270)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1213)
        at org.apache.hadoop.mapred.Child.main(Child.java:264)

{code}

(Some additional LOG.info statements were added to produce the above output which are not in this patch).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Feb/13 02:31;ekoontz;PIG-3208.patch;https://issues.apache.org/jira/secure/attachment/12570406/PIG-3208.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-18 22:34:21.296,,,no_permission,,,,,,,,,,,,314030,Reviewed,,,Tue Mar 19 23:34:44 UTC 2013,,,Patch Available,,,,0|i1i6vz:,314375,,,,,,,,,,22/Feb/13 02:31;ekoontz;Removes the two cases where {{io.compression.codec.lzo.buffersize}} is set to 64*1024.,"18/Mar/13 22:34;daijy;Hi, Eugene, Zebra is no longer supported except for compilation errors. You can continue to use it but we will not commit patches/fix bugs. ",18/Mar/13 22:46;dvryaboy;[~daijy] why wouldn't we commit fixes provided by community? ,"19/Mar/13 00:05;daijy;Ok, I am not objecting to that. My concern is if a fix introduces other issues unintentionally, nobody would look at it. But I am fine if that happens, we simply revert the patch.","19/Mar/13 03:46;xuefuz;I guess Daniel meant to say that Zebra isn't in active development, which is true. I think if people uses it problem will be discovered. As long as any fix contributed meets the quality requirement, the whole community benefits from it. ","19/Mar/13 03:55;xuefuz;Eugene,

If we remove the two lines, will Zebra take whatever the default is from the cluster settings? What happens if it's not set in the cluster?

Also, for your changes, we'll need to run the tests to watch for regressions.
",19/Mar/13 04:05;daijy;Thanks Xuefu chime in. I am happy to check in any Zebra patches you are Ok with.,"19/Mar/13 20:35;ekoontz;Hi Xuefu, Daniel and Dmitriy,

Thanks for considering this patch!

Xuefu, yes, Zebra will take its default from the cluster. If you don't set it, then it will be set by the version of hadoop-lzo that you use. For example, if you use trunk, it will be 256*1024; please see:

https://github.com/twitter/hadoop-lzo/blob/master/src/java/com/hadoop/compression/lzo/LzoCodec.java#L51

See also discussion between @kevinweil and @tlipcon here: 

https://github.com/twitter/hadoop-lzo/commit/3afba3037be4152fcc088ddb13f2ec12e659ed9c

-Eugene","19/Mar/13 20:45;ekoontz;Correction, it will take its *setting* from the cluster - the *default* is what is supplied by the LZO code in the mentioned link: https://github.com/twitter/hadoop-lzo/blob/master/src/java/com/hadoop/compression/lzo/LzoCodec.java#L51.","19/Mar/13 20:50;xuefuz;Got it. 

+1 to the patch. 

However, I forgot the regression testing process. Otherwise, the patch seems fine.","19/Mar/13 23:33;daijy;The zebra unit tests on trunk is already broken:
    [junit] Running org.apache.hadoop.zebra.io.TestCheckin
    [junit] Tests run: 107, Failures: 0, Errors: 8, Time elapsed: 15.197 sec
    [junit] Running org.apache.hadoop.zebra.types.TestCheckin
    [junit] Tests run: 62, Failures: 0, Errors: 32, Time elapsed: 0.222 sec
This patch does not make things worse. I am going to checkin.","19/Mar/13 23:34;daijy;Patch committed to trunk. Thanks Eugene, Xuefu!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBaseStorage does not work with Oozie pig action and secure HBase,PIG-3206,12633508,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,22/Feb/13 00:18,11/Aug/14 11:21,14/Mar/19 03:07,26/Feb/13 14:32,0.10.1,,,,,,,0.11.1,0.12.0,,,,,,0,,,,,,,,,,,,,"HBaseStorage always tries to fetch delegation token for a secure hbase cluster. But when pig is launched through Oozie, it will fail as TGT is not available in the map job. In that case, it should try and reuse the hbase delegation token in JobConf passed to pig through mapreduce.job.credentials.binary property.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Feb/13 14:59;rohini;PIG-3206-1.patch;https://issues.apache.org/jira/secure/attachment/12570798/PIG-3206-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-25 17:57:49.773,,,no_permission,,,,,,,,,,,,314003,,,,Tue Feb 26 14:32:27 UTC 2013,,,,,,,0|i1i6pz:,314348,,,,,,,,,,25/Feb/13 14:59;rohini;Resorted to fetching delegation token only if kerberos credentials are there. Other approach is to to check if the JobConf has delegation token using TokenSelector for that particular hbase cluster and if not fetch the token. But that requires getting the cluster ID of hbase to match the service name and there is no clean API available in hbase to fetch the cluster id. ,"25/Feb/13 17:57;dvryaboy;Should we be catching the NoSuchMethodException for all those methods unavailable in 0.92 and 0.20.2? We'll definitely encounter is on m2.invoke in hadoop 0.20.2, according to the comment. 

Looks like now that the Class.forName line moved into an if block, the corresponding ClassNotFoundException should move in there as well.

The description in this JIRA says "" it should try and reuse the hbase delegation token in JobConf "" if the kerberos token is not available. Where does this reuse happen? I am not sure how this patch solves the problem, but then I don't really know how the kerberos integration in HBaseStorage works in the first place :).

Maybe a test would be helpful?","25/Feb/13 17:59;dvryaboy;Cancelling patch to clear from review queue (feel free to set patch available if you have a new patch or if this one is sufficient and I am wrong :)).

Should we make the fix apply on 0.11.1 as well?","25/Feb/13 19:00;rohini;Dmitriy,
   I had added a comment to clarify in the beginning of the first if block. If there is no security defined in hbase-site.xml, the whole block will not get executed. So we don't expect the block to be executed with hadoop 0.20.2 or hbase 0.90.x which do not have security. We are using reflection so that when HBaseStorage class compiled against hadoop 1.x but run against 0.20.2 or 0.90.x does not throw NoSuchMethodError for UserGroupInformation methods when executing the addHBaseDelegationToken method.

bq. Looks like now that the Class.forName line moved into an if block, the corresponding ClassNotFoundException should move in there as well.
  ClassNotFoundException happens in the outer if block as well. So not using a separate try catch block for the inner if block. 

bq. The description in this JIRA says "" it should try and reuse the hbase delegation token in JobConf "" if the kerberos token is not available. Where does this reuse happen?
   Pig in the front end (when run via command line) has access to the user TGT (either keytab or kinit from command line). It fetches the HBase delegation token by authenticating to HBase using KERBEROS and adds it to the JobConf. In the backend, HTable initiation picks up the delegation token from the JobConf and does further operations on HBase using DIGEST authentication. The getDelegationToken call is one call which cannot be done using DIGEST authentication(delegation token). The client always needs to authenticate with Kerberos to get the delegation token. 

   The case with Oozie is that the pig frontend run happens on a mapper and there is no access to user TGT. The JobConf passed to pig (using mapreduce.job.credentials.binary) already has the required delegation tokens to talk to all services - HDFS, JT and HBASE. In HBaseStorage, we were always trying to get the hbase delegation token. Since there was no TGT, it was failing in the pig launcher mapper. Now just added an extra check, so that we don't try to fetch the token if we don't have TGT.

Yes. It would be good to apply this to 0.11.1 as well. I will do it. 

  I will mark it back as patch available if it clarifies your questions. If you need me to add any comments to make it clear, I can do it.","25/Feb/13 21:28;rohini;bq. Maybe a test would be helpful?
  Manually tested with command line and Oozie against secure hbase for now. HADOOP-8078 which makes security unit tests possible is only available in Hadoop 3.0. ","25/Feb/13 23:39;dvryaboy;got it.
+1, go ahead and commit.

D",26/Feb/13 14:32;rohini;Thanks Dmitriy. Checked into 0.11.1 and trunk. Added a new section in CHANGES.txt for Release 0.11.1.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Passing arguments to python script does not work with -f option,PIG-3205,12633492,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,21/Feb/13 23:08,14/Oct/13 16:46,14/Mar/19 03:07,18/Mar/13 12:59,0.10.1,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"With ""pig sample.py arg1 arg2"", arguments can be accessed in the embedded python script using sys.argv[]. But not in the case ""pig -f sample.py arg1 arg2"". 

In case of ExecMode.FILE, we don't set PigContext.PIG_CMD_ARGS_REMAINDERS and so the arguments are not passed to JythonScriptEngine or GroovyScriptEngine. This is specially a problem with Oozie as it always uses -f option to specify the pig script.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Feb/13 00:36;rohini;PIG-3205.patch;https://issues.apache.org/jira/secure/attachment/12570381/PIG-3205.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-15 17:32:14.83,,,no_permission,,,,,,,,,,,,313987,,,,Mon Mar 18 12:59:08 UTC 2013,,,,,,,0|i1i6mf:,314332,,,,,,,,,,"15/Mar/13 17:32;cheolsoo;+1.

Do you mind deleting the following line if it's not necessary when you commit?
{code}
+        System.out.println(""---"");
{code}",15/Mar/13 17:34;rohini;Ah. Sure. Thanks for catching it. Left over from some debug statements.,"18/Mar/13 12:59;rohini;Committed to trunk. Thanks Cheolsoo. 

Note: Check in has misleading commit message ""Fix compilation error due to PIG-2507"" as I intended to commit the compilation failure fix for TestGrunt first, but ended up committing files in this patch along with it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CUBE operator not documented in user docs,PIG-3202,12633440,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,prasanth_j,billgraham,billgraham,21/Feb/13 18:04,02/Apr/13 15:54,14/Mar/19 03:07,25/Feb/13 04:49,0.11,,,,,,,0.11.1,0.12.0,,,,,,0,,,,,,,,,,,,,This is not documented in the user docs.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Feb/13 00:14;prasanth_j;PIG-3202.1.git.patch;https://issues.apache.org/jira/secure/attachment/12570566/PIG-3202.1.git.patch,25/Feb/13 04:49;billgraham;PIG-3202.2.patch;https://issues.apache.org/jira/secure/attachment/12570724/PIG-3202.2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-02-21 19:10:51.608,,,no_permission,,,,,,,,,,,,313935,,,,Mon Feb 25 04:49:48 UTC 2013,,,Patch Available,,,,0|i1i6av:,314280,,,,,,,,,,21/Feb/13 19:10;prasanth_j;[~billgraham] Can I move the relevant parts of release notes from PIG-2765 ( http://goo.gl/Jf1Qy ) to user docs? ,"21/Feb/13 19:52;billgraham;Sure, that would be great. See https://cwiki.apache.org/PIG/howtodocument.html for a description of how to modify user docs.","23/Feb/13 00:15;prasanth_j;Hi [~billgraham] The attached patch adds user documentation for CUBE and ROLLUP (PIG-3203). ""ant clean docs"" builds successfully. Please review it and let me know your feedback/improvements. ","25/Feb/13 04:49;billgraham;Looks great, thanks Prasanth! I just made a few minor formatting tweaks and committed.",25/Feb/13 04:49;billgraham;Committed to trunk and pig 11 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MiniCluster should delete hadoop-site.xml on shutDown,PIG-3200,12633151,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,prkommireddi,prkommireddi,prkommireddi,20/Feb/13 07:21,14/Oct/13 16:46,14/Mar/19 03:07,24/Feb/13 03:13,0.10.0,,,,,,,0.12.0,,,,,,,1,test,,,,,,,,,,,,"MiniCluster creates hadoop-site.xml under build/classes which is used by Pig tests in creating a (mini) hadoop cluster to run against. 

MiniCluster.buildCluster() deletes this file from previous run before re-generating it. However, this config file is not deleted when mini-cluster is shutdown. This can sometimes lead to unpredictable behavior when hadoop-site.xml is not expected on classpath (See PIG-3135). 

The cleanest operation would be to delete this file when the mini-cluster is shutdown.

Here is a more detailed explanation of the issue 
https://issues.apache.org/jira/browse/PIG-3135?focusedCommentId=13581825&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13581825",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Feb/13 19:45;prkommireddi;PIG-3200.patch;https://issues.apache.org/jira/secure/attachment/12570174/PIG-3200.patch,20/Feb/13 20:13;prkommireddi;PIG-3200_1.patch;https://issues.apache.org/jira/secure/attachment/12570176/PIG-3200_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-02-20 19:51:10.36,,,no_permission,,,,,,,,,,,,313647,,,,Sun Feb 24 03:13:14 UTC 2013,,,Patch Available,,,,0|i1i4iv:,313992,,,,,,,,,,20/Feb/13 19:45;prkommireddi;Made conf dir(build/classes) and conf file (hadoop-site.xml) static final variables. Modified shutdownMiniMrClusters() to delete the conf file.,20/Feb/13 19:51;cheolsoo;Can you do the same in hadoop23 shim?,"20/Feb/13 20:13;prkommireddi;[~cheolsoo] hadoop23 shim done. It wasn't on my build path so never picked it up, thanks for catching that. I tested the change with a few other tests that use MiniCluster (TestPigServer, TestAccumulator) and it works fine. 

PIG-3135_1.patch also works fine now.",20/Feb/13 21:31;cheolsoo;Great! Please let me run full unit test with both 20 and 23.,"22/Feb/13 19:19;cheolsoo;+1. Unit tests pass in both 20 and 23.

Please let me run a quick test with the PIG-3135 patch and verify that this fixes the PIG-3135 issue.

Thanks!",24/Feb/13 03:13;cheolsoo;Committed to trunk. Thanks Prashant!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Let users use any function from PigType -> PigType as if it were builtlin,PIG-3198,12633009,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,jcoveney,jcoveney,19/Feb/13 16:33,14/Oct/13 16:46,14/Mar/19 03:07,20/Apr/13 04:22,,,,,,,,0.12.0,,,,,,,1,,,,,,,,,,,,,"This idea is an extension of PIG-2643. Ideally, someone should be able to call any function currently registered in Pig as if it were builtin.",,,,,,,,,,,,,,,,,PIG-2643,,,,,,,,PIG-3286,,,,,,,22/Feb/13 14:12;jcoveney;PIG-3198-0.patch;https://issues.apache.org/jira/secure/attachment/12570472/PIG-3198-0.patch,19/Apr/13 10:53;jcoveney;PIG-3198-1.patch;https://issues.apache.org/jira/secure/attachment/12579528/PIG-3198-1.patch,19/Apr/13 17:47;cheolsoo;PIG-3198-apache_header.patch;https://issues.apache.org/jira/secure/attachment/12579584/PIG-3198-apache_header.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-04-18 21:55:36.743,,,no_permission,,,,,,,,,,,,313505,,,,Sat Apr 20 04:22:54 UTC 2013,,,,,,,0|i1i3nb:,313850,,,,,,,,,,"22/Feb/13 14:12;jcoveney;So I actually implemented this. You can check TestBuilinInvoker for some examples, but generally the syntax is as such:

{code}
a = foreach @ generate invoke(x)concat(x);
{code}

in the case of a function on another type and

{code}
a = foreach @ generate invoke&Integer.valueOf(x);
{code}

in the case of static types.

Currently it should support any function taking 0+ PigType arguments and returning a PigType argument...in the future we could allow people to cast Object, or to chain together non-PigTypes but that was a bit out of the scope of what I wanted to work on for this.

I actually don't love the syntax and would love to evolve it, but that portion of the parser is really hairy and it is really difficult not to introduce ambiguities...after about 10 hours of banging my head on it I went with the above. I'd love to have some eyes on this for technical merit etc.

Essentially, this turns any PigType->PigType method into a UDF without having to have a builtin, which I think is cool. This means people can have an arbitrary method on their classpath and don't have to go through the annoyance of wrapping it in a UDF. Ideally this cuts down on the number of lame builtin functions we need to add as people can just use this (it uses bytecode generation so is as performant as any code we'd write, though there are a couple of bytecode optimizations I could do down the line).",22/Feb/13 14:14;jcoveney;https://reviews.apache.org/r/9559/,18/Apr/13 21:55;alangates;I looked through this.  Other than spare tabs (rather than spaces) in some of the files it looks good.  +1.  I think this is exciting functionality.  I'm glad to see it added.,19/Apr/13 10:53;jcoveney;Awesome Alan. Fixed the tabs and will commit shortly.,"19/Apr/13 17:42;cheolsoo;[~jcoveney], thank you for implementing this!

You forgot to add the Apache header to the new files. Here is a patch that adds them. I also fixed more white spaces while doing it.",19/Apr/13 18:14;dvryaboy;Please add docs!,19/Apr/13 23:08;jcoveney;Dmitriy: https://issues.apache.org/jira/browse/PIG-3284 :),20/Apr/13 04:22;cheolsoo;PIG-3198-apache_header.patch is committed. Closing the jira again.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changes to ObjectSerializer.java break compatibility with Hadoop 0.20.2,PIG-3194,12632807,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,prkommireddi,kadeng,kadeng,18/Feb/13 09:51,02/Apr/13 15:54,14/Mar/19 03:07,16/Mar/13 00:04,0.11,,,,,,,0.11.1,0.12.0,,,,,,0,,,,,,,,,,,,,"The changes to ObjectSerializer.java in the following commit
http://svn.apache.org/viewvc?view=revision&revision=1403934 break compatibility with Hadoop 0.20.2 Clusters.

The reason is, that the code uses methods from Apache Commons Codec 1.4 - which are not available in Apache Commons Codec 1.3 which is shipping with Hadoop 0.20.2.

The offending methods are Base64.decodeBase64(String) and Base64.encodeBase64URLSafeString(byte[])

If I revert these changes, Pig 0.11.0 candidate 2 works well with our Hadoop 0.20.2 Clusters.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Mar/13 19:26;prkommireddi;PIG-3194.patch;https://issues.apache.org/jira/secure/attachment/12572374/PIG-3194.patch,15/Mar/13 22:20;prkommireddi;PIG-3194_2.patch;https://issues.apache.org/jira/secure/attachment/12573959/PIG-3194_2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-02-18 14:18:07.933,,,no_permission,,,,,,,,,,,,313303,,,,Sat Mar 16 00:34:39 UTC 2013,,,Patch Available,,,,0|i1i2ef:,313648,,,,,,,,,,18/Feb/13 14:18;jcoveney;Do we want to consider just using commons codec 1.3? Do the newer versions of Hadoop still use that version of commons codec? ,"18/Feb/13 14:51;kadeng;I would propose to try and stay as compatible as possible with reasonable effort. Here, the effort to stay compatible with a still widely used version of Hadoop seems to be minimal. I see 3 alternative solutions here:

1.) Revert the mentioned commit. 

2.) Reproduce the required functionality using code that's compatible to commons codec 1.3 (i.e. use Base64.decodeBase64(str.getBytes()) instead of the shortcut Base64.decodeBase64(str).

3.) Embed a rebased commons codec 1.4+ into pig. With that, I mean a version of commons codec 1.4+ which has been automatically refactored such that it has a new base package, for example pig.org.apache.commons.codec or something like that.

",18/Feb/13 15:37;jcoveney;Assuming that all hadoop uses commons codec 1.3 (is that what hadoop 2 uses?) then #2 seems like a no-brainer to me,"18/Feb/13 18:50;jcoveney;Hmm, so I simply set common-codec.version in ivy/library.properties back to 1.3 and everything worked fine, but it looks like something else is pulling in the 1.4 lib (it appears in build/ivy/lib/Pig/). Need to figure out what is causing that and then my plan is to roll back to 1.3, see what breaks, and fix it. I doubt there is very much.

Need to get to the bottom of why 1.4 is still getting brought in, though.","18/Feb/13 19:14;ashutoshc;Hadoop has moved to 1.4 long way back. If I remember correctly 0.20.2 was last release with 1.3. In hadoop2 I believe they are at 1.5. Though this is debatable but IMO consistency with hadoop-1 is more important than 0.20.2. 
http://svn.apache.org/viewvc/hadoop/common/branches/branch-1/ivy/libraries.properties?revision=1342816&view=markup","18/Feb/13 20:26;kadeng;I'd recommend to build the way you do now with 1.4. But test with Hadoop 0.20.2 as well. Mostly, commons-codec 1.3 and 1.4 are compatible. 1.3 is just missing some Methods. Actually the compatibility problem is just these 2 methods. Maybe it's even easier to copy the Base64 class from commons-codec 1.4 into some utility package of pit itself.","02/Mar/13 02:30;prkommireddi;Base64 seems to have been refactored a bit for codec 1.4. It now extends a base class (BaseNCodec) and uses a util (StringUtils). There's not much of dependency on StringUtils. Though BaseNCodec and Base64 are slightly tied up (even though it's just the 2 methods we use in pig). We could do a few things here

1. Bring Base* classes into a pig util
2. Eliminate call to encodeBase64URLSafeString and use byte[] encodeBase64 (available in 1.3) instead.

Looking at the Base64 code, there isn't much difference in the 2 methods except that '-' is used for '+' and '_' is used for '/' with URLSafe. Doesn't look like we need the encoding be url safe either?

And as Kai suggested, we can use Base64.decodeBase64(str.getBytes()) from 1.3 for decoding. 

Thoughts?




",05/Mar/13 01:23;prkommireddi;Would be great to have some ideas from others and discuss :),"05/Mar/13 11:19;jcoveney;I don't love the idea of bringing the classes in, and would rather just use the methods available in 1.3 if possible. I think that's a fine path, as you are correct: we don't need to be url safe.","05/Mar/13 18:46;julienledem;same as Jon. we can just use the methods present in 1.3 and we don't need to be URL safe.
Let's not repackage commons.codec or duplicate part of it just for this. ","05/Mar/13 18:50;prkommireddi;Sounds good to me. 1 was a just an option to put out there, I agree using 1.3 methods is a lot cleaner/better approach.","06/Mar/13 19:26;prkommireddi;Changed ObjectSerializer to use codec 1.3 methods. Tested with unit tests that are using ser/deser (TestPruneColumn, TestEvalPipeline2) and ""ant test-commit"" - all pass. Also tested against Load/Store funcs that use a lot of this (TestPigStorage, TestHBaseStorage). 

Please review.","07/Mar/13 20:07;dvryaboy;Prashant, were you able to test in environments where 1.4 is and is not available?","08/Mar/13 00:01;prkommireddi;Dmitriy, yes. Here is what I have done after the fix:

*1. compile pig with 1.4*
1a. start up pig with hadoop 0.20.2 - runs [expected]
1b. start up pig with hadoop 1 - runs [expected]
1c. test-commit and few other tests mentioned in my previous comment all pass.
1d. run sample scripts against 1a and 1b - works good.


*2. compile pig with 1.3*
2a. start up pig with hadoop 0.20.2 - runs [expected]
2b. start up pig with hadoop-1 - runs [expected]
2c. test-commit and other tests pass but TestMRCompiler fails - *[not expected]*
2d. run sample scripts against 2a and 2b - works good


The issue with 2c (TestMRCompiler.testMergeJoin failing against 1.3) is that the gold file being used contains a serialized string that was generated using 1.4. There seem to be a few differences between 1.3 and 1.4 with encode/decode behavior - please see https://issues.apache.org/jira/browse/CODEC-89, https://issues.apache.org/jira/browse/CODEC-91

Considering our tests run against hadoop-1 and all pass I am not sure if we should be spending time in making the test case codec 1.3 aware? 
","11/Mar/13 18:41;prkommireddi;Dmitriy, let me know what you think?","11/Mar/13 19:30;dvryaboy;We can skip the test if we detect that we are on Hadoop 2.0 by using org.junit.Assume 

Let's do that so we don't have to come back and fix this later.",15/Mar/13 22:20;prkommireddi;Uploading a new patch with Dmitriy's feedback incorporated.,15/Mar/13 23:36;dvryaboy;+1,"16/Mar/13 00:04;dvryaboy;Committed to 0.11.1 and trunk.

Thanks Kai for reporting and Prashant for fixing!","16/Mar/13 00:10;prkommireddi;Thanks for review/commit, Dmitriy.","16/Mar/13 00:34;prkommireddi;Kai, can you confirm 11.1 works for you?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix ""ant docs"" warnings",PIG-3193,12632774,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,18/Feb/13 02:28,14/Oct/13 16:46,14/Mar/19 03:07,16/Apr/13 19:01,0.11,,,,,,,0.12.0,,,,build,documentation,,0,newbie,,,,,,,,,,,,"I see many warnings every time when I run ""ant clean docs"". They don't break build, but it would be nice if we could clean them if possible.",,,,,,,,,,,,,,,,,,,PIG-3278,,,,,,,,,,,,,21/Mar/13 00:38;cheolsoo;PIG-3193.patch;https://issues.apache.org/jira/secure/attachment/12574690/PIG-3193.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-16 17:54:25.413,,,no_permission,,,,,,,,,,,,313270,,,,Tue Apr 16 19:01:18 UTC 2013,,,,,,,0|i1i273:,313615,,,,,,,,,,"21/Mar/13 00:38;cheolsoo;I am attaching a patch that includes the following changes:
* Fixed Javadoc symbol not found warnings. For example,
{code}
[javadoc] /home/cheolsoo/workspace/pig-svn/src/org/apache/pig/tools/grunt/GruntParser.java:302: cannot find symbol
{code}
I updated the ""javadoc-all"" target in build.xml to fix the classpath.
* Fixed Forrest plugin installation error:
{code}
[exec] Can't get http://forrest.apache.org/plugins//0.9/org.apache.forrest.plugin.input.simplifiedDocbook.zip to /home/cheolsoo/workspace/apache-forrest-0.9/build/plugins/org.apache.forrest.plugi     n.input.simplifiedDocbook.zip
{code}
The forrest 0.9 no longer has the ""simplifiedDocbook"" plugin, so I removed it from ""project.required.plugins"" in forrest.properties.
* Removed a legacy workaround for Forrest 0.8.
{code}
# PIG-1508: Workaround for http://issues.apache.org/jira/browse/FOR-984
# Remove when forrest-0.9 is available
forrest.validate.sitemap=false
{code} 
Apache jenkins uses Forrest 0.9, so this should be fine.
* Fixed Forrest unresolved id warnings. For example, 
{code}
[exec] WARN - Page 2: Unresolved id reference ""Case-Sensitivity"" found.
{code}
These warnings result with broken links in html docs.

I am NOT fixing the following warnings.
* Javadoc comment warnings. e.g.
{code}
LoadCaster.java:143: warning - @param argument ""fieldSchema"" is not a parameter name.
{code}
There are 165 of them, so I will address them in a separate jira.
* Forrest overflow warnings. e.g.
{code}
[exec] WARN - Line 1 of a paragraph overflows the available area by 1750mpt. (fo:block, ""chararray"")
{code}
This is because the text of the source tag is too wide, and it is apparently a bug in Forrest 0.9 (FOR-1104).","16/Apr/13 17:54;alangates;+1.  For the two you didn't fix, why don't you open a separate JIRA so that you can resolve this one with the issues you addressed.","16/Apr/13 18:25;cheolsoo;Thanks Alan for reviewing the patch. Per your suggestion, I filed PIG-3278 for javadoc warnings.",16/Apr/13 19:01;cheolsoo;Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Better call to action to download Pig in docs,PIG-3192,12632770,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,russell.jurney,russell.jurney,russell.jurney,18/Feb/13 00:02,14/Oct/13 16:45,14/Mar/19 03:07,19/Feb/13 15:33,0.11,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,The docs should link to the download page up top.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Feb/13 00:03;russell.jurney;PIG-3192.patch;https://issues.apache.org/jira/secure/attachment/12569740/PIG-3192.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-19 15:33:46.343,,,no_permission,,,,,,,,,,,,313266,,,,Tue Feb 19 15:33:46 UTC 2013,,,,,,,0|i1i267:,313611,,,,,,,,,,18/Feb/13 00:03;russell.jurney;Adds call to action to download Pig.,"19/Feb/13 15:33;jcoveney;+1, committed. Thanks Russell!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MultiStorage output filenames are not sortable,PIG-3191,12632765,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,dannyant,dannyant,dannyant,17/Feb/13 22:28,14/Oct/13 16:46,14/Mar/19 03:07,19/Feb/13 14:16,0.10.0,,,,,,,0.12.0,,,,piggybank,,,0,,,,,,,,,,,,,"The MultiStorage output filenames are not properly sortable.
The output filenames do not pad the index, so the first split will be 
xxxxx-0

as opposed to what the class documentation says, which is 
xxxxx-0000


When retrieving these files the sorted order may not be as expected.


fs -ls /data/testmultistorage/12006391.gz/
Found 18 items
-rw-r--r--   3 dev supergroup        158 2013-02-17 07:54 /data/testmultistorage/12006391.gz/12006391-0.gz
-rw-r--r--   3 dev supergroup        158 2013-02-17 07:54 /data/testmultistorage/12006391.gz/12006391-1.gz
-rw-r--r--   3 dev supergroup        173 2013-02-17 07:54 /data/testmultistorage/12006391.gz/12006391-10.gz
-rw-r--r--   3 dev supergroup        173 2013-02-17 07:54 /data/testmultistorage/12006391.gz/12006391-11.gz
-rw-r--r--   3 dev supergroup        172 2013-02-17 07:54 /data/testmultistorage/12006391.gz/12006391-12.gz
-rw-r--r--   3 dev supergroup        172 2013-02-17 07:54 /data/testmultistorage/12006391.gz/12006391-13.gz
-rw-r--r--   3 dev supergroup        171 2013-02-17 07:54 /data/testmultistorage/12006391.gz/12006391-14.gz
-rw-r--r--   3 dev supergroup        184 2013-02-17 07:54 /data/testmultistorage/12006391.gz/12006391-15.gz
-rw-r--r--   3 dev supergroup        174 2013-02-17 07:54 /data/testmultistorage/12006391.gz/12006391-16.gz
-rw-r--r--   3 dev supergroup        132 2013-02-17 07:54 /data/testmultistorage/12006391.gz/12006391-17.gz
-rw-r--r--   3 dev supergroup        165 2013-02-17 07:54 /data/testmultistorage/12006391.gz/12006391-2.gz
-rw-r--r--   3 dev supergroup        173 2013-02-17 07:54 /data/testmultistorage/12006391.gz/12006391-3.gz
-rw-r--r--   3 dev supergroup        173 2013-02-17 07:54 /data/testmultistorage/12006391.gz/12006391-4.gz
-rw-r--r--   3 dev supergroup        173 2013-02-17 07:54 /data/testmultistorage/12006391.gz/12006391-5.gz
-rw-r--r--   3 dev supergroup        162 2013-02-17 07:54 /data/testmultistorage/12006391.gz/12006391-6.gz
-rw-r--r--   3 dev supergroup        173 2013-02-17 07:54 /data/testmultistorage/12006391.gz/12006391-7.gz
-rw-r--r--   3 dev supergroup        174 2013-02-17 07:54 /data/testmultistorage/12006391.gz/12006391-8.gz
-rw-r--r--   3 dev supergroup        172 2013-02-17 07:54 /data/testmultistorage/12006391.gz/12006391-9.gz
",Any usage of Multi-Storage,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Feb/13 22:36;dannyant;MultiStorage.patch;https://issues.apache.org/jira/secure/attachment/12569734/MultiStorage.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-19 14:15:56.109,,,no_permission,,,,,,,,,,,,313261,,,,Fri Aug 16 19:12:15 UTC 2013,,,Patch Available,,,,0|i1i253:,313606,"MultiStorage output file format will now be as described, with the split index being padded.",,,,,,,,,17/Feb/13 22:31;dannyant;Without this change the output of multistorage will not be in the correct sorted order.,"19/Feb/13 14:15;jcoveney;+1 and committed, thanks danny. Note that this is a non-backwards compatible change, but I think it is reasonable and in all sense, the contradiction of the documentation can be considered a bug",16/Aug/13 19:12;cheolsoo;Updating assignee.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove ivy/pig.pom and improve build mvn targets,PIG-3189,12632690,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,billgraham,billgraham,billgraham,16/Feb/13 02:07,14/Oct/13 16:46,14/Mar/19 03:07,22/Feb/13 06:10,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"{{ivy/pig.pom}} in SVN seems to no longer be used.  At build time ({{ant set-version}} via {{ant mvn-deploy}}) {{ivy/pig.pom}} is generated from {{ivy/pig-template.xml}}. We should remove {{ivy/pig.pom}} from SVN.

It would also be good to decouple building the maven artifacts from publishing them, since those two tasks might be done on different hosts.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Feb/13 02:08;billgraham;PIG-3189.1.patch;https://issues.apache.org/jira/secure/attachment/12569637/PIG-3189.1.patch,19/Feb/13 16:22;billgraham;PIG-3189.2.patch;https://issues.apache.org/jira/secure/attachment/12569956/PIG-3189.2.patch,20/Feb/13 06:34;billgraham;PIG-3189.3.patch;https://issues.apache.org/jira/secure/attachment/12570087/PIG-3189.3.patch,22/Feb/13 06:05;billgraham;PIG-3189.4.patch;https://issues.apache.org/jira/secure/attachment/12570421/PIG-3189.4.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-02-19 14:11:37.879,,,no_permission,,,,,,,,,,,,313186,,,,Fri Feb 22 06:05:38 UTC 2013,,,,,,,0|i1i1on:,313532,,,,,,,,,,16/Feb/13 02:08;billgraham;Attaching a patch. Patch doesn't include deletion of {{ivy/pig.pom}} since I can't seem to get that reflected once the file is deleted. ,"19/Feb/13 14:11;jcoveney;Bill, are you using git? git rm file should reflect the deletion. Does it not?","19/Feb/13 14:12;jcoveney;Also, +1","19/Feb/13 16:22;billgraham;{{git rm}} did not, but just {{rm}} did the trick. Here's patch 2 which reflects the delete.","19/Feb/13 19:17;billgraham;There have been changes to {{ivy/pig.pom}} that are not reflected in {{ivy/pig-template.xml}}. Particularly antlr is not being included in the published pom because it's not in the template. Will submit a new patch.

{nofomat}
$ diff ivy/pig-template.xml ivy/pig.pom
24c24
<   <version>@version</version>
---
>   <version>0.9.0-SNAPSHOT</version>
85,86c85,86
<     </dependency>
<     <dependency>
---
>    </dependency>
>    <dependency>
122c122,132
<       <groupId>org.apache.avro</groupId>
---
>       <groupId>org.antlr</groupId>
>       <artifactId>antlr-runtime</artifactId>
>       <version>3.4</version>
>     </dependency>
>     <dependency>
>       <groupId>org.antlr</groupId>
>       <artifactId>ST4</artifactId>
>       <version>4.0.4</version>
>     </dependency>
>    <dependency>
>       <groupId>org.apache.hadoop</groupId>
124c134
<       <version>1.5.3</version>
---
>       <version>1.3.2</version>
{noformat}",20/Feb/13 06:34;billgraham;Adding patch #3 which includes antlr in the template.,22/Feb/13 06:05;billgraham;Patch #3 had my *.iws changes in it. Uploading cleaned up patch #4. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tar/deb/pkg ant targets should depend on piggybank,PIG-3186,12632466,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,lbendig,billgraham,billgraham,14/Feb/13 20:44,14/Oct/13 16:46,14/Mar/19 03:07,16/Apr/13 18:24,,,,,,,,0.12.0,,,,,,,0,low-hanging-fruit,simple,,,,,,,,,,,"The tar, deb and rpm artifacts should contain piggybank but they don't when built via ant unless piggybank is built separately.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Mar/13 13:26;lbendig;piggy.patch;https://issues.apache.org/jira/secure/attachment/12575701/piggy.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-27 13:26:00.783,,,no_permission,,,,,,,,,,,,312962,,,,Tue Apr 16 18:24:51 UTC 2013,,,,,,,0|i1i0av:,313308,,,,,,,,,,"27/Mar/13 13:26;lbendig;Hi,

I'd like submit my very first patch.
Note: couldn't test deb and rpm targets, I assume these have already been removed ([PIG-3174|https://issues.apache.org/jira/browse/PIG-3174])

--Lorand","29/Mar/13 20:18;alangates;Is this ready for review?  If so please click ""Submit Patch"" so we know to review it.  Thanks for the patch.",16/Apr/13 18:24;alangates;Patch checked in.  Thanks Lorand.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partition filter push down does not happen partition keys condition include a AND and OR construct,PIG-3173,12631473,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,08/Feb/13 16:16,14/Oct/13 16:46,14/Mar/19 03:07,29/Apr/13 21:20,0.10.1,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"A = load 'db.table' using org.apache.hcatalog.pig.HCatLoader();
B = filter A by (region=='usa' AND dt=='201302051800') OR (region=='uk' AND dt=='201302051800');
C = foreach B generate name, age;
DUMP C;

gives the below warning and scans the whole table.

2013-02-06 22:22:16,233 [main] WARN  org.apache.pig.newplan.PColFilterExtractor  - No partition filter push down: You have an partition column (region ) in a construction like: (pcond  and ...) or (pcond and ...) where pcond is a condition on a partition column.
2013-02-06 22:22:16,233 [main] WARN  org.apache.pig.newplan.PColFilterExtractor  - No partition filter push down: You have an partition column (datestamp ) in a construction like: (pcond  and ...) or (pcond and ...) where pcond is a condition on a partition column.",,,,,,,,,,,,,,,,,,,PIG-3395,,,,,,,,,,,,,20/Mar/13 00:17;rohini;PIG-3173-1.patch;https://issues.apache.org/jira/secure/attachment/12574459/PIG-3173-1.patch,29/Apr/13 20:22;rohini;PIG-3173-2.patch;https://issues.apache.org/jira/secure/attachment/12581032/PIG-3173-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-04-18 21:58:17.488,,,no_permission,,,,,,,,,,,,311969,,,,Mon Apr 29 21:20:42 UTC 2013,,,,,,,0|i1hu5z:,312315,,,,,,,,,,20/Mar/13 00:17;rohini;https://reviews.apache.org/r/10035/,18/Apr/13 21:58;alangates;Canceling patch until feedback from Dmitriy is addressed.,"29/Apr/13 17:52;cheolsoo;If I understand comments on RB, there is no real issue with the patch other than that we can do better on the '(A and B) or (C and D)' case.

Currently, Pig rejects all of the following expressions even if A, B, C, and D are all partition conditions:
- (A and B) or (C and D)
- (A and B) or C
- A or (C and D)

But this patch at least lets Pig push down expressions when A, B, C, and D are ALL partition conditions. IMO, this alone is a big win. Can we get this patch in and do further optimization on the '(A and B) or (C and D)' case in a separate jira?

Thanks!","29/Apr/13 18:17;rohini;> But this patch at least lets Pig push down expressions when A, B, C, and D are ALL partition conditions. IMO, this alone is a big win. Can we get this patch in and do further optimization on the '(A and B) or (C and D)' case in a separate jira?
  Sure. I had started on the optimization patch but I did not complete it before leaving for vacation. Wanted to be careful as there was lot of change and I had to almost evaluate the whole tree and ensure it works for all combinations as we are extracting partial conditions. I will create a separate jira for that and put the patch later. I will update this patch (first one had a bug) with just pushing down all partition conditions.",29/Apr/13 20:52;cheolsoo;+1  to PIG-3173-2.patch.,29/Apr/13 21:20;rohini;Checked into trunk (0.12). Thanks Dmitriy and Cheolsoo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partition filter push down does not happen when there is a non partition key map column filter,PIG-3172,12631470,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,08/Feb/13 16:11,14/Oct/13 16:46,14/Mar/19 03:07,18/Mar/13 22:08,0.10.1,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"A = LOAD 'job_confs' USING org.apache.hcatalog.pig.HCatLoader();
B = FILTER A by grid == 'cluster1' and dt < '2012_12_01' and dt > '2012_11_20';
C = FILTER B by params#'mapreduce.job.user.name' == 'userx';
D = FOREACH B generate dt, grid, params#'mapreduce.job.user.name' as user,
params#'mapreduce.job.name' as job_name, job_id,
params#'mapreduce.job.cache.files';
dump D;

The query gives the below warning and ends up scanning the whole table instead of pushing the partition key filters grid and dt.

[main] WARN  org.apache.pig.newplan.PColFilterExtractor - No partition filter
push down: Internal error while processing any partition filter conditions in
the filter after the load

Works fine if the second filter is on a column with simple datatype like chararray instead of map.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Mar/13 00:14;rohini;PIG-3172-1.patch;https://issues.apache.org/jira/secure/attachment/12571693/PIG-3172-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-18 06:54:22.795,,,no_permission,,,,,,,,,,,,311966,,,,Mon Mar 18 22:08:04 UTC 2013,,,,,,,0|i1hu5b:,312312,,,,,,,,,,02/Mar/13 00:14;rohini;Fixed for cases where non partition column is  a map or tuple,"18/Mar/13 06:54;daijy;+1, looks good.",18/Mar/13 22:08;rohini;Committed to trunk (0.12). Thanks Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestMultiQueryBasic.testMultiQueryWithSplitInMapAndMultiMerge fails in trunk,PIG-3168,12631187,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,07/Feb/13 01:11,14/Oct/13 16:46,14/Mar/19 03:07,22/Aug/13 19:27,0.12.0,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"PIG-2994 made explain with no alias be equivalent to explain on the previous alias. This breaks TestMultiQueryBasic.testMultiQueryWithSplitInMapAndMultiMerge because the previous alias is an auto-generated alias not a user-defined alias.

The following fixes the test:
{code}
         ""I = GROUP F2 BY (f7, f8);"" +
         ""STORE I into 'foo4'  using BinStorage();"" +
-        ""explain;"";
+        ""explain I;"";
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Aug/13 20:11;cheolsoo;PIG-3168-2.patch;https://issues.apache.org/jira/secure/attachment/12599020/PIG-3168-2.patch,22/Aug/13 17:19;cheolsoo;PIG-3168-3.patch;https://issues.apache.org/jira/secure/attachment/12599452/PIG-3168-3.patch,07/Feb/13 01:13;cheolsoo;PIG-3168.patch;https://issues.apache.org/jira/secure/attachment/12568346/PIG-3168.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-02-07 18:40:04.967,,,no_permission,,,,,,,,,,,,311683,,,,Thu Aug 22 19:27:39 UTC 2013,,,,,,,0|i1hsen:,312029,,,,,,,,,,07/Feb/13 18:40;daijy;+1,08/Feb/13 03:06;cheolsoo;Committed to trunk. Thank you Daniel for your review!,"20/Aug/13 18:25;rohini;bq. PIG-2994 made explain with no alias be equivalent to explain on the previous alias. 
  Shouldn't we revert back the behavior of explain with no alias to older behavior of explaining the whole script instead of fixing the test? It is kind of breaking backward compatibility.",20/Aug/13 20:06;cheolsoo;Thanks Rohini. I will post a patch that reverts it to the old behavior in batch mode.,"20/Aug/13 20:10;cheolsoo;So here is what it does now:
* In interactive mode, explain with no alias == explain on the last relation.
* In batch mode, explain with no alias == explain on the entire script.

Let me know whether this is not good.",20/Aug/13 20:12;cheolsoo;TestMultiQueryBasic passes.,20/Aug/13 21:47;rohini;+1,"22/Aug/13 17:19;cheolsoo;I noticed that TestShortcuts is broken with my changes to explain in batch mode. I fixed the test in a new patch.

In fact, I didn't commit PIG-3168-2.patch at all. Since additional changes in the new patch are very minor, I will go ahead commit it.",22/Aug/13 18:23;rohini;+1,22/Aug/13 19:27;cheolsoo;Thanks Rohini. Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job stats are printed incorrectly for map-only jobs,PIG-3167,12631173,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,mwagner,mwagner,mwagner,07/Feb/13 00:09,14/Oct/13 16:46,14/Mar/19 03:07,19/Feb/13 15:31,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"Right now, printing for map-only jobs depends on maxReduceTime < 0, but that value defaults to 0. Also, the number of ""n/a""'s is incorrect.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Feb/13 00:11;mwagner;PIG-3167.1.patch;https://issues.apache.org/jira/secure/attachment/12568332/PIG-3167.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-19 15:31:56.991,,,no_permission,,,,,,,,,,,,311669,,,,Thu Feb 21 03:25:56 UTC 2013,,,,,,,0|i1hsbj:,312015,,,,,,,,,,"19/Feb/13 15:31;jcoveney;+1, committed. Thanks Mark!",21/Feb/13 03:25;cheolsoo;Updating assignee.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update reserved keywords in Pig docs,PIG-3161,12630964,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,russell.jurney,cheolsoo,cheolsoo,05/Feb/13 22:37,14/Oct/13 16:46,14/Mar/19 03:07,18/Feb/13 01:08,0.11,,,,,,,0.12.0,,,,documentation,,,0,,,,,,,,,,,,,"I noticed that the reserved keywords in the doc is out of date. The new keywords such as RANK, BIGINTEGER, and BIGDECIMAL are missing in
src/docs/src/documentation/content/xdocs/basic.xml.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Feb/13 23:19;russell.jurney;PIG-3161.patch;https://issues.apache.org/jira/secure/attachment/12569737/PIG-3161.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-17 23:19:00.27,,,no_permission,,,,,,,,,,,,311460,,,,Mon Feb 18 01:08:13 UTC 2013,,,,,,,0|i1hr13:,311806,,,,,,,,,,17/Feb/13 23:19;russell.jurney;Please let me know i there are any other keywords to add.,"18/Feb/13 01:02;cheolsoo;+1.

I can't think of any other words.","18/Feb/13 01:08;cheolsoo;Committed to trunk.

Thank you Russell!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Errors in the document ""Control Structures""",PIG-3158,12630564,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,miyakawataku,miyakawataku,miyakawataku,03/Feb/13 06:44,14/Oct/13 16:46,14/Mar/19 03:07,18/Feb/13 02:38,,,,,,,,0.12.0,,,,documentation,,,0,documentation,,,,,,,,,,,,"This is a patch to fix errors in document ""Control Structures.""

# ""from ... import Pig"" statement is commented out in several example programs.
# Several examples include invalid shebang lines ""#! /usr/bin/python."" No space character is permitted between ""!"" and the path.
# The sentence ""... a map of parameters (...) must be provided as was illustrated in the example above"" does not make a sence, because the example actually shows implicit binding.
# An invalid comment line ""// In a jython script""
# ""1. param"" and ""2. Command line arguments"" in the section ""Passing Parameters to a Script"" are effectively heading lines, but not formatted in that way.
# The python module ""sys"" is used but not imported in several example programs.
# An example program in the section ""Automated Pig Latin Generation"" does not actually generate Pig Latin statements. The program does parameter binding. The patch removes the program and merges the sections ""Automated Pig Latin Generation"", ""Conditional Compilation"" and ""Parallel Execution.""
# An example program in ""PigProgressNotificationListener Object"" lacks the first line of the class definition.
# An input file of the example program idmapreduce.java is indicated to be placed on the local working directory, but the file must be placed on the home direcotry on the HDFS.
# From the statement ""Unlike user defined functions (UDFs), which only allow quoted strings as its parameters"", the ""which"" clause should be removed because UDFs can actually take types of parameters other than strings.
# In several example programs, smart quotation marks (‘ and ’) are wrongly used to quote strings. The patch replaces them by straight quotation marks ( ' ).
# In several examples, endash marks (U+2013) are wrongly used as hyphens. The patch replaces them by hyphens ( - ).
# Removes a meaningless statement: ""If parameters are specified using the preprocessor statements, the script should include either %declare or %default.""
# A parameter precedence list says ""parameters defined in a script"" have the lowest precedence, but actually parameters in parameter files have the lowest precedence.
# The last example of the page declares a parameter with a command output, but it wrongly uses quotation marks, not back ticks.
# Improves indentation of example programs.
# Updates a link: [PigServer|http://pig.apache.org/docs/r0.10.0/api/org/apache/pig/PigServer.html]
# Fixes typos:
#* ""Compile is a static function on the Pig -object- +class+ ""
#* ""a Java Properties object -and- or +a+ file containing a list of properties""
#* ""The example -above- +in the previous section+ shows how to make use of this call""
#* ""PigStats as it -is today will- +was before has+ become SimplePigStats""
#* "" -pig.java- +Pig+ Object""
#* ""BoundScript -.java- Object""
#* ""PigStats -.java- Object""
#* ""PigProgressNotificationListener -.java- Object""
#* ""a fully substituted Pig script +is+ produced""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Feb/13 13:40;miyakawataku;PIG-3158.patch;https://issues.apache.org/jira/secure/attachment/12568564/PIG-3158.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-05 21:35:02.033,,,no_permission,,,,,,,,,,,,311058,,,,Mon Feb 18 02:38:01 UTC 2013,,,Patch Available,,,,0|i1hojj:,311403,,,,,,,,,,"05/Feb/13 21:35;cheolsoo;[~miyakawataku], thank you very much for your thorough work. I have a few comments:
* {quote}
From the statement ""Unlike user defined functions (UDFs), which only allow quoted strings as its parameters"", the ""which"" clause should be removed because UDFs can actually take types of parameters other than strings.
{quote}
This is not entirely true. Load/Store UDFs only take strings, so I think the original statement is correct.
* {quote}
Removes a meaningless statement: ""If parameters are specified using the preprocessor statements, the script should include either %declare or %default.""
{quote}
I don't think this is meaningless because it is valid to use the preprocessor statements to specify parameters: http://pig.apache.org/docs/r0.10.0/cont.html#Parameter-Sub
* {quote}
A parameter precedence list says ""parameters defined in a script"" have the lowest precedence, but actually parameters in parameter files have the lowest precedence.
{quote}
I believe that ""parameters defined in a script"" means the default values defined by %default statements. I can verify that the following precedence holds: %default statements < -param_file < -param option in command line < %declare statements.

Please let me know what you think.","08/Feb/13 13:40;miyakawataku;Thank you for your comments! I updated the patch.

About the first two points, I reverted the changes.

About the last point, I updated the list as: %default < -param_file < -param < %declare.",18/Feb/13 02:33;cheolsoo;+1. I will commit it soon.,18/Feb/13 02:38;cheolsoo;Committed to trunk. Thanks Miyakawa!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestSchemaTuple fails in trunk,PIG-3156,12630444,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,01/Feb/13 20:59,14/Oct/13 16:45,14/Mar/19 03:07,10/Feb/13 20:19,0.12.0,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"To reproduce the issue, do:
{code}
ant clean test -Dtestcase=TestSchemaTuple
{code}
All 3 test cases fail with the following error:
{code}
Caused by: java.lang.RuntimeException: Unable to compile
    at org.apache.pig.impl.util.JavaCompilerHelper.compile(JavaCompilerHelper.java:83)
    at org.apache.pig.data.SchemaTupleClassGenerator.compileCodeString(SchemaTupleClassGenerator.java:233)
    at org.apache.pig.data.SchemaTupleClassGenerator.generateSchemaTuple(SchemaTupleClassGenerator.java:186)
    at org.apache.pig.data.SchemaTupleFrontend$SchemaTupleFrontendGenHelper.generateAll(SchemaTupleFrontend.java:203)
    at org.apache.pig.data.SchemaTupleFrontend$SchemaTupleFrontendGenHelper.access$100(SchemaTupleFrontend.java:91)
    at org.apache.pig.data.SchemaTupleFrontend.copyAllGeneratedToDistributedCache(SchemaTupleFrontend.java:278)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJob(JobControlCompiler.java:656)
{code}
I found that this was introduced by PIG-2764.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Feb/13 01:23;cheolsoo;PIG-3156.patch;https://issues.apache.org/jira/secure/attachment/12568724/PIG-3156.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-08 07:28:00.063,,,no_permission,,,,,,,,,,,,310938,,,,Sun Feb 10 20:19:20 UTC 2013,,,,,,,0|i1hnsv:,311283,,,,,,,,,,"08/Feb/13 07:28;jcoveney;This makes sense. When it runs into a type that it doesn't recognize, it dies. Not sure why the DateTime stuff didn't kill it, though.

I won't be able to code anything for probably about a week (I am in transit), but I'd be happy to review anything. I will try and bang out the fix if I have a spare moment though. I do not think it is a very complicated fix.",10/Feb/13 01:23;cheolsoo;Attached is a patch that fixes the test.,10/Feb/13 07:11;jcoveney;+1 thanks Cheolsoo!,10/Feb/13 20:19;cheolsoo;Committed to trunk. Thank you Jonathan for the review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestTypeCheckingValidatorNewLP.testSortWithInnerPlan3 fails in trunk,PIG-3155,12630435,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,01/Feb/13 20:20,14/Oct/13 16:46,14/Mar/19 03:07,10/Feb/13 20:17,0.12.0,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"To reproduce the failure, do:
{code}
ant clean test -Dtestcase=TestTypeCheckingValidatorNewLP
{code}
The test fails with the following error:
{code}
Error expected
junit.framework.AssertionFailedError: Error expected
    at org.apache.pig.test.TestTypeCheckingValidatorNewLP.testSortWithInnerPlan3(TestTypeCheckingValidatorNewLP.java:1570)
{code}
I found that this was introduced by PIG-2764.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Feb/13 02:03;cheolsoo;PIG-3155.patch;https://issues.apache.org/jira/secure/attachment/12568727/PIG-3155.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-10 07:13:00.72,,,no_permission,,,,,,,,,,,,310929,,,,Sun Feb 10 20:17:01 UTC 2013,,,,,,,0|i1hnqv:,311274,,,,,,,,,,"10/Feb/13 02:03;cheolsoo;The test case expects an exception to be thrown from an expression of ""bytearray % chararray"", but it isn't.

The following fixes the test case:
{code}
--- a/src/org/apache/pig/newplan/logical/visitor/TypeCheckingExpVisitor.java
+++ b/src/org/apache/pig/newplan/logical/visitor/TypeCheckingExpVisitor.java
@@ -209,6 +209,8 @@ public class TypeCheckingExpVisitor extends LogicalExpressionVisitor{
         } else if (lhsType == DataType.BYTEARRAY) {
             if (rhsType == DataType.INTEGER || rhsType == DataType.LONG || rhsType == DataType.BIGINTEGER) {
                 insertCast(binOp, rhsType, binOp.getLhs());
+            } else {
+                error = true;
             }
{code}",10/Feb/13 07:13;jcoveney;+1,10/Feb/13 20:17;cheolsoo;Committed to trunk. Thank you Jonathan for the review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestPackage.testOperator fails in trunk,PIG-3154,12630431,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dreambird,cheolsoo,cheolsoo,01/Feb/13 20:04,14/Oct/13 16:46,14/Mar/19 03:07,09/Feb/13 23:23,0.12.0,,,,,,,0.12.0,,,,,,,0,newbie,,,,,,,,,,,,"To reproduce the issue, do:
{code}
ant clean test -Dtestcase=TestPackage
{code}
The test fails with the following error:
{code}
No test case for type biginteger
junit.framework.AssertionFailedError: No test case for type biginteger
    at org.apache.pig.test.TestPackage.pickTest(TestPackage.java:153)
    at org.apache.pig.test.TestPackage.testOperator(TestPackage.java:171)
{code}
Apparently, this is broken by PIG-2764.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Feb/13 00:53;dreambird;PIG-3154.patch.txt;https://issues.apache.org/jira/secure/attachment/12568140/PIG-3154.patch.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-06 00:53:23.278,,,no_permission,,,,,,,,,,,,310925,,,,Sat Feb 09 23:23:08 UTC 2013,,,,,,,0|i1hnpz:,311270,,,,,,,,,,"06/Feb/13 00:53;dreambird;the fix is straightforward, add section for BigInteger and BigDecimal. I run the test and it pass now.",08/Feb/13 03:28;cheolsoo;+1.,09/Feb/13 23:23;cheolsoo;Committed to trunk. Thanks Johnny!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestScriptUDF.testJavascriptExampleScript fails in trunk,PIG-3153,12630430,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,01/Feb/13 19:54,14/Oct/13 16:46,14/Mar/19 03:07,26/Feb/13 07:02,0.12.0,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"To reproduce the failure, do:
{code}
ant clean test -Dtestcase=TestScriptUDF
{code}
The test fails with the following error:
{code}
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 0: Given UDF returns an improper Schema. Schema should only contain one field of a Tuple, Bag, or a single type. Returns: {word: chararray,num: long}
    at org.apache.pig.newplan.logical.expression.UserFuncExpression.getFieldSchema(UserFuncExpression.java:206)
    at org.apache.pig.newplan.logical.optimizer.FieldSchemaResetter.execute(SchemaResetter.java:264)
    at org.apache.pig.newplan.logical.expression.AllSameExpressionVisitor.visit(AllSameExpressionVisitor.java:143)
    at org.apache.pig.newplan.logical.expression.UserFuncExpression.accept(UserFuncExpression.java:88)
    at org.apache.pig.newplan.ReverseDependencyOrderWalker.walk(ReverseDependencyOrderWalker.java:70)
    at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:52)
    at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visitAll(SchemaResetter.java:67)
    at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:122)
    at org.apache.pig.newplan.logical.relational.LOGenerate.accept(LOGenerate.java:240)
    at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
    at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:114)
    at org.apache.pig.newplan.logical.relational.LOForEach.accept(LOForEach.java:76)
    at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
    at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:52)
    at org.apache.pig.parser.LogicalPlanBuilder.expandAndResetVisitor(LogicalPlanBuilder.java:402)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Feb/13 07:25;cheolsoo;PIG-3153-2.patch.txt;https://issues.apache.org/jira/secure/attachment/12570741/PIG-3153-2.patch.txt,14/Feb/13 21:52;dreambird;PIG-3153.patch.txt;https://issues.apache.org/jira/secure/attachment/12569398/PIG-3153.patch.txt,14/Feb/13 21:44;dreambird;PIG-3153.patch.txt;https://issues.apache.org/jira/secure/attachment/12569396/PIG-3153.patch.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-02-14 21:44:01.26,,,no_permission,,,,,,,,,,,,310924,,,,Tue Feb 26 07:02:09 UTC 2013,,,,,,,0|i1hnpr:,311269,,,,,,,,,,"14/Feb/13 21:44;dreambird;I fix the javascript to define a schema of result, and update the test a little bit. With this patch, the test pass for me.",14/Feb/13 21:52;dreambird;a minor fix (using original $0 instead of $a0) on top of previous patch,"19/Feb/13 21:39;dreambird;please note in javascript, I use the x:{t:(word:chararray,num:long)} to define the return schema, which uses a bag to wrapper a tuple. This is the same as test testPythonAbsolutePath() did.

I tried to define the schema as t:(word:chararray,num:long), but didn't successfully to load the data into it correctly in 8 line of the javascript. I think this might be the case to all javascript. will look further.

","25/Feb/13 07:22;cheolsoo;[~dreambird], I looked into this and am attaching a patch (PIG-3153-2.patch.txt) that allows outputSchema to be a tuple. Here is my reasoning:

1. The reason why defining outputSchema as ""t:(word:chararray,num:long)"" causes a NPE is because the JavaScript object doesn't have a field called ""t"". If you read ""JsFunction.jsToPigTuple()"", it recursively searches for every field of outputSchema in the JavaScript object.

IMO, the right way to fix this is to wrap the JavaScript object with another object and name that wrapper object as ""t"". 

2. Regarding my change to JsFunction, I removed ""outputSchema.size() == 1"" in addition to adding ""outputSchema.getField(0).type == DataType.TUPLE"" to the if condition.

The former is redundant because the size of outputSchema is always equal to 1 now due to PIG-3082. The latter is added to wrap the JavaScript object into another object as explained above.

With this patch, the test code doesn't have to be modified at all. Please let me know what you think. Thanks!","25/Feb/13 19:47;dreambird;[~cheolsoo], thanks a lot for review my patch. 

This is interesting. your change of JsFunction ""outputSchema.getField(0).type == DataType.TUPLE"" makes it possible to adding ""()"" to define the return schema as tuple type. I should looks more in the those code when I fix the test failure. +1 for your patch.","25/Feb/13 20:04;cheolsoo;{quote}
Your change of JsFunction ""outputSchema.getField(0).type == DataType.TUPLE"" makes it possible to adding ""()"" to define the return schema as tuple type.
{quote}
No, that's not what I do. The point of that condition is to wrap a JS object with another object if the outputSchema is a tuple. By wrapping outputSchema with a tuple, we add an extra layer in Pig. So we should add the same to the JavaScript object.",25/Feb/13 23:56;sms;+1 for the patch.,26/Feb/13 07:02;cheolsoo;Committed to trunk. Thank you Santhosh for the review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e2e Scripting_5 fails in trunk,PIG-3150,12630177,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dreambird,cheolsoo,cheolsoo,31/Jan/13 17:33,14/Oct/13 16:46,14/Mar/19 03:07,26/Feb/13 07:04,0.12.0,,,,,,,0.12.0,,,,e2e harness,,,0,,,,,,,,,,,,,"To reproduce the error, please do:
{code}
ant -Dhadoopversion=20 -Dharness.old.pig=`pwd` -Dharness.cluster.conf=/etc/hadoop/conf/ -Dharness.cluster.bin=/usr/lib/hadoop/bin/hadoop test-e2e -Dtests.to.run=""-t Scripting_5""
{code}
PIG-3082 introduced a new Front-end exception, and that makes e2e Scripting_5 fail with the following error:
{code}
2013-01-30 15:08:41,340 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1200: Pig script failed to parse:
<file ./out/pigtest/cheolsoo/cheolsoo-1359586527-nightly.conf-Scripting/Scripting_5.pig, line 4, column 4> pig script failed to validate: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 0: Given UDF returns an improper Schema. Schema should only contain one field of a Tuple, Bag, or a single type. Returns: {outm: map[],outt: (name: chararray,age: int,gpa: double),outb: {t: (name: chararray,age: int,gpa: double)}}
{code}
And here is the stack trace:
{code}
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 0: Given UDF returns an improper Schema. Schema should only contain one field of a Tuple, Bag, or a single type. Returns: {outm: map[],outt: (name: chararray,age: int,gpa: double),outb: {t: (name: chararray,age: int,gpa: double)}}
        at org.apache.pig.newplan.logical.expression.UserFuncExpression.getFieldSchema(UserFuncExpression.java:206)
        at org.apache.pig.newplan.logical.optimizer.FieldSchemaResetter.execute(SchemaResetter.java:264)
        at org.apache.pig.newplan.logical.expression.AllSameExpressionVisitor.visit(AllSameExpressionVisitor.java:143)
        at org.apache.pig.newplan.logical.expression.UserFuncExpression.accept(UserFuncExpression.java:88)
        at org.apache.pig.newplan.ReverseDependencyOrderWalker.walk(ReverseDependencyOrderWalker.java:70)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:52)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visitAll(SchemaResetter.java:67)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:122)
        at org.apache.pig.newplan.logical.relational.LOGenerate.accept(LOGenerate.java:240)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:114)
        at org.apache.pig.newplan.logical.relational.LOForEach.accept(LOForEach.java:76)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:52)
        at org.apache.pig.parser.LogicalPlanBuilder.expandAndResetVisitor(LogicalPlanBuilder.java:402)
        ... 22 more
{code}",,,,,,,,,,,,,,,,,,,PIG-3153,,,,,,,,,,,,,25/Feb/13 21:29;dreambird;PIG-3150.patch.txt;https://issues.apache.org/jira/secure/attachment/12570864/PIG-3150.patch.txt,25/Feb/13 20:12;dreambird;PIG-3150.patch.txt;https://issues.apache.org/jira/secure/attachment/12570848/PIG-3150.patch.txt,16/Feb/13 07:20;dreambird;PIG-3150.patch.txt;https://issues.apache.org/jira/secure/attachment/12569657/PIG-3150.patch.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-02-16 07:20:06.49,,,no_permission,,,,,,,,,,,,310673,,,,Tue Feb 26 07:04:12 UTC 2013,,,,,,,0|i1hm67:,311018,,,,,,,,,,16/Feb/13 07:20;dreambird;I fix the test by define the schema of the result in the jython script. The test pass for me,"25/Feb/13 18:35;cheolsoo;[~dreambird], the same here as in PIG-3153. I think we should wrap the outputSchema with a tuple:
{code}
-@outputSchema(""outm:[], outt:(name:chararray, age:int, gpa:double), outb:{t:(name:chararray, age:int, gpa:double)}"")
+@outputSchema(""(outm:[], outt:(name:chararray, age:int, gpa:double), outb:{t:(name:chararray, age:int, gpa:double)})"")
{code}
I can verify that the test case passes with no other changes. Can you please resubmit a patch?

Thanks!","25/Feb/13 20:12;dreambird;[~cheolsoo], thanks a lot for review my patch. Here is the new patch as you suggested. It passes. It changes only two lines.","25/Feb/13 21:17;cheolsoo;[~dreambird], why do you need this change?
{code}
-    return (outm, outt, outb)
+    return ((outm,outt,outb))
{code}
The return object is already a PyTuple, so you don't have to wrap it again. Can you please resubmit a patch?","25/Feb/13 21:29;dreambird;sorry, misunderstood your comments. Here is the new patch.",26/Feb/13 00:28;cheolsoo;+1. I will commit it soon.,26/Feb/13 07:04;cheolsoo;Committed to trunk. Thanks Johnny!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e2e build.xml still refers to jython 2.5.0 jar even though it's replaced by jython standalone 2.5.2 jar,PIG-3149,12629940,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,30/Jan/13 17:06,14/Oct/13 16:46,14/Mar/19 03:07,31/Jan/13 16:59,0.11,,,,,,,0.12.0,,,,e2e harness,,,0,,,,,,,,,,,,,PIG-2665 replaced jython-2.5.0.jar with jython-standalone-2.5.2.jar. But e2e build.xml still refers to the former. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30/Jan/13 17:15;cheolsoo;PIG-3149.patch;https://issues.apache.org/jira/secure/attachment/12567167/PIG-3149.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-30 18:16:13.743,,,no_permission,,,,,,,,,,,,310436,,,,Thu Jan 31 16:59:15 UTC 2013,,,,,,,0|i1hkpr:,310781,,,,,,,,,,30/Jan/13 17:15;cheolsoo;Attached is a patch that parameterized jython.version and jruby.version and load them from ivy/libraries.properties.,"30/Jan/13 18:16;jarcec;+1 (non-binding)

Seems simple and straightforward to me.",31/Jan/13 00:04;rohini;+1,31/Jan/13 00:08;dreambird;+1 (non-binding),31/Jan/13 16:59;cheolsoo;Committed to trunk. Thanks everyone for the review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Spill failing with ""java.lang.RuntimeException: InternalCachedBag.spill() should not be called""",PIG-3147,12629759,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,knoguchi,knoguchi,knoguchi,29/Jan/13 19:34,22/Feb/13 04:54,14/Mar/19 03:07,30/Jan/13 17:44,0.11,,,,,,,0.11,0.12.0,,,impl,,,0,,,,,,,,,,,,,"Tried 0.11 jar with spilling, my job failed to spill with the following stack trace.  Anyone else seeing this?

{noformat}
java.lang.RuntimeException: InternalCachedBag.spill() should not be called
	at org.apache.pig.data.InternalCachedBag.spill(InternalCachedBag.java:167)
	at org.apache.pig.impl.util.SpillableMemoryManager.handleNotification(SpillableMemoryManager.java:243)
	at sun.management.NotificationEmitterSupport.sendNotification(NotificationEmitterSupport.java:138)
	at sun.management.MemoryImpl.createNotification(MemoryImpl.java:171)
	at sun.management.MemoryPoolImpl$PoolSensor.triggerAction(MemoryPoolImpl.java:272)
	at sun.management.Sensor.trigger(Sensor.java:120)
Exception in thread ""Low Memory Detector"" java.lang.InternalError: Error in invoking listener
	at sun.management.NotificationEmitterSupport.sendNotification(NotificationEmitterSupport.java:141)
	at sun.management.MemoryImpl.createNotification(MemoryImpl.java:171)
	at sun.management.MemoryPoolImpl$PoolSensor.triggerAction(MemoryPoolImpl.java:272)
	at sun.management.Sensor.trigger(Sensor.java:120)
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Jan/13 21:41;knoguchi;pig-3147-v01.txt;https://issues.apache.org/jira/secure/attachment/12567055/pig-3147-v01.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-30 17:14:43.287,,,no_permission,,,,,,,,,,,,310255,,,,Wed Jan 30 17:44:25 UTC 2013,,,,,,,0|i1hjlj:,310600,,,,,,,,,,"29/Jan/13 19:37;knoguchi;Dumping a stacktrace when InternalCacheBag is added to the SpillableMemoryManager.spillables,

{noformat}
java.lang.Exception: Stack trace
	at java.lang.Thread.dumpStack(Thread.java:1206)
	at org.apache.pig.impl.util.SpillableMemoryManager.registerSpillable(SpillableMemoryManager.java:296)
	at org.apache.pig.data.DefaultAbstractBag.markSpillableIfNecessary(DefaultAbstractBag.java:101)
	at org.apache.pig.data.InternalCachedBag.addDone(InternalCachedBag.java:131)
	at org.apache.pig.data.InternalCachedBag.iterator(InternalCachedBag.java:159)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:456)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:308)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter.getNext(POFilter.java:95)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:308)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:241)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:308)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSortedDistinct.getNext(POSortedDistinct.java:62)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:432)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.processInputBag(POProject.java:581)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PORelationToExprProject.getNext(PORelationToExprProject.java:107)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:334)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.processInput(POUserFunc.java:228)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:282)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:416)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:348)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:372)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:297)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:465)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:433)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:413)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:257)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1093)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)
{noformat}

Is this from a change in PIG-2923?
","29/Jan/13 21:41;knoguchi;Reading PIG-975, InternalCachedBag should not register with SpillableMemoryManager.

This is just a pure guess, but uploading a patch that takes out markSpillableIfNecessary from InternalCachedBag.java.","30/Jan/13 17:14;dvryaboy;Yeah that's my fault, fix looks right. +1, will commit.","30/Jan/13 17:44;dvryaboy;committed to 0.11 and trunk.

way to catch that before we roll the release! :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parameters in core-site.xml and mapred-site.xml are not correctly substituted,PIG-3145,12629628,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,29/Jan/13 03:10,14/Oct/13 16:45,14/Mar/19 03:07,26/Feb/13 00:32,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"To reproduce the issue, please do the following:

# Parameterize the address of name node in core-site.xml.
{code}
  <property>
    <name>fs.default.name</name>
    <value>hdfs://${foo}:8020</value>
  </property>
{code}
# Set the value of ""foo"" via -D option.
{code}
export PIG_OPTS=""-Dfoo=mr1-0.cheolsoo.com""
{code}
# Pig fails with the following error.
{code}
2013-01-28 18:54:02,786 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://${foo}:8020
2013-01-28 18:54:02,805 [main] ERROR org.apache.pig.Main - ERROR 2999: Unexpected internal error. null
Details at logfile: /home/cheolsoo/pig-cdh/pig_1359428042522.log
{code}
Note that the parameter $\{foo\} in core-site.xml is not expanded. This is because the addresses of name node and job tracker are read directly from core-site.xml instead of reading via Configuration.get().
{code:title=HExecutionEngine.java}
// properties is Java Properties
cluster = properties.getProperty(JOB_TRACKER_LOCATION);
nameNode = properties.getProperty(FILE_SYSTEM_LOCATION);
{code}
Replacing these lines with Configuration.get() fixes the issue.
{code:title=HExecutionEngine.java}
// jc is Hadoop Configuration
cluster = jc.get(JOB_TRACKER_LOCATION);
nameNode = jc.get(FILE_SYSTEM_LOCATION);
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Feb/13 00:27;cheolsoo;PIG-3145-2.patch;https://issues.apache.org/jira/secure/attachment/12570033/PIG-3145-2.patch,30/Jan/13 18:18;cheolsoo;PIG-3145.patch;https://issues.apache.org/jira/secure/attachment/12567177/PIG-3145.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-01-29 03:15:37.701,,,no_permission,,,,,,,,,,,,310124,,,,Tue Feb 26 00:32:04 UTC 2013,,,,,,,0|i1hisf:,310469,,,,,,,,,,"29/Jan/13 03:15;prkommireddi;Hi Cheolsoo, this might also be in a similar category as PIG-3135. Currently Pig expects the site xml files on the classpath - again Configuration is not used.","29/Jan/13 03:26;cheolsoo;Hi Prashant,

Thank you very much for pointing that out.

Indeed, PIg-3135 is in a similar category, but but I think that my problem is a bit different from yours. In my case, *-site.xml files are present in classpath, but system properties (-Dkey=value) that are passed to JVM are not honored. They are different, aren't they?",29/Jan/13 03:30;prkommireddi;Agreed. I am guessing your changes would go into HExecutionEngine primarily. Would be good to get both changes in around the same time.,"30/Jan/13 18:18;cheolsoo;Attached is a patch that includes proposed changes.

Note that I also removed 4 lines of {{properties.setProperty()}} for local mode. This is because they're redundant as those properties are loaded from *-default.xml files.

I ran commit-test with both hadoop 20 and 23 and saw no failures. To be safe, I will run a full suite of unit test as well.","30/Jan/13 20:03;jarcec;+1 (non-binding)

The change seems reasonable to me. 

Jarcec","01/Feb/13 21:11;cheolsoo;I ran full unit test and e2e test and found no regression.

Note that the following test cases are failing in trunk:
* org.apache.pig.test.TestScriptUDF PIG-3153
* org.apache.pig.test.TestPackage PIG-3154
* org.apache.pig.test.TestTypeCheckingValidatorNewLP PIG-3155
* org.apache.pig.data.TestSchemaTuple PIG-3156

However, they are not relevant, and I filed jiras for them.",01/Feb/13 21:58;sms;+1 - the changes look good.,01/Feb/13 22:38;cheolsoo;Thank you Santhosh for the review. Committed to trunk.,10/Feb/13 23:02;cheolsoo;I realized that my patch broke TestHBaseStorage. I will back out and rework it.,"20/Feb/13 00:27;cheolsoo;Attaching a patch that fixes TestHBaseStorage.

In my previous patch, I removed the following lines:
{code}
properties.setProperty(""mapreduce.framework.name"", ""local"");
properties.setProperty(JOB_TRACKER_LOCATION, LOCAL );
properties.setProperty(FILE_SYSTEM_LOCATION, ""file:///"");
properties.setProperty(ALTERNATIVE_FILE_SYSTEM_LOCATION, ""file:///"");
{code}
But that causes a problem when PigServer is constructed in local mode with the mini-cluster-generated configuration because fs.default.name and mapred.job.tracker are set to mini-cluster-values while PigServer runs in local mode.

In my new patch, I not only kept these lines but also moved them before  ""recomputeProperties(jc, properties);"", so fs.default.name and mapred.job.tracker in JobConf will be always overwritten in local mode.

Now in local mode JobConf always returns ""file:///"" and ""local"" regardless whether configuration is passed to the PigServer constructor or not.

I am running full unit test now.","25/Feb/13 23:45;cheolsoo;To be clear, I ran unit test with hadoop23 which excludes TestHBaseStorage. That's why I didn't identify the issue.

Using my new patch, I can verify that TestHBaseStorage passes with hadoop20. ","25/Feb/13 23:49;sms;The patch looks good. Chatted offline with Cheolsoo about how this was missed in the first place and he has explained that in his previous comment. The unit tests were run for Hadoop 23 which excluded TestHBaseStorage. However, it failed for Hadoop 20. This is now fixed.

+1",26/Feb/13 00:32;cheolsoo;Committed to trunk again. Thank you Santhosh for the review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Erroneous map entry alias resolution leading to ""Duplicate schema alias"" errors",PIG-3144,12629602,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,kadeng,kadeng,28/Jan/13 22:59,18/Oct/13 20:28,14/Mar/19 03:07,04/Mar/13 19:37,0.10.1,0.11,,,,,,0.11.1,0.12.0,,,,,,0,,,,,,,,,,,,,"The following code illustrates a problem concerning alias resolution in pig 

The schema of D2 will incorrectly be described as containing two ""age"" fields. And the last step in the following script will lead to a ""Duplicate schema alias"" error message.

I only encountered this bug when using aliases for map fields. 

{code}
DATA = LOAD 'file:///whatever' as (a:map[chararray], b:chararray);

D1 = FOREACH DATA GENERATE a#'name' as name, a#'age' as age, b;

D2 = FOREACH D1 GENERATE name, age, b;

DESCRIBE D2;

{code}

Output:
{code}
D2: {
    age: chararray,
    age: chararray,
    b: chararray
}
{code}

{code}

D3 = FOREACH D2 GENERATE *;

DESCRIBE D3;
{code}

Output:

{code}
<file file:///.../pig-bug-example.pig, line 20, column 16> Duplicate schema alias: age
{code}

This error occurs in this form in Apache Pig version 0.11.0-SNAPSHOT (r6408). A less severe variant of this bug is also present in pig 0.10.1. In 0.10.1, the ""Duplicate schema alias"" error message won't occur, but the schema of D2 (see above) will still have wrong duplicate alias entries.

",,,,,,,,,,,,,,,,,,,PIG-3492,,,,,,,,,,,,,19/Feb/13 13:53;jcoveney;PIG-3144-0.patch;https://issues.apache.org/jira/secure/attachment/12569941/PIG-3144-0.patch,04/Mar/13 19:39;cheolsoo;PIG-3144-1-branch-0.11.patch;https://issues.apache.org/jira/secure/attachment/12571933/PIG-3144-1-branch-0.11.patch,04/Mar/13 15:03;jcoveney;PIG-3144-1.patch;https://issues.apache.org/jira/secure/attachment/12571885/PIG-3144-1.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-02-19 13:53:37.99,,,no_permission,,,,,,,,,,,,310098,,,,Fri Oct 18 20:28:13 UTC 2013,,,,,,,0|i1himn:,310443,,,,,,,,,,"19/Feb/13 13:53;jcoveney;I agree that this is an issue, and it isn't too hard to fix. It uses a method that was developed for PIG-3020.","19/Feb/13 13:54;jcoveney;Thanks for reporting this Kai. I had seen similar issues before, and this should be a generic fix for any case like this in a foreach.",02/Mar/13 11:38;jcoveney;Someone should review this :),"04/Mar/13 06:51;cheolsoo;Hi Jonathan,
Can you update the comment in {{LogicalRelationalOperator.fixDuplicateUids()}}?
{code}
/**
 * In the case of a join it is possible for multiple columns to have been derived from the same
 * column and thus have duplicate UID's. This detects that case and resets the uid.
 * See PIG-3022 and PIG-3093 for more information.
 * @param fss a list of LogicalFieldSchemas to check the uids of
 */
{code}
# This is not a join-specific issue, so ""in the case of a join"" should be removed.
# PIG-3022 should be replaced with PIG-3020.

Otherwise, the patch looks good to me. I will run unit tests.","04/Mar/13 15:03;jcoveney;Updated. Let me know how the tests come back. Thanks, Cheolsoo!","04/Mar/13 19:16;cheolsoo;+1.

The unit tests pass. I will commit it soon.","04/Mar/13 19:37;cheolsoo;Committed to trunk and 0.11.

Note that I replaced @'s with relation names from the new test case in 0.11 because it isn't supported in 0.11.",04/Mar/13 19:39;cheolsoo;Attaching the 0.11 patch for the record.,"18/Oct/13 20:14;knoguchi;FYI, I'm trying to revert the change from this jira at PIG-3492.",18/Oct/13 20:28;cheolsoo;Sounds good. Thanks [~knoguchi] for cleaning up the mess!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document PigProgressNotificationListener configs,PIG-3140,12629485,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,billgraham,billgraham,billgraham,28/Jan/13 07:36,22/Feb/13 04:54,14/Mar/19 03:07,29/Jan/13 07:32,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,Add docs to describe what PPNL is and how to configure it.,,,,,,,,,,,PIG-2756,,,,,,,,,,,,,,,,,,,,,28/Jan/13 07:38;billgraham;PIG-3140_1.patch;https://issues.apache.org/jira/secure/attachment/12566725/PIG-3140_1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-28 15:57:08.185,,,no_permission,,,,,,,,,,,,309981,,,,Tue Jan 29 07:34:56 UTC 2013,,,,,,,0|i1hhwn:,310326,,,,,,,,,,"28/Jan/13 07:39;billgraham;Adding this to the Testing and Diagnostics page below the Pig Statistics section, since it seemed like the best place. Let me know if anyone has a better suggestion.

http://pig.apache.org/docs/r0.10.0/test.html#pig-statistics",28/Jan/13 15:57;jarcec;+1 (non-binding),28/Jan/13 23:34;dvryaboy;+1,29/Jan/13 00:32;julienledem;+1,29/Jan/13 07:34;billgraham;Committed to trunk an 0.11 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document reducer estimation,PIG-3139,12629484,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,billgraham,billgraham,billgraham,28/Jan/13 06:55,22/Feb/13 04:54,14/Mar/19 03:07,29/Jan/13 07:32,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,Add docs to describe how default reducer estimation algo works and how to override it.,,,,,,,,,,,PIG-2756,,,,,,,,,,,,,,,,,,,,,28/Jan/13 07:08;billgraham;PIG-3139_1.patch;https://issues.apache.org/jira/secure/attachment/12566724/PIG-3139_1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-28 23:33:52.973,,,no_permission,,,,,,,,,,,,309980,,,,Tue Jan 29 07:34:36 UTC 2013,,,Patch Available,,,,0|i1hhwf:,310325,,,,,,,,,,"28/Jan/13 07:09;billgraham;Adding this to the performance page under the Memory Management section, since it seemed like the best place. Let me know if anyone has a better suggestion.

http://pig.apache.org/docs/r0.10.0/perf.html#memory-management",28/Jan/13 23:33;dvryaboy;+1,29/Jan/13 00:32;julienledem;+1,29/Jan/13 07:34;billgraham;Committed to trunk and 0.11 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix Piggybank test to not using /tmp dir,PIG-3137,12629367,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dreambird,dreambird,dreambird,26/Jan/13 01:40,14/Oct/13 16:46,14/Mar/19 03:07,05/Feb/13 01:22,0.11,,,,,,,0.12.0,,,,piggybank,,,0,,,,,,,,,,,,,"right now several Piggybank tests create directory under /tmp to store test data, the test could fail because user doesn't have permission to create directory under /tmp. It is better to move test data dir under build dir to avoid this problem.

I will submit a patch soon.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Feb/13 21:53;dreambird;PIG-3137.nows.patch.txt;https://issues.apache.org/jira/secure/attachment/12567646/PIG-3137.nows.patch.txt,01/Feb/13 21:53;dreambird;PIG-3137.patch.txt;https://issues.apache.org/jira/secure/attachment/12567647/PIG-3137.patch.txt,30/Jan/13 01:48;dreambird;PIG-3137.patch.txt;https://issues.apache.org/jira/secure/attachment/12567090/PIG-3137.patch.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-01-26 15:40:08.586,,,no_permission,,,,,,,,,,,,309763,,,,Tue Feb 05 01:22:49 UTC 2013,,,,,,,0|i1g9wf:,303197,,,,,,,,,,"26/Jan/13 15:40;jarcec;Hi Jimmy,
I'm currently working on PIG-2591, that is trying to fix buid-in usage of ""/tmp"" for all tests.

Jarcec","30/Jan/13 01:48;dreambird;this patch changes TestDBStorage and TestAvroStorage not use /tmp to store the testing data.

It uses the FileLocalizer to gain the temp dir from PigContext. The good side about it is user will have fully permission on the dir used by PigServer, so test won't fail in case user doesn't have enough permission on /tmp. For example, the TestAvroStorage right not using $PIG_DIR/contrib/piggybank/java/file:/tmp/temp1371024088/tmp1070213954/TestAvroStorage/ instead of /tmp/TestAvroStorage to store testing data.

The fix for TestAvroStorage is straightforward, the fix for TestDBStorage is a little bit different, since hsqldb doesn't work in minicluster hdfs, we need to create another local PigServer to host the hsqldb database.

[~cheolsoo], what do you think about it? Thanks.","01/Feb/13 14:39;cheolsoo;[~dreambird], thank you very much for the patch. I have two suggestions:
* FileLocalizer.getTemporaryPath() is for generating random paths in Hadoop cluster (either it's local, mini cluster, or real cluster). So it makes sense to use FileLocalizer in TestAvroStorage where we need temp paths for test outputs. But in TestDBStorage, we need a temp dir for Hsqldb, so I don't think we want to use FileLocalizer there. Using a temporary path under build (e.g. contrib/piggybank/java/build/blah) would be better.
* You can control the root dir of FileLocalizer.getTemporaryPath() using the pig.temp.dir property. It would be nice if it's set to somewhere under the build directory, so temporary dirs can be deleted by ant clean.

Let me know what you think. Thanks!

",01/Feb/13 14:41;cheolsoo;Canceling patch until it gets updated.,"01/Feb/13 21:59;dreambird;[~cheolsoo], thanks for your comments, new patch just posted
1. change contrib/piggybank/java/build.xml to create log in user.dir instead of /tmp
2. not using FileLocalizer but using contrib/piggybank/java/build/test/ to store hsqldb in TestDBStorage
3. use pig.temp.dir to update PigContext's temp dir to contrib/piggybank/java/build/test/tmp/  before using FileLocalizer in TestAvroStorage",05/Feb/13 01:12;cheolsoo;+1. Looks good. I will commit it after running test.,05/Feb/13 01:22;cheolsoo;Committed to trunk. Thanks Johnny!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HExecutionEngine should look for resources in user passed Properties,PIG-3135,12628834,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,prkommireddi,prkommireddi,prkommireddi,23/Jan/13 01:15,14/Oct/13 16:46,14/Mar/19 03:07,24/Feb/13 03:38,0.10.0,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"Looking at this snippet:

{code}
    private void init(Properties properties) throws ExecException {
          .
          .
          .

            // Check existence of hadoop-site.xml or core-site.xml
            Configuration testConf = new Configuration();
            ClassLoader cl = testConf.getClassLoader();
            URL hadoop_site = cl.getResource( HADOOP_SITE );
            URL core_site = cl.getResource( CORE_SITE );
           
            if( hadoop_site == null && core_site == null ) {
                throw new ExecException(""Cannot find hadoop configurations in classpath (neither hadoop-site.xml nor core-site.xml was found in the classpath)."" +
                        "" If you plan to use local mode, please put -x local option in command line"",
                        4010);
            }
{code}

This assumes the resources (*-site.xml) are set on the classpath, but this will not always be the case when run with Pig's Java APIs. One could want to programatically set the resources and the code here should additionally check if they are available in there. 

Example: When a Configuration object is created and resources are added before passing it on to Pig.

{code}
Configuration conf = new Configuration(false);
conf.addResource(""foo/core-site.xml"");
conf.addResource(""bar/hadoop-site.xml"");

PigServer pServer = new PigServer(ExecType.MAPREDUCE, conf);
{code}

The above conf is not used right now to obtain resources.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Jan/13 02:04;prkommireddi;PIG-3135.patch;https://issues.apache.org/jira/secure/attachment/12566436/PIG-3135.patch,04/Feb/13 05:33;prkommireddi;PIG-3135_1.patch;https://issues.apache.org/jira/secure/attachment/12567801/PIG-3135_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-01-30 20:00:24.165,,,no_permission,,,,,,,,,,,,308313,,,,Sun Feb 24 21:47:58 UTC 2013,,,Patch Available,,,,0|i1b467:,272974,,,,,,,,,,"24/Jan/13 00:26;prkommireddi;To get around the current limitation of depending on *site.xml files being set on classpath, we could add a property ""pig.use.override.configs"" to be able to provide your own configs and have JobConf be built from that. 

Currently:
{code}
jc = new JobConf();
{code}

Proposal:

1. If ""pig.use.override.configs"" is present, generate JobConf using properties
{code}
jc = new JobConf(ConfigurationUtil.toConfiguration(properties));
{code}

This change would be backward compatible, and those who wish to bypass classpath limitation can do so by setting the override property.

Thoughts?
","25/Jan/13 02:04;prkommireddi;Would appreciate if someone can take a look. I have added a new Test class for HExecutionEngine as these couple tests don't fit in any other place, and it makes sense to have its own Test for it.","30/Jan/13 20:00;cheolsoo;[~prkommireddi] Overall looks good to me. I have 3 questions.
* Can we call the property ""pig.use.overriden.hadoop.configs""? What we're overriding here are basically Hadoop confs, so I think that a more specific name is better. Do you agree?
* On a related note, can you update the following comment in testJobConfGeneration?
From:
{code}
// This should fail as pig expects classpath to be set
{code}
To something like:
{code}
// This should fail as pig expects Hadoop configs are present in classpath.
{code}
* Can you add this new property to conf/pig.properties with some explanation, so people can know about it? It would be nice if we could mention that this is a Mr-mode-specific property too. Please let me know if you have a better suggestion regarding how to document this.

Thanks!","30/Jan/13 22:54;prkommireddi;Thanks for the quick review. Your comments make sense, I will make the above changes and upload a new patch.","04/Feb/13 05:33;prkommireddi;Hi Cheolsoo, find attached a revised patch as per our previous discussion.",04/Feb/13 06:27;cheolsoo;+1. I will commit it after running tests.,04/Feb/13 23:10;cheolsoo;Committed to trunk. Thanks Prashant!,"04/Feb/13 23:18;prkommireddi;Thanks for the review and commit, Cheolsoo.","10/Feb/13 23:01;cheolsoo;[~prkommireddi], very sorry for reopening the jira, but this makes unit test fail.

Note that {{ant clean test -Dtestcase=TestHExecutionEngine}} always passes. But when you run the test as part of unit test (i.e. ant clean test), it fails.

To reproduce, please do the following:
# Run a unit test that uses mini-cluster (e.g. TestAccumulator).
{code}
ant clean test -Dtestcase=TestAccumulator
{code}
# Run TestHExecutionEngine *without ant clean*.
{code}
ant test -Dtestcase=TestHExecutionEngine
{code}
This will fail with the following errors in logs:
{code}
java.net.ConnectException: Call to localhost.localdomain/127.0.0.1:59119 failed on connection exception: java.net.ConnectException: Connection refused
{code}
In fact, the same issue was discussed in [PIG-2769|https://issues.apache.org/jira/browse/PIG-2769?focusedCommentId=13541974&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13541974] before. What I don't fully understand is why this happens even though you're doing [what Rohini suggests|https://issues.apache.org/jira/browse/PIG-2769?focusedCommentId=13547105&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13547105].

For the time being, I will back out the patch to stabilize the build. As soon as you update the patch, I will test it again. Does this make sense?","11/Feb/13 00:07;prkommireddi;Sure, I will take a look.","20/Feb/13 01:05;prkommireddi;Hey [~cheolsoo], I think the issue is with the fact that MiniCluster.buildCluster() generates a hadoop-site.xml file under build/classes. However, MiniCluster.shutDown() does not delete this file. Ideally, hadoop-site.xml should be deleted on cluster shutdown. 

With respect to TestHExecutionEngine, the issue is with the fact a previous test run that uses mini-cluster generates hadoop-site.xml and does not delete it. At the time TestHExecutionEngine runs this file is present on the classpath but mini-dfs and mini-mr were shutdown at the completion of previous test. pigContext.connect() in TestHExecutionEngine tries to establish a connection with mini-cluster which is no longer up.

The fix would be to:

1. Delete hadoop-site-xml on mini-cluster shutdown (to be done in another JIRA)
2. Check if this patch stabilizes (and nothing else breaks)

Does that make sense?","20/Feb/13 17:48;cheolsoo;[~prkommireddi], I totally agree with you. Can we fix PIG-3200 first then?","24/Feb/13 03:38;cheolsoo;+1 again.

Verified that PIG-3200 fixes the test failure, and committed the patch to trunk again.

Thanks Prashant!","24/Feb/13 20:18;prkommireddi;Hi [~cheolsoo], I don't see the new tests ""test/org/apache/pig/test/TestHExecutionEngine.java"" making it in with this commit. Can you please take a look when you get a chance?

{code}
$ svn diff --summarize -c r1449436
M       conf/pig.properties
M       CHANGES.txt
M       src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java
{code}","24/Feb/13 21:40;cheolsoo;My bad. I forgot to add it. It is committed now:
http://svn.apache.org/viewvc?view=revision&revision=1449553

Thanks!",24/Feb/13 21:47;prkommireddi;Thanks Cheolsoo.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 NPE when illustrating a relation with HCatLoader,PIG-3132,12628814,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,22/Jan/13 22:50,02/Apr/13 15:54,14/Mar/19 03:07,18/Mar/13 17:27,,,,,,,,0.11.1,,,,,,,0,,,,,,,,,,,,,"Get NPE exception when illustrate a relation with HCatLoader:
{code}
A = LOAD 'studenttab10k' USING org.apache.hcatalog.pig.HCatLoader();
illustrate A;
{code}
Exception:
{code}
java.lang.NullPointerException
        at org.apache.hcatalog.pig.PigHCatUtil.transformToTuple(PigHCatUtil.java:274)
        at org.apache.hcatalog.pig.PigHCatUtil.transformToTuple(PigHCatUtil.java:238)
        at org.apache.hcatalog.pig.HCatBaseLoader.getNext(HCatBaseLoader.java:61)
        at org.apache.pig.impl.io.ReadToEndLoader.getNextHelper(ReadToEndLoader.java:210)
        at org.apache.pig.impl.io.ReadToEndLoader.getNext(ReadToEndLoader.java:190)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad.getNext(POLoad.java:129)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:267)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:262)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
        at org.apache.pig.pen.LocalMapReduceSimulator.launchPig(LocalMapReduceSimulator.java:194)
        at org.apache.pig.pen.ExampleGenerator.getData(ExampleGenerator.java:257)
        at org.apache.pig.pen.ExampleGenerator.readBaseData(ExampleGenerator.java:222)
        at org.apache.pig.pen.ExampleGenerator.getExamples(ExampleGenerator.java:154)
        at org.apache.pig.PigServer.getExamples(PigServer.java:1245)
        at org.apache.pig.tools.grunt.GruntParser.processIllustrate(GruntParser.java:698)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.Illustrate(PigScriptParser.java:591)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:306)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:188)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:164)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:67)
{code}

HCatalog side is tracked with HCATALOG-163.",,,,,,,,,,,,,,,,,,,HCATALOG-163,,,,,,,,,,,,,22/Jan/13 22:53;daijy;PIG-3132-1.patch;https://issues.apache.org/jira/secure/attachment/12566037/PIG-3132-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-18 08:04:27.46,,,no_permission,,,,,,,,,,,,308292,Reviewed,,,Mon Mar 18 17:27:57 UTC 2013,,,,,,,0|i1b1jr:,272549,,,,,,,,,,"22/Jan/13 22:54;daijy;There are multiple root causes:
1. There is no distinguish between backend and frontend in illustrate
2. ReadToEndLoader (invoked by POLoader, only used in illustrate) does not invoke setLocation
3. ReadToEndLoader does not have signature

There is also a fix in HCat side tracked by HCATALOG-163.",18/Mar/13 03:58;daijy;All unit tests pass.,18/Mar/13 08:04;dvryaboy;+1,18/Mar/13 17:27;daijy;Patch committed to 0.11 branch and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix zebra compilation error,PIG-3125,12628593,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,21/Jan/13 19:07,22/Feb/13 04:53,14/Mar/19 03:07,21/Jan/13 20:50,0.11,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,Zebra doesn't compile in trunk/branch-11.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Jan/13 19:10;cheolsoo;PIG-3125.patch;https://issues.apache.org/jira/secure/attachment/12565827/PIG-3125.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-21 19:22:17.437,,,no_permission,,,,,,,,,,,,307044,,,,Mon Jan 21 20:50:11 UTC 2013,,,,,,,0|i1a43r:,267131,,,,,,,,,,"21/Jan/13 19:10;cheolsoo;Attached is a patch that fixes the compilation.

I see several unit test failures, but they can be resolved in a separate jira.","21/Jan/13 19:22;jarcec;+1 (non-binding)

Seems to be solving the issue on my side.",21/Jan/13 20:21;daijy;+1,"21/Jan/13 20:50;cheolsoo;Thank you for the review, Daniel!

Committed to trunk and branch-0.11.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Operators should not implicitly become reserved keywords,PIG-3122,12627928,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,jcoveney,jcoveney,16/Jan/13 21:12,14/Oct/13 16:46,14/Mar/19 03:07,16/Apr/13 21:58,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"As a byproduct of how ANTLR lexes things, whenever we introduce a new operator (RANK, CUBE, and any special keyword really) we are implicitly introducing a reserved word that can't be used for relations, columns, etc (unless give to us by the framework, as in the case of group).

The following, for example, fails:

{code}
a = load 'foo' as (x:int);
a = foreach a generate x as rank;
{code}

I'll include a patch to fix this essentially by whitelisting tokens. I currently just whitelist cube, rank, and group. We can add more as people want them? Can anyone think of reasonable ones they'd like to add?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Jan/13 21:14;jcoveney;PIG-3122-0.patch;https://issues.apache.org/jira/secure/attachment/12565184/PIG-3122-0.patch,18/Feb/13 16:25;jcoveney;PIG-3122-1.patch;https://issues.apache.org/jira/secure/attachment/12569828/PIG-3122-1.patch,12/Apr/13 22:57;cheolsoo;PIG-3122-2.patch;https://issues.apache.org/jira/secure/attachment/12578535/PIG-3122-2.patch,12/Apr/13 23:48;cheolsoo;PIG-3122-3.patch;https://issues.apache.org/jira/secure/attachment/12578552/PIG-3122-3.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-02-04 17:46:58.582,,,no_permission,,,,,,,,,,,,304776,,,,Tue Apr 16 21:58:10 UTC 2013,,,,,,,0|i17vwf:,254137,,,,,,,,,,04/Feb/13 17:46;alangates;Reviewing this.,"04/Feb/13 17:55;alangates;Sorry Jonathan, but I think the checkin of the big decimal stuff totally broke this patch.  It fails all over the place in QueryParser.g and I'm not sure I'm putting it back together correctly.  Marking this as open pending a new patch being uploaded.","18/Feb/13 16:25;jcoveney;Alan, thanks for the heads up. I've updated the patch, would love it if you can take a look again.","12/Apr/13 21:43;cheolsoo;While working on PIG-3268 and PIG-3269, I didn't want to introduce backward incompatibilities due to new keywords. Conveniently, this patch addresses my problem!

I updated the patch as follows:
* Rebased to trunk.
* Resolved ambiguity between {{col_ref_without_identifier}} and {{identifier_plus}}.

Thanks!","12/Apr/13 23:48;cheolsoo;Actually, we can't whitelist every special keywords (e.g. group, etc) because it introduces ambiguity to the grammar. So I am removing {{group}} from the whitelist.

However, we can certainly whitelist whatever is possible. For example, in PIG-3268 and PIG-3269, I can whitelist IN, WHEN, THEN, and ELSE but not CASE.",13/Apr/13 18:21;cheolsoo;All unit tests pass.,"16/Apr/13 20:59;aniket486;+1 
I checked that all the places in query parser that require this change are covered.",16/Apr/13 21:58;cheolsoo;Committed to trunk. Thanks Jonathan and Aniket!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
setStoreFuncUDFContextSignature called with null signature,PIG-3120,12627467,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,jadler,jadler,jadler,14/Jan/13 18:05,13/Jul/14 01:13,14/Mar/19 03:07,16/Jan/13 18:44,0.12.0,,,,,,,0.12.0,,,,impl,,,0,,,,,,,,,,,,,"I'm currently working on PIG-3015 and am having trouble passing the UDFContextSignature to the store func. It looks like the store func on the head end is being set to a non-null value, but a null value is being passed to setStoreFuncUDFContextSignature on the back end. I'm opening this ticket to track this issue; I'll follow up with a reproducible test case when I have a clean one.

I suspect this problem occurs when running on a real cluster, but may not occur in the standard unit tests.",,,,,,,,,,,,,,PIG-2689,,,,,,,,,,,,,,,,,,15/Jan/13 00:50;jadler;PIG-3120.patch;https://issues.apache.org/jira/secure/attachment/12564826/PIG-3120.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-15 22:12:30.966,,,no_permission,,,,,,,,,,,,304251,,,,Wed Jan 16 18:44:02 UTC 2013,,,,,,,0|i17krz:,252335,,,,,,,,,,"15/Jan/13 00:46;jadler;OK, tracked down the issue. It looks like the UDFContextSignature is not getting propagated if there is a LIMIT statement in the pig code.

Very specifically, in org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.LimitAdjuster.adjust, it looks like Pig was creating a new POStore object but not copying the signature. Here is the offending code:

{code}
            // this is line 132...
            POStore st = new POStore(new OperatorKey(scope,nig.getNextNodeId(scope)));
            st.setSFile(oldSpec);
            st.setIsTmpStore(oldIsTmpStore);
            st.setSchema(((POStore)mpLeaf).getSchema());
            
            limitAdjustMROp.reducePlan.addAsLeaf(st);
{code}

This is easily fixable by inserting this statement at line 137:

{code}
            st.setSignature(((POStore)mpLeaf).getSignature());
{code}
I'll follow up with a path for this issue.",15/Jan/13 00:50;jadler;This patch resolves an issue with UDF StoreFunc signatures when using LIMIT statements,"15/Jan/13 22:12;cheolsoo;Hi Joe, 

Thanks for the patch. I am running unit test and e2e test. I will commit it once I verify that all tests pass.",16/Jan/13 17:35;cheolsoo;+1. I will commit this soon.,16/Jan/13 18:44;cheolsoo;Committed to trunk. Thanks Joe!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig e2e tests fail on RHEL6 because of incorrect sort command options,PIG-3116,12626283,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,vikram.dixit,vikram.dixit,vikram.dixit,07/Jan/13 04:04,22/Feb/13 04:53,14/Mar/19 03:07,07/Jan/13 18:03,0.10.0,,,,,,,0.11,,,,e2e harness,,06/Jan/13 00:00,0,,,,,,,,,,,,,the sort command options currently used in pig are non-compliant with posix standard and hence fail on rhel-6.,,,,,,,,,,,,,,,PIG-2782,PIG-3045,,,,,,,,,,,,,,,,07/Jan/13 04:05;vikram.dixit;PIG-3116.patch;https://issues.apache.org/jira/secure/attachment/12563516/PIG-3116.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-07 17:50:08.61,,,no_permission,,,,,,,,,,,,302868,,,,Mon Jan 07 18:01:55 UTC 2013,,,,,,,0|i1763j:,249956,,,,,,,,,,07/Jan/13 17:50;rohini;PIG-2782 and PIG-3045 has already fixed them in 0.11. What branch is this patch against?,"07/Jan/13 17:59;vikram.dixit;I generated this patch off branch-0.10 and it looks like the those JIRAs you mention fix it in branch-0.11 and trunk. Closing this issue.

Thanks
Vikram.",07/Jan/13 18:01;vikram.dixit;Duplicate of issues mentioned by Rohini.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Distinct Build-in Function Doesn't Handle Null Bags,PIG-3115,12626281,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,njw45,njw45,njw45,07/Jan/13 03:21,14/Oct/13 16:46,14/Mar/19 03:07,09/Jan/13 05:31,0.10.0,,,,,,,0.12.0,,,,impl,,,0,,,,,,,,,,,,,"Calling Distinct(NULL) throws NPEs - it should handle this more gracefully. The attached patch makes Distinct(NULL) == {}, although it could return NULL.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Jan/13 03:22;njw45;PIG-3115.1.patch;https://issues.apache.org/jira/secure/attachment/12563514/PIG-3115.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-09 02:20:38.673,,,no_permission,,,,,,,,,,,,302866,Reviewed,,,Wed Jan 09 05:31:08 UTC 2013,,,Patch Available,,,,0|i1762v:,249953,,,,,,,,,,"09/Jan/13 02:20;daijy;Looks good. Only have one question: in getDistinctFromNestedBags, do we need null check if Initial always generate legitimate bag? Is it just for bullet proof?","09/Jan/13 03:37;njw45;Yes, it's just defensive programming; I hit one of the NullPointerExceptions when writing a Pig script, so when I wrote the unit test to track down where it came from I added tests for all three static classes of Distinct...and so came across it then.",09/Jan/13 05:31;daijy;Patch committed to trunk. Thanks Nick!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicated macro name error when using pigunit,PIG-3114,12625959,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,chetan.nadgire,chetan.nadgire,04/Jan/13 01:49,23/Oct/15 20:22,14/Mar/19 03:07,27/Sep/13 20:40,0.11,,,,,,,0.12.0,,,,parser,,,2,,,,,,,,,,,,,"I'm using PigUnit to test a pig script within which a macro is defined.
Pig runs fine on cluster but getting parsing error with pigunit.
So I tried very basic pig script with macro and getting similar error.

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1000: Error during parsing. <line 9> null. Reason: Duplicated macro name 'my_macro_1'
	at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1607)
	at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1546)
	at org.apache.pig.PigServer.registerQuery(PigServer.java:516)
	at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:988)
	at org.apache.pig.pigunit.pig.GruntParser.processPig(GruntParser.java:61)
	at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:412)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:194)
	at org.apache.pig.pigunit.pig.PigServer.registerScript(PigServer.java:56)
	at org.apache.pig.pigunit.PigTest.registerScript(PigTest.java:160)
	at org.apache.pig.pigunit.PigTest.assertOutput(PigTest.java:231)
	at org.apache.pig.pigunit.PigTest.assertOutput(PigTest.java:261)
	at FirstPigTest.MyPigTest.testTop2Queries(MyPigTest.java:32)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:176)
	at junit.framework.TestCase.runBare(TestCase.java:141)
	at junit.framework.TestResult$1.protect(TestResult.java:122)
	at junit.framework.TestResult.runProtected(TestResult.java:142)
	at junit.framework.TestResult.run(TestResult.java:125)
	at junit.framework.TestCase.run(TestCase.java:129)
	at junit.framework.TestSuite.runTest(TestSuite.java:255)
	at junit.framework.TestSuite.run(TestSuite.java:250)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: Failed to parse: <line 9> null. Reason: Duplicated macro name 'my_macro_1'
	at org.apache.pig.parser.QueryParserDriver.makeMacroDef(QueryParserDriver.java:406)
	at org.apache.pig.parser.QueryParserDriver.expandMacro(QueryParserDriver.java:277)
	at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:178)
	at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1599)
	... 30 more

 
Pig script which is failing :


{code:title=test.pig|borderStyle=solid}
DEFINE my_macro_1 (QUERY, A) RETURNS C {
    $C = ORDER $QUERY BY total DESC, $A;
} ;

data =  LOAD 'input' AS (query:CHARARRAY);

queries_group = GROUP data BY query;

queries_count = FOREACH queries_group GENERATE group AS query, COUNT(data) AS total;

queries_ordered = my_macro_1(queries_count, query);

queries_limit = LIMIT queries_ordered 2;

STORE queries_limit INTO 'output';
{code}


If I remove macro pigunit works fine. Even just defining macro without using it results in parsing error.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Sep/13 23:39;daijy;PIG-3114-2.patch;https://issues.apache.org/jira/secure/attachment/12605354/PIG-3114-2.patch,27/Sep/13 00:59;daijy;PIG-3114-3.patch;https://issues.apache.org/jira/secure/attachment/12605370/PIG-3114-3.patch,10/Jan/13 02:23;chetan.nadgire;PIG-3114.patch;https://issues.apache.org/jira/secure/attachment/12564092/PIG-3114.patch,10/Jan/13 02:11;chetan.nadgire;PIG-3114.patch;https://issues.apache.org/jira/secure/attachment/12564090/PIG-3114.patch,25/Jul/13 00:01;sajidraza;PatchedPigTest.java;https://issues.apache.org/jira/secure/attachment/12594068/PatchedPigTest.java,25/Feb/15 01:48;jserrano;bug_pig.tbz2;https://issues.apache.org/jira/secure/attachment/12700643/bug_pig.tbz2,,,,,,6.0,,,,,,,,,,,,,,,,,,,2013-01-09 02:05:15.54,,,no_permission,,,,,,,,,,,,302541,Reviewed,,,Fri Oct 23 20:22:01 UTC 2015,,,,,,,0|i173uf:,249590,Clearing cache before calling PigServer::registerScript() to fix macro redefinition error.,,,,,,,,,09/Jan/13 02:05;daijy;Runs good for me with 0.10.1. Which version are you using?,"09/Jan/13 05:32;chetan.nadgire;Hi Daniel,

I am building pig.jar and pigunit.jar from branch-0.11. When I tried branch-0.10 getting different error:

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1000: Error during parsing. null
	at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1606)
	at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1549)
	at org.apache.pig.PigServer.registerQuery(PigServer.java:549)
	at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:968)
	at org.apache.pig.pigunit.pig.GruntParser.processPig(GruntParser.java:61)
	at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:386)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:190)
	at org.apache.pig.pigunit.pig.PigServer.registerScript(PigServer.java:53)
	at org.apache.pig.pigunit.PigTest.registerScript(PigTest.java:160)
	at org.apache.pig.pigunit.PigTest.assertOutput(PigTest.java:251)
	at FirstPigTest.MyPigTest.testTop2Queries(MyPigTest.java:32)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:168)
	at junit.framework.TestCase.runBare(TestCase.java:134)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:124)
	at junit.framework.TestSuite.runTest(TestSuite.java:232)
	at junit.framework.TestSuite.run(TestSuite.java:227)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:79)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: java.lang.NullPointerException
	at java.io.File.<init>(File.java:222)
	at org.apache.pig.parser.QueryParserUtils.getFileFromImportSearchPath(QueryParserUtils.java:205)
	at org.apache.pig.parser.QueryParserDriver.getMacroFile(QueryParserDriver.java:352)
	at org.apache.pig.parser.QueryParserDriver.makeMacroDef(QueryParserDriver.java:411)
	at org.apache.pig.parser.QueryParserDriver.expandMacro(QueryParserDriver.java:270)
	at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:171)
	at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1598)
	... 29 more 

Thanks,
Chetan",09/Jan/13 22:35;daijy;You are right. I can see it with pigunit. It runs fine with command line.,"10/Jan/13 01:32;chetan.nadgire;Here's my investigation:

I am using assertOutput(aliasInput, input, alias, expected) to verify output. 

1. 'PigTest::assertOutput(String aliasInput, String[] input, String alias, String[] expected)' calls  'PigTest::registerScript()' to parse input query and to substitute aliases with input/output relations and load/store commands and generate query to be submitted to pigserver.

It copies input (from pigtest) to destination file and calls 'PigTest::assertOutput(String alias, String[] expected)'

2. PigTest::assertOutput(String alias, String[] expected) again calls PigTest::registerScript() and submits generated query to pigserver again. Note that PigTest uses same instance on PigServer

It then appends newline to expected output and calls 'assertEquals(StringUtils.join(expected, ""\n""), StringUtils.join(getAlias(alias), ""\n""));'

3. PigTest::getAlias() also calls registerScript()


So PigServer::registerScript() it called 3 times with modification to same query. PigServer::Graph::scriptCache is used to store query and since same PigServer is used for three calls, same query gets appended in scriptCache.
When this query is parse it errors out due to multiple definitions of macro.

Here's flow: 

PigTest::assertOutput(String aliasInput, String[] input, String alias, String[] expected)
 -> PigTest::registerScript()
    ->  PigServer::registerScript()

query submitted to pigserver (scriptCache):


DEFINE my_macro_1 (QUERY, A) RETURNS C {
    $C = ORDER $QUERY BY total DESC, $A;
} 

data =  LOAD 'input' AS (query:CHARARRAY);
queries_group = GROUP data BY query;
queries_count = FOREACH queries_group GENERATE group AS query, COUNT(data) AS total;
queries_ordered = my_macro_1(queries_count, query);
queries_limit = LIMIT queries_ordered 2;
STORE queries_limit INTO 'output';

----------------------------------------------------------------------------------------

PigTest::assertOutput(String alias, String[] expected)
 -> PigTest::registerScript()
    ->  PigServer::registerScript()

query submitted to pigserver (scriptCache):

DEFINE my_macro_1 (QUERY, A) RETURNS C {
    $C = ORDER $QUERY BY total DESC, $A;
} 

data =  LOAD 'input' AS (query:CHARARRAY);
queries_group = GROUP data BY query;
queries_count = FOREACH queries_group GENERATE group AS query, COUNT(data) AS total;
queries_ordered = my_macro_1(queries_count, query);
queries_limit = LIMIT queries_ordered 2;
STORE queries_limit INTO 'output';
DEFINE my_macro_1 (QUERY, A) RETURNS C {
    $C = ORDER $QUERY BY total DESC, $A;
} 

data =  LOAD 'input' AS (query:CHARARRAY);
queries_group = GROUP data BY query;
queries_count = FOREACH queries_group GENERATE group AS query, COUNT(data) AS total;
queries_ordered = my_macro_1(queries_count, query);
queries_limit = LIMIT queries_ordered 2;
STORE queries_limit INTO 'output';

----------------------------------------------------------------------------------------

PigTest::assertEquals(String expected, String current)
 -> PigTest::registerScript()
    ->  PigServer::registerScript()

query submitted to pigserver (scriptCache):

DEFINE my_macro_1 (QUERY, A) RETURNS C {
    $C = ORDER $QUERY BY total DESC, $A;
} 

data =  LOAD 'input' AS (query:CHARARRAY);
queries_group = GROUP data BY query;
queries_count = FOREACH queries_group GENERATE group AS query, COUNT(data) AS total;
queries_ordered = my_macro_1(queries_count, query);
queries_limit = LIMIT queries_ordered 2;
STORE queries_limit INTO 'output';
DEFINE my_macro_1 (QUERY, A) RETURNS C {
    $C = ORDER $QUERY BY total DESC, $A;
} 

data =  LOAD 'input' AS (query:CHARARRAY);
queries_group = GROUP data BY query;
queries_count = FOREACH queries_group GENERATE group AS query, COUNT(data) AS total;
queries_ordered = my_macro_1(queries_count, query);
queries_limit = LIMIT queries_ordered 2;
STORE queries_limit INTO 'output';
DEFINE my_macro_1 (QUERY, A) RETURNS C {
    $C = ORDER $QUERY BY total DESC, $A;
} 

data =  LOAD 'input' AS (query:CHARARRAY);
queries_group = GROUP data BY query;
queries_count = FOREACH queries_group GENERATE group AS query, COUNT(data) AS total;
queries_ordered = my_macro_1(queries_count, query);
queries_limit = LIMIT queries_ordered 2;
STORE queries_limit INTO 'output';
","10/Jan/13 02:11;chetan.nadgire;Added method in PigServer to clear scripCache.
Clearing cache before calling PigServer::registerScript() will fix macro redefinition error.","15/Jan/13 21:46;chetan.nadgire;PigServer::registerScript() it called multiple times by PigTest for same query. PigServer::Graph::scriptCache is used to store query and since same PigServer is used for all calls, same query gets appended in scriptCache.
When this query is parse it errors out due to multiple definitions of macro.
Clearing cache before calling PigServer::registerScript() will fix macro redefinition error.",22/Jan/13 23:06;daijy;Looks good. Would you mind add a unit test case?,"20/Feb/13 00:04;rohini;[~chetan.nadgire],
   Is it not possible for you to use a different PigServer instance for each test case? Clearing the scriptCache in the registerScript() might break test cases of other users who register script multiple times with different scripts. Also clearing just the current DAG might not work when there are nested scripts (Look for the graphs LinkedList in PigServer which stores multiple graphs)","20/Feb/13 21:26;chetan.nadgire;Taking a look. PigTest checks for existing instance of PigServer and reuses it instead of creating new instance, will debug more. 
Also did not see any unit test failures after applying this patch.

Right now pigunit build is failing on branch 0.11, opened https://issues.apache.org/jira/browse/PIG-3201 to track this issue.",22/Feb/13 05:49;rohini;Then we should probably look at adding a reInitialize method to PigTest which will reset the PigServer instance and can be used in tests like this. ,18/Apr/13 22:15;alangates;Canceling patch pending agreement on how to address the issue.,"21/Jun/13 19:39;sajidraza;I see the same behavior, and I would urge the maintainers to come to an agreement on the fix.","21/Jun/13 19:44;sajidraza;BTW, to get around the issues I created my own sub-class of PigTest with an overridden registerScript method that resets the PigServer instance on every invocation.

Doing so is inefficient since I'm reinitializing PigServer and registerScript is called a number of times.

Why does PigTest register the text of the script multiple times with the PigServer?

I would suggest lazily parsing the script when the user calls assertOutput. Registering the script once when the input alias has been specified would probably make life easier.

Either way, it would be nice to have a component that a) does not rely on global (static) state, and b) that allows the end-user some transparency into how it manages resources.","24/Jul/13 09:39;metaruslan;Sajid Raza:
Can you please paste the code for your workaround?",25/Jul/13 00:01;sajidraza;Attached the workaround to this JIRA.,25/Jul/13 00:01;sajidraza;Temporary workaround in end-user code to get PigTest to work.,"25/Jul/13 19:24;metaruslan;Sajid Raza:
Thanks a lot!","30/Jul/13 08:06;metaruslan;Sajid,

The patch didn't work for me:( I am getting the same ""Reason: Duplicated macro name...""
Also I see the message ""No matching static PigServer field found to patch."" appearing in the console two times. My Pig version is
[cloudera@localhost pig-scripts]$ pig -version
Apache Pig version 0.11.0-cdh4.3.0 (rexported) 
compiled May 27 2013, 20:48:21

Any thoughts? Any help would be much appreciated.

I also noticed another problem with Macros:
Here org.apache.pig.pigunit.PigTest.PigTest(String[], String[], String)
the STORE and DUMP statements are erased as far as I understand, but if there is a STORE statement in my Macro, it doesn't get erased:(
Should we open another jira for it?","16/Aug/13 21:19;sajidraza;Sorry, I haven't looked into the Cloudera's version of Pig. I'd suggest poking through their source code snapshot.",26/Sep/13 23:39;daijy;The issue is script is accidentally registered twice in some cases. Attach a patch.,27/Sep/13 00:06;rohini;patch does not have top_queries_macro.pig. Looks good otherwise,"27/Sep/13 00:59;daijy;Thanks [~rohini], attached new patch include the missing file.",27/Sep/13 18:33;rohini;+1,27/Sep/13 20:40;daijy;Patch committed to both branch 0.12 and trunk.,"25/Feb/15 01:46;jserrano;It looks like the issue is still present in version 0.12.0, 0.12.1, 0.13.0 and 0.14.0. The same script (test.pig) is failing with the exact same error. I'm attaching the code I used to reproduce the error (bug_pig.tbz2).","25/Feb/15 01:48;jserrano;Maven project to reproduce the issue. ""mvn test"" to run the test.","26/Feb/15 06:31;daijy;The original fix does not apply to the new function introduced in PIG-2692. I cannot find a easy fix for that. We do need to register the query multiple times to get alias mock working, but that causes ""Duplicated macro"" error.

One simple work around is to change the test to register macro separately:
{code}
        String[] script = {
                ""data =  LOAD 'input' AS (query:CHARARRAY);"",
                ""queries_group = GROUP data BY query;"",
                ""queries_count = FOREACH queries_group GENERATE group AS query, COUNT(data) AS total;"",
                ""queries_ordered = my_macro_1(queries_count, query);"",
                ""queries_limit = LIMIT queries_ordered 2;"",
                ""STORE queries_limit INTO 'output';""
        };
        PigTest test = new PigTest(script, new String[]{});
        test.getPigServer().registerQuery(""DEFINE my_macro_1 (QUERY, A) RETURNS C {"" +
                ""$C = ORDER $QUERY BY total DESC, $A;"" +
                ""} ;"");
        test.assertOutput(""data"", new String[] {""1""},
                ""queries_limit"", new String[] {""(1)""});
{code}","22/Oct/15 20:33;opensauce;Has there been any update for this ticket? I am getting this error for the versions above as well as 0.15.0.
Yet it looks like this ticket has been closed.","23/Oct/15 20:22;opensauce;I had a problem in pigunit 0.15.0. It looks like this is fixed only if you  unoverride your STORE method. Be sure to build the PigTest object (and pig unit objects in general) the same was as the TestPigTest class. This class has testMacro which I used as a template.
https://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/pigunit/TestPigTest.java

{code}
  @Test
    // Script should only be registered once, otherwise Pig will complain
    // Macro defined twice. See PIG-3114
    public void testMacro() throws ParseException, IOException {
        String[] args = {
                        ""n=3"",
                        ""reducers=1"",
                        ""input=top_queries_input_data.txt"",
                        ""output=top_3_queries"",
        };
        test = new PigTest(PIG_SCRIPT_MACRO, args);

        // By default PigUnit removes all the STORE and DUMP
        test.unoverride(""STORE"");

        test.runScript();

        assertTrue(cluster.delete(new Path(""top_3_queries"")));
    }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Errors and lacks in document ""User Defined Functions""",PIG-3112,12625608,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,miyakawataku,miyakawataku,miyakawataku,01/Jan/13 09:53,14/Oct/13 16:46,14/Mar/19 03:07,01/Jan/13 22:05,0.10.0,,,,,,,0.12.0,,,,documentation,,,0,documentation,patch,,,,,,,,,,,"This patch fixes following errors in the document ""User Defined Functions"".

# Malformed type parameters such as ""List (FuncSpec)"" and ""new ArrayList (FuncSpec) ()""
# ""Reporting Progress"" subsection describes EvalFunc#progress, but the example program calls PigProgressable#progress. The program works, but it would be better to be consistent with the description calling EvalFunc#progress.
# Several sentences about LoadFunc and StoreFunc refer to the current load/store API as ""the new API"" such as ""The meaning of putNext() has not changed ... - in the new API, this is the method ..."". It is not good because the document is not about migration from the old API. The patch removes those words.
# ""Passing Configurations to UDFs"" subsection refers to ""describeSchema"" method, but it does not exist. The patch replaces it with ""checkSchema"" method.
# Updates links
#* [Using Java Reflection|http://www.oracle.com/technetwork/articles/java/javareflection-1536171.html]
#* [javadoc - The Java API Documentation Generator|http://docs.oracle.com/javase/6/docs/technotes/tools/solaris/javadoc.html]
#* [How to Contribute to Pig|https://cwiki.apache.org/confluence/display/PIG/HowToContribute]
# Fixes typos
#* ""implemented in -three- +five+ languages: Java, Python, JavaScript, Ruby and Groovy""
#* ""The exact signature of the function should +be+ clear from its documentation""
#* The EvalFunc -function- +class+ provides a progress function
#* ""Inside -EvalFunc- +exec method+, you can assume that these files already exist in distributed cache""
#* ""throw +new+ IOException(...);""
#* ""throw -newIOException- +new IOException+ (...);""
#* ""throw new -OException- +IOException+ (...);""
#* ""to use the UPPER -command- +function+ ""
#* "" -Square- +square+ - Square of a number of any data type""
#* ""ships the required scriptengine (-js- +Rhino+) to interpret it""

The patch creates a new section ""Using Short Names"" including two subsections:

* ""Import Lists"" moved from ""Eval Functions"": this subsection should be out of ""Eval Functions"", because import lists can be used for load/store functions, and the example program imports a load function.
* ""Defining Aliases"" describing DEFINE statement: it seems good to explain the statement here, next to the description of import lists. DEFINE statement currently has not been referred from the document.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Jan/13 09:57;miyakawataku;PIG-3112.patch;https://issues.apache.org/jira/secure/attachment/12562836/PIG-3112.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-01 21:58:09.385,,,no_permission,,,,,,,,,,,,302154,,,,Tue Jan 01 22:05:00 UTC 2013,,,Patch Available,,,,0|i16z1j:,248812,,,,,,,,,,01/Jan/13 09:57;miyakawataku;A patch which fixes the issue.,01/Jan/13 09:57;miyakawataku;A patch which fixes the issue.,01/Jan/13 21:58;cheolsoo;+1.,01/Jan/13 22:05;cheolsoo;Committed to trunk. Thanks Miyakawa!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig corrupts chararrays with trailing whitespace when converting them to long,PIG-3110,12625569,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,prkommireddi,ihadanny,ihadanny,31/Dec/12 13:35,14/Oct/13 16:46,14/Mar/19 03:07,21/Mar/13 00:05,0.10.0,,,,,,,0.12.0,,,,data,,,0,,,,,,,,,,,,,"when trying to convert the following string into long, pig corrupts it. data:

1703598819951657279 ,44081037

data1 = load 'data' using CSVLoader as (a: chararray ,b: int);
data2 = foreach data1 generate (long)a as a;
dump data2;
(1703598819951657216)    <--- last 2 digits are corrupted

data2 = foreach data1 generate (long)TRIM(a) as a;
dump data2;
(1703598819951657279)    <--- correct


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Mar/13 02:28;prkommireddi;PIG-3110.patch;https://issues.apache.org/jira/secure/attachment/12574300/PIG-3110.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-09 06:43:01.518,,,no_permission,,,,,,,,,,,,302113,Reviewed,,,Thu Mar 21 00:05:35 UTC 2013,,,,,,,0|i16yon:,248754,,,,,,,,,,"09/Jan/13 06:43;daijy;Here is the code introduce the issue:
{code}
			try {
				return Long.parseLong(str);
			} catch (NumberFormatException e) {
				try {
					Double d = Double.valueOf(str);
					// Need to check for an overflow error
					if (d.doubleValue() > mMaxLong.doubleValue() + 1.0) {
						LogUtils.warn(CastUtils.class, ""Value "" + d
								+ "" too large for long"",
								PigWarning.TOO_LARGE_FOR_INT, mLog);
						return null;
					}
					return Long.valueOf(d.longValue());
				}
                         }
{code}
Not sure why we still try double, seems adding more confusion. Shall we change?","18/Mar/13 23:13;prkommireddi;I think the difference between Double and Integer/Long behavior here is that the former handles whitespaces. Here is a snippet from Double.java

{code}
    public static FloatingDecimal
    readJavaFormatString( String in ) throws NumberFormatException {
        boolean isNegative = false;
        boolean signSeen   = false;
        int     decExp;
        char    c;

    parseNumber:
        try{
            in = in.trim(); // don't fool around with white space.
{code}

Another reason we use Double is for conversion from floating points.

Is there a reason other than these we use Double? I think case 1 (whitespace) could be handled in pig code rather than having to use Double and then converting it back to Integer/Long. PIG-2835 added sanityCheckIntegerLong(String s) for performance reasons. Trimming whitespace in addition to that should handle the issue described in this JIRA.","18/Mar/13 23:59;daijy;Agree, in case of parseLong fail, we can try to do a trim. Doing a double conversion introduce the rounding problem unnecessarily.","19/Mar/13 02:28;prkommireddi;Patch contains changes to Utf8StorageConverter and TestConversions. In addition to making the above discussed changes I have also added an additional check

{code}
        if(b == null || b.length == 0) {
            return null;
        }
{code}
We don't need to parse further if the input byte array is empty, thereby avoiding expensive valueOf(String s) calls.

Also, this could further be optimized if the only reason now for falling back on Double.valueOf() is to handle floating points. The current process for bytesToLong and bytesToInteger in case of floating point numbers is:
1. Integer/Long.valueOf(String)
2. If 1 results in null, call Double.valueOf
3. Convert result of 2 back to Integer/Long.

Input bytearray can be determined to be a floating point thereby avoiding call 1.

Last thing, the above process takes place regardless of whether input byte array is numeric or not. This is unnecessary in case of strings like ""1234abcd"". 

If all agree, we should open another JIRA and optimize these methods further.",20/Mar/13 01:00;prkommireddi;[~daijy] would you like to take a look at the patch?,"20/Mar/13 01:05;daijy;Yes, I will take a look.","20/Mar/13 20:12;daijy;Looks good. Do we still need to check ""ret == null"" case if we do trim and sanity check before processing? That code is very confusing.","20/Mar/13 22:24;prkommireddi;Yes, we do need that in the case input is decimal number (123.45)

In my previous comment I proposed calling Double.valueOf() directly and then turning that into a Long/Integer in such cases. Also we can avoid these calls altogether by checking for ""numericness"". Let's do that in a separate JIRA?","21/Mar/13 00:05;daijy;Sounds good. 

Patch committed to trunk. Thanks Prashant!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBaseStorage returns empty maps when mixing wildcard- with other columns,PIG-3108,12625414,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,christoph.bauer,christoph.bauer,christoph.bauer,28/Dec/12 12:05,14/Oct/13 16:45,14/Mar/19 03:07,12/Feb/13 05:04,0.10.0,0.10.1,0.11,0.12.0,0.9.0,0.9.1,0.9.2,0.12.0,,,,,,,0,,,,,,,,,,,,,"Consider the following:
A and B should be the same (with different order, of course).
{code}
/*
in hbase shell:

create 'pigtest', 'pig'
put 'pigtest' , '1', 'pig:name', 'A'
put 'pigtest' , '1', 'pig:has_legs', 'true'
put 'pigtest' , '1', 'pig:has_ribs', 'true'
*/
A = LOAD 'hbase://pigtest' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage('pig:name pig:has*') AS (name:chararray,parts);

B = LOAD 'hbase://pigtest' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage('pig:has* pig:name') AS (parts,name:chararray);

dump A;
dump B;
{code}

This is due to a bug in setLocation and initScan.
For _A_ 
# scan.addColumn(pig,name); // for 'pig:name'
# scan.addFamily(pig); // for the 'pig:has*'

So that's silently right.

But for _B_
# scan.addFamily(pig)
# scan.addColumn(pig,name)

will override the first call to addFamily, because you cannot mix them on the same family.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Jan/13 11:19;christoph.bauer;PIG-3108.patch;https://issues.apache.org/jira/secure/attachment/12563553/PIG-3108.patch,28/Dec/12 13:19;christoph.bauer;PIG-3108.patch;https://issues.apache.org/jira/secure/attachment/12562580/PIG-3108.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-01-03 20:02:02.261,,,no_permission,,,,,,,,,,,,301956,,,,Fri Oct 04 01:22:26 UTC 2013,,,,,,,0|i16xnr:,248586,,,,,,,,,,28/Dec/12 13:19;christoph.bauer;fixes this bug in trunk,"03/Jan/13 20:02;billgraham;Thanks for digging into this one. I'm a confused either by the example or the implementation. The example seems to point out a bug when there are column descriptors with prefixes (i.e. {{has*}} or {{name*}}), but the fix is in {{addFiltersWithoutColumnPrefix}} which should only apply when there is no column with a descriptor. This is an intentional fork in how filters and/or scans are used which happens in this block of code:

{noformat}
if (!columnPrefixExists) {
    addFiltersWithoutColumnPrefix(columnInfo_);
}
else {
    addFiltersWithColumnPrefix(columnInfo_);
}
{noformat}
","04/Jan/13 10:08;christoph.bauer;Sorry. I've been with this code too long. I will try to explain.

addFiltersWithColumnPrefix and  addFiltersWithoutColumnPrefix actually do
different things:

   - addFiltersWithColumnPrefix creates HBase scan filters
   - addFiltersWithoutColumnPrefix tells the scan object the families and
   columns to retrieve. This is much quicker than adding filters, thats why it
   was changed.

The thing is: the scan object should always be limited to the
family/columns needed to speed things up. In fact we did this already - see
setLocation (it's basicly the same as in addFiltersWithoutColumnPrefix).

So what I did was to replace the code in setLocation with a call to
addFiltersWithoutColumnPrefix


To make things clear, we could remove addFiltersWithoutColumnPrefix  from
the if/else in initScan() ()setLocation will called anyway) and rename it
to setScanColumns or something.



2013/1/3 Bill Graham (JIRA) <jira@apache.org>

","07/Jan/13 05:17;billgraham;Got it, thanks for the clarification. A few comments then:

1. Yes, we should rename {{addFiltersWithoutColumnPrefix}} to {{addScans}}, since the filter part is misleading and it seems we can always call that method as implemented.
2. Instead of calling {{addFiltersWithoutColumnPrefix}} from {{setLocation}} let's just remove the  addFamily/addColumn block from {{setLocation}}. Then in {{initScan()}} we can handle both scans and filters in one place with something like this (to replace the existing conditional):

{noformat}
addScans(columnInfo_);

if (!columnPrefixExists) {
  addFiltersWithoutColumnPrefix(columnInfo_);
}
{noformat}

3. Would you please update the javadocs in {{addFiltersWithoutColumnPrefix}} (or {{addScan}}) to describe the new logic as best you can. This section of the filter/scan code has been particularly nasty in the past so we should be as clear as possible about what's happening here.

","07/Jan/13 09:56;christoph.bauer;I will generate a new patch.

The reason addFamily/addColumn is in {{setLocation}} is that {{pushProjection}} might have been called leading to a reduced number of columns actually needed in the scan.

I looked around but haven't found a better place for the scanColumns code.","07/Jan/13 10:03;christoph.bauer;Come to think of it.

column filter initialization should also be postponed if possible.
I will/did adress that in https://issues.apache.org/jira/browse/PIG-3067 if you agree.","07/Jan/13 11:22;christoph.bauer;1. Renamed addFilterWithoutColumnPrefix to addColumnsToScan
2. Removed addColumnsToScan from setLocation, because pushProjection is not working as expected anyway.
3. add/changed javadoc
4. replaced if/else logic in initScan","12/Jan/13 22:58;billgraham;Thanks Cristoph this patch looks good. I'm on vaca for a few days but after that I will test this before committing.

Let's limit PIG-3067 to only refactoring the code without any new features, improvements or bug fixes.   ","04/Oct/13 01:22;qwertymaniac;Moved from Release Notes to comments:

bq. Tested and committed. Thanks for the patch Christoph and sorry for the delay!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bin and autocomplete are missing in src release,PIG-3107,12625372,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,27/Dec/12 23:29,06/Jan/13 23:57,14/Mar/19 03:07,29/Dec/12 00:45,,,,,,,,0.10.1,0.11,0.12.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Dec/12 23:29;daijy;PIG-3107-1.patch;https://issues.apache.org/jira/secure/attachment/12562530/PIG-3107-1.patch,28/Dec/12 22:30;daijy;PIG-3107-2.patch;https://issues.apache.org/jira/secure/attachment/12562613/PIG-3107-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-12-27 23:49:04.782,,,no_permission,,,,,,,,,,,,301914,Reviewed,,,Sat Dec 29 00:45:18 UTC 2012,,,,,,,0|i16xe7:,248542,,,,,,,,,,27/Dec/12 23:49;alangates;+1,28/Dec/12 00:05;daijy;Patch committed to 0.10/0.11/trunk.,"28/Dec/12 22:30;daijy;Need another fix. Otherwise, we will have both pig-0.10.1-src/bin and pig-0.10.1-src-0.10.1/bin in the tarball.",29/Dec/12 00:02;cheolsoo;+1 for PIG-3107-2.patch.,29/Dec/12 00:45;daijy;PIG-3107-2.patch committed to 0.10/0.11/trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing license header in several java file,PIG-3106,12625345,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,27/Dec/12 17:38,06/Jan/13 23:57,14/Mar/19 03:07,27/Dec/12 17:46,,,,,,,,0.10.1,0.11,0.12.0,,,,,0,,,,,,,,,,,,,"Missing license header for:
test/org/apache/pig/TestAlgebraicEvalWithParameterizedReturnType.java
test/org/apache/pig/builtin/mock/TestMockStorage.java
src/org/apache/pig/builtin/mock/Storage.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Dec/12 17:39;daijy;PIG-3106-1.patch;https://issues.apache.org/jira/secure/attachment/12562494/PIG-3106-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,301887,,,,Thu Dec 27 17:46:08 UTC 2012,,,,,,,0|i16x7z:,248514,,,,,,,,,,27/Dec/12 17:46;daijy;Committed to 0.10/0.11/trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make mockito a test dependency (instead of compile),PIG-3103,12625038,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nkollar,julienledem,julienledem,21/Dec/12 18:37,21/Jun/17 09:15,14/Mar/19 03:07,25/May/17 09:21,,,,,,,,0.17.0,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/May/17 12:56;nkollar;PIG-3103.patch;https://issues.apache.org/jira/secure/attachment/12869644/PIG-3103.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-05-24 13:10:57.935,,,no_permission,,,,,,,,,,,,301574,,,,Thu May 25 09:21:44 UTC 2017,,,,,,,0|i16urj:,248116,,,,,,,,,,24/May/17 13:10;szita;+1,"25/May/17 09:21;szita;[^PIG-3103.patch] committed to trunk, thanks [~nkollar]!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Increase io.sort.mb in YARN MiniCluster,PIG-3101,12624830,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,20/Dec/12 15:38,14/Oct/13 16:46,14/Mar/19 03:07,20/Dec/12 21:21,0.11,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"I see some unit test cases intermittently fail with the following error with Hadoop-2.0.x:
{code}
2012-12-17 18:28:39,235 FATAL [Low Memory Detector] org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[Low Memory Detector,9,system] threw an Error.  Shutting down now...
java.lang.InternalError: Error in invoking listener
    at sun.management.NotificationEmitterSupport.sendNotification(NotificationEmitterSupport.java:141)
    at sun.management.MemoryImpl.createNotification(MemoryImpl.java:171)
    at sun.management.MemoryPoolImpl$CollectionSensor.triggerAction(MemoryPoolImpl.java:300)
    at sun.management.Sensor.trigger(Sensor.java:120)
2012-12-17 18:28:39,285 INFO [main] org.apache.hadoop.mapred.MapTask: Finished spill 0
2012-12-17 18:28:39,342 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.IllegalStateException: Shutdown in progress, cannot add a shutdownHook
    at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:152) 
    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2296)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2268)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:302)
    at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:273)
    at org.apache.hadoop.mapred.SpillRecord.writeToFile(SpillRecord.java:124)
    at org.apache.hadoop.mapred.SpillRecord.writeToFile(SpillRecord.java:119)
    at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.mergeParts(MapTask.java:1760)
    at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1440)
    at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:664)
    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:732)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)
    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:154)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:396)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1332)
    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:149)
{code}
By default, {{mapred.io.sort.mb}} is set to {{100}}, and {{mapred.child.java.opts}} is set to {{-Xmx200m}}. Increasing them to {{200}} and {{512m}} respectively makes the intermittent test failures go away.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Dec/12 15:40;cheolsoo;PIG-3101.patch;https://issues.apache.org/jira/secure/attachment/12561905/PIG-3101.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-20 17:47:58.892,,,no_permission,,,,,,,,,,,,301338,,,,Thu Dec 20 21:21:22 UTC 2012,,,,,,,0|i16rrz:,247632,,,,,,,,,,20/Dec/12 15:40;cheolsoo;Attach a patch for it.,20/Dec/12 17:47;sms;+1,20/Dec/12 21:21;cheolsoo;Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If a .pig_schema file is present, can get an index out of bounds error",PIG-3100,12624683,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,jcoveney,jcoveney,19/Dec/12 19:04,30/Sep/15 17:43,14/Mar/19 03:07,20/Dec/12 18:51,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"In the case that a .pig_schema file is present, if you have a record with fewer than expected fields, pig errors out with an index out of bounds exception that is annoying, unnecessary, and unhelpful.

Instead of improving logging, I decided to just do what pig should do, which is fill in the records.

Patch will include a test and the fix.",,,,,,,,,,,,,,,,,,,PIG-3828,,,,,,,,,,,,,19/Dec/12 19:07;jcoveney;PIG-3100-0.patch;https://issues.apache.org/jira/secure/attachment/12561767/PIG-3100-0.patch,19/Dec/12 19:07;jcoveney;PIG-3100-0_nows.patch;https://issues.apache.org/jira/secure/attachment/12561766/PIG-3100-0_nows.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-12-20 16:42:41.287,,,no_permission,,,,,,,,,,,,300537,,,,Thu Dec 20 20:20:40 UTC 2012,,,,,,,0|i16fpj:,245677,,,,,,,,,,19/Dec/12 19:07;jcoveney;Here is the patch with test and fix.,20/Dec/12 16:42;rohini;+1,20/Dec/12 18:51;jcoveney;Committed. Thanks Rohini!,"20/Dec/12 19:21;knoguchi;I should have commented on PIG-3056, but when our users hit this issue, those affected records tend to contain a record-separator as part of the data by mistake. And that resulted in a single record separated into two incomplete ones.

For that case, I wasn't sure if we wanted to fill the incomplete records with null or have an option like PIG-3059.","20/Dec/12 19:25;jcoveney;Hah, I didn't see that ticket. Whoops!

I think it is risky to be too clever about trying to figure out  delimiters and whatnot, but I can see why we might want to be. Hmm. Perhaps there can be a flag or setting for PigStorage that is a ""strict"" mode. IE throw an error if a record does not conform to the schema. If it's not see, pad nulls (or cut off extra columns). I think that might be a good compromise?","20/Dec/12 19:40;knoguchi;bq.  Perhaps there can be a flag or setting for PigStorage that is a ""strict"" mode
That sounds like a nice feature to have.  Come to think of it, problem of delimiters are not unique to this .pig_schema file loading. 
",20/Dec/12 20:00;jcoveney;Want to make a ticket for it and link it to this one for the discussion?,"20/Dec/12 20:20;knoguchi;bq. Want to make a ticket for it and link it to this one for the discussion?

Sure. Created PIG-3102.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Pig unit test fixes for TestGrunt(1), TestStore(2), TestEmptyInputDir(3)",PIG-3099,12624561,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,vikram.dixit,vikram.dixit,vikram.dixit,19/Dec/12 01:30,06/Jan/13 23:57,14/Mar/19 03:07,20/Dec/12 16:34,0.10.0,,,,,,,0.10.1,0.11,,,,,18/Dec/12 00:00,0,,,,,,,,,,,,,"Fix race condition for TestGrunt, and test failures with Hadoop 1.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Dec/12 01:32;vikram.dixit;PIG-3099.patch;https://issues.apache.org/jira/secure/attachment/12561621/PIG-3099.patch,19/Dec/12 22:42;vikram.dixit;PIG-3099_2.patch;https://issues.apache.org/jira/secure/attachment/12561798/PIG-3099_2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-12-20 16:34:00.265,,,no_permission,,,,,,,,,,,,300382,Reviewed,,,Thu Dec 20 16:34:00 UTC 2012,,,,,,,0|i167lb:,244362,,,,,,,,,,19/Dec/12 01:32;vikram.dixit;Fixes the tests mentioned in the description.,19/Dec/12 22:42;vikram.dixit;Removing extraneous parts of the patch.,20/Dec/12 16:34;daijy;Patch committed to 0.10/0.11/trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add another test for the self join case,PIG-3098,12624398,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,jcoveney,jcoveney,18/Dec/12 02:22,14/Oct/13 16:46,14/Mar/19 03:07,18/Feb/13 17:27,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"This adds a test to TestJoin that doesn't just make sure that self joins work semantically in the parser, but also that it pulls the right data through. Thought it'd be easier to just make a new JIRA than to reopen PIG-3020.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Dec/12 02:22;jcoveney;PIG-3098-0.patch;https://issues.apache.org/jira/secure/attachment/12561402/PIG-3098-0.patch,18/Jan/13 19:22;jcoveney;PIG-3098-1.patch;https://issues.apache.org/jira/secure/attachment/12565536/PIG-3098-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-01-17 22:50:41.029,,,no_permission,,,,,,,,,,,,299237,,,,Mon Feb 18 17:27:41 UTC 2013,,,,,,,0|i160fz:,243204,,,,,,,,,,17/Jan/13 19:52;jcoveney;Bump,"17/Jan/13 22:50;julienledem;one minor comment regarding asserts:
{noformat}
  assertEquals(tuples.size(), out.size());
  for (Tuple t : out) {
    assertTrue(tuples.remove(t));
  }
  assertTrue(tuples.isEmpty());
{noformat}
if wrong it is not going to give much information.
please add a message as the first parameter with some info:
{noformat}
  assertEquals(""tuple count for "" + out, tuples.size(), out.size());
  for (Tuple t : out) {
    assertTrue(""existence of "" + t, tuples.remove(t));
  }
  assertTrue(""all tuples consumed in "" + tuples, tuples.isEmpty());
{noformat}

","18/Jan/13 19:22;jcoveney;Thanks for the comments, Julien. I updated it accordingly.","04/Feb/13 18:49;alangates;+1, patch looks good, new test passes.",18/Feb/13 17:27;jcoveney;Committed. Thanks Alan!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveColumnarLoader doesn't correctly load partitioned Hive table ,PIG-3097,12624220,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,maczech,rding,rding,17/Dec/12 01:41,14/Oct/13 16:45,14/Mar/19 03:07,02/May/13 17:41,0.10.1,,,,,,,0.12.0,,,,,,,0,patch,,,,,,,,,,,,"
Given a partitioned Hive table:

{code}
hive> describe mytable;
OK
f1    string  
f2     string  
f3     string  
partition_dt    string
{code}

The following Pig script gives the correct schema:

{code}
grunt> A = load '/hive/warehouse/mytable' using org.apache.pig.piggybank.storage.HiveColumnarLoader('f1 string,f2string,f3 string');
grunt> describe A
A: {f1: chararray,f2: chararray,f3: chararray,partition_dt: chararray}
{code}

But, the command

{code}
grunt> dump A
{code}

only produces the first column of all records in the table (all four columns are expected).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/May/13 10:31;maczech;PIG-3097.patch;https://issues.apache.org/jira/secure/attachment/12581353/PIG-3097.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-01 10:30:15.788,,,no_permission,,,,,,,,,,,,298984,Reviewed,,,Thu May 02 17:41:40 UTC 2013,,,,,,,0|i15xpj:,242761,,,,,,,,,,"01/May/13 10:30;maczech;We are using 0.10.1 version, so this fix is for this version. The fix is extremely simple so it should be easy to put it to trunk.",01/May/13 22:49;daijy;The patch applies to trunk and all piggybank tests pass. Is it Ok to committed to trunk?,02/May/13 10:45;maczech;In my opinion yes.,02/May/13 17:41;daijy;Patch committed to trunk. Thanks Marcin!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make PigUnit thread safe,PIG-3096,12624219,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,17/Dec/12 01:25,14/Oct/13 16:46,14/Mar/19 03:07,20/Dec/12 16:31,0.11,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"Currently, {{PigUnit}} is not thread-safe because {{Cluster}} and {{PigServer}} are declared as static. Converting them to ThreadLocal allows PigUnit to run in multi-threaded environment.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Dec/12 01:30;cheolsoo;PIG-3096.patch;https://issues.apache.org/jira/secure/attachment/12561232/PIG-3096.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-19 01:17:16.767,,,no_permission,,,,,,,,,,,,298983,,,,Thu Dec 20 16:31:47 UTC 2012,,,,,,,0|i15xpb:,242760,,,,,,,,,,"17/Dec/12 01:30;cheolsoo;Attached is a patch that does the following:
- Convert Cluster and PigServer to ThreadLocal
- Randomize the destination of input file in the cluster so that input files don't conflict when multiple jobs run in parallel. ",19/Dec/12 01:17;billgraham;+1 lgtm.,"20/Dec/12 16:31;cheolsoo;Thanks Bill for reviewing it.

I committed it to trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""which"" is called many, many times for each Pig STREAM statement",PIG-3095,12623663,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,njw45,njw45,njw45,13/Dec/12 04:23,14/Oct/13 16:45,14/Mar/19 03:07,16/Dec/12 20:13,0.12.0,,,,,,,0.12.0,,,,grunt,impl,,0,patch,performance,,,,,,,,,,,"STREAM statements are checked by the LogicalPlanBuilder as it comes across them - and these checks include running the system utility ""which"". However, due to the backtracking parsing mechanism ""which"" is called repeatedly with the same arguments (I noticed this while profiling a script with 4 STREAM statements - ""which"" was run over 230 times!). The attached patch just caches the return value of ""which"", reducing the overhead of running a system process to a Map lookup.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Dec/12 06:05;njw45;PIG-3095.1.patch;https://issues.apache.org/jira/secure/attachment/12561165/PIG-3095.1.patch,13/Dec/12 04:24;njw45;PIG-3095.patch;https://issues.apache.org/jira/secure/attachment/12560714/PIG-3095.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-12-15 21:56:59.518,,,no_permission,,,,,,,,,,,,297391,,,,Sun Dec 16 20:13:36 UTC 2012,,,Patch Available,,,,0|i14ocn:,235410,,,,,,,,,,"15/Dec/12 21:56;cheolsoo;Hello Nick,

Thank you very much for the patch! I have a few comments:
- The [guava doc|http://code.google.com/p/guava-libraries/wiki/CachesExplained] says ""If you have defined a CacheLoader that does not declare any checked exceptions then you can perform cache lookups using getUnchecked(K)"".
{code}
private static final class Which extends CacheLoader<String, String> {
     public String load(String file) {
        ...
         } catch (Exception e) {}
         return null;
     }
}
{code}
Since {{Which.load()}} does not declare any checked exceptions, can't we use {{getUnchecked()}} without the {{try/catch}} block? In fact, I think that no exception will be ever caught by the following {{catch}} since {{Which.load()}} already catches any exception. Please correct me if I am wrong.
{code}
        try {
            argPath = whichCache.getUnchecked(arg);
        } catch(ExecutionException e) {
            argPath = null;
        }
{code}
With this change, ""{{import java.util.concurrent.ExecutionException}}"" is not needed.
- Can you please remove the following statement since it's unused?
{code}
import com.google.common.cache.Cache;
{code}
- Can you please fix indentation in {{CacheLoader}}? This is probably because you used ""{{git diff -w}}"".
{code}
private static final class Which extends CacheLoader<String, String> {
public String load(String file) {
...
}
}
{code}","15/Dec/12 22:37;cheolsoo;I also noticed that your patch breaks {{TestSctreaming}}. Here is one example among others:
{code}
Testcase: testInputOutputSpecs took 0.022 sec
    Caused an ERROR
Error during parsing. CacheLoader returned null for key script1259692341619822790pl.
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1000: Error during parsing. CacheLoader returned null for key script1259692341619822790pl.
    at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1617)
    at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1556)
    at org.apache.pig.PigServer.registerQuery(PigServer.java:526)
    at org.apache.pig.PigServer.registerQuery(PigServer.java:539)
    at org.apache.pig.test.TestStreaming.testInputOutputSpecs(TestStreaming.java:640)
Caused by: Failed to parse: CacheLoader returned null for key script1259692341619822790pl.
    at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:193)
    at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1609)
Caused by: com.google.common.cache.CacheLoader$InvalidCacheLoadException: CacheLoader returned null for key script1259692341619822790pl.
    at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2383)
    at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2351)
    at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2313)
    at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2228)
    at com.google.common.cache.LocalCache.get(LocalCache.java:3967)
    at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3971)
    at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4831)
    at org.apache.pig.parser.StreamingCommandUtils.checkAndShip(StreamingCommandUtils.java:155)
    at org.apache.pig.parser.StreamingCommandUtils.checkAutoShipSpecs(StreamingCommandUtils.java:135)
    at org.apache.pig.parser.LogicalPlanBuilder.buildCommand(LogicalPlanBuilder.java:1134)
    at org.apache.pig.parser.LogicalPlanBuilder.buildCommand(LogicalPlanBuilder.java:1087)
    at org.apache.pig.parser.LogicalPlanGenerator.cmd(LogicalPlanGenerator.java:2097)
    at org.apache.pig.parser.LogicalPlanGenerator.define_clause(LogicalPlanGenerator.java:1802)
    at org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1295)
    at org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:799)
    at org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:517)
    at org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:392)
    at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:184)
{code}
To reproduce, please run:
{code}
ant clean test -Dtestcase=TestStreaming
{code}","16/Dec/12 06:05;njw45;Ah, sorry about that. I've fixed the issues you mentioned (and read the guava javadoc a bit more carefully :) in the attached PIG-3095.1.patch, and the tests now pass. Thanks -","16/Dec/12 08:13;cheolsoo;+1.

I will run e2e test to be safe and commit your patch unless I see any failure. Thank you!",16/Dec/12 20:13;cheolsoo;Committed to trunk. Thanks Nick!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor TestLogicalPlanBuilder to be meaningful,PIG-3087,12623202,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,szita,jcoveney,jcoveney,10/Dec/12 19:59,21/Jun/17 09:15,14/Mar/19 03:07,06/Oct/16 19:10,,,,,,,,0.17.0,,,,,,,1,newbie,,,,,,,,,,,,"I started doing this as part of another patch, but there are some bigger issues, and I don't have the time to dig in atm.

That said, a lot of the tests as written don't test anything. I used more modern junit patterns, and discovered we had a lot of tests that weren't functioning properly. Making them function properly unveiled that the general buildLp pattern doesn't work properly anymore for many cases where it would throw an error in grunt, but for whatever reason no error is thrown in the tests.

Any test with _1 is a test that previous failed, that now doesn't. Some, however, don't make sense so I think what really needs to be done is figure out which should be failing, which shouldn't, and then fix buildLp accordingly.

I will attach my pass at it, but it is incomplete and needs work.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Dec/12 20:00;jcoveney;PIG-3087-0.patch;https://issues.apache.org/jira/secure/attachment/12560262/PIG-3087-0.patch,16/Sep/16 12:31;szita;PIG-3087.1.patch;https://issues.apache.org/jira/secure/attachment/12828825/PIG-3087.1.patch,06/Oct/16 09:12;szita;PIG-3087.2.patch;https://issues.apache.org/jira/secure/attachment/12831925/PIG-3087.2.patch,06/Oct/16 17:14;szita;PIG-3087.3.patch;https://issues.apache.org/jira/secure/attachment/12831993/PIG-3087.3.patch,06/Oct/16 19:01;szita;PIG-3087.4.patch;https://issues.apache.org/jira/secure/attachment/12832010/PIG-3087.4.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2014-08-26 21:15:19.725,,,no_permission,,,,,,,,,,,,296814,Reviewed,,,Thu Oct 06 20:16:55 UTC 2016,,,,,,,0|i14hgv:,234293,,,,,,,,,,26/Aug/14 21:15;haogao;Can I try this?,"26/Aug/14 21:52;jcoveney;Please do!
","16/Sep/16 12:57;szita;Taken a look on this. It seems that quite some mysterious (failing in grunt if inputted manually, but passing in unit test mode) test cases are resulted from query validation being off.
In the patch I attached (PIG-3087.1.patch) I have this enabled and also went through and fixed several test cases so that they pass (or fail with expected exception) properly.
I also found a bug in ForEachUserSchemaVisitor during the process: it seems that it can't handle use cases where casting is in the latest (foreach) step of the plan - so I fixed this as well: testQuery90a tests this.

[~daijy] can you please take a look when you get a chance?","05/Oct/16 22:39;daijy;The bug fix in ForEachUserSchemaVisitor looks good. I have comments on the test fix:
1. ExecType.LOCAL->Util.getLocalTestMode()
2. Seems now you are validate plan for most test, why we miss some validation exception (eg, testQuery22_a)","06/Oct/16 09:18;szita;Thanks for taking a look.
1. Fixed - see PIG-3087.2.patch
2. testQuery22_a was testQuery22Fail before, indicating that is should fail and exception should be caught. This is not the case with that Pig script (at least with the trunk version), plan is built successfully without throwing any errors. So all-in-all this one is not expected to fail anymore. (We just didn't see this because this test case always went through, just not entering in the catch clause)","06/Oct/16 16:58;daijy;I think the original test is intended to test the fail path. It's better to change the query to trigger the validation exception:
A = load 'a';
B = group A by (*, $0);",06/Oct/16 17:15;szita;Ah I see your point. Patch revised,"06/Oct/16 17:24;jcoveney;So cool to see this get some love! Adam, you are a prince!","06/Oct/16 18:42;daijy;Another question, why setting ""pigServer.setValidateEachStatement(false);"" in testQueryFail67? Doesn't seem necessary.","06/Oct/16 19:01;szita;Indeed that's not needed there - apparently we'll end up in the catch clause either way: with the validation_on we get a ParserException, with validation_off we get the same, just wrapped into a FrontendException","06/Oct/16 19:10;daijy;Patch committed to trunk. Thanks Adam, Jonathan!",06/Oct/16 20:16;szita;Thanks for the review,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Errors and lacks in document ""Built In Functions""",PIG-3085,12623042,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,miyakawataku,miyakawataku,miyakawataku,09/Dec/12 09:55,14/Oct/13 16:46,14/Mar/19 03:07,15/Dec/12 20:34,0.10.0,,,,,,,0.12.0,,,,documentation,,,0,documentation,,,,,,,,,,,,"This is a patch to fix errors in document ""Built In Functions"".

# Heading of the result table of ROUND function is wrongly written as ??CEIL\(x\)??
# ""Syntax"" clause of LAST_INDEX_OF function is wrongly written as ??LAST_INDEX_OF(expression)??, but the function actually takes three arguments
# ""Syntax"" clause of REGEX_EXTRACT_ALL function is wrongly written as ??REGEX_EXTRACT (string, regex)??
# An example of REPLACE function is terminated by a semicolon: ??REPLACE(string,'software','wiki');??, but the semicolon is not necessary in Pig Latin syntax
# ""limit"" argument of STRSPLIT function is described as ??The number of times the pattern ... is applied??. But actually, _limit-1_ is the maximum number of times the pattern is applied, and ""limit"" means the maximum length of the result tuple (if the value is positive). If the value is 0 or negative, not limit is applied.
# An example program of TOP function does not work, because it uses ??COUNT(\*\)??
# Updates links to pages in the Java API Reference
# Fixes typos
#* ""COUNT_STAR is used -the- +to+ count ...""
#* ""delimit in JsonLoader or -JsonStorer- +JsonStorage+ ...""
#* ""using field -deliminters- +delimiters+ ...""
#* ""If you store -reading- +using+ delimiter ""#"" ...""
#* ""The STORE -function- +statement+ specifies that the files ...""
#* ""Limit"" -> ""limit"" in a term definition of STRSPLIT function",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Dec/12 12:18;miyakawataku;PIG-3085.patch;https://issues.apache.org/jira/secure/attachment/12561111/PIG-3085.patch,09/Dec/12 10:07;miyakawataku;PIG-3085.patch;https://issues.apache.org/jira/secure/attachment/12560065/PIG-3085.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-12-11 08:09:28.333,,,no_permission,,,,,,,,,,,,296634,,,,Sat Dec 15 20:34:03 UTC 2012,,,Patch Available,,,,0|i14dk7:,233660,,,,,,,,,,09/Dec/12 10:07;miyakawataku;The patch which fixes the errors.,"09/Dec/12 10:11;miyakawataku;The patch to fix errors in the document ""Built In Functions"" in trunk/","11/Dec/12 08:09;cheolsoo;Hello Miyakawa,

Thank you very much for the patch! I have one comment:
{quote}
""Syntax"" clause of LAST_INDEX_OF function is wrongly written as LAST_INDEX_OF(expression), but the function actually takes three arguments
{quote}
Looking at source code, {{LAST_INDEX_OF}} takes only 2 args: string to be searched and substring to search for. Inside the UDF, {{string.lastIndexOf(substring)}} is called. In fact, this is inconsistent with {{INDEXOF}} that takes the 3rd optional arg ({{fromIndex}}).

Can you please update the patch?","15/Dec/12 12:18;miyakawataku;Thank you for the comment.

I updated the patch.",15/Dec/12 20:34;cheolsoo;Committed to trunk. Thanks Miyakawa!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve exceptions messages in POPackage,PIG-3084,12622983,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,julienledem,julienledem,julienledem,07/Dec/12 22:47,14/Oct/13 16:45,14/Mar/19 03:07,07/Dec/12 23:19,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Dec/12 23:01;julienledem;PIG-3084.patch;https://issues.apache.org/jira/secure/attachment/12559969/PIG-3084.patch,07/Dec/12 23:16;julienledem;PIG-3084_1.patch;https://issues.apache.org/jira/secure/attachment/12559977/PIG-3084_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-12-07 23:03:32.731,,,no_permission,,,,,,,,,,,,296570,,,,Fri Dec 07 23:16:53 UTC 2012,,,,,,,0|i14a9r:,233127,,,,,,,,,,07/Dec/12 23:01;julienledem;better exception in PIG-3084.patch,07/Dec/12 23:03;jcoveney;+1,07/Dec/12 23:16;julienledem;PIG-3084_1.patch same patch with white space adjusted,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
outputSchema of a UDF allows two usages when describing a Tuple schema,PIG-3082,12622733,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,julienledem,julienledem,06/Dec/12 22:41,30/Mar/16 21:05,14/Mar/19 03:07,23/Jan/13 23:14,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"When defining an evalfunc that returns a Tuple there are two ways you can implement outputSchema().
- The right way: return a schema that contains one Field that contains the type and schema of the return type of the UDF
- The unreliable way: return a schema that contains more than one field and it will be understood as a tuple schema even though there is no type (which is in Field class) to specify that. This is particularly deceitful when the output schema is derived from the input schema and the outputted Tuple sometimes contain only one field. In such cases Pig understands the output schema as a tuple only if there is more than one field. And sometimes it works, sometimes it does not.

We should at least issue a warning (backward compatibility) if not plain throw an exception when the output schema contains more than one Field.",,,,,,,,,,,,,,,,,,,,,,,,,PIG-3153,PIG-3150,,,,,,11/Dec/12 00:02;jcoveney;PIG-3082-0.patch;https://issues.apache.org/jira/secure/attachment/12560311/PIG-3082-0.patch,23/Jan/13 22:17;jcoveney;PIG-3082-1.patch;https://issues.apache.org/jira/secure/attachment/12566202/PIG-3082-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-01-23 22:17:38.165,,,no_permission,,,,,,,,,,,,296398,Incompatible change,,,Fri Oct 04 22:47:07 UTC 2013,,,,,,,0|i1493r:,232938,,,,,,,,,,"17/Jan/13 22:44;julienledem;Thanks for fixing Jon!
I find the error message a little confusing:
{noformat}
 throw new FrontendException(""Given UDF returns an improper Schema. Should only return Tuple, Bag, or a single item. Returns: "" + udfSchema);
{noformat}
It should contain something along the lines of ""... outputSchema should return a Schema containing a single Field ..."".
Otherwise, it looks good to me.
Thanks",23/Jan/13 22:17;jcoveney;Updated with a better message. Testing then will commit,"03/Oct/13 01:08;dvryaboy;So... that's a breaking change, a bunch of UDF will fail under 12. 

Intended?","03/Oct/13 18:41;julienledem;This is intended.
The second behavior described above is really problematic.
If a UDF breaks because it returns a schema of more than one field it should be changed to return one field of type tuple.
Once fixed it works in all versions of Pig.
This is only removing an unsafe use of outputSchema in favor of the existing correct use.","04/Oct/13 22:47;aniket486;But, we should document this as incompatible change so that there are no surprises?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig progress stays at 0% for the first job in hadoop 23,PIG-3081,12618939,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,05/Dec/12 20:26,14/Oct/13 16:46,14/Mar/19 03:07,05/Mar/13 13:01,0.10.0,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"  We are seeing that for many scripts if there are multiple jobs in the job graph, progress stays at 0% for the first job and jumps to 33% when the first job completes. There is no intermediate progress. After that intermediate progress gets reported for the subsequent jobs. Noticed this with jobs that do filtering and order by. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Dec/12 21:57;rohini;PIG-3081-1.patch;https://issues.apache.org/jira/secure/attachment/12556156/PIG-3081-1.patch,05/Dec/12 20:45;rohini;PIG-3081.patch;https://issues.apache.org/jira/secure/attachment/12556141/PIG-3081.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-03-04 23:14:18.472,,,no_permission,,,,,,,,,,,,296205,,,,Tue Mar 05 13:01:53 UTC 2013,,,,,,,0|i146xr:,232587,,,,,,,,,,"05/Dec/12 20:45;rohini;calculateProgress=(mapprog+reduceprog)/2
prog=calculateProgress/totalMRJobs  (Pig displays progress as a percentage of
all the jobs combined together and not per job)

mapprog=0.6962389945983887, reduceprog=0.0
numMRJobsCompl=0,totalMRJobs=3,calculateProgress=0.34811949729919434
prog=0.11603983243306477,lastProg=0.11490649978319804
mapprog=0.7032797336578369, reduceprog=0.0
numMRJobsCompl=0,totalMRJobs=3,calculateProgress=0.35163986682891846
prog=0.11721328894297282,lastProg=0.11603983243306477
.....
mapprog=0.7190613150596619, reduceprog=0.0
numMRJobsCompl=0,totalMRJobs=3,calculateProgress=0.35953065752983093
prog=0.11984355250994365,lastProg=0.1190514365832011
mapprog=0.725161075592041, reduceprog=0.0
numMRJobsCompl=0,totalMRJobs=3,calculateProgress=0.3625805377960205
prog=0.12086017926534016,lastProg=0.11984355250994365

if(prog>=(lastProg+0.01)){
            int perCom = (int)(prog * 100);
            if(perCom!=100) {
                log.info( perCom + ""% complete"");
        }  

  The job progress is checked every 500ms. Since the frequency of checking is
very often and the prog is never > lastProg by more than 0.01 , it stays at 0%
and goes directly to 33% when the first job completed.",05/Dec/12 21:57;rohini;Took out the sleep added in the first patch.,04/Mar/13 23:14;cheolsoo;+1. LGTM.,05/Mar/13 13:01;rohini;Committed to trunk. Thanks Cheolsoo.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Make a UDF that, given a string, returns just the columns prefixed by that string",PIG-3078,12618812,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,jcoveney,jcoveney,05/Dec/12 01:20,14/Oct/13 16:45,14/Mar/19 03:07,22/Jan/13 23:30,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"This comes up fairly often, usually as the result of a join. Given that the resulting schema has the column name prepended, a udf in the following form could give just the columns from the desired relation:

Pluck('relation_name', *)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Dec/12 21:29;jcoveney;PIG-3078-0.patch;https://issues.apache.org/jira/secure/attachment/12559956/PIG-3078-0.patch,10/Dec/12 18:44;jcoveney;PIG-3078-1.patch;https://issues.apache.org/jira/secure/attachment/12560235/PIG-3078-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-01-22 22:44:24.523,,,no_permission,,,,,,,,,,,,296060,,,,Tue Jan 22 23:11:38 UTC 2013,,,,,,,0|i144zb:,232270,,,,,,,,,,10/Dec/12 18:44;jcoveney;Updated documentation on the udf with usage,17/Jan/13 19:53;jcoveney;Bump,22/Jan/13 22:44;daijy;+1. Need some document (in addition to javadoc) though.,"22/Jan/13 22:51;jcoveney;Made a doc jira. Will commit.

Note: I think it's fine to defer documentation w/JIRA's because it's possible that future details will change how it looks. For example, PIG-3010 would change how I'd implement this, potentially!",22/Jan/13 23:11;daijy;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
POUserFunc creating log spam for large scripts,PIG-3073,12618650,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,jcoveney,jcoveney,03/Dec/12 22:45,14/Oct/13 16:46,14/Mar/19 03:07,23/Jan/13 23:17,0.11,0.12.0,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"POUserFunc got some new logging in the case where a JobConf isn't available. In certain cases (huge scripts with tons of UDFs), this is creating gigantic logs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Dec/12 22:46;jcoveney;PIG-3073-0.patch;https://issues.apache.org/jira/secure/attachment/12555843/PIG-3073-0.patch,23/Jan/13 22:10;jcoveney;PIG-3073-1.patch;https://issues.apache.org/jira/secure/attachment/12566200/PIG-3073-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-12-11 08:28:39.219,,,no_permission,,,,,,,,,,,,295811,,,,Wed Jan 23 22:39:17 UTC 2013,,,,,,,0|i13y4f:,231158,,,,,,,,,,"03/Dec/12 22:46;jcoveney;Here is a fix for it... I'm open to other opinions on how to fix this. I used a ThreadLocal because the JobConf is stored in a ThreadLocal, so I wanted to mirror that.","11/Dec/12 08:28;cheolsoo;Hi Jonathan,

How about simply removing that warning message?
{code}
LOG.warn(""jobConf not available. Not tracking UDF timing regardless of user preference."");
{code}
According to the comment for {{UdfContext.getJobConf()}}, {{jobConf}} will be null only if it's called on the front-end. In that case, I don't think that this warning message is particularly helpful after all.
{code}
     * Get the JobConf.  This should only be called on
     * the backend.  It will return null on the frontend.
{code}
Please let me know what you think.

Thanks!
",11/Dec/12 17:51;jcoveney;Sounds good to me.,23/Jan/13 19:16;alangates;Canceling patch since it seems the feedback was to take a different direction.,23/Jan/13 22:10;jcoveney;ok enough laziness. Here is a patch which removes it.,23/Jan/13 22:39;daijy;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig job reporting negative progress,PIG-3072,12618627,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,knoguchi,knoguchi,knoguchi,03/Dec/12 20:22,14/Oct/13 16:45,14/Mar/19 03:07,04/Dec/12 23:55,0.10.0,,,,,,,0.12.0,,,,impl,,,0,,,,,,,,,,,,,"Our users pointed out that their jobs reporting negative progress.

2012-11-02 21:43:11,538 [main] INFO 
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher
- -795% complete
...

(due to TFileRecordReader)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Dec/12 20:46;knoguchi;pig-3072-v01.txt;https://issues.apache.org/jira/secure/attachment/12555826/pig-3072-v01.txt,03/Dec/12 23:42;knoguchi;pig-3072-v02.txt;https://issues.apache.org/jira/secure/attachment/12555852/pig-3072-v02.txt,04/Dec/12 04:36;knoguchi;pig-3072-v03.txt;https://issues.apache.org/jira/secure/attachment/12555893/pig-3072-v03.txt,04/Dec/12 20:15;knoguchi;pig-3072-v04.txt;https://issues.apache.org/jira/secure/attachment/12555980/pig-3072-v04.txt,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-12-03 23:05:19.439,,,no_permission,,,,,,,,,,,,295787,,,,Tue Dec 04 23:56:07 UTC 2012,,,,,,,0|i13xyv:,231133,,,,,,,,,,03/Dec/12 20:46;knoguchi;'pos' inside TFileRecordReader was never updated.  Using fileIn.getPos() instead for reporting progress.,"03/Dec/12 23:05;rohini;Just a minor comment. Can we create the ""basic.tfile"" using Util.createTempFileDelOnExit(). Tests clash because of hardcoded file paths and it is hard to run unit tests for different versions at the same time. At least for newer tests we should try to avoid using hardcoded paths. ","03/Dec/12 23:42;knoguchi;bq. Can we create the ""basic.tfile"" using Util.createTempFileDelOnExit().

That makes sense. 
Since Util.createTempFileDelOnExit() creates an empty file and TFileRecordWriter was failing with ""File already exists"" error, I added an extra delete before passing the path to TFileRecordWriter.",03/Dec/12 23:54;rohini;+1. Will commit soon.,"04/Dec/12 04:36;knoguchi;(Minor changes to previous v02 patch.)

* Added comment in TFileRecordReader.  ""//TFile.Reader reads into buffer so, progress is updated in chunks.""

* Printing out progress to stderr.

* Changed loop size from 1000 to 25000 so that at least progress changes once for codec='none'. 66%->99%  For 'gz', it immediately jumps to 99% with this small tfile.

* Added one extra check to make sure value is being read by recordReader. (curval == LOOP_SIZE)
","04/Dec/12 18:18;rohini;Koji,
   Can you use HadoopShims to create the TaskAttemptContext in your test. The test fails to compile with H23.

{noformat}
 [javac] /apache/pig/trunk/test/org/apache/pig/test/TestTmpFileCompression.java:369: org.apache.hadoop.mapreduce.TaskAttemptContext is abstract; cannot be instantiated
    [javac]                             new TaskAttemptContext(conf, new TaskAttemptID()));
{noformat}","04/Dec/12 20:15;knoguchi;bq. Can you use HadoopShims to create the TaskAttemptContext in your test. The test fails to compile with H23.

Thanks Rohini.  Uploading another patch with your suggestion.  

Ran both
$ ant clean test -Dtestcase=TestTmpFileCompression
$ ant -Dhadoopversion=23 clean test -Dtestcase=TestTmpFileCompression",04/Dec/12 23:56;rohini;Committed to trunk. Thanks Koji.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
update hcatalog jar and path to hbase storage handler jar in pig script,PIG-3071,12618114,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,arpitgupta,arpitgupta,arpitgupta,29/Nov/12 04:23,14/Oct/13 16:46,14/Mar/19 03:07,22/Jan/13 05:09,,,,,,,,0.12.0,,,,,,,0,hcatalog,,,,,,,,,,,,"Due to changes in hcatalog 0.5 packaging we need to update the hcatalog jar name and the path to the hbase storage handler jar. We also need to add the pig storage adapter to the class path

pig script should be updated to work with either version.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11/Dec/12 23:07;arpitgupta;PIG-3071.patch;https://issues.apache.org/jira/secure/attachment/12560472/PIG-3071.patch,11/Dec/12 22:37;arpitgupta;PIG-3071.patch;https://issues.apache.org/jira/secure/attachment/12560466/PIG-3071.patch,29/Nov/12 04:25;arpitgupta;PIG-3071.patch;https://issues.apache.org/jira/secure/attachment/12555314/PIG-3071.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-01-21 23:18:08.486,,,no_permission,,,,,,,,,,,,292721,,,,Wed Jan 23 02:31:08 UTC 2013,,,,,,,0|i0sbiv:,163343,,,,,,,,,,29/Nov/12 04:25;arpitgupta;attached is patch that takes a stab at fixing this,"11/Dec/12 22:37;arpitgupta;Updated patch to the jars being added for hbase storage handler, we now need to add all jars in the folder rather than just the hbase storage handler jar.","11/Dec/12 23:07;arpitgupta;updated patch just changed var names from hcatVersion to hcatJarPath to reflect what the var is actually used for.

Similar changes made for pig and hbase jar paths.","21/Jan/13 23:18;cheolsoo;+1.

Tested with HCat 0.5 branch. I will commit it unless anyone has an objection.",22/Jan/13 05:09;cheolsoo;Committed to trunk. Thanks Arpit!,"23/Jan/13 02:16;rvs;Guys, this is extremely useful! Question though: what's the recommended way of dealing with hive.metastore.uris for a permanent deployment? I am looking at the docs and it feels like that property should just be picked up from Hive configuration. Am I missing anything?",23/Jan/13 02:31;rohini;Yes it should be picked from hive-site.xml. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix TestPigRunner in trunk,PIG-3066,12617260,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,22/Nov/12 02:37,14/Oct/13 16:46,14/Mar/19 03:07,27/Dec/12 04:43,0.12.0,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"This is a regression from PIG-2924. The size of output in PigStats used to be off by 1 byte per store, but that was fixed by PIG-2924. However, two test cases in {{TestPigRunner}} wasn't corrected, so they fail in trunk.

The reason why the size of output in PigStats were off by 1 byte per store is because the size variable was initialized by -1 instead of 0.
{code}
long bytes = -1;
...
bytes += status.getLen();
...
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Nov/12 02:39;cheolsoo;PIG-3066.patch;https://issues.apache.org/jira/secure/attachment/12554629/PIG-3066.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-27 02:26:58.929,,,no_permission,,,,,,,,,,,,259467,Reviewed,,,Thu Dec 27 04:43:07 UTC 2012,,,,,,,0|i0lpcf:,124756,,,,,,,,,,22/Nov/12 02:39;cheolsoo;Attached is a patch that fixes test cases.,27/Dec/12 02:26;sms;The review is at https://reviews.apache.org/r/8313/,27/Dec/12 02:27;sms;+1,27/Dec/12 02:31;sms;Linking the JIRA to the review board URL,"27/Dec/12 04:43;cheolsoo;Thank you for the review, Santhosh.

I committed it to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLATTEN in nested foreach fails when the input contains an empty bag,PIG-3060,12616902,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,ywkim,ywkim,20/Nov/12 08:29,02/Apr/14 14:14,14/Mar/19 03:07,08/Apr/13 18:16,0.10.0,,,,,,,0.12.0,,,,impl,,,0,,,,,,,,,,,,,"FLATTEN inside a foreach statement produces wrong results, if the input contains an empty bag.
{code}
A = load 'flatten.txt' as (a0:int, a1:bag{(t:chararray)});
B = group A by a0;
C = foreach B {
  c1 = foreach A generate FLATTEN(a1);
  generate COUNT(c1);
};
{code}
The easy workaround is to filter out empty bags.
",,,,,,,,,,,,,,,,,,,,,,,,,PIG-3570,,,,,,,05/Apr/13 21:47;daijy;PIG-3060-1.patch;https://issues.apache.org/jira/secure/attachment/12577299/PIG-3060-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-20 23:37:10.455,,,no_permission,,,,,,,,,,,,258789,Reviewed,,,Mon Apr 08 18:16:12 UTC 2013,,,,,,,0|i0l3yn:,121292,,,,,,,,,,"20/Dec/12 23:37;arnabguin;

Not sure the issue has been fixed on the trunk between the filed date and current date. I tried out the example on the latest trunk. 

Here is my input file: (with empty bag)

flatten.txt:

2 {}
3 {}
4 {}

I essentially used the attached program with some minor modifications (like adding dump, load, store etc.). The number of bags is 0 as expected.

flatten.pig:

A = load './flatten.txt' using PigStorage(' ') as (a0:int, a1:bag{(t:chararray)});
B = group A by $0;
dump B;
C = foreach B {
    c1 = foreach A generate FLATTEN(a1);
    generate COUNT(c1);
};
dump B;
dump C;


shell> pig -x local -f flatten.pig 
(2,{(2,{})})
(3,{(3,{})})
(4,{(4,{})})
(0)
(0)
(0)

With another example where the bag is non-empty:

flatten.txt:
2 {(a),(b),(c)}
3 {(x),(y),(z)}

shell> pig -x local -f flatten.pig 
(2,{(2,{(a),(b),(c)})})
(3,{(3,{(x),(y),(z)})})
(3)
(3)

Did I get something wrong?

-Arnab","02/Jan/13 07:00;ywkim;The below is the sample input.
{code}
2 {}
2 {(x),(y),(z)}
{code}

When running the script in 0.10, I get
(0)

while the expected result is
(3)

When running the script in trunk, I get the error below.

org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing [PORelationToExprProject (Name: RelationToExpressionProject[bag][*] - scope-26 Operator Key: scope-26) children: null at []]: java.lang.NullPointerException
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:360)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.processInput(POUserFunc.java:228)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:282)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:416)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:349)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:376)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:282)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:465)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:433)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:413)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:257)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:260)
Caused by: java.lang.NullPointerException
	at org.apache.pig.data.DefaultAbstractBag.getMemorySize(DefaultAbstractBag.java:155)
	at org.apache.pig.data.DefaultAbstractBag.markSpillableIfNecessary(DefaultAbstractBag.java:100)
	at org.apache.pig.data.DefaultAbstractBag.add(DefaultAbstractBag.java:92)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:440)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.processInputBag(POProject.java:583)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PORelationToExprProject.getNext(PORelationToExprProject.java:107)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:335)
	... 14 more
","05/Apr/13 21:47;daijy;There could be several fixes. The most proper fix I feel is to fix the way we flatten a bag. If we run out of a bag, return EOP instead of NULL. Running tests.",07/Apr/13 17:03;daijy;All unit tests pass.,07/Apr/13 21:33;cheolsoo;+1.,08/Apr/13 18:16;daijy;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade junit to at least 4.8,PIG-3058,12616869,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,fang fang chen,fang fang chen,fang fang chen,20/Nov/12 01:00,22/Feb/13 04:54,14/Mar/19 03:07,03/Dec/12 03:09,0.11,,,,,,,0.11,0.12.0,,,build,,,0,,,,,,,,,,,,,"Pig needs to upgrade junit version to at least 4.8. Otherwise, one gets following warnings.

  [javadoc] org/apache/hadoop/hbase/mapreduce/TestWALPlayer.class(org/apache/hadoop/hbase/mapreduce:TestWALPlayer.class): warning: Cannot find annotation method 'value()' in type 'org.junit.experimental.categories.Category': class file for org.junit.experimental.categories.Category not found",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Nov/12 05:53;fang fang chen;PIG-3058.patch;https://issues.apache.org/jira/secure/attachment/12554635/PIG-3058.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-21 23:18:12.958,,,no_permission,,,,,,,,,,,,258742,,,,Mon Dec 03 03:09:03 UTC 2012,,,,,,,0|i0l2wn:,121121,,,,,,,,,,"20/Nov/12 01:02;fang fang chen;Can reproduce by following cmd.
command: ant -DPigPatchProcess= clean javadoc","21/Nov/12 05:57;fang fang chen;Hi Cheolsoo,

After upgrade junit to latest version 4.11. Only TestPigRunner failed with following information. But the failures are not contributed by junit upgrade. The same situation also happened at original trunk workspace(junit 4.5).
My environment: 
java version ""1.6.0_20""
ant version 1.8.2

Failed Testcases:
Testcase: simpleMultiQueryTest took 26.965 sec
        FAILED
expected:<28> but was:<30>
junit.framework.AssertionFailedError: expected:<28> but was:<30>
        at org.apache.pig.test.TestPigRunner.simpleMultiQueryTest(TestPigRunner.java:262)

Testcase: simpleMultiQueryTest2 took 39.002 sec
        FAILED
expected:<18> but was:<20>
junit.framework.AssertionFailedError: expected:<18> but was:<20>
        at org.apache.pig.test.TestPigRunner.simpleMultiQueryTest2(TestPigRunner.java:301)
","21/Nov/12 23:18;rding;This two failures were introduced by PIG-2924 which actually fixed a bug in JobStats class. But the corresponding errors in TestPigRunner didn't get fixed.

","22/Nov/12 00:40;cheolsoo;@Richard, thank you for pointing that out. I will get them fixed.

@Fangfang, I will run unit test with junit 4.11 and see if I get the same result as yours. Please free to submit the patch.
","22/Nov/12 02:31;fang fang chen;Hi Cheolsoo,

Attached the patch file.
Could you please help review the patch in PIG-3033?
With this fix, some javadoc warnings are removed. But there are still other 25 javadoc warnings left. 

Thanks",22/Nov/12 02:41;cheolsoo;I put up a patch for the failing test cases in PIG-3066.,"22/Nov/12 04:31;rohini;[~fang fang chen],
  You would also have to change junit version in .eclipse.templates/.classpath.",22/Nov/12 05:54;fang fang chen;Thanks Rohini. Updated the patch.,"30/Nov/12 14:58;cheolsoo;+1.

I ran the full unit test suite with hadoop 20/23 and don't see any additional test failures.",03/Dec/12 03:09;cheolsoo;Committed to trunk/0.11. Thanks Fangfang!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.IndexOutOfBoundsException  failure with LimitOptimizer + ColumnPruning,PIG-3051,12616267,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,15/Nov/12 17:21,22/Feb/13 04:53,14/Mar/19 03:07,27/Dec/12 22:12,0.10.0,0.11,,,,,,0.11,0.12.0,,,parser,,,0,,,,,,,,,,,,,"Had a user hitting 
""Caused by: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1"" error when he had multiple stores and limit in his code.

I couldn't reproduce this with short pig code (due to ColumnPruning somehow not happening when shortened), but here's a snippet. 

{noformat}
...
G3 = FOREACH G2 GENERATE sortCol, FLATTEN(group) as label, (long)COUNT(G1) as cnt;
G4 = ORDER G3 BY cnt DESC PARALLEL 25;
ONEROW = LIMIT G4 1;
U1 = FOREACH ONEROW GENERATE 3 as sortcol, 'somelabel' as label, cnt;
store U1 into 'u1' using PigStorage();
store G4 into 'g4' using PigStorage();
{noformat}

With '-t ColumnMapKeyPrune', job didn't hit the error.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Nov/12 18:10;knoguchi;pig-3051-v1-withouttest.txt;https://issues.apache.org/jira/secure/attachment/12553667/pig-3051-v1-withouttest.txt,26/Nov/12 22:50;knoguchi;pig-3051-v1.1-withe2etest.txt;https://issues.apache.org/jira/secure/attachment/12554929/pig-3051-v1.1-withe2etest.txt,20/Dec/12 17:27;knoguchi;pig-3051-v2.1-withe2etest.txt;https://issues.apache.org/jira/secure/attachment/12561941/pig-3051-v2.1-withe2etest.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-11-15 17:28:27.096,,,no_permission,,,,,,,,,,,,258025,,,,Thu Dec 27 22:12:46 UTC 2012,,,,,,,0|i0km1z:,118390,,,,,,,,,,"15/Nov/12 17:23;knoguchi;Stack dump from task failure using trunk
{noformat}
2012-11-09 22:45:09,484 WARN [main] org.apache.hadoop.mapred.YarnChild:
Exception running child : org.apache.pig.backend.executionengine.ExecException:
ERROR 0: Error while executing ForEach at []
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:306)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:308)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLimit.getNext(POLimit.java:117)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:308)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNext(POLocalRearrange.java:263)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.processOnePackageOutput(PigCombiner.java:185)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.reduce(PigCombiner.java:163)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.reduce(PigCombiner.java:51)
    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:170) at org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1615)
    at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1567)
    at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1416)
    at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:663)
    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:730)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)
    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:157)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:396)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1212)
    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:152)
Caused by: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
    at java.util.ArrayList.RangeCheck(ArrayList.java:547)
    at java.util.ArrayList.get(ArrayList.java:322)
    at org.apache.pig.data.DefaultTuple.get(DefaultTuple.java:116)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage.getValueTuple(POPackage.java:345)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackageLite.getValueTuple(POPackageLite.java:198)
    at org.apache.pig.data.ReadOnceBag$ReadOnceBagIterator.next(ReadOnceBag.java:241)
    at org.apache.pig.data.ReadOnceBag$ReadOnceBagIterator.next(ReadOnceBag.java:222)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:440)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:298)
    ... 19 more
{noformat}","15/Nov/12 17:28;billgraham;We've seen similar exceptions when loading data that contains text with the column delimiter in it, which produces shorter than expected tuples. Could that be the case here?","15/Nov/12 17:48;knoguchi;Tracking the LogicalPlan change.

Original.
{noformat}
U1: (Name: LOStore Schema: sortCol#1871:int,label#1872:chararray,cnt#1870:long)
|
|---U1: (Name: LOForEach Schema: sortCol#1871:int,label#1872:chararray,cnt#1870:long)
    |   |
    |   (Name: LOGenerate[false,false,false] Schema: sortCol#1871:int,label#1872:chararray,cnt#1870:long)
    |   |   |
    |   |   (Name: Constant Type: int Uid: 1871)
    |   |   |
    |   |   (Name: Constant Type: chararray Uid: 1872)
    |   |   |
    |   |   cnt:(Name: Project Type: long Uid: 1870 Input: 0 Column: (*))
    |   |
    |   |---(Name: LOInnerLoad[2] Schema: cnt#1870:long)
    |   *****HERE*****
    |---ONEROW: (Name: LOLimit Schema: sortCol#1868:int,label#1869:chararray,cnt#1870:long)
        |
        |---G4: (Name: LOSplitOutput Schema: sortCol#1868:int,label#1869:chararray,cnt#1870:long)
            |   |
            |   (Name: Constant Type: boolean Uid: 1867)
            |
            |---G4: (Name: LOSplit Schema: sortCol#1864:int,label#1857:chararray,cnt#1865:long)
                |
                |---G4: (Name: LOSort Schema: sortCol#1864:int,label#1857:chararray,cnt#1865:long)
                    |   |
                    |   cnt:(Name: Project Type: long Uid: 1865 Input: 0 Column: 2)
                    |
                    |---G3: (Name: LOForEach Schema: sortCol#1864:int,label#1857:chararray,cnt#1865:long)
                        |   |
{noformat}

After org.apache.pig.newplan.logical.rules.LimitOptimizer

{noformat}
U1: (Name: LOStore Schema: sortCol#1871:int,label#1872:chararray,cnt#1870:long)
|
|---U1: (Name: LOForEach Schema: sortCol#1871:int,label#1872:chararray,cnt#1870:long)
    |   |
    |   (Name: LOGenerate[false,false,false] Schema: sortCol#1871:int,label#1872:chararray,cnt#1870:long)
    |   |   |
    |   |   (Name: Constant Type: int Uid: 1871)
    |   |   |
    |   |   (Name: Constant Type: chararray Uid: 1872)
    |   |   |
    |   |   cnt:(Name: Project Type: long Uid: 1870 Input: 0 Column: (*))
    |   |
    |   |---(Name: LOInnerLoad[2] Schema: cnt#1870:long)
    |
    |---(Name: LOSort Schema: sortCol#1868:int,label#1869:chararray,cnt#1870:long)
        |   | *****HERE*****
        |   cnt:(Name: Project Type: long Uid: 1865 Input: 0 Column: 2)
        |
        |---G4: (Name: LOSplitOutput Schema: sortCol#1868:int,label#1869:chararray,cnt#1870:long)
            |   |
            |   (Name: Constant Type: boolean Uid: 1867)
            |
            |---G4: (Name: LOSplit Schema: sortCol#1864:int,label#1857:chararray,cnt#1865:long)
                |
                |---G4: (Name: LOSort Schema: sortCol#1864:int,label#1857:chararray,cnt#1865:long)
                    |   |
                    |   cnt:(Name: Project Type: long Uid: 1865 Input: 0 Column: 2)
                    |
                    |---G3: (Name: LOForEach Schema: sortCol#1864:int,label#1857:chararray,cnt#1865:long)

{noformat}

After org.apache.pig.newplan.logical.rules.ColumnMapKeyPrune,
{noformat}
U1: (Name: LOStore Schema: sortCol#1871:int,label#1872:chararray,cnt#1870:long)
|
|---U1: (Name: LOForEach Schema: sortCol#1871:int,label#1872:chararray,cnt#1870:long)
    |   |
    |   (Name: LOGenerate[false,false,false] Schema: sortCol#1871:int,label#1872:chararray,cnt#1870:long)
    |   |   |
    |   |   (Name: Constant Type: int Uid: 1871)
    |   |   |
    |   |   (Name: Constant Type: chararray Uid: 1872)
    |   |   |
    |   |   cnt:(Name: Project Type: long Uid: 1870 Input: 0 Column: (*))
    |   |
    |   |---(Name: LOInnerLoad[2] Schema: cnt#1870:long)
    |
    |---(Name: LOSort Schema: sortCol#1868:int,label#1869:chararray,cnt#1870:long)
        |   | 
        |   cnt:(Name: Project Type: long Uid: 1865 Input: 0 Column: 2)
        |
        |---G4: (Name: LOSplitOutput Schema: sortCol#1868:int,label#1869:chararray,cnt#1870:long)
            |   |
            |   (Name: Constant Type: boolean Uid: 1867)
            |
            |---G4: (Name: LOSplit Schema: sortCol#1864:int,label#1857:chararray,cnt#1865:long)
                |
                |---G4: (Name: LOSort Schema: sortCol#1864:int,label#1857:chararray,cnt#1865:long)
                    |   |
                    |   cnt:(Name: Project Type: long Uid: 1865 Input: 0 Column: 2)
                    |
                    |---G3: (Name: LOForEach Schema: sortCol#1864:int,label#1857:chararray,cnt#1865:long)
{noformat}","15/Nov/12 17:51;knoguchi;Sorry, After ColumnMapKeyPrune, I pasted the wrong one. Here's the one after Pruning.

{noformat}
U1: (Name: LOStore Schema: sortCol#1871:int,label#1872:chararray,cnt#1870:long)ColumnPrune:InputUids=[1870, 1871, 1872]ColumnPrune:OutputUids=[1870, 1871, 1872]
|
|---U1: (Name: LOForEach Schema: sortCol#1871:int,label#1872:chararray,cnt#1870:long)ColumnPrune:InputUids=[1870]ColumnPrune:OutputUids=[1870, 1871, 1872]
    |   |
    |   (Name: LOGenerate[false,false,false] Schema: sortCol#1871:int,label#1872:chararray,cnt#1870:long)ColumnPrune:InputUids=[1870]ColumnPrune:OutputUids=[1870, 1871, 1872]
    |   |   |
    |   |   (Name: Constant Type: int Uid: 1871)
    |   |   |
    |   |   (Name: Constant Type: chararray Uid: 1872)
    |   |   |
    |   |   cnt:(Name: Project Type: long Uid: 1870 Input: 0 Column: (*))
    |   |
    |   |---(Name: LOInnerLoad[0] Schema: cnt#1870:long)
    |
    |---(Name: LOSort Schema: cnt#1870:long)ColumnPrune:InputUids=[1865, 1870]ColumnPrune:OutputUids=[1870]
        |   |  *****HERE*****
        |   cnt:(Name: Project Type: long Uid: 1865 Input: 0 Column: ***2***)
        |
        |---G4: (Name: LOSplitOutput Schema: cnt#1870:long)ColumnPrune:InputUids=[1865]ColumnPrune:OutputUids=[1870]
            |   |
            |   (Name: Constant Type: boolean Uid: 1867)
            |
            |---(Name: LOForEach Schema: cnt#1865:long)
                |   |
                |   (Name: LOGenerate[false] Schema: cnt#1865:long)
                |   |   |
                |   |   cnt:(Name: Project Type: long Uid: 1865 Input: 0 Column: (*))
                |   |
                |   |---(Name: LOInnerLoad[2] Schema: cnt#1865:long)
                |
                |---G4: (Name: LOSplit Schema: sortCol#1864:int,label#1857:chararray,cnt#1865:long)ColumnPrune:InputUids=[1864, 1865, 1857]ColumnPrune:OutputUids=[1864, 1865, 1857]
                    |
                    |---G4: (Name: LOSort Schema: sortCol#1864:int,label#1857:chararray,cnt#1865:long)ColumnPrune:InputUids=[1864, 1865, 1857]ColumnPrune:OutputUids=[1864, 1865, 1857]
                        |   |
                        |   cnt:(Name: Project Type: long Uid: 1865 Input: 0 Column: 2)
                        |
                        |---G3: (Name: LOForEach Schema: sortCol#1864:int,label#1857:chararray,cnt#1865:long)ColumnPrune:InputUids=[1857, 1862]ColumnPrune:OutputUids=[1864, 1865, 1857]

{noformat}

So I believe the new LOSort introduced by the LimitOptimizer has the projection pointing to the previous LOSOrt which breaks when columns are pruned and column index is not being updated.","15/Nov/12 18:10;knoguchi;There must be a better way/method than this but I hope the patch shows what I'm thinking.

I'm trying to redirect all the ProjectExpression inside the newSort to point to the newSort instead of the original Sort.

After the patch, ColumnPruning successfully updates the column to 
{noformat}
    |---(Name: LOSort Schema: cnt#1870:long)ColumnPrune:InputUids=[1870]ColumnPrune:OutputUids=[1870]
        |   |
        |   cnt:(Name: Project Type: long Uid: 1870 Input: 0 Column: 0)
{noformat}","15/Nov/12 18:14;knoguchi;Sorry, I missed Bill's comment.

bq. We've seen similar exceptions when loading data that contains text with the column delimiter in it, which produces shorter than expected tuples. Could that be the case here?

I don't think that's the case here.  However, I've made many silly mistakes&misunderstandings in pig before.  Let me double check.","17/Nov/12 04:20;knoguchi;bq. I couldn't reproduce this with short pig code (due to ColumnPruning somehow not happening when shortened), 

Learned that columnprune does not kick in unless there is column-or-map to prune inside load. (even though columnprune does more than just pruning at the load part.)

By adding one extra line to force columnpruning, i was able to reproduce this issue.  First example hitting IndexOutOfBoundsException and second one producing incorrect result.

{noformat}
% cat test/pig-3051-1.pig 
A = load 'a.txt' using PigStorage() as (a1:chararray, a2:chararray, a3:chararray, a4:chararray);
B = foreach A generate a2,a3,a4;  --to force columnprune algo to cover
G = order B by a4;
U1 = limit G 3;
U2 = foreach U1 generate a4;
store G into 'g' using PigStorage();
store U2 into 'u2' using PigStorage(); 
% cat a.txt
1       2       3       4
2       3       4       1
3       4       1       2
4       1       2       3
% pig -x local test/pig-3051-1.pig 
...
fails with Caused by: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
{noformat}

Now adding extra 2 columns, job finishes but result incorrect.

{noformat}
% cat test/pig-3051-2.pig 
A = load 'b.txt' using PigStorage() as (a1:chararray, a2:chararray, a3:chararray, a4:chararray, a5:chararray, a6:chararray);
B = foreach A generate a2,a3,a4,a5,a6;  --to force columnprune algo to cover
G = order B by a4;
U1 = limit G 4;
U2 = foreach U1 generate a4,a5,a6;
store G into 'g' using PigStorage();
store U2 into 'u2' using PigStorage(); 
% cat b.txt 
1       2       3       4       5       6
2       3       4       5       6       1
3       4       5       6       1       2
4       5       6       1       2       3
5       6       1       2       3       4
6       1       2       3       4       5
% pig -x local test/pig-3051-2.pig 
...
success
% cat u2/part-r-00000 
5       6       1
6       1       2
1       2       3
2       3       4
{noformat}

And last, taking out store G (to take out LOSplit).  This produces a correct output.
{noformat}
% cat test/pig-3051-3.pig A = load 'b.txt' using PigStorage() as (a1:chararray, a2:chararray, a3:chararray, a4:chararray, a5:chararray, a6:chararray);
B = foreach A generate a2,a3,a4,a5,a6;  --to force columnprune algo to cover
G = order B by a4;
U1 = limit G 4;
U2 = foreach U1 generate a4,a5,a6;
--store G into 'g' using PigStorage();
store U2 into 'u2' using PigStorage(); 

% pig -x local test/pig-3051-3.pig 
... Success.

% cat u2/part-r-00000 
1       2       3
2       3       4
3       4       5
4       5       6
% 
{noformat}

Also tested the patch(pig-3051-v1-withouttest.txt) and it does fix the incorrect result case.","26/Nov/12 22:50;knoguchi;Added an e2e test since I needed columnprune + limitoptimizer.  Let me know if unit test is better for this.

Also, appreciate if someone can take a look at my fix since this issue can lead to incorrect output.","26/Nov/12 22:55;knoguchi;Requesting for 0.11 if possible. (it's not a regression in 0.10&0.11 but result can get incorrect due to this.)
","17/Dec/12 16:11;rohini;  Resetting the attached LOSort operator of the ProjectExpression to the newSort is good. But found an issue with the copy not setting the label, type and Uid.  
{code}
@Test
    public void testPIG3051() throws Exception {
        String[] input = {
                ""1,2,3,4"",
                ""2,3,4,1"",
                ""3,4,1,2"",
                ""4,1,2,3""
        };

        Util.createLocalInputFile( ""a.txt"", input);
        String query = ""A =load 'a.txt' using PigStorage(',') as (a1:chararray, a2:chararray, a3:chararray, a4:chararray);"" +
                ""B = foreach A generate a2,a3,a4;"" +
                ""G = order B by a4;"" +
                ""U1 = limit G 3;"" +
                ""U2 = foreach U1 generate a4;"" +
                ""store G into 'g' using PigStorage();"" +
                ""store U2 into 'u2' using PigStorage(); "";
        try {
            PigServer pigServer = new PigServer(ExecType.LOCAL);
            pigServer.registerQuery(query);
        } catch (Exception e) {
            e.printStackTrace();
        }
        
    }
{code}
sort.mSortColPlans - a4:(Name: Project Type: chararray Uid: 4 Input: 0 Column: 2)
newSort.mSortColPlans - (Name: Project Type: null Uid: null Input: 0 Column: 2)
{code}

{code}","20/Dec/12 17:27;knoguchi;Thanks Rohini for the review. 

bq .But found an issue with the copy not setting the label, type and Uid.

I wasn't sure why my test worked even when the above fields were not set. Turns out that they are filled by the SchemaPatcher.

LimitOptimizer.reportChanges() simply returns currentPlan, so SchemaPatcher goes through the entire currentPlan including the newSort.mSortColPlans mentioned and update them accordingly.

BTW, reading back my patch, I felt that logic of making a new copy of LOSort should be kept inside LOSort.java.  Uploading a new version.  Logic is same from the previous patch.","27/Dec/12 22:12;rohini;+1. Ran the unit tests, new e2e test and committed to branch 0.11 and trunk. Thanks Koji. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix FindBugs multithreading warnings,PIG-3050,12616178,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,15/Nov/12 01:58,14/Oct/13 16:46,14/Mar/19 03:07,27/Dec/12 04:53,0.11,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"There was a race condition reported when running Pig in local mode on the user mailing list. This motivated me to fix potential multithreading bugs that can be identified by FindBugs.

FindBugs identifies the following potential bugs:
# Mutable static field
# Inconsistent synchronization
# Incorrect lazy initialization of static field
# Incorrect lazy initialization and update of static field
# Unsynchronized get method, synchronized set method

There are in total 1153 warnings that FindBugs complains, but they're outside of the scope of this jira.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Dec/12 19:01;cheolsoo;PIG-3050.patch;https://issues.apache.org/jira/secure/attachment/12561332/PIG-3050.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-15 20:17:37.574,,,no_permission,,,,,,,,,,,,257927,,,,Thu Dec 27 04:53:27 UTC 2012,,,,,,,0|i0kkwv:,118205,,,,,,,,,,"15/Nov/12 20:17;jcoveney;For context, when you say multithreading do you mean running two Pig jobs in the same VM, or something else?","15/Nov/12 21:47;cheolsoo;Hi Jonathan,

There are two things:
# Single Pig job in local mode (LocalJobRunner).
# Multiple Pig jobs in the same VM as you mentioned.

The direct motivation for this ticket is (1), but I am doing some experiments with (2) where I execute multiple jobs in a single VM.

Thanks!","17/Dec/12 19:01;cheolsoo;Attached is a patch that fixes the following issues:
- Mutual static field
{code:title=PhysicalOperator.java}
public static PigProgressable reporter;
{code}
There was a reported race condition due to this static field (For details, see [here|http://search-hadoop.com/m/2OdLNRMwXa2/Intermittent+NullPointerException&subj=Intermittent+NullPointerException]). Since {{reporter}} should be local to thread, I converted it to ThreadLocal.
- Inconsistent synchronization
{code:title=POStream.java}
public Result getNext(Tuple t) throws ExecException {
    ...
    if(initialized) {
       ...
    }
    ...
}
...
public Result getNextHelper(Tuple t) throws ExecException {
    ...
    synchronized(this) {
       ...
       if(!initialized) {
          ...
       }
       ...
       initialized = true;
       ...
    }
}
{code}
Synchronized access to {{initialized}} is performed inside {{getNextHelper()}}, but unsynchronized access was performed inside {{getNext()}}. I added a synchronized getter method and used that method inside {{getNext()}}.
- Incorrect lazy initialization of static field
{code:title=SpillableMemoryManager.java}    
public static SpillableMemoryManager getInstance() {
    if (manager == null) {
        manager = new SpillableMemoryManager();
    }
    return manager;
}
{code}
FindBugs says, ""Because the compiler may reorder instructions, threads are not guaranteed to see a completely initialized object if the method can be called by multiple threads."" So I declared {{manager}} as volatile.
- Incorrect lazy initialization and update of static field
{code:title=SchemaTupleBackend.java}
public static void initialize(Configuration jConf, PigContext pigContext, boolean isLocal) throws IOException {
    if (stb != null) {
        LOG.warn(""SchemaTupleBackend has already been initialized"");
    } else {
        SchemaTupleFrontend.lazyReset(pigContext);
        SchemaTupleFrontend.reset();
        stb = new SchemaTupleBackend(jConf, isLocal);
        stb.copyAndResolve();
    }
}
{code}
FindBugs says, ""After the field is set, the object stored into that location is further updated. The setting of the field is visible to other threads as soon as it is set. If further accesses in the method that set the field serve to initialize the object, then you have a very serious multithreading bug."" So I moved the assignment to the end of the method after all initialization is done.
- Unsynchronized get method, synchronized set method
{code:title=PigHadoopLogger.java}
public synchronized void setReporter(PigStatusReporter rep) {
    this.reporter = rep;
}
public boolean getAggregate() {
    return aggregate;
}
{code}
I made {{getAggregate()}} synchronized.","27/Dec/12 04:53;cheolsoo;Thank you for the review in the RB, Santhosh!
(https://reviews.apache.org/r/8649/)

I committed it to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
An empty file name in -Dpig.additional.jars throws an error,PIG-3046,12615922,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,prkommireddi,cheolsoo,cheolsoo,13/Nov/12 17:14,04/Jan/17 09:02,14/Mar/19 03:07,14/Nov/12 17:55,0.10.0,0.11,,,,,,0.12.0,,,,,,,0,newbie,,,,,,,,,,,,"This issue was raised on the user mailing list. To reproduce it, please run the following command in MR mode:
{code}
pig -Dpig.additional.jars=<jar1>::<jar2> <pig script>
{code}
As can be seen, I put {{::}} in the middle of {{-Dpig.additional.jars}}, and this causes the following error:
{code}
Caused by: java.lang.IllegalArgumentException: Can not create a Path from
an empty string at org.apache.hadoop.fs.Path.checkPathArg(Path.java:82)
at org.apache.hadoop.fs.Path.<init>(Path.java:90)
at org.apache.hadoop.fs.Path.<init>(Path.java:45)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.shipToHDFS(JobControlCompiler.java:1455)
at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.putJarOnClassPathThroughDistributedCache(JobControlCompiler.java:1432)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJob(JobControlCompiler.java:508)
{code}
Although it's not too hard to see what's wrong, it's not always easy to track down where an empty file name is from. In particular if various environment variables are set in another start-up script, it's time-consuming to identify the root cause.

In fact, Pig should just skip an empty file name instead attempts to convert it to a path and throws an exception like this.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Nov/12 22:18;prkommireddi;PIG-3046.patch;https://issues.apache.org/jira/secure/attachment/12553387/PIG-3046.patch,13/Nov/12 22:30;prkommireddi;PIG-3046_1.patch;https://issues.apache.org/jira/secure/attachment/12553389/PIG-3046_1.patch,13/Nov/12 22:48;dreambird;PIG-3046_2.patch;https://issues.apache.org/jira/secure/attachment/12553397/PIG-3046_2.patch,14/Nov/12 07:20;prkommireddi;PIG-3046_3.patch;https://issues.apache.org/jira/secure/attachment/12553467/PIG-3046_3.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-11-13 17:32:38.044,,,no_permission,,,,,,,,,,,,257502,,,,Wed Jan 04 09:02:21 UTC 2017,,,Patch Available,,,,0|i0k2vj:,115283,,,,,,,,,,13/Nov/12 17:32;mcz;Also this issue occurs whenever you specify in the -Dpig.additional.jars a directory path instead of the file path. This is quite often happening because its advised on forums to include HIVE_HOME and HADOOP_HOME in the PIG_CLASSPATH which is then passed to  -Dpig.additional.jars.,"13/Nov/12 19:56;prkommireddi;Should pig.additional.jars support globbing, something like 
{code}
-Dpig.additonal.jars=/home/pig/jars/*.jar
{code}

Or we rather check for jars within if the path is a directory?
I think Pig already supports globbing via REGISTER.

I am not sure why we need HIVE_HOME or HADOOP_HOME to be shipped to HDFS, those confs are required only on the client?","13/Nov/12 20:04;cheolsoo;Hi Prashant,

You're probably right that you can workaround this problem in many ways. I opened this jira because it's trivial to skip an empty string when handling {{pig.additional.jars}} in Pig, and that seems to improve usability. Let me know if you think otherwise.

Thanks!","13/Nov/12 20:31;prkommireddi;Hi Cheolsoo,

How do you feel about making the quick easy fix on this ticket (I can add a patch soon). And open a new one for adding globbing support?

",13/Nov/12 21:16;cheolsoo;Sure!,13/Nov/12 22:18;prkommireddi;Patch contains a fix for empty jar path. This does not handle the case when a user specifies a directory instead of a jar file. We will handle that in a separate JIRA where all jars within a dir can be loaded.,"13/Nov/12 22:27;dreambird;[~prkommireddi], not insist on it, but I think it might be better it is WARN instead of INFO",13/Nov/12 22:30;prkommireddi;You are right!,"13/Nov/12 22:36;dreambird;also, the patch seems also impact the common REGISTER jar function. I think in registerJar(String name), which already handle this to some extend
{noformat}
......
if (!f.canRead()) {
                    int errCode = 4002;
                    String msg = ""Can't read jar file: "" + name;
                    throw new FrontendException(msg, errCode, PigException.USER_ENVIRONMENT);
                  }
......
{noformat}

our fix should be specifically for -Dpig.additional.jars, should we handle this before it goes to function registerJar(String name) ?",13/Nov/12 22:48;dreambird;this patch handles it in function addJarsFromProperties(),"13/Nov/12 22:50;dreambird;actually I think even PIG-3046_2.patch is not proper enough. I am working on another patch, thanks.","13/Nov/12 22:59;prkommireddi;registerJar is the common place where jars are added. It does not make a lot of sense to do it specifically for pig.additional.jars only. A call PigServer.registerJar("""") is valid and making a change to addJarsFromProperties only would not handle that.

Also, registerJar(String name) does not handle the case as you have mentioned in the snippet above. If you take a look at the following 
{code}
URL resource = locateJarFromResources(name);
{code}

You will notice that ""resource"" in case of an empty string is not null and if(!f.canRead) loop never gets invoked.","14/Nov/12 06:45;cheolsoo;+1 for Prashant's patch (PIG-3046_1.patch).

I will commit it after running tests.

@Johnny,
I think that you misunderstood something. Let me know if you have any questions/concerns.","14/Nov/12 07:02;cheolsoo;@Prashant,
Can you make a very small change to your patch? Can you please move your code to after {{if(name != null)}} in {{registerJar(String name)}}. Since {{registerJar(String name)}} is a public method, I think that it's possible for somebody to call it with a null. Then, a NullPointerException will be thrown.

Also, is there any reason why you didn't use {{name.isEmpty()}}?

Thanks!","14/Nov/12 07:20;prkommireddi;Thanks for the review, Cheolsoo. Added a new patch.",14/Nov/12 17:55;cheolsoo;Committed to trunk. Thanks Prashant!,"29/Dec/16 10:03;AsmaData;Hi,
I have the same problem. And i hope find your help. I'm blocked here.
I use the Talend Open Studion for Big Data 6.3.0 with the Cloudera CDH 5.8.
What i must do in my case?
Thank you",03/Jan/17 21:31;daijy;Most probably you don't have this patch in CDH. The best option is to patch by yourself and replace relevant jars in CDH.,"04/Jan/17 08:54;szita;[~AsmaData],
CDH 5.8 has Pig 0.12.0 so this patch should be included.
I also ran a small test to see this in action, and the WARN message does come up:
{code}
-bash-4.1$ pig -Dpig.additional.jars=:test1.jar:test2.jar

log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
2017-01-04 00:50:32,904 [main] INFO  org.apache.pig.Main - Apache Pig version 0.12.0-cdh5.8.4-SNAPSHOT (rexported) compiled Jan 03 2017, 01:36:52
2017-01-04 00:50:32,905 [main] INFO  org.apache.pig.Main - Logging error messages to: /var/lib/hadoop-hdfs/pig_1483519832863.log
2017-01-04 00:50:32,950 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /var/lib/hadoop-hdfs/.pigbootup not found
2017-01-04 00:50:34,104 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2017-01-04 00:50:34,104 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
2017-01-04 00:50:34,104 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://ns1
2017-01-04 00:50:36,160 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
2017-01-04 00:50:36,170 [main] WARN  org.apache.pig.PigServer - Empty string specified for jar path
{code}","04/Jan/17 09:02;AsmaData;Thank's for [~daijy] and [~szita], 
i'll try with another example of project and i'll send my result.

Thank for all",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Specifying sorting field(s) at nightly.conf - fix sortArgs,PIG-3045,12615809,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,esoren,esoren,12/Nov/12 21:57,22/Feb/13 04:53,14/Mar/19 03:07,14/Nov/12 06:22,0.11,,,,,,,0.11,0.12.0,,,e2e harness,,,0,test,,,,,,,,,,,,"PIG-2782 fixed a number of tests where the parameters passed to the verification sort was incorrect.
However, there are still problems with the patch in PIG-2782 where it introduced new errors while checking. E.g. the pig script sorts on column one and two, but the verification only checks that output is sorted on column one.

",,,,,,,,,,,,,PIG-2782,,,,,,,,,,,,,,,,,,,14/Nov/12 00:56;rohini;PIG-3045.patch;https://issues.apache.org/jira/secure/attachment/12553415/PIG-3045.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-13 19:58:24.205,,,no_permission,,,,,,,,,,,,257288,,,,Wed Nov 14 17:46:00 UTC 2012,,,,,,,0|i0jxjb:,114363,,,,,,,,,,"13/Nov/12 19:58;cheolsoo;Hi Egil,

I think that you're totally right. Do you want to put up a patch for this? https://cwiki.apache.org/confluence/display/PIG/HowToContribute#HowToContribute-Creatingapatch

I am asking because you already provided the fix so deserve a credit, but please let me know if you don't have time.

Thanks!","13/Nov/12 20:05;rohini;Moving the description from Egil outlining the issues to Comment section as it is too long. The line numbers in the diff come from applying the patch of PIG-2782 to branch-0.10. 


For file test/e2e/pig/tests/nightly.conf:
===================

@@ -1728,7 +1728,7 @@
                                'pig' =>q\a = load
':INPATH:/singlefile/studentnulltab10k' as (name:chararray, age:int,
gpa:double);
 b = order a by name, age, gpa;
 store b into ':OUTPATH:';\,
-                'sortArgs' => ['-t', ' ', '+0', '-1', '+1n', '-2'],
+                'sortArgs' => ['-t', ' ', '-k', '1,1', '-k', '2n,2n'],
                        },

Should have been: 
+                'sortArgs' => ['-t', ' ', '-k', '1,1', '-k', '2n,3n'],

===================

Similar

@@ -1736,7 +1736,7 @@
                                'pig' =>q\a = load
':INPATH:/singlefile/studentnulltab10k' as (name:chararray, age:int,
gpa:double);
 b = order a by name desc, age desc, gpa desc;
 store b into ':OUTPATH:';\,
-                'sortArgs' => ['-t', ' ', '+0r', '-1', '+1nr', '-2'],
+                'sortArgs' => ['-t', ' ', '-k', '1r,1r', '-k', '2nr,2nr'],
                        },


Should have been: 
+                'sortArgs' => ['-t', ' ', '-k', '1r,1r', '-k', '2nr,3nr'],

===================

and 

@@ -1752,7 +1752,7 @@
                                'pig' =>q\a = load
':INPATH:/singlefile/studentnulltab10k' as (name, age:long, gpa:float);
 b = order a by name desc, age desc, gpa desc;
 store b into ':OUTPATH:';\,
-                'sortArgs' => ['-t', ' ', '+0r', '-1', '+1nr', '-2'],
+                'sortArgs' => ['-t', ' ', '-k', '1r,1r', '-k', '2nr,2nr'],
                        },

Should have been: 
+                'sortArgs' => ['-t', ' ', '-k', '1r,1r', '-k', '2nr,3nr'],

===================

@@ -1847,7 +1847,7 @@
                                'pig' => q\a = load
':INPATH:/singlefile/studentnulltab10k' as (name:chararray, age:int,
gpa:double);
 b = order a by *;
 store b into ':OUTPATH:';\,
-                'sortArgs' => ['-t', ' ', '+0', '-1', '+1n', '-2'],
+                'sortArgs' => ['-t', ' ', '-k', '1,1', '-k', '2n,2n'],
                        },

Should have been: 
+                'sortArgs' => ['-t', ' ', '-k', '1,1', '-k', '2n,3n'],

===================

@@ -1855,7 +1855,7 @@
                                'pig' => q\a = load
':INPATH:/singlefile/studentnulltab10k' as (name:chararray, age:int,
gpa:double);
 b = order a by * desc;
 store b into ':OUTPATH:';\,
-                'sortArgs' => ['-t', ' ', '+0r', '-1', '+1nr', '-2'],
+                'sortArgs' => ['-t', ' ', '-k', '1r,1r', '-k', '2nr,2nr'],
                        },

Should have been: 
+                'sortArgs' => ['-t', ' ', '-k', '1,1', '-k', '2nr,3nr'],

===================

@@ -1943,7 +1943,7 @@
 c = filter b by $0 > 'a'; -- break the sort/limit optimization
 d = limit c 100;
 store d into ':OUTPATH:';\,
-               'sortArgs' => ['-t', '  ', '+0', '-1'],
+               'sortArgs' => ['-t', '  ', '-k', '1,1'],

Should have been: 
+               'sortArgs' => ['-t', '  ', '-k', '1,2'],

===================

@@ -1952,7 +1952,7 @@
 b = order a by $0, $1;
 c = limit b 100;
 store c into ':OUTPATH:';\,
-               'sortArgs' => ['-t', '  ', '+0', '-1'],
+               'sortArgs' => ['-t', '  ', '-k', '1,1'],

Should have been: 
+               'sortArgs' => ['-t', '  ', '-k', '1,2'],

===================

@@ -2222,7 +2222,7 @@
 D = order B by age, extra;
 store D into ':OUTPATH:';\,

-                       'sortArgs' => ['-t', '  ', '+1n', '-2'],
+                       'sortArgs' => ['-t', '  ', '-k', '2n,2n'],
                        },

Should have been: 
+                       'sortArgs' => ['-t', '  ', '-k', '2n,2n', '-k', '4,4'],

(This last is decidedly minor, as the 'extra' column is empty, but for sake of consistency...) 

  ","14/Nov/12 00:56;rohini;Ran the e2e tests with -Dtests.to.run=""-t Types -t Limit -t MissingColumns"" to test the attached patch. ",14/Nov/12 05:26;cheolsoo;+1. I will commit it after running the tests.,14/Nov/12 06:22;cheolsoo;Committed to trunk/0.11. Thanks Rohini!,"14/Nov/12 17:46;esoren;Thanks, Rohini.
And, thanks Cheolsoo, for you consideration in #comment-13496489; It is just fine that Rohini took this one, as it was her who got me looking at these tests in the first place. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not possible to use custom version of jackson jars,PIG-3039,12615217,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,07/Nov/12 19:44,22/Feb/13 04:53,14/Mar/19 03:07,19/Nov/12 12:02,0.10.0,,,,,,,0.11,0.12.0,,,,,,0,,,,,,,,,,,,,"User is trying

register jackson_core_asl-1.9.4_1.jar;
register jackson_mapper_asl-1.9.4_1.jar;
register jackson_xc-1.9.4_1.jar;

But pig.jar/pig-withouthadoop.jar has jackson jars and JarManager packages the jackson from pig.jar into job.jar(PIG-2457). We could not find any possible workaround with mapreduce framework to put the user jar first in the classpath as job.jar always takes precedence.

 The pig script works fine with 0.9 and is a regression in 0.10.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Nov/12 15:00;cheolsoo;PIG-3039-download-jackson.patch;https://issues.apache.org/jira/secure/attachment/12553121/PIG-3039-download-jackson.patch,09/Nov/12 02:35;rohini;PIG-3039-trunk.patch;https://issues.apache.org/jira/secure/attachment/12552760/PIG-3039-trunk.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-11-09 21:22:15.76,,,no_permission,,,,,,,,,,,,255816,,,,Mon Nov 19 12:02:29 UTC 2012,,,,,,,0|i0fv3r:,90637,,,,,,,,,,"09/Nov/12 02:35;rohini;Attached patch fixes the scenario. Patch does not include jackson jars that need to be added to test/resources directory.

{noformat}
ls test/resources/
jackson-core-asl-1.9.9.jar	jackson-mapper-asl-1.9.9.jar	org
{noformat} 

[~alangates],
   The test case I added needs a specific version of jackson jars in the test/resources folder. Just wanted to check with you if there is any restriction to checking in jars like that.",09/Nov/12 02:36;rohini;  I would also like to include this in 0.11 if others agree as this is a regression from 0.9. ,"09/Nov/12 21:22;julienledem;Could you explain how it was working before ? Were the classes from the registered jar added to the same job jar and overwriting the pig.jar classes?
","09/Nov/12 21:25;rohini;You mean how it works in 0.9? It is simple. Before PIG-2457, jackson was not packaged as part of pig.jar. So users could register their version of jackson jar in pig with mapreduce.user.classpath.first=true to avoid hadoop's jackson jars being in classpath.  ",09/Nov/12 21:29;julienledem;I see. Thanks!,"12/Nov/12 15:00;cheolsoo;Hi Rohini,

The patch looks good. My only concern is that we have to remember this test case when we bump jackson to 1.9.9 in future.

I suggest that we should at least make a comment in ivy/libraries.properties regarding this test case so that we won't forget to update this test case when we update the version of jackson.

In addition, wouldn't it better to download the jackson 1.9.9 binaries using ant instead of checking them in? I made a quick patch that does this, so please feel free to use it if you like to. This is just a suggestion, and I won't insist.

Thanks!","16/Nov/12 21:20;rohini;Thanks Cheolsoo. I will use your patch while checking in. Its more clean and better. Tested it out and it works fine.

bq.I would also like to include this in 0.11 if others agree as this is a regression from 0.9.
  Any thoughts on this?","16/Nov/12 21:29;cheolsoo;I am +1 for adding this to 0.11. In fact, I ran a full unit test and e2e with your patch and didn't find any regression.",17/Nov/12 01:38;dvryaboy;Doesn't PIG-3043 solve the problem in a general way?,"17/Nov/12 05:39;julienledem;Pig-3039 solves it in the backend while pig-3043 solves in the frontend

Julien


",19/Nov/12 12:02;rohini;Thanks Cheolsoo for the ivy download patch and the review. Committed the patch to 0.11 and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
With latest version of hadoop23 pig does not return the correct exception stack trace from backend,PIG-3035,12615010,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,06/Nov/12 19:24,06/Jan/13 23:57,14/Mar/19 03:07,08/Nov/12 04:45,0.10.0,,,,,,,0.10.1,0.11,0.12.0,,,,,0,,,,,,,,,,,,,"With latest version of hadoop, the job failure shown to user is always Container killed by the ApplicationMaster. For eg: 

""Backend error : Unable to recreate exception from backed error: AttemptID:attempt_1352163563357_0002_m_000000_1 Info:Container killed by the ApplicationMaster""


Steps to Reproduce:
  Change hadoop version from 2.0.0-alpha to 0.23.5-SNAPSHOT in ivy/libraries.properties. Tests TestScalarAliases.testScalarErrMultipleRowsInInput and TestEvalPipeline2.testNonStandardData will fail when run with -Dhadoopversion=23.   ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Nov/12 19:34;rohini;PIG-3035.patch;https://issues.apache.org/jira/secure/attachment/12552327/PIG-3035.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-07 06:41:32.247,,,no_permission,,,,,,,,,,,,255553,,,,Thu Nov 08 04:45:29 UTC 2012,,,,,,,0|i0fr9j:,90014,,,,,,,,,,"06/Nov/12 19:30;rohini;In Launcher.java getErrorMessages(), the TaskReport reports[] gives the first element as the actual stacktrace and the second one as ""Container killed by the ApplicationMaster"". The parsing logic to construct StackTrace out of the error message has an issue, so it fails to construct the Exception and the message from the last TaskReport which is Container killed is thrown as error. Fixing the logic which constructs the Exception fixes the issue. 

{code}
if(items.length > 0) {
            fileName = items[0];
            lineNumber = Integer.parseInt(items[1]);
        }
{code} 

   items.length is 1 in case of ""at java.security.AccessController.doPrivileged(Native Method)"" in the stack trace which does not have a line number. ","06/Nov/12 19:34;rohini;The attached patch fixes the ArrayIndexOutOfBoundsException for elements in stacktrace that do not have a line number by setting it to -1.

According to javadoc,
lineNumber - the line number of the source line containing the execution point represented by this stack trace element, or a negative number if this information is unavailable. A value of -2 indicates that the method containing the execution point is a native method ",07/Nov/12 06:41;daijy;+1,"08/Nov/12 04:45;rohini;Thanks Daniel. Committed to 0.10.1, 0.11 and trunk",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test-patch failed with javadoc warnings,PIG-3033,12614910,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,fang fang chen,fang fang chen,fang fang chen,06/Nov/12 06:53,22/Feb/13 04:53,14/Mar/19 03:07,10/Dec/12 06:42,0.11,,,,,,,0.11,0.12.0,,,build,,,0,,,,,,,,,,,,,"During test-patch, the build failed with javadoc warning. But these warnings are not contributed by new patch.

Can reproduce by following cmd.
command: ant  -DPigPatchProcess= clean javadoc
38 javadoc warnings:
  [javadoc] /root/ff/trunk/src/org/apache/pig/ExecType.java:41: warning - @param argument ""str"" is not a parameter name.
  [javadoc] /root/ff/trunk/src/org/apache/pig/StoreFuncWrapper.java:111: warning - @return tag has no arguments.
  [javadoc] /root/ff/trunk/src/org/apache/pig/TerminatingAccumulator.java:30: warning - Tag @link: reference not found: IsEmpty
  [javadoc] /root/ff/trunk/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/PORank.java:147: warning - @result is an unknown tag.
  [javadoc] org/apache/hadoop/hbase/mapreduce/TestWALPlayer.class(org/apache/hadoop/hbase/mapreduce:TestWALPlayer.class): warning: Cannot find annotation method 'value()' in type 'org.junit.experimental.categories.Category': class file for org.junit.experimental.categories.Category not found
  [javadoc] org/apache/hadoop/hbase/mapreduce/TestTimeRangeMapRed.class(org/apache/hadoop/hbase/mapreduce:TestTimeRangeMapRed.class): warning: Cannot find annotation method 'value()' in type 'org.junit.experimental.categories.Category'
  [javadoc] org/apache/hadoop/hbase/mapreduce/TestTableSplit.class(org/apache/hadoop/hbase/mapreduce:TestTableSplit.class): warning: Cannot find annotation method 'value()' in type 'org.junit.experimental.categories.Category'
  [javadoc] org/apache/hadoop/hbase/mapreduce/TestTableMapReduce.class(org/apache/hadoop/hbase/mapreduce:TestTableMapReduce.class): warning: Cannot find annotation method 'value()' in type 'org.junit.experimental.categories.Category'
  [javadoc] org/apache/hadoop/hbase/mapreduce/TestTableInputFormatScan.class(org/apache/hadoop/hbase/mapreduce:TestTableInputFormatScan.class): warning: Cannot find annotation method 'value()' in type 'org.junit.experimental.categories.Category'
  [javadoc] org/apache/hadoop/hbase/mapreduce/TestSimpleTotalOrderPartitioner.class(org/apache/hadoop/hbase/mapreduce:TestSimpleTotalOrderPartitioner.class): warning: Cannot find annotation method 'value()' in type 'org.junit.experimental.categories.Category'
  [javadoc] org/apache/hadoop/hbase/mapreduce/TestMultithreadedTableMapper.class(org/apache/hadoop/hbase/mapreduce:TestMultithreadedTableMapper.class): warning: Cannot find annotation method 'value()' in type 'org.junit.experimental.categories.Category'
  [javadoc] org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFilesSplitRecovery.class(org/apache/hadoop/hbase/mapreduce:TestLoadIncrementalHFilesSplitRecovery.class): warning: Cannot find annotation method 'value()' in type 'org.junit.experimental.categories.Category'
  [javadoc] org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.class(org/apache/hadoop/hbase/mapreduce:TestLoadIncrementalHFiles.class): warning: Cannot find annotation method 'value()' in type 'org.junit.experimental.categories.Category'
  [javadoc] org/apache/hadoop/hbase/mapreduce/TestImportTsv.class(org/apache/hadoop/hbase/mapreduce:TestImportTsv.class): warning: Cannot find annotation method 'value()' in type 'org.junit.experimental.categories.Category'
  [javadoc] org/apache/hadoop/hbase/mapreduce/TestImportExport.class(org/apache/hadoop/hbase/mapreduce:TestImportExport.class): warning: Cannot find annotation method 'value()' in type 'org.junit.experimental.categories.Category'
  [javadoc] org/apache/hadoop/hbase/mapreduce/TestHLogRecordReader.class(org/apache/hadoop/hbase/mapreduce:TestHLogRecordReader.class): warning: Cannot find annotation method 'value()' in type 'org.junit.experimental.categories.Category'
  [javadoc] org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat.class(org/apache/hadoop/hbase/mapreduce:TestHFileOutputFormat.class): warning: Cannot find annotation method 'value()' in type 'org.junit.experimental.categories.Category'
  [javadoc] /root/ff/trunk/src/org/apache/pig/builtin/mock/Storage.java:142: warning - @return tag has no arguments.
  [javadoc] /root/ff/trunk/src/org/apache/pig/builtin/mock/Storage.java:133: warning - @return tag has no arguments.
  [javadoc] /root/ff/trunk/src/org/apache/pig/data/SchemaTuple.java:84: warning - @return tag has no arguments.
  [javadoc] /root/ff/trunk/src/org/apache/pig/data/SchemaTupleClassGenerator.java:173: warning - @param argument ""schema"" is not a parameter name.
  [javadoc] /root/ff/trunk/src/org/apache/pig/data/SchemaTupleClassGenerator.java:173: warning - @param argument ""true"" is not a parameter name.
  [javadoc] /root/ff/trunk/src/org/apache/pig/data/SchemaTupleClassGenerator.java:173: warning - @param argument ""identifier"" is not a parameter name.
  [javadoc] /root/ff/trunk/src/org/apache/pig/data/SchemaTupleClassGenerator.java:173: warning - @param argument ""a"" is not a parameter name.
  [javadoc] /root/ff/trunk/src/org/apache/pig/data/SchemaTupleClassGenerator.java:136: warning - @return tag has no arguments.
  [javadoc] /root/ff/trunk/src/org/apache/pig/data/SchemaTupleClassGenerator.java:146: warning - @return tag has no arguments.
  [javadoc] /root/ff/trunk/src/org/apache/pig/data/SchemaTupleFactory.java:54: warning - @param argument ""schema"" is not a parameter name.
  [javadoc] /root/ff/trunk/src/org/apache/pig/data/SchemaTupleFactory.java:100: warning - @param argument ""identifier"" is not a parameter name.
  [javadoc] /root/ff/trunk/src/org/apache/pig/data/SchemaTupleFactory.java:118: warning - @param argument ""schema"" is not a parameter name.
  [javadoc] /root/ff/trunk/src/org/apache/pig/impl/PigContext.java:544: warning - @return tag has no arguments.
  [javadoc] /root/ff/trunk/src/org/apache/pig/impl/util/UDFContext.java:213: warning - @return tag has no arguments.
  [javadoc] /root/ff/trunk/src/org/apache/pig/scripting/groovy/GroovyUtils.java:215: warning - @return tag has no arguments.
  [javadoc] /root/ff/trunk/src/org/apache/pig/scripting/jruby/PigJrubyLibrary.java:313: warning - Tag @link: can't find pigToRuby(Object) in org.apache.pig.scripting.jruby.PigJrubyLibrary
  [javadoc] /root/ff/trunk/src/org/apache/pig/scripting/jruby/PigJrubyLibrary.java:313: warning - Tag @link: can't find pigToRuby(Object) in org.apache.pig.scripting.jruby.PigJrubyLibrary
  [javadoc] /root/ff/trunk/src/org/apache/pig/scripting/jruby/RubyDataBag.java:138: warning - @Param is an unknown tag -- same as a known tag except for case.
  [javadoc] /root/ff/trunk/src/org/apache/pig/scripting/jruby/RubyDataBag.java:230: warning - @return tag cannot be used in method with void return type.
  [javadoc] /root/ff/trunk/src/org/apache/pig/scripting/jruby/RubySchema.java:938: warning - @Return is an unknown tag -- same as a known tag except for case.
  [javadoc] /root/ff/trunk/src/org/apache/pig/tools/pigstats/PigStats.java:227: warning - @return tag has no arguments.
  [javadoc] Building index for all the packages and classes...
  [javadoc] Building index for all classes...
  [javadoc] Generating /root/ff/trunk/build/docs/api/stylesheet.css...
  [javadoc] 38 warnings
","OS: Red Hat Enterprise Linux Server release 6.2 (Santiago)
Install findbugs, svn, forrest, Java5, patch, java6, ant",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Dec/12 07:35;fang fang chen;PIG-3033-trunk-2.patch;https://issues.apache.org/jira/secure/attachment/12555903/PIG-3033-trunk-2.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-03 03:34:12.247,,,no_permission,,,,,,,,,,,,255422,,,,Mon Dec 10 06:42:40 UTC 2012,,,,,,,0|i0erun:,84276,,,,,,,,,,"06/Nov/12 06:57;fang fang chen;This blocked patch applying as test-patch will failed with following information, and this failure is not contributed by new patches:
     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 12 new or modified tests.
     [exec]
     [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec]
     [exec]
     [exec]
     [exec]
     [exec] ======================================================================
     [exec] ======================================================================
     [exec]     Finished build.
     [exec] ======================================================================
     [exec] ======================================================================

",19/Nov/12 05:31;fang fang chen;The fix should be there no additional javadoc warnings during test-patch.,"19/Nov/12 07:54;fang fang chen;Test result with change set:

     [exec] There appear to be 38 javadoc warnings before the patch and 38 javadoc warnings after applying the patch.
     [exec]
     [exec]
     [exec]
     [exec]
     [exec] +1 overall.
     [exec]
     [exec]     +1 javadoc.  The applied patch does not increase the total number of javadoc warnings.
","21/Nov/12 07:41;fang fang chen;After upgrading junit to latest version 4.11(PIG-3058), javadoc warnings like:
[javadoc] org/apache/hadoop/hbase/mapreduce/TestWALPlayer.class(org/apache/hadoop/hbase/mapreduce:TestWALPlayer.class): warning: Cannot find annotation method 'value()' in type 'org.junit.experimental.categories.Category': class file for org.junit.experimental.categories.Category not found
are removed and there are 25 javadoc warnings left.
","22/Nov/12 02:35;fang fang chen;The design is: if the patch did not contribute new jabadoc warnings, then javadoc part in test-patch will pass.","22/Nov/12 02:37;fang fang chen;HI Cheolsoo,

Please help review the patch and provide your suggestions.

Thanks","03/Dec/12 03:34;cheolsoo;Hi Fangfang,
{quote}
The design is: if the patch did not contribute new jabadoc warnings, then javadoc part in test-patch will pass.
{quote}
I am sorry for the late reply. But I think that we should fix the javadoc warnings instead of changing the test-patch script. I understand that warnings that are not related to the patch are annoying, but the goal should be to fix them regardless whether they're introduced by the patch or not.

Do you agree?","03/Dec/12 03:42;fang fang chen;Hi Cheolsoo,

Do you mean we should fix this by removing all the javadoc warnings? If yes, I agree.

Thanks","03/Dec/12 03:47;cheolsoo;Yes, that's what I meant. Thanks!","03/Dec/12 05:20;fang fang chen;Ok, Cheolsoo. Let me do this. 
Thanks","04/Dec/12 07:36;fang fang chen;Hi Cheolsoo,

Updated the patch per our discussion. Please help to review.

Thanks","10/Dec/12 05:58;cheolsoo;+1.

LGTM. I will commit it after running tests.",10/Dec/12 06:42;cheolsoo;Committed to trunk/0.11. Thanks Fangfang!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Pig to use a newer version of joda-time,PIG-3031,12614870,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,zjshen,jcoveney,jcoveney,05/Nov/12 23:12,14/Oct/13 16:45,14/Mar/19 03:07,25/Jan/13 00:11,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"The current version is 1.6, which is quite old (~4 years at this point). Is there any reason not to bring us up to a newer version? I tried to compile the 1.6 source and it didn't work because dependencies are outdated, and so on. Also, the interfaces have matured.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Nov/12 07:35;zjshen;PIG-3031.patch;https://issues.apache.org/jira/secure/attachment/12552230/PIG-3031.patch,24/Jan/13 06:40;zjshen;PIG-3031_1.patch;https://issues.apache.org/jira/secure/attachment/12566259/PIG-3031_1.patch,24/Jan/13 20:02;cheolsoo;PIG-3031_2.patch;https://issues.apache.org/jira/secure/attachment/12566357/PIG-3031_2.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-11-06 07:35:47.377,,,no_permission,,,,,,,,,,,,255368,,,,Fri Jan 25 00:11:22 UTC 2013,,,,,,,0|i0ercn:,84193,,,,,,,,,,"06/Nov/12 07:35;zjshen;It should be safe to update joda to the latest version (2.1).

I've built the sources successfully, but haven't tried the test cases.",20/Jan/13 20:17;cheolsoo;I will commit the patch after running tests.,"22/Jan/13 18:51;jcoveney;Sounds good, thanks y'all.","23/Jan/13 19:58;cheolsoo;Unfortunately, there is a failure:
{code}
Testcase: testConversionBetweenDateTimeAndString took 0.041 sec
        Caused an ERROR
The datetime zone id 'asia/singapore' is not recognised
java.lang.IllegalArgumentException: The datetime zone id 'asia/singapore' is not recognised
        at org.joda.time.DateTimeZone.forID(DateTimeZone.java:223)
        at org.apache.pig.builtin.ToDate3ARGS.exec(ToDate3ARGS.java:21)
        at org.apache.pig.test.TestBuiltin.testConversionBetweenDateTimeAndString(TestBuiltin.java:408)
{code}","23/Jan/13 20:06;zjshen;Hi Cheolsoo,

Which version are you going to upgrade to? 2.1? I'd like to investigate the problem.

Thanks,
Zhijie","23/Jan/13 20:08;cheolsoo;Yes, I used the patch that you provided on this jira. Thanks for taking a look at it.","24/Jan/13 06:40;zjshen;Hi Cheolsoo,

The problem is that DateTimeZone#forID becomes case-insensitively when receiving the timezone ID. See the following url:

http://joda-time.sourceforge.net/upgradeto200.html
 
And please try the newest patch.

In addition, I think it should be good to include the following link:

http://joda-time.sourceforge.net/timezones.html

somewhere in the Pig manual, such that users can know what timezone IDs they can input.

Thanks,
Zhijie

","24/Jan/13 20:02;cheolsoo;+1.

Thank you for the explanation Zhijie! The test passes for me too.

I added that link to the doc for the ToDate built-in function.
{code}
Please see <a href=""http://joda-time.sourceforge.net/timezones.html"">the Joda-Time doc</a> for available timezone IDs.
{code}",25/Jan/13 00:11;cheolsoo;Committed to trunk. Thanks Zhijie!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split results missing records when there is null values in the column comparison,PIG-3021,12614312,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jeffjee617,pokerincome,pokerincome,01/Nov/12 04:08,21/Jun/17 09:15,14/Mar/19 03:07,25/May/17 07:09,0.10.0,,,,,,,0.17.0,,,,,,,2,,,,,,,,,,,,,"Suppose a(x, y)

split a into b if x==y, c otherwise;

One will expect the union of b and c will be a.  However, if x or y is null, the record won't appear in either b or c.

To workaround this, I have to change to the following:
split a into b if x is not null and y is not null and x==y, c otherwise;

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Jun/13 05:28;cheolsoo;PIG-3021-2.patch;https://issues.apache.org/jira/secure/attachment/12589711/PIG-3021-2.patch,07/Jul/13 20:47;cheolsoo;PIG-3021-3.patch;https://issues.apache.org/jira/secure/attachment/12591144/PIG-3021-3.patch,07/Apr/17 23:20;jeffjee617;PIG-3021-4.patch;https://issues.apache.org/jira/secure/attachment/12862541/PIG-3021-4.patch,25/Jun/13 14:30;cheolsoo;PIG-3021.patch;https://issues.apache.org/jira/secure/attachment/12589607/PIG-3021.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-11-01 15:55:28.906,,,no_permission,,,,,,,,,,,,253555,Reviewed,,,Thu May 25 07:25:39 UTC 2017,,,,,,,0|i0dv93:,78992,,,,,,,,,,"01/Nov/12 15:55;cheolsoo;Hi Chang,

Thank you very much for raising an issue.

I understand your argument, but that's an expected behavior. Given your example, here are the plans built by Pig:
{code:title=EXPLAIN b}
b: (Name: LOStore Schema: x#9:int,y#10:int)
|
|---b: (Name: LOSplitOutput Schema: x#9:int,y#10:int)
    |   |
    |   (Name: Equal Type: boolean Uid: 8)
    |   |
    |   |---x:(Name: Project Type: int Uid: 3 Input: 0 Column: 0)
    |   |
    |   |---y:(Name: Project Type: int Uid: 4 Input: 0 Column: 1)
    |
    |---1-4: (Name: LOSplit Schema: x#3:int,y#4:int)
...
{code}
{code:title=EXPLAIN c}
c: (Name: LOStore Schema: x#21:int,y#22:int)
|
|---c: (Name: LOSplitOutput Schema: x#21:int,y#22:int)
    |   |
    |   (Name: Not Type: boolean Uid: 20)
    |   |
    |   |---(Name: Equal Type: boolean Uid: 19)
    |       |
    |       |---x:(Name: Project Type: int Uid: 13 Input: 0 Column: 0)
    |       |
    |       |---y:(Name: Project Type: int Uid: 14 Input: 0 Column: 1)
    |
    |---1-7: (Name: LOSplit Schema: x#13:int,y#14:int)b: (Name: LOStore Schema: x#9:int,y#10:int)
...
{code}
As can be seen, b and c are filtered by expressions == and !=, and the Pig manual says the following [1]:
- Comparison operators: If either subexpression is null, the result is null.
- FILTER operator: If a filter expression results in null value, the filter does not pass them through

So if either x or y is null, they will be dropped.

In addition, the Pig manual also says [2]:
- SPLIT operator: A tuple may not be assigned to any relation.

Does this make sense?

[1] http://pig.apache.org/docs/r0.10.0/basic.html#nulls
[2] http://pig.apache.org/docs/r0.10.0/basic.html#SPLIT","01/Nov/12 17:57;pokerincome;Hi Cheolsoo,
Thanks for the quick response.  NULL is very tricky as in the SQL world.  I can understand your arguments for expected behavior.  However, I don't think this semantic is intuitive.  When a user use OTHERWISE to define default collection, he expects the default collection is a ""catch all"" collection.

In the above example, if x or y is null, it will fail the condition x==y and users expect it goes to the default collection defined by OTHERWISE.","01/Nov/12 18:15;cheolsoo;Hi Chang,

I think that the current behavior of OTHERWISE is from that we translate it to a ""FILTER BY NOT (expr)"", and I am open to discussion about whether this is good or bad.

But it is tricky to to change this because it is backward incompatible. Some people may rely on the current behavior, so if we change semantics, their scripts have to be modified. Therefore, we should change it only if there are more benefits than inconveniences by introducing a backward incompatibility.

Nevertheless, I am happy to listen to other opinions.

Thanks!","17/Nov/12 19:43;cheolsoo;Given the discussion on the user mailing list, I don't think that we're going to change the current behavior:
http://search-hadoop.com/m/5KtFWXN4me1/CONCAT%2528null%252C+%2522something%2522%2529+%253D%253D+NULL+%253F&subj=CONCAT+null+something+NULL+

I am going to close this jira as a 'won't fix'. Please let me if anyone has objections.
","18/Nov/12 04:56;pokerincome;I agree with the discussion about NULL.  I also agreed CONCAT(null, something) = null.  My issue is on the OTHERWISE keyword not NULL.  Maybe it only has issues when NULL is involved but maybe there might be other cases where the same confusion occurs.

I understand it's breaking change and ok with won't-fix.  But if I were to redesign SPLIT/OTHERWISE from beginning, I would make OTHERWISE a catch-all default collection. 

One suggestion is to give an explicit example of combination of NULL and OTHERWISE keywords in the SPLIT documentation.

I can't post to the users mailing list.  Could you please forward my comments to that list?  Thanks!","18/Nov/12 05:35;cheolsoo;Hi Chang,

I won't close this jira if you disagree. :-) We can definitely add an example to the SPLIT doc, and you're welcome to submit a patch for that if you like to. I will be happy to review/commit it.

{quote}
I can't post to the users mailing list. Could you please forward my comments to that list?
{quote}
You can subscribe user@pig.apache.org. You're welcome to open a discussion on this matter on the mailing list.
http://pig.apache.org/mailing_lists.html

Thanks!","24/Jun/13 08:44;vweijo;Hi,

I strongly suggest either fixing the implementation or at least changing documentation of OTHERWISE. At the moment what is documented (http://pig.apache.org/docs/r0.11.1/basic.html#SPLIT) and what is implemented does not follow the principle of least surprise, and without knowing the implementation details of 'OTHERWISE', the documentation is even outright wrong.

Please excuse my perhaps strong wording, but I spent some hours trying to figure out what on earth is happening. :)","24/Jun/13 16:42;cheolsoo;OK, I will implement the ""catch-all"" semantics for OTHERWISE. I will add an ""IS NULL"" expression to the otherwise branch, so null values can be caught instead of being discarded.

Please let me know if anyone has objection with the change.","25/Jun/13 14:30;cheolsoo;Attached is a patch that passes through null values in the otherwise branch of SPLIT.

I also re-wrote TestSplit.java using mock.Storage while updating it.

I am going to run full unit tests to ensure that I am not breaking any test cases.","26/Jun/13 05:28;cheolsoo;All unit tests pass. I made some minor change in a new patch.

Btw, if we really don't want to break backward compatibility, we can introduce an optional keyword like NULLABLE. For example,
{code}
SPLIT foo INTO x IF <cond>, y OTHERWISE [ NULLABLE ];
{code}
If NULLABLE is specified, nulls will be stored in y; otherwise, nulls will be discarded.

Currently, I didn't implement this idea, but please let me know if you like it more. It should be straightforward to implement it.",26/Jun/13 19:09;rohini;Looks good. But the concern is it breaks backward compatibility and users will see different results without any warning. Should we do the NULLABLE idea or have it as a pig configuration that can be turned on in deployments?  ,"27/Jun/13 04:27;cheolsoo;Rohini, Sure. I totally agree that we shouldn't break backward compatibility. Let me update the patch.

Personally, I like adding a keyword better than adding a property. But please let me know anyone thinks otherwise. Also let me know if anyone has a better name than NULLABLE.

Thanks!","03/Jul/13 17:25;vweijo;Hi,

I think adding a keyword is ok, as long as documentation is updated to make it clear what is the difference between the plain OTHERWISE and OTHERWISE NULLABLE. Of course, that should be done in any case.

About the name of the optional keyword, something like plain ALL might also do. That way it would be clear from a pig script that the rest of the records will go to the specified place and perhaps notify the reader that it is a good idea to check the documentation on what is going on, although NULLABLE should raise a question on exact mechanism of SPLIT, too.","07/Jul/13 20:47;cheolsoo;Attached is a patch that introduces the ALL keyword into the split otherwise. If ALL is specified, nulls are stored in the default relation. By default, nulls are discarded (i.e. backward compatible).

I also updated the document and added a test case to TestSplit.

ReviewBoard:
https://reviews.apache.org/r/12321/",25/Sep/13 18:59;cheolsoo;Moving it to 0.13 since the 0.12 branch is already cut.,27/Oct/13 01:33;cheolsoo;Canceling the patch until I have time to answer Aniket's comment in the RB.,"07/Apr/17 23:20;jeffjee617;We shouldn't change POIsNull.java file. It works well when using ""otherwise all"" keywords right now. I already attached a new patch(PIG-3021-4.patch).","07/Apr/17 23:43;daijy;[~cheolsoo], do you remember why you change POIsNull.java? If all tests pass without the change, I'd like to commit the patch as is.","18/Apr/17 20:37;cheolsoo;[~daijy], no I don't remember. Sorry for the late reply.","25/May/17 07:09;daijy;+1 for PIG-3021-4.patch. Patch committed to trunk. Thanks Nian, Cheolsoo!

[~jeffjee617], do you mind adding some documentation as well (in other Jira)?","25/May/17 07:25;jeffjee617;[~daijy], thank you. I will create another Jira and add a patch with adding documentation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Duplicate uid in schema"" error when joining two relations derived from the same load statement",PIG-3020,12614280,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,julienledem,julienledem,31/Oct/12 21:54,18/Oct/13 20:13,14/Mar/19 03:07,18/Dec/12 02:41,0.11,,,,,,,0.11,0.12.0,,,,,,0,,,,,,,,,,,,,"The following validates OK with pig 0.9 and fails with the following error in 0.11 (and I suspect 0.10)

pig -c debug2.pig

Script: debug2.pig
{noformat}
A = LOAD 'foo' AS (group:tuple(uid, dst_id), uids_with_recs:bag{} , uids_with_flock:bag{});
edges_both = FILTER A BY NOT IsEmpty(uids_with_recs) AND NOT IsEmpty(uids_with_flock);
edges_both = FOREACH edges_both GENERATE
    group.uid AS src_id,
    group.dst_id AS dst_id;
both_counts = GROUP edges_both BY src_id;
both_counts = FOREACH both_counts GENERATE
    group AS src_id, SIZE(edges_both) AS size_both;

edges_bq = FILTER A BY NOT IsEmpty(uids_with_recs);
edges_bq = FOREACH edges_bq GENERATE
    group.uid AS src_id,
    group.dst_id AS dst_id;
bq_counts = GROUP edges_bq BY src_id;
bq_counts = FOREACH bq_counts GENERATE
    group AS src_id, SIZE(edges_bq) AS size_bq;

per_user_set_sizes = JOIN bq_counts BY src_id LEFT OUTER, both_counts BY src_id;
store per_user_set_sizes into  'foo';
{noformat}

Error:
{noformat}
ERROR 2270: Logical plan invalid state: duplicate uid in schema : bq_counts::src_id#417:bytearray,bq_counts::size_bq#468:long,both_counts::src_id#417:bytearray,both_counts::size_both#480:long

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1067: Unable to explain alias null
	at org.apache.pig.PigServer.explain(PigServer.java:999)
	at org.apache.pig.tools.grunt.GruntParser.explainCurrentBatch(GruntParser.java:398)
	at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:330)
	at org.apache.pig.tools.grunt.Grunt.checkScript(Grunt.java:98)
	at org.apache.pig.Main.run(Main.java:600)
	at org.apache.pig.Main.main(Main.java:154)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:186)
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2000: Error processing rule LoadTypeCastInserter
	at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:122)
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:277)
	at org.apache.pig.PigServer.compilePp(PigServer.java:1322)
	at org.apache.pig.PigServer.explain(PigServer.java:984)
	... 10 more
Caused by: org.apache.pig.impl.plan.PlanValidationException: ERROR 2270: Logical plan invalid state: duplicate uid in schema : bq_counts::src_id#417:bytearray,bq_counts::size_bq#468:long,both_counts::src_id#417:bytearray,both_counts::size_both#480:long
	at org.apache.pig.newplan.logical.optimizer.SchemaResetter.validate(SchemaResetter.java:232)
	at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:105)
	at org.apache.pig.newplan.logical.relational.LOJoin.accept(LOJoin.java:171)
	at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
	at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:52)
	at org.apache.pig.newplan.logical.optimizer.SchemaPatcher.transformed(SchemaPatcher.java:43)
	at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:113)
	... 13 more
{noformat}",,,,,,,,,,,,,,,,,,,PIG-3492,,,,,,,,,,,,,17/Dec/12 19:38;jcoveney;PIG-3020-2.patch;https://issues.apache.org/jira/secure/attachment/12561340/PIG-3020-2.patch,17/Dec/12 19:38;jcoveney;PIG-3020-2_ws.patch;https://issues.apache.org/jira/secure/attachment/12561339/PIG-3020-2_ws.patch,07/Dec/12 19:53;julienledem;PIG-3020.patch;https://issues.apache.org/jira/secure/attachment/12559938/PIG-3020.patch,14/Dec/12 00:43;julienledem;PIG-3020_branch-0.11_1.patch;https://issues.apache.org/jira/secure/attachment/12560888/PIG-3020_branch-0.11_1.patch,14/Dec/12 01:08;jcoveney;PIG-3093-testcase.patch;https://issues.apache.org/jira/secure/attachment/12560892/PIG-3093-testcase.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2012-12-07 23:21:10.742,,,no_permission,,,,,,,,,,,,253521,,,,Fri Oct 18 20:13:48 UTC 2013,,,Patch Available,,,,0|i0ds07:,78466,,,,,,,,,,31/Oct/12 21:56;julienledem;the check added in PIG-2176 is breaking this script,"07/Dec/12 19:53;julienledem;PIG-3020.patch fixes the issue
","07/Dec/12 23:21;jcoveney;This looks good to me, though I wonder if there is anyone who knows this code better than can take a look.",08/Dec/12 00:33;dvryaboy;are the manifest changes related?,"12/Dec/12 19:52;julienledem;[~dvryaboy] I just noticed it was logging a warning with a NullPointerException when running tests from eclipse. I just fixed the log line to something clearer. It is not related but I feel it is small enough to be done here.
[~jcoveney] I also added a unit test with a pig script that was failing before and works now to validate my change.","14/Dec/12 01:08;jcoveney;Julien,

I've included a test that I think you should add to this patch (and it may turn out that pig 3093 is a duplicate of this).

Either way, my test fails on trunk, but it fails with a different error on your branch. Looks like when you change the uid you whack the alias.","17/Dec/12 19:38;jcoveney;I've attached a fix, with and without whitespace changes (would like to attach _ws, but easier to review without). This include and also fixes PIG-3093","17/Dec/12 23:10;julienledem;looks good to me
+1",18/Dec/12 00:20;jcoveney;This is in trunk. Not sure if it meets the criteria to be in pig-11?,18/Dec/12 01:13;dvryaboy;existing scripts that work on pig 9 don't work on 11 without this so I think it needs to be in 11 (to prevent breaking changes).,18/Dec/12 02:06;jcoveney;I am inclined to agree. Will commit to 0.11,15/Jan/13 06:36;elifinkelshteyn;Is there any work around for this for 0.10 aside from substituting cogroup for join?,"18/Oct/13 20:13;knoguchi;FYI, I'm trying to revert the change from this jira at PIG-3492.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor TestScriptLanguage to remove duplication and write script in different files,PIG-3018,12614073,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,julienledem,julienledem,julienledem,30/Oct/12 19:14,22/Feb/13 04:53,14/Mar/19 03:07,31/Oct/12 20:44,,,,,,,,0.11,0.12.0,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30/Oct/12 19:15;julienledem;PIG-3018.patch;https://issues.apache.org/jira/secure/attachment/12551410/PIG-3018.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-31 18:52:36.777,,,no_permission,,,,,,,,,,,,253198,,,,Wed Oct 31 20:44:26 UTC 2012,,,Patch Available,,,,0|i0dcdj:,75934,,,,,,,,,,31/Oct/12 18:52;cheolsoo;This should be fixed in both trunk and 0.11.,31/Oct/12 18:52;cheolsoo;+1,31/Oct/12 20:44;julienledem;merged in trunk (0.12) and branch-0.11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig's object serialization should use compression,PIG-3017,12614059,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,jcoveney,jcoveney,30/Oct/12 18:02,14/Oct/13 16:45,14/Mar/19 03:07,30/Oct/12 23:52,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"We have run into cases of very large JobConf objects, and part of this is the fact that serialized objects are quite large. There is no reason not to use compression here, and ratios should be quite high.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30/Oct/12 18:05;jcoveney;PIG-3017-0.patch;https://issues.apache.org/jira/secure/attachment/12551390/PIG-3017-0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-30 18:58:17.354,,,no_permission,,,,,,,,,,,,253184,,,,Tue Oct 30 23:52:46 UTC 2012,,,,,,,0|i0dcaf:,75920,,,,,,,,,,"30/Oct/12 18:05;jcoveney;This passes test-commit, though I had to change one golden file because this serialization affect results (golden files *shakes fist*).","30/Oct/12 18:58;prkommireddi;Hey Jon, out of curiosity - have you done any comparison between object sizes before and after this patch, and also comparisons w.r.t time? Just trying to understand if Deflater.BEST_COMPRESSION is the ideal choice.","30/Oct/12 20:21;jcoveney;Well, I don't know the absolute size because I had a script where the JobConf was failing out at about 6.5MB...I'm not sure if it fails as soon as it crosses the thresh-hold, or if it fails after serializing everything. That said, after this patch, the same JobConf was 600KB, so about 10x (note that I also changed it to use Base64 encoding). Also, as far as serialization time, it's still in the realm  of ~5MB, so compression time is negligible. I did not do extensive testing around the specifics, though. ","30/Oct/12 20:31;prkommireddi;Sounds reasonable, thanks.",30/Oct/12 21:45;julienledem;+1 looks good to me,"30/Oct/12 23:52;jcoveney;Added to 0.11 and trunk, thanks Julien",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CurrentTime() UDF has undesirable characteristics,PIG-3014,12613930,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,jcoveney,jcoveney,29/Oct/12 18:07,14/Oct/13 16:45,14/Mar/19 03:07,30/Nov/12 23:54,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"As part of the explanation of the new DateTime datatype I noticed that we had added a CurrentTime() UDF. The issue with this UDF is that it returns the current time _of every exec invocation_, which can lead to confusing results. In PIG-1431 I proposed a way such that every instance of the same NOW() will return the same time, which I think is better. Would enjoy thoughts.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PIG-1431,,06/Nov/12 00:35;jcoveney;PIG-3014-0.patch;https://issues.apache.org/jira/secure/attachment/12552192/PIG-3014-0.patch,06/Nov/12 22:14;jcoveney;PIG-3014-1.patch;https://issues.apache.org/jira/secure/attachment/12552347/PIG-3014-1.patch,18/Nov/12 22:39;cheolsoo;PIG-3014-2.patch;https://issues.apache.org/jira/secure/attachment/12554095/PIG-3014-2.patch,30/Nov/12 21:30;cheolsoo;PIG-3014-3.patch;https://issues.apache.org/jira/secure/attachment/12555562/PIG-3014-3.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-11-05 18:50:11.789,,,no_permission,,,,,,,,,,,,252821,,,,Sat Dec 01 01:41:38 UTC 2012,,,,,,,0|i0d8gf:,75146,,,,,,,,,,05/Nov/12 18:50;alangates;+1.  It's hard to envision why you'd want the current behavior.,"06/Nov/12 00:35;jcoveney;I've attached a fix and a couple light tests. Note that I uncovered PIG-3032 while developing this, though this isn't affected by that bug.","06/Nov/12 07:27;zjshen;Hi Jonathan,

thanks for correcting my error. According to you patch, I've also some comments.

As far as I know, getArgToFuncMapping() is called when generating the logic plan, while exec() is called at runtime. Hence the DateTime object generated in getArgToFuncMapping() reflects the timestamp when the pig latin statements are parsed. If there's several statements containing CurrentTime(), the timestamps will be similar. Please correct me if I'm wrong.

For example,

""
A = load 'justSomeRows' using mock.Storage();
B = foreach A generate *, CurrentTime();
......
C = foreach B generate *, CurrentTime();
""

In this case, there're a bunch of statements between B and C. In B, we want to get the timestamp before executing the statements, while in C, want to get the timestamp after executing them. The difference between the two timestamps should reflect the runtime interval instead of the interval between parsing two CurrentTime() UDFs.

I think the more accurate behavior of CurrentTime() should be generating a unique timestamp for a statement when it is executed.","06/Nov/12 07:59;jcoveney;Zhijie,

I think that the semantics in my patch are sufficient. You are correct that in some cases, they might be ""closer together"" than we might want, but what does that even mean? The semantics are not well specified. What if the optimizer in fact put C before B? What if the optimizer had them run at the same time? What if my cluster happens to be tuned to a certain workload...and so on and so on. I think as long as ""now"" is defined as ""after the script runs,"" and as long as it is the same for every value in a given relation that uses it, that's the only guarantee that we can make. We can document this limitation (i.e. that ""now"" is a more or less arbitrary value in between the beginning of your script and when it is finished being parsed).

I suppose there would be some utility in a CurrentTime() where the time is with respect to the beginning of execution, but it could easily suffer from the same issue if it was in a foreach with a really time consuming value, where the ""now"" value quickly becomes stale. I think the incremental gain is minimal, and the incremental complexity is quite high. If you deeply disagree, though, we can discuss how to do it. I think the following would work: per each instantiation of the UDF, we create two unique files and put them in HDFS (I do not think the distributed cache will work in this specific case, but it may). Those files will be the constructor argument. On first execution, each mapper tries to delete the file. Since delete is atomic, only one should succeed. This is the leader. It will record the current time and serialize it to the second file. We would have to coordinate atomicity...perhaps it could write a magic value at the end of the serialized date time, so all of the mappers would read the file until they read the magic number, and then they'd know it was done.

This would be pretty complicated for what I see as a minimal gain, but it would probably be a ""more correct"" now() implementation. I do not know if Hadoop has a more convenient coordination mechanism between mappers (this sort of goes against the whole point).

I welcome more thoughts","06/Nov/12 08:13;jcoveney;(as a sidenote, I was thinking about this, and IF (big if) Hadoop can guarantee an atomic write action (I don't think it can?) then we only need one file. Each mapper can attempt to read it, and if it is empty, it appends the current time, and then it reads the first date time in that file. It would avoid a race condition because of the atomic write action. If writing isn't atomic though you'd have to abuse some atomic action for coordination, a la delete above. In fact, we could even make this a generic API that let's you coordinate some runtime value for udf invocations, but once again, it's not really a pattern we want to encourage).

Now I sort of want to do this just for the challenge of it...","06/Nov/12 22:14;jcoveney;I think this patch is a good compromise. I was talking to Bill earlier today and he mentioned that we have in the job conf a timestamp for around when a given job starts. IMHO this is close enough, and it will be the same. Easy peasy.","10/Nov/12 22:49;cheolsoo;Hi Jonathan,

I agree with using the job start time. That sounds reasonable to me.

But I have two comments regarding your patch:
- The Apache license header shouldn't be removed in {{src/org/apache/pig/builtin/CurrentTime.java}}.
- After applying your patch, {{src/org/apache/pig/builtin/CurrentTime.java}} looks like this. Can you please fix indentations?
{code}
/**
     * This is a default constructor for Pig reflection purposes. It should
     * never actually be used.
 */
    public CurrentTime() {}

    @Override
    public DateTime exec(Tuple input) throws IOException {
        if (!isInitialized) {
            String dateTimeValue = UDFContext.getUDFContext().getJobConf().get(""pig.job.submitted.timestamp"");
            if (dateTimeValue == null) {
                throw new ExecException(""pig.job.submitted.timestamp was not set!"");
    }   
            dateTime = new DateTime(Long.parseLong(dateTimeValue));
            isInitialized  = true;
    }   
        return dateTime;
    }
{code}

Thanks!","17/Nov/12 19:51;cheolsoo;I was going to commit the patch after fixing whitespaces.

But I realized that the new test case {{TestCurrentTime}} fails with hadoop-2.0.x.
{code}
ERROR 0: pig.job.submitted.timestamp was not set!
{code}","18/Nov/12 22:29;cheolsoo;Here is how I ended up fixing the test with hadoop-2.0.x:

from
{code:title=MapReduceLauncher.java}
for (Job job : jc.getWaitingJobs()) {
    job.getJobConf().set(""pig.script.submitted.timestamp"", Long.toString(scriptSubmittedTimestamp));
    job.getJobConf().set(""pig.job.submitted.timestamp"", Long.toString(System.currentTimeMillis()));
}
{code}
to
{code:title=MapReduceLauncher.java}
for (Job job : jc.getWaitingJobs()) {
    JobConf jobConfCopy = job.getJobConf();
    jobConfCopy.set(""pig.script.submitted.timestamp"", Long.toString(scriptSubmittedTimestamp));
    jobConfCopy.set(""pig.job.submitted.timestamp"", Long.toString(System.currentTimeMillis()));
    job.setJobConf(jobConfCopy);
}
{code}
Apparently, {{job.getJobConf()}} returns a different JobConf object each time, so properties that are set by {{job.getJobConf().set()}} do not last at all.

This is quite surprising to me because this means that there are many other properties that are not properly set with hadoop-2.0.x now. I will open another jira to get this issue fixed.","18/Nov/12 22:39;cheolsoo;Attaching a patch that makes {{TestCurrentTime}} pass in both hadoop 20 and 23. I also fixed whitespace and Apache header issues that I mentioned in a previous comment.

Thanks!","19/Nov/12 11:19;rohini;bq. Apparently, job.getJobConf() returns a different JobConf object each time, so properties that are set by job.getJobConf().set() do not last at all.

  This is got me worried. I don't see a clone happening in mapreduce code. Were you able to get to the root cause of the behaviour?","19/Nov/12 18:10;jcoveney;Thanks for fixing my patch over the weekend, Cheolsoo! Feel free to commit, or I can later. The reviewer has become the reviewed :)

I agree with Rohini that we need to be vigilant about this getJobConf situation.","26/Nov/12 17:49;rohini;bq. I don't see a clone happening in mapreduce code. Were you able to get to the root cause of the behaviour?
   I was looking at the wrong version of hadoop code. H23 indeed returns a copy of the JobConf for jobcontrol.Job. Checked that we don't do a set in other places. Even the submitted.timestamp that was set was not being used in code elsewhere before this case. May be it was just set for debugging purposes. So we should be good with 0.9 and 0.10 for h23. 

{code}
public synchronized JobConf getJobConf() {
    return new JobConf(super.getJob().getConfiguration());
  }
{code} 

 My +1 as well. ",28/Nov/12 21:42;cheolsoo;Committed to trunk. Thanks Jonathan and Rohini!,"30/Nov/12 19:59;julienledem;I see a failing test:
org.apache.pig.test.TestBuiltin.testConversionBetweenDateTimeAndString

java.lang.NullPointerException
	at org.apache.pig.builtin.CurrentTime.exec(CurrentTime.java:41)
	at org.apache.pig.test.TestBuiltin.testConversionBetweenDateTimeAndString(TestBuiltin.java:450)
","30/Nov/12 21:15;cheolsoo;Hi Julien,

Sorry for that. It is failing because {{TestBuiltin}} is not set {{pig.job.submitted.timestamp}}. I will get it fixed now.","30/Nov/12 21:30;cheolsoo;Attached a patch that fixes {{TestBuiltin}}.

The CurrentTime() must get called only in the back-end because it reads the value of ""pig.job.submitted.timestamp"" out of JobConf. But the unit test case was calling it in the front-end, resulting in a NullPointerException.

Since the test case is not valid, I simply removed it.

Thanks!","30/Nov/12 22:53;rohini;bq. Since the test case is not valid, I simply removed it.

+1. TestCurrentTime covers CurrentTime udf adequately.

Just an observation though. All builtin udf tests are in TestBuiltin, but CurrentTime alone has a separate test class with just one test. Should we move that to TestBuiltin?","30/Nov/12 22:58;cheolsoo;Thanks Rohini.

In fact, I asked that question on the dev mailing list a while ago:
http://search-hadoop.com/m/OVyoR1Ktpcy/Adding+new+test+cases+to+TestBuiltin.java&subj=Adding+new+test+cases+to+TestBuiltin+java

Julien said that each built-in UDF should have its own test suite, so I followed it in PIG-2881. I guess that the same applies to CurrentTime().

Please anyone correct me if I am wrong.","30/Nov/12 23:25;julienledem;I think it's better to have one test class per UDF.
Usually tests are grouped per class or functional group of classes.
All builtin UDFs do not make a functional group as they have various different purposes. It just makes a huge Test class which is undesirable.",30/Nov/12 23:54;cheolsoo;Patch committed to trunk.,01/Dec/12 01:41;rohini;Ah. I had forgotten about that question. Agree with Julien.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BinInterSedes improve chararray sort performance,PIG-3013,12613810,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,28/Oct/12 18:54,14/Oct/13 16:46,14/Mar/19 03:07,04/Nov/12 17:12,0.10.0,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"In PIG-2975, Koji found that there is a extra array copy during byte array comparison and fixing that gave a 15-20% performance improvement. Same is the case with chararray.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Oct/12 19:52;rohini;PIG-3013.patch;https://issues.apache.org/jira/secure/attachment/12551122/PIG-3013.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-28 18:56:40.033,,,no_permission,,,,,,,,,,,,252612,,,,Sun Nov 04 17:12:21 UTC 2012,,,,,,,0|i0cvfz:,73038,,,,,,,,,,28/Oct/12 18:56;jcoveney;Excellent,"28/Oct/12 19:52;rohini;Ran the full suite of unit tests and they passed (no new errors than the pending ones in PIg-2972). Did not add a new unit test as TestOrderBy3.testValuesASC and testValuesDESC already test the ordering for chararray and exercise this codepath.

[~jcoveney],
   Do you want to include this in 0.11 as well? ","28/Oct/12 20:04;jcoveney;+1 for trunk

regarding inclusion in 0.11, as much as I want to say we should, the ""apache"" part of me says we should follow the line in the sand we drew for 0.11 and try to get it out or else we're just going to be too tempted to keep dumping new stuff there.

Does anyone else feel differently?",28/Oct/12 20:06;rohini;No problem Jon. That's why marked it for 0.12. Was checking just in case.,"03/Nov/12 17:27;azaroth;This seems ready to be committed to trunk.
[~rohini] do you want to commit it yourself?",04/Nov/12 17:12;rohini;Checked into trunk. Thanks for the reminder Gianmarco.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig client should handle CountersExceededException,PIG-3002,12613305,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jarcec,billgraham,billgraham,24/Oct/12 17:51,14/Oct/13 16:46,14/Mar/19 03:07,01/Mar/13 06:21,,,,,,,,0.12.0,,,,,,,0,newbie,simple,,,,,,,,,,,"Running a pig job that uses more than 120 counters will succeed, but a grunt exception will occur when trying to output counter info to the console. This exception should be caught and handled with friendly messaging:

{noformat}
org.apache.pig.backend.executionengine.ExecException: ERROR 2043: Unexpected error during execution.
        at org.apache.pig.PigServer.launchPlan(PigServer.java:1275)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1249)
        at org.apache.pig.PigServer.execute(PigServer.java:1239)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:333)
        at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:136)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:197)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:169)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:84)
        at org.apache.pig.Main.run(Main.java:604)
        at org.apache.pig.Main.main(Main.java:154)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:186)
Caused by: org.apache.hadoop.mapred.Counters$CountersExceededException: Error: Exceeded limits on number of counters - Counters=120 Limit=120
        at org.apache.hadoop.mapred.Counters$Group.getCounterForName(Counters.java:312)
        at org.apache.hadoop.mapred.Counters.findCounter(Counters.java:431)
        at org.apache.hadoop.mapred.Counters.getCounter(Counters.java:495)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.computeWarningAggregate(MapReduceLauncher.java:707)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:442)
        at org.apache.pig.PigServer.launchPlan(PigServer.java:1264)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Feb/13 07:19;billgraham;PIG-3002.2.patch;https://issues.apache.org/jira/secure/attachment/12570736/PIG-3002.2.patch,09/Jan/13 16:10;jarcec;PIG-3002.patch;https://issues.apache.org/jira/secure/attachment/12563951/PIG-3002.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-01-09 16:10:48.012,,,no_permission,,,,,,,,,,,,250795,,,,Fri Mar 01 06:21:39 UTC 2013,,,,,,,0|i0b1j3:,62360,,,,,,,,,,"09/Jan/13 16:10;jarcec;I've tried to fix this issue by moving code that is responsible for getting counter value to shim layer. By doing this I can properly catch CountersExceededException only for Hadoop 0.23 and higher.

My current patch do not include any tests as I'm not sure where to put shim related tests. I did however verified the functionality on real clusters with Hadoop 1.x and Hadoop 2.x.","09/Jan/13 16:45;billgraham;Thanks for digging into this one Jarek.

If we swallow the exception in the shim and return 0 it would be misleading to the user.
Instead what if we catch the exception during the call to computeWarningAggregate in MapReduceLauncher and log a message about how the counters could not be fetched for this job. Likewise the messaging in CompilationMessageCollector.logAggregate should be updated as well to indicate that aggregates might be off.","09/Jan/13 21:17;jarcec;Hi Bill,
thank you very much for your review, I appreciate your time. My first implementation actually took the way you've proposed. But after that I dig into the logic and realized that it might not be necessary. As far as I understand purpose of the method computeWarningAggregate() is to aggregate existing counters from predefined Enum PigWarning. Therefore this method should not create any new counters. However the method call Counters.getCounter(Enum) will create new counter in case that it's not there (as side effect) and throw CountersExceededException in case that it can't create it.

That's why I've introduced getCounterValue() method on the shim layer that silently ignore this exception. The purpose of this method is only to get the value, not create the counter. And if we got the exception, than it means that the counter is not there (and can't be created) and thus it seems to me that returning 0 should be valid operation at this point.

I believe that overall aggregates should be valid.

Does this reasoning make any sense?

Jarcec","12/Jan/13 22:45;billgraham;This exception gets thrown when a job creates more than 120 counters and we then go to check on expected counters at the conclusion of the job. Each call to getCounter will throw CountersExceededException. Your patch will swallow that exception and result in the user being messaged that the counter exists and has a value of 0. We don't want to do that in this case. Instead we want to message the user that we couldn't access counters due to the limit issue and that we don't know what the counter values are.

Also, the logic behind getCounterValue shouldn't be driven my just this one caller. Other callers who want to get a counter value wouldn't (and shouldn't) expect that the method swallows exceptions and returns 0.","13/Jan/13 17:50;jarcec;Hi Bill,
again thank you for your comment. I appreciate your input. Please do not understand me wrong, I'll be more than happy to change my patch by including your suggestions. However I would like to make sure that we're on the same page first.

I've dug into Hadoop source code and the call getCounter(Enum) is defined in [1]. This call is just forwarded to findCounter(Enum) defined in AbstractCounters in [2]. This method will firstly check if the counter exists in object ""cache"", if so, then this object is returned without any exception being raised. If the object is not present, then it call method findCounter(String, String) that will eventually create the counter and throw CountersExceededException exception if needed. 

Let's assume for example that we have following counters with maximal counter limit set to 2:

* A => 1
* B => 2

If we try to call method getCounter(Enum) with values A, B and C, then we will get:

* A => 1
* B => 2
* C => CountersExceededException, because C is not in map ""cache"" and thus the code will try to create new counter and fail on configured limitation.

By this example, I'm trying to explain that the method getCounter(Enum) won't throw CountersExceededException for all subsequent calls after the Counters gets full, but just for those that do not yet exists. I'm also trying to show that the method computeWarningAggregate by itself won't affect generated aggregates.

You're definitely right that newly introduced function getCounterValue() is driven by single caller and that it's not the best way to do it. I've tried to put the code directly inside computeWarningAggregate(), but I failed on compilation error as CountersExceededException is not available in Hadoop 1.0. Therefore I've moved the code into the shim layer.

I've defined the method getCounterValue() to return long instead of Counter object to narrow down the purpose to just get the long value of the counters, not the counter object itself. I believe that in such case, swallowing the exception is reasonable, because in such case the counter do not exists there and thus it's value is 0. Maybe providing descriptive javadoc to method getCounterValue() would help here?

Jarcec

Links:
1: https://github.com/apache/hadoop-common/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Counters.java#L516
2: https://github.com/apache/hadoop-common/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java#L163","24/Jan/13 17:15;jarcec;Hi Bill,
did you by any chance have a time to look at my explanation?

I'll be more than happy to refactore my code if needed, I just wanted to be sure that we're on the same page first.

Jarcec","24/Jan/13 22:17;billgraham;Sorry for the delay Jarcec. I actually read your explanation after you wrote it and had concerns that we might be thinking of two different uses cases. I wrote a test script that uses a UDF to produce 100 counters. The script runs and the jobs complete successfully, but grunt outputs an error message.

I wanted to dig in more before replying but I got pulled away onto something else. I want to asset what the behavior becomes with your patch to see how it changes the behavior. Will report back.","25/Jan/13 00:49;jarcec;Thank you very much for your feedback Bill, I appreciate your time and effort to verify my suggestion. Please take your time, I'm not in hurry for this.

Jarcec","23/Feb/13 18:35;jarcec;Hi [~billgraham],
I know that you are super busy with driving the pig release and all other things. I just wanted to check whether you had by any chance time to take a look into my proposed patch?

Jarcec","25/Feb/13 07:19;billgraham;I was able to run some tests using both the patched and unpatched version and both behave the same w.r.t. what's output to the console.

The behavior of {{MapReduceLauncher.computeWarningAggregate()}} is  already to catch {{IOException}} and log a warning, so it would be aceptable to catch {{Exception}} around the iterator on the {{PigWarning}} enum and just {{log.warn}} there as well. No need to modify the shim code. Just log the error and move on. This will cause the console output to show the success state of the jobs, along with the exception with the counters.","26/Feb/13 16:46;jarcec;Hi [~billgraham], 
thank you very much for taking a look on this Jira and my patch. I was considering similar solution as you proposed in my early work, but I've notice one side effect during experiments with my early patches.

I've created quite pathological case when my cluster was using default configuration, but I've limit the number of allowed counters to 3 on machine where I've executed pig. I've noticed that with similar fix, pig will print out couple of counters and than bail out on exception on first non existing Counter. As a result not all the counters will be printed out even though they are available in the {{Couter}} object.

My experiment is obviously not entirely real as it's unlikely that users will have different hadoop configuration. However I believe that it model the edge situation when mapreduce job will create almost all available counters, but because client is iterating over predefined set, not all of them will be printed out.

I've also did one step further and put the {{try-catch}} block inside the {{for}} iteration. I've noticed that in this situation we might print out the error message several times, which is kind of distracting. This lead me to the idea of doing the changes on the shim layer that I've submitted.

Jarcec","28/Feb/13 07:23;billgraham;I don't think we should be modifying the shims code in this way for the contrived case. Swallowing exceptions and returning 0 doesn't seem like the right thing to do for the reasons I've described above. If Hadoop is throwing exceptions because we've used too many counters, let's catch it and log it and move on. Surfacing the exception to a user in the console is better than trying to print some of them. Any counters captured by Hadoop will still be reported in the JT and the job history. ","28/Feb/13 19:32;jarcec;Understood. Thank you for your time [~billgraham], I appreciate your help and support.

I'm +1 on the patch ""PIG-3002.2.patch"" (non-binding).

Jarcec",01/Mar/13 06:21;billgraham;Committed. Thanks [~jarcec] for digging into this one and sorry for the delay.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The buildtin function ""ToDate"" has been mapped with wrong class names",PIG-2996,12612900,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,zjshen,zjshen,zjshen,21/Oct/12 23:54,22/Feb/13 04:53,14/Mar/19 03:07,03/Dec/12 03:40,,,,,,,,0.11,,,,data,,,0,,,,,,,,,,,,,"In ToDate#getArgToFuncMapping(), the method to get the mapped function's class name is incorrect:

s = new Schema();
        s.add(new Schema.FieldSchema(null, DataType.CHARARRAY));
        funcList.add(new FuncSpec(ToDateISO.class.getClass().getName(), s));
        s = new Schema();
        s.add(new Schema.FieldSchema(null, DataType.CHARARRAY));
        s.add(new Schema.FieldSchema(null, DataType.CHARARRAY));
        funcList.add(new FuncSpec(ToDate2ARGS.class.getClass().getName(), s));
        s = new Schema();
        s.add(new Schema.FieldSchema(null, DataType.CHARARRAY));
        s.add(new Schema.FieldSchema(null, DataType.CHARARRAY));
        s.add(new Schema.FieldSchema(null, DataType.CHARARRAY));
        funcList.add(new FuncSpec(ToDate3ARGS.class.getClass().getName(), s));

XXXX.class.getClass().getName() should be changed to XXXX.class.getName().",,,,,,,,,,,,,,,,,,,PIG-2982,PIG-1314,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-12-03 03:40:44.244,,,no_permission,,,,,,,,,,,,250212,,,,Mon Dec 03 03:40:44 UTC 2012,,,,,,,0|i0aw8n:,61496,,,,,,,,,,22/Oct/12 00:16;zjshen;The fix is included in the patch for PIG-2982.,"03/Dec/12 03:40;cheolsoo;PIG-2982 incorporated the fix, so closing the jira.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Illustrate for Rank Operator,PIG-2989,12612331,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,xalan,xalan,xalan,17/Oct/12 22:35,14/Oct/13 16:46,14/Mar/19 03:07,19/Nov/12 01:36,0.11,,,,,,,0.12.0,,,,build,,,0,,,,,,,,,,,,,"Specifically useful, when it's required a quick view of final results of Rank operator use.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Oct/12 22:37;xalan;patch_1;https://issues.apache.org/jira/secure/attachment/12549595/patch_1,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-06 14:52:44.476,,,no_permission,,,,,,,,,,,,249427,,,,Tue Nov 13 20:27:35 UTC 2012,,,,,,,0|i0a87b:,57602,,,,,,,,,,17/Oct/12 22:37;xalan;All junit and e2e tests passed.,17/Oct/12 22:39;xalan;a small update of Rank operator,"17/Oct/12 22:42;xalan;While I was testing, I noticed this issue.",06/Nov/12 14:52;alangates;Patch looks good.  Unit tests look good.  I'll run the e2e tests and commit if all is well.,06/Nov/12 16:17;xalan;Thanks Alan for checking it! ,13/Nov/12 16:37;alangates;Patch committed to trunk.  Thanks Allan.,13/Nov/12 16:58;xalan;Thanks to you!,"13/Nov/12 17:19;azaroth;Has this been committed?
When I try to apply the patch to trunk I get an error:
{code}
> patch -p0 < patch_1 
patching file src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
Reversed (or previously applied) patch detected!  Assume -R? [n] n
Apply anyway? [n] y
Hunk #1 FAILED at 326.
1 out of 1 hunk FAILED -- saving rejects to file src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java.rej
patching file src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/PORank.java
Reversed (or previously applied) patch detected!  Assume -R? [n] n
Apply anyway? [n] y
Hunk #1 FAILED at 156.
1 out of 1 hunk FAILED -- saving rejects to file src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/PORank.java.rej
{code}
",13/Nov/12 20:27;alangates;It's complaining because I've already applied the patch.  See http://svn.apache.org/viewvc?view=revision&revision=1408827 for a record of what was committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
documentation for DateTime datatype,PIG-2980,12611930,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,zjshen,thejas,thejas,15/Oct/12 23:12,22/Feb/13 04:53,14/Mar/19 03:07,20/Nov/12 03:28,,,,,,,,0.11,,,,documentation,,,0,,,,,,,,,,,,,"Documentation for new DateTime type needs to be added.
",,,,,,,,,,,PIG-2756,,,,,,,,,,,,,,,,,,,,,20/Nov/12 03:23;thejas;PIG-2980.2.patch;https://issues.apache.org/jira/secure/attachment/12554286/PIG-2980.2.patch,05/Nov/12 03:02;zjshen;PIG-2980.patch;https://issues.apache.org/jira/secure/attachment/12552058/PIG-2980.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-22 23:28:47.756,,,no_permission,,,,,,,,,,,,248850,,,,Tue Nov 20 03:28:11 UTC 2012,,,,,,,0|i0a0n3:,56377,,,,,,,,,,22/Oct/12 23:28;olgan;Who would be a good person to provide content. I would be happy to create and commit the patch,"22/Oct/12 23:44;thejas;Olga,
Zhijie is planning to work on this. If you can help with the formatting, that would be great!","22/Oct/12 23:45;olgan;Sounds good. Zhijie, please, re-assign to me once you provide the information, thanks!","05/Nov/12 03:02;zjshen;Hi Thejas and Olga,

here's the documentation for datetime. Please help proof-reading. Thanks!

Regards,
Zhijie","08/Nov/12 04:09;thejas;Thanks for the patch Zhijie. It looks good.
But it says that datestamp constants are supported, but I guess if you pass '1970-01-01T00:00:00.000+00:00' to pig (say as an argument to a udf), i believe it would get interpreted as a string . Ie, we support chararray constants that can be cast to datetime, but not a datetime constant per se. Is that correct ? 
(I think it makes sense to support datetime constants, using a format that does not cause ambiguity wrt chararray type. But that would be another jira).

","08/Nov/12 16:10;zjshen;Yes, I mean ToDate('1970-01-01T00:00:00.000+00:00'). where users can specify a constant string to create a datetime object. Let me rephrase the description here.

","09/Nov/12 20:08;thejas;bq. Yes, I mean ToDate('1970-01-01T00:00:00.000+00:00'). where users can specify a constant string to create a datetime object. Let me rephrase the description here.
I think we can just remove datestamp from the constants table and add a note under the table, that users should use ToDate udf to generate datetime from string constants. 
 ","20/Nov/12 03:23;thejas;Updated patch with small change to remove datestamp field from the constants table.
","20/Nov/12 03:28;thejas;Patch committed to trunk and branch-0.11
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Umbrella ticket for test failures in 0.11/trunk,PIG-2972,12611880,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,rohini,rohini,15/Oct/12 18:51,22/Feb/13 04:53,14/Mar/19 03:07,23/Jan/13 00:19,0.11,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-01-23 00:19:19.86,,,no_permission,,,,,,,,,,,,248790,,,,Wed Jan 23 00:19:19 UTC 2013,,,,,,,0|i09zzj:,56271,,,,,,,,,,"15/Oct/12 19:04;rohini;Test failures with existing jiras:
General:
  * TestIndexedStorage [PIG-2737]
  * TestPigServerWithMacros [PIG-2913]

Platform related failures:
  * TestPoissonSampleLoader [PIG-2926]
  * CENT OS 6 Failures [PIG-2966]
  * jdk7 failures  [PIG-2908]

e2e Test Failures:
  * FilterBoolean_23/24 [PIG-2928]
",23/Jan/13 00:19;cheolsoo;Closing it since all sub-tasks are resolved or moved out of 0.11.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add new parameter to specify the streaming environment,PIG-2971,12611878,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,jcoveney,jcoveney,15/Oct/12 18:48,22/Feb/13 04:54,14/Mar/19 03:07,15/Oct/12 20:51,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,The current proposal in PIG-2925 is a bit arbitrary. How do we set what is and is not too big? This instead let's the user set what they need.,,,,,,,,,,,,,,,,,,,PIG-3001,,,,,,,,,,,PIG-2925,,15/Oct/12 19:00;jcoveney;PIG-2971-0.patch;https://issues.apache.org/jira/secure/attachment/12549188/PIG-2971-0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-15 20:47:54.879,,,no_permission,,,,,,,,,,,,248788,,,,Mon Oct 15 21:13:23 UTC 2012,,,,,,,0|i09zz3:,56269,,,,,,,,,,15/Oct/12 19:00;jcoveney;Here is a patch with tests. Passes test-commit.,"15/Oct/12 20:47;julienledem;+1 looks good
Thanks!","15/Oct/12 21:13;dvryaboy;Please update documentation, and a commented out example in the default pig.properties.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nested foreach getting incorrect schema when having unrelated inner query,PIG-2970,12611632,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,daijy,knoguchi,knoguchi,12/Oct/12 20:58,19/Mar/14 18:44,14/Mar/19 03:07,02/May/13 00:46,0.10.0,,,,,,,0.12.0,,,,parser,,,0,,,,,,,,,,,,,"While looking at PIG-2968, hit a weird error message.

{noformat}
$ cat -n test/foreach2.pig
     1  daily = load 'nyse' as (exchange, symbol);
     2  grpd = group daily by exchange;
     3  unique = foreach grpd {
     4          sym = daily.symbol;
     5          uniq_sym = distinct sym;
     6          --ignoring uniq_sym result
     7          generate group, daily;
     8  };
     9  describe unique;
    10  zzz = foreach unique generate group;
    11  explain zzz;

% pig -x local -t ColumnMapKeyPrune test/foreach2.pig
...
unique: {symbol: bytearray}

2012-10-12 16:55:44,226 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1025: 
<file test/foreach2.pig, line 10, column 30> Invalid field projection. Projected field [group] does not exist in schema: symbol:bytearray.
...
{noformat}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Apr/13 22:16;daijy;PIG-2970-0.patch;https://issues.apache.org/jira/secure/attachment/12577304/PIG-2970-0.patch,06/Apr/13 00:18;daijy;PIG-2970-1.patch;https://issues.apache.org/jira/secure/attachment/12577334/PIG-2970-1.patch,07/Apr/13 06:25;daijy;PIG-2970-2.patch;https://issues.apache.org/jira/secure/attachment/12577431/PIG-2970-2.patch,12/Oct/12 23:09;knoguchi;pig-2970-trunk-v01.txt;https://issues.apache.org/jira/secure/attachment/12548981/pig-2970-trunk-v01.txt,02/Nov/12 15:40;knoguchi;pig-2970-trunk-v02.txt;https://issues.apache.org/jira/secure/attachment/12551874/pig-2970-trunk-v02.txt,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2012-10-12 23:13:59.775,,,no_permission,,,,,,,,,,,,248119,Reviewed,,,Wed Mar 19 18:44:11 UTC 2014,,,,,,,0|i09llb:,53931,,,,,,,,,,"12/Oct/12 23:09;knoguchi;Parser assumes foreach's innerPlan has a single generate op as sink.  When this fails, it is returning an incorrect schema.  Attaching a patch that moves all the unrelated ops to LOGenerate.

My understanding of pig is weak.  Appreciate if someone can take a careful look. Thanks.","12/Oct/12 23:13;azaroth;Haven't had time to look at the patch, but I guess it is related to PIG-2119.
I thought we had solved it though.","31/Oct/12 17:06;rohini;Gianmarco,
 Thanks for the pointer. They are related. The plan for your example in PIG-2219 gets generated as 

{noformat}
a = load 'b.txt' AS (id:chararray, num:int);
b = group a by id;
c = foreach b { 
  d = order a by num DESC;
  n = COUNT(a);
  e = limit d 1;
  generate n;
}

|---c: (Name: LOForEach Schema: id#1:chararray,num#2:int)
    |   |
    |   e: (Name: LOLimit Schema: id#1:chararray,num#2:int)
    |   |
    |   |---d: (Name: LOSort Schema: id#1:chararray,num#2:int)
    |       |   |
    |       |   num:(Name: Project Type: int Uid: 2 Input: 0 Column: 1)
    |       |
    |       |---a: (Name: LOInnerLoad[1] Schema: id#1:chararray,num#2:int)
    |   
    |   (Name: LOGenerate[false] Schema: #6:long)
    |   |   |
    |   |   (Name: UserFunc(org.apache.pig.builtin.COUNT) Type: long Uid: 6)
    |   |   |
    |   |   |---a:(Name: Project Type: bag Uid: 3 Input: 0 Column: (*))
    |   |
    |   |---a: (Name: LOInnerLoad[1] Schema: id#1:chararray,num#2:int)

For the query in this example:
|---c: (Name: LOForEach Schema: v1#2:bytearray)
        |   |
        |   e: (Name: LODistinct Schema: v1#2:bytearray)
        |   |
        |   |---d: (Name: LOForEach Schema: v1#2:bytearray)
        |       |   |
        |       |   (Name: LOGenerate[false] Schema: v1#2:bytearray)
        |       |   |   |
        |       |   |   v1:(Name: Project Type: bytearray Uid: 2 Input: 0 Column: (*))
        |       |   |
        |       |   |---(Name: LOInnerLoad[1] Schema: v1#2:bytearray)
        |       |
        |       |---a: (Name: LOInnerLoad[1] Schema: id#1:bytearray,v1#2:bytearray)
        |   
        |   (Name: LOGenerate[false,false] Schema: group#1:bytearray,a#3:bag{#5:tuple(id#1:bytearray,v1#2:bytearray)})
        |   |   |
        |   |   group:(Name: Project Type: bytearray Uid: 1 Input: 0 Column: (*))
        |   |   |
        |   |   a:(Name: Project Type: bag Uid: 3 Input: 1 Column: (*))
        |   |
        |   |---(Name: LOInnerLoad[0] Schema: group#1:bytearray)
        |   |
        |   |---a: (Name: LOInnerLoad[1] Schema: id#1:bytearray,v1#2:bytearray)


{noformat}

The problem is Schema for ForEach gets set based on the first leaf. In your case, Schema for both the leaves contained the same required fields and so there was no error. In Koji's case the schema is different for both the leaves and hence the error. 

Koji,
   Connecting both the separate nodes just to get the schema correct changes the schema in such a way that it is not dangling anymore. My take on this is that we should move the DanglingNestedNodeRemover (which was wrote to handle this scenario) from HExecutionEngine to LogicalPlanBuilder.buildForeachOp() before the SchemaResetter is called through expandAndResetVisitor, so that the dangling node is removed during construction itself and the correct schema is set by SchemaResetter. Thoughts?","31/Oct/12 20:29;azaroth;If it works and does not interfere with other parts of the plan building, I think this is a good approach.","02/Nov/12 15:40;knoguchi;bq. (from PIG-2119) LOGenerate is the only sink in nested plan. This is one of the basic assumption we made throughout logical plan

Can we revisit this assumption to 'sink in nested plan includes a single LOGenerate and optional dangling nodes'?  Attaching a patch that replaces foreach.getInnerPlan().getSinks().get(0) to foreach.findGenearate() that I added.  Would something like this work?


","02/Nov/12 17:24;rohini;Koji,
   The idea seems to be to keep the dangling nodes intact in the plan and do a search for LOGenerate node in LOForeach every time we are trying to get it from LOForEach. I don't prefer this approach much. If something is not used, it is better to discard it instead of keeping it and stepping around it all the time. The approach might work but not sure what it achieves or gives any benefit. Also it is not efficient. I would prefer to stay with the original assumption of LOGenerate is the only sink in nested plan. It keeps the plan simple, clean and sticks to the intent of the execution. Can you give more info on why you would like to keep the dangling nodes in the plan?","02/Nov/12 17:42;knoguchi;bq. Can you give more info on why you would like to keep the dangling nodes in the plan?

Even if we move DanglingNestedNodeRemover to earlier stage as suggested, isn't there always a risk of some other optimization rewrite creating another danglingnested nodes?  Instead of getting rid of them every time it pops up, I thought we can clean them up later in code for optimization purposes (and not for correctness).



","02/Nov/12 17:46;rohini;bq. Even if we move DanglingNestedNodeRemover to earlier stage as suggested, isn't there always a risk of some other optimization rewrite creating another danglingnested nodes
   If an optimization rewrite is causing a dangling node then it is doing something wrong. We should fix that to not create dangling nodes instead of having another step to remove it later.  ","02/Nov/12 18:03;knoguchi;bq.  for optimization purposes (and not for correctness).
I guess I'm wanting to treat DanglingNestedNodeRemover as equivalent of ColumnMapKeyPrune such that it's turned on for performance purposes but could be turned off and still produces the same (correct) results. 

As for, 
bq.  it is not efficient. 
I don't think it adds any noticeable overhead and also it's only at compile time.
","20/Nov/12 10:39;snnn;I encountered the same problem.
The test data file is a text file, each line contains a integer number.
[app_admin@test14 data]$ more /tmp/data/data1.txt 
1
2
3
4
5
6

The pig script:

a =  load '/tmp/data/data1.txt' USING PigStorage()  as (id);
g = group a by id;
d = foreach g {
b1 = FILTER a by id > 1;
b2 = FILTER a by id > 2;
b3 = FILTER a by id > 3;
GENERATE COUNT(b1.id) as c1 ,COUNT(b2.id) as c2 ;
}

describe d;

the result is : d: {id: bytearray}","05/Apr/13 22:16;daijy;I feel the fix is too complex. ""explain"" can figure out the right schema but ""describe"" cannot. The difference is ""explain"" does the optimization but ""describe"" does not. We only need to bring the optimization to ""describe"" to fix the issue. I attach a temporary fix, but we only need ""optimization"" part of ""compilePp"". So a better fix is separate ""compilePp"" and only do the optimization part before ""describe"". I will attach a patch shortly.","06/Apr/13 00:18;daijy;Koji also point to me the query cannot run. That's because DanglingNestedNodeRemover runs too late. In PIG-2970-1.patch, I did the following:
1. Move DanglingNestedNodeRemover before all other logical plan adjustment
2. Make sure ""describe"" go through optimizor

Most of the code is restructure. I separate HExecutionEngine.compile into three function unit. In ""describe"", we only need the first unit.",07/Apr/13 06:25;daijy;Some unit test fail with PIG-2970-1.patch. Actually we cannot do optimization when do describe. Optimizor will move operators around and distort the schema. Attach a more conservative patch which move DanglingNestedNodeRemover earlier. All unit tests pass with PIG-2970-2.patch.,29/Apr/13 20:34;rohini;+1. ,02/May/13 00:46;daijy;Patch committed to trunk.,"18/Mar/14 23:44;daijy;When working on PIG-3807, realize no matter how we push up DanglingNestedNodeRemover, we will still invoke getSchema in LogicalPlanBuilder. [~horaguchi] is right, we shall iterate inner plan sinks to find LOGenerate. This is applicable to LOForeach.getSchema(). After DanglingNestedNodeRemover, it is safe to assume LOGenerate is the only sink, so we don't need to sprawl the change to optimizer. I will bring some of Koji's change in PIG-3807.","19/Mar/14 16:52;knoguchi;bq. Koji Horaguchi is right, we shall iterate inner plan sinks to find LOGenerate. This is applicable to LOForeach.getSchema(). After DanglingNestedNodeRemover, it is safe to assume LOGenerate is the only sink, so we don't need to sprawl the change to optimizer. I will bring some of Koji's change in PIG-3807.

Thanks Daniel, but a different Koji again.  It's [~knoguchi]. :)
","19/Mar/14 18:44;daijy;Oh, sorry [~knoguchi] :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnMapKeyPrune fails to prune a subtree inside foreach,PIG-2968,12611617,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,knoguchi,knoguchi,knoguchi,12/Oct/12 19:01,22/Feb/13 04:53,14/Mar/19 03:07,30/Oct/12 17:56,,,,,,,,0.11,,,,parser,,,0,,,,,,,,,,,,,"Sample code 

{noformat}
$ cat test/foreach.pig 
daily = load 'nyse' as (exchange, symbol);
grpd = group daily by exchange;
uniquecnt = foreach grpd {
        sym = daily.symbol;
        uniq_sym = distinct sym;
        generate group, uniq_sym;
};
another = FOREACH uniquecnt GENERATE group;
explain another;

{noformat}

This breaks when it tries to prune uniq_sym->sym->innerload_daily

bq. 2012-10-12 14:54:11,031 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2000: Error processing rule ColumnMapKeyPrune. Try -t ColumnMapKeyPrune

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Oct/12 20:04;knoguchi;pig-2968-trunk_v01.txt;https://issues.apache.org/jira/secure/attachment/12548949/pig-2968-trunk_v01.txt,15/Oct/12 13:01;knoguchi;pig-2968-trunk_v02.txt;https://issues.apache.org/jira/secure/attachment/12549143/pig-2968-trunk_v02.txt,29/Oct/12 23:33;knoguchi;pig-2968-trunk_v03.txt;https://issues.apache.org/jira/secure/attachment/12551279/pig-2968-trunk_v03.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-10-15 03:55:17.737,,,no_permission,,,,,,,,,,,,248100,,,,Tue Oct 30 17:56:57 UTC 2012,,,,,,,0|i09lfz:,53907,,,,,,,,,,"12/Oct/12 19:02;knoguchi;Log showing
{noformat}
$ cat /Users/knoguchi/git/pig/pig_1350068049281.logPig Stack Trace
---------------
ERROR 2000: Error processing rule ColumnMapKeyPrune. Try -t ColumnMapKeyPrune

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1067: Unable to explain alias another        at org.apache.pig.PigServer.explain(PigServer.java:999)
        at org.apache.pig.tools.grunt.GruntParser.explainCurrentBatch(GruntParser.java:398)
        at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:330)
        at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:293)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.Explain(PigScriptParser.java:715)        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:342)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:193)        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:169)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:84)        at org.apache.pig.Main.run(Main.java:604)
        at org.apache.pig.Main.main(Main.java:154)
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2000: Error processing rule ColumnMapKeyPrune. Try -t ColumnMapKeyPrune        at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:122)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:277)
        at org.apache.pig.PigServer.compilePp(PigServer.java:1322)        at org.apache.pig.PigServer.explain(PigServer.java:984)
        ... 10 more
Caused by: java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.pig.newplan.logical.rules.ColumnPruneVisitor.removeSubTree(ColumnPruneVisitor.java:451)
        at org.apache.pig.newplan.logical.rules.ColumnPruneVisitor.removeSubTree(ColumnPruneVisitor.java:452)
        at org.apache.pig.newplan.logical.rules.ColumnPruneVisitor.visit(ColumnPruneVisitor.java:431)
        at org.apache.pig.newplan.logical.relational.LOForEach.accept(LOForEach.java:76)
        at org.apache.pig.newplan.ReverseDependencyOrderWalker.walk(ReverseDependencyOrderWalker.java:70)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:52)
        at org.apache.pig.newplan.logical.rules.ColumnMapKeyPrune$ColumnMapKeyPruneTransformer.transform(ColumnMapKeyPrune.java:141)
        at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:110)
        ... 13 more
================================================================================
{noformat}","12/Oct/12 20:04;knoguchi;ConcurrentModificationException is happening in ColumnPruneVisitor.removeSubTree

{noformat}
List<Operator> ll = p.getPredecessors(op);
if (ll != null) { 
    for(Operator pred: ll) { 
        removeSubTree((LogicalRelationalOperator)pred);
    } 
} 
{noformat} 
where List ll is updated within (recursive-)removeSubTree
while being traversed.

Attaching a patch that creates a shallow copy of the list.

For the test, attaching one that will fail with Exception.  But I couldn't come up with _expected_ raw query that matches the pruned result.",15/Oct/12 03:55;rohini;ll.toArray(new Operator[ll.size()]) can be done instead of ll.toArray(new Operator[0]) for little more efficiency. Looks good otherwise. ,"15/Oct/12 13:01;knoguchi;bq. ll.toArray(new Operator[ll.size()]) can be done instead of ll.toArray(new Operator[0]) 

Uploading with the above change.",29/Oct/12 13:09;knoguchi;I'd like to see this fixed in 0.11.,"29/Oct/12 20:45;cheolsoo;+1.

I will commit it after running tests. Thanks Koji!","29/Oct/12 20:53;cheolsoo;Actually, can you please fix the indentation of testPruneSubTreeForEach() using 4 spaces? We're trying to clean up white spaces in code base.

Thanks!
","29/Oct/12 20:55;rohini;Koji/Cheolsoo,
bq. But I couldn't come up with expected raw query that matches the pruned result.
   I missed commenting on this in the initial review. Should we take a crack at this before committing this. Or at least some Assert? ","29/Oct/12 21:10;cheolsoo;Hi Rohini, sure I can wait until your concern is addressed.

If Koji can improve the test case, that will be of course great. But since I don't have a better suggestion, I won't insist.

Thanks!","29/Oct/12 23:33;knoguchi;bq. Actually, can you please fix the indentation of testPruneSubTreeForEach() using 4 spaces?

Changed. (Sorry, I recently learned it's 4 spaces instead of 2.)

bq. Or at least some Assert? 

Added a catch and fail call.


bq. Should we take a crack at this before committing this

I tried simply taking out the lines but nested foreach with pruned inputs were different from a simple foreach with just one output. (one extra foreach on former)
","30/Oct/12 15:14;rohini;bq. I tried simply taking out the lines but nested foreach with pruned inputs were different from a simple foreach with just one output. (one extra foreach on former)
  Got it Koji. ColumnPruneVisitor.addForEachIfNecessary adds an additional foreach to prune columns for optimization even though the next statement from user does exactly the same. Don't think it is possible to have a equivalent query as the attached LOForEach is without an alias. I am good Cheolsoo. You can go ahead with the commit.",30/Oct/12 17:56;cheolsoo;Committed to 0.11/trunk. Thanks Koji!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Illustrate command and POPackageLite,PIG-2963,12611188,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,cheolsoo,xalan,xalan,10/Oct/12 14:52,22/Feb/13 04:54,14/Mar/19 03:07,15/Oct/12 19:56,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"While trying to execute a simple script like:

A = LOAD 'test01' AS (f1:chararray,f2:int,f3:chararray);
B = order A by f1;
illustrate B;

or 

C = foreach B generate f1, f2; 
illustrate C;

I got the following exception:

java.lang.RuntimeException: ReadOnceBag does not support getMemorySize operation
	at org.apache.pig.data.ReadOnceBag.getMemorySize(ReadOnceBag.java:74)
	at org.apache.pig.data.SizeUtil.getPigObjMemSize(SizeUtil.java:61)
	at org.apache.pig.data.DefaultTuple.getMemorySize(DefaultTuple.java:180)
	at org.apache.pig.pen.util.ExampleTuple.getMemorySize(ExampleTuple.java:97)
	at org.apache.pig.data.DefaultAbstractBag.getMemorySize(DefaultAbstractBag.java:148)
	at org.apache.pig.data.DefaultAbstractBag.markSpillableIfNecessary(DefaultAbstractBag.java:100)
	at org.apache.pig.data.DefaultAbstractBag.add(DefaultAbstractBag.java:92)
	at org.apache.pig.pen.Illustrator.addData(Illustrator.java:116)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackageLite.illustratorMarkup(POPackageLite.java:227)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackageLite.getNext(POPackageLite.java:182)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:422)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:413)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:257)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.pig.pen.LocalMapReduceSimulator.launchPig(LocalMapReduceSimulator.java:235)
	at org.apache.pig.pen.ExampleGenerator.getData(ExampleGenerator.java:257)
	at org.apache.pig.pen.ExampleGenerator.getData(ExampleGenerator.java:238)
	at org.apache.pig.pen.LineageTrimmingVisitor.init(LineageTrimmingVisitor.java:103)
	at org.apache.pig.pen.LineageTrimmingVisitor.<init>(LineageTrimmingVisitor.java:98)
	at org.apache.pig.pen.ExampleGenerator.getExamples(ExampleGenerator.java:166)
	at org.apache.pig.PigServer.getExamples(PigServer.java:1180)
	at org.apache.pig.tools.grunt.GruntParser.processIllustrate(GruntParser.java:738)
	at org.apache.pig.tools.pigscript.parser.PigScriptParser.Illustrate(PigScriptParser.java:626)
	at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:323)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:193)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:169)
	at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:69)
	at org.apache.pig.Main.run(Main.java:538)
	at org.apache.pig.Main.main(Main.java:154)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
======================================================================

At log file, the following:

Pig Stack Trace
---------------
ERROR 2997: Encountered IOException. Exception

java.io.IOException: Exception
        at org.apache.pig.PigServer.getExamples(PigServer.java:1186)
        at org.apache.pig.tools.grunt.GruntParser.processIllustrate(GruntParser.java:738)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.Illustrate(PigScriptParser.java:626)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:323)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:193)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:169)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:69)
        at org.apache.pig.Main.run(Main.java:538)
        at org.apache.pig.Main.main(Main.java:154)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
Caused by: java.lang.RuntimeException: ReadOnceBag does not support getMemorySize operation
        at org.apache.pig.data.ReadOnceBag.getMemorySize(ReadOnceBag.java:74)
        at org.apache.pig.data.SizeUtil.getPigObjMemSize(SizeUtil.java:61)
        at org.apache.pig.data.DefaultTuple.getMemorySize(DefaultTuple.java:180)
        at org.apache.pig.pen.util.ExampleTuple.getMemorySize(ExampleTuple.java:97)
        at org.apache.pig.data.DefaultAbstractBag.getMemorySize(DefaultAbstractBag.java:148)
        at org.apache.pig.data.DefaultAbstractBag.markSpillableIfNecessary(DefaultAbstractBag.java:100)
        at org.apache.pig.data.DefaultAbstractBag.add(DefaultAbstractBag.java:92)
        at org.apache.pig.pen.Illustrator.addData(Illustrator.java:116)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackageLite.illustratorMarkup(POPackageLite.java:227)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackageLite.getNext(POPackageLite.java:182)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:422)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:413)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:257)
        at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
        at org.apache.pig.pen.LocalMapReduceSimulator.launchPig(LocalMapReduceSimulator.java:235)
        at org.apache.pig.pen.ExampleGenerator.getData(ExampleGenerator.java:257)
        at org.apache.pig.pen.ExampleGenerator.getData(ExampleGenerator.java:238)
        at org.apache.pig.pen.LineageTrimmingVisitor.init(LineageTrimmingVisitor.java:103)
        at org.apache.pig.pen.LineageTrimmingVisitor.<init>(LineageTrimmingVisitor.java:98)
        at org.apache.pig.pen.ExampleGenerator.getExamples(ExampleGenerator.java:166)
        at org.apache.pig.PigServer.getExamples(PigServer.java:1180)",,,,,,,,,,,PIG-2989,,,,,,,,,,,,,,,,,,,,,10/Oct/12 22:32;cheolsoo;PIG-2963.patch;https://issues.apache.org/jira/secure/attachment/12548652/PIG-2963.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-10 22:32:32.202,,,no_permission,,,,,,,,,,,,246998,,,,Tue Oct 16 08:07:42 UTC 2012,,,,,,,0|i07xqn:,44233,,,,,,,,,,"10/Oct/12 22:32;cheolsoo;This is a regression from PIG-2923. Verified that the error goes away by reverting it.

What's happening is that illustrate adds to DefautAbstractBag a tuple that has a ReadOnceBag field.

The problem is that PIG-2923 made DefautAbstractBag check whether or not it should spill to disk every time a new element is added to it. To compute the memory size of the new element, DefautAbstractBag iterates through every field of the tuple, in this case which is ReadOnceBag. Unfortunately, ReadOnceBag doesn't support getMemorySize() and throws a runtime exception.

I am attaching a simple fix that makes ReadOnceBag.getMemorySize() return 0 instead of throwing a runtime exception. I am returning 0 here because the comment in ReadOnceBag says ""this bag does not store the tuples in memory"".

I am not familiar with ReadOnceBag, so please correct me if this is not a proper fix.

Thanks!","15/Oct/12 12:09;xalan;Thank Cheolsoo! It's working now. 
Now, I'm wondering which is the process to include it into the trunk. 

Thanks again!","15/Oct/12 19:56;jcoveney;Allan: it comes down to pestering a committer to reviewing and committing, or just hoping they look at it.

Which I just did. Thanks for the fix, Cheolsoo. It's in!",16/Oct/12 08:07;xalan;Thanks Jonathan for the explanation and for committing it!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BinInterSedesRawComparator broken by TUPLE_number patch,PIG-2961,12610848,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,jcoveney,jcoveney,jcoveney,08/Oct/12 17:30,22/Feb/13 04:53,14/Mar/19 03:07,08/Oct/12 18:04,0.11,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"In trunk, this code is broken:

A = LOAD 'test01' AS (f1:chararray,f2:int,f3:chararray);
B = order A by f1,f2,f3 DESC;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Oct/12 17:31;jcoveney;PIG-2961-0.patch;https://issues.apache.org/jira/secure/attachment/12548259/PIG-2961-0.patch,08/Oct/12 18:00;jcoveney;PIG-2961-1.patch;https://issues.apache.org/jira/secure/attachment/12548263/PIG-2961-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-08 17:37:33.551,,,no_permission,,,,,,,,,,,,245599,,,,Mon Oct 08 18:01:57 UTC 2012,,,,,,,0|i06bzz:,34878,,,,,,,,,,"08/Oct/12 17:37;julienledem;+1
Also please turn the example you provide above into a unit test.
Thank you Jonathan",08/Oct/12 18:00;jcoveney;Attached a new patch with tests.,"08/Oct/12 18:01;julienledem;+1
Thank you!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix tiny documentation error in BagToString builtin.,PIG-2950,12610625,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,initialcontext,initialcontext,initialcontext,05/Oct/12 20:13,22/Feb/13 04:53,14/Mar/19 03:07,22/Oct/12 04:25,,,,,,,,0.11,,,,documentation,,,0,newbie,,,,,,,,,,,,"The default delimiter character for BagToString is not '-' as listed in the docs but actually '_'

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Oct/12 20:15;initialcontext;PIG-2950-1.patch;https://issues.apache.org/jira/secure/attachment/12548035/PIG-2950-1.patch,09/Oct/12 15:02;initialcontext;PIG-2950-2.patch;https://issues.apache.org/jira/secure/attachment/12548409/PIG-2950-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-22 04:25:51.419,,,no_permission,,,,,,,,,,,,244067,Reviewed,,,Mon Oct 22 04:25:51 UTC 2012,,,Patch Available,,,,0|i05glj:,29786,,,,,,,,,,"09/Oct/12 15:02;initialcontext;Small fix: as long as we're repairing docs, lets use proper sentence structure, etc.",22/Oct/12 04:25;daijy;Patch committed to 0.11/trunk. Thanks Eli!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JsonLoader only reads arrays of objects ,PIG-2949,12610618,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,eyal,foodneutrino,foodneutrino,05/Oct/12 19:11,02/Feb/17 22:48,14/Mar/19 03:07,06/Jan/15 01:52,0.10.0,,,,,,,0.15.0,,,,,,,0,,,,,,,,,,,,,"I'm trying to load a vendor file that's json ecoded into pig. One of the fields is an array of strings. The builtin JsonLoader only reads arrays composed of json objects


{""object_array}:[{""element"":""value1""},{""element"":""value2""}]} works
but
{""string_array""}:[""value1"",""value2""] does not",os x mountain lion,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Oct/12 19:13;foodneutrino;JsonLoader.java;https://issues.apache.org/jira/secure/attachment/12548028/JsonLoader.java,27/Nov/14 15:33;eyal;PIG-2949.patch;https://issues.apache.org/jira/secure/attachment/12684077/PIG-2949.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-05-20 00:04:07.637,,,no_permission,,,,,,,,,,,,244051,Reviewed,,,Thu Feb 02 22:48:31 UTC 2017,,,Patch Available,,,,0|i05fjz:,29617,,,,,,,,,,05/Oct/12 19:13;foodneutrino;fix for reading json encoded string arrays,"05/Oct/12 19:18;foodneutrino;the schema i tested with was something like 
words:{(word:chararray)}","20/May/13 00:04;siva_2680;Hi ,
Please let me know anybody resolved with the following issue.

Thanks
Siva",24/May/13 21:10;daijy;Does the attached fix work for you? We can commit this patch if works.,"02/Jun/13 15:14;foodneutrino;I switched to elephant bird's loader which handles this case and more complex json objects.

David

Sent from my iPad


","13/Nov/14 09:38;eyal;David's fix seems to work, but maybe we want to accept arrays of the general form {""arrayfield"":[fld, fld, fld]} where fld is any field type, not just a string? Non-string ""primitive"" arrays are also valid json.","27/Nov/14 15:33;eyal;This is a patch + unit test that is a modification of David's original patch. It will allow loading any array of primitives (any field other than bag, map or tuple) without repeating the object name, as this issue describes.

For example [ 1,2,3 ] or [ ""ab"", ""cd"", ""ef""]",06/Jan/15 01:52;daijy;Patch committed to trunk. Thanks Eyal!,"15/Jan/15 09:33;eyal;Will there be a 0.13.1 or 0.14.1 release? If so, are bug fixes like this and the other json fixes (which are independent of any other changes, probably) candidates for inclusion?","15/Jan/15 19:04;daijy;There might be a 0.14.1 release, but I don't think there will be a 0.13.1 release. Usually we only include critical bug fixes in minor releases. New features only goes to major releases.","03/May/16 19:46;rohini;[~sivashankar.c],
   Assignee field is for the person who is going to work on the issue and provide or has provided the fix for the issue.","02/Feb/17 22:48;shravan.padakanti;What if we want to store array in the same format? When we store using JSONStorage(), I see it stores as col:[""col1"":""val1"", ""col2"":val2]. How can we store col:[""val1"",""val2""].
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ivysettings.xml does not let you override .m2/repository,PIG-2944,12610367,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,raluri,raja@cmbasics.com,raja@cmbasics.com,04/Oct/12 18:00,06/Jan/13 23:57,14/Mar/19 03:07,04/Oct/12 18:29,0.10.0,0.11,0.9.3,,,,,0.10.1,0.11,0.9.3,,build,,,0,,,,,,,,,,,,,"ivysettings.xml does not let you override .m2/repository. This will be useful for people who wants to use a localrepository, thats outside of ~/.m2/repository",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Oct/12 18:07;raja@cmbasics.com;PIG-2944-1.patch;https://issues.apache.org/jira/secure/attachment/12547794/PIG-2944-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-04 18:29:58.257,,,no_permission,,,,,,,,,,,,240685,Reviewed,,,Thu Oct 04 18:29:58 UTC 2012,,,,,,,0|i014nr:,4524,,,,,,,,,,04/Oct/12 18:29;daijy;Patch committed to 0.9/0.10/trunk. Thanks Raja!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ivy resolvers in pig don't have consistent chaining and don't have a kitchen sink option for novices,PIG-2941,12610217,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jgordon,jgordon,jgordon,04/Oct/12 10:34,14/Oct/13 16:46,14/Mar/19 03:07,23/Oct/12 01:01,0.10.0,,,,,,,0.12.0,,,,build,,,0,,,,,,,,,,,,,"The Ivy resolvers in Pig are split into default, external, and internal -- and they are all actually distinct.  There isn't a resolver that rolls over all three, and fallbacks aren't in place.  Ideally, these resolver should chain right through with the default following a best practice fallback for novices.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Oct/12 10:35;jgordon;0001-IvySettings.xml-refactor-to-simplify-resolution.patch;https://issues.apache.org/jira/secure/attachment/12547711/0001-IvySettings.xml-refactor-to-simplify-resolution.patch,21/Oct/12 07:50;jgordon;PIG-2941.trunk.002.patch;https://issues.apache.org/jira/secure/attachment/12550174/PIG-2941.trunk.002.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-09 19:10:49.434,,,no_permission,,,,,,,,,,,,240036,,,,Tue Oct 23 01:42:11 UTC 2012,,,,,,,0|i00vm7:,3054,,,,,,,,,,04/Oct/12 10:35;jgordon;Patch to refactor resolvers into 3 cascading chains,"09/Oct/12 19:10;azaroth;Can you regenerate the patch with --no-prefix option and without the email headers?

Also, can you explain why maven2 is both on internal and external resolver?

Canceling patch for the moment.","12/Oct/12 05:10;jgordon;Great eye, thank you!  I think maven2 is in both due to a merge conflict.  Next patch will fix both.","21/Oct/12 07:50;jgordon;Updated to remove prefix and email headers.  Also adding update checks that route down to the fs resolver.

This allows the local mvn cache resolvers to pick up local cross-product changes instead of reading the stale .ivy2 cache.","21/Oct/12 07:54;jgordon;Allows cleaner resolution of changes to local builds of dependent projects for cross-project development, and leverages the local .m2 cache by default.  Default resolver is now the kitchen sink resolver that picks up all the rest, in order of locality.  Internal (default .m2 cache), then external.","23/Oct/12 01:01;azaroth;+1
Committed to trunk.

Thanks John!","23/Oct/12 01:42;jgordon;Thank you!
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBaseStorage store fails in secure cluster,PIG-2940,12609847,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,02/Oct/12 00:43,06/Jan/13 23:57,14/Mar/19 03:07,21/Oct/12 00:07,,,,,,,,0.10.1,0.11,,,,,,0,hbase,,,,,,,,,,,,"To reproduce ths issue, please do the following in secure hadoop/hbase cluster:
# On a gateway node, run kinit to obtain kerberos credentials and run a Pig script that includes a HBaseStorage load/store.
# In the front-end, HBaseStorage obtains a delegation token from hbase server and adds it to the JobConf object.
# In the back-end, mappers connect to hbase using the delegation token w/o kerberos credentials.

While load-from-hbase works perfectly fine, store-to-hbase fails. This is because at step 3, mappers attempt to obtain a delegation token from hbase in the back-end.
{code:title=setStoreLocation()}
// Not setting a udf property and getting the hbase delegation token
// only once like in setLocation as setStoreLocation gets different Job
// objects for each call and the last Job passed is the one that is
// launched. So we end up getting multiple hbase delegation tokens.
addHBaseDelegationToken(m_conf, job);
{code}
The problem is that mappers in the back-end don't have kerberos credentials, so the call to addHBaseDelegationToken() fails with the following error:
{code}
2012-09-30 14:33:42,310 ERROR [main] org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:testuser (auth:SIMPLE) cause:org.apache.hadoop.hbase.security.AccessDeniedException: org.apache.hadoop.hbase.security.AccessDeniedException: Token generation only allowed for Kerberos authenticated clients
	at org.apache.hadoop.hbase.security.token.TokenProvider.getAuthenticationToken(TokenProvider.java:87)
{code}
This is not an issue with load because a delegation token is only obtained in the front-end for the first time when HBASE_TOKEN_SET is not set.
{code:title=setLocation()}
String delegationTokenSet = udfProps.getProperty(HBASE_TOKEN_SET);
if (delegationTokenSet == null) {
    addHBaseDelegationToken(m_conf, job);
    udfProps.setProperty(HBASE_TOKEN_SET, ""true"");
}
{code}
The proposed fix is to modify addHBaseDelegationToken() so that tokens are obtained only if the current user has kerberos credentials, which is true in the front-end while false in the back-end.",,,,,,,,,,,,,,,,,,,PIG-2821,,,,,,,,,,,,,03/Oct/12 01:00;cheolsoo;PIG-2940-2.patch;https://issues.apache.org/jira/secure/attachment/12547476/PIG-2940-2.patch,02/Oct/12 00:48;cheolsoo;PIG-2940.patch;https://issues.apache.org/jira/secure/attachment/12547320/PIG-2940.patch,02/Oct/12 05:36;cheolsoo;container_log;https://issues.apache.org/jira/secure/attachment/12547345/container_log,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-10-02 04:45:11.979,,,no_permission,,,,,,,,,,,,242066,Reviewed,,,Mon Oct 22 06:54:36 UTC 2012,,,,,,,0|i02ifb:,12586,,,,,,,,,,"02/Oct/12 00:48;cheolsoo;Attached is a patch that implements the proposed fix.

I tested that both load/store with secure hbase and non-secure hbase.","02/Oct/12 04:45;rohini;[~cheolsoo],
   Are you sure it tries to get delegation token in the backend? Because the first check in the method is if it is front end.

{code}
private void addHBaseDelegationToken(Configuration hbaseConf, Job job) {
 	
 	if (!UDFContext.getUDFContext().isFrontend()) {
 	return;
 	}
        if (""kerberos"".equalsIgnoreCase(hbaseConf.get(HBASE_SECURITY_CONF_KEY))) {
 	..}

} 
{code}
     ","02/Oct/12 05:39;cheolsoo;Hi Rohini,

Thank you for chiming in. Yes, I am positive that this is happening in the back-end. Please see the attached screenshot.

In fact, I didn't realize that addHBaseDelegationToken() checks whether it's the front-end. That is really strange.","02/Oct/12 06:16;cheolsoo;Given that this is not supposed to happen, I am canceling patch available for now.","02/Oct/12 06:39;rohini;I think I know the problem. I did not test it with hadoop 23.  

{code}
 public boolean isFrontend() {
    	return (this.jconf == null || jconf.get(""mapred.task.id"") == null);
    }
{code}

mapred.task.id most likely is not in hadoop 0.23/2.0 and UDFContext.isFrontend() method needs to be fixed for hadoop 2.x.  ","02/Oct/12 06:58;cheolsoo;You're right. I ran into this problem only with MR2 and couldn't understand how that's possible. Now everything makes sense!

Fixing  UDFContext.isFrontend() seems like the right thing to do here.

Thanks! :-)","03/Oct/12 01:00;cheolsoo;Attaching a new patch that fixes UDFContext.isFrontend() and as a result, HBaseStorage store in MR2.

For MR1, it checks ""mapred.task.id"", and for MR2, it checks ""mapreduce.job.application.attempt.id"".",03/Oct/12 04:52;rohini;+1 non-binding.,03/Oct/12 05:03;rohini;Requesting that this patch be applied to 0.10 branch too. Thanks.,"21/Oct/12 00:07;daijy;Patch committed to trunk/0.11/0.10. Thanks Cheolsoo, Rohini!",21/Oct/12 05:05;sms;[~daijy] the patch did not have unit test cases!,"21/Oct/12 17:17;rohini;Santosh,
   At the moment it is not possible to have a unit test for this because of security. Whole security code path in Hadoop itself does not have unit tests primarily becoz of Kerberos. To actually add unit test to this, need a way to first bring up secure mini Hadoop and hbase clusters.
   Also hbase unit tests in pig do not run in 23 now. All stack components - pig, hbase, etc are yet to publish jars in maven compiled with hadoop23. Need to do that for pig with 0.11.","21/Oct/12 21:35;daijy;[~sms] The issue occurs in secure cluster only, there is no way to write a test right now. The patch itself looks pretty safe and I feel it is Ok to commit as per the criteria we adopted before.","22/Oct/12 06:54;sms;[~rohini] and [~daijy]: while the symptom was exhibited with HBaseStorage on a secure cluster, the actual fix was in the code that determined if the execution environment was the front end or the back end. IMHO, that part of the code should be unit tested independent of the HBaseStorage (secure or otherwise).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
All unit tests that use MR2 MiniCluster are broken in trunk,PIG-2938,12609463,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,cheolsoo,cheolsoo,cheolsoo,27/Sep/12 22:28,22/Feb/13 04:54,14/Mar/19 03:07,01/Oct/12 17:08,,,,,,,,0.11,,,,build,,,0,,,,,,,,,,,,,"To reproduce the issue, run:
{code}
ant test -Dhadoopversion=23 -Dtestcase=TestAccumulator
{code}
This fails with the following error:
{code}
Caused by: java.lang.ClassNotFoundException: org.jboss.netty.channel.group.ChannelGroup
{code}

This is a regression from PIG-2844 where netty is commented out from ivy.xml.

I guess that the intention was to exclude netty from the generated pom file, but this breaks unit tests.

I propose that we change the ivy conf of netty from ""compile-> master"" to ""test->master"" instead of commenting it out. That would prevent netty from being pulled down at compile time by 3rd party projects that consume the pig pom file.

I am speculating here, so please correct me if I am wrong.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Sep/12 22:32;cheolsoo;PIG-2938.patch;https://issues.apache.org/jira/secure/attachment/12546923/PIG-2938.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-28 18:11:05.37,,,no_permission,,,,,,,,,,,,242089,,,,Mon Oct 01 17:08:22 UTC 2012,,,,,,,0|i02imn:,12619,,,,,,,,,,"27/Sep/12 22:32;cheolsoo;Uploading a patch that puts back netty to ivy.xml, but changing the conf attribute from ""compile->master"" to ""test->master"".","28/Sep/12 18:11;julienledem;Sorry I broke the hadoop23 build.
This looks like a hadoop 23 runtime dependency, it should be in the hadoop23 configuration in ivy.xml
any reason it is not pulled automatically?","28/Sep/12 18:25;julienledem;It does not look like we have a separate hadoop23 test configuration.
I guess test is better than nothing.
+1","28/Sep/12 18:42;cheolsoo;Thanks Julien for the quick response.

I am open to suggestions, so please let me know if anyone has a better idea.","01/Oct/12 17:08;dvryaboy;Committed to trunk, thanks Cheolsoo.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
generated field in nested foreach does not inherit the variable name as the field name,PIG-2937,12609458,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,pengfeng,pengfeng,27/Sep/12 21:35,22/Feb/13 04:53,14/Mar/19 03:07,15/Nov/12 00:39,,,,,,,,0.11,0.12.0,,,,,,0,,,,,,,,,,,,,"{code}
raw_data = load 'xyz' using Loader() as (field_a, field_b, field_c);
records = foreach raw_data {
  generated_field = (field_a is null ? '-' : someUDF(field_b)); 
  GENERATE
    field_c,
    generated_field
  ;
}
describe records;
{code}

One would expect the generated_field to have a field name, similar to the field_c that is from the original relation. However, Pig currently doesn't assign the field name by default. It'd be nice if we can assign the variable name as the default field name. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Oct/12 18:01;jcoveney;PIG-2937-0.patch;https://issues.apache.org/jira/secure/attachment/12550657/PIG-2937-0.patch,24/Oct/12 18:21;jcoveney;PIG-2937-1.patch;https://issues.apache.org/jira/secure/attachment/12550660/PIG-2937-1.patch,07/Nov/12 00:05;jcoveney;PIG-2937-2.patch;https://issues.apache.org/jira/secure/attachment/12552367/PIG-2937-2.patch,13/Nov/12 23:18;jcoveney;PIG-2937-3_nowhitespace.patch;https://issues.apache.org/jira/secure/attachment/12553402/PIG-2937-3_nowhitespace.patch,13/Nov/12 23:18;jcoveney;PIG-2937-3_whitespace.patch;https://issues.apache.org/jira/secure/attachment/12553403/PIG-2937-3_whitespace.patch,15/Nov/12 00:30;jcoveney;PIG-2937-4_whitespace.patch;https://issues.apache.org/jira/secure/attachment/12553596/PIG-2937-4_whitespace.patch,,,,,,6.0,,,,,,,,,,,,,,,,,,,2012-10-22 21:51:56.045,,,no_permission,,,,,,,,,,,,242087,,,,Thu Nov 15 00:39:55 UTC 2012,,,,,,,0|i02im7:,12617,,,,,,,,,,"22/Oct/12 21:51;jcoveney;This is definitely important and useful.

To my eye, the way that this should work is that in any case where you don't have a schema (in this case, generated_field inside of the GENERATE) we should do our best to fill it in. In the case of a binary conditional, etc, we know the return type, so that gives us the type, and the field name (ie generated_field) would give us the name.

I think that this is not a deep change, but it is a tricky one as getting Pig to thread through Schema information like this that isn't currently threaded through can be tricky.","24/Oct/12 18:01;jcoveney;I've attached a patch that fixes this and passes test-commit.

The approach it takes is in the case of a generate where you have a field referencing a valid LogicalExpression that for some reason does not have an alias associated with it (this is not common, only really happens afaik in the context Feng posted), then the alias name becomes the alias in the Schema.","24/Oct/12 18:35;pengfeng;Thanks [~jcoveney]!

* The change in ProjectExpression.java seems to be a whitespace change only.
* The change in LogicalPlanBuilder.java seems to be a reasonable best-effort to me.

Non-comitter +1",24/Oct/12 18:50;jcoveney;I fixed a typo in ProjectExpression :),30/Oct/12 20:47;jcoveney;Bump. Would love it if someone could take a look or opine,"31/Oct/12 18:18;pengfeng;co-bump, can any committer take a look at the change? Thanks!","01/Nov/12 04:07;rohini;Jon, 
   I compiled the logical plan for the given query with your patch. The schema for the generated field still does not have a variable name associated with it. Did not dig further to see where is the problem though. 
   Also can we include a unit test? Thanks. ","07/Nov/12 00:05;jcoveney;Rohini,

Not sure what happened. I thought it worked locally, but maybe I was wrong. Either way, I uploaded a version that should work. Would love some eyes. Thanks!",11/Nov/12 05:33;rohini;Sorry about the delay Jon. The patch looks good and the logical plan schema is now correct. It would be good if we can add a testcase. Just had a recent reminder from Santhosh to do a +1 after ensuring there is a unit test. ,"13/Nov/12 23:18;jcoveney;Rohini,

Thanks for taking a look. I've attached a patch with and without whitespace changes. I added a test, and refactored TestLogicalPlanGenerator (where I put the test) a little bit. The patch without whitespace is to make reviewing easier.

Please let me know your thoughts.","14/Nov/12 18:28;rohini;Thanks Jon. +1. Patch looks good. 

Minor nitpick I have is that it would be nice to give the test a more relevant name than testAutomaticallyMadeName. Something like testRelationAliasForBinCond. But I am not going to insist. ","14/Nov/12 18:38;jcoveney;I am horrible at naming. I'll change the name to something more like that, and then commit.","14/Nov/12 20:29;pengfeng;Great, thanks guys!",15/Nov/12 00:39;jcoveney;It's in. Thanks Rohini!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tuple serialization bug,PIG-2936,12609454,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,jcoveney,jcoveney,jcoveney,27/Sep/12 21:10,22/Feb/13 04:53,14/Mar/19 03:07,28/Sep/12 00:11,0.11,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,While porting some code I messed up a switch statement that breaks Tuple serialization in trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Sep/12 21:12;jcoveney;PIG-2936-0.patch;https://issues.apache.org/jira/secure/attachment/12546907/PIG-2936-0.patch,27/Sep/12 22:38;jcoveney;PIG-2936-1.patch;https://issues.apache.org/jira/secure/attachment/12546926/PIG-2936-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-09-27 21:26:01.198,,,no_permission,,,,,,,,,,,,242085,,,,Thu Sep 27 23:28:37 UTC 2012,,,,,,,0|i02ilb:,12613,,,,,,,,,,"27/Sep/12 21:26;julienledem;I see. So a tuple of size > 35 would trigger the bug.
+1
please add a test as well.","27/Sep/12 23:28;julienledem;Thank you Jon!
good call removing the catch blocks from the test.
+1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Catch NoSuchMethodError when StoreFuncInterface's new cleanupOnSuccess method isn't implemented.,PIG-2935,12609411,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,alangates,alangates,alangates,27/Sep/12 16:50,22/Feb/13 04:53,14/Mar/19 03:07,02/Oct/12 17:06,0.11,,,,,,,0.11,,,,impl,,,0,,,,,,,,,,,,,We should catch a NoSuchMethodError here so that old implementations of StoreFuncInterface still work.  This will still be incompatible for people who re-compile their store functions.,,,,,,,,,,,,,,,,,,,PIG-1891,,,,,,,,,,,,,01/Oct/12 22:59;dvryaboy;PIG-2935.2.patch;https://issues.apache.org/jira/secure/attachment/12547302/PIG-2935.2.patch,27/Sep/12 18:53;alangates;PIG-2935.patch;https://issues.apache.org/jira/secure/attachment/12546893/PIG-2935.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-09-27 19:25:55.479,,,no_permission,,,,,,,,,,,,242080,,,,Tue Oct 02 17:06:50 UTC 2012,,,,,,,0|i02ijz:,12607,,,,,,,,,,"27/Sep/12 19:25;dvryaboy;Alan, that won't work -- the error thrown is not NoSuchMethod, but AbstractMethodError (the abstract method is declared in the new interface, so the method exists, but has no implementation). These are both children of IncompatibleClassChangeError , which would capture the case of somehow using the new PigServer but the old StoreFuncInterface, though that's probably overkill (I don't know how one would get into such a bind).","27/Sep/12 20:56;alangates;Hmm, that's interesting, because there are other places where we catch NoSuchMethodError and it seems to be the same situation.  Checkout SyncProgressNotificationAdaptor.initialPlanNotification() and ScriptState.emitInitialPlanNotification().  Are these errors or is something different happening here?  But I can change the patch to catch the right error.","27/Sep/12 20:59;dvryaboy;I would guess those are problems (one way to find out..).

I know for sure that the AME is what gets thrown in this case, cause that's what we saw when running an existing ElephantBird storefunc implementation with trunk.","01/Oct/12 22:59;dvryaboy;same, but with the right method being caught.","01/Oct/12 22:59;dvryaboy;Alan, quick sanity check?",02/Oct/12 16:41;alangates;+1.,02/Oct/12 17:06;dvryaboy;Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBaseStorage is using setScannerCaching which is deprecated,PIG-2933,12609282,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,prkommireddi,ted.m,ted.m,26/Sep/12 18:15,14/Oct/13 16:46,14/Mar/19 03:07,01/Nov/12 02:31,,,,,,,,0.12.0,,,,,,,0,hbase,,,,,,,,,,,,"HTable.setScannerCaching is deprecated use Scan.setCaching(int)

Note: I'm on vacation starting tomorrow.  If you want I can fix this next week.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Oct/12 07:32;prkommireddi;PIG-2933.patch;https://issues.apache.org/jira/secure/attachment/12549653/PIG-2933.patch,29/Oct/12 23:16;prkommireddi;PIG-2933_1.patch;https://issues.apache.org/jira/secure/attachment/12551270/PIG-2933_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-18 07:32:57.475,,,no_permission,,,,,,,,,,,,242076,,,,Thu Nov 01 02:31:33 UTC 2012,,,Patch Available,,,,0|i02iin:,12601,,,,,,,,,,18/Oct/12 07:32;prkommireddi;Smallest patch ever :),"28/Oct/12 21:05;rohini;Prashant,
   Thanks for the fix. I just have two minor comments. 

   * Can you move the scan.setCaching(caching_); next to scan.setCacheBlocks(false); in initScan()? It will help keeping all the scan settings in a single method. 
   * The patch corrects the order of imports. But Map.Entry is moved after Properties. Can you fix that?","29/Oct/12 23:13;prkommireddi;1. Makes sense
2. I think inner classes ordering on imports is different, it's usually placed below. Let me know if you think otherwise as per Apache pig guidelines. ","29/Oct/12 23:31;rohini;bq. I think inner classes ordering on imports is different, it's usually placed below. Let me know if you think otherwise as per Apache pig guidelines.
  Its just a standard ordering and should work with any IDE. Checked that doing a Ctrl+Shift+O in Eclipse puts Map.Entry immediately after Map. 

+1. Will commit this once I get access. ","29/Oct/12 23:35;prkommireddi;Sounds good, thanks.",01/Nov/12 02:31;rohini;Committed to trunk. Thanks Prashant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setting high default_parallel causes IOException in local mode,PIG-2932,12609238,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,cheolsoo,azaroth,azaroth,26/Sep/12 13:41,24/Jan/15 21:53,14/Mar/19 03:07,05/Oct/12 21:45,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"This bug has been confirmed only in local mode.

When setting a high default_parallel, Pig fails on some operations.
The following data and script reproduce the bug.

Data:
{code}
grunt> cat file.txt                                  
11	1	qwer
12	2	qwerty
13	3	ert
13	3	ertyu
14	4	zxcv
16	6	fsdfg
16	6	fdfghj
18	8	fjklopi
{code}

Script:
{code}
SET default_parallel 9
a = load 'file.txt' as (id1:int, id2:int, str:chararray);
b = group a by (id1,id2);
c = foreach b generate flatten(group), a;
d = order c by group::id1 ASC, group::id2 ASC;
dump d
{code}

Error:
{code}
2012-09-26 15:28:13,230 [Thread-32] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map - Aliases being processed per job phase (AliasName[line,offset]): M: d[12,4] C:  R: 
2012-09-26 15:28:13,232 [Thread-32] WARN  org.apache.hadoop.mapred.LocalJobRunner - job_local_0009
java.io.IOException: Illegal partition for Null: false index: 0 (12,2) (1)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1073)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:691)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map.collect(PigGenericMapReduce.java:123)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:285)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:278)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)
{code}

The script succeeds if default_parallel is set to 2.
I guess it depends on the fact that the default_parallel is higher than the number of unique keys, probably some quirk with ORDER BY.",,,,,,,,,,,,,,,,,,,PIG-4392,,,,,,,,,,,,,27/Sep/12 02:42;cheolsoo;PIG-2932.patch;https://issues.apache.org/jira/secure/attachment/12546794/PIG-2932.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-27 02:34:03.453,,,no_permission,,,,,,,,,,,,242073,,,,Sat Jan 24 21:53:27 UTC 2015,,,,,,,0|i02ihr:,12597,,,,,,,,,,"27/Sep/12 02:34;cheolsoo;In fact, the example script works fine with MR2 in local mode, so that makes me believe that this is an issue with LocalJobRunner of hadoop 0.20. Please see MAPREDUCE-1367.

The best fix is probably disabling the tests in local mode. I don't think that there is a way to disable tests only when hadoopversion == 20 && execonly == local.

Thanks!",27/Sep/12 02:42;cheolsoo;Attaching a patch that disables Rank tests in local mode.,"27/Sep/12 11:17;xalan;I think the patch can solve the problem with rank tests execution in local mode. 
But, I tried this code and failed again:

{code}
SET default_parallel 3;
a = load 'file.txt' as (id1:int, id2:int, str:chararray);
b = order id1 ASC, id2 ASC;
dump b;
{code}

I'm using Hadoop 1.0.0","27/Sep/12 18:18;cheolsoo;Hi Allan,

That's expected. Any parallelism in local mode w/o MAPREDUCE-1367 will fail. If you compare LocalJobRunner of hadoop 1.0.0 and that of hadoop 2.0.0, you will see that MAPREDUCE-1367 is included in 2.0.0 but not in 1.0.0.

Again, you code runs fine in MR2 (hadoop-2.0.0). To really ""fix"" it, we have to replace the hadoop dependency in pig.jar, but all the hadoop 0.20.x and 1.0.0 have the same problem.

In fact, I ran into the same issue at PIG-2852, and I documented it in Pig manual.

I suggest that we should change the title of the jira and disable Rank tests in local mode. Please let me know if you have a better suggestion.

Thanks!","27/Sep/12 23:56;azaroth;Cheolsoo, thanks for the explanation.
Now it is more clear.

I agree with your proposals.
Will test and commit the patch tomorrow.",28/Sep/12 08:53;xalan;Thanks for the explanation Cheolsoo. ,05/Oct/12 21:45;alangates;Patch checked in.  Thanks Cheolsoo.,"13/Nov/14 12:45;alex_davliatov;Apache Pig version 0.12.1 (r1585011) 
compiled Apr 05 2014, 01:41:34

Happened again.",13/Nov/14 21:53;daijy;What is your hadoop version?,"18/Nov/14 07:37;alex_davliatov;Sorry, didn't read the comments properly: I use hadoop 1.2.1, and fix is hadoop 2.+.","24/Jan/15 21:53;erwaman;I encountered this issue, too, in distributed mode with Pig 0.11.1 and Hadoop 2.3.0. I filed PIG-4392 for my issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
$ signs in the replacement string make parameter substitution fail,PIG-2931,12608966,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,25/Sep/12 05:40,22/Feb/13 04:54,14/Mar/19 03:07,19/Oct/12 00:06,0.10.0,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"To reproduce the issue, use the following pig script:
{code:title=test.pig}
a = load 'data';
b = filter by $FILTER;
{code}

and run the following command:
{code}
pig -x local -dryrun -f test.pig -p FILTER=""(\$0 == 'a')""
{code}

This generates the following script:
{code:title=test.pig.substituted}
a = load 'data';
b = filter by ($FILTER == 'a');
{code}

However this should be:
{code}
a = load 'data';
b = filter by ($0 == 'a');
{code}

This is because Pig calls replaceFirst() with a replacement string that include a $ sign as follows:
{code}
""$FILTER"".replaceFirst(""\\$FILTER"", ""($0 == 'a')""));
{code}

To treat $ signs as literals in the replacement string, we must escape them. Please see the [Java doc|http://docs.oracle.com/javase/6/docs/api/java/util/regex/Matcher.html#replaceFirst(java.lang.String)] for Matcher class for explanation:
{quote}
Note that backslashes (\) and dollar signs ($) in the replacement string may cause the results to be different than if it were being treated as a literal replacement string. Dollar signs may be treated as references to captured subsequences as described above, and backslashes are used to escape literal characters in the replacement string.
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Sep/12 06:09;cheolsoo;PIG-2931.patch;https://issues.apache.org/jira/secure/attachment/12546450/PIG-2931.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-01 17:17:52.443,,,no_permission,,,,,,,,,,,,242069,,,,Mon Oct 22 21:26:17 UTC 2012,,,,,,,0|i02ign:,12592,"This changes the behavior of parameter substitution so that dollar signs in the replacement string are no longer treated as references to captured subsequences, but they are now treated as a literal replacement string. Therefore, this will cause backward compatibility issues for users who rely on the previous behavior of parameter substitution.",,,,,,,,,"25/Sep/12 06:05;cheolsoo;Attaching a patch that fixes the issue by escaping $ signs in the replacement string.

I also added a test case to the TestParamSubPreproc suite and verified that it passes.","01/Oct/12 17:17;dvryaboy;The patch looks correct, but I can't help but wonder if anyone might be (ab)using the current behavior.. second opinion?","01/Oct/12 17:50;cheolsoo;Hi Dmitriy,

That's an interesting thought. But in this case, $0 and $1 will only be able to refer to parameter names. I can't think why someone would want to replace parameter names with strings that include themselves. I might not be seeing a useful case.

Thanks!","18/Oct/12 01:16;jcoveney;Dmitriy, I agree with Cheolsoo. Seems unlikely, and I'd rather fix it as it's a totally fair use of positionals. +1","18/Oct/12 01:56;dvryaboy;ok, +1. Please make a note about the highly unlikely backwards incompatibility.",19/Oct/12 00:06;jcoveney;Thanks Cheolsoo! It's in,"22/Oct/12 20:58;julienledem;sorry to reply late on this. This would break users that are already double escaping $ to insert values containing $
We should escape $ only if it is not escaped yet.","22/Oct/12 21:26;cheolsoo;Hi Julien,

Thank you very much for pointing that out. In fact, I've just realized that I broke TestScriptLanguange for the reason that you're describing. Pig#bind() escapes '$', so my change makes bindLocalVariableTest2() fail.

Please let me open a jira under PIG-2972 and post a patch.

Thanks!
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ant test doesn't compile in trunk,PIG-2930,12608951,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,cheolsoo,cheolsoo,cheolsoo,25/Sep/12 00:42,22/Feb/13 04:53,14/Mar/19 03:07,25/Sep/12 00:58,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"To reproduce the compile error, please do:
{code}
ant test
{code}

Itfails with the following error:

{code}
    [javac] /home/cheolsoo/workspace/pig-svn/test/org/apache/pig/test/TestBuiltin.java:195: cannot find symbol
    [javac] symbol  : class Properties
    [javac] location: class org.apache.pig.test.TestBuiltin
    [javac]         pigServer = new PigServer(ExecType.LOCAL, new Properties());
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Sep/12 00:43;cheolsoo;PIG-2930.patch;https://issues.apache.org/jira/secure/attachment/12546418/PIG-2930.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-25 00:58:19.237,,,no_permission,,,,,,,,,,,,256494,Reviewed,,,Tue Sep 25 00:58:19 UTC 2012,,,,,,,0|i0h6nr:,98341,,,,,,,,,,"25/Sep/12 00:43;cheolsoo;Trivial fix that adds the missing import statement.

Thanks!",25/Sep/12 00:58;daijy;Patch committed. Thanks Cheolsoo!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Improve documentation around AVG, CONCAT, MIN, MAX",PIG-2929,12608880,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,nerdynick,nerdynick,24/Sep/12 14:38,22/Feb/13 04:53,14/Mar/19 03:07,27/Sep/12 18:10,0.9.0,0.9.1,0.9.2,,,,,0.11,,,,documentation,internal-udfs,,0,,,,,,,,,,,,,Just stumbled upon this issue over the weekend. It appears the built-in CONCAT function will return null in the advent of 1 item in the Tuple being null. If this is as intended it would be nice if the docs made note of this. Otherwise it would be nice if nulls where just skipped and a proper string of what was left was returned. If all items are null then just return null in that case.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Sep/12 17:53;cheolsoo;PIG-2929-2.patch;https://issues.apache.org/jira/secure/attachment/12546886/PIG-2929-2.patch,24/Sep/12 18:02;cheolsoo;PIG-2929.patch;https://issues.apache.org/jira/secure/attachment/12546349/PIG-2929.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-09-24 18:02:35.15,,,no_permission,,,,,,,,,,,,242093,,,,Thu Sep 27 18:10:07 UTC 2012,,,,,,,0|i02inj:,12623,,,,,,,,,,"24/Sep/12 18:02;cheolsoo;Thank you very much for reporting an issue Nicholas!

In fact, this is documented [here|http://pig.apache.org/docs/r0.10.0/basic.html#nulls] although it is not very visible.

I am attaching a patch that updats the built-in function doc (including CONCAT, AVG, MIN, MAX, and SIZE) regarding nulls.

Regarding your suggestion of changing the behavior of CONCAT, even though it is total valid, I'm afraid that that could introduce backward incompatibilities. That is, some people may rely on that CONCAT returns nulls in their scripts. Given that this can be easily achieved by writing your own UDF, I prefer to keep the current behavior.

Please let me know if anyone thinks otherwise. Thanks! ","27/Sep/12 16:03;billgraham;+1 to Cheolsoo's comments. A null value passed to a concat could be seen as an invalid record where a null result it warranted. Plus there's the backward compatibility issue.

If you want a concat with the semantics you describe, I think a custom UDF is the way go.

@Chealsoo, thanks for the documentation patch. Would you please make one minor edit, which is to remove the *now* from the text. Comments should be non-temporal.

{noformat}
The MIN function now ignores NULL values.
{noformat}","27/Sep/12 17:19;nerdynick;This all makes since. The addition of comments was more the main goal I was looking for. Just to prevent anyone else from having the assumptions I had.

However I did managed to solve the issue without a UDF by leveraging the ternary operations. ie. CONCAT(domain is null?"""":domain, path is null?"""":path). The case I had was concating parts of a URL back together. Where the Query String might be null if there wasn't one.","27/Sep/12 17:53;cheolsoo;Thank you for reviewing my patch Bill!

I removed ""now"". Please let me know if you want to edit anything more.","27/Sep/12 18:10;billgraham;Looks good, thanks Cheolsoo! Renaming JIRA and committing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigStats should not be assuming all Storage classes to be file-based storage,PIG-2924,12608249,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,qwertymaniac,qwertymaniac,19/Sep/12 14:37,23/Apr/14 01:10,14/Mar/19 03:07,19/Nov/12 19:54,0.10.0,0.9.2,,,,,,0.12.0,,,,tools,,,1,,,,,,,,,,,,,"Using PigStatsUtil (like Oozie does) to collect JobStats for jobs that use a HBaseStorage blows up when the stats are asked to be accumulated.

This is because JobStats (which adds stuff up) is assuming all storages are file based and that it can do listStatus/etc. operations on their filespec-provided filename. For HBaseStorage, this is set to the tablename and there's no such file, leading to an exception (FileNotFound or Invalid URI - depending on using 'tablename' or 'hbase://tablename').",,,,,,,,,,,,,,,,,,,PIG-3891,,,,,,PIG-3066,,,,,,,29/Oct/12 03:47;cheolsoo;PIG-2924-2.patch;https://issues.apache.org/jira/secure/attachment/12551152/PIG-2924-2.patch,30/Oct/12 05:19;cheolsoo;PIG-2924-3.patch;https://issues.apache.org/jira/secure/attachment/12551305/PIG-2924-3.patch,31/Oct/12 01:48;cheolsoo;PIG-2924-4.patch;https://issues.apache.org/jira/secure/attachment/12551472/PIG-2924-4.patch,08/Nov/12 22:24;cheolsoo;PIG-2924-5.patch;https://issues.apache.org/jira/secure/attachment/12552726/PIG-2924-5.patch,27/Oct/12 01:19;cheolsoo;PIG-2924.patch;https://issues.apache.org/jira/secure/attachment/12551048/PIG-2924.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2012-09-19 22:25:41.334,,,no_permission,,,,,,,,,,,,242356,,,,Mon Nov 19 19:54:52 UTC 2012,,,,,,,0|i02u27:,14471,,,,,,,,,,19/Sep/12 14:42;qwertymaniac;A straight forward fix may be to intercept the FileNotFound issues. A proper fix may be to have storages report if they are file-based or otherwise?,"19/Sep/12 22:25;billgraham;We ran into similar issues with HCatalog and reducer estimation (PIG-2573, PIG-2574), since an HDFS path was assumed.

For this issue we could register different classes that know how to look up (or not support) stats based on the URI prefix of the data location (hdfs, hbase, s3, etc).","27/Oct/12 01:19;cheolsoo;I am attaching a patch that implements Bill's suggestion. To make the computation of the output size plugable, I did the following:
- Added a new interface called PigStatsOutputSizeComputer.
- Added a default implementation of this interface called FileBasedOutputSizeComputer.
- Added a new flag via which a custom output size computer can be registered.
- Added unit test cases for both file-based and non-file-based systems.

Basically, I followed the pattern that Bill introduced for the reducer estimator in PIG-2574. Any comments would be appreciated.

Thanks!",29/Oct/12 03:47;cheolsoo;Adding the new property (pig.stats.output.size.computer) to the default pig.properties file with comments.,"29/Oct/12 18:15;billgraham;This looks great, thanks for taking this one. I think we need to make a few changes to the pattern used PIG-2574 though, because we could have a case where we have multiple store funcs that each write to a different data source.

* Instead of registering a single new computer it would be ideal if we could register a list of computers.
* Each computer could have a {{boolean supports(POStore poStore)}} method that returns whether this class supports a given POStore. This can often be done by inspecting the output path. A default URI-based abstract class could help with that part.
* The computers would then be consulted in order, where the first to support the POStore wins.
* If a computer can't determine a size for some reason (i.e., it doesn't support it or an exception occurred), it shouldn't return 0. Instead maybe we reserve -1 for this case and document it as such. 
* Having the word Computer in the interface name and configs could cause confusion, due to how it's an overloaded term. I don't have any great suggestions though. {{PigStatsOutputSizeReader}}?

Thoughts? 
 
","29/Oct/12 22:55;cheolsoo;Hi Bill, thank you very much for reviewing my patch!

I totally agree with most of your comments. In particular, adding a supports() method seems like an elegant way to support multiple computers. I will make that change in a new patch.

But I am wondering if you would agree to remove POStore from the interface. The reason why I want to remove it is because I don't think that POStore is needed to implement supports() and getOutputSize() for any kinds of computers. All we need is probably the uri string, so it seems to make sense to pass the uri string (or a URI object) instead of the whole POStore. Please let me know if you think otherwise. 

Regarding the name of the interface, I couldn't come up with a better name. Reader sounds good to me. Maybe reporter or calculator?

Thanks!","30/Oct/12 05:19;cheolsoo;I updated the patch as follows:
{quote}
Having the word Computer in the interface name and configs could cause confusion, due to how it's an overloaded term. I don't have any great suggestions though. PigStatsOutputSizeReader?
{quote}
Changed to {{PigStatsOutputSizeReader}}.
{quote}
Instead of registering a single new computer it would be ideal if we could register a list of computers.
{quote}
Fixed.
{quote}
Each computer could have a boolean supports(POStore poStore) method that returns whether this class supports a given POStore. This can often be done by inspecting the output path. A default URI-based abstract class could help with that part.
{quote}
Each reader implements {{boolean supports(String uri)}} method. For {{FileBasedOutputSizeReader}}, the output of {{UriUtil.isHDFSFileOrLocalOrS3N()}} is returned.
{quote}
The computers would then be consulted in order, where the first to support the POStore wins.
{quote}
Fixed.
{quote}
If a computer can't determine a size for some reason (i.e., it doesn't support it or an exception occurred), it shouldn't return 0. Instead maybe we reserve -1 for this case and document it as such.
{quote}
Fixed.

In addition, I replaced {{POStore}} with {{String}}. Please let me know what you think.

Thanks!","31/Oct/12 01:48;cheolsoo;Updating the comments in pig.properties to make it clear that the user can register multiple readers, and the 1st reader that supports the given uri will be used.","08/Nov/12 07:24;billgraham;Sorry for the delay on the review Chelsoo. Looking good. A few more comments. Let me know what you think.

- I think we need to pass the {{POStore}} instead of the location. PigStorage impls provided by random parties might not all abide by a unique namespacing convention in their location syntax. For example, {{VerticaStorer}} uses a syntax like ""{[db_schema].[table_name]}"" (curly brackets included). Another implementor could use the same syntax.  
- JobStats.getOuputSize could be simplified by doing this, which is more commonly done:
{noformat}
String reporterNames = conf.get(
   PigStatsOutputSizeReader.OUTPUT_SIZE_READER_KEY,
   FileBasedOutputSizeReader.class.getCanonicalName());
{noformat}
- Does {{PigContext.instantiateFuncFromSpec(className)}} (without appending ""()"") not work?
- It seems like it would be reasonable for {{PigStatsOutputSizeReader.getOutputSize}} to throw IOException all the way up to {{JobStats}}.
- Let's make {{DummyOutputSizeReader}} an inner class of {{TestJobStats}} since that package is already totally bloated.
- In {{pig.properties}} reducers' should not have an apostrophe (no possessive for inanimate objects).","08/Nov/12 22:24;cheolsoo;Thank you very much for reviewing my patch, Bill!

I agree that passing POStore is better. I also agree with the other points and incorporated them in this new patch. Please let me know what you think.","19/Nov/12 19:54;cheolsoo;Bill gave +1 in the RB:
https://reviews.apache.org/r/8122/

Committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e2e tests override PERL5LIB environment variable,PIG-2920,12607469,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,azaroth,azaroth,azaroth,13/Sep/12 14:14,22/Feb/13 04:53,14/Mar/19 03:07,02/Oct/12 07:18,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"I am not sure why but e2e tests set PERL5LIB like this:
{code}
<env key=""PERL5LIB"" value=""./libexec""/>
{code}

This overrides any env variable, so there is no way to use custom Perl installations.

This patch just removes the line, thus we will rely on the user to configure PERL5LIB appropriately.
With this modification I am able to use my custom Perl installation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Sep/12 15:18;azaroth;PIG-2920.2.patch;https://issues.apache.org/jira/secure/attachment/12546998/PIG-2920.2.patch,13/Sep/12 14:15;azaroth;PIG-2920.patch;https://issues.apache.org/jira/secure/attachment/12544986/PIG-2920.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-09-25 05:56:14.096,,,no_permission,,,,,,,,,,,,242068,,,,Mon Oct 01 21:34:31 UTC 2012,,,,,,,0|i02igf:,12591,,,,,,,,,,"25/Sep/12 05:56;rohini;Removing it altogether might cause tests to fail as test/e2e/pig/testdist/libexec has some perl modules.

Can you instead set a property with default value as ./libexec and use the property name in the env element. It would be similar to other overridable properties with default value like harness.PH_LOCAL.","28/Sep/12 15:18;azaroth;Addressed the comments by Rohini.

Now the user can set the property harness.PERL5LIB to control the PERL5LIB environment variable in the tests.",28/Sep/12 17:42;rohini;+1 non-binding.,"28/Sep/12 18:49;jcoveney;+1. Good call, Rohini","01/Oct/12 21:34;azaroth;Committed to trunk.
Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid Spillable bag overhead where possible,PIG-2918,12607402,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,dvryaboy,dvryaboy,13/Sep/12 05:39,22/Feb/13 04:53,14/Mar/19 03:07,15/Sep/12 02:45,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"We use BagFactory.newDefaultBag() liberally, and pay a price -- each such bag registers with the spillable memory manager, and if we allocate a lot of tiny bags, we wind up paying for maintaining and cleaning up the internal linked list of weak references. 
In many cases, we know a-priori that the bags are smal, and should probably be creating non-spillable bags for those cases.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Sep/12 05:42;dvryaboy;PIG-2918.patch;https://issues.apache.org/jira/secure/attachment/12544943/PIG-2918.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-14 20:47:59.856,,,no_permission,,,,,,,,,,,,248586,,,,Sat Sep 15 02:45:24 UTC 2012,,,,,,,0|i09w7z:,55661,,,,,,,,,,13/Sep/12 05:42;dvryaboy;Attaching a quick pass over builtin functions.,14/Sep/12 20:47;alangates;Patch looks good.  I'm running it through some of the e2e tests.,"15/Sep/12 01:01;alangates;+1, tests pass.","15/Sep/12 02:45;dvryaboy;Committed to 0.11 (trunk). Thanks for the review Alan.

Btw, I profiled this change -- prior to this patch, calling TOBAG() on about 60,000 tuples resulted in 530Kb worth of DefaultDataBags, and 2Mb of WeakReferences (!). After the patch, practically no WeakReferences, and only 175K NonSpillableDataBags. Turns out this sort of thing adds up fast.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SpillableMemoryManager memory leak for WeakReference,PIG-2917,12607393,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,haitao.yao,haitao.yao,haitao.yao,13/Sep/12 03:25,22/Feb/13 04:53,14/Mar/19 03:07,01/Oct/12 19:19,0.11,,,,,,,0.11,,,,impl,,,0,,,,,,,,,,,,,"Sine the SpillableMemoryManager subscribed the memory notifications, and hold a weak reference of all the spillable, if there's too many weak reference and the memory exceeds the threshold, the current code will not clear the spillable weak references.

the details is illustrated in the attached screenshot of a MemoryAnalyzer.
So we need to clear the spillables when the memory notifications is received.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Sep/12 03:27;haitao.yao;SpillableMemoryManager.patch;https://issues.apache.org/jira/secure/attachment/12544933/SpillableMemoryManager.patch,13/Sep/12 03:31;haitao.yao;aa.jpg;https://issues.apache.org/jira/secure/attachment/12544934/aa.jpg,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-09-13 05:54:56.546,,,no_permission,,,,,,,,,,,,242090,,,,Mon Oct 01 19:19:08 UTC 2012,,,Patch Available,,,,0|i02imv:,12620,,,,,,,,,,"13/Sep/12 03:27;haitao.yao;here's the patch
","13/Sep/12 03:31;haitao.yao;here's the screenshots of MemoryAnalyzer.
","13/Sep/12 05:54;dvryaboy;Haitao, I am curious what your script is doing that causes it to allocate that many bags?
I just created a ticket ripping out some unnecessary usage of Spillable objects; suspect your usage is pointing us at another place where we don't really need to be dealing with SMM in the first place.

Patch seems reasonable. Stylistically, I prefer the code below for iteration of Iterables:
{code}
for (WeakReference<Spillable> ref : spillables) {
  ...
}
{code}","01/Oct/12 19:19;dvryaboy;patch committed.
thanks Haitao.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig should clone JobConf while creating JobContextImpl and TaskAttemptContextImpl in Hadoop23,PIG-2912,12606686,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,07/Sep/12 21:37,06/Jan/13 23:57,14/Mar/19 03:07,10/Sep/12 22:20,0.10.1,0.9.3,,,,,,0.10.1,0.11,0.9.3,,,,,0,,,,,,,,,,,,,"There is change in the semantics of
JobContext::JobContext(Configuration, JobID). While in .20, the Config was
cloned, in .23 the Config is adopted (if it's a JobConf). This causes the same
Configuration instance to be written-to for different tables in the same job.

It would affect multi store commands in pig on Hadoop 23/2.0. The
cloning in HadoopShims was part of PIG-2578 but was reverted to other issues.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Sep/12 15:28;rohini;PIG-2912-branch09.patch;https://issues.apache.org/jira/secure/attachment/12544478/PIG-2912-branch09.patch,10/Sep/12 15:28;rohini;PIG-2912-branch10.patch;https://issues.apache.org/jira/secure/attachment/12544479/PIG-2912-branch10.patch,10/Sep/12 15:28;rohini;PIG-2912-trunk.patch;https://issues.apache.org/jira/secure/attachment/12544480/PIG-2912-trunk.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-09-10 16:48:50.004,,,no_permission,,,,,,,,,,,,256490,,,,Mon Sep 10 22:20:11 UTC 2012,,,,,,,0|i0h6iv:,98319,,,,,,,,,,"10/Sep/12 15:28;rohini;The patch creates a clone if JobConf is passed. 

Testcase added in TestMultiQueryLocal ensures that if new settings are added in the backend by a store, they are not passed to other stores in a multi store script.",10/Sep/12 16:48;dvryaboy;Could you create a test in which the storage or loader function uses UDFContext to ship around information? I don't think the current test probes the issue that caused us to roll back PIG-2578,"10/Sep/12 18:15;rohini;Dmitriy,
   This patch is mainly to address a behaviour change in hadoop between 20 and 23 in the way instantiation of new JobContext is done and to deal with it in HadoopShims so that JobContext objects in the backend do not get overwritten in case of multiple stores. PIG-2578 main problem was with JobControlCompiler and it changed frontend behaviour. I will create a separate jira to add test case for PIG-2578. 

Hadoop 20:
{code}
JobContext(JobConf conf, org.apache.hadoop.mapreduce.JobID jobId, 
             Progressable progress) {
    super(conf, jobId); //Gets cloned
    this.job = conf;
    this.progress = progress;
  }
{code}

Hadoop 23:
{code}
 public JobContextImpl(Configuration conf, JobID jobId) {
    if (conf instanceof JobConf) {
      this.conf = (JobConf)conf; //Gets assigned
    } else {
      this.conf = new JobConf(conf);
    }
{code}",10/Sep/12 21:14;daijy;+1. We shall keep the same behavior between 20 and 23. Put the abstraction in shims is the right approach. Will commit soon.,10/Sep/12 22:20;daijy;Patch committed to 0.9/0.10/trunk. Thanks Rohini!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig does not fail anymore if two macros are declared with the same name,PIG-2896,12605414,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,julienledem,julienledem,julienledem,29/Aug/12 19:04,22/Feb/13 04:53,14/Mar/19 03:07,30/Aug/12 23:04,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PIG-2279,,,,,,,,,,,,,29/Aug/12 19:50;julienledem;PIG-2896.patch;https://issues.apache.org/jira/secure/attachment/12542971/PIG-2896.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-30 05:15:06.201,,,no_permission,,,,,,,,,,,,256482,,,,Thu Aug 30 22:51:23 UTC 2012,,,Patch Available,,,,0|i0h6en:,98300,,,,,,,,,,29/Aug/12 19:48;julienledem;org.apache.pig.test.TestMacroExpansion.duplicationTest() is failing,29/Aug/12 19:50;julienledem;PIG-2896.patch restores the old behavior,30/Aug/12 05:15;dvryaboy;How do we reconcile this and PIG-2279 ?,30/Aug/12 18:56;julienledem;PIG-2279 is still open and the source of the problem still has to be found.,"30/Aug/12 20:32;julienledem;* TestPigServerWithMacros (containing tests added in PIG-2850) still passes
* TestMacroExpansion (broken by PIG-2850) now passes",30/Aug/12 22:51;dvryaboy;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revert  PIG-2578,PIG-2890,12604709,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,dvryaboy,dvryaboy,24/Aug/12 03:37,06/Jan/13 23:57,14/Mar/19 03:07,24/Aug/12 03:38,,,,,,,,0.10.1,0.11,,,,,,0,,,,,,,,,,,,,PIG-2870 and PIG-2872 contain the discussion on why that patch needs to be reverted. ,,,,,,,,,,,,,,,,,,,PIG-2872,PIG-2870,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-08-24 17:07:09.99,,,no_permission,,,,,,,,,,,,248528,,,,Sat Aug 25 22:02:43 UTC 2012,,,,,,,0|i09vrz:,55589,,,,,,,,,,24/Aug/12 03:37;dvryaboy;Reverting PIG-2578 given Daniel Dai's +1 in that ticket.,"24/Aug/12 17:07;rohini;Dmitriy, 
  Can we have it reverted in 0.10 too? Thanks.",25/Aug/12 22:02;dvryaboy;committed to 0.10 branch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Macro cannot handle negative number,PIG-2887,12604522,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,knoguchi,knoguchi,knoguchi,22/Aug/12 21:52,22/Feb/13 04:53,14/Mar/19 03:07,13/Sep/12 11:23,,,,,,,,0.11,,,,parser,,,0,,,,,,,,,,,,,"Whenever the macro includes some negative number, parser bails out with ""/AliasMasker.g:
node from line __:__ mismatched tree node: 1 expecting <DOWN>""

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Aug/12 22:11;knoguchi;pig-2887-v1.patch;https://issues.apache.org/jira/secure/attachment/12542041/pig-2887-v1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-13 04:51:32.382,,,no_permission,,,,,,,,,,,,256477,,,,Thu Sep 13 11:23:56 UTC 2012,,,,,,,0|i0h6bj:,98286,,,,,,,,,,"22/Aug/12 21:55;knoguchi;Sample input and error message
{noformat}
% cat sample.pig
define moreThanMinus1(myinput) RETURNS myoutput {
        $myoutput = filter $myinput by points > -1;
}

A = LOAD 'tmp/input.txt' as (user:chararray,points:int); 
B = moreThanMinus1(A);
DUMP B;

% java -cp org.apache.pig.Main -x local -dryrun sample.pig 
...
2012-08-22 17:53:51,650 [main] ERROR org.apache.pig.Main - ERROR 1200: <file sample.pig.substituted, line 6> Failed to mask macro 'moreThanMinus1'. Reason: /Users/knoguchi/git/pig/src/org/apache/pig/parser/AliasMasker.g: node from line 2:27 mismatched tree node: 1 expecting <DOWN>
macro content: 
        B = filter A by points > -1;
...
% 
{noformat}","22/Aug/12 22:11;knoguchi;Updated AliasMasker.g based on QueryParser.g and also added a testcase.

(I just started learning pig but do we need to have such similar grammer files?)

{noformat}
% grep -A1 ""THIS FILE IS BASED ON QueryParser.g""  src/org/apache/pig/parser/*.g
src/org/apache/pig/parser/AliasMasker.g: * NOTE: THIS FILE IS BASED ON QueryParser.g, SO IF YOU CHANGE THAT FILE, YOU WILL 
src/org/apache/pig/parser/AliasMasker.g- *       PROBABLY NEED TO MAKE CORRESPONDING CHANGES TO THIS FILE AS WELL.
--
src/org/apache/pig/parser/AstPrinter.g: * NOTE: THIS FILE IS BASED ON QueryParser.g, SO IF YOU CHANGE THAT FILE, YOU WILL 
src/org/apache/pig/parser/AstPrinter.g- *       PROBABLY NEED TO MAKE CORRESPONDING CHANGES TO THIS FILE AS WELL.
--
src/org/apache/pig/parser/AstValidator.g: * NOTE: THIS FILE IS BASED ON QueryParser.g, SO IF YOU CHANGE THAT FILE, YOU WILL 
src/org/apache/pig/parser/AstValidator.g- *       PROBABLY NEED TO MAKE CORRESPONDING CHANGES TO THIS FILE AS WELL.
--
src/org/apache/pig/parser/LogicalPlanGenerator.g: * NOTE: THIS FILE IS BASED ON QueryParser.g, SO IF YOU CHANGE THAT FILE, YOU WILL 
src/org/apache/pig/parser/LogicalPlanGenerator.g- *       PROBABLY NEED TO MAKE CORRESPONDING CHANGES TO THIS FILE AS WELL.
% 
{noformat} ",13/Sep/12 04:51;alangates;Code looks good.  I'll run the relevant tests and check in if all is good.,13/Sep/12 11:23;alangates;Patch checked in.  Thanks Koji.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Scan TimeRange to HBaseStorage ,PIG-2886,12604311,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,malaskat,ted.m,ted.m,22/Aug/12 00:39,26/Feb/13 14:51,14/Mar/19 03:07,02/Sep/12 22:01,0.9.0,0.9.1,0.9.2,,,,,0.11,,,,,,,0,newbie,,,,,,,,,,,,I have a client that wants to use pig.  They are using MR now.  They can't use PIG right now because they only want to fetch the last day's worth of data in HBase.  A filter with time range would require reading all the HStore files.  If we hold major compaction until after the fetch and use Scan Time Range we only need to read very little in compression. ,,,,,,,,,,,,,,,PIG-1832,,,,PIG-2114,,,,,,,,,,,,,22/Aug/12 01:30;ted.m;PIG-2886-0.patch;https://issues.apache.org/jira/secure/attachment/12541850/PIG-2886-0.patch,23/Aug/12 02:08;ted.m;PIG-2886-1.patch;https://issues.apache.org/jira/secure/attachment/12542073/PIG-2886-1.patch,01/Sep/12 13:22;ted.m;PIG-2886-2.patch;https://issues.apache.org/jira/secure/attachment/12543423/PIG-2886-2.patch,02/Sep/12 00:46;ted.m;PIG-2886-3.patch;https://issues.apache.org/jira/secure/attachment/12543457/PIG-2886-3.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-08-22 00:53:09.939,,,no_permission,,,,,,,,,,,,256476,,,,Tue Feb 26 14:51:45 UTC 2013,,,,,,,0|i0h6b3:,98284,"Added the ability to set HBase Scan's maxTimestamp, minTimestamp and timestamp in HBaseStorage.  I also added unit tests.",,,,,,HBaseStorage timeRange timeStamp,,,22/Aug/12 00:45;ted.m;Submitted fix with pull request from tmalaska/pig github.  Let me know what I need to get this into PIG.  My client would like to use HBaseStorage from PIG instead of a fixed version I gave them in a jar.,"22/Aug/12 00:53;cheolsoo;Hi Ted, I think that you need to post your patch to this jira and wait for a committer to review/commit it. Please refer to:

https://cwiki.apache.org/confluence/display/PIG/HowToContribute","22/Aug/12 01:00;ted.m;OK, Sounds good. I can't do it tonight.  I'll read the directions and do it tomorrow.",22/Aug/12 01:30;ted.m;Adds timeRange to HBaseStorage,"22/Aug/12 04:57;billgraham;Thanks for the patch Ted! The code looks good, just a few nits about style mainly.

- That patch has a bunch of diff info about your git internal files, so it doesn't apply.
- Standard indents in Pig are 4 spaces (no tabs).
- Use a single space after brackets and between close parens and open brackets in your else/if statements.
- else should be on one line, i.e., } else {
- A pair of empty newlines were added after the {{ignoreWhitespace_}} block, which should be removed.
- Typos: TimeRagne and ""Timestamp most be""

Also would you please add a unit test to TestHBaseStorage.
 

",23/Aug/12 02:08;ted.m;Making progress.  Need to check some more things before I'm totally done.,"23/Aug/12 13:08;ted.m;Question.  

I added the test cases and ran the following command and I noticed the TestHBaseStore doesn't run.

ant -Djavac.args=""-Xlint -Xmaxwarns 1000"" clean jar test-commit

I'm thinking that because TestHBaseStorage doesn't extend TestCase, also no other classes call TestHBaseStorage.

So the question is: Is there a design reason why TestHBaseStorage is not running when running unit test?  Is it ok if I make TestHBaseStorage run during unit tests?
","23/Aug/12 15:11;billgraham;Only a subset of tests run during test-commit. test will run all of them (and take a while).  Also annotations are used to indicate that that class contains tests.

You can do this to test just one test:

{noformat}
ant clean test -Dtestcase=TestHBaseStorage
{noformat}
","28/Aug/12 21:30;ted.m;Thanks Bill,

I tried running TestHBaseStorage and it freezes on SetUp.  

>public void setUp() throws Exception {
>        // This is needed by Pig
>    	
>        cluster = MiniCluster.buildCluster();
>        conf = cluster.getConfiguration();
>
>        util = new HBaseTestingUtility(conf);
>        util.startMiniZKCluster();
>        util.startMiniHBaseCluster(1, 1);
>    }

Just wondering if you know what I'm missing to make this work.  Hopefully I will get time in the next couple of days to research this.","28/Aug/12 23:08;cheolsoo;Hi Ted,

Regarding TestHBaseStorage, does it hang in hadoop 20 or 23? I assume that you're not setting ""-Dhadoopversion"" so using hadoop 20 by default. In hadoop 20, TestHBaseStorage passes for me with your patch. I.e. ""ant clean test -Dtestcase=TestHBaseStorage -Dhadoopversion=20"" passes.
{code}
[junit] Running org.apache.pig.test.TestHBaseStorage
[junit] Tests run: 23, Failures: 0, Errors: 0, Time elapsed: 131.728 sec
{code}
If it doesn't pass for you, it should be some environment issue. (e.g. did you set umask 0022?)

However, it does time out in hadoop 23, and I believe that it's expected since hbase jar from the maven repository is not binary compatible with hadoop 23. I.e. ""ant clean test -Dtestcase=TestHBaseStorage -Dhadoopversion=23"" fails with time out error, and the following error can be found in the test log (build/test/logs/TEST-org.apache.pig.test.TestHBaseStorage.txt):
{code}
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hdfs.protocol.FSConstants$SafeModeAction
    at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
    ... 7 more
{code}

I ran into the same issue while bumping hbase to 0.94, but it seem applied to 0.90 (current version in trunk) as well. Please see HBASE-5680 for more details.

Please anyone corrects me if I am wrong about TestHBaseStorage in hadoop 23.

Thanks!","28/Aug/12 23:16;ted.m;Great thanks.  Got it.  

I was first doing in on my local (no Hadoop) and it would freezy.  Then I tried it on CDH4 and it didn't work either.  I will try it on CDH3 tonight.

By the way do you see anything else in the code I should add or clean up.

I should have time to work on it tonight.

Ted Malaska  ","30/Aug/12 05:26;dvryaboy;Hi Ted,
Great to see clouderians contributing to Pig again! :)

Couple of notes:

minTimeRange, maxTimeRange -- maybe better names would be minTimestamp and maxTimestamp ?
That's the signature for HBase's scanTimeRange.

Also, please fix up documentation -- minTimestamp in scan.setTimeRange is *inclusive* (so, not strictly greater then). maxTimestamp is, indeed, exclusive -- the range is [min, max)

space between } and ""else"" around maxTimeRange  handling.

HBase scan also provides setTimestamp(). Might as well throw that in?

Does your client care about # of returned versions? That's a much tricker change.. 


",31/Aug/12 17:45;ted.m;Thx will work on it now.,01/Sep/12 13:22;ted.m;Cleaned things up.  Change maxTimeRange and minTimeRange to maxTimestamp and minTimestamp.  Plus I added a timestamp option.  Along with unit tests.,"01/Sep/12 13:28;ted.m;Hey Dmitriy,

I made the changes you requested and added in the setTimestamp() option.  

I would love to do the # of versions change, but can I do that in another jira issue so I can have this one closed. :)

Thanks",01/Sep/12 23:32;ted.m;Found two type-os.  I will have the fix and the maxVersion functionality soon,01/Sep/12 23:32;ted.m;Making more changes,02/Sep/12 00:33;dvryaboy;Let's keep the issue of multiple versions separate -- it's not entirely clear how those should be returned (a bag?),02/Sep/12 00:35;ted.m;OK sounds good.  I will just update the type-o then,"02/Sep/12 00:35;dvryaboy;Ted, let me know if you want me to review, k? Wasn't clear to me from the last message if you are in a 'done' state or if you are just posting intermediate work right now.",02/Sep/12 00:37;ted.m;Give me a couple minutes I need to check in a new comment and help message,"02/Sep/12 00:40;dvryaboy;Ok I'll take a look tomorrow. Going rogue for a bit, disconnecting :).",02/Sep/12 00:46;ted.m;Fixed type-os,"02/Sep/12 22:00;dvryaboy;Urk.. git patch. You need to generate it with 'git diff --no-prefix' otherwise we can't apply it. I mean, we can, and I did, but for next time, --no-prefix makes life easier :).","02/Sep/12 22:01;dvryaboy;Applied to trunk.

As there any need to apply this to 0.10 branch?

Not sure we'll release a 0.10.1 branch at this point..",02/Sep/12 22:43;ted.m;Wow thx.  Sorry about the --no-prefix I will make sure to do that in the future.,26/Feb/13 14:51;zeph;seems to duplicate (and fix) PIG-1832,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestJobSumission and TestHBaseStorage don't work with HBase 0.94 and ZK 3.4.3,PIG-2885,12604301,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,cheolsoo,cheolsoo,cheolsoo,21/Aug/12 23:39,14/Oct/13 16:46,14/Mar/19 03:07,24/Oct/12 03:29,,,,,,,,0.12.0,,,,,,,0,hbase,,,,,,,,,,,,"I ran into two unit test failures (TestJobSubmission and TestHBaseStorage) by bumping the version of HBase and ZK to 0.94 and 3.4.3 respectively in hadoop 1.0.3. I am opening a jira to capture what I found for future reference.

- Two dependency libraries of HBase 0.94 are missing in ivy.xml - high-scale-lib and protobuf-java.
- The HTable constructor in HBase 0.94 changed:
{code}
-        HTable table = new HTable(TESTTABLE_2);
+        HTable table = new HTable(conf, TESTTABLE_2);
{code}
- The default client port of MiniZooKeeperCluster in HBase 0.94 is no longer 21818. Since it is chosen randomly at runtime, it has to be set in PigContext.
{code}
@@ -541,7 +543,7 @@ public class TestJobSubmission {
         // use the estimation
         Configuration conf = cluster.getConfiguration();
         HBaseTestingUtility util = new HBaseTestingUtility(conf);
-        util.startMiniZKCluster();
+        int clientPort = util.startMiniZKCluster().getClientPort();
         util.startMiniHBaseCluster(1, 1); 
     
         String query = ""a = load '/passwd';"" + 
@@ -553,6 +555,7 @@ public class TestJobSubmission {
     
         pc.getConf().setProperty(""pig.exec.reducers.bytes.per.reducer"", ""100"");
         pc.getConf().setProperty(""pig.exec.reducers.max"", ""10"");
+        pc.getConf().setProperty(HConstants.ZOOKEEPER_CLIENT_PORT, Integer.toString(clientPort));
         ConfigurationValidator.validatePigProperties(pc.getProperties());
         conf = ConfigurationUtil.toConfiguration(pc.getProperties());
         JobControlCompiler jcc = new JobControlCompiler(pc, conf);
{code}

With the attached patch, both tests pass with hadoop 1.0.3. Please note that TestHBaseStorage fails in hadoop 0.23, and I haven't investigated that.","Hadoop 1.0.3, CentOS 6.3 64 bit",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PIG-2977,20/Oct/12 01:00;cheolsoo;PIG-2885-2.patch;https://issues.apache.org/jira/secure/attachment/12550116/PIG-2885-2.patch,20/Oct/12 19:49;cheolsoo;PIG-2885-3.patch;https://issues.apache.org/jira/secure/attachment/12550151/PIG-2885-3.patch,23/Oct/12 16:52;cheolsoo;PIG-2885-4.patch;https://issues.apache.org/jira/secure/attachment/12550495/PIG-2885-4.patch,21/Aug/12 23:40;cheolsoo;PIG-2885.patch;https://issues.apache.org/jira/secure/attachment/12541835/PIG-2885.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-08-25 21:13:30.184,,,no_permission,,,,,,,,,,,,242067,Reviewed,,,Wed Oct 24 03:26:32 UTC 2012,,,,,,,0|i02ifj:,12587,"This makes hbase.jar and zookeeper.jar no longer be embedded in pig.jar, and thus, to use HBaseStorage with pig.jar, hbase.jar and zookeeper.jar must be present in classpath. To add them to classpath, the user can set either PIG_CLASSPATH=<path_to_hbase.jar:path_to_zookeeper.jar>, or HBASE_HOME=<hbase_home_dir> and ZOOKEEPER_HOME=<zookeeper_home_dir>.",,,,,,,,,25/Aug/12 21:13;dvryaboy;In regards to HBase 0.94 dependencies -- shouldn't they get pulled in directly via hbase's pom? Are they not declared? ,"26/Aug/12 21:13;cheolsoo;Hi Dmitriy, they're defined in hbase's pom, but they are not pulled down by pig's ivy because the dependency is defined as follows:
{code}
<dependency org=""org.apache.hbase"" name=""hbase"" rev=""${hbase.version}"" conf=""compile->master"">
{code}

As can be seen, the ""compile"" configuration depends on ""master"" not ""runtime"", so runtime dependencies such as protobuf and high-scale-lib are not pulled down at compile time. I could change ""master"" to ""runtime"" instead of adding separate dependency tags, but then we would pull down unnecessary dependencies at compile time even though they're only needed at runtime.

Thanks!","26/Aug/12 21:36;dvryaboy;I think the right thing to do here would be to just stop bundling HBase altogether, and still depend on compile->master.
I'm not sure what that would look like (meaning, how users would provide path to their runtime hbase jar). Probably we just have to document that HBASE_CLASSPATH has to be set?","26/Aug/12 22:03;cheolsoo;I agree that we should stop bundling hbase jar and not change ""compile->master"". It also makes sense to ask the user to set HBASE_CLASSPATH.

But we still need to pull down hbase jar and its dependencies for *test*, right? For that, I added two dependency tags for protobuf and high-scale-lib with ""test->default"".

Thanks!","26/Aug/12 22:59;dvryaboy;Great, I think we are getting close.

I still don't like manually adding hbase's dependencies. Pretty sure we can add configurations to our ivy and make a similar distinction to HBase's, and depend on hbase compile->runtime in test, but on compile->master for the normal build. 

Also, we should update all the docs that mention compatibility and mention somewhere that one needs to set HBASE_CLASSPATH.

I noticed that hadoop version bump. Is that required for hbase, or just something you took care of while you were in there? I'm not sure what we want to do about minor hadoop version compatibility, but just thought I'd bring that up...","27/Aug/12 20:27;cheolsoo;Actually, we have a bigger problem. The hbase jar in the maven repository is not binary-compatible with hadoop 0.23 (HBASE-5680). To get hbase working with hadoop 0.23, we have to recompile the source code against hadoop 0.23.

Using the hbase jar from the maven repository makes ""TestHBaseStorage -Dhadoopversion=23"" fail with the following error:
{code}
2012-08-25 12:55:47,100 FATAL [Master:0;localhost.localdomain,49603,1345924546912] master.HMaster (HMaster.java:abort(1388)) - HBase is having a problem with its Hadoop jars.  You may need to recompile HBase against Hadoop version 0.23.1 or change your hadoop jars to start properly
java.lang.NoClassDefFoundError: org/apache/hadoop/hdfs/protocol/FSConstants$SafeModeAction
    at org.apache.hadoop.hbase.util.FSUtils.waitOnSafeMode(FSUtils.java:524)
    at org.apache.hadoop.hbase.master.MasterFileSystem.checkRootDir(MasterFileSystem.java:324)
    at org.apache.hadoop.hbase.master.MasterFileSystem.createInitialFileSystemLayout(MasterFileSystem.java:127)
    at org.apache.hadoop.hbase.master.MasterFileSystem.<init>(MasterFileSystem.java:112)
    at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:480)
    at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:343)
    at java.lang.Thread.run(Thread.java:662)
{code}

I am going to table this for now.","27/Sep/12 19:20;dvryaboy;HBASE-5680 is marked as resolved -- does that unblock us, or are we depending on them to make a release?","27/Sep/12 21:29;cheolsoo;Hi Dmitriy,

What HBASE-5680 did is improving the error message (as shown below) when there is incompatibility between hbase and hadoop:
{code}
You may need to recompile HBase against Hadoop version 0.23.1 or change your hadoop jars to start properly
{code}
So the hbase jar in maven repository is still not compatible with hadoop 0.23.x and 2.0.x.

What I am thinking is to disable the hbase tests when hadoopversion=23. Or we have to download the source tarball, compile, and run the test. I think that this is a bit too much for a unit test.

If this sounds fine to everyone, I will resume working on this.

Thanks!","27/Sep/12 21:44;dvryaboy;Yeah let's disable them for now and move on.
I am sure this will bite us at some point.. is there an HBase ticket to publish jars compatible with hadoop-23?",29/Sep/12 07:06;ionignat;Is this patch compatible with older HBase&Hadoop versions or it just fixes problem with Hbase 0.94 and Hadoop 1.0.x?,"29/Sep/12 16:29;cheolsoo;Hi Ionut,

If you're asking about compatibility in terms of using pig with hbase, please see my [comment|https://issues.apache.org/jira/browse/PIG-2891?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13466255#comment-13466255].

This patch is not compatible with older versions of hbase, but that's fine because this is only for testing. Currently, pig already works fine with hbase-0.90/92/94, so nothing has to be fixed in that regard.

Thanks!","20/Oct/12 01:00;cheolsoo;Updating the patch:

1. Stop bundling hbase.jar and zookeoper.jar with pig.jar. So there should be no longer incompatibility issues when using pig.jar with different versions of hbase.jar. But to use HBaseStorage, HBASE_HOME and ZOOKEEPER_HOME must be set by the user.

Note that I am adding protobuf-java.jar to pig.jar because otherwise it has to be explicitly added to PIG_CLASSPATH to use HBaseStorage, which is not very intuitive.

2. Bump hbase and zk to 0.94.1 and 3.4.3 respectively. Since we no longer bundle them in pig.jar, which versions we use doesn't matter. These jar files will be used for unit test only.

3. Make the unit test cases work with newer versions of hbase and zk.

4. Add hbase runtime dependencies to ivy.xml. I tried to find an alternative way of specifying distinct dependencies in ivy.xml without adding them explicitly but no luck.

Basically, what I want to do is as follows:
- pulls down hbase.jar only at compile time.
- pulls down runtime dependencies as well at test time.

What I tried was ""conf=compile->master;test->default"", but apparently, it doesn't work. Instead, runtime depedencies are always pulled down. Please let me know if anyone has a better idea about how to achieve this.

Thanks!","20/Oct/12 19:49;cheolsoo;Removed tabs and tailing whitespaces. It may be easier to see code changes in the RB by hiding whitespace changes: https://reviews.apache.org/r/7676/

Tests done:
- ant test-commit
- ant test -Dtestcase=TestHBaseStorage -Dhadoopversion=20
- ant test -Dtestcase=TestJobSubmission -Dhadoopversion=20
- Manual testing HBaseStorage using pig.jar with hbase-0.90 and 0.94

Note that TestHBaseStorage is excluded in hadoopversion=23.
",23/Oct/12 16:52;cheolsoo;Bumping ZK to 3.4.4 since ZK 3.4.3 has known issues. (Please see Santhosh's comment for details.),24/Oct/12 03:26;sms;Patch has been committed. Thanks Cheolsoo!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig current releases lack a UDF equalIgnoreCase.This function returns a Boolean value indicating whether string left is equal to string right. This check is case insensitive.,PIG-2878,12603687,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,shamib007,arjunkr,arjunkr,16/Aug/12 05:27,16/Apr/14 19:50,14/Mar/19 03:07,01/Feb/13 23:07,0.10.0,,,,,,,0.12.0,,,,internal-udfs,,,0,features,,,,,,,,,,,,Pig current releases lack a UDF equalIgnoreCase.This function returns a Boolean value indicating whether string left is equal to string right. This check is case insensitive.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Feb/13 19:52;alangates;PIG-2878-1.patch;https://issues.apache.org/jira/secure/attachment/12567626/PIG-2878-1.patch,29/Jan/13 13:15;shamib007;PIG-2878-UnitTest.patch;https://issues.apache.org/jira/secure/attachment/12566947/PIG-2878-UnitTest.patch,07/Dec/12 14:05;shamib007;PIG-2878.patch;https://issues.apache.org/jira/secure/attachment/12559859/PIG-2878.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-12-05 14:45:18.672,,,no_permission,,,,,,,,,,,,249037,,,,Wed Apr 16 19:50:57 UTC 2014,,,,,,,0|i0a3vj:,56901,,,,,,,"Pig UDF,equalIgnoreCase",,,"05/Dec/12 14:45;shamib007;We can have a solution by extending the FilterFunc class in Pig. The exec() method implementation will be like :
    String firstStr = (String)input.get(0);
    String secondStr = (String)input.get(1);
    return new Boolean(firstStr.equalsIgnoreCase(secondStr));

where input is the argument to exec() method.",07/Dec/12 14:05;shamib007;Please find the attached UDF for Equals Ignore.,"23/Jan/13 18:56;alangates;First, let me apologize for taking so long to get to this.  We should have reviewed it a lot sooner.  

The patch looks fine.  It needs tests however.  You need to add unit tests to check that this UDF correctly compares strings.","29/Jan/13 12:16;shamib007;Thanks for the review comments.
I am working on the unit test cases and will submit it soon.",29/Jan/13 13:15;shamib007;Please find the attached unit test case for the EqualsIgnore UDF.,29/Jan/13 13:16;shamib007;Please find the attached unit test case for EqualsIgnoreCase UDF,29/Jan/13 17:33;alangates;I'll review this.,01/Feb/13 19:52;alangates;Attaching a single patch with the previous two combined.  I also took the liberty of expanding the unit test to have a negative case.  This patch represents what I will check in.,01/Feb/13 23:07;alangates;Patch 1 checked into trunk.  Thanks Shami for your work on this.,"16/Apr/14 19:50;mrflip;It should fail when more than one input is passed to the UDF: 

    ff = FILTER franchises BY EqualsIgnoreCase(franch_id, 'bOs', 'shouldfail'); -- should fail, doesn't
    DUMP(ff);
    -- (BOS,Boston Red Sox,Y,)

Should I re-post as a separate bug, or is a comment here appropriate?
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump up Xerces version,PIG-2876,12603520,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,jcoveney,jcoveney,14/Aug/12 22:04,22/Feb/13 04:53,14/Mar/19 03:07,14/Aug/12 23:24,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"In some cases on some environments, our version of xerces has errors with Hadoop. Bumping the version and adding Xalan fixes this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Aug/12 22:05;jcoveney;PIG-2876-0.patch;https://issues.apache.org/jira/secure/attachment/12540963/PIG-2876-0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-14 22:50:30.597,,,no_permission,,,,,,,,,,,,256473,,,,Tue Aug 14 22:50:30 UTC 2012,,,,,,,0|i0h68f:,98272,,,,,,,,,,"14/Aug/12 22:50;billgraham;+1!

Thanks for this! This fix is what I need to run tests that use mini cluster on my macbook pro.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Converting bin/pig shell script to python,PIG-2873,12603311,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,vikram.dixit,vikram.dixit,vikram.dixit,13/Aug/12 19:05,14/Oct/13 16:46,14/Mar/19 03:07,03/May/13 22:13,0.10.0,,,,,,,0.12.0,,,,tools,,13/Aug/12 00:00,0,,,,,,,,,,,,,Converted the shell script in a platform independent way in python. Should work with version 2.7.x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Aug/12 19:07;vikram.dixit;PIG-2873.patch;https://issues.apache.org/jira/secure/attachment/12540727/PIG-2873.patch,14/Aug/12 00:12;vikram.dixit;PIG-2873_2.patch;https://issues.apache.org/jira/secure/attachment/12540809/PIG-2873_2.patch,25/Aug/12 00:21;vikram.dixit;PIG-2873_3.patch;https://issues.apache.org/jira/secure/attachment/12542380/PIG-2873_3.patch,22/Mar/13 23:47;vikram.dixit;PIG-2873_4.patch;https://issues.apache.org/jira/secure/attachment/12575136/PIG-2873_4.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-08-13 22:30:43.422,,,no_permission,,,,,,,,,,,,256472,Reviewed,,,Fri May 03 22:13:29 UTC 2013,,,,,,,0|i0h67j:,98268,,,,,,,"Python, Pig",,,13/Aug/12 19:07;vikram.dixit;This has been tested with python 2.7.1,"13/Aug/12 19:09;vikram.dixit;To run this script, download and install python 2.7.x. Run the script as python bin/pig.py. Ensure that you have HADOOP_HOME set to the correct location although the script tries to intelligently deduce that.","13/Aug/12 22:30;traviscrawford;Very cool the pig script will be switched over to python!

Just an an FYI though – even though python 2.7 was released quite some time back its likely not installed on a lot of production machines. For example, RHEL5 is probably the most widely deployed version and that ships with python 2.4 as a default. Of course people could install newer versions but it may not be available by default.",14/Aug/12 00:12;vikram.dixit;Tested this on 2.4.3 as well. It needs HADOOP_HOME and JAVA_HOME to be set or present as a set of key-value pairs in the PIG_CONF_DIR/pig.conf. Please let me know if you run into issues.,"14/Aug/12 01:11;vikram.dixit;Works with Python 2.4 and 2.7 as well. To run successfully, please set the HADOOP_HOME and JAVA_HOME values in the environment or provide them as key value pairs in a pig.conf file in PIG_CONF_HOME.","22/Aug/12 03:57;alangates;Comments:

1)
{code}
try:
  confFileHdl = open(os.path.join(os.environ['PIG_CONF_DIR'], 'pig.conf'), 'r')
  for line in confFileHdl:
    words = line.split()
    if len(words) > 2: # since we expect only key value pairs
      continue
    else:
      os.eviron[words[0]] = words[1]
{code}
Won't the test ""len(words) > 2"" mean we reject lines with comments?  E.g. ""key=value # this is a comment""

2) In the HCat section we should look for hive jar in /usr/lib/hive when HIVE_HOME isn't set, since that's where Bigtop RPMs put them?  Same for HCAT_HOME.",25/Aug/12 00:21;vikram.dixit;Addressed Alan's comments.,"04/Feb/13 17:40;alangates;Vikram,

Patch looks reasonable.  But we need tests to assure that pig.py responds in the same way as the current pig bash shell.  These could easily be written as a new driver in the e2e framework.","22/Mar/13 23:47;vikram.dixit;Integrated the python script with the e2e tests. While running the test-e2e target we can use the python script to run the tests by using the flag

{noformat}
-Dharness.use.python=true

e.g. ant -Dharness.old.pig=/grid/0/pig/old_pig/ -Dharness.cluster.conf=/usr/lib/hadoop/conf/ -Dharness.cluster.bin=/usr/lib/hadoop/bin/hadoop -Dharness.use.python=true test-e2e

{noformat}

","03/May/13 22:13;daijy;Patch committed to trunk. Though pig.py need more tests, that could be a new Jira. Thanks Vikram!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor signature for PigReducerEstimator,PIG-2871,12603159,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,billgraham,billgraham,billgraham,13/Aug/12 01:53,22/Feb/13 04:53,14/Mar/19 03:07,14/Aug/12 00:09,0.11,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"The signature to this method should be refactored to provide more context so the implementer has access to the map and reduce plans. Also passing both {{Job}} and {{Configuration}} is redundant, since the latter can be obtained by the former.

I propose changing this:
{noformat}
int estimateNumberOfReducers(Configuration conf, List<POLoad> lds, Job job) throws IOException;
{noformat}

To this:
{noformat}
int estimateNumberOfReducers(Job job, MapReduceOper mapReduceOper) throws IOException;
{noformat}

This interface is evolving and hasn't yet been released so this is safe.",,,,,,,,,,,,,,,,,,,PIG-2574,,,,,,,,,,,,,13/Aug/12 02:16;billgraham;PIG-2871.1.patch;https://issues.apache.org/jira/secure/attachment/12540597/PIG-2871.1.patch,13/Aug/12 19:34;billgraham;PIG-2871.2.patch;https://issues.apache.org/jira/secure/attachment/12540735/PIG-2871.2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-08-13 17:14:54.715,,,no_permission,,,,,,,,,,,,256470,,,,Tue Aug 14 00:09:38 UTC 2012,,,,,,,0|i0h66n:,98264,,,,,,,,,,13/Aug/12 02:16;billgraham;Here's a patch with the proposed refactor.,"13/Aug/12 17:14;dvryaboy;Overall, I approve, but given the MRO, do we still need to pass in POLoads? They are trivially accessible from the MRO, and passing them in separately seems like an opportunity to pass in something wrong (an mro that doesn't actually have inputs from the supplied loaders).","13/Aug/12 19:33;billgraham;Good call, simplify the signatures.

Updating the JIRA description now, patch to follow.",13/Aug/12 19:34;billgraham;Patch 2 with changed signature.,"13/Aug/12 21:16;dvryaboy;+1
#shipit","14/Aug/12 00:09;billgraham;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigServer fails with macros without a script file,PIG-2866,12602680,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,billgraham,billgraham,billgraham,09/Aug/12 22:21,22/Feb/13 04:53,14/Mar/19 03:07,14/Aug/12 21:12,0.10.0,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,Making a call to {{PigServer.registerQuery}} with an {{InputStream}} will fail if the script contains a macro. This is because {{QueryParserDriver.makeMacroDef}} requires a filename. {{QueryParserDriver.makeMacroDef}} should be changed to not assume that the script input is from a file.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Aug/12 00:16;billgraham;PIG-2866.1.patch;https://issues.apache.org/jira/secure/attachment/12540164/PIG-2866.1.patch,13/Aug/12 21:26;billgraham;PIG-2866.2.patch;https://issues.apache.org/jira/secure/attachment/12540760/PIG-2866.2.patch,13/Aug/12 22:42;billgraham;PIG-2866.3.patch;https://issues.apache.org/jira/secure/attachment/12540789/PIG-2866.3.patch,13/Aug/12 23:44;billgraham;PIG-2866.4.patch;https://issues.apache.org/jira/secure/attachment/12540800/PIG-2866.4.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-08-10 01:38:58.592,,,no_permission,,,,,,,,,,,,256467,,,,Tue Aug 14 00:00:52 UTC 2012,,,Patch Available,,,,0|i0h64n:,98255,,,,,,,,,,"09/Aug/12 22:34;billgraham;The job fails with an exception like this:
{noformat}
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1000: Error during parsing. Can not create a Path from a null string
	at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1595)
	at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1534)
	at org.apache.pig.PigServer.registerQuery(PigServer.java:516)
	at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:990)
	at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:412)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:193)
	at org.apache.pig.PigServer.registerScript(PigServer.java:590)
	at org.apache.pig.PigServer.registerScript(PigServer.java:555)
Caused by: java.lang.IllegalArgumentException: Can not create a Path from a null string
	at org.apache.hadoop.fs.Path.checkPathArg(Path.java:78)
	at org.apache.hadoop.fs.Path.<init>(Path.java:90)
	at org.apache.pig.impl.io.FileLocalizer.fetchFilesInternal(FileLocalizer.java:766)
	at org.apache.pig.impl.io.FileLocalizer.fetchFile(FileLocalizer.java:733)
	at org.apache.pig.parser.QueryParserDriver.getMacroFile(QueryParserDriver.java:350)
	at org.apache.pig.parser.QueryParserDriver.makeMacroDef(QueryParserDriver.java:406)
	at org.apache.pig.parser.QueryParserDriver.expandMacro(QueryParserDriver.java:268)
	at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:169)
	at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1587)
{noformat}","09/Aug/12 22:45;billgraham;Here's a workaround for now. Instead of passing an {{InputStream}}, pass a file:

{noformat}
InputStream scriptInputStream = ...

File file = File.createTempFile(""foo"", ""bar"");
file.deleteOnExit();
OutputStream out = new FileOutputStream(file);

int read = 0;
byte[] bytes = new byte[1024];
while ((read = scriptInputStream.read(bytes)) != -1) {
   out.write(bytes, 0, read);
}

scriptInputStream.close();
out.flush();
out.close();

ScriptState.get().setFileName(file.getAbsolutePath()); <---- This is important
pigServer.registerScript(file.getAbsolutePath());
{noformat}

",10/Aug/12 00:16;billgraham;Here's a unit test with the fix.,10/Aug/12 01:38;arov;Thanks Bill!,"10/Aug/12 01:39;arov;Should we change the affected version?
","13/Aug/12 17:11;dvryaboy;2 comments:
1) having 2 variables, one named fname and the other named filename is confusing. Please rename.

2) For the test, you are creating an actual temp file. It's better to use mock.Storage, we should encourage use of that utility throughout tests.",13/Aug/12 21:26;billgraham;Good call. Here's a patch that changes my new test and the existing tests to use mock.Storage.,"13/Aug/12 21:37;billgraham;Actually, hold off on reviewing patch #2. I need to look into something else.",13/Aug/12 22:42;billgraham;Here's patch #3. It moves the macro tests to their own class so they can use {{PigServer}} in local mode and hence use {{mock.Storage}}.,"13/Aug/12 23:27;dvryaboy;+1, commit after adding Apache headers to TestPigServerWithMacros.java",13/Aug/12 23:28;dvryaboy;oh and no @author annotations per project guidelines.,"13/Aug/12 23:44;billgraham;Aww man, rookie mistakes! Adding version 4 of the patch, which gets it prostyle. Will commit shortly.","13/Aug/12 23:50;billgraham;Committed.

This JIRA should be marked resolved fixed if we can it out of it's bad state.",14/Aug/12 00:00;dvryaboy;was trying to see if reassigning would bring the close button back.. alas.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PlanHelper imports org.python.google.common.collect.Lists instead of org.google.common.collect.Lists,PIG-2861,12601641,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,jcoveney,jcoveney,07/Aug/12 00:01,22/Feb/13 04:54,14/Mar/19 03:07,07/Aug/12 00:05,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,Fix is easy.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Aug/12 00:05;jcoveney;PIG-2861-0.patch;https://issues.apache.org/jira/secure/attachment/12539382/PIG-2861-0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,256462,,,,2012-08-07 00:01:20.0,,,Patch Available,,,,0|i0h62n:,98246,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestAvroStorageUtils.testGetConcretePathFromGlob fails on some version of hadoop,PIG-2860,12601639,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,06/Aug/12 23:43,22/Feb/13 04:53,14/Mar/19 03:07,13/Aug/12 20:27,0.10.0,,,,,,,0.11,,,,piggybank,,,0,,,,,,,,,,,,,"I found that TestAvroStorageUtils.testGetConcretePathFromGlob fails on some versions of hadoop (not ones that upstream Pig is currently using) with the following error:

{code}
Call From localhost.localdomain/127.0.0.1 to localhost.localdomain:55883 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
java.net.ConnectException: Call From localhost.localdomain/127.0.0.1 to localhost.localdomain:55883 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
    at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:722)
    at org.apache.hadoop.ipc.Client.call(Client.java:1164)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:195)
    at $Proxy12.getFileInfo(Unknown Source)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)
    at $Proxy12.getFileInfo(Unknown Source)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:613)
    at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1399)
    at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:740)
    at org.apache.hadoop.fs.FileSystem.getFileStatus(FileSystem.java:2083)
    at org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:1547)
    at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1488)
    at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.getConcretePathFromGlob(AvroStorageUtils.java:146)
    at org.apache.pig.piggybank.test.storage.avro.TestAvroStorageUtils.testGetConcretePathFromGlob(TestAvroStorageUtils.java:142)
{code}

The fix is to explicitly add the URI scheme ""file://"" to the path that is used in the test.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Aug/12 23:45;cheolsoo;PIG-2860.patch;https://issues.apache.org/jira/secure/attachment/12539376/PIG-2860.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-09 16:53:28.41,,,no_permission,,,,,,,,,,,,242339,,,,Mon Aug 13 20:27:51 UTC 2012,,,,,,,0|i02tvr:,14442,,,,,,,,,,"06/Aug/12 23:49;cheolsoo;Review board:
https://reviews.apache.org/r/6412/","09/Aug/12 16:53;jcoveney;+1

Thanks Chelsoo!","13/Aug/12 20:27;cheolsoo;Closing it since the patch is committed.

Thanks Jonathan for committing my patch!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix few e2e test failures,PIG-2859,12601522,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,rohini,rohini,rohini,06/Aug/12 01:45,06/Jan/13 23:57,14/Mar/19 03:07,07/Aug/12 18:27,0.10.1,,,,,,,0.10.1,0.11,,,,,,0,,,,,,,,,,,,,"ClassResolution_1, Native_1_Local, Native_2_Local, Native_3_Local, Jython_Macro_1_local and NegForeach_8 are some of the failing e2e tests that need to be fixed. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Aug/12 01:52;rohini;PIG-2859.patch;https://issues.apache.org/jira/secure/attachment/12539232/PIG-2859.patch,06/Aug/12 01:48;rohini;PIG-2859.patch;https://issues.apache.org/jira/secure/attachment/12539231/PIG-2859.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-08-07 18:27:54.115,,,no_permission,,,,,,,,,,,,256461,,,,Tue Aug 07 18:27:54 UTC 2012,,,,,,,0|i0h61r:,98242,,,,,,,,,,"06/Aug/12 01:48;rohini;1) ClassResolution_1 - UDF was referred by class name without the fully qualifed package name
2) Native_x_local - The mapredjars were not configured in local.conf
3) Jython_Macro_1_Local - Was failing because the input file was the data directory itself instead
of studenttab10K. In case of mapred and local, the number of files under data
directory was different so Jython_Macro_1 passed with the benchmark created during mapred, but
Jython_Macro_1_local failed with the benchmark from mapred.
4)NegForeach - The regex string had a hardcoded input path instead of considering :INPATH:",06/Aug/12 01:52;rohini;Reattaching same patch with Grant License checked. ,07/Aug/12 18:27;daijy;Committed to 0.10/trunk. Thanks Rohini!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroStorage doesn't load files in the directories when a glob pattern matches both files and directories.,PIG-2856,12601189,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,02/Aug/12 09:25,22/Feb/13 04:54,14/Mar/19 03:07,06/Aug/12 05:22,0.11,,,,,,,0.11,,,,piggybank,,,0,,,,,,,,,,,,,"This is a regression from PIG-2492.

When a glob pattern such as '*' matches not only files but also directories, AvroStorage does not load files in the directories. This is a bug in getAllSubDirs() that can be fixed as follows:

{code}
static boolean getAllSubDirs(Path path, Job job, Set<Path> paths)
...
FileStatus[] matchedFiles = fs.globStatus(path, PATH_FILTER);
...
for (FileStatus file : matchedFiles) {
    if (file.isDir()) {
-        for (FileStatus sub : fs.listStatus(path)) {
+        for (FileStatus sub : fs.listStatus(file.getPath())) {
            getAllSubDirs(sub.getPath(), job, paths);
        }
    }
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Aug/12 04:30;cheolsoo;PIG-2856-2.patch;https://issues.apache.org/jira/secure/attachment/12539174/PIG-2856-2.patch,02/Aug/12 09:34;cheolsoo;PIG-2856.patch;https://issues.apache.org/jira/secure/attachment/12538887/PIG-2856.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-08-06 05:21:38.333,,,no_permission,,,,,,,,,,,,242340,,,,Mon Aug 06 07:41:07 UTC 2012,,,Patch Available,,,,0|i02tw7:,14444,,,,,,,,,,"02/Aug/12 09:34;cheolsoo;Attach is a patch that fixes the bug in getAllSubDirs() and updates the unit test testGlob1.

Regarding the test, ""expected_test_dir_1.avro"" includes files in test_dir1 but doesn't include ones in its sub-directory test_subdir. On the other hand, ""expected_testDir.avro"" includes files not only test_dir1 but also its sub-directory test_subdir.

Since all files in test_dir1 and its sub-directory are supposed to be loaded, ""expected_testDir.avro"" is used.","02/Aug/12 09:38;cheolsoo;Review board:
https://reviews.apache.org/r/6318/","05/Aug/12 04:30;cheolsoo;Regarding why this problem was not caught by testGlob1, there are actually two reasons:

# The expected output was incorrect (as mentioned above).
# The job status was not checked at all. So even though the job failed, the test still passed if it generated the expected output. In testGlob1, the job failed after loading 3 files, but since that happened to be the expected output, the test still passed. 

I've updated the patch so that not only is the expected output for testGlob1 updated, but the job status also is checked.

Thanks!",06/Aug/12 05:21;sms;All unit tests pass except TestDBStorage (Hadoop 20) and TestMultiStorage (Hadoop 20 and Hadoop 23). Patch has been committed. Thanks Cheolsoo!,06/Aug/12 07:40;sms;Forgot to add that TestLookupInFiles in Hadoop 23 is erroring out.,06/Aug/12 07:41;sms;Addendum to previous comment - its unrelated to this patch and existed previously.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroStorage doesn't work with Avro 1.7.1,PIG-2854,12601135,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,01/Aug/12 22:13,22/Feb/13 04:53,14/Mar/19 03:07,02/Aug/12 02:44,0.10.0,,,,,,,0.11,,,,piggybank,,,0,,,,,,,,,,,,,"PigAvroDatumWriter fails to compile against Avro 1.7.1 with the following error:

{code}
/home/cheolsoo/workspace/pig-trunk/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/avro/PigAvroDatumWriter.java:119: resolveUnion(org.apache.avro.Schema,java.lang.Object) in org.apache.pig.piggybank.storage.avro.PigAvroDatumWriter cannot override resolveUnion(org.apache.avro.Schema,java.lang.Object) in org.apache.avro.generic.GenericDatumWriter; overridden method does not throw java.io.IOException
protected int resolveUnion(Schema union, Object datum) throws IOException {
{code}

The problem is that a new method resolveUnion() is added to GenericDatumWriter in Avro 1.6.x and 1.7.x, but this method conflicts with the one that already exists in PigAvroDatatumWriter.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Aug/12 22:17;cheolsoo;PIG-2854.patch;https://issues.apache.org/jira/secure/attachment/12538842/PIG-2854.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-02 01:56:03.117,,,no_permission,,,,,,,,,,,,242343,Reviewed,,,Thu Aug 02 02:44:20 UTC 2012,,,,,,,0|i02txb:,14449,,,,,,,,,,"01/Aug/12 22:17;cheolsoo;Attached is a patch that changes the name of the method in PigAvroDatumWriter from resolveUnion() to resolveUnionSchema() to avoid the conflict.

I tested against Avro 1.5.3, 1.6.3, and 1.7.1.

Thanks!",02/Aug/12 01:56;sms;+1 looks good. I will commit it after running the tests.,"02/Aug/12 02:44;sms;The patch has been committed to trunk. I tested the changes for both Hadoop 20 and Hadoop 23. There are unrelated failure in TestDBStorage (20 and 23) and TestMultiStorage (23)

Thanks Cheolsoo!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestBuiltInBagToTupleOrString fails now that mock.Storage enforces not overwriting output,PIG-2848,12600533,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,julienledem,julienledem,julienledem,27/Jul/12 23:54,22/Feb/13 04:54,14/Mar/19 03:07,23/Aug/12 20:31,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Jul/12 23:57;julienledem;PIG-2848.patch;https://issues.apache.org/jira/secure/attachment/12538220/PIG-2848.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-23 18:39:12.301,,,no_permission,,,,,,,,,,,,256452,,,,Thu Aug 23 18:51:08 UTC 2012,,,Patch Available,,,,0|i0h5xb:,98222,,,,,,,,,,"23/Aug/12 18:39;knoguchi;What does it take to get this in? 
I want to see successful unit-test on trunk...","23/Aug/12 18:46;julienledem;I just got back from a trip. 
I need a +1 to check this in.",23/Aug/12 18:51;jcoveney;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ant makepom is misconfigured,PIG-2844,12600404,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,julienledem,julienledem,julienledem,27/Jul/12 00:21,22/Feb/13 04:53,14/Mar/19 03:07,12/Sep/12 00:18,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,Currently we manually maintain a pom. We should use the ant makepom target for this,,,,,,,,,,,,,,,,,,,,,,,,,PIG-2938,,,,,,,27/Jul/12 00:25;julienledem;PIG-2844_0.patch;https://issues.apache.org/jira/secure/attachment/12538099/PIG-2844_0.patch,12/Sep/12 00:17;julienledem;PIG-2844_1.patch;https://issues.apache.org/jira/secure/attachment/12544739/PIG-2844_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-08-22 23:19:53.848,,,no_permission,,,,,,,,,,,,242074,,,,Wed Sep 26 22:17:54 UTC 2012,,,Patch Available,,,,0|i02ihz:,12598,,,,,,,,,,22/Aug/12 23:19;alangates;Patch looks fine to me.  The generated pom is very different than previously.  Is that expected?,"23/Aug/12 21:56;julienledem;I believe that the previously generated pom was incorrect.
There's no good way of being sure unless we try to build Pig with Maven","23/Aug/12 22:01;alangates;Ok, +1 I guess then.",12/Sep/12 00:17;julienledem;rebased PIG-2844_1.patch,"26/Sep/12 22:17;rohini;[~julienledem]
    This patch comments out netty in ivy.xml. It is causing tests to fail in H23 mode. I am not sure how makepom works. Can you tell what was the problem with netty as a dependency? I can open a jira and work on trying to have netty as a dependency without conflicting with makepom. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestNewPlanOperatorPlan fails when new Configuration() picks up a previous minicluster conf file,PIG-2842,12600374,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,julienledem,julienledem,julienledem,26/Jul/12 20:06,22/Feb/13 04:53,14/Mar/19 03:07,26/Jul/12 20:16,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Jul/12 20:07;julienledem;PIG-2842.patch;https://issues.apache.org/jira/secure/attachment/12538067/PIG-2842.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-26 20:09:03.449,,,no_permission,,,,,,,,,,,,256448,,,,Thu Jul 26 20:09:03 UTC 2012,,,Patch Available,,,,0|i0h5vb:,98213,,,,,,,,,,26/Jul/12 20:07;julienledem;PIG-2842.patch fixes this,26/Jul/12 20:09;jcoveney;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix SchemaTuple bugs,PIG-2840,12600350,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,jcoveney,jcoveney,26/Jul/12 17:17,22/Feb/13 04:54,14/Mar/19 03:07,21/Sep/12 18:57,0.11,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"SchemaTuple had some subtle bugs that are now fixed.
- hashCode should now be consistent with any normal Tuple
- comparison had a subtle but nasty bug that is now fixed
- some minor performance improvements",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Jul/12 17:18;jcoveney;PIG-2840-0.patch;https://issues.apache.org/jira/secure/attachment/12538030/PIG-2840-0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-26 20:13:14.469,,,no_permission,,,,,,,,,,,,256446,,,,Thu Jul 26 20:13:14 UTC 2012,,,Patch Available,,,,0|i0h5uf:,98209,,,,,,,,,,26/Jul/12 20:13;julienledem;+1 Looks good to me,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mock.Storage overwrites output with the last relation written when storing UNION,PIG-2839,12600347,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,julienledem,julienledem,julienledem,26/Jul/12 16:55,22/Feb/13 04:53,14/Mar/19 03:07,26/Jul/12 20:49,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Jul/12 17:28;julienledem;PIG-2839.patch;https://issues.apache.org/jira/secure/attachment/12538031/PIG-2839.patch,26/Jul/12 18:55;julienledem;PIG-2839_a.patch;https://issues.apache.org/jira/secure/attachment/12538045/PIG-2839_a.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-07-26 19:09:03.773,,,no_permission,,,,,,,,,,,,256445,,,,Fri Jul 27 23:57:59 UTC 2012,,,,,,,0|i0h5tz:,98207,,,,,,,,,,26/Jul/12 17:28;julienledem;Attaching a test reproducing the bug,26/Jul/12 18:55;julienledem;PIG-2839_a.patch fixes the bug,26/Jul/12 19:09;jcoveney;+1 assuming test-commit passes (since that depends on it),"26/Jul/12 21:19;aneeshs;Thanks, Julien!!","27/Jul/12 21:35;knoguchi;I'm seeing two unit tests failing that use mock.Storage.  Can this be related?

org.apache.pig.test.TestBuiltInBagToTupleOrString.testPigScriptrForBagToStringUDF
{noformat} 
junit.framework.AssertionFailedError: expected:<(a==b==c)> but was:<({(a),(b),(c)})>
	at org.apache.pig.test.TestBuiltInBagToTupleOrString.testPigScriptrForBagToStringUDF(TestBuiltInBagToTupleOrString.java:507)
{noformat} 


org.apache.pig.test.TestBuiltInBagToTupleOrString.testPigScriptMultipleElmementsPerTupleForBagToStringUDF 
{noformat} 
junit.framework.AssertionFailedError: expected:<(a^b^c^d^e^f)> but was:<({(a,b),(c,d),(e,f)})>
	at org.apache.pig.test.TestBuiltInBagToTupleOrString.testPigScriptMultipleElmementsPerTupleForBagToStringUDF(TestBuiltInBagToTupleOrString.java:528)
{noformat} ","27/Jul/12 23:51;julienledem;The mock.Storage now enforces that you don't overwrite data.
Which this test is doing.
I'll provide a patch.",27/Jul/12 23:57;julienledem;see: https://issues.apache.org/jira/browse/PIG-2848,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroStorage throws StackOverFlowError,PIG-2837,12600256,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,mubarakseyed,mubarakseyed,26/Jul/12 02:22,22/Feb/13 04:54,14/Mar/19 03:07,06/Aug/12 07:47,0.10.0,,,,,,,0.11,,,,piggybank,,,0,,,,,,,,,,,,,"When i try to dump avro data using


{code}
records = LOAD '/logs/records/07262012/01/1/Record.1343265732700.avro' using org.apache.pig.piggybank.storage.avro.AvroStorage(); 
dump records;
{code}

{code}
Pig Stack Trace 
--------------- 
ERROR 2998: Unhandled internal error. null
java.lang.StackOverflowError 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:258) 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:262) 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:262) 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:271) 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:284) 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:262) 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:271) 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:284) 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:262) 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:271) 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:284) 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:262) 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:271) 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:284) 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:262) 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:271) 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:284) 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:262) 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:271) 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:284) 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:262) 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:271) 
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.containsGenericUnion(AvroStorageUtils.java:284)
{code}

I did verify the avro schema using avro-tools and dump the data as json format, data looks good.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Aug/12 06:50;cheolsoo;PIG-2837-2.patch;https://issues.apache.org/jira/secure/attachment/12539243/PIG-2837-2.patch,27/Jul/12 02:49;cheolsoo;PIG-2837.patch;https://issues.apache.org/jira/secure/attachment/12538117/PIG-2837.patch,27/Jul/12 02:30;cheolsoo;avro_test_files.tar.gz;https://issues.apache.org/jira/secure/attachment/12538114/avro_test_files.tar.gz,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-07-26 11:37:12.893,,,no_permission,,,,,,,,,,,,242341,,,,Mon Aug 06 23:15:46 UTC 2012,,,,,,,0|i02twf:,14445,,,,,,,,,,"26/Jul/12 11:37;qwertymaniac;I can imagine this happening if there's a self-reference in the schema. For example, an array of the record name itself being a field inside the schema. In this case, the record's type and schema is re-resolved and it enters an infinite loop.

I suppose we could keep a reference of names (with namespace, for correctness) and only re-call if we've not already visited it?","27/Jul/12 02:47;cheolsoo;It seems that AvroStorage does not support recursive record and generic union:

{quote}
1. Limited support for ""record"": we do not support recursively defined record because the number of fields in such records is data dependent.
2. Limited support for ""union"": we only accept nullable union like [""null"", ""some-type""].
{quote}
https://cwiki.apache.org/PIG/avrostorage.html

AvroStorage checks the above limitations and throws exceptions when violated; however, since #2 is checked before #1, we ends up with stack overflow if schema is recursive. This can be avoided by changing the order of the checks so that AvroStorage fails fast if schema is recursive.

I uploaded a patch that changes the order of the checks and adds two test cases to TestAvroStorage to verify that proper exceptions are thrown for two cases. My test can be run with the following commands:
{code}
tar -xf avro_test_files.tar.gz
ant clean compile-test piggybank -Dhadoopversion=20
cd contrib/piggybank/java
ant test -Dtestcase=TestAvroStorage
{code}","02/Aug/12 09:53;cheolsoo;Review board:
https://reviews.apache.org/r/6319/",06/Aug/12 06:50;cheolsoo;Fixed typos in comments.,"06/Aug/12 07:46;sms;Patch has been committed. TestDBStorage (Hadoop 20 and Hadoop 23), TestMultiStorage (Hadoop 23) are failing and TestLookupInFiles in Hadoop 23 is erroring out. All of these are unrelated to this patch.

Thanks Cheolsoo!","06/Aug/12 17:23;cheolsoo;@Santhosh,

Thanks for reviewing my patch!

Regarding the other test failures,

1) If I run the following commands, I don't see TestDBStorage failure in Hadoop 20:
{code}
ant clean compile-test jar-withouthadoop -Dhadoopversion=20 // Note jar-withouthadoop
cd contrib/piggybank/java
ant clean test -Dtestcase=TestAvroStorage -Dhadoopversion=20
{code}

2) I believe that TestDBStorage/TestMultiStorage/TestLookupInFiles failures are due to some mini cluster mis-configuration:
{code}
2012-08-06 01:13:52,340 INFO  [AsyncDispatcher event handler] rmapp.RMAppImpl (RMAppImpl.java:transition(533)) - Application application_1344240816627_0001 failed 1 times due to AM Container for appatt     empt_1344240816627_0001_000001 exited with  exitCode: 127 due to:
{code}

While I see them failing on Mac, they run fine on CentOS 6. I am wondering if it would be better to modify these tests so that they will run in local mode instead of mr mode.

Thanks! ","06/Aug/12 23:15;cheolsoo;Actually, I have clean runs on Mac in both hadoop 20 and 23 with following commands:

{code:title=hadoop 20}
ant clean compile-test jar-withouthadoop -Dhadoopversion=20
cd contrib/piggybank/java
ant clean test -Dhadoopversion=20
{code}

{code:title=hadoop 23}
ant clean compile-test jar-withouthadoop -Dhadoopversion=23
cd contrib/piggybank/java
JAVA_HOME=`/usr/libexec/java_home` ant clean test -Dhadoopversion=23
{code}

Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.pig.pigunit.pig.PigServer does not initialize udf.import.list of PigContext,PIG-2832,12599639,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,prkommireddi,johannesch,johannesch,20/Jul/12 16:19,22/Feb/13 04:53,14/Mar/19 03:07,01/Nov/12 01:41,0.10.0,,,,,,,0.11,0.12.0,,,,,,0,,,,,,,,,,,,,"PigServer does not initialize udf.import.list. 

So, if you have a pig script that uses UDFs and want to pass the udf.import.list via a property file you can do so using the -propertyFile command line to pig. But you should also be able to do it using pigunits PigServer class that already has the corresponding contructor, e.g. doing something similar to :

{code}
Properties props = new Properties();
props.load(new FileInputStream(""./testdata/test.properties""));
pig = new PigServer(ExecType.LOCAL, props);
String[] params = {""data_dir=testdata""};
test = new PigTest(""test.pig"", params, pig, cluster);
test.assertSortedOutput(""aggregated"", new File(""./testdata/expected.out""));
{code}

While udf.import.list is defined in test.properties and test.pig uses names of UDFs which should be resolved using that list.

This does not work!

I'd say the org.apache.pig.PigServer class is the problem. It should initialize the import list of the PigContext. 

{code}
if(properties.get(""udf.import.list"") != null) {
    PigContext.initializeImportList((String)properties.get(""udf.import.list""));
}{code}

Right now this is done in org.apache.pig.Main.","pig-0.10.0, Hadoop 2.0.0-cdh4.0.1 on Kubuntu 12.04 64Bit.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Oct/12 08:26;prkommireddi;PIG-2832.patch;https://issues.apache.org/jira/secure/attachment/12550425/PIG-2832.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-23 08:26:27.452,,,no_permission,,,,,,,,,,,,250515,,,,Thu Nov 01 01:41:10 UTC 2012,,,Patch Available,,,,0|i0az1j:,61950,,,,,,,,,,"23/Oct/12 08:26;prkommireddi;Thanks Johannes from reporting this. It seems like there is no reason for this initialization to exist on Main (grunt) or on PigServer. This is more of a PigContext behavior. 

Adding a patch that contains the necessary initialization moved to PigContext and removed from Main. PigServer benefits from this directly.",28/Oct/12 21:15;rohini;+1. Will commit this soon.,01/Nov/12 01:41;rohini;Committed to 0.11 and trunk. Thanks Prashant. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle nulls in DataType.compare,PIG-2828,12599437,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,haitao.yao,haitao.yao,19/Jul/12 10:03,14/Oct/13 16:46,14/Mar/19 03:07,04/Jun/13 07:02,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"While using TOP, and if the DataBag contains null value to compare, it will generate the following exception:

Caused by: java.lang.NullPointerException
	at org.apache.pig.data.DataType.compare(DataType.java:427)
	at org.apache.pig.builtin.TOP$TupleComparator.compare(TOP.java:97)
	at org.apache.pig.builtin.TOP$TupleComparator.compare(TOP.java:1)
	at java.util.PriorityQueue.siftUpUsingComparator(PriorityQueue.java:649)
	at java.util.PriorityQueue.siftUp(PriorityQueue.java:627)
	at java.util.PriorityQueue.offer(PriorityQueue.java:329)
	at java.util.PriorityQueue.add(PriorityQueue.java:306)
	at org.apache.pig.builtin.TOP.updateTop(TOP.java:141)
	at org.apache.pig.builtin.TOP.exec(TOP.java:116)


code: (TOP.java, starts with line 91)
                Object field1 = o1.get(fieldNum);
                Object field2 = o2.get(fieldNum);
                if (!typeFound) {
                    datatype = DataType.findType(field1);
                    typeFound = true;
                }
                return DataType.compare(field1, field2, datatype, datatype);

The reason is that if the typeFound is true , and the dataType is not null, and field1 is null, the script failed.
So we need to judge the field1 whether is null.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Jul/12 10:04;haitao.yao;DataType.patch;https://issues.apache.org/jira/secure/attachment/12537156/DataType.patch,03/Jun/13 19:11;aniket486;PIG-2828-format.patch;https://issues.apache.org/jira/secure/attachment/12585921/PIG-2828-format.patch,03/Jun/13 08:04;aniket486;PIG-2828.patch;https://issues.apache.org/jira/secure/attachment/12585808/PIG-2828.patch,23/Oct/12 02:56;haitao.yao;test.patch;https://issues.apache.org/jira/secure/attachment/12550405/test.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-10-22 20:50:53.678,,,no_permission,,,,,,,,,,,,250436,Reviewed,,,Tue Jun 04 07:00:32 UTC 2013,,,,,,,0|i0ayhz:,61862,,,,,,,,,,"19/Jul/12 10:04;haitao.yao;here's my patch. 
","20/Jul/12 01:37;haitao.yao;I've read the latest code from github, and the code is patched from the github master",22/Oct/12 20:50;jcoveney;It would be nice to add a unit test that isolates this case.,"23/Oct/12 02:56;haitao.yao;unit test  for the DataType
","03/Jun/13 07:59;aniket486;DataType compare api is little broken.
public static int compare(Object o1, Object o2) - uses reflection to infer datatypes of o1 and o2.
public static int compare(Object o1, Object o2, byte dt1, byte dt2) - doesn't use reflection, however callers of this api use reflection and also deal with NULLs. 
Currently, callers of second API handle NULLs somewhat similarly but its not consistent. We can refactor the api to avoid reflection and handle NULLs consistently in a separate jira.
Right now, TOP that uses second api directly fails with NPE if o1 or o2 has null data. We should fix that with NULL < non-NULL semantics. ",03/Jun/13 08:34;julienledem;Sounds good to me.,03/Jun/13 17:00;aniket486;I have created https://issues.apache.org/jira/browse/PIG-3343 to track api refactor.,"03/Jun/13 18:06;cheolsoo;+1 to PIG-2828.patch. Looks good to me.

[~aniket486], can you please replace all the tabs with 4 spaces when committing your patch? Thanks!",04/Jun/13 07:00;aniket486;Committed to trunk. Thanks [~cheolsoo] for the review.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TOP exception bug,PIG-2827,12599419,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,haitao.yao,haitao.yao,19/Jul/12 07:59,22/Feb/13 04:54,14/Mar/19 03:07,19/Jul/12 17:58,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"the current exception will swallow the inner exception's stacktrace like this:

java.lang.RuntimeException: General Exception executing function: java.lang.NullPointerException
	at org.apache.pig.builtin.TOP.exec(TOP.java:133)
	at org.apache.pig.builtin.TOP.exec(TOP.java:1)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:216)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:258)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:316)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:332)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:284)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:290)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:233)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:460)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:428)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:408)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:1)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:650)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:418)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1131)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)

we can not locate the real errors.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Jul/12 08:02;haitao.yao;TOP.patch;https://issues.apache.org/jira/secure/attachment/12537146/TOP.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-19 17:17:07.13,,,no_permission,,,,,,,,,,,,256440,,,,Thu Jul 19 17:17:07 UTC 2012,,,,,,,0|i0h5qn:,98192,,,,,,,,,,19/Jul/12 08:02;haitao.yao;here's my patch.,"19/Jul/12 08:27;haitao.yao;the modified stacktrace should be like this:
java.lang.RuntimeException: General Exception executing function: 
	at org.apache.pig.builtin.TOP.exec(TOP.java:133)
	at org.apache.pig.builtin.TOP.exec(TOP.java:1)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:216)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:258)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:316)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:332)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:284)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:290)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLimit.getNext(POLimit.java:85)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:460)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:428)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:408)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:1)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:650)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:418)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1131)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)
Caused by: java.lang.NullPointerException
	at org.apache.pig.data.DataType.compare(DataType.java:427)
	at org.apache.pig.builtin.TOP$TupleComparator.compare(TOP.java:97)
	at org.apache.pig.builtin.TOP$TupleComparator.compare(TOP.java:1)
	at java.util.PriorityQueue.siftUpUsingComparator(PriorityQueue.java:649)
	at java.util.PriorityQueue.siftUp(PriorityQueue.java:627)
	at java.util.PriorityQueue.offer(PriorityQueue.java:329)
	at java.util.PriorityQueue.add(PriorityQueue.java:306)
	at org.apache.pig.builtin.TOP.updateTop(TOP.java:141)
	at org.apache.pig.builtin.TOP.exec(TOP.java:116)
	... 20 more","19/Jul/12 17:17;jcoveney;+1. We want to get rid of all of these cases where exceptions are swallowed, good catch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StoreFunc signature setting in LogicalPlan broken,PIG-2825,12599340,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,julienledem,jcoveney,jcoveney,18/Jul/12 20:50,22/Feb/13 04:53,14/Mar/19 03:07,19/Jul/12 23:56,0.11,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,There is a bug in LogicalPlanGenerator that is leading to improper signatures being set for storefuncs.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Jul/12 20:51;jcoveney;PIG-2825-0.patch;https://issues.apache.org/jira/secure/attachment/12537067/PIG-2825-0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-18 20:56:53.485,,,no_permission,,,,,,,,,,,,256439,,,,Wed Jul 18 20:56:53 UTC 2012,,,Patch Available,,,,0|i0h5pr:,98188,,,,,,,,,,18/Jul/12 20:56;julienledem;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestPigContext.testImportList() does not pass if another javac in on the PATH,PIG-2823,12599170,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,julienledem,julienledem,julienledem,17/Jul/12 21:31,22/Feb/13 04:53,14/Mar/19 03:07,17/Jul/12 22:58,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"it uses ""javac"" on the PATH.
if this resolves to jdk7 the test will fail to load the resulting class",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Jul/12 21:32;julienledem;PIG-2823.patch;https://issues.apache.org/jira/secure/attachment/12536895/PIG-2823.patch,17/Jul/12 22:19;julienledem;PIG-2823_a.patch;https://issues.apache.org/jira/secure/attachment/12536898/PIG-2823_a.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-07-17 21:38:38.497,,,no_permission,,,,,,,,,,,,256437,,,,Tue Jul 17 22:58:34 UTC 2012,,,,,,,0|i0h5ov:,98184,,,,,,,,,,17/Jul/12 21:32;julienledem;PIG-2823.patch uses the API access to the compiler instead,"17/Jul/12 21:38;jcoveney;Since we use the same class in 2 places, can you pull that class into a common place? Besides that, lgtm. +1 pending that",17/Jul/12 22:19;julienledem;PIG-2823_a.patch addresses comments,17/Jul/12 22:30;jcoveney;+1 :) just make sure TestSchemaTuple still works,17/Jul/12 22:58;julienledem;done,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBaseStorage should load hbase configuration into Job configuration under the covers,PIG-2822,12599150,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,rohini,toffer,toffer,17/Jul/12 18:28,06/Jan/13 23:57,14/Mar/19 03:07,19/Sep/12 17:35,0.10.0,,,,,,,0.10.1,0.11,,,,,,0,,,,,,,,,,,,,"In order to get HBaseStorage to work I have to explicitly load the conf. 


ie

pig --conf $HBASE_HOME/conf hbase.pig

It would be much cleaner if HBaseStorage merged the hbase configuration into the job configuration when setLocation() is called on the FE.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-09-19 17:35:04.585,,,no_permission,,,,,,,,,,,,256436,,,,Wed Sep 19 20:50:00 UTC 2012,,,,,,,0|i0h5of:,98182,,,,,,,,,,"19/Sep/12 17:35;cheolsoo;I have verified that this issue is fixed by PIG-2821, so I am marking this jira as resolved now. Thanks for the fix Rohini!

My test set up is as follows:

Host1:
Pig + NN + JT + standalone ZK + HBase master + HBase RS

Host2:
DN + TT

This ensure that tasks that run on host2 obtain the hbase conf (ZK quorum, etc) only from the job conf shipped by Pig.",19/Sep/12 20:50;rohini;Thanks Cheolsoo. I had missed marking this one resolved when PIG-2821 was committed. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBaseStorage should work with secure hbase,PIG-2821,12599148,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,toffer,toffer,17/Jul/12 18:20,06/Jan/13 23:57,14/Mar/19 03:07,27/Aug/12 20:30,0.10.0,,,,,,,0.10.1,0.11,,,,,,0,,,,,,,,,,,,,HBaseStorage needs to add HBase delegation token to the Job object if hbase security is enabled.,,,,,,,,,,,,,,,,,PIG-2822,,,,,,,,,,,,,,,14/Aug/12 18:32;rohini;PIG-2821-1.patch;https://issues.apache.org/jira/secure/attachment/12540910/PIG-2821-1.patch,13/Aug/12 18:37;rohini;PIG-2821-branch10.patch;https://issues.apache.org/jira/secure/attachment/12540720/PIG-2821-branch10.patch,13/Aug/12 18:37;rohini;PIG-2821-trunk.patch;https://issues.apache.org/jira/secure/attachment/12540721/PIG-2821-trunk.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-08-13 18:37:31.717,,,no_permission,,,,,,,,,,,,242070,Reviewed,,,Mon Aug 27 20:30:25 UTC 2012,,,,,,,0|i02igv:,12593,,,,,,,hbase,,,"13/Aug/12 18:37;rohini;Added code to fetch delegation tokens and put hbase-site.xml as part of jobconf(PIG-2822) if it is in classapth instead of having to specify --conf option. 

Reverted PIG-2578 as because of that setting config or credentials in job was not actually being passed to the actual job launched. The testcase in MultiQueryLocal was also not valid as the condition if (job.getConfiguration().get(key)==null) was always true. It was always using the suffix initialized in the constructor and never from the job. If a StoreFunc adds config to  Job then it should be its responsibility to handle multiple instances of it. HCATALOG-314 handles it for hcatalog. We should not remove the ability for a StoreFunc to set a config on the job.",14/Aug/12 05:39;rohini;Will rework patch to add the hbase-site.xml to UDFContext instead of job conf.,"14/Aug/12 18:32;rohini;Removed reverting of PIG-2578. Will deal with 
issues from PIG-2578 in PIG-2870 or another jira. 

 Changed HBaseStorage to store all hbase properties in UDFContext properties. Credentials was getting added to the Job inadvertently when PigOutputFormat.checkOutputSpecs called setStoreLocation. So there is a way adding credentials works even with PIG-2578 though unexpected. ","21/Aug/12 22:25;daijy;+1, will checkin soon.",27/Aug/12 20:30;daijy;Patch committed to 0.10/trunk. Thanks Rohini!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
relToAbsolutePath is not replayed properly when Grunt reparses the script after PIG-2699,PIG-2820,12599145,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,julienledem,julienledem,julienledem,17/Jul/12 17:52,22/Feb/13 04:53,14/Mar/19 03:07,20/Jul/12 00:11,0.11,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Jul/12 18:06;julienledem;PIG-2820.patch;https://issues.apache.org/jira/secure/attachment/12536858/PIG-2820.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-17 18:21:46.206,,,no_permission,,,,,,,,,,,,256435,,,,Tue Jul 17 18:21:46 UTC 2012,,,Patch Available,,,,0|i0h5nz:,98180,,,,,,,,,,17/Jul/12 18:06;julienledem;PIG-2820.patch fixes this,17/Jul/12 18:21;jcoveney;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
class loader management in PigContext,PIG-2815,12598667,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rangadi,rangadi,rangadi,13/Jul/12 06:41,22/Feb/13 04:53,14/Mar/19 03:07,06/Dec/12 23:31,0.9.0,,,,,,,0.11,,,,impl,,,0,,,,,,,,,,,,,"The way {{PigContext.classloader}} and resolveClassName() are managed can lead to strange class loading issues, especially when not all {{register}} statements are at the top (example in the first comment).

Two factors contribute to this: sometimes only one of them and sometimes together:

 # a new classloader (CL) is created after registering each jar.
    ** but the new jar's parent is the root CL rather than previous CL, effectively throwing previous CL away.
 # resolveClassName() caches classes based on just the name
    ** A class is not defined by name alone. Classes loaded by two different unrelated CLs are different objects even if both extract the class from same physical jar file.
    ** because of (1), the cached class is not necessarily same as the class that would be loaded based on 'current' CL

having different class objects for same class have many subtle side effects. e.g. there would be two instances of static variables. 

I think both should be fixed.. thought fixing one of them might be good enough in many cases. I will add a patch.
       


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Jul/12 17:48;rangadi;PIG-2815-branch-0.9.patch;https://issues.apache.org/jira/secure/attachment/12537029/PIG-2815-branch-0.9.patch,17/Jul/12 21:27;rangadi;PIG-2815-branch-0.9.patch;https://issues.apache.org/jira/secure/attachment/12536893/PIG-2815-branch-0.9.patch,16/Jul/12 07:37;rangadi;PIG-2815.patch;https://issues.apache.org/jira/secure/attachment/12536600/PIG-2815.patch,13/Jul/12 07:18;rangadi;PIG-2815.patch;https://issues.apache.org/jira/secure/attachment/12536343/PIG-2815.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-07-13 20:40:14.1,,,no_permission,,,,,,,,,,,,256431,,,,Thu Dec 06 23:31:05 UTC 2012,,,,,,,0|i0h5lz:,98171,,,,,,,,,,"13/Jul/12 07:08;rangadi;
An example:

{noformat}
register elephant-bird.jar; -- for working with Thrift objects.
-- (1)

register T_One.jar;
-- (2)

-- ThriftPigLoader takes name of a Thrift class that corresponds to input.
a = load '/logs/T_One' using ThriftPigLoader('thrift.gen.T_One');
-- (3)

register second.jar; 
-- (4)

b = load '/logs/T_two' using ThriftPigLoader('thrift.gen.T_two');

-- (5)
-- FAIL!
{noformat}

 * (1): new classlaoder cl_A is created with root classloader as the parent.
 * (2): cl_B is created with root as the parent.
 * (3): {{ThirftPigLoader.class}} is instantiated with cl_B and cached.
 * (4): cl_C is created with root as the parent.
 * (5): {{thrift.gen.T_two.class}} is instantiated with cl_C, but '{{ThriftPigLoader.class}}' from cl_B is reused by Pig. So all the Thrift classes seen by ThriftPigLoader are entirely different from all the Thrift classes seen by {{thrift.gen.T_two}} since cl_B is not a parent of cl_C. That can lead to a number of issues and it does.

","13/Jul/12 07:18;rangadi;changes in the patch :

 # remove 'classCache':
    * may be resolveClassName() could cache the 'prefix' that successfully loaded a class to avoid checking other prefixes. Reloading an existing class is not costly since JVM has a cache anyway.
    * I don't think this is a perf issue either way.
 # use a classloader that supports adding jars.
     * not sure if it is common practice to add resources.. or if there are any side effects.

","13/Jul/12 20:40;jcoveney;Raghu,

This is awesome. Is there any hope that you could add a test that fails in the current setup? I think that would be important as far as conveying a concrete use case when this fails (which your example does, but if we have a test that fails it'll be super super concrete.

Thanks for doing this. Awesome.

Jon",16/Jul/12 07:37;rangadi;unit test : update {{TestRegisteredJarVisibility}} to test for classloader 'consistency'.,"16/Jul/12 08:51;ashutoshc;[~rangadi] Thanks for digging into it. Till now atleast in some cases if I registered the jar and then if I want to use it in Pig client frontend, I also necessarily need to add it in {{PIG_CLASSPATH}}. Does this patch removes that restriction? Put another way, will registering the jar will automatically add the jar in pig's classpath?","16/Jul/12 17:37;rangadi;
Ashutosh, that issue is fixed in PIG-2532. Let us know otherwise.

Still there are some corner cases: e.g. if you use {{ObjectSerializer}} on frontend, that class should be in CLASSPATH (the fix is to extend ObjectSerializer to take a classloader or it should use current threads classloader).

","16/Jul/12 18:11;jcoveney;+1, will commit once test-commit passes",16/Jul/12 18:53;ashutoshc;[~rangadi] I think I have hit on corner cases. Will file jira for it. Do you happen to know any other places where it will manifest ?,16/Jul/12 18:57;jcoveney;Committed. Thanks Raghu! Also: want to make a ticket for that ObjectSerializer CLASSPATH issue you mentioned?,"16/Jul/12 23:00;rangadi;Thanks for the quick review Jon. filed PIG-2819 for ObjectSerializer.

Ashutosh, new jira will be useful. not sure of specific corner cases, but I am sure they are there :).",17/Jul/12 21:27;rangadi;patch for 0.9.,"17/Jul/12 22:09;jcoveney;Raghu,

Thanks for this. Note: in your patch for pig-0.9, you removed a default PigContext constructor. Not sure if you meant to do that on purpose.

{code}
public PigContext() {
   this(ExecType.MAPREDUCE, new Properties());
}
{code}","18/Jul/12 17:48;rangadi;Thanks Jon.

updated the patch. There was another small spurious change.",19/Nov/12 19:46;julienledem;It sounds like this has been committed. Can you resolve it?,"03/Dec/12 04:02;cheolsoo;It seems that the patch is committed to 0.11/trunk but not to 0.9/0.10.

Please let me know if anyone wants this to be committed to 0.9/0.10. Or I will close the jira.

Thanks!","06/Dec/12 22:09;julienledem;0.11/trunk sounds good to me
",06/Dec/12 23:31;cheolsoo;Closed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix issues with Sample operator documentation,PIG-2814,12598657,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,prasanth_j,prasanth_j,prasanth_j,13/Jul/12 02:49,22/Feb/13 04:53,14/Mar/19 03:07,31/Jul/12 03:43,0.10.0,0.7.0,0.8.0,0.8.1,0.9.0,0.9.1,0.9.2,0.11,,,,documentation,,,0,docs,,,,,,,,,,,,Description for SAMPLE operator in documentation is not appropriate.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Jul/12 03:01;prasanth_j;PIG-2814.patch;https://issues.apache.org/jira/secure/attachment/12536328/PIG-2814.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-31 03:42:21.909,,,no_permission,,,,,,,,,,,,256430,,,,Tue Jul 31 03:42:21 UTC 2012,,,,,,,0|i0h5lj:,98169,,,,,,,,,,"13/Jul/12 03:01;prasanth_j;Tested by running ""forrest run"" locally. ",31/Jul/12 03:42;dvryaboy;Looks like the patch was made from the wrong root directory -- the full path to patched file is src/docs/src/documentation/content/xdocs/basic.xml . Committing.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updating .eclipse.templates/.classpath with the Newest Jython Version,PIG-2811,12598337,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,zjshen,zjshen,zjshen,11/Jul/12 09:05,22/Feb/13 04:53,14/Mar/19 03:07,24/Aug/12 00:29,,,,,,,,0.11,,,,tools,,,0,,,,,,,,,,,,,"Jython library version has been upgraded to 2.5.2 by the PIG-2665 patch, but the related modification is not made in the Eclipse template file.",,,,,,,,,,,,,,,,,,,PIG-2665,,,,,,,,,,,,,11/Jul/12 09:08;zjshen;PIG-2811.patch;https://issues.apache.org/jira/secure/attachment/12536001/PIG-2811.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-24 00:29:40.031,,,no_permission,,,,,,,,,,,,256428,,,,Fri Aug 24 00:29:40 UTC 2012,,,,,,,0|i0h5kf:,98164,,,,,,,,,,"24/Aug/12 00:29;thejas;Fixed in the PIG-1314 patch.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestUDFContext broken by PIG-2699,PIG-2809,12598214,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,julienledem,julienledem,julienledem,10/Jul/12 16:35,22/Feb/13 04:53,14/Mar/19 03:07,10/Jul/12 18:14,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Jul/12 16:36;julienledem;PIG-2809.patch;https://issues.apache.org/jira/secure/attachment/12535867/PIG-2809.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-10 18:14:49.093,,,no_permission,,,,,,,,,,,,256426,Reviewed,,,Tue Jul 10 18:14:49 UTC 2012,,,Patch Available,,,,0|i0h5jj:,98160,,,,,,,,,,10/Jul/12 16:36;julienledem;PIG-2809.patch fixes it,10/Jul/12 18:14;daijy;Patch committed to trunk. Thanks Julien!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestParser TestPigStorage TestNewPlanOperatorPlan broken by PIG-2699,PIG-2807,12598128,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,julienledem,julienledem,julienledem,10/Jul/12 04:57,22/Feb/13 04:53,14/Mar/19 03:07,10/Jul/12 07:05,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Jul/12 04:59;julienledem;PIG-2807.patch;https://issues.apache.org/jira/secure/attachment/12535783/PIG-2807.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-10 07:05:43.078,,,no_permission,,,,,,,,,,,,256424,Reviewed,,,Tue Jul 10 07:05:43 UTC 2012,,,Patch Available,,,,0|i0h5in:,98156,,,,,,,,,,10/Jul/12 04:59;julienledem;PIG-2807.patch fixes the tests,10/Jul/12 07:05;daijy;+1. Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix merge join test regression,PIG-2806,12598113,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,jcoveney,jcoveney,10/Jul/12 00:21,22/Feb/13 04:53,14/Mar/19 03:07,12/Jul/12 21:26,0.11,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,SchemaTuple introduced a regression in TestMergeJoin. Fix available shortly,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Jul/12 00:23;jcoveney;PIG-2806-0.patch;https://issues.apache.org/jira/secure/attachment/12535771/PIG-2806-0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-10 00:27:26.763,,,no_permission,,,,,,,,,,,,256423,,,,Tue Jul 10 00:27:26 UTC 2012,,,,,,,0|i0h5i7:,98154,,,,,,,,,,"10/Jul/12 00:23;jcoveney;Passed test-commit, and TestMergeJoin",10/Jul/12 00:27;julienledem;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Remove ""PIG"" exec type ",PIG-2804,12598097,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,dvryaboy,dvryaboy,09/Jul/12 21:37,10/Jul/12 20:33,14/Mar/19 03:07,10/Jul/12 20:33,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"the PIG exectype is sitting around doing nothing. It's said ""not available yet"" for years. We should kill it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Jul/12 19:17;dvryaboy;PIG-2804.patch;https://issues.apache.org/jira/secure/attachment/12535899/PIG-2804.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-10 20:16:37.574,,,no_permission,,,,,,,,,,,,256421,,,,Tue Jul 10 20:16:37 UTC 2012,,,,,,,0|i0h5hb:,98150,,,,,,,,,,"09/Jul/12 21:38;dvryaboy;Also, we should pull public static ExecType parseExecType(String str)  from PigServer to the ExecType enum. For cleanliness.",10/Jul/12 20:16;julienledem;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Pig Work on Windows without Cygwin,PIG-2793,12597859,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,jgordon,jgordon,07/Jul/12 00:17,14/Oct/13 16:46,14/Mar/19 03:07,12/Sep/13 06:14,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"For pig to really work well on Windows, it needs hadoop core changes.  Right now, those are in progress in branch-1-win.  For this work, I am running Pig on Windows against branch-1-win and removing Cygwin dependencies as capabilities open up.  Branch-1-win is fairly stable now, and has opened up enough functionality to see the few things needed in Pig to run E2E on top of a cross-platform Hadoop core without Cygwin.  This uber-JIRA should track the whole of the work to get pig running well on Windows without Cygwin.

There are a few types of work that I think are needed right now (will break-out sub-jiras to track them):

TEST:
--------
1.) Tests that generate pig script strings with paths in them (e.g. dynamically build load/store commands) need to have Pig escape (""\"") characters encoded -- as they can now occur in both Hadoop and local paths.

2.) Tests that generate local temporary files with createTempFile, and then try to use those as HDFS paths need to remove "":"" from the generated file name to create valid HDFS paths.

3.) Tests that hand-generate URIs via string concatenation (e.g. ""file:"" + strFileName) need to use Util.generateURI instead to get a valid URI for the target platform.

4.) Tests that assume the first line in a script (e.g. #!/bin/sh) auto-resolves interpreters need to explicitly call the interpreter (e.g. instead of calling ""perlscript.pl"" they should call ""perl perlscript.pl"".

5.) Changes in quotes or command syntax between shells (e.g. "" or ', dir or ls) need to be tuned a little here and there.

PROD:
--------

1.) The streaming interface needs to be fixed to run without a Cygwin dependency.

2.) The pig.additional.jars separator is currently hardcoded to "":"", and should be File.pathSeparator instead ("":"" on linux, "";"" on Windows) to be able to accept Windows paths (C:\file.jar for instance).

3.) The Grunt ""sh"" command highly surfaces the behavior of the exec API.  If you use a built-in, it fails with file not found.  This surfaces a lot of differences in shell implementation differences (e.g. ls is an exe, but dir is builtin) -- and many of the cases in TestGrunt end up running (sh bash -c ""command"").  For portability and ease of use, sh should actually exec ""sh -c <command> on Linux and ""cmd /C <command>"" on Windows to improve usability and make it possible to use aliases and bat files on either platform to make the interface more platform independent to end-users.

4.) (eventual) Update Pig's dependencies to pick up a stable Hadoop core that runs on Windows from a release branch.","Windows without Cygwin as a whole, but with some key binaries such as perl, diff, gawk, gzip, sed.",,,,,,,,,,,,,,,,,,PIG-243,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-09-12 06:14:44.076,,,no_permission,,,,,,,,,,,,245447,Reviewed,,,Thu Sep 12 06:14:44 UTC 2013,,,,,,,0|i06adr:,34613,,,,,,,,,,07/Jul/12 00:18;jgordon;Initial effort to get Pig running on Windows under Cygwin.,12/Sep/13 06:14;daijy;All underlining Jiras are solved.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wonderdog stopped working in Pig 0.10.0 (worked in 0.9.2),PIG-2792,12597858,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,,russell.jurney,russell.jurney,07/Jul/12 00:06,06/Jan/13 23:57,14/Mar/19 03:07,07/Jul/12 01:02,0.10.0,0.10.1,0.11,,,,,0.10.1,,,,piggybank,,,0,a,about,area,book,did,i,moving,of,omg,technology,why,write,"The Pig UDFs in Wonderdog for ElasticSearch integration, which worked in 0.9.2 stopped working in 0.10.0.

Now in 0.10.0 there is an error, as Wonderdog is unable to read its configuration from the hadoop cache.

If someone can help identify what the issue is, or advise how Wonderdog or Pig can be modified so that wonderdog works with with Pig 0.10, it would be greatly appreciated.

This issue is duped in the Wonderdog project here: https://github.com/infochimps-labs/wonderdog/issues/6 https://github.com/infochimps-labs/wonderdog/issues/5 and https://github.com/infochimps-labs/wonderdog/issues/7

The error is below:

2012-07-06 16:50:51,501 [main] INFO  org.apache.pig.Main - Apache Pig version 0.10.0-SNAPSHOT (rexported) compiled Jun 22 2012, 15:56:16
2012-07-06 16:50:51,502 [main] INFO  org.apache.pig.Main - Logging error messages to: /private/tmp/pig_1341618651472.log
2012-07-06 16:50:51,829 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: file:///
{""ok"":true}
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100    11  100    11    0     0    647      0 --:--:-- --:--:-- --:--:--   733
2012-07-06 16:50:53,206 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: UNKNOWN
2012-07-06 16:50:53,379 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false
2012-07-06 16:50:53,403 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 1
2012-07-06 16:50:53,403 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 1
2012-07-06 16:50:53,441 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added to the job
2012-07-06 16:50:53,449 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2012-07-06 16:50:53,494 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job
2012-07-06 16:50:53,560 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.
2012-07-06 16:50:53,587 [Thread-7] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2012-07-06 16:50:53,597 [Thread-7] WARN  org.apache.hadoop.mapred.JobClient - No job jar file set.  User classes may not be found. See JobConf(Class) or JobConf#setJar(String).
****file:/tmp/emails.json
2012-07-06 16:50:53,711 [Thread-7] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 2
2012-07-06 16:50:53,711 [Thread-7] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 2
2012-07-06 16:50:53,734 [Thread-7] WARN  org.apache.hadoop.io.compress.snappy.LoadSnappy - Snappy native library not loaded
2012-07-06 16:50:53,737 [Thread-7] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 3
2012-07-06 16:50:54,008 [Thread-8] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorPlugin : null
2012-07-06 16:50:54,023 [Thread-8] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/tmp/emails.json/part-m-00000:0+33554432
2012-07-06 16:50:54,029 [Thread-8] INFO  com.infochimps.elasticsearch.ElasticSearchOutputFormat - Using field:[message_id] for document ids
2012-07-06 16:50:54,029 [Thread-8] INFO  com.infochimps.elasticsearch.ElasticSearchOutputFormat - Using [null] as es.config
2012-07-06 16:50:54,029 [Thread-8] INFO  com.infochimps.elasticsearch.ElasticSearchOutputFormat - Using [null] as es.plugins.dir
2012-07-06 16:50:54,033 [Thread-8] WARN  org.apache.hadoop.mapred.FileOutputCommitter - Output path is null in cleanup
2012-07-06 16:50:54,034 [Thread-8] WARN  org.apache.hadoop.mapred.LocalJobRunner - job_local_0001
java.lang.RuntimeException: java.lang.NullPointerException
	at com.infochimps.elasticsearch.ElasticSearchOutputFormat$ElasticSearchRecordWriter.<init>(ElasticSearchOutputFormat.java:133)
	at com.infochimps.elasticsearch.ElasticSearchOutputFormat.getRecordWriter(ElasticSearchOutputFormat.java:262)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.getRecordWriter(PigOutputFormat.java:84)
	at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.<init>(MapTask.java:628)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:753)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)
Caused by: java.lang.NullPointerException
	at java.util.Hashtable.put(Hashtable.java:394)
	at java.util.Properties.setProperty(Properties.java:143)
	at java.lang.System.setProperty(System.java:746)
	at com.infochimps.elasticsearch.ElasticSearchOutputFormat$ElasticSearchRecordWriter.<init>(ElasticSearchOutputFormat.java:130)
	... 6 more
2012-07-06 16:50:54,506 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_local_0001
2012-07-06 16:50:54,506 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete
2012-07-06 16:50:59,022 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - job job_local_0001 has failed! Stop running all dependent jobs
2012-07-06 16:50:59,023 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete
2012-07-06 16:50:59,024 [main] ERROR org.apache.pig.tools.pigstats.PigStatsUtil - 1 map reduce job(s) failed!
2012-07-06 16:50:59,024 [main] INFO  org.apache.pig.tools.pigstats.SimplePigStats - Detected Local mode. Stats reported below may be incomplete
2012-07-06 16:50:59,025 [main] INFO  org.apache.pig.tools.pigstats.SimplePigStats - Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
1.0.2	0.10.0-SNAPSHOT	rjurney	2012-07-06 16:50:53	2012-07-06 16:50:59	UNKNOWN

Failed!

Failed Jobs:
JobId	Alias	Feature	Message	Outputs
job_local_0001	json_emails	MAP_ONLY	Message: Job failed! Error - NA	es://email/email?id=message_id&json=true&size=1000,

Input(s):
Failed to read data from ""/tmp/emails.json""

Output(s):
Failed to produce result in ""es://email/email?id=message_id&json=true&size=1000""

Job DAG:
job_local_0001


2012-07-06 16:50:59,025 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Failed!
2012-07-06 16:50:59,029 [main] ERROR org.apache.pig.tools.grunt.GruntParser - ERROR 2244: Job failed, hadoop does not return any error message
2012-07-06 16:50:59,029 [main] ERROR org.apache.pig.tools.grunt.GruntParser - org.apache.pig.backend.executionengine.ExecException: ERROR 2244: Job failed, hadoop does not return any error message
	at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:140)
	at org.apache.pig.tools.grunt.GruntParser.processShCommand(GruntParser.java:1025)
	at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:167)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:189)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:165)
	at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:84)
	at org.apache.pig.Main.run(Main.java:555)
	at org.apache.pig.Main.main(Main.java:111)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)

Details also at logfile: /private/tmp/pig_1341618651472.log
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

{
  ""took"" : 75,
  ""timed_out"" : false,
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0  ""_shards"" : {
    ""total"" : 5,
    ""successful"" : 5,
    ""failed"" : 0
  },
  ""hits"" : {
    ""total"" : 0,
    ""max_score"" : null,
    ""hits"" : [ ]
  }
}

100   193  100   193    0     0   2475      0 --:--:-- --:--:-- --:--:--  2539
2012-07-06 16:50:59,140 [main] ERROR org.apache.pig.tools.grunt.GruntParser - ERROR 2244: Job failed, hadoop does not return any error message
2012-07-06 16:50:59,140 [main] ERROR org.apache.pig.tools.grunt.GruntParser - org.apache.pig.backend.executionengine.ExecException: ERROR 2244: Job failed, hadoop does not return any error message
	at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:140)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:193)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:165)
	at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:84)
	at org.apache.pig.Main.run(Main.java:555)
	at org.apache.pig.Main.main(Main.java:111)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)",Pig with Wonderdog https://github.com/infochimps-labs/wonderdog for elasticsearch integration. Elasticsearch 0.18.6. Pig local mode.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,256419,,,,Sat Jul 07 01:02:59 UTC 2012,,,,,,,0|i0h5e7:,98136,,,,,,,,,,"07/Jul/12 00:22;russell.jurney;The properties that need to be set in the Hadoop configuration object are:

           Instantiates a new RecordWriter for Elasticsearch
           <p>
           The properties that <b>MUST</b> be set in the hadoop Configuration object
           are as follows:
           <ul>
           <li><b>elasticsearch.index.name</b> - The name of the elasticsearch index data will be written to. It does not have to exist ahead of time</li>
           <li><b>elasticsearch.bulk.size</b> - The number of records to be accumulated into a bulk request before writing to elasticsearch.</li>
           <li><b>elasticsearch.is_json</b> - A boolean indicating whether the records to be indexed are json records. If false the records are assumed to be tsv, in which case <b>elasticsearch.field.names</b> must be set and contain a comma separated list of field names</li>
           <li><b>elasticsearch.object.type</b> - The type of objects being indexed</li>
           <li><b>elasticsearch.config</b> - The full path the elasticsearch.yml. It is a local path and must exist on all machines in the hadoop cluster.</li>
           <li><b>elasticsearch.plugins.dir</b> - The full path the elasticsearch plugins directory. It is a local path and must exist on all machines in the hadoop cluster.</li>
           </ul>
           <p>
           The following fields depend on whether <b>elasticsearch.is_json</b> is true or false.
           <ul>
           <li><b>elasticsearch.id.field.name</b> - When <b>elasticsearch.is_json</b> is true, this is the name of a field in the json document that contains the document's id. If -1 is used then the document is assumed to have no id and one is assigned to it by elasticsearch.</li>
           <li><b>elasticsearch.field.names</b> - When <b>elasticsearch.is_json</b> is false, this is a comma separated list of field names.</li>
           <li><b>elasticsearch.id.field</b> - When <b>elasticsearch.is_json</b> is false, this is the numeric index of the field to use as the document id. If -1 is used the document is assumed to have no id and one is assigned to it by elasticsearch.</li>
           </ul>       
","07/Jul/12 01:02;russell.jurney;Fixed here: https://github.com/infochimps-labs/wonderdog/pull/8

Pull in the conf object if the hadoop cache call misses.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
After Pig-2699 the script schema (LOAD ... USING ... AS {script schema}) is passed after getSchema is called ,PIG-2790,12597826,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,julienledem,julienledem,julienledem,06/Jul/12 20:34,22/Feb/13 04:53,14/Mar/19 03:07,07/Jul/12 00:41,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Jul/12 22:17;julienledem;PIG-2790.patch;https://issues.apache.org/jira/secure/attachment/12535442/PIG-2790.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-07 00:17:14.969,,,no_permission,,,,,,,,,,,,256417,Reviewed,,,Sat Jul 07 00:41:24 UTC 2012,,,Patch Available,,,,0|i0h5db:,98132,,,,,,,,,,06/Jul/12 22:17;julienledem;PIG-2790.patch fixes the problem,"07/Jul/12 00:17;daijy;I see you add TestLOLoadDeterminedSchema to commit-tests, is that intentional?","07/Jul/12 00:35;julienledem;It seems to be a short test, so I added it. I can remove it. How do we decide what to put in commit-tests?","07/Jul/12 00:40;daijy;We just want commit-tests to be short. I am fine if you want to put it in, just check if you realize it.",07/Jul/12 00:41;daijy;Patch committed to trunk. Thanks Julien!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
improved string interpolation of variables,PIG-2788,12597378,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,jmhodges,jmhodges,04/Jul/12 00:50,14/Oct/13 16:46,14/Mar/19 03:07,17/Jan/13 00:36,0.10.0,0.9.2,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"The simplest example of the failure of the current string interpolation is 

{code}
store my_rel into '$OUTPUT_';
{code}

This will raise an error saying that OUTPUT_ is not a variable passed in. Similar errors happen with a variety of other trailing characters.

It would be nice if '${OUTPUT}_', or something similar, worked.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Dec/12 05:25;jcoveney;PIG-2788-0.patch;https://issues.apache.org/jira/secure/attachment/12561431/PIG-2788-0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-18 05:25:48.198,,,no_permission,,,,,,,,,,,,250435,,,,Thu Jan 17 00:36:27 UTC 2013,,,,,,,0|i0ayhr:,61861,,,,,,,,,,"18/Dec/12 05:25;jcoveney;I bet nobody thought this would ever get some love :) but this has something that has long annoyed me, and I wanted to familiarize myself a bit more with that path of the code.

This has the syntax Jeff proposed. Nothing has changed, except now you can optionally do ${stuff} to ally ambiguity, thus allow ${tmp}_ and other such things.

It's a pretty easy change, too.",16/Jan/13 16:31;alangates;I'm reviewing this patch.,"16/Jan/13 21:15;alangates;+1, lgtm",17/Jan/13 00:36;jcoveney;Thanks alan! It's in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
change the module name in ivy to lowercase to match the maven repo,PIG-2787,12597377,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,julienledem,julienledem,julienledem,04/Jul/12 00:41,22/Feb/13 04:53,14/Mar/19 03:07,04/Jul/12 01:03,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"ivy.xml
{noformat}
 <ivy-module version=""2.0"" xmlns:m=""http://ant.apache.org/ivy/maven""
     xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
     xsi:noNamespaceSchemaLocation=""http://ant.apache.org/ivy/schemas/ivy.xsd"">
-  <info organisation=""org.apache.pig"" module=""${ant.project.name}"" revision=""${version}"">
+  <info organisation=""org.apache.pig"" module=""${name}"" revision=""${version}"">
     <license name=""Apache 2.0""/>
     <ivyauthor name=""Apache Hadoop Team"" url=""http://hadoop.apache.org/pig""/>
     <description>Pig</description>
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Jul/12 01:00;julienledem;PIG-2787.patch;https://issues.apache.org/jira/secure/attachment/12535028/PIG-2787.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-04 00:49:25.662,,,no_permission,,,,,,,,,,,,256415,,,,Wed Jul 04 00:49:25 UTC 2012,,,,,,,0|i0h5c7:,98127,,,,,,,,,,04/Jul/12 00:49;jcoveney;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoClassDefFoundError after upgrading to pig 0.10.0 from 0.9.0,PIG-2785,12596937,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,matterhayes,matterhayes,matterhayes,03/Jul/12 06:38,22/Feb/13 04:53,14/Mar/19 03:07,22/Aug/12 01:48,0.10.0,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"It appears that the versions in the pom for pig 0.10.0 are inconsistent with the versions specified in the ivy file used to build pig.  I am building a separate project, and I am getting pig and its dependencies using ivy.  

Looking in ivy.xml in the pig 0.10.0 release:

    <dependency org=""org.apache.avro"" name=""avro"" rev=""${avro.version}""
      conf=""compile->default;checkstyle->master""/>

...

    <dependency org=""org.codehaus.jackson"" name=""jackson-mapper-asl"" rev=""${jackson.version}""
      conf=""compile->master""/>
    <dependency org=""org.codehaus.jackson"" name=""jackson-core-asl"" rev=""${jackson.version}""
      conf=""compile->master""/>

Where avro.version is avro.version=1.5.3 and jackson.version=1.7.3.

However, in the pom.xml for pig 0.10.0:

      <groupId>org.apache.hadoop</groupId>
      <artifactId>avro</artifactId>
      <version>1.3.2</version>

And when I look up the pom for org.apache.hadoop's avro 1.3.2 in the central repository, I see a version of jackson inconsistent with what pig was compiled with:

    <dependency>
      <groupId>org.codehaus.jackson</groupId>
      <artifactId>jackson-mapper-asl</artifactId>
      <version>1.4.2</version>
      <scope>compile</scope>
    </dependency>

It's 1.4.2, not 1.7.3. 

Below is my ivy.xml.  It's the same as what I used for 0.9.0 but I changed the pig version to 0.10.0.

<ivy-module version=""2.0"">
    <info organisation=""datafu"" module=""datafu""/>
    <dependencies>
        <dependency org=""org.apache.pig"" name=""pig"" rev=""0.10.0""/>
        <dependency org=""it.unimi.dsi"" name=""fastutil"" rev=""6.3""/>
        <dependency org=""joda-time"" name=""joda-time"" rev=""1.6""/>
        <dependency org=""org.apache.commons"" name=""commons-math"" rev=""2.1""/>
        <dependency org=""commons-io"" name=""commons-io"" rev=""1.4""/>
        <dependency org=""org.apache.hadoop"" name=""hadoop-core"" rev=""0.20.2""/>
        <dependency org=""org.testng"" name=""testng"" rev=""6.2""/>
        <dependency org=""com.google.guava"" name=""guava"" rev=""r06"" />
          
        <!-- needed for pigunit to work -->
        <dependency org=""log4j"" name=""log4j"" rev=""1.2.14"" />
        <dependency org=""jline"" name=""jline"" rev=""0.9.94"" />
        <dependency org=""org.antlr"" name=""antlr"" rev=""3.2"" />
    </dependencies>
</ivy-module>",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,31/Jul/12 04:37;matterhayes;avro_version_fix.diff;https://issues.apache.org/jira/secure/attachment/12538497/avro_version_fix.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-06 20:25:52.009,,,no_permission,,,,,,,,,,,,242336,,,,Wed Aug 22 01:48:21 UTC 2012,,,,,,,0|i02tuv:,14438,,,,,,,,,,"06/Jul/12 20:25;daijy;Thanks, Matthew. I can see the problem. You are welcome to submit a patch.",31/Jul/12 04:37;matterhayes;Fixed the avro version.  I tested it using the DataFu project.  When built against this version of the POM all DataFu unit tests pass.,22/Aug/12 01:45;sms;+1 Commit tests passed. I will commit it shortly,22/Aug/12 01:47;sms;Committed. Thanks Matthew.,22/Aug/12 01:48;sms;Please assign the ticket to Matthew when the release is cut. I could not find him in the assignee list.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Iterator_1 e2e test for Hadoop 23,PIG-2783,12596668,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,rohini,rohini,rohini,02/Jul/12 16:26,06/Jan/13 23:57,14/Mar/19 03:07,09/Jul/12 18:50,0.10.0,,,,,,,0.10.1,0.11,,,e2e harness,,,1,,,,,,,,,,,,,"Iterator_1 e2e test is failing because the script tries to copy output to a directory that does not exist. From Hadoop 23, the copy commands do not auto-create non-existent paths anymore.

+Pig.fs(""-mkdir :OUTPATH:"")
 Pig.fs(""-copyFromLocal :TMP:/iterator_output.txt :OUTPATH:/part-m-00000"")",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Jul/12 16:32;rohini;PIG-2783.patch;https://issues.apache.org/jira/secure/attachment/12534286/PIG-2783.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-09 18:50:49.573,,,no_permission,,,,,,,,,,,,256413,Reviewed,,,Mon Jul 09 18:50:49 UTC 2012,,,,,,,0|i0h5av:,98121,,,,,,,,,,"02/Jul/12 16:32;rohini;Comments from the hadoop team on Hadoop 23 copy command behaviour:

Many commands used to auto-create non-existent paths. This behavior is most commonly relied upon in the copy commands. The target directory path will no longer be created. Please use ""mkdir"" or ""mkdir -p"" to create the target directory path prior to the copy. The ""-p"" flag will avoid an error if the directory already exists. If other commands are still auto-creating paths, do not rely upon that behavior. ",09/Jul/12 18:50;daijy;Patch committed to 0.10/trunk. Thanks Rohini!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Specifying sorting field(s) at nightly.conf,PIG-2782,12596654,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,xalan,xalan,02/Jul/12 13:51,22/Feb/13 04:53,14/Mar/19 03:07,09/Jul/12 20:28,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"After running the Checkin tests, it fails because one of the parameters passed to the sort is incorrect (instead of +1 -2, on POSIX is -k2,2). 

According to this http://ss64.com/bash/sort.html, it was on an old notation.","Mac OS X Lion 10.7.3
Hadoop 1.0.1-SNAPSHOT
Apache Pig version 0.11.0-SNAPSHOT (r1355798)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Jul/12 05:34;cheolsoo;PIG-2782.patch;https://issues.apache.org/jira/secure/attachment/12535041/PIG-2782.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-02 16:55:03.493,,,no_permission,,,,,,,,,,,,242338,Reviewed,,,Mon Nov 12 23:55:28 UTC 2012,,,,,,,0|i02tvj:,14441,,,,,,,,,,02/Jul/12 16:55;jay23jack;＋1. I also had the same issue.,"04/Jul/12 05:34;cheolsoo;Attached is the patch that replaces obsolete sort options with posix options in e2e tests.

Modified tests are as follows:

Checkin, Foreach, Order, Types, Limit, Split, MissingColumns, Jython_Checkin, and BigData

I verified that all tests pass in local mode.",09/Jul/12 20:28;daijy;Patch committed to trunk. Thanks Cheolsoo!,"12/Nov/12 23:55;esoren;There were still problems with the patch here. E.g. the pig script sorts on column one and two, but the verification only checks that output is sorted on column one.
For details please see the cloned PIG-3045.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LOSort isEqual method,PIG-2781,12596645,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,xalan,xalan,xalan,02/Jul/12 13:18,22/Feb/13 04:53,14/Mar/19 03:07,02/Sep/12 22:53,,,,,,,,0.11,,,,build,,,0,,,,,,,,,,,,,"While I was checking the logical operators, I found that LOSort does this while comparing two logical operators:

if (mSortFunc.equals(otherSort.getUserFunc()))
    return false;
...
if (mSortColPlans.equals(otherSort.getSortColPlans()))
    return false;
        
If UDF sorting functions are equal, then it returns false. Also, in the case of same sorting columns plans.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Jul/12 20:03;daijy;PIG-2781-1.patch;https://issues.apache.org/jira/secure/attachment/12535413/PIG-2781-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-06 20:03:25.484,,,no_permission,,,,,,,,,,,,256412,,,,Sun Sep 02 22:53:41 UTC 2012,,,,,,,0|i0h5a7:,98118,,,,,,,,,,06/Jul/12 20:03;daijy;That's an obvious bug. Good catch.,"02/Sep/12 22:53;dvryaboy;Committed to trunk.
Thanks for pointing this out, Allan.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MapReduceLauncher should break early when one of the jobs throws an exception,PIG-2780,12596434,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jay23jack,pengfeng,pengfeng,29/Jun/12 18:48,22/Feb/13 04:53,14/Mar/19 03:07,12/Jul/12 07:36,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"Right now MapReduceLauncher caches the job exception in jobControlException and only processes it when all the jobs are done:

{noformat}
  jcThread.setUncaughtExceptionHandler(jctExceptionHandler);
  ...
  jcThread.start();
  // Now wait, till we are finished.
  while(!jc.allFinished()){
  ...
  }
  //check for the jobControlException first
  //if the job controller fails before launching the jobs then there are
  //no jobs to check for failure
  if (jobControlException != null) {
    ...
  }
{noformat}

There are two problems with this approach:
1. There is only one jobControlException variable. If two jobs are throwing exceptions, the first one will be lost.
2. If there are multiple jobs, the exceptions will not be reported until other jobs are finished, which is a waste of system resource.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Jul/12 00:13;jay23jack;PIG-2780.0.patch;https://issues.apache.org/jira/secure/attachment/12535480/PIG-2780.0.patch,11/Jul/12 21:57;jay23jack;PIG-2780.1.patch;https://issues.apache.org/jira/secure/attachment/12536126/PIG-2780.1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-06-29 21:04:23.962,,,no_permission,,,,,,,,,,,,256411,Reviewed,,,Thu Jul 12 07:36:07 UTC 2012,,,,,,,0|i0h59r:,98116,,,,,,,,,,"29/Jun/12 21:04;jay23jack;The jobControlException is different from the job exception. Pig submit jobs in batches and check exceptions at the end of each batch. Jobs without inter-dependency can run in parallel in the same batch, otherwise they are in different batches. Usually the number of jobs in one batch is very small, so checking exception at the end looks acceptable.

If you specify ""-stop_on_failure"" in the command line option, then Pig will stop after the failed batch. 

If it doesn't satisfy your need, i.e. you also want Pig to stop on failure within batch, then please keep this jira open.","29/Jun/12 22:48;pengfeng;We have an example where Pig tries to submit 4 jobs simultaneously but only reports the exception after all other three jobs are finished (after 6 hours). Neither problem would cause errors in the results, but fixing them would help debugging errors in large complex jobs.  ","07/Jul/12 00:13;jay23jack;Attached a patch that checks frequently whether any job has failed, and if so stops immediately. 

Also includes a unit test where Pig submits three jobs at the same time and one of them will fail. With this patch, Pig will stop without finishing the other two jobs.",11/Jul/12 21:57;jay23jack;Update the patch to log a early warning for the failure when stop_on_failure is not enabled.,12/Jul/12 07:36;daijy;Patch committed to trunk. Thanks Jie!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactoring the code for setting number of reducers,PIG-2779,12596346,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jay23jack,jay23jack,jay23jack,29/Jun/12 02:03,22/Feb/13 04:53,14/Mar/19 03:07,31/Jul/12 17:36,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"As PIG-2652 observed, currently the code for setting number of reducers is a little messy. MapReduceOper.requestedParallelism seems being misused in some plases, and now we support runtime estimation of #reducer which further complicates the problem.

For example, if we specify parallel 1 for the order-by, the estimated #reducer will be used. If we specify parallel 2 while it estimates 4, order-by will fail due to ""Illegal partition for Null"". If we specify parallel 4 while it estimates 2, then some reducers will have nothing to do. ",,,,,,,,,,,PIG-2772,PIG-483,,,,,,,,,,,,,,,,,,,,03/Jul/12 20:35;jay23jack;PIG-2779.0.patch;https://issues.apache.org/jira/secure/attachment/12534972/PIG-2779.0.patch,16/Jul/12 22:41;jay23jack;PIG-2779.1.patch;https://issues.apache.org/jira/secure/attachment/12536740/PIG-2779.1.patch,17/Jul/12 06:31;jay23jack;PIG-2779.2.patch;https://issues.apache.org/jira/secure/attachment/12536780/PIG-2779.2.patch,26/Jul/12 18:38;jay23jack;PIG-2779.3.patch;https://issues.apache.org/jira/secure/attachment/12538041/PIG-2779.3.patch,28/Jul/12 05:22;jay23jack;PIG-2779.4.patch;https://issues.apache.org/jira/secure/attachment/12538230/PIG-2779.4.patch,31/Jul/12 00:24;billgraham;PIG-2779.5.patch;https://issues.apache.org/jira/secure/attachment/12538462/PIG-2779.5.patch,31/Jul/12 17:35;billgraham;PIG-2779.6.patch;https://issues.apache.org/jira/secure/attachment/12538578/PIG-2779.6.patch,02/Jul/12 17:31;jay23jack;TestNumberOfReducers.java;https://issues.apache.org/jira/secure/attachment/12534295/TestNumberOfReducers.java,01/Jul/12 18:38;jay23jack;TestNumberOfReducers.java;https://issues.apache.org/jira/secure/attachment/12534168/TestNumberOfReducers.java,,,9.0,,,,,,,,,,,,,,,,,,,2012-06-30 16:50:26.036,,,no_permission,,,,,,,,,,,,256410,,,,Tue Jul 31 17:36:26 UTC 2012,,,,,,,0|i0h59b:,98114,,,,,,,,,,"29/Jun/12 17:48;jay23jack;For the order-by, we need to pass its *final* #reducer (not the estimated one) to the sample job to generate the partition file, otherwise the partition file will be inconsistent and cause errors.

The final #reducer is calculated based on the requested one and the estimated one, the latter of which is calculated based on the input data size. Luckily the sample job has the same input data with the order-by, thus it can calculate in advance the final #reducer of the order-by.",30/Jun/12 16:50;billgraham;Jie do you have a sample script with dummy input that illustrates this problem that you can attach?,"01/Jul/12 18:38;jay23jack;Attached a unit test file. 

In testEstimate2Parallel1 Pig uses wrong #reducers for the order-by.

In testEstimate2Default1 and testEstimate6Default2 queries fail due to inconsistent #reducers used by the sample and the order-by.

In testEstimate2Parallel4, the test passes but actually two reducers have nothing to do.  ","02/Jul/12 17:31;jay23jack;Update the unit test file as somehow we can't set ""order A by x parallel -1"". (is that useful feature?)

Now testEstimate2Default1 produces wrong #reducers instead of failure.","02/Jul/12 18:47;jay23jack;Maybe we can refactor like this: 

*Merge default parallel* 
First, let's merge PigContext.default_parallel into LogicalRelationalOperator's requestedParallelism after the logical plan is built, so we don't need to worry about the default parallel later in Physical phase and MR phase.

*Assignable only once*
Then, let's make all requestedParallelism fields in Logical/Physical/MR operators *assignable only once*, and add other fields if necessary (e.g. runtimeParallel introduced below), as we don't want to mess different semantics together. If we detect it's being reassigned, we can throw a runtime exception.

*Synchronize sample's #partitions and order-by's #reducers*
For the order-by, it's a little tricky: the sample job's requestedParallelism is set to 1 as it only needs one reducer, and it also needs the #partitions (which is the runtime #reducers of the order-by) to generate the partition file. 

Right now we set sample's #partitions twice (once in compilation-time and once in runtime), and we want to set only once at runtime. 

In order to prevent the sample job using a different #partition than the order-by's runtime #reducer, we can introduce another field *runtimeParallel*. The sample job will estimate and calculate #partitions and set it to the order-by's runtimeParallel, which won't be re-calculated later. 

Any comment is appreciated.","03/Jul/12 00:42;jay23jack;This would be better solved with PIG-2784, but maybe we also want a quick fix just in case?","03/Jul/12 20:35;jay23jack;Attached a patch that fixed the failed cases. The idea is that the sample job shouldn't determine the order-by's parallelism at compile-time, because we'll estimate and adjust it at run-time. 

A more elegant refactoring may be possible after PIG-2784. 

Note this patch is necessary for us to remove the sample job at runtime.","09/Jul/12 05:52;billgraham;+1 for the suggested refactor, as it would be good to clear up distinctions between default parallel, requested parallelism and runtime parallelism.

Regarding the patch, you're changing logic that was added in the particularly nasty PIG-2652. Do all the tests in the comments and final patch from PIG-2652 pass? ",09/Jul/12 17:16;jay23jack;Thanks Bill! Yeah this patch would break some tests and I'm fixing them now:),"16/Jul/12 18:41;jay23jack;Currently Pig doesn't look at mapred.reduce.tasks when deciding #reducer. Shall we look at it after request_parallel and default_parallel, and before estimating parallel? It'll look like:
{code}
        if (mro.requestedParallelism > 0) {
            jobParallelism = mro.requestedParallelism;
        } else if (pigContext.defaultParallel > 0) {
            jobParallelism = pigContext.defaultParallel;
        } else if (conf.getInt(""mapred.reduce.tasks"", -1) > 0) {
            jobParallelism = conf.getInt(""mapred.reduce.tasks"", -1);
        } else {
            jobParallelism = estimatedParallelism;
        }  
{code}     ","16/Jul/12 22:41;jay23jack;The latest PIG-2779.1.patch introduces the notion of runtimeParallelism, which is set to the first positive number of parallel, default_parallel and estimated parallel.

For sampler jobs, we used to set #partitions at compile-time and reset it at runtime; this patch will remove the compile-time setting and only keep the runtime setting. 

For the runtime setting of #partitions, we used to estimate based on the sampler's input; this patch will instead estimate based on the next job's input, as for skew-join they are different. 

For sampler's next job, e.g. order-by and skew join, we used to calculate their #reducers independently from the sampler; this patch will instead calculate them together with the sampler, so we can keep sampler's #partitions and the next job's #reducers synchronized.
","17/Jul/12 05:34;billgraham;Jie, these changes make sense to me, assuming tests pass. Just a few questions and minor nits:

{{MRCompiler}}
1. You removed some logic which seems to now be in JCC. What about the removed call to {{eng.getJobConf().getNumReduceTasks()}}? Is that still being picked up elsewhere? It appears like that check was dropped.
2. {{HExecutionEngine}} inport no longer needed.

{{JobControlCompiler}}
3. Should {{ParallelConstantVisitor}} be called with {{mro.reducePlan}} or {{nextMro.reducePlan}}?
4. {{calculateRuntimeReducers}} doesn't need {{MROperPlan plan}} in it's signature.
5. Line 804, if statements should have curly braces.
6. Line 819, should have space between {{else}} and trailing curly bracket.

{{test/smoke-tests}}
7. Did you mean to include this in the patch?","17/Jul/12 06:31;jay23jack;Thanks Bill for the review! Attached PIG-2779.2.patch with fixes.

{quote}
MRCompiler
1. You removed some logic which seems to now be in JCC. What about the removed call to eng.getJobConf().getNumReduceTasks()? Is that still being picked up elsewhere?

It appears like that check was dropped.
{quote}

Yeah I purposely leave out the parameter ""mapred.reduce.tasks"", as the runtime adjustment used to ignore it. 

I thought about including it as an option for setting #reducers, but the problem is its default value is 1 instead of -1, so we can't distinguish whether users set it to 1 or not. (this was one of bugs before). Hive supports this parameter by making it default to -1 and let users override it. We can also add this support if we want.

{quote} 
2. HExecutionEngine inport no longer needed.
{quote}

Nice catch. Removed.

{quote}
JobControlCompiler
3. Should ParallelConstantVisitor be called with mro.reducePlan or nextMro.reducePlan?
{quote}

Should be mro, the sample job.

{quote}
4. calculateRuntimeReducers doesn't need MROperPlan plan in it's signature.
{quote}

Sure. Removed.

{quote}
5. Line 804, if statements should have curly braces.
6. Line 819, should have space between else and trailing curly bracket.
{quote}

Nice catch. Btw do you use any syntax checking tool?

{quote}
test/smoke-tests
7. Did you mean to include this in the patch?
{quote}

Sorry for that. Removed.
","25/Jul/12 04:55;billgraham;Jie, as part of this clean up it would be really useful to record the various parallelism values in the job conf for later analysis. I was thinking we capture defaultParallel, requestedParallelism and runtimeParallelism (which should == {{mapred.reduce.tasks}}, right). That way we can see later which values were set and which was used. It would be great to know whether parallelism was determined by a {{PARALLEL}} statement, via estimation, or via a default. This would be in addition to the following related params we currently capture:

{noformat}
pig.exec.reducers.max
pig.exec.reducers.bytes.per.reducer
{noformat}

Do you want to add this to this issue or do you think we should we do this in a separate JIRA? 

I use IntelliJ and I've just set the syntax to match Apaches.",25/Jul/12 17:56;jay23jack;Great point Bill! Yeah I'll set defaultParallel and requestedParallelism into the jobconf in this patch. What could be good names for them?,"25/Jul/12 22:20;billgraham;Great! We should probably set {{estimatedParallelism}} too, in case that's what was used. If default parallel is used, it seems like that should show up already as {{default_parallel}} actually, so we can omit that one. How about this:

{noformat}
pig.info.reducers.requested.parallel
pig.info.reducers.estimated.parallel
pig.info.reducers.runtime.parallel
{noformat}

I'm prefixing with 'info' to denote that these fields are not accepted as input. Instead they are produced as output for debugging and analysis. I'll send an email to pig-dev about the suggested syntax.

","26/Jul/12 00:21;jay23jack;estimatedParallelism is a good one to add and these names look good to me. Has default parallel already shown up in the job conf? For pig.info.reducers.runtime.parallel, it should be same as mapred.reduce.tasks, so do we still need it? ","26/Jul/12 06:12;billgraham;I just checked and {{default_parallel}} doesn't show up in the jobconf when set so we should add it in the same format:

{noformat}
pig.info.reducers.default.parallel
{noformat}

Since {{pig.info.reducers.runtime.parallel}} is what becomes {{mapred.reduce.tasks}}, no we don't need that one.
","26/Jul/12 18:38;jay23jack;Attached PIG-2779.3.patch for setting various parallelism into job conf for later ananlysis, as suggested by Bill. Also add unit tests for testing them.","27/Jul/12 19:03;billgraham;Thanks Jie, I've been running the test suite on our CI server and so far things look good. We're close. A few more comments though:

- In {{JobControlCompiler}} we should only call {{estimateNumberOfReducers}} if we need to, since we don't need it if {{requestedParallelism}} or {{defaultParallel}} will govern and the call could be expensive. So we'd have this:
{noformat}
} else {
  mro.estimatedParallelism = estimateNumberOfReducers(conf, lds, nwJob);
  jobParallelism = mro.estimatedParallelism;
}
{noformat}

- The semantics of {{pig.info.reducers.requested.parallel}} is a bit misleading as implemented. I would expect that would be the value set via the {{PARALLEL}} statement, but that's not the case, since {{requestedParallel}} gets set to {{jobParallelism}} on line 796 of JCC. Would you please add to the tests in {{TestJobSubmission}} that each of the {{pig.info.reducers.*}} fields are set as expected (or not set) after each of the scenarios. I suspect there are cases where {{pig.info.reducers.requested.parallel}} is being set when {{PARALLEL}} isn't used.
","27/Jul/12 20:00;jay23jack;Hi Bill, regarding the first comment, I agree we can avoid estimating and should be adjusted before. But now would you need it for later analysis in any way? (e.g. comparing estimated #reducer and PALALLEL keyword). If not I'll fix it then.

Regarding the second comment, yes requestedParallel has been used for multiple purposes for a long time and adjusted across logical/physical/mr phases, and also set as the final number of reducers. Many unit tests also assume it as the final number of reducers. I'm afraid cleaning it up can be another ticket? Or maybe an easier approach is to add another field just recording the PARALLEL keyword?","27/Jul/12 20:23;billgraham;Re #1, for later analysis it would be useful to know the estimated parallelism only if estimation was in-fact kicked in because it was needed. If it didn't kick in, that's also useful to know. Hence I think we should move that call into the {{else}} block.

Agreed that {{requestedParallel}} is used all over and we shouldn't mess with that now. Let's instead set another field when {{PARALLEL}} is passed. {{pig.info.reducers.keyword.parallel}}? That said, do you still see value in setting {{pig.info.reducers.requested.parallel}}, since it could be used for so many different things?","28/Jul/12 00:00;jay23jack;Agreed with #1. 

Re #2, according to http://pig.apache.org/docs/r0.10.0/perf.html#parallel, these operators support PARALLEL:

COGROUP, CROSS, DISTINCT, GROUP, JOIN (inner), JOIN (outer), and ORDER BY.

We need to make sure the PARALLEL associate with these operators remains same across logic/phycical/mr phases. Seems it suffers from the same complexity faced by requestedParallel, such as query transformation, multi-query optimization, etc. Seems it's not trivial?","28/Jul/12 05:22;jay23jack;Attached PIG-2779.4.patch that incorporates #1 discussed above.

For #2, we probably can do it later with the cleaning up of requestedParallel?","29/Jul/12 20:32;billgraham;I'm going to take a look to see what it would take to implement {{pig.info.reducers.keyword.parallel}} without refactoring {{requestedParallel}}, since I'm not sure we want to take that on.
","30/Jul/12 20:18;jay23jack;The bugs addressed by this patch would block PIG-2772 and PIG-483, where we need the right number of reducers to remove small jobs at runtime. So I guess we can probably move new features into separate tickets?","31/Jul/12 00:24;billgraham;Agreed, let's hold off on feature creep. I've made a few edits and updated the patch. A few things to note:

- I added more assertions for all parallel values after each test in {{TestNumberOfReducers}} and {{TestJobSubmission}}.
- I changed the contract of {{PigReducerEstimator}} to return -1 if estimation can't be done (i.e. HBaseStorage), since returning 1 was misleading.
- I refactored some common code in {{TestNumberOfReducers}}.

Take a look and let me know what you think Jie. I'll run the test suite with our CI server tonight.",31/Jul/12 00:54;jay23jack;Thanks Bill! These changes sound perfect to me. Let's see if any tests need to be fixed.,"31/Jul/12 17:35;billgraham;There are a few tests that failed, but when run individually, they all pass so I think it's just flakeyness in the full CI test target. Attaching patch #6 that contains the ASF 2.0 license headers in {{TestNumberOfReducers.java}}. ","31/Jul/12 17:36;billgraham;Committed, thanks Jie for seeing this one through!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add 'matches' operator to predicate pushdown,PIG-2778,12596336,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,dvryaboy,dvryaboy,28/Jun/12 22:30,24/Mar/15 21:40,14/Mar/19 03:07,18/Oct/12 23:40,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,Currently the regex match operation does not get pushed down to LoadMetadata (and Expression does not have an enum value for it); it would be quite useful to enable this for some optimizations.,,,,,,,,,,,,,,,,,,,,,,,,,PIG-4482,,,,,,,13/Sep/12 01:51;cheolsoo;PIG-2778.patch;https://issues.apache.org/jira/secure/attachment/12544919/PIG-2778.patch,23/Sep/12 20:35;cheolsoo;test_e2e.log;https://issues.apache.org/jira/secure/attachment/12546222/test_e2e.log,23/Sep/12 20:35;cheolsoo;test_unit.log;https://issues.apache.org/jira/secure/attachment/12546223/test_unit.log,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-09-13 01:51:55.592,,,no_permission,,,,,,,,,,,,249629,,,,Fri Dec 13 01:39:14 UTC 2013,,,,,,,0|i0ag0n:,58868,,,,,,,,,,"13/Sep/12 01:51;cheolsoo;Updating a patch that adds the 'matches' op to predicate pushdown.

I added a simple test case to TestPartitionFilterPushDown and ran test-commit. (TestPoissonSampleLoader fails, but it seems broken in trunk.)

Please let me know if I should add a test to somewhere else, or I overlooked something.

Thanks!",21/Sep/12 21:42;cheolsoo;TestPoissonSampleLoader failure that I mentioned in my previous comment is PIG-2926.,"22/Sep/12 17:54;dvryaboy;Didn't realize that would be so simple! THanks, Cheolsoo.
Were you able to verify that existing loader implementations that do not accept ""matches"" pushdowns still work correctly?","22/Sep/12 21:41;cheolsoo;Hi Dmitriy,

I verified with PigStorage and AvroStorage that there is no regression.

Nevertheless, it is probably a good idea to run a full test suite for this kind of change, so I've kicked off unit test and e2e test. I'll let you know if they all pass.

Thanks!","23/Sep/12 20:35;cheolsoo;Attaching test logs - unit/e2e test.

I see *no regression from this change* although I see some unrelated failures as below:

*Unit test*
- TestInvokerSpeed: known issue PIG-2569
- TestPoissonSampleLoader: known issue PIG-2926
- TestPigServerWithMacros: failed because of an exception during the call to getFileSystem(). Passes when rerun the test. 
- TestStreaming: timeout even without this change.

*e2e test in local mode*
- FilterBoolean_23,24: know issue PIG-2928
- Rank_1,2,3,5,6,7,8,9,10: fail even without this change in trunk.
- The test run hang after executing 474 test cases, so it wasn't complete.","18/Oct/12 17:45;jcoveney;Dmitriy,

I see no reason not to commit this given the test report looks good. Agreed?",18/Oct/12 18:57;dvryaboy;+1,"18/Oct/12 21:36;jcoveney;Cheolsoo, it doesn't apply cleanly anymore. Any chance you can rebase it off trunk?","18/Oct/12 21:54;cheolsoo;Hi Jonathan, sorry about that.

It applies to the github mirror, but I haven't tried to the svn trunk. The svn repository is not responding at the moment... I will rebase it as soon as I can checkout.

Thanks!","18/Oct/12 23:39;jcoveney;This has been applied to trunk. Thanks, Cheolsoo!","06/Dec/13 18:16;rohini;[~daijy],
    We are not using 0.12 yet. But saw that http://svn.apache.org/viewvc/hive/trunk/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hcatalog/pig/HCatLoader.java?view=markup getHCatComparisonString() does not convert matches to LIKE. Wouldn't that cause issues? Have you encountered it?  According to https://issues.apache.org/jira/browse/HIVE-1609 - LIKE is supported. So HCatLoader will have to change to support that or if there is a matches condition on a partition column I am suspecting there will be a exception looking at the InitializeInput code in hcat. Will try it out next week and confirm. ","07/Dec/13 21:12;aniket486;[~rohini], you are right about this. I have noticed this earlier. Wouldn't this be fixed by changing string representation OP_MATCH to https://github.com/apache/pig/blob/branch-0.12/src/org/apache/pig/Expression.java#l48?","13/Dec/13 01:08;rohini;No. HCatLoader needs to be fixed to understand ""matches"" in the expression and convert it into ""LIKE"". Actually come to think of it, the LIKE will only take %  and won't work with all regex patterns. i.e .* can be replaced with %, but other complex java regex patterns would not work.  

[~cheolsoo],
   Do you remember for what loader or case, you wanted to push the 'matches' operator? ","13/Dec/13 01:39;cheolsoo;[~rohini], no, I didn't have a particular load in mind. Although we're currently discussing to support 'matches' in our internal storage at work, we haven't implement one yet. As far as I am concerned, I don't use it for now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Docs are broken due to malformed xml after PIG-2673,PIG-2777,12596278,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,dvryaboy,dvryaboy,28/Jun/12 16:50,22/Feb/13 04:54,14/Mar/19 03:07,28/Jun/12 17:25,,,,,,,,0.11,,,,documentation,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Jun/12 17:23;dvryaboy;PIG-2777.patch;https://issues.apache.org/jira/secure/attachment/12533849/PIG-2777.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,242342,,,,Thu Jun 28 17:25:23 UTC 2012,,,,,,,0|i02twv:,14447,,,,,,,,,,28/Jun/12 17:23;dvryaboy;Tested by running the ant docs target. Will commit.,28/Jun/12 17:25;dvryaboy;Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Register jar does not goes to classpath in some cases,PIG-2775,12596193,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,28/Jun/12 00:03,06/Jan/13 23:57,14/Mar/19 03:07,28/Jun/12 00:36,0.10.0,0.11,0.9.2,,,,,0.10.1,0.11,0.9.3,,impl,,,0,,,,,,,,,,,,,"In PIG-2532, we fix this issue in load side, but we still have issue in store side, see http://mail-archives.apache.org/mod_mbox/incubator-hcatalog-user/201206.mbox/%3CCAB2zpW9MY6t-NMdOJ-%2B0ezt0NJOFrxsJ7kc%3DR4WJh%2Bn-9bDW2g%40mail.gmail.com%3E",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Jun/12 00:06;daijy;PIG-2775-1.patch;https://issues.apache.org/jira/secure/attachment/12533748/PIG-2775-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-28 00:27:37.64,,,no_permission,,,,,,,,,,,,242345,Reviewed,,,Tue Jul 24 14:48:14 UTC 2012,,,,,,,0|i02ty7:,14453,,,,,,,,,,28/Jun/12 00:27;dvryaboy;+1,28/Jun/12 00:36;daijy;Patch committed to 0.9/0.10/trunk. Thanks Dmitriy for reviewing!,24/Jul/12 14:48;traviscrawford;Accidentally assigned to me. Reverting.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a simple logic causes very long compiling time on pig 0.10.0,PIG-2769,12596000,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,njw45,dli@operasolutions.com,dli@operasolutions.com,26/Jun/12 23:16,04/Nov/13 20:03,14/Mar/19 03:07,08/Jan/13 23:14,0.10.0,,,,,,,0.11.2,0.12.0,,,build,,,1,,,,,,,,,,,,,"We found the following simple logic will cause very long compiling time for pig 0.10.0, while using pig 0.8.1, everything is fine.

A = load 'A.txt' using PigStorage()  AS (m: int);

B = FOREACH A {
    days_str = (chararray)
        (m == 1 ? 31: 
        (m == 2 ? 28: 
        (m == 3 ? 31: 
        (m == 4 ? 30: 
        (m == 5 ? 31: 
        (m == 6 ? 30: 
        (m == 7 ? 31: 
        (m == 8 ? 31: 
        (m == 9 ? 30: 
        (m == 10 ? 31: 
        (m == 11 ? 30:31)))))))))));
GENERATE
   days_str as days_str;
}   
store B into 'B';

and here's a simple input file example: A.txt
1
2
3

The pig version we used in the test
Apache Pig version 0.10.0-SNAPSHOT (rexported)
","Apache Pig version 0.10.0-SNAPSHOT (rexported)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Dec/12 19:11;njw45;PIG-2769.0.patch;https://issues.apache.org/jira/secure/attachment/12562392/PIG-2769.0.patch,08/Jan/13 20:16;njw45;PIG-2769.1.patch;https://issues.apache.org/jira/secure/attachment/12563813/PIG-2769.1.patch,08/Jan/13 22:14;alangates;PIG-2769.2.patch;https://issues.apache.org/jira/secure/attachment/12563839/PIG-2769.2.patch,31/Dec/12 16:03;alangates;TEST-org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.TestInputSizeReducerEstimator.txt;https://issues.apache.org/jira/secure/attachment/12562799/TEST-org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.TestInputSizeReducerEstimator.txt,26/Jun/12 23:17;dli@operasolutions.com;case1.tar;https://issues.apache.org/jira/secure/attachment/12533559/case1.tar,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2012-06-26 23:23:37.56,,,no_permission,,,,,,,,,,,,240701,,,,Thu Apr 04 20:41:52 UTC 2013,,,,,,,0|i014rz:,4543,,,,,,,,,,26/Jun/12 23:17;dli@operasolutions.com;example code and data file,"26/Jun/12 23:18;dli@operasolutions.com;It's worth pointing out that Pig 0.9.2 also runs quickly; we only see the degradation with Pig 0.10.0.

The degradation in performance seems to have a knee as 4 or 5 conditionals works as expected but as presented, the script takes about 6 minutes at the GRUNT> prompt after hitting enter; before any Hadoop execution.

-Clay
","26/Jun/12 23:23;jcoveney;My guess is that this has to do with recursive backtracking.

from QueryParser.g
{code}
foreach_statement : ( ( alias EQUAL )?  FOREACH rel LEFT_CURLY ) => foreach_complex_statement
                  | foreach_simple_statement
;
{code}

The fact that it works fine up until some n and then dies is typical of when these things die. It could be some interaction with another recursive backtracking rule that is blowing up.

The ideal solution is to get rid of recursive backtracking in the parser...

",27/Jun/12 18:43;daijy;It takes me 5 min to compile the logical plan.,"10/Jul/12 17:45;dli@operasolutions.com;Hi, Daniel,

I tried to check out the revision 0.10.0 [ 12316246 ] , but got ""No such revision 12316246"" error. The command I used is following

svn co -r 12316246 http://svn.apache.org/repos/asf/pig/branches/branch-0.10/

Thanks.
Dan",10/Jul/12 17:50;daijy;What is revision 12316246? Where did you get this?,"10/Jul/12 17:53;dli@operasolutions.com;From your comment on 27/Jun/12 18:43, is the number [ 12316246 ] the revision number?

","11/Jul/12 16:59;dli@operasolutions.com;Hi, Daniel,

I'm little bit confused. is this problem fixed or not?

Thanks.
Dan","11/Jul/12 18:22;daijy;No, it is not fixed. 5 min to compile a plan is not acceptable. I am not saying a particular version works, just to post what I observe. Sorry for the confusion. We need to fix it.","11/Jul/12 20:37;dli@operasolutions.com;Thanks for the clarification, and looking forward to the fix.

Dan","04/Oct/12 18:40;tnachen;I'd like to look at this for my first bug fix for pig, as I've already glanced through the parsing and query plan code. Can't find a way to assign this to myself via Jira though, probably need someone else to do so?","04/Oct/12 18:46;thejas;I have assigned it to you. I have also added you to contributors list, now you should be able to assign jiras to yourself.
","22/Oct/12 17:36;tnachen;I decided to leave this issue open to grab for now since this issue is mostly in the ANTLR generated code and I am still learning about ANTLR.
It does seems that it's recursively calling foreach_complex_statement pretty excessively, but I don't have enough knowledge to deduce the grammer so it still works.
I did verify that the AST generated in 0.9.2 and 0.10-SNAPSHOT generated is exactly the same if that helps.","26/Dec/12 19:11;njw45;I've attached a patch that refactors the QueryParser's grammar so that it doesn't need global backtracking (in fact, it only has backtracking in one rule as empty map casts look very much like map literals in brackets). I've added a lot of comments, so hope it's clear what's going on, and have checked all the unit tests pass. I've also added a lot more unit tests, including the query on this ticket (which now parses in about 0.03s), so I'm pretty sure it's just a drop-in replacement.",29/Dec/12 00:11;alangates;I'll take a look at this and start running the tests.,"29/Dec/12 01:36;alangates;I see failures in TestInputSizeReducerEstimator with this patch applied.  There may be others, as the test run hasn't finished yet.","29/Dec/12 21:27;njw45;Were there any others? What do the test logs say? It passes for me locally, so I'd appreciate some help identifying the failure. Thanks -",31/Dec/12 16:02;alangates;That's the only failure I see in the unit tests that's new after I apply your patch.  It fails consistently for me with your patch and passes consistently without it.  I'll attach the logs from the test.,31/Dec/12 16:03;alangates;Log file of failed test.  Not intended as a patch.,"02/Jan/13 02:49;cheolsoo;Hi Alan,

The test failure that you're seeing may be because of a stale {{hadoop-site.xml}} in {{build/classes}} from a previous test. Any test that uses MiniCluster generates {{hadoop-site.xml}} in {{build/classes}}, and since {{build/classes}} is in the classpath of unit test, a stale {{hadoop-site.xml}} will be picked up by {{TestInputSizeReducerEstimator}} if exists.

I am able to reproduce the exactly same error by adding a {{hadoop-site.xml}} to {{build/classes}} as follows:
{code:title=hadoop-site.xml}
<configuration>
  <property>
    <name>fs.default.name</name>
    <value>hdfs://localhost:45399</value>
  </property>
</configuration>
{code}
With this {{hadoop-site.xml}} in classpath, the test tries to connect to a NN on port 45399, which it shouldn't.

Can you verify that running {{ant clean test}} instead of {{ant test}} lets the error go away? When I do {{ant clean test}}, the test always passes for me.","03/Jan/13 23:48;jcoveney;Nick, this its awesome. Well done, and thanks for tackling it. It's some cleanup pig sorely needed.","08/Jan/13 18:22;alangates;When I do a clean first as Cheolsoo advises it works, though I don't fully understand that since I started out with a clean checkout.

In the system tests NegForeach_7, NegForeach_9, SyntaxErrors_4, Macro_Error_4 all fail because the error messages have changed.  You can find these in test/e2e/pig/tests/negative.conf and macro.conf.  Search on each of the group names (NegForeach, ...) and then find the test number under that.  In each case you can run the query and change the expected error message to match the new one.  

Other than that, +1, patch looks good.","08/Jan/13 18:43;rohini;bq. When I do a clean first as Cheolsoo advises it works, though I don't fully understand that since I started out with a clean checkout.

Order of tests run must have been the cause. It might have passed after doing a clean because you might have just run TestInputSizeReducerEstimator without running other tests. TestInputSizeReducerEstimator needs to be fixed to do new Configuration(false); instead of new Configuration(); which makes it pick up hadoop-site.xml from a previously run test. 
",08/Jan/13 20:16;njw45;Thanks! I've attached a version of the patch which fixes the e2e tests you mentioned.,"08/Jan/13 22:14;alangates;Two small changes from the last patch.  I fixed one issue in negative.conf where there was a """" instead of "".  Also changed TestInputSizeReducerEstimator as suggested by Rohini which fixed the unit test issue (thanks Rohini).",08/Jan/13 23:14;alangates;Patch checked in.  Thanks Nick.,"11/Jan/13 01:57;dvryaboy;this is great, thank you!","04/Apr/13 07:39;dvryaboy;Didn't see earlier that this only went into trunk (thanks [~knoguchi] for pointing this out!).
We should put this into 0.11 branch, maybe there will be an 0.11.2 before 12 comes out.","04/Apr/13 20:17;knoguchi;bq. We should put this into 0.11 branch, maybe there will be an 0.11.2 before 12 comes out.

If we can fix this in 0.11, that would be really nice.  
On our clusters, there were multiple users hit with this issue on 0.10.
",04/Apr/13 20:41;dvryaboy;in 0.11 branch now.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig creates wrong schema after dereferencing nested tuple fields,PIG-2767,12595791,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,onetangentburn,onetangentburn,25/Jun/12 14:11,14/Oct/13 16:46,14/Mar/19 03:07,22/Apr/13 20:13,0.10.0,,,,,,,0.12.0,,,,parser,,,1,,,,,,,,,,,,,"The following script fails:

data = LOAD 'test_data.txt' USING PigStorage() AS (f1: int, f2: int, f3:
int, f4: int);

nested = FOREACH data GENERATE f1, (f2, f3, f4) AS nested_tuple;

dereferenced = FOREACH nested GENERATE f1, nested_tuple.(f2, f3);
DESCRIBE dereferenced;

uses_dereferenced = FOREACH dereferenced GENERATE nested_tuple.f3;
DESCRIBE uses_dereferenced;

The schema of ""dereferenced"" should be {f1: int, nested_tuple: (f2: int,
f3: int)}. DESCRIBE thinks it is {f1: int, f2: int} instead. When dump is
used, the data is actually in form of the correct schema however, ex.

(1,(2,3))
(5,(6,7))
...

This is not just a problem with DESCRIBE. Because the schema is incorrect,
the reference to ""nested_tuple"" in the ""uses_dereferenced"" statement is
considered to be invalid, and the script fails to run. The error is:

Invalid field projection. Projected field [nested_tuple] does not exist in
schema: f1:int,f2:int.","Amazon EMR, patched to use Pig 0.10.0",,,,,,,,,,,,,,,,,,,,,,,,PIG-3290,,,,,,,02/Apr/13 21:01;daijy;PIG-2767-1.patch;https://issues.apache.org/jira/secure/attachment/12576652/PIG-2767-1.patch,25/Jun/12 21:33;onetangentburn;test_data.txt;https://issues.apache.org/jira/secure/attachment/12533384/test_data.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-11-19 20:12:17.717,,,no_permission,,,,,,,,,,,,256402,Reviewed,,,Mon Apr 22 20:13:20 UTC 2013,,,,,,,0|i0h54v:,98094,,,,,,,,,,25/Jun/12 21:33;onetangentburn;The 'text_data.txt' used in the script.,19/Nov/12 20:12;julienledem;This will be fixed in a future release,16/Apr/13 18:09;alangates;+1.,"20/Apr/13 21:24;prkommireddi;With Alan's +1, this can be committed?","20/Apr/13 22:10;cheolsoo;Wait. I think there is a case that is not covered by the patch. What if we have multiple levels of nesting?

Consider the following example:
{code}
a = load 'a' as (field1: int, field2: int, field3: int );
b = foreach a generate field1, TOTUPLE(TOTUPLE(field2, field3));
c = foreach a generate field1, ((field2, field3));
describe b;
describe c;
{code}
With the patch, this still gives me incorrect results:
{code}
b: {field1: int,org.apache.pig.builtin.totuple_org.apache.pig.builtin.totuple_field3_13_14: (org.apache.pig.builtin.totuple_field3_13: (field2: int,field3: int))}
c: {field1: int,org.apache.pig.builtin.totuple_field3_23: (field2: int,field3: int)}
{code}","22/Apr/13 17:57;daijy;[~cheolsoo], thanks for testing more. I believe that's a different issue. In ""((field2, field3))"", the inner parentheses translate to TOTUPLE, and the outer one does not. There is ambiguity in the grammar. I remember there is a discussion, we agree that we translate parentheses into TOTUPLE only when there are more than 1 items inside. This is equivalent to the python style.","22/Apr/13 19:20;cheolsoo;[~daijy], thank you very much for the clarification. That case, I have no objection.",22/Apr/13 20:13;daijy;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig-HCat Usability,PIG-2766,12595663,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,vikram.dixit,vikram.dixit,vikram.dixit,23/Jun/12 02:56,06/Jan/13 23:57,14/Mar/19 03:07,28/Jun/12 21:49,0.10.0,,,,,,,0.10.1,0.11,0.9.3,,grunt,tools,,0,,,,,,,,,,,,,"Currently to use hcat from pig (via HCatLoader/HCatStorer) user need to register bunch of jars and set couple of configuration. For a novice user, it is non-trivial to find all the relevant jars and config params. We should have better integration between Pig & HCat by pre-configuring Pig to load all these jars and configs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Jun/12 03:11;vikram.dixit;PIG-2766.patch;https://issues.apache.org/jira/secure/attachment/12533158/PIG-2766.patch,25/Jun/12 22:47;vikram.dixit;PIG-2766_2.patch;https://issues.apache.org/jira/secure/attachment/12533393/PIG-2766_2.patch,27/Jun/12 20:44;vikram.dixit;PIG-2766_3.patch;https://issues.apache.org/jira/secure/attachment/12533699/PIG-2766_3.patch,27/Jun/12 22:06;vikram.dixit;PIG-2766_4.patch;https://issues.apache.org/jira/secure/attachment/12533714/PIG-2766_4.patch,28/Jun/12 00:58;vikram.dixit;PIG-2766_5.patch;https://issues.apache.org/jira/secure/attachment/12533756/PIG-2766_5.patch,28/Jun/12 02:19;vikram.dixit;PIG-2766_6.patch;https://issues.apache.org/jira/secure/attachment/12533765/PIG-2766_6.patch,28/Jun/12 18:55;vikram.dixit;PIG-2766_7.patch;https://issues.apache.org/jira/secure/attachment/12533869/PIG-2766_7.patch,28/Jun/12 19:16;vikram.dixit;PIG-2766_Branch0.9.patch;https://issues.apache.org/jira/secure/attachment/12533873/PIG-2766_Branch0.9.patch,,,,8.0,,,,,,,,,,,,,,,,,,,2012-06-27 00:46:18.039,,,no_permission,,,,,,,,,,,,242344,Reviewed,,,Thu Jun 28 21:49:27 UTC 2012,,,,,,,0|i02txz:,14452,The patch introduces a new command line parameter for pig: -useHCatalog which imports the appropriate jars for pig use with HCatalog. This reduces the burden of finding all the relevant jars for using HCatalog with pig. If the user has setup the home directories for hive/hcatalog those would be used else defaults (/usr/lib/hive...) are assumed.,,,,,,,,,23/Jun/12 03:11;vikram.dixit;This patch adds a new command line parameter to the bin/pig shell script and includes the appropriate jars for pig's consumption.,25/Jun/12 22:47;vikram.dixit;Added more dependent jars.,"27/Jun/12 00:46;daijy;Couple of comments:
1. We shall not hard code version, instead, find those jars using prefix
2. Why put $additionalJars in the middle? Put it in the end seems more intuitive
3. In the change [[ $f = ""-secretDebugCmd"" ]], anything wrong with the original syntax?",27/Jun/12 20:44;vikram.dixit;Updated with comments incorporated.,"27/Jun/12 20:46;vikram.dixit;Fixed 1 and 2.
3 was just to keep it consistent with the rest of the file.",27/Jun/12 22:06;vikram.dixit;Removed hive conf from the additional jars list. Added it only to the classpath. ,28/Jun/12 00:58;vikram.dixit;Added handling for PIG_OPTS route for adding jars.,28/Jun/12 18:55;vikram.dixit;Moved the pig opts jars to the beginning of the additional jars.,28/Jun/12 19:16;vikram.dixit;Patch for branch 0.9.,28/Jun/12 21:49;daijy;Patch committed to 0.9/0.10/trunk. Thanks Vikram!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
With hadoop23 importing modules inside python script does not work,PIG-2761,12595165,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,19/Jun/12 21:16,06/Jan/13 23:57,14/Mar/19 03:07,26/Jun/12 20:28,0.10.1,,,,,,,0.10.1,0.11,0.9.3,,,,,1,,,,,,,,,,,,,"Because unjar has been removed from 23, registering scripts has issue. PIG-2745 addresses the issue of registering scripts with pig. But if the registered py script imports other modules then it does not work. Steps to reproduce the issue in https://issues.apache.org/jira/browse/PIG-2745?focusedCommentId=13396965&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13396965",,,,,,,,,,,,,,,,,PIG-2760,,,,,,,,,,,,,,,02/Aug/12 06:04;rohini;PIG-2761-branch09.patch;https://issues.apache.org/jira/secure/attachment/12538868/PIG-2761-branch09.patch,25/Jun/12 17:42;rohini;PIG-2761-branch10_1.patch;https://issues.apache.org/jira/secure/attachment/12533340/PIG-2761-branch10_1.patch,19/Jun/12 22:16;rohini;PIG-2761-initial.patch;https://issues.apache.org/jira/secure/attachment/12532612/PIG-2761-initial.patch,25/Jun/12 17:42;rohini;PIG-2761-trunk.patch;https://issues.apache.org/jira/secure/attachment/12533341/PIG-2761-trunk.patch,21/Jun/12 01:54;rohini;PIG-2761.patch;https://issues.apache.org/jira/secure/attachment/12532813/PIG-2761.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2012-06-25 13:18:56.573,,,no_permission,,,,,,,,,,,,242346,Reviewed,,,Tue Aug 07 17:49:20 UTC 2012,,,,,,,0|i02tyf:,14454,,,,,,,,,,"19/Jun/12 22:16;rohini;Initial patch for review. Reverted 2745 as removing leading / is moved to PigContext itself. Easier to do that in PigContext.addScriptFile() than repeat it in each of the ScriptEngine implementations and PigServer. Changed the ScriptEngine.getScriptAsStream() to try all classloaders. 

Working on writing a e2e test for this.

This patch does not address 2760. Trying to see if there is a easy way to accomodate that in this patch without impacting the changes for s3 PIG-2623. The easier thing would be to add two copies of the script file to the jar - one with absolute path and one relative path but it is not efficient. ","21/Jun/12 01:54;rohini;Modified scriptingudf.py to import another module so that it is exercised as part of current e2e scripting tests. 

Also included the fix from PIG-2760. ","21/Jun/12 02:11;rohini;Patch contains a newly added file. svn add needs to be done before committing.
 
  svn add test/e2e/pig/udfs/python/stringutil.py",25/Jun/12 13:18;herberts;Fixes the issue of PIG-2760.,25/Jun/12 17:42;rohini;Attaching separate patches for branch 0.10.1 and trunk. scriptingudf.py has conflicts with PIG-2761.patch in trunk. ,"26/Jun/12 20:28;daijy;+1

Patch committed to 0.10 branch/trunk.

Thanks Rohini!",02/Aug/12 06:04;rohini;Patch for pig 0.9 to work with python scripts in H23. Had to port it to pig-0.9 as many of our users have not migrated to 0.10 yet and so we need 0.9 to work with Hadoop 23.,07/Aug/12 17:49;daijy;Committed to 0.9 branch as well as per requested by Rohini.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
resources added with a relative path are added to the JobXXXX jar file under their absolute path,PIG-2760,12594935,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,herberts,herberts,18/Jun/12 13:23,06/Jan/13 23:57,14/Mar/19 03:07,26/Jun/12 20:35,0.10.0,,,,,,,0.10.1,0.11,,,,,,0,,,,,,,,,,,,,"When registering a local resource using a relative path, the resource is added to the JobXXXX jar under its absolute path.

If a pig script contains the following:

REGISTER etc/foo;

and is executed from a directory /PATH/TO/DIR, the JobXXXX jar file will contain the following:

/PATH/TO/DIR/etc/foo

instead of

etc/foo

which was the previous behavior",,,,,,,,,,,,,,,,,,,PIG-2761,,,,,,,,,,,,,19/Jun/12 09:29;herberts;PIG-2760.patch;https://issues.apache.org/jira/secure/attachment/12532520/PIG-2760.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-18 16:40:32.314,,,no_permission,,,,,,,,,,,,256397,Reviewed,,,Tue Jun 26 20:35:39 UTC 2012,,,,,,,0|i0h51z:,98081,,,,,,,,,,"18/Jun/12 16:40;cheolsoo;This is a regression of PIG-2623:

{code}
-        File f = new File(path);
+        File f = FileLocalizer.fetchFile(pigContext.getProperties(), path).file;
{code}

where fetchFile() converts a relative path to absolute path.

In fact, converting a relative path to an absolute path isn't an issue, but the leading ""/"" makes registered files not found. That is fixed at PIG-2745.

Thanks!","19/Jun/12 08:13;herberts;Converting a relative path to an absolute one may be an issue when accessing a resource in a UDF using getResourceAsStream

Previously, if we used 'REGISTER foo/bar;' in a script, you could access 'bar' by calling this.getClass().getClassLoader().getResourceAsStream('foo/bar'); in your UDF, and this would work whatever directory the pig script is run from.

If converting the relative path to an absolute one (with no leading '/'), the argument to getResourceAsStream will need to be dependent on the directory from which the pig script is run, this kinds of defeat usability.","19/Jun/12 09:29;herberts;This patch changes the name used in the job jar to relative paths if the added resource lies under the current working directory.

It also strips leading '/' as PIG-2745","19/Jun/12 19:37;cheolsoo;Hi Mathias,

Agreed. I haven't thought about the use case that you're describing. :-) Thanks for explaining!

I like your patch because it solves all the cases that I can think of. Just a minor comment. Can't you collapse the following lines of code into a single line?

{code}
String nameInJar = cp.startsWith(cwd) ? cp.substring(cwd.length() + 1) : cp;
// Strip leading path.sep
if (nameInJar.startsWith(""/"")) {
    nameInJar = nameInJar.substring(1);
}
{code}

=>

{code}
String nameInJar = cp.startsWith(cwd) ? cp.substring(cwd.length() + 1) : cp.substring(1);
{code}

Given that cp is always going to be an absolute path (as a relative path is converted to an absolute one by fetchfile()), the ""if"" condition seems redundant to me. Please correct me if I am wrong.

Thanks!",19/Jun/12 20:03;herberts;I guess with an appropriate comment the two code chunks could be collapsed into one yes.,"19/Jun/12 22:16;rohini;<code>
se.registerFunctions(f.getPath(), namespace, pigContext);
String cwd = new File(System.getProperty(""user.dir"")).getCanonicalPath();
String cp = f.getCanonicalPath();
String nameInJar = cp.startsWith(cwd) ? cp.substring(cwd.length() + 1) : cp.substring(1);
pigContext.addScriptFile(nameInJar,f.getPath());
</code>

  The function is still registered with f.getPath() even though nameInJar is going to be relative to current directory. This will cause getScriptAsStream() on the backend as f.getPath() minus leading / will not be in the jar.  ",19/Jun/12 22:17;rohini;*This will cause getScriptAsStream() to error out on the backend as f.getPath() minus leading / will not be in the jar.,19/Jun/12 23:07;cheolsoo;Indeed. Good catch!,"20/Jun/12 09:18;herberts;So this means we should do the following:

<code>
String cwd = new File(System.getProperty(""user.dir"")).getCanonicalPath();
String cp = f.getCanonicalPath();
String nameInJar = cp.startsWith(cwd) ? cp.substring(cwd.length() + 1) : cp.substring(1);
pigContext.addScriptFile(nameInJar,f.getPath());
se.registerFunctions(nameInJar, namespace, pigContext);
</code>

right?","21/Jun/12 02:04;rohini;Mathias,
 It requires one more minor change. If we just do cp.startsWith(cwd), then even if absolute path was specified and if the script is in a subdirectory under current directory, the jar entry only has the relative path instead of the absolute path. Need to do cp.equals(cwd + ""/"" + patch). 

I was addressing script loading issue in PIG-2761 and had some modifications for the same line of code. So added your fix to it also and tested and also made it part of PIG-2761 patch. Hope you don't mind. If you could, I would appreciate you reviewing PIG-2761.",25/Jun/12 13:19;herberts;Your patch attached to PIG-2761 Looks Good To Me for what concerns PIG-2760.,26/Jun/12 20:35;daijy;This is fixed along with PIG-2761. Thanks folks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Typo in document ""Built In Functions""",PIG-2759,12593988,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,daijy,daijy,daijy,18/Jun/12 02:10,06/Jan/13 23:57,14/Mar/19 03:07,18/Jun/12 02:15,,,,,,,,0.10.1,0.11,,,documentation,,,0,,,,,,,,,,,,,"""X = FOREACH A DIFF(B1,B2);""
should be
""X = FOREACH A GENERATE DIFF(B1,B2);""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Jun/12 02:13;daijy;PIG-2759-1.patch;https://issues.apache.org/jira/secure/attachment/12532359/PIG-2759-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,242347,Reviewed,,,Mon Jun 18 02:15:09 UTC 2012,,,,,,,0|i02tyn:,14455,,,,,,,,,,18/Jun/12 02:15;daijy;Patch committed to 0.10/trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation for 0.11,PIG-2756,12560811,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,billgraham,billgraham,15/Jun/12 17:46,22/Feb/13 04:53,14/Mar/19 03:07,29/Jan/13 07:34,0.11,,,,,,,0.11,,,,documentation,,,0,,,,,,,,,,,,,"Tracking areas where we need documentation on the pig.apache.org site (Javadocs are typically pretty good). We can open child tasks as needed. Please add to the list if you know of others.

* Pluggable {{PigProgressNotificationListener}} isn't in the docs
* Pluggable reducer estimators (see PIG-2574)
* ILLUSTRATE seems to have dropped off the docs
* {{HBaseStorage}} (see PIG-2341)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,247270,,,,Tue Jan 29 07:34:15 UTC 2013,,,,,,,0|i089j3:,46143,,,,,,,,,,18/Jan/13 23:00;billgraham;Olga can we close this out or do you want to keep it open to track additional documentation efforts?,"28/Jan/13 16:49;billgraham;Adding dep JIRAs for documenting reducer est and PPNL, both of which have patches available.

Verified that ILLUSTRATE is in fact still in the docs, despite my initial description.","29/Jan/13 07:34;billgraham;All known documentation issues for Pig 0.11 have been resolved, closing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change the names of the jar produced in the build folder to match maven conventions,PIG-2748,12560219,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,julienledem,julienledem,julienledem,11/Jun/12 18:42,22/Feb/13 04:53,14/Mar/19 03:07,27/Jun/12 21:58,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"{noformat}
pig-{version}-core.jar becomes pig-{version}.jar
pig-{version}.jar becomes pig-{version}-withdependencies.jar


- <property name=""output.jarfile"" value=""${build.dir}/${final.name}.jar"" />
+ <property name=""output.jarfile"" value=""${build.dir}/${final.name}-withdependencies.jar"" />
- <property name=""output.jarfile.core"" value=""${build.dir}/${final.name}-core.jar"" />
+ <property name=""output.jarfile.core"" value=""${build.dir}/${final.name}.jar"" />
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Jun/12 23:02;julienledem;PIG-2748.patch;https://issues.apache.org/jira/secure/attachment/12532260/PIG-2748.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-11 18:49:30.052,,,no_permission,,,,,,,,,,,,256387,,,,Wed Jun 27 21:03:58 UTC 2012,,,Patch Available,,,,0|i0h4xb:,98060,,,,,,,,,,"11/Jun/12 18:49;jcoveney;Given that this is apropos of nothing (well, apropos of a much needed conversion to maven which may or may not ever happen), is there a compelling reason to change this? Why not offer a new target with the new names, but keep it as is? I feel like this would break builds for dubious gain.

Though, in theory, I 100% support it.","11/Jun/12 19:12;daijy;Fine as long as we don't change top level pig.jar, pig-withouthadoop.jar. Also need to make sure mvn-install/mvn-deploy is doing the right thing after change.
","15/Jun/12 17:47;julienledem;We are already publishing pig-{version}-core.jar as pig-{version}.jar, so this would make things consistent and less confusing. It is also a step in the direction of a more maven friendly build. ","15/Jun/12 23:02;julienledem;PIG-2748.patch
suppressed duplicated properties.
renamed properties to match the new naming convention.","27/Jun/12 18:56;julienledem;mvn-install works correctly.
mvn-deploy fails with 401 as it should because I'm not authorized to deploy.
Daniel, can you take a quick look ? I'd like to check this in.",27/Jun/12 21:03;daijy;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig e2e test RubyUDFs fails in MR mode when running from tarball,PIG-2745,12560059,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,09/Jun/12 19:19,06/Jan/13 23:57,14/Mar/19 03:07,18/Jun/12 01:54,0.10.1,,,,,,,0.10.1,0.11,,,,,,0,,,,,,,,,,,,,"To reproduce the issue, please run the e2e test ""RubyUDFs_1"" in MR mode from the tarball (not from installed Pig - please see why below). Either pseudo-distributed-mode or full-mode Hadoop can be used.

{code}
ant -Dhadoopversion=23 -Dharness.old.pig=`pwd` -Dharness.cluster.conf=/etc/hadoop/conf/ -Dharness.cluster.bin=/usr/lib/hadoop/bin/hadoop test-e2e -Dtests.to.run=""-t RubyUDFs_1""
{code}

The test fails with the following error:

{code}
java.lang.IllegalStateException: Could not initialize interpreter (from file system or classpath) with /home/cheolsoo/pig-0.10/test/e2e/pig/testdist/libexec/ruby/scriptingudfs.rb
{code}

Looking at the job jar generated by Pig, ""scriptingudfs.rb"" can be found as follows:

{code}
[cheolsoo@c1405 pig-cheolsoo]$ jar tvf bad.jar | grep scriptingudfs.rb
  2491 Fri Jun 08 15:52:08 PDT 2012 /home/cheolsoo/pig-0.10/test/e2e/pig/testdist/libexec/ruby/scriptingudfs.rb
{code}

Looking at getScriptAsStream() method in ScriptEngine.java, ""scriptingudfs.rb"" is supposed to be read from the job jar, but it is not. The reason is because getResourceAsStream(""/x"") looks for ""x"" (without the leading ""/"") not ""/x"". Since ""scriptingudfs.rb"" is stored with it absolute path, it ends up being not found by getResourceAsStream(scriptPath).

{code}
File file = new File(scriptPath);
if (file.exists()) {
    try {
        is = new FileInputStream(file);
    } catch (FileNotFoundException e) {
        throw new IllegalStateException(""could not find existing file ""+scriptPath, e);
    }
} else {
    if (file.isAbsolute()) {
        is = ScriptEngine.class.getResourceAsStream(scriptPath);
    } else {
        is = ScriptEngine.class.getResourceAsStream(""/"" + scriptPath);
    }
}
{code}

In fact, the test passes if you run in local mode or from installed Pig. The reason is because ""scriptingudfs.rb"" is found in local file system (e.g /usr/share/pig/test/e2e/pig/udfs/ruby/scriptingudfs.rb).

The fix seems straightforward. Attached is the patch that removes the leading ""/"" when registering UDF scripts so that they are stored without the leading ""/"" in the job jar as follows:

{code}
[cheolsoo@c1405 pig-cheolsoo]$ jar tvf good.jar | grep scriptingudfs.rb
  2491 Fri Jun 08 15:52:08 PDT 2012 home/cheolsoo/pig-0.10/test/e2e/pig/testdist/libexec/ruby/scriptingudfs.rb
{code}

Thanks!",,,,,,,,,,,,,,,,,,,PIG-2761,PIG-2623,PIG-2760,,,,,,,,,,,15/Jun/12 19:01;cheolsoo;PIG-2745-2.patch;https://issues.apache.org/jira/secure/attachment/12532228/PIG-2745-2.patch,10/Jun/12 22:42;cheolsoo;PIG-2745.patch;https://issues.apache.org/jira/secure/attachment/12531620/PIG-2745.patch,18/Jun/12 01:48;daijy;Test001.java;https://issues.apache.org/jira/secure/attachment/12532358/Test001.java,19/Jun/12 18:25;daijy;enable_scripting_tests_23.patch;https://issues.apache.org/jira/secure/attachment/12532590/enable_scripting_tests_23.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-06-16 00:39:22.504,,,no_permission,,,,,,,,,,,,242348,Reviewed,,,Tue Jun 19 20:39:56 UTC 2012,,,,,,,0|i02tyv:,14456,,,,,,,,,,09/Jun/12 19:25;cheolsoo;I also see the same issue with e2e Scripting tests where Jython UDF scripts are not found in classpath. Applying the change that I described let those test pass as well.,"15/Jun/12 19:01;cheolsoo;I updated the patch to handle not only a leading ""/"" but also ""./"".

In fact, it is not necessary to worry about ""./"" since FileLocalizer.fetchFile() already converts relative paths to absolute paths; nevertheless, it seems like a good idea to make this method more robust anyway. 

In addition, I replaced substring(1) with
{code}replaceFirst(""^\\./|^/"", """"){code} because the latter seems more intuitive.","16/Jun/12 00:39;rohini;+1. Tested this patch with relative path and absolute path for 20.205 and 23. Works fine. 

Daniel,
   Can you include this in 0.10 also. Without this scripting udfs do not work in 23 for both relative and absolute path. e2e tests currently marked ignored for MAPREDUCE-3700 also will have to be enabled again. ",18/Jun/12 01:47;daijy;Looks good. I also attach a java code to demonstrate the problem. The patch fix the issue in Ruby. The issue for Python relative path is still there.,18/Jun/12 01:54;daijy;Patch committed to 0.10/trunk. Thanks Cheolsoo!,"18/Jun/12 04:26;cheolsoo;Hi Daniel, thanks for submitting my patch!

I am wondering why you think that the issue with relative paths for Python still exists. In my YARN cluster, the Scripting_* tests (excluded due to MAPREDUCE-3700) all pass. (Technically, I am using Hadoop-2.0.0, but that shouldn't make a difference.) I can also manually verify that it works in Grunt shell.

My fix shouldn't be Ruby-specific since the problem is with PigServer stuffing any UDF scripts into the job jar.

Looking at your test code, one thing that I haven't thought about is ""../"" although that shouldn't be an issue now as in the registerCode() method, relative paths are always converted to absolute paths by FileLocalizer.fetchFile(). Nevertheless, handling ""../"" as well might be a good idea to make that method more robust.

Thanks!","19/Jun/12 18:22;daijy;Hi, Cheolsoo,
You are right. This issue is fixed as a byproduct of PIG-2623, which convert the relative path to absolute path. All the Scripting tests pass for hadoop 23 now. I will enable those tests for 23.

However, there is one another hole left. If we import another python module, Pig cannot pack/refer the path of dependent python module correctly. Here is one example:

udf.py:
from base import square

@outputSchemaFunction(""squaresquareSchema"")
def squaresquare(num):
    if num == None:
        return None
    return (square(num)*square(num))

@schemaFunction(""squaresquareSchema"")
def squaresquareSchema(input):
    return input

base.py
def square(num):
    if num == None:
        return None
    return ((num)*(num))

Pig script:
register 'udf.py' using jython as myfuncs;

a = load '1.txt' as (a0:int);
b = foreach a generate myfuncs.squaresquare(a0);
dump b;

Pig incorrectly pack the base.py as /base.py in job.jar, and fail to refer it in backend. It happens in both 20 and 23.","19/Jun/12 20:21;cheolsoo;Thanks for your explanation, Daniel.

I will open a new jira and look into that unless someone's already doing.

Btw, there is another jira PIG-2760 related to the UDF script path. That seems a valid bug as well.","19/Jun/12 20:39;rohini;Cheolsoo,
   I am looking into the issue Daniel mentioned. Should have a fix soon. 
   ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle Pig command line with XML special characters,PIG-2744,12559913,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,fang fang chen,rding,rding,09/Jun/12 00:18,22/Feb/13 04:54,14/Mar/19 03:07,13/Sep/12 21:28,0.10.0,,,,,,,0.11,,,,impl,,,0,,,,,,,,,,,,,Pig stores Pig command line string to the Hadoop job XML file. It will fail if the command line string contains XML special characters. Pig should treat the command string like Pig script by first encoding it. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Sep/12 08:48;fang fang chen;PIG-2744.patch;https://issues.apache.org/jira/secure/attachment/12544962/PIG-2744.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-13 08:50:15.284,,,no_permission,,,,,,,,,,,,256385,Reviewed,,,Thu Sep 13 21:28:34 UTC 2012,,,,,,,0|i0h4vr:,98053,,,,,,,,,,13/Sep/12 08:50;fang fang chen;Patch files are based on pig-trunk.,13/Sep/12 21:28;daijy;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python script throws an NameError: name 'Configuration' is not defined in case cache dir is not created,PIG-2741,12559474,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,viraj,viraj,06/Jun/12 01:28,06/Jan/13 23:57,14/Mar/19 03:07,08/Jun/12 07:07,0.10.0,,,,,,,0.10.1,0.11,,,impl,,,0,,,,,,,,,,,,,"I have a Python script which writes out data to HDFS
{code}
from org.apache.hadoop.conf import *
from org.apache.hadoop.fs import *

config = Configuration()
hdfs = FileSystem.get(config)
out = hdfs.create(Path(""/user/viraj/junk.txt""))
out.write(""Hello World!"")
{code}

When I run this I get the following error:
{quote}
2012-06-06 01:20:43,101 [main] INFO  org.apache.pig.Main - Logging error messages to: /home/viraj/pig_1338945643097.log
2012-06-06 01:20:43,502 [main] INFO  org.apache.pig.Main - Run embedded script: jython
2012-06-06 01:20:43,603 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://namenode:8020
2012-06-06 01:20:44,069 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to map-reduce job tracker at: jobtracker:50300
*sys-package-mgr*: can't create package cache dir, '/mydir/xx'
2012-06-06 01:20:45,815 [main] INFO  org.apache.pig.scripting.jython.JythonScriptEngine - created tmp python.cachedir=/tmp/pig_jython_7126458276821733512
2012-06-06 01:20:45,904 [main] ERROR org.apache.pig.Main - ERROR 1121: Python Error. Traceback (most recent call last):
  File ""/homes/viraj/test.py"", line 4, in <module>
    config = Configuration()
NameError: name 'Configuration' is not defined

{quote}

I tried to solve it in various ways:

1) Override pig.properties to specify python.cachedir.skip=false but it does not seem to work

2) The only workaround is to: specify: -Dpython.cachedir=/mydirectory/tmp on the command line

Viraj",Pig 0.10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Jun/12 21:33;knoguchi;pig-2741-no-test-yet-v1.patch.txt;https://issues.apache.org/jira/secure/attachment/12531159/pig-2741-no-test-yet-v1.patch.txt,07/Jun/12 06:48;knoguchi;pig-2741-testfailing-pig2665-v2.patch.txt;https://issues.apache.org/jira/secure/attachment/12531230/pig-2741-testfailing-pig2665-v2.patch.txt,07/Jun/12 07:00;knoguchi;pig-2741-testfailing-pig2665-v3.patch.txt;https://issues.apache.org/jira/secure/attachment/12531233/pig-2741-testfailing-pig2665-v3.patch.txt,07/Jun/12 09:11;daijy;pig-2741-testfailing-pig2665-v4.patch.txt;https://issues.apache.org/jira/secure/attachment/12531244/pig-2741-testfailing-pig2665-v4.patch.txt,08/Jun/12 03:27;knoguchi;pig-2741-testfailing-pig2665-v5.patch.txt;https://issues.apache.org/jira/secure/attachment/12531363/pig-2741-testfailing-pig2665-v5.patch.txt,08/Jun/12 01:40;daijy;pig-2741-testfailing-pig2665-v5.patch.txt;https://issues.apache.org/jira/secure/attachment/12531357/pig-2741-testfailing-pig2665-v5.patch.txt,08/Jun/12 03:41;knoguchi;pig-2741-testfailing-pig2665-v6.patch.txt;https://issues.apache.org/jira/secure/attachment/12531365/pig-2741-testfailing-pig2665-v6.patch.txt,,,,,7.0,,,,,,,,,,,,,,,,,,,2012-06-06 05:17:27.457,,,no_permission,,,,,,,,,,,,242349,Reviewed,,,Fri Jun 08 07:07:30 UTC 2012,,,,,,,0|i02tz3:,14457,,,,,,,,,,"06/Jun/12 05:17;daijy;The problem is: user don't have privilege in default python.cachedir, the workaround is to give a python.cachedir with privilege. ","06/Jun/12 19:31;knoguchi;Note that ""can't create"" error is coming before JythonScriptEngine is creating the python.cachedir dir.

bq. sys-package-mgr: can't create package cache dir, '/mydir/xx'
bq. 2012-06-06 01:20:45,815 [main] INFO org.apache.pig.scripting.jython.JythonScriptEngine - created tmp python.cachedir=/tmp/pig_jython_7126458276821733512

This seems to be a regression from 
   PIG-2548: Support for providing parameters to python script

bq. src/org/apache/pig/scripting/jython/JythonScriptEngine.java
bq. 359                 PythonInterpreter.initialize(null, null, argv);

such that this initialization is called before python.cachedir is being updated by the JythonScriptEngine leading to this error.
",06/Jun/12 21:33;knoguchi;Took out the PythonInterpreter.initialize and delayed the argument setting to later.  Would this work?,"07/Jun/12 00:16;daijy;Patch looks good for me. But the weird thing is the script fail with the same error message if I apply the patch PIG-2665.

To make sure later patch does not break this script, please add a test case. I will take a look of PIG-2665.","07/Jun/12 01:05;rohini;>The problem is: user don't have privilege in default python.cachedir, the workaround is to give a > python.cachedir with privilege.
   
PythonInterpreter.initialize calls PMPySystemState.initCacheDirectory which does cachedir = new File(props.getProperty(PYTHON_CACHEDIR, CACHEDIR_DEFAULT_NAME));  This tries to create the cachedir in the same directory as the pig installation because PYTHON_CACHEDIR is not set by then. For it to work, we need a cachedir directory in the same directory as the pig.jar with 777 permissions which is not desirable. ","07/Jun/12 01:08;rohini;Also, because the default cache directory is always pig_lib_dir/cachedir/packages, it might cause issues if multiple users are running the scripts. Koji's fix will get us back to the old code path in JythonScriptEngine, which generated and set a random directory for python cache directory.","07/Jun/12 05:13;knoguchi;bq. But the weird thing is the script fail with the same error message if I apply the patch PIG-2665.
This jira was failing because python.cachedir was set to incorrect path when initialized.
For PIG-2665 with jython-standalone-2.5.2.jar, it seems to be failing due to 'python.cachedir.skip' somehow set to true as default.
Error message is same but the cause is different.

bq. To make sure later patch does not break this script, please add a test case. 
Adding testcase for PIG-2665 failing is probably easy.  As for this jira, I don't know of a good way.  Owner of jython-2.5.0.jar (dir) and the user of the test needs to be different for this issue to happen. When I manually tested, I just mkdir ./build/ivy/lib/Pig/cachedir ; followed by chmod 000 ./build/ivy/lib/Pig/cachedir. ","07/Jun/12 06:48;knoguchi;bq. Adding testcase for PIG-2665 failing is probably easy. As for this jira, 

Added a testcase that would fail when tried with pig-2665 patch. 

For testing this jira itself, I manually tested it by 

$ chmod 000 ./build/ivy/lib/Pig/cachedir

and confirmed it fails with ""NameError: name 'Configuration' is not defined""
and succeeds after the patch


","07/Jun/12 07:00;knoguchi;Daniel asked me to add a brief comment on what the test is doing.  Updated.

Also, forgot to mention that during the testing, I found out that PIG_CMD_ARGS_REMAINDERS could be null when called from the unit test.  Added extra checking with warning to avoid the NPE.","07/Jun/12 08:56;daijy;The test case only applicable to pig trunk. If we plan to commit to 0.10 branch, need make a test case for both 0.10 and trunk.",07/Jun/12 09:11;daijy;Add a different test case which applicable to ,07/Jun/12 09:11;daijy;both 0.10 and trunk.,"08/Jun/12 01:26;rohini;Daniel,
   You seem to have put a hardcoded path in the test by mistake.
 +sys.path.append(""/Users/daijy/hadoop-1.0.0/hadoop-core-1.0.0.jar"")",08/Jun/12 01:40;daijy;That's an accident. Reattach the patch. Thanks.,"08/Jun/12 03:27;knoguchi;Thanks Daniel for adding e2e testcase. 
Added 1 line to the testase so that it would now fail without this patch.
{noformat}
#jython uses 'python.home'/cachedir when python.cachedir is not specified.
#To test python.cachedir is set correctly by the framework,
#setting python.home to a random path
                        'java_params' => ['-Dpython.home=/dev/null/fake'],
{noformat}

Confirmed that this test case
i) Fails without the patch (due to using /dev/null/fake as the cache dir)
ii) Succeeds with the patch (by using cache dir set by the framework.)
iii) Fails with PIG-2665 current patch due to 'python.cachedir.skip set to true in a standalone mode.
","08/Jun/12 03:41;knoguchi;Hopefully this is the last one. Took out the 

bq. +sys.path.append(""/Users/daijy/hadoop-1.0.0/hadoop-core-1.0.0.jar"")",08/Jun/12 07:07;daijy;Patch committed to both 0.10 branch and trunk. Thanks Koji!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyList should map to Bag automatically in Jython,PIG-2739,12559290,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,04/Jun/12 23:06,06/Jan/13 23:57,14/Mar/19 03:07,06/Jun/12 05:10,0.10.0,0.11,,,,,,0.10.1,0.11,,,impl,,,0,,,,,,,,,,,,,"The following script does not work:
{code}
register 'util.py' using jython as util;
A = load '1.txt' as (sentence:chararray);
B = foreach A generate flatten(util.tokenize(sentence));
dump B;
{code}

util.py
{code}
outputSchema(""words:{(word:chararray)}"")
def tokenize(sentence):
    return sentence.split(' ')
{code}

Error message:
org.apache.pig.backend.executionengine.ExecException: ERROR 2078: Caught error from UDF: org.apache.pig.scripting.jython.JythonFunction [Error executing function]
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:288)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:304)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:332)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:353)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:294)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:273)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:268)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)
Caused by: java.io.IOException: Error executing function
	at org.apache.pig.scripting.jython.JythonFunction.exec(JythonFunction.java:122)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:262)
	... 11 more
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 0: Cannot convert jython type (org.python.core.PyList) to pig datatype java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.pig.data.Tuple
	at org.apache.pig.scripting.jython.JythonUtils.pythonToPig(JythonUtils.java:113)
	at org.apache.pig.scripting.jython.JythonFunction.exec(JythonFunction.java:117)
	... 12 more
Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.pig.data.Tuple
	at org.apache.pig.scripting.jython.JythonUtils.pythonToPig(JythonUtils.java:69)
	... 13 more

The problem is Pig expects a tuple inside a list, which is unintuitive in Python.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Jun/12 23:10;daijy;PIG-2739-0.patch;https://issues.apache.org/jira/secure/attachment/12530869/PIG-2739-0.patch,06/Jun/12 05:06;daijy;PIG-2739-1.patch;https://issues.apache.org/jira/secure/attachment/12531068/PIG-2739-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-06-05 22:25:04.448,,,no_permission,,,,,,,,,,,,242350,Reviewed,,,Wed Jun 06 05:10:21 UTC 2012,,,,,,,0|i02tzj:,14459,,,,,,,,,,"05/Jun/12 22:25;julienledem;This looks good to me.
Don't forget to update the comment right above the changed code:
{noformat}
// In jython, list need not be a bag of tuples, as it is in case of pig
// So we fail with cast exception if we dont find tuples inside bag
// This is consistent with java udf (bag should be filled with tuples)
{noformat}","06/Jun/12 05:06;daijy;Add test case, and change the comment as Julien point out.",06/Jun/12 05:10;daijy;Patch committed to both 0.10/trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TFileStorage getStatistics incorrectly throws an exception instead of returning null,PIG-2730,12558648,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,traviscrawford,traviscrawford,traviscrawford,30/May/12 17:36,06/Jan/13 23:57,14/Mar/19 03:07,30/May/12 23:04,,,,,,,,0.10.1,0.11,,,,,,0,,,,,,,,,,,,,"[TFileStorage.java|http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/impl/io/TFileStorage.java] throws an exception in getStatistics:

{code}
@Override
public ResourceStatistics getStatistics(String location, Job job) throws IOException {
  throw new UnsupportedOperationException();
}
{code}

However the interface for [LoadMetadata.java|http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/LoadMetadata.java] states null should be returned if statistics are not available:

{code}
/**
 * Get statistics about the data to be loaded.  If no statistics are
 * available, then null should be returned. If the implementing class also extends
 * {@link LoadFunc}, then {@link LoadFunc#setLocation(String, org.apache.hadoop.mapreduce.Job)}
 * is guaranteed to be called before this method.
 * @param location Location as returned by 
 * {@link LoadFunc#relativeToAbsolutePath(String, org.apache.hadoop.fs.Path)}
 * @param job The {@link Job} object - this should be used only to obtain 
 * cluster properties through {@link Job#getConfiguration()} and not to set/query
 * any runtime job information.  
 * @return statistics about the data to be loaded.  If no statistics are
 * available, then null should be returned.
 * @throws IOException if an exception occurs while retrieving statistics
 */
ResourceStatistics getStatistics(String location, Job job) 
throws IOException;
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30/May/12 22:27;traviscrawford;PIG-2730.1.patch;https://issues.apache.org/jira/secure/attachment/12530287/PIG-2730.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-30 20:15:19.052,,,no_permission,,,,,,,,,,,,242351,Reviewed,,,Wed May 30 23:04:34 UTC 2012,,,,,,,0|i02tzr:,14460,,,,,,,,,,"30/May/12 20:15;dvryaboy;Yep, implementation is wrong. Travis, can you submit a patch for this?","30/May/12 21:08;daijy;Yes, similar to BinStorage, should return null instead.",30/May/12 23:04;daijy;Patch committed to 0.10/trunk. Thanks Travis!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Macro expansion does not use pig.import.search.path - UnitTest borked,PIG-2729,12558593,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,johannesch,johannesch,johannesch,30/May/12 13:39,06/Jan/13 23:57,14/Mar/19 03:07,01/Aug/12 21:37,0.10.0,0.9.2,,,,,,0.10.1,0.11,,,,,,0,,,,,,,,,,,,,"org.apache.pig.test.TestMacroExpansion, in function importUsingSearchPathTest the import statement is provided with the full path to /tmp/mytest2.pig so the pig.import.search.path is never used. I changed the import to 

import 'mytest2.pig';

and ran the UnitTest again. This time the test failed as expected from my experience from earlier this day trying in vain to get pig eat my pig.import.search.path property! Other properties in the same custom properties file (provided via -propertyFile command line option) like udf.import.list get read without any problem.","pig-0.9.2 and pig-0.10.0, hadoop-0.20.2 from Clouderas distribution cdh3u3 on Kubuntu 12.04 64Bit.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Jul/12 08:22;johannesch;PIG-2729.patch;https://issues.apache.org/jira/secure/attachment/12537813/PIG-2729.patch,23/Jul/12 13:41;johannesch;PIG-2729.patch;https://issues.apache.org/jira/secure/attachment/12537565/PIG-2729.patch,20/Jul/12 11:15;johannesch;PIG-2729.patch;https://issues.apache.org/jira/secure/attachment/12537329/PIG-2729.patch,17/Jul/12 16:48;johannesch;PIG-2729.patch;https://issues.apache.org/jira/secure/attachment/12536846/PIG-2729.patch,31/May/12 13:09;johannesch;test-macros.tar.gz;https://issues.apache.org/jira/secure/attachment/12530385/test-macros.tar.gz,31/May/12 16:37;johannesch;use-search-path-for-imports.patch;https://issues.apache.org/jira/secure/attachment/12530407/use-search-path-for-imports.patch,,,,,,6.0,,,,,,,,,,,,,,,,,,,2012-07-16 21:58:45.249,,,no_permission,,,,,,,,,,,,256375,Reviewed,,,Wed Aug 01 21:41:10 UTC 2012,,,,,,,0|i0h4q7:,98028,Import search path property pig.import.search.path is now correctly used...,,,,,,,,,"31/May/12 13:09;johannesch;Attached scripts to further illustrate this issue.

To test this extract to your home. Then execute ./run-me .","31/May/12 16:34;johannesch;This should fix the issue. I ran the tests in org.apache.pig.test.TestMacroExpansion against this. Something is really borked here, since the tests seem to be depending on each other. The (fixed, see above) test importUsingSearchPathTest now completes successfully. The test importTwoFilesTest fails however, when executed together with the other tests. but also succeeds if run as the only test in the class. Renaming mytest1.pig and mytest2.pig in the test to something different made it pass also, so it might be a cleanup issue.

So this patch should definitely work, but the tests for the class should be reworked and included in the test/unit-tests file. I will open a separate issue on that.",31/May/12 16:35;johannesch;Sorry forgot patch! Will resubmit...,"31/May/12 16:36;johannesch;This should fix the issue. I ran the tests in org.apache.pig.test.TestMacroExpansion against this. Something is really borked here, since the tests seem to be depending on each other. The (fixed, see above) test importUsingSearchPathTest now completes successfully. The test importTwoFilesTest fails however, when executed together with the other tests. but also succeeds if run as the only test in the class. Renaming mytest1.pig and mytest2.pig in the test to something different made it pass also, so it might be a cleanup issue.
So this patch should definitely work, but the tests for the class should be reworked and included in the test/unit-tests file. I will open a separate issue on that.",31/May/12 16:37;johannesch;I am sorry for all the noise!,"16/Jul/12 21:58;rohini;Johannes,
   Do you have the updated patch which fixes the unit tests also? The static map in QueryParserDriver which caches the macro files is the reason for the test failure once the importUsingSearchPathTest is corrected.
 
{code}
 String srchPath = pigContext.getProperties().getProperty(""pig.import.search.path"");
+                if (!fname.startsWith(""/"") && !fname.startsWith(""."") && srchPath != null) {
+                    String[] paths = srchPath.split("","");
+                    for (String path : paths) {
+                        String resolvedPath = path + File.separator + fname;
+                        if ((new File(resolvedPath)).exists()) {
+                            fname = resolvedPath;
+                            break;
+                        }
+                    }
+                }
{code}

  The above code also does not handle ""../"". If the script had ""import ../readme.pig"" and pig.import.search.path was ""/x/y"", then /x/y/../readme.pig will be searched for. Also need to remove the redundant QueryParserUtils.getImportScriptAsReader code as getMacroFile will now return a absolute path. ","17/Jul/12 16:48;johannesch;Hi Rohini,

I attached a new patch that fixes the issue with relative paths. It also includes the patch for TestMacroExpansion. Further I removed the redundant code in QueryParserUtils. The tests in TestQueryParser and TestMacroExpansion all succeed. All tests should be run though, to verify that I have not broken other things...

As to the failing importTwoFilesTest I mentioned earlier: I cannot reproduce this right now. Could you verify that the tests work as expected Rohini?

Thanks,
Johannes","17/Jul/12 17:40;rohini;Johannes,
  You need to do ""git diff --no-prefix"" to get the patch so that it can be applied on svn. And it would be easier to review if you can also post the patch in reviewboard.

1) Small nitpick. The paths variable need not be defined outside the if block. 
{code}
String[] paths = {};
+                if (srchPath != null) {
+                    paths = srchPath.split("",""); //Could just be String[] paths = srchPath.split("","");
{code}

2) The problem I mentioned in the previous comment about ""../"" still exists. You have also removed the f.exists() || f.isAbsolute() || scriptPath.startsWith(""./"") checks. They are required. Now the search will be looking at the wrong paths even if the file existed or was an absolute path and makes the behavior unpredictable. For eg: If there was a statement, import '/dir1/file1.pig' and the pig.import.search.path was '/dir2,/dir3', then you would be considering files /dir2/dir1/file1.pig and /dir3/dir1/file1.pig. The same thing will happen for relative paths from base dir and ./ and ../. 

3) Doing a canonical path for f.exists() is not required. canonicalize results internally in a native call and would just add overhead.

Thanks,
Rohini","18/Jul/12 10:14;johannesch;Hi Rohini,

sorry I didn't know about svn having problems with git diffs - I will fix that and post patches on reviewboard from now on...

1. Will fix that.
2. Sorry, I think I missunderstood you: I somehow thought that you wanted /x/y/../readme.pig beeing resolved to /x/readme.pig for ""import '../readme.pig'; if the import path contained /x/y. This is obvoiusly not the case... So the correct behaviour is:

    1. Check if fname points to an existing file, is absolute, or relative
        yes => return localFileRet
        no => goto 2.
    2. For each importPath in pig.import.search.path
        - concatenate: importPath + File.separator + fname
        - check if file exists
            yes => return localFileRet
            no => continue with 2.
    3. Return localFileRet for fname.
    4. While doing 1.-3. throw RuntimeException if an IOException was encountered

Have I understood this correctly?

3. Yes, i removed this.

Thanks,
Johannes","18/Jul/12 17:51;rohini;No worries. For future reference, https://cwiki.apache.org/confluence/display/PIG/HowToContribute has all the instructions. Its a really nice writeup and I had found it very helpful. 

You are right with 1 and 2. It makes it same as the old logic. I was thinking it would keep it simpler if instead of copying all the logic to getMacroFile method, we could just change the name of QueryParserUtil.getImportScriptAsReader to something like getFileFromSearchImportPath and make it return a file instead of bufferedReader and null instead of FileNotFoundException. That way the logic remains exactly same and we don't have to worry about missing something.

{code}
QueryParserUtils.java
-    static BufferedReader getImportScriptAsReader(String scriptPath)
-            throws FileNotFoundException {
+    
+    static File getFileFromSearchImportPath(String scriptPath) {
         
-            return new BufferedReader(new FileReader(f));
+            return f;

-                        return new BufferedReader(new FileReader(f1));
+                        return f1;

-
-        throw new FileNotFoundException(""Can't find the Specified file ""
-                + scriptPath);
+        return null;

QueryParserDriver.java getMacroFile():
File localFile = QueryParserUtils.getFileFromSearchImportPath(fname);
localFileRet = localFile == null
 ? FileLocalizer.fetchFile(pigContext.getProperties(), fname)
   : new FetchFileRet(localFile.getCanonicalFile(), false);
{code}

  And I checked. importTwoFilesTest still fails. I had to make mytest1.pig and mytest2.pig to mytest4.pig and mytest5.pig to get it to pass. The cleaner thing to do would be to clear the cache in QueryParserDriver before each test. But I think this should be ok for now as the cache is private.

 You can verify your patch by running
ant -Djavac.args=""-Xlint -Xmaxwarns 1000""  clean jar-withouthadoop test -Dtestcase=TestMacroExpansion  -logfile /tmp/log","20/Jul/12 11:15;johannesch;Hi Rohini,

I changed the PIG to your suggestion. I would post this on the review board, but I currently get an Error 500 everytime I try to submit. Test cases in TestMacroExpansion all succeed. Could you take a look again please?

Thanks,
Johannes","20/Jul/12 16:55;rohini;Few comments:
1) Exception should not be thrown here as it will break Amazon s3 filesystem support.
{code}
File macroFile = QueryParserUtils.getFileFromSearchImportPath(fname);
+                if (macroFile == null) {
+                    throw new FileNotFoundException(""Could not find the specified file '""
+                                                    + fname + ""' using import search path"");
+                }
+                localFileRet = FileLocalizer.fetchFile(pigContext.getProperties(),
+                                                       macroFile.getAbsolutePath());
{code}
  It should be
{code}
File localFile = QueryParserUtils.getFileFromSearchImportPath(fname);
localFileRet = localFile == null
 ? FileLocalizer.fetchFile(pigContext.getProperties(), fname)
   : new FetchFileRet(localFile.getCanonicalFile(), false);
{code}
   The reason is the macro path could be fully qualified s3 or some other supported file system path. So if we could not find it in the local filesystem with getFileFromSearchImportPath, then FileLocalizer.fetchFile will take care of looking at other filesystems and downloading it locally and returning the local file path. Also it will throw the FileNotFoundException if the file is missing.

2.  Again for the same reason of s3 support, it is incorrect to use getFileFromSearchImportPath in this code. And getMacroFile already fetches the file.

{code}
FetchFileRet localFileRet = getMacroFile(fname);
File macroFile = QueryParserUtils.getFileFromSearchImportPath(
+                    localFileRet.file.getAbsolutePath());
         try {
-            in = QueryParserUtils.getImportScriptAsReader(localFileRet.file.getAbsolutePath());
+            in = new BufferedReader(new FileReader(macroFile));
{code}

should be

{code}
in = new BufferedReader(new FileReader(localFileRet.file));
{code}

3. For the tests, can you extract out the common code to a method to cut down on the repetition of code. Something like

{code}
importUsingSearchPathTest() {
   verifyImportUsingSearchPath(""/tmp/mytest2.pig"", ""mytest2.pig"", ""/tmp"");
}

importUsingSearchPathTest2() {
   verifyImportUsingSearchPath(""/tmp/mytest2.pig"", ""./mytest2.pig"", ""/tmp"");
}

importUsingSearchPathTest3() {
   verifyImportUsingSearchPath(""/tmp/mytest2.pig"", ""../mytest2.pig"", ""/tmp"");
}

importUsingSearchPathTest4() {
   verifyImportUsingSearchPath(""/tmp/mytest2.pig"", ""/tmp/mytest2.pig"", ""/foo/bar"");
}

verifyImportUsingSearchPath(String macroFilePath, String importFilePath, String importSearchPath) {
.....
}

{code}

4) negtiveUsingSearchPathTest2 and 3 are not very useful, unless some file with same name and garbage text are created in the search path location. That way we can ensure that the right file is being picked up and not the other file.","23/Jul/12 13:41;johannesch;Hi Rohini,

thank you very much for your comments! I am still new to the project so some things slip my attention - your advice and patience are much appreciated!

I attached the new patch incorporating your corrections. Could you take a look again?

On a side note: Do you know what is the matter with reviews.apache.org? I still always get the ""Error 500"" message when I try to submit my patch for review.

Thanks!
","23/Jul/12 22:10;rohini;No issues. Even I am new to pig :). I was just applying what I learnt from the previous jira that I was working on about s3 support. 

I uploaded a patch in review board yesterday and it works fine. Not sure what problem you are facing.

The patch looks good. Still have few comments though. Won't bother you more. These are the last ones from me :).

1) Can we add createFile(""/tmp/mytest2.pig"", garbageMacroContent); as the first line in importUsingSearchPathTest, importUsingSearchPathTest2 and importUsingSearchPathTest3 and createFile(""/foo/bar/tmp/mytest2.pig"", garbageMacroContent); in importUsingSearchPathTest4. Just an additional way to ensure the right file is being picked up.

2) Delete the mytest3.pig file in negativeUsingSearchPathTest, just in case negativeUsingSearchPathTest2 is executed first and a garbage mytest3.pig file is created. ie:
{code}
public void negativeUsingSearchPathTest() throws Exception {
	new File(""mytest3.pig"").delete();
    assertFalse(verifyImportUsingSearchPath(""/tmp/mytest3.pig"", ""mytest3.pig"", null));
}
{code}

3) Use Assert.assertFalse instead of assertTrue(!verifyImportUsingSearchPath(..)) in negativeTests.

4) Minor nitpick. It would be nice to declare the static variables (groupAndCountMacro and garbageMacroContent) in the beginning of the class. Not a big deal though.",25/Jul/12 08:22;johannesch;I included your corrections and good advice. I think the patch has evolved a lot thanks to you Rohini!,"25/Jul/12 18:46;rohini;Thanks Johannes. My +1. 

I will ask Daniel to review and commit it. Will also ask you to add to the contributors list in jira so that this jira can be assigned to you. ","26/Jul/12 09:22;johannesch;Thanks Rohini!

Meanwhile I was able to create a review request after a hint on the mailing list.

https://reviews.apache.org/r/6150/","01/Aug/12 21:37;daijy;Patch committed to 0.10 branch. Thanks Johannes, Rohini!",01/Aug/12 21:41;daijy;Committed to trunk as well.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigStorage Source tagging does not need pig.splitCombination to be turned off,PIG-2727,12558372,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,prkommireddi,prkommireddi,prkommireddi,28/May/12 23:32,06/Jan/13 23:57,14/Mar/19 03:07,30/May/12 01:45,0.10.0,,,,,,,0.10.1,0.11,,,,,,0,,,,,,,,,,,,,"With https://issues.apache.org/jira/browse/PIG-2462, PigStorage does not need ""pig.splitCombination to be turned off. I did not take the fix made in 2462 into account while implementing this feature. I will be making changes to PigStorage to reflect this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/May/12 23:34;prkommireddi;PIG-2727.patch;https://issues.apache.org/jira/secure/attachment/12529993/PIG-2727.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-30 01:33:57.589,,,no_permission,,,,,,,,,,,,242354,,,,Wed May 30 02:11:10 UTC 2012,,,Patch Available,,,,0|i02u0f:,14463,,,,,,,,,,30/May/12 01:33;dvryaboy;Documentation fix! Love it. +1. Will commit.,30/May/12 01:35;dvryaboy;.. except you forgot --no-prefix when generating it.,30/May/12 01:45;dvryaboy;Committed to 10.1 and 11. Thanks,30/May/12 02:11;prkommireddi;Thanks Dmitriy.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong output generated while loading bags as input,PIG-2721,12557440,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,vivekp,vivekp,24/May/12 13:34,06/Jan/13 23:57,14/Mar/19 03:07,30/May/12 07:15,0.10.0,0.11,0.9.0,0.9.2,,,,0.10.1,0.11,0.9.3,,,,,0,,,,,,,,,,,,,"{code}
A = LOAD '/user/pvivek/sample' as (id:chararray,mybag:bag{tuple(bttype:chararray,cat:long)});
B = foreach A generate id,FLATTEN(mybag) AS (bttype, cat);
C = order B by id;
dump C;
{code}

The above code generates wrong results when executed with Pig 0.10 and Pig 0.9
The below is the sample input;
{code}
...LKGaHqg--	{(aa,806743)}
..0MI1Y37w--	{(aa,498970)}
..0bnlpJrw--	{(aa,806740)}
..0p0IIhbA--	{(aa,498971),(se,498995)}
..1VkGqvXA--	{(aa,805219)}
{code}

I think the Pig optimizers are causing this issue.From the logs I can see that the $1 is pruned for the relation A.

[main] INFO  org.apache.pig.newplan.logical.rules.ColumnPruneVisitor - Columns pruned for A: $1

One workaround for this is to disable -t ColumnMapKeyPrune.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/May/12 14:21;knoguchi;pig-2721-trunk-notestyet.patch;https://issues.apache.org/jira/secure/attachment/12529717/pig-2721-trunk-notestyet.patch,25/May/12 19:44;knoguchi;pig-2721-trunk-withtest_v1.patch;https://issues.apache.org/jira/secure/attachment/12529777/pig-2721-trunk-withtest_v1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-05-25 14:04:16.873,,,no_permission,,,,,,,,,,,,242353,Reviewed,,,Tue Aug 07 17:53:28 UTC 2012,,,,,,,0|i02u07:,14462,,,,,,,,,,"25/May/12 14:04;knoguchi;When running the query, it dumps
{noformat}
(...LKGaHqg--)
(..0MI1Y37w--)
(..0bnlpJrw--)
(..0p0IIhbA--)
(..1VkGqvXA--)
{noformat} 

but when running with -t ColumnMapKeyPrune or -t PushDownForEachFlatten, it gives the correct output of 
{noformat}
(...LKGaHqg--,aa,806743)
(..0MI1Y37w--,aa,498970)
(..0bnlpJrw--,aa,806740)
(..0p0IIhbA--,aa,498971)
(..0p0IIhbA--,se,498995)
(..1VkGqvXA--,aa,805219)
{noformat}

","25/May/12 14:21;knoguchi;Taking the logical plan when used with -t ColumnMapKeyPrune

{noformat}
#-----------------------------------------------
# New Logical Plan:
#-----------------------------------------------
C: (Name: LOStore Schema: id#11:chararray,bttype#14:chararray,cat#15:long)
|
|---B: (Name: LOForEach Schema: id#11:chararray,bttype#14:chararray,cat#15:long)
    |   |
    |   (Name: LOGenerate[false,true] Schema: id#11:chararray,bttype#14:chararray,cat#15:long)
    |   |   |
    |   |   id:(Name: Project Type: chararray Uid: 11 Input: 0 Column: (*))
    |   |   |
    |   |   mybag:(Name: Project Type: bag Uid: 12 Input: 1 Column: (*))   <==*HERE1*
    |   |
    |   |---(Name: LOInnerLoad[0] Schema: id#11:chararray)
    |   |
    |   |---mybag: (Name: LOInnerLoad[1] Schema: bttype#14:chararray,cat#15:long)
    |
    |---C: (Name: LOSort Schema: id#11:chararray,bttype#14:chararray,cat#15:long) <==*HERE2*
        |   |
        |   id:(Name: Project Type: chararray Uid: 11 Input: 0 Column: 0)
        |
        |---A: (Name: LOForEach Schema: id#11:chararray,mybag#12:bag{#18:tuple(bttype#14:chararray,cat#15:long)})
            |   |
            |   (Name: LOGenerate[false,false] Schema: id#11:chararray,mybag#12:bag{#18:tuple(bttype#14:chararray,cat#15:long)})
            |   |   |
            |   |   (Name: Cast Type: chararray Uid: 11)
            |   |   |
            |   |   |---id:(Name: Project Type: bytearray Uid: 11 Input: 0 Column: (*))
            |   |   |
            |   |   (Name: Cast Type: bag Uid: 12)
            |   |   |
            |   |   |---mybag:(Name: Project Type: bytearray Uid: 12 Input: 1 Column: (*))
            |   |
            |   |---(Name: LOInnerLoad[0] Schema: id#11:bytearray)
            |   |
            |   |---(Name: LOInnerLoad[1] Schema: mybag#12:bytearray)
            |
            |---A: (Name: LOLoad Schema: id#11:bytearray,mybag#12:bytearray)RequiredFields:null

{noformat}


Tracing the ColumnPrune*, use of 'Uid:12' gets lost at first LOGenerate (*HERE*) part when its projection refers to LOSort (*HERE2*) and checks the schema.

Looking further, LOSort schema was not getting updated by SchemaPatcher when PushDownForEachFlatten swapped ForEach and Sort.  This was due to PushDownForEachFlatten.reportChange() not passing the changes it made.

I believe attached patch fixes this issue.
Confirmed logical plan now changes to 
{noformat}
$ diff /tmp/before /tmp/after
18c18
<     |---C: (Name: LOSort Schema: id#11:chararray,bttype#14:chararray,cat#15:long)
---
>     |---C: (Name: LOSort Schema: id#11:chararray,mybag#12:bag{#18:tuple(bttype#14:chararray,cat#15:long)})
{noformat}

and it produces the correct output.",25/May/12 19:44;knoguchi;Added a test case.,"25/May/12 19:46;knoguchi;This affects 0.9, 0.10, and trunk.","25/May/12 21:22;daijy;Thanks Koji, you find the right fix. We forget to add changed operator to subPlan in PushDownForEachFlatten. Will commit when tests pass.","30/May/12 07:15;daijy;All unit tests pass. Patch committed to 0.10/trunk.

Thanks, Koji!",07/Aug/12 17:53;daijy;Committed to 0.9 branch as well as per requested by Rohini.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tuple field mangled during flattening,PIG-2717,12556938,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,daijy,smalltalk80,smalltalk80,22/May/12 23:21,06/Jan/13 23:57,14/Mar/19 03:07,30/May/12 22:58,0.10.0,0.9.2,,,,,,0.10.1,0.11,,,,,,0,,,,,,,,,,,,,"1.txt:
1, 2, 3
4, 5, 6
7, 8, 9

a = load '1.txt' using PigStorage(',') as (x:int, y:int, z:int);
b = foreach a generate TOTUPLE(x, y) as t, z;
c = group b by t;
d = foreach c generate flatten(b);
e = foreach d generate t;

Describing d shows the schema as d: {b::t: (x: int,y: int),b::z: int}
However, when dumping d the output is
(1,3)
(4,6)
(7,9)

Describing e shows the schema as e: {b::t: (x: int,y: int)}
However, when dumping I got the following error:
java.lang.ClassCastException: java.lang.Integer cannot be cast to org.apache.pig.data.Tuple",RHEL5 (Linux 2.6.18-164.el5 #1 SMP Tue Aug 18 15:51:54 EDT 2009 i686 i686 i386 GNU/Linux),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/May/12 02:10;daijy;PIG-2717-1.patch;https://issues.apache.org/jira/secure/attachment/12529561/PIG-2717-1.patch,30/May/12 22:56;daijy;PIG-2717-2.patch;https://issues.apache.org/jira/secure/attachment/12530293/PIG-2717-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-05-23 08:10:30.499,,,no_permission,,,,,,,,,,,,242352,Reviewed,,,Wed May 30 22:58:44 UTC 2012,,,,,,,0|i02tzz:,14461,,,,,,,,,,23/May/12 08:10;daijy;This seems to be a bug in POPackageAnnotator. I will submit a patch shortly.,27/May/12 21:46;dvryaboy;+1,30/May/12 22:56;daijy;Fix a unit test failure.,30/May/12 22:58;daijy;Patch committed to 0.10/trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig documentation on TOP funcation has issues ,PIG-2714,12556642,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,daijy,bejoyks,bejoyks,21/May/12 05:34,22/Feb/13 04:53,14/Mar/19 03:07,22/May/12 21:55,0.9.2,,,,,,,0.11,,,,documentation,,,0,,,,,,,,,,,,,"The following documentation gives a picture that the columns are indexed from 1 but is actually indexed from 0.

http://pig.apache.org/docs/r0.8.1/piglatin_ref2.html#TOP

Specifically on the sample code snippet
result = TOP(10, 2, C); // and retain top 10 occurrences of 'second' in first

'second' is the second column, in that case we need to have the TOP function as TOP(10, 1, C) instead of TOP(10, 2, C)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/May/12 21:55;daijy;PIG-2714-1.patch;https://issues.apache.org/jira/secure/attachment/12528658/PIG-2714-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-22 21:55:00.985,,,no_permission,,,,,,,,,,,,248760,Reviewed,,,Tue May 22 21:55:00 UTC 2012,,,,,,,0|i09ycn:,56006,,,,,,,,,,22/May/12 21:55;daijy;Add clarification to the doc. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig does not call OutputCommitter.abortJob() on the underlying OutputFormat,PIG-2712,12556638,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,toffer,toffer,21/May/12 04:55,06/Jan/13 23:56,14/Mar/19 03:07,20/Sep/12 04:06,0.9.3,,,,,,,0.10.1,0.11,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HCATALOG-451,,,,,,,,,,,,,22/May/12 23:14;daijy;PIG-2712-0.patch;https://issues.apache.org/jira/secure/attachment/12528669/PIG-2712-0.patch,06/Sep/12 06:31;rohini;PIG-2712-1-branch10.patch;https://issues.apache.org/jira/secure/attachment/12543994/PIG-2712-1-branch10.patch,06/Sep/12 06:31;rohini;PIG-2712-1-trunk.patch;https://issues.apache.org/jira/secure/attachment/12543995/PIG-2712-1-trunk.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-05-22 23:08:29.936,,,no_permission,,,,,,,,,,,,256363,,,,Thu Sep 20 04:06:28 UTC 2012,,,,,,,0|i0h4jr:,97999,,,,,,,,,,"22/May/12 23:08;daijy;Yes, we are not calling abortJob. Actually abortJob does not exist in 20.2. We shall add the hook now.",22/May/12 23:14;daijy;Attach a draft patch.,"23/Jul/12 21:12;rohini;Daniel,

The signature of abortJob is
{code}
   public void abortJob(JobContext jobContext, JobStatus.State state)
{code}

 The method API and the reflection invocation in the patch need to be corrected. ",05/Sep/12 21:19;toffer;I'm going to take a crack at this patch.,"05/Sep/12 21:48;toffer;Just spoke with Rohini offline, she has a patch for this. Which just needs to be tested on 0.20.2.",06/Sep/12 06:31;rohini;Tested with 20.2 jars in classpath and invoking cleanupJob. It does not throw any error for JobStatus.,"10/Sep/12 00:04;toffer;+1 (non-binding), I've tested the 0.10 patch on hadoop-0.23 and verified it fixes HCATALOG-451.",19/Sep/12 19:51;alangates;Patch looks good.  I'm running the tests and will commit it if passes.,20/Sep/12 03:08;alangates;Committed on trunk.  I'll test it on 0.10 next.,20/Sep/12 04:06;alangates;branch10 patch checked into branch-10.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce the number of instances of Load and Store Funcs down to 2+1. It should be 1 in the front-end and 1 in the backend,PIG-2699,12555163,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,julienledem,julienledem,julienledem,14/May/12 17:37,24/May/17 16:20,14/Mar/19 03:07,19/Jun/12 00:33,0.10.0,,,,,,,0.11,,,,internal-udfs,,,0,,,,,,,,,,,,,"Attached: a patch to get it down to 3
Here is the report of the remaining calls.
some methods are unnecessarily called multiple times, this should be improved as well.
{noformat}
A = LOAD 'foo' USING TestLoadStoreFuncLifeCycle$Loader();
STORE A INTO 'bar' USING TestLoadStoreFuncLifeCycle$Storer();


report:
3 instances of Loader
20 calls to Loader
3 instances of Storer
24 calls to Storer

all calls:
Loader[1].<init>()
Loader[1].relativeToAbsolutePath(foo, file:/Users/julien/svn/pig/trunk-LoadStoreFunc-lifecycle)
Loader[1].setUDFContextSignature(A_1-0)
Loader[1].getSchema(foo, org.apache.hadoop.mapreduce.Job@7ee49dcd)
Storer[1].<init>()
Storer[1].setStoreFuncUDFContextSignature(A_1-1)
Storer[1].relToAbsPathForStoreLocation(bar, file:/Users/julien/svn/pig/trunk-LoadStoreFunc-lifecycle)
Storer[1].setStoreLocation(bar, org.apache.hadoop.mapreduce.Job@776be68f)
Storer[1].getOutputFormat()
Loader[1].getStatistics(foo, org.apache.hadoop.mapreduce.Job@11e9c82e)
Loader[1].setLocation(foo, org.apache.hadoop.mapreduce.Job@11e9c82e)
Storer[1].setStoreLocation(bar, org.apache.hadoop.mapreduce.Job@57d840cd)
Storer[2].<init>()
Storer[2].setStoreFuncUDFContextSignature(A_1-1)
Storer[2].setStoreLocation(bar, org.apache.hadoop.mapreduce.Job@76996cca)
Storer[2].getOutputFormat()
Loader[2].<init>()
Loader[2].setUDFContextSignature(A_1-0)
Loader[2].setLocation(foo, org.apache.hadoop.mapreduce.Job@317cfd38)
Loader[2].getInputFormat()
Storer[3].<init>()
Storer[3].setStoreFuncUDFContextSignature(A_1-1)
Storer[3].setStoreLocation(bar, org.apache.hadoop.mapreduce.Job@459d3b3a)
Storer[3].getOutputFormat()
Storer[3].setStoreLocation(bar, org.apache.hadoop.mapreduce.Job@225f1ae9)
Loader[3].<init>()
Loader[3].setUDFContextSignature(A_1-0)
Loader[3].setLocation(foo, org.apache.hadoop.mapreduce.Job@6b98e8b4)
Loader[3].getInputFormat()
Storer[3].setStoreLocation(bar, org.apache.hadoop.mapreduce.Job@5fb11b79)
Storer[3].getOutputFormat()
Storer[3].prepareToWrite(org.apache.pig.builtin.mock.Storage$MockRecordWriter@49b09282)
Loader[3].setUDFContextSignature(A_1-0)
Loader[3].prepareToRead(org.apache.pig.builtin.mock.Storage$MockRecordReader@2c8c7d6, Number of splits :1...)
Loader[3].getNext()
Storer[3].putNext((a))
Loader[3].getNext()
Storer[3].putNext((b))
Loader[3].getNext()
Storer[3].putNext((c))
Loader[3].getNext()
Storer[3].setStoreLocation(bar, org.apache.hadoop.mapreduce.Job@3ebfbbe3)
Storer[3].setStoreLocation(bar, org.apache.hadoop.mapreduce.Job@14d964af)
Storer[1].setStoreLocation(bar, org.apache.hadoop.mapreduce.Job@644ca6b6)

constructor calls:
Loader[1].<init> called by 
org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:565)
org.apache.pig.parser.LogicalPlanBuilder.buildLoadOp(LogicalPlanBuilder.java:426)
org.apache.pig.parser.LogicalPlanGenerator.load_clause(LogicalPlanGenerator.java:3170)
org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1293)
org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:791)
org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:509)
org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:384)
org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:175)
org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1602)
org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1549)
org.apache.pig.PigServer.registerQuery(PigServer.java:534)
org.apache.pig.PigServer.registerQuery(PigServer.java:547)
Storer[1].<init> called by 
org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:565)
org.apache.pig.parser.LogicalPlanBuilder.buildStoreOp(LogicalPlanBuilder.java:486)
org.apache.pig.parser.LogicalPlanGenerator.store_clause(LogicalPlanGenerator.java:6336)
org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1337)
org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:791)
org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:509)
org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:384)
org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:175)
org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1602)
org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1549)
org.apache.pig.PigServer.registerQuery(PigServer.java:534)
org.apache.pig.PigServer.registerQuery(PigServer.java:547)
Storer[2].<init> called by 
org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:565)
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore.getStoreFunc(POStore.java:232)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.setLocation(PigOutputFormat.java:168)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.checkOutputSpecsHelper(PigOutputFormat.java:200)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.checkOutputSpecs(PigOutputFormat.java:187)
org.apache.pig.backend.hadoop20.PigJobControl.mainLoopAction(PigJobControl.java:157)
org.apache.pig.backend.hadoop20.PigJobControl.run(PigJobControl.java:134)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:258)
Loader[2].<init> called by 
org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:565)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:254)
org.apache.pig.backend.hadoop20.PigJobControl.mainLoopAction(PigJobControl.java:157)
org.apache.pig.backend.hadoop20.PigJobControl.run(PigJobControl.java:134)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:258)
Storer[3].<init> called by 
org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:565)
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore.getStoreFunc(POStore.java:232)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter.getCommitters(PigOutputCommitter.java:84)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter.<init>(PigOutputCommitter.java:66)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.getOutputCommitter(PigOutputFormat.java:279)
Loader[3].<init> called by 
org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:565)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getLoadFunc(PigInputFormat.java:158)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.createRecordReader(PigInputFormat.java:106)
{noformat}

In trunk this was:
{noformat}
5 instances of Loader
31 calls to Loader
6 instances of Storer
30 calls to Storer

all calls:
Loader[1].<init>()
Loader[2].<init>()
Loader[2].relativeToAbsolutePath(foo, file:/Users/julien/svn/pig/trunk)
Storer[1].<init>()
Storer[2].<init>()
Storer[2].setStoreFuncUDFContextSignature(A_bar_org.apache.pig.TestLoadStoreFuncLifeCycle$Storer)
Storer[2].relToAbsPathForStoreLocation(bar, file:/Users/julien/svn/pig/trunk)
Storer[3].<init>()
Storer[3].setStoreFuncUDFContextSignature(1-0_bar_org.apache.pig.TestLoadStoreFuncLifeCycle$Storer)
Loader[3].<init>()
Loader[3].setUDFContextSignature(A)
Loader[3].getSchema(foo, org.apache.hadoop.mapreduce.Job@4c349471)
Loader[3].getSchema(foo, org.apache.hadoop.mapreduce.Job@24c0f1ec)
Loader[3].getSchema(foo, org.apache.hadoop.mapreduce.Job@900bac2)
Loader[3].getSchema(foo, org.apache.hadoop.mapreduce.Job@635aed57)
Loader[3].getSchema(foo, org.apache.hadoop.mapreduce.Job@2d7cec96)
Loader[3].getSchema(foo, org.apache.hadoop.mapreduce.Job@4b947496)
Storer[3].setStoreFuncUDFContextSignature(A_bar_org.apache.pig.TestLoadStoreFuncLifeCycle$Storer)
Loader[3].getSchema(foo, org.apache.hadoop.mapreduce.Job@776be68f)
Loader[3].getSchema(foo, org.apache.hadoop.mapreduce.Job@560c3014)
Loader[3].getSchema(foo, org.apache.hadoop.mapreduce.Job@5773ec72)
Storer[3].setStoreLocation(bar, org.apache.hadoop.mapreduce.Job@bb273cc)
Storer[3].getOutputFormat()
Loader[3].getSchema(foo, org.apache.hadoop.mapreduce.Job@45660d6)
Loader[3].setLocation(foo, org.apache.hadoop.mapreduce.Job@d2368df)
Loader[3].getStatistics(foo, org.apache.hadoop.mapreduce.Job@d2368df)
Storer[4].<init>()
Storer[4].setStoreFuncUDFContextSignature(A_bar_org.apache.pig.TestLoadStoreFuncLifeCycle$Storer)
Storer[4].setStoreLocation(bar, org.apache.hadoop.mapreduce.Job@78ff9053)
Storer[5].<init>()
Storer[5].setStoreFuncUDFContextSignature(A_bar_org.apache.pig.TestLoadStoreFuncLifeCycle$Storer)
Storer[5].setStoreLocation(bar, org.apache.hadoop.mapreduce.Job@336d8196)
Storer[5].getOutputFormat()
Loader[4].<init>()
Loader[4].setUDFContextSignature(A)
Loader[4].setLocation(foo, org.apache.hadoop.mapreduce.Job@61250ff2)
Loader[4].getInputFormat()
Storer[6].<init>()
Storer[6].setStoreFuncUDFContextSignature(A_bar_org.apache.pig.TestLoadStoreFuncLifeCycle$Storer)
Storer[6].setStoreLocation(bar, org.apache.hadoop.mapreduce.Job@604788d5)
Storer[6].getOutputFormat()
Storer[6].setStoreLocation(bar, org.apache.hadoop.mapreduce.Job@7f342545)
Loader[5].<init>()
Loader[5].setUDFContextSignature(A)
Loader[5].setLocation(foo, org.apache.hadoop.mapreduce.Job@459d3b3a)
Loader[5].getInputFormat()
Storer[6].setStoreLocation(bar, org.apache.hadoop.mapreduce.Job@795e0c2b)
Storer[6].getOutputFormat()
Storer[6].prepareToWrite(org.apache.pig.builtin.mock.Storage$MockRecordWriter@7c34151f)
Loader[5].setUDFContextSignature(A)
Loader[5].prepareToRead(org.apache.pig.builtin.mock.Storage$MockRecordReader@62114b17, Number of splits :1...)
Loader[5].getNext()
Storer[6].putNext((a))
Loader[5].getNext()
Storer[6].putNext((b))
Loader[5].getNext()
Storer[6].putNext((c))
Loader[5].getNext()
Storer[6].setStoreLocation(bar, org.apache.hadoop.mapreduce.Job@bf47ae8)
Storer[6].setStoreLocation(bar, org.apache.hadoop.mapreduce.Job@4bb7b407)
Storer[4].setStoreLocation(bar, org.apache.hadoop.mapreduce.Job@3cee6ad6)

constructor calls:
Loader[1].<init> called by 
org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:565)
org.apache.pig.parser.LogicalPlanBuilder.validateFuncSpec(LogicalPlanBuilder.java:791)
org.apache.pig.parser.LogicalPlanBuilder.buildFuncSpec(LogicalPlanBuilder.java:780)
org.apache.pig.parser.LogicalPlanGenerator.func_clause(LogicalPlanGenerator.java:4670)
org.apache.pig.parser.LogicalPlanGenerator.load_clause(LogicalPlanGenerator.java:3117)
org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1293)
org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:791)
org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:509)
org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:384)
org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:175)
org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1602)
org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1549)
org.apache.pig.PigServer.registerQuery(PigServer.java:534)
org.apache.pig.PigServer.registerQuery(PigServer.java:547)
Loader[2].<init> called by 
org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:565)
org.apache.pig.parser.LogicalPlanBuilder.getAbolutePathForLoad(LogicalPlanBuilder.java:417)
org.apache.pig.parser.LogicalPlanBuilder.buildLoadOp(LogicalPlanBuilder.java:436)
org.apache.pig.parser.LogicalPlanGenerator.load_clause(LogicalPlanGenerator.java:3170)
org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1293)
org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:791)
org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:509)
org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:384)
org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:175)
org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1602)
org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1549)
org.apache.pig.PigServer.registerQuery(PigServer.java:534)
org.apache.pig.PigServer.registerQuery(PigServer.java:547)
Storer[1].<init> called by 
org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:565)
org.apache.pig.parser.LogicalPlanBuilder.validateFuncSpec(LogicalPlanBuilder.java:791)
org.apache.pig.parser.LogicalPlanBuilder.buildFuncSpec(LogicalPlanBuilder.java:780)
org.apache.pig.parser.LogicalPlanGenerator.func_clause(LogicalPlanGenerator.java:4670)
org.apache.pig.parser.LogicalPlanGenerator.store_clause(LogicalPlanGenerator.java:6312)
org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1337)
org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:791)
org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:509)
org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:384)
org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:175)
org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1602)
org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1549)
org.apache.pig.PigServer.registerQuery(PigServer.java:534)
org.apache.pig.PigServer.registerQuery(PigServer.java:547)
Storer[2].<init> called by 
org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:565)
org.apache.pig.parser.LogicalPlanBuilder.getAbolutePathForStore(LogicalPlanBuilder.java:478)
org.apache.pig.parser.LogicalPlanBuilder.buildStoreOp(LogicalPlanBuilder.java:499)
org.apache.pig.parser.LogicalPlanGenerator.store_clause(LogicalPlanGenerator.java:6336)
org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1337)
org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:791)
org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:509)
org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:384)
org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:175)
org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1602)
org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1549)
org.apache.pig.PigServer.registerQuery(PigServer.java:534)
org.apache.pig.PigServer.registerQuery(PigServer.java:547)
Storer[3].<init> called by 
org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:565)
org.apache.pig.newplan.logical.relational.LOStore.<init>(LOStore.java:55)
org.apache.pig.parser.LogicalPlanBuilder.buildStoreOp(LogicalPlanBuilder.java:505)
org.apache.pig.parser.LogicalPlanGenerator.store_clause(LogicalPlanGenerator.java:6336)
org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1337)
org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:791)
org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:509)
org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:384)
org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:175)
org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1602)
org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1549)
org.apache.pig.PigServer.registerQuery(PigServer.java:534)
org.apache.pig.PigServer.registerQuery(PigServer.java:547)
Loader[3].<init> called by 
org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:565)
org.apache.pig.newplan.logical.relational.LOLoad.getLoadFunc(LOLoad.java:77)
org.apache.pig.newplan.logical.relational.LOLoad.getSchemaFromMetaData(LOLoad.java:149)
org.apache.pig.newplan.logical.relational.LOLoad.getSchema(LOLoad.java:110)
org.apache.pig.newplan.logical.relational.LOStore.getSchema(LOStore.java:68)
org.apache.pig.newplan.logical.visitor.SchemaAliasVisitor.validate(SchemaAliasVisitor.java:60)
org.apache.pig.newplan.logical.visitor.SchemaAliasVisitor.visit(SchemaAliasVisitor.java:84)
org.apache.pig.newplan.logical.relational.LOStore.accept(LOStore.java:77)
org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
org.apache.pig.PigServer$Graph.compile(PigServer.java:1630)
org.apache.pig.PigServer$Graph.compile(PigServer.java:1624)
org.apache.pig.PigServer$Graph.access$2(PigServer.java:1623)
org.apache.pig.PigServer.execute(PigServer.java:1246)
org.apache.pig.PigServer.access$0(PigServer.java:1237)
org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1556)
Storer[4].<init> called by 
org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:565)
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore.getStoreFunc(POStore.java:232)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJob(JobControlCompiler.java:499)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.compile(JobControlCompiler.java:281)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:178)
org.apache.pig.PigServer.launchPlan(PigServer.java:1279)
org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1264)
org.apache.pig.PigServer.execute(PigServer.java:1254)
org.apache.pig.PigServer.access$0(PigServer.java:1237)
org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1556)
org.apache.pig.PigServer.registerQuery(PigServer.java:534)
org.apache.pig.PigServer.registerQuery(PigServer.java:547)
Storer[5].<init> called by 
org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:565)
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore.getStoreFunc(POStore.java:232)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.setLocation(PigOutputFormat.java:168)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.checkOutputSpecsHelper(PigOutputFormat.java:200)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.checkOutputSpecs(PigOutputFormat.java:187)
org.apache.pig.backend.hadoop20.PigJobControl.mainLoopAction(PigJobControl.java:157)
org.apache.pig.backend.hadoop20.PigJobControl.run(PigJobControl.java:134)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:258)
Loader[4].<init> called by 
org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:565)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:254)
org.apache.pig.backend.hadoop20.PigJobControl.mainLoopAction(PigJobControl.java:157)
org.apache.pig.backend.hadoop20.PigJobControl.run(PigJobControl.java:134)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:258)
Storer[6].<init> called by 
org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:565)
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore.getStoreFunc(POStore.java:232)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter.getCommitters(PigOutputCommitter.java:84)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter.<init>(PigOutputCommitter.java:66)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.getOutputCommitter(PigOutputFormat.java:279)
Loader[5].<init> called by 
org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:565)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getLoadFunc(PigInputFormat.java:158)
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.createRecordReader(PigInputFormat.java:106)
{noformat}",,,,,,,,,,,,,,,,,PIG-2648,,,,,,,,PIG-2809,PIG-2807,PIG-2820,PIG-2790,,,,14/May/12 17:44;julienledem;PIG-2699.patch;https://issues.apache.org/jira/secure/attachment/12526784/PIG-2699.patch,15/May/12 16:55;julienledem;PIG-2699_a.patch;https://issues.apache.org/jira/secure/attachment/12527371/PIG-2699_a.patch,15/May/12 16:56;julienledem;PIG-2699_b.patch;https://issues.apache.org/jira/secure/attachment/12527372/PIG-2699_b.patch,18/May/12 00:24;julienledem;PIG-2699_c.patch;https://issues.apache.org/jira/secure/attachment/12527963/PIG-2699_c.patch,05/Jun/12 21:07;julienledem;PIG-2699_d.patch;https://issues.apache.org/jira/secure/attachment/12531015/PIG-2699_d.patch,05/Jun/12 21:20;julienledem;PIG-2699_e.patch;https://issues.apache.org/jira/secure/attachment/12531018/PIG-2699_e.patch,19/Jun/12 00:30;julienledem;PIG-2699_f.patch;https://issues.apache.org/jira/secure/attachment/12532491/PIG-2699_f.patch,,,,,7.0,,,,,,,,,,,,,,,,,,,2012-05-15 00:14:08.803,,,no_permission,,,,,,,,,,,,239415,,,,Thu Jul 26 18:57:38 UTC 2012,,,Patch Available,,,,0|i0h4en:,97976,,,,,,,,,,"14/May/12 17:44;julienledem;PIG-2699.patch:
test/org/apache/pig/TestLoadStoreFuncLifeCycle.java
generates the report in the description.
The patch removes some of the instatiations:
* type checking: can be done with the class without creating a new instance
* LogicalPlanBuilder creates an instance to convert relative to absolute path and then creates a LogicalOperator that instantiates a new one. I changed to reuse the one just created

left to do:
the PigInputFormat (sam for Output format) get the PhysicalOperator by deserializing form the conf. There should be a lookup mechanism to get the instance of Load/StoreFunc from a registry (by LogicalOperator signature?)","15/May/12 00:14;dvryaboy;Thanks for cleaning this up.

Please add Apache headers to new file.

Minor nits:

Use log4j instead of printlns in the test.. in fact, why are there printlns in the test? Just to get the report? Should that be a separate utility?

Looks like indentation might be somewhat broken in a few places:

{code}
         if (absolutePath == null) {
-            absolutePath = stoFunc.relToAbsPathForStoreLocation( filename, 
+                absolutePath = stoFunc.relToAbsPathForStoreLocation(
{code}

","15/May/12 16:55;julienledem;PIG-2699_a.patch
addresses the comments","15/May/12 16:56;julienledem;PIG-2699_b.patch
with Apache headers","18/May/12 00:24;julienledem;PIG-2699_c.patch
* The setContextSignatures methods being called multiple times with different values I changed the logic so that the signature is unique from the begining and can not be modified.
* I refactored one of the unit test which had a lot of duplication and needed to be modified to be more stable.
* The getSchema() method was called multiple time because of a caching mechanism that did not handle null. I changed for a single initialization at the begining.
","18/May/12 00:29;julienledem;See review:
https://reviews.apache.org/r/5159/","18/May/12 00:30;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/5159/
-----------------------------------------------------------

Review request for pig.


Summary
-------

Pig creates too many instances of Load and Store Funcs. It should be 1 in the front-end and 1 in the backend


This addresses bug PIG-2699.
    https://issues.apache.org/jira/browse/PIG-2699


Diffs
-----

  /trunk/src/org/apache/pig/PigServer.java 1339793 
  /trunk/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java 1339793 
  /trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java 1339793 
  /trunk/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POStore.java 1339793 
  /trunk/src/org/apache/pig/newplan/logical/relational/LOLoad.java 1339793 
  /trunk/src/org/apache/pig/newplan/logical/relational/LOStore.java 1339793 
  /trunk/src/org/apache/pig/newplan/logical/relational/LogToPhyTranslationVisitor.java 1339793 
  /trunk/src/org/apache/pig/newplan/logical/rules/LoadStoreFuncDupSignatureValidator.java 1339793 
  /trunk/src/org/apache/pig/newplan/logical/rules/PartitionFilterOptimizer.java 1339793 
  /trunk/src/org/apache/pig/newplan/logical/rules/TypeCastInserter.java 1339793 
  /trunk/src/org/apache/pig/newplan/logical/visitor/ScalarVisitor.java 1339793 
  /trunk/src/org/apache/pig/parser/FunctionType.java 1339793 
  /trunk/src/org/apache/pig/parser/LogicalPlanBuilder.java 1339793 
  /trunk/src/org/apache/pig/parser/QueryParserUtils.java 1339793 
  /trunk/test/org/apache/pig/TestLoadStoreFuncLifeCycle.java PRE-CREATION 
  /trunk/test/org/apache/pig/parser/TestScalarVisitor.java 1339793 
  /trunk/test/org/apache/pig/test/TestInputOutputFileValidator.java 1339793 
  /trunk/test/org/apache/pig/test/TestLogToPhyCompiler.java 1339793 
  /trunk/test/org/apache/pig/test/TestLogicalPlanBuilder.java 1339793 
  /trunk/test/org/apache/pig/test/TestMRCompiler.java 1339793 
  /trunk/test/org/apache/pig/test/TestNewPlanFilterRule.java 1339793 
  /trunk/test/org/apache/pig/test/TestNewPlanListener.java 1339793 
  /trunk/test/org/apache/pig/test/Util.java 1339793 

Diff: https://reviews.apache.org/r/5159/diff


Testing
-------

test-commit + new test


Thanks,

Julien

","18/May/12 00:33;jeromatron;Nice - we've seen many instances where we just know that methods are called a bunch of times and one of those times, the argument will be non-null.  The trick has been when it's not null, keep hold of it!  Thanks for doing all of this!",19/May/12 07:43;ashutoshc;+1 thanks for doing this Julien! I was waiting since ages for someone to pick up this tricky but highly useful refactoring n cleanup.,"05/Jun/12 21:07;julienledem;PIG-2699_d.patch minor changes.
prefix signatures with alias.
separate Test helper method
","05/Jun/12 21:20;julienledem;PIG-2699_e.patch
change signature in helper method for consistency",19/Jun/12 00:30;julienledem;PIG-2699_f.patch fixing TestLoad error,"06/Jul/12 17:10;daijy;Seems TestLOLoadDeterminedSchema is broken with the patch. Julien, Do you have time to take a look?","06/Jul/12 18:53;daijy;TestNewPlanOperatorPlan, TestParser, TestPigStorage as well.",06/Jul/12 23:15;julienledem;see PIG-2790 for TestLOLoadDeterminedSchema,"09/Jul/12 18:58;daijy;Julien, how about other failures:
TestNewPlanOperatorPlan, TestParser, TestPigStorage, TestUDFContext, TestGrunt.

Are you still working on it?",09/Jul/12 20:50;julienledem;I have a patch for those. Will submit it soon.,"10/Jul/12 05:00;julienledem;see PIG-2807 
I haven't looked into TestUDFContext, TestGrunt yet","16/Jul/12 17:04;knoguchi;bq. I haven't looked into TestUDFContext, TestGrunt yet

org.apache.pig.test.TestGrunt.testCD and 
org.apache.pig.test.TestGrunt.testFsCommand failing for me. Is this related?
","17/Jul/12 17:53;julienledem;Hi Koji
This is indeed related.
TestUDFCOntext was fixed by PIG-2809
TestGrunt will be fixed by PIG-2820 ","23/Jul/12 20:36;knoguchi;bq. This is indeed related.
bq. TestUDFCOntext was fixed by PIG-2809
bq. TestGrunt will be fixed by PIG-2820
bq.
Thanks Julien!  Only one more unit test remamining.  
TestNewPlanOperatorPlan.testRelationalSameOpDifferentPreds
Is this also related?  
","26/Jul/12 18:57;julienledem;Hi Koji,
Yes this is ""new Configuration()"" picking up the config file generated for minicluster in the previous test.
I'll fix the test",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LoadFunc.setLocation should be called before LoadMetadata.getStatistics ,PIG-2693,12554832,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,billgraham,billgraham,billgraham,10/May/12 22:53,22/Feb/13 04:53,14/Mar/19 03:07,18/May/12 21:21,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"We ran into a bug with Pig/HCatalog integration on the trunk. The issue is that {{JobControlCompiler}} calls the {{adjustNumReducers}} method just before it calls {{setLocation}} on all of the {{LoadFuncs}}. This causes problems, since some loaders (i.e.,  {{HCatLoader}}) need {{setLocation}} to be called before it can respond to {{getStatistics}} with it's data size.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/May/12 23:00;billgraham;PIG-2693.1.patch;https://issues.apache.org/jira/secure/attachment/12526443/PIG-2693.1.patch,15/May/12 21:58;billgraham;PIG-2693.2.patch;https://issues.apache.org/jira/secure/attachment/12527531/PIG-2693.2.patch,18/May/12 21:18;julienledem;PIG-2693.3.patch;https://issues.apache.org/jira/secure/attachment/12528160/PIG-2693.3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-05-15 20:59:51.693,,,no_permission,,,,,,,,,,,,239085,,,,Fri May 18 21:18:51 UTC 2012,,,,,,,0|i0h4bz:,97964,,,,,,,,,,"10/May/12 22:55;billgraham;When {{HCatalog}} blows up, the stack trace looks like this:

{noformat}
2012-05-05 00:43:09,684 [main] WARN  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Couldn't get statistics from LoadFunc: com.twitter.twadoop.dal.pig.DALPigLoader@2a2096d7
java.io.IOException: java.lang.NullPointerException
        at org.apache.hcatalog.pig.HCatLoader.getStatistics(HCatLoader.java:194)
        at com.twitter.twadoop.dal.pig.DALPigLoader.getStatistics(DALPigLoader.java:135)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getInputSizeFromLoader(JobControlCompiler.java:866)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getInputSize(JobControlCompiler.java:824)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.estimateNumberOfReducers(JobControlCompiler.java:805)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.adjustNumReducers(JobControlCompiler.java:745)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJob(JobControlCompiler.java:378)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.compile(JobControlCompiler.java:264)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:149)
        at org.apache.pig.PigServer.launchPlan(PigServer.java:1265)
...
{noformat}

","10/May/12 23:00;billgraham;Attaching a patch that fixes the issue by calling setLocation before estimating the number of reducers. This passes unit tests and doesn't seem like it would have adverse effects, but please comment if you know a reason why it would.

If this approach seems sound I can write up a unit test.","15/May/12 20:59;traviscrawford;Looking into this, I confirmed the reducer estimation worked as of https://github.com/apache/pig/commit/826f4333a2546a72adebf93e5d045420d0413c69 but throws the NPE on trunk.

I'm looking through the changes since to see what's changed. Since there's no contract between {{LoadFunc}} and {{LoadMetadata}} whatever's changed is not invalid per-se, but it may be worth restoring the old behavior.","15/May/12 21:31;traviscrawford;This patch is causing the issue: 

https://github.com/apache/pig/commit/d49f903e0d041ce7d9e8e2d0a9066cd666f99da7

Looking into what's changed.

{code}
commit d49f903e0d041ce7d9e8e2d0a9066cd666f99da7
Author: Dmitriy V. Ryaboy <dvryaboy@apache.org>
Date:   Fri Apr 27 23:34:03 2012 +0000

    PIG-2652:  Skew join and order by dont trigger reducer estimation (dvryaboy)
    
    git-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1331637 13f79535-47bb-0310-9956-ffa450edef68
{code}","15/May/12 21:39;billgraham;Thanks Travis, that's great sleuthing work. The patch shows the issue in the {{JobControlCompiler}}. The reducer estimation got moved up about 300 lines from around 600 to 380, which put that operation before the {{setLocation}} calls. I think the proposed patch seems valid then, which is to move it back down a bit, below {{setLocation}}.

I aslo want to document this contract in {{LoadMetadata}}, which is that {{setLocation}} will be called first.",15/May/12 21:54;traviscrawford;Confirmed the patch above fixes this issue - thanks Bill! It didn't apply cleanly but is quite small and was easy to reproduce against trunk. After updating the patch I can test again with the final version.,15/May/12 21:58;billgraham;Here's a second patch generated against the current trunk. It also contains changes to the javadocs of LoadMetadata. Let me know if this one applies ok for you.,15/May/12 22:10;traviscrawford;+1 (nonbinding). Thanks Bill! I just tested with this patch against trunk and reducer estimation works correctly with HCatLoader.,"16/May/12 23:39;julienledem;+1 this looks good to me.
Dmitriy said he will re run the check for the reducer estimator to make sure there's no unexpected side effect.
","17/May/12 21:58;julienledem;Based on Dmitriy's patch the following tests should be run to check there is no regression:
* TestEvalPipeline2
* TestJobSubmission
* TestPigRunner
* TestPigStats

Also the patch needs to be rebased",17/May/12 22:05;billgraham;I verified that all of test-commit passes.,17/May/12 22:48;julienledem;those tests are not part of it. That's why I'm mentioning it.,"18/May/12 21:18;julienledem;PIG-2693.3.patch rebased patch
all 4 tests pass:
* TestEvalPipeline2
* TestJobSubmission
* TestPigRunner
* TestPigStats
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate TOKENIZE schema,PIG-2691,12554533,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jay23jack,azaroth,azaroth,09/May/12 18:21,22/Feb/13 04:54,14/Mar/19 03:07,31/May/12 15:02,,,,,,,,0.11,,,,,,,0,simple,,,,,,,,,,,,"TOKENIZE produces a fixed named schema that results in duplicates if used more than once in the same generate statement.
We could paramenterize the schema on the name of the field being tokenized.

{code}
grunt> q = LOAD 'file' AS (source:chararray, target:chararray);
grunt> e = FOREACH q GENERATE TOKENIZE(source), TOKENIZE(target);
2012-05-09 20:18:37,235 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1108: 
<line 2, column 14> Duplicate schema alias: bag_of_tokenTuples
grunt> e = FOREACH q GENERATE TOKENIZE(source) as s_entities, TOKENIZE(target) as t_entities;
grunt> describe e
e: {s_entities: {tuple_of_tokens: (token: chararray)},t_entities: {tuple_of_tokens: (token: chararray)}}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/May/12 00:04;jay23jack;PIG-2691.patch;https://issues.apache.org/jira/secure/attachment/12528825/PIG-2691.patch,24/May/12 00:43;jay23jack;PIG-2691.patch.2;https://issues.apache.org/jira/secure/attachment/12528833/PIG-2691.patch.2,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-05-24 00:03:19.414,,,no_permission,,,,,,,,,,,,238783,Incompatible change,,,Tue May 29 12:41:44 UTC 2012,,,,,,,0|i0h4bb:,97961,"TOKENIZE: the default name of the field in the schema produced by this UDF now depends on the input field. This change could break your script if you were relying on the field being called ""bag_of_tokenTuples"" (i.e. you were not using an AS clause to rename the field).",,,,,,,,,"24/May/12 00:03;jay23jack;Changed the field alias of the TOKENIZE result from ""bag_of_tokenTuples"" to ""bag_of_tokenTuples_from_{original_alias}"". For example, the result alias of TOKENIZE(a) would be bag_of_tokenTuples_from_a. ","24/May/12 00:14;jay23jack;Oops, broke some unit tests. Fixing.","24/May/12 00:43;jay23jack;Fixed one unit test for TOKENIZE in TestLogicalPlanBuilder, and added another unit test using the query in the description.

Passed all the other unit tests of test-commit.","24/May/12 01:18;daijy;Patch looks good. 

One potential issue is it introduces some backward compatibility. The output schema name for TOKENIZE change. If anyone rely on it, he/she has to change the script. Is that fine or do we need a flag?","24/May/12 02:10;jay23jack;As there was no documentation on the field schema of TOKENIZE, can we assume that if users want to use the field name, she would explicitly name it by AS? If so, then this change wouldn't break the script.","24/May/12 04:48;azaroth;I agree with Jie, though I can imagine people are anyway using it somewhere. We should add something like this to the release notes:

TOKENIZE: the default name of the field in the schema produced by this UDF now depends on the input field. This change could break your script if you were relying on the field being called ""bag_of_tokenTuples"" (i.e. you were not using an AS clause to rename the field).

Apart from that, I would commit once tests pass.",24/May/12 05:00;daijy;Sounds good. +1,"29/May/12 12:41;azaroth;Got some failures during testing, but I am not sure about the sanity of my environment.
I am rerunning them in a different one.
Sorry for the delay.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JsonStorage fails to find schema when LimitAdjuster runs,PIG-2689,12554394,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ddaniels888,ddaniels888,ddaniels888,08/May/12 22:11,21/Nov/14 05:58,14/Mar/19 03:07,17/Jul/14 23:06,0.10.0,,,,,,,0.14.0,,,,,,,2,,,,,,,,,,,,,"Scripts that both save out data with JsonStorage and trigger the LimitAdjuster (e.g. doing an order by followed by a limit) yield the following Exception:

java.io.IOException: Could not find schema in UDF context
        at org.apache.pig.builtin.JsonStorage.prepareToWrite(JsonStorage.java:125)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.<init>(PigOutputFormat.java:125)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.getRecordWriter(PigOutputFormat.java:86)
        at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.<init>(ReduceTask.java:569)
        at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:638)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:260)

This happens b/c the LimitAdjuster does not copy the signature into it's newly created POStore, and hence JsonStorage looks for the schema for a null signature.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Jul/14 01:12;rohini;PIG-2689-2-nowhitespacechange.patch;https://issues.apache.org/jira/secure/attachment/12655428/PIG-2689-2-nowhitespacechange.patch,13/Jul/14 01:12;rohini;PIG-2689-2.patch;https://issues.apache.org/jira/secure/attachment/12655429/PIG-2689-2.patch,08/May/12 22:12;ddaniels888;PIG-2689.patch;https://issues.apache.org/jira/secure/attachment/12526055/PIG-2689.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-05-08 22:26:29.709,,,no_permission,,,,,,,,,,,,238636,Reviewed,,,Thu Jul 17 23:06:15 UTC 2014,,,,,,,0|i05fkf:,29619,,,,,,,,,,"08/May/12 22:13;ddaniels888;Attached a patch that fixes this by copying the signature to the new POStore.  It also copies the alias, which helps in illustrate.","08/May/12 22:26;azaroth;The modification looks OK, but I am not sure about the tests.
Should we test this as an e2e test?","05/Oct/12 19:22;alangates;This patch no longer applies because PhysicalOperator no longer has a setAlias method.  It's not clear to me why that was removed.  It also wasn't clear to me whether it was required for this patch (it looked like the setSignature was the one that mattered, but I wanted to confirm that before proceeding).

As for the e2e tests for this, ideally I agree we should have one.  But we don't generate any json data in the tests yet, so it seems too much to ask to add a new data set and tests for it.

I'm going to set this JIRA to open since the patch as is doesn't apply.  But if you feel setAlias isn't required I'm fine to apply the patch.",13/Jul/14 01:12;rohini;The issue is already fixed by PIG-3120. But that does not have a unit test. So added a unit test as part of this patch and also set the alias on the new limit operator based on [~ddaniels888] original patch. The unit test on the original patch does not apply cleanly anymore as having files for test is removed from TestJsonStorage and they are generated at run time.,17/Jul/14 21:30;daijy;+1 for PIG-2689-2.patch.,17/Jul/14 23:06;rohini;Committed to trunk (0.14). Thanks Daniel for the review.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TOBAG output schema reporting,PIG-2680,12553707,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,hazen,hazen,03/May/12 05:09,06/Jan/13 23:56,14/Mar/19 03:07,08/May/12 22:52,0.10.0,,,,,,,0.10.1,0.11,,,internal-udfs,,,0,,,,,,,,,,,,,"TOBAG only reports an output schema beyond {{{(NULL)}}} when all input field schemas match deeply, including field schema aliases. This seems wrong to me. Shouldn't it just require recursive type equality?

For relevant code, see:

http://svn.apache.org/viewvc/pig/tags/release-0.9.2/src/org/apache/pig/builtin/TOBAG.java?view=markup#l142

{code:java}
    private boolean nullEquals(Schema currentSchema, Schema newSchema) {
        if(currentSchema == null){
            if(newSchema != null){
                return false;
            }
            return true;
        }
        return currentSchema.equals(newSchema);
    }
{code}

The included patch modifies the return line to use {{Schema.equals(currentSchema, newSchema, false, true)}} to avoid alias matching requirement.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/May/12 03:00;hazen;fixes_tobag_input_schema_validation-r2.patch;https://issues.apache.org/jira/secure/attachment/12525797/fixes_tobag_input_schema_validation-r2.patch,03/May/12 05:11;hazen;fixes_tobag_input_schema_validation.patch;https://issues.apache.org/jira/secure/attachment/12525393/fixes_tobag_input_schema_validation.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-05-03 16:00:26.355,,,no_permission,,,,,,,,,,,,237901,,,,Tue May 08 22:51:11 UTC 2012,,,Patch Available,,,,0|i02u0v:,14465,,,,,,,,,,03/May/12 16:00;jcoveney;This looks good to me. I will try to commit it this weekend if I have a chance to run test-commit against 0.10 and 0.11 (I am still in Asia).,"04/May/12 06:23;jcoveney;Andy,

I am testing this now and it looks good. One thing you still need to do is update the comments in TOBAG, because it explicitly makes note of the fact that if the schema of the tuples does not agree, that it will return a null schema. Now, it will take the first one.

Thanks,
Jon","04/May/12 16:23;hazen;Hi Jon, Thanks for the test! As for the documented semantics of the UDF, they haven't really changed-- I've just relaxed the constraint on alias matching. If there's a mismatch on types at any level, the same null output schema will be returned.","05/May/12 00:54;jcoveney;This comes from the current comments on TOBAG:

{code}
 67  *  example 3
 68  *  grunt> describe a;                                                             
 69  *  a: {a0: (x: int),a1: (y: int)}
 70  * -- note that the inner schema is different because the alises (x & y) are different
 71  *  grunt> b = foreach a generate TOBAG(a0,a1);                                    
 72  *  grunt> describe b;                                                             
 73  *  b: {{NULL}}
{code}

your patch changes that. It's an ok change IMHO, since it is a more permissive change and the old version would have forced people to manually cast if the schema mattered, but we still need to update the comments accordingly, as in the above example, it'd now be b: {{(x:int)}}.","07/May/12 03:00;hazen;I totally missed that bit. Thanks for pasting the example. I'll update the patch to include changes to the javadoc. Let me know if you like how it reads.
",07/May/12 03:00;hazen;Updated patch which includes updates to javadoc.,"08/May/12 22:51;jcoveney;Committed. Thanks for the submissions, Andy! r1335807 and r1335806",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e2e harness: Reference local test path via  :LOCALTESTPATH:,PIG-2671,12553076,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thw,thw,thw,27/Apr/12 01:34,06/Jan/13 23:56,14/Mar/19 03:07,27/Apr/12 07:11,,,,,,,,0.10.1,0.11,0.9.3,,e2e harness,,,0,,,,,,,,,,,,,It's commented out currently. We require it to reference local files for tests with shell commands.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Apr/12 01:50;thw;PIG-2671.patch;https://issues.apache.org/jira/secure/attachment/12524802/PIG-2671.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-27 07:11:31.142,,,no_permission,,,,,,,,,,,,237027,Reviewed,,,Fri Apr 27 07:11:31 UTC 2012,,,,,,,0|i02u1b:,14467,,,,,,,,,,"27/Apr/12 01:37;thw;{code}
...
store B into ':OUTPATH:/B.out';
store C into ':OUTPATH:/C.out';
copyToLocal  :OUTPATH: :LOCALTESTPATH:/Grunt_11.:RUNID:
sh bash -c 'diff :LOCALTESTPATH:/Grunt_11.:RUNID:/B.out/part-r-00000 :LOCALTESTPATH:/Grunt_11.:RUNID:/C.out/part-r-00000'
...
{code}",27/Apr/12 07:11;daijy;Patch committed to 0.9/0.10/trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
glitches on copyright years in documentation,PIG-2670,12553063,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,daijy,daijy,daijy,26/Apr/12 23:24,06/Jan/13 23:57,14/Mar/19 03:07,26/Apr/12 23:59,,,,,,,,0.10.1,,,,documentation,,,0,,,,,,,,,,,,,"There are several places in Pig site still use ""Copyright © 2007"". Should be ""Copyright © 2011-2012"". Thanks Lefty Leverenz point it out.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Apr/12 23:47;daijy;PIG-2670-1.patch;https://issues.apache.org/jira/secure/attachment/12524787/PIG-2670-1.patch,26/Apr/12 23:47;daijy;PIG-2670-1_site.patch;https://issues.apache.org/jira/secure/attachment/12524788/PIG-2670-1_site.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,237004,Reviewed,,,Thu Apr 26 23:59:16 UTC 2012,,,,,,,0|i02u1r:,14469,,,,,,,,,,26/Apr/12 23:59;daijy;Regenerated the site.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig release should include pig-default.properties after rebuild,PIG-2669,12553036,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,daijy,daijy,daijy,26/Apr/12 20:58,06/Jan/13 23:57,14/Mar/19 03:07,07/Jun/12 22:30,,,,,,,,0.10.1,0.11,,,,,,0,,,,,,,,,,,,,"Pig release does not contain conf/pig-default.properties, it bundles pig-default.properties into pig.jar/pig-withouthadoop.jar. However, if user rebuild pig, the new jar does not contain this file.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Apr/12 21:12;daijy;PIG-2669-1.patch;https://issues.apache.org/jira/secure/attachment/12524769/PIG-2669-1.patch,06/Jun/12 05:40;daijy;PIG-2669-2.patch;https://issues.apache.org/jira/secure/attachment/12531069/PIG-2669-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-06-07 17:43:19.028,,,no_permission,,,,,,,,,,,,236898,Reviewed,,,Thu Jun 07 22:30:45 UTC 2012,,,,,,,0|i02tzb:,14458,,,,,,,,,,06/Jun/12 05:40;daijy;Missing the new src/pig-default.properties in patch.,07/Jun/12 17:43;thejas;+1,07/Jun/12 22:30;daijy;Patch committed to 0.10/trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LoadFunc.setLocation() is not called when pig script only has Order By,PIG-2666,12552434,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,toffer,toffer,24/Apr/12 20:54,06/Jan/13 23:57,14/Mar/19 03:07,11/May/12 01:00,0.9.3,,,,,,,0.10.1,0.11,0.9.3,,,,,0,,,,,,,,,,,,,"HCatLoader.setLocation() needs setLocation() called on the frontend. This doesn't happen with this pig script:

A = LOAD 'foo' USING org.apache.hcatalog.pig.HCatLoader();
B = ORDER A BY id;
DUMP B;",,,,,,,,,,,,,,,,,,,HCATALOG-380,,,,,,,,,,,,,25/Apr/12 18:35;daijy;PIG-2666-0.patch;https://issues.apache.org/jira/secure/attachment/12524318/PIG-2666-0.patch,09/May/12 00:18;daijy;PIG-2666-1.patch;https://issues.apache.org/jira/secure/attachment/12526075/PIG-2666-1.patch,09/May/12 22:15;daijy;PIG-2666-2.patch;https://issues.apache.org/jira/secure/attachment/12526224/PIG-2666-2.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-04-24 22:11:44.77,,,no_permission,,,,,,,,,,,,237011,Reviewed,,,Fri May 11 01:00:21 UTC 2012,,,,,,,0|i02u0n:,14464,,,,,,,,,,"24/Apr/12 22:11;daijy;Yes, we does not call setLocation in some cases. Specifically in JobControlCompiler:
{code}
LoadFunc lf = ld.getLoadFunc();
if (lf !=null) {
    lf.setLocation(ld.getLFile().getFileName(), nwJob);
}
{code}
It can be null when doing sampler job and order by job. we can solve it by forcing getLoadFunc() instantiate LoadFunc.

Note in the design of LoadFunc, setLocation is not guaranteed to be called in the frontend. But seems now some LoadFunc depends on it. Shall we make the statement that setLocation should be called in the frontend?","25/Apr/12 18:35;daijy;Francis, can you try the patch?",25/Apr/12 20:51;toffer;It would make sense to have something that guarantees to be get called on the FE so that there's a way to pass information to the inputformat as well as to the MR job. Not unless there's already something existing that does this?,"25/Apr/12 21:23;daijy;Yes, that's the motivation of the patch. It try to always call setLocation on FE.","25/Apr/12 21:32;toffer;tested patch, still failing for me.","26/Apr/12 04:11;jcoveney;In the short term, it seems fair to ensure setLocation is called on the frontend...in the (ideally near) future, though, we need to work on working some lifecycle into LoadFunc. The fact that there is a convention of stuffing a bunch of logic in setLocation is pretty confusing and suboptimal.","26/Apr/12 23:37;toffer;{quote}In the short term, it seems fair to ensure setLocation is called on the frontend...in the (ideally near) future, though, we need to work on working some lifecycle into LoadFunc. The fact that there is a convention of stuffing a bunch of logic in setLocation is pretty confusing and suboptimal.{quote}

That would be great. StoreFunc needs to be taken care of as well. I haven't used the other UDF interfaces.",02/May/12 02:01;toffer;patch calls SampleLoader.setLocation->HCatLoader.setLocation and it does not propagate values set in UDFProperties,09/May/12 00:18;daijy;Can you try PIG-2666-1.patch?,09/May/12 22:15;daijy;Adding test case.,11/May/12 00:50;thejas;+1,11/May/12 01:00;daijy;Patch committed to 0.9/0.10/trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bundled Jython jar in Pig 0.10.0-RC breaks module import in Python scripts with embedded Pig Latin,PIG-2665,12552296,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,miguno,miguno,24/Apr/12 12:32,22/Feb/13 04:53,14/Mar/19 03:07,08/Jun/12 07:57,0.10.0,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"Using Pig 0.9.0 I was running into PIG-1824 when using import statements (e.g. {{import os}}) in a Python script with embedded Pig Latin.  Dmitriy Ryaboy pointed me to the new Pig 0.10 release candidate (http://people.apache.org/~daijy/pig-0.10.0-candidate-0/pig-0.10.0.tar.gz) so that I could test whether my issue was solved by the new Pig version.  During testing I run into the error described below.

*Summary (TL;DR)*

* Even a minimal Python script with embedded Pig Latin will throw an error if there is a single import statement in the Python code.
* The fix is to replace the bundled {{lib/jython.jar}} with a standalone version of the same jar.

*Error message: ""ERROR 1121: Python Error (ImportError: No module named <yourmodule>)""*

{code}
$ /path/to/pig-0.10.0-RC1/bin/pig rctest.py 
2012-04-24 11:20:44,224 [main] INFO  org.apache.pig.Main - Apache Pig version 0.10.0 (r1328203) compiled Apr 19 2012, 22:54:12
[...snip...]
*sys-package-mgr*: can't create package cache dir, '/path/to/pig-0.10.0-RC1/lib/cachedir/packages'
2012-04-24 11:20:44,816 [main] INFO  org.apache.pig.scripting.jython.JythonScriptEngine - created tmp python.cachedir=/tmp/pig_jython_4081589571886870123
2012-04-24 11:20:45,033 [main] ERROR org.apache.pig.Main - ERROR 1121: Python Error. Traceback (most recent call last):
  File ""/home/mnoll/pig10rc/rctest.py"", line 5, in <module>
    import os
ImportError: No module named os
{code}

In the Pig log file:

{code}
Error before Pig is launched
----------------------------
ERROR 1121: Python Error. Traceback (most recent call last):
  File ""/home/mnoll/pig10rc/rctest.py"", line 5, in <module>
    import os
ImportError: No module named os

org.apache.pig.backend.executionengine.ExecException: ERROR 1121: Python Error. Traceback (most recent call last):
  File ""/home/mnoll/pig10rc/rctest.py"", line 5, in <module>
    import os
ImportError: No module named os

        at org.apache.pig.scripting.jython.JythonScriptEngine$Interpreter.execfile(JythonScriptEngine.java:210)
        at org.apache.pig.scripting.jython.JythonScriptEngine.load(JythonScriptEngine.java:384)
        at org.apache.pig.scripting.jython.JythonScriptEngine.main(JythonScriptEngine.java:368)
        at org.apache.pig.scripting.ScriptEngine.run(ScriptEngine.java:275)
        at org.apache.pig.Main.runEmbeddedScript(Main.java:929)
        at org.apache.pig.Main.run(Main.java:510)
        at org.apache.pig.Main.main(Main.java:111)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
Caused by: Traceback (most recent call last):
{code}

*How to reproduce*

Create a simple Python script that uses embedded Pig Latin AND that imports Python standard modules (any import statement will work):

{code}
#!/usr/bin/python 

from org.apache.pig.scripting import Pig 

# this import statement will trigger the error;
# remove it and everything will work fine
import os

if __name__ == ""__main__"":
    pig_script = """"""
        set job.name 'Pig 0.10.0-RC1 Python test';
    """"""
    P = Pig.compile(pig_script)
    bound = P.bind()
    result = bound.runSingle()

    if result.isSuccessful() :
        print ""Pig job succeeded""
    else:
        raise ""Pig job failed""
{code}

Then proceed as follows.

{code}

#
# Install the Pig 0.10.0 release candidate [1].
#

# run the Python test script
$ /path/to/pig-0.10.0-RC1/bin/pig rctest.py 

#
# see section above for error message
#
{code}

*Test Environment*

Apart from the ""Environment"" JIRA field please note that none of the TaskTracker boxes in my test cluster has Pig or Jython installed.  Pig with Jython is only available on a gateway box from which analysis jobs are run.

*Bug description*

During my investigation I discovered that the {{jython.jar}} that is shipped with the 0.10.0 RC package is NOT a standalone version of Jython.  I compared (diffed) the unpacked contents of the existing jython.jar with a standalone jar for Jython 2.5.0, and noticed that the main difference is that the standalone jar comes with a {{Lib/}} directory containing the various Python standard modules:

{code}
$ diff -r jython2.5.0 jython2.5.0-standalone/
Only in jython2.5.0-standalone/: Lib
diff -r jython2.5.0/META-INF/MANIFEST.MF jython2.5.0-standalone//META-INF/MANIFEST.MF
2a3
> Built-By: frank
5d5
< Built-By: frank
8,10d7
< version: 2.5.0
< svn-build: true
< oracle: true
11a9
> svn-build: true
13d10
< jdk-target-version: 1.5
14a12,14
> oracle: true
> version: 2.5.0
> jdk-target-version: 1.5
{code}

The essential difference is the missing {{Lib/}} directory in the non-standalone jar.

{code}
$ ls -l jython2.5.0-standalone/Lib
total 5236
-rw-r--r-- 1 mnoll mnoll  33417 2012-04-24 09:28 aifc.py
-rw-r--r-- 1 mnoll mnoll   2620 2012-04-24 09:28 anydbm.py
-rw-r--r-- 1 mnoll mnoll  11347 2012-04-24 09:28 ast.py
-rw-r--r-- 1 mnoll mnoll  10764 2012-04-24 09:28 asynchat.py
-rw-r--r-- 1 mnoll mnoll  17276 2012-04-24 09:28 asyncore.py
-rw-r--r-- 1 mnoll mnoll   1631 2012-04-24 09:28 atexit.py
-rw-r--r-- 1 mnoll mnoll  11296 2012-04-24 09:28 base64.py
-rw-r--r-- 1 mnoll mnoll  21289 2012-04-24 09:28 BaseHTTPServer.py
-rw-r--r-- 1 mnoll mnoll  20143 2012-04-24 09:28 bdb.py
[...snip...]
{code}

Apparently Jython (and thereby Pig) requires these Python module filesto be included in the {{jython.jar}} file -- at least in cluster environments where TaskTrackers DO NOT have Pig or Jython installed.

*How to fix*

In the Pig release package replace the {{jython.jar}} in {{lib/}} with a standalone version of the same jar.

Here's how I creatd the standalone version of Jython 2.5.0 on my box:

{code}
$ java -jar jython_installer-2.5.0.jar -s -d /tmp/jython-install -t standalone -j $JAVA_HOME
{code}

This will create the standalone jar in {{/tmp/jython-install/jython.jar}}.  Place this file into {{$PIG_HOME/lib/}}, thereby overwriting the existing (non-standalone) version.  After that the Python test script above will work successfully.

For completeness I also want to mention that I observed the following WARN messages before and after the Pig job was actually executed in the cluster:

{code}
$ /path/to/pig-0.10.0-RC1/bin/pig rctest.py 
[...snipp...]

# before job submission
#
2012-04-24 14:16:58,463 [main] WARN  org.apache.pig.scripting.jython.JythonScriptEngine - jython cachedir skipped, jython may not work
2012-04-24 14:16:58,467 [main] WARN  org.apache.pig.scripting.jython.JythonScriptEngine - module file does not exist: os, /path/to/pig-0.10.0-RC1/lib/jython-2.5.0-standalone.jar/Lib/os.py
2012-04-24 14:16:58,467 [main] WARN  org.apache.pig.scripting.jython.JythonScriptEngine - module file does not exist: os.path, /path/to/pig-0.10.0-RC1/lib/jython-2.5.0-standalone.jar/Lib/posixpath.py
2012-04-24 14:16:58,467 [main] WARN  org.apache.pig.scripting.jython.JythonScriptEngine - module file does not exist: posixpath, /path/to/pig-0.10.0-RC1/lib/jython-2.5.0-standalone.jar/Lib/posixpath.py
2012-04-24 14:16:58,468 [main] WARN  org.apache.pig.scripting.jython.JythonScriptEngine - module file does not exist: stat, /path/to/pig-0.10.0-RC1/lib/jython-2.5.0-standalone.jar/Lib/stat.py

# after the job finished (and succeeded)
#
2012-04-24 14:16:58,548 [main] WARN  org.apache.pig.scripting.jython.JythonScriptEngine - module file does not exist: os, /path/to/pig-0.10.0-RC1/lib/jython-2.5.0-standalone.jar/Lib/os.py
2012-04-24 14:16:58,548 [main] WARN  org.apache.pig.scripting.jython.JythonScriptEngine - module file does not exist: os.path, /path/to/pig-0.10.0-RC1/lib/jython-2.5.0-standalone.jar/Lib/posixpath.py
2012-04-24 14:16:58,548 [main] WARN  org.apache.pig.scripting.jython.JythonScriptEngine - module file does not exist: posixpath, /path/to/pig-0.10.0-RC1/lib/jython-2.5.0-standalone.jar/Lib/posixpath.py
2012-04-24 14:16:58,548 [main] WARN  org.apache.pig.scripting.jython.JythonScriptEngine - module file does not exist: stat, /path/to/pig-0.10.0-RC1/lib/jython-2.5.0-standalone.jar/Lib/stat.py
{code}

*Jython 2.5.0 vs. Jython 2.5.2*

FWIW I also tested whether switching to Jython 2.5.2 (up from 2.5.0 as bundled with the Pig 0.10 RC package) changes the results.  It did not.  That is, the Python script fails with non-standalone 2.5.2 jar but works with the standalone 2.5.2 jar.

Best,
Michael


PS: Is there a reason Jython version 2.5.0 is bundled instead of the latest stable release 2.5.2?

PPS: The 0.10.0-RC did solve my original PIG-1824 problem.  I could run the problematic Python/Pig script successfully using the 0.10.0-RC with a standalone Jython 2.5.0 jar. Cool!

[1] http://people.apache.org/~daijy/pig-0.10.0-candidate-0/pig-0.10.0.tar.gz","Verified bug on RHEL6 and on Ubuntu 11.10 with Sun JDK 1.6, and both Jython 2.5.0 (shipped with the Pig 0.10.0 RC package) and Jython 2.5.2.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Jun/12 23:01;daijy;PIG-2665-1.patch;https://issues.apache.org/jira/secure/attachment/12530868/PIG-2665-1.patch,08/Jun/12 00:24;daijy;PIG-2665-2.patch;https://issues.apache.org/jira/secure/attachment/12531351/PIG-2665-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-05-05 01:32:32.396,,,no_permission,,,,,,,,,,,,237164,Reviewed,,,Fri Jun 08 07:57:18 UTC 2012,,,,,,,0|i09ybr:,56002,,,,,,,,,,"05/May/12 01:32;daijy;Sounds like we shall pull jython-standalone instead of jython. And since jython-standalone 2.5.0 does not exist in maven, why not upgrade it to 2.5.2?",11/May/12 19:01;smalltalk80;The same problem is observed on 0.9.2. Following the steps above I was able to resolve it.,"05/Jun/12 22:26;julienledem;Looks good to me
+1","08/Jun/12 00:24;daijy;With standalone, the test case in PIG-2741 fail. Attach another patch to make it work.",08/Jun/12 07:57;daijy;Patch committed to trunk. Thank Julien for reviewing!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
skew join does not honor its config parameters,PIG-2662,12552244,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rajesh.balamohan,thejas,thejas,24/Apr/12 04:57,22/Feb/13 04:53,14/Mar/19 03:07,16/Aug/12 19:00,0.10.0,0.9.2,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"Skew join can be configured using pig.sksampler.samplerate and pig.skewedjoin.reduce.memusage. But the section of code the retrieves the config values from properties (PoissonSampleLoader.computeSamples) is not getting called. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,31/Jul/12 10:30;rajesh.balamohan;PIG-2662-0.9.2.patch;https://issues.apache.org/jira/secure/attachment/12538529/PIG-2662-0.9.2.patch,08/Aug/12 05:10;thejas;PIG-2662.2.patch;https://issues.apache.org/jira/secure/attachment/12539773/PIG-2662.2.patch,15/Aug/12 10:41;rajesh.balamohan;PIG-2662.3.patch;https://issues.apache.org/jira/secure/attachment/12541034/PIG-2662.3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-07-31 10:30:35.007,,,no_permission,,,,,,,,,,,,237165,,,,Fri Sep 21 18:53:34 UTC 2012,,,,,,,0|i0h41j:,97917,,,,,,,,,,31/Jul/12 10:30;rajesh.balamohan;Attaching the patch for 0.9.2 release codebase,08/Aug/12 05:10;thejas;PIG-2662.2.patch - This patch fixes compile error in previous one (conf variable is not declared). Running tests with this one. ,"14/Aug/12 17:40;thejas;Rajesh, 
With the patch, TestPoissonSampleLoader test cases fail. Can you please take a look ? 
Please let me know if you need any help with that.
","15/Aug/12 10:41;rajesh.balamohan;Review request : https://reviews.apache.org/r/6627/

Thejas,

This patch fixes TestPoissonSampleLoader and also has additional checks in TestPoissonSampleLoader. I have tested it and posting the output here.


   [junit] Running org.apache.pig.test.TestPoissonSampleLoader
   [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 45.92 sec
","16/Aug/12 19:00;thejas;+1  Patch committed to trunk.
Thanks Rajesh!
","23/Aug/12 18:52;knoguchi;It seems like org.apache.pig.test.TestPoissonSampleLoader.testNumSamples is failing with 
""expected:<47> but was:<42>"" after this patch.  Can someone take a look?","23/Aug/12 22:01;rajesh.balamohan;@Koji, which version of Pig are you using?","23/Aug/12 22:06;knoguchi;bq. @Koji, which version of Pig are you using?
Trunk. ","23/Aug/12 23:28;rajesh.balamohan;With the trunk code, I quickly tried to run only this testcase.

git status
# On branch trunk
nothing to commit (working directory clean)

ant -Dtestcase=TestPoissonSampleLoader test-core

   [junit] Running org.apache.pig.test.TestPoissonSampleLoader
   [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 25.917 sec

Plz let me know, if I am missing anything here to reproduce the issue you are seeing in your environment.","23/Aug/12 23:47;thejas;Koji, What OS, JVM are you using ?
","24/Aug/12 03:23;knoguchi;bq. Koji, What OS, JVM are you using ?

It is failing me on linux rhel5.6 + jvm1.6.0_32.  However, now I see that it's succeeding on my Mac. I'll take a look tomorrow.  ","21/Sep/12 18:53;knoguchi;bq. It is failing me on linux rhel5.6 + jvm1.6.0_32. However, now I see that it's succeeding on my Mac. I'll take a look tomorrow. 

Sorry for the delay. Created PIG-2926.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skew join and order by don't trigger reducer estimation,PIG-2652,12550904,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,billgraham,billgraham,13/Apr/12 23:43,22/Feb/13 04:53,14/Mar/19 03:07,28/Jun/12 20:51,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"If neither PARALLEL, default parallel or {{mapred.reduce.tasks}} are set, the number of reducers is not estimated based on input size for skew joins or order by. Instead, these jobs get only 1 reducer.",,,,,,,,,,,,,,,,,,,PIG-2779,,,,,,,,,,,,,13/Apr/12 23:54;billgraham;PIG-2652_1.patch;https://issues.apache.org/jira/secure/attachment/12522639/PIG-2652_1.patch,15/Apr/12 22:44;daijy;PIG-2652_2.patch;https://issues.apache.org/jira/secure/attachment/12522727/PIG-2652_2.patch,16/Apr/12 00:20;daijy;PIG-2652_3.patch;https://issues.apache.org/jira/secure/attachment/12522729/PIG-2652_3.patch,16/Apr/12 00:44;daijy;PIG-2652_3_10.patch;https://issues.apache.org/jira/secure/attachment/12522731/PIG-2652_3_10.patch,18/Apr/12 19:30;dvryaboy;PIG-2652_4.patch;https://issues.apache.org/jira/secure/attachment/12523239/PIG-2652_4.patch,18/Apr/12 22:29;dvryaboy;PIG-2652_5.patch;https://issues.apache.org/jira/secure/attachment/12523267/PIG-2652_5.patch,23/Apr/12 17:22;dvryaboy;PIG-2652_6.patch;https://issues.apache.org/jira/secure/attachment/12523824/PIG-2652_6.patch,26/Apr/12 08:19;dvryaboy;PIG-2652_7.patch;https://issues.apache.org/jira/secure/attachment/12524406/PIG-2652_7.patch,,,,8.0,,,,,,,,,,,,,,,,,,,2012-04-13 23:57:22.162,,,no_permission,,,,,,,,,,,,235768,,,,Sun Nov 04 18:44:57 UTC 2012,,,,,,,0|i0epmv:,83915,Fix how reducers are estimated for skew join and order operators.,,,,,,,,,"13/Apr/12 23:54;billgraham;Here's a patch that sets the number of reducers to -1 in {{MRController}} for sampled operations if it hasn't been set larger than 1. This will then trigger the reducer estimator in {{JobControlCompiler}}.

A related fix would be to not do the sampling  if someone has set number of reducers explicitly to 1.","13/Apr/12 23:57;dvryaboy;+1.
This code is quite the mess. At some point we need to refactor it.

Not running sampling for skewed / merge / order jobs when parallelism is set to 1 should be a separate ticket (I think I filed it already? Maybe just meant to).

Please commit to 10 and 11.","14/Apr/12 00:08;dvryaboy;Committed to 0.9.3, 0.10.0, 0.11","14/Apr/12 20:02;daijy;Get couple of unit test failures:
    [junit] Test org.apache.pig.test.TestCounters FAILED
    [junit] Test org.apache.pig.test.TestEvalPipeline2 FAILED
    [junit] Test org.apache.pig.test.TestFRJoin FAILED
    [junit] Test org.apache.pig.test.TestGrunt FAILED
    [junit] Test org.apache.pig.test.TestJobSubmission FAILED
    [junit] Test org.apache.pig.test.TestJoin FAILED
    [junit] Test org.apache.pig.test.TestLimitVariable FAILED
    [junit] Test org.apache.pig.test.TestMultiQueryLocal FAILED
    [junit] Test org.apache.pig.test.TestPigRunner FAILED
    [junit] Test org.apache.pig.test.TestPigSplit FAILED
    [junit] Test org.apache.pig.test.TestSampleOptimizer FAILED",14/Apr/12 20:43;dvryaboy;I am guessing the tests need to be adjusted. Sorry we rushed this out. Looking.,"14/Apr/12 20:49;dvryaboy;These tests take forever. While I am waiting, Daniel, do you mind attaching the error logs?","14/Apr/12 21:09;dvryaboy;Ok, TestCounters appears to have a problem rooted in this exception:

{code}

12/04/14 13:50:10 INFO mapred.TaskInProgress: Error from attempt_20120414134535045_0008_m_000000_0: java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.setConf(WeightedRangePartitioner.java:157)
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
        at org.apache.hadoop.mapred.MapTask$NewOutputCollector.<init>(MapTask.java:677)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:756)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)
        at org.apache.hadoop.mapred.Child.main(Child.java:249)
Caused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
        at java.util.ArrayList.RangeCheck(ArrayList.java:547)
        at java.util.ArrayList.get(ArrayList.java:322)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.convertToArray(WeightedRangePartitioner.java:199)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.setConf(WeightedRangePartitioner.java:142)
        ... 10 more
{code}","15/Apr/12 02:26;dvryaboy;Figured it out. 
The parallelism from MRCompiler is used to set the number of quantiles that FindQuantiles needs to produce. If we set it to -1, nothing gets generated. We need to somehow do the proper, stat-guided estimation prior to getSamplingJob getting called (or fix it up in JobControlCompiler).","15/Apr/12 02:43;dvryaboy;Patching things up in JobControlCompiler, or getting JCC to run before MRCompiler appears to require a significant rewrite.

Alternate proposal: 
If parallelism is not set explicitly, and no default is specified, set the number of quantiles to pig.exec.reducers.max. WeightedPartitioner will then need to look at its actual parallelism and evenly distribute the (up to max-reducers) quantiles among partitions.  We'd need to do something like that anyway if we used LoadFunc-reported histograms, or existing samples, to do the weighted partitioning, instead of running a sampling job every time.

Thoughts?","15/Apr/12 10:02;daijy;In which context do you see the issue stated in the Jira? Seems order by with PigStorage set the right #reduce even before the patch. Sample job get initial value 1, but get reset in SampleOptimizer with the right value.","15/Apr/12 16:15;dvryaboy;Rolled back for 0.9.3 and 0.10.

Thanks for the pointer to SampleOptimizer. 

I'll produce a reproducible test case.","15/Apr/12 17:59;dvryaboy;Ok, I have a test case. Estimation isn't triggered when the skewed join is preceded by another join (and perhaps anything else that has a reduce phase?).

Try this script:
{code}
-- lower this so that multiple reducers are forced
set pig.exec.reducers.bytes.per.reducer 118024;

x = load 'tmp/camac10/part*' as (foo:chararray);
y = load 'tmp/camac10/part*' as (foo:chararray);
x2 = load 'tmp/camac10/part*' as (bar:chararray);
x = join x by $0, x2 by $0;
z = join x by $0, y by $0;
store z into 'tmp/x11';
{code}

With both joins being regular hash joins, the stats look like this:
{code}
JobId	Maps	Reduces	MaxMapTime	MinMapTIme	AvgMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	Alias	Feature	Outputs
job_201204041958_154682	2	12	6	6	6	19	19	19	x2,x	HASH_JOIN	
job_201204041958_154690	2	9	6	6	6	19	19	19	y,z	HASH_JOIN	hdfs://hadoop-nn/user/dmitriy/tmp/x11,
{code}

Note that *9 reducers* were used by the second join.

Now let's make the second join skewed (just add ""using 'skewed'"" to the second join statement).
New stats:
{code}
JobId	Maps	Reduces	MaxMapTime	MinMapTIme	AvgMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	Alias	Feature	Outputs
job_201204041958_154662	2	12	6	6	6	26	19	23	x2,x	HASH_JOIN	
job_201204041958_154664	1	1	6	6	6	19	19	19		SAMPLER	
job_201204041958_154667	2	1	6	6	6	19	19	19	y	SKEWED_JOIN	hdfs://hadoop-nn/user/dmitriy/tmp/x10,
{code}

(by the way -- I now notice that the z alias doesn't show up..).

Note a single reducer being used for the skewed join job.

Here's the plan:

{code}
#--------------------------------------------------
# Map Reduce Plan                                  
#--------------------------------------------------
MapReduce node scope-32
Map Plan
Union[tuple] - scope-33
|
|---x: Local Rearrange[tuple]{chararray}(false) - scope-14
|   |   |
|   |   Project[chararray][0] - scope-15
|   |
|   |---x: New For Each(false)[bag] - scope-4
|       |   |
|       |   Cast[chararray] - scope-2
|       |   |
|       |   |---Project[bytearray][0] - scope-1
|       |
|       |---x: Load(hdfs://hadoop-nn/user/dmitriy/tmp/camac10/part*:org.apache.pig.builtin.PigStorage) - scope-0
|
|---x: Local Rearrange[tuple]{chararray}(false) - scope-16
    |   |
    |   Project[chararray][0] - scope-17
    |
    |---x2: New For Each(false)[bag] - scope-9
        |   |
        |   Cast[chararray] - scope-7
        |   |
        |   |---Project[bytearray][0] - scope-6
        |
        |---x2: Load(hdfs://hadoop-dw-nn.smf1.twitter.com/user/dmitriy/tmp/camac10/part*:org.apache.pig.builtin.PigStorage) - scope-5--------
Reduce Plan
Store(hdfs://hadoop-nn/tmp/temp1728845767/tmp973408893:org.apache.pig.impl.io.InterStorage) - scope-35
|
|---POJoinPackage(true,true)[tuple] - scope-64--------
Global sort: false
----------------

MapReduce node scope-39
Map Plan
Local Rearrange[tuple]{tuple}(false) - scope-42
|   |
|   Constant(all) - scope-41
|
|---New For Each(true,true)[tuple] - scope-40
    |   |
    |   Project[chararray][0] - scope-26
    |   |
    |   POUserFunc(org.apache.pig.impl.builtin.GetMemNumRows)[tuple] - scope-37
    |   |
    |   |---Project[tuple][*] - scope-36
    |
    |---Load(hdfs://hadoop-nn/tmp/temp1728845767/tmp973408893:org.apache.pig.impl.builtin.PoissonSampleLoader('org.apache.pig.impl.io.InterStorage','100')) - scope-38--------
Reduce Plan
Store(hdfs://hadoop-nn/tmp/temp1728845767/tmp-1828327704:org.apache.pig.impl.io.InterStorage) - scope-51
|
|---New For Each(false)[tuple] - scope-50
    |   |
    |   POUserFunc(org.apache.pig.impl.builtin.PartitionSkewedKeys)[tuple] - scope-49
    |   |
    |   |---Project[tuple][*] - scope-48
    |
    |---New For Each(false,false)[tuple] - scope-47
        |   |
        |   Constant(1) - scope-46
        |   |
        |   Project[bag][1] - scope-44
        |
        |---Package[tuple]{chararray} - scope-43--------
Global sort: false
Secondary sort: true
----------------

MapReduce node scope-57
Map Plan
Union[tuple] - scope-58
|
|---Local Rearrange[tuple]{chararray}(false) - scope-54
|   |   |
|   |   Project[chararray][0] - scope-26
|   |
|   |---Load(hdfs://hadoop-nn/tmp/temp1728845767/tmp973408893:org.apache.pig.impl.io.InterStorage) - scope-52
|
|---Partition rearrange [bag]{chararray}(false) - scope-55
    |   |
    |   Project[chararray][0] - scope-27
    |
    |---y: New For Each(false)[bag] - scope-25
        |   |
        |   Cast[chararray] - scope-23
        |   |
        |   |---Project[bytearray][0] - scope-22
        |
        |---y: Load(hdfs://hadoop-nn/user/dmitriy/tmp/camac10/part*:org.apache.pig.builtin.PigStorage) - scope-21--------
Reduce Plan
z: Store(hdfs://hadoop-nn/user/dmitriy/tmp/x11:org.apache.pig.builtin.PigStorage) - scope-29
|
|---POJoinPackage(true,true)[tuple] - scope-66--------
Global sort: false
----------------
{code}

Daniel, what do you think about fixing this in SampleOptimizer vs making the multi-partition change I proposed (make it unnecessary to push the constant around, always generate stats for a large number of partitions and distributed them in WeightedPartitioner)?","15/Apr/12 18:23;billgraham;I was able to reproduce with a similar script that didn't have a reducer in the first MR job. The code in questions is this block in {{SampleOptimizer}}. It returns in the second conditional with {{Predecessor should be a root of the plan}} before reducers can be estimated.

{{noformat}}
// Get this job's predecessor.  There should be exactly one.;
List<MapReduceOper> preds = mPlan.getPredecessors(mr);
if (preds.size() != 1) {
    log.debug(""Too many predecessors to sampling job."");
    return;
}
MapReduceOper pred = preds.get(0);

// The predecessor should be a root.
List<MapReduceOper> predPreds = mPlan.getPredecessors(pred);
if (predPreds != null && predPreds.size() > 0) {
    log.debug(""Predecessor should be a root of the plan"");
    return; 
}

// The predecessor should have just a load and store in the map, and nothing
// in the combine or reduce.
if ( !(pred.reducePlan.isEmpty() && pred.combinePlan.isEmpty())) {
    log.debug(""Predecessor has a combine or reduce plan"");
    return;
}
{{noformat}}","15/Apr/12 18:30;billgraham;FYI, here's my script that reproduces with an initial Map-only job:

{noformat}
L = LOAD 'data1.txt' AS (owner:chararray,pet:chararray,age:int,phone:chararray);
R = LOAD 'data2.txt' AS (owner:chararray,pet:chararray,age:int,phone:chararray);

L2 = FILTER L BY ((int)age > 0);
UNIONED = UNION L, L2;
JOINED = JOIN UNIONED BY owner, R BY owner USING 'skewed';

STORE JOINED INTO 'tmp/skew_join_union';
{noformat}
","15/Apr/12 20:57;dvryaboy;Looks like the third rule isn't correct either.

The problems seems to be that the SampleOptimizer used to do one thing, but now does (at least) two things. It used to remove an unnecessary MR job, as described in the class javadoc. As of PIG-1642, though, it's also responsible for reducer estimation. However, that optimization is not always possible -- which means reducer estimation also doesn't happen.

I think we should separate the two functionalities, either by reworking the SampleOptimizer code, or changing how WeightedPartitioner works. The former is less intrusive, the latter is probably a more architecturally sound solution.

Opinions?","15/Apr/12 22:44;daijy;I agree SampleOptimizer did more than it suppose to be, better to separate into two rule. 

As Bill observes, the rule does not proceed because some precondition fail. For now we can adjust the precondition check to solve some problem. I attach a patch for it. It solves Dmitriy's test case, however, Bill's test case is more involved. It is also related to plan merge of MultQuery. If I rewrite the query to get rid of the alias reuse, I can make it work:

{code}
L = LOAD '1.txt' AS (owner:chararray,pet:chararray,age:int,phone:chararray);
LN = LOAD '1.txt' AS (owner:chararray,pet:chararray,age:int,phone:chararray);
R = LOAD '2.txt' AS (owner:chararray,pet:chararray,age:int,phone:chararray);

L2 = FILTER L BY ((int)age > 0);
UNIONED = UNION LN, L2;
JOINED = JOIN UNIONED BY owner, R BY owner USING 'skewed';

dump JOINED;
{code}","16/Apr/12 00:20;daijy;SampleOptimizer is not a good place to do #reducer adjust. It is done once before the job launch, intermediate file is not yet generated, there is no way to get the size of job input. We shall move this logic to JobControlCompiler. Attach PIG-2652_3.patch.",16/Apr/12 00:28;dvryaboy;oh yeah.. we should reopen this :),"16/Apr/12 00:34;dvryaboy;Awesome, Daniel, thanks.
This last version looks much less fragile, and more easy to understand.

I will test (more thoroughly this time, maybe add a test case or two even..).

Stylistically, I'd prefer the code that specifically deals with sampling jobs to be moved out into its own function called from adjustNumReducers. I'll post my version once I'm done with tests (and taxes...).",16/Apr/12 00:44;daijy;The same patch for 0.10 branch. One side affect is explain will see different number of reducer cuz intermediate file is not available at explain time.,"17/Apr/12 06:37;dvryaboy;Spent some time debugging my refactoring and decided maybe there's a bug in your patch, Daniel. As written, we look at the inputs to the sampling job and estimating reducers for the successor based on those inputs. However, the successor actually has two inputs -- the sampled dataset, and the second joined relation. That means the earlier estimate is incorrect.

I tried running the estimator on the post-sample job, but there doesn't seem to be a way to connect the plan to its predecessor -- the plan passed in is already trimmed at the top. I'll try the following instead: identify a sampling job's children, and set them aside somewhere; then check against the saved list of known post-sample jobs and re-run the estimator for them if parallelism is set to 1.","17/Apr/12 08:02;dvryaboy;I've verified that the same bug (undercounting the input when estimating reducers) is in effect on trunk, when SampleOptimizer is able to estimate reducers.

Starting to uncover quite a few issues in skewed join implementation.. for example even if I explicitly set parallelism to 56, and have a half-dozen unique values in the skewed relation, the output is only split across 16 reducers.","17/Apr/12 08:03;daijy;Do you mean in skewed join, we only estimate the size based on the big table? I can see that, and it happens even before the patch. But since usually the skewed side is larger, so this might be acceptable.

I find another issue however, some rules such as LimitAdjuster depends on the right #reducer, and those rules are triggered before job launch. Seems we need a hook to adjust plan after finishing one job.","17/Apr/12 08:05;daijy;Also, I try to starting rolling 0.10.0 tomorrow, may we unlink it from 0.10.0?","17/Apr/12 08:14;dvryaboy;Agreed with unlinking from 0.10, this is clearly becoming a major patch rather than a minor one. 0.10.1, maybe.. crossing fingers. We should document this for 0.10, at least.

Interesting about LimitAdjuster. Separate jira or do you think we can kill both birds with one stone?

I really think avoiding having to know # of reducers in any optimizers will serve us better in the long term. Can LimitAdjuster be done without this knowledge?

Re: size estimation for skewed join, yes, I mean the ""big"" table -- except it's not the big table, it's the one with data skew. The other table might be the same size, or even bigger!

","17/Apr/12 18:31;daijy;Here is LimitAdjuster does: If the last job has more than 1 reducer, and it contains a limit, we will add a final MR job with 1 reducer. Otherwise we get N records per reducer rather than N records total. LimitAdjuster depends on #reducer of the MR Job.","17/Apr/12 19:01;dvryaboy;Could we make the Limit job a special case of MapReduce job (extends basemr) that, prior to kicking off, checks the number of input files and quietly returns if there is only a single file? Then we can always add it without performance overhead.","18/Apr/12 19:27;dvryaboy;Ok I have a much cleaner version of the patch, but it still suffers from a critical flaw -- the sampler needs to know the number of buckets to get samples for, and if it's not explicitly specified, and the SampleOptimizer stuff doesn't kick in, it still gets a single bucket to ""distribute"" skewed keys between.

I think we have to go ahead with the approach of always sampling for a large number of buckets (10000?), and distributing them evenly in the partitioner. 

Daniel -- comments? Will attach my work-in-progress patch.",18/Apr/12 19:30;dvryaboy;note that I completely yanked that visitor. Don't think we need it anymore.,18/Apr/12 21:54;dvryaboy;Visitor is back in :). Duh. Patch coming.,"18/Apr/12 22:29;dvryaboy;Ok this works, I think. TestSampleOptimizer passes. I don't have the infra set up at the moment to run the rest (currently running TestFRJoin on my laptop..). Manual skewed join job run did the right thing.

Still estimating based on the left side only, but running using an estimate for both left and right for skewed joins. ","20/Apr/12 08:44;daijy;Seems it still does not solve the LimitAdjuster issue. Imagine the following script:
{code}
A = load '1.txt';
B = order A by a0;
C = limit B 100;
dump C;
{code}

It will generate 2 jobs, sampler job and order by job. When we launch the first job, we check the size of input file, and realize we need N>1 reducer, so we adjust both jobs to set #reducer to N. But since there is a limit operator beneath, it then needs to add a third job with #reducer=1 to impose the limit 100. LimitAdjuster is assumed to add the third job, but it runs before JobControlCompiler, so it cannot see #reducers=N. 

There is a testcase TestEvalPipeline2.testLimitAutoReducer, and it fails because of the above reason.","20/Apr/12 08:51;dvryaboy;I see.
I think the way to solve this is to always produce the third job in physical compilation, and remove it if it's not necessary in LimitAdjuster as not running this is an optimization (correctness won't suffer from running the extra MR). Agreed?","21/Apr/12 01:25;daijy;Yes, it's doable. We can mark the job SKIP when we find the job is no longer needed. When JobControlCompiler see a SKIP job, simply discard. ","23/Apr/12 00:53;dvryaboy;Ok, not that simple. The adjuster messes with the inputs / outputs of the limiting job in pretty complex ways, so one would have to unroll all of that before running the pre-limit job.

Also, apparently one can't simply run the adjuster inside the JobControlCompiler -- it does correctly add a new job, but that job fails with {code}12/04/22 17:27:31 INFO mapred.TaskInProgress: Error from attempt_20120422172532429_0004_m_000000_0: org.apache.pig.backend.executionengine.ExecException: ERROR 2044: The type null cannot be collected as a Key type
{code} 

Moreover, the extra job is not accounted for in stats, progress, etc. 

Looking at how we can do this better.","23/Apr/12 17:22;dvryaboy;Ok, attaching latest.

This always adds the limiting MR job, even though it's not always strictly necessary. I feel that getting number of reducers estimated well for big jobs is more important than saving a tiny MR job (and this job will be tiny -- if it's unnecessary, by definition all that it does is read a LIMIT-ed number of rows, and output them back out).

We will create a separate ticket to apply an optimization that eliminates this extra limit job when possible.",23/Apr/12 20:46;daijy;Agree it's more important to fix big job. Did you run through unit test? I am afraid some tests will fail due to the extra job.,23/Apr/12 23:33;dvryaboy;I haven't had any luck getting hudson to actually finish a full unit test run.. but let me try to set that up again. ,"24/Apr/12 01:53;dvryaboy;Unsurprisingly, org.apache.pig.test.TestLimitAdjuster failed. I'll adjust tests as appropriate.","24/Apr/12 04:44;dvryaboy;Actually, it passes if I clean the environment. TestSkewedJoin also passed. I spot-checked a few others and they pass as well. A grep through tests and golden files didn't show anything that's specifically testing the added (or not-added) limiter job, just that the results are correct.

Currently test-commit fails on trunk for me on hudson (org.apache.pig.test.TestStore.testSuccessFileCreation1 fails), so running the whole test suite doesn't seem possible.. do you have a more stable testing environment you could try this on?","24/Apr/12 07:11;daijy;Yes, I will run the test.","25/Apr/12 01:46;dvryaboy;Daniel, did you get a chance to run this?","25/Apr/12 06:47;daijy;Yes, I get the following failures, most are not surprising:
TestExampleGenerator.testLimit
TestJobSubmission.testReducerNumEstimationForOrderBy
TestPigRunner.orderByTest
TestPigStats.testPigStatsAlias",26/Apr/12 08:19;dvryaboy;Fixed tests. Please try again.,27/Apr/12 18:15;daijy;Unit tests pass. +1 for commit. But we need to fix the extra job issue before next release.,"27/Apr/12 23:34;dvryaboy;Committed to 0.11

Can I commit to 0.10.1 or do you feel the extra MR job is too much?
","28/Apr/12 00:16;daijy;If we can fix the extra job before 0.10.1 release, I am fine to commit it.","28/Apr/12 00:24;dvryaboy;Created PIG-2675.
I am not sure I want to commit to doing that as a blocker for 0.10.1. It's a very small job, there are more fundamental improvements to be had for the same amount of effort, I think. But we should still do it at some point.",28/Apr/12 00:26;daijy;I don't feel it's too hard to fix. I can take a look.,"14/Jul/12 00:22;jay23jack;{quote}
Still estimating based on the left side only, but running using an estimate for both left and right for skewed joins.
{quote}

Does this means the skew join will allocate more reducers than what the sampler assumes, and those extra reducers will have nothing to do?","14/Jul/12 00:38;dvryaboy;I believe the non-skewed keys will get hashed to all reducers, not just those used for sampling.","16/Jul/12 18:03;jay23jack;Then the skewed keys will only go to some of the reducers, and ideally we want to distribute them across all reducers right? I can fix it in PIG-2779.",04/Nov/12 18:44;rohini;Correcting the fixed version. Removing 0.9.3 and 0.10.1,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.pig.parser.ParserValidationException does not expose the cause exception,PIG-2649,12550717,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,julienledem,julienledem,julienledem,12/Apr/12 17:32,22/Feb/13 04:54,14/Mar/19 03:07,13/Apr/12 03:45,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Apr/12 17:34;julienledem;PIG-2649.patch;https://issues.apache.org/jira/secure/attachment/12522457/PIG-2649.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-12 23:58:28.669,,,no_permission,,,,,,,,,,,,235581,,,,Thu Apr 12 23:58:28 UTC 2012,,,Patch Available,,,,0|i0h3xb:,97898,,,,,,,,,,12/Apr/12 17:34;julienledem;attaching PIG-2649.patch that allows the stacktrace of the cause to be displayed.,12/Apr/12 23:58;dvryaboy;+1 please commit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split Combining drops splits with empty getLocations(),PIG-2647,12550580,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,tmwoodruff,alexlevenson,alexlevenson,11/Apr/12 21:00,07/Jun/15 03:48,14/Mar/19 03:07,05/Jan/15 23:01,0.13.0,0.13.1,0.14.0,,,,,0.15.0,,,,impl,,,0,,,,,,,,,,,,,"in:
org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil#getCombinePigSplits
which is used by PigInputFormat

There is an assumption that every split's getLocations() will return a non-empty array.
If the following criteria are met:
1) Split combining is turned on
2) There is more than one split
3) There is at least one split that is smaller than the maxCombineSplitSize

splits with empty getLocations() will simply be dropped (ignored) without warning.

The hadoop API does not specify that all splits must return a location and there are cases where a split may want to return no locations (if the data is not in HDFS for example, or if the data is a directory full of HDFS files in which case there's not much gained by having locality)

This is due to the implementation of org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil#getCombinePigSplits
scans all splits eligible for combining and creates a map of Nodes -> splits, then laster iterates through the MAP (not the splits) to do the combining.

One solution would be to inject a dummy ""empty node"" into the map.

Overall the logic in getCombinePigSplits is very complicated and has a lot of edge cases, it might be worth cleaning up.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Dec/14 16:38;tmwoodruff;PIG-2647.patch;https://issues.apache.org/jira/secure/attachment/12684682/PIG-2647.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-12-02 16:34:45.217,,,no_permission,,,,,,,,,,,,235444,Reviewed,,,Mon Jan 05 23:01:20 UTC 2015,,,,,,,0|i0h3wn:,97895,Don't ignore unavailable blocks when combining input splits into PigSplits.,,,,,,,,,"02/Dec/14 16:34;tmwoodruff;Ran into this in a production environment, We lost several DataNodes, and this caused incorrect results to be produced (!!!), as files with no available blocks were silently ignored.","02/Dec/14 16:38;tmwoodruff;This patch explicitly checks for the case when no locations are returned and returns non-combined splits in this case.

No test, since I don't know of a way to create a non-replicated file without killing DataNodes.","05/Jan/15 22:59;daijy;This is nasty and we shall fix immediately. I will commit the patch as is. 

A better fix should allow those split combinable in getCombinePigSplits. This would involve more testing and we shall create a separate Jira to track. ",05/Jan/15 23:01;daijy;Patch committed to trunk. Thanks Travis!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigSplit does not handle the case where SerializationFactory returns null,PIG-2645,12550577,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,shamib007,alexlevenson,alexlevenson,11/Apr/12 20:18,22/Feb/13 04:53,14/Mar/19 03:07,25/Jan/13 19:08,0.10.0,,,,,,,0.11,,,,impl,,,0,patch,,,,,,,,,,,,"In PigSplit.java, line 254:
{code}
SerializationFactory sf = new SerializationFactory(conf);
Serializer s = sf.getSerializer(wrappedSplits[0].getClass());
s.open((OutputStream) os);
{code}
sf.getSerializer returns null when it cannot find a serializer for a given object. Instead of handling this properly, a NPE is thrown when s.open() is called.

This is easy to encounter when creating a custom InputSplit from the mapreduce package which is an abstract class that DOES NOT implement Writable.
However it's easy to miss because InputSplit from the mapred package is an interface that extends Writable, and InputSplits often both extend and implement both the new and old InputSplit abstract class and interface (thereby becoming Writable).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Dec/12 12:13;shamib007;PIG-2645.patch;https://issues.apache.org/jira/secure/attachment/12555747/PIG-2645.patch,07/Dec/12 13:34;shamib007;patch_2645.patch;https://issues.apache.org/jira/secure/attachment/12559855/patch_2645.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-09-12 14:01:12.095,,,no_permission,,,,,,,,,,,,235441,,,,Fri Jan 25 19:08:58 UTC 2013,,,,,,,0|i0h3vz:,97892,Added a NULL check for Serializer before calling open() method on it.,,,,,,,,,"12/Sep/12 14:01;shamib007;We can add a NULL check before the line 
s.open((OutputStream) os);","12/Sep/12 14:16;shamib007;The code change required at line 254 is :

if (s != null) {
	s.open((OutputStream) os);
}","12/Sep/12 18:04;alexlevenson;The change should be something more like this:
{code}
if (s == null) {
  throw new IllegalArgumentException(""InputSplits must implement Writable."");
}
{code}
or
{code}
Preconditions.checkNotNull(s, ""InputSplits must implement Writable."");
{code}","17/Sep/12 12:37;shamib007;Thanks Alex. The fix can be implemented as :

if (s == null) {
  throw new IllegalArgumentException(""InputSplits must implement Writable."");
} else {
  s.open((OutputStream) os);
}",03/Dec/12 12:13;shamib007;Please find the attached file containing the fix for the issue.,03/Dec/12 12:18;shamib007;Added a NULL check for Serializer before calling open() method on it.,"03/Dec/12 22:03;alexlevenson;Style nitpicks:
Don't use a javadoc, use a regular comment
Comment probably not even needed
No need for the else clause

How about:
{code}
if (s == null) {
  throw new IllegalArgumentException(""Could not find serializer for class "" 
            + wrappedSplits[0].getClass() 
            + "". InputSplits should implement Writable."");
}
s.open((OutputStream) os);
for (int i = 0; i < wrappedSplits.length; i++)
{code}",07/Dec/12 13:33;shamib007;Please find the attached patch after incorporating the review comments.,07/Dec/12 13:34;shamib007;Please find the patch with the review comments incorporated,23/Jan/13 19:54;alangates;Reviewing this patch,25/Jan/13 19:08;alangates;Fix checked into trunk and branch.  Thanks Shami.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Piggybank's HadoopJobHistoryLoader throws NPE when reading broken history file,PIG-2644,12550363,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,herberts,herberts,herberts,10/Apr/12 12:15,22/Feb/13 04:54,14/Mar/19 03:07,21/Apr/12 00:56,0.8.1,,,,,,,0.11,,,,piggybank,,,0,,,,,,,,,,,,,"When a history file has incomplete infos, HadoopJobHistoryLoader may throw NPE, thus making the whole job fail which can be cumbersome when analyzing thousands of files at once.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Apr/12 12:17;herberts;PIG-2644.patch;https://issues.apache.org/jira/secure/attachment/12522097/PIG-2644.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-21 00:56:08.969,,,no_permission,,,,,,,,,,,,235228,Reviewed,,,Sat Apr 21 01:04:57 UTC 2012,,,,,,,0|i0h3vj:,97890,,,,,,,,,,10/Apr/12 12:17;herberts;The following patch adds nullity tests prior to accessing a value which may otherwise lead to an NPE being thrown.,21/Apr/12 00:56;daijy;Patch committed to trunk. Thanks Mathias! Glad to see JobHistoryLoader is being used outside Yahoo.,21/Apr/12 01:04;prkommireddi;Is there any documentation on how this is used?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StoreMetadata.storeSchema can't access files in the output directory (Hadoop 0.23),PIG-2642,12550312,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thw,thw,thw,10/Apr/12 04:41,26/Apr/12 20:33,14/Mar/19 03:07,12/Apr/12 21:21,,,,,,,,0.10.0,0.11,0.9.3,,,,,0,,,,,,,,,,,,,"With Hadoop 0.20/1.0, storeSchema will be called after the temporary task files have been promoted to the output directory.

In 0.23, the timing of temporary task file promotion has changed by design, now when storeSchema is called the files have not been promoted to the output directory.

We have existing applications that depend on the temporary task files being promoted when storeSchema is called. This is a regression from 20.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11/Apr/12 20:30;thw;PIG-2642.patch;https://issues.apache.org/jira/secure/attachment/12522309/PIG-2642.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-10 17:31:25.88,,,no_permission,,,,,,,,,,,,235177,Reviewed,,,Thu Apr 12 21:21:58 UTC 2012,,,,,,,0|i0h3uv:,97887,,,,,,,,,,"10/Apr/12 04:47;thw;Reversing order of commitJob and storeCleanup calls in PigOutputCommitter to maintain 0.20 behavior under 0.23.
","10/Apr/12 17:31;jeagles;Thomas, I have tested the patch and it is working for me. Great! One thing I would add to update the comment for commitJob reflecting this code is intended for 0.23 instead of 0.20.203.",11/Apr/12 20:30;thw;Updated comment.,12/Apr/12 21:18;daijy;+1,"12/Apr/12 21:21;daijy;Committed to 0.9/0.10/trunk. 

Thanks Thomas!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Usage message gives wrong information for Pig additional jars,PIG-2640,12550249,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,prkommireddi,alangates,alangates,09/Apr/12 17:58,22/Feb/13 04:53,14/Mar/19 03:07,07/May/12 18:34,0.11,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"The usage statement in Main.java says:
{code}
pig.additional.jars=<comma seperated list of jars>. Used in place of register command.
{code}

But in PigServer.java it actually splits on ':', not ','.  See the method addJarsFromProperties().  We need to change the usage statement to match the code.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Apr/12 18:07;prkommireddi;PIG-2640.patch;https://issues.apache.org/jira/secure/attachment/12521979/PIG-2640.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-09 18:07:28.354,,,no_permission,,,,,,,,,,,,235114,Reviewed,,,Mon May 07 18:34:52 UTC 2012,,,Patch Available,,,,0|i0h3un:,97886,,,,,,,,,,"09/Apr/12 18:07;prkommireddi;Changing usage to ""Colon separated list of jars""",07/May/12 18:34;daijy;Patch committed to trunk. Thanks Prashant!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Utils.getSchemaFromString should automatically give name to all types, but fails on boolean",PIG-2639,12550135,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,jcoveney,jcoveney,jcoveney,08/Apr/12 00:24,22/Feb/13 04:53,14/Mar/19 03:07,11/May/12 22:33,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"This is related to https://issues.apache.org/jira/browse/PIG-2509. When I did that fix, I don't think boolean was a first class data type (and if it was, I forgot it was). Either way, it's a one line fix... please find it attached. Passes test-commit.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Apr/12 00:24;jcoveney;PIG-2639-0.patch;https://issues.apache.org/jira/secure/attachment/12521867/PIG-2639-0.patch,08/Apr/12 00:52;jcoveney;PIG-2639-1.patch;https://issues.apache.org/jira/secure/attachment/12521868/PIG-2639-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-04-08 00:27:19.107,,,no_permission,,,,,,,,,,,,235000,,,,Fri May 11 22:18:11 UTC 2012,,,,,,,0|i0h3uf:,97885,,,,,,,,,,"08/Apr/12 00:27;dvryaboy;lgtm.
add a test for all known types so we don't have this problem next time around?

Also: PIG-1429 was committed to 10. So this patch should not go into 9.3.","08/Apr/12 00:52;jcoveney;Good call Dmitriy, added a couple of tests.","11/May/12 22:18;dvryaboy;+1 
#shipit",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Command-line option -e throws TokenMgrError exception,PIG-2637,12550046,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,fang fang chen,rding,rding,07/Apr/12 00:14,22/Feb/13 04:53,14/Mar/19 03:07,13/Sep/12 21:10,0.9.2,,,,,,,0.11,,,,grunt,,,0,,,,,,,,,,,,,"The command-line:

{code}
java -cp pig.jar org.apache.pig.Main -x local -e ""a = load '1.txt';""
{code}

fails with exception:

{code}
ERROR 1000: Error during parsing. Lexical error at line 1, column 18.  Encountered: <EOF> after : """"
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Sep/12 08:37;fang fang chen;PIG-2637.patch;https://issues.apache.org/jira/secure/attachment/12544956/PIG-2637.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-13 08:39:08.422,,,no_permission,,,,,,,,,,,,234947,Reviewed,,,Thu Sep 13 21:10:27 UTC 2012,,,,,,,0|i0h3tr:,97882,,,,,,,,,,13/Sep/12 08:39;fang fang chen;Attach the patch file. Test passed in my environment.,13/Sep/12 08:50;fang fang chen;Patch files are based on pig-trunk.,13/Sep/12 21:10;daijy;Patch committed to trunk. Thanks fang fang!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong Usage of Scalar which is null causes high namenode operation ,PIG-2629,12549345,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,anitharaju,anitharaju,03/Apr/12 14:52,15/Apr/14 20:44,14/Mar/19 03:07,06/Dec/13 00:45,0.10.0,0.8.1,0.9.2,,,,,0.12.1,0.13.0,,,,,,0,,,,,,,,,,,,,"Hi,

Script
{code}
A = LOAD 'test3.txt'   AS (from:chararray);
B = LOAD 'test2.txt'    AS (source:chararray,to:chararray);
C = FILTER A BY (from == 'temp' );
D = FILTER B BY (source MATCHES '.*xyz*.');
E = JOIN C by (from) left outer,D by (to);
F = FILTER E BY (D.to IS NULL);
dump F;
{code}

Inputs
{code}
$ cat test2.txt
temp    temp
temp    temp
temp    temp
temp    temp
temp    temp
tepm    tepm

$ cat test3.txt  |head
temp
temp
temp
temp
temp
temp
tepm
temp
temp
temp
{code}

Here I have by mistake called 'to' using 'D.to' instead of 'D::to'. The D relation gives null output. 

First Map Reduce job computes D which give null results.
The MapPlan of 2nd job

{code}
Union[tuple] - scope-56
|
|---E: Local Rearrange[tuple]{chararray}(false) - scope-36
|   |   |
|   |   Project[chararray][0] - scope-37
|   |
|   |---C: Filter[bag] - scope-26
|       |   |
|       |   Equal To[boolean] - scope-29
|       |   |
|       |   |---Project[chararray][0] - scope-27
|       |   |
|       |   |---Constant(temp) - scope-28
|       |
|       |---A: New For Each(false)[bag] - scope-25
|           |   |
|           |   Cast[chararray] - scope-23
|           |   |
|           |   |---Project[bytearray][0] - scope-22
|           |
|           |---F: Filter[bag] - scope-17
|               |   |
|               |   POIsNull[boolean] - scope-21
|               |   |
|               |   |---POUserFunc(org.apache.pig.impl.builtin.ReadScalars)[chararray] - scope-20
|               |       |
|               |       |---Constant(1) - scope-18
|               |       |
|               |       |---Constant(hdfs://nn-nn1/tmp/temp-1607149525/tmp281350188) - scope-19
|               |
|               |---A: Load(hdfs://nn-nn1/user/anithar/test3.txt:org.apache.pig.builtin.PigStorage) - scope-0
|
|---E: Local Rearrange[tuple]{chararray}(false) - scope-38
    |   |
    |   Project[chararray][1] - scope-39
    |
    |---Load(hdfs://nn-nn1/tmp/temp-1607149525/tmp-458164144:org.apache.pig.impl.io.TFileStorage) - scope-53--------
{code}

Here at F , the file /tmp/temp-1607149525/tmp281350188 which is the output of the 1st Mapreduce Job is repeatedly read. 
If the input to F was non empty, since I am calling the scalar wrongly, it would have failed with the expected error message 'Scalar has more than 1 row in the output'.

But since its null, it returns in ReadScalars before the exception is thrown and gives these in the task logs repeatedly 

{code}
2012-04-03 11:46:58,824 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input paths to process : 1
2012-04-03 11:46:58,824 INFO org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil: Total input paths to process : 1
2012-04-03 11:46:58,827 WARN org.apache.pig.impl.builtin.ReadScalars: No scalar field to read, returning null
....
....
{code}

That is its reading the '/tmp/temp-1607149525/tmp281350188' file again and again which was causing high namenode operation. 
The cost of one small mistake had ended up causing heavy namenode operations.

Regards,
Anitha

",,,,,,,,,,,,,,,,,,,PIG-3669,,,,,,,,,,,,,23/Oct/13 17:49;rohini;PIG-2629-1.patch;https://issues.apache.org/jira/secure/attachment/12609889/PIG-2629-1.patch,06/Dec/13 00:30;rohini;PIG-2629-2.patch;https://issues.apache.org/jira/secure/attachment/12617277/PIG-2629-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-04-08 07:37:08.539,,,no_permission,,,,,,,,,,,,234336,Reviewed,,,Fri Dec 06 00:45:45 UTC 2013,,,,,,,0|i0h3qv:,97869,,,,,,,,,,08/Apr/12 07:37;aniket486;I think we should make scalar syntax more explicit to avoid such problems. D->to or SCALAR(D.to) are few options.,08/Apr/12 07:37;aniket486;I think we should make scalar syntax more explicit to avoid such problems. D->to or SCALAR(D.to) are few options.,22/Oct/13 18:41;rohini;We hit this again. Need to also fix ReadScalars to not read the file again if the value is null. ,"24/Oct/13 19:42;cheolsoo;+1.

One minor comment: You can just do if(!valueLoaded), can't you?
{code}
+        if (value == null && !valueLoaded) {
{code}",06/Dec/13 00:30;rohini;Addressed Cheolsoo's comment.,06/Dec/13 00:45;rohini;Committed to branch-0.12 (0.12.1) and trunk (0.13). Thanks Cheolsoo for the review.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Custom partitioner not set when POSplit is involved in Plan,PIG-2627,12549289,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,vivekp,vivekp,03/Apr/12 08:35,22/Feb/13 04:53,14/Mar/19 03:07,21/Apr/12 00:49,0.10.0,0.9.2,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"{code}
A = load 'i3.txt' as (k,v);
L = FILTER A BY k == 'k1';
B = GROUP A BY k PARTITION BY SimpleCustomPartitioner PARALLEL 3;
C = FOREACH B GENERATE FLATTEN(A) PARALLEL 3;
STORE C INTO 'output1';
STORE L INTO 'output2';
{code}

For the above script the custom partitioner mentioned in the GROUP BY statement is ignored by Pig.
To workaround this issue I had to disable Multiquery or put exec statements in between.

The configuration is getting missed out while the Physical plan is getting converted into the MR plan.
(Note the POGlobalRearrange is visited on a different MROper than what the actual MRPlan returns)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Apr/12 00:47;daijy;PIG-2627-2.patch;https://issues.apache.org/jira/secure/attachment/12523601/PIG-2627-2.patch,10/Apr/12 09:20;aniket486;PIG-2627.patch;https://issues.apache.org/jira/secure/attachment/12522084/PIG-2627.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-04-21 00:46:13.206,,,no_permission,,,,,,,,,,,,234280,Reviewed,,,Sat Apr 21 00:49:43 UTC 2012,,,,,,,0|i0h3qf:,97867,,,,,,,,,,21/Apr/12 00:46;daijy;Add test case.,21/Apr/12 00:49;daijy;Patch committed to trunk. Thanks Aniket!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation inaccurate regarding Pig Properties in trunk,PIG-2621,12548450,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,prkommireddi,prkommireddi,prkommireddi,28/Mar/12 08:23,26/Apr/12 20:32,14/Mar/19 03:07,29/Mar/12 19:06,0.10.0,0.9.2,,,,,,0.10.0,0.11,0.9.3,,,,,0,,,,,,,,,,,,,"Pig Properties documentation reverses the precedence order with respect to the various options available.

Currently it says: 
""The following precedence order is supported: pig.properties > -D Pig property > -P properties file > set command. This means that if the same property is provided using the -D command line option as well as the -P command line option and a properties file, the value of the property in the properties file will take precedence.""

The precedence order is actually the other way around:
pig.properties < -D Pig property < -P properties file < set command
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Mar/12 08:31;prkommireddi;PIG-2621.patch;https://issues.apache.org/jira/secure/attachment/12520247/PIG-2621.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-29 19:06:51.533,,,no_permission,,,,,,,,,,,,233547,Reviewed,,,Thu Mar 29 19:06:51 UTC 2012,,,Patch Available,,,,0|i0h3of:,97858,,,,,,,docs,,,"28/Mar/12 08:31;prkommireddi;Doc corrected, patch available.",29/Mar/12 19:06;daijy;Committed to 0.9/0.10/trunk. Thanks Prashant!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig e2e local fails to build,PIG-2618,12548206,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,jcoveney,jcoveney,jcoveney,26/Mar/12 23:27,26/Apr/12 20:33,14/Mar/19 03:07,27/Mar/12 01:12,,,,,,,,0.10.0,0.11,,,,,,0,,,,,,,,,,,,,"There is a failure in pig trunk. This is because a ""numbers.txt"" was added, but the directory it is moved to (""types"") was not. Patch incoming.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Mar/12 23:28;jcoveney;PIG-2618.patch;https://issues.apache.org/jira/secure/attachment/12520035/PIG-2618.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-27 00:32:16.847,,,no_permission,,,,,,,,,,,,233303,,,,Tue Mar 27 01:12:30 UTC 2012,,,Patch Available,,,,0|i0h3n3:,97852,,,,,,,,,,"27/Mar/12 00:32;daijy;+1, please commit to 0.10/trunk",27/Mar/12 01:12;jcoveney;Committed. r1305701 has the 0.10 commit and r1305698 has the trunk commit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobControlCompiler.getInputSizeFromLoader must handle exceptions from LoadFunc.getStatistics.,PIG-2616,12548058,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,billgraham,billgraham,billgraham,26/Mar/12 04:43,22/Feb/13 04:54,14/Mar/19 03:07,26/Apr/12 22:50,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"PIG-2573 made it possible for {{JobControlCompiler}} to get input size from {{LoadFuncs}}, but in some cases (i.e. {{InterStorage}}), {{getStatistics}} throws an exception:

{noformat}
Caused by: java.lang.UnsupportedOperationException
        at org.apache.pig.impl.io.InterStorage.getStatistics(InterStorage.java:189)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getInputSizeFromLoader(JobControlCompiler.java:839)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getInputSize(JobControlCompiler.java:799)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.estimateNumberOfReducers(JobControlCompiler.java:777)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJob(JobControlCompiler.java:599)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Apr/12 20:53;billgraham;PIG-2616.3.patch;https://issues.apache.org/jira/secure/attachment/12524766/PIG-2616.3.patch,26/Mar/12 04:55;billgraham;pig-2616.1.patch;https://issues.apache.org/jira/secure/attachment/12519903/pig-2616.1.patch,26/Mar/12 15:32;billgraham;pig-2616.2.patch;https://issues.apache.org/jira/secure/attachment/12519962/pig-2616.2.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-03-26 08:18:28.55,,,no_permission,,,,,,,,,,,,233155,,,,Thu Apr 26 20:53:38 UTC 2012,,,,,,,0|i0h3m7:,97848,,,,,,,,,,26/Mar/12 08:18;daijy;The fix is fine. You can also drop UnsupportedOperationException in InterStorage.,"26/Mar/12 15:32;billgraham;Here's a patch that returns null from {{InterStorage.getStatistics}}, which aligns with the contract of the {{LoadMetadata}} javadoc.",26/Mar/12 17:49;dvryaboy;+1. Don't think tests are needed. Will commit later today.,"26/Apr/12 20:53;billgraham;PIG-2574 dealt with half of this patch, which is catching the exception. Attaching a new patch which deals with the other half, which is that {{InterStorage.getStatistics(..)}} should return null.

Will commit this shortly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e2e harness: should not require PH_OLD_CLUSTER_CONF,PIG-2612,12547974,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thw,thw,thw,24/Mar/12 23:36,26/Apr/12 20:33,14/Mar/19 03:07,26/Mar/12 08:12,0.10.0,,,,,,,0.10.0,,,,e2e harness,,,0,,,,,,,,,,,,,"Needs to be treated same as OLD_HADOOP_HOME.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Mar/12 23:44;thw;PIG-2612.patch;https://issues.apache.org/jira/secure/attachment/12519827/PIG-2612.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-26 08:12:03.928,,,no_permission,,,,,,,,,,,,233071,Reviewed,,,Mon Mar 26 08:12:03 UTC 2012,,,,,,,0|i0h3kn:,97841,,,,,,,,,,26/Mar/12 08:12;daijy;Committed to 0.10. It is not needed for trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e2e harness: make hdfs base path configurable (outside default.conf),PIG-2609,12547657,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thw,thw,thw,22/Mar/12 18:28,26/Apr/12 20:32,14/Mar/19 03:07,23/Mar/12 00:01,,,,,,,,0.10.0,0.11,0.9.3,,e2e harness,,,0,,,,,,,,,,,,,"Should allow to control this through environment setting so that it is easy to run the harness as part of automation and have parallel runs on same cluster.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Mar/12 18:30;thw;PIG-2609.patch;https://issues.apache.org/jira/secure/attachment/12519485/PIG-2609.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-23 00:01:55.576,,,no_permission,,,,,,,,,,,,232755,Reviewed,,,Fri Mar 23 00:01:55 UTC 2012,,,,,,,0|i0h3jj:,97836,,,,,,,,,,23/Mar/12 00:01;daijy;Patch committed to 0.9/0.10/trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in PigStorage documentation for source tagging,PIG-2608,12547293,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,prkommireddi,prkommireddi,prkommireddi,20/Mar/12 21:58,22/Feb/13 04:53,14/Mar/19 03:07,21/Mar/12 00:35,0.11,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"There is a typo with the source tagging Javadocs on PigStorage. 

Usage: A = LOAD 'input' using PigStorage(',','-tagschema'), this option should be changed to '-tagsource'.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Mar/12 22:01;prkommireddi;PIG-2608.patch;https://issues.apache.org/jira/secure/attachment/12519132/PIG-2608.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-21 00:35:50.253,,,no_permission,,,,,,,,,,,,232451,Reviewed,,,Wed Mar 21 00:35:50 UTC 2012,,,Patch Available,,,,0|i0h3jb:,97835,,,,,,,,,,20/Mar/12 22:01;prkommireddi;Adding patch.,21/Mar/12 00:35;daijy;Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
union/ join operations are not accepting same alias as multiple inputs,PIG-2606,12546879,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,hsubramaniyan,thejas,thejas,17/Mar/12 01:49,14/Oct/13 16:46,14/Mar/19 03:07,29/Aug/13 21:17,0.10.0,0.9.2,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"grunt> l = load 'x';   
grunt> u = union l, l; 
2012-03-16 18:48:45,687 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2998: Unhandled internal error. Union with Count(Operand) < 2

grunt> a = load 'a0.txt' as (a0, a1);
grunt> b = join a by a0, a by a1;    
2013-08-27 13:36:21,807 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2225: Projection with nothing to reference!

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Aug/13 07:06;hsubramaniyan;PIG-2606.2.patch.txt;https://issues.apache.org/jira/secure/attachment/12600549/PIG-2606.2.patch.txt,27/Aug/13 21:15;hsubramaniyan;PIG-2606.patch.txt;https://issues.apache.org/jira/secure/attachment/12600255/PIG-2606.patch.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-08-27 20:27:23.552,,,no_permission,,,,,,,,,,,,232037,Reviewed,,,Thu Aug 29 21:17:13 UTC 2013,,,,,,,0|i0h3if:,97831,,,,,,,,,,"17/Mar/12 01:50;thejas;The plan is incorrect for the following union, when same input is used multiple times -
{code}
grunt> u = union l  , l2, l;
grunt> explain u;
#-----------------------------------------------
# New Logical Plan:
#-----------------------------------------------
u: (Name: LOStore Schema: null)
|
|---u: (Name: LOUnion Schema: null)
    |
    |---l: (Name: LOLoad Schema: null)RequiredFields:null
    |
    |---l2: (Name: LOLoad Schema: null)RequiredFields:null
{code}","27/Aug/13 20:27;hsubramaniyan;There are 2 possible fixes:
1. Improve the error message and say that Pig does not allow this for operations like UNION, JOIN.
2. Add a Split operator to split the alias and connect the Union node to Splitoutput. 
I will add the patch which implements option 1. Option 2 might be added in future depending on whether users actually encounter this scenario.

- Hari",27/Aug/13 20:49;hsubramaniyan;Improving the error message.,"27/Aug/13 20:51;hsubramaniyan;Added patch to improve the error message.

grunt> a = load 'a0.txt' as (a0, a1);
grunt> b = union a, a;               
2013-08-27 13:51:01,679 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1200: Pig script failed to parse: 
<line 2, column 4> pig script failed to validate: Pig does not accept same alias as input for UNION operation : a

grunt> b = join a by a0, a by a0; 
2013-08-27 13:51:32,838 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1200: Pig script failed to parse: 
<line 2, column 4> pig script failed to validate: Pig does not accept same alias as input for JOIN operation : a

",27/Aug/13 21:15;hsubramaniyan;Improving the error message,27/Aug/13 21:17;hsubramaniyan;Adding the patch to improve the error message,"28/Aug/13 22:31;daijy;[~hsubramaniyan], can we add a test case?",29/Aug/13 07:06;hsubramaniyan;Adding unit tests.,29/Aug/13 21:17;daijy;Patch committed to trunk. Thanks Hari!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jython UDF does not handle boolean output,PIG-2596,12546686,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,daijy,daijy,15/Mar/12 23:40,18/Jun/13 09:44,14/Mar/19 03:07,20/Apr/12 22:02,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"When a Jython UDF returns boolean, we still get integer object in Pig, through describe show the right boolean type.
Jython UDF:
{code}
@outputSchema(""retired:boolean"")
def isretired(age):
    if age == None:
        return None
    elif age>=60:
        return True
    else:
        return False
{code}
Pig script:
{code}
register 'scriptingudf.py' using jython as myfuncs;
a = load 'student.txt' as (name:chararray, age:int, gpa:double);
b = foreach a generate name, myfuncs.isretired(age);
describe b;
dump b;
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Apr/12 21:55;daijy;PIG-2596-2.patch;https://issues.apache.org/jira/secure/attachment/12523573/PIG-2596-2.patch,10/Apr/12 08:18;aniket486;PIG-2596.patch;https://issues.apache.org/jira/secure/attachment/12522080/PIG-2596.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,231844,Reviewed,,,Fri Apr 20 22:02:00 UTC 2012,,,,,,,0|i0h3en:,97814,,,,,,,,,,"20/Apr/12 21:55;daijy;Looks good, enable ignored test fixed by the patch.",20/Apr/12 22:02;daijy;Committed to trunk. Thanks Aniket!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filter by a boolean value does not work,PIG-2593,12546653,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jay23jack,daijy,daijy,15/Mar/12 20:13,22/Feb/13 04:53,14/Mar/19 03:07,08/Jun/12 18:42,,,,,,,,0.11,,,,build,,,0,,,,,,,,,,,,,"The following script does not work:
{code}
a = load 'allscalar10k' as (name, age, gpa, instate);
b = filter a by instate;
explain b;
{code}

Exception:
ERROR 1200: <file 18.pig, line 2, column 23>  mismatched input ';' expecting IS

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1000: Error during parsing. <file 18.pig, line 2, column 23>  mismatched input ';' expecting IS
        at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1598)
        at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1541)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:541)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:945)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:392)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:190)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:166)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:84)
        at org.apache.pig.Main.run(Main.java:599)
        at org.apache.pig.Main.main(Main.java:153)
Caused by: Failed to parse: <file 18.pig, line 2, column 23>  mismatched input ';' expecting IS
        at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:222)
        at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:164)
        at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1590)
        ... 9 more

It works if we change the script into:
{code}
a = load 'allscalar10k' as (name, age, gpa, instate);
b = filter a by instate==TRUE;
explain b;
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Jun/12 17:29;jay23jack;PIG-2593.patch;https://issues.apache.org/jira/secure/attachment/12531125/PIG-2593.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-01 18:49:31.571,,,no_permission,,,,,,,,,,,,231811,Reviewed,,,Fri Jun 08 18:42:23 UTC 2012,,,,,,,0|i0h3dj:,97809,,,,,,,,,,"01/Jun/12 18:49;jay23jack;I got slightly different results:

||Case||Description||Query||Result||
|1|declare as boolean and filter as boolean|a = load 'allscalar10k' as (name, age, gpa, instate:boolean);
b = filter a by instate==TRUE;
dump b;|successful|
|2|explicitly cast to boolean|a = load 'allscalar10k' as (name, age, gpa, instate);
b = filter a by (boolean) instate == TRUE;
dump b;|successful|
|3|implicitly conversion to string|a = load 'allscalar10k' as (name, age, gpa, instate);
b = filter a by instate == 'true';
dump b;|successful|
|4|implicitly conversion to boolean|a = load 'allscalar10k' as (name, age, gpa, instate);
b = filter a by instate==TRUE;
dump b;|Error:In alias b, incompatible types in Equal Operator left hand side:bytearray right hand side:boolean|
|5|declare as boolean and filter| a = load 'allscalar10k' as (name, age, gpa, instate:boolean);
b = filter a by instate;
dump b;|Error: <file 2.pig, line 2, column 23>  mismatched input ';' expecting IS|

So we have two cases not working here. We want to make Case 4 work by supporting implicit conversion of boolean, as we've already supported implicit conversion of chararray in Case 3.  Also it makes sense that Case 5 should work as boolean is a valid conditional expression.","01/Jun/12 20:38;daijy;Yes, you are right. We need to fix both 4 & 5.

There exists some e2e test cases in FilterBoolean, which is disabled due to the issue. You can make use of them.","05/Jun/12 19:33;jay23jack;Fixed case 4 in the sub-task PIG-2736.

For case 5, one alternative is to rewrite the expression ""by instate"" to ""by instate == true;"" in QueryParser, but the downside is if instate is not a boolean type, then the error messages would be confusing: ""the left side is a xxx type and the right side is a boolean type"", as there is actually no right side. Any means to customize the error message?

Another alternative is to create a BooleanExpression to handle this case, which would involve more changes all the way from the parser to the physical operator.","05/Jun/12 23:20;jay23jack;Fixed case 5 in a light way: added a virtual node ""BOOL_COND"" in QueryParser and implemented it with the original expression in LogicalPlanGenerator. Will attach the patch soon.

Actually there is a Case 6 in the e2e test:

|6|no schema and filter|a = load 'allscalar10k' as (name, age, gpa, instate);
b = filter a by instate;
dump b;| Error: <file 2.pig, line 9, column 4> Filter's condition must evaluate to boolean. Found: bytearray

After discussing with Daniel, we decided to leave this case for now, as it may require some heavy-weight changes.",06/Jun/12 17:29;jay23jack;Attached the patch of case 5. The patch for case 4 is in PIG-2736.,"08/Jun/12 18:42;daijy;All unit tests pass. 

Patch committed to trunk. Thanks Jie!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
running ant tar and rpm targets on same copy of pig source results in problems,PIG-2590,12546500,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,14/Mar/12 23:09,26/Apr/12 20:32,14/Mar/19 03:07,16/Mar/12 21:39,0.10.0,,,,,,,0.10.0,0.11,0.9.3,,,,,0,,,,,,,,,,,,,"- If ""ant tar"" is run, followed by ""ant rpm"" then the tar.gz that was generated gets overwritten by an intermediate tar.gz used by the rpm install
- Running both of them results in  duplicate files in the 2nd one, because both use the same staging directory before packaging.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Mar/12 23:19;thejas;PIG-2590.1.patch;https://issues.apache.org/jira/secure/attachment/12518399/PIG-2590.1.patch,20/Apr/12 02:07;daijy;PIG-2590.2.patch;https://issues.apache.org/jira/secure/attachment/12523441/PIG-2590.2.patch,20/Apr/12 02:59;daijy;PIG-2590.3.patch;https://issues.apache.org/jira/secure/attachment/12523443/PIG-2590.3.patch,20/Apr/12 05:48;daijy;PIG-2590.4.patch;https://issues.apache.org/jira/secure/attachment/12523453/PIG-2590.4.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-03-16 05:17:31.491,,,no_permission,,,,,,,,,,,,231658,,,,Fri Apr 20 05:38:56 UTC 2012,,,Patch Available,,,,0|i09ynj:,56055,"Patch committed to 0.9, 0.10 and 0.11 branches.
",,,,,,,,,14/Mar/12 23:19;thejas;PIG-2590.1.patch -uses different staging directory for rpm and tar install . Tested the artifacts manually by running simple pig queries.,16/Mar/12 05:17;daijy;+1,"20/Apr/12 02:07;daijy;There is a bug in deb target, attach PIG-2590.2.patch.",20/Apr/12 02:59;daijy;Another fix for releaseaudit target.,"20/Apr/12 05:38;daijy;Another bug exposed by the patch, which double the size of deb package. Attach PIG-2590.4.patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e2e harness: use pig command for cluster deploy,PIG-2588,12546391,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thw,thw,thw,14/Mar/12 13:58,26/Apr/12 20:33,14/Mar/19 03:07,14/Mar/12 23:20,,,,,,,,0.10.0,0.11,0.9.3,,tools,,,0,,,,,,,,,,,,,"We are running into issues with latest Hadoop 0.23.2 builds that have changed behavior of fs -copyFromLocal to no longer create target path silently if it does not exist. Using instead pig copyFromLocal works same across Hadoop versions.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Mar/12 14:05;thw;PIG-2588.patch;https://issues.apache.org/jira/secure/attachment/12518323/PIG-2588.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-14 20:29:44.879,,,no_permission,,,,,,,,,,,,231549,Reviewed,,,Wed Mar 14 23:20:51 UTC 2012,,,Patch Available,,,,0|i0h3br:,97801,,,,,,,,,,"14/Mar/12 20:29;daijy;+1, will commit shortly.",14/Mar/12 23:20;daijy;Patch committed to 0.9/0.10/trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable ignored e2e test cases,PIG-2585,12546335,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,14/Mar/12 00:52,26/Apr/12 20:32,14/Mar/19 03:07,18/Mar/12 08:29,0.10.0,,,,,,,0.10.0,0.11,,,,,,0,,,,,,,,,,,,,There are couple of test cases marked as ignored needing further investigation. We shall enable those.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Mar/12 23:14;daijy;PIG-2585-1.patch;https://issues.apache.org/jira/secure/attachment/12518398/PIG-2585-1.patch,16/Mar/12 08:28;daijy;PIG-2585-2.patch;https://issues.apache.org/jira/secure/attachment/12518632/PIG-2585-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-03-17 00:08:44.641,,,no_permission,,,,,,,,,,,,231493,Reviewed,,,Sun Mar 18 08:29:31 UTC 2012,,,,,,,0|i09ymn:,56051,,,,,,,,,,17/Mar/12 00:08;thejas;+1,18/Mar/12 08:29;daijy;Patch committed to 0.10/trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Store size in bytes (not mbytes) in ResourceStatistics,PIG-2582,12546285,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,prkommireddi,traviscrawford,traviscrawford,13/Mar/12 20:21,14/Oct/13 16:46,14/Mar/19 03:07,08/Nov/12 06:23,,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"In [ResourceStatistics.java|http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/ResourceStatistics.java?view=markup] we see mBytes is public, and has a public getter/setter.

{code}
47	    public Long mBytes; // size in megabytes

196	    public Long getmBytes() {
197	        return mBytes;
198	    }
199	    public ResourceStatistics setmBytes(Long mBytes) {
200	        this.mBytes = mBytes;
201	        return this;
202	    }
{code}

Typically sizes are stored as bytes, potentially having convenience functions to return with different units.

If mBytes can be marked private without causing woes it might be worth storing size as bytes instead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Oct/12 19:32;prkommireddi;PIG-2582.patch;https://issues.apache.org/jira/secure/attachment/12549742/PIG-2582.patch,29/Oct/12 23:28;prkommireddi;PIG-2582_1.patch;https://issues.apache.org/jira/secure/attachment/12551277/PIG-2582_1.patch,31/Oct/12 17:49;prkommireddi;PIG-2582_2.patch;https://issues.apache.org/jira/secure/attachment/12551587/PIG-2582_2.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-10-18 07:26:52.501,,,no_permission,,,,,,,,,,,,231443,,,,Mon Nov 05 18:28:05 UTC 2012,,,Patch Available,,,,0|i0af47:,58722,"Committed, thanks Prashant!",,,,,,,,,"18/Oct/12 07:26;prkommireddi;Hi Travis, trying to understand the goal here. Seems like ResourceStatistics has a method getSizeInBytes(), what would be the advantage of storing size in bytes and instead have getmBytes() do the reverse calculation? Is it for clarity and conforming with the norm? ","18/Oct/12 16:10;traviscrawford;Hey [~prkommireddi], this came up while working on PIG-2573. Basically it felt a bit janky to round the size to MB rather than just keep the size in bytes. Feel free to close if the consensus is to keep it as-is.","18/Oct/12 17:48;prkommireddi;I agree on storing size in bytes, just wanted to make sure I understand all the reasons you had in mind. It wouldn't be a huge change to make it happen, but changing the scope might be tricky if someone is using it outside of the Pig project. What do you think about marking the setter ""setmBytes(Long)"" deprecated and creating a new setter for bytes? To start with, we can atleast have Pig refer to byte-based methods.",18/Oct/12 18:00;traviscrawford;Sounds good!,"18/Oct/12 18:51;prkommireddi;Should methods like getAvgRecordSize() be returning size in bytes? Its not called from within the project, but not sure if such a change would be acceptable to users.",18/Oct/12 23:30;prasanth_j;I am using ResourceStatistics to store intermediate stats in PIG-2831. I am not using getmBytes() method but I use getAvgRecordSize() for storing and returning average record size in bytes. ,"18/Oct/12 23:44;prkommireddi;Hi Prasanth, I looked at PIG-2831. The current impl of getAvgRecordSize in trunk returns size in MB. That is not what you want?
Also, it might be better to access the values from getters instead of directly accessing them. That will allow us to clean-up the class further in future. Those members should really be private.","19/Oct/12 00:01;prasanth_j;I am not using setmBytes() or mBytes at all. So as per the implementation logic, only if mBytes!=null and numRecords!=null it will return the size in MB else it will return whatever it contains which in my case works fine. I also din't see any places using this. Please let me know if there is going to be any changes to the APIs, so that I will modify the patch accordingly. ","19/Oct/12 00:23;prkommireddi;Sure. 

""So as per the implementation logic, only if mBytes!=null and numRecords!=null it will return the size in MB else it will return whatever it contains which in my case works fine."" - this works but might be confusing/inconsistent though. I will wait for Travis/others to comment on this patch and let's stay in touch on how this might affect PIG-2831.","22/Oct/12 08:26;prkommireddi;Hi [~traviscrawford], can you please review the patch when you get a chance?","22/Oct/12 18:11;jcoveney;Thanks for handling this, ya'll. I'll +1 and commit, pending this making Travis happy.","22/Oct/12 18:17;prasanth_j;Hi Prashant

Looks like some of the class member variables are still public. Is there any reason to leave it public? ","22/Oct/12 18:30;prkommireddi;Hey Prasanth, the reason is that we don't want to break backward compatibility in case these member variables are accessed directly and not through getters. There are no such references from within the Pig project, but I'm being wary of any users who use this from outside of it.

The interface stability is marked Unstable on this, so I am ok if all decide its cool to change the scope of these variables :) ","22/Oct/12 18:30;billgraham;As much as we'd love to make these members private, we should resist the urge and keep them public for backward compatibility.",22/Oct/12 18:32;billgraham;I didn't notice the Unstable annotation. I'm ok changing scope if others agree.,"27/Oct/12 15:08;rohini;Agree with Bill. It would be good to make them private. Also the getters and setters were already available and the interface is marked unstable. We can wait another week to see if anyone disagrees, else we can go ahead with a patch that makes the member variables private but retains the getter/setter for mBytes marked deprecated.

setmBytes also needs to set the bytes. If we are making the variables private we can remove mbytes variable altogether and only retain bytes.","29/Oct/12 21:43;jcoveney;Given it is unstable, I say just go ahead with the plan to make it private, and have the deprecated getter/setter","29/Oct/12 21:59;prkommireddi;Also, there is a common theme of returning the object ""return this;"" on all setters. I don't think this should exist, but we should probably tackle that in the next release to make sure we are fine with the existing changes at first.
","29/Oct/12 23:28;prkommireddi;Making instance variables private. The setter on mBytes is being made deprecated, however getter can be thought of as a convenience method going forward and the implementation changed once we get rid of ""mBytes"" variable in the next iteration. We cannot *safely* get rid of mBytes until the setter is completely taken out of this class.","29/Oct/12 23:58;jcoveney;As long as the instance variables are private, then how come we can't get rid of the mBytes variable?",30/Oct/12 00:15;prkommireddi;What would we have the deprecated mBytes setter set the value on?,"30/Oct/12 00:37;jcoveney;Couldn't it just set realBytes = mBytes * 1024 * 1024 ? There would be no loss of precision. And if they happened to do getMBytes we just divided (in this case, we can in fact even just do shifts).","30/Oct/12 00:47;prkommireddi;:)

If we are ok with abusing this setter for now, we need to definitely open a JIRA to completely get rid of the deprecated method in 0.12.","31/Oct/12 17:49;prkommireddi;Making member variables private, and eliminating mBytes. Reviewers, please take a look.",31/Oct/12 18:56;prasanth_j;Once this is committed.. i will fix PIG-2831 to reflect the change..,"01/Nov/12 05:42;prkommireddi;Thanks for keeping an eye out on this, Prasanth!",02/Nov/12 15:35;billgraham;+1 LGTM. I can commit after the weekend unless anyone else has comments.,05/Nov/12 18:28;prkommireddi;Thanks Bill.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HashFNV inconsistent/non-deterministic due to default platform encoding,PIG-2581,12546242,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,prkommireddi,koda,koda,13/Mar/12 14:29,22/Feb/13 04:53,14/Mar/19 03:07,16/Mar/12 06:17,0.8.1,,,,,,,0.11,,,,piggybank,,,0,,,,,,,,,,,,,"HashFNV (org/apache/pig/piggybank/evaluation/string/HashFNV) bases its computation on String.getBytes(), which uses the platform default encoding. This leads to different results on different platforms. Worse, if any character is not supported by the encoding, the behavior is completely undefined. We have observed non-deterministic behavior that seems to be caused by this.

Suggested fix is to instead use String.getBytes(""UTF-8""), which will be well-defined and consistent on every platform.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Mar/12 06:15;daijy;PIG-2581-2.patch;https://issues.apache.org/jira/secure/attachment/12518623/PIG-2581-2.patch,14/Mar/12 07:06;prkommireddi;PIG-2581.patch;https://issues.apache.org/jira/secure/attachment/12518300/PIG-2581.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-03-13 19:20:22.436,,,no_permission,,,,,,,,,,,,231400,Reviewed,,,Fri Mar 16 06:17:53 UTC 2012,,,Patch Available,,,,0|i0h3b3:,97798,,,,,,,,,,"13/Mar/12 19:20;daijy;Sounds good, will you make a patch?",14/Mar/12 07:06;prkommireddi;Adding a patch with the fix.,16/Mar/12 06:15;daijy;Need to catch exception.,16/Mar/12 06:17;daijy;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple Store-commands mess up mapred.output.dir.,PIG-2578,12545939,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,mithun,mithun,10/Mar/12 02:23,24/Aug/12 03:41,14/Mar/19 03:07,14/Apr/12 08:03,0.8.1,0.9.2,,,,,,0.10.0,0.11,,,,,,0,,,,,,,,,,,,,"When one runs a pig-script with multiple storers, one sees the following:
1. When run as a script, Pig launches a single job.
2. PigOutputCommitter::setupJob() calls the underlyingOutputCommitter::setupJob(), once for each storer. But the mapred.output.dir is the same for both calls, even though the storers write to different locations. 

This was originally seen in HCATALOG-276, when HCatalog's end-to-end tests are run against Pig.
(https://issues.apache.org/jira/browse/HCATALOG-276)

Sample pig-script (near identical to HCatalog's Pig_Checkin_4 test):

a = load 'keyvals' using org.apache.hcatalog.pig.HCatLoader();
split a into b if key<200, c if key >=200;
store b into 'keyvals_lt200' using org.apache.hcatalog.pig.HCatStorer();
store c into 'keyvals_ge200' using org.apache.hcatalog.pig.HCatStorer();

I've suggested a workaround in HCat for the time being, but I think this might be something that needs fixing in Pig.

Thanks.
",,,,,,,,,,,HCATALOG-276,,,,,,,,,,,,,,,,,,,,,13/Apr/12 19:07;daijy;PIG-2578-1.patch;https://issues.apache.org/jira/secure/attachment/12522606/PIG-2578-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-13 16:40:32.651,,,no_permission,,,,,,,,,,,,231120,Reviewed,,,Fri Aug 24 03:41:55 UTC 2012,,,,,,,0|i09ygf:,56023,,,,,,,,,,"13/Mar/12 16:40;ashutoshc;I am finding this a bit surprising. If this were to be true, multi-query cannot work effectively, since then both stores using FileOutputFormat will effectively write in same directory, messing up the outputs of each other. I suspect problem may exist in HCatalog. A test-case independent of HCatalog demonstrating Pig bug will be highly appreciated. ","13/Apr/12 19:06;daijy;There two folds of this issue:
1. I found one bug in Pig which we didn't make a copy of hadoop configuration before invoking various StoreFunc hooks. This is more obvious under hadoop 23 for HCat when hadoop need ""mapred.output.dir"" in OutputCommitter to move promote output.
2. There is also a fix on HCat side. setStoreLocation suppose to set up the right ""mapred.output.dir""",13/Apr/12 19:07;daijy;Attach the patch in Pig side.,13/Apr/12 19:18;daijy;HCat side fix is part of HCATALOG-375.,13/Apr/12 19:19;thejas;+1,14/Apr/12 08:03;daijy;Patch committed to 0.10/trunk.,"13/Aug/12 01:41;billgraham;This patch has caused some issues, see PIG-2870.","13/Aug/12 20:09;rohini;PIG-2821 is another issue which has problems because of this patch. 

We should revert this patch.

With PIG-2578, setting config or credentials in job is not actually being passed to the actual job launched. The testcase in MultiQueryLocal is not valid as the condition (job.getConfiguration().get(key)==null) is always true. It was always using the suffix initialized in the constructor and never from the job. If a StoreFunc adds config to Job then it should be its responsibility to handle multiple instances of it. HCATALOG-314 handles it for hcatalog and fixes the original issue reported. We should not remove the ability for a StoreFunc to set a config on the job.",14/Aug/12 00:21;rohini;Spoke with Daniel. He said it was intentional to make the JobConf read-only so that each store does not override another. But the problem with that is it does not allow addition of Credentials and setting of JT specific config like Distributed cache configuration on the Job. We need a more cleaner solution to solve it and prevent StoreFunc implementations to not put something in JobConf that will not work correctly with multiple stores. We got rid of the multiple stores messing up problem in hcat by putting the properties in UDFContext instead of Job. But cannot expect all StoreFunc implementations to do that unless forced to which was the intention of this JIRA. ,"14/Aug/12 01:06;dvryaboy;I am aware of many StoreFunc implementations that rely on being able to mess with the JobConf. This is an undocumented and backwards incompatible change.. I can see why we need it, but the proper way to do this would be to document it, provide explicit instructions on using UDFContext (and how/where/when to get it), and migrate piggybank and builtin storefuncs that rely on mutable jobconfs. Further, to make the contract clear, rather than passing in a dummy new jobConf that gets GC'd immediately, we should pass in a wrapper job conf which throws exceptions on any set() call, to prevent surprises.","14/Aug/12 02:08;billgraham;Regarding the wrapper job conf, in some cases I'm sure it's justified to set a conf. What if we throw an exception if a value set attempt occurs where a different value already exists? We could include messaging about how UDFContext if probably what they want. This approach would be backward compatible with jobs that use conf properly with a single-store job, for example.","14/Aug/12 21:45;rohini;Did some debugging with and without PIG-2578. Multiple storage using PigStorage worked fine in both cases. This is because before every getOutputFormat call, there is a setLocation with a copy of JobContext or TaskAttemptContext and that copy was passed to getOutputCommitter(), getRecordWriter() or checkOutputSpecs() calls. So the output format actually runs with the correct configuration. So multiple store commands don't always get messed up. The corner case problem I see is that, if one instance of the store set a configuration to a specific value and another instance of the store does not set any value at all for that config it will still get the config with the value set from the copy of the job put by the first instance(without PIG-2578).

The actual problem was with the hcat code when this jira was filed. It set the mapred.output.dir and lot of other properties in front end but not in the backened during setStoreLocation. 
http://svn.apache.org/viewvc/incubator/hcatalog/branches/branch-0.4/src/java/org/apache/hcatalog/pig/HCatStorer.java?revision=1325867&view=markup
If it had set the mapred.output.dir in the backend also, it would have worked fine. It was later fixed to do so.","21/Aug/12 18:31;rangadi;Thanks for the analysis Rohini. +1 for reverting this patch. 

For the larger issue, I think Pig should clearly define the contract for job/conf passed setLocation() and setStoreLocation() so the user's StoreFunc can be implemented properly. I would suggest resisting the temptation to say ""this method might be called any number of times"" (a variant of this appears multiple places in Pig interface). While this made UDF implementors think twice about what they are doing, it allowed Pig to implement work arounds rather than proper fixes (i.e. why is ""setStoreLocation()"" called so many places?).
","21/Aug/12 19:13;billgraham;I think the problem is not that as much that setStoreLocation can get called multiple times, but that from the Javadocs it's not clear what the effects (or side-effects) will occur when setStoreLocation sets values in the Config.

+1 on reverting this patch and adding better javadocs for starters since the build is currently broken for a number of common use cases. We can then add examples and safeguards to illustrate proper usage in a multi-store environment.","21/Aug/12 22:20;daijy;I am fine with reverting the patch. The underlying problem is setStoreLocation is the only hook for StoreFunc for multiple purpose. In the javadoc, we shall make it clear:
1. Need to distinguish frontend/backend (using UDFContext.isFrontend()), user can setup global configuration in the frontend, but can only setup store only configuration in the backend
2. When setting up global configuration, need to bear in mind there could be multiple store, so config entries can overwrite each other.",24/Aug/12 03:41;dvryaboy;Reverted in PIG-2890. I don't see a way to reopen this jira and change it to won't fix..,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change in behavior for UDFContext.getUDFContext().getJobConf() in front-end,PIG-2576,12545796,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thw,vivekp,vivekp,09/Mar/12 09:07,26/Apr/12 20:32,14/Mar/19 03:07,16/Mar/12 22:20,0.9.3,,,,,,,0.10.0,0.11,0.9.3,,,,,0,,,,,,,,,,,,,"We read a file in the UDF constructor. (The file is transferred to the compute nodes via distache)
To avoid this case in the front-end while the script is in the compile stage,
we differentiate between front end and back end execution depending upon a condition ( UDFContext.getUDFContext().getJobConf() == null )

This was working till Pig 0.9.1, in the current Pig 0.9 version this is breaking.
ie, If I have any 'fs' commands after the STORE statement, the GruntParser invokes the udf constructor again and the above condition check returns false causing errors.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Mar/12 20:11;thw;PIG-2576_0.9.patch;https://issues.apache.org/jira/secure/attachment/12517772/PIG-2576_0.9.patch,09/Mar/12 09:22;vivekp;PIG-2576_Script_UDF.txt;https://issues.apache.org/jira/secure/attachment/12517693/PIG-2576_Script_UDF.txt,16/Mar/12 21:22;thw;PIG-2576_e2e.patch;https://issues.apache.org/jira/secure/attachment/12518738/PIG-2576_e2e.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-03-09 10:35:57.027,,,no_permission,,,,,,,,,,,,230979,Reviewed,,,Fri Mar 16 22:20:23 UTC 2012,,,,,,,0|i0h3an:,97796,,,,,,,,,,"09/Mar/12 09:22;vivekp;Attaching a sample script, udf and exception trace to illustrate the issue.
Looks like the change came in as part of PIG-2532 in which Pig MR Launcher will set the current UDFContext before the MR jobs are
launched. 
Hence after the job launch , the UDFContext.getUDFContext().getJobConf() will
always be a non null value.","09/Mar/12 10:35;daijy;This might hardly count as defect. But if we absolutely need to bring the old behavior, we shall clone UDFContext instead of passing reference.","09/Mar/12 11:02;vivekp;Daniel, currently is there any api calls to differentiate the local and MR mode ?
If not how about providing this information in UDFContext. ","09/Mar/12 14:07;thw;For the user it is a regression or incompatibility introduced in a minor/patch release. Whether the previous behavior of UDFContext.getUDFContext().getJobConf() was intended/documented and what workarounds there are (see below) is not really important, it was in place for long and users have come to rely on it. It is impossible for us to find out how many UDFs have been written with this dependency or change them. 

A better way for UDF developers to ensure initialization only runs in the backend is another issue. It should be possible to determine that easily and in a standard way. The UDF constructor is executed by the frontend (several times) and mapred, hence there is a need to determine where the code runs (for access to files etc.) 

UDF developer can workaround this by just being a bit more ""safe"" with the ""where do I run"" check:

1) The initialization can be done lazily from exec(...), which is never called in the frontend?

2) Instead of just checking for null, do this:

{code}

Configuration jobConf = UDFContext.getUDFContext().getJobConf();
if (jobConf != null && jobConf.get(""mapred.task.id"") != null) {
   System.err.println(""Executing in BACKEND:"" + jobConf.get(""mapred.task.id""));    
} else {
   System.err.println(""Executing in FRONTEND"" + jobConf);    
}

{code}

Both of the above work on 0.9.2 and previous releases.

","09/Mar/12 16:54;ashutoshc;Until PIG-2344 is resolved this (wanting to know running in front/backend) will keep coming back, since state is exposed to user, where it should not have been. Long term fix of this is PIG-2344, in short term we can provide a static method which can return boolean telling its running in frontend or not, instead of user doing it themselves and getting into issues.","09/Mar/12 20:11;thw;Patch to clone UDFContext. Also adding convenience method for frontend check.

Patch tested on cluster and run ant test -Dtestcase=TestRegisteredJarVisibility
",09/Mar/12 20:18;daijy;+1,16/Mar/12 21:22;thw;Attached e2e test to verify conf is always null in frontend.,16/Mar/12 22:20;daijy;+1. Patch committed to 0.9/0.10/trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Automagically setting parallelism based on input file size does not work with HCatalog,PIG-2573,12545590,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,traviscrawford,traviscrawford,traviscrawford,08/Mar/12 01:54,22/Feb/13 04:53,14/Mar/19 03:07,16/Mar/12 22:11,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"PIG-2334 was helpful in understanding this issue. Short version is input file size is only computed if the path begins with a whitelisted prefix, currently:

* /
* hdfs:
* file:
* s3n:

As HCatalog locations use the form {{dbname.tablename}} the input file size is not computed, and the size-based parallelism optimization breaks.

DETAILS:

I discovered this issue comparing two runs on the same script, one loading regular HDFS paths, and one with HCatalog db.table names. I just happened to notice the ""Setting number of reducers"" line difference.

{code:title=Loading HDFS files reducers is set to 99}
2012-03-08 01:33:56,522 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - BytesPerReducer=1000000000 maxReducers=999 totalInputFileSize=98406674162
2012-03-08 01:33:56,522 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Neither PARALLEL nor default parallelism is set for this job. Setting number of reducers to 99
{code}

{code:title=Loading with an HCatalog db.table name}
2012-03-08 01:06:02,283 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - BytesPerReducer=1000000000 maxReducers=999 totalInputFileSize=0
2012-03-08 01:06:02,283 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Neither PARALLEL nor default parallelism is set for this job. Setting number of reducers to 1
{code}

Possible fix: Pig should just ask the loader for the size of its inputs rather than special-casing certain location types.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11/Mar/12 05:14;traviscrawford;PIG-2573_get_size_from_stats_if_possible.diff;https://issues.apache.org/jira/secure/attachment/12517877/PIG-2573_get_size_from_stats_if_possible.diff,12/Mar/12 23:24;traviscrawford;PIG-2573_get_size_from_stats_if_possible_2.diff;https://issues.apache.org/jira/secure/attachment/12518118/PIG-2573_get_size_from_stats_if_possible_2.diff,13/Mar/12 19:03;traviscrawford;PIG-2573_get_size_from_stats_if_possible_3.diff;https://issues.apache.org/jira/secure/attachment/12518219/PIG-2573_get_size_from_stats_if_possible_3.diff,13/Mar/12 23:17;traviscrawford;PIG-2573_get_size_from_stats_if_possible_4.diff;https://issues.apache.org/jira/secure/attachment/12518267/PIG-2573_get_size_from_stats_if_possible_4.diff,08/Mar/12 21:07;traviscrawford;PIG-2573_move_getinputbytes_to_loadfunc.diff;https://issues.apache.org/jira/secure/attachment/12517609/PIG-2573_move_getinputbytes_to_loadfunc.diff,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2012-03-08 03:16:59.781,,,no_permission,,,,,,,,,,,,230776,,,,Thu Mar 15 05:21:10 UTC 2012,,,,,,,0|i0h39z:,97793,,,,,,,,,,"08/Mar/12 03:16;daijy;Yes, we should put the logic into LoadFunc. There will also be a patch for HCatLoader in HCat side. Anyone is working on a patch?","08/Mar/12 03:26;traviscrawford;Yup, I'm looking into this. After pig supports it I can also do the HCatLoader work, since it's what I'm primarily interested in.","08/Mar/12 21:07;traviscrawford;The attached patch shows what moving the ""get size of input"" logic into {{LoadFunc}} looks like.

I have some concerns about how invasive this approach is. Specifically it requires loaders to implement a new {{getLocation}} method for the optimization to work. While I can update the loaders shipped by pig, this breaks the optimization for all other loaders out there. Option: fall back to the current behavior if the loader returns null for getLocation (meaning its likely using the default implementation).

If we like this general approach I can polish & update all the included loaders. If we don't like this approach - any suggestions?","08/Mar/12 23:32;billgraham;Another approach that would be backward compatible would be to introduce an new interface that a LoadFunc can implement to advertise that it supports load statistics, like input size. Something similar to how StoreFuncs include a number of optional interfaces.

Also, should we think more broadly being able to return other info besides just total input bytes? Like we could instead return a new LoadStats object (or the InputStats class) that could encapsulate more than just the input size. Things like input size, records read, etc? It would be a move towards pushing stats collection into the loaders and store funcs.","09/Mar/12 01:02;dvryaboy;Bill, I know, let's call it LoadMetadata :-)

Discussed this with Travis, he'll just do instanceof LoadMetadata and go from there.","11/Mar/12 05:14;traviscrawford;Patch has been updated to get the size from loader-reported statistics, if possible. If not possible, the current behavior remains.","12/Mar/12 00:49;dvryaboy;Nice work, Travis.

Tests pass.

I think we might as well take PigStorageWithStatistics and move it into PigStorage. Might make sense to save a transient LRU map of location->stats in case getStats gets called a few times, so we don't keep hitting the NN for file sizes over and over.

It looks like the current behavior is that if you have multiple POLoads, and only a subset of them implement LoadMetadata and return non-0 size, FS is used. Meaning, if I have 2 loaders, and one of them reports size and the other does not, the first loader's reported size is ignored. That's ok (better than it is now!) but not ideal. Perhaps we can move the logic of checking metadata or the FS into the inner loop of getInputSizeFromLoadMetadata?  

Also, just stylistically, lets rename that method to getInputSizeFromMetadata (more readable, and refers to the concept, not an interface).
","12/Mar/12 21:59;traviscrawford;PIGSTORAGEWITHSTATISTICS COMMENT:

Originally I did something similar to what you suggested, but after a bit more thought kept PigStorage unchanged, and used a test-specific loader. Since we fall back to the existing ""get size from supported filesystems"" lookup, PigStorage already has this feature for most users. JobControlCompiler and PigStorage would call the same utility method to report size, so I think the code is actually more complex by updating PigStorage.

The goal here is letting a loader report the size of its input for non-filesystems (hcatalog db.table names, rows from hbase/vertica/mysql/...) , or when doing something fancy with files on a filesystem (indexed files where blocks/splits are pre-filtered). If you're doing something fancy you probably have a fancy loader too.

PARTIAL SIZE REPORTING COMMENT:

Having size be all-or-none was intentional. It seemed very confusion for pig to base a decision on one number (and log that input size) then have the MR job read a different amount of data. I think its best to keep the current behavior and only make this optimization if its based on the actual input size.

METHOD NAME COMMENT:

How does {{getInputSizeFromLoader}} sound?","12/Mar/12 23:24;traviscrawford;Updated patch renamed method, and total size is reported as the size of inputs size is available for. Rereading the existing code this is how it currently behaves.","13/Mar/12 17:01;julienledem;Hey Travis,
Looks good to me.

Some comments:

* in test/org/apache/pig/test/PigStorageWithStatistics.java
{code}
    private Long getInputmBytes() throws IOException {
...
        return inputBytes / 1024;
    }
{code}
I guess it should be / 1024 again to return mega bytes

* Let's add getByteSize() and setByteSize(size) to org.apache.pig.ResourceStatistics (with backward compatible implementations of getmBytes)

* in src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java, the following line seems unnecessary as long as the file system implementation returns a size. But we can open this as a separate issue.
{code}
 if (UriUtil.isHDFSFileOrLocalOrS3N(location)) {
{code}","13/Mar/12 19:03;traviscrawford;Good comments Julien, thanks.

Please see the comments for why we keep mbytes in ResourceStatistics - basically its public and changing could break things.","13/Mar/12 23:17;traviscrawford;Whoops, the previous diff missed a file. I ran test-commit with this one.","15/Mar/12 05:21;julienledem;ant test-patch

    [exec] -1 overall.  
    [exec] 
    [exec]     +1 @author.  The patch does not contain any @author tags.
    [exec] 
    [exec]     +1 tests included.  The patch appears to include 5 new or modified tests.
    [exec] 
    [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.
    [exec] 
    [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
    [exec] 
    [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
    [exec] 
    [exec]     -1 release audit.  The applied patch generated 541 release audit warnings (more than the trunk's current 534 warnings).

the new warnings are unrelated to this patch.

+1 on my side ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e2e harness deploy fails when using pig that does not bundle hadoop,PIG-2572,12545385,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thw,thw,thw,06/Mar/12 18:24,26/Apr/12 20:32,14/Mar/19 03:07,07/Mar/12 22:17,,,,,,,,0.10.0,0.11,0.9.3,,tools,,,0,,,,,,,,,,,,,"When deploying the pig e2e harness to run with our internal pig package (which does not bundle hadoop), the deploy target fails after generating the data and creating the out directory on HDFS. This is because HADOOP_HOME is not set when running pig (harness.hadoop.home ignored).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Mar/12 22:16;daijy;PIG-2572-2.patch;https://issues.apache.org/jira/secure/attachment/12517477/PIG-2572-2.patch,06/Mar/12 18:36;thw;PIG-2572.patch;https://issues.apache.org/jira/secure/attachment/12517277/PIG-2572.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-03-07 22:16:30.989,,,no_permission,,,,,,,,,,,,230571,Reviewed,,,Wed Mar 07 22:17:02 UTC 2012,,,,,,,0|i0h39r:,97792,,,,,,,,,,"06/Mar/12 18:36;thw;Set HADOOP_HOME on deploy, also allow overrides for PH_LOCAL and PH_OUT and log proper logfile path.",07/Mar/12 22:16;daijy;PIG-2572-2.patch is for 0.10/trunk.,"07/Mar/12 22:17;daijy;Patch committed to 0.9/0.10/trunk.

Thanks Thomas!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LimitOptimizer fails with dynamic LIMIT argument,PIG-2570,12545269,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,billgraham,billgraham,06/Mar/12 01:50,26/Apr/12 20:33,14/Mar/19 03:07,13/Mar/12 23:22,,,,,,,,0.10.0,0.11,,,,,,0,,,,,,,,,,,,,"The following script fails with the exception shown below. Passing {{-t LimitOptimizer}} makes it work, as does adding an {{ORDER}} clause before the limit.

{noformat}
A = LOAD 'data1.txt' AS (owner:chararray,pet:chararray,age:int,phone:chararray);
B = group A all; 
C = foreach B generate SUM(A.age) as total; 
D = foreach A generate owner, age/(double)C.total AS percentAge; 
F = LIMIT D C.total/8;
DUMP F;
{noformat}

{noformat}
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2000: Error processing rule LimitOptimizer. Try -t LimitOptimizer
	at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:122)
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:287)
	at org.apache.pig.PigServer.compilePp(PigServer.java:1317)
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1254)
	at org.apache.pig.PigServer.storeEx(PigServer.java:953)
	... 14 more
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2243: Attempt to remove operator LOLimit that is still softly connected in the plan
	at org.apache.pig.newplan.BaseOperatorPlan.remove(BaseOperatorPlan.java:174)
	at org.apache.pig.newplan.BaseOperatorPlan.removeAndReconnect(BaseOperatorPlan.java:449)
	at org.apache.pig.newplan.logical.rules.LimitOptimizer$OptimizeLimitTransformer.transform(LimitOptimizer.java:124)
	at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:110)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Mar/12 09:03;daijy;PIG-2570-1.patch;https://issues.apache.org/jira/secure/attachment/12517381/PIG-2570-1.patch,13/Mar/12 19:09;daijy;PIG-2570-2.patch;https://issues.apache.org/jira/secure/attachment/12518222/PIG-2570-2.patch,06/Mar/12 01:52;billgraham;data1.txt;https://issues.apache.org/jira/secure/attachment/12517183/data1.txt,06/Mar/12 01:52;billgraham;limit-fails.pig;https://issues.apache.org/jira/secure/attachment/12517184/limit-fails.pig,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-03-13 01:57:41.653,,,no_permission,,,,,,,,,,,,230455,Reviewed,,,Tue Mar 13 23:22:37 UTC 2012,,,,,,,0|i09ypb:,56063,,,,,,,,,,06/Mar/12 01:52;billgraham;Attaching a sample script and datafile to reproduce with.,"07/Mar/12 19:08;billgraham;+1

Thanks Daniel! This patch does the trick.","13/Mar/12 01:57;thejas;It is possible to have multiple scalar variables in the limit expression, this will result in limit having multiple 'soft' predecessors. This case also needs to be handled in the patch.
",13/Mar/12 19:09;daijy;Good catch. Attach PIG-2570-2.patch.,"13/Mar/12 19:14;dvryaboy;+1, please commit.","13/Mar/12 23:22;daijy;test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 535 release audit warnings (more than the trunk's current 532 warnings).

javadoc and release audit warning is unrelated. 

Patch committed to 0.10/trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix org.apache.pig.test.TestInvoker.testSpeed,PIG-2569,12545013,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aklochkov,dreambird,dreambird,03/Mar/12 00:37,30/May/13 00:17,14/Mar/19 03:07,03/Aug/12 23:09,0.9.2,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"the Pig unit test org.apache.pig.test.TestInvoker.testSpeed pass sometimes and fail sometimes. I think this test need further polish, look at the code:
{noformat}
@Test
    public void testSpeed() throws IOException, SecurityException, ClassNotFoundException, NoSuchMethodException {
        EvalFunc<Double> log = new Log();
        Tuple tup = tf_.newTuple(1);
        long start = System.currentTimeMillis();
        for (int i=0; i < 1000000; i++) {
            tup.set(0, (double) i);
            log.exec(tup);
        }
        long staticSpeed = (System.currentTimeMillis()-start);
        start = System.currentTimeMillis();
        log = new InvokeForDouble(""java.lang.Math.log"", ""Double"", ""static"");
        for (int i=0; i < 1000000; i++) {
            tup.set(0, (double) i);
            log.exec(tup);
        }
        long dynamicSpeed = System.currentTimeMillis()-start;
        System.err.println(""Dynamic to static ratio: ""+((float) dynamicSpeed)/staticSpeed);
        assertTrue( ((float) dynamicSpeed)/staticSpeed < 5);
    }
{noformat}

I understand this test is trying to prevent the initicialization time of InvokeForDouble doesn't take too long, but the ratio 5 is hardcoded, and there is no solid logic behind it why it is 5. For my understand, when the server resouce is low, ratio could be larger than 5, but it doesn't mean code has problem. For our case, the code never change, but it pass in the first run, but fail in the second run.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Aug/12 05:35;aklochkov;PIG-2569.patch;https://issues.apache.org/jira/secure/attachment/12538997/PIG-2569.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-05 22:28:08.748,,,no_permission,,,,,,,,,,,,230199,,,,Thu May 30 00:17:59 UTC 2013,,,,,,,0|i02tvz:,14443,,,,,,,,,,"05/Mar/12 22:28;dvryaboy;When I wrote that test, the ratio was more like two, and the idea behind hardcoding 5 was that if it's that slow, it's not worth using and we should rethink. 

I am curious why it just started failing for people -- the code's been around for a year, and I got reports of the test failing here and in Elephant Bird a week apart. Is there a new Java build that drastically slowed down reflection or something?

What is your java environment?",05/Mar/12 22:36;dreambird;The Java build I am using is JDK6u26.,"03/Aug/12 00:53;aklochkov;This test fails for me when code coverage calculation is enabled, it happens permanently on my MacBook 2.26GHz with Sun JDK 1.6.0_33-b03-424. The ratio calculated by test is between 5 and 6 in this case.

How about excluding this test explicitly when Clover is enabled?

Johnny, could it be the case that Clover was enabled when you were hitting the problem? ","03/Aug/12 01:13;dreambird;Andrey, yes Clover is enabled when test was running. I think it is a good idea to disable this test when Clover is enabled, in build.xml",03/Aug/12 05:36;aklochkov;Attaching patch. The test was moved into a separate class to be able to disable it in build.xml when Clover is enabled. ,03/Aug/12 22:12;dvryaboy;Can you guys confirm that the test passes with Clover disabled?,"03/Aug/12 22:22;aklochkov;Per my observations, with Clover disabled the ratio is ~2 or less. When Clover is enabled, it's between 5 and 6. I applied the fix in our fork of Pig and haven't seen failures in our CI after that (with Clover enabled). ",03/Aug/12 23:09;dvryaboy;Committed to trunk. Thanks Andrey!,"30/May/13 00:17;aniket486;With java version 1.6.0_45+, the ratio is almost about 4-5, so the test fails sometimes. It could be JVM problem. Do we need this test in pig?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigOutputCommitter hide exception in commitJob,PIG-2568,12544844,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,01/Mar/12 21:30,26/Apr/12 20:33,14/Mar/19 03:07,02/Mar/12 19:52,,,,,,,,0.10.0,0.11,0.9.3,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Mar/12 21:31;daijy;PIG-2568-1.patch;https://issues.apache.org/jira/secure/attachment/12516735/PIG-2568-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-02 18:08:46.853,,,no_permission,,,,,,,,,,,,230030,Reviewed,,,Fri Mar 02 19:52:03 UTC 2012,,,,,,,0|i09yun:,56087,,,,,,,,,,02/Mar/12 18:08;alangates;+1.  Swallowing exceptions and asserting they shouldn't happen is always bad.,02/Mar/12 19:52;daijy;Patch committed to 0.9/0.10/trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build fails - Hadoop 0.23.1-SNAPSHOT no longer available,PIG-2564,12544651,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,thw,thw,thw,29/Feb/12 18:16,26/Apr/12 20:32,14/Mar/19 03:07,01/Mar/12 08:02,0.10.0,0.11,0.9.2,,,,,0.10.0,0.11,0.9.3,,,,,2,,,,,,,,,,,,,"Builds for 0.23 currently fail. 0.23.1-SNAPSHOT no longer available, 0.23.2-SNAPSHOT is:

https://repository.apache.org/content/groups/snapshots-group/org/apache/hadoop/hadoop-hdfs/0.23.1-SNAPSHOT/
https://repository.apache.org/content/groups/snapshots-group/org/apache/hadoop/hadoop-hdfs/0.23.2-SNAPSHOT/

We should switch to 0.23.1 release.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Feb/12 19:15;thw;PIG-2564.patch;https://issues.apache.org/jira/secure/attachment/12516590/PIG-2564.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-01 08:02:57.624,,,no_permission,,,,,,,,,,,,229837,Reviewed,,,Thu Mar 01 08:02:57 UTC 2012,,,,,,,0|i0h393:,97789,,,,,,,,,,29/Feb/12 19:04;thw;Running tests.,"01/Mar/12 02:14;thw;Tests pass (run for 0.9 branch).
",01/Mar/12 08:02;daijy;Patch committed to 0.9/0.10/trunk. Thanks Thomas!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexOutOfBoundsException: while projecting fields from a bag,PIG-2563,12544636,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,vivekp,vivekp,29/Feb/12 15:57,26/Apr/12 20:32,14/Mar/19 03:07,18/Mar/12 08:24,0.10.0,0.9.1,,,,,,0.10.0,0.11,,,,,,0,,,,,,,,,,,,,"The below script fails with Pig 0.9 / Pig 0.10 but works fine for Pig 0.8.
{code}
A = load 'i1' as (a,b,c:chararray);
B = load 'i2' as (d,e,f:chararray);
C = cogroup A by a, B by d;
D = foreach C { 
  tmp = B.d;
  tmp_dis = distinct tmp;
  generate A,B,tmp_dis ; } ;
E = foreach D generate B.(d,e) as v;
dump E;
{code}

The script fails with the below exception. Looks like DereferenceExpression is using wrong schema to build inner schema.

java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.pig.newplan.logical.relational.LogicalSchema.getField(LogicalSchema.java:653)
	at org.apache.pig.newplan.logical.expression.DereferenceExpression.getFieldSchema(DereferenceExpression.java:167)
	at org.apache.pig.newplan.logical.relational.LOGenerate.getSchema(LOGenerate.java:88)
	at org.apache.pig.newplan.logical.visitor.TypeCheckingRelVisitor.visit(TypeCheckingRelVisitor.java:160)
	at org.apache.pig.newplan.logical.relational.LOGenerate.accept(LOGenerate.java:242)

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Mar/12 22:12;daijy;PIG-2563-1.patch;https://issues.apache.org/jira/secure/attachment/12517135/PIG-2563-1.patch,16/Mar/12 18:56;daijy;PIG-2563-2.patch;https://issues.apache.org/jira/secure/attachment/12518719/PIG-2563-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-03-01 17:43:50.042,,,no_permission,,,,,,,,,,,,229822,Reviewed,,,Sun Mar 18 08:24:31 UTC 2012,,,,,,,0|i09ymv:,56052,,,,,,,,,,"01/Mar/12 17:43;jcoveney;Good catch! I've seen similar errors before, but was never able to isolate it. I actually isolated it with an even smaller snippet below:

{code}
A = load 'i1' as (a,b,c:chararray);
B = GROUP A BY a;
C = foreach B { 
  tmp = A.a;
  generate A, tmp; } ;
D = foreach C generate A.(a,b) as v;
{code}

Perhaps related is the following, which has a weird parse error:

{code}
A = load 'i1' as (a,b,c:chararray);
B = GROUP A BY a;
C = foreach B { 
  tmp = A.a;
  generate A; } ;
D = foreach C generate A.(a,b) as v;
{code}","03/Mar/12 02:21;daijy;Logical plan for the foreach nested plan is wrong. Using Jonathan's first test case as example:

    |---C: (Name: LOForEach Schema: A#21:bag{#22:tuple(a#18:bytearray,b#19:bytearray,c#20:chararray)},tmp#21:bag{#22:tuple(a#18:bytearray)})
        |   |
        |   (Name: LOGenerate[false,false] Schema: A#21:bag{#22:tuple(a#18:bytearray,b#19:bytearray,c#20:chararray)},tmp#21:bag{#22:tuple(a#18:bytearray)})
        |   |   |
        |   |   A:(Name: Project Type: bag Uid: 21 Input: 0 Column: (*))
        |   |   |
        |   |   tmp:(Name: Project Type: bag Uid: 21 Input: 1 Column: (*))
        |   |
        |   |---A: (Name: LOInnerLoad[1] Schema: a#18:bytearray,b#19:bytearray,c#20:chararray)

Here we only generate one LOInnerLoad. We should generate two LOInnerLoad instead.","03/Mar/12 02:22;daijy;The plan is misformatted:
{code}
    |---C: (Name: LOForEach Schema: A#21:bag{#22:tuple(a#18:bytearray,b#19:bytearray,c#20:chararray)},tmp#21:bag{#22:tuple(a#18:bytearray)})
        |   |
        |   (Name: LOGenerate[false,false] Schema: A#21:bag{#22:tuple(a#18:bytearray,b#19:bytearray,c#20:chararray)},tmp#21:bag{#22:tuple(a#18:bytearray)})
        |   |   |
        |   |   A:(Name: Project Type: bag Uid: 21 Input: 0 Column: (*))
        |   |   |
        |   |   tmp:(Name: Project Type: bag Uid: 21 Input: 1 Column: (*))
        |   |
        |   |---A: (Name: LOInnerLoad[1] Schema: a#18:bytearray,b#19:bytearray,c#20:chararray)
{code}",05/Mar/12 22:12;daijy;The real cause is the uid conflict in nested project. ,"05/Mar/12 22:25;dvryaboy;Cheap code style comments:

The if/else blocks should either be bracketed by {}s, or converted to 
fieldSchema.uid = needNewUid? LogicalExpression.getNextUid() : innerLoads.get(0).getProjection().getFieldSchema().uid;

(Otherwise we get oddness when people try to edit the code and get confused about what is and isn't inside the if/else blocks).

More expensive code content comments:

Feels wrong to rely on checking for the parent, determining if it's a foreach, and returning a boolean to enforce id uniqueness. It's entirely possible that someone touches one of these operators without going through findReachableInnerLoad.... and still needs a unique id.

Can we ensure the id is unique to begin with, when the tree is constructed? ","07/Mar/12 07:41;daijy;bq. Cheap code style comments
sure will change

bq. More expensive code content comments
Not sure if I completely understand your point, let me explain the design of foreach nested plan and why I make the change. Let me know if you need further explanation. Uid and schema inference process is very core to logical plan. If one changes anywhere in the process, he needs to make sure the existing functionality is not broken. In the patch, I change the way project infer its uid, because earlier, it does not generate new uid for the new bag after nested foreach. Here is how uid for foreach inner plan works:
# every foreach statement starts with LOInnerLoad, ends with LOGenerate
# simple foreach should keep uid, eg: foreach a generate $1, $2, we shall keep the uid for $1, $2, even if it is a bag column, there are couple of places make this assumption
# if input column is a bag, LOInnerLoad take the schema of its inner schema, eg, if $1 is bag#2{t#3(x#4, y#5)}, LOInnerLoad will have the schema (x#4, y#5), it can be followed with nested operator
# LOGenerate regenerates the bag after the inner operator pipeline, in this case, bag#2{t#3(x#4, y#5)}, we need to keep uid
# currently all nested operator does not change uid, except ForEach, that is the approach I took in the patch: unless see a ForEach, reuse uid

Here are complete examples:
{code}
b = foreach a generate a1, a2; (a0:xxxx, a1:chararray#1, a2:bag#2{t#3(x#4, y#5)})

LOInnerLoad(a1:chararray)     LOInnerLoad(x#4, y#5)
                    \            /
                    LOGenerate(a1:chararray#1, a2:bag#2{t#3(x#4, y#5)})
{code}

{code}
b = foreach a { c = filter a2 by x==1;generate a1, c; }; (a0:xxxx, a1:chararray#1, a2:bag#2{t#3(x#4, y#5)})

LOInnerLoad(a1:chararray)     LOInnerLoad(x#4, y#5)
                    \            /
                     \        LOFilter(x#4, y#5)
                      \        /
                    LOGenerate(a1:chararray#1, c:bag#2{t#3(x#4, y#5)})
{code}

{code}
b = foreach a { c = a2.x;generate a1, c; }; (a0:xxxx, a1:chararray#1, a2:bag#2{t#3(x#4, y#5)})

LOInnerLoad(a1:chararray)     LOInnerLoad(x#4, y#5)
                    \            /
                     \        LOForEach(x#4)
                      \        /
                    LOGenerate(a1:chararray#1, c:bag#7{t#6(x#4)})
{code}","08/Mar/12 07:49;dvryaboy;Daniel, I think I understand. But the method looks a bit off to me, codestyle-wise, as it moves logic that properly belongs elsewhere into itself and conflates what it's supposed to do (find a reachable inner load) with what you want to do with the thing it found (change the uid).","14/Mar/12 23:34;daijy;Hi, Dmitriy, yes I also check LOForEach along the way finding innerload. Otherwise I need to traverse the plan two times. Maybe I can change the name of the function to reveal this?","16/Mar/12 18:56;daijy;Address Dmitriy's comment, put a comment to findReacheableInnerLoadFromBoundaryProject to describe the nature of the function.",17/Mar/12 01:41;thejas;+1,"18/Mar/12 08:24;daijy;test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

javadoc warning is unrelated.

Patch committed to 0.10/trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Embedded pig in python; invoking sys.exit(0) causes script failure,PIG-2559,12544242,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,vivekp,vivekp,vivekp,27/Feb/12 11:27,26/Apr/12 20:32,14/Mar/19 03:07,01/Mar/12 22:59,0.10.0,0.9.1,,,,,,0.10.0,0.11,,,,,,0,,,,,,,,,,,,,"In embedded pig in python, if I have a sys.exit (0) the script always fails and returns exit code as 6.
While it is agreeable that Pig will reinterpret the exit code from Python, sys.exit(0) should be considered as a normal termination.

A sample code;
{code}
#!/usr/bin/python
from org.apache.pig.scripting import Pig
import sys
if 1 == 2:
        sys.exit(1)
else:
        sys.exit(0)
{code}

Exception from Pig
{code}
org.apache.pig.backend.executionengine.ExecException: ERROR 1121: Python Error. Traceback (most recent call last):
  File ""a.py"", line 9, in <module>
    sys.exit(0)
SystemExit: 0

	at org.apache.pig.scripting.jython.JythonScriptEngine$Interpreter.execfile(JythonScriptEngine.java:107)
	at org.apache.pig.scripting.jython.JythonScriptEngine.load(JythonScriptEngine.java:210)
	at org.apache.pig.scripting.jython.JythonScriptEngine.main(JythonScriptEngine.java:202)
	at org.apache.pig.scripting.ScriptEngine.run(ScriptEngine.java:275)
	at org.apache.pig.Main.runEmbeddedScript(Main.java:925)
	at org.apache.pig.Main.run(Main.java:516)
	at org.apache.pig.Main.main(Main.java:111)
Caused by: Traceback (most recent call last):
  File ""a.py"", line 9, in <module>
    sys.exit(0)
SystemExit: 0

	at org.python.core.PyException.fillInStackTrace(PyException.java:70)
	at java.lang.Throwable.<init>(Throwable.java:181)
	at java.lang.Exception.<init>(Exception.java:29)
	at java.lang.RuntimeException.<init>(RuntimeException.java:32)
	at org.python.core.PyException.<init>(PyException.java:46)
	at org.python.core.PyException.<init>(PyException.java:43)
	at org.python.core.PySystemState.exit(PySystemState.java:1206)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.python.core.PyReflectedFunction.__call__(PyReflectedFunction.java:175)
	at org.python.core.PyObject.__call__(PyObject.java:355)
	at org.python.core.PyMethod.__call__(PyMethod.java:215)
	at org.python.core.PyMethod.instancemethod___call__(PyMethod.java:221)
	at org.python.core.PyMethod.__call__(PyMethod.java:206)
	at org.python.core.PyObject.__call__(PyObject.java:397)
	at org.python.core.PyObject.__call__(PyObject.java:401)
	at org.python.pycode._pyx0.f$0(a.py:9)
	at org.python.pycode._pyx0.call_function(a.py)
	at org.python.core.PyTableCode.call(PyTableCode.java:165)
	at org.python.core.PyCode.call(PyCode.java:18)
	at org.python.core.Py.runCode(Py.java:1197)
	at org.python.util.PythonInterpreter.execfile(PythonInterpreter.java:166)
	at org.apache.pig.scripting.jython.JythonScriptEngine$Interpreter.execfile(JythonScriptEngine.java:104)
	... 6 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Feb/12 13:16;vivekp;PIG-2559_1.patch;https://issues.apache.org/jira/secure/attachment/12516163/PIG-2559_1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-27 19:36:54.732,,,no_permission,,,,,,,,,,,,229479,Reviewed,,,Thu Mar 01 22:59:28 UTC 2012,,,,,,,0|i0h387:,97785,,,,,,,,,,"27/Feb/12 11:34;vivekp;I ran a test script with jython command line and there is no such exceptions happening.
Looking further into jython source, I think PyException will be thrown in case of sys.exit is invoked. But it is handled further by org.python.core.Py.

I believe Pig should also check the Exception thrown by python and not re-throw the exception if the exit code is set to 0.",27/Feb/12 13:16;vivekp;Attaching an initial patch,"27/Feb/12 19:36;daijy;Patch looks good to me. Is there anything else you want to do since you mention it is ""initial patch""?","28/Feb/12 11:51;vivekp;Nothing more, I was just anticipating any comments. test-commit was passing for this patch.
","01/Mar/12 22:59;daijy;Unit test pass. test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 533 release audit warnings (more than the trunk's current 530 warnings).

javadoc and release audit warning is unrelated. 

Patch committed to 0.10/trunk. Thanks Vivek!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CSVExcelStorage load: quoted field with newline as first character sees newline as record end ,PIG-2556,12544087,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,tivv,stickyhipp,stickyhipp,25/Feb/12 01:01,22/Feb/13 04:53,14/Mar/19 03:07,20/Aug/12 20:14,0.9.1,,,,,,,0.11,,,,piggybank,,,1,newbie,patch,,,,,,,,,,,Loading a record that contains a newline as the first character in a quoted field is broken.  The loader interprets the quoted newline as the record delimiter.  I've identified and fixed the bug and added a new testcase to expose it.  I'll post a patch soon.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Aug/12 14:04;tivv;csv3.patch;https://issues.apache.org/jira/secure/attachment/12541594/csv3.patch,27/Feb/12 19:27;stickyhipp;patch_PIG2556_CSVExcelStorage.patch;https://issues.apache.org/jira/secure/attachment/12516200/patch_PIG2556_CSVExcelStorage.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-08-17 12:24:12.245,,,no_permission,,,,,,,,,,,,229325,,,,Mon Aug 20 20:14:43 UTC 2012,,,Patch Available,,,,0|i0h37j:,97782,,,,,,,,,,"27/Feb/12 19:25;stickyhipp;Index: src/test/java/org/apache/pig/piggybank/test/storage/TestCSVExcelStorage.java
===================================================================
--- src/test/java/org/apache/pig/piggybank/test/storage/TestCSVExcelStorage.java	(revision 1294285)
+++ src/test/java/org/apache/pig/piggybank/test/storage/TestCSVExcelStorage.java	(working copy)
@@ -49,6 +49,7 @@
     
     String testFileCommaName = ""testFileComma.csv"";
     String testFileTabName = ""testFileTab.csv"";
+    String testFileNewlines = ""testFileNewlines.csv"";
 
     String testStrComma = 
     	""John,Doe,10\n"" +
@@ -124,7 +125,36 @@
     		add(Util.createTuple(new String[] {""Frank"",""Clean"",""70""}));
     	}
     };
-    
+
+    String[] testFileNewlinesArray = new String[] {
+            ""One,Two,Three"",
+            ""123,\""\nSecond line\nThird line\"", \""456\""""  // notice that the space after the comma but before the quote
+            // is considered to be part of the 3rd field.  TBD if that's correct.
+    };
+
+    @SuppressWarnings(""serial"")
+	ArrayList<Tuple> testStrNewlinesResultTuples =
+    	new ArrayList<Tuple>() {
+    	{
+    		add(Util.createTuple(new String[] {""One"",""Two"",""Three""}));
+    		add(Util.createTuple(new String[] {""123"", ""\nSecond line\nThird line"","" 456""}));
+    	}
+    };
+
+
+    @Test
+    public void testNewline() throws IOException {
+
+        // Read the test file:
+        String script =
+        	""a = LOAD '"" + testFileNewlines + ""' "" +
+        	""USING org.apache.pig.piggybank.storage.CSVExcelStorage(',', 'YES_MULTILINE');"";
+        Util.registerMultiLineQuery(pigServer, script);
+        compareExpectedActual(testStrNewlinesResultTuples, ""a"");
+
+
+    }
+
     public TestCSVExcelStorage() throws ExecException, IOException {
 
         pigServer = new PigServer(ExecType.LOCAL);
@@ -135,6 +165,7 @@
         
         Util.createLocalInputFile(testFileCommaName, testStrCommaArray);
         Util.createLocalInputFile(testFileTabName, testStrTabArray);
+        Util.createLocalInputFile(testFileNewlines, testFileNewlinesArray);
     }
 
     @Test
@@ -148,7 +179,7 @@
         assertEquals(Util.createTuple(new String[] {""foo"", ""bar"", ""baz""}), it.next());
     }
    
-    @Test 
+    @Test
     public void testQuotedCommas() throws IOException {
         String inputFileName = ""TestCSVExcelStorage-quotedcommas.txt"";
         Util.createLocalInputFile(inputFileName, new String[] {""\""foo,bar,baz\"""", ""fee,foe,fum""});
Index: src/main/java/org/apache/pig/piggybank/storage/CSVExcelStorage.java
===================================================================
--- src/main/java/org/apache/pig/piggybank/storage/CSVExcelStorage.java	(revision 1294285)
+++ src/main/java/org/apache/pig/piggybank/storage/CSVExcelStorage.java	(working copy)
@@ -622,6 +622,10 @@
 				// that entire field is quoted:
 				getNextInQuotedField = true;
 				evenQuotesSeen = true;
+                if (i == recordLen - 1) {
+                    fieldBuffer.put(b);
+					sawEmbeddedRecordDelimiter = true;
+                }
 			} else if (b == FIELD_DEL) {
 				readField(fieldBuffer, getNextFieldID++); // end of the field
 			} else {
",27/Feb/12 19:27;stickyhipp;Patch file attached,"17/Aug/12 12:24;tivv;I've also got into this bug. 
Should have check jira for patch before fixing :), but here is my pull request with patch in case this would make fix progress faster: https://github.com/apache/pig/pull/6","17/Aug/12 12:55;tivv;Actually there is one more problem: ""Test """"quoted""""\ndata"" also does not work OK. Fixed in second commit. Will attach updated patch to this ticket. Also code is in pull request",17/Aug/12 12:59;tivv;Patch from https://github.com/apache/pig/pull/6/,"17/Aug/12 20:53;dvryaboy;Looks good, but the spacing is kind of crazy -- did tabs get in or something?

+                if (i == recordLen - 1) {
+                    fieldBuffer.put(b);
+					sawEmbeddedRecordDelimiter = true;
+                }","17/Aug/12 20:54;dvryaboy;Let me try that again..

{code}
+                if (i == recordLen - 1) {
+                    fieldBuffer.put(b);
+					sawEmbeddedRecordDelimiter = true;
+                }
{code}","17/Aug/12 20:56;dvryaboy;Assigning to Vitalii :-)

",18/Aug/12 16:32;tivv;I can see spacing problems in my patch (CSVExcelProblems-2.patch). I will fix  settings im my IDE and resubmit.,20/Aug/12 14:04;tivv;Patch with better padding,"20/Aug/12 14:06;tivv;OK, I've recreated patch with better spacing (csv3.patch). I've set up my IDE for tab to be 8 chars, don't use tabs, use 4-chars indent. There still can be problems because original file uses tabs. I can reformat it without tabs, but did not do it for now.","20/Aug/12 20:14;dvryaboy;Committed to trunk. Thanks Vitalii and Peter!

D",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Custom tuple results in ""Unexpected datatype 110 while reading tuplefrom binary file"" while spilling",PIG-2550,12543829,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,vivekp,vivekp,23/Feb/12 12:31,26/Apr/12 20:32,14/Mar/19 03:07,23/Mar/12 23:59,0.10.0,0.8.1,0.9.1,,,,,0.10.0,0.11,0.9.3,,,,,0,,,,,,,,,,,,,"In the below script ;

{code}
a = load 'gen_data/' AS (f1,f2);
b = load 'gen_data_02/' AS (f1,f2);
c = cogroup a by f1,b by f1;
d = foreach c generate group,flatten(a),COUNT(b),flatten(UDFReturningMyCustomTuple(b,a));
store d into 'test006';
{code}

The udf (UDFReturningMyCustomTuple) returns a bag which contains custom tuples.
The script execution fails at the reducer side with the below exception while reading back the spilled data,

2012-02-23 10:37:16,840 FATAL org.apache.pig.data.DefaultDataBag: Unable to read our spill file.
org.apache.pig.backend.executionengine.ExecException: ERROR 2112: Unexpected datatype 110 while reading tuple from binary file.
	at org.apache.pig.data.BinInterSedes.getTupleSize(BinInterSedes.java:133)
	at org.apache.pig.data.BinInterSedes.addColsToTuple(BinInterSedes.java:556)
	at org.apache.pig.data.BinSedesTuple.readFields(BinSedesTuple.java:66)
	at org.apache.pig.data.DefaultDataBag$DefaultDataBagIterator.next(DefaultDataBag.java:215)
	at org.apache.pig.data.DefaultDataBag$DefaultDataBagIterator.hasNext(DefaultDataBag.java:158)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:301)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:208)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:459)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:427)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:407)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:261)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1082)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)

It looks like while spilling we do MyCustomTuple.write(DataOutput out) which writes the type as DataType.TUPLE (110),
but while reading back we always use BinSedesTuple.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Mar/12 08:02;daijy;PIG-2550-1.patch;https://issues.apache.org/jira/secure/attachment/12516799/PIG-2550-1.patch,23/Feb/12 12:37;vivekp;REPRODUCING_SPILL_ERROR.txt;https://issues.apache.org/jira/secure/attachment/12515746/REPRODUCING_SPILL_ERROR.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-02-27 19:24:01.366,,,no_permission,,,,,,,,,,,,229068,Reviewed,,,Fri Mar 23 23:59:24 UTC 2012,,,,,,,0|i09yk7:,56040,,,,,,,,,,"23/Feb/12 12:37;vivekp;Attaching the artifacts used to reproduce this issue.

-Dmapred.reduce.child.java.opts=""-Xmx512M""

It looks like this job runs fine with Pig 0.7","23/Feb/12 16:21;vivekp;I got the script running after overriding read and write methods in the custom tuple

{code}
public class MyCustomTuple extends DefaultTuple {
    private static final long serialVersionUID = 8156382697467819543L;
    private static final InterSedes sedes = InterSedesFactory.getInterSedesInstance();
    public MyCustomTuple() {
        super();
    }
    public MyCustomTuple(Object t) {
        super();
        append(t);
    }
    public void write(DataOutput out) throws IOException {
        sedes.writeDatum(out, this);
    }
    public void readFields(DataInput in) throws IOException {
        // Clear our fields, in case we're being reused.
        mFields.clear();
        sedes.addColsToTuple(in, this);
    } 

}
{code}
I am not sure whether overriding write() will have any other impacts. Could this be considered as a workaround ?","27/Feb/12 19:24;daijy;I follow your instructions but not able to reproduce it. In step 2, you mention how gen_data_02 is generated, but how gen_data is generated? Is that reproducible in local mode?","28/Feb/12 11:49;vivekp;For the error to happen DefaultDataBag should be spilling data on reduce side. This test was run on MR mode.

gen_data is same as asgen_data_02 but with a smaller value.

{code}
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
public class Gen {

    public static void main(String[] args) throws IOException {
        BufferedWriter bw = new BufferedWriter( new FileWriter( new File (""tmp_data"")));

        StringBuffer sb = new StringBuffer ();
        for(int i=0;i < 200; i ++)
            sb.append(i);
        bw.write(sb.toString());
        bw.write(""\t"");
        
	for(int i=0;i < 5*1000 ; i ++)
            bw.write(""""+i%10);
        bw.close();        
    }
}
{code}

BTW I tried to set pig.data.tuple.factory.name to org.apache.pig.data.DefaultTupleFactory but the property is not picked up in the map/reduce tasks.",02/Mar/12 00:49;amolkekre;Any updates on when we can get a fix?,"02/Mar/12 03:35;daijy;I was able to reproduce now. The problem is we spill data using custom tuple's serializer, but when we read it back, we use BinInterSedes. I try to make a fix tomorrow. ","02/Mar/12 08:02;daijy;Vivek, can you try the patch?","05/Mar/12 10:37;vivekp;Thanks Daniel, the script goes through fine with the patch",05/Mar/12 18:03;dvryaboy;That only fixes the spilling behavior. It does not fix the fact that custom tuples whose serialization does not match BinInterSedes can't be used across the Map-Reduce boundary.,"05/Mar/12 18:58;daijy;That's true. After serialization/deserialization, you will get BinSedesTuple instead of custom tuple. ","16/Mar/12 00:17;thejas;bq. That only fixes the spilling behavior. It does not fix the fact that custom tuples whose serialization does not match BinInterSedes can't be used across the Map-Reduce boundary.
Yes, the tuples will be serialized using the BinInterSedes format at the map-reduce boundary, and BinInterTuples will be created after the boundary. I don't see a bug there.

+1 for the patch.

","23/Mar/12 23:40;amolkekre;Daniel, Thejas,
Can this patch be committed. We are looking to launch it on grids asap.","23/Mar/12 23:59;daijy;Committed to 0.9/0.10/trunk. For trunk, it is actually fixed in PIG-2359. Only commit the test case to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.pig.piggybank.storage.avro - Broken documentation link for AvroStorage,PIG-2549,12543727,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,chrisas,chrisas,chrisas,22/Feb/12 18:19,22/Feb/13 04:53,14/Mar/19 03:07,23/Feb/12 03:49,0.9.1,0.9.2,,,,,,0.11,,,,documentation,,,0,,,,,,,,,,,,,"The current link to the AvroStorage document points to ""http://snaprojects.jira.com/wiki/display/HTOOLS/AvroStorage+-+Pig+support+for+Avro+data"" which doesn't exist AFAIK.

However, the Wiki has the relevant page and even references the broken link as the source of the information.

Link needs to be updated to point to the Wiki page: https://cwiki.apache.org/PIG/avrostorage.html",,,1800,1800,,0%,1800,1800,,,,,,,,,,,,,,,,,,,,,,,,23/Feb/12 03:46;daijy;PIG-2549-1.patch;https://issues.apache.org/jira/secure/attachment/12515691/PIG-2549-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-23 03:49:23.09,,,no_permission,,,,,,,,,,,,228966,,,,Thu Feb 23 03:49:23 UTC 2012,,,,,,,0|i0h36f:,97777,,,,,,,,,,23/Feb/12 03:49;daijy;Patch committed. Thanks Chris for reporting!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigStats.isSuccessful returns false if embedded pig script has sh commands,PIG-2543,12543320,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,vivekp,vivekp,20/Feb/12 10:02,26/Apr/12 20:32,14/Mar/19 03:07,13/Mar/12 22:00,0.10.0,0.9.1,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"For the below script even though the command is successful, result.isSuccessful() returns false and hence raise exception

{code}
#!/usr/bin/python
from org.apache.pig.scripting import Pig
Q = Pig.compile(""sh echo mymessage"")
result = Q.bind().runSingle()
if result.isSuccessful() :
    print 'Pig job succeeded'
else :
    raise 'Cant run sh command'
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Mar/12 23:14;daijy;PIG-2543-1.patch;https://issues.apache.org/jira/secure/attachment/12517483/PIG-2543-1.patch,13/Mar/12 01:09;daijy;PIG-2543-2.patch;https://issues.apache.org/jira/secure/attachment/12518134/PIG-2543-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-03-07 08:04:02.392,,,no_permission,,,,,,,,,,,,228566,Reviewed,,,Tue Mar 13 22:00:03 UTC 2012,,,,,,,0|i09ypj:,56064,,,,,,,,,,"07/Mar/12 08:04;daijy;The current design of PigStats is all based on mapreduce jobs. It does not have the position for ""sh"" commands. I propose to adding a flag auxiliaryErrors to PigStats to track non mapreduce errors, and isSuccessful returns true only if all mapreduce job success and no auxiliaryErrors.
","07/Mar/12 23:14;daijy;For this particular issue, we don't need to change PigStats interface. All we need is to return success if no mapreduce job is running. I change ""sh"" command so it throws exception when error occur. This is in par with ""fs"" and ""sql"" command.",13/Mar/12 01:09;daijy;PIG-2543-2.patch fix unit test failures.,13/Mar/12 20:23;thejas;+1,13/Mar/12 22:00;daijy;Patch committed to 0.10/trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroStorage can't read schema on amazon s3 in elastic mapreduce,PIG-2540,12542952,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,russell.jurney,russell.jurney,russell.jurney,16/Feb/12 22:16,22/Jun/12 03:36,14/Mar/19 03:07,27/Mar/12 02:56,0.10.0,0.9.1,,,,,,0.10.0,0.11,0.9.3,,data,piggybank,,0,avro,avro_udf,aws,emr,pants,pig,s3,sad,unhappy,,,,"grunt> emails = load 's3://agile.data/again_inbox' using AvroStorage();
grunt> describe emails
Schema for emails unknown.
grunt> a = limit emails 10;
grunt> dump a
2012-02-16 22:15:58,347 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: LIMIT
2012-02-16 22:15:58,483 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false
2012-02-16 22:15:58,542 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 1
2012-02-16 22:15:58,542 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 1
2012-02-16 22:15:58,632 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added to the job
2012-02-16 22:15:58,658 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2012-02-16 22:15:58,665 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2017: Internal error creating job configuration.
2012-02-16 22:15:58,665 [main] ERROR org.apache.pig.tools.grunt.Grunt - org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias a
	at org.apache.pig.PigServer.openIterator(PigServer.java:901)
	at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:652)
	at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:303)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:188)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:164)
	at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:67)
	at org.apache.pig.Main.run(Main.java:497)
	at org.apache.pig.Main.main(Main.java:111)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
Caused by: org.apache.pig.PigException: ERROR 1002: Unable to store alias a
	at org.apache.pig.PigServer.storeEx(PigServer.java:1000)
	at org.apache.pig.PigServer.store(PigServer.java:963)
	at org.apache.pig.PigServer.openIterator(PigServer.java:876)
	... 12 more
Caused by: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobCreationException: ERROR 2017: Internal error creating job configuration.
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJob(JobControlCompiler.java:731)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.compile(JobControlCompiler.java:263)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:149)
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1314)
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1299)
	at org.apache.pig.PigServer.storeEx(PigServer.java:996)
	... 14 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:352)
	at org.apache.pig.piggybank.storage.avro.AvroStorage.setLocation(AvroStorage.java:138)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJob(JobControlCompiler.java:387)
	... 19 more
",Amazon Elastic MapReduce,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Mar/12 02:35;russell.jurney;PIG-2540.tests_fail.patch;https://issues.apache.org/jira/secure/attachment/12519009/PIG-2540.tests_fail.patch,21/Mar/12 03:56;russell.jurney;PIG-2540.tests_fail.patch.2;https://issues.apache.org/jira/secure/attachment/12519169/PIG-2540.tests_fail.patch.2,27/Mar/12 02:52;jcoveney;PIG-2540_4.patch;https://issues.apache.org/jira/secure/attachment/12520052/PIG-2540_4.patch,20/Mar/12 23:17;jcoveney;PIG-2540_almost_there.patch;https://issues.apache.org/jira/secure/attachment/12519149/PIG-2540_almost_there.patch,20/Mar/12 05:43;russell.jurney;TEST-org.apache.pig.piggybank.test.storage.avro.TestAvroStorage.txt;https://issues.apache.org/jira/secure/attachment/12519025/TEST-org.apache.pig.piggybank.test.storage.avro.TestAvroStorage.txt,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2012-03-20 23:17:48.058,,,no_permission,,,,,,,,,,,,228238,Reviewed,,,Thu May 17 07:16:40 UTC 2012,,,Patch Available,,,,0|i05ifb:,30082,,,,,,,,,,"20/Mar/12 02:35;russell.jurney;This patch converts the API used to get the current directory from a Path to a URI.  This enables S3, file:// and /foo all work.

The outstanding issue is that 7 out of 11 tests fail.  I cannot reproduce these failures outside of the unit tests.

I call for help.","20/Mar/12 02:35;russell.jurney;This fails 7 out of 11 tests, but works in practice.  Help.",20/Mar/12 05:43;russell.jurney;These are the test failures I get.,"20/Mar/12 23:17;jcoveney;Russell, here is the change. Previously, there was this function in TestAvroStorage:
{code}
    private static String getInputFile(String file) {
        return ""file:///"" + System.getProperty(""user.dir"") + ""/"" + basedir + file;
    }   
{code}

The issue is that System.getProperty(""user.dir"") returns a directory that begins with a /, so you were getting
{code}
file:////etc
{code}

By changing it accordingly, now all but one test run. The last test errors out, but this is because the file ""test_no_extension"" doesn't exist.","20/Mar/12 23:28;russell.jurney;The fix for test_no_extension was merged in PIG-2505.

So this is ready for merging? :D","21/Mar/12 01:02;jcoveney;Favor 1: can you try this on trunk and see if it works? No reason not to have it in both.
Point 2: in getAllSubDirs, you added a bunch of System.err.println. This should all be logged instead, via the logger, though some of it looks like vestiges of when you were debugging :) either way, could be useful to clean it up and log some useful statz. Otherwise, looks good to me.",21/Mar/12 03:56;russell.jurney;printlns removed,"21/Mar/12 05:54;jcoveney;Russell, I think you need to rebase. That has some changes to PigAvroInputFormat.java that I think are from a different patch, and it doesn't have the change I made to fix the tests.","22/Mar/12 20:54;russell.jurney;I am using git.  I commit locally, and pull from the github copy... so I am not sure how to do this?","22/Mar/12 21:21;russell.jurney;I don't understand how to do this.  I did this:

russell-jurneys-macbook-pro:newpig rjurney$ git remote -v
origin	https://github.com/apache/pig.git (fetch)
origin	https://github.com/apache/pig.git (push)

russell-jurneys-macbook-pro:newpig rjurney$ git branch -v
* branch-0.10 14f4606 [ahead 5] Merge branch 'branch-0.10' of https://github.com/apache/pig into branch-0.10
  trunk       cb49401 [behind 7] PIG-2589: Additional e2e test for 0.10 new features

russell-jurneys-macbook-pro:newpig rjurney$ git pull
remote: Counting objects: 77, done.
remote: Compressing objects: 100% (6/6), done.
remote: Total 39 (delta 17), reused 39 (delta 17)
Unpacking objects: 100% (39/39), done.
From https://github.com/apache/pig
   b8ce196..d1f6cb1  branch-0.10 -> origin/branch-0.10
   8b21cc4..841f336  trunk      -> origin/trunk
Merge made by recursive.

git diff --no-prefix 73bb67f8cc3974d76e034d09da96995e887b4c30 contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/avro/ > PIG-2540.tests_fail.patch.3

This patch is identical to the other one.  What am I missing?","24/Mar/12 01:44;russell.jurney;Can anyone help me out here?  I don't know how to patch other than git, and it is not working :(",27/Mar/12 02:48;jcoveney;Committed to 0.10 (r1305716) and trunk (r1305715). Testing on 0.9 now.,27/Mar/12 02:52;jcoveney;Attaching applied patch.,"27/Mar/12 02:56;jcoveney;Applied to pig0.9 r1305717

Onward and upwards!",27/Mar/12 06:06;russell.jurney;Oh wow!,"17/May/12 07:16;russell.jurney;Note: this patch seems to have accidentally fixed PIG-2527

Rejoice!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in new logical plan results in no output for join,PIG-2535,12542750,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,vivekp,vivekp,15/Feb/12 20:25,26/Apr/12 20:32,14/Mar/19 03:07,01/Mar/12 07:58,0.10.0,0.8.1,0.9.1,,,,,0.10.0,0.11,0.9.3,,,,,0,,,,,,,,,,,,,"The below script is a snippet of a much larger script. The join in the script results in 0 output for Pig 0.8,0.9 and 0.10 though there are matching records.


{code}
event_serve = LOAD 'input1'   USING MyMapLoader() AS (s:map[], m:map[], l:map[]);
raw = LOAD 'input2'  USING MyMapLoader() AS (s:map[], m:map[], l:map[]);

SPLIT raw INTO
    serve_raw IF (( (chararray) (s#'type') == '0') AND ( (chararray) (s#'source') == '5')),
    cm_click_raw IF (( (chararray) (s#'type') == '1') AND ( (chararray) (s#'source') == '5'));

cm_serve = FOREACH serve_raw GENERATE  s#'cm_serve_id' AS cm_event_guid,  s#'cm_serve_timestamp_ms' AS cm_receive_time, s#'p_url' AS ctx ;
cm_serve_lowercase = stream cm_serve through `tr [:upper:] [:lower:]`;
cm_serve_final = FOREACH cm_serve_lowercase GENERATE  $0 AS cm_event_guid, $1 AS cm_receive_time, $2 AS ctx;
filtered = FILTER event_serve BY (chararray) (s#'filter_key') neq 'xxxx' AND (chararray) (s#'filter_key') neq 'yyyy';
event_serve_project = FOREACH filtered GENERATE s#'event_guid' AS event_guid, s#'receive_time' AS receive_time;
event_serve_join = join cm_serve_final by (cm_event_guid, cm_receive_time), event_serve_project by (event_guid, receive_time) PARALLEL 800;
STORE event_serve_join INTO 'output' ;
{code}

The script produces correct results if I disable ColumnMapKeyPrune optimizer.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Feb/12 08:46;daijy;PIG-2535-0.patch;https://issues.apache.org/jira/secure/attachment/12514965/PIG-2535-0.patch,21/Feb/12 02:53;daijy;PIG-2535-1.patch;https://issues.apache.org/jira/secure/attachment/12515314/PIG-2535-1.patch,01/Mar/12 07:57;daijy;PIG-2535-2.patch;https://issues.apache.org/jira/secure/attachment/12516655/PIG-2535-2.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-02-17 08:46:49.244,,,no_permission,,,,,,,,,,,,228036,Reviewed,,,Thu Mar 01 07:58:32 UTC 2012,,,,,,,0|i09yvb:,56090,,,,,,,,,,"17/Feb/12 08:46;daijy;MapKeysPruneHelper erroneously remove 'type' and 'source' which are split condition. Attach the draft patch. Vivek, can you give a try?","17/Feb/12 09:20;vivekp;Thanks Daniel. I ran the script with this patch, but it seems that now the script is generating infinite map outputs.
(PIG-2534) Pig generating infinite map outputs

I am getting lots of ACCESSING_NON_EXISTENT_FIELD = 21,146,912,208 and this is keeping on increasing.","20/Feb/12 15:28;vivekp;Hi Daniel, the script goes through fine with the patch after applying output schema for 
| cm_serve_lowercase = stream cm_serve through `tr [:upper:] [:lower:]`;",21/Feb/12 02:53;daijy;Thanks Vivek for verifying. Attach a full patch.,28/Feb/12 02:46;thejas;+1,01/Mar/12 07:57;daijy;PIG-2535-2.patch resync with trunk.,"01/Mar/12 07:58;daijy;Unit test pass. test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 533 release audit warnings (more than the trunk's current 530 warnings).

javadoc and release audit warning is unrelated.

Patch committed to 0.9/0.10/trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig generating infinite map outputs,PIG-2534,12542742,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,vivekp,vivekp,15/Feb/12 19:57,26/Jul/13 17:59,14/Mar/19 03:07,01/Mar/12 07:22,0.9.1,,,,,,,0.10.0,0.11,0.9.3,,,,,0,,,,,,,,,,,,,"I am getting a strange behavior by Pig in the below script for Pig 0.9.

{code}
event_serve = LOAD 'input1'   AS (s, m, l);
cm_data_raw = LOAD 'input2'  AS (s, m, l);

SPLIT cm_data_raw INTO
    cm_serve_raw IF (( (chararray) (s#'key1') == '0') AND ( (chararray) (s#'key2') == '5')),
    cm_click_raw IF (( (chararray) (s#'key1') == '1') AND ( (chararray) (s#'key2') == '5'));

cm_serve = FOREACH cm_serve_raw GENERATE  s#'key3' AS f1,  s#'key4' AS f2, s#'key5' AS f3 ;
cm_serve_lowercase = stream cm_serve through `echo val3`;

cm_serve_final = FOREACH cm_serve_lowercase GENERATE  $0 AS cm_event_guid, $1 AS cm_receive_time, $2 AS cm_ctx_url;

event_serve_filtered = FILTER event_serve BY  (chararray) (s#'key1') neq 'xxx' AND (chararray) (s#'key2') neq 'yyy' ;

event_serve_project = FOREACH event_serve_filtered GENERATE  s#'key3' AS event_guid, s#'key4' AS receive_time;

event_serve_join = join cm_serve_final by (cm_event_guid),
    event_serve_project by (event_guid);

store event_serve_join into 'somewhere';
{code}

Input (both input1 and input2 is same)
---
[key1#0,key2#5,key3#val3,key4#val4,key5#val5]


If i run this pig script with ColumnMapKeyPrune disabled, the job goes through fine and 1 output is created.
But if I run this script by default, then it keeps on generating map output records infinitely. 
",,,,,,,,,,,,,,,,,,,PIG-2566,,,,,,,,,,,,,21/Feb/12 02:30;daijy;PIG-2534-1.patch;https://issues.apache.org/jira/secure/attachment/12515312/PIG-2534-1.patch,01/Mar/12 07:16;daijy;PIG-2534-2.patch;https://issues.apache.org/jira/secure/attachment/12516650/PIG-2534-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-02-18 01:55:13.913,,,no_permission,,,,,,,,,,,,228028,Reviewed,,,Thu Mar 01 07:25:19 UTC 2012,,,,,,,0|i09yvj:,56091,,,,,,,,,,"17/Feb/12 09:24;vivekp;Posting a test case without SPLIT to reproduce this issue
{code}
@Test
    public void testINFINITE() throws Exception {

        File f1 = Util.createFile(new String [] {""[key1#0,key2#5,key3#val3,key4#val4,key5#val5]""} );
        File f2 = Util.createFile(new String [] {""[key1#0,key2#5,key3#val3,key4#val4,key5#val5]""} );

        PigServer ps = new PigServer(ExecType.LOCAL);
        ps.registerQuery(""event_serve = LOAD 'file://""+f1.getAbsolutePath()+""' AS (s, m, l);"");
        ps.registerQuery(""cm_data_raw = LOAD 'file://""+f2.getAbsolutePath()+""' AS (s, m, l);"");
        ps.registerQuery(""cm_serve = FOREACH cm_data_raw GENERATE  s#'key3' AS f1,  s#'key4' AS f2, s#'key5' AS f3 ;"");
        ps.registerQuery(""cm_serve_lowercase = stream cm_serve through `tr [:upper:] [:lower:]`  ;"");
        ps.registerQuery(""cm_serve_final = FOREACH cm_serve_lowercase GENERATE  $0 AS cm_event_guid, $1 AS cm_receive_time, $2 AS cm_ctx_url;"");
        ps.registerQuery(""event_serve_project = FOREACH  event_serve GENERATE  s#'key3' AS event_guid, s#'key4' AS receive_time;"");
        ps.registerQuery(""event_serve_join = join cm_serve_final by (cm_event_guid), event_serve_project by (event_guid);"");

        Iterator<Tuple> itr = ps.openIterator(""event_serve_join"");
        while(itr.hasNext())
            System.out.println(itr.next());

    }
{code}","18/Feb/12 01:55;daijy;It happens when stream does not have an output schema. A workaround is to change
stream cm_serve through `tr [:upper:] [:lower:]`
into
stream cm_serve through `tr [:upper:] [:lower:]` as (a, b, c)

I will upload a patch shortly.","28/Feb/12 02:55;thejas;+1 
I think we should add a javadoc comment to the setOutputUids function, saying that it checks for null schema and throws an exception to stop column pruning from happening. We should also consider moving such checks (such as null schema) that need to happen on all logical operators to a separate visitor (subclass of a AllLogicalExpressionVisitor?), which gets called from the Transformer.check().


",28/Feb/12 07:56;dvryaboy;It throws an exception as part of normal flow control? Can we refactor that?,"01/Mar/12 07:16;daijy;Attach PIG-2534-2.patch to add comments to setOutputUids, also fix a findbug warning. For refactory, we can certainly do it, but since that is lower priority, I will open a separate ticket to address it.","01/Mar/12 07:22;daijy;Unit test pass. test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 533 release audit warnings (more than the trunk's current 530 warnings).

javadoc and release audit warning is unrelated.

Patch committed to 0.9/0.10/trunk",01/Mar/12 07:25;daijy;Open PIG-2566 to track Dmitriy's comment.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig MR job exceptions masked on frontend,PIG-2533,12542627,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,traviscrawford,traviscrawford,traviscrawford,15/Feb/12 01:35,26/Apr/12 20:32,14/Mar/19 03:07,16/Feb/12 01:59,,,,,,,,0.10.0,0.11,,,,,,0,,,,,,,,,,,,,"Pig MR jobs failures produce misleading error messages because just the first line of the error message is reported. Printing the whole error message is very useful when debugging issues as the current version is very misleading about the source of an issue.

PRINTING WHOLE ERROR MESSAGE:

{code}
2012-02-14 21:55:53,936 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 6017: java.io.IOException: Deserialization error: org.apache.hcatalog.data.schema.HCatSchema
	at org.apache.pig.impl.util.ObjectSerializer.deserialize(ObjectSerializer.java:55)
	at org.apache.pig.impl.util.UDFContext.deserialize(UDFContext.java:181)
	at org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil.setupUDFContext(MapRedUtil.java:159)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.setupUdfEnvAndStores(PigOutputFormat.java:229)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.checkOutputSpecs(PigOutputFormat.java:186)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:811)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:771)
	at org.apache.hadoop.mapred.jobcontrol.Job.submit(Job.java:378)
	at org.apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs(JobControl.java:247)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigJobControl.mainLoopAction(PigJobControl.java:144)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigJobControl.run(PigJobControl.java:121)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.ClassNotFoundException: org.apache.hcatalog.data.schema.HCatSchema
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:247)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:603)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1574)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1495)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1731)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:350)
	at java.util.Hashtable.readObject(Hashtable.java:859)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:974)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1848)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:350)
	at java.util.HashMap.readObject(HashMap.java:1030)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:974)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1848)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:350)
	at org.apache.pig.impl.util.ObjectSerializer.deserialize(ObjectSerializer.java:53)
	... 15 more
{code}


CURRENT ERROR MESSAGE:

{code}
Pig Stack Trace
---------------
ERROR 6017: java.io.IOException: Deserialization error: org.apache.hcatalog.data.schema.HCatSchema

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias l
        at org.apache.pig.PigServer.openIterator(PigServer.java:857)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:655)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:303)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:188)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:164)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:84)
        at org.apache.pig.Main.run(Main.java:561)
        at org.apache.pig.Main.main(Main.java:111)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
Caused by: org.apache.pig.PigException: ERROR 1002: Unable to store alias l
        at org.apache.pig.PigServer.storeEx(PigServer.java:956)
        at org.apache.pig.PigServer.store(PigServer.java:919)
        at org.apache.pig.PigServer.openIterator(PigServer.java:832)
        ... 12 more
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 6017: java.io.IOException: Deserialization error: org.apache.hcatalog.data.schema.HCatSchema
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:301)
        at org.apache.pig.PigServer.launchPlan(PigServer.java:1270)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1255)
        at org.apache.pig.PigServer.storeEx(PigServer.java:952)
        ... 14 more
================================================================================
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Feb/12 21:59;traviscrawford;PIG-2533.patch;https://issues.apache.org/jira/secure/attachment/12514721/PIG-2533.patch,15/Feb/12 01:37;traviscrawford;PIG-2533.patch;https://issues.apache.org/jira/secure/attachment/12514587/PIG-2533.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-02-15 03:58:44.462,,,no_permission,,,,,,,,,,,,227913,,,,Thu Feb 16 00:44:56 UTC 2012,,,,,,,0|i0h33z:,97766,,,,,,,,,,"15/Feb/12 03:58;dvryaboy;Looks like this is also in  JobControlThreadExceptionHandler in the same class.
It was clearly put in there for some reason.. but I, too, find this extremely frustrating to debug.

Will ask Daniel to see if he remembers the motivation for only printing the first line of the error stack.",15/Feb/12 08:49;daijy;I don't really sure why we put it this way. I will +1 for full stack which will ease the debugging.,15/Feb/12 21:59;traviscrawford;Updated patch to remove {{getFirstLineFromMessage}} and update all references. Looking at the context for all four usages of this method I believe all make sense to use the full stack trace (instead of just the first line).,"15/Feb/12 22:13;dvryaboy;Travis, thanks for following up.

If you removed all the references to it, mind removing getFirstLineFromMessage method altogether? #deadreduce

Also, procedural -- add patch versions when you upload them, otherwise it gets tricky to figure out which of the identical file names is the latest.

Thanks
-D","15/Feb/12 22:23;traviscrawford;The now unused method actually is removed in that patch - take another look?

Going forward, I'll add version numbers to patches.","16/Feb/12 00:44;dvryaboy;Hah, I was looking at the wrong patch. Ok looks good. Committing to 0.10 and trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Registered classes fail deserialization in frontend,PIG-2532,12542626,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,traviscrawford,traviscrawford,traviscrawford,15/Feb/12 01:17,09/Jun/12 02:06,14/Mar/19 03:07,07/Mar/12 23:45,,,,,,,,0.10.0,0.11,0.9.3,,,,,0,,,,,,,,,,,,,"This issue came up while integrating HCatalog with our environment. HCatalog jars are added to the pig command-line with {{-Dpig.additional.jars}} but fails (exception below). When added to the pig classpath the error goes away.

We identified the issue as deserialization using the root class loader, not the context class loader set when the thread is created. This causes HCatSchema which is serialized into the context to fail deserialization in the thread.

{code}
2012-02-14 21:55:53,936 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 6017: java.io.IOException: Deserialization error: org.apache.hcatalog.data.schema.HCatSchema
	at org.apache.pig.impl.util.ObjectSerializer.deserialize(ObjectSerializer.java:55)
	at org.apache.pig.impl.util.UDFContext.deserialize(UDFContext.java:181)
	at org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil.setupUDFContext(MapRedUtil.java:159)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.setupUdfEnvAndStores(PigOutputFormat.java:229)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.checkOutputSpecs(PigOutputFormat.java:186)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:811)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:771)
	at org.apache.hadoop.mapred.jobcontrol.Job.submit(Job.java:378)
	at org.apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs(JobControl.java:247)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigJobControl.mainLoopAction(PigJobControl.java:144)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigJobControl.run(PigJobControl.java:121)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.ClassNotFoundException: org.apache.hcatalog.data.schema.HCatSchema
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:247)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:603)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1574)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1495)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1731)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:350)
	at java.util.Hashtable.readObject(Hashtable.java:859)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:974)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1848)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:350)
	at java.util.HashMap.readObject(HashMap.java:1030)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:974)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1848)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:350)
	at org.apache.pig.impl.util.ObjectSerializer.deserialize(ObjectSerializer.java:53)
	... 15 more
{code}",,,,,,,,,,,,,,,,,,,PIG-2576,,,,,,,,,,,,,07/Mar/12 23:45;daijy;PIG-2532-h23.patch;https://issues.apache.org/jira/secure/attachment/12517484/PIG-2532-h23.patch,02/Mar/12 02:28;thw;PIG-2532-log.zip;https://issues.apache.org/jira/secure/attachment/12516779/PIG-2532-log.zip,17/Feb/12 21:11;traviscrawford;PIG-2532-v2.patch;https://issues.apache.org/jira/secure/attachment/12515026/PIG-2532-v2.patch,22/Feb/12 20:18;julienledem;PIG-2532-v3.patch;https://issues.apache.org/jira/secure/attachment/12515643/PIG-2532-v3.patch,27/Feb/12 01:28;julienledem;PIG-2532-v4-branch-0.9.patch;https://issues.apache.org/jira/secure/attachment/12516128/PIG-2532-v4-branch-0.9.patch,25/Feb/12 01:26;julienledem;PIG-2532-v4.patch;https://issues.apache.org/jira/secure/attachment/12516013/PIG-2532-v4.patch,15/Feb/12 01:39;traviscrawford;PIG-2532.patch;https://issues.apache.org/jira/secure/attachment/12514588/PIG-2532.patch,17/Feb/12 02:08;traviscrawford;PIG-253_javax.zip;https://issues.apache.org/jira/secure/attachment/12514929/PIG-253_javax.zip,,,,8.0,,,,,,,,,,,,,,,,,,,2012-02-15 04:03:46.541,,,no_permission,,,,,,,,,,,,227912,Reviewed,,,Sat Jun 09 02:06:21 UTC 2012,,,,,,,0|i0h33r:,97765,,,,,,,,,,"15/Feb/12 04:03;dvryaboy;Nice find, guys.
We probably need an e2e test for this. Looks like they already create a separate udfs jar, so it should be possible to reproduce the scenario of not having the jar on the classpath, but only registering it.","17/Feb/12 02:04;traviscrawford;Status update:

I reproduced the issue in a unit test, and verified the patch fixes the issue. The issue presents itself when a jar is registered at runtime, and a class in that registered jar is stored in the UDFContext. Without the patch deserilization fails.

To setup the test I store two java files as resources, and compile+jar them at runtime (using javax classes). I took this approach to keep the test setup isolated.

However, since it uses javax classes this approach may not be ideal. I'm preparing another version that performs these setup steps in ant (generate the test-specific jar, runs the test with an appropriate classpath. That version will likely be the better choice to commit.","17/Feb/12 02:08;traviscrawford;Uploading zip file of version that compiles+jars at runtime, mainly so it exists somewhere other than my laptop :)","17/Feb/12 21:11;traviscrawford;Internally we discussed and think this approach is best because it keeps the test relatively self-contained (instead of majorly leaking into the build file). Additionally, we checked OpenJDK and confirmed these classes are present.",21/Feb/12 16:24;julienledem;+1,22/Feb/12 00:17;daijy;+1. Nice catch Travis! This does bother us for a long time.,"22/Feb/12 20:18;julienledem;I rebased the patch (v3)
will commit",25/Feb/12 01:26;julienledem;I rebased the patch again and did a small change so that the test can be run from eclipse,"25/Feb/12 01:40;ashutoshc;Awesome. Thanks, Travis for this. Can I request this to be back-ported to 0.8 ?","25/Feb/12 01:44;ashutoshc;Sorry, I meant 0.9 branch.",25/Feb/12 01:49;daijy;+1 for backport. Some resync is needed though.,"26/Feb/12 23:51;julienledem;I checked-in PIG-2532-v4.patch in TRUNK.
I will look into back porting to 0.9","27/Feb/12 01:28;julienledem;Adding PIG-2532-v4-branch-0.9.patch
The only difference is regarding the eclipse-files generation.
Otherwise the patch applies cleanly",27/Feb/12 01:32;julienledem;checked in in branch-0.9,"29/Feb/12 23:01;thw;After this change, following test fails on 0.23:

{code}
ant -Dhadoopversion=23 clean test -Dtestcase=TestRegisteredJarVisibility


    [junit] Tests run: 2, Failures: 0, Errors: 2, Time elapsed: 113.172 sec
    [junit] Test org.apache.pig.test.TestRegisteredJarVisibility FAILED
{code}

Will this go into 0.10 also?
","01/Mar/12 02:41;traviscrawford;Thomas, thanks for the report. I will take a look at this test failure.","02/Mar/12 01:09;julienledem;I've been looking at this problem and does not seem related to the patch. Thomas, could you send the content of the related junit log file? I see a bunch of Guice related exceptions.
",02/Mar/12 02:28;thw;Attaching test log.,"07/Mar/12 23:45;daijy;PIG-2532-h23.patch fix h23 test failure. Also I find the patch is not committed to 0.10 branch, I committed it to 0.10 branch as well.","09/Jun/12 02:06;aniket486;I tested on trunk, its not fully fixed. If we register the jar from s3, it fails with the same error. I will open another jira for the same.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reusing alias name in nested foreach causes incorrect results,PIG-2530,12542382,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,daijy,vivekp,vivekp,13/Feb/12 09:48,01/Aug/13 19:30,14/Mar/19 03:07,16/Feb/12 20:13,0.10.0,0.9.2,,,,,,0.10.0,0.11,0.9.3,,,,,0,,,,,,,,,,,,,"The below script results in incorrect output for Pig 0.10 but runs fine with Pig 0.8,

{code}
input.txt
1	4
1	3
2	3
2	4
{code}

{code}
a = load 'input.txt' as (v1:int, v2:int);
b = group a by v1;
c = foreach b { x = a; x = order x by v2 asc; generate flatten(x); }
store c into 'c1';
{code}


Output from Pig 0.10
--------------------
1       4
1       3
2       3
2       4

Looking at the explain, it seems like the sorting is entirely missed out. 
The script produces correct results if I change the alias name ie;

c = foreach b { x = a; x1 = order x by v2 asc; generate flatten(x1); }

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Feb/12 09:44;daijy;PIG-2530-0.patch;https://issues.apache.org/jira/secure/attachment/12514474/PIG-2530-0.patch,16/Feb/12 00:27;thw;PIG-2530-1.patch;https://issues.apache.org/jira/secure/attachment/12514739/PIG-2530-1.patch,16/Feb/12 07:32;daijy;PIG-2530-2.patch;https://issues.apache.org/jira/secure/attachment/12514766/PIG-2530-2.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-02-14 09:44:46.901,,,no_permission,,,,,,,,,,,,227669,Reviewed,,,Thu Aug 01 19:30:02 UTC 2013,,,,,,,0|i09yyn:,56105,,,,,,,,,,14/Feb/12 09:44;daijy;Attach a draft patch,16/Feb/12 00:27;thw;Adding test case.,16/Feb/12 07:32;daijy;Attach a better fix PIG-2530-2.patch. Thanks Thomas for adding tests.,"16/Feb/12 19:26;daijy;Unit test pass. test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.
     [exec] 
     [exec]     +1 javac. 
     [exec]  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 529 release audit warnings (more than the trunk's current 526 warnings).

javac and release audit warning are unrelated.

Patch is ready to review.","16/Feb/12 19:48;thw;Unit tests pass on 0.10
","16/Feb/12 20:09;thejas;+1 .

Note that this bug happens when an nested foreach operator is assigned to another and then same is reused for another operator. In the example in description -

{code}
c = foreach b { 
        x = a; -- a is assinged to x
        x = order x by v2 asc;      -- x is reused 
        generate flatten(x);        -- because of the bug, x was substituted with a
    }

{code}",16/Feb/12 20:13;daijy;Patch committed to 0.10/trunk.,17/Feb/12 05:54;vivekp;Thanks a lot Daniel and Thomas. Does the patch apply for 0.9.2 as well ? or is 0.9.2 having a different behavior.,"18/Feb/12 02:58;thw;Unit tests pass on 0.9 branch, would be good to commit there since it is producing false positives.",21/Feb/12 02:32;daijy;Committed to 0.9 branch as well.,"01/Aug/13 16:59;xuefuz;[~daijy] Looking at the patch, I don't think we can simply removed previous its exprPlan when an alias is redefined. For instance, the query in PIG-3379 doesn't work with this patch.

The fix is probably not at the grammar, but rather at the data structure we use to represent these aliases and their expr plans.

Any thoughts? ","01/Aug/13 19:30;xuefuz;I just realized that this patch were to fix duplicated command operators, not expression alias.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ILLUSTRATE fails for relations LOADed with the AvroStorage UDF,PIG-2527,12542213,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,jcoveney,russell.jurney,russell.jurney,11/Feb/12 02:16,18/Mar/13 22:51,14/Mar/19 03:07,19/Dec/12 16:33,0.10.0,0.10.1,0.11,0.9.2,,,,0.10.1,,,,piggybank,,,0,avro,avro_udf,avrostorage,happy,pig,storage,udf,,,,,,"grunt> describe emails
emails: {message_id: chararray,from: {PIG_WRAPPER: (ARRAY_ELEM: chararray)},to: {PIG_WRAPPER: (ARRAY_ELEM: chararray)},cc: {PIG_WRAPPER: (ARRAY_ELEM: chararray)},bcc: {PIG_WRAPPER: (ARRAY_ELEM: chararray)},reply_to: {PIG_WRAPPER: (ARRAY_ELEM: chararray)},in_reply_to: {PIG_WRAPPER: (ARRAY_ELEM: chararray)},subject: chararray,body: chararray,date: chararray}

grunt> illustrate emails                     
2012-02-10 18:15:01,591 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: file:///
2012-02-10 18:15:01,592 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false
2012-02-10 18:15:01,649 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 1
2012-02-10 18:15:01,649 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 1
2012-02-10 18:15:01,649 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added to the job
2012-02-10 18:15:01,649 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2012-02-10 18:15:01,668 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 5
2012-02-10 18:15:02,719 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false
2012-02-10 18:15:02,719 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 1
2012-02-10 18:15:02,719 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 1
2012-02-10 18:15:02,720 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added to the job
2012-02-10 18:15:02,720 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2012-02-10 18:15:02,733 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false
2012-02-10 18:15:02,734 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 1
2012-02-10 18:15:02,734 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 1
2012-02-10 18:15:02,734 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added to the job
2012-02-10 18:15:02,734 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2012-02-10 18:15:02,749 [main] ERROR org.apache.pig.pen.AugmentBaseDataVisitor - No (valid) input data found!
java.lang.RuntimeException: No (valid) input data found!
        at org.apache.pig.pen.AugmentBaseDataVisitor.visit(AugmentBaseDataVisitor.java:579)
        at org.apache.pig.newplan.logical.relational.LOLoad.accept(LOLoad.java:218)
        at org.apache.pig.pen.util.PreOrderDepthFirstWalker.depthFirst(PreOrderDepthFirstWalker.java:82)
        at org.apache.pig.pen.util.PreOrderDepthFirstWalker.walk(PreOrderDepthFirstWalker.java:66)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.pen.ExampleGenerator.getExamples(ExampleGenerator.java:180)
        at org.apache.pig.PigServer.getExamples(PigServer.java:1245)
        at org.apache.pig.tools.grunt.GruntParser.processIllustrate(GruntParser.java:698)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.Illustrate(PigScriptParser.java:591)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:306)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:188)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:164)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:69)
        at org.apache.pig.Main.run(Main.java:495)
        at org.apache.pig.Main.main(Main.java:111)
2012-02-10 18:15:02,752 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2997: Encountered IOException. Exception : No (valid) input data found!


================================================================================
Pig Stack Trace
---------------
ERROR 2997: Encountered IOException. Exception : No (valid) input data found!

java.io.IOException: Exception : No (valid) input data found!
        at org.apache.pig.PigServer.getExamples(PigServer.java:1251)
        at org.apache.pig.tools.grunt.GruntParser.processIllustrate(GruntParser.java:698)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.Illustrate(PigScriptParser.java:591)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:306)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:188)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:164)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:69)
        at org.apache.pig.Main.run(Main.java:495)
        at org.apache.pig.Main.main(Main.java:111)
================================================================================
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-12-19 16:33:03.277,,,no_permission,,,,,,,,,,,,227500,,,,Mon Mar 18 22:51:30 UTC 2013,,,,,,,0|i0h32v:,97761,"The fix for this wasn't committed for 0.10.0, and I'm not sure where the fix is/if it was committed at all.

Can anyone remember what happened with the fixes for AvroStorage that didn't go in 0.10.0?",,,,,,,,,29/Feb/12 09:14;russell.jurney;I don't remember how to do this.  Does anyone remember how to do this?,17/May/12 07:13;russell.jurney;This issue was inadvertently resolved in Pig 0.10.  WOOT!,"23/Jul/12 23:18;russell.jurney;The fix for AvroStorage that fixed ILLUSTRATE didn't go in Pig 0.10.0, so this is still open.",19/Dec/12 16:33;daijy;Verified it is fixed in 0.10 branch. Mark it as fixed in 0.10.1.,"27/Feb/13 18:56;rohini;[~daijy],
   Do you know which jira incorporates this fix? I don't see any patch attached to this jira. One of our users encountered a similar stacktrace with illustrate and PigStorage. ","18/Mar/13 22:51;daijy;[~rohini], I didn't investigate which jira fix the issue. I did a trivial illustrate and it works. If you have a script which fail, can you open a new ticket and provide more details?
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
REGEX_EXTRACT not returning correct group with non greedy regex,PIG-2514,12541546,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,romainr,romainr,romainr,07/Feb/12 01:40,22/Feb/13 04:54,14/Mar/19 03:07,02/Mar/12 00:08,0.11,,,,,,,0.11,,,,internal-udfs,,,0,,,,,,,,,,,,,"Hello,

REGEX_EXTRACT is using Matcher.find() instead of Matcher.matches() and so does not work with some non greedy regular expression.

Is it the wanted behavior?

Thanks,

Romain


http://docs.oracle.com/javase/1.4.2/docs/api/java/util/regex/Matcher.html


The matches method attempts to match the entire input sequence against the pattern.

The find method scans the input sequence looking for the next subsequence that matches the pattern.



    System.out.println(""Pig's way with m.find()"");
    String a = ""hdfs://mygrid.com/projects/"";
    Matcher m = Pattern.compile(""(.+?)/?"").matcher(a);
    System.out.println(m.find());
    System.out.println(m.group(1));
    System.out.println(m.start());
    System.out.println(m.end());

    System.out.println(""\nm.matches()"");
    a = ""hdfs://mygrid.com/projects/"";
    m = Pattern.compile(""(.+?)/?"").matcher(a);
    System.out.println(m.matches());
    System.out.println(m.group(1));
    System.out.println(m.start());
    System.out.println(m.end());

    System.out.println(""\nREGEX_EXTRACT m.find()"");
    Tuple t = TupleFactory.getInstance().newTuple();
    t.append(a);
    t.append(""(.+?)/?"");
    t.append(1);
    System.out.println(new TestPigExtractAll().new REGEX_EXTRACT().exec(t));",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Feb/12 19:25;romainr;PIG-2514-doc.patch;https://issues.apache.org/jira/secure/attachment/12515636/PIG-2514-doc.patch,29/Feb/12 20:45;romainr;PIG-2514.2.patch;https://issues.apache.org/jira/secure/attachment/12516603/PIG-2514.2.patch,22/Feb/12 19:25;romainr;PIG-2514.patch;https://issues.apache.org/jira/secure/attachment/12515635/PIG-2514.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-02-07 18:57:08.726,,,no_permission,,,,,,,,,,,,226833,Reviewed,,,Fri Mar 02 00:08:39 UTC 2012,,,Patch Available,,,,0|i0h30n:,97751,,,,,,,,,,"07/Feb/12 01:44;romainr;Patch adding a non greedy regex test that fails currently.

Patch changing the Matcher method used from find() to matches(). It makes the tests pass.",07/Feb/12 18:57;daijy;Well this is backward incompatible. Shall we have a new UDF for that?,"09/Feb/12 23:06;romainr;A simple workaround is to use REGEX_EXTRACT_ALL and get the $index you need:

e.g. REGEX_EXTRACT_ALL(value, '(.+?)/?').$0

Maybe just documenting REGEX_EXTRACT is enough: for non greedy matching, use REGEX_EXTRACT_ALL instead

Having 3 REGEX_EXTRACT UDF seems confusing.
","13/Feb/12 08:02;daijy;I agree adding another UDF adds confusion, however, the workaround requires change of existing Pig script, which we try to avoid. How about using a boolean construct argument for greedy?
","14/Feb/12 01:06;romainr;This is a good idea.

I am going to add a default constructor and another one with a boolean argument for setting to greedy (switching from find() to matches()).","22/Feb/12 19:25;romainr;Here is the constructor with optional boolean version (probably better than adding one more (optional) parameter to the exec tuple?).

I put 2 lines of doc too.",23/Feb/12 03:40;daijy;Looks good. Better to change REGEX_EXTRACT_ALL to make it symmetric.,"28/Feb/12 01:54;romainr;I can add a it to REGEX_EXTRACT_ALL, just the defaults will be reversed as REGEX_EXTRACT_ALL is using matches() by default (and REGEX_EXTRACT find() by default).","29/Feb/12 20:45;romainr;Here it is, and I put a more explicit wording.","02/Mar/12 00:08;daijy;Unit test pass. test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 535 release audit warnings (more than the trunk's current 530 warnings).

javadoc and release audit warning is unrelated. 

Patch committed to trunk.

Thanks Romain!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Util.getSchemaFromString fails with java.lang.NullPointerException when a tuple in a bag has no name (as when used in MongoStorage UDF),PIG-2509,12541513,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,jcoveney,russell.jurney,russell.jurney,06/Feb/12 21:56,25/Jul/12 04:04,14/Mar/19 03:07,01/Mar/12 23:42,0.9.1,,,,,,,0.10.0,0.11,,,piggybank,,,0,fun,happy,mongo,pants,sad,udf,util,,,,,,"The MongoStorage UDF ( https://github.com/mongodb/mongo-hadoop ) fails on its call to Util.getSchemaFromString when a tuple is not named.  The call succeeds when the tuple is explicitly named.  As tuple naming is optional, this is a bug.

See http://www.mail-archive.com/user%40pig.apache.org/msg04199.html and http://www.mail-archive.com/dev%40pig.apache.org/msg08016.html

This makes me sad :(","No, genetics!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Feb/12 19:06;jcoveney;PIG-2509-0.patch;https://issues.apache.org/jira/secure/attachment/12513845/PIG-2509-0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-08 19:05:48.33,,,no_permission,,,,,,,,,,,,226800,Reviewed,,,Wed Jul 25 04:04:59 UTC 2012,,,,,,,0|i0h2zj:,97746,,,,,,,,,,06/Feb/12 22:04;russell.jurney;See also https://jira.mongodb.org/browse/HADOOP-19,"06/Feb/12 22:45;russell.jurney;The offending code that is called is here: class org.apache.pig.newplan.logical.Util

...

    public static LogicalSchema.LogicalFieldSchema translateFieldSchema(Schema.FieldSchema fs) {      
        LogicalSchema newSchema = null;
        if (fs.schema!=null) {
            newSchema = translateSchema(fs.schema);
        }
        
        LogicalSchema.LogicalFieldSchema newFs = new LogicalSchema.LogicalFieldSchema(fs.alias, newSchema, fs.type);
        return newFs;
    }
    
    /**
     * This function translates the new LogicalSchema into old Schema format required
     * by PhysicalOperators
     * @param schema LogicalSchema to be converted to Schema
     * @return Schema that is converted from LogicalSchema
     * @throws FrontendException 
     */
    public static Schema translateSchema(LogicalSchema schema) {       
        if (schema == null) {
            return null;
        }
        
        Schema s2 = new Schema();
        List<LogicalSchema.LogicalFieldSchema> ll = schema.getFields();
        for (LogicalSchema.LogicalFieldSchema f: ll) {
            Schema.FieldSchema f2 = null;
            try {
                f2 = new Schema.FieldSchema(f.alias, translateSchema(f.schema), f.type);
                f2.canonicalName = ((Long)f.uid).toString();
                s2.add(f2);
            } catch (FrontendException e) {
            }
        }
        
        return s2;
    }","08/Feb/12 19:05;jcoveney;The fix basically just allows a tuple where it is explicitly called ""null:"" as in the case of ""b:{null:(a:int)}"". In such a case, it will just throw out the tuple name, which will make it equivalent to ""b:{(a:int)}"". Russell ran it and it worked. Russell, I did change it slightly, so it'd be lovely if you can try it again just to be sure.

I successfully applied to 0.9 and ran ""ant test-commit"", and applied it to 0.10 and 0.11 and ran ""ant -Dtestcase=TestSchema test.""","26/Feb/12 05:54;russell.jurney;I have applied this patch, and have been using it since.  It works.  Tests pass.  Can we resolve and include this in 0.10?",26/Feb/12 05:55;russell.jurney;Tested. Working well in MongoStorage.,"28/Feb/12 17:52;jcoveney;This was closed incorrectly, and needs to be reviewed and committed","01/Mar/12 23:42;daijy;Unit test pass. test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.
     [exec] 
     [exec]     -1 javac.  The applied patch generated 904 javac compiler warnings (more than the trunk's current 900 warnings).
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 533 release audit warnings (more than the trunk's current 530 warnings).

javac warning is caused by Antlr. javadoc and release audit warning is not related.

I don't feel a compelling reason to check it into 0.9 branch. Commit into 0.10/trunk first.

Thanks Jonathan!","25/Jul/12 04:04;russell.jurney;MongoStorage is still not working when tuples aren't named, but as there is no longer a call to Util.getSchemaFromString at all in mongo-hadoop, this problem appears to be unrelated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PIG can unpredictably ignore deprecated Hadoop config options,PIG-2508,12541431,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,thw,anupamseth,anupamseth,06/Feb/12 16:20,31/May/12 21:48,14/Mar/19 03:07,24/Feb/12 01:06,0.10.0,0.9.2,,,,,,0.10.0,0.11,0.9.3,,,,,0,,,,,,,,,,,,,"When deprecated config options are passed to a Pig job, it can unpredictably ignore them and override them with values provided in the defaults due to a ""race condition""-like issue.

This problem was first noticed as part of MAPREDUCE-3665, which was re-filed as HADOOP-7993 so as for it to fall in the right component bucket of the code being fixed. This JIRA fixed the bug on the Hadoop side of the code that caused older deprecated config options to be ignored when they were also specified in the defaults xml file with the newer config name or vice versa.

However, the problem seemed to persist with Pig jobs and HADOOP-8021 was filed to address the issue. 

A careful step-by-step execution of the code in a debugger reveals an second overlapping bug because of the way PIG is dealing with the configs.

Not sure how / why this was not seen earlier, but the code in HExecutionEngine.java#recomputeProperties currently mashes together the default Hadoop configs and the user-specified properties into a Properties object. Given that it uses a HashTable to store the properties, if we have a config called ""old.config.name"" which is now deprecated and replaced by ""new.config.name"" and if one type is specified in the defaults and another by the user, we get a strange condition in which the repopulated Properties object has [in an unpredictable ordering] the following:

{code}
config1.name=config1.value
config2.name=config2.value
...
old.config.name=old.config.value
...
new.config.name=new.config.value
...
configx.name=configx.value
{code}

When this Properties object gets converted into a Configuration object by the ConfigurationUtil#toConfiguration() routine, the deprecation kicks in and tries to resolve all old configs. Because the ordering is not guaranteed (and because in the case of compress, the hash function consistently gives the new config loaded from the defaults after the old one), the user-specified config is ignored in favor of the default config (which from the point of view of the Hadoop Configuration object is expected standard behavior to replace an earlier specification of a config value with a later one).

The fix for this is probably straightforward, but will require a re-write of the a chunk of code in HExecutionEngine.java. Instead of mashing together a JobConf object and a Properties object into a Configuration object that is finally re-converted into a JobConf object, the code simply needs to consistently and correctly populate a JobConf / Configuration object that can handle deprecation instead of a ""dumb"" Java Properties object.

We recently saw another potential occurrence of this bug where Pig seems to honor only mapreduce.job.queuename parameter for specifying queue name and ignores the parameter mapred.job.queue.name.

Since this can break a lot of existing jobs that run fine on 0.20, marking this as a blocker.
",,,,,,,,,,,,,,,HADOOP-8021,,,,PIG-2552,,,,,,,,,,,,,16/Feb/12 21:57;thw;PIG-2508-0.10-TestAvroStorage.patch;https://issues.apache.org/jira/secure/attachment/12514895/PIG-2508-0.10-TestAvroStorage.patch,07/Feb/12 18:52;thw;PIG-2508.3.patch;https://issues.apache.org/jira/secure/attachment/12513655/PIG-2508.3.patch,09/Feb/12 02:46;thw;PIG-2508.4.patch;https://issues.apache.org/jira/secure/attachment/12513898/PIG-2508.4.patch,10/Feb/12 02:38;thw;PIG-2508.5.patch;https://issues.apache.org/jira/secure/attachment/12514068/PIG-2508.5.patch,07/Feb/12 04:07;thw;PIG-2508.patch;https://issues.apache.org/jira/secure/attachment/12513567/PIG-2508.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2012-02-07 04:12:43.738,,,no_permission,,,,,,,,,,,,226718,Reviewed,,,Thu May 31 20:52:04 UTC 2012,,,,,,,0|i0h2zb:,97745,,,,,,,,,,"07/Feb/12 04:12;thw;Initial patch. With this change deprecated properties, when passed from the command line will be passed through to .23:

bin/pig -Dmapred.output.compress=true -Dmapred.output.compression.codec=org.apache.hadoop.io.compress.BZip2Codec -x local tst.pig

Same properties set in the script will still be ignored, more work to be done..

","07/Feb/12 18:40;thw;Updated patch to take care of properties set within the script. This is not a pretty fix - too many references exist to the shared properties object in PigContext.

test-commit passes",07/Feb/12 19:04;daijy;I will take a look.,"07/Feb/12 23:13;anupamseth;Tested Thomas' new patch on a 10-node cluster and ee the following:

On 0.23:
========
In local mode while setting configuration with deprecated name in script:
-------------------------------------------------------------------------
Fails with Kerberos exception as follows

{code}
2012-02-07 22:00:08,696 [main] INFO  org.apache.pig.Main - Logging error messages to: /homes/<user>/pig_1328652008690.log
2012-02-07 22:00:09,010 [main] WARN  org.apache.hadoop.conf.Configuration - mapred.used.genericoptionsparser is deprecated. Instead, use mapreduce.client.genericoptionsparser.used
2012-02-07 22:00:09,011 [main] WARN  org.apache.hadoop.conf.Configuration - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2012-02-07 22:00:09,011 [main] WARN  org.apache.hadoop.conf.Configuration - fs.default.name is deprecated. Instead, use fs.defaultFS
2012-02-07 22:00:09,011 [main] WARN  org.apache.hadoop.conf.Configuration - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2012-02-07 22:00:09,011 [main] WARN  org.apache.hadoop.conf.Configuration - fs.default.name is deprecated. Instead, use fs.defaultFS
2012-02-07 22:00:09,011 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: file:///
2012-02-07 22:00:09,404 [main] WARN  org.apache.hadoop.conf.Configuration - mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
2012-02-07 22:00:09,405 [main] WARN  org.apache.hadoop.conf.Configuration - mapred.output.compression.codec is deprecated. Instead, use mapreduce.output.fileoutputformat.compress.codec
2012-02-07 22:00:10,386 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: UNKNOWN
2012-02-07 22:00:10,540 [main] WARN  org.apache.hadoop.conf.Configuration - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator
2012-02-07 22:00:10,553 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 6000: 
<file script2-hadoop.pig, line 7, column 0> Output Location Validation Failed for: 'file:///homes/ghimport/script2-hadoop-results More info to follow:
Can't get Master Kerberos principal for use as renewer
Details at logfile: /homes/<user>/pig_1328652008690.log
{code}

Contents of pig log file
{code}
Pig Stack Trace
---------------
ERROR 6000:
<file script2-hadoop.pig, line 7, column 0> Output Location Validation Failed for: 'file:///homes/<user>/script2-hadoop-results More info to follow:
Can't get Master Kerberos principal for use as renewer

org.apache.pig.impl.plan.VisitorException: ERROR 6000:
<file script2-hadoop.pig, line 7, column 0> Output Location Validation Failed for: 'file:///homes/<user>/script2-hadoop-results More info to follow:
Can't get Master Kerberos principal for use as renewer
        at org.apache.pig.newplan.logical.rules.InputOutputFileValidator$InputOutputFileVisitor.visit(InputOutputFileValidator.java:95)
        at org.apache.pig.newplan.logical.relational.LOStore.accept(LOStore.java:77)
        at org.apache.pig.newplan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:64)
        at org.apache.pig.newplan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:66)
        at org.apache.pig.newplan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:66)
        at org.apache.pig.newplan.DepthFirstWalker.walk(DepthFirstWalker.java:53)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.newplan.logical.rules.InputOutputFileValidator.validate(InputOutputFileValidator.java:45)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:293)
        at org.apache.pig.PigServer.compilePp(PigServer.java:1360)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1297)
        at org.apache.pig.PigServer.execute(PigServer.java:1289)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:360)
        at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:130)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:191)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:163)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:84)
        at org.apache.pig.Main.run(Main.java:561)
        at org.apache.pig.Main.main(Main.java:111)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:200)
Caused by: java.io.IOException: Can't get Master Kerberos principal for use as renewer
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:104)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:87)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)
        at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:137)
        at org.apache.pig.newplan.logical.rules.InputOutputFileValidator$InputOutputFileVisitor.visit(InputOutputFileValidator.java:80)
        ... 23 more
================================================================================
{code}

In cluster mode while setting configuration with deprecated name in script:
---------------------------------------------------------------------------
Passes

In local mode while setting configuration with new name in script:
------------------------------------------------------------------
Same issue as with local mode above

In cluster mode while setting configuration with new name in script:
--------------------------------------------------------------------
Fails as below
{code}
2012-02-07 22:08:27,164 [main] INFO  org.apache.pig.Main - Logging error messages to: /homes/<user>/pig_1328652507159.log
2012-02-07 22:08:27,663 [main] WARN  org.apache.hadoop.conf.Configuration - mapred.used.genericoptionsparser is deprecated. Instead, use mapreduce.client.genericoptionsparser.used
2012-02-07 22:08:27,665 [main] WARN  org.apache.hadoop.conf.Configuration - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2012-02-07 22:08:27,665 [main] WARN  org.apache.hadoop.conf.Configuration - fs.default.name is deprecated. Instead, use fs.defaultFS
2012-02-07 22:08:27,665 [main] WARN  org.apache.hadoop.conf.Configuration - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2012-02-07 22:08:27,665 [main] WARN  org.apache.hadoop.conf.Configuration - fs.default.name is deprecated. Instead, use fs.defaultFS
2012-02-07 22:08:27,665 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://<host>
2012-02-07 22:08:32,052 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: UNKNOWN
2012-02-07 22:08:32,349 [main] WARN  org.apache.hadoop.conf.Configuration - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator
2012-02-07 22:08:32,360 [main] INFO  org.apache.hadoop.hdfs.DFSClient - Created HDFS_DELEGATION_TOKEN token 28 for <user> on <host>
2012-02-07 22:08:32,360 [main] INFO  org.apache.hadoop.mapreduce.security.TokenCache - Got dt for hdfs://<host>
2012-02-07 22:08:32,787 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false
2012-02-07 22:08:32,964 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 1
2012-02-07 22:08:32,964 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 1
2012-02-07 22:08:33,919 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added to the job
2012-02-07 22:08:33,963 [main] WARN  org.apache.hadoop.conf.Configuration - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent
2012-02-07 22:08:33,963 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2012-02-07 22:08:33,963 [main] WARN  org.apache.hadoop.conf.Configuration - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent
2012-02-07 22:08:33,963 [main] WARN  org.apache.hadoop.conf.Configuration - mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
2012-02-07 22:08:33,964 [main] WARN  org.apache.hadoop.conf.Configuration - mapred.output.compression.codec is deprecated. Instead, use mapreduce.output.fileoutputformat.compress.codec
2012-02-07 22:08:33,974 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 0: 'mapred.output.compress' is set but no value is specified for 'mapred.output.compression.codec'.
Details at logfile: /homes/<user>/pig_1328652507159.log
{code}

Contents of log file:
{code}
Pig Stack Trace
---------------
ERROR 0: 'mapred.output.compress' is set but no value is specified for 'mapred.output.compression.codec'.

org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobCreationException: ERROR 0: 'mapred.output.compress' is set but no value is specified for 'mapred.output.compression.codec'.
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJob(JobControlCompiler.java:365)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.compile(JobControlCompiler.java:258)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:150)
        at org.apache.pig.PigServer.launchPlan(PigServer.java:1314)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1299)
        at org.apache.pig.PigServer.execute(PigServer.java:1289)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:360)
        at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:130)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:191)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:163)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:84)
        at org.apache.pig.Main.run(Main.java:561)
        at org.apache.pig.Main.main(Main.java:111)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:200)
================================================================================
{code}

In local mode while setting configuration with deprecated name from cmd line:
-----------------------------------------------------------------------------
Same issue as with local mode above

In cluster mode while setting configuration with deprecated name from cmd line:
-------------------------------------------------------------------------------
Passes

In local mode while setting configuration with new name from cmd line:
----------------------------------------------------------------------
Same issue as with local mode above

In cluster mode while setting configuration with new name from cmd line:
------------------------------------------------------------------------
Passes


On 0.20.2xx:
============
Cannot get it to work at all (tried removing my ivy2 directory, doing ant clean, and then re-compiling the tarball for 0.20 - still, it smells like I have 0.23 libs being referenced somewhere!)

{code}
2012-02-07 23:08:46,237 [main] INFO  org.apache.pig.Main - Logging error messages to: /homes/ghimport/pig_1328656126229.log
2012-02-07 23:08:46,716 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: file:///
2012-02-07 23:08:47,140 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2998: Unhandled internal error. org/apache/hadoop/mapreduce/task/JobContextImpl
Details at logfile: /homes/<user>/pig_1328656126229.log
Exception in thread ""main"" java.lang.NoClassDefFoundError: Could not initialize class org.apache.pig.tools.pigstats.PigStatsUtil
        at org.apache.pig.Main.run(Main.java:593)
        at org.apache.pig.Main.main(Main.java:111)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
{code}

Contents of pig log file:
{code}Pig Stack Trace
---------------
ERROR 2998: Unhandled internal error. org/apache/hadoop/mapreduce/task/JobContextImpl

java.lang.NoClassDefFoundError: org/apache/hadoop/mapreduce/task/JobContextImpl
        at org.apache.pig.tools.pigstats.PigStatsUtil.<clinit>(PigStatsUtil.java:54)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:82)
        at org.apache.pig.Main.run(Main.java:561)
        at org.apache.pig.Main.main(Main.java:111)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.mapreduce.task.JobContextImpl
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
        ... 9 more
================================================================================
{code}

","08/Feb/12 01:14;thw;The failures in local mode look more like a .23 Hadoop cluster environment issue (I see the same running on my cluster with 0.9 head w/ and w/o patch).

In cluster mode, passing the compression properties from command line and from script pass.

Script:

{code}
set mapred.output.compress true
set mapred.output.compression.codec org.apache.hadoop.io.compress.BZip2Codec
rmf t1-results
raw = LOAD 't1.txt' USING PigStorage('\t') AS (user, time);
STORE raw INTO 't1-results' USING PigStorage();
{code}

Command line (w/o set in script):
{code}
bin/pig -Dmapred.output.compress=true -Dmapred.output.compression.codec=org.apache.hadoop.io.compress.BZip2Codec tst.pig
{code}
","08/Feb/12 19:12;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/3805/
-----------------------------------------------------------

Review request for pig.


Summary
-------

See PIG-2508


This addresses bug PIG-2508.
    https://issues.apache.org/jira/browse/PIG-2508


Diffs
-----

  trunk/src/org/apache/pig/backend/datastorage/DataStorage.java 1241308 
  trunk/src/org/apache/pig/backend/hadoop/datastorage/HDataStorage.java 1241308 
  trunk/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java 1241308 
  trunk/src/org/apache/pig/tools/grunt/GruntParser.java 1241308 
  trunk/test/org/apache/pig/test/TestInputOutputFileValidator.java 1241308 

Diff: https://reviews.apache.org/r/3805/diff


Testing
-------


Thanks,

Daniel

","08/Feb/12 23:40;thw;Current patch causes failure in TestHBaseStorage
","09/Feb/12 02:46;thw;New patch for 0.9: Before using jobConf to set new values from script, set all current properties from pigContext.
",09/Feb/12 02:59;thw;TestHBaseStorage passes with this patch. Will run full tests again.,"09/Feb/12 06:37;daijy;PIG-2508.4.patch is much better. Now new property will not add to pigcontext.properties directly, instead, it injects into jobconf and then read back to pigcontext.properties. Jobconf will have a chance to process depredation key. It is cumbersome but should work. I am fine with it for 0.9 cuz it requires less change. For 0.10/trunk, I would like to make things more cleaner.","09/Feb/12 17:31;thw;All unit tests pass with .4 patch.

@Daniel: Key is to set properties through an instance of JobConf, as this is the only way to apply deprecation logic. The interface contracts of java.util.Properties and JobConf variant of Configuration are not compatible. What we put into JobConf is not necessarily what we get back for a given key. A cleaner solution could be to change all relate code to work with Configuration instead of Properties and use a JobConf instance as prototype.
","09/Feb/12 19:22;thw;Zebra unit tests fail with the following error:

{code}

Unable to open iterator for alias records
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias records
        at org.apache.pig.PigServer.openIterator(PigServer.java:901)
        at org.apache.hadoop.zebra.pig.TestMapTableLoader.testReader(TestMapTableLoader.java:136)
Caused by: org.apache.pig.PigException: ERROR 1002: Unable to store alias records
        at org.apache.pig.PigServer.storeEx(PigServer.java:1000)
        at org.apache.pig.PigServer.store(PigServer.java:963)
        at org.apache.pig.PigServer.openIterator(PigServer.java:876)
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 2043: Unexpected error during execution.
        at org.apache.pig.PigServer.launchPlan(PigServer.java:1325)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1299)
        at org.apache.pig.PigServer.storeEx(PigServer.java:996)
Caused by: java.lang.IllegalStateException: Variable substitution depth too large: 20 ${fs.default.name}
        at org.apache.hadoop.conf.Configuration.substituteVars(Configuration.java:399)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:469)
        at org.apache.hadoop.fs.FileSystem.getDefaultUri(FileSystem.java:131)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:242)
        at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:225)
        at org.apache.hadoop.mapred.LocalJobRunner.<init>(LocalJobRunner.java:418)
        at org.apache.hadoop.mapred.JobClient.init(JobClient.java:472)
        at org.apache.hadoop.mapred.JobClient.<init>(JobClient.java:457)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:125)
        at org.apache.pig.PigServer.launchPlan(PigServer.java:1314)


{code}",10/Feb/12 02:38;thw;Adding fix to zebra build.xml to not pass placeholders for undefined properties into unit test.,10/Feb/12 04:36;daijy;+1 for 0.9 branch. I will commit it shortly.,"10/Feb/12 06:02;daijy;test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Patch committed to 0.9 branch. For 0.10/trunk, we might use a different approach. I will open a separate Jira to track.

Thanks Thomas!","16/Feb/12 00:56;thw;Fix needed for 0.10 until better solution can be developed.
",16/Feb/12 19:48;thw;Unit tests pass on 0.10,"16/Feb/12 20:00;daijy;Committed to 0.10 as Thomas requested. I need couple of more days to work on a better fix, which will go to trunk.","16/Feb/12 22:00;thw;0.10 also needs a change to TestAvroStorage, which otherwise fails on 0.23 (patch attached). See PIG-2319

{code}

Testcase: testArrayWithSnappyCompression took 0.033 sec
        Caused an ERROR
'mapred.output.compress' is set but no value is specified for 'mapred.output.compression.codec'.
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobCreationException: ERROR 0: 'mapred.output.compress' is set but no value is specified for 'mapred.output.compression.codec'.
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJob(JobControlCompiler.java:366)
{code}

",16/Feb/12 22:13;daijy;PIG-2508-0.10-TestAvroStorage.patch committed to 0.10/trunk.,"24/Feb/12 01:06;daijy;Fixed in 0.9/0.10. For trunk, I would like to address it in PIG-2552.","31/May/12 16:55;julienledem;Hi Daniel,
As it seems PIG-2552 has not made progress so far, can we commit this patch in trunk to avoid a regression when using trunk?
PIG-2552 can still happen when it's ready.
",31/May/12 17:40;daijy;That's fine for me.,31/May/12 20:52;dvryaboy;Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Semicolon in parameters for UDF results in parsing error,PIG-2507,12541392,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,tnachen,vivekp,vivekp,06/Feb/12 10:40,02/Oct/17 07:27,14/Mar/19 03:07,18/Mar/13 04:08,0.10.0,0.11,0.11.1,0.8.0,0.9.1,,,0.11.2,0.12.0,,,,,,0,,,,,,,,,,,,,"If I have a semicolon in the parameter passed to a udf, the script execution will fail with a parsing error.

a = load 'i1' as (f1:chararray);
c = foreach a generate REGEX_EXTRACT(f1, '.;' ,1);
dump c;

The above script fails with the below error 
[main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1200: <file test.pig, line 3, column 0>  mismatched character '<EOF>' expecting '''

Even replacing the semicolon with Unicode \u003B results in the same error.
c = foreach a generate REGEX_EXTRACT(f1, '.\u003B',1);",,,,,,,,,,,,,,,,,,,PIG-4889,PIG-4818,,,,,,,,,,,,04/Mar/13 06:18;tnachen;PIG_2507.patch;https://issues.apache.org/jira/secure/attachment/12571848/PIG_2507.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-24 08:54:22.703,,,no_permission,,,,,,,,,,,,226680,Reviewed,,,Mon Oct 02 07:27:01 UTC 2017,,,Patch Available,,,,0|i0azqf:,62067,,,,,,,,,,"06/Feb/12 10:42;vivekp;Btw, the same script when run on Pig 0.8 provides the exact source location. The below is the error message fro, Pig 0.8;
[main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Lexical error at line 2, column 45.  Encountered: <EOF> after : ""\'.;""",24/Oct/12 08:54;tnachen;Fixes the semicolon issue,"24/Oct/12 08:54;tnachen;I tried to find where I can add tests for loading script in via grunt, but not sure how to do that",27/Oct/12 14:44;rohini;You should be able to add tests for grunt in http://svn.apache.org/repos/asf/pig/branches/branch-0.10/test/org/apache/pig/test/TestGrunt.java,"25/Jan/13 19:16;alangates;Changes to the code look fine, but we definitely need a unit test to check that they work.  Adding it in TestGrunt as Rohini suggested makes sense.  Canceling the patch pending adding of tests.",04/Mar/13 06:18;tnachen;Added unit test in TestGruntParser for this bug,"04/Mar/13 06:18;tnachen;Updated the patch now to include unit test for this bug, please review again.",18/Mar/13 04:08;daijy;Patch committed to trunk. Thanks Timothy!,"10/May/13 05:50;satyaharish;Trying Using '\\u003B', it works. I am using it on Pig 0.10.0 version.","14/Aug/13 21:31;cheolsoo;Committed to branch 11:
http://svn.apache.org/viewvc?view=revision&revision=1514056

Updated the fix version.","02/Oct/17 07:25;githubbot;GitHub user deepika087 opened a pull request:

    https://github.com/apache/pig/pull/32

    Fixed the PIG-2507: related to grunt error

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/deepika087/pig branch-0.7

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/pig/pull/32.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #32
    
----
commit 050a4697408acbe62a0d87398d35f821c36479ad
Author: Deepika Anand <deepika@deepikas-macbook-pro-2.local>
Date:   2017-10-02T07:23:50Z

    Fixed the PIG-2507: related to grunt error

----
","02/Oct/17 07:27;githubbot;Github user deepika087 closed the pull request at:

    https://github.com/apache/pig/pull/32
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroStorage won't read any file not ending in .avro,PIG-2505,12541142,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,russell.jurney,russell.jurney,russell.jurney,03/Feb/12 22:01,22/Jun/12 03:36,14/Mar/19 03:07,20/Mar/12 02:33,0.10.0,0.9.1,,,,,,0.10.0,0.11,,,piggybank,,,0,avro,fun,happy,pants,pig,pig_avro,pig_udf,,,,,,"AvroStorage will not load any files that do not end in .avro; This is problematic when you are reading part of the output of a hadoop job, such as 'part-000001', etc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Mar/12 02:29;daijy;PIG-2505-2.patch;https://issues.apache.org/jira/secure/attachment/12519007/PIG-2505-2.patch,03/Feb/12 22:03;russell.jurney;PIG-2505.patch;https://issues.apache.org/jira/secure/attachment/12513185/PIG-2505.patch,28/Feb/12 02:36;russell.jurney;PIG-2505.tests.patch;https://issues.apache.org/jira/secure/attachment/12516265/PIG-2505.tests.patch,20/Mar/12 02:29;daijy;expected_testFileWithNoExtension.avro;https://issues.apache.org/jira/secure/attachment/12519008/expected_testFileWithNoExtension.avro,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-03-15 18:39:44.516,,,no_permission,,,,,,,,,,,,226494,Reviewed,,,Wed Mar 21 00:33:09 UTC 2012,,,Patch Available,,,,0|i0h2yv:,97743,,,,,,,,,,03/Feb/12 22:03;russell.jurney;Removes the check for .avro in PigAvroInputFormat.java,28/Feb/12 02:36;russell.jurney;Test added for the other patch.,15/Mar/12 18:39;daijy;Shall we check the directory name ends with .avro in this case?,"15/Mar/12 19:34;russell.jurney;We want it to read any directory or file, regardless of filename.",17/Mar/12 00:28;daijy;The input file for test test_no_extension is not in the patch. Can you attach?,17/Mar/12 01:33;daijy;Merge patches and resync with trunk. Still waiting for test_no_extension.,20/Mar/12 00:26;russell.jurney;Coming up.,"20/Mar/12 00:35;russell.jurney;I don't know how to submit a patch when a binary file is added.  So here's a recipe.

cd contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/avro/avro_test_files/
cp test_record.avro test_no_extension

",20/Mar/12 00:40;daijy;Let me try,20/Mar/12 02:33;daijy;Patch committed to 0.10/trunk. Thanks Russell!,20/Mar/12 23:46;jcoveney;Daniel: the test_no_exception file didn't get added.,"21/Mar/12 00:33;daijy;Thanks, added now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Make ""hcat.bin"" configurable in e2e test",PIG-2502,12540992,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,02/Feb/12 20:30,22/Feb/13 04:53,14/Mar/19 03:07,03/Feb/12 00:59,0.11,,,,,,,0.11,,,,impl,,,0,,,,,,,,,,,,,"After PIG-2482, e2e test fail if hcat binary is not in standard location. We need to make it configurable in e2e test.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Feb/12 23:25;daijy;PIG-2502-1.patch;https://issues.apache.org/jira/secure/attachment/12513065/PIG-2502-1.patch,22/Feb/12 22:35;daijy;PIG-2502-2.patch;https://issues.apache.org/jira/secure/attachment/12515656/PIG-2502-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-02-02 23:34:35.507,,,no_permission,,,,,,,,,,,,226345,Reviewed,,,Wed Feb 22 22:35:04 UTC 2012,,,,,,,0|i09z0f:,56113,,,,,,,,,,02/Feb/12 23:34;thejas;+1,03/Feb/12 00:59;daijy;Patch committed to trunk.,22/Feb/12 22:35;daijy;Missing -Dhcat.bin in one test. Attach PIG-2502-2.patch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Order of execution of fs, store and sh commands in Pig is not maintained",PIG-2497,12540488,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,viraj,viraj,31/Jan/12 03:03,26/Apr/12 20:33,14/Mar/19 03:07,07/Feb/12 19:49,0.9.1,,,,,,,0.10.0,0.11,0.9.3,,impl,,,0,,,,,,,,,,,,,"I have a pig script like this :
--Load data, process it and store to two outputs
{code}
a = load 'dummy.txt' as (cookie: chararray,timestamp: long,url: chararray);
b = group a by (cookie);
c = foreach b generate group, COUNT_STAR(a);
store c into '$COUNT_OUTPUT' using PigStorage();
store b into '$GRID_OUTPUT' using PigStorage();
--Remove local file, copy to local and remove processed file from grid
sh rm -rf '$LOCAL_OUTPUT';
fs -getmerge '$GRID_OUTPUT' '$LOCAL_OUTPUT';
fs -rmr '$GRID_OUTPUT';
{code}

Pig does not guarantee the order of command execution in the above script i.e. the ""store"" ""sh rm..."", ""fs -getmerge ..."" and ""fs -rmr ..."" will not be executed in the written order.

Pig guarantees that ""fs"" commands and pig ""store"" commands will be executed in sequence. But ""sh"" commands will get executed before anything else (in normal multi-query mode) because ""sh""  commands are executed when the parser sees them. They go through a different code path within Pig. This behavior needs to be changed.


Thanks
Viraj",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,31/Jan/12 06:17;daijy;PIG-2497-1.patch;https://issues.apache.org/jira/secure/attachment/12512536/PIG-2497-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-31 03:49:46.284,,,no_permission,,,,,,,,,,,,225901,Reviewed,,,Tue Feb 07 19:49:16 UTC 2012,,,,,,,0|i09yzr:,56110,,,,,,,,,,"31/Jan/12 03:49;daijy;Seems we didn't enforce an exec for sh, but we did for fs.",31/Jan/12 20:10;viraj;Daniel can we target this patch for Pig 0.9.3 and Pig 0.10.1,07/Feb/12 03:42;thejas;+1 . Created PIG-2516 to track other issues remaining in sh implementation that Daniel found.,"07/Feb/12 19:49;daijy;Unit test pass. test-patch:     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 527 release audit warnings (more than the trunk's current 524 warnings).

javadoc warning is unrelated. No new file added, ignore release audit warning. 

Patch committed to 0.9/0.10/trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using merge JOIN from a HBaseStorage produces an error,PIG-2495,12540391,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,bridiver,kados,kados,30/Jan/12 16:31,21/Nov/14 05:59,14/Mar/19 03:07,30/Oct/14 17:12,0.13.1,0.9.1,0.9.2,,,,,0.14.0,,,,,,,0,,,,,,,,,,,,,"To increase performance of my computation, I would like to use a merge join between two tables to increase speed computation but it produces an error.

Here is the script:
{noformat}
start_sessions = LOAD 'hbase://startSession.bea000000.dev.ubithere.com' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage('meta:infoid meta:imei meta:timestamp', '-loadKey') AS (sid:chararray, infoid:chararray, imei:chararray, start:long);
end_sessions = LOAD 'hbase://endSession.bea000000.dev.ubithere.com' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage('meta:timestamp meta:locid', '-loadKey') AS (sid:chararray, end:long, locid:chararray);
sessions = JOIN start_sessions BY sid, end_sessions BY sid USING 'merge';
STORE sessions INTO 'sessionsTest' USING PigStorage ('*');
{noformat} 

Here is the result of this script :
{noformat}
2012-01-30 16:12:43,920 [main] INFO  org.apache.pig.Main - Logging error messages to: /root/pig_1327939963919.log
2012-01-30 16:12:44,025 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://lxc233:9000
2012-01-30 16:12:44,102 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to map-reduce job tracker at: lxc233:9001
2012-01-30 16:12:44,760 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: MERGE_JION
2012-01-30 16:12:44,923 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false
2012-01-30 16:12:44,982 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 2
2012-01-30 16:12:44,982 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 2
2012-01-30 16:12:45,001 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added to the job
2012-01-30 16:12:45,006 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2012-01-30 16:12:45,039 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:zookeeper.version=3.3.2-1031432, built on 11/05/2010 05:32 GMT
2012-01-30 16:12:45,039 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:host.name=lxc233.machine.com
2012-01-30 16:12:45,039 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.version=1.6.0_22
2012-01-30 16:12:45,039 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.vendor=Sun Microsystems Inc.
2012-01-30 16:12:45,039 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.home=/usr/lib/jvm/java-6-sun-1.6.0.22/jre
2012-01-30 16:12:45,039 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.class.path=/opt/hadoop/conf:/usr/lib/jvm/java-6-sun/jre/lib/tools.jar:/opt/hadoop:/opt/hadoop/hadoop-0.20-append-core.jar:/opt/hadoop/lib/commons-cli-1.2.jar:/opt/hadoop/lib/commons-codec-1.3.jar:/opt/hadoop/lib/commons-el-1.0.jar:/opt/hadoop/lib/commons-httpclient-3.0.1.jar:/opt/hadoop/lib/commons-logging-1.0.4.jar:/opt/hadoop/lib/commons-logging-api-1.0.4.jar:/opt/hadoop/lib/commons-net-1.4.1.jar:/opt/hadoop/lib/core-3.1.1.jar:/opt/hadoop/lib/hadoop-fairscheduler-0.20-append.jar:/opt/hadoop/lib/hadoop-gpl-compression-0.2.0-dev.jar:/opt/hadoop/lib/hadoop-lzo-0.4.14.jar:/opt/hadoop/lib/hsqldb-1.8.0.10.jar:/opt/hadoop/lib/jasper-compiler-5.5.12.jar:/opt/hadoop/lib/jasper-runtime-5.5.12.jar:/opt/hadoop/lib/jets3t-0.6.1.jar:/opt/hadoop/lib/jetty-6.1.14.jar:/opt/hadoop/lib/jetty-util-6.1.14.jar:/opt/hadoop/lib/junit-4.5.jar:/opt/hadoop/lib/kfs-0.2.2.jar:/opt/hadoop/lib/log4j-1.2.15.jar:/opt/hadoop/lib/mockito-all-1.8.2.jar:/opt/hadoop/lib/oro-2.0.8.jar:/opt/hadoop/lib/servlet-api-2.5-6.1.14.jar:/opt/hadoop/lib/slf4j-api-1.4.3.jar:/opt/hadoop/lib/slf4j-log4j12-1.4.3.jar:/opt/hadoop/lib/xmlenc-0.52.jar:/opt/hadoop/lib/jsp-2.1/jsp-2.1.jar:/opt/hadoop/lib/jsp-2.1/jsp-api-2.1.jar:/opt/pig/bin/../conf:/usr/lib/jvm/java-6-sun/jre/lib/tools.jar:/opt/hadoop/lib/commons-codec-1.3.jar:/opt/hbase/lib/guava-r06.jar:/opt/hbase/hbase-0.90.3.jar:/opt/hadoop/lib/log4j-1.2.15.jar:/opt/hadoop/lib/commons-cli-1.2.jar:/opt/hadoop/lib/commons-logging-1.0.4.jar:/opt/pig/pig-withouthadoop.jar:/opt/hadoop/conf_computation:/opt/hbase/conf:/opt/pig/bin/../lib/hadoop-0.20-append-core.jar:/opt/pig/bin/../lib/hadoop-gpl-compression-0.2.0-dev.jar:/opt/pig/bin/../lib/hbase-0.90.3.jar:/opt/pig/bin/../lib/pigudfs.jar:/opt/pig/bin/../lib/zookeeper-3.3.2.jar:/opt/pig/bin/../pig-withouthadoop.jar:
2012-01-30 16:12:45,039 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.library.path=/opt/hadoop/lib/native/Linux-amd64-64
2012-01-30 16:12:45,039 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.io.tmpdir=/tmp
2012-01-30 16:12:45,039 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.compiler=<NA>
2012-01-30 16:12:45,039 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:os.name=Linux
2012-01-30 16:12:45,039 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:os.arch=amd64
2012-01-30 16:12:45,039 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:os.version=2.6.32-5-amd64
2012-01-30 16:12:45,039 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:user.name=root
2012-01-30 16:12:45,039 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:user.home=/root
2012-01-30 16:12:45,039 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:user.dir=/root
2012-01-30 16:12:45,039 [main] INFO  org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=lxc233.machine.com:2222,lxc231.machine.com:2222,lxc234.machine.com:2222 sessionTimeout=180000 watcher=hconnection
2012-01-30 16:12:45,048 [main-SendThread()] INFO  org.apache.zookeeper.ClientCnxn - Opening socket connection to server lxc231.machine.com/192.168.1.231:2222
2012-01-30 16:12:45,049 [main-SendThread(lxc231.machine.com:2222)] INFO  org.apache.zookeeper.ClientCnxn - Socket connection established to lxc231.machine.com/192.168.1.231:2222, initiating session
2012-01-30 16:12:45,081 [main-SendThread(lxc231.machine.com:2222)] INFO  org.apache.zookeeper.ClientCnxn - Session establishment complete on server lxc231.machine.com/192.168.1.231:2222, sessionid = 0x134c294771a073f, negotiated timeout = 180000
2012-01-30 16:12:46,569 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job
2012-01-30 16:12:46,590 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.
2012-01-30 16:12:46,870 [Thread-13] INFO  org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=lxc233.machine.com:2222,lxc231.machine.com:2222,lxc234.machine.com:2222 sessionTimeout=180000 watcher=hconnection
2012-01-30 16:12:46,871 [Thread-13-SendThread()] INFO  org.apache.zookeeper.ClientCnxn - Opening socket connection to server lxc233.machine.com/192.168.1.233:2222
2012-01-30 16:12:46,871 [Thread-13-SendThread(lxc233.machine.com:2222)] INFO  org.apache.zookeeper.ClientCnxn - Socket connection established to lxc233.machine.com/192.168.1.233:2222, initiating session
2012-01-30 16:12:46,872 [Thread-13-SendThread(lxc233.machine.com:2222)] INFO  org.apache.zookeeper.ClientCnxn - Session establishment complete on server lxc233.machine.com/192.168.1.233:2222, sessionid = 0x2343822449935e1, negotiated timeout = 180000
2012-01-30 16:12:46,880 [Thread-13] INFO  org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=lxc233.machine.com:2222,lxc231.machine.com:2222,lxc234.machine.com:2222 sessionTimeout=180000 watcher=hconnection
2012-01-30 16:12:46,880 [Thread-13-SendThread()] INFO  org.apache.zookeeper.ClientCnxn - Opening socket connection to server lxc233.machine.com/192.168.1.233:2222
2012-01-30 16:12:46,880 [Thread-13-SendThread(lxc233.machine.com:2222)] INFO  org.apache.zookeeper.ClientCnxn - Socket connection established to lxc233.machine.com/192.168.1.233:2222, initiating session
2012-01-30 16:12:46,882 [Thread-13-SendThread(lxc233.machine.com:2222)] INFO  org.apache.zookeeper.ClientCnxn - Session establishment complete on server lxc233.machine.com/192.168.1.233:2222, sessionid = 0x2343822449935e2, negotiated timeout = 180000
2012-01-30 16:12:47,091 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete
2012-01-30 16:12:47,703 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_201201201546_0890
2012-01-30 16:12:47,703 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - More information at: http://lxc233:50030/jobdetails.jsp?jobid=job_201201201546_0890
2012-01-30 16:12:55,723 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 25% complete
2012-01-30 16:13:49,312 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 33% complete
2012-01-30 16:13:55,322 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 50% complete
2012-01-30 16:13:57,327 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - job job_201201201546_0890 has failed! Stop running all dependent jobs
2012-01-30 16:13:57,327 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete
2012-01-30 16:13:57,337 [main] ERROR org.apache.pig.tools.pigstats.SimplePigStats - ERROR: Could create instance of class org.apache.pig.backend.hadoop.hbase.HBaseStorage$1, while attempting to de-serialize it. (no default constructor ?)
2012-01-30 16:13:57,337 [main] ERROR org.apache.pig.tools.pigstats.PigStatsUtil - 1 map reduce job(s) failed!
2012-01-30 16:13:57,338 [main] INFO  org.apache.pig.tools.pigstats.SimplePigStats - Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
0.20-append	0.9.2-SNAPSHOT	root	2012-01-30 16:12:44	2012-01-30 16:13:57	MERGE_JION

Failed!

Failed Jobs:
JobId	Alias	Feature	Message	Outputs
job_201201201546_0890	end_sessions	INDEXER	Message: Job failed!	

Input(s):
Failed to read data from ""hbase://endSession.bea000000.dev.ubithere.com""

Output(s):

Counters:
Total records written : 0
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_201201201546_0890	->	null,
null


2012-01-30 16:13:57,338 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Failed!
2012-01-30 16:13:57,339 [main] ERROR org.apache.pig.tools.grunt.GruntParser - ERROR 2997: Encountered IOException. Could create instance of class org.apache.pig.backend.hadoop.hbase.HBaseStorage$1, while attempting to de-serialize it. (no default constructor ?)
Details at logfile: /root/pig_1327939963919.log
2012-01-30 16:13:57,339 [main] ERROR org.apache.pig.tools.grunt.GruntParser - ERROR 2244: Job failed, hadoop does not return any error message
Details at logfile: /root/pig_1327939963919.log
{noformat} 

And here is the result in the log file :
{noformat}
Backend error message
---------------------
java.io.IOException: Could create instance of class org.apache.pig.backend.hadoop.hbase.HBaseStorage$1, while attempting to de-serialize it. (no default constructor ?)
	at org.apache.pig.data.BinInterSedes.readWritable(BinInterSedes.java:235)
	at org.apache.pig.data.BinInterSedes.readDatum(BinInterSedes.java:336)
	at org.apache.pig.data.BinInterSedes.readDatum(BinInterSedes.java:251)
	at org.apache.pig.data.BinInterSedes.addColsToTuple(BinInterSedes.java:556)
	at org.apache.pig.data.BinSedesTuple.readFields(BinSedesTuple.java:64)
	at org.apache.pig.impl.io.PigNullableWritable.readFields(PigNullableWritable.java:114)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:40)
	at org.apache.hadoop.mapreduce.ReduceContext.nextKeyValue(ReduceContext.java:113)
	at org.apache.hadoop.mapreduce.ReduceContext.nextKey(ReduceContext.java:92)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:175)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:566)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)
	at org.apache.hadoop.mapred.Child.main(Child.java:170)
Caused by: java.lang.InstantiationException: org.apache.pig.backend.hadoop.hbase.HBaseStorage$1
	at java.lang.Class.newInstance0(Class.java:340)
	at java.lang.Class.newInstance(Class.java:308)
	at org.apache.pig.data.BinInterSedes.readWritable(BinInterSedes.java:231)
	... 13 more

Pig Stack Trace
---------------
ERROR 2997: Encountered IOException. Could create instance of class org.apache.pig.backend.hadoop.hbase.HBaseStorage$1, while attempting to de-serialize it. (no default constructor ?)

java.io.IOException: Could create instance of class org.apache.pig.backend.hadoop.hbase.HBaseStorage$1, while attempting to de-serialize it. (no default constructor ?)
	at org.apache.pig.data.BinInterSedes.readWritable(BinInterSedes.java:235)
	at org.apache.pig.data.BinInterSedes.readDatum(BinInterSedes.java:336)
	at org.apache.pig.data.BinInterSedes.readDatum(BinInterSedes.java:251)
	at org.apache.pig.data.BinInterSedes.addColsToTuple(BinInterSedes.java:556)
	at org.apache.pig.data.BinSedesTuple.readFields(BinSedesTuple.java:64)
	at org.apache.pig.impl.io.PigNullableWritable.readFields(PigNullableWritable.java:114)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:40)
	at org.apache.hadoop.mapreduce.ReduceContext.nextKeyValue(ReduceContext.java:113)
	at org.apache.hadoop.mapreduce.ReduceContext.nextKey(ReduceContext.java:92)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:175)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:566)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)
	at org.apache.hadoop.mapred.Child.main(Child.java:170)
Caused by: java.lang.InstantiationException: org.apache.pig.backend.hadoop.hbase.HBaseStorage$1
	at java.lang.Class.newInstance0(Class.java:340)
	at java.lang.Class.newInstance(Class.java:308)
	at org.apache.pig.data.BinInterSedes.readWritable(BinInterSedes.java:231)
================================================================================
Pig Stack Trace
---------------
ERROR 2244: Job failed, hadoop does not return any error message

org.apache.pig.backend.executionengine.ExecException: ERROR 2244: Job failed, hadoop does not return any error message
	at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:139)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:192)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:164)
	at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)
	at org.apache.pig.Main.run(Main.java:561)
	at org.apache.pig.Main.main(Main.java:111)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
================================================================================
{noformat}

The same script without using merge works without any problem.","HBase 0.90.3, Hadoop 0.20-append",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Oct/14 19:26;daijy;PIG-2495-2.patch;https://issues.apache.org/jira/secure/attachment/12674569/PIG-2495-2.patch,30/Oct/14 06:25;daijy;PIG-2495-Collectable.patch;https://issues.apache.org/jira/secure/attachment/12678129/PIG-2495-Collectable.patch,15/Mar/12 14:36;kados;PIG-2495.patch;https://issues.apache.org/jira/secure/attachment/12518467/PIG-2495.patch,29/Oct/14 21:16;bridiver;patch;https://issues.apache.org/jira/secure/attachment/12678000/patch,29/Jun/14 00:52;bridiver;patch;https://issues.apache.org/jira/secure/attachment/12653035/patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2012-03-06 16:44:57.867,,,no_permission,,,,,,,,,,,,225804,Reviewed,,,Thu Oct 30 17:15:44 UTC 2014,,,,,,,0|i0eptz:,83947,,,,,,,,,,"06/Mar/12 16:44;billgraham;Thanks for the patch Kevin! A few note about Pig code style:

* Indentation should be 4 spaces, you have 2 in some spots.
* Curly brackets should go at the end of the class name or constructor/method signature, not below it.
* Please include the standard apache header above the package name for TableSplitComparable
* I _think_ we favor brackets in if/else clauses but I'll let someone else confirm.

And a few more notes comments:

* I would think {{TableSplitComparable}} should implement {{WritableComparable<TableSplit>}} instead of {{WritableComparable<TableSplitComparable>}}, right?

* Your hashcode method seems like it could just be
{noformat}
return ((tsplit == null) ? 0 : tsplit.hashCode());
{noformat}

since it's just delegating to tsplit. 

* Also, the condition in equals could just be:

{noformat}
else {
  return tsplit.equals(other.tsplit);
}
{noformat}


* I don't think WritableComparable needs to implement Serializable and serialVersionUID.
* Should the wrapped TableSplit be initialized to an empty split? It seems like it should have to be explicitly set, right?
* In getSplitComparable you can just return {{new TableSplitComparable((TableSplit)split);}} after a ! instanceof check that throws an exception.","13/Mar/12 17:30;kados;Thanks for those remarks. I've modified my patch with all your comments.
Does it seems okay for you ?","14/Mar/12 04:53;billgraham;Looks good to me besides one last nit, which is to include {{TableSplit}} as the generic type in the {{HBaseStorage.getSplitComparable}} signature like so:

{noformat}
public WritableComparable<TableSplit> getSplitComparable(InputSplit split) throws IOException
{noformat}
","14/Mar/12 04:53;billgraham;Looks good to me besides one last nit, which is to include {{TableSplit}} as the generic type in the {{HBaseStorage.getSplitComparable}} signature like so:

{noformat}
public WritableComparable<TableSplit> getSplitComparable(InputSplit split) throws IOException
{noformat}
","14/Mar/12 09:57;kados;Okay, it's done, thanks for your help! What must I do next? ""Resolve"" the issue?
","14/Mar/12 22:23;alangates;Resolve is when a committer has checked it in.  The next step will be for one of the committers to review it and run tests, and possibly commit it.","14/Mar/12 23:20;dvryaboy;Assigning to Kevin (you don't have to do anything, that's just to give you credit). I'll review the patch.","14/Mar/12 23:30;dvryaboy;A few minor comments:

The @since annotation is wrong -- even if we decide to backport this all the way to 0.9 branch, we have to make it 0.9.3 since 0.9.2 is released. 

toString -- should probably return something more useful than just the class name? Maybe concatenate the actual split's toString()?

Overall, I'm not sure what caused the old code to not work and how this is supposed to fix the issue. Just for my edification, can you explain? The difference is only that before, we implemented WritableComparable<InputSplit> and now you implement WritableComparable<TableSplit>?  The test you added fails when I apply it to trunk:


Testcase: testMergeJoin took 21.401 sec
        FAILED
expected:<0> but was:<48>
junit.framework.AssertionFailedError: expected:<0> but was:<48>
        at org.apache.pig.test.TestHBaseStorage.testMergeJoin(TestHBaseStorage.java:910)

","15/Mar/12 14:47;kados;OKay, I've modified the @since to 0.9.3. toString() now return the class name and the split's toString().

There was an issue because the getSplitComparable must return something Serializable (with a default constructor). The previously anonymous class hadn't a such thing.

I've also modified the unit test but I can't test it on my computer because I've a timeout when I run it: does someone knows why?","05/Nov/12 03:45;cheolsoo;Hi Kevin,

Sorry for the late reply. I was looking into your patch to commit it, but I ran into the same test failure as what Dmitriy mentioned.
{quote}
I've also modified the unit test but I can't test it on my computer because I've a timeout when I run it: does someone knows why?
{quote}
Does your test log contain anything? Can you please upload your test log to the jira? It can be found at build/test/logs/TEST-org.apache.pig.test.TestHBaseStorage.txt.

I am canceling patch available for now until the test is fixed.

Thanks!","24/Jul/13 02:56;pradeepg26;Hi Kevin,

I have a very minor request for your patch. When throwing the RuntimeException, could you also include the class information for the given type? This could potentially be useful for debugging purposes.","03/Apr/14 17:38;yzou;Hi, I wonder if this patch is still being worked on or not? is it still planned to be pulled to the main tip of pig? I am hitting the exactly the same bug doing a merge JOIN on 0.12.0 stable, only that it is actually a self join from the same input hbase table, thought I would not think that matter for the sake of this bug being still there. If you guys need help on getting the patch move forward, I will be glad to help, it's a good fix in my opinion.

thanks","26/Jun/14 18:03;bridiver;I am working on it. I fixed the test, it was a mismatch with binary with string types in hbase. I'm also implementing IndexableLoadFunc and CollectableLoadFunc to give HBaseStorage the full range of optimized joins and groups",26/Jun/14 18:04;bridiver;There was also an issue with the join key in the test,"29/Jun/14 00:52;bridiver;Here is my patch for 0.12.1 that includes the existing patch, but fixes the test and adds support for IndexableLoadFunc and CollectableLoadFunc","13/Oct/14 19:26;daijy;I am fine with SplitComparable changes since I see all the review comments from Bill and Dmitriy are addressed and TestHBaseStorage pass for me.

In seekNear, we'd better to use existing objToBytes method, so that we can deal alternative caster, additional primary key (DateTime, BigInterger), and complex key. I modified the patch with this change and also rebase with trunk.","13/Oct/14 20:08;bridiver;Although the test passes, there appeared to be some problems when I ran this on a larger data set so I think there might be an issue with the implementation. I'll try it again with your changes and verify whether or not there was an issue. Perhaps it's best to split off the changes for IndexableLoadFunc from the CollectableLoadFunc ones?","13/Oct/14 21:39;daijy;Thanks for verifying. Do you see any exception?

Also what do you mean ""split off the changes for IndexableLoadFunc from the CollectableLoadFunc""?","13/Oct/14 21:53;bridiver;Two different interfaces were implemented in this patch. IndexableLoadFunc, which possibly still has an issue and CollectableLoadFunc which is a no-op on hbase because keys are unique. I'm suggesting maybe I should make two separate patches because CollectableLoadFunc works great (although there is a related bug in pig https://issues.apache.org/jira/browse/PIG-4166), but I'm not 100% sure on IndexableLoadFunc. I didn't get any exceptions, but the data returned by the script on our actual data didn't look right. ",13/Oct/14 22:48;daijy;Sounds good. We can get CollectableLoadFunc part in first.,"13/Oct/14 23:06;daijy;Does it work if you put ""--caster HBaseBinaryConverter"" in HBaseStorage option? By default, HBaseStorage uses Utf8StorageConverter, which is Pig specific.",29/Oct/14 21:16;bridiver;here is the patch against branch-0.13 for merge join and collected group,"30/Oct/14 06:25;daijy;LGTM, attach a patch resync with trunk.

How about IndexableLoadFunc? Do you still want to do it?","30/Oct/14 16:43;bridiver;Yes, but I think there are some problems with the current implementation. Probably best to open another ticket for outer merge join so this one can be closed out with the patch.",30/Oct/14 17:08;bridiver;https://issues.apache.org/jira/browse/PIG-4254,30/Oct/14 17:12;daijy;PIG-2495-Collectable.patch committed to both trunk and 0.14 branch. Created PIG-4255 for IndexableLoadFunc change. Thanks Brian!,30/Oct/14 17:13;bridiver;I think we doubled up on the new ticket. I'll let you decide which one to keep so we don't end up deleting both of them.,30/Oct/14 17:15;daijy;I removed PIG-4255. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UNION causes casting issues,PIG-2493,12539995,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,vivekp,anitharaju,anitharaju,27/Jan/12 04:44,06/Jan/13 23:57,14/Mar/19 03:07,18/Dec/12 21:33,0.10.0,0.9.1,,,,,,0.10.1,0.11,0.9.3,,,,,1,,,,,,,,,,,,,"Hi,

For the below script,

{code}
A = load '/user/anithar/ip' as (a);
B = load '/user/anithar/ip1' as (a);
C = union  A , B ;
D = foreach C generate (chararray)a;
dump D;
{code}

it gives casting error at runtime

{code}
org.apache.pig.backend.executionengine.ExecException: ERROR 1075: Received a bytearray from the UDF. Cannot determine how to convert the bytearray to string.
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(POCast.java:660)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:322)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:332)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:284)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:267)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:262)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1082)
        at org.apache.hadoop.mapred.Child.main(Child.java:249)
{code}

It looks like in POCast.java the value of ""funcSpec"" is not getting any value(stays null when there is a UNION involved), causing ""caster"" to get null and thus the exception.

The same works in 0.8 without any issue.

Regards,
Anitha
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Feb/12 00:18;daijy;PIG-2493-3.patch;https://issues.apache.org/jira/secure/attachment/12513075/PIG-2493-3.patch,01/Feb/12 11:34;vivekp;PIG-2493.patch;https://issues.apache.org/jira/secure/attachment/12512743/PIG-2493.patch,02/Feb/12 16:51;vivekp;PIG-2493_2.patch;https://issues.apache.org/jira/secure/attachment/12513001/PIG-2493_2.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-01-31 12:50:05.686,,,no_permission,,,,,,,,,,,,225498,Reviewed,,,Tue Dec 18 21:33:36 UTC 2012,,,,,,,0|i0h2x3:,97735,,,,,,,,,,"31/Jan/12 12:50;vivekp;This looks like a bug introduced in Pig 0.9  as part of new parser changes. 
Unlike Pig 0.8, 0.9 uses ""uid"" to find the function spec. But for this script it seems like LOUnion 
always increments the ""uid"" and hence the func spec is never resolved.

Moreover, there is an internal error also generated;
""Cannot resolve load function to use for casting from bytearray to chararray""

Can we propagate this message to the actual exception.

",01/Feb/12 11:34;vivekp;Attaching an initial patch.,"02/Feb/12 06:46;daijy;Couple of comments:
1. Shall we use addUidLoadFuncToMap instead of manipulating uid2LoadFuncMap directly?
2. Do we need to add every field to uid2LoadFuncMap instead of just field 0?",02/Feb/12 16:51;vivekp;Thanks Daniel for the comments. Attaching a patch considering both comments and modified test case.,02/Feb/12 19:34;daijy;Looks good. I will commit it upon tests pass.,03/Feb/12 00:18;daijy;PIG-2493-3.patch resync with trunk.,"06/Feb/12 20:19;daijy;Unit tests pass. test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 527 release audit warnings (more than the trunk's current 524 warnings).

javadoc warning is unrelated, no new file added so ignore release audit warning.

Patch committed to 0.9/0.10/trunk.","17/Apr/12 15:52;arov;Facing similar issue. I have a feeling this bug is not fully resolved. I have a fairly complicated script that I can not easily share. The main though is the following: 

I can successfully store A & B relation.
I cannot store C which is A UNION B operation.

Getting the following exception:

java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Integer
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(POCast.java:432)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:330)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:332)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:284)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POUnion.getNext(POUnion.java:165)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:271)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:266)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:647)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:323)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:210)


Schema:
a: {timestamp: chararray,date_time: chararray,conversion_type: chararray,channel_type: chararray,campaign_id: int,adgroup_id: int,order_id: chararray,order_sales: double,delta: long,uuid: chararray,ctid: long,advertiser_id: int,client_tid: int,conversion_provenance: chararray}

b: {timestamp: chararray,date_time: chararray,iponweb_conversions::conversion_type: chararray,channel_type: chararray,iponweb_conversions::campaign_id: int,iponweb_conversions::adgroup_id: int,order_id: chararray,order_sales: double,delta: long,iponweb_conversions::uuid: chararray,ctid: long,iponweb_conversions::advertiser_id: int,client_tid: long,conversion_provenance: chararray}

Resulting union schema:

c: {timestamp: chararray,date_time: chararray,conversion_type: chararray,channel_type: chararray,campaign_id: int,adgroup_id: int,order_id: chararray,order_sales: double,delta: long,uuid: chararray,ctid: long,advertiser_id: int,client_tid: long,conversion_provenance: chararray}


I tried replicating this issue with a simple script that mimics the two schemas and could not reproduce the issue. (This makes me believe the plan is at fault?)
",19/Nov/12 20:16;julienledem;please resolve this ticket and open a separate one for changes.,"18/Dec/12 21:33;julienledem;I'm closing this issue as it has been committed and we are stabilizing a release.
[~arov] please open a new JIRA if you still see problems",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Input Path Globbing{} not working with PigStorageSchema or PigStorage('\t', '-schema');",PIG-2489,12539565,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,miteshsjat,miteshsjat,24/Jan/12 04:47,26/Apr/12 20:32,14/Mar/19 03:07,07/Feb/12 19:29,0.10.0,0.9.0,0.9.1,,,,,0.10.0,0.11,,,,,,1,,,,,,,,,,,,,"
{code:title=test.pig}
-- For Pig 0.9
--A = LOAD 'input/PigStorageSchema/Temp{1,2}/pss*' USING org.apache.pig.piggybank.storage.PigStorageSchema();
-- For Pig 0.10
A = LOAD 'input/PigStorageSchema/Temp{1,2}/pss*' USING PigStorage('\t', '-schema');

DESCRIBE A;

DUMP A
{code}

Schema file _input/PigStorageSchema/Temp{1,2}.pig_schema_
{code}
{""fields"":[{""name"":""name"",""type"":55,""schema"":null,""description"":""autogenerated from Pig Field Schema""},{""name"":""val"",""type"":10,""schema"":null,""description"":""autogenerated from Pig Field Schema""}],""version"":0,""sortKeys"":[],""sortKeyOrders"":[]}
{code}

Header file _input/PigStorageSchema/Temp{1,2}/.pig_header_
{code}
name    val
{code}

Sample input file _input/PigStorageSchema/Temp1/pss.in_
{code}
peter   1
samir   3
michael 4
peter   2
peter   4
samir   1
{code}

On running the above pig script _test.pig_ with pig 0.10, the following error is received.
{noformat}
012-01-24 04:07:42,210 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1131: Could not find schema file for hdfs://nameNode:8020/user/mitesh/input/PigStorageSchema/Temp{1,2}/pss*
{noformat}

Pig Stack Trace
{noformat}
Pig Stack Trace
---------------
ERROR 1131: Could not find schema file for hdfs://nameNode:8020/user/mitesh/input/PigStorageSchema/Temp{1,2}/pss*

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias A
    at org.apache.pig.PigServer.openIterator(PigServer.java:858)
    at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:655)
    at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:303)
    at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:188)
    at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:164)
    at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)
    at org.apache.pig.Main.run(Main.java:567)
    at org.apache.pig.Main.main(Main.java:111)
Caused by: org.apache.pig.PigException: ERROR 1002: Unable to store alias A
    at org.apache.pig.PigServer.storeEx(PigServer.java:957)
    at org.apache.pig.PigServer.store(PigServer.java:920)
    at org.apache.pig.PigServer.openIterator(PigServer.java:833)
    ... 7 more
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2245: 
<file test.pig, line 2, column 4> Cannot get schema from loadFunc org.apache.pig.builtin.PigStorage
    at org.apache.pig.newplan.logical.relational.LOLoad.getSchemaFromMetaData(LOLoad.java:154)
    at org.apache.pig.newplan.logical.relational.LOLoad.getSchema(LOLoad.java:109)
    at org.apache.pig.newplan.logical.relational.LOStore.getSchema(LOStore.java:68)
    at org.apache.pig.newplan.logical.visitor.SchemaAliasVisitor.validate(SchemaAliasVisitor.java:60)
    at org.apache.pig.newplan.logical.visitor.SchemaAliasVisitor.visit(SchemaAliasVisitor.java:84)
    at org.apache.pig.newplan.logical.relational.LOStore.accept(LOStore.java:77)
    at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
    at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
    at org.apache.pig.PigServer$Graph.compile(PigServer.java:1618)
    at org.apache.pig.PigServer$Graph.compile(PigServer.java:1612)
    at org.apache.pig.PigServer$Graph.access$200(PigServer.java:1335)
    at org.apache.pig.PigServer.storeEx(PigServer.java:952)
    ... 9 more
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1131: Could not find schema file for hdfs://nameNode:8020/user/mitesh/input/PigStorageSchema/Temp{1,2}/pss*
    at org.apache.pig.builtin.JsonMetadata.nullOrException(JsonMetadata.java:222)
    at org.apache.pig.builtin.JsonMetadata.getSchema(JsonMetadata.java:191)
    at org.apache.pig.builtin.PigStorage.getSchema(PigStorage.java:438)
    at org.apache.pig.newplan.logical.relational.LOLoad.getSchemaFromMetaData(LOLoad.java:150)
{noformat}


Whereas PigStorageSchema() or PigStorage('\t', '-schema') works with *wildcard* *.
For example, following script works
{code:title=test2.pig}
A = LOAD 'input/PigStorageSchema/Temp*/pss*' USING PigStorage('\t', '-schema');

DESCRIBE A;

DUMP A;
{code}


As a workaround to make Temp{1,2} globbing work, the *,(comma)* separated multiple input paths (with no globbing)
can given as input. 
{code:title=test2.pig}
A = LOAD 'input/PigStorageSchema/Temp1/pss*,input/PigStorageSchema/Temp2/pss*' USING PigStorage('\t', '-schema');

DESCRIBE A;

DUMP A;
{code}",,,,,,,,,,,,,,,,,,,PIG-2453,PIG-2209,,,,,,,,,,,,31/Jan/12 03:48;daijy;PIG-2489-1.patch;https://issues.apache.org/jira/secure/attachment/12512528/PIG-2489-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-24 05:11:58.554,,,no_permission,,,,,,,,,,,,225067,Reviewed,,,Tue Feb 07 19:29:24 UTC 2012,,,,,,,0|i09yzz:,56111,,,,,,,,,,24/Jan/12 05:11;dvryaboy;This is fixed in trunk.,"24/Jan/12 13:01;vivekp;With the updated branch, for a script like below (i1,i2 are directories), 

A = LOAD '2489/i{1,2}/' USING PigStorage('\t', '-schema');
DESCRIBE A;
DUMP A;

I am still getting the below error (even with patch from PIG-2209 );
""Caused by: java.net.URISyntaxException: Illegal character in path at index 65:  hdfs://namnode:8020/user/pvivek/2489/i{1,2}"" 


Looking at JsonMetadata.findMetaFile(String path, String metaname, Configuration conf)
| storage = new HDataStorage(new URI(loc), ConfigurationUtil.toProperties(conf));
If I change this to 
|storage = new HDataStorage((new org.apache.hadoop.fs.Path(loc)).toUri(),ConfigurationUtil.toProperties(conf));

The scripts are running fine for Pig 0.10.
Please help me in understanding how this case is handled in trunk. 
Thanks
",24/Jan/12 17:02;ciemo;Yes! PLEASE FIX THIS! Get it into Pig 0.10 ASAP.,07/Feb/12 02:24;thejas;+1 ,"07/Feb/12 19:29;daijy;Unit tests pass. test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 527 release audit warnings (more than the trunk's current 524 warnings).

javadoc warning is unrelated. No new file added, ignore release audit warning.

Patch committed to 0.10/trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix several e2e test failures/aborts for 23,PIG-2484,12539074,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,20/Jan/12 00:05,25/Aug/12 00:17,14/Mar/19 03:07,21/Jan/12 01:03,0.10.0,0.11,0.9.2,,,,,0.10.0,0.11,0.9.3,,impl,,,0,,,,,,,,,,,,,"There are still a couple of e2e test aborts/failures for hadoop23. Most of them are due to test infrastructure, minor backward incompatibility change in 23, or recent changes in Pig. Here is a list:

Scripting_1/Scripting_2: MAPREDUCE-3700

Native_3: 23 test need a hadoop23-steaming.jar

MonitoredUDF_1: Seems related to guava upgrade (PIG-2460), Pig's guava is newer than hadoop23's

UdfException_1, UdfException_2, UdfException_3, UdfException_4: Error message change

Checkin_2, GroupAggFunc_7, GroupAggFunc_9, GroupAggFunc_12, GroupAggFunc_13, Types_6, Scalar_1: float precision

Limit_2: The specific output records change, test infrastructure should allow this",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Jan/12 02:37;daijy;PIG-2484-1.patch;https://issues.apache.org/jira/secure/attachment/12511213/PIG-2484-1.patch,20/Jan/12 08:19;daijy;PIG-2484-2.patch;https://issues.apache.org/jira/secure/attachment/12511238/PIG-2484-2.patch,20/Jan/12 21:12;daijy;PIG-2484-3.patch;https://issues.apache.org/jira/secure/attachment/12511321/PIG-2484-3.patch,06/Aug/12 01:27;rohini;PIG-2484-4-branch0.9.patch;https://issues.apache.org/jira/secure/attachment/12539230/PIG-2484-4-branch0.9.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-01-20 02:08:37.501,,,no_permission,,,,,,,,,,,,224576,Reviewed,,,Sat Aug 25 00:17:14 UTC 2012,,,,,,,0|i09z3z:,56129,,,,,,,,,,20/Jan/12 02:08;dvryaboy;For MonitoredUDF -- we should be putting our jars ahead of hadoop's. ,"20/Jan/12 02:40;daijy;Exactly. In hadoop 20.x, I see an option HADOOP_USER_CLASSPATH_FIRST (though guava never cause an issue in 20.x since 20.x does not use guava). However, I didn't see an equivalent option in hadoop 23. I will follow up with hadoop folks.",20/Jan/12 02:52;daijy;Attached PIG-2484-1.patch in an attempt to fix all e2e tests (or disable in 23). We also need to copy hadoop-0.23.0-streaming.jar into test/e2e/pig/lib.,"20/Jan/12 23:39;thejas;+1
The 23 specific test case parameters are very useful. We know what to revisit later (ignore23 etc).
I think we should see if the "" register './python/scriptingudf.py'.. "" use case would work with hadoop 23 if pig removes the ""./"" prefix. Opened PIG-2486 to track that.
","21/Jan/12 00:56;daijy;MonitoredUDF_1 exposes a general issue of mapreduce, how to override hadoop libraries. We see similar issue in AvroLoader as well. MAPREDUCE-1938 uses HADOOP_USER_CLASSPATH_FIRST (frontend) and mapreduce.user.classpath.first (backend). But that patch does not go into hadoop23. We need to push hadoop folks to apply this patch to 23, or provide a mechanism to solve the issue.","21/Jan/12 01:03;daijy;Patch committed to 0.10/trunk. I don't commit to 0.9 branch now, but we may when we feel a need.","06/Aug/12 01:27;rohini;Patch for Branch 0.9. 

- Removed ignore for Scripting_1 and Scripting_2 as PIG-2761 fixes the issue.
- Included the benchmark patch from PIG-2711 to make testing faster.
- Additional tests fixed from what is mentioned in the description. Will create a patch for these for 0.10 and trunk on a separate jira.
1)ClassResolution
   - Fully qualified UDF names were not used.
2)Native_x_Local 
   - This was failing as local.conf did not set mapredjars
3)Jython_Macro_1_Local
    - - Was failing because the input file was the data directory itself instead
of studenttab10K. In case of mapred and local, the number of files under data
directory was different so Jython_Macro_1 passed with the benchmark created during mapred, but
Jython_Macro_1_local failed with the benchmark from mapred.

Still Failing Tests not addressed in the patch:
1)Jython_CompileBindRun_3_local
   - Problem exists with 0.10 also. It launches 3 threads to launch multiple jobs. But with LocalJobRunner it does not work as all write to test/e2e/pig/testdist/build/test/mapred/local/localRunner/job_local_0001.xml and parsing it fails with org.xml.sax.SAXParseException: Content is not allowed in trailing section. 

Additional steps for checkin:
  test/e2e/pig/lib/hadoop-0.23.0-streaming.jar is not part of the patch. Copy file from branch-0.10 and svn add before committing the patch.
","06/Aug/12 01:54;rohini;Still Failing Tests not addressed in the patch:
2) MonitoredUDF_1_Local
    - It is skipped only for 0.23 mapred and not 0.23 local. 

","07/Aug/12 16:59;rohini;Did some debugging to see why the float precision is different between 20 and 23 to ensure that is not a cause for concern. Summing up of doubles in java has known issues (http://www.velocityreviews.com/forums/t139008-java-double-precision.html). The precision differs based on the order in which the numbers are summed up. That is the reason for precision differing between H20 and H23. The order in which key and values come to the output.collect from the map are same, but when the reduce of the combiner is called the order of values in Iterable<values> is different in 20 and 23. There must be some algo change somewhere for grouping of elements for the combiner. Did not take time and dig deeper to see what the actual change in mapred is. 
  We should be able to safely ignore the float precision change in the e2e tests.",07/Aug/12 18:21;daijy;Commit PIG-2484-4-branch0.9.patch to 0.9 branch as per requested by Rohini. Note this patch also include fix for PIG-2859.,14/Aug/12 19:51;rohini;test/e2e/pig/lib/hadoop-0.23.0-streaming.jar also needs to be checked in. It was not part of the patch as it is a binary. Had mentioned it as a additional step during checkin. ,25/Aug/12 00:17;daijy;Added hadoop-0.23.0-streaming.jar to 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestUDFContext.testUDFContext failing against hadoop 23,PIG-2480,12538743,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,phunt,phunt,18/Jan/12 06:40,23/Jan/12 07:31,14/Mar/19 03:07,18/Jan/12 07:47,0.9.2,,,,,,,0.9.2,,,,,,,1,,,,,,,,,,,,,"I'm seeing one remaining test failure against hadoop 23 -- TestUDFContext, run as:

ant -Dhadoopversion=23 -Dtestcase=TestUDFContext clean test

it fails with the following using 0.9.2 rc1

Failed to read data from ""/home/phunt/Downloads/p/pig-0.9.2/a.txt""
Failed to read data from ""/home/phunt/Downloads/p/pig-0.9.2/b.txt""

Looks like a test setup issue to me but it's not entirely clear what the test is shooting for.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Jan/12 07:43;daijy;PIG-2480-1.patch;https://issues.apache.org/jira/secure/attachment/12510963/PIG-2480-1.patch,18/Jan/12 07:16;phunt;TEST-org.apache.pig.test.TestUDFContext.txt.gz;https://issues.apache.org/jira/secure/attachment/12510961/TEST-org.apache.pig.test.TestUDFContext.txt.gz,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-01-18 07:05:46.541,,,no_permission,,,,,,,,,,,,224246,,,,Wed Jan 18 07:50:33 UTC 2012,,,,,,,0|i09z4n:,56132,,,,,,,,,,"18/Jan/12 07:05;qwertymaniac;While the cluster setup seems redundant and can be removed, the real failure seems to be cause of a class-not-found, arising out of the need for commons-httpclient which doesn't seem to be available for the test's LocalJobRunner to use:

{code}

------------- Standard Error -----------------
Jan 18, 2012 12:33:15 PM com.google.inject.servlet.GuiceFilter setPipeline
WARNING: Multiple injectors detected. Please install only one ServletModule in your web application. While you may have more than one injector, you should only configure guice-servlet in one of them. (Hint: look for legacy ServetModules or multiple calls to Servlets.configure()).
Jan 18, 2012 12:33:17 PM com.google.inject.servlet.GuiceFilter setPipeline
WARNING: Multiple injectors detected. Please install only one ServletModule in your web application. While you may have more than one injector, you should only configure guice-servlet in one of them. (Hint: look for legacy ServetModules or multiple calls to Servlets.configure()).
Jan 18, 2012 12:33:20 PM com.google.inject.servlet.GuiceFilter setPipeline
WARNING: Multiple injectors detected. Please install only one ServletModule in your web application. While you may have more than one injector, you should only configure guice-servlet in one of them. (Hint: look for legacy ServetModules or multiple calls to Servlets.configure()).
Jan 18, 2012 12:33:22 PM com.google.inject.servlet.GuiceFilter setPipeline
WARNING: Multiple injectors detected. Please install only one ServletModule in your web application. While you may have more than one injector, you should only configure guice-servlet in one of them. (Hint: look for legacy ServetModules or multiple calls to Servlets.configure()).
Jan 18, 2012 12:33:23 PM com.google.inject.servlet.GuiceFilter setPipeline
WARNING: Multiple injectors detected. Please install only one ServletModule in your web application. While you may have more than one injector, you should only configure guice-servlet in one of them. (Hint: look for legacy ServetModules or multiple calls to Servlets.configure()).
XXX: Setting fs.default.name to: hdfs://localhost:55361
Exception in thread ""Thread-366"" java.lang.NoClassDefFoundError: org/apache/commons/httpclient/HttpMethod
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:475)
Caused by: java.lang.ClassNotFoundException: org.apache.commons.httpclient.HttpMethod
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
	... 1 more
------------- ---------------- ---------------

Testcase: testUDFContext took 39.467 sec
	Caused an ERROR
Unable to open iterator for alias D
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias D
	at org.apache.pig.PigServer.openIterator(PigServer.java:858)
	at org.apache.pig.test.TestUDFContext.testUDFContext(TestUDFContext.java:72)
Caused by: java.io.IOException: Job terminated with anomalous status FAILED
	at org.apache.pig.PigServer.openIterator(PigServer.java:850)

Testcase: testUDFContextReset took 0.253 sec
{code}",18/Jan/12 07:16;phunt;Harsh that's different from what I see - attached. fwiw I was running with 0.9.2rc1.,"18/Jan/12 07:48;daijy;Commit to 0.9 branch (Since the fix is trivial and is to fix a failed unit test, skip the review)

The same issue does not exist in 0.10/trunk.",18/Jan/12 07:50;daijy;The issue Harsh saw is fixed by PIG-2347.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestBuiltin testLFText/testSFPig failing against 23 due to invalid test setup -- InvalidInputException,PIG-2477,12538692,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,phunt,phunt,phunt,17/Jan/12 20:07,23/Jan/12 07:31,14/Mar/19 03:07,18/Jan/12 00:15,0.9.2,,,,,,,0.10.0,0.11,0.9.2,,,,,0,,,,,,,,,,,,,"I see the following exception when running against hadoop 23: (all test-commit tests pass except for these two)

{noformat}
org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/var/lib/hudson/workspace/CDH4-Pig-0.9.2-test-commit/testSFPig-output.txt
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:243)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigTextInputFormat.listStatus(PigTextInputFormat.java:36)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:269)
	at org.apache.pig.impl.io.ReadToEndLoader.init(ReadToEndLoader.java:154)
	at org.apache.pig.impl.io.ReadToEndLoader.<init>(ReadToEndLoader.java:116)
	at org.apache.pig.test.TestBuiltin.testSFPig(TestBuiltin.java:2177)
{noformat}

This happens in both testLFText/testSFPig.

Looking at these two tests it seems that toConfiguration is being called improperly, I have a patch (momentarily) that will address this similar to:

{noformat}
-                toConfiguration(new Properties()), ""testLFTest-input1.txt"", 0);
+            toConfiguration(cluster.getProperties()), ""testLFTest-input1.txt"", 0);
{noformat}","ubuntu, hadoop23",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Jan/12 20:10;phunt;PIG-2477.patch;https://issues.apache.org/jira/secure/attachment/12510889/PIG-2477.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-17 21:02:14.706,,,no_permission,,,,,,,,,,,,224195,Reviewed,,,Wed Jan 18 00:15:08 UTC 2012,,,,,,,0|i0h2v3:,97726,,,,,,,,,,17/Jan/12 20:10;phunt;This patch fixes the issue in my environment.,"17/Jan/12 21:02;daijy;Thanks, I will commit the patch to 0.9 branch.","17/Jan/12 22:02;azaroth;I see similar problems in trunk, I think we need to apply the patch also there.","17/Jan/12 22:18;dvryaboy;I just had a different patch of mine get the exact opposite treatment from Daniel because tests were failing when doing cluster.getProperties().

Daniel, how does one determine when to use which method?","17/Jan/12 22:38;daijy;Hi, Dmitriy,
The affected tests seems still test against MiniCluster, right? Then we should use cluster.getProperties() in this case. Sorry I miss it when I review the patch.","17/Jan/12 22:44;dvryaboy;My mistake. The change I had in mind was replacing System.getProperties() with ""new Properties()"", not cluster.getProperties() (in PIG-2448)

+1",18/Jan/12 00:15;daijy;Patch committed to 0.9/0.10/trunk. Thanks Patrick!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix eclipse files for pig 9,PIG-2473,12538282,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,13/Jan/12 02:38,23/Jan/12 07:31,14/Mar/19 03:07,13/Jan/12 23:57,0.9.2,,,,,,,0.9.2,,,,build,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Jan/12 09:09;daijy;PIG-2473-1.patch;https://issues.apache.org/jira/secure/attachment/12510476/PIG-2473-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-13 23:22:23.763,,,no_permission,,,,,,,,,,,,223788,Reviewed,,,Fri Jan 13 23:57:18 UTC 2012,,,,,,,0|i09z5j:,56136,,,,,,,,,,"13/Jan/12 23:22;thejas;+1
Verified that it works with eclipse.
",13/Jan/12 23:57;daijy;Patch committed to 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
piggybank unit tests write directly to /tmp,PIG-2472,12538277,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thw,thw,thw,13/Jan/12 00:27,23/Jan/12 07:31,14/Mar/19 03:07,14/Jan/12 00:13,0.10.0,0.9.1,,,,,,0.10.0,0.11,0.9.2,,piggybank,,,0,,,,,,,,,,,,,"Unlike the pig unit tests, java.io.tmpdir does not point to a test run specific directory. This causes CI failures if previous runs leave behind artifacts.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Jan/12 01:56;thw;PIG-2472-1.patch;https://issues.apache.org/jira/secure/attachment/12510448/PIG-2472-1.patch,13/Jan/12 00:33;thw;PIG-2472.patch;https://issues.apache.org/jira/secure/attachment/12510443/PIG-2472.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-01-13 00:46:54.604,,,no_permission,,,,,,,,,,,,223783,Reviewed,,,Fri Jan 13 02:38:05 UTC 2012,,,,,,,0|i0h2uf:,97723,,,,,,,,,,"13/Jan/12 00:46;daijy;Patch looks good.

Committed to 0.9/0.10/trunk.","13/Jan/12 01:56;thw;The test tmp dir also should be deleted after the run. Extra patch attached - sorry for the noise.
 ",13/Jan/12 02:38;daijy;PIG-2472-1.patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issue with CSVEXcelStorage piggy bank function,PIG-2470,12538168,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jpacker,pkarkele,pkarkele,12/Jan/12 09:17,14/Oct/13 16:46,14/Mar/19 03:07,26/Mar/13 04:42,0.9.0,,,,,,,0.12.0,,,,piggybank,,,1,,,,,,,,,,,,,"CSVExcelStorage piggy bank function skips the record, which has 1 or more  null column(s) in it. The record is not written to the file",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Apr/12 09:42;prkommireddi;PIG-2470.patch;https://issues.apache.org/jira/secure/attachment/12525008/PIG-2470.patch,04/May/12 00:24;prkommireddi;PIG-2470_2.patch;https://issues.apache.org/jira/secure/attachment/12525540/PIG-2470_2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-02-25 00:53:05.194,,,no_permission,,,,,,,,,,,,223674,,,,Tue Mar 26 04:42:46 UTC 2013,,,Patch Available,,,,0|i0h2tz:,97721,,,,,,,,,,25/Feb/12 00:53;stickyhipp;Can you provide an example?,"25/Apr/12 23:30;juan.gentile;I'm having the same problem, this is what I'm doing:

raw = load file...;
var = foreach raw generate udf(raw.field);/*the udf returns null*/
store var using CSVExcelStorage(',');

hope this helps,
thanks","25/Apr/12 23:45;prkommireddi;The bug seems to be with putNext()

{code}
// Do the escaping:
    	for (Object field : tupleToWrite.getAll()) {
    		fieldCounter++;
    		if (field == null) {
    			logger.warn(""Field "" + fieldCounter + "" within tuple '"" + tupleToWrite + ""' is null."");
    			return;
    		}
    
{code}

I think the intent here must have been to ""continue"" instead of ""return"" on coming across a null field.","29/Apr/12 09:42;prkommireddi;Fixed bug. Adding string ""null"" to output record when field is null. 

Also removed @author from the file.","02/May/12 19:37;juan.gentile;Instead of using ""null"" can we use and empty string? (in case we need to store empty string we use an empty string with quotes). BTW this way is how PigStorage interprets nulls (having 2 consecutive commas without quotes). CSVLoader seems to have a bug on this regard as it never interprets null.","03/May/12 10:19;prkommireddi;Does PigStorage generate quotes in the output? 

PigStorage differs from CSVExcelStorage in that output could be any data type, whereas it is always String in the latter case. I feel being able to *clearly* distinguish between a null return (from a UDF) vs an empty string return would be useful. But it also makes sense if CSV storefunc is expected to behave similar to PigStorage. Open to both!

Thoughts?","03/May/12 12:14;juan.gentile;PigStorage doesn't escape strings with quotes (which is why we are attempting to use CSVExcelStorage), in fact, while reading, it considers the quotes as part of the values. About null, since PigStorage already though about null I think it'd be great if we could somehow have a similar way of handling it. This is just a thought.

thanks","04/May/12 00:01;thejas;CSVExcelStorage is supposed to follow the Excel conventions. I can't find any official documentation on the convention, but it looks like excel CSV format does not support null. I think empty fields is what most people would expect for nulls. Having null string has its own issues, specially if the string null is a legitimate column value. This change is less likely to break existing scripts.

Changing the behavior of CSVLoader to return null for empty fields will be non backward compatible change, that can break people's existing code.
",04/May/12 00:24;prkommireddi;Thanks for the feedback Juan and Thejas. New patch adds an empty field.,"07/May/12 18:31;daijy;Thanks Prashant, can you also include a test case?","19/Mar/13 21:39;cheolsoo;PIG-3141 incorporates the fix. It also adds a unit test case for storing null values:
https://reviews.apache.org/r/9697/diff/#1.75

Can we mark this jira ""as part of"" PIG-3141, or do we want to resolve it separately? I am pointing out because we probably want to avoid duplicate effort.","19/Mar/13 22:22;prkommireddi;Sure, let's do that. You are right, it would not make sense fixing it at both places.

Thanks Cheolsoo.",26/Mar/13 04:42;cheolsoo;Closing the jira since it's fixed as part of PIG-3141.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getWrappedSplit is incorrectly returning the first split instead of the current split.,PIG-2462,12537799,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,arov,arov,arov,09/Jan/12 18:12,23/Jan/12 07:31,14/Mar/19 03:07,14/Jan/12 00:04,0.11,0.9.1,,,,,,0.10.0,0.11,0.9.2,,,,,0,,,,,,,,,,,,,"If your loader needs information regarding what file is currently is being read (lets say for schema information), currently provides this ability by calling prepareToRead every time we read a new split. This is critical for ComibinedInputFormat as each mapper can read more then one file. In order for the load function to know what file we are currently reading, it should call getWrappedSplit() to get that information. How ever, getWrappedSplit always returns the first split in the list. Code from PigSplit.java:

    /**
     * This methods returns the actual InputSplit (as returned by the 
     * {@link InputFormat}) which this class is wrapping.
     * @return the wrappedSplit
     */
    public InputSplit getWrappedSplit() {
        return wrappedSplits[0];
    }


Furthermore, in PigRecordReader.java the splitIndex is never incremented when changing from split to split. So in fact, even if getWrappedSplit() wold be changed to return wrappedSplits[splitIndex]; it would still return the incorrect index. 

This can be fixed by changing PigRecordReader to increment PigSplit.splitIndex everytime the split chagnes in the following code:


    /**
     * Get the record reader for the next chunk in this CombineFileSplit.
     */
    protected boolean initNextRecordReader() throws IOException, InterruptedException {

        if (curReader != null) {
            curReader.close();
            curReader = null;
            if (idx > 0) {
                progress += pigSplit.getLength(idx-1);    // done processing so far
            }
        }

        // if all chunks have been processed, nothing more to do.
        if (idx == pigSplit.getNumPaths()) {
            return false;
        }

        // get a record reader for the idx-th chunk
        try {
          

            curReader =  inputformat.createRecordReader(pigSplit.getWrappedSplit(idx), context);
            LOG.info(""Current split being processed ""+pigSplit.getWrappedSplit(idx));

            if (idx > 0) {
                // initialize() for the first RecordReader will be called by MapTask;
                // we're responsible for initializing subsequent RecordReaders.
                curReader.initialize(pigSplit.getWrappedSplit(idx), context);
                pigSplit.get
                loadfunc.prepareToRead(curReader, pigSplit);
            }
        } catch (Exception e) {
            throw new RuntimeException (e);
        }
        idx++;
        return true;
    }
}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Jan/12 20:42;daijy;PIG-2462-2.patch;https://issues.apache.org/jira/secure/attachment/12510527/PIG-2462-2.patch,13/Jan/12 20:50;daijy;PIG-2462-2_0.9.patch;https://issues.apache.org/jira/secure/attachment/12510529/PIG-2462-2_0.9.patch,12/Jan/12 17:46;arov;split_fix_take2.patch;https://issues.apache.org/jira/secure/attachment/12510396/split_fix_take2.patch,10/Jan/12 13:40;arov;splitsfix.patch;https://issues.apache.org/jira/secure/attachment/12510042/splitsfix.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-01-10 09:27:12.39,,,no_permission,,,,,,,,,,,,223305,Reviewed,,,Thu Jan 19 23:09:57 UTC 2012,,,Patch Available,,,,0|i0h2s7:,97713,,,,,,,,,,"09/Jan/12 19:17;arov;I am attempting to make a patch. Proposed fixes:

Add pigSplit.setSplitIndex(idx) before curReader.initialize(pigSplit.getWrappedSplit(idx), context);

change return wrappedSplits[0]; to  return wrappedSplits[splitIndex]; in PigSplit.getWrappedSplit();
","10/Jan/12 09:27;daijy;splitIndex is the index of PigSplit, idx keeps track of current InputSplit within PigSplit. I feel changing wrappedSplits[0] into wrappedSplits[idx] should be enough.",10/Jan/12 13:40;arov;Attaching patch generated by git format-patch. I couldn't verify all unit tests since there is currently an issue with them in trunk. Most of the unit tests passed and I have verified this patch with my loader which had the mentioned issue.,"10/Jan/12 13:46;arov;Daniel -- there is no  member idx in PigSplit.java. The index thats supposed to be tracked is splitIndex. Furthermore with combined input format, PigRecordReader does not increment the value of splitIndex when switching the reading from one split to the next even though it does increment and uses this index internally. Therefore if we just change wrappedSplits[0] to wrappedSplits[splitIndex] you will still have this issue. I have verified that splitIndex is not modified anywhere except through the constructor in PigSplit.java. 

I have made the needed code changes and have verified them with my loader. Now my log messages from my loader correspond to the log messages from PigRecordReader (originally that was not the case).","10/Jan/12 19:00;daijy;Yes, you are right. That's why you pass index from PigRecordReader to PigSplit. Your approach looks right. Except for the name ""splitIndex"", we usually refer it to the index of PigSplit itself, not the index of InputSplit inside PigSplit. It's better to use ""idx"" to make it less confused. ",10/Jan/12 19:50;prkommireddi;How would this change affect MergeJoinIndexer.java which uses pigSplit.getWrappedSplit() ?,"10/Jan/12 20:09;daijy;We need to test. But I feel get the right split instead 0 should be the right way. Just browse through the code, I didn't find anything wrong with the change.","10/Jan/12 20:15;prkommireddi;I did not find anything wrong either by looking at the code, just making sure.","10/Jan/12 23:16;aniket486;splitIndex's definition is getting changed here. (its not newly added).
splitIndex is used for keeping track of Pigsplit itself. While reading records with pigrecordreader, should we really change this index?

From MergeJoinIndexer code, consider a case where one pigsplit is associate with one wrappedsplit and we have a couple of pigsplit. Before we do wrapperTuple.set(keysCnt+1, pigSplit.getSplitIndex()); on line 179, we do loader.getnext(). I am not very sure about this, but this might down the stack hit PigRecordReader.initNextRecordReader that will reset the splitIndex on pigsplit to 0 for every pigsplit.

It would be safer to keep the idx on pigsplit as a separate variable and copy down from PigRecordReader as we need.

Thoughts?","10/Jan/12 23:45;daijy;Thanks Aniket, I just realize the patch try to reuse splitIndex. We shall add a new variable idx to PigSplit, it is set by PigRecordReader.initNextRecordReader and consumed by PigSplit.getWrappedSplit, to track the current InputSplit. It has nothing to do with PigSplit.splitIndex.","10/Jan/12 23:55;aniket486;Hi Daniel,
Is there a way to work around this issue elegantly? Basically, information on split needs to be available to loadfunc. I can think of getting this at createRecordReader level on the inputformat returned by getInputFormat. But, how do I pass it down elegantly to the loadfunc from there.
Can you suggest an idea?
Thanks,
Aniket","11/Jan/12 00:29;daijy;Correct me if wrong, but I thought this is exactly the issue we want to solve. LoadFunc will be passed PigSplit in prepareToRead, and we want user call pigSplit.getWrappedSplit() to get split specific information. The problem in current Pig is pigSplit.getWrappedSplit() always get #0 split. So we have this Jira to fix it.",11/Jan/12 00:41;aniket486;Yes. I am just wondering if there is a way to workaround this (to avoid porting it back in pig). I think pig-user list might be a better place to discuss it. Thanks for your comments.,"11/Jan/12 16:30;arov;One way to avoid this issue is to disable combinedinputformat in your pigs jobs.

I guess I am a bit confused about the comments on the splitIndex as I am not very familiar with PIG's code base. Is split index used elsewhere and is not really meant to track the index of the current pigsplit that we are reading? If so, I can certainly change the patch to include another variable ""idx"" as suggested to keep track of this value.

How ever judging from the PigInputFormat.getPigSplits code:
for (int i = 0; i < combinedSplits.size(); i++)
                pigSplits.add(createPigSplit(combinedSplits.get(i), inputIndex, targetOps, i, conf));

Seems like the intention was to use splitIndex to track the current split?
","11/Jan/12 19:22;daijy;Hi, Alex,
We have two level of split, PigSplit and InputSplit, PigSplit is a wrap of several InputSplit. In PigInputFormat, we combine multiple InputSplit into one PigSplit. splitIndex track current PigSplit, idx track current InputSplit within PigSplit.","11/Jan/12 20:38;arov;Thanks Daniel for the info. 

Some questions:
splitIndex within the PigInputFormat tracks the current PigSplit correct?
What does splitIndex within the PigSplit track? (From my understanding it should track the current wrapped InputSplit)
There is also inputIndex within PigSplit. Wouldn't that track the InputSplit index?

Finally, do we need to introduce an ""idx"" in PigSplit or my patch would suffice?","11/Jan/12 20:54;daijy;bq. splitIndex within the PigInputFormat tracks the current PigSplit correct?
Yes
bq. What does splitIndex within the PigSplit track? (From my understanding it should track the current wrapped InputSplit)
It is the way PigSplit identify itself
bq. There is also inputIndex within PigSplit. Wouldn't that track the InputSplit index?
If a mapreduce job need more than 1 input (eg, join a, b, we have two input a & b in the same map), inputIndex tracks which input is it",12/Jan/12 17:45;arov;Attached the changes based on the comments,"12/Jan/12 18:59;daijy;Patch looks good. Test is a little complex, but is possible. We need to add a testcase.","12/Jan/12 19:27;arov;Is it possible to use CombinedInputFormat in PigUnit?
Any existing test you can point me to as an example?","12/Jan/12 22:30;daijy;You will need to write an inputformat, a loadfunc, and use PigUnit to invoke this loadfunc. Unfortunately I cannot find a sample with a custom inputformat in existing tests.",13/Jan/12 08:15;daijy;I may think too much. We don't need InputFormat in the test. We only need a LoadFunc. I attached patch with test case.,13/Jan/12 09:17;daijy;PIG-2462-2_0.9.patch is the same patch for 0.9 branch.,"14/Jan/12 00:04;daijy;+1 for patch.

Unit test pass.

test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 510 release audit warnings (more than the trunk's current 502 warnings).

All new file has Apache header, ignore release audit warning.

Patch committed to 0.9/0.10/trunk.

Thanks Alex!",19/Jan/12 22:05;yulia;This bug also exist in pig 0.8,"19/Jan/12 23:09;daijy;Yes, however, we don't have plan for another 0.8 release. Can you apply the patch to 0.8 branch and build by yourself?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't have spaces in parameter substitution,PIG-2458,12537456,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,jcoveney,jcoveney,06/Jan/12 01:47,28/Apr/12 00:05,14/Mar/19 03:07,07/Jan/12 01:49,0.11,0.9.2,,,,,,0.10.0,0.11,0.9.2,,,,,0,,,,,,,,,,,,,"I believe that there is a regression. We can no longer do the following:

bin/pig -x local -p THING=""other thing""

as is, I can't seem to escape this to make it work, either, which is worse than in the other issue.

I am testing a patch that should fix this.",,,,,,,,,,,,,,,,,,,PIG-2162,,,,,,,,,,,,,09/Jan/12 21:12;daijy;PIG-2458-2.patch;https://issues.apache.org/jira/secure/attachment/12509959/PIG-2458-2.patch,06/Jan/12 02:29;jcoveney;PIG2458.patch;https://issues.apache.org/jira/secure/attachment/12509649/PIG2458.patch,06/Jan/12 22:57;jcoveney;PIG2458_1.patch;https://issues.apache.org/jira/secure/attachment/12509734/PIG2458_1.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-01-06 18:58:58.956,,,no_permission,,,,,,,,,,,,222964,Reviewed,,,Fri Apr 27 23:40:34 UTC 2012,,,,,,,0|i0h2rj:,97710,,,,,,,,,,"06/Jan/12 02:28;jcoveney;Find patch attached. I'm not a bash master, so there is slight code repetition...someone else can feel free to make it nicer. It's affecting production so I went with what worked and was easy :)",06/Jan/12 02:44;jcoveney;This fixes a regression that was introduced at some point,"06/Jan/12 05:06;jcoveney;Question: is there a test for bin/pig? I can't seem to find an obvious one. I'd love to test for this since it's obviously not super commonly used, but has had a couple of regressions...just nto sure if we already have tests for bin/pig that I can lump this in with.","06/Jan/12 18:58;daijy;There is parameter tests in e2e tests. See test/e2e/pig/tests/nightly.conf, search for section ""Parameters"".",06/Jan/12 22:57;jcoveney;This adds a simple e2e test,07/Jan/12 01:46;daijy;+1. I will commit it shortly.,07/Jan/12 01:49;daijy;Patch committed to 0.9/0.10/trunk. Thanks Jonathan!,09/Jan/12 21:12;daijy;PIG-2458-2.patch fix e2e test Parameters_5 failure.,"27/Apr/12 23:40;aniket486;I tried to run this on pig-0.9 branch and pig-trunk. This isn't fixed! 

Try--
{code}
java -cp pig.jar org.apache.pig.Main -x local -f temp.pig -p ""NAME=Aniket Mokashi""
{code}

temp.pig-
{code}
a = load '1.txt';
b = foreach a generate '$NAME';
dump b;
{code}

Am I missing something?
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JsonLoaderStorage tests is broken for e2e,PIG-2457,12537240,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,04/Jan/12 20:58,26/Apr/12 20:32,14/Mar/19 03:07,04/Jan/12 21:08,0.10.0,0.11,,,,,,0.10.0,0.11,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Jan/12 21:03;daijy;PIG-2457-1.patch;https://issues.apache.org/jira/secure/attachment/12509459/PIG-2457-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-04 21:06:27.054,,,no_permission,,,,,,,,,,,,222749,Reviewed,,,Thu Jan 05 01:41:27 UTC 2012,,,,,,,0|i09z7z:,56147,,,,,,,,,,"04/Jan/12 21:03;daijy;PIG-2453 introduced jackson utilities into JsonMetaData, which is use in JsonLoader/JsonStorage in backend. We need to ship these classes.","04/Jan/12 21:06;alangates;+1, looks good.",04/Jan/12 21:08;daijy;Patch committed to trunk/0.10.,"04/Jan/12 21:27;dvryaboy;How did this work before? JsonMetaData already used jackson (but not the util package).

 import org.codehaus.jackson.JsonParseException;
 import org.codehaus.jackson.map.JsonMappingException;
 import org.codehaus.jackson.map.ObjectMapper;","04/Jan/12 21:28;dvryaboy;(the reason I ask is, if we are blowing up the size of our job jar and it's not necessary, I can just write my own LRUMap, it's pretty trivial)","04/Jan/12 21:37;daijy;Good question. Is that possible jackon-core.jar is part of hadoop, but jackson-mapper is not? Surely large job.jar is not good.","04/Jan/12 21:42;daijy;Here is the stack BTW:

Error: java.lang.ClassNotFoundException: org.codehaus.jackson.map.util.LRUMap at java.net.URLClassLoader$1.run(URLClassLoader.java:200) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:188) at java.lang.ClassLoader.loadClass(ClassLoader.java:306) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:276) at java.lang.ClassLoader.loadClass(ClassLoader.java:251) at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:319) at org.apache.pig.builtin.JsonMetadata.<init>(JsonMetadata.java:75) at org.apache.pig.builtin.JsonStorage.storeSchema(JsonStorage.java:269) at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter.storeCleanup(PigOutputCommitter.java:143) at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter.commitJob(PigOutputCommitter.java:180) at org.apache.hadoop.mapred.Task.runJobCleanupTask(Task.java:1002) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:357) at org.apache.hadoop.mapred.Child$4.run(Child.java:255) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:396) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059) at org.apache.hadoop.mapred.Child.main(Child.java:249) ","04/Jan/12 21:47;daijy;I see the problem. Hadoop 20.205 bundles jackson 1.0.1, which does not contain LRUMap.","04/Jan/12 23:45;dvryaboy;We should probably ensure our classes are used then. I checked the size of the jars, they are about 800K:

-rw-r--r--  1 dmitriy  staff   203K Feb 14  2011 build/ivy/lib/Pig/jackson-core-asl-1.7.3.jar
-rw-r--r--  1 dmitriy  staff   611K Feb 14  2011 build/ivy/lib/Pig/jackson-mapper-asl-1.7.3.jar

Kind of painful. We do need both the mapper and core. Do Hadoop's jars override ours on the classpath?","05/Jan/12 00:07;daijy;I think OutputCommitter is in backend. We will have to ship it or use hadoop's version of jackson. Another possible solution is to find out more specific package than org.codehaus.jackson(cuz only LRU class is not in 1.0.1), so we can ship less classes.","05/Jan/12 00:18;daijy;But PigStorage works fine, seems there is still something missing. I am looking.","05/Jan/12 00:32;dvryaboy;Like I said, I can just write an LRUMap of our own, it's not hard.
But I am worried about the version mismatch. This will affect AvroStorage too, Avro folks are the ones who requested that we bump our jackson version in the first place.","05/Jan/12 01:41;daijy;PigStorage also fail with ""-schema"" option (before I thought it by default creates the schema file).

Yes, version mismatch is a bigger problem. In frontend, user can use ""HADOOP_USER_CLASSPATH_FIRST"" to override hadoop bundled jars, but for backend, hadoop always use bundled jars first. In our case, hadoop cannot find LRUMap in bundled jackson, so it goes to job.jar to find it. For Avro, it is not that lucky. I don't know a solution for that. We shall definitely push hadoop to bundled a newer jackson in next version (I am testing on 20.205).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fetching schema can be very slow for multi-thousand LOADs,PIG-2453,12536769,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,dvryaboy,dvryaboy,02/Jan/12 01:25,26/Apr/12 20:32,14/Mar/19 03:07,03/Jan/12 04:59,0.10.0,0.11,,,,,,0.10.0,0.11,,,,,,0,,,,,,,,,,,,,"When a user tries to load resources with thousands of files using PigStorage, we spend an inordinate amount of time looking for schema files. This is because we check for a schema file per loaded file.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Jan/12 04:11;dvryaboy;PIG-2453.patch;https://issues.apache.org/jira/secure/attachment/12509026/PIG-2453.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-03 04:40:08.057,,,no_permission,,,,,,,,,,,,222452,,,,Tue Jan 03 04:59:27 UTC 2012,,,,,,,0|i0h2qn:,97706,"PigStorage will no longer check for file-specific schemas, and only look for the .pig_schema file in the loaded directory. The file-specific behavior never made it to a release, so this is not a change in expected functionality.",,,,,,,,,02/Jan/12 01:30;dvryaboy;One proposed solution is to only check for .pig_schema files on a per-directory level instead of per-file. We can also probably do fewer NN calls by caching all found schema files and not checking metaFilePath.exists() redundantly.,"02/Jan/12 04:11;dvryaboy;Added caching, changed JsonMetadata behavior to ignore file-specific schemas. I don't think this feature was used anywhere except tests anyway (?).",03/Jan/12 04:40;daijy;Patch looks good. +1 for less fs lookup. Would be even happier if no recursive lookup :),03/Jan/12 04:59;dvryaboy;Committed to 0.10 and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple Stores in pig streaming causes infinite waiting,PIG-2442,12535965,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xutingz,anitharaju,anitharaju,21/Dec/11 06:52,26/Apr/12 20:32,14/Mar/19 03:07,23/Mar/12 01:07,0.8.1,0.9.0,,,,,,0.10.0,0.11,0.9.3,,,,,0,,,,,,,,,,,,,"Hi,

If there are multiple store in a pig streaming script, it goes into infinite waiting. 

Script
{code}
DEFINE SCRIPT `./a.pl` SHIP ('/homes/anithar/a.pl');;
DEFINE SCRIPT1 `./b.pl` SHIP ('/homes/anithar/b.pl');;
A = LOAD 'test.txt' USING PigStorage() ;
B1 = STREAM A THROUGH SCRIPT ;
B1 = foreach B1 generate $0;
STORE B1 INTO 'B1' USING PigStorage();
B2 =  STREAM B1 THROUGH SCRIPT1;
STORE B2 INTO 'B2' USING PigStorage();
{code}

a.pl
--------
#! /usr/bin/perl -w
while (my $line = <STDIN>) {
        print uc($line);
}
--------

b.pl
---------
#! /usr/bin/perl -w
while (my $line = <STDIN>) {
        print $line;
}
---------

Input (test.txt)
{code}
test
hi
hello
{code}

This infinite waiting happens randomly causing the job to fail with ""Task attempt failed to report
status for 605 seconds. Killing!"". 
Same happens with 0.8 version too.

Regards,
Anitha",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Mar/12 18:24;daijy;PIG-2442-1.patch;https://issues.apache.org/jira/secure/attachment/12518917/PIG-2442-1.patch,21/Mar/12 01:48;jcoveney;PIG-2442_pig9.patch;https://issues.apache.org/jira/secure/attachment/12519165/PIG-2442_pig9.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-01-10 17:51:13.8,,,no_permission,,,,,,,,,,,,221648,Reviewed,,,Fri Mar 23 01:07:22 UTC 2012,,,,,,,0|i0h2of:,97696,,,,,,,,,,"10/Jan/12 17:51;xutingz;A few observations:
    (1)If the multiple stores store independent variables, then it works fine. For example, 
DEFINE SCRIPT `a.pl` SHIP ('/homes/a.pl');
A = LOAD 'test.txt' USING PigStorage() ;
B1 = STREAM A THROUGH SCRIPT;
B2 = STREAM A THROUGH SCRIPT;
STORE B1 INTO 'B2' USING PigStorage();
STORE B2 INTO 'B2' USING PigStorage();
    (2) The problem seems to be the ExecutableManager and the POstream which only read one tuple into the I/O thread in the second stream operation and then close.

I will look further into it. Anyone can give me some suggestion? ：）

Xuting

",19/Mar/12 18:24;daijy;The root cause is POStream inside POSplit does not work. ,"20/Mar/12 23:49;thejas;+1 . Can you make this change before commit ? - 
In POSplit.getStreamCloseResult
{code}
if (inpEOP && parentPlan.endOfAllInput) {
{code}

can be changed to 
{code}
if (inpEOP ) {
{code}
as the function is called only if parentPlan.endOfAllInput is true. 
","21/Mar/12 01:48;jcoveney;This is a rebase of this patch (with Thejas's comment incorporated) for Pig9. I think we should continue to try and backport as many bugfixes as possible to pig0.9, as it's the defacto ""stable"" branch.",23/Mar/12 01:07;daijy;Patch committed to 0.9/0.10/trunk. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jython import module not working if module path is in classpath,PIG-2433,12535551,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,daijy,daijy,17/Dec/11 02:16,14/Oct/13 16:46,14/Mar/19 03:07,09/Jan/13 01:26,0.10.0,,,,,,,0.12.0,,,,impl,,,0,,,,,,,,,,,,,"This is a hole of PIG-1824. If the path of python module is in classpath, job die with the message could not instantiate 'org.apache.pig.scripting.jython.JythonFunction'.

Here is my observation:
If the path of python module is in classpath, fileEntry we got in JythonScriptEngine:236 is __pyclasspath__/script$py.class instead of the script itself. Thus we cannot locate the script and skip the script in job.xml. 

For example:

{code}
register 'scriptB.py' using org.apache.pig.scripting.jython.JythonScriptEngine as pig

A = LOAD 'table_testPythonNestedImport' as (a0:long, a1:long);
B = foreach A generate pig.square(a0);

dump B;

scriptB.py:

#!/usr/bin/python
import scriptA
@outputSchema(""x:{t:(num:double)}"")
def sqrt(number):
 return (number ** .5)
@outputSchema(""x:{t:(num:long)}"")
def square(number):
 return long(scriptA.square(number))

scriptA.py:

#!/usr/bin/python
def square(number):
 return (number * number)
{code}

When we register scriptB.py, we use jython library to figure out the dependent modules scriptB relies on, in this case, scriptA. However, if current directory is in classpath, instead of scriptA.py, we get __pyclasspath__/scriptA.class. Then we try to put __pyclasspath__/script$py.class into job.jar, Pig complains __pyclasspath__/script$py.class does not exist. 

This is exactly TestScriptUDF.testPythonNestedImport is doing. In hadoop 20.x, the test still success because MiniCluster will take local classpath so it can still find scriptA.py even if it is not in job.jar. However, the script will fail in real cluster and MiniMRYarnCluster of hadoop 23.",,,,,,,,,,,,,,,,,,,PIG-1824,,,,,,,,,,,,,08/Jan/13 01:05;rohini;PIG-2433-1.patch;https://issues.apache.org/jira/secure/attachment/12563658/PIG-2433-1.patch,23/Oct/12 18:15;rohini;PIG-2433.patch;https://issues.apache.org/jira/secure/attachment/12550504/PIG-2433.patch,06/Jan/13 20:58;cheolsoo;TEST-org.apache.pig.test.TestScriptUDF.txt;https://issues.apache.org/jira/secure/attachment/12563502/TEST-org.apache.pig.test.TestScriptUDF.txt,07/Jan/13 19:37;cheolsoo;bad.log;https://issues.apache.org/jira/secure/attachment/12563609/bad.log,07/Jan/13 19:37;cheolsoo;good.log;https://issues.apache.org/jira/secure/attachment/12563610/good.log,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2012-10-23 18:15:31.12,,,no_permission,,,,,,,,,,,,221235,,,,Wed Jan 09 01:26:58 UTC 2013,,,,,,,0|i0azlz:,62047,,,,,,,,,,"23/Oct/12 18:15;rohini;Fixed the issue and added unit tests to import os and re. 

Note: If jython-standalone.jar is in pig classpath, found that in real cluster had to add -Dmapred.child.env=""JYTHONPATH=job.jar/Lib"" to pick up the builtin modules as the jar gets extracted on the datanode and Lib is not in classpath. Might apply to using with oozie too. Could not simulate the error in unit test environment even after removing jython jar from mr-apps-classpath. If the extracted Lib directory is in classpath instead of standalone jar while launching pig the env setting is not required. ",24/Oct/12 17:01;rohini;Seeing some errors with import unicodedata. Will update the patch after fixing that case too. ,"24/Oct/12 23:13;rohini;Changing status to Patch Available again. Issue was something that cannot be fixed in code and can be worked around. 

For anyone interested in the issue and the solution. 
The issue had to do with unicodedata.py loading UnicodeData.txt and EastAsianWidth.txt files inside its code. There is no way to determine them like imports and ship them with the jar. Also note that this happens when Lib directory is in classpath and not with standalone jython jar file. 

{code} 
loader = pkgutil.get_loader('unicodedata')
init_unicodedata(StringIO.StringIO(loader.get_data(os.path.join(my_path,'UnicodeData.txt'))))
init_east_asian_width(StringIO.StringIO(loader.get_data(os.path.join(my_path,'EastAsianWidth.txt'))))
{code}

The workaround for that is to ship those two files with hadoop's tmpfiles or mapred.cache.files option and set -Dmapred.child.env=""JYTHONPATH=.""

{noformat}
pig -Dmapred.child.env=""JYTHONPATH=.""
-Dtmpfiles=""file:///homes/rohinip/jython/UnicodeData.txt,file:///homes/rohinip/jython/EastAsianWidth.txt""
norm_test.pig
{noformat}

On a different note, found that progress is not reported in case of jython functions. Is this a known issue? Could not find any jiras.",02/Jan/13 23:19;rohini;bump. Review anyone?,"04/Jan/13 09:23;cheolsoo;Hi Rohini,

After applying the patch to trunk, I see the following error in TestScriptUDF.testPythonNestedImportClassPath:
{code}
Testcase: testPythonNestedImportClassPath took 0.182 sec
    Caused an ERROR
Python Error. Traceback (most recent call last):
  File ""/home/cheolsoo/workspace/pig-svn/scriptB.py"", line 2, in <module>
    import scriptA
  File ""__pyclasspath__/scriptA.py"", line 3, in <module>
NameError: name 'outputSchema' is not defined
{code}
Does this test pass for you? ",05/Jan/13 00:39;rohini;ant clean test -Dtestcase=TestScriptUDF passes for me. ,05/Jan/13 00:43;rohini;One cause for this error could be that your python cache dir is not writable and so the pig jar was not processed. Try running with -Dpython.cachedir=/<dir with write perms> if that is the case. Or are you running from eclipse?,"06/Jan/13 20:58;cheolsoo;Hi Rohini,

I tried what you suggested, but I still get the same error.
{code}
ant clean test -Dtestcase=TestScriptUDF -Dpython.cachedir=/home/cheolsoo
{code}

I see that the test fails on Mac, CentOS 6, and Ubuntu 12. It's not clear what's the root cause. I am attaching my test log.","07/Jan/13 02:15;rohini;   Suspecting that the following code execution is failing for you based on the stack trace. But the attached log does not have any error and the comment also says it will fail silently. 

{code}
// attempt addition of schema decorator handler, fail silently
                interpreter.exec(""def outputSchema(schema_def):\n""
                        + ""    def decorator(func):\n""
                        + ""        func.outputSchema = schema_def\n""
                        + ""        return func\n""
                        + ""    return decorator\n\n"");
{code}

Test ran fine for me in Mac and RHEL 5. I will see if I can try and reproduce. Can you add org.python.core.Options.verbose = Py.DEBUG; in the static block of JythonScriptEngine and see if that gives any other additional error messages for you? ","07/Jan/13 19:37;cheolsoo;Hi Rohini,

I found that the order in which test cases run matters. I am attaching two log files: good.log and bad.log. If I forced using OrderedJUnit4Runner that testPythonNestedImportClassPath runs before 
testPythonBuiltinModuleImport1, they all pass. But if testPythonBuiltinModuleImport1 runs before testPythonNestedImportClassPath, testPythonNestedImportClassPath fails:
{code:title=good.log}
Testcase: testPythonNestedImportClassPath took 38.565 sec
Testcase: testPythonBuiltinModuleImport1 took 35.904 sec
{code}
{code:title=good.log}
Testcase: testPythonBuiltinModuleImport1 took 38.756 sec
Testcase: testPythonNestedImportClassPath took 0.124 sec
    Caused an ERROR
Python Error. Traceback (most recent call last):
  File ""/Users/cheolsoo/workspace/pig/scriptB.py"", line 2, in <module>
    import scriptA
   File ""__pyclasspath__/scriptA.py"", line 3, in <module>
NameError: name 'outputSchema' is not defined
{code}","07/Jan/13 19:39;cheolsoo;I also turned on DEBUG as per your request, so you can see extra debug messages in the log files.",07/Jan/13 21:06;rohini;Thanks Cheolsoo. I think this has something to do with PythonInterpreter being static in JythonScriptEngine. And you must be running with jdk7 so the test order was different. I was running with jdk6 and that's why did not see it. Will investigate and fix it. ,08/Jan/13 01:05;rohini;Used different names for different modules. Tests pass when run with jdk7 now.,"08/Jan/13 17:51;cheolsoo;+1.

Thanks for the fix. The test passes for me too. I also ran e2e test and found no failure.

Minor comment:
When you commit the patch, can you remove a tab char in the following line?
{code}
+    	<!-- Remove jython jar from mrapp-generated-classpath -->
{code}",09/Jan/13 01:26;rohini;Thanks for the review Cheolsoo. Removed the tab before committing. Committed to trunk.  ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Eclipse .classpath file is out of date,PIG-2432,12535538,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,alangates,alangates,alangates,17/Dec/11 00:34,22/Feb/13 04:53,14/Mar/19 03:07,03/Jan/12 21:05,0.11,,,,,,,0.11,,,,tools,,,0,,,,,,,,,,,,,log4j and slf4j-log4j jars have changed versions.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Dec/11 00:34;alangates;PIG-2432.patch;https://issues.apache.org/jira/secure/attachment/12507746/PIG-2432.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-20 02:05:08.457,,,no_permission,,,,,,,,,,,,221222,,,,Tue Jan 03 21:05:28 UTC 2012,,,,,,,0|i09z87:,56148,,,,,,,,,,20/Dec/11 02:05;daijy;+1,03/Jan/12 21:05;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
An EvalFunc which overrides getArgToFuncMapping with FuncSpec with constructor arguments is not properly instantiated with said arguments,PIG-2430,12535273,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,jcoveney,jcoveney,15/Dec/11 08:43,26/Apr/12 20:32,14/Mar/19 03:07,06/Jan/12 22:08,0.10.0,0.11,0.9.0,,,,,0.10.0,0.11,,,,,,0,,,,,,,,,,,,,"If you override getArgToFuncMapping and any of the FuncSpec's specify constructor arguments, those arguments are currently ignored. Thankfully, there is a one line fix (it's funny that this has never been run into before, but is an). Patch with tests incoming. I assume that this affects 0.10 and 0.9 but haven't tested, I was just working with trunk.",,,,,,,,,,,,,,,,,,,PIG-2421,,,,,,,,,,,,,20/Dec/11 01:59;thejas;DummySize.patch;https://issues.apache.org/jira/secure/attachment/12508019/DummySize.patch,15/Dec/11 08:46;jcoveney;PIG2430.patch;https://issues.apache.org/jira/secure/attachment/12507489/PIG2430.patch,16/Dec/11 06:20;jcoveney;PIG2430_1.patch;https://issues.apache.org/jira/secure/attachment/12507658/PIG2430_1.patch,06/Jan/12 22:03;thejas;PIG2430_2.010.patch;https://issues.apache.org/jira/secure/attachment/12509722/PIG2430_2.010.patch,21/Dec/11 05:47;jcoveney;PIG2430_2.patch;https://issues.apache.org/jira/secure/attachment/12508201/PIG2430_2.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-12-16 03:52:40.927,,,no_permission,,,,,,,,,,,,220957,,,,Fri Jan 06 22:08:34 UTC 2012,,,,,,,0|i0h2mf:,97687,,,,,,,,,,15/Dec/11 08:47;jcoveney;I added a new test. Ran ant test-commit successful. Should test on 0.10 and 0.9,15/Dec/11 23:51;jcoveney;I patched .9 and .10 and ran ant test-commit successfully too :),"16/Dec/11 03:52;dvryaboy;Nice fix, but make the test use local mode. Also might want to move the UDF classes into the test itself, it's not like someone is likely to reuse it from the utils package.","16/Dec/11 06:19;jcoveney;This takes into account Dmitriy's suggestions. I successfully ran ant test-commit on trunk, and ant -Dtestcase=TestUDF test on 0.9 and 0.10.",16/Dec/11 07:12;jcoveney;I ran ant test-commit successfully on 0.9 and 0.10,"20/Dec/11 01:59;thejas;{code}
grunt> define SZ SIZE('10');        
grunt> l = load 't.txt' as (a,b);   
grunt> f = foreach l generate SZ(a);
grunt> dump f;

{code}

I have attached a patch with a change to size udf to return a dummy size value passed in the constructor. (the change is just meant to demonstrate the issue)

With the change proposed in PIG2430_1.patch, the constructor argument does not get set. Without the PIG2430_1.patch, the SZ(a) call will return 10, with it returns 0.","20/Dec/11 05:09;jcoveney;Thejas, thanks again for taking a look. I see what you mean now. I think there are a couple of resolutions to this, the question comes down to: in the case of an explicit define statement, but where there is also a matching getArgToFuncMapping, which should take precedence? For an explicit parameter (ie you dummy sIZE example), it seems pretty clear that the user defined parameter should take precedence. With no parameter it is less clear...there are 2 reasons why people would use a define statement.
1) to explicitly define a constructor parameter (or intentionally 0 constructor parameters)
2) to be able to refer to a UDF by an alias (in which case, the argument by the getArgToFuncMapping may be appropriate)

This ambiguity could be why this argument was ignored in the first place, but given it's there, I think it could be useful in UDFs, we just have to be clear on the semantics.","20/Dec/11 16:27;dvryaboy;Is there a way for us to tell if a UDF was DEFINEd or just used directly? That way we could tell if no-arguments was forced through a define, or if we should obey the args passed via argToFuncMapping.","20/Dec/11 19:24;thejas;bq. Is there a way for us to tell if a UDF was DEFINEd or just used directly? 
The information is not currently available in TypeCheckingExpVisitor, but it should be possible to pass that along. LogicalPlanBuilder.buildUDF is where the replacement with defined udf happens.

bq. This ambiguity could be why this argument was ignored in the first place, but given it's there, I think it could be useful in UDFs, we just have to be clear on the semantics.
I think we can define the semantics as the getArgsToFuncMapping() returns function with default constructor arguments, which can be overridden by define statement. I think it makes sense to add this info to the javadoc comments for getArgsToFuncMapping.

We need to think about how this constructor argument semantics should be defined with the new EvalFunc interface in PIG-2421.
","21/Dec/11 05:46;jcoveney;I agree that this experience should definitely inform the new EvalFunc design. I added a new comment

As far as this patch, I added your case to TestUDF, and took it into account. There is a part of LogicalPlanBuilder.java's buildUDF that is only called if there was no define statement for the alias in question. Thus, I added a field to UserFuncExpression which holds true if it was created via a define, and false otherwise.

Thus, if you were to do...

{code}
define my myudf('val');
a = load ...
b = foreach a generate my($0), myudf($0);
{code}

In this case, the my would have any FuncSpec values overriden by the define statement, but myudf would not, since it was not via an alias.

Let me know if this seems like an appropriate way to do this!","05/Jan/12 18:40;jcoveney;Thejas,

Was wondering if you might be able to get this a look to see if it seems reasonable.","05/Jan/12 19:34;thejas;Sorry, I had forgot about this jira. Thanks for reminding!
This looks good. I will commit after running unit tests and test-patch.
","05/Jan/12 19:56;thejas;Does anybody feel strongly to have this in 0.9 branch? While I don't see the change as being risky, I think it is better to minimize the patches to 0.9 in the interest of keeping it stable. 
",05/Jan/12 20:07;dvryaboy;I'm ok with keeping this out of 0.9 branch.,"05/Jan/12 22:12;jcoveney;I'm fine keeping this out of 0.9. The genesis of this patch was that it enables other things I'd like to do in trunk. 0.9 should probably really only have bug fixes and minor improvements at this point, keeping it stable, and we should make 0.10 the next bastion of the high tech pigs.","06/Jan/12 22:03;thejas;PIG2430_2.010.patch - patch re-based with 0.10 branch .
","06/Jan/12 22:08;thejas;test-patch and unit tests pass. Committed the patches to 0.10 branch and trunk.
Thanks Jonathan!
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In pig9, can't have limit(order by) without getting a null error",PIG-2428,12535042,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,jcoveney,jcoveney,13/Dec/11 21:58,23/Jan/12 07:31,14/Mar/19 03:07,14/Jan/12 00:07,0.9.0,0.9.2,,,,,,0.10.0,0.11,0.9.2,,,,,0,,,,,,,,,,,,,"{code}
a = load 'thing' as (x:int);
b = group a by x;
c = foreach b generate group as x, COUNT(a) as count;
d = limit (order c by count DESC) 2000;
describe d;
{code}

This gives the following error:

2011-12-13 13:56:32,144 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1005: No plan for d to describe

In trunk, it ran without issue. Not sure what the difference is, but it'd be nice to patch 0.9.2 since a lot of people (including amazon!) are using pig 9 now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Jan/12 08:45;daijy;PIG-2428-0.patch;https://issues.apache.org/jira/secure/attachment/12509034/PIG-2428-0.patch,12/Jan/12 22:36;jcoveney;PIG-2428-1.patch;https://issues.apache.org/jira/secure/attachment/12510433/PIG-2428-1.patch,13/Jan/12 08:59;daijy;PIG-2428-2.patch;https://issues.apache.org/jira/secure/attachment/12510475/PIG-2428-2.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-01-02 08:45:46.295,,,no_permission,,,,,,,,,,,,220726,Reviewed,,,Sat Jan 14 00:07:35 UTC 2012,,,,,,,0|i0h2lz:,97685,,,,,,,,,,02/Jan/12 08:45;daijy;Seems some part of PIG-1926 does the trick. Attach that part.,"12/Jan/12 22:14;daijy;Hi, Jonathan, 
Do you want me to add a test case? Or you want to work on that?","12/Jan/12 22:36;jcoveney;Daniel, I actually just added an (admittedly lightweight) test.

I added it to TestParser since this error was coming up there, but it also tests that the sort/limit itself works. I wasn't sure where to put the test...originally I wanted it to be in TestOrderBy, but alas, those aren't included tests. I kept the test fairly contained so it could be moved easily.

If you think there should be more, feel free to do so!",12/Jan/12 22:45;daijy;The test case looks fine with me. I can commit it upon test passing.,"12/Jan/12 23:11;dvryaboy;just noticed Daniel attached the fix, and Jonathan only added the test... so assign as appropriate :)

The test should be added to 9.2, 0.10 and trunk, not just 9.2",13/Jan/12 08:59;daijy;PIG-2428-2.patch fix unit test failure.,"14/Jan/12 00:07;daijy;Unit tests pass.

test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

I checked javadoc warning and it does not seems to be related. 

Patch committed to 0.9 branch (0.10/trunk already has this fix).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getSchemaFromString throws away the name of the tuple that is in a bag,PIG-2427,12534995,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,jcoveney,jcoveney,jcoveney,13/Dec/11 18:34,23/Jan/12 07:31,14/Mar/19 03:07,16/Dec/11 03:38,0.10.0,0.11,0.9.0,,,,,0.10.0,0.11,0.9.2,,,,,0,,,,,,,,,,,,,"{code}Schema thing = Utils.getSchemaFromString(""b:bag{t:tuple(x:int,y:int,z:int)}"");
System.out.println(thing.getField(0).schema.getField(0).alias); //returns null{code}

This isn't a huge issue, but it does seem odd to throw away this information unnecessarily. While any bag will of course have thus tuple and no other elements, it seems to violate the principle of least astonishment: if the tuple name was specified, shouldn't it be kept around?

Edit: I just found a case where this actually can be a bit annoying. If you want to pull that tuple out of the bag, for whatever reason, you've now lost its name.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Dec/11 01:37;daijy;PIG-2427-2.patch;https://issues.apache.org/jira/secure/attachment/12507857/PIG-2427-2.patch,14/Dec/11 06:17;jcoveney;PIG2427.patch;https://issues.apache.org/jira/secure/attachment/12507308/PIG2427.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-12-16 02:29:06.7,,,no_permission,,,,,,,,,,,,220679,,,,Mon Dec 19 04:56:31 UTC 2011,,,,,,,0|i0h2lr:,97684,,,,,,,,,,"14/Dec/11 06:20;jcoveney;I ran ant test-commit and ant -Dtestcase=parser/Test* test successfully. Not sure how else to test this sort of change. It didn't seem like I needed to change AstPrinter, but let me know if I do.

I added a test case to TestSchema that makes sure that Util.getSchemaFromString can will create an identical schema via its Schema.toString. I also had to change one existing test case, because it was assuming (incorrectly) that the tuple data should be thrown out (of course, that depends on whether you think it should be, but I strongly believe it shouldn't be thrown out if it is given).

Haven't tested on anything but trunk",16/Dec/11 01:27;jcoveney;I ran ant test-commit successfully on 0.10 and 0.9 as well,"16/Dec/11 02:29;dvryaboy;Looks harmless enough.
I'll commit to trunk and 10. Is this causing errors that require us to backport to 9?","16/Dec/11 03:07;jcoveney;It hasn't caused anything imminent. It mainly messed with another feature I wanted to add. Maybe slate it for 0.9.2? I see no reason why not, and I feel like until we are a couple of releases deep, it's worth keeping 0.9 up to date.",16/Dec/11 03:25;ashutoshc;Will this help PIG-2268 too ?,"16/Dec/11 03:38;dvryaboy;Committed to 0.9.2, 0.10, 0.11","16/Dec/11 03:45;ashutoshc;Awesome. Thanks, Jonathan for fixing this.",19/Dec/11 01:37;daijy;Some unit tests are broken. PIG-2427-2.patch is the fix.,19/Dec/11 02:05;dvryaboy;+1 thanks Daniel,"19/Dec/11 04:56;jcoveney;Awesome, thanks Daniel",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ProgressableReporter.progress(String msg) is an empty function,PIG-2426,12534911,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,vivekp,vivekp,vivekp,13/Dec/11 07:08,26/Apr/12 20:33,14/Mar/19 03:07,03/Jan/12 05:38,0.8.1,0.9.1,,,,,,0.10.0,0.11,,,,,,0,,,,,,,,,,,,,"In current implementation the reporter function ProgressableReporter.progress(String msg)  is an empty function.
If I have a long running UDF and I want update the status using a message, the preferred way is to use this api.  

The previous implementation of ProgressableReporter used org.apache.hadoop.mapred.Reporter api directly.
But the currently used org.apache.hadoop.util.Progressable interface  does not have api to set status as a given message. 
Hence I believe the empty method.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Jan/12 20:32;daijy;PIG-2426-3.patch;https://issues.apache.org/jira/secure/attachment/12511314/PIG-2426-3.patch,13/Dec/11 07:23;vivekp;PIG-2426_1.patch;https://issues.apache.org/jira/secure/attachment/12507141/PIG-2426_1.patch,19/Dec/11 15:14;vivekp;PIG-2426_2.patch;https://issues.apache.org/jira/secure/attachment/12507912/PIG-2426_2.patch,17/Dec/11 01:53;alangates;TEST-org.apache.pig.test.TestAccumulator.txt;https://issues.apache.org/jira/secure/attachment/12507758/TEST-org.apache.pig.test.TestAccumulator.txt,19/Dec/11 15:24;vivekp;partial_run.txt;https://issues.apache.org/jira/secure/attachment/12507913/partial_run.txt,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-12-16 01:17:04.193,,,no_permission,,,,,,,,,,,,220595,Reviewed,,,Fri Jan 20 20:32:44 UTC 2012,,,,,,,0|i0h2lj:,97683,,,,,,,,,,"13/Dec/11 07:23;vivekp;I was able to set the status message after replacing Progressable interface with TaskAttemptContext. 
Not sure whether using TaskAttemptContext will cause any other dependency,test-commit is passing.",16/Dec/11 01:17;alangates;The code looks fine.  I'm running the tests.  You need to add a test that calls the progress(String) method.,"17/Dec/11 01:50;alangates;When I run the tests, I get failures in TestAccumulator.  I'll attach the log file.",17/Dec/11 01:53;alangates;The tail of the test log.,"19/Dec/11 15:24;vivekp;Added a test cases for the patch. 
I am not getting the above mentioned error while running test-core.Attached a log of partial run for test-core.

From the actual test case failure , 
""Map output lost, rescheduling: getMapOutput(attempt_20111217011841808_0004_m_000000_0,0) failed "",
could this be any issue with node.",02/Jan/12 19:11;daijy;Looks good. Will commit once tests pass.,"03/Jan/12 05:38;daijy;Unit test pass. test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 509 release audit warnings (more than the trunk's current 501 warnings).

All new files have proper header, ignore release audit warning.

Patch committed to 0.10/trunk.

Thanks Vivek!",20/Jan/12 20:32;daijy;TestPigProgressReporting hit a NPE in hadoop 23. Attach PIG-2426-3.patch to fix it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Aggregate Warning does not work as expected on Embedding Pig in Java 0.9.1,PIG-2425,12534893,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,prkommireddi,prkommireddi,prkommireddi,13/Dec/11 01:35,26/Apr/12 20:33,14/Mar/19 03:07,16/Dec/11 02:45,0.9.1,,,,,,,0.10.0,,,,,,,0,patch,,,,,,,,,,,,"Property ""aggregate.warning"" is not being set by default when running PigServer, embedding Pig in Java.

I was initially creating a PigServer object this way:
{code}
  PigServer pigServer = new PigServer(ExecType.MAPREDUCE);
{code}

But this generated detailed logs in the log directory. To code around this on the client-side you could do

{code}
Properties properties = PropertiesUtil.loadDefaultProperties();
properties.setProperty(""aggregate.warning"", ""true"");
PigServer pigServer = new PigServer(ExecType.MAPREDUCE, properties);
{code}

The behavior between Pig scripting and Embedded Pig should be similar. Looking at
the main constructor of PigServer, it looks like ""aggregateWarning"" is set
to false if its not loaded in to Properties object.

{code}
public PigServer(PigContext context, boolean connect) throws ExecException {
         this.pigContext = context;
         currDAG = new Graph(false);

         aggregateWarning =
""true"".equalsIgnoreCase(pigContext.getProperties().getProperty(""aggregate.warning""));
         isMultiQuery =
""true"".equalsIgnoreCase(pigContext.getProperties().getProperty(""opt.multiquery"",""true""));

         jobName = pigContext.getProperties().getProperty(
                 PigContext.JOB_NAME,
                 PigContext.JOB_NAME_PREFIX + "":DefaultJobName"");

         if (connect) {
             pigContext.connect();
         }

         addJarsFromProperties();
     }
{code}

I suggest adding ""aggregate.warning"" to Properties object of PigContext so its picked up across all users of this property (MapReduceLauncher)
{code}
public PigServer(PigContext context, boolean connect) throws ExecException {
        this.pigContext = context;
        currDAG = new Graph(false);

        aggregateWarning = ""true"".equalsIgnoreCase(pigContext.getProperties().getProperty(""aggregate.warning"", ""true""));
        if(aggregateWarning) {
        	pigContext.getProperties().setProperty(""aggregate.warning"", ""true"");
        }
        	
        isMultiQuery = ""true"".equalsIgnoreCase(pigContext.getProperties().getProperty(""opt.multiquery"",""true""));
        
        jobName = pigContext.getProperties().getProperty(
                PigContext.JOB_NAME,
                PigContext.JOB_NAME_PREFIX + "":DefaultJobName"");

        if (connect) {
            pigContext.connect();
        }

        addJarsFromProperties();
    }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Dec/11 01:36;prkommireddi;aggregateWarning.patch;https://issues.apache.org/jira/secure/attachment/12507112/aggregateWarning.patch,14/Dec/11 21:02;prkommireddi;aggregateWarning2425.txt;https://issues.apache.org/jira/secure/attachment/12507433/aggregateWarning2425.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-12-14 02:00:58.886,,,no_permission,,,,,,,,,,,,220577,,,,Fri Dec 16 02:45:03 UTC 2011,,,Patch Available,,,,0|i0h2lb:,97682,,,,,,,,,,"14/Dec/11 02:00;thejas;To ensure that the default properties are same when commandline or PigServer is used, it would be better to have same function set it from Main.run and PigServer constructor. 

The following section in Main can be moved to a function in PropertiesUtil and get called from PropertiesUtil.loadDefaultProperties. We also need a test case. Can you add one unit test on the lines of testPigProperties in  TestPigServer.java ?
{code}
  if (properties.getProperty(""aggregate.warning"") == null) {
            //by default warning aggregation is on
            properties.setProperty(""aggregate.warning"", """"+true);
        }

        if (properties.getProperty(""opt.multiquery"") == null) {
            //by default multiquery optimization is on
            properties.setProperty(""opt.multiquery"", """"+true);
        }

        if (properties.getProperty(""stop.on.failure"") == null) {
            //by default we keep going on error on the backend
            properties.setProperty(""stop.on.failure"", """"+false);
        }

{code}","14/Dec/11 05:16;prkommireddi;That makes sense. Thejas, about naming does ""setDefaultsIfUnset(Properties properties)"" sound ok? This would be a private method within PropertiesUtil and called by loadDefaultProperties(Properties properties). 

{code}
 /**
     * Sets properties to their default values if not set by Client
     * @param properties
     */
    private static void setDefaultsIfUnset(Properties properties) {
    	if (properties.getProperty(""aggregate.warning"") == null) {
            //by default warning aggregation is on
            properties.setProperty(""aggregate.warning"", """"+true);
        }

        if (properties.getProperty(""opt.multiquery"") == null) {
            //by default multiquery optimization is on
            properties.setProperty(""opt.multiquery"", """"+true);
        }

        if (properties.getProperty(""stop.on.failure"") == null) {
            //by default we keep going on error on the backend
            properties.setProperty(""stop.on.failure"", """"+false);
        }
    }
{code}","14/Dec/11 06:35;prkommireddi;I would be adding a test case to TestPigServer
{code}
@Test
	public void testDefaultPigProperties() throws Throwable {
		File propertyFile = new File(""pig.properties"");

		Properties properties = PropertiesUtil.loadDefaultProperties();
		Assert
		.assertTrue(properties.getProperty(
				""pig.exec.reducers.max"").equals(""999""));
		Assert.assertTrue(properties.getProperty(""aggregate.warning"").equals(""true""));
		Assert.assertTrue(properties.getProperty(""opt.multiquery"").equals(""true""));
		Assert.assertTrue(properties.getProperty(""stop.on.failure"").equals(""false""));
		
		PrintWriter out = new PrintWriter(new FileWriter(propertyFile));
		out.println(""aggregate.warning=false"");
		out.println(""opt.multiquery=false"");
		out.println(""stop.on.failure=true"");
		
		out.close();

		properties = PropertiesUtil.loadDefaultProperties();
		Assert.assertTrue(properties.getProperty(""aggregate.warning"")
				.equals(""false""));
		Assert.assertTrue(properties.getProperty(""opt.multiquery"")
				.equals(""false""));
		Assert.assertTrue(properties.getProperty(""stop.on.failure"")
				.equals(""true""));

		propertyFile.delete();
	}
{code}","14/Dec/11 21:00;prkommireddi;Slightly modified the test case to add testing with PigServer. 
{code}
@Test
	public void testDefaultPigProperties() throws Throwable {
    	//Test with PigServer
    	PigServer pigServer = new PigServer(ExecType.MAPREDUCE);
    	Properties properties = pigServer.getPigContext().getProperties();
    	
    	Assert
		.assertTrue(properties.getProperty(
				""pig.exec.reducers.max"").equals(""999""));
		Assert.assertTrue(properties.getProperty(""aggregate.warning"").equals(""true""));
		Assert.assertTrue(properties.getProperty(""opt.multiquery"").equals(""true""));
		Assert.assertTrue(properties.getProperty(""stop.on.failure"").equals(""false""));
    	
		//Test with properties file
		File propertyFile = new File(""pig.properties"");

		properties = PropertiesUtil.loadDefaultProperties();
		
		Assert
		.assertTrue(properties.getProperty(
				""pig.exec.reducers.max"").equals(""999""));
		Assert.assertTrue(properties.getProperty(""aggregate.warning"").equals(""true""));
		Assert.assertTrue(properties.getProperty(""opt.multiquery"").equals(""true""));
		Assert.assertTrue(properties.getProperty(""stop.on.failure"").equals(""false""));
		
		PrintWriter out = new PrintWriter(new FileWriter(propertyFile));
		out.println(""aggregate.warning=false"");
		out.println(""opt.multiquery=false"");
		out.println(""stop.on.failure=true"");
		
		out.close();

		properties = PropertiesUtil.loadDefaultProperties();
		Assert.assertTrue(properties.getProperty(""aggregate.warning"")
				.equals(""false""));
		Assert.assertTrue(properties.getProperty(""opt.multiquery"")
				.equals(""false""));
		Assert.assertTrue(properties.getProperty(""stop.on.failure"")
				.equals(""true""));

		propertyFile.delete();
	}
{code}","16/Dec/11 02:45;thejas;+1. Patch committed to 0.10 branch and trunk.
Prashant, Thanks for the contribution!
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
rpm release package does not take PIG_CLASSPATH,PIG-2418,12534554,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,09/Dec/11 22:11,23/Jan/12 07:31,14/Mar/19 03:07,06/Jan/12 22:39,0.10.0,0.11,0.9.2,,,,,0.10.0,0.11,0.9.2,,impl,,,0,,,,,,,,,,,,,"If I install Pig using rpm, pig script does not take PIG_CLASSPATH",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Dec/11 22:16;daijy;PIG-2418-1.patch;https://issues.apache.org/jira/secure/attachment/12506803/PIG-2418-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-06 02:08:32.074,,,no_permission,,,,,,,,,,,,220276,Reviewed,,,Fri Jan 06 22:39:54 UTC 2012,,,,,,,0|i09z7b:,56144,,,,,,,,,,06/Jan/12 02:08;thejas;+1,06/Jan/12 22:39;daijy;Patch committed to 0.9/0.10/trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"A fix for 0.23 local mode: put ""yarn-default.xml"" into the configuration",PIG-2415,12534387,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,08/Dec/11 19:21,23/Jan/12 07:31,14/Mar/19 03:07,09/Dec/11 00:54,0.10.0,0.11,0.9.2,,,,,0.10.0,0.11,0.9.2,,impl,,,0,hadoop023,,,,,,,,,,,,"Otherwise I get:

Caused by: java.io.IOException: Can't get JobTracker Kerberos principal for use as renewer
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:106)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:90)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:83)
        at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:137)
        at org.apache.pig.newplan.logical.rules.InputOutputFileValidator$InputOutputFileVisitor.visit(InputOutputFileValidator.java:80)
        ... 24 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Dec/11 19:25;daijy;PIG-2415-1.patch;https://issues.apache.org/jira/secure/attachment/12506646/PIG-2415-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-09 00:35:42.57,,,no_permission,,,,,,,,,,,,220109,Reviewed,,,Fri Dec 09 00:54:34 UTC 2011,,,,,,,0|i09z93:,56152,,,,,,,,,,09/Dec/11 00:35;thejas;+1,09/Dec/11 00:54;daijy;Patch committed to trunk/0.10/0.9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e2e test should support testing against two cluster,PIG-2413,12534289,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,08/Dec/11 02:53,23/Jan/12 07:31,14/Mar/19 03:07,11/Jan/12 02:28,0.10.0,0.11,0.9.2,,,,,0.10.0,0.11,0.9.2,,impl,,,0,,,,,,,,,,,,,Current e2e test is comparing different version of Pig using the same cluster. We would like to comparing against different cluster as well.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Dec/11 02:55;daijy;PIG-2413-1.patch;https://issues.apache.org/jira/secure/attachment/12506562/PIG-2413-1.patch,09/Jan/12 21:30;daijy;PIG-2413-2.patch;https://issues.apache.org/jira/secure/attachment/12509963/PIG-2413-2.patch,11/Jan/12 02:25;daijy;PIG-2413-3.patch;https://issues.apache.org/jira/secure/attachment/12510146/PIG-2413-3.patch,11/Jan/12 02:27;daijy;PIG-2413-3_0.9.patch;https://issues.apache.org/jira/secure/attachment/12510147/PIG-2413-3_0.9.patch,13/Jan/12 21:59;daijy;PIG-2413-4.patch;https://issues.apache.org/jira/secure/attachment/12510533/PIG-2413-4.patch,19/Jan/12 07:20;daijy;PIG-2413-5.patch;https://issues.apache.org/jira/secure/attachment/12511098/PIG-2413-5.patch,,,,,,6.0,,,,,,,,,,,,,,,,,,,2012-01-11 01:31:26.339,,,no_permission,,,,,,,,,,,,220011,Reviewed,,,Thu Jan 19 07:20:50 UTC 2012,,,,,,,0|i09z67:,56139,,,,,,,,,,"08/Dec/11 02:55;daijy;Add one optional ant parameter ""harness.old.cluster.conf""",08/Dec/11 02:59;daijy;Also need to pass hadoop binary. Please hold on.,"11/Jan/12 01:31;thejas;The changes to build.xml seem unrelated. 
In test/e2e/pig/build.xml,  the change to add "" <property environment=""env""/>"" seems to be unnecessary.
Other changes in patch looks good.
",11/Jan/12 02:25;daijy;Reattach patch to address Thejas' comment.,11/Jan/12 02:27;daijy;The same patch for 0.9 branch.,11/Jan/12 02:28;daijy;Patch committed to 0.9/0.10/trunk.,13/Jan/12 21:59;daijy;There is regular e2e test failure introduced by the patch. PIG-2413-4.patch is the fix.,13/Jan/12 23:56;daijy;PIG-2413-4.patch committed to 0.9/0.10/trunk.,19/Jan/12 07:20;daijy;Need another fix PIG-2413-5.patch. Will commit to trunk first.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroStorage UDF in PiggyBank fails to STORE a bag of single-field tuples as Avro arrays,PIG-2411,12534278,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,russell.jurney,russell.jurney,russell.jurney,07/Dec/11 23:46,22/Jun/12 03:36,14/Mar/19 03:07,17/Mar/12 01:19,0.9.0,0.9.1,0.9.2,,,,,0.10.0,,,,piggybank,,,0,patch,,,,,,,,,,,,"This patch is a fix for a bug for persisting bags of tuples via AvroStorage.

The script that alerted me to a bug is:

messages = LOAD '/tmp/messages.avro' USING AvroStorage();
user_groups = GROUP messages by user_id;
per_user = FOREACH user_groups { sorted = ORDER messages BY message_id DESC; GENERATE group AS user_id, sorted AS messages; }
DESCRIBE per_user
> per_user: {user_id: int,messages: {(message_id: int,topic: chararray,user_id: int)}}
STORE per_user INTO '/tmp/per_user.avro' USING AvroStorage();

The error is:

Pig Stack Trace
---------------
ERROR 1002: Unable to store alias per_user

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1002: Unable to store alias per_user
at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1596)
at org.apache.pig.PigServer.registerQuery(PigServer.java:584)
at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:942)
at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:386)
at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:188)
at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:164)
at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:67)
at org.apache.pig.Main.run(Main.java:487)
at org.apache.pig.Main.main(Main.java:108)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
Caused by: java.lang.NullPointerException
at org.apache.pig.piggybank.storage.avro.AvroStorageUtils.isTupleWrapper(AvroStorageUtils.java:327)
at org.apache.pig.piggybank.storage.avro.PigSchema2Avro.convert(PigSchema2Avro.java:82)
at org.apache.pig.piggybank.storage.avro.PigSchema2Avro.convert(PigSchema2Avro.java:105)
at org.apache.pig.piggybank.storage.avro.PigSchema2Avro.convertRecord(PigSchema2Avro.java:151)
at org.apache.pig.piggybank.storage.avro.PigSchema2Avro.convert(PigSchema2Avro.java:62)
at org.apache.pig.piggybank.storage.avro.AvroStorage.checkSchema(AvroStorage.java:502)
at org.apache.pig.newplan.logical.rules.InputOutputFileValidator$InputOutputFileVisitor.visit(InputOutputFileValidator.java:65)
at org.apache.pig.newplan.logical.relational.LOStore.accept(LOStore.java:77)
at org.apache.pig.newplan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:64)
at org.apache.pig.newplan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:66)
at org.apache.pig.newplan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:66)
at org.apache.pig.newplan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:66)
at org.apache.pig.newplan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:66)
at org.apache.pig.newplan.DepthFirstWalker.walk(DepthFirstWalker.java:53)
at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
at org.apache.pig.newplan.logical.rules.InputOutputFileValidator.validate(InputOutputFileValidator.java:45)
at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:292)
at org.apache.pig.PigServer.compilePp(PigServer.java:1360)
at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1297)
at org.apache.pig.PigServer.execute(PigServer.java:1286)
at org.apache.pig.PigServer.access$400(PigServer.java:125)
at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1591)
... 13 more",Mac OS X 10.6.8 in local mode,,21600,21600,,0%,21600,21600,,,,,,,,,,,,,,,,,,,,,,,,17/Mar/12 01:15;daijy;PIG-2411-1.patch;https://issues.apache.org/jira/secure/attachment/12518763/PIG-2411-1.patch,07/Dec/11 23:47;russell.jurney;avrobug.patch;https://issues.apache.org/jira/secure/attachment/12506541/avrobug.patch,17/Mar/12 01:15;daijy;messages.avro;https://issues.apache.org/jira/secure/attachment/12518764/messages.avro,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-01-12 22:12:27.477,,,no_permission,,,,,,,,,,,,220000,Reviewed,,,Sat Mar 17 01:19:47 UTC 2012,,,Patch Available,,,,0|i0h2j3:,97672,,,,,,,avro udf piggybank pig storage,,,07/Dec/11 23:47;russell.jurney;Patch attached.,07/Dec/11 23:48;russell.jurney;See also https://issues.apache.org/jira/browse/PIG-1748,"12/Jan/12 22:12;daijy;Hi, Russell,
Do you still want to get it into 0.9.2? If so, can you add a test to your patch? Or if you think existing tests already cover it, can you describe it?

Thanks",14/Jan/12 00:12;daijy;Unlink to 0.9.2 since it's too late to get in. Change fix version to 0.10,"29/Feb/12 01:48;russell.jurney;This now works in my branch of Pig on github, and includes unit tests, input data and expected data in Avro.  See: https://github.com/rjurney/pig/tree/branch-0.9

Committer: please get with me about sorting out this mess.","29/Feb/12 08:54;russell.jurney;Started over.  The fix is here: https://github.com/rjurney/pig/commit/47811554beeabed433a3cb7942418ab1d354cb3b
The test is here: https://github.com/rjurney/pig/commit/6308b04171af4de5d66aa359ee12227674f09e34","29/Feb/12 08:59;russell.jurney;https://github.com/apache/pig/pull/2
",17/Mar/12 01:15;daijy;Attach the patch from Russell's github.,17/Mar/12 01:19;daijy;Patch committed to 0.10/trunk. Thanks Russell!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Piggybank does not compile in 23,PIG-2410,12534266,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,07/Dec/11 22:19,23/Jan/12 07:31,14/Mar/19 03:07,07/Jan/12 01:42,0.10.0,0.11,0.9.2,,,,,0.10.0,0.11,0.9.2,,piggybank,,,0,hadoop023,,,,,,,,,,,,"These does not compile:
AllLoader.java
HiveRCInputFormat.java
HadoopJobHistoryLoader.java
HiveColumnarLoader.java
PathPartitionHelper.java
IndexedStorage.java",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Dec/11 01:17;daijy;PIG-2410-0.patch;https://issues.apache.org/jira/secure/attachment/12508172/PIG-2410-0.patch,23/Dec/11 00:02;daijy;PIG-2410-1.patch;https://issues.apache.org/jira/secure/attachment/12508469/PIG-2410-1.patch,06/Jan/12 21:33;daijy;PIG-2410-2.patch;https://issues.apache.org/jira/secure/attachment/12509717/PIG-2410-2.patch,06/Jan/12 22:14;daijy;PIG-2410-2_0.9.patch;https://issues.apache.org/jira/secure/attachment/12509724/PIG-2410-2_0.9.patch,01/Jan/12 01:45;thw;PIG-2410_branch-0.9-1.patch;https://issues.apache.org/jira/secure/attachment/12509003/PIG-2410_branch-0.9-1.patch,21/Dec/11 23:38;thw;PIG-2410_branch-0.9.patch;https://issues.apache.org/jira/secure/attachment/12508319/PIG-2410_branch-0.9.patch,,,,,,6.0,,,,,,,,,,,,,,,,,,,2011-12-14 23:14:29.751,,,no_permission,,,,,,,,,,,,219988,Reviewed,,,Sat Jan 07 08:43:45 UTC 2012,,,,,,,0|i02txr:,14451,,,,,,,,,,"14/Dec/11 23:14;olgan;I beleive that HadoopJobHistoryLoader.java issue was traced to JobHistory interface changes. I also remember that there was a discussion of having 2 version of the function.

I believe that is confusing for users and we should make an effort to shim it the same way we did for Pif core code",21/Dec/11 01:17;daijy;First take to make piggybank compile except JobHistoryLoader,21/Dec/11 23:38;thw;Patch for 0.9 branch that includes relevant portion of trunk changes plus conditional exclude of JobHistoryLoader for 0.23,23/Dec/11 00:02;daijy;The equivalent patch for trunk.,01/Jan/12 01:45;thw;Updated patch for branch-0.9 with avro 1.5.3  (included PIG-2202),06/Jan/12 21:33;daijy;PIG-2410-2.patch fix all piggybank tests except for JobHistoryLoader.,06/Jan/12 22:14;daijy;The same patch for 0.9 branch.,"06/Jan/12 23:15;thw;Thanks Daniel - all tests pass for branch-0.9 / 0.23
",07/Jan/12 00:51;thejas;+1,07/Jan/12 01:42;daijy;Patch committed to 0.9/0.10/trunk.,"07/Jan/12 08:43;daijy;test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 16 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 509 release audit warnings (more than the trunk's current 502 warnings).

No new file added, ignore release audit warning.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig show wrong tracking URL for hadoop 2,PIG-2409,12534243,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,lbendig,daijy,daijy,07/Dec/11 20:20,21/Nov/14 05:59,14/Mar/19 03:07,06/Jun/14 21:51,0.10.0,0.11,0.9.2,,,,,0.14.0,,,,impl,,,0,hadoop023,,,,,,,,,,,,"Pig used to show a tracking url for hadoop job:
More information at: http://localhost:50030/jobdetails.jsp?jobid=job_201112071119_0001

This information does not show up in hadoop 23.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Jun/14 21:37;lbendig;PIG-2409-2.patch;https://issues.apache.org/jira/secure/attachment/12648731/PIG-2409-2.patch,06/Jun/14 20:43;lbendig;PIG-2409.patch;https://issues.apache.org/jira/secure/attachment/12648718/PIG-2409.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-28 21:47:16.089,,,no_permission,,,,,,,,,,,,219965,Reviewed,,,Fri Jun 06 21:59:43 UTC 2014,,,,,,,0|i0cvj3:,73052,,,,,,,,,,14/Mar/12 23:30;daijy;Hadoop client will show the URL. The only annoying part is Pig will first complain it cannot get tracking url. Lower the priority and unlink it from 0.9/0.10,"28/Oct/12 21:47;rohini;Daniel,
   Can we move this to 0.12 as the priority is low?

Just to note, the log message from hadoop core (example below) has the relevant information.

{noformat}
2012-10-05 20:45:12,954 [Thread-5] INFO 
org.apache.hadoop.mapred.ResourceMgrDelegate - Submitted application
application_1348097917603_11448 to ResourceManager at
nodex.yahoo.com/xx.xx.xx.xx:8032
2012-10-05 20:45:12,997 [Thread-5] INFO  org.apache.hadoop.mapreduce.Job - The
url to track the job:
http://nodex.yahoo.com:8088/proxy/application_1348097917603_11448/
{noformat}","25/Sep/13 18:44;daijy;Hadoop 2 shows the right tracking url now. However, Pig will print a redundant message which contains a wrong url. We need to remove it in Pig on Hadoop 2.",06/Jun/14 20:59;rohini;Version matching will require change when there is hadoop 3.x. Instead can you add the method isHadoopYARN() to HadoopShims.java instead and just return true or false?,"06/Jun/14 21:37;lbendig;[~rohini], thanks for the tip, that's definitely a better solution. I updated the patch and @[~daijy] : I also assigned to me.",06/Jun/14 21:51;rohini;+1. Committed to trunk. Thanks Lorand.,"06/Jun/14 21:59;lbendig;Rohini, thanks for committing it!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix counters for hadoop 23,PIG-2408,12534242,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,07/Dec/11 20:16,26/Apr/12 20:33,14/Mar/19 03:07,24/Feb/12 01:54,0.10.0,0.11,0.9.2,,,,,0.10.0,0.11,0.9.3,,impl,,,0,hadoop023,,,,,,,,,,,,"All the counters in hadoop 23 is incorrect:

Counters:
Total records written : 0
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0",,,,,,,,,,,,,,,,,,,PIG-2446,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,219964,,,,Fri Feb 24 01:54:02 UTC 2012,,,,,,,0|i09ywn:,56096,,,,,,,,,,"24/Feb/12 01:54;daijy;With the current hadoop 23, I get the right counter except for the map bytes read, which will be addressed in PIG-2446. Close this ticket.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
svn tags/release-0.9.1: some unit test case failed with open JDK,PIG-2405,12534181,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,fang fang chen,fang fang chen,fang fang chen,07/Dec/11 13:01,22/Feb/13 04:53,14/Mar/19 03:07,08/Nov/12 04:48,0.9.1,,,,,,,0.11,0.12.0,,,,,,0,,,,,,,,,,,,,"    [junit] Test org.apache.pig.test.TestDataModel FAILED
Testcase: testTupleToString took 0.004 sec
        FAILED
toString expected:<...ad a little lamb)},[[hello#world,goodbye#all]],42,5000000000,3.14...> but was:<...ad a little lamb)},[[goodbye#all,hello#world]],42,5000000000,3.14...>
junit.framework.ComparisonFailure: toString expected:<...ad a little lamb)},[[hello#world,goodbye#all]],42,5000000000,3.14...> but was:<...ad a little lamb)},[[goodbye#all,hello#world]],42,5000000000,3.14...>
         at org.apache.pig.test.TestDataModel.testTupleToString(TestDataModel.java:269

    [junit] Test org.apache.pig.test.TestHBaseStorage FAILED
Tests run: 18, Failures: 0, Errors: 12, Time elapsed: 188.612 sec

Testcase: testHeterogeneousScans took 0.018 sec
        Caused an ERROR
java.io.FileNotFoundException: /root/pigtest/conf/hadoop-site.xml (Too many open files)
java.lang.RuntimeException: java.io.FileNotFoundException: /root/pigtest/conf/hadoop-site.xml (Too many open files)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1162)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1035)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:980)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:436)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.<init>(HConnectionManager.java:271)
        at org.apache.hadoop.hbase.client.HConnectionManager.getConnection(HConnectionManager.java:155)
        at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:167)
        at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:130)
        at org.apache.pig.test.TestHBaseStorage.prepareTable(TestHBaseStorage.java:809)
        at org.apache.pig.test.TestHBaseStorage.testHeterogeneousScans(TestHBaseStorage.java:741)
Caused by: java.io.FileNotFoundException: /root/pigtest/conf/hadoop-site.xml (Too many open files)
        at java.io.FileInputStream.<init>(FileInputStream.java:112)
        at java.io.FileInputStream.<init>(FileInputStream.java:72)
        at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:70)
        at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:161)
        at org.apache.xerces.impl.XMLEntityManager.setupCurrentEntity(Unknown Source)
        at org.apache.xerces.impl.XMLVersionDetector.determineDocVersion(Unknown Source)
        at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
        at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
        at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)
        at org.apache.xerces.parsers.DOMParser.parse(Unknown Source)
        at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)
        at javax.xml.parsers.DocumentBuilder.parse(Unknown Source)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1079)

        Caused an ERROR
Could not resolve the DNS name of hostname:39611
java.lang.IllegalArgumentException: Could not resolve the DNS name of hostname:39611
        at org.apache.hadoop.hbase.HServerAddress.checkBindAddressCanBeResolved(HServerAddress.java:105)
        at org.apache.hadoop.hbase.HServerAddress.<init>(HServerAddress.java:66)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:755)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:590)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:555)
        at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:171)
        at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:145)
        at org.apache.pig.test.TestHBaseStorage.deleteAllRows(TestHBaseStorage.java:120)
        at org.apache.pig.test.TestHBaseStorage.tearDown(TestHBaseStorage.java:112)

    [junit] Test org.apache.pig.test.TestMRCompiler FAILED
Testcase: testSortUDF1 took 0.045 sec
        FAILED
null expected:<...---MapReduce(20,SUM,[COUNT,TestMRCompiler$WeirdComparator]) - -18:
        |  ...> but was:<...---MapReduce(20,SUM,[TestMRCompiler$WeirdComparator,COUNT]) - -18:
        |  ...>
junit.framework.ComparisonFailure: null expected:<...---MapReduce(20,SUM,[COUNT,TestMRCompiler$WeirdComparator]) - -18:
        |  ...> but was:<...---MapReduce(20,SUM,[TestMRCompiler$WeirdComparator,COUNT]) - -18:
        |  ...>
        at org.apache.pig.test.TestMRCompiler.run(TestMRCompiler.java:1080)
        at org.apache.pig.test.TestMRCompiler.testSortUDF1(TestMRCompiler.java:791

    [junit] Test org.apache.pig.test.TestNewPlanLogToPhyTranslationVisitor FAILED
(1)
Testcase: testSimplePlan took 0.675 sec
        FAILED
expected:<class org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject> but was:<class org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.ConstantExpression>
junit.framework.AssertionFailedError: expected:<class org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject> but was:<class org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.ConstantExpression>
        at org.apache.pig.test.TestNewPlanLogToPhyTranslationVisitor.testSimplePlan(TestNewPlanLogToPhyTranslationVisitor.java:127)
(2)
Testcase: testJoinPlan took 0.064 sec
        FAILED
expected:<0> but was:<1>
junit.framework.AssertionFailedError: expected:<0> but was:<1>
        at org.apache.pig.test.TestNewPlanLogToPhyTranslationVisitor.testJoinPlan(TestNewPlanLogToPhyTranslationVisitor.java:158)
(3)
Testcase: testMultiStore took 0.041 sec
        FAILED
expected:<0> but was:<1>
junit.framework.AssertionFailedError: expected:<0> but was:<1>
        at org.apache.pig.test.TestNewPlanLogToPhyTranslationVisitor.testMultiStore(TestNewPlanLogToPhyTranslationVisitor.java:239)

    [junit] Test org.apache.pig.test.TestPruneColumn FAILED
(1)
Testcase: testMapKey2 took 7.1 sec
        FAILED
null
junit.framework.AssertionFailedError: null
        at org.apache.pig.test.TestPruneColumn.testMapKey2(TestPruneColumn.java:1206)
(2)
Testcase: testMapKey3 took 7.088 sec
        FAILED
null
junit.framework.AssertionFailedError: null
        at org.apache.pig.test.TestPruneColumn.testMapKey3(TestPruneColumn.java:1222)
(3)
Testcase: testMapKeyInSplit1 took 7.1 sec
        FAILED
null
junit.framework.AssertionFailedError: null
        at org.apache.pig.test.TestPruneColumn.testMapKeyInSplit1(TestPruneColumn.java:1296)
(4)
Testcase: testSharedSchemaObject took 7.1 sec
        FAILED
null
junit.framework.AssertionFailedError: null
        at org.apache.pig.test.TestPruneColumn.testSharedSchemaObject(TestPruneColumn.java:1619)
","ant-1.8.2
open jdk: 1.6
",,,,,,,,,,,,,,,,,,PIG-2908,,,,,,,,,,,,,08/Nov/12 02:47;fang fang chen;PIG-2405-trunk.patch;https://issues.apache.org/jira/secure/attachment/12552602/PIG-2405-trunk.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-23 08:21:46.548,,,no_permission,,,,,,,,,,,,219903,,,,Thu Nov 08 04:48:16 UTC 2012,,,,,,,0|i0agaf:,58912,,,,,,,,,,"23/May/12 02:24;fang fang chen;Before testing pig UT, we should increase ""max user processes"" and ""open files"" variables by ""ulimit"". This is workaround for ""Too many files"" error.
",23/May/12 02:26;fang fang chen;The left failures are caused by HashMap. The outputs in open source JDK are also correct.,23/May/12 08:21;daijy;I am sure this is the cause for some unit test failures for OpenJDK and Sun JDK 1.7. Would you like to submit a patch?,23/May/12 08:46;fang fang chen;Will try my best to fix this when available.,"13/Sep/12 08:47;fang fang chen;Current all UT passed.
2405_1.patch is for TestDataModel, TestNewPlanLogToPhyTranslationVisitor, TestMRCompiler  
2405_2.patch is for TestPruneColumn
",13/Sep/12 08:50;fang fang chen;Patch files are based on pig-trunk.,"18/Oct/12 20:59;lrangel;in TestDataModel, the underlying problem is using an order-independent structure in an order-dependent test. What about keeping it simple and removing the HashMap from the Tuple, instead of replacing it with a LinkedHashMap?",05/Nov/12 02:50;fang fang chen;This patch is for branch 0.11.,"05/Nov/12 06:33;cheolsoo;Hi Fangfang,

I ran the test cases that you mentioned (TestDataModel, TestNewPlanLogToPhyTranslationVisitor, TestMRCompiler, and TestPruneColumn) with trunk using OpenJDK 1.6. But I cannot reproduce the failures. Can you verify that you still see these failures with the current trunk, or is there anything that I have to do to reproduce them?

I tested with
- [revision 1405587|http://svn.apache.org/viewvc?view=revision&revision=1405587].
- java version ""1.6.0_24""
OpenJDK Runtime Environment (IcedTea6 1.11.5) (6b24-1.11.5-0ubuntu1~12.04.1)
OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)

Thanks!","05/Nov/12 07:41;fang fang chen;Hi Cheolsoo Park,

With current trunk/0.11 branch, I still encounter output order problem to TestDataModel, TestMRCompiler, and TestPruneColumn with OpenSource JDK, and did not with TestNewPlanLogToPhyTranslationVisitor. Seems this UT has been modified a lot.

Thanks


","05/Nov/12 18:53;cheolsoo;Hi Fangfang,

Thank for the clarification. Your changes seem reasonable, but I have a few comments as below.
- Please remove tabs.
- Please use Unix newline chars (i.e. no CRs).
- Regarding TestPruneColumn, can't we use reg. exp. instead of sorting log messages? If I am not mistaken, the sortLogMessages() method is added to sort entries inside the ""[ ]"". But the same can be done by reg. exp. For example, the following pattern will match all [key1,key2], [key2,key1], [key1, key2], and [key2, key1]. I think that this is simpler. Do you agree?
{code}
"".*\\[((key2,[ ]?key1)|(key1,[ ]?key2))\\]""
{code}
- Regarding TestMRCompiler, I would prefer to fix the test rather than changing the code like Rohini commented in PIG-2908. But I don't have a better suggestion, so I won't disagree with your solution.
{quote} Fang has solved it by changing it to a LinkedHashMap in MapReduceOper.java and OperatorPlan.java. I have just modified tests to sort the entrySet results and then assert. If LinkedHashMap approach is preferred, I can remove TestNewPlanLogToPhyTranslationVisitor from this patch. But in general, would prefer not changing code for tests to work.{quote}

Can you please update the patch and upload it to the RB?

Thanks!","05/Nov/12 19:41;rohini;I think we should avoid changing to LinkedHashMap. Recently, for a different project for test result comparison where the order was required we moved from LinkedHashMap to TreeMap because the order of insertion into LinkedHashMap was different in jdk7 and LinkedHashMap did not work because of that to compare golden files. That might not be the case here because we insert iterating over an array. But would still prefer sorting the test results when comparing instead of changing code to use LinkedHashMap or TreeMap. ","05/Nov/12 19:50;cheolsoo;Thanks Rohini for your comment. Sure, agreed. But I don't really have a better suggestion on how to fix TestMRCompiler otherwise. I am open to better suggestions.",06/Nov/12 06:42;fang fang chen;For 0.11 branch.,06/Nov/12 06:42;fang fang chen;For trunk branch.,"06/Nov/12 06:45;fang fang chen;Hi Cheolsoo,

Thanks for your review.

- You are right, the sortLogMessages() method is sorting the output with some spacial order.
- Actually, I though about to fix by using reg. exp. as which is much more simpler. The reason of not use it is we need to do the same steps to all related test cases(current 4 tests). In future, if we need to add new tests, the same action would also be needed. 
- Besides, the reg. exp. will be too complex when there are too many items in [], like:
""Map key required for raw: $0->[cm_serve_id, cm_serve_timestamp_ms, p_url, source, type]""

So I insist the original fix. Do you agree? Please let me know if I miss anything. Thanks.
I regenerated the patch files by following ""how to contribute"" steps, and attach them at ""Attachments"". Could you please tell what is ""RB""? Please help load them to RB and let me know if modifications are needed. Thanks
","06/Nov/12 06:49;fang fang chen;BTW, following is the test-patch result, and the failure is not caused by the patches. There are  [javadoc] 38 warnings in original trunk branch. I will open a JIRA for this.
    [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 12 new or modified tests.
     [exec]
     [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
 ","06/Nov/12 07:02;cheolsoo;- RB is review board: https://reviews.apache.org
- I still see tabs in your patch. Please use 4 spaces instead.
- OK, let's sort log messages then.

But can you please comment on Rohini's comment? She doesn't seem to agree with replacing HashMap with LinkedHashMap in {{MapReduceOper.java}} and {{OperatorPlan.java}}. I understand that you can't sort the result to fix TestMRCompiler. But I also agree with Rohini that we should avoid modifying code if possible. Unfortunately, I don't really have a better idea.","06/Nov/12 08:09;fang fang chen;Agree with you and Rohini that we should avoid modifying core code.
Will generate new patches based via sorting outputs.","06/Nov/12 08:15;fang fang chen;Yes, you are right. We can not sort the output:-)
Maybe we can generate two kinds of golden files. Any concern?",06/Nov/12 08:51;fang fang chen;Update patches: remove tabs.,"06/Nov/12 11:19;fang fang chen;Hi Rohini and Cheolsoo,

TestMRCompiler is fixed by sorting UDFs in this class. Remove all the LinkedHash... change set now. This patch works for both 0.11 and trunk branch.
Current change set should be OK. Please help review. 

Thanks","06/Nov/12 12:43;fang fang chen;Loaded the patch to RB:
https://reviews.apache.org/r/7898/","07/Nov/12 21:03;rohini;+1 from me. Actually did not realize that TestMRCompiler case was more complicated until I saw Cheolsoo's comment that he does not have a better suggestion. After taking a deeper look, I couldn't think of one too without changing src code. This one is a pretty good solution. Thanks for fixing this Fang. I will let Cheolsoo take a final look and commit because he has been working with you so far.","07/Nov/12 21:43;cheolsoo;Hi Fangfang,

I was running tests with your new patch in the RB to commit it, but I found that TestPruneColumn.testStream2 fails. I believe that you omitted the following change in your new patch.
{code:title=TestPruneColumn.java}
-        assertTrue(checkLogFileMessage(new String[]{""Map key required for event_serve: $0->[key4, key3]"", 
-                ""Map key required for cm_data_raw: $0->[key4, key3, key5]""}));
+        assertTrue(checkLogFileMessage(new String[]{""Map key required for event_serve: $0->[key3, key4]"", 
+                ""Map key required for cm_data_raw: $0->[key3, key4, key5]""}));
{code}
Can you please upload a new patch to this JIRA? I will commit it as soon as you upload a new patch.

I also have a super minor comment. This is just a suggestion, so I won't insist. I found that you replaced {{assertEquals}} with {{assertTrue}}:
{code:title=TestPruneColumn.java}
-        assertEquals(""([2#1,1#1])"", t.toString());
+        assertTrue(TestHelper.sortString(""\\[(.*)\\]"", t.toString(), "","")
+                .equals(""([1#1, 2#1])""));
{code}
Can you please not change assertEquals to assertTrue? We made a good amount of effort to modernize test code in PIG-3006, and this was one of patterns that we fixed.

Thanks for your patience!","08/Nov/12 02:54;fang fang chen;Hi Cheolsoo,

Sorry for missing fix. Updated the patch based on your comments. 
Thanks for your comments.

Thanks","08/Nov/12 04:48;cheolsoo;Committed to trunk/0.11. Thank you for your contribution, Fagnfang!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inIllustrator condition in PigMapReduce is wrong for hadoop 23,PIG-2402,12534127,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,07/Dec/11 01:31,23/Jan/12 07:31,14/Mar/19 03:07,07/Dec/11 21:55,0.10.0,0.11,0.9.2,,,,,0.10.0,0.11,0.9.2,,impl,,,0,hadoop023,,,,,,,,,,,,"Current logic is:
context instanceof WrappedMapper.Context

It is no longer working in current 0.23 branch. I see test failure in some tests, such as SkewedJoin_6.

Here is the stack:
Error: java.lang.NullPointerException at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore.getNext(POStore.java:146) at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.runPipeline(POSplit.java:254) at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.processPlan(POSplit.java:236) at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.getNext(POSplit.java:228) at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:271) at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:266) at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64) at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:711) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:328) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:396) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1154) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142) 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Dec/11 01:33;daijy;PIG-2402-1.patch;https://issues.apache.org/jira/secure/attachment/12506370/PIG-2402-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-07 21:27:16.642,,,no_permission,,,,,,,,,,,,219849,Reviewed,,,Wed Dec 07 21:55:17 UTC 2011,,,,,,,0|i09z9b:,56153,,,,,,,,,,07/Dec/11 21:27;thejas;+1,"07/Dec/11 21:55;daijy;Unit tests pass. test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 465 release audit warnings (more than the trunk's current 458 warnings).

No tests added since it is a regression. No new file added so ignore release audit warning.

Patch committed to trunk/0.10/0.9",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bzip_2 test is broken,PIG-2391,12533518,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xutingz,olgan,olgan,01/Dec/11 21:53,26/Apr/12 20:32,14/Mar/19 03:07,08/Dec/11 22:05,0.10.0,,,,,,,0.10.0,0.11,,,,,,0,,,,,,,,,,,,,"This test is currently commented out but if you uncomment it it fails with Pig 10 but runs successfully with Pig 9.

Script:

a = load '/homes/olgan/studenttab10k' using PigStorage() as (name, age, gpa);
store a into 'intermediate.bz';
b = load 'intermediate.bz';
store b into 'final.bz';

A couple of observations:

(1) Identical script (represented by Bzip_1 test) that has bz2 instead of bz extension in the script succeeds in Pig 10
(2) The problem occurs while reading intermediate.bz which has different size with Pig 9 and Pig 10
(3) Problem can be reproduced in local mode with small subset of data in the file
(4) The following stack trace is observed:

2011-12-01 13:53:12,280 [Thread-22] WARN  org.apache.hadoop.mapred.LocalJobRunner - job_local_0002
java.lang.RuntimeException: java.io.IOException: compressedStream EOF
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.initNextRecordReader(PigRecordReader.java:237)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.<init>(PigRecordReader.java:109)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.createRecordReader(PigInputFormat.java:119)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:588)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
Caused by: java.io.IOException: compressedStream EOF
        at org.apache.tools.bzip2r.CBZip2InputStream.cadvise(CBZip2InputStream.java:92)
        at org.apache.tools.bzip2r.CBZip2InputStream.compressedStreamEOF(CBZip2InputStream.java:96)
        at org.apache.tools.bzip2r.CBZip2InputStream.bsR(CBZip2InputStream.java:451)
        at org.apache.tools.bzip2r.CBZip2InputStream.initBlock(CBZip2InputStream.java:348)
        at org.apache.tools.bzip2r.CBZip2InputStream.<init>(CBZip2InputStream.java:220)
        at org.apache.pig.bzip2r.Bzip2TextInputFormat$BZip2LineRecordReader.<init>(Bzip2TextInputFormat.java:105)
        at org.apache.pig.bzip2r.Bzip2TextInputFormat.createRecordReader(Bzip2TextInputFormat.java:244)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.initNextRecordReader(PigRecordReader.java:227)
        ... 5 more


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Dec/11 01:42;xutingz;PIG-2391-1.patch;https://issues.apache.org/jira/secure/attachment/12506371/PIG-2391-1.patch,07/Dec/11 19:37;xutingz;PIG-2391-2.patch;https://issues.apache.org/jira/secure/attachment/12506500/PIG-2391-2.patch,06/Dec/11 21:36;xutingz;PIG-2391.patch;https://issues.apache.org/jira/secure/attachment/12506310/PIG-2391.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-12-06 21:48:41.451,,,no_permission,,,,,,,,,,,,219246,Reviewed,,,Thu Dec 08 22:05:55 UTC 2011,,,,,,,0|i0h2fr:,97657,,,,,,,,,,"06/Dec/11 21:48;xutingz;This problem is caused by PIG-2143 where the setCompression function is added and codecFactory.getCodec(path) is called to determine whether there is a corresponding CompressionCodec class for the compression file. codecFactory.getCodec() will return null for .bz file while return BZip2Codec for .bz2 file. In 0.9, the suffix of the file path is used to determine this. 

In this patch, I modified the setCompression function to treat .bz file as .bz2 file and added new unit test for this case. ant test-commit has been run successfully on .10 branch and trunk.","06/Dec/11 22:04;olgan;Hi Xuting,

Could you explain what is causing this regression? It is not obvious to me what the fix is doing. Also, what would happen if another store function like BinStorage is used?

Thanks",06/Dec/11 22:07;olgan;Looks like our comments crossed. So the issue is that Hadoop does not understand .bz extension and you need to fake it by saying it is actually bz2.,"06/Dec/11 22:09;olgan;It would be good to have this as comment in your patch. It is a bit of a hack but I don't see a better way to do this. Please, update the patch with better comment and I will commit it once tests pass","07/Dec/11 01:43;xutingz;I added extra comments in the PIG-2391-1.patch to explain the modifications.
Xuting",07/Dec/11 01:50;daijy;Thanks Xuting. Do you have idea why this pass in 0.9?,07/Dec/11 01:54;olgan;I assume it is because we merged PigStorage and PigStorageSchema in 10,"07/Dec/11 06:23;xutingz;Hi Daniel, I think that is because the patch in PIG-2143 only applied on 0.10 and later branches.
In 0.9, instead of calling the codecFactory.getCodec function which can cause this problem, it directly checks the suffix of the path to determine what kind of compressionCodec class should be applied.

To be specific, In 0.9, the code is like the following:

      if (location.endsWith("".bz2"") || location.endsWith("".bz"")) {
                FileOutputFormat.setCompressOutput(job, true);
                FileOutputFormat.setOutputCompressorClass(job,  BZip2Codec.class);
            }  else if (location.endsWith("".gz"")) {
                FileOutputFormat.setCompressOutput(job, true);
                FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);
            } else {
                FileOutputFormat.setCompressOutput( job, false);
            }

In .10, the code is like:
       CompressionCodec codec = codecFactory.getCodec(location);
        if (codec != null) {
            FileOutputFormat.setCompressOutput(job, true);
            FileOutputFormat.setOutputCompressorClass(job, codec.getClass());
        }else {
            FileOutputFormat.setCompressOutput(job, false);  
        }","07/Dec/11 08:21;daijy;It seems in Pig 0.10, we mistakenly drop some code of 0.9. Will it work if we just bring back the 0.9 code?

Here is the code we removed in 0.10:
{code}
if (location.endsWith("".bz2"") || location.endsWith("".bz"")) {
    FileOutputFormat.setCompressOutput(job, true);
    FileOutputFormat.setOutputCompressorClass(job,  BZip2Codec.class);
}  else if (location.endsWith("".gz"")) {
    FileOutputFormat.setCompressOutput(job, true);
    FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);
} else {
    FileOutputFormat.setCompressOutput( job, false);
}
{code}",07/Dec/11 19:38;xutingz;I bring the removed code into 0.10 and it passes the commit test and my new unit test. the PIG-2391-2.patch is the new patch on that.,07/Dec/11 20:04;daijy;Thanks Xuting! I will commit patch once tests pass.,"08/Dec/11 22:05;daijy;Unit tests pass. test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 465 release audit warnings (more than the trunk's current 458 warnings).

No new file added, so ignore release audit warning.

Patch committed to trunk/0.10. Thanks Xuting!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BinStorageRecordReader causes negative progress,PIG-2387,12532588,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xutingz,anitharaju,anitharaju,24/Nov/11 09:23,23/Jan/12 07:31,14/Mar/19 03:07,06/Dec/11 21:59,0.9.0,,,,,,,0.10.0,0.11,0.9.2,,,,,0,,,,,,,,,,,,,"Hi,

When an input file of size greater than default split size is loaded using BinStorage() and some processing is done, the task returns negative progress

Script
{code}

A = load 'input' using BinStorage() as (a:chararray);
B = filter A by (a matches '.*blinds.*');
store B into 'op';

{code}

Looking at the code, BinStorage which uses BinStorageRecordReader, has getProgress()

{code}
public float getProgress() {
    if (start == end) {
      return 0.0f;
    } else {
          return Math.min(1.0f, (pos - start) / (float)(end - start));
    }
  }
{code}

In BinStorageRecordReader, pos is always 0 and not getting updated at any point.
So when the input file of size greater than default split size is loaded and processed, the getProgress() method returns negative value, thus showing negative progress.

Regards,
Anitha 


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Dec/11 02:30;xutingz;PIG-2387.patch;https://issues.apache.org/jira/secure/attachment/12505960/PIG-2387.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-28 01:46:37.127,,,no_permission,,,,,,,,,,,,218321,Reviewed,,,Tue Dec 06 21:59:38 UTC 2011,,,,,,,0|i0h2ev:,97653,,,,,,,,,,"28/Nov/11 01:46;daijy;It seems ""pos remains 0"" is wrong. It should get updated in nextKeyValue.","30/Nov/11 02:42;olgan;Hi Xuting,

Could you, please, take a look. This needs to go onto 0.9, 10 branches and trunk","03/Dec/11 02:43;xutingz;test-commit has been successfully run on trunk, 0.10 and 0.9.

I fail to find a user case where the RecordReader.getProgess() is called by a Pig class during the running of a Pig script. So instead of adding a unit test to this patch, I ran the following scripts and printed out the result of the .getProgess() in InterRecordReader/BinStorageRecordReader class manually to check the correctness of the patch.

     Properties props = new Properties();
      for (Entry<Object, Object> entry : cluster.getProperties().entrySet()) {
          props.put(entry.getKey(), entry.getValue());
      }
      props.setProperty(""mapred.max.split.size"", ""100"");
      props.setProperty(""pig.overrideBlockSize"", ""100"");
      System.setProperty(""pig.overrideBlockSize"", ""100"");
        
        PigServer pigServer = new PigServer(ExecType.MAPREDUCE, props);
        pigServer.registerQuery(""a = load '"" + file + ""' AS (s:chararray);"");
        ExecJob job = pigServer.store(""a"", ""output"", ""BinStorage"");
        pigServer.registerQuery(""b = load 'output' using BinStorage() AS (s:chararray);"");
        
        Iterator<Tuple> it = pigServer.openIterator(""b"");
        while(it.hasNext()) {
        	it.next();
        }

The prints information shows:

1.When the iterator is created, a new BinStorageRecorderReader is created and the Progress is updated when the value is updated: 

new BinStorageRecordReader, its start and ends are: 0 100
Progress: 0.13


2.When the iterator.next() function is called, series of InterRecordReaders are instantiated and each time a tuple in the BinStorage is visited, the pos is updated:

new InterRecordReader, its start and ends are: 0 100
Progress: 0.1
Progress: 0.2
Progress: 0.29
Progress: 0.39
Progress: 0.49
Progress: 0.59
Progress: 0.69
Progress: 0.79
Progress: 0.89
Progress: 0.99
Progress: 1.0
new InterRecordReader, its start and ends are: 100 200
Progress: 0.19
Progress: 0.29
Progress: 0.39
Progress: 0.49
Progress: 0.59
Progress: 0.69
Progress: 0.79
Progress: 0.89
Progress: 0.99
Progress: 1.0

....
new InterRecordReader, its start and ends are: 800 900
Progress: 0.16
Progress: 0.26
Progress: 0.36
Progress: 0.45
Progress: 0.55
Progress: 0.65
Progress: 0.75
Progress: 0.85
Progress: 0.95
Progress: 1.0
new InterRecordReader, its start and ends are: 900 995
Progress: 0.15789473
Progress: 0.2631579
Progress: 0.36842105
Progress: 0.47368422
Progress: 0.57894737
Progress: 0.68421054
Progress: 0.7894737
Progress: 0.8947368
Progress: 1.0","06/Dec/11 21:59;daijy;Patch looks good. 

Unit tests pass. test-patch: 
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 463 release audit warnings (more than the trunk's current 456 warnings).

No tests included, Xuting tested it manually. No new files added, ignore release audit warnings.

Patch committed to trunk/0.10/0.9

Thanks Xuting!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Store statements not getting processed,PIG-2385,12532436,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,daijy,vivekp,vivekp,23/Nov/11 08:04,23/Jan/12 07:31,14/Mar/19 03:07,04/Dec/11 07:08,0.9.1,,,,,,,0.10.0,0.11,0.9.2,,,,,0,,,,,,,,,,,,,"The actual script in which we got this issue is pretty big and complex. The script has total 4 STORE statements and one of the STORE statement is not getting executed.
The script executes 3 sets of jobs (excluding one STORE which is not getting executed) consisting of 10, 11 and 19 jobs.

The below script could be used to illustrate the issue but with Multiquery turned off;

{code}
A = LOAD 'input1' as (f1:chararray,f2:chararray,f3:chararray);
Z = group A all;
Z1 = foreach Z generate COUNT(A) as count;
B = foreach A generate f1,f2,f3,(100-Z1.count) as diff;
C = order B by diff;
STORE C INTO 'output/C_out';

D = DISTINCT C ;
store D into 'output/F_out';
{code}

For this script, if run with Multiquery turned off, the Store command for D is not getting executed.
I can see that the statements are getting parsed and LOStore created for D , but still, it is not getting executed.
The above script works fine with Pig 0.8.(This issue still exists in Trunk as well)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Nov/11 19:17;daijy;PIG-2385-0.patch;https://issues.apache.org/jira/secure/attachment/12505372/PIG-2385-0.patch,02/Dec/11 08:40;daijy;PIG-2385-1.patch;https://issues.apache.org/jira/secure/attachment/12505862/PIG-2385-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-11-28 07:35:12.419,,,no_permission,,,,,,,,,,,,218169,Reviewed,,,Tue Dec 06 07:49:58 UTC 2011,,,,,,,0|i09zav:,56160,,,,,,,,,,"23/Nov/11 08:26;vivekp;I think that this issue is introduced as part of new parser changes in Pig 0.9. 
As per my analysis, the Scalar ref used,""Z1.count"" introduces another additional LOStore with a tmp location.
While processing for STORE D, it considers this extra LOStore also and thinks that it has processed all store statements.
Hence,the store operator for the alias 'D' is skipped (this is happening at PigServer.Graph.skipStores() ).

The above script, could be used to replicate the scenario without multiquery, if we use PigServer to register and execute the queries.","28/Nov/11 07:35;daijy;We use processedStores to track how many stores we've already processed. However, we counting different things. When we count how many stores we processed, we use the plan after postProcess. When we skip stores in the logical plan, we use the plan before postProcess. The difference is the former count the scalar LOStore added by postProcess, the later not. We should make the count on the same plan.",28/Nov/11 19:17;daijy;Attach a draft patch.,"30/Nov/11 02:58;olgan;Daniel,

When do you think this can be checked in? We would like this on 0.9 branch in addition to 10 and trunk, thanks","01/Dec/11 20:07;daijy;I only need to add test case, I will try to upload the patch today.",02/Dec/11 23:40;thejas;+1,"04/Dec/11 07:08;daijy;Unit tests pass. test-patch: 
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 463 release audit warnings (more than the trunk's current 456 warnings).

No new file added, ignore release audit warning.

Patch committed to trunk/0.10/0.11","05/Dec/11 18:38;olgan;What are the conditions under which the issue observed?

Also, I assume not even an empty directory created when a store is missed, is this correct?","05/Dec/11 18:50;daijy;Hi, Olga, the conditions are:
1. Use scalar
2. Multiple stores in script
3. Muti-query is off, or the multi-query is on, but you have ""exec"" in the middle of script(the script before exec contains scalar definition)

When this happens, Pig does not even create empty directory.","06/Dec/11 07:49;vivekp;Thanks for the patch Daniel. I verified the patch with the actual script and its working fine.
Just for the update, the actual script in which we saw this issue does not have an exec statement and multiquery is on.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generic Invokers should use PigContext to resolve classes,PIG-2384,12532432,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,dvryaboy,dvryaboy,23/Nov/11 06:41,29/Nov/11 23:59,14/Mar/19 03:07,29/Nov/11 23:59,,,,,,,,0.10.0,0.11,,,,,,0,,,,,,,,,,,,,"Some users report having to put their jars on Pig's classpath in addition to ""register""ing them in order to get Generic Invokers to pick up their classes. This is due to us using Class.forName instead of relying on PigContext's resolveClassName (which handles all the dynamically registered classes).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Nov/11 06:44;dvryaboy;PIG-2384.patch;https://issues.apache.org/jira/secure/attachment/12504852/PIG-2384.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-29 18:55:39.863,,,no_permission,,,,,,,,,,,,218165,,,,Tue Nov 29 23:59:44 UTC 2011,,,,,,,0|i0h2ef:,97651,,,,,,,,,,"23/Nov/11 06:44;dvryaboy;Attaching patch that reportedly fixes the issue.

It's fairly trivial, and I don't think the cost of setting up the multiple-classloaders test case is worth the time.. will commit in a few days if no objections are raised.","29/Nov/11 18:55;alangates;+1, looks fine.",29/Nov/11 23:59;dvryaboy;Committed to trunk and 0.10 branch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in Schema.getPigSchema(ResourceSchema rSchema) improperly adds two level access,PIG-2379,12531739,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,jcoveney,jcoveney,17/Nov/11 04:15,28/Nov/11 18:53,14/Mar/19 03:07,22/Nov/11 01:19,0.10.0,0.11,0.9.2,,,,,0.10.0,0.11,0.9.2,,,,,0,,,,,,,,,,,,,"If you do this:
Schema s1=Utils.getSchemaFromString(""b:bag{t:tuple(name:chararray,age:int)}"");
Schema s2=Schema.getPigSchema(new ResourceSchema(s1));
System.out.println(s1.equals(s2)); //false!!

That's super weird! The reason is that getPigSchema was setting two level access to true for bags. I added a test, and deleted the piece that set it. I worked with trunk, but a cursory glance makes it appear that it should be able to work with other versions as well, as I don't think that code has changed for a while. I ran test-commit without issue, but haven't ran the full test suite.

I appreciate any feedback on this!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Nov/11 04:35;jcoveney;PIG2379.patch;https://issues.apache.org/jira/secure/attachment/12504026/PIG2379.patch,21/Nov/11 18:01;jcoveney;PIG2379_1.patch;https://issues.apache.org/jira/secure/attachment/12504524/PIG2379_1.patch,28/Nov/11 18:51;daijy;PIG2379_2.patch;https://issues.apache.org/jira/secure/attachment/12505368/PIG2379_2.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-11-21 17:34:09.966,,,no_permission,,,,,,,,,,,,217475,,,,Mon Nov 28 18:53:57 UTC 2011,,,,,,,0|i0h2db:,97646,,,,,,,,,,17/Nov/11 04:30;jcoveney;got rid of an unnecessary comment,"17/Nov/11 04:31;jcoveney;Only ant test-committed on 11, probably would work on the rest","21/Nov/11 17:34;dvryaboy;Jonathan, can you regenerate patch with ""-w"" (skip whitespace-only changes)?  Makes it easier to review.

If you like, open another ticket to fix whitespace ...

D","21/Nov/11 18:01;jcoveney;good call, dmitriy. this has no whitespace changes (which themselves were largely a byproduct of my vim settings)","22/Nov/11 01:19;dvryaboy;+1

committed to trunk.",22/Nov/11 01:26;dvryaboy;Also committed to 0.9.2 and 0.10,28/Nov/11 18:51;daijy;TestResourceSchema.testToPigSchemaWithTwoLevelAccess fail after patch checkin. Attach patch PIG2379_2.patch.,28/Nov/11 18:53;daijy;PIG2379_2.patch committed to trunk/0.10/0.9.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
streaming regression with dotNext,PIG-2374,12531552,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,araceli,araceli,15/Nov/11 23:29,23/Jan/12 07:31,14/Mar/19 03:07,07/Dec/11 20:25,,,,,,,,0.9.2,,,,,,,0,hadoop023,,,,,,,,,,,,"Streaming seems to be broken in dotNext. There are several tests that are failing.
The results from C below produce clean results.
The results from D which are streamed through CMD produce control characters on some of the output.

define CMD `perl GroupBy.pl '\t' 0` ship('/homes/monster/pigtest/pigtest_next/pigharness/dist/pig_harness/libexec/PigTest/GroupBy.pl');
A = load '/user/user1/pig/tests/data/singlefile/studenttab10k';
B = group A by $0;
C = foreach B generate flatten(A);
D = stream C through CMD;
store C into '/user/user1/pig/out/user1.1321117428/ComputeSpec_7_C.out';
store D into '/user/user1/pig/out/user1.1321117428/ComputeSpec_7_D.out';



Other streaming tests that fail with control characters:
EST FAILED <ComputeSpec_7>
TEST FAILED <ComputeSpec_8>
TEST FAILED <ComputeSpec_10>
TEST FAILED <ComputeSpec_11>
TEST FAILED <ComputeSpec_12>
TEST FAILED <JobManagement_2>
TEST FAILED <JobManagement_3>
TEST FAILED <StreamingIO_4>
TEST FAILED <NonStreaming_1>
TEST FAILED <MultiQuery_21>
...
","hadoopApache Pig version 0.9.2.1111101150 (r1200499)
compiled Nov 10 2011, 19:50:15
 -bash-3.1$ hadoop version
Hadoop 0.23.0.1111080202

Subversion http://svn.apache.org/repos/asf/hadoop/common/branches/branch-0.23.0/hadoop-common-project/hadoop-common -r 1196973
Compiled by hadoopqa on Tue Nov  8 02:12:04 PST 2011
From source with checksum 4e42b2d96c899a98a8ab8c7cc23f27ae
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Dec/11 00:51;daijy;PIG-2374-1.patch;https://issues.apache.org/jira/secure/attachment/12506074/PIG-2374-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-02 01:53:36.445,,,no_permission,,,,,,,,,,,,217288,Reviewed,,,Wed Dec 07 22:14:59 UTC 2011,,,,,,,0|i09z9r:,56155,,,,,,,,,,"02/Dec/11 01:53;daijy;This is caused by HADOOP-6109 (0.21 and beyond). After 6109, Text.getBytes() will return a bytearray larger than Text.length. In Pig code, OutputHandler:92, we use the bytearray and ignore length. We need to either:
1. Ask Hadoop to rollback HADOOP-6109
2. Hunting down all occurrence we use getBytes() but ignore length in Pig",05/Dec/11 00:52;daijy;PIG-2374-1.patch use approach 2.,05/Dec/11 18:52;daijy;Unit tests pass. No tests included cuz the current e2e tests already have it covered.,06/Dec/11 22:48;thejas;+1,"07/Dec/11 20:25;daijy;Unit tests pass. test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 463 release audit warnings (more than the trunk's current 456 warnings).

No tests included since it is a regression. No new file added so ignore release audit warning.

Patch committed to trunk/0.10/0.9","07/Dec/11 21:32;ashutoshc;We should push for backward compatibility of getBytes() on Hadoop for this. The way it is fixed with this patch will necessitate an extra buffer copy in Pig, an unnecessary performance hit.","07/Dec/11 21:44;daijy;Yes, this is a break of contract and might hit other projects as well. ","07/Dec/11 22:14;olgan;I think Ashutosh is brining a really good point. We seemed to always fixing things in Pig because understandably it is easier for us. However, if Hadoop is breaking contract they should be fixing this especially if we have to be paying performance penalty on this",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SkewedParitioner results in Kerberos error,PIG-2370,12531528,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,araceli,araceli,15/Nov/11 20:29,23/Jan/12 07:31,14/Mar/19 03:07,07/Dec/11 21:23,0.9.2,,,,,,,0.10.0,0.11,0.9.2,,,,,0,hadoop023,,,,,,,,,,,,"Most of the SkewedJoin tests fail.

a = load '/user/user1/pig/tests/data/singlefile/studenttab10k' using PigStorage() as (name, age, gpa);
b = load '/user/user1/pig/tests/data/singlefile/votertab10k' as (name, age, registration, contributions);
e = join a by name, b by name using 'skewed' parallel 8;
store e into '/user/user1/pig/out/user1.1321044742/SkewedJoin_1.out';

Backend error message
---------------------
AttemptID:attempt_1321041443489_0400_m_000000_0 Info:Error: java.lang.RuntimeException: java.io.IOException: Can't get JobTracker Kerberos principal for use as renewer
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.SkewedPartitioner.setConf(SkewedPartitioner.java:119)
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:70)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:125)
        at org.apache.hadoop.mapred.MapTask$NewOutputCollector.<init>(MapTask.java:627)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:695)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:328)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1152)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)
Caused by: java.io.IOException: Can't get JobTracker Kerberos principal for use as renewer
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:106)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:90)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:83)
        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:205)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigFileInputFormat.listStatus(PigFileInputFormat.java:37)
        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:269)
        at org.apache.pig.impl.io.ReadToEndLoader.init(ReadToEndLoader.java:154)
        at org.apache.pig.impl.io.ReadToEndLoader.<init>(ReadToEndLoader.java:116)
        at org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil.loadPartitionFileFromLocalCache(MapRedUtil.java:101)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.SkewedPartitioner.setConf(SkewedPartitioner.java:114)
        ... 10 more
","-bash-3.1$ hadoop version
Hadoop 0.23.0.1111080202
Subversion http://svn.apache.org/repos/asf/hadoop/common/branches/branch-0.23.0/hadoop-common-project/hadoop-common -r 1196973
Compiled by hadoopqa on Tue Nov  8 02:12:04 PST 2011
From source with checksum 4e42b2d96c899a98a8ab8c7cc23f27ae
-bash-3.1$ pig -version
USING: /homes/araceli/pighome/dotNext/current
Apache Pig version 0.9.2.1111101150 (r1200499)
compiled Nov 10 2011, 19:50:15
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Dec/11 19:33;daijy;PIG-2370-1.patch;https://issues.apache.org/jira/secure/attachment/12506295/PIG-2370-1.patch,07/Dec/11 18:52;daijy;PIG-2370-2.patch;https://issues.apache.org/jira/secure/attachment/12506496/PIG-2370-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-12-06 22:55:57.929,,,no_permission,,,,,,,,,,,,217264,Reviewed,,,Wed Dec 07 21:23:00 UTC 2011,,,,,,,0|i09z9j:,56154,,,,,,,,,,06/Dec/11 22:55;thejas;+1,07/Dec/11 18:52;daijy;PIG-2370-2.patch fix a unit test failure.,"07/Dec/11 21:23;daijy;Unit tests pass. test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 465 release audit warnings (more than the trunk's current 456 warnings).

No tests added since this is a regression. No new file added so ignore release audit warning.

Patch committed to trunk/0.10/0.9",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
_logs for streaming commands bug in new parser,PIG-2363,12531333,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,vivekp,vivekp,vivekp,14/Nov/11 16:18,26/Apr/12 20:33,14/Mar/19 03:07,03/Jan/12 05:17,0.9.1,,,,,,,0.10.0,0.11,,,,,,0,,,,,,,,,,,,,"For Pig scripts which has streaming commands , the stderr is saved into hdfs under _logs folder in the output directory.
This behavior was not seen with Pig 0.8 by default, but from 0.9 onwards,  we are seeing _logs folder.
Hence it would be nice to have a configuration to disable this feature.

Sample script
{code}
DEFINE mycmd `t.pl` ship ('t.pl');
a = load 'i1' as (f1:chararray,f2:chararray);
b = stream a through mycmd;
store b into 'output';
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Nov/11 16:20;vivekp;PIG-2363_1.patch;https://issues.apache.org/jira/secure/attachment/12503636/PIG-2363_1.patch,21/Dec/11 18:37;vivekp;PIG-2363_2.patch;https://issues.apache.org/jira/secure/attachment/12508277/PIG-2363_2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-11-14 19:44:42.635,,,no_permission,,,,,,,,,,,,217069,Reviewed,,,Tue Jan 03 05:17:27 UTC 2012,,,,,,,0|i0h2a7:,97632,,,,,,,,,,"14/Nov/11 16:23;vivekp;Added a boolean configuration ""pig.streaming.log.persist"".
Not sure whether anything more has to be done to introduce a new configuration.",14/Nov/11 19:44;daijy;That should be good enough. Do you have idea why 0.8 does not have such issue?,"14/Nov/11 21:21;olgan;I looked at it at some point and it seems that we regressed and create _log directory in more places than before and instead of doing it lazily we do it proactively. I think rather than adding more config options, we need to figure out why this is happenning.","15/Nov/11 12:03;vivekp;As I understand in Pig 0.8, the logging happens only when the user explicitly specifies the logging option in the define clause. 
DEFINE mycmd `t.pl` stderr('mylogs' limit 100);
 
This is defined in QueryParser.jjt, in DefineClause;
<INPUT> ""("" InputOutputSpec(command, StreamingCommand.Handle.INPUT) "")""
           |
           <OUTPUT> ""("" InputOutputSpec(command, StreamingCommand.Handle.OUTPUT) "")""
           |
           <ERROR> ""("" ErrorSpec(command, t.image) "")""


But in Pig 0.9, with the parser changes, the LogicalPlanBuilder.buildCommand always sets the logging to true.
This is because from the LogicalPlanGenerator.g the log directory is is set to the alias name if none specified.
($error_clause.dir == null? $alias : $error_clause.dir)

So this change looks intentional, if not may be as Olga mentioned we should change this rather than adding a config.","23/Nov/11 08:36;vivekp;Please let me know whether we should change LogicalPlanGenerator.g so that, null will be considered as it.
My only concern is that, if the fix version is 0.10 or higher, we might be confusing the users with the default behavior across the versions.","19/Dec/11 09:50;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/3256/
-----------------------------------------------------------

Review request for pig.


Summary
-------

For Pig scripts which has streaming commands , the stderr is saved into hdfs under _logs folder in the output directory.
This behavior was not seen with Pig 0.8 by default, but from 0.9 onwards, we are seeing _logs folder.
Hence it would be nice to have a configuration to disable this feature.


Sample script
DEFINE mycmd `t.pl` ship ('t.pl');
a = load 'i1' as (f1:chararray,f2:chararray);
b = stream a through mycmd;
store b into 'output';


This addresses bug PIG-2363.
    https://issues.apache.org/jira/browse/PIG-2363


Diffs
-----


Diff: https://reviews.apache.org/r/3256/diff


Testing
-------


Thanks,

Vivek

","20/Dec/11 22:15;daijy;Hi, Vivek,
I don't think we intentionally drop stderr statement in 0.9. It must be a bug in the new parser. Can we fix the parser instead of introducing a new config?",21/Dec/11 18:37;vivekp;Attaching a patch which has a fix for the parser.,02/Jan/12 18:46;daijy;+1. Will commit once tests pass.,"03/Jan/12 05:17;daijy;Unit test pass. test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 508 release audit warnings (more than the trunk's current 501 warnings).

No new file added, ignore release audit warning.

Patch committed to 0.10/trunk.

Thanks Vivek!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobStats.getHadoopCounters() is never set and always returns null,PIG-2358,12531110,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xutingz,vivekp,vivekp,11/Nov/11 09:41,26/Apr/12 20:33,14/Mar/19 03:07,02/Dec/11 19:48,0.10.0,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"The api  JobStats.getHadoopCounters() always returns null since the instance variable returned from the method (""counters"") is never set.
{code}
PigStats stats = PigRunner.run(args, null);
PigStats.JobGraph jobGraph = stats.getJobGraph();
		
for (JobStats jobStats :  jobGraph) {
   Counters counters = jobStats.getHadoopCounters();
   System.out.println(counters);// prints null
}
{code}
But of course I can get individual counter information through the other APIs (getHdfsBytesWritten(),getMapInputRecords() etc).
I guess the change came in as part of PIG-1389 (the instance variable got replaced with method local variable in JobStats.addCounters).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Nov/11 19:40;xutingz;PIG-2358-1.patch;https://issues.apache.org/jira/secure/attachment/12505525/PIG-2358-1.patch,21/Nov/11 23:41;xutingz;PIG-2358.patch;https://issues.apache.org/jira/secure/attachment/12504671/PIG-2358.patch,23/Nov/11 19:48;alangates;TEST-org.apache.pig.test.TestPigRunner.txt;https://issues.apache.org/jira/secure/attachment/12504917/TEST-org.apache.pig.test.TestPigRunner.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-11-11 18:44:09.629,,,no_permission,,,,,,,,,,,,216848,Reviewed,,,Fri Dec 02 19:48:49 UTC 2011,,,,,,,0|i0h29b:,97628,,,,,,,,,,"11/Nov/11 18:44;virag;The API JobStats.getHadoopCounters() is very important as I can't get all hadoop counters through other API's (E.g. FILE_BYTES_READ, FILE_BYTES_WRITTEN, COMBINE_INPUT_RECORDS and many more)
","21/Nov/11 22:40;xutingz;Ant test-commit has been successfully run on this patch. 
A Junit test has been added on the getHadoopCounter method.",23/Nov/11 19:47;alangates;I see quite a few failures in TestPigRunner when I apply this patch.  I'll upload the log file from the test.,23/Nov/11 19:48;alangates;Log file from failing test.,"23/Nov/11 22:20;xutingz;Hi Alan,

   Thanks for the comments. I downloaded the trunk code from svn, applied the patch and run "" ant test -Dtestcase=TestPigRunner"" and it seems ran successfully. May I know what is your build environment? Thanks.

Xuting",28/Nov/11 18:48;daijy;Unit tests pass for me.,29/Nov/11 01:05;xutingz;I modified the unit test and run successfully the test-commit as well as the TestPigRunner Junit test on trunk. ,29/Nov/11 20:06;xutingz;I have successfully run the additional unit test and the ant test-commit on both trunk and 0.10 branch. Please let me know if there is any problems. Thanks :),"30/Nov/11 23:41;olgan;Alan, Daniel - could you please confirm if this patch is ready to go and commit if possible, thanks",01/Dec/11 19:57;daijy;Running tests now.,02/Dec/11 08:34;daijy;Tests pass. Patch looks good. I will commit it.,"02/Dec/11 19:48;daijy;Unit tests pass. test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 463 release audit warnings (more than the trunk's current 456 warnings).

There is no new file added, ignore release audit warning.

Patch committed to both trunk and 0.10 branch.

Thanks Xuting!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ant clean does not clean e2e test build artifacts,PIG-2355,12530916,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,daijy,daijy,daijy,09/Nov/11 19:30,22/Feb/13 04:53,14/Mar/19 03:07,09/Nov/11 21:35,,,,,,,,0.11,,,,build,,,0,,,,,,,,,,,,,"After ""ant clean"", the build artifacts in test/e2e is still there. Need to invoke ""ant clean"" in test/e2e/pig to clean those up.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Nov/11 19:32;daijy;PIG-2355-1.patch;https://issues.apache.org/jira/secure/attachment/12503115/PIG-2355-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-09 21:32:39.57,,,no_permission,,,,,,,,,,,,216654,Reviewed,,,Wed Nov 09 21:35:09 UTC 2011,,,,,,,0|i09zcf:,56167,,,,,,,,,,09/Nov/11 21:32;alangates;+1,09/Nov/11 21:35;daijy;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Several fixes for bin/pig,PIG-2354,12530455,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,05/Nov/11 21:25,23/Jan/12 07:31,14/Mar/19 03:07,04/Dec/11 07:13,0.10.0,0.11,0.9.1,,,,,0.10.0,0.11,0.9.2,,impl,,,0,,,,,,,,,,,,,"1. Need to find hadoop binary in PATH first
2. Add HADOOP_CONF_DIR into CLASSPATH
3. A bug in hadoop binary detection Rajat discovered",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Nov/11 21:44;daijy;PIG-2354-1.patch;https://issues.apache.org/jira/secure/attachment/12502623/PIG-2354-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-07 03:19:21.76,,,no_permission,,,,,,,,,,,,216193,Reviewed,,,Sun Dec 04 07:13:18 UTC 2011,,,,,,,0|i09zan:,56159,,,,,,,,,,"07/Nov/11 03:19;dvryaboy;Daniel, I am getting some odd errors trying to use bin/pig with the default version of hadoop that's bundled (20.2). Stack trace below. Is this related?

I don't have any of the env vars short of JAVA_HOME set, just to make sure bin/pig gets to do whatever it wants.

{code}

bash-3.2$ cat /Users/dmitriy/src/pig-svn/pig_1320635602890.log 
Error before Pig is launched
----------------------------
ERROR 2998: Unhandled internal error. org.apache.hadoop.conf.Configuration.addDeprecation(Ljava/lang/String;[Ljava/lang/String;)V

java.lang.NoSuchMethodError: org.apache.hadoop.conf.Configuration.addDeprecation(Ljava/lang/String;[Ljava/lang/String;)V
	at org.apache.hadoop.mapreduce.util.ConfigUtil.addDeprecatedKeys(ConfigUtil.java:49)
	at org.apache.hadoop.mapreduce.util.ConfigUtil.loadResources(ConfigUtil.java:40)
	at org.apache.hadoop.mapred.JobConf.<clinit>(JobConf.java:120)
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.init(HExecutionEngine.java:173)
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.init(HExecutionEngine.java:119)
	at org.apache.pig.impl.PigContext.connect(PigContext.java:206)
	at org.apache.pig.PigServer.<init>(PigServer.java:246)
	at org.apache.pig.PigServer.<init>(PigServer.java:231)
	at org.apache.pig.tools.grunt.Grunt.<init>(Grunt.java:46)
	at org.apache.pig.Main.run(Main.java:498)
	at org.apache.pig.Main.main(Main.java:111)
================================================================================
{code}","07/Nov/11 03:44;daijy;#1 happens when you don't set HADOOP_HOME but hadoop binary is in PATH. #3 happens when you have both Hadoop rpm and tarball installed. 

Your stack indicate that Pig linked to the wrong hadoop. Configuration.addDeprecation is a method for hadoop 0.21+. Can you paste your ""bin/pig -secretDebugCmd""?","07/Nov/11 03:58;dvryaboy;It's using the bundled 20.2 but calling methods from 21. So I guess it's a problem in the shim layer then?


bash-3.2$ ./bin/pig -secretDebugCmd
*Cannot find local hadoop installation, using bundled hadoop 20.2*
dry run:
/Library/Java/Home//bin/java -Xmx1000m  -Dpig.log.dir=/Users/dmitriy/src/pig-svn/bin/../logs -Dpig.log.file=pig.log -Dpig.home.dir=/Users/dmitriy/src/pig-svn/bin/.. -classpath /Users/dmitriy/src/pig-svn/bin/../conf:/Library/Java/Home//lib/tools.jar:/Users/dmitriy/src/pig-svn/bin/../lib/automaton.jar:/Users/dmitriy/src/pig-svn/bin/../lib/avro-1.4.0-SNAPSHOT.jar:/Users/dmitriy/src/pig-svn/bin/../lib/hadoop-hdfs-test-0.22.0-SNAPSHOT.jar:/Users/dmitriy/src/pig-svn/bin/../lib/hadoop-mapred-0.22.0-SNAPSHOT.jar:/Users/dmitriy/src/pig-svn/bin/../build/ivy/lib/Pig/jython-2.5.0.jar:/Users/dmitriy/src/pig-svn/bin/../pig.jar org.apache.pig.Main 
",07/Nov/11 04:05;daijy;I see hadoop-mapred-0.22.0-SNAPSHOT.jar in classpath. Might be confused with bundled 20.2 jars.,"07/Nov/11 05:10;dvryaboy;you are right, that was my fault.",02/Dec/11 23:44;thejas;+1,04/Dec/11 07:13;daijy;Patch committed to trunk/0.10/0.9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e2e test harness' use of environment variables causes unintended effects between tests,PIG-2352,12530387,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,04/Nov/11 21:56,26/Apr/12 20:33,14/Mar/19 03:07,08/Nov/11 16:38,0.9.1,,,,,,,0.10.0,0.11,,,site,,,0,,,,,,,,,,,,,"If you run the new Bloom tests by themselves they pass.  If you run them after running the Accumulator tests they fail.  The accumulator tests set the java parameters, including turning off the combiner.  These properties are then kept set for the Bloom tests, which causes them to fail.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Nov/11 22:49;alangates;PIG-2352.patch;https://issues.apache.org/jira/secure/attachment/12502544/PIG-2352.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-07 18:44:20.942,,,no_permission,,,,,,,,,,,,216125,,,,Tue Nov 08 16:38:21 UTC 2011,,,,,,,0|i09zcv:,56169,,,,,,,,,,"04/Nov/11 21:57;alangates;The issue is that the java parameters are stored in an environment variable, PIG_OPTS.  This is not being unset when a new test is ran.",04/Nov/11 22:49;alangates;I did cheat a bit.  This patch also adds an hdfs temp path to the test harness so we don't have to worry about whether the bloom filters are already existing in HDFS.,07/Nov/11 18:44;daijy;+1,08/Nov/11 16:38;alangates;Patch checked into both 0.10 and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Pig Unit tests for hadoop 23,PIG-2347,12529984,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,03/Nov/11 00:23,02/May/13 02:29,14/Mar/19 03:07,31/Dec/11 18:33,0.10.0,0.11,0.9.1,,,,,0.10.0,0.11,0.9.2,,impl,,,0,,,,,,,,,,,,,This is the continuation work for PIG-2125. There are still 20+ unit test suit for hadoop 23. We need to fix them.,,,,,,,,,,,,,,,,,,,PIG-2449,PIG-2433,MAPREDUCE-3586,PIG-2446,PIG-2410,BIGTOP-162,,,,,,,MAPREDUCE-3389,12/Dec/11 21:20;daijy;PIG-2347-1.patch;https://issues.apache.org/jira/secure/attachment/12507061/PIG-2347-1.patch,13/Dec/11 22:53;acmurthy;PIG-2347-2.patch;https://issues.apache.org/jira/secure/attachment/12507259/PIG-2347-2.patch,22/Dec/11 22:42;daijy;PIG-2347-3.patch;https://issues.apache.org/jira/secure/attachment/12508459/PIG-2347-3.patch,13/Jan/12 20:13;daijy;PIG-2347-3_0.9.patch;https://issues.apache.org/jira/secure/attachment/12510521/PIG-2347-3_0.9.patch,18/Jan/12 06:16;daijy;PIG-2347-4.patch;https://issues.apache.org/jira/secure/attachment/12510960/PIG-2347-4.patch,02/Jan/12 18:53;thw;PIG-2347-AvroDatumWriter.java;https://issues.apache.org/jira/secure/attachment/12509069/PIG-2347-AvroDatumWriter.java,14/Dec/11 05:37;acmurthy;PIG-2347.patch;https://issues.apache.org/jira/secure/attachment/12507305/PIG-2347.patch,06/Dec/11 23:54;tomwhite;PIG-2347.patch;https://issues.apache.org/jira/secure/attachment/12506336/PIG-2347.patch,02/Dec/11 19:49;tomwhite;PIG-2347.patch;https://issues.apache.org/jira/secure/attachment/12505923/PIG-2347.patch,06/Dec/11 23:54;tomwhite;syslog;https://issues.apache.org/jira/secure/attachment/12506337/syslog,,10.0,,,,,,,,,,,,,,,,,,,2011-12-02 19:49:12.824,,,no_permission,,,,,,,,,,,,215843,Reviewed,,,Wed Jan 18 22:01:21 UTC 2012,,,,,,,0|i09z8f:,56149,,,,,,,,,,02/Dec/11 19:49;tomwhite;Tests that were failing were due to incomplete classpaths causing class not found exceptions. With this patch I get past that but now hit MAPREDUCE-3389.,"06/Dec/11 23:54;tomwhite;Here's a new patch which generates a classpath file to take advantage of MAPREDUCE-3389. 

However, lots of tests are still failing. For example, TestAccumulator times out, with the following exception in the log (see attached syslog file)

Exception: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Application doesn't exist in cache appattempt_1323213758124_0001_000001

I'm not sure what is happening here, any pointers appreciated.
","07/Dec/11 00:17;daijy;Thanks, Tom! I will take a look. There is still lots of tests fail for me as well. ","09/Dec/11 00:46;ahmed.radwan;Thanks Tom! I have tried the new patch on a small (4-node) hadoop-0.23 cluster.
I checked out trunk, applied your patch, but I also needed to manually edit the build.xml to change the ""hadoopversion"" to ""23"" (otherwise it doesn't run: ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2998: Unhandled internal error. org.apache.hadoop.mapred.jobcontrol.JobControl.addJob(Lorg/apache/hadoop/mapred/jobcontrol/Job;)Ljava/lang/String;)

Changing the version to 23 and rebuilding solves this issue. I have run some basic Pig scripts without problems.

Can we add a command line argument to ant to specify the hadoopVersion so we don't need to manually edit the build.xml?","09/Dec/11 00:52;thw;ant -Dhadoopversion=23 clean test
 ","12/Dec/11 21:20;daijy;Arun Murthy provides PIG-2347-1.patch, which fix a large amount of failures related to MiniMRYarnCluster and counters. Now I see 22 failed test suits:
TestEvalPipeline
TestEvalPipeline2
TestEvalPipelineLocal
TestFRJoin
TestFRJoin2
TestGrunt
TestHBaseStorage
TestJobSubmission
TestJoinSmoke
TestKeyTypeDiscoveryVisitor
TestMergeJoin
TestMultiQuery
TestMultiQueryBasic
TestMultiQueryCompiler
TestNestedForeach
TestParser
TestPigRunner
TestPruneColumn
TestScalarAliases
TestScriptLanguage
TestScriptUDF
TestSkewedJoin","13/Dec/11 21:45;tomwhite;Arun/Daniel - thanks for the update. I ran a few of the tests that were previously failing for me (e.g. TestAccumulator) and they now pass.

Apart from the change to PigStatsUtil, I think this could be committed, and the remaining failures fixed in another JIRA. I opened MAPREDUCE-3542 to fix the counter name incompatibility.","13/Dec/11 22:53;acmurthy;More fixes to ensure Pig uses multiple NMs for unit tests.

With this and MAPREDUCE-3537 we are down to 15 test failures.","13/Dec/11 23:00;acmurthy;Related: TestJobSubmission fails because Pig depends on hbase-0.90.0 which doesn't play nice with hadoop-0.23.

I guess we should bump up hbase version to 0.90.x or 0.92?","13/Dec/11 23:08;tomwhite;Yes to 0.92, although it has not been released yet.","14/Dec/11 00:46;acmurthy;So, hbase-0.92 will work with hadoop-0.23 by default? Or is it a different mvn profile?",14/Dec/11 01:05;tomwhite;A different profile (pass -Dhadoop.profile=23).,"14/Dec/11 01:50;acmurthy;Ok, this means hbase-0.92 will have to ship jars of both profiles? (20.2xx & 23)? Is that already the case?",14/Dec/11 01:57;rvs;HBase 0.92 has decided to make dependency on hadoop optional. Downstream of HBase is now required to manage that explicitly.,"14/Dec/11 02:22;acmurthy;So, if pig depends on hadoop-0.23 and hbase-0.92 it should work fine?","14/Dec/11 02:26;rvs;@Arun: correct. The HBase .jar is agnostic to the version of the Hadoop that it runs against. The hbase-0.92-test.jar, however, is not. So if you're depending on any code from there -- your original comment applies.",14/Dec/11 05:35;acmurthy;[~rvs] Pig needs both hbase-0.92.jar and hbase-0.92-test.jar. Will hbase-0.92 ship with test jars for both hadoop-0.20.2xx and hadoop-0.23?,"14/Dec/11 05:37;acmurthy;Updated patch, should fix TestFRJoin also.","14/Dec/11 21:39;rvs;@Arun, not currently. There's a plan to make test artifact Hadoop version agnostic, though: HBASE-4850",15/Dec/11 08:23;acmurthy;MAPREDUCE-3563 fixes a couple of Pig unit tests too.,"21/Dec/11 23:36;thw;Patch for 0.9 branch that includes relevant portion of trunk changes plus conditional exclude of JobHistoryLoader for 0.23
 ","22/Dec/11 22:42;daijy;PIG-2347-3.patch should fix all 23 unit tests theoretically. I still haven't finish a complete successful run in 23 yet, but I want to share early so folks can help test.

There are still couple of holes:
1. PIG-2446, Arun is still investigating alternatives for map input bytes
2. PIG-2433 & PIG-2449, which are test failures revealed in 23 test, but they are not 23 related
3. TestHBaseStorage & TestJobSubmission.testReducerNumEstimation are blocked by HBASE-4850

I skip the above mentioned tests for 23 in the patch.",26/Dec/11 20:10;daijy;Tests run complete successfully for both 20 and 23 with PIG-2347-3.patch.,"27/Dec/11 19:21;daijy;test-patch result:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 43 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 508 release audit warnings (more than the trunk's current 500 warnings).

The only new file is excluded-tests-23, which has no way to add Apache header. So ignore release audit warning.

Patch is ready for review.",28/Dec/11 02:40;thejas;+1,28/Dec/11 02:49;daijy;Patch committed to trunk/0.10. I need to do some resync for 0.9 branch.,28/Dec/11 08:00;daijy;Attach patch for 0.9 branch PIG-2347-3_0.9.patch.,"31/Dec/11 02:43;thw;For 0.9, had to make a small change to the ivy.xml to get past CNF exception (missing slf4j-api dependency):


     <dependency org=""org.slf4j"" name=""slf4j-log4j12"" rev=""${slf4j-log4j12.version}""
      conf=""compile->master;test->default""/>

In general it looks like we could rely more on the maven dependencies (refer to default configuration rather than master and if necessary blacklist problematic dependencies).
",31/Dec/11 18:25;daijy;All unit tests pass on 0.9 branch as well.,31/Dec/11 18:33;daijy;Patch also committed to 0.9 branch.,"01/Jan/12 00:37;thw;Thanks Daniel. Please change default hadoopversion back to 20. As result of avro update from 1.4.1 to 1.5.3 there is a compile error in piggybank. I will update PIG-2410 to account for the same.
","01/Jan/12 02:32;daijy;Yes, default version should be 20. I change it back. Thanks Thomas.",02/Jan/12 18:53;thw;PigAvroDatumWriter for 0.9 piggybank as separate patch.,02/Jan/12 19:21;daijy;+1 for PigAvroDatumWriter change.,03/Jan/12 04:53;daijy;I committed PIG-2347-AvroDatumWriter.java to 0.9 branch to fix piggybank compilation failure. Thanks Thomas!,"04/Jan/12 17:39;tomwhite;> Tests run complete successfully for both 20 and 23 with PIG-2347-3.patch.

Daniel, is this running test-commit, test-core, or test-unit?","04/Jan/12 18:25;daijy;It's all unit tests, include test-commit, test-core, test-unit. Are you able to run?","04/Jan/12 18:40;rvs;Daniel, we're seeing ~100 test failures when the tests are executed against the tip of the branch-0.9:
  http://bigtop01.cloudera.org:8080/view/0.23%20Unit%20Tests/job/Bigtop-hadoop23-pig-unit-test/lastCompletedBuild/testReport/

Do you have any suggestions for us as what could be wrong. E.g.:
  http://bigtop01.cloudera.org:8080/view/0.23%20Unit%20Tests/job/Bigtop-hadoop23-pig-unit-test/lastCompletedBuild/testReport/org.apache.pig.test/TestAccumulator/testAccumWithBuildinAvg/ ","04/Jan/12 18:56;daijy;I see lots of ""Too many open files"" errors. Seems like you still hit MAPREDUCE-3586. Can you check if you have lots of MRAppMaster process running? Can you try to remove .ivy2 and pull all jars again?","05/Jan/12 00:10;rvs;Thanks for the suggestion. I've added removal of .m2 and .ivy2 caches and also I'm running the # of process monitoring script. So far it seems that the issue is not really relate to the # of processes. But I'll let the testrun finish:
   http://bigtop01.cloudera.org:8080/job/Bigtop-hadoop23-pig-unit-test/13/console","05/Jan/12 00:17;daijy;If TestAccumulator fails, I bet a lot test cases will fail. 

Have you run it locally? Thomas and I run the tests on several different machines, the result seems good.","05/Jan/12 00:36;daijy;I still see ""Too many open files"", is that hit the system limit?",05/Jan/12 01:00;rvs;The previous limit was a couple of thousand open files. I bumped it to 32768 (as confirmed by ulimit -a) and re-started the tests.,"05/Jan/12 19:33;daijy;Hi, Roman,
I see #15 finish, seems it takes much more time than on my machine (mine takes 8h). TestSkewedJoin & TestEvalPipeline2 fail due to timeout, might blame to the slowness. You can try to increase timeout in build.xml. Many of other failures seems related to ant 1.8 (PIG-2172). Use ant 1.7 should solve the issue. ",18/Jan/12 06:16;daijy;Some new development in hadoop23 require commons-httpclient.jar. PIG-2347-4.patch add it.,"18/Jan/12 07:29;dvryaboy;Daniel, seems odd that commons-httpclient is required by hadoop23. This library has been EOLed and replaced by httpcore and httpclient (see http://hc.apache.org/httpclient-3.x/). We pull that in via requiring httpcomponents.version=4.1.  What was the failure you got without commons-httpclient?
","18/Jan/12 07:41;daijy;Here is the stack:
java.lang.NoClassDefFoundError: org/apache/commons/httpclient/HttpMethod
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:458)
Caused by: java.lang.ClassNotFoundException: org.apache.commons.httpclient.HttpMethod
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
	... 1 more

This happens when running local mode test (PIG-2480, Harsh's comment)
","18/Jan/12 13:12;dvryaboy;I see, thanks. 

This is a Hadoop 0.23 bug, not a Pig issue. Theoretically Hadoop should be listing commons-httpclient as a dependency in its pom, it should come in from ivy via transitive dependencies, and automatically get on the classpath.

In fact, that's happening: if I remove this dependency from ivy/libraries.properties, it still shows up in build/ivy/lib/Pig when I build for hadoop 20, but not if I build for hadoop 23.

Should be fixed upstream...
","18/Jan/12 15:20;acmurthy;Dmitriy - hadoop does declare a dep on commons-httpclient in our pom. The problem is that Pig doesn't do transitive closure of Hadoop's deps for various reasons (differences in versions etc.) and hence we have to manually include them in Pig. I agree this isn't ideal, but I don't see a way around without a massive change in Pig. ","18/Jan/12 15:55;dvryaboy;Arun -- it's not a big deal to include it, I am just curious why it works for 20 and doesn't work for 23, given the same Pig. Commons-httpclient does get pulled in via transitive dependencies when we depend on 0.20 progeny.","18/Jan/12 18:42;thw;@Dmitriy: 0.20 is referenced through its default configuration (which will allow for the dependencies to be pulled), while 0.23 uses master, which will only pull the artifact. We should really try to improve this to let ivy/maven do the work for 0.23, the current setup is quite fragile and verbose.

 conf=""hadoop20->default""/>

vs.

 conf=""hadoop23->master""/>","18/Jan/12 19:16;dvryaboy;Thanks for the explanation.
I tried switching the conf to ""default"" and got a few maven errors.. Seems like default pulls in the world, and master pulls in nothing.. perhaps a ""runtime"" target or some minimal set of dependencies could be published. But that's a separate issue, probably better surfaced on the Hadoop project(s).

fwiw, here are the errors I got when pulling default conf:

{code}

[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 		::          UNRESOLVED DEPENDENCIES         ::
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 		:: commons-daemon#commons-daemon;1.0.3: java.text.ParseException: inconsistent module descriptor file found in 'http://repo2.maven.org/maven2/commons-daemon/commons-daemon/1.0.3/commons-daemon-1.0.3.pom': bad organisation: expected='commons-daemon' found='org.apache.commons'; 
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 		::              FAILED DOWNLOADS            ::
[ivy:resolve] 		:: ^ see resolution messages for details  ^ ::
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 		:: com.sun.jdmk#jmxtools;1.2.1!jmxtools.jar
[ivy:resolve] 		:: com.sun.jmx#jmxri;1.2.1!jmxri.jar
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
{code}","18/Jan/12 19:24;thw;Those are issues with dependency POMs. You can see excludes for those elsewhere in the file:

        <exclude org=""commons-daemon"" module=""commons-daemon""/><!--bad POM-->
        <exclude org=""org.apache.commons"" module=""commons-daemon""/><!--bad POM-->

I think we should follow the hbase setup in which we specifically exclude what we don't want vs. excluding all.
","18/Jan/12 22:01;azaroth;I am not an expert but I think the errors are just due to maven1 vs maven2 naming conventions and should be easily solvable.
I will try to have a look into it soonish.

I am not sure I would like to have a blacklist rather than a whitelist.
Pulling in dependencies without control is itself a fragile setup.",,,,,,,,,,,,,,,,,,,,,,,
TypeCastInsert should not insert Foreach if there is no as statement,PIG-2346,12529973,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,02/Nov/11 23:28,23/Jan/12 07:31,14/Mar/19 03:07,05/Nov/11 19:35,0.10.0,0.11,0.9.1,,,,,0.10.0,0.11,0.9.2,,,,,0,,,,,,,,,,,,,"Currently, TypeCastInsert always insert foreach statement below the loader. But if the load statement doesn't have ""as"" clause, the foreach should not be added.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Nov/11 01:11;daijy;PIG-2346-1.patch;https://issues.apache.org/jira/secure/attachment/12502083/PIG-2346-1.patch,03/Nov/11 18:32;daijy;PIG-2346-2.patch;https://issues.apache.org/jira/secure/attachment/12502196/PIG-2346-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-11-03 19:10:52.198,,,no_permission,,,,,,,,,,,,215832,Reviewed,,,Sat Nov 05 19:35:39 UTC 2011,,,,,,,0|i09zdr:,56173,,,,,,,,,,03/Nov/11 18:32;daijy;PIG-2346-2.patch fix unit test failures,03/Nov/11 19:10;ashutoshc;Instead of adding new methods of getScriptSchema() I would expect getSchema() to return correct Schema and not null. It seems there is an underlying bug in LoLoad::getSchema().,"03/Nov/11 20:44;ashutoshc;Daniel explained in this case there is a need to find out if schema is coming from as clause or through loadfunc. So, these new methods are required.
+1","05/Nov/11 19:35;daijy;All tests pass. test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 468 release audit warnings (more than the trunk's current 459 warnings).

No new file added, ignore release audit warning.

Patch committed to 0.9 branch, 0.10 branch and trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig tutorial documentation needs to update about building tutorial,PIG-2342,12529710,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,ashutoshc,ashutoshc,01/Nov/11 18:16,23/Jan/12 07:31,14/Mar/19 03:07,11/Jan/12 02:15,0.9.0,,,,,,,0.10.0,0.9.2,,,documentation,,,0,,,,,,,,,,,,,"Step 4 says:
{code}
Edit the build.xml file in the tutorial directory.

Change this:   <property name=""pigjar"" value=""../pig.jar"" />
To this:       <property name=""pigjar"" value=""../pig-0.9.1-core.jar"" />

{code}

This should read as:
{code}
Edit the build.xml file in the tutorial directory.

Change this:   <property name=""pigjar"" value=""../pig.jar"" />
To this:       <property name=""pigjar"" value=""../pig-0.9.1.jar"" />

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Jan/12 21:14;daijy;PIG-2342-1.patch;https://issues.apache.org/jira/secure/attachment/12509800/PIG-2342-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-11 01:07:17.782,,,no_permission,,,,,,,,,,,,215569,Reviewed,,,Wed Jan 11 02:15:32 UTC 2012,,,,,,,0|i09z6f:,56140,,,,,,,,,,11/Jan/12 01:07;thejas;+1,11/Jan/12 02:15;daijy;Patch committed to 0.9/0.10/trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HCatLoader loads all the partitions in a partitioned table even though a filter clause on the partitions is specified in the Pig script,PIG-2339,12529340,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,viraj,viraj,29/Oct/11 03:18,23/Apr/12 21:51,14/Mar/19 03:07,05/Nov/11 21:17,0.9.0,,,,,,,0.10.0,0.11,0.9.2,,,,,0,,,,,,,,,,,,,"A table created by HCAT has the following partitions; 

hcat -e ""show partitions paritionedtable""

{quote}
grid=AB/dt=2011_07_01
grid=AB/dt=2011_07_02
grid=AB/dt=2011_07_03
grid=XY/dt=2011_07_01
grid=XY/dt=2011_07_02
grid=XY/dt=2011_07_03
grid=XY/dt=2011_07_04
...
{quote}

The total number of partitions in the table is around 3200.

A Pig script of this nature tries to access this data using the partitions in it's filter. 

{script}
A = LOAD 'paritionedtable' USING org.apache.hcatalog.pig.HCatLoader();
B = FILTER A BY grid=='AB' AND dt=='2011_07_04';
C = LIMIT B 10;
store C into 'HCAT' using PigStorage();
{script}


This script, fails to run as the job.xml generated by Pig is so large (8MB), that the Hadoop Fred's limitation does not allow it to submit the job. 

After debugging it was found that in the HCatTableInfo class the function gets a null filter value. getInputTableInfo(filter=null ..)

I suspect that ""setPartitionFilter"" function in Pig does not pass the filter correctly to the HCatLoader. This is happening with both Pig 0.9 and 0.8

Viraj",,,,,,,,,,,,,,,,,,,HCATALOG-209,,,,,,,,,,,,,30/Oct/11 00:48;daijy;PIG-2339-1.patch;https://issues.apache.org/jira/secure/attachment/12501484/PIG-2339-1.patch,05/Nov/11 21:16;daijy;PIG-2339-2.patch;https://issues.apache.org/jira/secure/attachment/12502621/PIG-2339-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-10-30 00:21:50.8,,,no_permission,,,,,,,,,,,,215200,Reviewed,,,Tue Jan 17 23:36:57 UTC 2012,,,,,,,0|i09zdj:,56172,,,,,,,"HCatalog, Pig",,,"30/Oct/11 00:21;daijy;TypeCastInserter adds one ForEach between load and filter, so PartitionFilterOptimizer does not push filter into loader. This is new in Pig 0.9 since 0.8 does not insert foreach in this case. 
We do push filter up later, so we can adjust the sequence of rule: run PushUpFilter before PartitionFilterOptimizer to solve the problem.",30/Oct/11 00:57;daijy;Pig 0.8 should be fine. Can you double check?,"31/Oct/11 17:39;viraj;Hi Daniel,
 Thanks for quickly resolving the issue. I need to check if this works with Pig 0.8
Viraj","31/Oct/11 18:16;viraj;Hi Daniel,
 I quickly checked the debug logs of HCatalog and it works with Pig 0.8

{quote}
HCatTableInfo getInputTableInfo(filter=((grid = 'AB') and (dt = '2011_10_01'))
2011-10-31 18:04:15,821 [main] INFO  hive.metastore - Trying to connect to metastore with URI thrift://myserver.com:9080
2011-10-31 18:04:15,830 [main] INFO  hive.metastore - Connected to metastore.
listPartitionsByFilter(default, paritionedtable, ((grid = 'AB') and (dt = '2011_10_01'))) returns 1 partitions.
[hcat.metastore.maxpartitions] = [100000]
{quote}

This is an issue with only Pig 0.9 and beyond.

Viraj
","01/Nov/11 23:32;thejas;With the order of optimization rules in the patch, the limit optimizer would push a limit between the load and filter, and that would prevent the PartitionFilterOptimizer from pushing filter into load function. So the limit optimizer should come after PartitionFilterOptimizer.

","02/Nov/11 06:32;daijy;Hi, Thejas,
I think it again after our discussion, actually we will never push limit before filter cuz it will change the number of output rows.","02/Nov/11 21:54;ashutoshc;@Daniel,
TypeCastInserter shouldn't there be in first place in this plan. Correct?","02/Nov/11 23:28;daijy;Yes, opened PIG-2346 for it.",04/Nov/11 00:09;ashutoshc;+1 Looks good. I would also request to add javadoc in LogicalPlanOptimizer for the reasoning behind the ordering of logical optimizer rules.,05/Nov/11 21:16;daijy;PIG-2339-2.patch resync with trunk.,"05/Nov/11 21:17;daijy;All tests pass. test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 468 release audit warnings (more than the trunk's current 461 warnings).

No new file added. Ignore release audit warning.

Patch committed to 0.9 branch, 0.10 branch and trunk.","25/Nov/11 06:20;coderplay;do you try a non-equal expression like below, where pt is the partition column.
{noformat}
A = LOAD 'partitioned_nation' USING org.apache.hcatalog.pig.HCatLoader();
B = FILTER A BY pt <= '2';
DUMP V;
{noformat}

an exception TApplicationException would thrown.  check the metastore service side, we found an internal exception
{noformat}
11/11/25 13:11:55 ERROR api.ThriftHiveMetastore$Processor: Internal error processing get_partitions_by_filter
java.lang.NullPointerException
        at org.datanucleus.store.mapped.mapping.MappingHelper.getMappingIndices(MappingHelper.java:35)
        at org.datanucleus.store.mapped.expression.StatementText.applyParametersToStatement(StatementText.java:194)
        at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getPreparedStatementForQuery(RDBMSQueryUtils.java:233)
        at org.datanucleus.store.rdbms.query.legacy.SQLEvaluator.evaluate(SQLEvaluator.java:115)
        at org.datanucleus.store.rdbms.query.legacy.JDOQLQuery.performExecute(JDOQLQuery.java:288)
        at org.datanucleus.store.query.Query.executeQuery(Query.java:1657)
        at org.datanucleus.store.rdbms.query.legacy.JDOQLQuery.executeQuery(JDOQLQuery.java:245)
        at org.datanucleus.store.query.Query.executeWithMap(Query.java:1526)
        at org.datanucleus.jdo.JDOQuery.executeWithMap(JDOQuery.java:334)
        at org.apache.hadoop.hive.metastore.ObjectStore.listMPartitionsByFilter(ObjectStore.java:1329)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilter(ObjectStore.java:1241)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler$40.run(HiveMetaStore.java:2369)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler$40.run(HiveMetaStore.java:2366)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.executeWithRetry(HiveMetaStore.java:307)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_filter(HiveMetaStore.java:2366)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions_by_filter.process(ThriftHiveMetastore.j
ava:6099)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor.process(ThriftHiveMetastore.java:4789)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$TLoggingProcessor.process(HiveMetaStore.java:3167)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

It very likely be a bug of datanucleus 2.0.3. 

","25/Nov/11 07:11;ashutoshc;@Min,
Which version of HCatalog you tried with? Can you try this with 0.3 and if problem exists please feel free to open hcatalog jira as it is hcatalog bug then, not Pig's.","25/Nov/11 08:11;coderplay;I've found the reason. It's indeed a bug of datanucleus 2.0.3, neither pig's nor hcatalog's.

try this code: 
{code:borderStyle=solid}
TTransport transport = new TSocket(""your_thrift_server_host"", your_thrift_server_port);
TProtocol protocol = new TBinaryProtocol(transport);
ThriftHiveMetastore.Iface client =
    new ThriftHiveMetastore.Client(protocol);
boolean open = false;
for (int i = 0; i < 5 && !open; ++i) {
  try {
    transport.open();
    open = true;
  } catch (TTransportException e) {
    System.out.println(""failed to connect to MetaStore, re-trying..."");
    try {
      Thread.sleep(1000);
    } catch (InterruptedException ignore) {}
  }
}

try {
  List<Partition> parts =
      client.get_partitions_by_filter(""default"", ""partitioned_nation"",
          ""pt < '2'"", (short) -1);
  for (Partition part : parts) {
    System.out.println(part.getSd().getLocation());
  }
} catch (Exception te) {
  te.printStackTrace();
}
{code}

The same exception would be thrown.  
A null JavaTypeMapping was passed into org.datanucleus.store.mapped.mapping.MappingHelper.(int initialPosition, JavaTypeMapping mapping), that caused NPE.

After digged into the datanucleus source, I found that the null value was born in the constructor of org.datanucleus.store.mapped.expression.SubstringExpression.  see
{code} 
    /**
     * Constructs the substring
     * @param str the String Expression
     * @param begin The start position
     * @param end The end position expression
     **/   
    public SubstringExpression(StringExpression str, NumericExpression begin, NumericExpression end)
    {
        super(str.getQueryExpression());

        st.append(""SUBSTRING("").append(str).append("" FROM "")
            .append(begin.add(new IntegerLiteral(qs, mapping, BigInteger.ONE)))
            .append("" FOR "").append(end.sub(begin)).append(')');
    }
{code}

The field mapping hasn't been instanced at that moment.


How do you deal with such a external bug?




","25/Nov/11 08:21;coderplay;@Ashutosh

I think hcatalog 0.3 has the same problem since it use the same thrift method get_partitions_by_filter.",25/Nov/11 08:23;ashutoshc;One possibility is to upgrade to latest datanucleus version and see if that fixes it. Some work for it is going on at HIVE-2084,"25/Nov/11 08:43;coderplay;@Ashutosh

It's a risk we try that version, datanucleus is always not a stable software. we can't expect whether new bugs would be brought. 
","25/Nov/11 09:42;ashutoshc;That is true that new version may bring along new bugs, but it will be good to know if it is fixing old bugs.","25/Nov/11 10:50;coderplay;@Ashutosh

I've solve the problem by injecting a new org.datanucleus.store.rdbms.adapterMySQLAdapter into datanucleus-rdbms-2.0.3.jar, because this phenomenon is only appear in mysql backend metastore.

It's not graceful, but it works :)","17/Jan/12 23:36;aniket486;Min, have you opened a jira for this?! This is really a major bug...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bin/pig does not work with bash 3.0,PIG-2335,12529004,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,azaroth,thejas,thejas,27/Oct/11 03:07,26/Apr/12 20:32,14/Mar/19 03:07,02/Nov/11 17:57,0.9.0,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"With bash 3.0, bin/pig fails with the error -
./pig: line 52: syntax error near unexpected token `""$f""'
./pig: line 52: `        remaining+=(""$f"")'

This is because the += syntax was added in bash 3.1 . 
RHEL 4 machines come with bash 3.0 .",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Nov/11 14:21;azaroth;PIG-2335.1.patch;https://issues.apache.org/jira/secure/attachment/12501963/PIG-2335.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-02 14:21:37.02,,,no_permission,,,,,,,,,,,,214864,,,,Wed Nov 02 17:57:02 UTC 2011,,,,,,,0|i0h267:,97614,,,,,,,,,,02/Nov/11 14:21;azaroth;Reverted to a syntax compatible with bash 3.0,02/Nov/11 17:41;daijy;+1,02/Nov/11 17:57;azaroth;Committed both to 0.10 branch and trunk!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
META-INF/*.SF files in REGISTERed jars can cause problems,PIG-2333,12528717,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,egh,egh,25/Oct/11 17:00,26/Apr/12 20:33,14/Mar/19 03:07,16/Feb/12 20:37,0.10.0,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"I am parsing data using tika, which depends on a jar that contains a META-INF/*.SF (signature) file. This file contains a signature for the MANIFEST.MF file, which is obviously different when pig creates its jar. Processing works locally, but fails on a hadoop cluster.

I am attaching a patch that skips adding any zip entry that starts with META-INF and ends with .SF which fixes the problem I have.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Oct/11 17:39;egh;pig-jar.patch;https://issues.apache.org/jira/secure/attachment/12500727/pig-jar.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-26 05:27:57.788,,,no_permission,,,,,,,,,,,,214577,,,,Thu Feb 16 20:37:43 UTC 2012,,,Patch Available,,,,0|i0h25r:,97612,,,,,,,,,,"26/Oct/11 05:27;daijy;Looks good, but let's wait for PIG-2318 check in. PIG-2318 change the way JarManager works. Need to make sure this patch works after PIG-2318.","26/Oct/11 05:43;egh;Thanks, Daniel. That makes sense.","26/Oct/11 17:08;dvryaboy;I think that since we stop repackaging jars in PIG-2318, this patch will simply become unnecessary? Erik, can you try your workflow with trunk + 2318?",26/Oct/11 21:16;egh;Hi Dmitriy - the PIG-2318 patch seems to fix the issue! thanks.,26/Oct/11 21:24;ashutoshc;Resolved as part of PIG-2138,02/Nov/11 22:51;ashutoshc;Reopening since PIG-2318 didn't get checked-in.,"03/Nov/11 05:00;dvryaboy;Ashutosh I believe PIG-2010, which we are pursuing instead of 2138, will also fix this in the same fashion.",03/Nov/11 16:24;ashutoshc;Oh..k Lets keep this open till PIG-2010 goes in.,"16/Feb/12 20:37;dvryaboy;PIG-2010 is now in trunk, so I believe this issue is fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BinStorage in LOAD statement failing when input has curly braces,PIG-2331,12528251,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xutingz,anitharaju,anitharaju,21/Oct/11 10:35,14/Dec/11 21:51,14/Mar/19 03:07,14/Dec/11 21:50,0.9.0,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"Hi,

When one uses curly braces in load statement when BinStorage() is used, it fails with the following exception

{code}
Pig Stack Trace
---------------
ERROR 2245:
<file rep1.pig, line 1, column 4> Cannot get schema from loadFunc org.apache.pig.builtin.BinStorage

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias a
        at org.apache.pig.PigServer.openIterator(PigServer.java:901)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:655)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:303)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:188)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:164)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)
        at org.apache.pig.Main.run(Main.java:561)
        at org.apache.pig.Main.main(Main.java:111)
Caused by: org.apache.pig.PigException: ERROR 1002: Unable to store alias a
        at org.apache.pig.PigServer.storeEx(PigServer.java:1000)
        at org.apache.pig.PigServer.store(PigServer.java:963)
        at org.apache.pig.PigServer.openIterator(PigServer.java:876)
        ... 7 more
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2245:
<file rep1.pig, line 1, column 4> Cannot get schema from loadFunc org.apache.pig.builtin.BinStorage
        at org.apache.pig.newplan.logical.relational.LOLoad.getSchemaFromMetaData(LOLoad.java:154)
        at org.apache.pig.newplan.logical.relational.LOLoad.getSchema(LOLoad.java:109)
        at org.apache.pig.newplan.logical.relational.LOStore.getSchema(LOStore.java:68)
        at org.apache.pig.newplan.logical.visitor.SchemaAliasVisitor.validate(SchemaAliasVisitor.java:60)
        at org.apache.pig.newplan.logical.visitor.SchemaAliasVisitor.visit(SchemaAliasVisitor.java:84)
        at org.apache.pig.newplan.logical.relational.LOStore.accept(LOStore.java:77)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.PigServer$Graph.compile(PigServer.java:1659)
        at org.apache.pig.PigServer$Graph.compile(PigServer.java:1653)
        at org.apache.pig.PigServer$Graph.access$200(PigServer.java:1378)
        at org.apache.pig.PigServer.storeEx(PigServer.java:995)
        ... 9 more
Caused by: java.io.IOException: java.net.URISyntaxException: Illegal character in path at index 72: hdfs://xyz/user/anithar/input/part-m-0000{0,1}
        at org.apache.pig.builtin.BinStorage.getSchema(BinStorage.java:377)
        at org.apache.pig.newplan.logical.relational.LOLoad.getSchemaFromMetaData(LOLoad.java:150)
        ... 20 more
Caused by: java.net.URISyntaxException: Illegal character in path at index 72: hdfs://xyz/user/anithar/input/part-m-0000{0,1}
        at java.net.URI$Parser.fail(URI.java:2809)
        at java.net.URI$Parser.checkChars(URI.java:2982)
        at java.net.URI$Parser.parseHierarchical(URI.java:3066)
        at java.net.URI$Parser.parse(URI.java:3014)
        at java.net.URI.<init>(URI.java:578)
        at org.apache.pig.builtin.BinStorage.getSchema(BinStorage.java:375)
        ... 21 more
{code}

Script

{code}
a = load 'input/part-m-0000{0,1}' using BinStorage();
dump a;
{code}

 The script runs well in 0.8 version. 

Regards,
Anitha
 
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Dec/11 00:26;xutingz;PIG-2331.patch;https://issues.apache.org/jira/secure/attachment/12506686/PIG-2331.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-21 19:41:48.823,,,no_permission,,,,,,,,,,,,97605,,,,Wed Dec 14 21:51:22 UTC 2011,,,,,,,0|i0h25j:,97611,,,,,,,,,,21/Oct/11 19:41;olgan;This looks like a regression since it works on 0.8. Can somebody take a look please,"21/Oct/11 21:06;thejas;The regression caused by changes in PIG-1865 (load data from a different namenode), where 'new URI(loc)' is called to connect to a different file system. ","21/Oct/11 21:26;olgan;Thejas, thanks for quick investigation! How complex will the change need to be?","22/Oct/11 18:48;thejas;The file name string is converted into an URI, which is then used to create a FileSystem object. But the curly braces are not part of a valid URI. 
One way to get a valid URI in such cases would be to expand the glob pattern in file name, but unfortunately that code is not exposed through a public API. So the code will have to copied to pig or accessed through reflection.
","31/Oct/11 18:15;thejas;Hadoop provides a way to get a valid uri from this location String, - (new org.apache.hadoop.fs.Path(location)).toUri().  That would be a clean way to fix this. 

","06/Dec/11 01:44;olgan;Hi Xuting, please, take a look. Thejas will be helping you with this one - reach out to him if need help","09/Dec/11 00:27;xutingz;I have modified the code according to Thejas's suggestion and added a unit test. test-commit has been run successfully on 0.9, 0.10 and trunk.","14/Dec/11 21:50;thejas;+1 Patch committed to trunk and 0.10 branch.
I made a minor change in the test case before checking in the patch - replaced ""i=i+1;"" with ""i++;"" .","14/Dec/11 21:51;thejas;Xuting,Thanks for the contribution!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig minicluster tests can not be run from eclipse,PIG-2326,12527787,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,julienledem,julienledem,julienledem,19/Oct/11 17:08,22/Feb/13 04:54,14/Mar/19 03:07,19/Jan/12 06:48,,,,,,,,0.11,,,,,,,1,,,,,,,,,,,,,"Some of the setup of the minicluster tests are in the ant config hence they don't run in eclipse.
In particular:
System.setProperty(""hadoop.log.dir"", ""build/test/logs"");
hadoop-site.xml in the classpath",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Jan/12 06:43;daijy;PIG-2326-3.patch;https://issues.apache.org/jira/secure/attachment/12511095/PIG-2326-3.patch,19/Oct/11 17:11;julienledem;PIG-2326.patch;https://issues.apache.org/jira/secure/attachment/12499715/PIG-2326.patch,23/Dec/11 01:14;julienledem;PIG-2326_a.patch;https://issues.apache.org/jira/secure/attachment/12508477/PIG-2326_a.patch,23/Dec/11 23:31;julienledem;PIG-2326_b.patch;https://issues.apache.org/jira/secure/attachment/12508580/PIG-2326_b.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2011-11-03 14:52:44.383,,,no_permission,,,,,,,,,,,,89284,Reviewed,,,Tue Aug 07 18:49:25 UTC 2012,,,Patch Available,,,,0|i0h24n:,97607,"After this patch, Pig does not create ~/pigtest any more when running unit test",,,,,,,,,19/Oct/11 17:11;julienledem;PIG-2326.patch adds the missing required System.property and changes the location of the hadoop-site.xml so that it works from eclipse without additional setup,03/Nov/11 14:52;azaroth;+1,"20/Dec/11 17:22;azaroth;Hi Julien,
I think you can commit this patch if you want.",22/Dec/11 22:15;dvryaboy;+1 but please remove dead code instead of leaving it commented out.,23/Dec/11 01:12;julienledem;Here is a new patch (PIG-2326_a.patch) addressing Dmitriy's comment and applying the same to the 0.23 shim layer.,"23/Dec/11 14:11;azaroth;Now that I think of it, this makes the tests run through eclipse slightly different from the ones run using ant.
This could generate confusion.

I would also change build.xml (also in piggybank and penny) to reference hadoop-site.xml from build/classes:
{code}
./build.xml:    <property name=""junit.hadoop.conf"" value=""${user.home}/pigtest/conf/""/>
./contrib/piggybank/java/build.xml:    <property name=""junit.hadoop.conf"" value=""${user.home}/pigtest/conf/""/>
./contrib/penny/java/build.xml:    <property name=""junit.hadoop.conf"" value=""${user.home}/pigtest/conf/""/>
{code}

And also change the docs:
{code}
./src/docs/src/documentation/content/xdocs/pigunit.xml:          The default value is ~/pigtest/conf.
./src/docs/src/documentation/content/xdocs/test.xml:          The default value is ~/pigtest/conf.
{code}
","23/Dec/11 23:23;julienledem;Some more files refering to pigtest/conf:

./build/classes/docs/src/documentation/content/xdocs/pigunit.xml:          The default value is ~/pigtest/conf.
./build/classes/docs/src/documentation/content/xdocs/test.xml:          The default value is ~/pigtest/conf.
./build.xml:    <property name=""junit.hadoop.conf"" value=""${user.home}/pigtest/conf/""/>
./CHANGES.txt:PIG-2219: Pig tests fail if ${user.home}/pigtest/conf does not already exist (cwsteinbach via gates)
./contrib/penny/java/build.xml:    <property name=""junit.hadoop.conf"" value=""${user.home}/pigtest/conf/""/>
./contrib/piggybank/java/build.xml:    <property name=""junit.hadoop.conf"" value=""${user.home}/pigtest/conf/""/>
./contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestPathPartitioner.java:    File oldConf = new File(System.getProperty(""user.home"")+""/pigtest/conf/hadoop-site.xml"");
./contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestPathPartitionHelper.java:    File oldConf = new File(System.getProperty(""user.home"")+""/pigtest/conf/hadoop-site.xml"");
./src/docs/src/documentation/content/xdocs/pigunit.xml:          The default value is ~/pigtest/conf.
./src/docs/src/documentation/content/xdocs/test.xml:          The default value is ~/pigtest/conf.
./src/org/apache/pig/backend/hadoop/datastorage/ConfigurationUtil.java:            // so pigtest/conf/hadoop-site.xml contains such entry. This prevents some tests from 
./test/org/apache/pig/pigunit/MiniClusterRunner.java: *       System.getProperty(""user.home""), ""pigtest/conf/""</li>
./test/org/apache/pig/test/TestNativeMapReduce.java:    // the presence of ~/pigtest/conf/hadoop-site.xml created by MiniCluster

I'm updating the patch accordingly. You'll notice that some tests delete the file to avoid having it on the classpath.
The root problem is Pig checking for the existence of hadoop-site.xml on the classpath in Map/Reduce mode ( in org.apache.pig.backend.hadoop.executionengine.HExecutionEngine)
This makes unit testing hard as we cannot just pass Properties to PigServer and use the ""Hadoop local"" mode for tests. Tests step on each other through the config file.

","24/Dec/11 07:53;daijy;+1. I like the relocation of ~/pigtest/conf/hadoop-site to build/classes. 

The patch does not contain the change for hadoop 23 part. I can add it.","24/Dec/11 09:16;azaroth;bq. This makes unit testing hard as we cannot just pass Properties to PigServer and use the ""Hadoop local"" mode for tests. Tests step on each other through the config file.

Hi Julien,
can we simply delete hadoop-site.xml in shutdownMiniMrClusters(), so that MiniCluster is ""self contained""?
Would that solve the issue?","25/Dec/11 07:54;daijy;For some background of hadoop-site.xml checking in mapreduce mode, if not doing that, Pig would launch normally but into an inconsistent state, some command will run as if it is in local mode, and some command will fail. To avoid that, we mandate the existence of hadoop-site.xml in mapreduce mode.","09/Jan/12 21:55;julienledem;Hi Daniel,
That means there are places in Pig where it does not take into account properties passed to PigServer. Probably in static initializers and similar.
Mandating hadoop-site.xml does not fix those bugs.","09/Jan/12 22:07;daijy;Hi, Julien,
It does not take hadoop-site.xml. It does take Pig properties. What do you mean by static initializer?","09/Jan/12 23:35;julienledem;Sorry, I'm just guessing here. I need to investigate more. I was wondering if the requirement for the file to be there was because some piece of code was statically initializing a new configuration without getting passed the properties. The behavior you describe makes me think of this.","12/Jan/12 00:43;dvryaboy;Daniel, is there a test for the commands you mention will fail if we fix this issue? 
Seems like there's a bug there to begin with, and we should fix both this classpath thing and the commands that are otherwise inconsistent.","12/Jan/12 01:36;daijy;There are seems several threads in our discussion. I want to make sure we understand clearly:
1. The patch itself. I am totally fine with it if tests pass

2. For the reason why we check existence hadoop-site.xml in mapreduce mode
Yes, remove this check will simplify MiniCluster test. However, I only want to solve one problem: If user run pig in mapreduce mode, but he forget putting config file in classpath, pig will remind user rather than enter into an agnostic mode. By agnostic mode, I mean some Pig operator run, but some are not (like order by).","12/Jan/12 03:05;daijy;I checked again it seems much better on current hadoop in that:
1. If Pig does not find hadoop config file, it is actually run as local mode, much better than agnostic mode previously (more test is needed to verify this is always the case)
2. bin/pig script has been improved, it will use HADOOP_HOME/HADOOP_CONF_DIR, the chance user get it wrong is drastically reduced

Based on these, I am fine to drop this check on trunk.","12/Jan/12 04:19;julienledem;Hi Daniel,
1) yes we should focus on just running tests in eclipse on this JIRA. I created another one to discuss the hadoop-site.xml requirement: PIG-2451
2) I agree with you that we should not get in a mode where the behavior is inconsistent when there is no config available. However it should work in the default Hadoop mode in that case: local file system and in-process execution. We should track down and fix any inconsistent behavior in that case.
I +1 on your last comment (and suggest we follow up on PIG-2451 for this).",19/Jan/12 06:43;daijy;PIG-2326-3.patch resync with trunk.,"19/Jan/12 06:48;daijy;Unit test pass.

test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 29 new or modified tests.
     [exec] 
     [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 524 release audit warnings (more than the trunk's current 517 warnings).

javadoc warning seems unrelated. No new file added, ignore release audit warning.

Patch committed to trunk.","07/Aug/12 18:37;rohini;Daniel,
  Can we have this patch committed to 0.10 also. Without this running multiple builds on same hudson box is very difficult. Builds step on each other and fail due to hadoop-site.xml in user directory. With pig builds taking 10 hrs it wastes a lot of time.","07/Aug/12 18:49;aklochkov;Please consider committing the fix to 0.9.x also, by all the same reasons. Should we create separate tasks for that?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
varargs functions do not get passed the arguments in Python embedding,PIG-2322,12527489,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,julienledem,julienledem,julienledem,17/Oct/11 19:52,22/Feb/13 04:53,14/Mar/19 03:07,16/Feb/12 18:19,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,"from Stan:

Hi,

I have a simple python udf which takes a variable number of (string)
arguments and returns the first non-empty one.
I can see that the udf is invoked from pig but no arguments are being passed.

Here is the script:
=========================================================

#!/usr/bin/python

from org.apache.pig.scripting import *

@outputSchema(""s:chararray"")
def firstNonempty(*args):
   print args
   for v in args:
       if len(v) != 0:
          return v
   return ''

if __name__ == ""__main__"":
  Pig.compile(""""""
  data = load 'input.txt' AS (string1:chararray, string2:chararray);
  data = foreach data generate firstNonempty(string1, string2) as id,
string1, string2;
  dump data;
  """""").bind().runSingle()

===========================================================

Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Feb/12 22:23;daijy;PIG-2322-2.patch;https://issues.apache.org/jira/secure/attachment/12515654/PIG-2322-2.patch,17/Oct/11 20:01;julienledem;PIG-2322.patch;https://issues.apache.org/jira/secure/attachment/12499424/PIG-2322.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-10-19 17:54:50.827,,,no_permission,,,,,,,,,,,,88679,,,,Thu Feb 23 17:16:02 UTC 2012,,,Patch Available,,,,0|i0h23r:,97603,,,,,,,,,,17/Oct/11 20:01;julienledem;PIG-2322.patch fixes it and adds a test based on the example above,19/Oct/11 17:54;alangates;+1,16/Feb/12 18:19;julienledem;Patch allows using vararg UDFs in Jython,22/Feb/12 22:23;daijy;PIG-2322-2.patch fix a unit test related to this patch.,23/Feb/12 17:16;julienledem;Thanks Daniel. That was bad.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Error: ""projection with nothing to reference""",PIG-2320,12527310,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,jcoveney,jcoveney,15/Oct/11 18:30,23/Jan/12 07:31,14/Mar/19 03:07,25/Oct/11 22:43,0.10.0,0.9.0,,,,,,0.10.0,0.9.2,,,,,,0,,,,,,,,,,,,,"a = load 'data1' as (x:int);
b = load 'data2' as (y:int);

val1 = foreach (filter (cogroup a by x, b by y) by COUNT(b) == 0) generate flatten(a);
describe val1;

I found that this script works in 0.8, but messes up in 0.9 and trunk (error is from trunk).

the error:
2011-10-14 13:12:48,526 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1200: Pig script failed to parse:
<file pig9bug.pig, line 4, column 16> pig script failed to validate: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2225: Projection with nothing to reference!

stack trace:
Pig Stack Trace
---------------
ERROR 1200: Pig script failed to parse:
<file pig9bug.pig, line 4, column 16> pig script failed to validate: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2225: Projection with nothing to reference!

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1000: Error during parsing. Pig script failed to parse:
<file pig9bug.pig, line 4, column 16> pig script failed to validate: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2225: Projection with nothing to reference!
    at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1598)
    at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1541)
    at org.apache.pig.PigServer.registerQuery(PigServer.java:541)
    at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:943)
    at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:386)
    at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:188)
    at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:164)
    at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)
    at org.apache.pig.Main.run(Main.java:561)
    at org.apache.pig.Main.main(Main.java:111)
Caused by: Failed to parse: Pig script failed to parse:
<file pig9bug.pig, line 4, column 16> pig script failed to validate: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2225: Projection with nothing to reference!
    at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:178)
    at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1590)
    ... 9 more
Caused by:
<file pig9bug.pig, line 4, column 16> pig script failed to validate: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2225: Projection with nothing to reference!
    at org.apache.pig.parser.LogicalPlanBuilder.buildFilterOp(LogicalPlanBuilder.java:173)
    at org.apache.pig.parser.LogicalPlanGenerator.filter_clause(LogicalPlanGenerator.java:6122)
    at org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1153)
    at org.apache.pig.parser.LogicalPlanGenerator.inline_op(LogicalPlanGenerator.java:5502)
    at org.apache.pig.parser.LogicalPlanGenerator.rel(LogicalPlanGenerator.java:5432)
    at org.apache.pig.parser.LogicalPlanGenerator.foreach_clause(LogicalPlanGenerator.java:12109)
    at org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1373)
    at org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:692)
    at org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:492)
    at org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:378)
    at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:171)
    ... 10 more
================================================================================",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Oct/11 17:18;daijy;PIG-2320-1.patch;https://issues.apache.org/jira/secure/attachment/12500724/PIG-2320-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-25 21:27:35.527,,,no_permission,,,,,,,,,,,,86717,Reviewed,,,Tue Oct 25 22:43:35 UTC 2011,,,,,,,0|i09zh3:,56188,,,,,,,,,,"24/Oct/11 22:35;jcoveney;I've found a much simpler way to replicate

{code}
load 'data1' as (x:int);
a_1 = filter (group a by x) by COUNT(a) > 0;
describe a_1;
{code}","25/Oct/11 21:27;thejas;+1 
(the root cause: input relation index is incremented for joins and group-by statement. This index is used while ProjectExpression is constructed. When the group or join is nested in another statement, this index was not being reset as it should have been.)
",25/Oct/11 22:43;daijy;Patch committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect results for FILTER *** BY ( *** OR ***) with FilterLogicExpressionSimplifier optimizer turned on,PIG-2316,12526297,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,knoguchi,wenfengbx,wenfengbx,08/Oct/11 00:58,18/Sep/13 04:10,14/Mar/19 03:07,11/Oct/11 23:28,0.8.0,0.8.1,0.9.0,0.9.1,,,,0.10.0,0.8.2,0.9.2,,,,,0,,,,,,,,,,,,,"An example for this bug: 

cat weird.txt
1,a
2,b
3,c

When running pig with the following statements:

A = LOAD 'weird.txt' using PigStorage(',') AS (col1:int,col2);
B = FILTER A BY ((col1==1) OR (col1 != 1));
DUMP B;

I expect to get the result of all three rows back, but I receive only two rows.

(2,b)
(3,c)

When we start pig with optimizer turning off.

pig -optimizer_off All

With optimizer turning off, we get the expected results and I get three rows for the same statements.

(1,a)
(2,b)
(3,c)

--------------------------------------------------------

This bug was test on: 

pig-0.9.1, 
pig-0.9.0, 
pig-0.8.1, 
pig-0.8.0

All produced same incorrect results.

--------------------------------------------------------

When looked at the logical plan for this example, we found FilterlogicExpressionSimplifier optimizer produced incorrect logical plan. So we guess the bug is caused by FilterlogicExpressionSimplifier optimizer. ",,,,,,,,,,,,,,,,,,,PIG-3465,,,,,,,,,,,,,11/Oct/11 17:44;thejas;pig-2316-08-v3.txt;https://issues.apache.org/jira/secure/attachment/12498630/pig-2316-08-v3.txt,11/Oct/11 00:48;thejas;pig-2316-09-v3.txt;https://issues.apache.org/jira/secure/attachment/12498497/pig-2316-09-v3.txt,10/Oct/11 21:27;knoguchi;pig-2316-trunk-v1.txt;https://issues.apache.org/jira/secure/attachment/12498473/pig-2316-trunk-v1.txt,11/Oct/11 00:47;thejas;pig-2316-trunk-v3.txt;https://issues.apache.org/jira/secure/attachment/12498496/pig-2316-trunk-v3.txt,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2011-10-08 02:29:24.98,,,no_permission,,,,,,,,,,,,50555,,,,Tue Oct 11 23:28:01 UTC 2011,,,,,,,0|i0h22v:,97599,"FilterLogicExpressionSimplifier optimization rule is disabled by default. To enable the optimization rule, set the property pig.exec.filterLogicExpressionSimplifier to true.
",,,,,,,,,"08/Oct/11 02:29;knoguchi;Huanyu later discovered that result is only incorrect when both the left variables and the right values are the same.
bq. ((col1==1) OR (col1 != 1));

Since this is not a common query, lowering priority.","09/Oct/11 07:37;knoguchi;Our user gave us a more realistic testcase.  
Looks serious.

{noformat} 
-bash-3.2$ cat weird.txt 
1,a
1,c
2,b
3,c
-bash-3.2$ cat test2.pig 
A = LOAD 'weird.txt' using PigStorage(',') AS (col1:int,col2);
B = FILTER A BY ((col1==1 AND col2=='c') OR (col1 != 1));
DUMP B;

-bash-3.2$ pig -x local  test2.pig  
(2,b)
(3,c)
-bash-3.2$
-bash-3.2$ pig -x local  -optimizer_off ALL test2.pig 
(1,c)
(2,b)
(3,c)
-bash-3.2$ 
{noformat}

","09/Oct/11 07:52;knoguchi;It seems like 'equals' part is failing inside handleComparison method in LogicalExpressionSimplifier.

{noformat}
else if (isEqual1 && isNotEqual2) {
    if (val1.equals(val2)) return Exclusive;
{noformat}
Since val1,val2 are ConstantExpression, should it be calling 
  val1.isEqual(val2) 
or
  val1.getValue().equals(val2.getValue()) ?

This patch does the latter.  
(My understanding of pig is very limited so this could be way off.)

","10/Oct/11 21:01;thejas;{code}
Applying pig-2316-trunk-v1.txt triggers another bug. For the following filter clause, note that filter plan in MR plan is incomplete.
 

B = FILTER A BY ((col1==1) OR (col1 != 2));

Filter in MR plan - 

B: Store(fakefile:org.apache.pig.builtin.PigStorage) - scope-11
|
|---B: Filter[bag] - scope-7
    |   |
    |   Not Equal To[boolean] - scope-10
    |   |
    |   |---Project[int][0] - scope-8
    |   |
    |   |---Constant(2) - scope-9
    |
    |---A: New For Each(false,false)[bag] - scope-6

{code}

pig-2316-trunk-v2.txt has the fix for this issue.
",10/Oct/11 21:25;thejas;Koji pointed out that the logical transformation in above case is actually correct. Deleting the v2 patch I submitted. ,10/Oct/11 21:27;knoguchi;Resubmitting the original patch since I forgot to grant license on the original one.,"10/Oct/11 21:27;thejas;The LogicalExpressionSimplifier rules are complex and it has a large number of them. This is the fourth bug originating from this optimization rule that causes correctness issues, and that is related to the complexity of this rule. It is hard to understand and maintain this code. I don't expect these rules to show enough performance gains to justify these costs (complexity, maintainability, chances of bugs).

I think this rule should be disabled by default in 0.9.next and 0.10. In next versions of pig, we can extract simpler rules from this one and have more exhaustive test coverage before turning it on by default.
","10/Oct/11 21:43;viraj;Thejas, I agree with your comments. I also agree that we should disable this optimization, for the next releases of Pig, since we are getting wrong results without any warnings? At the same time, what is the speedup in script runtime due to this rule? ","10/Oct/11 22:35;thejas;bq. what is the speedup in script runtime due to this rule?
I think most queries would not benefit from this optimization rule. Among the one that do, I doubt if there is going to be a noticeable improvement in runtime. I am also not aware of any performance benchmarks that have been done for this rule.

",11/Oct/11 00:47;thejas;pig-2316-trunk-v3.txt - patch for trunk to have FilterLogicExpressionSimplifier disabled by default. Incorporates Koji's changes in v1.txt .,11/Oct/11 00:48;thejas;pig-2316-09-v3.txt - v3 version for 0.9 branch,"11/Oct/11 01:00;olgan;Thejas, we also need a patch for 0.8, thanks",11/Oct/11 17:44;thejas;pig-2316-08-v3.txt - patch for 0.8 branch,"11/Oct/11 21:04;olgan;+1 on the pateches.

The name that we are using to specify exclusion rules (optimizerRules) is confusing but we do not need to fix it in this patch","11/Oct/11 21:27;thejas;unit tests passed for 0.9 branch. patch committed to 0.9 branch. Running tests for 0.8 branch and trunk.
",11/Oct/11 23:28;thejas;tests passed. patch committed to 0.8 branch and trunk as well.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make as clause work in generate,PIG-2315,12526132,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,olgan,olgan,07/Oct/11 00:01,21/Jun/17 09:15,14/Mar/19 03:07,14/Jun/16 19:17,,,,,,,,0.17.0,,,,,,,5,,,,,,,,,,,,,"Currently, the following syntax is supported and ignored causing confusing with users:

A1 = foreach A1 generate a as a:chararray ;

After this statement a just retains its previous type",,,,,,,,,,,,,,,,,,,PIG-4933,,,,,,,,,,,,,06/May/16 21:38;knoguchi;PIG-2315-1-rebase.patch;https://issues.apache.org/jira/secure/attachment/12802743/PIG-2315-1-rebase.patch,03/Oct/13 21:13;daijy;PIG-2315-1.patch;https://issues.apache.org/jira/secure/attachment/12606642/PIG-2315-1.patch,02/Oct/13 19:46;daijy;PIG-2315-1.patch;https://issues.apache.org/jira/secure/attachment/12606425/PIG-2315-1.patch,09/May/16 19:06;knoguchi;pig-2315-2-after-rebase.patch;https://issues.apache.org/jira/secure/attachment/12803055/pig-2315-2-after-rebase.patch,10/May/16 19:15;knoguchi;pig-2315-3-merged.patch;https://issues.apache.org/jira/secure/attachment/12803295/pig-2315-3-merged.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-10-24 11:34:54.5,,,no_permission,,,,,,,,,,,,49798,Reviewed,,,Wed Jun 22 17:13:46 UTC 2016,,,,,,,0|i0er0n:,84139,,,,,,,,,,"24/Oct/11 11:34;azaroth;What is the desired solution?
To make the aforementioned syntax work as a cast or to allow only explicit casting?
E.g.
{code}
A1 = foreach A1 generate (chararray) a;
{code}
","24/Oct/11 17:35;daijy;I had a discussion with Thejas before, we want to perform the ""as"" cast after we evaluate the generated item:

A1 = foreach A1 generate (int)a as a:chararray;
=> A1 = foreach A1 generate (chararray)((int)a) as a;","16/Nov/11 19:20;prkommireddi;How would this work when a Tuple is being mapped using ""AS"" 

{code}
vLogFields = FOREACH vLogs GENERATE FLATTEN(LFV(TOTUPLE(*), ('timestamp', 'runTime', 'cpuTime'))) as 
            (ts, runTime, cpuTime);
{code}

Here I call a function LFV which returns a Tuple which is being mapped to {ts, runTime, cpuTime}. ","16/Nov/11 23:25;azaroth;I assume that as long as you don't cast it to some types this should have no effect on your code.
In your example you are just renaming fields, not casting them.","18/Nov/11 19:03;prkommireddi;Apologies, it should have read 
{code}
vLogFields = FOREACH vLogs GENERATE FLATTEN(LFV(TOTUPLE(*), ('timestamp', 'runTime', 'cpuTime'))) as 
            (ts:chararray, runTime:double, cpuTime:double);
{code}


","18/Nov/11 19:06;prkommireddi;Return type of UDF ""LFV"" is bytearray. The issue here is that when I pass the field ""ts"" to a MIN function, or ""runTime"" to a SUM (after a group by) the scripts errors out with a ClassCastException (Cannot convert bytearray to String/Double)","19/Apr/13 20:49;metaruslan;As a user I would desire the way as it was suggested here:
https://issues.apache.org/jira/browse/PIG-2216
to allow just one syntax for casting (and forbid/deprecate the other).

Daniel, don't you think that:
{quote}
A1 = foreach A1 generate (int)a as a:chararray;
=> A1 = foreach A1 generate (chararray)((int)a) as a;
{quote}
will make things more complicated?","02/May/13 21:56;daijy;Agree. However, we cannot break backward incompatibility by disallowing ""()"" style cast. We definitely don't encourage using different style of cast. We can mark ""()"" deprecate though.","06/May/13 16:42;metaruslan;Daniel, I think that deprecating/removing the ""cast in the as clause"" is easier, because it is not working anyway. I guess the ""()"" should stay.

I also have a suggestion to make this issue a duplicate of PIG-2216 instead of PIG-2216 being a duplicate of this issue. It seems that the description of PIG-2216 explains just everything and does not cause confusion.","03/Sep/13 21:24;knoguchi;> because it is not working anyway.
>
There's at least one case it's working for our users.

{noformat}
a = load 'input.txt' as (nb:bag{});
b = foreach a generate flatten(nb) as (year, name:bytearray);
c = filter b by name == 'user1';
dump c;
{noformat}

Above case works. But without the ':bytearray' in relation b, it fails.

{noformat}
a = load 'input.txt' as (nb:bag{});
b = foreach a generate flatten(nb) as (year, name);
c = filter b by name == 'user1';
dump c;
{noformat}
""Front End: ERROR 1052: Cannot cast bytearray to chararray""

Please keep the first case valid.  (Thanks [~fuding] for this example.)
Error message in the second case is misleading that it's actually trying to typecast NULL to chararray.

","02/Oct/13 19:46;daijy;Attach a patch to fix the issue by adding a cast only foreach below:

A1 = foreach A1 generate a as a:chararray;
=>
A1 = foreach A1 generate a;
A1 = foreach A1 generate (chararray)a;

b = foreach a generate flatten(nb) as (year, name:chararray);
=>
b = foreach a generate flatten(nb) as (year, name);
b = foreach b generate year, (chararray)name;

vLogFields = FOREACH vLogs GENERATE FLATTEN(LFV(TOTUPLE(*), ('timestamp', 'runTime', 'cpuTime'))) as (ts:chararray, runTime:double, cpuTime:double);
=> vLogFields = FOREACH vLogs GENERATE FLATTEN(LFV(TOTUPLE(*), ('timestamp', 'runTime', 'cpuTime'))) as (ts, runTime, cpuTime);
vLogFields = FOREACH vLogFields GENERATE (chararray)ts, (double)runTime, (double)cpuTime;",03/Oct/13 21:13;daijy;Fix unit test failures.,15/Jul/14 16:15;olgan;Are there plans to get this one into Pig 14?,"15/Jul/14 18:57;daijy;Yes, the patch is ready and I need a review.","06/May/16 21:38;knoguchi;This has been on my todo list forever... Finally took a look at the patch.

Uploading a rebased patch to trunk. (PIG-2315-1-rebase.patch)

[~daijy], can you take a look?  

One feedback.  
With current patch, added typecast foreach is no-op.
For example 

{code:title=input.txt}
1.1
2.2
3.3
4.6
5.7
{code}

{code:title=test.pig}
A = load 'input.txt' as a1;
B = FOREACH A generate (double) a1 as (a2:int) , 20151205 as (generated_date:chararray);
store B into '/tmp/deleteme';
{code}

Extra typecast foreach is added and logical plan looks like 
{noformat}
#-----------------------------------------------
# New Logical Plan:
#-----------------------------------------------
B: (Name: LOStore Schema: a2#1:int,generated_date#7:chararray)
|
|---B: (Name: LOForEach Schema: a2#1:int,generated_date#7:chararray)
    |   |
    |   (Name: LOGenerate[false,false] Schema: a2#1:int,generated_date#7:chararray)ColumnPrune:OutputUids=[1, 7]ColumnPrune:InputUids=[1, 7]
    |   |   |
    |   |   (Name: Cast Type: int Uid: 1)
    |   |   |
    |   |   |---a2:(Name: Project Type: int Uid: 1 Input: 0 Column: 0)
    |   |   |
    |   |   (Name: Cast Type: chararray Uid: 7)
    |   |   |
    |   |   |---generated_date:(Name: Project Type: chararray Uid: 7 Input: 1 Column: 0)
    |   |
    |   |---(Name: LOInnerLoad[0] Schema: a2#1:int)
    |   |
    |   |---(Name: LOInnerLoad[1] Schema: generated_date#7:chararray)
    |
    |---****B: (Name: LOForEach Schema: a2#1:int,generated_date#11:chararray) 
        |   |
        |   (Name: LOGenerate[false,false] Schema: a2#1:int,generated_date#11:chararray)
        |   |   |
        |   |   (Name: Cast Type: double Uid: 1)
        |   |   |
        |   |   |---a1:(Name: Project Type: bytearray Uid: 1 Input: 0 Column: (*))
        |   |   |
        |   |   (Name: Constant Type: int Uid: 11)
        |   |
        |   |---(Name: LOInnerLoad[0] Schema: a1#1:bytearray)
        |
        |---A: (Name: LOLoad Schema: a1#1:bytearray)RequiredFields:null
{noformat}

Original foreach 
{{B: (Name: LOForEach Schema: a2#1:int,generated_date#11:chararray)}} 
should have been 
{{B: (Name: LOForEach Schema: a2#1:double,generated_date#11:int)}}

otherwise these extra typecasts are only doing double2double and chararray2chararray.

I believe we need to drop the types from userdefinedschema for the _original foreach_ since the inserted foreach-typecast handles that part.","09/May/16 19:06;knoguchi;bq. I believe we need to drop the types from userdefinedschema for the original foreach since the inserted foreach-typecast handles that part.

Attaching a patch that does this.  This patch {{pig-2315-2-after-rebase.patch}} applies after {{PIG-2315-1-rebase.patch}}.

It contains 
* Taking away unnecessary comment I've left from original rebase patch.
* Adding type check that would verify that the original foreach has the right schema (ignoring the userdefined ones). 
* Two more test cases showing how this bug can produce incorrect results.","09/May/16 19:08;knoguchi;bq. It contains ...

Forgot to mention, it also contains the suggested change in dropping userdefinedschema types for the original foreach.","10/May/16 19:15;knoguchi;Uploading a merged patch that simply combine 
* {{PIG-2315-1-rebase.patch}} Rebase of Daniel's original patch
and
* {{pig-2315-2-after-rebase.patch}} My suggested change that would drop the userdefined types from the original foreach along with some more tests.","18/May/16 21:33;knoguchi;[~daijy], when you have time, can you take a look at my suggested change to your original patch?  ",18/May/16 21:34;daijy;I will try this Friday.,"01/Jun/16 22:52;daijy;+1.

Also note there is a performance regression in some cases. For example:
{code}
crawl = load 'webcrawl' as (url, pageid);
extracted = foreach crawl generate flatten(REGEX_EXTRACT_ALL(url, '(http|https)://(.*?)/(.*)')) as (protocol:chararray, host:chararray, path:chararray);
{code}

Here the users just try to give additional information to Pig since REGEX_EXTRACT_ALL didn't declare types inside tuple and not intend to cast. With the change, Pig force a cast and there is no way to avoid that. The performance hit should be small and I believe it worth to clarify the syntax.","14/Jun/16 19:17;knoguchi;Thanks [~daijy] !!! 

Committed the changes to trunk (0.17).  ","22/Jun/16 17:13;knoguchi;bq. Committed the changes to trunk (0.17).

After this commit, one test started failing.  Sorry I've missed it.  Created PIG-4933 for tracking.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in ILLUSTRATE trying to get StatusReporter in STORE,PIG-2313,12525555,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,ddaniels888,ddaniels888,03/Oct/11 17:41,26/Apr/12 20:32,14/Mar/19 03:07,03/Nov/11 00:15,0.10.0,0.9.0,,,,,,0.10.0,0.11,,,,,,0,,,,,,,,,,,,,"I'm seeing an NPE trying to do an illustrate on a script.  So far the simplest version of the script that exhibits the issue is:

{code}
raw = LOAD 'data.txt' USING PigStorage() AS (x:int, y:int);
filtered = FILTER raw BY x < 5;
grouped = GROUP filtered BY x;
counted = FOREACH grouped
         GENERATE group AS x,
                  COUNT(filtered) AS the_count;
rmf output;
STORE counted INTO 'output';
{code}

I had to pass a few nested Exceptions along to get it, but the bottom stack trace looks like:

{code}
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.getCounter(TaskInputOutputContext.java:88)
	at org.apache.pig.tools.pigstats.PigStatusReporter.getCounter(PigStatusReporter.java:60)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReducePOStoreImpl.createRecordCounter(MapReducePOStoreImpl.java:121)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore.setUp(POStore.java:108)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.cleanup(PigGenericMapReduce.java:525)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:178)
	at org.apache.pig.pen.LocalMapReduceSimulator.launchPig(LocalMapReduceSimulator.java:222)
	at org.apache.pig.pen.ExampleGenerator.getData(ExampleGenerator.java:257)
	at org.apache.pig.pen.ExampleGenerator.getData(ExampleGenerator.java:275)
	at org.apache.pig.pen.LineageTrimmingVisitor.checkNewBaseData(LineageTrimmingVisitor.java:418)
	... 21 more
{code}

It looks like the IllustratorContext in the hadoop20 PigMapReduce.java shim (line 73) is getting setup with a null StatusReporter.  This seems to be for the Reduce phase.  On the other hand, the PigMapBase.java sets up the IllustratorContext with an IllustratorDummyReporter for the Map phase.

Eventually when the code in MapReducePOStoreImpl line 121 tries to get the reporter, it fails with the NPE.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Oct/11 17:22;daijy;PIG-2313-2.patch;https://issues.apache.org/jira/secure/attachment/12501121/PIG-2313-2.patch,02/Nov/11 23:35;daijy;PIG-2313-3.patch;https://issues.apache.org/jira/secure/attachment/12502070/PIG-2313-3.patch,03/Oct/11 19:36;ddaniels888;PIG-2313-example.tar.gz;https://issues.apache.org/jira/secure/attachment/12497525/PIG-2313-example.tar.gz,03/Oct/11 19:36;ddaniels888;PIG-2313-first_shot.patch;https://issues.apache.org/jira/secure/attachment/12497526/PIG-2313-first_shot.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2011-10-27 17:22:07.932,,,no_permission,,,,,,,,,,,,43834,Reviewed,,,Thu Nov 03 00:15:59 UTC 2011,,,,,,,0|i09zfb:,56180,,,,,,,,,,"03/Oct/11 19:35;ddaniels888;I created a patch that passes an IllustratorDummyReporter into the IlustratorContext in hadoop20 PigMapReduce.Reduce.  This fixes the NPE, but yields a new Exception about being unable to setup the store function:

{code}
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 0: Exception: Unable to setup the store function.
	at org.apache.pig.pen.LineageTrimmingVisitor.checkNewBaseData(LineageTrimmingVisitor.java:420)
	at org.apache.pig.pen.LineageTrimmingVisitor.processLoad(LineageTrimmingVisitor.java:374)
	at org.apache.pig.pen.LineageTrimmingVisitor.processOperator(LineageTrimmingVisitor.java:438)
	... 19 more
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 2081: Unable to setup the store function.
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore.setUp(POStore.java:111)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.cleanup(PigGenericMapReduce.java:525)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:178)
	at org.apache.pig.pen.LocalMapReduceSimulator.launchPig(LocalMapReduceSimulator.java:222)
	at org.apache.pig.pen.ExampleGenerator.getData(ExampleGenerator.java:260)
	at org.apache.pig.pen.ExampleGenerator.getData(ExampleGenerator.java:278)
	at org.apache.pig.pen.LineageTrimmingVisitor.checkNewBaseData(LineageTrimmingVisitor.java:418)
	... 21 more
Caused by: java.io.IOException: File already exists:file:/Users/ddaniels/code/sandbox/jira/output_new/_temporary/_attempt__0000_r_000000_0/part-r-00000
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:228)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:335)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:368)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:484)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:465)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:372)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigTextOutputFormat.getRecordWriter(PigTextOutputFormat.java:98)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReducePOStoreImpl.createStoreFunc(MapReducePOStoreImpl.java:85)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore.setUp(POStore.java:103)
	... 27 more
================================================================================
{code}

Does anyone understand what would cause that?  The Exception above is from running it in local mode, but the same problem happens in MapReduce mode (I checked that the output dir does not exist too).

Also, I tried to add a test in TestExampleGenerator.java in the patch, but it complains ""Internal error. Did not find roots in the physical plan.""  Is there something special that needs to be done to get the store to appear in the plan?","27/Oct/11 17:22;daijy;Hi, Daniels,
Your fix for Reporter is right. We didn't pass reporter to reducer in illustrate. I attached PIG-2313-2.patch also fix the ""store func setup"" issue. I include your test case in patch.",01/Nov/11 23:12;thejas;+1,02/Nov/11 23:35;daijy;PIG-2313-3.patch resync with trunk.,03/Nov/11 00:15;daijy;Patch committed to both trunk and 0.10 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Class cast exception thrown in STRSPLIT even after casting properly,PIG-2311,12525285,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xutingz,anitharaju,anitharaju,30/Sep/11 09:24,26/Apr/12 20:33,14/Mar/19 03:07,01/Dec/11 02:08,0.8.0,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"Hi,

The below script treats 'a' as bytearray thus throwing class cast exception at STRSPLIT

=========
A = load 'input.txt' using PigStorage() as (a);
A1 = filter A by NOT (a is null);
A1 = foreach A1 generate a as a:chararray ;
describe A1;
B = foreach A1 generate STRSPLIT(a);
dump B
=========

Input.txt (space separated)
=========
a b
=========

Here i am casting as ""a as a:chararray"". If you see the output of describe it gives

A1: {a: chararray}

When its passed in STRSPLIT, it treats as a bytearray thus throwing the following exception in the task logs

=======
WARN org.apache.pig.builtin.STRSPLIT: class cast exception at org.apache.pig.builtin.STRSPLIT.exec(STRSPLIT.java:55)
=======

hence giving null output at B.

If we cast 'a' using

=========
A1 = foreach A1 generate (chararray)a as a ;
=========

it gives the correct output.

((a,b))

In both the cases the describe gives 

A1: {a: chararray}

Regards,
Anitha",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Nov/11 00:43;xutingz;PIG-2311-1.patch;https://issues.apache.org/jira/secure/attachment/12504001/PIG-2311-1.patch,18/Nov/11 21:46;xutingz;PIG-2311-2.patch;https://issues.apache.org/jira/secure/attachment/12504271/PIG-2311-2.patch,17/Nov/11 00:02;xutingz;PIG-2311.patch;https://issues.apache.org/jira/secure/attachment/12503995/PIG-2311.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-09-30 16:55:21.124,,,no_permission,,,,,,,,,,,,40988,,,,Thu Dec 01 02:08:26 UTC 2011,,,,,,,0|i0h227:,97596,,,,,,,,,,"30/Sep/11 16:55;olgan;There seems to be actually 2 issues here:

(1) Type information is getting lost
(2) STRSPLIT does not perform conversion from bytearray t0o chararray. It is missing mapping table","30/Sep/11 20:04;daijy;In addition, foreach does achieve the intention.

A1 = foreach A1 generate a as a:chararray ;

a:chararray does not actually do the type conversion. This is a known issue and need clarification on the Pig grammar.","30/Sep/11 20:13;dvryaboy;I think that's supposed to be 
A1 = foreach A1 generate (chararray) a as a:chararray;

The explicit cast will force a conversion.","30/Sep/11 20:29;daijy;Yes, it works. But ""as a:chararray"" gives user the impression that it will do the conversion, which is confusing and should be clarified.","07/Oct/11 00:02;olgan;Lets keep this issue just for STRSPLIT.

I have created a separate ticket for support as syntax in FOREACH: https://issues.apache.org/jira/browse/PIG-2315","28/Oct/11 01:03;daijy;STRSPLIT return a tuple of variable length. We have no way to specify the inner schema now. The query will run once PIG-2315 is fixed. Seems no other working items in the ticket, right?",28/Oct/11 18:11;olgan;But the input schema is always fixed (bytearray or string) so we can create a mapping table to allow both. That's what I was planning to do there,"17/Nov/11 00:04;xutingz;In this patch, the outputschema and the mapping table have been added into STRSPLIT function.
An e2e test case has been added into the nightly.conf.
The test-commite has been successfully run with this changes.
This patch should work on both two versions.
                                                 Xuting","17/Nov/11 01:51;olgan;+1, committed to trunk. branch-10 is next","18/Nov/11 16:40;olgan;Xuting, patch failed to apply to .10 branch. Can you please submit a separate on for 10 and make sure that it applies and that tests run succesfuly, thanks","18/Nov/11 21:50;xutingz;Hi Olga,

    Sorry about that. The PIG-2311-2.patch is for .10 branch and I have run ant test-commit and the new e2e test case successfully. 
                                Xuting","01/Dec/11 02:08;olgan;Changes committed to 0.10 branch. Thanks, Xuting!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bin/pig fail when both pig-0.9.1.jar and pig.jar are in PIG_HOME,PIG-2310,12525223,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,daijy,daijy,daijy,29/Sep/11 18:21,05/Oct/11 17:20,14/Mar/19 03:07,29/Sep/11 22:45,0.10.0,0.9.1,,,,,,0.10.0,0.9.1,,,impl,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Sep/11 18:21;daijy;PIG-2310-1.patch;https://issues.apache.org/jira/secure/attachment/12497031/PIG-2310-1.patch,29/Sep/11 21:41;daijy;PIG-2310-2.patch;https://issues.apache.org/jira/secure/attachment/12497064/PIG-2310-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-09-29 19:02:13.093,,,no_permission,,,,,,,,,,,,40338,Reviewed,,,Thu Sep 29 22:45:09 UTC 2011,,,,,,,0|i09zl3:,56206,,,,,,,,,,"29/Sep/11 19:02;alangates;There's more to this than just the jars being present.  When I tried the patch in the directory /homes/hortonal/src/pig/branches/0.9/2310/branch-0.9 I got:

{code}
 bin/pig -x local
bin/pig: line 68: [: ./bin: binary operator expected
bin/pig: line 262: [: ./bin: binary operator expected
Exception in thread ""main"" java.lang.NoClassDefFoundError: /home/hortonal/src/pig/branches/0/9/2310/branch-0/9/bin////logs
Caused by: java.lang.ClassNotFoundException: .home.hortonal.src.pig.branches.0.9.2310.branch-0.9.bin....logs
        at java.net.URLClassLoader$1.run(URLClassLoader.java:200)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:276)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:251)
        at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:319)
{code}

Notice the way it's mangling my path by replacing dots with slashes.

Also, I get a similar error when I try to run this in a directory without dots.","29/Sep/11 21:41;daijy;The error is because of the CDPATH setting on the target machine. Once CDPATH is set, cd will echo the path. 

Note this is issue is there for a long time and is not related to any recent change.","29/Sep/11 22:31;alangates;+1, looks good, and works both before and after ""ant jar"".",29/Sep/11 22:45;daijy;Patch committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Keyword 'NOT' is wrongly treated as a UDF in split statement,PIG-2309,12525172,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,vivekp,vivekp,vivekp,29/Sep/11 11:18,26/Apr/12 20:32,14/Mar/19 03:07,06/Oct/11 23:33,0.9.0,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"In the below script, the keyword NOT is wrongly interpreted as  UDF. 

register empty.jar;
a = load 'myinput' as (f1:chararray);
SPLIT a INTO a1 IF (EMPTY((chararray)f1) ), a2 IF(NOT(EMPTY((chararray)f1)) );
dump a2;


UDF EMPTY
---------
import java.io.IOException;
import org.apache.pig.FilterFunc;
import org.apache.pig.data.Tuple;
public class EMPTY extends FilterFunc {
	    @Override
	    public Boolean exec(Tuple input) throws IOException {
	            return new Boolean ( ((String)input.get(0)).isEmpty() );
	    }

}

This is issue is observed in 0.9 most likely because of the new parser, Pig 0.8 works fine with this script. 
Would be helpful to know any workarounds in 0.9. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Oct/11 19:06;vivekp;PIG_2309_1.patch;https://issues.apache.org/jira/secure/attachment/12497515/PIG_2309_1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-29 17:48:59.487,,,no_permission,,,,,,,,,,,,39772,,,,Thu Oct 06 23:33:41 UTC 2011,,,,,,,0|i0h21z:,97595,,,,,,,,,,"29/Sep/11 17:48;daijy;As a workaround, remove parenthesis after NOT:

SPLIT a INTO a1 IF (EMPTY((chararray)f1) ), a2 IF(NOT EMPTY((chararray)f1)) ;",03/Oct/11 19:09;vivekp;Attaching an initial patch for 0.10,"06/Oct/11 23:33;thejas;+1
Unit tests passed. Patch committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Jetty version should be updated in .eclipse.templates/.classpath, pig-template.xml and pig.pom as well",PIG-2307,12525067,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,zjshen,zjshen,zjshen,28/Sep/11 15:00,05/Oct/11 17:20,14/Mar/19 03:07,29/Sep/11 22:47,,,,,,,,0.10.0,0.9.1,,,build,,,0,,,,,,,,,,,,,"Jetty version has been updated to 6.1.26, because jetty 6.1.14 startup issue causes unit tests to fail in CI. (https://issues.apache.org/jira/browse/PIG-2299)

However, the corresponding version numbers in .eclipse.templates/.classpath, pig-template.xml and pig.pom has not been updated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Sep/11 15:01;zjshen;PIG-2307.patch;https://issues.apache.org/jira/secure/attachment/12496888/PIG-2307.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-28 23:51:14.878,,,no_permission,,,,,,,,,,,,36735,Reviewed,,,Thu Sep 29 22:47:56 UTC 2011,,,Patch Available,,,,0|i0h21j:,97593,,,,,,,,,,"28/Sep/11 23:51;daijy;Patch committed to trunk. It is too late for 0.9.1 release, but I will commit to 0.9 branch after 0.9.1 release. Thanks Zhijie!","29/Sep/11 22:47;daijy;Since we will reroll 0.9.1 release anyway, commit the patch to 0.9.1 as well.",29/Sep/11 22:47;daijy;Patch committed to 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Some more bin/pig, build.xml cleanup for 0.9.1",PIG-2301,12524474,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,daijy,daijy,daijy,23/Sep/11 21:02,05/Oct/11 17:20,14/Mar/19 03:07,26/Sep/11 20:26,,,,,,,,0.10.0,0.9.1,,,impl,,,0,,,,,,,,,,,,,"There are several more cleanup need to go into 0.9.1:
1. Cleanup bin/pig
2. Use ""HADOOP_PREFIX"" in addition to ""HADOOP_HOME"" to avoid deprecate warning
3. ant default target should build both ""pig.jar"" and ""pig-withouthadoop.jar""
4. add jython.jar into classpath in develop setup",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Sep/11 04:08;daijy;PIG-2301-1.patch;https://issues.apache.org/jira/secure/attachment/12496436/PIG-2301-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-26 17:47:36.756,,,no_permission,,,,,,,,,,,,464,Reviewed,,,Mon Sep 26 20:26:24 UTC 2011,,,,,,,0|i09zlr:,56209,,,,,,,,,,26/Sep/11 17:47;thejas;+1,26/Sep/11 20:26;daijy;Patch committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jetty 6.1.14 startup issue causes unit tests to fail in CI,PIG-2299,12524306,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,thw,thw,22/Sep/11 16:46,05/Oct/11 17:20,14/Mar/19 03:07,26/Sep/11 21:57,0.8.0,,,,,,,0.9.1,,,,build,,,0,,,,,,,,,,,,,"We have observed random unit test failures in our pig CI. Same tests working just fine when run on developer machine).

The cause is a bug in jetty version used by pig, example below and more here:

https://issues.apache.org/jira/browse/HADOOP-6386
https://issues.apache.org/jira/browse/HADOOP-6428
https://issues.apache.org/jira/browse/HADOOP-6528
http://jira.codehaus.org/browse/JETTY-748


The issue is resolved by using jetty 6.1.21 instead of 6.1.14


java.lang.ExceptionInInitializerError
	at org.apache.pig.test.TestAlgebraicInstantiation.&lt;init&gt;(TestAlgebraicInstantiation.java:41)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
Caused by: java.lang.IllegalArgumentException: port out of range:-1
	at java.net.InetSocketAddress.&lt;init&gt;(InetSocketAddress.java:118)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:250)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:202)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:279)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:956)
	at org.apache.hadoop.hdfs.MiniDFSCluster.&lt;init&gt;(MiniDFSCluster.java:275)
	at org.apache.hadoop.hdfs.MiniDFSCluster.&lt;init&gt;(MiniDFSCluster.java:119)
	at org.apache.pig.test.MiniCluster.setupMiniDfsAndMrClusters(MiniCluster.java:49)
	at org.apache.pig.test.MiniGenericCluster.&lt;init&gt;(MiniGenericCluster.java:49)
	at org.apache.pig.test.MiniCluster.&lt;init&gt;(MiniCluster.java:31)
	at org.apache.pig.test.MiniGenericCluster.&lt;clinit&gt;(MiniGenericCluster.java:45)",RHEL CI server,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Sep/11 16:56;thw;PIG-2299_jetty-6.1.26.patch;https://issues.apache.org/jira/secure/attachment/12496141/PIG-2299_jetty-6.1.26.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-22 18:12:15.767,,,no_permission,,,,,,,,,,,,3062,Reviewed,,,Mon Sep 26 21:57:55 UTC 2011,,,Patch Available,,,,0|i0h1zz:,97586,,,,,,,,,,"22/Sep/11 17:55;thw;To confirm, ant test passes in devel and CI environment with the patch above.
","22/Sep/11 18:12;daijy;Newer version(eg:20.204) hadoop fixed this problem. However, since Pig bundles hadoop 20.2, the fix does not goes to Pig. The issue only affects tests. When running on cluster, Pig does not use and will not send jetty to backend.","22/Sep/11 18:34;daijy;Though fix hadoop issue on Pig side doesn't sounds right, but I still think it is better to check in this, reasons are:
1. This block RE folks
2. This change only affects unit test, and all unit tests pass with the patch
3. We don't have plan to upgrade the bundled hadoop version (at least 0.9.1)

Any objection?",26/Sep/11 21:41;olgan;+1 for checking the patch in for the reasons described by Daniel,26/Sep/11 21:57;daijy;Patch committed to trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigStats.isSuccessful returns false if embedded pig script has dump,PIG-2291,12523533,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xutingz,vivekp,vivekp,19/Sep/11 08:55,23/Jan/12 07:31,14/Mar/19 03:07,20/Dec/11 19:27,0.9.0,,,,,,,0.10.0,0.9.2,,,,,,0,,,,,,,,,,,,,"The below is my python script, 
{code}
#! /usr/bin/python
from  org.apache.pig.scripting import Pig

P = Pig.compileFromFile(""""""a.pig"""""")
result = P.bind().runSingle()

if result.isSuccessful():
    print 'Pig job succeeded'
else:
    print 'Pig job failed'
{code}


The below is the pig script embedded (a.pig)
A = LOAD 'a1' USING PigStorage(',') AS (f1:chararray,f2:chararray);
B = GROUP A by f1;
dump B;


For this script execution, even though the job is successful the output printed is 'Pig job failed'
This is because result.isSuccessful() is returning false whenever the pig script is having a dump statement.

If i run the pig script alone, then the error code returned is proper.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Dec/11 19:23;daijy;PIG-2291-2.patch;https://issues.apache.org/jira/secure/attachment/12508121/PIG-2291-2.patch,14/Dec/11 21:50;xutingz;PIG-2291.patch;https://issues.apache.org/jira/secure/attachment/12507441/PIG-2291.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-12-09 00:43:45.002,,,no_permission,,,,,,,,,,,,35728,Reviewed,,,Tue Dec 20 19:27:56 UTC 2011,,,,,,,0|i0h1y7:,97578,,,,,,,,,,"09/Dec/11 00:43;olgan;Xuting, could you, please, take a look, thanks","13/Dec/11 01:03;xutingz;I think this problem is caused by the specialty of dump operation. From the Pig tutorial, it says:

With multi-query exection, you want to use STORE to save (persist) your results. You do not want to use DUMP as it will disable multi-query execution and is likely to slow down execution. (If you have included DUMP statements in your scripts for debugging purposes, you should remove them.)

DUMP Example: In this script, because the DUMP command is interactive, the multi-query execution will be disabled and two separate jobs will be created to execute this script. The first job will execute A > B > DUMP while the second job will execute A > B > C > STORE.

A = LOAD 'input' AS (x, y, z);
B = FILTER A BY x > 5;
DUMP B;
C = FOREACH B GENERATE y, z;
STORE C INTO 'output';

Similarly, adding dump B into the python script will lead to two logical plan to be executed and the second logical plan is empty and will return a unsuccessful status to the exec function in the BoundScript. I am not sure if we need to fix this because this problem seems an ""expected"" behavior of the dump operation.","14/Dec/11 00:28;daijy;In PigServer.execute(), we call ""PigStatsUtil.getEmptyPigStats()"" when execute an empty plan. getEmptyPigStats does have a side effect to clear up the global stats, which I think we should avoid. I think instead of calling ""getEmptyPigStats"", we shall simply return ""new SimplePigStats()"", so the global stats retain. ","14/Dec/11 01:23;xutingz;Hi Daniel, 

   Thanks for the advice! I re-exam the code and the PigStatsUtil.getEmptyPigStats() should be the cause of the problem. However, it seems what the getEmptyStats does is to create a new SimplePigStats() and return it back. How about changing the return stats to : return PigStats.get(), which I think, will return the current PigStats of the PigServer so the global stats retains?

Xuting","14/Dec/11 02:26;daijy;That's fine for me. But seems other code also use getEmptyPigStats, I am not sure whether other code relies on the old behavior of getEmptyPigStats or not. Can you check it out? If not, then go ahead and change it.","14/Dec/11 21:55;xutingz;I have changed the return type to return PigStats.get(); The ant test-commit has been ran successfully on trunk, 0.9 and 0.10 branch. Additional unit test cases are added into TestScriptLanguage and this specific unit test has ran successfully on 0.9, 0.10 and trunk as well. If there is additional unit test that I missed to run, please let me know. Thanks.
Xuting","15/Dec/11 22:08;daijy;Xuting,
I get one failure TestGrunt.testScriptMissingLastNewLine, can you take a look?","16/Dec/11 19:46;xutingz;Hi Daniel,

   I downloaded the trunk source code from svn and without any modification, the test on TestGrunt seems to fail. The following is the code that I ran:
ant test -Dtestcase=TestGrunt

Xuting",16/Dec/11 21:58;daijy;Seems trunk is broken. I will take a look. Thanks!,"20/Dec/11 19:23;daijy;Change the patch slightly. All unit tests pass. test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 507 release audit warnings (more than the trunk's current 500 warnings).

No new file added, ignore release audit warning.",20/Dec/11 19:27;daijy;Patch committed to 0.9/0.10/trunk. Thanks Xuting!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TOBAG wraps tuple parameters in another tuple,PIG-2290,12523349,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ryan.hoegg,ryan.hoegg,ryan.hoegg,16/Sep/11 15:42,26/Apr/12 20:33,14/Mar/19 03:07,12/Oct/11 19:06,0.9.0,,,,,,,0.10.0,,,,internal-udfs,,,2,,,,,,,,,,,,,"The TOBAG function indiscriminately wraps all parameters in a tuple.  When I pass a list of tuples to the function, I would expect it to return a bag containing those tuples.  Instead, it returns a bag containing single element tuples, where each tuple contains one of the tuples passed in.

Example:
{code:title=tuples.txt}
(mike,608)
(ryan,11624)
(justin,2317)
{code}

{code:title=Demonstration using pig 0.9.0}
grunt> TUPLE_DATA = LOAD 'tuples.txt' AS (T:tuple(name:chararray,street_number:int));
grunt> BAGGED = FOREACH TUPLE_DATA GENERATE TOBAG(T);
grunt> DESCRIBE BAGGED;
BAGGED: {{(name: chararray,street_number: int)}}
grunt> DUMP BAGGED;
({((mike,608))})
({((ryan,11624))})
({((justin,2317))})
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Oct/11 18:08;thejas;pig-2290.1.patch;https://issues.apache.org/jira/secure/attachment/12498786/pig-2290.1.patch,16/Sep/11 19:19;ryan.hoegg;pig-2290.patch;https://issues.apache.org/jira/secure/attachment/12494850/pig-2290.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-09-16 18:41:57.345,,,no_permission,,,,,,,,,,,,35729,,,,Wed Oct 12 19:06:22 UTC 2011,,,Patch Available,,,,0|i0h1xz:,97577,"If the argument to TOBAG is a tuple, it does not get inserted into another tuple before being added to the bag.",,,,,,,,,"16/Sep/11 15:48;ryan.hoegg;Patch to TOBAG function to check if the parameter is a tuple before creating one, including unit test","16/Sep/11 18:41;dvryaboy;Looks good, though the formatting is a little off:
{code}
+                } else {
                 Tuple tp2 = TupleFactory.getInstance().newTuple(1);
                 tp2.set(0, object);
                 bag.add(tp2);
             }
+            }
{code}

The else block is not indented properly.

Ok to put in trunk or do you feel this must go into 0.9.1?",16/Sep/11 19:19;ryan.hoegg;Fixed formatting in TOBAG.java,"16/Sep/11 21:48;thejas;This change is not backward compatible. It can break existing pig queries. 
It can be argued that the TOBAG current implementation has a correct/consistent behavior - it puts each argument into a tuple and adds it to a bag. The bag always contains tuple with single element. 

I think there has to be a very strong reason to break backward compatibility.  This could go into a new UDF (say TOBAG_2 ?). Or this could go into some major version upgrade of pig where we make bunch of non backward compatible changes.
","16/Sep/11 22:31;ryan.hoegg;This is probably true, but the current implementation behaves inconsistently (output does not match schema).  Plus, it makes it impossible to generate a bag containing tuples with more than one element.

Is 0.9 -> 0.10 considered a ""major version upgrade""?","16/Sep/11 23:14;thejas;I didn't notice that the output does not match schema. That means one can't use the output of the TOBAG later in the query. So fixing this is not likely to have an impact on most of the existing queries. It will affect only some cases where the output of TOBAG is stored without further processing on it into a storage that does not store schema.
But this use case is something that users would need to check for, when the upgrade. I would vote for making this change only in 0.10, and not for 0.9.1 .
","12/Oct/11 18:08;thejas;pig-2290.1.patch - fix patch format issue, updated javadoc.","12/Oct/11 19:06;thejas;+1 . Patch committed to trunk.
Thanks for the contribution Ryan!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig 0.9 error message not useful as compared to 0.8 in case of group by,PIG-2288,12523295,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,vivekp,vivekp,vivekp,16/Sep/11 06:42,26/Apr/12 20:33,14/Mar/19 03:07,12/Oct/11 18:24,0.9.0,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"Another instance of change in error message from 0.8 to 0.9.This time in group by statement. In 0.8 the error message is proper while 0.9 returns NullPointer

A = LOAD 'i1' as (f1:chararray,f2:chararray);
B = GROUP B by f1;
C = foreach B generate group as f1, COUNT(A);
dump C;

Error message from 0.8
 ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Unrecognized alias B


Error message from 0.9
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1000: Error during parsing. null
        at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1652)
        at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1597)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:583)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:942)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:386)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:188)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:164)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)
        at org.apache.pig.Main.run(Main.java:553)
        at org.apache.pig.Main.main(Main.java:108)
Caused by: Failed to parse: null
        at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:180)
        at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1644)
        ... 9 more
Caused by: java.lang.NullPointerException
        at org.apache.pig.parser.LogicalPlanGenerator.alias_col_ref(LogicalPlanGenerator.java:12990)
        at org.apache.pig.parser.LogicalPlanGenerator.col_ref(LogicalPlanGenerator.java:12854)
        at org.apache.pig.parser.LogicalPlanGenerator.projectable_expr(LogicalPlanGenerator.java:7789)
        at org.apache.pig.parser.LogicalPlanGenerator.var_expr(LogicalPlanGenerator.java:7549)
        at org.apache.pig.parser.LogicalPlanGenerator.expr(LogicalPlanGenerator.java:6959)
        at org.apache.pig.parser.LogicalPlanGenerator.join_group_by_expr(LogicalPlanGenerator.java:10580)
        at org.apache.pig.parser.LogicalPlanGenerator.join_group_by_clause(LogicalPlanGenerator.java:10416)
        at org.apache.pig.parser.LogicalPlanGenerator.group_item(LogicalPlanGenerator.java:4727)
        at org.apache.pig.parser.LogicalPlanGenerator.group_clause(LogicalPlanGenerator.java:4345)
        at org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1020)
        at org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:638)
        at org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:459)
        at org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:357)
        at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:171)
        ... 10 more


Checked this case with latest code in trunk and patch from PIG-2238, the message is still improper.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Oct/11 12:03;vivekp;PIG_2288_1.patch;https://issues.apache.org/jira/secure/attachment/12497617/PIG_2288_1.patch,12/Oct/11 18:20;thejas;PIG_2288_2.patch;https://issues.apache.org/jira/secure/attachment/12498791/PIG_2288_2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-10-12 18:20:05.664,,,no_permission,,,,,,,,,,,,29031,,,,Wed Oct 12 18:24:30 UTC 2011,,,,,,,0|i0h1xj:,97575,,,,,,,,,,04/Oct/11 12:03;vivekp;Attaching an initial patch,12/Oct/11 18:20;thejas;PIG_2288_2.patch - patch regenerated for latest trunk.,"12/Oct/11 18:24;thejas;+1 . Patch committed to trunk.
Thanks for the contribution Vivek!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using COR function in Piggybank results in ERROR 2018: Internal error. Unable to introduce the combiner for optimization,PIG-2286,12523133,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,viraj,viraj,15/Sep/11 00:04,05/Oct/11 17:20,14/Mar/19 03:07,21/Sep/11 21:47,0.9.0,,,,,,,0.10.0,0.9.1,,,impl,piggybank,,0,,,,,,,,,,,,,"Usage of the COR function in a Pig script, results in an error. The ""studenttab5"" contains student, age and gpa separated by ""tab"".
{code}
register /home/viraj/pig-svn/trunk/contrib/piggybank/java/piggybank.jar;
A = LOAD '/user/viraj/studenttab5' AS (name, age:double,gpa:double);
B = group A all;
C = foreach B generate group, COR(A.a, A.b);
dump C;
{code}

{quote}
2011-09-14 17:03:22,001 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://localhost:9000
2011-09-14 17:03:22,088 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to map-reduce job tracker at: localhost:9001
2011-09-14 17:03:22,960 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: GROUP_BY
2011-09-14 17:03:23,168 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false
2011-09-14 17:03:23,179 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.CombinerOptimizer - Choosing to move algebraic foreach to combiner
2011-09-14 17:03:23,186 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2018: Internal error. Unable to introduce the combiner for optimization.
{quote}

Viraj",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Sep/11 00:35;daijy;PIG-2286-1.patch;https://issues.apache.org/jira/secure/attachment/12494732/PIG-2286-1.patch,21/Sep/11 21:19;daijy;PIG-2286-2.patch;https://issues.apache.org/jira/secure/attachment/12495443/PIG-2286-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-09-15 22:23:43.151,,,no_permission,,,,,,,,,,,,35731,Reviewed,,,Wed Sep 21 21:47:56 UTC 2011,,,,,,,0|i09zmn:,56213,,,,,,,,,,"15/Sep/11 22:23;daijy;I discovered two issues:
1. CombinerOptimizer does not handle UDF with two inputs
2. COR Algebraic version seems not working

I am working on a patch.","16/Sep/11 18:11;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1929/
-----------------------------------------------------------

Review request for pig and Thejas Nair.


Summary
-------

See PIG-2286


This addresses bug PIG-2286.
    https://issues.apache.org/jira/browse/PIG-2286


Diffs
-----

  trunk/src/org/apache/pig/builtin/COR.java 1171325 
  trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/CombinerOptimizer.java 1171325 
  trunk/test/e2e/pig/tests/nightly.conf 1171325 

Diff: https://reviews.apache.org/r/1929/diff


Testing
-------

Unit-test:
    all pass

Piggybank-test:
    TestDBStorage fail for other reason, unrelated to patch

Test-patch:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.


Thanks,

Daniel

","20/Sep/11 18:11;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1929/#review1974
-----------------------------------------------------------



trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/CombinerOptimizer.java
<https://reviews.apache.org/r/1929/#comment4462>

    I think a comment will be useful - 
    // The algebraic udf can have more than one input. Add the udf only once



trunk/src/org/apache/pig/builtin/COR.java
<https://reviews.apache.org/r/1929/#comment4463>

    The size of the tuple would need to be size*(size-1).
    Details -
    the inner loop is executed - (n-1) + (n-2) + .. (n - (n-1)) = n(n-1)/2 . 
    Each time the inner loop is executed two columns are being added. So 2 * n(n-1)/2 = n(n-1)
    



trunk/src/org/apache/pig/builtin/COR.java
<https://reviews.apache.org/r/1929/#comment4464>

    I don't understand why the values are being added to a tuple as columns. That does not look right.
    


- Thejas


On 2011-09-16 18:11:08, Daniel Dai wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/1929/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-09-16 18:11:08)
bq.  
bq.  
bq.  Review request for pig and Thejas Nair.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  See PIG-2286
bq.  
bq.  
bq.  This addresses bug PIG-2286.
bq.      https://issues.apache.org/jira/browse/PIG-2286
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    trunk/src/org/apache/pig/builtin/COR.java 1171325 
bq.    trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/CombinerOptimizer.java 1171325 
bq.    trunk/test/e2e/pig/tests/nightly.conf 1171325 
bq.  
bq.  Diff: https://reviews.apache.org/r/1929/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  Unit-test:
bq.      all pass
bq.  
bq.  Piggybank-test:
bq.      TestDBStorage fail for other reason, unrelated to patch
bq.  
bq.  Test-patch:
bq.       [exec] +1 overall.  
bq.       [exec] 
bq.       [exec]     +1 @author.  The patch does not contain any @author tags.
bq.       [exec] 
bq.       [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
bq.       [exec] 
bq.       [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
bq.       [exec] 
bq.       [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
bq.       [exec] 
bq.       [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
bq.       [exec] 
bq.       [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Daniel
bq.  
bq.

",21/Sep/11 21:19;daijy;PIG-2286-2.patch address Thejas's review comment.,21/Sep/11 21:30;thejas;+1,21/Sep/11 21:47;daijy;Patch committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig-0.8.1: some unit test case failed with open source JDK,PIG-2281,12522889,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,fang fang chen,fang fang chen,fang fang chen,13/Sep/11 12:45,22/Feb/13 04:53,14/Mar/19 03:07,19/Nov/12 10:30,0.8.1,,,,,,,0.11,0.8.1,,,,,,0,,,,,,,,,,,,,"Based on my analysis, I think this is because HashMap does not guarantee ordering.
1.
[junit] Running org.apache.pig.test.TestDataModel
[junit] Tests run: 22, Failures: 1, Errors: 0, Time elapsed: 0.382 sec
Caused by different output of HashMap.toString() with open source JDK and sun jdk. Based on the discussion with Thejas Nair(pig committer) the output in open source jdk is also correct.
Detail: 
Testcase: testTupleToString took 0.002 sec
        FAILED
toString expected:<...ad a little lamb)},[[hello#world,goodbye#all]],42,5000000000,3.14...> but was:<...ad a little lamb)},[[goodbye#all,hello#world]],42,5000000000,3.14...>
junit.framework.ComparisonFailure: toString expected:<...ad a little lamb)},[[hello#world,goodbye#all]],42,5000000000,3.14...> but was:<...ad a little lamb)},[[goodbye#all,hello#world]],42,5000000000,3.14...>
at org.apache.pig.test.TestDataModel.testTupleToString(TestDataModel.java:269)

    public void testTupleToString() throws Exception {
        Tuple t = giveMeOneOfEach();

        assertEquals(""toString"", ""((3,3.0),{(4),(mary had a little lamb)},[hello#world,goodbye#all],42,5000000000,3.1415927,2.99792458E8,true,hello,goodbye,)"", t.toString());//line 269
    }
comment:
    private Tuple giveMeOneOfEach() throws Exception {
        TupleFactory tf = TupleFactory.getInstance();

        Tuple t1 = tf.newTuple(11);
        Tuple t2 = tf.newTuple(2);

        t2.set(0, new Integer(3));
        t2.set(1, new Float(3.0));

        DataBag bag = BagFactory.getInstance().newDefaultBag();
        bag.add(tf.newTuple(new Integer(4)));
        bag.add(tf.newTuple(new String(""mary had a little lamb"")));

        Map<String, Object> map = new HashMap<String, Object>(2);
        map.put(new String(""hello""), new String(""world""));
        map.put(new String(""goodbye""), new String(""all""));

        t1.set(0, t2);
        t1.set(1, bag);
        t1.set(2, map);//when run t.toString(), HashMap.toString() will be invoked
        t1.set(3, new Integer(42));
        t1.set(4, new Long(5000000000L));
        t1.set(5, new Float(3.141592654));
        t1.set(6, new Double(2.99792458e8));
        t1.set(7, new Boolean(true));
        t1.set(8, new DataByteArray(""hello""));
        t1.set(9, new String(""goodbye""));

        return t1;
    }
}

2. 
[junit] Running org.apache.pig.test.TestLogToPhyCompiler
[junit] Tests run: 23, Failures: 1, Errors: 0, Time elapsed: 1.16 sec
Maybe caused by different output of HashMap.keySet() with open source JDK and sun jdk. 
Detail:
Failure information:
Testcase: testSplit took 0.226 sec
        FAILED
Plan not match
junit.framework.AssertionFailedError: Plan not match
        at org.apache.pig.test.TestLogToPhyCompiler.testSplit(TestLogToPhyCompiler.java:444)
    public void testSplit() throws VisitorException, IOException {
    	String query = ""split (load 'a') into x if $0 < '7', y if $0 > '7';"";
    	LogicalPlan plan = buildPlan(query);
        log.info(""ff test plan:""+plan);	
    	PhysicalPlan pp = buildPhysicalPlan(plan);
    	log.info(""ff test pp:""+pp);
    	int MAX_SIZE = 100000;
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
       
        pp.explain(baos);
        baos.write((int)'\n');
        String compiledPlan = baos.toString();
        compiledPlan = compiledPlan.replaceAll(""Load(.*)"",""Load()"");

        if(generate){
            FileOutputStream fos = new FileOutputStream(""test/org/apache/pig/test/data/GoldenFiles/Split1.gld"");
            fos.write(baos.toByteArray());
            return;
        }
        
    	FileInputStream fis1 = new FileInputStream(""test/org/apache/pig/test/data/GoldenFiles/Split1.gld"");
    	FileInputStream fis2 = new FileInputStream(""test/org/apache/pig/test/data/GoldenFiles/Split2.gld"");
        byte[] b1 = new byte[MAX_SIZE];
        byte[] b2 = new byte[MAX_SIZE];
        int len = fis1.read(b1);
        int test = fis2.read(b2);
        //System.out.println(""Length of first plan = "" + len + "" of second = "" + test);
        String goldenPlan1 = new String(b1, 0, len);
        String goldenPlan2 = new String(b2, 0, len);
        goldenPlan1 = goldenPlan1.replaceAll(""Load(.*)"",""Load()"");
        goldenPlan2 = goldenPlan2.replaceAll(""Load(.*)"",""Load()"");

        System.out.println();
        System.out.println(compiledPlan);
        System.out.println(""-------------"");

        if(compiledPlan.compareTo(goldenPlan1) == 0 || compiledPlan.compareTo(goldenPlan2) == 0) {
            // good
        }
        else {
            System.out.println(""Expected plan1="") ;
            System.out.println(goldenPlan1) ;
            System.out.println(""Expected plan2="") ;
            System.out.println(goldenPlan1) ;
            System.out.println(""Actual plan="") ;
            System.out.println(compiledPlan) ;
            System.out.println(""**END**"") ;
            fail(""Plan not match"") ;//line 444

        }
    	
    }

comment:
variable compiledPlan initialize invoke the following methods:
        pp.explain(baos);// explain(OutputStream out) method
        baos.write((int)'\n');
        String compiledPlan = baos.toString();
explain(OutputStream out)explain(OutputStream out, boolean verbose)print(OutputStream printer)depthFirstPP()getLeaves()
    public List<E> getLeaves() {
        if (mLeaves.size() == 0 && mOps.size() > 0) {
            for (E op : mOps.keySet()) {//mOps is HashMap structure, and keySet method output different with OPEN SOURCE JDK compared with SUN JDK.
                if (mFromEdges.get(op) == null) {
                    mLeaves.add(op);
                }
            }
        }
        return mLeaves;
    }
baos variable output:
> open source jdk:
> x: Filter[tuple] - Test-Plan-Builder-240
> | |
> | Less Than[boolean] - Test-Plan-Builder-243
> | |
> | |---Project[bytearray][0] - Test-Plan-Builder-241
> | |
> | |---Constant(7) - Test-Plan-Builder-242
> |
> |---Split - Test-Plan-Builder-239
> |
> |---229: Load()
>
> y: Filter[tuple] - Test-Plan-Builder-244
> | |
> | Greater Than[boolean] - Test-Plan-Builder-247
> | |
> | |---Project[bytearray][0] - Test-Plan-Builder-245
> | |
> | |---Constant(7) - Test-Plan-Builder-246
> |
> |---Split - Test-Plan-Builder-239
> |
> |---229: Load()
> sun jdk:
> y: Filter[tuple] - Test-Plan-Builder-240
> | |
> | Greater Than[boolean] - Test-Plan-Builder-243
> | |
> | |---Project[bytearray][0] - Test-Plan-Builder-241
> | |
> | |---Constant(7) - Test-Plan-Builder-242
> |
> |---Split - Test-Plan-Builder-239
> |
> |---229: Load()
>
> x: Filter[tuple] - Test-Plan-Builder-244
> | |
> | Less Than[boolean] - Test-Plan-Builder-247
> | |
> | |---Project[bytearray][0] - Test-Plan-Builder-245
> | |
> | |---Constant(7) - Test-Plan-Builder-246
> |
> |---Split - Test-Plan-Builder-239
> |
> |---229: Load()


3. 
 [junit] Running org.apache.pig.test.TestMRCompiler
 [junit] Tests run: 25, Failures: 1, Errors: 0, Time elapsed: 0.729 sec
Maybe, caused by different output of HashMap.keySet() with OPEN SOURCE JDK and sun jdk. 

Detail:
Testcase: testSortUDF1 took 0.02 sec
        FAILED
null expected:<...---MapReduce(20,SUM,[COUNT,TestMRCompiler$WeirdComparator]) - -18:
        |  ...> but was:<...---MapReduce(20,SUM,[TestMRCompiler$WeirdComparator,COUNT]) - -18:
        |  ...>
junit.framework.ComparisonFailure: null expected:<...---MapReduce(20,SUM,[COUNT,TestMRCompiler$WeirdComparator]) - -18:
        |  ...> but was:<...---MapReduce(20,SUM,[TestMRCompiler$WeirdComparator,COUNT]) - -18:
        |  ...>
        at org.apache.pig.test.TestMRCompiler.run(TestMRCompiler.java:1056)
        at org.apache.pig.test.TestMRCompiler.testSortUDF1(TestMRCompiler.java:790)

    private void run(PhysicalPlan pp, String expectedFile) throws Exception {
        String compiledPlan, goldenPlan = null;
        int MAX_SIZE = 100000;
        MRCompiler comp = new MRCompiler(pp, pc);
        comp.compile();

        MROperPlan mrp = comp.getMRPlan();
        PlanPrinter ppp = new PlanPrinter(mrp);
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        ppp.print(baos);//see ""comments""
        compiledPlan = baos.toString();//compiledPlan's initialize is based on baos

        if(generate ){
            FileOutputStream fos = new FileOutputStream(expectedFile);
            fos.write(baos.toByteArray());
            return;
        }
        FileInputStream fis = new FileInputStream(expectedFile);
        byte[] b = new byte[MAX_SIZE];
        int len = fis.read(b);
        goldenPlan = new String(b, 0, len);
        if (goldenPlan.charAt(len-1) == '\n')
            goldenPlan = goldenPlan.substring(0, len-1);

        pp.explain(System.out);
        System.out.println();
        System.out.println(""<<<"" + compiledPlan + "">>>"");
        System.out.println(""-------------"");
        System.out.println(""Golden"");
        System.out.println(""<<<"" + goldenPlan + "">>>"");
        System.out.println(""-------------"");
        assertEquals(goldenPlan, compiledPlan);//line 1056
    }
comment: 
ppp.print(baos) invokes method as following:
print(OutputStream printer) depthFirstPP()getLeaves()
    public List<E> getLeaves() {
        if (mLeaves.size() == 0 && mOps.size() > 0) {
            for (E op : mOps.keySet()) { //mOps is HashMap structure, and keySet method output different with OPEN SOURCE JDK compared with SUN JDK.
                if (mFromEdges.get(op) == null) {
                    mLeaves.add(op);
                }
            }
        }
        return mLeaves;
    }


4. 
[junit] Running org.apache.pig.test.TestMergeJoinOuter
[junit] Tests run: 5, Failures: 1, Errors: 0, Time elapsed: 132.66 sec
Caused by different output of HashMap.keySet() with OPEN SOURCE JDK and sun jdk. 

Testcase: testCompilation took 0.443 sec
        FAILED
junit.framework.AssertionFailedError:
        at org.apache.pig.test.TestMergeJoinOuter.testCompilation(TestMergeJoinOuter.java:116)
Iterator<MapReduceOper> itr = mrPlan.iterator();// see comments
MapReduceOper oper = itr.next();
assertTrue(oper.reducePlan.isEmpty());//line 116
comments: 
iterator() method:
    public Iterator<E> iterator() { 
        return mOps.keySet().iterator();//mOps is HashMap structure, and keySet method output different with OPEN SOURCE JDK compared with SUN JDK.
}
with same mrPlan:
MapReduce(-1,IsEmpty,TestMapSideCogroup$DummyCollectableLoader,PigStorage) - scope-39:
Reduce Plan Empty
|   C: Store(hdfs://localhost.localdomain:34390/user/root/out:org.apache.pig.builtin.PigStorage) - scope-38
|   |
|   |---C: New For Each(true,true)[tuple] - scope-37
|       |   |
|       |   Project[bag][1] - scope-31
|       |   |
|       |   POBinCond[bag] - scope-36
|       |   |
|       |   |---Project[bag][2] - scope-32
|       |   |
|       |   |---POUserFunc(org.apache.pig.builtin.IsEmpty)[boolean] - scope-34
|       |   |   |
|       |   |   |---Project[bag][2] - scope-33
|       |   |
|       |   |---Constant({(,,)}) - scope-35
|       |
|       |---C: MergeCogroup[tuple] - scope-30
|           |
|           |---A: Load(hdfs://localhost.localdomain:34390/user/root/data1:org.apache.pig.test.TestMapSideCogroup$DummyCollectableLoader) - scope-22
|
|---MapReduce(1) - scope-41:
    |   Store(hdfs://localhost.localdomain:34390/tmp/temp-1456742965/tmp2077335416:org.apache.pig.impl.io.InterStorage) - scope-48
    |   |
    |   |---New For Each(true)[bag] - scope-47
    |       |   |
    |       |   Project[tuple][1] - scope-46
    |       |
    |       |---Package[tuple]{tuple} - scope-45
    |   Local Rearrange[tuple]{tuple}(false) - scope-44
    |   |   |
    |   |   Project[tuple][*] - scope-43
|   |
variable itr is in different order:
open source jdk:
(Name: MapReduce(1) - scope-41(itr.next())
(Name: MapReduce(-1,IsEmpty,TestMapSideCogroup$DummyCollectableLoader,PigStorage) - scope-39:
sun jdk:
(Name: MapReduce(-1,TestMapSideCogroup$DummyCollectableLoader,IsEmpty,PigStorage) - scope-39: (itr.next())
(Name: MapReduce(1) - scope-41:

5.
[junit] Running org.apache.pig.test.TestNewPlanLogToPhyTranslationVisitor
[junit] Tests run: 25, Failures: 3, Errors: 0, Time elapsed: 1.081 sec
Caused by different output of HashMap.keySet() with OPEN SOURCE JDK and sun jdk. 
(1). 
Testcase: testSimplePlan took 0.295 sec
        FAILED
expected:<class org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject> but was:<class org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.ConstantExpression>
junit.framework.AssertionFailedError: expected:<class org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject> but was:<class org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.ConstantExpression>
        at org.apache.pig.test.TestNewPlanLogToPhyTranslationVisitor.testSimplePlan(TestNewPlanLogToPhyTranslationVisitor.java:131)
        public void testSimplePlan() throws Exception {
        LogicalPlanTester lpt = new LogicalPlanTester(pc);
        lpt.buildPlan(""a = load 'd.txt';"");
        lpt.buildPlan(""b = filter a by $0==NULL;"");        
        LogicalPlan plan = lpt.buildPlan(""store b into 'empty';"");  
        
        org.apache.pig.newplan.logical.relational.LogicalPlan newLogicalPlan = migratePlan(plan);
        PhysicalPlan phyPlan = translatePlan(newLogicalPlan);
        
        assertEquals( 3, phyPlan.size() );
        assertEquals( 1, phyPlan.getRoots().size() );
        assertEquals( 1, phyPlan.getLeaves().size() );
        
        PhysicalOperator load = phyPlan.getRoots().get(0);
        assertEquals( POLoad.class, load.getClass() );
        assertTrue(  ((POLoad)load).getLFile().getFileName().contains(""d.txt"") );
        
        // Check for Filter
        PhysicalOperator fil = phyPlan.getSuccessors(load).get(0);
        assertEquals( POFilter.class, fil.getClass() );
        PhysicalPlan filPlan = ((POFilter)fil).getPlan();
        assertEquals( 2, filPlan.getRoots().size() );
        assertEquals( 1, filPlan.getLeaves().size() );
        
        PhysicalOperator eq = filPlan.getLeaves().get(0);
        assertEquals( EqualToExpr.class, eq.getClass() );
        
        PhysicalOperator prj1 = filPlan.getRoots().get(0);
        assertEquals( POProject.class, prj1.getClass() );//line 131
        assertEquals( 0, ((POProject)prj1).getColumn() );
        PhysicalOperator constExp = filPlan.getRoots().get(1);
        assertEquals( ConstantExpression.class, constExp.getClass() );
        assertEquals( null, ((ConstantExpression)constExp).getValue() );
        
        // Check for Store
        PhysicalOperator stor = phyPlan.getSuccessors(fil).get(0);
        assertEquals( POStore.class, stor.getClass() );
        assertTrue(  ((POStore)stor).getSFile().getFileName().contains(""empty""));
}
comment:
    public List<E> getRoots() {
        if (mRoots.size() == 0 && mOps.size() > 0) {
            for (E op : mOps.keySet()) { mOps is HashMap structure, and keySet method output different with OPEN SOURCE JDK compared with SUN JDK.
                if (mToEdges.get(op) == null) {
                    mRoots.add(op);
                }
            }
        }
        return mRoots;
    

(2). 
Testcase: testJoinPlan took 0.062 sec
        FAILED
expected:<class org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.ConstantExpression> but was:<class org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject>
junit.framework.AssertionFailedError: expected:<class org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.ConstantExpression> but was:<class org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject>
        at org.apache.pig.test.TestNewPlanLogToPhyTranslationVisitor.testJoinPlan(TestNewPlanLogToPhyTranslationVisitor.java:201)

    public void testJoinPlan() throws Exception {
        LogicalPlanTester lpt = new LogicalPlanTester(pc);
        lpt.buildPlan(""a = load 'd1.txt' as (id, c);"");
        lpt.buildPlan(""b = load 'd2.txt'as (id, c);"");
        lpt.buildPlan(""c = join a by id, b by c;"");
        lpt.buildPlan(""d = filter c by a::id==NULL AND b::c==NULL;"");        
        LogicalPlan plan = lpt.buildPlan(""store d into 'empty';"");
        
        // check basics
        org.apache.pig.newplan.logical.relational.LogicalPlan newPlan = migratePlan(plan);
        PhysicalPlan physicalPlan = translatePlan(newPlan);
        assertEquals(9, physicalPlan.size());
        assertEquals(physicalPlan.getRoots().size(), 2);
        
        // Check Load and LocalRearrange and GlobalRearrange
        PhysicalOperator LoR = (PhysicalOperator)physicalPlan.getSuccessors(physicalPlan.getRoots().get(0)).get(0);
        assertEquals( POLocalRearrange.class, LoR.getClass() );
        POLocalRearrange Lor = (POLocalRearrange) LoR;
        PhysicalOperator prj3 = Lor.getPlans().get(0).getLeaves().get(0);
        assertEquals( POProject.class, prj3.getClass() );
        assertEquals(0, ((POProject)prj3).getColumn() );
        PhysicalOperator inp1 = Lor.getInputs().get(0);
        assertEquals( POLoad.class, inp1.getClass() );
        assertTrue(  ((POLoad)inp1).getLFile().getFileName().contains(""d1.txt"") );
                
        PhysicalOperator LoR1 = (PhysicalOperator)physicalPlan.getSuccessors(physicalPlan.getRoots().get(1)).get(0);
        assertEquals( POLocalRearrange.class, LoR1.getClass() );
        POLocalRearrange Lor1 = (POLocalRearrange) LoR1;
        PhysicalOperator prj4 = Lor1.getPlans().get(0).getLeaves().get(0);
        assertEquals( POProject.class, prj4.getClass() );
        assertEquals(1, ((POProject)prj4).getColumn() );
        PhysicalOperator inp2 = Lor1.getInputs().get(0);
        assertEquals( POLoad.class, inp2.getClass() );
        assertTrue(  ((POLoad)inp2).getLFile().getFileName().contains(""d2.txt"") );
        
        PhysicalOperator GoR = (PhysicalOperator)physicalPlan.getSuccessors(LoR).get(0);
        assertEquals( POGlobalRearrange.class, GoR.getClass() );
        
        PhysicalOperator Pack = (PhysicalOperator)physicalPlan.getSuccessors(GoR).get(0);
        assertEquals( POPackage.class, Pack.getClass() );

        // Check for ForEach
        PhysicalOperator ForE = (PhysicalOperator)physicalPlan.getSuccessors(Pack).get(0);
        assertEquals( POForEach.class, ForE.getClass() );
        PhysicalOperator prj5 = ((POForEach)ForE).getInputPlans().get(0).getLeaves().get(0);
        assertEquals( POProject.class, prj5.getClass() );
        assertEquals( 1, ((POProject)prj5).getColumn() ); 
        PhysicalOperator prj6 = ((POForEach)ForE).getInputPlans().get(1).getLeaves().get(0);
        assertEquals( POProject.class, prj6.getClass() );
        assertEquals( 2, ((POProject)prj6).getColumn() );
        
        // Filter Operator
        PhysicalOperator fil = (PhysicalOperator)physicalPlan.getSuccessors(ForE).get(0);
        assertEquals( POFilter.class, fil.getClass() );        
        
        PhysicalPlan filPlan = ((POFilter)fil).getPlan();
        List<PhysicalOperator> filRoots = filPlan.getRoots();
        
        assertEquals( ConstantExpression.class, filRoots.get(1).getClass() );//line 201
        ConstantExpression ce1 = (ConstantExpression) filRoots.get(1);
        assertEquals( null, ce1.getValue() ); 
        assertEquals( ConstantExpression.class, filRoots.get(3).getClass() );
        ConstantExpression ce2 = (ConstantExpression) filRoots.get(3);
        assertEquals( null, ce2.getValue() );
        assertEquals( POProject.class, filRoots.get(0).getClass() );
        POProject prj1 = (POProject) filRoots.get(0);
        assertEquals( 3, prj1.getColumn() );
        assertEquals( POProject.class, filRoots.get(2).getClass() );
        POProject prj2 = (POProject) filRoots.get(2);
        assertEquals( 0, prj2.getColumn() );


        // Check Store Operator
        PhysicalOperator stor = (PhysicalOperator)physicalPlan.getSuccessors(fil).get(0);
        assertEquals( POStore.class, stor.getClass() );
        assertTrue(  ((POStore)stor).getSFile().getFileName().contains(""empty"") );
    }
comment:
    public List<E> getRoots() {
        if (mRoots.size() == 0 && mOps.size() > 0) {
            for (E op : mOps.keySet()) { mOps is HashMap structure, and keySet method output different with OPEN SOURCE JDK compared with SUN JDK.
                if (mToEdges.get(op) == null) {
                    mRoots.add(op);
                }
            }
        }
        return mRoots;
    }
(3). 
Testcase: testMultiStore took 0.083 sec
        FAILED
expected:<1> but was:<0>
junit.framework.AssertionFailedError: expected:<1> but was:<0>
        at org.apache.pig.test.TestNewPlanLogToPhyTranslationVisitor.testMultiStore(TestNewPlanLogToPhyTranslationVisitor.java:255)
PhysicalOperator prj2 = Lor1.getPlans().get(0).getLeaves().get(0);
assertEquals(1, ((POProject)prj2).getColumn() );//line 255
comment:
    public List<E> getLeaves() {
        if (mLeaves.size() == 0 && mOps.size() > 0) {
            for (E op : mOps.keySet()) { //mOps is HashMap structure, and keySet method output different with OPEN SOURCE JDK compared with SUN JDK.
                if (mFromEdges.get(op) == null) {
                    mLeaves.add(op);
                }
            }
        }
        return mLeaves;
    }

6.
[junit] Running org.apache.pig.test.TestPruneColumn
[junit] Tests run: 67, Failures: 4, Errors: 0, Time elapsed: 528.047 sec
Maybe caused by different output of HashMap.keySet() with OPEN SOURCE JDK and sun jdk. 
(1).
Testcase: testMapKey2 took 6.291 sec
        FAILED
null
junit.framework.AssertionFailedError: null
        at org.apache.pig.test.TestPruneColumn.testMapKey2(TestPruneColumn.java:1213)
    public void testMapKey2() throws Exception{
        pigServer.registerQuery(""A = load '""+ Util.generateURI(tmpFile3.toString(), pigServer.getPigContext()) + ""' as (a0:int, a1:map[]);"");
        pigServer.registerQuery(""B = foreach A generate a1, a1#'key1';"");// see comment3
        pigServer.registerQuery(""C = foreach B generate $0#'key2', $1;"");
        
        Iterator<Tuple> iter = pigServer.openIterator(""C"");//see comment1
        
        assertTrue(iter.hasNext());
        Tuple t = iter.next();
        assertTrue(t.size()==2);
        assertTrue(t.get(0).toString().equals(""2""));
        assertTrue(t.get(1).toString().equals(""1""));
        
        assertTrue(iter.hasNext());
        t = iter.next();
        assertTrue(t.size()==2);
        assertTrue(t.get(0).toString().equals(""4""));
        assertTrue(t.get(1).toString().equals(""2""));
        
        assertFalse(iter.hasNext());
        
        assertTrue(checkLogFileMessage(new String[]{""Columns pruned for A: $0"", 
                ""Map key required for A: $1->[key2, key1]""}));// line 1213 see comment2
}
comments1: 
pigServer.openIterator(""C"") invokes the following method to save pigServer information to filename:
      store(String id, String filename, String func)
comment2:
    public boolean checkLogFileMessage(String[] messages)
    {
        BufferedReader reader = null;
        
        try {
            reader = new BufferedReader(new FileReader(logFile));//logFile=filename
            List<String> logMessages=new ArrayList<String>();
            String line;
            while ((line=reader.readLine())!=null)
            {
                logMessages.add(line);
            }
            
            // Check if all messages appear in the log
            for (int i=0;i<messages.length;i++)
            {
                boolean found = false;
                for (int j=0;j<logMessages.size();j++)
                if (logMessages.get(j).contains(messages[i])) {
                    found = true;
                    break;
                }
                if (!found)
                    return false;
            }
            
            // Check no other log besides messages
            for (int i=0;i<logMessages.size();i++) {
                boolean found = false;
                for (int j=0;j<messages.length;j++) {
                    if (logMessages.get(i).contains(messages[j])) {
                        found = true;
                        break;
                    }
                }
                if (!found) {
                    if (logMessages.get(i).contains(""Columns pruned for"")||
                            logMessages.get(i).contains(""Map key required for"")) {
                        return false;
                    }
                }
            }
            return true;
        }
        catch (IOException e) {
            return false;
        }
    }
comment3: 
the content in filename is different, because pigServer(invoke HashMap.keySet()) is different.
pigServer.registerQuery(...)getSingleLeafPlanOutputOp()getLeaves()
    public List<E> getLeaves() {
        if (mLeaves.size() == 0 && mOps.size() > 0) {
            for (E op : mOps.keySet()) { //mOps is HashMap structure, and keySet method output different with OPEN SOURCE JDK compared with SUN JDK.
                if (mFromEdges.get(op) == null) {
                    mLeaves.add(op);
                }
            }
        }
        return mLeaves;
}

(2)
Testcase: testMapKey3 took 6.319 sec
        FAILED
null
junit.framework.AssertionFailedError: null
        at org.apache.pig.test.TestPruneColumn.testMapKey3(TestPruneColumn.java:1229)
    public void testMapKey3() throws Exception {
        pigServer.registerQuery(""A = load '""+ Util.generateURI(tmpFile3.toString(),      pigServer.getPigContext()) + ""' as (a0:int, a1:map[]);"");
        pigServer.registerQuery(""B = foreach A generate a1, a1#'key1';"");
        pigServer.registerQuery(""C = group B all;"");
        
        Iterator<Tuple> iter = pigServer.openIterator(""C"");
        
        assertTrue(iter.hasNext());
        Tuple t = iter.next();
        assertTrue(t.size()==2);
        assertTrue(t.get(0).toString().equals(""all""));
        assertTrue(t.get(1).toString().equals(""{([key2#2,key1#1],1),([key2#4,key1#2],2)}""));
//line 1229
        
        assertFalse(iter.hasNext());
        
        assertTrue(checkLogFileMessage(new String[]{""Columns pruned for A: $0""}));
    }
comment: 
variable ""t"" initialize process:
titerpigServer.registerQuery(...)getSingleLeafPlanOutputOp()getLeaves()
    public List<E> getLeaves() {
        if (mLeaves.size() == 0 && mOps.size() > 0) {
            for (E op : mOps.keySet()) { //mOps is HashMap structure, and keySet method output different with OPEN SOURCE JDK compared with SUN JDK.
                if (mFromEdges.get(op) == null) {
                    mLeaves.add(op);
                }
            }
        }
        return mLeaves;
}
different t.get(1).toString():
OPEN SOURCE JDK:
{([key1#1,key2#2],1),([key1#2,key2#4],2)}
SUN JDK:
{([key2#2,key1#1],1),([key2#4,key1#2],2)}
(3).
Testcase: testMapKeyInSplit1 took 6.3 sec
        FAILED
null
junit.framework.AssertionFailedError: null
        at org.apache.pig.test.TestPruneColumn.testMapKeyInSplit1(TestPruneColumn.java:1303)

    public void testMapKeyInSplit1() throws Exception {
        pigServer.registerQuery(""A = load '""+ Util.generateURI(tmpFile12.toString(), pigServer.getPigContext()) + ""' as (m:map[]);"");
        pigServer.registerQuery(""B = foreach A generate m#'key1' as key1;"");
        pigServer.registerQuery(""C = foreach A generate m#'key2' as key2;"");
        pigServer.registerQuery(""D = join B by key1, C by key2;"");
        
        Iterator<Tuple> iter = pigServer.openIterator(""D"");
        
        assertTrue(iter.hasNext());
        Tuple t = iter.next();
        assertTrue(t.size()==2);
        assertTrue(t.get(0).toString().equals(""2""));
        assertTrue(t.get(1).toString().equals(""2""));
        
        assertFalse(iter.hasNext());
        
        assertTrue(checkLogFileMessage(new String[]{""Map key required for A: $0->[key2, key1]""}));//line 1303
    }
comment: same with (1).
(4).
Testcase: testSharedSchemaObject took 6.327 sec
        FAILED
null
junit.framework.AssertionFailedError: null
        at org.apache.pig.test.TestPruneColumn.testSharedSchemaObject(TestPruneColumn.java:1626)
    public void testSharedSchemaObject() throws Exception {
        pigServer.registerQuery(""A = load '""+ Util.generateURI(tmpFile10.toString(), pigServer.getPigContext()) + ""' AS (a0, a1:map[], a2);"");
        pigServer.registerQuery(""B = foreach A generate a1;"");
        pigServer.registerQuery(""C = limit B 10;"");
        
        Iterator<Tuple> iter = pigServer.openIterator(""C"");
        
        assertTrue(iter.hasNext());
        Tuple t = iter.next();
        assertTrue(t.toString().equals(""([2#1,1#1])""));
        
        assertFalse(iter.hasNext());
        
        assertTrue(checkLogFileMessage(new String[]{""Columns pruned for A: $0, $2""}));// line 1626
}
comment: same with (2).
7.
[junit] Running org.apache.pig.test.TestUnionOnSchema
[junit] Tests run: 21, Failures: 1, Errors: 0, Time elapsed: 196.841 sec 

Testcase: testUnionOnSchemaScopedColumnNameNeg took 0.008 sec
        FAILED
Expected exception message matching 'Found more than one match: l1::i, l2::i' but got 'Error during parsing. Found more than one match: l2::i, l1::i'
        at org.apache.pig.test.TestUnionOnSchema.checkSchemaEx(TestUnionOnSchema.java:604)
        at org.apache.pig.test.TestUnionOnSchema.testUnionOnSchemaScopedColumnNameNeg(TestUnionOnSchema.java:370)

8..
[junit] Running org.apache.pig.test.TestPushDownForeachFlatten
[junit] Tests run: 37, Failures: 0, Errors: 8, Time elapsed: 1.455 sec
Caused by different output of HashMap.keySet() with OPEN SOURCE JDK and sun jdk. 

(1)
Testcase: testForeachUnion took 0.039 sec
        Caused an ERROR
Expected LOForEach, got LOUnion
org.apache.pig.impl.plan.optimizer.OptimizerException: ERROR 2005: Expected LOForEach, got LOUnion
        at org.apache.pig.impl.logicalLayer.optimizer.PushDownForeachFlatten.getOperator(PushDownForeachFlatten.java:338)
        at org.apache.pig.impl.logicalLayer.optimizer.PushDownForeachFlatten.check(PushDownForeachFlatten.java:101)
        at org.apache.pig.test.TestPushDownForeachFlatten.testForeachUnion(TestPushDownForeachFlatten.java:275)
    public void testForeachUnion() throws Exception {
        planTester.buildPlan(""A = load 'myfile' as (name, age, gpa);"");
        planTester.buildPlan(""B = foreach A generate $0, $1, flatten($2);"");
        planTester.buildPlan(""C = load 'anotherfile' as (name, age, preference);"");
        LogicalPlan lp = planTester.buildPlan(""D = union B, C;"");        
        
        planTester.setPlan(lp);
        planTester.setProjectionMap(lp);
        
        PushDownForeachFlatten pushDownForeach = new PushDownForeachFlatten(lp);
        
        LOLoad load = (LOLoad) lp.getRoots().get(0);// see comment
        
        assertTrue(!pushDownForeach.check(lp.getSuccessors(load)));//line 275
        assertTrue(pushDownForeach.getSwap() == false);
        assertTrue(pushDownForeach.getInsertBetween() == false);
        assertTrue(pushDownForeach.getFlattenedColumnMap() == null);        
    }
comment: 
LOLoad load = (LOLoad) lp.getRoots().get(0);//
    public List<E> getRoots() {
        if (mRoots.size() == 0 && mOps.size() > 0) {
            for (E op : mOps.keySet()) {//mOps is HashMap structure, and keySet method output different with OPEN SOURCE JDK compared with SUN JDK.
                if (mToEdges.get(op) == null) {
                    mRoots.add(op);
                }
            }
        }
        return mRoots;
    }
(2)
Testcase: testForeachCogroup took 0.038 sec
        Caused an ERROR
Expected LOForEach, got LOCogroup
org.apache.pig.impl.plan.optimizer.OptimizerException: ERROR 2005: Expected LOForEach, got LOCogroup
        at org.apache.pig.impl.logicalLayer.optimizer.PushDownForeachFlatten.getOperator(PushDownForeachFlatten.java:338)
        at org.apache.pig.impl.logicalLayer.optimizer.PushDownForeachFlatten.check(PushDownForeachFlatten.java:101)
        at org.apache.pig.test.TestPushDownForeachFlatten.testForeachCogroup(TestPushDownForeachFlatten.java:295)
    public void testForeachCogroup() throws Exception {
        planTester.buildPlan(""A = load 'myfile' as (name, age, gpa);"");
        planTester.buildPlan(""B = foreach A generate $0, $1, flatten($2);"");
        planTester.buildPlan(""C = load 'anotherfile' as (name, age, preference);"");
        LogicalPlan lp = planTester.buildPlan(""D = cogroup B by $0, C by $0;"");
        
        planTester.setPlan(lp);
        planTester.setProjectionMap(lp);
        
        PushDownForeachFlatten pushDownForeach = new PushDownForeachFlatten(lp);
        
        LOLoad load = (LOLoad) lp.getRoots().get(0);
        
        assertTrue(!pushDownForeach.check(lp.getSuccessors(load)));//line 295
        assertTrue(pushDownForeach.getSwap() == false);
        assertTrue(pushDownForeach.getInsertBetween() == false);
        assertTrue(pushDownForeach.getFlattenedColumnMap() == null);        
    }
comment: same with (1)
(3)
Testcase: testForeachCross took 0.035 sec
        Caused an ERROR
Expected LOForEach, got LOCross
org.apache.pig.impl.plan.optimizer.OptimizerException: ERROR 2005: Expected LOForEach, got LOCross
        at org.apache.pig.impl.logicalLayer.optimizer.PushDownForeachFlatten.getOperator(PushDownForeachFlatten.java:338)
        at org.apache.pig.impl.logicalLayer.optimizer.PushDownForeachFlatten.check(PushDownForeachFlatten.java:101)
        at org.apache.pig.test.TestPushDownForeachFlatten.testForeachCross(TestPushDownForeachFlatten.java:427)
    public void testForeachCross() throws Exception {
        planTester.buildPlan(""A = load 'myfile' as (name, age, gpa:(letter_grade, point_score));"");
        planTester.buildPlan(""B = foreach A generate $0, $1, flatten($2);"");
        planTester.buildPlan(""C = load 'anotherfile' as (name, age, preference);"");
        planTester.buildPlan(""D = cross B, C;"");
        LogicalPlan lp = planTester.buildPlan(""E = limit D 10;"");
        
        planTester.setPlan(lp);
        planTester.setProjectionMap(lp);
        planTester.rebuildSchema(lp);
        
        PushDownForeachFlatten pushDownForeach = new PushDownForeachFlatten(lp);

        LOLoad load = (LOLoad) lp.getRoots().get(0);
        LOLimit limit = (LOLimit) lp.getLeaves().get(0);
        LOCross cross = (LOCross)lp.getPredecessors(limit).get(0);
        LOForEach foreach = (LOForEach) lp.getPredecessors(cross).get(0);
        
        Schema limitSchema = limit.getSchema();
        
        assertTrue(pushDownForeach.check(lp.getSuccessors(load)));
        assertTrue(pushDownForeach.getSwap() == false);
        assertTrue(pushDownForeach.getInsertBetween() == true);
        assertTrue(pushDownForeach.getFlattenedColumnMap() != null);

        pushDownForeach.transform(lp.getSuccessors(load));//line 427
        
        planTester.rebuildSchema(lp);
        
        for(Boolean b: foreach.getFlatten()) {
            assertEquals(b.booleanValue(), false);
        }
        
        LOForEach newForeach = (LOForEach)lp.getSuccessors(cross).get(0);
        
        
        List<Boolean> newForeachFlatten = newForeach.getFlatten();
        Map<Integer, Integer> remap = pushDownForeach.getFlattenedColumnMap();        
        for(Integer key: remap.keySet()) {
            Integer value = remap.get(key);
            assertEquals(newForeachFlatten.get(value).booleanValue(), true);
        }
        
        assertTrue(Schema.equals(limitSchema, limit.getSchema(), false, true));        
        
    }
comment: same with (1)

(4)
Testcase: testForeachFlattenAddedColumnCross took 0.034 sec
        Caused an ERROR
Expected LOForEach, got LOCross
org.apache.pig.impl.plan.optimizer.OptimizerException: ERROR 2005: Expected LOForEach, got LOCross
        at org.apache.pig.impl.logicalLayer.optimizer.PushDownForeachFlatten.getOperator(PushDownForeachFlatten.java:338)
        at org.apache.pig.impl.logicalLayer.optimizer.PushDownForeachFlatten.check(PushDownForeachFlatten.java:101)
        at org.apache.pig.test.TestPushDownForeachFlatten.testForeachFlattenAddedColumnCross(TestPushDownForeachFlatten.java:545)
    public void testForeachFlattenAddedColumnCross() throws Exception {
        planTester.buildPlan(""A = load 'myfile' as (name, age, gpa:(letter_grade, point_score));"");
        planTester.buildPlan(""B = foreach A generate $0, $1, flatten(1);"");
        planTester.buildPlan(""C = load 'anotherfile' as (name, age, preference:(course_name, instructor));"");
        planTester.buildPlan(""D = cross B, C;"");
        LogicalPlan lp = planTester.buildPlan(""E = limit D 10;"");
        
        planTester.setPlan(lp);
        planTester.setProjectionMap(lp);
        planTester.rebuildSchema(lp);
        
        PushDownForeachFlatten pushDownForeach = new PushDownForeachFlatten(lp);

        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        
        assertTrue(!pushDownForeach.check(lp.getSuccessors(loada)));//line 545
        assertTrue(pushDownForeach.getSwap() == false);
        assertTrue(pushDownForeach.getInsertBetween() == false);
        assertTrue(pushDownForeach.getFlattenedColumnMap() == null);

    }
comment: same with (1)
(5)
Testcase: testForeachFRJoin took 0.027 sec
        Caused an ERROR
Expected LOForEach, got LOJoin
org.apache.pig.impl.plan.optimizer.OptimizerException: ERROR 2005: Expected LOForEach, got LOJoin
        at org.apache.pig.impl.logicalLayer.optimizer.PushDownForeachFlatten.getOperator(PushDownForeachFlatten.java:338)
        at org.apache.pig.impl.logicalLayer.optimizer.PushDownForeachFlatten.check(PushDownForeachFlatten.java:101)
        at org.apache.pig.test.TestPushDownForeachFlatten.testForeachFRJoin(TestPushDownForeachFlatten.java:619)
    public void testForeachFRJoin() throws Exception {
        planTester.buildPlan(""A = load 'myfile' as (name, age, gpa:(letter_grade, point_score));"");
        planTester.buildPlan(""B = foreach A generate $0, $1, flatten($2);"");
        planTester.buildPlan(""C = load 'anotherfile' as (name, age, preference);"");
        planTester.buildPlan(""D = join B by $0, C by $0 using \""replicated\"";"");
        LogicalPlan lp = planTester.buildPlan(""E = limit D 10;"");
        
        planTester.setPlan(lp);
        planTester.setProjectionMap(lp);
        planTester.rebuildSchema(lp);
        
        PushDownForeachFlatten pushDownForeach = new PushDownForeachFlatten(lp);

        LOLoad load = (LOLoad) lp.getRoots().get(0);
        LOLimit limit = (LOLimit) lp.getLeaves().get(0);
        LOJoin frjoin = (LOJoin)lp.getPredecessors(limit).get(0);
        LOForEach foreach = (LOForEach) lp.getPredecessors(frjoin).get(0);
        
        Schema limitSchema = limit.getSchema();
        
        assertTrue(pushDownForeach.check(lp.getSuccessors(load)));//line 619
        assertTrue(pushDownForeach.getSwap() == false);
        assertTrue(pushDownForeach.getInsertBetween() == true);
        assertTrue(pushDownForeach.getFlattenedColumnMap() != null);

        pushDownForeach.transform(lp.getSuccessors(load));
        
        planTester.rebuildSchema(lp);
        
        for(Boolean b: foreach.getFlatten()) {
            assertEquals(b.booleanValue(), false);
        }
        
        LOForEach newForeach = (LOForEach)lp.getSuccessors(frjoin).get(0);
        
        
        List<Boolean> newForeachFlatten = newForeach.getFlatten();
        Map<Integer, Integer> remap = pushDownForeach.getFlattenedColumnMap();        
        for(Integer key: remap.keySet()) {
            Integer value = remap.get(key);
            assertEquals(newForeachFlatten.get(value).booleanValue(), true);
        }
        
        assertTrue(Schema.equals(limitSchema, limit.getSchema(), false, true));        

    }
comment: same with (1)
(6)
Testcase: testForeachFlattenAddedColumnFRJoin took 0.026 sec
        Caused an ERROR
Expected LOForEach, got LOJoin
org.apache.pig.impl.plan.optimizer.OptimizerException: ERROR 2005: Expected LOForEach, got LOJoin
        at org.apache.pig.impl.logicalLayer.optimizer.PushDownForeachFlatten.getOperator(PushDownForeachFlatten.java:338)
        at org.apache.pig.impl.logicalLayer.optimizer.PushDownForeachFlatten.check(PushDownForeachFlatten.java:101)
        at org.apache.pig.test.TestPushDownForeachFlatten.testForeachFlattenAddedColumnFRJoin(TestPushDownForeachFlatten.java:738)
   public void testForeachFlattenAddedColumnFRJoin() throws Exception {
        planTester.buildPlan(""A = load 'myfile' as (name, age, gpa:(letter_grade, point_score));"");
        planTester.buildPlan(""B = foreach A generate $0, $1, flatten(1);"");
        planTester.buildPlan(""C = load 'anotherfile' as (name, age, preference:(course_name, instructor));"");
        planTester.buildPlan(""D = join B by $0, C by $0 using \""replicated\"";"");
        LogicalPlan lp = planTester.buildPlan(""E = limit D 10;"");
        
        planTester.setPlan(lp);
        planTester.setProjectionMap(lp);
        planTester.rebuildSchema(lp);
        
        PushDownForeachFlatten pushDownForeach = new PushDownForeachFlatten(lp);

        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        
        assertTrue(!pushDownForeach.check(lp.getSuccessors(loada)));//line 738
        assertTrue(pushDownForeach.getSwap() == false);
        assertTrue(pushDownForeach.getInsertBetween() == false);
        assertTrue(pushDownForeach.getFlattenedColumnMap() == null);

    }
comment: same with (1)
(7)
Testcase: testForeachInnerJoin took 0.026 sec
        Caused an ERROR
Expected LOForEach, got LOJoin
org.apache.pig.impl.plan.optimizer.OptimizerException: ERROR 2005: Expected LOForEach, got LOJoin
        at org.apache.pig.impl.logicalLayer.optimizer.PushDownForeachFlatten.getOperator(PushDownForeachFlatten.java:338)
        at org.apache.pig.impl.logicalLayer.optimizer.PushDownForeachFlatten.check(PushDownForeachFlatten.java:101)
        at org.apache.pig.test.TestPushDownForeachFlatten.testForeachInnerJoin(TestPushDownForeachFlatten.java:812)
    public void testForeachInnerJoin() throws Exception {
        planTester.buildPlan(""A = load 'myfile' as (name, age, gpa:(letter_grade, point_score));"");
        planTester.buildPlan(""B = foreach A generate $0, $1, flatten($2);"");
        planTester.buildPlan(""C = load 'anotherfile' as (name, age, preference:(course_name, instructor));"");
        planTester.buildPlan(""D = join B by $0, C by $0;"");
        LogicalPlan lp = planTester.buildPlan(""E = limit D 10;"");
        
        planTester.setPlan(lp);
        planTester.setProjectionMap(lp);
        planTester.rebuildSchema(lp);
        
        PushDownForeachFlatten pushDownForeach = new PushDownForeachFlatten(lp);

        LOLoad load = (LOLoad) lp.getRoots().get(0);
        LOLimit limit = (LOLimit) lp.getLeaves().get(0);
        LOJoin join = (LOJoin)lp.getPredecessors(limit).get(0);
        LOForEach foreach = (LOForEach) lp.getPredecessors(join).get(0);
        
        Schema limitSchema = limit.getSchema();
        
        assertTrue(pushDownForeach.check(lp.getSuccessors(load)));//line 812
        assertTrue(pushDownForeach.getSwap() == false);
        assertTrue(pushDownForeach.getInsertBetween() == true);
        assertTrue(pushDownForeach.getFlattenedColumnMap() != null);

        pushDownForeach.transform(lp.getSuccessors(load));
        
        planTester.rebuildSchema(lp);
        
        for(Boolean b: foreach.getFlatten()) {
            assertEquals(b.booleanValue(), false);
        }
        
        LOForEach newForeach = (LOForEach)lp.getSuccessors(join).get(0);
        
        
        List<Boolean> newForeachFlatten = newForeach.getFlatten();
        Map<Integer, Integer> remap = pushDownForeach.getFlattenedColumnMap();        
        for(Integer key: remap.keySet()) {
            Integer value = remap.get(key);
            assertEquals(newForeachFlatten.get(value).booleanValue(), true);
        }
        
        assertTrue(Schema.equals(limitSchema, limit.getSchema(), false, true));        

}
comment: same with (1)
(8)
Testcase: testForeachFlattenAddedColumnInnerJoin took 0.021 sec
        Caused an ERROR
Expected LOForEach, got LOJoin
org.apache.pig.impl.plan.optimizer.OptimizerException: ERROR 2005: Expected LOForEach, got LOJoin
        at org.apache.pig.impl.logicalLayer.optimizer.PushDownForeachFlatten.getOperator(PushDownForeachFlatten.java:338)
        at org.apache.pig.impl.logicalLayer.optimizer.PushDownForeachFlatten.check(PushDownForeachFlatten.java:101)
        at org.apache.pig.test.TestPushDownForeachFlatten.testForeachFlattenAddedColumnInnerJoin(TestPushDownForeachFlatten.java:931)
    public void testForeachFlattenAddedColumnInnerJoin() throws Exception {
        planTester.buildPlan(""A = load 'myfile' as (name, age, gpa:(letter_grade, point_score));"");
        planTester.buildPlan(""B = foreach A generate $0, $1, flatten(1);"");
        planTester.buildPlan(""C = load 'anotherfile' as (name, age, preference:(course_name, instructor));"");
        planTester.buildPlan(""D = join B by $0, C by $0;"");
        LogicalPlan lp = planTester.buildPlan(""E = limit D 10;"");
        
        planTester.setPlan(lp);
        planTester.setProjectionMap(lp);
        planTester.rebuildSchema(lp);
        
        PushDownForeachFlatten pushDownForeach = new PushDownForeachFlatten(lp);

        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        
        assertTrue(!pushDownForeach.check(lp.getSuccessors(loada)));//line 931
        assertTrue(pushDownForeach.getSwap() == false);
        assertTrue(pushDownForeach.getInsertBetween() == false);
        assertTrue(pushDownForeach.getFlattenedColumnMap() == null);

    }

9.
[junit] Running org.apache.pig.test.TestTypeCheckingValidator
[junit] Tests run: 120, Failures: 0, Errors: 1, Time elapsed: 15.047 sec
Caused by different output of HashMap.keySet() with OPEN SOURCE JDK and sun jdk. Based on the discussion with Thejas Nair(pig committer) the output in OPEN SOURCE jdk is also correct.
Detail:
Testcase: testMapLookupLineage took 0.012 sec
        Caused an ERROR
org.apache.pig.impl.logicalLayer.LOAdd incompatible with org.apache.pig.impl.logicalLayer.LOCast
java.lang.ClassCastException: org.apache.pig.impl.logicalLayer.LOAdd incompatible with org.apache.pig.impl.logicalLayer.LOCast
        at org.apache.pig.test.TestTypeCheckingValidator.testMapLookupLineage(TestTypeCheckingValidator.java:5397)
    public void testMapLookupLineage() throws Throwable {
        planTester.buildPlan(""a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );"") ;
        planTester.buildPlan(""b = foreach a generate field1#'key1' as map1;"") ;
        LogicalPlan plan = planTester.buildPlan(""c = foreach b generate map1#'key2' + 1 ;"") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError(""Expect no  error"") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);// see comement1
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);//see comment2
        // the root would be the project and there would be cast
        // to map between the project and LOMapLookup
        LOCast cast1 = (LOCast)foreachPlan.getSuccessors(exOp).get(0);//line 5397
        assertTrue(cast1.getLoadFuncSpec().getClassName().startsWith(""BinStorage""));
        LOMapLookup map = (LOMapLookup)foreachPlan.getSuccessors(cast1).get(0);
        LOCast cast = (LOCast)foreachPlan.getSuccessors(map).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith(""BinStorage""));

}
comment1: 
foreachPlan initialize process, this will cause the different foreachPlan.
foreachPlanforeachplan.getLeaves
    public List<E> getLeaves() {
        if (mLeaves.size() == 0 && mOps.size() > 0) {
            for (E op : mOps.keySet()) { //mOps is HashMap structure, and keySet method output different with OPEN SOURCE JDK compared with SUN JDK.
                if (mFromEdges.get(op) == null) {
                    mLeaves.add(op);
                }
            }
        }
        return mLeaves;
}
comment2:
exOpforeachPlan.getRoots(), this is the root cause
    public List<E> getRoots() {
        if (mRoots.size() == 0 && mOps.size() > 0) {
            for (E op : mOps.keySet()) {//mOps is HashMap structure, and keySet method output different with OPEN SOURCE JDK compared with SUN JDK.
                if (mToEdges.get(op) == null) {
                    mRoots.add(op);
                }
            }
        }
        return mRoots;
    }

SUN JDK output:
foreachPlan:
Add scope-12 FieldSchema: int Type: int
|
|---Const scope-15( 1 ) FieldSchema: int Type: int
|
|---Cast scope-19 FieldSchema: int Type: int
    |
    |---MapLookup scope-14 FieldSchema: bytearray Type: bytearray
        |
        |---Cast scope-18 FieldSchema: map Type: map
            |
            |---Project scope-13 Projections: [0] Overloaded: false FieldSchema: map1: bytearray Type: bytearray
                Input: b: ForEach scope-6
getRoots method process:
keySet() method:
Cast scope-19
Cast scope-18
Add scope-12
Project scope-13 Projections: [0] Overloaded: false(add to mRoots)
MapLookup scope-14
Const scope-15( 1 )(add to mRoots)
foreachPlan.getRoots():(Name: Project scope-13 Projections: [0] Overloaded: false Operator Key: scope-13)
                                          (Name: Const scope-15( 1 ) Operator Key: scope-15)
exOp:(Name: Project scope-13 Projections: [0] Overloaded: false Operator Key: scope-13)
OPEN SOURCE JDK output:
foreachPlan:
Add scope-13 FieldSchema: int Type: int
|
|---Const scope-12( 1 ) FieldSchema: int Type: int
|
|---Cast scope-19 FieldSchema: int Type: int
    |
    |---MapLookup scope-15 FieldSchema: bytearray Type: bytearray
        |
        |---Cast scope-18 FieldSchema: map Type: map
            |
            |---Project scope-14 Projections: [0] Overloaded: false FieldSchema: map1: bytearray Type: bytearray
                Input: b: ForEach scope-6
getRoots method process: 
keySet() method:
Cast scope-18
Cast scope-19
Const scope-12( 1 )(add to mRoots)
Add scope-13
Project scope-14 Projections: [0] Overloaded: false(add to mRoots)
MapLookup scope-15
foreachPlan.getRoots():(Name: Const scope-12( 1 ) Operator Key: scope-12)
                                          (Name: Project scope-14 Projections: [0] Overloaded: false Operator Key: scope-14)//output in different order compared with SUN JDK output
exOp:(Name: Const scope-12( 1 ) Operator Key: scope-12)





","open source jdk
ant: 1.8.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-09-14 18:24:27.109,,,no_permission,,,,,,,,,,,,35734,,,,Mon Nov 19 10:30:58 UTC 2012,,,,,,,0|i0h1wf:,97570,,,,,,,,,,14/Sep/11 18:24;daijy;Have you tested against 0.9? We use ArrayList instead of HashMap in 0.9.,19/Nov/12 10:30;fang fang chen;Duplicated with PIG-2405,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong version numbers for libraries in eclipse template classpath,PIG-2278,12522747,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,azaroth,azaroth,azaroth,12/Sep/11 14:48,26/Apr/12 20:32,14/Mar/19 03:07,12/Sep/11 16:11,,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,Some libraries have not been updated in the .eclipse-templates/.classpath file,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Sep/11 14:50;azaroth;PIG-2278.patch;https://issues.apache.org/jira/secure/attachment/12494037/PIG-2278.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-12 15:43:59.64,,,no_permission,,,,,,,,,,,,4135,,,,Tue Sep 13 14:08:16 UTC 2011,,,,,,,0|i0h1vr:,97567,,,,,,,,,,12/Sep/11 14:52;azaroth;It might be a good idea to find a way to automate the generation/updating of these files.,12/Sep/11 15:43;alangates;+1 for the patch.,12/Sep/11 16:11;azaroth;Patch committed!,"12/Sep/11 16:56;ashutoshc;bq. It might be a good idea to find a way to automate the generation/updating of these files.

https://issues.apache.org/jira/browse/HADOOP-6407 should help.","13/Sep/11 14:08;azaroth;Thanks Ashutosh, it is exactly what I had in mind.
I will open a new Jira for this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException from ILLUSTRATE,PIG-2275,12522474,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,ddaniels888,ddaniels888,09/Sep/11 20:25,26/Apr/12 20:33,14/Mar/19 03:07,02/Nov/11 07:34,0.9.0,,,,,,,0.10.0,0.11,,,,,,0,,,,,,,,,,,,,"I'm getting a NullPointerException from ILLUSTRATE on a simple script using the webcrawl data from the ""Programming Pig"" book: https://github.com/alanfgates/programmingpig/blob/master/data/webcrawl

The script is:

{code}
A = load 'webcrawl' USING PigStorage('\t') AS ( url: chararray, pagerank: float, links:{ link: ( url: chararray ) } );
B = filter A by url is not null;
C = store B into 'output';
{code}

The stack trace I'm getting is:

{code}
java.lang.NullPointerException
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter.illustratorMarkup(POFilter.java:200)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter.getNext(POFilter.java:155)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:290)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore.getNext(POStore.java:138)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:261)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:256)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:58)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.pig.pen.LocalMapReduceSimulator.launchPig(LocalMapReduceSimulator.java:194)
	at org.apache.pig.pen.ExampleGenerator.getData(ExampleGenerator.java:257)
	at org.apache.pig.pen.ExampleGenerator.getData(ExampleGenerator.java:238)
	at org.apache.pig.pen.LineageTrimmingVisitor.init(LineageTrimmingVisitor.java:103)
	at org.apache.pig.pen.LineageTrimmingVisitor.<init>(LineageTrimmingVisitor.java:98)
	at org.apache.pig.pen.ExampleGenerator.getExamples(ExampleGenerator.java:166)
	at org.apache.pig.PigServer.getExamples(PigServer.java:1258)
	at org.apache.pig.tools.grunt.GruntParser.processIllustrate(GruntParser.java:698)
	at org.apache.pig.tools.pigscript.parser.PigScriptParser.Illustrate(PigScriptParser.java:591)
	at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:306)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:188)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:164)
	at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)
	at org.apache.pig.Main.run(Main.java:456)
	at org.apache.pig.Main.main(Main.java:108)
{code}

I'm running this in local mode, though I don't know if it makes a difference.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Sep/11 18:38;daijy;PIG-2275-1.patch;https://issues.apache.org/jira/secure/attachment/12495422/PIG-2275-1.patch,26/Oct/11 21:53;daijy;PIG-2275-2.patch;https://issues.apache.org/jira/secure/attachment/12500963/PIG-2275-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-09-10 14:34:41.663,,,no_permission,,,,,,,,,,,,35738,Reviewed,,,Wed Nov 02 07:34:13 UTC 2011,,,,,,,0|i09zfr:,56182,,,,,,,,,,"10/Sep/11 14:34;alangates;The last line of your script should read ""store B into 'output';"" (no ""C ="").  If you change that do you still see the NPE?","10/Sep/11 19:35;ddaniels888;Good catch.  I changed to:

<code>
A = load 'webcrawl' USING PigStorage('\t') AS ( url: chararray, pagerank: float, links:{ link: ( url: chararray ) } );
B = filter A by url is not null;
store B into 'output';
</code>

Same problem though, still an NPE in the same place.",26/Oct/11 21:53;daijy;There is a similar test case in PIG-2302. PIG-2275-2.patch address both.,01/Nov/11 06:35;thejas;+1,"02/Nov/11 07:34;daijy;Unit tests pass. test-patch result:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 458 release audit warnings (more than the trunk's current 447 warnings).

No new files added, ignore release audit warning.

Patch committed to both trunk and 0.10 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig.compileFromFile in embedded python fails when pig script starts with a comment,PIG-2273,12522465,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ddaniels888,ddaniels888,ddaniels888,09/Sep/11 19:35,26/Apr/12 20:33,14/Mar/19 03:07,14/Sep/11 00:51,0.9.0,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"When using embedded Pig inside python, Pig.compileFromFile fails when the referenced pig file starts with a comment.

When you start the referenced pig file with a comment, you get no plan back and hence cannot successfully execute the job.  When you start the same script without a comment, you get a plan and can execute the script successfully.

Strangely, this issue does not appear when using Pig.compile to compile directly from a string.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Sep/11 17:22;ddaniels888;PIG-2273.patch;https://issues.apache.org/jira/secure/attachment/12494274/PIG-2273.patch,09/Sep/11 19:39;ddaniels888;PIG-2273.tar.gz;https://issues.apache.org/jira/secure/attachment/12493845/PIG-2273.tar.gz,13/Sep/11 23:55;alangates;e2etest.patch;https://issues.apache.org/jira/secure/attachment/12494347/e2etest.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-09-13 17:03:11.323,,,no_permission,,,,,,,,,,,,4137,,,,Wed Sep 14 00:51:05 UTC 2011,,,,,,,0|i0h1vb:,97565,,,,,,,,,,"09/Sep/11 19:39;ddaniels888;I've attached two scripts to show the issue.

Running 'pig -x local withComment.py' fails to produce a plan or run the script.  Running 'pig -x local withoutComment.py' produces a plan and runs the script.","09/Sep/11 19:49;ddaniels888;It seems now like any comments in the script, not just ones at the beginning of the file, are hitting this bug.","12/Sep/11 19:09;ddaniels888;Newlines were being removed from the pig script when it was parsed with compileFromFile.  This caused line-style comments to extend through the whole file, rather than just the line they comment.

Attached a patch with test update.

Also: I can't see why the getScriptFromFile method is using a LineNumberReader and iterating through each line when it just returns the String output of what it gets and doesn't use the line numbers.  Might be simpler just to read the file directly.","13/Sep/11 17:03;alangates;Doug, when generating patches from git please use ""git diff --no-prefix""  This way we can use the patch tool to apply the patches.

Patch looks good.  If you can regenerate it, I'll run it through the tests.",13/Sep/11 17:22;ddaniels888;Here's an updated patch with git diff --no-prefix.,13/Sep/11 23:55;alangates;This patch adds an e2e test that tests the same functionality.,"14/Sep/11 00:21;alangates;Unit tests pass, test-patch returns:

{code}
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec] 
     [exec] 
{code}

",14/Sep/11 00:51;alangates;Patch committed to trunk.  Thanks Doug.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PIG regression in BinStorage/PigStorage in 0.9.1,PIG-2271,12522397,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,thejas,vbarat,vbarat,09/Sep/11 09:35,23/Jan/12 07:31,14/Mar/19 03:07,10/Oct/11 23:03,0.10.0,0.9.1,,,,,,0.10.0,0.9.2,,,,,,0,,,,,,,,,,,,,"I'm using the 0.9.1 official release.

My input data are read form a text file 'activity' (provided as attachment):

{code}
00,1239698069000, <- this is the line that is not correctly handled
01,1239698505000,b
01,1239698369000,a
02,1239698413000,b
02,1239698553000,c
02,1239698313000,a
03,1239698316000,a
03,1239698516000,c
03,1239698416000,b
03,1239698621000,d
04,1239698417000,c
{code}

My script is working correctly:

{code}
-- load input data
activities = LOAD 'activity' USING PigStorage(',') AS (sid:chararray, timestamp:long, name:chararray);

-- group input data
activities = GROUP activities BY sid;
activities = FOREACH activities GENERATE group, activities.(timestamp, name);

-- store grouped activities in a temporary file
STORE activities INTO 'tmp' USING PigStorage();

-- reload grouped activities from the temporary file
activities = LOAD 'tmp' USING PigStorage() AS (sid:chararray, acts:bag { act:tuple (timestamp:long, name:chararray) });

-- store grouped activities again in an output file
STORE activities INTO 'output' USING PigStorage();
{code}

After running this script, the 'output' file contains a correct result:

{code}
00	{(1239698069000,)}
01	{(1239698505000,b),(1239698369000,a)}
02	{(1239698413000,b),(1239698553000,c),(1239698313000,a)}
03	{(1239698316000,a),(1239698516000,c),(1239698416000,b),(1239698621000,d)}
04	{(1239698417000,c)}
{code}

But the issue occurs when I use BinStorage() instead of PigStorage() to store / reload my temporary files. The 'output' file in that case is not complete:

{code}
00	
01	{(1239698505000,b),(1239698369000,a)}
02	{(1239698413000,b),(1239698553000,c),(1239698313000,a)}
03	{(1239698316000,a),(1239698516000,c),(1239698416000,b),(1239698621000,d)}
04	{(1239698417000,c)}
{code}

The not working script is the following:

{code}
-- load input data
activities = LOAD 'activity' USING PigStorage(',') AS (sid:chararray, timestamp:long, name:chararray);

-- group input data
activities = GROUP activities BY sid;
activities = FOREACH activities GENERATE group, activities.(timestamp, name);

-- store grouped activities in a temporary file
STORE activities INTO 'tmp' USING PigStorage();

-- reload grouped activities from the temporary file
activities = LOAD 'tmp' USING PigStorage() AS (sid:chararray, acts:bag { act:tuple (timestamp:long, name:chararray) });

-- store grouped activities again in an output file
STORE activities INTO 'output' USING PigStorage();
{code}

So the issue seems to be located in the way the BinStorage() store or load bags.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Oct/11 18:52;thejas;PIG-2271.0.patch;https://issues.apache.org/jira/secure/attachment/12498048/PIG-2271.0.patch,06/Oct/11 21:14;thejas;PIG-2271.1.patch;https://issues.apache.org/jira/secure/attachment/12498070/PIG-2271.1.patch,06/Oct/11 15:40;vbarat;activity;https://issues.apache.org/jira/secure/attachment/12498014/activity,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-09-10 01:10:42.592,,,no_permission,,,,,,,,,,,,35739,,,,Mon Oct 10 22:53:41 UTC 2011,,,,,,,0|i09ziv:,56196,patch committed to 0.9 branch and trunk,,,,,,,,,"10/Sep/11 01:10;daijy;Can you do these:
1. Get the output schema for MyUDF. (describe activities)
2. Use a different construct for BinStorage: BinStorage(""org.apache.pig.builtin.Utf8StorageConverter"")","06/Oct/11 16:04;vbarat;Hi Daniel,

I did more investigations and fully reformulated the issue. There is no more UDF function involved, and I reproduce it with the 0.9.1 official release.

The issue is related to BinStorage (but I can also reproduce it using PigStorage(',')).

This is a really blocking issue for me, as I need to use BinStorage() to load some binary data. This issue prevent be from using pig 0.9.1.

Thanks a lot for your time.","06/Oct/11 18:52;thejas;PIG-2271.0.patch - initial patch. Test cases need to be added. 

The type conversion when user specified schema is present was not handling nulls correctly, it resulted in a cast failure. So the type conversion for the tuple that contained null was not successful. ","06/Oct/11 21:14;thejas;PIG-2271.1.patch - patch with test cases.
","06/Oct/11 21:16;thejas;I would like to clarify that the type conversion fails only when user casts a complex type (tuple/bag/map) using a schema with inner schema, and one of the values inside is null.
",10/Oct/11 22:53;daijy;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Put jython.jar in classpath,PIG-2270,12522371,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,09/Sep/11 06:30,05/Oct/11 17:20,14/Mar/19 03:07,21/Sep/11 21:28,0.9.0,,,,,,,0.10.0,0.9.1,,,impl,,,0,,,,,,,,,,,,,Seems jython.jar still not in classpath.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Sep/11 18:08;daijy;PIG-2270-1.patch;https://issues.apache.org/jira/secure/attachment/12494088/PIG-2270-1.patch,17/Sep/11 01:22;daijy;PIG-2270-2.patch;https://issues.apache.org/jira/secure/attachment/12494897/PIG-2270-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-09-20 18:45:20.139,,,no_permission,,,,,,,,,,,,35740,Reviewed,,,Wed Sep 21 21:28:18 UTC 2011,,,,,,,0|i09zn3:,56215,,,,,,,,,,"20/Sep/11 18:45;thejas;automaton.jar should be retained as part of the pig-without hadoop jar, as it is need in backend nodes at runtime. Other changes look good.",21/Sep/11 21:28;daijy;Patch committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make the name of the columns in schema optional,PIG-2267,12521824,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,ashutoshc,ashutoshc,07/Sep/11 22:01,22/Feb/13 04:53,14/Mar/19 03:07,09/Jan/12 06:56,0.10.0,0.11,0.9.0,,,,,0.11,,,,parser,,,0,,,,,,,,,,,,,"Following two works:
{code}
a = load 'data';
a = load 'data' as (f1:int);
{code}

Following doesn't:
{code}
a = load 'data' as (int);
{code}

It will be nice to make names of column optional.",,,,,,,,,,,HCATALOG-92,,,,,,,,,,,,,,,,,,,,,17/Dec/11 06:18;jcoveney;PIG2267.patch;https://issues.apache.org/jira/secure/attachment/12507770/PIG2267.patch,19/Dec/11 22:46;jcoveney;PIG2267_1.patch;https://issues.apache.org/jira/secure/attachment/12507991/PIG2267_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-12-17 06:17:01.904,,,no_permission,,,,,,,,,,,,35743,Reviewed,,,Mon Jan 09 06:56:09 UTC 2012,,,,,,,0|i0h1un:,97562,"Name part of the schema is optional in as clause in load and foreach statement. For example:
a = load 'data' as (int);
a = load 'data' as ({(int,int)});
y = foreach x generate x0 as int;
y = foreach x generate x0 as {(int, int)};

Pig will generate dummy name if the name part is missing.",,,,,,,,,"17/Dec/11 06:17;jcoveney;Find attached a patch that works with trunk. Note, I still need to write some new tests (I'm still deciding what sort of test is meaningful), but it passes ant test-commit.

Bags get a schema bag_0 and up, tuples tuple_0 and up, maps map_0 and up, and the rest get val_ and up (I originally did int_, long_, and so on, but julien brought up a good point: it's important to be able to change an int to a long, for example, without breaking the script).

Here are some examples of output:
{code}
a = load 'data' as (tuple(int));
{code}
yields
{code}
a: {tuple_0: (val_0: int)}
{code}

{code}
a = load 'data' as (int,int,int,long,bytearray);
{code}
yields
{code}
a: {val_0: int,val_1: int,val_2: int,val_3: long,val_4: bytearray}
{code}

{code}
a = load 'data' as ({(int,int,int)},{(int,int,int)},{(int,int,int)});
{code}
yields
{code}
a: {bag_0: {(val_0: int,val_1: int,val_2: int)},bag_1: {(val_0: int,val_1: int,val_2: int)},bag_2: {(val_0: int,val_1: int,val_2: int)}}
{code}

I welcome any suggestions, especially on good tests...","17/Dec/11 06:18;jcoveney;ant test-commit works, but I need to add tests specific to this (though I will say that the tests we have definitely fail if you mess up the grammar!)",19/Dec/11 22:46;jcoveney;A slightly more recent version of this with some tests.,07/Jan/12 02:40;jcoveney;Bump :),"07/Jan/12 22:54;daijy;+1. Pretty good change. The next step is to simply the syntax for cast as well (eg, foreach a generate (bag{tuple(int,double)})a0).

Will commit once tests pass.","09/Jan/12 06:56;daijy;Unit test pass.

test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     -1 javac.  The applied patch generated 898 javac compiler warnings (more than the trunk's current 894 warnings).
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 510 release audit warnings (more than the trunk's current 502 warnings).

javac warning is caused by antlr generated code. I added Apache header to all new files, so ignore release audit warning.

Patch committed to trunk. Thanks Jonathan!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bug with input file joining optimization in Pig,PIG-2266,12521482,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jadler,jadler,jadler,06/Sep/11 18:27,14/Oct/13 16:46,14/Mar/19 03:07,29/Jan/13 20:57,0.10.0,0.9.0,,,,,,0.12.0,,,,impl,,,1,,,,,,,,,,,,,"In src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java, the function hasTooManyInputFiles instantiated a LoadFunc instance, then calls setLocation before calling setUDFContextSignature. This is inconsistent with the documentation for the LoadFunc interface (see http://pig.apache.org/docs/r0.9.0/api/org/apache/pig/LoadFunc.html#setUDFContextSignature(java.lang.String)). (We've written UDFs that assume that setUDFContextSignature is called first.)

I think you can fix this by adding 

   loader.setUDFContextSignature(ld.getSignature());

Before

   loader.setLocation(location, job);",,,,,,,,,,,PIG-3015,,,,,,,,,,,,,,,,,,,,,27/Jan/13 21:58;cheolsoo;PIG-2266.patch;https://issues.apache.org/jira/secure/attachment/12566690/PIG-2266.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-06 19:08:47.751,,,no_permission,,,,,,,,,,,,35744,,,,Tue Jan 29 20:57:04 UTC 2013,,,,,,,0|i0h1uf:,97561,,,,,,,,,,06/Sep/11 19:08;daijy;That seems reasonable. Can you wrap up a patch?,"06/Sep/11 22:02;jadler;Index: MRCompiler.java
===================================================================
--- MRCompiler.java	(revision 1165764)
+++ MRCompiler.java	(working copy)
@@ -1353,7 +1353,8 @@
                             .instantiateFuncFromSpec(ld.getLFile()
                                     .getFuncSpec());
                             Job job = new Job(conf);
-                            loader.setLocation(location, job);
+                            loader.setUDFContextSignature(ld.getSignature()); 
+			    loader.setLocation(location, job);
                             InputFormat inf = loader.getInputFormat();
                             List<InputSplit> splits = inf.getSplits(HadoopShims.cloneJobContext(job));
                             List<List<InputSplit>> results = MapRedUtil
",27/Jan/13 21:58;cheolsoo;Attaching Joe's change as a patch. This is needed for PIG-3015.,28/Jan/13 23:59;sms;+1 to the patch.,29/Jan/13 00:54;cheolsoo;Thank you Santhosh for the review. I will commit it after running tests.,29/Jan/13 00:57;jadler;Thanks for adding this fix!,29/Jan/13 20:57;cheolsoo;Committed to trunk. Thanks Joe!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test case TestSecondarySort failure,PIG-2265,12521461,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,xinshengjun,xinshengjun,06/Sep/11 14:42,14/Oct/13 16:46,14/Mar/19 03:07,08/Apr/13 19:59,0.8.0,,,,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"Error message:
Testcase: testNestedSortEndToEnd3 took 53.076 sec
	Caused an ERROR
Unable to open iterator for alias E. Backend error : org.apache.pig.data.DataByteArray cannot be cast to org.apache.pig.data.Tuple
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias E. Backend error : org.apache.pig.data.DataByteArray cannot be cast to org.apache.pig.data.Tuple
	at org.apache.pig.PigServer.openIterator(PigServer.java:742)
	at org.apache.pig.test.TestSecondarySort.testNestedSortEndToEnd3(TestSecondarySort.java:550)
Caused by: java.lang.ClassCastException: org.apache.pig.data.DataByteArray cannot be cast to org.apache.pig.data.Tuple
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:392)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNext(POLocalRearrange.java:357)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:236)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:231)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Apr/13 18:11;daijy;PIG-2265-1.patch;https://issues.apache.org/jira/secure/attachment/12577248/PIG-2265-1.patch,08/Apr/13 19:30;daijy;PIG-2265-2.patch;https://issues.apache.org/jira/secure/attachment/12577597/PIG-2265-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-09-06 17:12:45.549,,,no_permission,,,,,,,,,,,,35745,Reviewed,,,Mon Apr 08 19:59:48 UTC 2013,,,,,,,0|i0h1u7:,97560,,,,,,,,,,06/Sep/11 17:12;daijy;Are you using ant 1.8? We find some test failures with ant 1.8 and fixed in PIG-2172.,"07/Sep/11 01:47;xinshengjun;Yes, I'm using ant 1.8, I'll try ant 1.7, thank you for your advice","07/Sep/11 10:17;xinshengjun;If I use ant 1.7 or apply the patch, the test case testNestedSortEndToEnd3 will not run. But if I let it run, it still fails and generates the same error message as before. Any idea?","27/Mar/13 00:35;dreambird;current the test is disabled in trunk.

I enable it and can reproduce the issue. I think it is the same root cause as PIG-3049. [~cheolsoo] help me debug this issue a while back, and explains to me idea.

The reason seems when secondary sort is enabled, the code needs inform POProject.java to process secondary sort key properly to avoid cast from the content of the tuple to tuple by
POProject.java line 481
{code}
res.result = (Tuple)ret;
{code}

the fix should be something like
POProject.java line 422
change
{code}
ret = inpValue.get(columns.get(0));
{code}

to
{code}
if (secondarySort) {
    ret = inpValue;
} else {
    ret = inpValue.get(columns.get(0));
}
{code}

it is not clear to me whether this is the right guess, and don't have idea how to get the boolean value secondarySort in POProject.java though.","05/Apr/13 18:11;daijy;There are two issues:
1. SecondaryKeyOptimizer does something wrong when foreach inner plan contains nested foreach. 
2. Order by sampler does not deal with nested tuples.

PIG-2265-1.patch fixed TestSecondarySort.testNestedSortEndToEnd3() by
1. Change SecondaryKeyOptimizer not to optimize when foreach inner plan contains nested foreach. This alone fix PIG-3049, but not testNestedSortEndToEnd3
2. Change MRCompiler not to flatten input tuple when doing sorting

Yet to finish all tests.","06/Apr/13 01:49;dreambird;Thanks for doing this, Daniel! I think it is not a bad idea to disable secondary key optimization for nested foreach, if it is too complex to get it right, as you suggested. I am reading SecondaryKeyOptimizer.java. Looks like I was in different direction before :)",06/Apr/13 06:34;daijy;All unit tests pass.,"07/Apr/13 19:48;cheolsoo;[~daijy], thank you very much for fixing this. I was curious about how to fix this.

I have few minor comments on your patch:
* Wouldn't it be better to remove the test cases that you're modifying in {{TestSecondarySort.java}}? Since you're disabling the optimization, these test cases don't test anything any more. So why not delete them?
* In addition, can you delete {{testDistinctOptimization1()}}, which is currently commented out due to PIG-2009? Looking at PIG-2009, this test case will be no longer valid since you're disabling the optimization anyway.
* Can you remove {{processForEach()}} in {{SecondaryKeyOptimizer.java}} instead of making it always return true? How about do the following instead?
{code}
@@ -481,10 +481,9 @@ public class SecondaryKeyOptimizer extends MROpPlanVisitor {
                     sawInvalidPhysicalOper = processSort((POSort)currentNode);
                 else if (currentNode instanceof POProject)
                     sawInvalidPhysicalOper = processProject((POProject)currentNode);
-                else if (currentNode instanceof POForEach)
-                    sawInvalidPhysicalOper = processForEach((POForEach)currentNode);
                 else if (currentNode instanceof POUserFunc ||
-                         currentNode instanceof POUnion)
+                         currentNode instanceof POUnion ||
+                         currentNode instanceof POForEach)
                     break;
{code}","08/Apr/13 19:30;daijy;[~cheolsoo], nice catch. PIG-2265-2.patch includes all your suggestions.",08/Apr/13 19:52;cheolsoo;+1. Looks good to me!,08/Apr/13 19:59;daijy;Patch committed to trunk. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change conf/log4j.properties to conf/log4j.properties.template,PIG-2264,12521319,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,05/Sep/11 00:00,05/Oct/11 17:20,14/Mar/19 03:07,08/Sep/11 23:32,0.10.0,0.9.0,,,,,,0.10.0,0.9.1,,,impl,,,0,,,,,,,,,,,,,"conf/log4j.properties is never used unless you specify -4 conf/conf/log4j.properties in the command line. This file serves as a template, name it log4j.properties is confusing. Also we should bundle it in the release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Sep/11 00:02;daijy;PIG-2264-1.patch;https://issues.apache.org/jira/secure/attachment/12492995/PIG-2264-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-08 23:25:45.079,,,no_permission,,,,,,,,,,,,35746,Reviewed,,,Thu Sep 08 23:32:24 UTC 2011,,,,,,,0|i09zpb:,56225,"The file conf/log4j.properties has been renamed log4j.properties.template, as it is intended to serve as a template for users to create their own log4j.properties files.  It is also now included in the release, where it was not in the past.",,,,,,,,,08/Sep/11 23:25;thejas;+1,08/Sep/11 23:32;daijy;Patch committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Restore support for parenthesis in Pig 0.9,PIG-2261,12521021,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,01/Sep/11 08:16,05/Oct/11 17:20,14/Mar/19 03:07,23/Sep/11 17:32,0.9.0,,,,,,,0.9.1,,,,impl,,,0,,,,,,,,,,,,,"Pig 0.8 and earlier versions used to support syntax such as 
 
{code}
A =(load ....)
{code}

This was removed as ""useless"" in 0.9 when the grammar was redone. It turns out that some user is using this for ease of code generation so we want to restore it back.

Just to clarify, Pig 0.9 continues to support composite statements such as

{code}
B = filter (load 'data' as (a, b)) by a > 0;
{code}

It just removed ""useless"" parenthesis and doesn't support statements like

{code}
A = (load 'data' as (a, b));
{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Sep/11 22:06;rding;PIG-2261.patch;https://issues.apache.org/jira/secure/attachment/12496187/PIG-2261.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-06 17:56:32.874,,,no_permission,,,,,,,,,,,,35749,Reviewed,,,Fri Sep 23 17:32:13 UTC 2011,,,,,,,0|i0h1tr:,97558,,,,,,,,,,"06/Sep/11 17:56;daijy;Hi, Richard, do you plan to work on it? Does it need to be fixed in 0.9.1?",22/Sep/11 22:06;rding;Attaching patch that restores the support for parenthesis.,"23/Sep/11 17:29;daijy;Unit test:
    all pass

test-patch:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

+1 for the patch.",23/Sep/11 17:32;daijy;Patch committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ant rpm doesn't work,PIG-2258,12520963,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,ashutoshc,ashutoshc,31/Aug/11 18:39,05/Oct/11 17:20,14/Mar/19 03:07,31/Aug/11 20:40,0.9.0,,,,,,,0.9.1,,,,build,,,0,,,,,,,,,,,,,"On a fresh checkout:
{code}
$ svn co http://svn.apache.org/repos/asf/pig/branches/branch-0.9/ .
$ ant
$ ant rpm
....
....
$ [rpm] Building the RPM based on the pig.spec file
  [rpm] error: line 221: second %prep
{code}","$ cat /etc/redhat-release 
Red Hat Enterprise Linux Server release 5.6 (Tikanga)

$ ant -version
Apache Ant(TM) version 1.8.2 compiled on December 20 2010",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-08-31 20:40:12.399,,,no_permission,,,,,,,,,,,,35751,,,,Wed Aug 31 20:40:12 UTC 2011,,,,,,,0|i09zqv:,56232,,,,,,,,,,31/Aug/11 20:40;daijy;Erroneously applied PIG-1857 patch twice on 0.9 branch. Fixed now.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroStorage doesn't recognize schema_file field when JSON isn't used in the constructor,PIG-2257,12520856,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,billgraham,billgraham,billgraham,30/Aug/11 23:38,22/Feb/13 04:54,14/Mar/19 03:07,24/Apr/12 06:59,,,,,,,,0.11,,,,,,,0,,,,,,,,,,,,,PIG-2195 introduced the {{schema_file}} constructor param to {{AvroStorage}}. This field is currently only supported when passing constructor data via JSON though (a different code path is used when constructor data is a String array).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Sep/11 00:03;billgraham;PIG-2257_1.patch;https://issues.apache.org/jira/secure/attachment/12492546/PIG-2257_1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-27 21:09:44.744,,,no_permission,,,,,,,,,,,,35752,,,,Wed Mar 28 02:25:33 UTC 2012,,,,,,,0|i02tvb:,14440,,,,,,,,,,01/Sep/11 00:03;billgraham;Attaching patch which fixes this issue.,"27/Mar/12 21:09;dvryaboy;Whoa, ancient times. Peeps who work with Avro, do you need this?","28/Mar/12 02:25;billgraham;This is actually a bug that should be patched. All constructs to {{AvroStorage}} should work whether being passed as json or a string array, but {{schema_file}} was only implemented in the former.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PIG leaks Zookeeper connections when using HBaseStorage,PIG-2251,12520569,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jamarkha,vbarat,vbarat,29/Aug/11 13:40,14/Oct/13 16:46,14/Mar/19 03:07,02/Jan/13 22:28,0.10.0,0.11,0.8.1,0.9.0,,,,0.12.0,,,,,,,0,,,,,,,,,,,,,"I run a set of PIG jobs from a Java process (using PigServer). Most of which use HBaseStorage to load data from HBase.
Each job is run using a new PigServer object, and I correctly call PigServer.shutdown() when my pig server is no longer used.

Nevertheless, after a few hours of run, I notice that the number of connections to my Zookeeper servers reach the limit (300 in my case).
It appears that each job leaks 4 or 5 Zookeeper connections.

It was not the case with PIG 0.6.1 + HBase 0.20.6

To solve this issue (temporarily) by killing the process running PIG after a few set of jobs have been run : connections are correctly closed.
My process don't use HBase by itself, only HBaseStorage, so I guess the leak is in the code of HBaseStorage: maybe to cnx to HBase are not closed.

All my request are simple request loading data from HBase, lik:

{code}

    pigServer.registerQuery(""start_sessions = LOAD '""
        + Analytics.getHBaseTableURL(""startSession"")
        + ""' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage('meta:sid meta:infoid meta:imei meta:timestamp') ""
        + ""AS (sid:chararray, infoid:chararray, imei:chararray, start:long);"");

    pigServer.registerQuery(""end_sessions = LOAD '""
        + Analytics.getHBaseTableURL(""endSession"")
        + ""' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage('meta:sid meta:timestamp meta:locid') ""
        + ""AS (sid:chararray, end:long, locid:chararray);"");

    pigServer.registerQuery(""sessions = JOIN start_sessions BY sid, end_sessions BY sid;"");

    pigServer.store(""sessions"", Analytics.getOutputFilePath(""sessions""), ""BinStorage"");


{code}


Code used to allocate a new PIG server:

{code}
  public static PigServer getNewPigServer() throws IOException
  {
    /* Get system properties */
    Properties properties = new Properties();

    /* Set specific Hadoop properties for PIG jobs */
    properties.setProperty(""mapred.child.java.opts"", ""-Xmx"" + childMemory + ""m"");

    /* Create PIG context */
    PigContext context = new PigContext(local ? ExecType.LOCAL : ExecType.MAPREDUCE, properties);

    /* Create the PIG server */
    PigServer pigServer = new PigServer(context);

    /* Register our User Defined Functions (UDFs) */
    pigServer.registerJar(pigUdfsPath);

    /* Register shortcuts for our UDFs */
    pigServer.registerFunction(""GetActivitiesLengthsRanges"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.GetActivitiesLengthsRanges""));
    pigServer.registerFunction(""GetActivitiesLinks"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.GetActivitiesLinks""));
    pigServer.registerFunction(""GetActivitiesPeriodsAndLengths"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.GetActivitiesPeriodsAndLengths""));
    pigServer.registerFunction(""GetCountRange"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.GetCountRange""));
    pigServer.registerFunction(""GetAllPeriods"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.GetAllPeriods""));
    pigServer.registerFunction(""GetCountRangeLabel"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.GetCountRangeLabel""));
    pigServer.registerFunction(""GetCountsAndLengthsByName"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.GetCountsAndLengthsByName""));
    pigServer.registerFunction(""GetCountsByName"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.GetCountsByName""));
    pigServer.registerFunction(""GetDayPeriod"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.GetDayPeriod""));
    pigServer.registerFunction(""GetDayWeekMonthPeriods"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.GetDayWeekMonthPeriods""));
    pigServer.registerFunction(""GetLengthRange"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.GetLengthRange""));
    pigServer.registerFunction(""GetLengthRangeLabel"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.GetLengthRangeLabel""));
    pigServer.registerFunction(""GetPeriods"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.GetPeriods""));
    pigServer.registerFunction(""GetPeriodsAndLengths"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.GetPeriodsAndLengths""));
    pigServer.registerFunction(""NormalizeCarrierName"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.NormalizeCarrierName""));
    pigServer.registerFunction(""NormalizeCountryCode"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.NormalizeCountryCode""));
    pigServer.registerFunction(""NormalizeLocaleCode"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.NormalizeLocaleCode""));
    pigServer.registerFunction(""NormalizeNetworkType"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.NormalizeNetworkType""));
    pigServer.registerFunction(""NormalizeNetworkSubType"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.NormalizeNetworkSubType""));
    pigServer.registerFunction(""NormalizePhoneManufacturer"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.NormalizePhoneManufacturer""));
    pigServer.registerFunction(""NormalizePhoneModel"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.NormalizePhoneModel""));
    pigServer.registerFunction(""NormalizeString"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.NormalizeString""));
    pigServer.registerFunction(""SubString"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.SubString""));
    pigServer.registerFunction(""GuessCountryCode"", new FuncSpec(
      ""com.ubikod.ermin.analytics.pigudf.GuessCountryCode""));

    /* Return this new instance of PIG server */
    return pigServer;
  }
{code}

Code used when PIG server no longer used:

{code}
    pigServer.shutdown();
{code}
","PIG 0.9 branch
HBase 0.90.3
HDFS 0.20-append
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Jan/13 06:45;jamarkha;PIG-2251.patch;https://issues.apache.org/jira/secure/attachment/12562887/PIG-2251.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-01 19:49:33.832,,,no_permission,,,,,,,,,,,,63381,,,,Thu Jan 03 04:27:33 UTC 2013,,,,,,,0|i0h1s7:,97551,,,,,,,,,,01/Jan/13 19:49;jamarkha;HTable cleanup to prevent ZooKeeper connection leaks.,01/Jan/13 19:52;jamarkha;HTable is always created but never closed.  Closing on job success/failure to release ZooKeeper connections.,"01/Jan/13 22:33;cheolsoo;Hello Jeff,

Thank you very much for the patch! I have one comment.

Looking at the HBaseStorage in trunk, HTable ({{m_table}}) is constructed but never used. Thus, I am wondering if we could just remove it. According to the commit history, PIG-2933 eliminated the use of that variable in HBaseStorage.
{code:title=PIG-2993}
-        m_table.setScannerCaching(caching_);
{code}
Removing {{m_table}} will address connection leaks, won't it?",02/Jan/13 06:43;jamarkha;Probably best.  Didn't look at the history.  Good catch.  Submitting a new patch to get rid of the HTable.  The constructor used would have led to HConnectionManager.getConnection(m_conf) which is where the connections were coming from.,02/Jan/13 06:45;jamarkha;Better patch to remove HTable.,"02/Jan/13 21:55;cheolsoo;+1. I will commit it after running tests.

Btw, next time could you generate patches with ""{{svn diff}}"" or ""{{git diff --no-prefix}}""? I usually apply patches by ""{{patch -p0 -i <filename>}}"", but your patch doesn't automatically apply. 
{code:title=Desired}
Index: src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java
===================================================================
--- src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java	(revision 1428068)
+++ src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java	(working copy)
{code}
{code:title=Yours}
Index: HBaseStorage.java
===================================================================
--- HBaseStorage.java   (revision 1427468)
+++ HBaseStorage.java   (working copy)
{code}",02/Jan/13 22:28;cheolsoo;Committed to trunk. Thanks Jeff!,03/Jan/13 04:27;jamarkha;Got it.  Thanks for the help.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig 0.9 error message not useful as compared to 0.8,PIG-2250,12520536,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,vivekp,vivekp,29/Aug/11 08:28,26/Apr/12 20:32,14/Mar/19 03:07,30/Aug/11 09:24,0.9.0,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"Another instance of change in error message from 0.8 to 0.9 due to parser modifications.
This improper error message is due to \n in the UDF arguments.

The below is a sample script;

a = load 'input' using myLoader('a1,a2,
        a3,a4');
dump a;


Error Message from 0.9
----------------------
ERROR 1200: Pig script failed to parse: MismatchedTokenException(93!=3)


Error Message from 0.8
------------------------
 ERROR 1000: Error during parsing. Lexical error at line 1, column 40.  Encountered: ""\n"" (10), after : ""\'a1,a2,""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-08-30 08:35:43.31,,,no_permission,,,,,,,,,,,,63203,,,,Tue Aug 30 09:24:35 UTC 2011,,,,,,,0|i0h1rz:,97550,,,,,,,,,,30/Aug/11 08:35;daijy;Seems it is fixed by PIG-2215 on trunk.,30/Aug/11 09:24;vivekp;Verified this with trunk. Sorry for the trouble.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LIMIT generates wrong number of records if pig determines no of reducers as more than 1,PIG-2237,12519944,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,anitharaju,anitharaju,24/Aug/11 10:37,05/Oct/11 17:20,14/Mar/19 03:07,23/Sep/11 18:18,0.8.0,0.9.0,,,,,,0.10.0,0.9.1,,,,,,0,,,,,,,,,,,,,"Hi,

For a script

========
A = load 'test.txt' using PigStorage() as (a:int,b:int);
B = order A by a ;
C = limit B 2;
store C into 'op1' using PigStorage();
========

Limit and ORDER BY are done in the same MR job if no explicit PARALLELism is mentioned.
In this case, the no of reducers are determined by pig and sometimes it is calculated > 1.
Since limit happens at the reduce side, each reduce tasks does a limit separately generating n*2 records where n is the no of reduce tasks calculated by pig.

If an explicit specification of no of reduce tasks using PARALLEL keyword is done on ORDER BY,

==========
B = order A by a PARALLEL 4;
==========

another MR is created with 1 reduce task where the limit is done. 

In short, the issue occurs when the no of reducers calculated by pig is greater than 1 and a limit is involved in the MR.

The issue can be replicated by specifying

==========
-Dpig.exec.reducers.bytes.per.reducer
==========

The issue is seen in 0.8 and 0.9 version. It works good in 0.7

Regards,
Anitha",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Aug/11 21:29;daijy;PIG-2237-1.patch;https://issues.apache.org/jira/secure/attachment/12491693/PIG-2237-1.patch,25/Aug/11 22:40;daijy;PIG-2237-2.patch;https://issues.apache.org/jira/secure/attachment/12491700/PIG-2237-2.patch,29/Aug/11 23:33;daijy;PIG-2237-3.patch;https://issues.apache.org/jira/secure/attachment/12492161/PIG-2237-3.patch,21/Sep/11 20:49;daijy;PIG-2237-4.patch;https://issues.apache.org/jira/secure/attachment/12495440/PIG-2237-4.patch,23/Sep/11 18:15;daijy;PIG-2237-4_0.9.patch;https://issues.apache.org/jira/secure/attachment/12496296/PIG-2237-4_0.9.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-08-24 23:00:02.572,,,no_permission,,,,,,,,,,,,35753,Reviewed,,,Fri Sep 23 18:18:42 UTC 2011,,,,,,,0|i09zlz:,56210,,,,,,,,,,"24/Aug/11 23:00;daijy;This is because SampleOptimizer will change the parallel size for ""order by"" according to input size, at this time, LimitAdjuster already determined whether or not to add one additional limit job. We need to do LimitAdjuster after SampleOptimizer.",25/Aug/11 22:40;daijy;PIG-2237-2.patch fix test-patch failures.,"25/Aug/11 22:42;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1664/
-----------------------------------------------------------

Review request for pig and Thejas Nair.


Summary
-------

See PIG-2237


This addresses bug PIG-2237.
    https://issues.apache.org/jira/browse/PIG-2237


Diffs
-----

  trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/LimitAdjuster.java PRE-CREATION 
  trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java 1161381 
  trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRUtil.java PRE-CREATION 
  trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java 1161381 
  trunk/test/org/apache/pig/test/TestEvalPipeline2.java 1161381 

Diff: https://reviews.apache.org/r/1664/diff


Testing
-------

Test-patch:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Unit test:
    all pass.


Thanks,

Daniel

","29/Aug/11 23:23;daijy;Find one problem, update PIG-2237-3.patch","29/Aug/11 23:25;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1664/
-----------------------------------------------------------

(Updated 2011-08-29 23:23:51.893824)


Review request for pig and Thejas Nair.


Summary
-------

See PIG-2237


This addresses bug PIG-2237.
    https://issues.apache.org/jira/browse/PIG-2237


Diffs (updated)
-----

  trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java 1162260 
  trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java 1162260 
  trunk/test/org/apache/pig/test/TestEvalPipeline2.java 1162260 
  trunk/test/org/apache/pig/test/TestMRCompiler.java 1162260 

Diff: https://reviews.apache.org/r/1664/diff


Testing
-------

Test-patch:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Unit test:
    all pass.


Thanks,

Daniel

","29/Aug/11 23:34;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1664/
-----------------------------------------------------------

(Updated 2011-08-29 23:34:23.448085)


Review request for pig and Thejas Nair.


Summary
-------

See PIG-2237


This addresses bug PIG-2237.
    https://issues.apache.org/jira/browse/PIG-2237


Diffs (updated)
-----

  trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/LimitAdjuster.java PRE-CREATION 
  trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java 1162260 
  trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRUtil.java PRE-CREATION 
  trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java 1162260 
  trunk/test/org/apache/pig/test/TestEvalPipeline2.java 1162260 
  trunk/test/org/apache/pig/test/TestMRCompiler.java 1162260 

Diff: https://reviews.apache.org/r/1664/diff


Testing
-------

Test-patch:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Unit test:
    all pass.


Thanks,

Daniel

","30/Aug/11 00:16;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1664/#review1684
-----------------------------------------------------------



trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/LimitAdjuster.java
<https://reviews.apache.org/r/1664/#comment3838>

    please fix the indentation for the contents of this if block, and add {}s 



trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/LimitAdjuster.java
<https://reviews.apache.org/r/1664/#comment3839>

    you keep having to cast it. Just add 
    
    POStore storeOp = (POStore) mpLeaf;
    
    at the beginning of the block; it'll clean up the code.



trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRUtil.java
<https://reviews.apache.org/r/1664/#comment3841>

    please add documentation for Pig Developers indicating when and how to use the methods in this helper class.



trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRUtil.java
<https://reviews.apache.org/r/1664/#comment3840>

    will using this mess up projection push-down?


- Dmitriy


On 2011-08-29 23:34:23, Daniel Dai wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/1664/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-08-29 23:34:23)
bq.  
bq.  
bq.  Review request for pig and Thejas Nair.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  See PIG-2237
bq.  
bq.  
bq.  This addresses bug PIG-2237.
bq.      https://issues.apache.org/jira/browse/PIG-2237
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/LimitAdjuster.java PRE-CREATION 
bq.    trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java 1162260 
bq.    trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRUtil.java PRE-CREATION 
bq.    trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java 1162260 
bq.    trunk/test/org/apache/pig/test/TestEvalPipeline2.java 1162260 
bq.    trunk/test/org/apache/pig/test/TestMRCompiler.java 1162260 
bq.  
bq.  Diff: https://reviews.apache.org/r/1664/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  Test-patch:
bq.       [exec] +1 overall.  
bq.       [exec] 
bq.       [exec]     +1 @author.  The patch does not contain any @author tags.
bq.       [exec] 
bq.       [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
bq.       [exec] 
bq.       [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
bq.       [exec] 
bq.       [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
bq.       [exec] 
bq.       [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
bq.       [exec] 
bq.       [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
bq.  
bq.  Unit test:
bq.      all pass.
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Daniel
bq.  
bq.

","21/Sep/11 20:49;jiraposter@reviews.apache.org;

bq.  On 2011-08-30 00:15:01, Dmitriy Ryaboy wrote:
bq.  > trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRUtil.java, line 36
bq.  > <https://reviews.apache.org/r/1664/diff/3/?file=36244#file36244line36>
bq.  >
bq.  >     will using this mess up projection push-down?

This function only used in map-reduce layer. ""projection push-down"" is in logical layer. They should not interfere each other. What's your concern?

I will address all other comments (Actually they all come from original code I restructured :) )


- Daniel


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1664/#review1684
-----------------------------------------------------------


On 2011-08-29 23:34:23, Daniel Dai wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/1664/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-08-29 23:34:23)
bq.  
bq.  
bq.  Review request for pig and Thejas Nair.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  See PIG-2237
bq.  
bq.  
bq.  This addresses bug PIG-2237.
bq.      https://issues.apache.org/jira/browse/PIG-2237
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/LimitAdjuster.java PRE-CREATION 
bq.    trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java 1162260 
bq.    trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRUtil.java PRE-CREATION 
bq.    trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java 1162260 
bq.    trunk/test/org/apache/pig/test/TestEvalPipeline2.java 1162260 
bq.    trunk/test/org/apache/pig/test/TestMRCompiler.java 1162260 
bq.  
bq.  Diff: https://reviews.apache.org/r/1664/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  Test-patch:
bq.       [exec] +1 overall.  
bq.       [exec] 
bq.       [exec]     +1 @author.  The patch does not contain any @author tags.
bq.       [exec] 
bq.       [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
bq.       [exec] 
bq.       [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
bq.       [exec] 
bq.       [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
bq.       [exec] 
bq.       [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
bq.       [exec] 
bq.       [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
bq.  
bq.  Unit test:
bq.      all pass.
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Daniel
bq.  
bq.

",21/Sep/11 20:49;daijy;PIG-2237-4.patch address Dmitriy's review comment.,21/Sep/11 21:07;thejas;+1,23/Sep/11 18:15;daijy;A slightly different patch for 0.9 branch.,23/Sep/11 18:18;daijy;Patch committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""declare"" document contains a typo",PIG-2232,12519722,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,daijy,daijy,daijy,22/Aug/11 18:50,26/Apr/12 20:32,14/Mar/19 03:07,22/Aug/11 19:14,0.10.0,0.8.1,0.9.1,,,,,0.10.0,,,,documentation,,,0,,,,,,,,,,,,,"This is reported by Dexin: http://mail-archives.apache.org/mod_mbox//pig-user/201108.mbox/%3CCAJG3nVGYgbOnRdqnpY2B8ET2X34VSMGq8Bh=SnsAVUvoNUOWEg@mail.gmail.com%3E

%declare CMD 'generate_date';

should use ""back ticks"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Aug/11 18:53;daijy;PIG-2232-1.patch;https://issues.apache.org/jira/secure/attachment/12491245/PIG-2232-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,64163,,,,Mon Aug 22 19:14:13 UTC 2011,,,,,,,0|i0h1pb:,97538,,,,,,,,,,22/Aug/11 19:14;daijy;Commit it since it is a trivial fix.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit produce wrong number of records after foreach flatten,PIG-2231,12519721,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,22/Aug/11 18:39,05/Oct/11 17:20,14/Mar/19 03:07,25/Aug/11 04:48,0.10.0,0.8.1,0.9.0,,,,,0.10.0,0.9.1,,,impl,,,0,,,,,,,,,,,,,"From user mailing list: http://mail-archives.apache.org/mod_mbox/pig-user/201108.mbox/%3CCAPad=8E032ksPjy2bOynQezo1x+=0jXBh4t+MstS7G_fvj_EhQ@mail.gmail.com%3E, Sungho reported the following script produce wrong result as expected:

data = LOAD '1.txt' AS (k, v);
grouped = GROUP data BY k;
selected = LIMIT grouped 2;
flattened = FOREACH selected GENERATE FLATTEN (data);
dump flattened;

1.txt:
1       A
1       B
2       C
3       D
3       E
3       F

Expected result:
(1, A)
(1, B)
(2, C)

We get:
(1, A)
(1, B)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Aug/11 23:22;daijy;PIG-2231-1.patch;https://issues.apache.org/jira/secure/attachment/12491288/PIG-2231-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-22 18:58:08.804,,,no_permission,,,,,,,,,,,,46540,Reviewed,,,Thu Aug 25 04:48:32 UTC 2011,,,,,,,0|i0h1p3:,97537,,,,,,,,,,22/Aug/11 18:58;olgan;I believe this should go into 9.1 given that we might producing wrong results,"23/Aug/11 17:08;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1627/
-----------------------------------------------------------

Review request for pig and Thejas Nair.


Summary
-------

See PIG-2231


This addresses bug PIG-2231.
    https://issues.apache.org/jira/browse/PIG-2231


Diffs
-----

  trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java 1160494 
  trunk/test/org/apache/pig/test/TestEvalPipeline2.java 1160494 

Diff: https://reviews.apache.org/r/1627/diff


Testing
-------

test-patch:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Unit test:
    all pass


Thanks,

Daniel

","24/Aug/11 23:51;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1627/#review1621
-----------------------------------------------------------

Ship it!


+1

- Thejas


On 2011-08-23 17:08:10, Daniel Dai wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/1627/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-08-23 17:08:10)
bq.  
bq.  
bq.  Review request for pig and Thejas Nair.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  See PIG-2231
bq.  
bq.  
bq.  This addresses bug PIG-2231.
bq.      https://issues.apache.org/jira/browse/PIG-2231
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java 1160494 
bq.    trunk/test/org/apache/pig/test/TestEvalPipeline2.java 1160494 
bq.  
bq.  Diff: https://reviews.apache.org/r/1627/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  test-patch:
bq.       [exec] +1 overall.  
bq.       [exec] 
bq.       [exec]     +1 @author.  The patch does not contain any @author tags.
bq.       [exec] 
bq.       [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
bq.       [exec] 
bq.       [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
bq.       [exec] 
bq.       [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
bq.       [exec] 
bq.       [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
bq.       [exec] 
bq.       [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
bq.  
bq.  Unit test:
bq.      all pass
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Daniel
bq.  
bq.

",25/Aug/11 04:48;daijy;Committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong jars copied into lib directory in e2e tests when invoked from top level,PIG-2227,12519190,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,17/Aug/11 21:39,26/Apr/12 20:33,14/Mar/19 03:07,19/Aug/11 20:02,0.10.0,,,,,,,0.10.0,,,,build,,,0,,,,,,,,,,,,,"If you do ""ant test-e2e"" in the top level directory the wrong jars get copied into test/e2e/pig/testdist/lib.  The hadoop example jars belong there, instead jars from the top level lib directory are copied there.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Aug/11 16:28;alangates;PIG-2227.patch;https://issues.apache.org/jira/secure/attachment/12490815/PIG-2227.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-19 17:50:23.853,,,no_permission,,,,,,,,,,,,64383,,,,Fri Aug 19 20:02:33 UTC 2011,,,,,,,0|i0h1of:,97534,,,,,,,,,,"18/Aug/11 16:28;alangates;Fixes build file so proper jar are copied into e2e lib directory.

Also adds ignores for a few tests that were failing and need investigation.",19/Aug/11 17:50;daijy;+1,19/Aug/11 20:02;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
error accessing column in output schema of udf having project-star input,PIG-2223,12519071,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,17/Aug/11 04:35,05/Oct/11 17:20,14/Mar/19 03:07,27/Sep/11 05:29,0.9.0,,,,,,,0.10.0,0.9.1,,,,,,0,,,,,,,,,,,,,"(from Grahame's email) -
{code}
describe a;
a: {f1: int,f2: int,f3: int,f4: int,f5: int,f6: int,f7: int,f8: int,f9: int,f10: int}

aa = FOREACH a GENERATE $0, TOTUPLE($2,$3,$4,$5);
aaa = FOREACH aa GENERATE $0, $1.$0; -- OK
aaa = FOREACH aa GENERATE $0, $1.f3; -- OK
aaa = FOREACH aa GENERATE $0, $1.$1; -- OK
aaa = FOREACH aa GENERATE $0, $1.f4; -- OK

aa = FOREACH a GENERATE $0, TOTUPLE($2..$5); -- should be the same as above?
aaa = FOREACH aa GENERATE $0, $1.$0; -- OK
aaa = FOREACH aa GENERATE $0, $1.f3; -- ERROR
aaa = FOREACH aa GENERATE $0, $1.$1; -- ERROR
aaa = FOREACH aa GENERATE $0, $1.f4; -- ERROR
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Aug/11 04:37;thejas;PIG-2223.1.patch;https://issues.apache.org/jira/secure/attachment/12490606/PIG-2223.1.patch,06/Sep/11 18:43;thejas;PIG-2223.2.patch;https://issues.apache.org/jira/secure/attachment/12493191/PIG-2223.2.patch,27/Sep/11 05:26;thejas;PIG-2223.3.09.patch;https://issues.apache.org/jira/secure/attachment/12496634/PIG-2223.3.09.patch,27/Sep/11 05:26;thejas;PIG-2223.3.patch;https://issues.apache.org/jira/secure/attachment/12496633/PIG-2223.3.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2011-09-08 21:50:24.969,,,no_permission,,,,,,,,,,,,14962,,,,Tue Sep 27 05:29:08 UTC 2011,,,,,,,0|i09zlj:,56208,,,,,,,,,,17/Aug/11 04:37;thejas;Initial patch without test case.,"06/Sep/11 18:13;thejas;Grahame reported another issue with the patch -
{code}
Hi Thejas,

I applied the patch and rebuilt.  The initial bug is gone, but there looks to be another:

describe a;
a: {f1: int,f2: int,f3: int,f4: int,f5: int,f6: int,f7: int,f8: int,f9: int,f10: int}

aa = FOREACH a GENERATE $0, TOTUPLE($2,$3,$4,$5);
aaa = FOREACH aa GENERATE $0, $1.f4 as v; -- OK
aaaa = FOREACH aaa GENERATE v; -- OK

aa = FOREACH a GENERATE $0, TOTUPLE($2..$5);
aaa = FOREACH aa GENERATE $0, $1.f4 as v; -- OK after patch
aaaa = FOREACH aaa GENERATE v; -- ERROR
2011-08-18 11:00:36,246 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1128: Cannot find field f4 in :tuple(f3:int,f4:int,f5:int,f6:int)

Thanks,
Grahame
{code}","06/Sep/11 18:42;thejas;The second error is thrown during a getSchema call made during parsing, to determine if an identifier is a scalar variable or not. The project star expansion has not happened at that point, resulting in exception such as the following.
{code}

Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1128: Cannot find field b in :tuple(a:int,b:int,c:int)
        at org.apache.pig.newplan.logical.expression.DereferenceExpression.translateAliasToPos(DereferenceExpression.java:201)
        at org.apache.pig.newplan.logical.expression.DereferenceExpression.getFieldSchema(DereferenceExpression.java:169)
        at org.apache.pig.newplan.logical.relational.LOGenerate.getSchema(LOGenerate.java:88)
        at org.apache.pig.newplan.logical.relational.LOForEach.getSchema(LOForEach.java:63)
        at org.apache.pig.parser.LogicalPlanGenerator.alias_col_ref(LogicalPlanGenerator.java:15110)
        ... 23 more

{code}
",07/Sep/11 17:34;thejas;Patch passes unit tests and test-patch,08/Sep/11 21:50;daijy;+1,"27/Sep/11 05:26;thejas;PIG-2223.3.patch - patch regenerated for latest trunk.
","27/Sep/11 05:26;thejas;PIG-2223.3.09.patch - patch regenerated for 0.9 branch.
",27/Sep/11 05:29;thejas;patch committed to 0.9 branch and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Couldnt find documentation for ColumnMapKeyPrune optimization rule,PIG-2221,12518965,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,vivekp,vivekp,16/Aug/11 07:56,05/Oct/11 17:20,14/Mar/19 03:07,31/Aug/11 23:04,0.8.1,,,,,,,0.10.0,0.9.1,,,documentation,,,0,,,,,,,,,,,,,"There are no documentations for some of the Optimization Rules, 
For ex; ColumnMapKeyPrune
in http://pig.apache.org/docs/r0.8.1/piglatin_ref1.html#Optimization+Rules

And moreover I believe the documentaion should be saying how to disable these rules using -t option. 
It would be nice if the documentation could talk of some uses cases where it makes sense to disable the optimization rule. 

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Aug/11 18:30;daijy;PIG-2221-help.patch;https://issues.apache.org/jira/secure/attachment/12490681/PIG-2221-help.patch,31/Aug/11 00:36;chandec;pig-2221.patch;https://issues.apache.org/jira/secure/attachment/12492401/pig-2221.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-08-16 17:56:56.43,,,no_permission,,,,,,,,,,,,35755,Reviewed,,,Wed Aug 31 23:04:51 UTC 2011,,,,,,,0|i0h1nj:,97530,,,,,,,,,,"16/Aug/11 17:56;olgan;Hi Daniel,

Could you provide information about missing optimization rules and then assign the ticket to Corinne, thanks!","17/Aug/11 18:30;daijy;There are couple of places need to change in http://pig.apache.org/docs/r0.9.0/perf.html#optimization-rules. Here is the summary:
1. I'd like to drop mandatory rules (ImplicitSplitInserter, StreamOptimizer, TypeCastInserter), since user don't have control over it. It also involves lots of implementation details and hard to explain. Or we can make a separate section for it. Thoughts?
2. Change name of some rules to match command line switch:
OpLimitOptimizer -> LimitOptimizer
LogicalExpressionSimplifier -> FilterLogicExpressionSimplifier
PushDownExplodes -> PushDownForEachFlatten
3. Change the order of rules to match actual running order
4. Add missing rules: SplitFilter, MergeFilter, ColumnMapKeyPrune, AddForEach, GroupByConstParallelSetter
5. Minor change in description of FilterLogicExpressionSimplifier to indicate this only optimize the filter expression

Since there are quite a few change, I put together a wiki page to show how the section look like after change: https://cwiki.apache.org/confluence/display/PIG/Optimization+Rules

Also attach a patch to align the help text with the document.",31/Aug/11 00:36;chandec;Includes fixes for (1) optimzation rules (perf.xml) and (2) required updates to the Pig Index (pig-index.xml),31/Aug/11 00:36;chandec;Apply patch to Trunk and Branch-9. ,31/Aug/11 23:04;daijy;Both pig-2221.patch and PIG-2221-help.patch have been committed to trunk and 0.9 branch. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig tests fail if ${user.home}/pigtest/conf does not already exist,PIG-2219,12518856,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,cwsteinbach,cwsteinbach,cwsteinbach,15/Aug/11 06:16,26/Apr/12 20:32,14/Mar/19 03:07,16/Aug/11 22:13,,,,,,,,0.10.0,,,,build,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Aug/11 06:59;cwsteinbach;PIG-2219.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12490406/PIG-2219.1.patch.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-15 16:22:43.688,,,no_permission,,,,,,,,,,,,37465,,,,Tue Aug 16 23:44:50 UTC 2011,,,,,,,0|i05ihb:,30091,,,,,,,,,,"15/Aug/11 06:43;cwsteinbach;{noformat}
% rm -rf ~/pigtest
% ant clean jar test -Dtestcase=TestAlgebraicEval
Buildfile: /Users/carl/Work/repos/pig/build.xml
...
test-core:
    [mkdir] Created dir: /Users/carl/Work/repos/pig/build/test/logs
    [mkdir] Created dir: /var/folders/b7/b7UUwNZdF1KKHtM+5la6f++++TI/-Tmp-/pig_junit_tmp964082329
    [junit] WARNING: multiple versions of ant detected in path for junit 
    [junit]          jar:file:/Users/carl/.local/java/ant/lib/ant.jar!/org/apache/tools/ant/Project.class
    [junit]      and jar:file:/Users/carl/.ivy2/cache/ant/ant/jars/ant-1.6.5.jar!/org/apache/tools/ant/Project.class
    [junit] Running org.apache.pig.test.TestAlgebraicEval
    [junit] Tests run: 6, Failures: 0, Errors: 6, Time elapsed: 4.589 sec
    [junit] Test org.apache.pig.test.TestAlgebraicEval FAILED
   [delete] Deleting directory /var/folders/b7/b7UUwNZdF1KKHtM+5la6f++++TI/-Tmp-/pig_junit_tmp964082329

BUILD FAILED
/Users/carl/Work/repos/pig/build.xml:733: The following error occurred while executing this line:
/Users/carl/Work/repos/pig/build.xml:669: The following error occurred while executing this line:
/Users/carl/Work/repos/pig/build.xml:728: Tests failed!

Total time: 2 minutes 16 seconds

% ls -altR ~/pigtest/

/Users/carl/pigtest/:
total 0
drwxr-xr-x    3 carl staff  102 Aug 14 23:14 conf
drwxr-xr-x    3 carl staff  102 Aug 14 23:14 .
drwxr-xr-x+ 106 carl staff 3604 Aug 14 23:14 ..

/Users/carl/pigtest/conf:
total 16
-rw-r--r-- 1 carl staff 15097 Aug 14 23:14 hadoop-site.xml
drwxr-xr-x 3 carl staff   102 Aug 14 23:14 .
drwxr-xr-x 3 carl staff   102 Aug 14 23:14 ..

% ant test -Dtestcase=TestAlgebraicEval
...
BUILD SUCCESSFUL
%
{noformat}","15/Aug/11 07:01;cwsteinbach;Turns out Ant will only add ${user.home}/pigtest/conf
to the classpath if the directory already exists. I updated
the build script to automatically create this directory
and now everything works fine.","15/Aug/11 16:22;eli;Lgtm.

Could a committer review/commit?",15/Aug/11 21:24;alangates;I'll review it.,15/Aug/11 21:28;eli;Thanks Alan!,"16/Aug/11 22:13;alangates;Patch checked in.  Thanks Carl.

I applied this to trunk.  I notice you marked fix version as 0.9.1.  Do you think this should be applied to the 0.9 branch as well?","16/Aug/11 23:44;eli;I think 0.10 is fine, I only marked it 0.9.1 because I thought that was the next release. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Newlines in function arguments still cause exceptions to be thrown,PIG-2215,12518608,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,awarring,awarring,awarring,11/Aug/11 16:36,26/Apr/12 20:32,14/Mar/19 03:07,13/Aug/11 15:47,0.9.0,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"PIG-1749 was an attempt to allow newlines in function arguments. It appears that the AstValidator and the LogicalPlanGenerator grammars were not updated, so the following exception and stracktrace will be thrown when executing a script that has newlines in function arguments:

ERROR 1200: Pig script failed to parse: MismatchedTokenException(93!=3)

Failed to parse: Pig script failed to parse: MismatchedTokenException(93!=3)
        at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:178)
        at org.apache.pig.PigServer$Graph.validateQuery(PigServer.java:1622)
        at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1595)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:583)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:942)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:386)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:188)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:164)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:67)
        at org.apache.pig.Main.run(Main.java:487)
        at org.apache.pig.Main.main(Main.java:108)
Caused by: MismatchedTokenException(93!=3)
        at org.apache.pig.parser.AstValidator.recoverFromMismatchedToken(AstValidator.java:209)
        at org.antlr.runtime.BaseRecognizer.match(BaseRecognizer.java:115)
        at org.apache.pig.parser.AstValidator.func_clause(AstValidator.java:3497)
        at org.apache.pig.parser.AstValidator.load_clause(AstValidator.java:2464)
        at org.apache.pig.parser.AstValidator.op_clause(AstValidator.java:934)
        at org.apache.pig.parser.AstValidator.general_statement(AstValidator.java:574)
        at org.apache.pig.parser.AstValidator.statement(AstValidator.java:396)
        at org.apache.pig.parser.AstValidator.query(AstValidator.java:306)
        at org.apache.pig.parser.QueryParserDriver.validateAst(QueryParserDriver.java:236)
        at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:168)
        ... 10 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11/Aug/11 16:37;awarring;PIG-2215-0.patch;https://issues.apache.org/jira/secure/attachment/12490131/PIG-2215-0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-12 17:07:47.649,,,no_permission,,,,,,,,,,,,65049,,,,Sun Aug 14 01:53:19 UTC 2011,,,,,,,0|i0h1mf:,97525,,,,,,,,,,"11/Aug/11 16:38;awarring;This patch updates the LogicalPlanGenerator and AstValidator grammars, and adds 2 unit tests to test the new functionality.",12/Aug/11 17:07;alangates;Patch looks good.  I'll run it through nightly and end-to-end tests.,12/Aug/11 17:21;awarring;Thanks Alan!,"12/Aug/11 17:45;alangates;Results from test patch:

     [exec]
     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec]
     [exec]
     [exec]",13/Aug/11 02:04;alangates;No regressions were seen in the end-to-end tests.,13/Aug/11 15:47;alangates;Unit tests passed as well.  Patch checked in.  Thanks Adam.,14/Aug/11 01:53;awarring;Thanks for committing the patch Alan.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InternalSortedBag two-arg constructor doesn't pass bagCount,PIG-2214,12518524,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,sallen,sallen,sallen,10/Aug/11 22:39,26/Apr/12 20:33,14/Mar/19 03:07,13/Aug/11 15:18,,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"The InternalSortedBag(int bagCount, Comparator<Tuple> comp) constructor doesn't properly pass bagCount to the three-arg constructor.

Fixing this issue may have an effect on POSort.java line 268 which uses the two-arg constructor but the specified bagCount (3) is being ignored and the default (1) is used instead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Aug/11 22:41;sallen;PIG-2214-r1156390.patch;https://issues.apache.org/jira/secure/attachment/12490044/PIG-2214-r1156390.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-12 16:52:40.483,,,no_permission,,,,,,,,,,,,65102,,,,Sat Aug 13 15:18:11 UTC 2011,,,Patch Available,,,,0|i0h1m7:,97524,,,,,,,,,,12/Aug/11 16:52;alangates;Patch looks good.  I'll run it through the tests.,13/Aug/11 01:02;alangates;Unit tests pass.,13/Aug/11 15:18;alangates;End-to-end tests showed no regressions.  Patch checked in.  Thanks Stephen.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
update documentation for use of exec with no-args,PIG-2211,12518492,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,thejas,thejas,10/Aug/11 18:15,05/Oct/11 17:20,14/Mar/19 03:07,11/Aug/11 22:52,0.9.0,,,,,,,0.10.0,0.9.1,,,,,,0,,,,,,,,,,,,,"In the description of the arguments of the exec commands, it shows that the script argument is compulsory.


{code}
exec [-param param_name = param_value] [-param_file file_name] script  
{code}
should be

{code} 
exec [-param param_name = param_value] [-param_file file_name] [script]  
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-08-11 22:52:16.501,,,no_permission,,,,,,,,,,,,46537,,,,Thu Aug 11 22:52:16 UTC 2011,,,,,,,0|i0h1lj:,97521,,,,,,,,,,11/Aug/11 22:52;chandec;Fix for this JIRA included in PIG-2213 (pig-2213-patch-1.patch). Add any comments to PIG-2213.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JsonMetadata fails to find schema for glob paths,PIG-2209,12518385,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,daijy,dvryaboy,dvryaboy,09/Aug/11 23:47,26/Apr/12 20:33,14/Mar/19 03:07,13/Nov/11 23:48,0.10.0,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"JsonMetadata, used in PigStorage to work with serialized schemas, does not correctly interpret paths like '/foo/bar/{1,2,3}' and throws an exception:

{code}
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1131: Could not find schema file for file:///foo/bar/{1,2}
	at org.apache.pig.builtin.JsonMetadata.nullOrException(JsonMetadata.java:217)
	at org.apache.pig.builtin.JsonMetadata.getSchema(JsonMetadata.java:186)
	at org.apache.pig.builtin.PigStorage.getSchema(PigStorage.java:438)
	at org.apache.pig.newplan.logical.relational.LOLoad.getSchemaFromMetaData(LOLoad.java:150)
	... 17 more
Caused by: java.io.IOException: Unable to read file:///foo/bar/z/{1,2}
	at org.apache.pig.builtin.JsonMetadata.findMetaFile(JsonMetadata.java:106)
	at org.apache.pig.builtin.JsonMetadata.getSchema(JsonMetadata.java:183)
	... 19 more
Caused by: java.net.URISyntaxException: Illegal character in path at index 36: file:///foo/bar/{1,2}
	at java.net.URI$Parser.fail(URI.java:2809)
	at java.net.URI$Parser.checkChars(URI.java:2982)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Nov/11 00:22;daijy;PIG-2209-1.patch;https://issues.apache.org/jira/secure/attachment/12502556/PIG-2209-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-03 00:30:50.548,,,no_permission,,,,,,,,,,,,41687,Reviewed,,,Sun Nov 13 23:48:10 UTC 2011,,,,,,,0|i09zc7:,56166,,,,,,,,,,11/Aug/11 18:50;dvryaboy;Slight correction: this only happens when the items referred to in the glob are directories. It works fine when they are files or parts of file names.,"17/Aug/11 21:15;dvryaboy;Upgraded to blocker, as this can break existing scripts. I'll work on this.",03/Nov/11 00:30;olgan;Dmitry are you planning to resolve this quickly? ,"03/Nov/11 00:39;dvryaboy;I can try but I am not sure I have the time at the moment.. it's my bad, though, so if no one can make time, I'll figure something out to unblock the release. ","03/Nov/11 18:36;daijy;Hi, Dmitriy, I can make time, can I help on this?",03/Nov/11 21:18;dvryaboy;that would be fantastic.,"03/Nov/11 22:55;daijy;I also see by default, PigStorage will search for schema files in input directory, is that intentional? That will add burden to namenode. Shall we flip it?",03/Nov/11 23:03;olgan;I don't think that's what we want. This could be costly and non-backward compatible,"03/Nov/11 23:24;dvryaboy;I think the behavior we want is 
1) if the schema exists, use it
2) if the schema does not exist, behave as it behaves currently.

If we get rid of the per-file schema thing, and instead always only look for a .pig_schema file in the directory that contains the files, the cost should be relatively small.","03/Nov/11 23:43;daijy;That way we avoid globStatus, but still several existence check is needed. Provide most PigStorage use cases don't use schema I believe, it should disable by default. How about a global flag? ","03/Nov/11 23:53;dvryaboy;Global flag works for me (in my case, most uses of pig storage *would* use the schema).
","03/Nov/11 23:56;dvryaboy;Actually, having given it 30 seconds more thought, I disagree with myself. It should be enabled by default. Otherwise most users won't get the benefit (because they won't know they can), and having this on by default is a huge usability benefit. The few users who are running clusters with heavy enough load that the extra NN call makes a difference can turn this off -- and we can call this out in release notes.  I think this is a case where the benefits are worth the cost. Every single file generated with PigStorage will have a schema associated with it, that'll be very useful.","04/Nov/11 00:10;daijy;You assume the input file is generated by PigStorage. But in many cases, it is not true.","04/Nov/11 00:14;dvryaboy;Not quite, I am saying there is huge benefit when it *is* generated by Pig, and there is fairly little cost when it isn't. If it's an actual problem (and I doubt it would be), we should give a property to turn this behavior off. Prematurely optimizing by kneecapping the feature just seems like a good way to ensure most people (who, by and large, barely hit the NN) will never discover it.","04/Nov/11 16:53;olgan;My main concern is backward compatibility. In use cases at Yahoo, we trained users to provide name and type of the data during load time. Are we sure (and do we have enough use cases) that they will still see the names and types they requested?

I think for 10, we should turn it off by default and see if it make sense to switch that down the road.","04/Nov/11 18:10;alangates;I agree with Dmitriy that this will be very useful to most users with pretty minimal cost.  My concern is in the glob case, where we're potentially doing thousands of stats on the NameNode.  I would suggest adding a cap on the number of directories it could read, and providing a variable users could set to up this if they need to.  For example, if a glob tried to access more than 100 directories, it would fail with a message like:

Error:  PigStorage exceeded max number of input directories.  To avoid this, you can turn of auto schema detection by setting what.ever.the.variable.is to false or you can increase the maximum allowed directories by setting what.ever.that.variable.is (warning, this will increase the load on your NameNode).

Olga, I don't understand your concern for backward compatibility.  If the user has both a schema and an as clause we try to massage the schema into the as clause.  The only issue will be if they store it with a schema and then give an as clause that is not compatible by our casting rules (e.g. the schema says a field is a long and they declare it as a string in the as clause).  Do you think that case is common?
","04/Nov/11 18:58;olgan;My concern is that the data is generated by one set of users but used by another and I am not sure how well we have tested the code that messages the data since that was not a very common use case. 

I could howevere see that if the schema file is not there nothing changes which could be good enough. I do recommend that we at least add more test case for schema provided both in file and in load statement","04/Nov/11 21:42;daijy;I checked the code again, we don't do extensive globStatus. Only one globStatus will be called in the case of globbing. So if we agree we want to pay this price in trade of the benefit of schema file, the only thing left is just bug fix. TestPigStorage do have test cases cover basic schema/noschema, we do need to add some globbing tests.

I will submit a patch shortly.","04/Nov/11 22:03;dvryaboy;Olga, we have a ""-noschema"" parameter that lets people ignore whatever's on disk w.r.t. schemas, so if a bug is tickled, it will be easy to work around it.","04/Nov/11 22:56;olgan;Our users especially production ones are not very keen on changing their processes.

I think this probably an ok change since our users do not have schema files right now. I am worried that we have done such a major re-work of PigStorage which is one of the main ways to get the data and how much problem we will see as the result like the one we are seeing in the bug","04/Nov/11 23:16;dvryaboy;FWIW, the reason I filed this bug when I did was that I tried switching it out in production at Twitter. I will do that again as soon as we have a fix, before the 0.10 release; that should surface issues pretty quickly, if they exist.","09/Nov/11 20:12;daijy;The patch is ready for review. Dmitriy, are you able to take a look?",09/Nov/11 20:45;dvryaboy;yep looking at it today.,09/Nov/11 21:38;daijy;Thanks!,"12/Nov/11 03:02;dvryaboy;Looks good.

I think we need to firm up schema resolution when dealing with globs, but that's a different ticket (loading foo/{bar,baz} where the schema is under foo/bar/.pig_schema doesn't give you the schema)","13/Nov/11 23:48;daijy;Unit tests pass. test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 4 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 473 release audit warnings (more than the trunk's current 466 warnings).

Patch committed to trunk and 0.10 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Restrict number of PIG generated Haddop counters ,PIG-2208,12518383,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,09/Aug/11 23:05,05/Oct/11 17:20,14/Mar/19 03:07,27/Sep/11 18:05,0.8.1,0.9.0,,,,,,0.10.0,0.9.1,,,impl,,,0,,,,,,,,,,,,,"PIG 8.0 implemented Hadoop counters to track the number of records read for each input and the number of records written for each output (PIG-1389 & PIG-1299). On the other hand, Hadoop has imposed limit on per job counters (MAPREDUCE-1943) and jobs will fail if the counters exceed the limit.

Therefore we need a way to cap the number of PIG generated counters.

Here are the two options:

1. Add a integer property (e.g., pig.counter.limit) to the pig property file (e.g., 20). If the number of inputs of a job exceeds this number, the input counters are disabled. Similarly, if the number of outputs of a job exceeds this number, the output counters are disabled.

2. Add a boolean property (e.g., pig.disable.counters) to the pig property file (default: false). If this property is set to true, then the PIG generated counters are disabled.

  

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Aug/11 00:29;rding;PIG-2208.patch;https://issues.apache.org/jira/secure/attachment/12490200/PIG-2208.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-09 23:19:47.431,,,no_permission,,,,,,,,,,,,2078,Reviewed,,,Tue Sep 27 18:05:06 UTC 2011,,,,,,,0|i0h1l3:,97519,,,,,,,,,,"09/Aug/11 23:19;dvryaboy;Can I propose that in addition to the above, we augment the grammar of store and load funcs with a ""with [no]counters""?

Sometimes you know which relations you care about and which you do not.",12/Aug/11 00:29;rding;This patch implements option 2. Augmenting Pig grammar will be more involved and could be done later.,"12/Aug/11 02:54;dvryaboy;This is just trading one issue for another. If we use too many counters, the job is killed by limits. If we don't, we spam the logs and the tasks are killed for using too much local disk.  We should at least do local aggregation -- keep counters local to task (a simple map), and log what we would otherwise put in counters. ",23/Aug/11 07:47;rding;It only logs once per job in the front end so that user is informed that the multi-inputs (or outputs) counters are disabled. In the back-end the counters are simply disabled without logging. ,"26/Sep/11 21:05;daijy;It seems no matter what else we want to do, option 2 is a good addition. I am going to commit the patch, objection?","26/Sep/11 22:00;dvryaboy;I still don't like it but it sounds like I am in the minority.

Can you add the new properties with default values and a bit of docs to pig.properties?
","26/Sep/11 22:53;daijy;Thanks, Dmitriy. We can do something in addition in this case, but I just don't see any downside to provide a way to disable input counter. How do you think?","26/Sep/11 23:01;dvryaboy;I just think that we keep relying on the crutch of providing a user-configurable option for things that shouldn't be issues in the first place. That's not a long-term strategy, and causes user confusion.","26/Sep/11 23:48;daijy;I cannot agree more on this. User will have to run a handicapped Pig in some cases, which is not good. However, I cannot find a workaround easier than the proposed option. 

""local aggregation"" is a good addition to this approach. When counter is disabled, user can at least check log. But counters are handier than logs, so keeping counters makes sense.

""with [no]counters"" also makes sense when user want finer control. But an overall control should still be there in case user don't want to change script.

I will add a comment in the pig.properties to explain ""pig.disable.counter"" option.","27/Sep/11 01:41;dvryaboy;It'd be nice if we could flip that bit automatically during runtime, but I suppose that requires changes to MR code.

Ok, you have my begrudging +1 :)","27/Sep/11 18:05;daijy;Thanks Dmitriy!

All unit tests pass.

Test-patch result:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 464 release audit warnings (more than the trunk's current 462 warnings).

No new files added, ignore release audit warnings.

Patch committed to both trunk and 0.9 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroStorage doesn't work with Avro 1.5.1,PIG-2202,12517729,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,billgraham,billgraham,billgraham,03/Aug/11 00:50,26/Apr/12 20:32,14/Mar/19 03:07,15/Aug/11 23:44,,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"See PIG-2195. Pig jobs that use AvroStorage to write records using the 'schema' or 'data' options fail when using Avro 1.5.1 with the following exception:

{noformat}
java.lang.ClassCastException: org.apache.pig.data.BinSedesTuple cannot be cast to org.apache.avro.generic.IndexedRecord
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Aug/11 00:56;billgraham;PIG-2202_1.patch;https://issues.apache.org/jira/secure/attachment/12489147/PIG-2202_1.patch,05/Aug/11 23:51;billgraham;PIG-2202_2.patch;https://issues.apache.org/jira/secure/attachment/12489552/PIG-2202_2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-08-12 20:10:39.638,,,no_permission,,,,,,,,,,,,37460,,,,Mon Aug 15 23:44:03 UTC 2011,,,,,,,0|i05ig7:,30086,AvroStorage upgraded to work with Avro 1.5.1.,,,,,,,,,"03/Aug/11 00:56;billgraham;Attached is patch 1, which allows you to compile against Avro 1.5.1. This patch still results in 2 unit test failures with exceptions as shown below.

Is it acceptable for {{AvroStorage}} to be upgraded to work with Avro 1.5.1 and not support earlier Avro releases?

{noformat}
11/08/02 17:40:35 WARN mapred.LocalJobRunner: job_local_0005
java.lang.ClassCastException: org.apache.pig.data.BinSedesTuple cannot be cast to org.apache.avro.generic.IndexedRecord
        at org.apache.avro.generic.GenericData.getField(GenericData.java:470)
        at org.apache.avro.generic.GenericDatumWriter.writeRecord(GenericDatumWriter.java:102)
        at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:65)
        at org.apache.pig.piggybank.storage.avro.PigAvroDatumWriter.write(PigAvroDatumWriter.java:99)
        at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:57)
        at org.apache.avro.file.DataFileWriter.append(DataFileWriter.java:244)
        at org.apache.pig.piggybank.storage.avro.PigAvroRecordWriter.write(PigAvroRecordWriter.java:49)
        at org.apache.pig.piggybank.storage.avro.AvroStorage.putNext(AvroStorage.java:580)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore.getNext(POStore.java:143)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.runPipeline(POSplit.java:254)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.processPlan(POSplit.java:236)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.getNext(POSplit.java:228)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.runPipeline(PigMapReduce.java:456)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.processOnePackageOutput(PigMapReduce.java:424)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:404)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:258)
        at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
        at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:566)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216)
{noformat}

","05/Aug/11 23:51;billgraham;Attached is patch 2 that upgrades {{PigAvroDatumWriter}} to work with changes to it's Avro superclass, {{GenericDatumWriter}}.

This patch is backward compatible with Avro 1.4.1.",12/Aug/11 20:10;alangates;I'll be reviewing this patch.,15/Aug/11 23:44;alangates;Patch checked in.  Thanks Bill.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Piggybank cannot be built from the Git mirror,PIG-2200,12517688,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,dvryaboy,dvryaboy,02/Aug/11 19:32,26/Apr/12 20:32,14/Mar/19 03:07,02/Aug/11 20:10,,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"Piggybank can't be built from trunk because the download-hive-deps task fails.
This is caused by a missing lib/ directory in contrib/piggybank/java/","Mac OS, Java 1.6, and 1.8.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,41688,,,,Tue Aug 02 20:10:59 UTC 2011,,,,,,,0|i0h1jz:,97514,,,,,,,,,,"02/Aug/11 19:37;dvryaboy;Here's a log that demonstrates the problem and the solution. 

Before:
{code}

tw-mbp13-dryaboy:java dmitriy$ pwd
/Users/dmitriy/src/pig-git/contrib/piggybank/java
tw-mbp13-dryaboy:java dmitriy$ ant
Buildfile: /Users/dmitriy/src/pig-git/contrib/piggybank/java/build.xml

download-hive-deps:
      [get] Getting: http://archive.apache.org/dist/hadoop/hive/hive-0.4.1/hive-0.4.1-bin.tar.gz
      [get] To: /Users/dmitriy/src/pig-git/contrib/piggybank/java/lib/hive-0.4.1-bin.tar.gz
      [get] Error getting http://archive.apache.org/dist/hadoop/hive/hive-0.4.1/hive-0.4.1-bin.tar.gz to /Users/dmitriy/src/pig-git/contrib/piggybank/java/lib/hive-0.4.1-bin.tar.gz

BUILD FAILED
/Users/dmitriy/src/pig-git/contrib/piggybank/java/build.xml:151: java.io.FileNotFoundException: /Users/dmitriy/src/pig-git/contrib/piggybank/java/lib/hive-0.4.1-bin.tar.gz (No such file or directory)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:194)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:145)
	at org.apache.tools.ant.taskdefs.Get$GetThread.downloadFile(Get.java:739)
	at org.apache.tools.ant.taskdefs.Get$GetThread.get(Get.java:586)
	at org.apache.tools.ant.taskdefs.Get$GetThread.run(Get.java:569)

Total time: 0 seconds
{code}

After:

{code}

tw-mbp13-dryaboy:java dmitriy$ ant
Buildfile: /Users/dmitriy/src/pig-git/contrib/piggybank/java/build.xml

download-hive-deps:
      [get] Getting: http://archive.apache.org/dist/hadoop/hive/hive-0.4.1/hive-0.4.1-bin.tar.gz
      [get] To: /Users/dmitriy/src/pig-git/contrib/piggybank/java/lib/hive-0.4.1-bin.tar.gz
      [get] ....................................................
      [get] ....................................................
      [get] ....................................................
      [get] ....................................................
      [get] ....................................................
      [get] ....................................................
      [get] ....................................................
{code}

I for some reason can't make a patch that just adds an empty directory, so I am going to go ahead and just do that directly on the repo. Cool?",02/Aug/11 19:59;dvryaboy;Ah. The lib directory actually exists in SVN. It's not replicated to the git mirror (see https://github.com/apache/pig/tree/trunk/contrib/piggybank/java). I'll see if this is a known issue with apache git mirrors.,"02/Aug/11 20:02;dvryaboy;I see the problem.. git doesn't really do empty directories:
https://git.wiki.kernel.org/index.php/GitFaq#Can_I_add_empty_directories.3F

I will add an empty .gitignore file to lib/",02/Aug/11 20:10;dvryaboy;Added an empty .gitignore file to contrib/piggybank/java/lib to make it show up in git.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Penny throws Exception when netty classes are missing,PIG-2199,12516852,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ddaniels888,ddaniels888,ddaniels888,02/Aug/11 00:37,05/Oct/11 17:20,14/Mar/19 03:07,02/Aug/11 06:30,0.10.0,0.9.0,,,,,,0.10.0,0.9.1,,,,,,0,,,,,,,,,,,,,"Running the data sampler tool from the penny library causes a ClassNotFoundException for a netty class.  Per the mailing list, this is because the netty classes are not accessible to Penny.

I've attached a patch that adds netty to the penny jar.

For reference, I'm running a simple script that uses pig test data from
test/org/apache/pig/test/data/InputFiles/jsTst1.txt :

    x = LOAD 'jsTst1.txt' USING PigStorage('\t');
    x_filtered = FILTER x BY (int)$1 > 100;
    STORE x_filtered INTO 'jsTst1Filtered';
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Aug/11 00:39;ddaniels888;PIG-2199.patch;https://issues.apache.org/jira/secure/attachment/12488844/PIG-2199.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-02 05:48:47.7,,,no_permission,,,,,,,,,,,,46541,Reviewed,,,Tue Aug 02 06:30:36 UTC 2011,,,,,,,0|i0h1jr:,97513,,,,,,,,,,02/Aug/11 05:48;daijy;Verified Penny runs with this patch. Committed to both trunk and 0.9 branch.,"02/Aug/11 05:54;daijy;Sorry, the above comment is not mean for this patch.",02/Aug/11 06:30;daijy;We do need this patch. Patch committed to both trunk and 0.9 branch. Thanks Doug contributing!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroStorage fails to STORE when LOADing via PigStorage,PIG-2195,12515714,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,billgraham,billgraham,billgraham,28/Jul/11 22:28,26/Apr/12 20:32,14/Mar/19 03:07,15/Aug/11 23:25,,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"Reading data via {{PigStorage}} and writing it via {{AvroStorage}} fails with an exception like this

{{java.lang.ClassCastException: org.apache.pig.data.BinSedesTuple cannot be cast to org.apache.avro.generic.IndexedRecord}}

The Pig script in this section of the documentation shows an example like this that fails:

http://linkedin.jira.com/wiki/display/HTOOLS/AvroStorage+-+Pig+support+for+Avro+data#AvroStorage-PigsupportforAvrodata-A.Howtostoredataindifferentways.

A workaround currently exists to produce avro from TSVs like this:

{noformat}
avro = LOAD 'inputPath/' AS (foo);
STORE avro INTO 'outputPath/' USING oap.piggybank.storage.avro.AvroStorage(
  '{""data"":""data_file.avro"",
    ""same"":""data_file.avro"", ""field0"":""def:bar""}');
{noformat}

This is redundant though and {{data}} and {{same}} seem to indicate the same thing. This approach also requires an existing avro data file to exist. This patch will make the following alternate constructor syntax's work as well.

# Read schema from an existing data file:
{noformat}
  '{""data"":""data_file.avro"", ""field0"":""def:bar""}');
{noformat}
# Read schema from an existing schema file:
{noformat}
  '{""schema_file"":""data_file.avsc"", ""field0"":""def:bar""}');
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Aug/11 23:47;billgraham;PIG-2195_1.patch;https://issues.apache.org/jira/secure/attachment/12489137/PIG-2195_1.patch,02/Aug/11 23:47;billgraham;expected_testRecordSplitFromText1.avro;https://issues.apache.org/jira/secure/attachment/12489138/expected_testRecordSplitFromText1.avro,02/Aug/11 23:47;billgraham;expected_testRecordSplitFromText2.avro;https://issues.apache.org/jira/secure/attachment/12489139/expected_testRecordSplitFromText2.avro,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-08-12 20:09:30.98,,,no_permission,,,,,,,,,,,,37461,,,,Mon Aug 15 23:25:00 UTC 2011,,,,,,,0|i05igf:,30087,AvroStorage support for using a schema from an Avro schema file.,,,,,,,,,"02/Aug/11 19:52;billgraham;After spending many hours getting my head around what AvroStorage does, I've learned a few things some of which lead me to amend my initial assessment of the issue:

# Reading from PigStorage and writing via AvroStorage works as advertised when using Avro 1.4.1. The ClassCastException I initially referenced happens when using Avro 1.5.1. Also the current trunk doesn't build against 1.5.1. We can address this in a separate JIRA.
# There is a nasty hidden bug that causes the unit tests to not run in isolation. This can cause newly added unit tests or re-ordered tests to fail. The issue is that {{PigSchema2Avro}} has a {{static int tupleIndex}} that it increments after each call to {{getRecordName()}} and the tests expect record names to have a certain {{tupleIndex}} value included. As a temporary hack I've added a {{public static void setTupleIndex(int index)}} method to that class to allow unit tests to reset it, but this static int approach should really be revisited.
# I've added additional unit tests for reading from a text file and producing Avro.
# I've added support for passing a {{schema_file}} value instead of a data file as shown above.

I'll upload a patch shortly.

 ","02/Aug/11 23:47;billgraham;Attached is a first patch that has:

* More unit tests reading from text files.
* A fix to how unit tests are run as described above.
* Support for specifying a JSON {{schema_file}}.

Also included are 2 new expected test result files that are needed for one of the new tests. The should live here:

{{contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/avro/avro_test_files}}","05/Aug/11 23:54;billgraham;This first patch is ready for review FYI. Also FYI, I've created PIG-2202 to track upgrading AvroStorage to work with Avro 1.5.1 and PIG-2201 to track a separate issue with the unit tests.",12/Aug/11 20:09;alangates;I'll be reviewing this patch.,"12/Aug/11 23:24;billgraham;Thanks Alan. Regarding the naming of the new {{schema_file}} field, with the recent commit of AVRO-872, Avro will supports parsing multiple schema files in 1.6.0. I'd like to contribute a patch to {{AvroStorage}} to support that at some point.

With that in mind, should {{schema_file}} instead be {{schema_files}} so it can at some point accept a comma-separated list perhaps? Or we could allow multiple files to be passed to {{schema_file}} at some point. Or we can later introduce {{schema_files}} with an {{s}}. Many options, but something to plan for now.",15/Aug/11 23:25;alangates;Patch checked in.  Thanks Bill.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using HBaseStorage to scan 2 tables in the same Map job produces bad data,PIG-2193,12515617,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rangadi,vbarat,vbarat,28/Jul/11 08:40,05/Oct/11 17:20,14/Mar/19 03:07,23/Aug/11 03:28,0.8.1,,,,,,,0.10.0,0.9.1,,,,,,0,,,,,,,,,,,,,"I've some data in HBase 0.90.3 and I run a simple script on them.

This script badly returns 0 records. From time to time, under yet undefined conditions, the same script on the same data works (it return correct data).

When data are loaded from HDFS instead of HBase, the script runs perfectly.

Here is the script loading from HDFS (works): 

start_sessions = LOAD 'start_sessions' AS (sid:chararray, infoid:chararray, imei:chararray, start:long);
end_sessions = LOAD 'end_sessions' AS (sid:chararray, end:long, locid:chararray);
infos = LOAD 'infos' AS (infoid:chararray, network_type:chararray, network_subtype:chararray, locale:chararray, version_name:chararray, carrier_country:chararray, carrier_name:chararray, phone_manufacturer:chararray, phone_model:chararray, firmware_version:chararray, firmware_name:chararray);
sessions = JOIN start_sessions BY sid, end_sessions BY sid;
sessions = FILTER sessions BY end > start AND end - start < 86400000L;
sessions = JOIN sessions BY infoid, infos BY infoid;
sessions = LIMIT sessions 100;
dump sessions;

The same script loading from HBase (don't work):

start_sessions = LOAD 'startSession' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage('meta:sid meta:infoid meta:imei meta:timestamp') AS (sid:chararray, infoid:chararray, imei:chararray, start:long);
end_sessions = LOAD 'endSession' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage('meta:sid meta:timestamp meta:locid') AS (sid:chararray, end:long, locid:chararray);
infos = LOAD 'info' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage('meta:infoid data:networkType data:networkSubtype data:locale data:applicationVersionName data:carrierCountry data:carrierName data:phoneManufacturer data:phoneModel data:firmwareVersion data:firmwareName') AS (infoid:chararray, network_type:chararray, network_subtype:chararray, locale:chararray, version_name:chararray, carrier_country:chararray, carrier_name:chararray, phone_manufacturer:chararray, phone_model:chararray, firmware_version:chararray, firmware_name:chararray);
sessions = JOIN start_sessions BY sid, end_sessions BY sid;
sessions = FILTER sessions BY end > start AND end - start < 86400000L;
sessions = JOIN sessions BY infoid, infos BY infoid;
sessions = LIMIT sessions 100;
dump sessions;

I guess it definitively means there is a nasty bug in the HBase loader.

","HBase 0.90.3, Hadoop 0.20-append",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Aug/11 20:56;rangadi;PIG-2193.patch;https://issues.apache.org/jira/secure/attachment/12488439/PIG-2193.patch,29/Jul/11 01:55;rangadi;PIG-2193.patch;https://issues.apache.org/jira/secure/attachment/12488164/PIG-2193.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-28 17:35:14.749,,,no_permission,,,,,,,,,,,,46535,,,,Tue Aug 23 01:12:54 UTC 2011,,,,,,,0|i0h1iv:,97509,Joining two HBase tables sometimes failed because mappers got confused about which of the tables they were supposed to scan.,,,,,,,,,"28/Jul/11 09:03;vbarat;Interesting:

I've reduced my request to:

start_sessions = LOAD 'start_sessions' AS (sid:chararray, infoid:chararray, imei:chararray, start:long);
end_sessions = LOAD 'end_sessions' AS (sid:chararray, end:long, locid:chararray);
sessions = JOIN start_sessions BY sid, end_sessions BY sid;
sessions = FILTER sessions BY end > start AND end - start < 86400000L;
dump sessions;

If I dump 'sessions' before the FILTER statement, I correctly values for the 'infoid' and 'locid' field.

{code}
(00000A2A33254B8FAE1E9AEAB2428EBE,b87ac86bcf1d4cb44202aa826554a7b2,4e77d62e1839a470ec8386d42b85a076,1310649832970,00000A2A33254B8FAE1E9AEAB2428EBE,1310649838390,)
(00001DCECDC842C0A745C151B9EC295F,4a4bb0fff26e368c8209f1e480fdf70b,db3924d2e4b88bd103fa19aaa30a9af4,1310628836846,00001DCECDC842C0A745C151B9EC295F,1310628839075,)
(00001F8F2B3148D393963928188C72B6,5d7e58f68366b55d55862815f863a996,79e1ba90aa555e3e1041df4be657a11d,1310681918742,00001F8F2B3148D393963928188C72B6,1310681949182,)
{code}

But if I dump 'sessions' after the FILTER statement, I have no values for the 'infoid' and 'locid' fields.

{code}
(00000A2A33254B8FAE1E9AEAB2428EBE,,,1310649832970,00000A2A33254B8FAE1E9AEAB2428EBE,1310649838390,)
(00001DCECDC842C0A745C151B9EC295F,,,1310628836846,00001DCECDC842C0A745C151B9EC295F,1310628839075,)
(00001F8F2B3148D393963928188C72B6,,,1310681918742,00001F8F2B3148D393963928188C72B6,1310681949182,)
{code}

so the next JOIN return an empty table.","28/Jul/11 09:05;vbarat;Of course if I do the same test reading data from HDFS instead of HBase, I got the good values for the 'infoid' and 'locid' fields in both case.","28/Jul/11 17:35;billgraham;@Vincent, can you please try applying the patch in PIG-2174 and report back if it makes a difference?","29/Jul/11 01:02;rangadi;I am able to reproduced this with PIG-0.8.1. But in my case commenting out FILTER does not make a difference. I think it is because of the how HBase scanner is initialized on the backend. This script results in 2 mappers which are supposed to have two different scanners, but both are initialized from the same config. I will verify.",29/Jul/11 01:55;rangadi;The patch ignores the scanner conf in jobConf. This seems to fix my script.,"29/Jul/11 10:10;vbarat;Do you have this patch adapted for PIG 0.8.1, apparently it does not apply directly","29/Jul/11 13:45;vbarat;We have tried PIG trunk patched with PIG-2174 without success: same behavior.
The patch does not apply on PIG 0.8.1, did you adapt it ?","29/Jul/11 15:06;rangadi;PIG-2193.patch I attached is for 0.8 branch. It applies fine. Did you try it on clean 0.8.1? If you want to try on your code, your just need to comment out 3 lines in HBaseStorage::setLocation().","29/Jul/11 17:44;dvryaboy;Moving the trace from ticket description to a comment (long descriptions make the ticket hard to work with in jira).


Here is the PIG dump for the HBase version:

aws09:~# pig
2011-07-28 08:17:36,329 [main] INFO  org.apache.pig.Main - Logging error messages to: /root/pig_1311841056328.log
2011-07-28 08:17:36,641 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://aws09.preprod.ubithere.com:9000
2011-07-28 08:17:36,923 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to map-reduce job tracker at: aws09.preprod.ubithere.com:9001
grunt> start_sessions = LOAD 'startSession.mde253811.preprod.ubithere.com' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage('meta:sid meta:infoid meta:imei meta:timestamp') AS (sid:chararray, infoid:chararray, imei:chararray, start:long);
grunt> end_sessions = LOAD 'endSession.mde253811.preprod.ubithere.com' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage('meta:sid meta:timestamp meta:locid') AS (sid:chararray, end:long, locid:chararray);
grunt> infos = LOAD 'info.mde253811.preprod.ubithere.com' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage('meta:infoid data:networkType data:networkSubtype data:locale data:applicationVersionName data:carrierCountry data:carrierName data:phoneManufacturer data:phoneModel data:firmwareVersion data:firmwareName') AS (infoid:chararray, network_type:chararray, network_subtype:chararray, locale:chararray, version_name:chararray, carrier_country:chararray, carrier_name:chararray, phone_manufacturer:chararray, phone_model:chararray, firmware_version:chararray, firmware_name:chararray);
grunt> sessions = JOIN start_sessions BY sid, end_sessions BY sid;
grunt> sessions = FILTER sessions BY end > start AND end - start < 86400000L;
grunt> sessions = JOIN sessions BY infoid, infos BY infoid;
grunt> sessions = LIMIT sessions 100;
grunt> dump sessions;
2011-07-28 08:17:50,275 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: HASH_JOIN,FILTER,LIMIT
2011-07-28 08:17:50,275 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - pig.usenewlogicalplan is set to true. New logical plan will be used.
2011-07-28 08:17:51,213 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - (Name: sessions: Store(hdfs://aws09.preprod.ubithere.com:9000/tmp/temp-1404953096/tmp819396740:org.apache.pig.impl.io.InterStorage) - scope-93 Operator Key: scope-93)
2011-07-28 08:17:51,225 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false
2011-07-28 08:17:51,281 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler$LastInputStreamingOptimizer - Rewrite: POPackage->POForEach to POJoinPackage
2011-07-28 08:17:51,281 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler$LastInputStreamingOptimizer - Rewrite: POPackage->POForEach to POJoinPackage
2011-07-28 08:17:51,350 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 3
2011-07-28 08:17:51,350 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 3
2011-07-28 08:17:51,402 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added to the job
2011-07-28 08:17:51,411 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2011-07-28 08:17:51,470 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:zookeeper.version=3.3.2-1031432, built on 11/05/2010 05:32 GMT
2011-07-28 08:17:51,470 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:host.name=aws09.machine.com
2011-07-28 08:17:51,470 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.version=1.6.0_22
2011-07-28 08:17:51,470 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.vendor=Sun Microsystems Inc.
2011-07-28 08:17:51,470 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.home=/usr/lib/jvm/java-6-sun-1.6.0.22/jre
2011-07-28 08:17:51,470 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.class.path=/opt/pig/bin/../conf:/usr/lib/jvm/java-6-sun/jre/lib/tools.jar:/opt/pig/bin/../pig-0.8.1-core.jar:/opt/pig/bin/../build/pig-*-SNAPSHOT.jar:/opt/pig/bin/../lib/commons-el-1.0.jar:/opt/pig/bin/../lib/commons-lang-2.4.jar:/opt/pig/bin/../lib/commons-logging-1.1.1.jar:/opt/pig/bin/../lib/guava-r06.jar:/opt/pig/bin/../lib/hbase-0.90.3.jar:/opt/pig/bin/../lib/hsqldb-1.8.0.10.jar:/opt/pig/bin/../lib/jackson-core-asl-1.0.1.jar:/opt/pig/bin/../lib/jackson-mapper-asl-1.0.1.jar:/opt/pig/bin/../lib/javacc-4.2.jar:/opt/pig/bin/../lib/javacc.jar:/opt/pig/bin/../lib/jetty-util-6.1.14.jar:/opt/pig/bin/../lib/jline-0.9.94.jar:/opt/pig/bin/../lib/joda-time-1.6.jar:/opt/pig/bin/../lib/jsch-0.1.38.jar:/opt/pig/bin/../lib/junit-4.5.jar:/opt/pig/bin/../lib/jython-2.5.0.jar:/opt/pig/bin/../lib/log4j-1.2.14.jar:/opt/pig/bin/../lib/pigudfs.jar:/opt/pig/bin/../lib/slf4j-log4j12-1.4.3.jar:/opt/pig/bin/../lib/zookeeper-3.3.2.jar:/opt/hadoop/conf_computation:/opt/hbase/conf:/opt/pig/lib/hadoop-0.20-append-core.jar
2011-07-28 08:17:51,470 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.library.path=/usr/lib/jvm/java-6-sun-1.6.0.22/jre/lib/amd64/server:/usr/lib/jvm/java-6-sun-1.6.0.22/jre/lib/amd64:/usr/lib/jvm/java-6-sun-1.6.0.22/jre/../lib/amd64:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2011-07-28 08:17:51,470 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.io.tmpdir=/tmp
2011-07-28 08:17:51,470 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.compiler=<NA>
2011-07-28 08:17:51,470 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:os.name=Linux
2011-07-28 08:17:51,470 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:os.arch=amd64
2011-07-28 08:17:51,470 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:os.version=2.6.21.7-2.fc8xen-ec2-v1.0
2011-07-28 08:17:51,470 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:user.name=root
2011-07-28 08:17:51,470 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:user.home=/root
2011-07-28 08:17:51,470 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:user.dir=/root
2011-07-28 08:17:51,471 [main] INFO  org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=aws09.machine.com:2222 sessionTimeout=60000 watcher=hconnection
2011-07-28 08:17:51,493 [main-SendThread()] INFO  org.apache.zookeeper.ClientCnxn - Opening socket connection to server aws09.machine.com/10.83.1.244:2222
2011-07-28 08:17:51,499 [main-SendThread(aws09.machine.com:2222)] INFO  org.apache.zookeeper.ClientCnxn - Socket connection established to aws09.machine.com/10.83.1.244:2222, initiating session
2011-07-28 08:17:51,508 [main-SendThread(aws09.machine.com:2222)] INFO  org.apache.zookeeper.ClientCnxn - Session establishment complete on server aws09.machine.com/10.83.1.244:2222, sessionid = 0x131617dada6054b, negotiated timeout = 60000
2011-07-28 08:17:51,575 [main] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Lookedup root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@ef894ce; hsa=aws03.machine.com:60020
2011-07-28 08:17:51,687 [main] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Cached location for .META.,,1.1028785192 is aws03.machine.com:60020
2011-07-28 08:17:51,696 [main] DEBUG org.apache.hadoop.hbase.client.MetaScanner - Scanning .META. starting at row=endSession.mde253811.preprod.ubithere.com,,00000000000000 for max=10 rows
2011-07-28 08:17:51,700 [main] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Cached location for endSession.mde253811.preprod.ubithere.com,,1311086199483.706685579 is aws03.machine.com:60020
2011-07-28 08:17:51,726 [main] DEBUG org.apache.hadoop.hbase.client.MetaScanner - Scanning .META. starting at row=startSession.mde253811.preprod.ubithere.com,,00000000000000 for max=10 rows
2011-07-28 08:17:51,729 [main] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Cached location for startSession.mde253811.preprod.ubithere.com,,1311086198252.1334391323 is aws03.machine.com:60020
2011-07-28 08:17:53,328 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job
2011-07-28 08:17:53,335 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - BytesPerReducer=1000000000 maxReducers=999 totalInputFileSize=0
2011-07-28 08:17:53,335 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Neither PARALLEL nor default parallelism is set for this job. Setting number of reducers to 1
2011-07-28 08:17:53,442 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.
2011-07-28 08:17:53,944 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete
2011-07-28 08:17:53,989 [Thread-13] INFO  org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=aws09.machine.com:2222 sessionTimeout=60000 watcher=hconnection
2011-07-28 08:17:53,990 [Thread-13-SendThread()] INFO  org.apache.zookeeper.ClientCnxn - Opening socket connection to server aws09.machine.com/10.83.1.244:2222
2011-07-28 08:17:53,991 [Thread-13-SendThread(aws09.machine.com:2222)] INFO  org.apache.zookeeper.ClientCnxn - Socket connection established to aws09.machine.com/10.83.1.244:2222, initiating session
2011-07-28 08:17:53,996 [Thread-13-SendThread(aws09.machine.com:2222)] INFO  org.apache.zookeeper.ClientCnxn - Session establishment complete on server aws09.machine.com/10.83.1.244:2222, sessionid = 0x131617dada6054c, negotiated timeout = 60000
2011-07-28 08:17:54,000 [Thread-13] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Lookedup root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2d9f90e3; hsa=aws03.machine.com:60020
2011-07-28 08:17:54,005 [Thread-13] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Cached location for .META.,,1.1028785192 is aws03.machine.com:60020
2011-07-28 08:17:54,006 [Thread-13] DEBUG org.apache.hadoop.hbase.client.MetaScanner - Scanning .META. starting at row=endSession.mde253811.preprod.ubithere.com,,00000000000000 for max=10 rows
2011-07-28 08:17:54,011 [Thread-13] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Cached location for endSession.mde253811.preprod.ubithere.com,,1311086199483.706685579 is aws03.machine.com:60020
2011-07-28 08:17:54,017 [Thread-13] INFO  org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=aws09.machine.com:2222 sessionTimeout=60000 watcher=hconnection
2011-07-28 08:17:54,017 [Thread-13-SendThread()] INFO  org.apache.zookeeper.ClientCnxn - Opening socket connection to server aws09.machine.com/10.83.1.244:2222
2011-07-28 08:17:54,018 [Thread-13-SendThread(aws09.machine.com:2222)] INFO  org.apache.zookeeper.ClientCnxn - Socket connection established to aws09.machine.com/10.83.1.244:2222, initiating session
2011-07-28 08:17:54,025 [Thread-13-SendThread(aws09.machine.com:2222)] INFO  org.apache.zookeeper.ClientCnxn - Session establishment complete on server aws09.machine.com/10.83.1.244:2222, sessionid = 0x131617dada6054d, negotiated timeout = 60000
2011-07-28 08:17:54,029 [Thread-13] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Lookedup root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@41f6321; hsa=aws03.machine.com:60020
2011-07-28 08:17:54,032 [Thread-13] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Cached location for .META.,,1.1028785192 is aws03.machine.com:60020
2011-07-28 08:17:54,033 [Thread-13] DEBUG org.apache.hadoop.hbase.client.MetaScanner - Scanning .META. starting at row=endSession.mde253811.preprod.ubithere.com,,00000000000000 for max=10 rows
2011-07-28 08:17:54,037 [Thread-13] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Cached location for endSession.mde253811.preprod.ubithere.com,,1311086199483.706685579 is aws03.machine.com:60020
2011-07-28 08:17:54,039 [Thread-13] DEBUG org.apache.hadoop.hbase.client.MetaScanner - Scanning .META. starting at row=endSession.mde253811.preprod.ubithere.com,,00000000000000 for max=2147483647 rows
2011-07-28 08:17:54,067 [Thread-13] DEBUG org.apache.hadoop.hbase.mapreduce.TableInputFormatBase - getSplits: split -> 0 -> aws03.machine.com:,
2011-07-28 08:17:54,068 [Thread-13] INFO  org.apache.pig.backend.hadoop.hbase.HBaseTableInputFormat - Got 1 splits.
2011-07-28 08:17:54,068 [Thread-13] INFO  org.apache.pig.backend.hadoop.hbase.HBaseTableInputFormat - Returning 1 splits.
2011-07-28 08:17:54,109 [Thread-13] INFO  org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=aws09.machine.com:2222 sessionTimeout=60000 watcher=hconnection
2011-07-28 08:17:54,110 [Thread-13-SendThread()] INFO  org.apache.zookeeper.ClientCnxn - Opening socket connection to server aws09.machine.com/10.83.1.244:2222
2011-07-28 08:17:54,111 [Thread-13-SendThread(aws09.machine.com:2222)] INFO  org.apache.zookeeper.ClientCnxn - Socket connection established to aws09.machine.com/10.83.1.244:2222, initiating session
2011-07-28 08:17:54,119 [Thread-13-SendThread(aws09.machine.com:2222)] INFO  org.apache.zookeeper.ClientCnxn - Session establishment complete on server aws09.machine.com/10.83.1.244:2222, sessionid = 0x131617dada6054e, negotiated timeout = 60000
2011-07-28 08:17:54,123 [Thread-13] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Lookedup root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@20c3e967; hsa=aws03.machine.com:60020
2011-07-28 08:17:54,140 [Thread-13] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Cached location for .META.,,1.1028785192 is aws03.machine.com:60020
2011-07-28 08:17:54,142 [Thread-13] DEBUG org.apache.hadoop.hbase.client.MetaScanner - Scanning .META. starting at row=startSession.mde253811.preprod.ubithere.com,,00000000000000 for max=10 rows
2011-07-28 08:17:54,148 [Thread-13] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Cached location for startSession.mde253811.preprod.ubithere.com,,1311086198252.1334391323 is aws03.machine.com:60020
2011-07-28 08:17:54,154 [Thread-13] INFO  org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=aws09.machine.com:2222 sessionTimeout=60000 watcher=hconnection
2011-07-28 08:17:54,158 [Thread-13-SendThread()] INFO  org.apache.zookeeper.ClientCnxn - Opening socket connection to server aws09.machine.com/10.83.1.244:2222
2011-07-28 08:17:54,159 [Thread-13-SendThread(aws09.machine.com:2222)] INFO  org.apache.zookeeper.ClientCnxn - Socket connection established to aws09.machine.com/10.83.1.244:2222, initiating session
2011-07-28 08:17:54,161 [Thread-13-SendThread(aws09.machine.com:2222)] INFO  org.apache.zookeeper.ClientCnxn - Session establishment complete on server aws09.machine.com/10.83.1.244:2222, sessionid = 0x131617dada6054f, negotiated timeout = 60000
2011-07-28 08:17:54,164 [Thread-13] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Lookedup root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@5ee771f3; hsa=aws03.machine.com:60020
2011-07-28 08:17:54,167 [Thread-13] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Cached location for .META.,,1.1028785192 is aws03.machine.com:60020
2011-07-28 08:17:54,169 [Thread-13] DEBUG org.apache.hadoop.hbase.client.MetaScanner - Scanning .META. starting at row=startSession.mde253811.preprod.ubithere.com,,00000000000000 for max=10 rows
2011-07-28 08:17:54,172 [Thread-13] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Cached location for startSession.mde253811.preprod.ubithere.com,,1311086198252.1334391323 is aws03.machine.com:60020
2011-07-28 08:17:54,173 [Thread-13] DEBUG org.apache.hadoop.hbase.client.MetaScanner - Scanning .META. starting at row=startSession.mde253811.preprod.ubithere.com,,00000000000000 for max=2147483647 rows
2011-07-28 08:17:54,180 [Thread-13] DEBUG org.apache.hadoop.hbase.mapreduce.TableInputFormatBase - getSplits: split -> 0 -> aws03.machine.com:,
2011-07-28 08:17:54,180 [Thread-13] INFO  org.apache.pig.backend.hadoop.hbase.HBaseTableInputFormat - Got 1 splits.
2011-07-28 08:17:54,180 [Thread-13] INFO  org.apache.pig.backend.hadoop.hbase.HBaseTableInputFormat - Returning 1 splits.
2011-07-28 08:17:55,037 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_201107251336_0314
2011-07-28 08:17:55,037 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - More information at: http://aws09.preprod.ubithere.com:50030/jobdetails.jsp?jobid=job_201107251336_0314
2011-07-28 08:19:06,924 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 8% complete
2011-07-28 08:19:15,971 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 16% complete
2011-07-28 08:19:18,985 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 19% complete
2011-07-28 08:19:25,035 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 27% complete
2011-07-28 08:20:14,810 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added to the job
2011-07-28 08:20:14,812 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2011-07-28 08:20:14,830 [main] INFO  org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=aws09.machine.com:2222 sessionTimeout=60000 watcher=hconnection
2011-07-28 08:20:14,831 [main-SendThread()] INFO  org.apache.zookeeper.ClientCnxn - Opening socket connection to server aws09.machine.com/10.83.1.244:2222
2011-07-28 08:20:14,832 [main-SendThread(aws09.machine.com:2222)] INFO  org.apache.zookeeper.ClientCnxn - Socket connection established to aws09.machine.com/10.83.1.244:2222, initiating session
2011-07-28 08:20:14,838 [main-SendThread(aws09.machine.com:2222)] INFO  org.apache.zookeeper.ClientCnxn - Session establishment complete on server aws09.machine.com/10.83.1.244:2222, sessionid = 0x131617dada60556, negotiated timeout = 60000
2011-07-28 08:20:14,842 [main] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Lookedup root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@5d4fa79d; hsa=aws03.machine.com:60020
2011-07-28 08:20:14,847 [main] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Cached location for .META.,,1.1028785192 is aws03.machine.com:60020
2011-07-28 08:20:14,849 [main] DEBUG org.apache.hadoop.hbase.client.MetaScanner - Scanning .META. starting at row=info.mde253811.preprod.ubithere.com,,00000000000000 for max=10 rows
2011-07-28 08:20:14,852 [main] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Cached location for info.mde253811.preprod.ubithere.com,,1311086202955.1975990008 is aws03.machine.com:60020
2011-07-28 08:20:16,311 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job
2011-07-28 08:20:16,324 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - BytesPerReducer=1000000000 maxReducers=999 totalInputFileSize=198330658
2011-07-28 08:20:16,324 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Neither PARALLEL nor default parallelism is set for this job. Setting number of reducers to 1
2011-07-28 08:20:16,341 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.
2011-07-28 08:20:16,656 [Thread-32] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1
2011-07-28 08:20:16,656 [Thread-32] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1
2011-07-28 08:20:16,693 [Thread-32] INFO  org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=aws09.machine.com:2222 sessionTimeout=60000 watcher=hconnection
2011-07-28 08:20:16,694 [Thread-32-SendThread()] INFO  org.apache.zookeeper.ClientCnxn - Opening socket connection to server aws09.machine.com/10.83.1.244:2222
2011-07-28 08:20:16,695 [Thread-32-SendThread(aws09.machine.com:2222)] INFO  org.apache.zookeeper.ClientCnxn - Socket connection established to aws09.machine.com/10.83.1.244:2222, initiating session
2011-07-28 08:20:16,702 [Thread-32-SendThread(aws09.machine.com:2222)] INFO  org.apache.zookeeper.ClientCnxn - Session establishment complete on server aws09.machine.com/10.83.1.244:2222, sessionid = 0x131617dada60557, negotiated timeout = 60000
2011-07-28 08:20:16,705 [Thread-32] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Lookedup root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@37285252; hsa=aws03.machine.com:60020
2011-07-28 08:20:16,709 [Thread-32] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Cached location for .META.,,1.1028785192 is aws03.machine.com:60020
2011-07-28 08:20:16,710 [Thread-32] DEBUG org.apache.hadoop.hbase.client.MetaScanner - Scanning .META. starting at row=info.mde253811.preprod.ubithere.com,,00000000000000 for max=10 rows
2011-07-28 08:20:16,714 [Thread-32] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Cached location for info.mde253811.preprod.ubithere.com,,1311086202955.1975990008 is aws03.machine.com:60020
2011-07-28 08:20:16,716 [Thread-32] INFO  org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=aws09.machine.com:2222 sessionTimeout=60000 watcher=hconnection
2011-07-28 08:20:16,717 [Thread-32-SendThread()] INFO  org.apache.zookeeper.ClientCnxn - Opening socket connection to server aws09.machine.com/10.83.1.244:2222
2011-07-28 08:20:16,718 [Thread-32-SendThread(aws09.machine.com:2222)] INFO  org.apache.zookeeper.ClientCnxn - Socket connection established to aws09.machine.com/10.83.1.244:2222, initiating session
2011-07-28 08:20:16,720 [Thread-32-SendThread(aws09.machine.com:2222)] INFO  org.apache.zookeeper.ClientCnxn - Session establishment complete on server aws09.machine.com/10.83.1.244:2222, sessionid = 0x131617dada60558, negotiated timeout = 60000
2011-07-28 08:20:16,723 [Thread-32] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Lookedup root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7418e252; hsa=aws03.machine.com:60020
2011-07-28 08:20:16,726 [Thread-32] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Cached location for .META.,,1.1028785192 is aws03.machine.com:60020
2011-07-28 08:20:16,727 [Thread-32] DEBUG org.apache.hadoop.hbase.client.MetaScanner - Scanning .META. starting at row=info.mde253811.preprod.ubithere.com,,00000000000000 for max=10 rows
2011-07-28 08:20:16,730 [Thread-32] DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation - Cached location for info.mde253811.preprod.ubithere.com,,1311086202955.1975990008 is aws03.machine.com:60020
2011-07-28 08:20:16,732 [Thread-32] DEBUG org.apache.hadoop.hbase.client.MetaScanner - Scanning .META. starting at row=info.mde253811.preprod.ubithere.com,,00000000000000 for max=2147483647 rows
2011-07-28 08:20:16,772 [Thread-32] DEBUG org.apache.hadoop.hbase.mapreduce.TableInputFormatBase - getSplits: split -> 0 -> aws03.machine.com:,
2011-07-28 08:20:16,772 [Thread-32] INFO  org.apache.pig.backend.hadoop.hbase.HBaseTableInputFormat - Got 1 splits.
2011-07-28 08:20:16,772 [Thread-32] INFO  org.apache.pig.backend.hadoop.hbase.HBaseTableInputFormat - Returning 1 splits.
2011-07-28 08:20:17,500 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_201107251336_0315
2011-07-28 08:20:17,500 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - More information at: http://aws09.preprod.ubithere.com:50030/jobdetails.jsp?jobid=job_201107251336_0315
2011-07-28 08:20:28,075 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 37% complete
2011-07-28 08:20:34,106 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 41% complete
2011-07-28 08:20:37,124 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 50% complete
2011-07-28 08:20:46,168 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 51% complete
2011-07-28 08:20:49,183 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 61% complete
2011-07-28 08:20:52,198 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 62% complete
2011-07-28 08:20:55,214 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 64% complete
2011-07-28 08:21:01,244 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 66% complete
2011-07-28 08:21:07,311 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added to the job
2011-07-28 08:21:07,312 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2011-07-28 08:21:08,770 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job
2011-07-28 08:21:08,778 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.
2011-07-28 08:21:08,910 [Thread-47] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1
2011-07-28 08:21:08,910 [Thread-47] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1
2011-07-28 08:21:08,911 [Thread-47] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 1
2011-07-28 08:21:09,280 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_201107251336_0316
2011-07-28 08:21:09,280 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - More information at: http://aws09.preprod.ubithere.com:50030/jobdetails.jsp?jobid=job_201107251336_0316
2011-07-28 08:21:16,321 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 83% complete
2011-07-28 08:21:34,439 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete
2011-07-28 08:21:34,441 [main] INFO  org.apache.pig.tools.pigstats.PigStats - Script Statistics:

HadoopVersion    PigVersion    UserId    StartedAt    FinishedAt    Features
0.20-append    0.8.1-SNAPSHOT    root    2011-07-28 08:17:51    2011-07-28 08:21:34    HASH_JOIN,FILTER,LIMIT

Success!

Job Stats (time in seconds):
JobId    Maps    Reduces    MaxMapTime    MinMapTIme    AvgMapTime    MaxReduceTime    MinReduceTime    AvgReduceTime    Alias    Feature    Outputs
job_201107251336_0314    2    1    75    66    70    63    63    63    end_sessions,sessions,start_sessions    HASH_JOIN
job_201107251336_0315    4    1    15    6    12    24    24    24    infos,sessions    HASH_JOIN
job_201107251336_0316    1    1    3    3    3    12    12    12            hdfs://aws09.preprod.ubithere.com:9000/tmp/temp-1404953096/tmp819396740,

Input(s):
Successfully read 2069446 records from: ""endSession.mde253811.preprod.ubithere.com""
Successfully read 2072419 records from: ""startSession.mde253811.preprod.ubithere.com""
Successfully read 19441 records from: ""info.mde253811.preprod.ubithere.com""

Output(s):
Successfully stored 0 records in: ""hdfs://aws09.preprod.ubithere.com:9000/tmp/temp-1404953096/tmp819396740""

Counters:
Total records written : 0
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 1
Total records proactively spilled: 1944943

Job DAG:
job_201107251336_0314    ->    job_201107251336_0315,
job_201107251336_0315    ->    job_201107251336_0316,
job_201107251336_0316


2011-07-28 08:21:34,472 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!
2011-07-28 08:21:34,500 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1
2011-07-28 08:21:34,501 [main] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1
grunt>
","29/Jul/11 17:49;dvryaboy;I have this nagging feeling that there was a reason I put the early termination in there (which is what appears to be causing the problem). Raghu, can you add a unit test for this?","29/Jul/11 21:21;rangadi;This was introduced in PIG-1870.
TestHBaseStorage passes with the patch.. though I expected projection tests to fail.","29/Jul/11 23:30;rangadi;Our tests verify that we return the correct columns to users with projections.
How can we verify that HBase scanner actually fetches only the projected columns? It seems hard to get hold of the Scan object.","01/Aug/11 20:56;rangadi;patch for trunk is attached.
  - This is the complete fix.
  - includes a unit test
  - earlier simple patch works, but does not set the project before the scan is initialized.
  - removed 'initialized' variable since we know when schema and projections are set:
    -- store schema is set inside checkSchema()
    -- projection is set in pushProjection()
    -- on the backend, projection is handled in side setLocation().
    -- on the backend store schema is set inside setStoreLocation().
",01/Aug/11 21:53;dvryaboy;I will review.,"03/Aug/11 10:04;vbarat;We have tested this patch on the PIG trunk with success.
Big thanks for your help guys!
I guess we will use PIG 0.9 + this patch in production. Or maybe wait for PIG 0.9.1 if this patch is to be included in.
",23/Aug/11 01:12;dvryaboy;+1 will commit to 0.9 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce amount of log spam generated by UDFs,PIG-2191,12515336,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,dvryaboy,dvryaboy,25/Jul/11 21:54,16/Mar/12 08:02,14/Mar/19 03:07,05/Aug/11 23:06,,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,Many builtin UDFs use log.warn instead of warn() to report warnings. This can produce an inordinate and unneeded amount of error logging. The warn() method is provided to aggregate that sort of thing.,,,,,,,,,,,,,,,,,,,PIG-2207,,,,,,,,,,,,,02/Aug/11 20:20;dvryaboy;PIG-2191.2.patch;https://issues.apache.org/jira/secure/attachment/12489114/PIG-2191.2.patch,25/Jul/11 22:15;dvryaboy;PIG-2191.patch;https://issues.apache.org/jira/secure/attachment/12487762/PIG-2191.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-27 00:35:27.504,,,no_permission,,,,,,,,,,,,41689,,,,Fri Mar 16 08:02:40 UTC 2012,,,,,,,0|i0h1if:,97507,,,,,,,,,,"25/Jul/11 22:15;dvryaboy;Attaching the patch. No unit tests, since the previous tests are sufficient.

While I was in there, I changed the deprecated piggybank funcs that had this problem to be empty shells for their builtin variants.

Please review.","27/Jul/11 00:35;rangadi;looks good. 
 - JsonMetadata.java changes seem unrelated (whitespace only).
 - +1 for making deprecated classes just extend the non-deprecated ones.
",02/Aug/11 20:20;dvryaboy;Attaching a version that actually compiles.. forgot some imports last time.,"05/Aug/11 22:19;thejas;+1 .
If there are multiple udfs being used in a pig script, it will be very useful to have different warning counters for each of them. I will create a jira to support that feature and link it to this jira.
",05/Aug/11 23:07;dvryaboy;Committed to trunk. Thanks for the review.,"16/Mar/12 08:02;prkommireddi;@Thejas, is there a JIRA to support separate warning counters across multiple UDFs?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigStorage new warnings about missing schema file can be confusing,PIG-2186,12514964,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,21/Jul/11 20:24,26/Apr/12 20:32,14/Mar/19 03:07,25/Jul/11 05:47,0.10.0,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"In PIG-2143,the ability to store and use schema files was added to PigStorage . By default, PigStorage as a load function checks for the schema file presence and logs a warning if there is no schema file.

{code}
2011-07-21 13:15:19,101 [main] WARN  org.apache.pig.builtin.JsonMetadata - Could not find schema file for file:///Users/thejas/pig_trunk_cp/trunk/t.txt
{code}
But in cases where user has not taken actions to store a schema, this warning can be confusing. There is no way for PigStorage() to know if the schema is actually expected.
To get rid of this warning, I think the default should be to not use schema. (I take back my suggestions in PIG-2143 !)

Also, the PigStorage.getSchema() call should cache the schema as the call is made multiple times.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Jul/11 06:31;thejas;PIG-2186.1.patch;https://issues.apache.org/jira/secure/attachment/12487398/PIG-2186.1.patch,22/Jul/11 21:58;thejas;PIG-2186.2.patch;https://issues.apache.org/jira/secure/attachment/12487563/PIG-2186.2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-23 19:31:36.315,,,no_permission,,,,,,,,,,,,67066,,,,Mon Jul 25 05:47:54 UTC 2011,,,,,,,0|i0h1hb:,97502,,,,,,,,,,"21/Jul/11 23:24;thejas;The tests TestTypeCheckingValidatorNewLP and TestNewPlanOperatorPlan are failing because PigStorage() now by default checks for schema file, and because the Configuration object passed  is null.
Fixing the PigStorage default behavior will fix these test cases as well.
",21/Jul/11 23:35;thejas;TestBZip.testBzipStoreInMultiQuery is also failing because of changes in PIG-2143. I will add a fix in this patch . ,"22/Jul/11 00:39;thejas;Here is a modified proposal -
Behavior for pigstorage as load func -
1. with ""PigStorage()"" (ie default), it tries to load schema , but does not warn if there is no schema . 
2. if user loads with ""PigStorage('-schema'), it gives an error if the schema is not present or it fails to get a schema.
3. if user loads with ""PigStorage('-noschema'), it does not load schema
","22/Jul/11 21:58;thejas;PIG-2186.2.patch - fixed unit test failures, passes test-patch .",23/Jul/11 19:31;alangates;+1.  Patch looks good.  Definitely need to get rid of the warnings about no schema when we don't have one.,25/Jul/11 05:45;thejas;Schema file caching was not done as part of  this patch. Created PIG-2190 for that. Updating summary.,25/Jul/11 05:47;thejas;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException while Accessing Empty Bag in FOREACH { FILTER },PIG-2185,12514891,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,miteshsjat,miteshsjat,21/Jul/11 11:17,26/Apr/12 20:33,14/Mar/19 03:07,19/Aug/11 21:35,0.9.0,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"On doing FILTERing on empty bag inside FOREACH, NullPointerException occurs.

The following pig script demonstrates the issue:
{code:title=empty_bag.pig}
A = LOAD 'input/empty_bag.in' USING PigStorage() AS (bg:bag{}, ch:chararray);
B = FOREACH A {
     x = FILTER bg BY $0 == '12';
         GENERATE
         *, 
         x;
         };
DUMP B;
{code}

Input is:
{code:title=input/empty_bag.in}
{(12)}  a
{(23)}  b
        c
{code}


Upon execution of Pig Script empty_bag.pig, the NullPointerException comes as shown below:
{code}
$ pig -x local empty_bag.pig
...
2011-07-11 09:52:56,810 [Thread-3] WARN  org.apache.hadoop.mapred.LocalJobRunner - job_local_0001
java.lang.NullPointerException
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:448)
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:290)
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter.getNext(POFilter.java:95)
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:406)
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.processInputBag(POProject.java:570)
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PORelationToExprProject.getNext(PORelationToExprProject.java:107)
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:316)
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.processInput(POUserFunc.java:159)
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:184)
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:258)
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:316)
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:332)
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:284)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:261)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:256)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:58)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:210)
...
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Aug/11 23:40;daijy;PIG-2185-1.patch;https://issues.apache.org/jira/secure/attachment/12490895/PIG-2185-1.patch,19/Aug/11 20:35;daijy;PIG-2185-2.patch;https://issues.apache.org/jira/secure/attachment/12491000/PIG-2185-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-21 21:10:16.71,,,no_permission,,,,,,,,,,,,64377,Reviewed,,,Fri Aug 19 22:13:27 UTC 2011,,,,,,,0|i0h1h3:,97501,,,,,,,,,,"21/Jul/11 21:10;olgan;If you pull filter outside of foreach, does that solve the problem?",21/Jul/11 21:10;olgan;Also which version of Pig is causing this behavior?,"21/Jul/11 21:17;daijy;I tried empty bag ""{}"" works, but here it is not empty bag, it is null. That cause the problem. ","21/Jul/11 21:32;thejas;this is seen in 0.9 . A workaround is -
{code}
A = LOAD 't.txt' USING PigStorage() AS (bg:bag{}, ch:chararray);
B = FOREACH A {
     x = FILTER bg BY $0 == '12';
         GENERATE
         *, 
         (bg is null ? null : x);
         };
DUMP B;
{code}","22/Jul/11 05:39;miteshsjat;@Thejas Thanks Thejas, that workaround worked.

Another workaround, which I was using is:
{code:title=empty_bag2.pig}
A = LOAD 'input/empty_bag.in' USING PigStorage() AS (bg:bag{}, ch:chararray);
A1 = FILTER A BY bg IS NOT NULL;
B = FOREACH A1 {
     x = FILTER bg BY $0 == '12';
         GENERATE
         *, 
         x;
         };
DUMP B;
{code}


@Olga: The NPE was observed on Pig 0.9.0.xx (r1127671)","19/Aug/11 19:52;daijy;test-patch pass:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Unit tests pass.

Patch is ready for review.","19/Aug/11 19:54;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1600/
-----------------------------------------------------------

Review request for pig and thejas.


Summary
-------

See PIG-2185


This addresses bug PIG-2185.
    https://issues.apache.org/jira/browse/PIG-2185


Diffs
-----

  trunk/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POProject.java 1159433 
  trunk/test/org/apache/pig/test/TestEvalPipeline2.java 1159433 

Diff: https://reviews.apache.org/r/1600/diff


Testing
-------

test-patch pass:
[exec] +1 overall.
[exec]
[exec] +1 @author. The patch does not contain any @author tags.
[exec]
[exec] +1 tests included. The patch appears to include 3 new or modified tests.
[exec]
[exec] +1 javadoc. The javadoc tool did not generate any warning messages.
[exec]
[exec] +1 javac. The applied patch does not increase the total number of javac compiler warnings.
[exec]
[exec] +1 findbugs. The patch does not introduce any new Findbugs warnings.
[exec]
[exec] +1 release audit. The applied patch does not increase the total number of release audit warnings.

Unit tests pass.


Thanks,

Daniel

","19/Aug/11 20:13;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1600/#review1560
-----------------------------------------------------------



trunk/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POProject.java
<https://reviews.apache.org/r/1600/#comment3532>

    The variable should be declared after the null check. If the null check is false  then the variable is never used.


- Santhosh


On 2011-08-19 19:54:30, Daniel Dai wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/1600/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-08-19 19:54:30)
bq.  
bq.  
bq.  Review request for pig and thejas.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  See PIG-2185
bq.  
bq.  
bq.  This addresses bug PIG-2185.
bq.      https://issues.apache.org/jira/browse/PIG-2185
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    trunk/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POProject.java 1159433 
bq.    trunk/test/org/apache/pig/test/TestEvalPipeline2.java 1159433 
bq.  
bq.  Diff: https://reviews.apache.org/r/1600/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  test-patch pass:
bq.  [exec] +1 overall.
bq.  [exec]
bq.  [exec] +1 @author. The patch does not contain any @author tags.
bq.  [exec]
bq.  [exec] +1 tests included. The patch appears to include 3 new or modified tests.
bq.  [exec]
bq.  [exec] +1 javadoc. The javadoc tool did not generate any warning messages.
bq.  [exec]
bq.  [exec] +1 javac. The applied patch does not increase the total number of javac compiler warnings.
bq.  [exec]
bq.  [exec] +1 findbugs. The patch does not introduce any new Findbugs warnings.
bq.  [exec]
bq.  [exec] +1 release audit. The applied patch does not increase the total number of release audit warnings.
bq.  
bq.  Unit tests pass.
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Daniel
bq.  
bq.

","19/Aug/11 20:33;jiraposter@reviews.apache.org;

bq.  On 2011-08-19 20:13:04, Santhosh Srinivasan wrote:
bq.  > trunk/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POProject.java, line 447
bq.  > <https://reviews.apache.org/r/1600/diff/1/?file=33803#file33803line447>
bq.  >
bq.  >     The variable should be declared after the null check. If the null check is false  then the variable is never used.

I will change it. Thanks Santhosh!


- Daniel


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1600/#review1560
-----------------------------------------------------------


On 2011-08-19 19:54:30, Daniel Dai wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/1600/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-08-19 19:54:30)
bq.  
bq.  
bq.  Review request for pig and thejas.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  See PIG-2185
bq.  
bq.  
bq.  This addresses bug PIG-2185.
bq.      https://issues.apache.org/jira/browse/PIG-2185
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    trunk/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POProject.java 1159433 
bq.    trunk/test/org/apache/pig/test/TestEvalPipeline2.java 1159433 
bq.  
bq.  Diff: https://reviews.apache.org/r/1600/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  test-patch pass:
bq.  [exec] +1 overall.
bq.  [exec]
bq.  [exec] +1 @author. The patch does not contain any @author tags.
bq.  [exec]
bq.  [exec] +1 tests included. The patch appears to include 3 new or modified tests.
bq.  [exec]
bq.  [exec] +1 javadoc. The javadoc tool did not generate any warning messages.
bq.  [exec]
bq.  [exec] +1 javac. The applied patch does not increase the total number of javac compiler warnings.
bq.  [exec]
bq.  [exec] +1 findbugs. The patch does not introduce any new Findbugs warnings.
bq.  [exec]
bq.  [exec] +1 release audit. The applied patch does not increase the total number of release audit warnings.
bq.  
bq.  Unit tests pass.
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Daniel
bq.  
bq.

",19/Aug/11 20:35;daijy;PIG-2185-2.patch address Santhosh's review comment.,"19/Aug/11 20:37;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1600/
-----------------------------------------------------------

(Updated 2011-08-19 20:36:09.595311)


Review request for pig and thejas.


Summary
-------

See PIG-2185


This addresses bug PIG-2185.
    https://issues.apache.org/jira/browse/PIG-2185


Diffs (updated)
-----

  trunk/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POProject.java 1159742 
  trunk/test/org/apache/pig/test/TestEvalPipeline2.java 1159742 

Diff: https://reviews.apache.org/r/1600/diff


Testing
-------

test-patch pass:
[exec] +1 overall.
[exec]
[exec] +1 @author. The patch does not contain any @author tags.
[exec]
[exec] +1 tests included. The patch appears to include 3 new or modified tests.
[exec]
[exec] +1 javadoc. The javadoc tool did not generate any warning messages.
[exec]
[exec] +1 javac. The applied patch does not increase the total number of javac compiler warnings.
[exec]
[exec] +1 findbugs. The patch does not introduce any new Findbugs warnings.
[exec]
[exec] +1 release audit. The applied patch does not increase the total number of release audit warnings.

Unit tests pass.


Thanks,

Daniel

","19/Aug/11 21:25;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1600/#review1568
-----------------------------------------------------------

Ship it!


- Santhosh


On 2011-08-19 20:36:09, Daniel Dai wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/1600/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-08-19 20:36:09)
bq.  
bq.  
bq.  Review request for pig and Thejas Nair.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  See PIG-2185
bq.  
bq.  
bq.  This addresses bug PIG-2185.
bq.      https://issues.apache.org/jira/browse/PIG-2185
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    trunk/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POProject.java 1159742 
bq.    trunk/test/org/apache/pig/test/TestEvalPipeline2.java 1159742 
bq.  
bq.  Diff: https://reviews.apache.org/r/1600/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  test-patch pass:
bq.  [exec] +1 overall.
bq.  [exec]
bq.  [exec] +1 @author. The patch does not contain any @author tags.
bq.  [exec]
bq.  [exec] +1 tests included. The patch appears to include 3 new or modified tests.
bq.  [exec]
bq.  [exec] +1 javadoc. The javadoc tool did not generate any warning messages.
bq.  [exec]
bq.  [exec] +1 javac. The applied patch does not increase the total number of javac compiler warnings.
bq.  [exec]
bq.  [exec] +1 findbugs. The patch does not introduce any new Findbugs warnings.
bq.  [exec]
bq.  [exec] +1 release audit. The applied patch does not increase the total number of release audit warnings.
bq.  
bq.  Unit tests pass.
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Daniel
bq.  
bq.

",19/Aug/11 21:35;daijy;Patch committed to trunk. Thanks Santhosh!,"19/Aug/11 22:13;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1600/#review1571
-----------------------------------------------------------

Ship it!


+1

- Thejas


On 2011-08-19 20:36:09, Daniel Dai wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/1600/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-08-19 20:36:09)
bq.  
bq.  
bq.  Review request for pig and Thejas Nair.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  See PIG-2185
bq.  
bq.  
bq.  This addresses bug PIG-2185.
bq.      https://issues.apache.org/jira/browse/PIG-2185
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    trunk/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POProject.java 1159742 
bq.    trunk/test/org/apache/pig/test/TestEvalPipeline2.java 1159742 
bq.  
bq.  Diff: https://reviews.apache.org/r/1600/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  test-patch pass:
bq.  [exec] +1 overall.
bq.  [exec]
bq.  [exec] +1 @author. The patch does not contain any @author tags.
bq.  [exec]
bq.  [exec] +1 tests included. The patch appears to include 3 new or modified tests.
bq.  [exec]
bq.  [exec] +1 javadoc. The javadoc tool did not generate any warning messages.
bq.  [exec]
bq.  [exec] +1 javac. The applied patch does not increase the total number of javac compiler warnings.
bq.  [exec]
bq.  [exec] +1 findbugs. The patch does not introduce any new Findbugs warnings.
bq.  [exec]
bq.  [exec] +1 release audit. The applied patch does not increase the total number of release audit warnings.
bq.  
bq.  Unit tests pass.
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Daniel
bq.  
bq.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not able to provide positional reference to macro invocations,PIG-2184,12514890,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xutingz,vivekp,vivekp,21/Jul/11 11:16,26/Apr/12 20:32,14/Mar/19 03:07,17/Nov/11 08:39,0.9.0,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"It looks like the macro functionality doesnt support for positional references. The below is an example script;

----------------------------------------------------------------------------
DEFINE my_macro (X,key) returns Y
        {
        tmp1 = foreach  $X generate TOKENIZE((chararray)$key) as tokens;
        tmp2 = foreach tmp1 generate flatten(tokens);
        tmp3 = order tmp2 by $0;
        $Y = distinct tmp3;
        }

A = load 'sometext' using TextLoader() as (row1) ;
E = my_macro(A,A.$0);
dump E;
----------------------------------------------------------------------------

This script execution fails at parsing staging itself;

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1000: Error during parsing. <file try1.pig, line 16,
column 16>  mismatched input '.' expecting RIGHT_PAREN

If i replace A.$0 with the field name ie row1 the script runs fine.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Nov/11 01:17;xutingz;2184.patch;https://issues.apache.org/jira/secure/attachment/12503703/2184.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-07 18:47:23.059,,,no_permission,,,,,,,,,,,,46024,Reviewed,,,Thu Nov 17 08:39:12 UTC 2011,,,,,,,0|i0h1gv:,97500,,,,,,,,,,"07/Nov/11 18:47;olgan;Hi Xuting, Could you take a look at this when you get a chance.","10/Nov/11 18:33;daijy;The script seems not right. Macro only do literal substitution, so with
E = my_macro(A,A.$0);

We will get ""tmp1 = foreach $X generate TOKENIZE((chararray)A.$0) as tokens;"", which is wrong.

However, ""E = my_macro(A,$0);"" also does not work. We should fix this case. There are two issues:
1. in parser, my_macro(A, $0) is not allowed, we need to allow this
2. in param substitution, $0 is not a valid value, we need to fix","10/Nov/11 19:03;xutingz;Hi Daniel,

   Thanks for the comments. I guess this also explains the error I got after modifing the parser and passing the A.$0 as parameter into the macrio function. One thing I am not sure is if we allow parameter $0, then how can we know which dataset $0 stands for? For example, if my_macro only contains one parameter and user called my_macro($0)?

Best,
Xuting","11/Nov/11 01:56;alangates;bq. in param substitution, $0 is not a valid value, we need to fix
How do we need to fix this?  $0 shouldn't be a valid parameter substitution.","11/Nov/11 02:03;daijy;Hi, Alan,
Here we need to replace 'key' to ""$0"". Parameter substitution does not allow ""$0"" to be the value, but seems it takes ""\\$0"". This works but I am not if there's a better fix.","14/Nov/11 18:57;daijy;As per Pig Macro doc, Pig macros support four types of parameters:
* alias (IDENTIFIER)
* integer
* float
* string literal (quoted string)

We shall add one more:
* field (IDENTIFIER or position in the form $int)

Which the new definition, we will not allow ""A.$0"", since this is a mix of alias and field.","15/Nov/11 01:25;xutingz; I use the method which adds ""\\"" in front of the dollar variable. If we call the Matcher.quoteReplacement(val) function in PreprocessorContext.java, it will be too general and as a result, there will be some problem in parameter substitution. For example, '\' will be added in front of URL parameters.

 
 Ant test-commit has been run successfully and a new Junit test has been added into the test/org/apache/pig/test/TestMacroExpansion.java. The old Junit test of PIG-2081 has been removed because it is about adding error message under this situation ",15/Nov/11 19:06;daijy;Sounds good. Running the tests.,"17/Nov/11 08:39;daijy;Unit test pass. test-patch:

     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     -1 javac.  The applied patch generated 964 javac compiler warnings (more than the trunk's current 962 warnings).
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 475 release audit warnings (more than the trunk's current 468 warnings).

javac warning is in generated file. No new file added so ignore release audit warning.

Patch committed to trunk and 0.10 branch.

Thanks Xuting!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig not working with Hadoop 0.20.203.0,PIG-2183,12514825,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,_iyp,_iyp,20/Jul/11 20:34,05/Oct/11 17:20,14/Mar/19 03:07,10/Aug/11 18:13,0.10.0,0.8.1,0.9.1,,,,,0.10.0,0.9.1,,,grunt,site,,0,,,,,,,,,,,,,"When running pig, I get the following error.
Error before Pig is launched
----------------------------
ERROR 2999: Unexpected internal error. Failed to create DataStorage

java.lang.RuntimeException: Failed to create DataStorage
	at org.apache.pig.backend.hadoop.datastorage.HDataStorage.init(HDataStorage.java:75)
	at org.apache.pig.backend.hadoop.datastorage.HDataStorage.<init>(HDataStorage.java:58)
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.init(HExecutionEngine.java:214)
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.init(HExecutionEngine.java:134)
	at org.apache.pig.impl.PigContext.connect(PigContext.java:183)
	at org.apache.pig.PigServer.<init>(PigServer.java:226)
	at org.apache.pig.PigServer.<init>(PigServer.java:215)
	at org.apache.pig.tools.grunt.Grunt.<init>(Grunt.java:55)
	at org.apache.pig.Main.run(Main.java:452)
	at org.apache.pig.Main.main(Main.java:107)
Caused by: java.io.IOException: Call to rasputin/192.168.1.3:9000 failed on local exception: java.io.EOFException
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:775)
	at org.apache.hadoop.ipc.Client.call(Client.java:743)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
	at $Proxy0.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:359)
	at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:106)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:207)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:170)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:82)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1378)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1390)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:196)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:95)
	at org.apache.pig.backend.hadoop.datastorage.HDataStorage.init(HDataStorage.java:72)
	... 9 more
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:375)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:501)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:446)
================================================================================

My env vars are defined in bin/pig as the following
export JAVA_HOME=""/etc/java-config-2/current-system-vm""
export PIG_CLASSPATH=""/var/hadoop/pig/pig-withouthadoop.jar:$HADOOP_HOME/hadoop-core-0.20.203.0.jar:$HADOOP_HOME/lib:$HADOOP_CONF_DIR""

","Gentoo Linux Kernel: 2.6.38-gentoo-r6
java version ""1.6.0_26""
Ant version 1.8.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Aug/11 07:21;daijy;PIG-2183-1.patch;https://issues.apache.org/jira/secure/attachment/12489326/PIG-2183-1.patch,10/Aug/11 04:53;daijy;PIG-2183-2.patch;https://issues.apache.org/jira/secure/attachment/12489937/PIG-2183-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-22 01:25:47.42,,,no_permission,,,,,,,,,,,,46538,Reviewed,,,Wed Aug 10 18:13:26 UTC 2011,,,,,,,0|i0h1gn:,97499,,,,,,,,,,"20/Jul/11 20:35;_iyp;Also, forgot to mention that I built pig with ""ant jar-withouthadoop""","20/Jul/11 20:44;_iyp;Google waited till after I posted this issue to show me this.
https://issues.apache.org/jira/browse/PIG-2148","22/Jul/11 01:04;_iyp;Finally got it to work. 

I built with the above mentioned ""ant jar-withouthadoop""
Hacked up the wrapper script to this -> http://pastebin.com/5WuLRUAN
Used this conf/pig-env.sh -> http://pastebin.com/vs5fAHpu
Replaced the hadoop jars in build/ivy/lib/Pig with the 0.20.203 core and test ones
Ran the wrapper script 



","22/Jul/11 01:25;daijy;Thanks John. That is a first step toward a patch. To make it a formal patch, we need to add all your did into bin/pig. Are you interested to make a patch?","31/Jul/11 15:37;_iyp;My bash script is pretty clunky, would anyone be able to spruce it up a bit?

Also, I wanted to add that for this fix you need to delete the old hadoop jars.","04/Aug/11 07:21;daijy;Restructure pig script to use pig-withouthadoop.jar, and link hadoop jars dynamically.","04/Aug/11 17:22;daijy;Here is the change I made in PIG-2183-1.patch:
1. When releasing Pig, include pig-withouthadoop.jar instead of fat pig.jar
2. For hadoop 203+, using HADOOP_HOME to find hadoop binary, run ""hadoop classpath"" to get all hadoop libraries
3. For hadoop 20.2, assume hadoop libraries is inside HADOOP_HOME
4. If no HADOOP_HOME defined, Pig will link hadoop 20.2 jars inside $PIG_HOME/lib
5. Still pick PIG_CLASSPATH first, so user can override hadoop conf dir",08/Aug/11 21:50;thejas;+1,"10/Aug/11 01:41;daijy;PIG-2183-2.patch include some bug fix, also change undesired behavior introduced by PIG-1857.","10/Aug/11 18:13;daijy;Manually tested with release build, rpm/deb package and svn checkout, all works fine.

Committed to both trunk and 0.9 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improvement : for error message when describe misses alias,PIG-2181,12514780,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,vivekp,vivekp,vivekp,20/Jul/11 11:35,26/Apr/12 20:33,14/Mar/19 03:07,05/Aug/11 17:21,0.9.0,,,,,,,0.10.0,,,,,,,0,newbie,,,,,,,,,,,,"In Pig 0.9, if I have a describe without an alias, it throws a NullPointerException like below.

ERROR 2999: Unexpected internal error. null

java.lang.NullPointerException
        at org.apache.pig.tools.grunt.GruntParser.processDescribe(GruntParser.java:270)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:317)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:188)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:164)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)
        at org.apache.pig.Main.run(Main.java:553)
        at org.apache.pig.Main.main(Main.java:108)


For example;
describe;

This message is of no use from a users perspective. Especially when my script becomes large and I have added couple of describe statements. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Jul/11 10:53;vivekp;PIG-2181_1.patch;https://issues.apache.org/jira/secure/attachment/12487282/PIG-2181_1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-22 18:01:32.17,,,no_permission,,,,,,,,,,,,639,Reviewed,,,Fri Aug 05 17:21:17 UTC 2011,,,,,,,0|i0h1g7:,97497,,,,,,,,,,"21/Jul/11 10:53;vivekp;This looks like a change in behaviour introduced due to the parser changes. In Pig 0.8 , ""describe ; "" would show the describe of the last alias
rather than NullPointerException. This is done through;
 alias = mPigServer.getPigContext().getLastAlias(); , but in 0.9, the last alias is not captured in Pig Context.


Attaching an initial patch.",22/Jul/11 18:01;daijy;Good catch. +1 for patch. I will commit it if tests pass.,"05/Aug/11 17:21;daijy;Unit tests pass. Test-patch:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Patch committed to trunk. 

Thanks Vivek!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tests in TestLoad are failing ,PIG-2179,12514713,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,19/Jul/11 22:13,26/Apr/12 20:32,14/Mar/19 03:07,21/Jul/11 15:50,0.10.0,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Jul/11 15:05;thejas;PIG-2179.1.patch;https://issues.apache.org/jira/secure/attachment/12487165/PIG-2179.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-21 00:33:43.706,,,no_permission,,,,,,,,,,,,67251,,,,Thu Jul 21 15:50:56 UTC 2011,,,,,,,0|i0h1fr:,97495,,,,,,,,,,"20/Jul/11 16:14;thejas;test-patch and test-commit succeeded. 
Changes in the patch -
- JsonMetadata.java - If the input file has comma separated filenames, lookup schema file corresponding to each file separately.
- TestLoad.java - the har file being loaded in  test testNonDfsLocation does not exist, so use '-noschema' option .",21/Jul/11 00:33;daijy;Patch looks good. Add the comments in findMetaFile that it deals with comma separated filenames before commit.,21/Jul/11 15:50;thejas;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBaseStorage column filters miss some fields,PIG-2174,12514500,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,billgraham,billgraham,billgraham,18/Jul/11 20:21,26/Apr/12 20:32,14/Mar/19 03:07,11/Aug/11 21:16,,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"When mixing static and dynamic column mappings, {{HBaseStorage}} sometimes doesn't pick up the static column values and nulls are returned. I believe this bug has been masked by HBase being a bit over-eager when it comes to respecting column filters (i.e. HBase is returning more columns than it should).

For example, this query returns nulls for the {{sc}} column, even when it contains data:
{noformat}
a = LOAD 'hbase://pigtable_1' USING
  org.apache.pig.backend.hadoop.hbase.HBaseStorage
  ('pig:sc pig:prefixed_col_*','-loadKey') AS
  (rowKey:chararray, sc:chararray, pig_cf_map:map[]);
{noformat}

What is very strange (about HBase), is that the same script will return values just fine if {{sc}} is instead {{col_a}}, assuming of course that both columns contain data:
{noformat}
a = LOAD 'hbase://pigtable_1' USING
  org.apache.pig.backend.hadoop.hbase.HBaseStorage
  ('pig:col_a pig:prefixed_col_*','-loadKey') AS
  (rowKey:chararray, col_a:chararray, pig_cf_map:map[]);
{noformat}

Potential HBase issues aside, I think there is a bug in the logic on the Pig side. Patch to follow. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Jul/11 20:42;billgraham;PIG-2174_1.patch;https://issues.apache.org/jira/secure/attachment/12486919/PIG-2174_1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-08 17:45:39.203,,,no_permission,,,,,,,,,,,,65299,,,,Thu Aug 11 21:16:06 UTC 2011,,,,,,,0|i0h1ev:,97491,Fix HBaseStorage column filtering bug.,,,,,,,,,"18/Jul/11 20:42;billgraham;Adding patch #1, please review.","18/Jul/11 22:19;billgraham;FYI, the HBase issue I mentioned was in fact a bug which has been fixed (HBASE-3550). The Pig bug is still valid though.",08/Aug/11 17:02;billgraham;Ping. This patch is starting to get stale. Can someone please review?,"08/Aug/11 17:45;dvryaboy;Sorry Bill, didn't see this.. if you file an hbase storage bug, just go ahead and add me to the watchlist so I can't miss it.
I'll add it to my growing stack of hbase tickets to review.","11/Aug/11 06:30;dvryaboy;+1 assuming test-patch passes.

Sadly at the moment TestHBaseStorage doesn't pass in trunk even without this patch..","11/Aug/11 21:16;dvryaboy;Committed to trunk. Thanks, Bill!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
piggybank datetime conversion javadocs not properly formatted,PIG-2173,12514478,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,hluu,joecrobak,joecrobak,18/Jul/11 18:20,22/Feb/13 04:53,14/Mar/19 03:07,30/May/12 18:34,0.8.1,,,,,,,0.11,,,,,,,0,newbie,,,,,,,,,,,,"e.g. http://pig.apache.org/docs/r0.8.1/api/org/apache/pig/piggybank/evaluation/datetime/convert/CustomFormatToISO.html

The sample code in the class description should be wrapped in a <pre> or otherwise formatted correctly.",,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,27/May/12 15:11;hluu;customformattoiso.diff;https://issues.apache.org/jira/secure/attachment/12529893/customformattoiso.diff,28/May/12 01:44;hluu;datetime.diff;https://issues.apache.org/jira/secure/attachment/12529919/datetime.diff,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-05-27 15:11:21.262,,,no_permission,,,,,,,,,,,,67611,Reviewed,,,Wed May 30 18:34:43 UTC 2012,,,,,,,0|i0h1en:,97490,,,,,,,,,,27/May/12 15:11;hluu;also added <ul> around the various data formats.,27/May/12 21:36;joecrobak;Thanks for looking at this Hien. Just a note that documentation for _all_ of the piggybank datetime functions are missing formatting. the CustomFormatToISO class was just one example.,27/May/12 21:48;hluu;OK.  Let me go through all the of them.,28/May/12 01:44;hluu;This patch should include all the datetime functions.  Let me know if I missed any.,30/May/12 18:34;daijy;Patch committed to trunk. Thanks Hien!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix test failure for ant 1.8.x,PIG-2172,12514470,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,18/Jul/11 17:16,23/Jan/12 07:31,14/Mar/19 03:07,18/Jul/11 22:21,0.10.0,0.9.1,,,,,,0.10.0,0.9.2,,,impl,,,0,,,,,,,,,,,,,"Some tests fail using ant 1.8.x. But in ant 1.7.x, these tests work.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Jul/11 17:41;daijy;PIG-2172-1.patch;https://issues.apache.org/jira/secure/attachment/12486892/PIG-2172-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-18 22:06:31.015,,,no_permission,,,,,,,,,,,,35757,Reviewed,,,Tue Jan 10 19:26:22 UTC 2012,,,,,,,0|i0h1ef:,97489,,,,,,,,,,18/Jul/11 22:06;thejas;+1,"13/Sep/11 08:22;fang fang chen;Only the three test cases mentioned in pig-2171-1.patch failed with ant-1.8.2. But succeeded with ant-1.8.0.
After patching, all test cases succeeded.",10/Jan/12 01:35;daijy;Try to get it into 0.9.2 since many build infrastructure is using ant 1.8.2 and is hard to change.,"10/Jan/12 19:17;thw;Applied this patch to branch-0.9: ant test-core passes.
",10/Jan/12 19:26;daijy;Committed to 0.9 branch as well.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestScriptLanguage is broken on trunk,PIG-2171,12514386,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,17/Jul/11 21:38,26/Apr/12 20:33,14/Mar/19 03:07,19/Jul/11 21:50,0.10.0,,,,,,,0.10.0,,,,impl,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Jul/11 21:40;daijy;PIG-2171-1.patch;https://issues.apache.org/jira/secure/attachment/12486780/PIG-2171-1.patch,18/Jul/11 23:54;thejas;PIG-2171.2.patch;https://issues.apache.org/jira/secure/attachment/12486941/PIG-2171.2.patch,19/Jul/11 20:29;thejas;PIG-2171.3.patch;https://issues.apache.org/jira/secure/attachment/12487055/PIG-2171.3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-07-18 23:54:51.244,,,no_permission,,,,,,,,,,,,67457,Reviewed,,,Tue Jul 19 21:50:00 UTC 2011,,,Patch Available,,,,0|i0h1e7:,97488,,,,,,,,,,"18/Jul/11 23:54;thejas;I have made a small change to the patch, so that the expected message including line number is checked - PIG-2171.2.patch ",19/Jul/11 20:29;thejas;PIG-2171.3.patch - Fixed an issue Daniel found with the new Util function used. Ran test-commit and test-patch. ,19/Jul/11 20:35;daijy;+1,"19/Jul/11 21:50;thejas;Patch committed to trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need a way to deal with params and param_file in embedded pig in python,PIG-2165,12514131,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,supreeth,supreeth,14/Jul/11 22:20,10/Feb/15 20:50,14/Mar/19 03:07,06/Nov/11 08:43,0.9.0,,,,,,,0.10.0,,,,impl,,,1,,,,,,,,,,,,,"I am using embedded pig in python and cannot pass param key value pairs to the python script. The only way to pass params  seem to be by passing it in the bind command.

Is there a plan to have command line parameters to a pig embedded python script? Similar needs for param_file and using the environment variables.

Thanks
Supreeth",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Sep/11 21:50;daijy;PIG-2165-1.patch;https://issues.apache.org/jira/secure/attachment/12494870/PIG-2165-1.patch,06/Nov/11 08:41;daijy;PIG-2165-2.patch;https://issues.apache.org/jira/secure/attachment/12502653/PIG-2165-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-15 05:38:07.042,,,no_permission,,,,,,,,,,,,35758,Reviewed,,,Sun Nov 06 08:57:21 UTC 2011,,,,,,,0|i09zdb:,56171,,,,,,,,,,"15/Jul/11 05:38;julienledem;I see two options:
- One way would be to use the pig parameters and assign the values to global variables. 
{code}pig -p foo=bar myscript.py{code}
and then in the script, the variable foo contains ""bar""

- The other would be to populate sys.argv with the command line parameters
{code}sys.argv = [ ""pig"", ""-p"", ""foo=bar"", ""myscript.py"" ]{code}
","15/Jul/11 16:14;alangates;I like the idea of only passing the parameters from either the command line or the parameters file.  I don't see why the Python script should care about other parameters that will be Pig specific.  Whether those parameters are placed in global variables, in sys.argv, or in a separate dictionary (pig_argv?) I don't have an opinion on.","15/Jul/11 17:15;julienledem;I agree, on second thoughts my second option is a bad idea as the script can be launched either through the command line or programmatically through the java API. In that case the only parameters that are available are the ones passed through -p.
There's no good way to reuse sys.argv here, it is not really a good fit as it is a list of strings.

Suggestions:
 - a separate dictionary: Pig.getParameters() ?
 - placed in global variables","08/Sep/11 23:00;daijy;I think we shall stick with the conventional Pig command line options (-p, -m). I don't think we need to invent new ways specific for Pig embedding.","17/Sep/11 00:00;julienledem;Hi Daniel,
Are you suggesting to use the preprocessor to replace $variable with its value passed in ""-p variable=value"" ?","17/Sep/11 00:08;daijy;Yes, for ""$var"" in your script, it can be either a bounded variable, or a command line parameter. I treat them the same in patch.","13/Oct/11 00:39;julienledem;Hi Daniel,
This works but I would rather have the parameters set to global variables in the Interpreter.
Using the pre-processor is less intuitive as parameter values will need different escaping depending of the language or the location of the script where they are used. For example in a string vs not. Assigning to a global variable avoids this kind of issue.
Julien  ",13/Oct/11 01:31;daijy;Thanks Julien. I'm fine with global variable approach. I can submit another patch.,"24/Oct/11 21:45;daijy;Hi, Julien, 
PIG-2165-1.patch does use global variable PigContext.params to pass params. What it does is:
1. Put params into PigContext.params when processing command line
2. Adding PigContext.params to do variable binding:
{code}
        // plist is the variable array to bind
        if (getScriptContext().getPigContext().getParams()!=null) {
            for (String param : getScriptContext().getPigContext().getParams()) {
                plist.add(param);
            }
        }
        // Use plist to do the binding
{code}
3. For param_file, instead of reading and put into params, I delegate it to ParameterSubstitutionPreprocessor, which already handles parameter_file:
{code}
    psp.genSubstitutedFile(in, writer, plist.toArray(params), scriptContext.getPigContext().getParamFiles().toArray(type1));
{code}
4. I also did some refactory in main() to use PigContext.params","04/Nov/11 23:52;ashutoshc;+1 Looks good to me. If its easy enough, it will be good to add e2e test with param file as well, since patch touches upon that part of code.",06/Nov/11 07:59;daijy;Add new test case as Ashutosh suggest.,06/Nov/11 08:43;daijy;Patch committed to both trunk and 0.10 branch.,"06/Nov/11 08:57;daijy;Unit test pass. test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 468 release audit warnings (more than the trunk's current 461 warnings).

No new files added, ignore release audit warning.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CombinerOptimizer exception with multi-query,PIG-2164,12514122,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,14/Jul/11 21:00,05/Oct/11 17:20,14/Mar/19 03:07,21/Sep/11 21:44,0.9.0,,,,,,,0.10.0,0.9.1,,,,,,0,,,,,,,,,,,,,"{code}
l = load 'x' as (a,b,c);
f1 = foreach (group l by (a,b)) generate COUNT_STAR(*);
f2 = foreach (group l by (a,null)) generate COUNT_STAR(*);
u = union f1, f2; 
explain u;  

Pig Stack Trace
---------------
ERROR 2018: Internal error. Unable to introduce the combiner for optimization.

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1067: Unable to explain alias u
        at org.apache.pig.PigServer.explain(PigServer.java:1063)
        at org.apache.pig.tools.grunt.GruntParser.explainCurrentBatch(GruntParser.java:393)
        at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:325)
        at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:288)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.Explain(PigScriptParser.java:665)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:325)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:188)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:164)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:67)
        at org.apache.pig.Main.run(Main.java:487)
        at org.apache.pig.Main.main(Main.java:108)
Caused by: org.apache.pig.impl.plan.optimizer.OptimizerException: ERROR 2018: Internal error. Unable to introduce the combiner for optimization.
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.CombinerOptimizer.visitMROp(CombinerOptimizer.java:313)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper.visit(MapReduceOper.java:252)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper.visit(MapReduceOper.java:42)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:69)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:71)
        at org.apache.pig.impl.plan.DepthFirstWalker.walk(DepthFirstWalker.java:52)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.compile(MapReduceLauncher.java:492)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.explain(MapReduceLauncher.java:454)
        at org.apache.pig.PigServer.explain(PigServer.java:1055)
        ... 10 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: 2
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.CombinerOptimizer.addAlgebraicFuncToCombineFE(CombinerOptimizer.java:515)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.CombinerOptimizer.visitMROp(CombinerOptimizer.java:227)
        ... 19 more
================================================================================


{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-09-21 21:44:49.824,,,no_permission,,,,,,,,,,,,35759,,,,Wed Sep 21 21:44:49 UTC 2011,,,,,,,0|i09zmv:,56214,,,,,,,,,,21/Sep/11 21:44;daijy;It is fixed by PIG-2286. Apply PIG-2286-2.patch and the error go away.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bin/pig should not modify user args,PIG-2162,12513995,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rangadi,rangadi,rangadi,13/Jul/11 22:19,26/Apr/12 20:33,14/Mar/19 03:07,18/Jul/11 04:00,0.8.0,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"
PIG launcher script (bin/pig) joins all the user arguments in one string. This leads to confusion when the arguments contain spaces.
i.e. '{{bin/pig -p sub=""i > 2""}}' is should not same as '{{bin/pig -p ""sub"" ""i"" "">"" ""2""}}', but it is.

",,,,,,,,,,,,,,,,,,,PIG-2180,,,,,,,,,,,,,13/Jul/11 22:20;rangadi;PIG-2162.patch;https://issues.apache.org/jira/secure/attachment/12486382/PIG-2162.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-18 04:00:05.529,,,no_permission,,,,,,,,,,,,67456,,,,Tue Jul 19 22:23:28 UTC 2011,,,,,,,0|i0h1cn:,97481,bin/pig handles args with spaces correctly.,,,,,,,,,13/Jul/11 22:20;rangadi;patch preservers the user args.,"13/Jul/11 22:34;rangadi;There is a related issue with parsing of params inside PIG. We need to escape '=' and any space before '=' which specifying params on command line, but there is is no such restriction for using '%default'. We might need merge the parser for these two. This might a relatively minor tweak to ParamLoader.jj. 

to get equivalent of {{%declare cond 'i == 2'}} we need to use {{ -p cond='i\ \=\= 2' }}


","18/Jul/11 04:00;thejas;+1. Patch committed to trunk.
Thanks Raghu!
Can you please open a new jira to track the issue of parsing of '=' in parameters ?",19/Jul/11 22:23;rangadi;Thanks Thejas. filed PIG-2180.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New logical plan uses incorrect class for  SUM causing for ClassCastException,PIG-2159,12513906,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,daijy,vivekp,vivekp,13/Jul/11 11:41,04/Aug/11 00:35,14/Mar/19 03:07,18/Jul/11 23:48,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"The below is my script;
{code}
A = load 'input1' using PigStorage(',')  as (f1:int,f2:int,f3:int,f4:long,f5:double);
B = load 'input2' using PigStorage(',')  as (f1:int,f2:int,f3:int,f4:long,f5:double);
C = load 'input_Main' using PigStorage(',')  as (f1:int,f2:int,f3:int);
U = UNION ONSCHEMA A,B;
J = join C by (f1,f2,f3) LEFT OUTER, U by (f1,f2,f3);
Porj = foreach J generate C::f1 as f1 ,C::f2 as f2,C::f3 as f3,U::f4 as f4,U::f5 as f5;
G = GROUP Porj by (f1,f2,f3,f5);
Final = foreach G generate SUM(Porj.f4) as total;
dump Final;
{code}


The script fails at while computing the sum with class cast exception.
Caused by: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Double
         at org.apache.pig.builtin.DoubleSum$Initial.exec(DoubleSum.java:82)
         ... 19 more

This is clearly a bug in the logical plan created in 0.9. The sum operation should have processed using org.apache.pig.builtin.LongSum, but instead 0.9 logical plan have used org.apache.pig.builtin.DoubleSum which is meant for sum of doubles. And hence the ClassCastException.

The same script works fine with Pig 0.8.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Jul/11 22:24;daijy;PIG-2159-1.patch;https://issues.apache.org/jira/secure/attachment/12486523/PIG-2159-1.patch,15/Jul/11 00:49;daijy;PIG-2159-2.patch;https://issues.apache.org/jira/secure/attachment/12486537/PIG-2159-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-14 18:39:50.294,,,no_permission,,,,,,,,,,,,66181,Reviewed,,,Mon Jul 18 23:48:18 UTC 2011,,,,,,,0|i0h1bz:,97478,,,,,,,,,,"13/Jul/11 11:46;vivekp;The issue could be reproduced using the above mentioned script. A sample input is provided below ;
{code}
input1
100,101,102,103,104,105
110,111,112,113,114,115

input2
200,201,202,203,204,205
210,211,212,213,214,215

input0
100,101,102,103,104,105
200,201,202,203,204,205
{code}



The logical plan from explain for 0.9;
#-----------------------------------------------
# New Logical Plan:
#-----------------------------------------------
Final: (Name: LOStore Schema: total#95:double)
|
|---Final: (Name: LOForEach Schema: total#95:double)
     |   |
     |   (Name: LOGenerate[false] Schema: 
total#95:double)ColumnPrune:InputUids=[91]ColumnPrune:OutputUids=[95]
     |   |   |
     |   |   (Name: UserFunc(org.apache.pig.builtin.DoubleSum) Type: 
double Uid: 95)




The logical plan from explain for 0.8;
#-----------------------------------------------
# New Logical Plan:
#-----------------------------------------------
fake: (Name: LOStore Schema: total#68:long)
|
|---Final: (Name: LOForEach Schema: total#68:long)
     |   |
     |   (Name: LOGenerate[false] Schema: 
total#68:long)ColumnPrune:InputUids=[66]ColumnPrune:OutputUids=[68]
     |   |   |
     |   |   (Name: UserFunc(org.apache.pig.builtin.LongSum) Type: long 
Uid: 68)
     |   |   |
     |   |   |---(Name: Dereference Type: bag Uid: 67 Column:[3])
     |   |       |
     |   |       |---Porj:(Name: Project Type: bag Uid: 66 Input: 0 
Column: (*))
     |   |
     |   |---Porj: (Name: LOInnerLoad[1] Schema: 
f1#43:int,f2#44:int,f3#45:int,f4#59:long,f5#60:double)
","14/Jul/11 18:39;daijy;The error is caused by schema generated by unionOnSchema, which only has empty uid. This would impact many queries containing unionOnSchema.","14/Jul/11 18:46;dvryaboy;Sounds like a blocker for the 0.9 release, changing the priority accordingly. 
Nice catch.",15/Jul/11 00:49;daijy;Fix test failure on TestUnionOnSchemaSetter.,"15/Jul/11 17:14;alangates;Dmitry, I don't see this as a blocker for 0.9.  It does not produce wrong results and users can rewrite their scripts to work around it.  I agree it should go on the 0.9 branch and be part of the anticipated 0.9.1 release.  ","18/Jul/11 04:43;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1138/
-----------------------------------------------------------

Review request for pig and thejas.


Summary
-------

See PIG-2159


This addresses bug PIG-2159.
    https://issues.apache.org/jira/browse/PIG-2159


Diffs
-----

  trunk/src/org/apache/pig/newplan/logical/relational/LOUnion.java 1146183 
  trunk/test/org/apache/pig/parser/TestUnionOnSchemaSetter.java 1146183 
  trunk/test/org/apache/pig/test/TestEvalPipeline2.java 1146183 

Diff: https://reviews.apache.org/r/1138/diff


Testing
-------

Unit-test:
    all pass

Test-patch:
    all pass


Thanks,

Daniel

","18/Jul/11 23:12;thejas;+1
",18/Jul/11 23:48;daijy;Patch committed to both 0.9 branch and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null pointer exception while reporting progress,PIG-2152,12512499,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,olgan,olgan,01/Jul/11 20:33,05/Oct/11 17:20,14/Mar/19 03:07,06/Sep/11 18:03,0.9.0,,,,,,,0.10.0,0.9.1,,,,,,0,,,,,,,,,,,,,"We have observed the following issues with code built from Pig 0.9 branch. We have not seen this with earlier versions; however, since this happens once in a while and is not reproducible at will it is not clear whether the issue is specific to 0.9 or not.

Here is the stack:

java.lang.NullPointerException at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.ProgressableReporter.progress(ProgressableReporter.java:37)
at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:399)
at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:284)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:290)
at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNext(POLocalRearrange.java:256)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:261) at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:256) at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:58) at
org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144) at
org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764) at
org.apache.hadoop.mapred.MapTask.run(MapTask.java:370) at org.apache.hadoop.mapred.Child$4.run(Child.java:261) at
java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:396) at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059) at
org.apache.hadoop.mapred.Child.main(Child.java:255) 


Note that the code in progress function looks as follows:

public void progress() {
        if(rep!=null)
            rep.progress();
    }

This points to some sort of synchronization issue 

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Aug/11 23:04;thejas;PIG-2152.1.patch;https://issues.apache.org/jira/secure/attachment/12491402/PIG-2152.1.patch,08/Jul/11 10:51;vivekp;null_pointer_traces (copy);https://issues.apache.org/jira/secure/attachment/12485721/null_pointer_traces+%28copy%29,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-08 10:51:15.633,,,no_permission,,,,,,,,,,,,35760,,,,Tue Sep 06 18:03:01 UTC 2011,,,,,,,0|i09zq7:,56229,,,,,,,,,,08/Jul/11 10:51;vivekp;Attaching a list of different traces,"08/Jul/11 10:56;vivekp;Even though not reproducible , the exception is happening randomly and quite frequently. 
From most of the failed jobs it looks like this is happening towards end of the task execution. Till now, it is seen only in Map Tasks.
The below is one NullPointer from progess() with a different call heirachy;

at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.ProgressableReporter.progress(ProgressableReporter.java:37)
        at org.apache.pig.data.DefaultAbstractBag.reportProgress(DefaultAbstractBag.java:369)
        at org.apache.pig.data.DefaultDataBag$DefaultDataBagIterator.next(DefaultDataBag.java:165)
        at org.apache.pig.data.DefaultDataBag$DefaultDataBagIterator.hasNext(DefaultDataBag.java:157)
        at org.apache.pig.data.BinInterSedes.writeBag(BinInterSedes.java:522)
        at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:361)
        at org.apache.pig.data.BinInterSedes.writeTuple(BinInterSedes.java:542)
        at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:357)
        at org.apache.pig.data.BinInterSedes.writeTuple(BinInterSedes.java:542)
        at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:357)
        at org.apache.pig.data.BinSedesTuple.write(BinSedesTuple.java:57)
        at org.apache.pig.impl.io.PigNullableWritable.write(PigNullableWritable.java:123)
        at
org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:90)
        at
org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:77)
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1069)
        at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:691)
        at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:124)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:263)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:256)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:58)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
        at org.apache.hadoop.mapred.Child$4.run(Child.java:261)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
        at org.apache.hadoop.mapred.Child.main(Child.java:255)","23/Aug/11 02:27;jwills;Hey, I hit this issue too, using CDH3u1. I'm wondering if you're hitting it when a) you're running over lots of data and b) using combiners. It looks to me that the issue is the setRep(null) call in the cleanup() method in PigCombiner, which is impacting the Map tasks b/c PhysicalOperator only uses a single global PigReporter and the Map tasks and the Combiner tasks run in the same JVM.

Could we reproduce this by messing with the settings to force spills more often than just once?

","23/Aug/11 18:00;olgan;Vivek has discovered the following:

ProgressableReporter object is set to null in the combiner cleanup.

protected void cleanup(Context context) throws IOException, InterruptedException {
            super.cleanup(context);
            leaf = null;
            pack = null;
>            pigReporter.setRep(null);
            pigReporter = null;
            pigContext = null;
            roots = null;
            cp = null;
        }

The same object (pigReporter) is retained and used by all PhysicalOperators
(PhysicalOperator.setReporter(pigReporter);)
This may have caused for the NullPointerException. The above changes were introduced as part of PIG-1815 ",23/Aug/11 22:00;daijy;Or we can make ProgressableReporter.progress synchronized.,"23/Aug/11 22:03;jwills;I think that simply removing the line:

pigReporter.setRep(null);

is the best solution-- I don't see what purpose it was supposed to serve.","23/Aug/11 23:04;thejas;Vivek and Josh, thanks for tracing the issue. I have the change to fix this in PIG-2152.1.patch, but I don't have the setup and query that I can use to verify the fix. It is not easy to test this in a unit test, so it does not have any.
",24/Aug/11 23:09;daijy;+1 for patch. Can anyone give a try?,06/Sep/11 17:47;daijy;I would suggest to commit the patch as is.,"06/Sep/11 18:03;thejas;Patch committed to 0.9 branch and trunk.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support nested tags for XMLLoader,PIG-2147,12511982,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,vivekp,vivekp,vivekp,28/Jun/11 12:38,26/Apr/12 20:33,14/Mar/19 03:07,12/Jul/11 17:19,0.8.1,0.9.0,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"Currently xmlloader does not support nested tags with same tag name, ie if i have the below content

{code}
<event>
 <relatedEvents>
   <event>x<\event>
   <event>y<\event>
   <event>z<\event>
 <\relatedEvents>
<\event>
{code}

And I load the above using XMLLoader,
events = load 'input' using org.apache.pig.piggybank.storage.XMLLoader('event') as (doc:chararray);


The output will be,
{code}
<event>
 <relatedEvents>
   <event>x<\event>
{code}

Whereas the desired output is ;
{code}
 <relatedEvents>
   <event>x<\event>
   <event>y<\event>
   <event>z<\event>
 <\relatedEvents>
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11/Jul/11 06:38;vivekp;PIG-2147_1.patch;https://issues.apache.org/jira/secure/attachment/12486021/PIG-2147_1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-11 06:50:00.318,,,no_permission,,,,,,,,,,,,68252,Reviewed,,,Tue Jul 12 17:19:46 UTC 2011,,,,,,,0|i0h19j:,97467,,,,,,,,,,11/Jul/11 06:38;vivekp;Attaching an initial patch.,"11/Jul/11 06:50;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1064/
-----------------------------------------------------------

Review request for pig.


Summary
-------


Currently xmlloader does not support nested tags with same tag name, ie if i have the below content

<event>
 <relatedEvents>
   <event>x<\event>
   <event>y<\event>
   <event>z<\event>
 <\relatedEvents>
<\event>

And I load the above using XMLLoader,
events = load 'input' using org.apache.pig.piggybank.storage.XMLLoader('event') as (doc:chararray);

The output will be,

<event>
 <relatedEvents>
   <event>x<\event>

Whereas the desired output is ;

<relatedEvents>
   <event>x<\event>
   <event>y<\event>
   <event>z<\event>
 <\relatedEvents>

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Modified the behaviour of XMLLoader such that it considers the nested tags also. This is implemented by simply counting the number of nesting and decrementing accordingly.


This addresses bug PIG-2147.
    https://issues.apache.org/jira/browse/PIG-2147


Diffs
-----


Diff: https://reviews.apache.org/r/1064/diff


Testing
-------


Thanks,

Vivek

",12/Jul/11 03:52;daijy;All test pass. test-patch show positive result. Committed to trunk first.,"12/Jul/11 17:19;daijy;This seems not an urgent issue, I don't think we need to back port to 0.9. Since we already commit the patch to trunk, close the ticket.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
POStore.getSchema() returns null because of which PigOutputCommitter is not storing schema while cleanup,PIG-2146,12511977,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,vivekp,vivekp,28/Jun/11 11:56,04/Aug/11 00:35,14/Mar/19 03:07,20/Jul/11 22:58,0.8.1,0.9.0,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"The below is my script;
{code}
register piggybank.jar;
a = load 'myinput' using PigStorage(',') as (f1:chararray,f2:chararray,f3:chararray);
b = distinct a;
c = limit b 2;
store c into 'pss001' using org.apache.pig.piggybank.storage.PigStorageSchema();
{code}

Input
-------
a,1,aa
b,2,bb
c,3,cc


For this script , PigStorageSchema is not generating  .pig_headers and .pig_schema files. While debugging I could see that storeSchema(..) method itself is not invoked.The schema object for the store is returned as  null (POStore.getSchema()) because of which PigOutputCommitter is not invoking the storSchema.

The same schema object is valid when I run it in local mode. This issue is happening for Pig 0.9 also.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Jul/11 21:57;thejas;PIG-2146.1.patch;https://issues.apache.org/jira/secure/attachment/12487067/PIG-2146.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-19 21:57:19.524,,,no_permission,,,,,,,,,,,,66120,,,,Wed Jul 20 22:56:44 UTC 2011,,,,,,,0|i0h19b:,97466,,,,,,,,,,"19/Jul/11 21:57;thejas;PIG-2146.1.patch - Fixes the issue in MRCompiler LimitAdjuster where a POStore added for new MR operation did not have the schema set.
","19/Jul/11 21:58;thejas;Ran test-commit and test-patch. One of the test-commit tests - TestLoad has failures, which are unrelated to this patch. ",19/Jul/11 23:26;daijy;+1,"20/Jul/11 22:56;thejas;Patch committed to 0.9 branch and trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassCastException when using IsEmpty(DIFF()) ,PIG-2144,12511652,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,miteshsjat,miteshsjat,27/Jun/11 14:23,04/Aug/11 00:34,14/Mar/19 03:07,29/Jun/11 00:32,0.8.0,0.8.1,0.9.0,,,,,0.8.0,0.8.1,0.9.0,,,,,0,,,,,,,,,,,,,"I have following input <name>:<nickname>, for which I want to find records where name is different from nickname.
{code:title=input/name_nickname.txt}
Bharat:Bharat
Amita:Amita
Mitesh:Mitesh
Reenu:Anshu
Shikha:Shikhu
Shilpa:Shilpi
{code}

I have following script to find records where name is different from nickname.
{code:title=isEmpty_diff.pig}

A = LOAD 'input/name_nickname.txt' using PigStorage(':');

B = FILTER A BY NOT IsEmpty(DIFF($0, $1));

DUMP B;
{code}


The above pig script works with older pig versions (e.g. 0.8.0 (r1043805)) and gives following output
{code:title=output of isEmpty_diff.pig}
(Reenu,Anshu)
(Shikha,Shikhu)
(Shilpa,Shilpi)
{code}


However, the above pig script (isEmpty_diff.pig) fails on Pig 0.9 (e.g. 0.9.0.xx (r1127671)) and newer version of Pig 0.8 (e.g. version 0.8.0.xx (r1102885)) , with ClassCastException
{code:title=ClassCastException}
java.lang.ClassCastException: org.apache.pig.data.DefaultDataBag cannot be cast to java.lang.Boolean
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PONot.getNext(PONot.java:75)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:318)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.processInput(POUserFunc.java:159)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:184)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:269)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PONot.getNext(PONot.java:71)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter.getNext(POFilter.java:148)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:261)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:256)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:58)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:676)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:336)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:210)
{code}


As a workaround, I used the following pig script.
{code:titlee=isEmpty_diff2.pig}
A = LOAD 'input/name_nickname.txt' using PigStorage(':');

--B = FILTER A BY NOT IsEmpty(DIFF($0, $1));
B1 = FOREACH A GENERATE $0, $1, DIFF($0, $1);
B2 = FILTER B1 BY NOT IsEmpty($2);
B = FOREACH B2 GENERATE $0, $1;

DUMP B;
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Jun/11 00:31;thejas;PIG-2144.08.1.patch;https://issues.apache.org/jira/secure/attachment/12484583/PIG-2144.08.1.patch,28/Jun/11 16:22;thejas;PIG-2144.1.patch;https://issues.apache.org/jira/secure/attachment/12484472/PIG-2144.1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-06-27 20:55:08.461,,,no_permission,,,,,,,,,,,,66281,,,,Wed Jun 29 00:31:41 UTC 2011,,,,,,,0|i0h18v:,97464,,,,,,,,,,"27/Jun/11 20:55;thejas;The bug is in LogicalExpressionSimplifier optimization rule, it is doing a wrong transformation and adding a NOT operator to output of DIFF.
The logical plan - 
{code}
#-----------------------------------------------
# New Logical Plan:
#-----------------------------------------------
B: (Name: LOStore Schema: null)
|
|---B: (Name: LOFilter Schema: null)
    |   |
    |   (Name: Not Type: boolean Uid: 12)
    |   |
    |   |---(Name: UserFunc(org.apache.pig.builtin.IsEmpty) Type: boolean Uid: 9)
    |       |
    |       |---(Name: Not Type: boolean Uid: 11)
    |           |
    |           |---(Name: UserFunc(org.apache.pig.builtin.DIFF) Type: bag Uid: 8)
    |               |
    |               |---(Name: Project Type: bytearray Uid: 6 Input: 0 Column: 0)
    |               |
    |               |---(Name: Project Type: bytearray Uid: 7 Input: 0 Column: 1)
    |
    |---A: (Name: LOLoad Schema: null)RequiredFields:null

{code}
Note: In the explain command, the logical optimizer is also called twice, which seems to eliminate the Not in the logical plan that is printed. I made local changes to code to find the actual logical plan gets used for generating the physical plan (pasted above). Will make changes to ensure that logical optimizer runs only once in the explain command, as part of this patch.

","27/Jun/11 21:07;thejas;For a filter condition where one filter udf (ie returns boolean) is called from another, and there is a NOT on the outer udf, the query would finish successfully inspite of having an incorrect plan.

{code}
eg.
 B = FILTER A BY NOT IsEmpty(IsEmpty($0));

grunt> explain B;
#-----------------------------------------------
# New Logical Plan:
#-----------------------------------------------
B: (Name: LOStore Schema: null)
|
|---B: (Name: LOFilter Schema: null)
    |   |
    |   (Name: Not Type: boolean Uid: 24)
    |   |
    |   |---(Name: UserFunc(org.apache.pig.builtin.IsEmpty) Type: boolean Uid: 21)
    |       |
    |       |---(Name: Not Type: boolean Uid: 23)
    |           |
    |           |---(Name: UserFunc(org.apache.pig.builtin.IsEmpty) Type: boolean Uid: 20)
    |               |
    |               |---(Name: Project Type: bytearray Uid: 19 Input: 0 Column: 0)
    |
    |---A: (Name: LOLoad Schema: null)RequiredFields:null


{code}",28/Jun/11 17:59;daijy;+1,"29/Jun/11 00:31;thejas;Patch passed unit tests and test-patch. Committed patch to trunk, 0.9. Committed slightly modified patch for 0.8 - PIG-2144.08.1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not bundle apache commons jars with pig-withouthadoop.jar,PIG-2141,12511425,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,24/Jun/11 00:21,05/Oct/11 17:20,14/Mar/19 03:07,07/Sep/11 18:11,0.9.0,site,,,,,,0.9.1,,,,build,,,0,,,,,,,,,,,,,These jars are already available with hadoop installation. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Jun/11 00:23;rding;PIG-2141.patch;https://issues.apache.org/jira/secure/attachment/12483652/PIG-2141.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-07 18:11:04.296,,,no_permission,,,,,,,,,,,,35761,,,,Wed Sep 07 18:11:04 UTC 2011,,,,,,,0|i0h187:,97461,,,,,,,,,,07/Sep/11 18:11;daijy;This is fixed as part of PIG-2239.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Usage printed from Main.java gives wrong option for disabling LogicalExpressionSimplifier,PIG-2140,12511280,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,22/Jun/11 20:06,04/Aug/11 00:35,14/Mar/19 03:07,22/Jun/11 23:35,0.10.0,0.9.0,,,,,,0.10.0,0.9.0,,,,,,0,,,,,,,,,,,,,"The correct command line option for disabling LogicalExpressionSimplifier is ""-t FilterLogicExpressionSimplifier"" , not ""-t LogicalExpressionSimplifier"".
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Jun/11 22:27;thejas;PIG-2140.1.patch;https://issues.apache.org/jira/secure/attachment/12483517/PIG-2140.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-22 22:29:41.602,,,no_permission,,,,,,,,,,,,66126,,,,Wed Jun 22 23:35:21 UTC 2011,,,,,,,0|i0h17z:,97460,,,,,,,,,,"22/Jun/11 20:09;thejas;
java -Xmx500m  -cp pig.jar org.apache.pig.Main -x local  -h
gives -
{code}
...
...
-t, -optimizer_off - Turn optimizations off. The following values are supported:
            ...
            LogicalExpressionSimplifier - Combine multiple expressions
...
{code}",22/Jun/11 22:29;daijy;+1,22/Jun/11 23:35;thejas;patch committed to trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogicalExpressionSimplifier optimizer rule should check if udf is deterministic while checking if they are equal,PIG-2139,12511279,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,22/Jun/11 19:58,04/Aug/11 00:34,14/Mar/19 03:07,25/Jun/11 00:07,0.10.0,0.8.0,0.8.1,0.9.0,,,,0.10.0,0.9.0,,,,,,0,,,,,,,,,,,,,"LogicalExpressionSimplifier simplifies filter expressions. In the process, it compares udfs to see if they are 'equal' (ie expected to produce same results). But it does not check if the udfs are annotated as @Nondeterministic. If such an annotation exists, then the udfs should not be considered equal. UserFuncition.isEqual() is being used to compare the udfs.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Jun/11 20:24;thejas;PIG-2139.1.patch;https://issues.apache.org/jira/secure/attachment/12483765/PIG-2139.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-24 01:49:27.001,,,no_permission,,,,,,,,,,,,66308,,,,Sat Jun 25 00:07:14 UTC 2011,,,,,,,0|i0h17r:,97459,,,,,,,,,,24/Jun/11 01:49;dvryaboy;Thejas can you add an example of what this breaks to the ticket? Does the rule only affect 0.10 or is this also in 8 and 9?,"24/Jun/11 20:17;thejas;bq. Thejas can you add an example of what this breaks to the ticket? 
If ND_FUNC is a non deterministic EvalFunc, then the following optimization that would be done by LogicalExpressionSimplifier is not correct -
{code}
f = filter l by ND_FUNC(a) > 0.1 and ND_FUNC(a) > 0.1;
=>
f = filter l by ND_FUNC(a) > 0.1 ;
{code}

bq. Does the rule only affect 0.10 or is this also in 8 and 9?
It affects 8 and 9 as well. I will apply the fix to 0.9 as well.",24/Jun/11 20:24;thejas;unit tests and test-patch passed with PIG-2139.1.patch. It also fixes a NPE issue seen in LogicalExpressionSimplifier when optimization runs on udfs with no arguments.,"24/Jun/11 21:03;daijy;+1. Note this changes the semantic of LogicalPlan.isEqual. Two logical plan will never consider equal if they contain non-deterministic UDF. Since we don't yet have a use case to compare two logical plan, there is no side effect for now.","25/Jun/11 00:07;thejas;Patch committed to 0.9 branch and trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SAMPLE should not be pushed above DISTINCT,PIG-2137,12511192,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,dvryaboy,dvryaboy,dvryaboy,22/Jun/11 03:33,04/Aug/11 00:35,14/Mar/19 03:07,24/Jun/11 02:39,0.10.0,0.8.0,0.8.1,0.9.0,,,,0.10.0,0.9.0,,,,,,0,,,,,,,,,,,,,"I have an input file that contains 50,000 distinct integers. Each integer is repeated twice, for a total of 100,000 lines.

Script 1, using GROUP BY to get distinct entries in the data, works:
{code}

grunt> f = load 'tmp/dupnumbers.txt';              
grunt> d = foreach (group f by $0) generate group; 
grunt> s = sample d 0.01;                          
grunt> n = foreach (group s all) generate COUNT(s);
grunt> dump n;
(493)
{code}

Script 2, using DISTINCT for the same purpose, allows sampling to be done before DISTINCT:

{code}
grunt> f = load 'tmp/dupnumbers.txt';              
grunt> d = distinct f;
grunt> s = sample d 0.01;                          
grunt> n = foreach (group s all) generate COUNT(s);
(980)
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Jun/11 00:58;thejas;PIG-2137.1.patch;https://issues.apache.org/jira/secure/attachment/12483534/PIG-2137.1.patch,23/Jun/11 01:19;thejas;PIG-2137.2.patch;https://issues.apache.org/jira/secure/attachment/12483536/PIG-2137.2.patch,22/Jun/11 04:31;dvryaboy;PIG-2137.patch;https://issues.apache.org/jira/secure/attachment/12483402/PIG-2137.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-06-23 00:02:36.762,,,no_permission,,,,,,,,,,,,41693,,,,Fri Jun 24 02:39:20 UTC 2011,,,,,,,0|i0h17b:,97457,,,,,,,,,,22/Jun/11 03:42;dvryaboy;Turning off PushUpFilter fixes the issue. It seems like the fix to PIG-2014 was incomplete.,22/Jun/11 04:31;dvryaboy;Easy fix. Attached. Please review.,"23/Jun/11 00:02;thejas;It helps to push filter before distinct operation (ie discard the rows early), and the results will be correct if the udf is deterministic .
I think the filter pushup should be disabled only for non deterministic udfs.
","23/Jun/11 00:43;dvryaboy;Thejas, I believe my fix still allows that -- it just doesn't early-terminate the optimizer when encountered distinct, but proceeds to check if the other conditions required for a successful filter push (such as the udf being deterministic) apply.","23/Jun/11 00:49;thejas;Sorry, my mistake, I didn't go through the function properly. Changes look good . +1
","23/Jun/11 01:00;thejas;(Sorry again!) Actually, it does disable the optimization for all filters with distinct as predecessor. Patch PIG-2137.1.patch has fix.","23/Jun/11 01:03;dvryaboy;Thanks
I'll commit PIG-2137.1 to 0.8, 0.9, and trunk.","23/Jun/11 01:19;thejas;PIG-2137.2.patch has a test case for the case when filter should be pushed above distinct. 
I haven't run unit tests and test-patch with my changes to the patch. I will start them today and it will take couple of hours. If you are able to run them they finish before i get back on results, please feel free to commit.",23/Jun/11 01:24;dvryaboy;I'll wait for the test-patch results.,"23/Jun/11 22:51;thejas;Dmitriy,
Unit tests and test-patch have passed. You can commit the patch.
But this patch can't be committed to 0.8, as the Nondeterministic annotation was added only in 0.9.
","24/Jun/11 00:38;dvryaboy;For 0.8 I was going to backport PIG-2014 before this one.. we are running both in production right now (on top of 8.1), they are fine.
Although I did have trouble backporting the tests, a bunch of the optimizer interfaces seem to have changed. I don't think 8 is as important, since it doesn't seem likely we'll release 8.2 what with 0.9.0 being almost out the door.",24/Jun/11 02:39;dvryaboy;Committed to 0.9 and 0.10 (trunk),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Piggybank: MIN and MAX functions should ignore nulls,PIG-2132,12510696,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,rekhajoshm,jcoveney,jcoveney,17/Jun/11 17:07,07/Jul/14 18:08,14/Mar/19 03:07,27/Nov/13 21:21,0.10.1,,,,,,,0.13.0,,,,tools,,,0,newbie,piggybank,,,,,,,,,,,"org.apache.pig.piggybank.evaluation.math.MAX and org.apache.pig.piggybank.evaluation.math.MIN throws a NullPointerException on a null input, when it should just ignore it.",,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,,,,,17/Jun/11 17:51;jcoveney;JcoMaxMinPatch.patch;https://issues.apache.org/jira/secure/attachment/12482962/JcoMaxMinPatch.patch,18/Jun/11 15:49;jcoveney;JcoMaxMinPatch_wtests.patch;https://issues.apache.org/jira/secure/attachment/12483046/JcoMaxMinPatch_wtests.patch,25/Nov/13 09:16;rekhajoshm;PIG-2132_1.patch;https://issues.apache.org/jira/secure/attachment/12615566/PIG-2132_1.patch,27/Nov/13 19:47;cheolsoo;PIG-2132_2.patch;https://issues.apache.org/jira/secure/attachment/12616104/PIG-2132_2.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2011-06-17 20:02:28.429,,,no_permission,,,,,,,,,,,,68116,,,,Wed Nov 27 21:21:47 UTC 2013,,,,,,,0|i0h167:,97452,"All I did was go through all of the Max and Min functions in Piggybank and add
if (first==null)
    return second;
if (second==null)
    return first;

Following conversation on the message board, the desired outputs are:
null,null -> null
1,null -> 1
null,1 -> 1
1,1 -> 1

This change gives that.",,,,,,piggybank,,,"17/Jun/11 17:50;jcoveney;This is my first patch, so I hope that I conformed to procedure properly. If I should do anything differently, please let me know.",17/Jun/11 20:02;dvryaboy;Please augment the existing tests to check for this.,"17/Jun/11 21:52;jcoveney;Dmitriy, I will definitely do that. A question on style: is it better to contain all tests of related functions to one function (in this case, textMAX() and testMIN()), or should I break it out?",18/Jun/11 15:49;jcoveney;I tried to include fairly extensive tests of all of the max and min functions.,"20/Jun/11 18:06;alangates;I know I said in the mail thread that these functions should ignore null.  But I missed the context and thought we were talking about the aggregate functions MIN and MAX.  Sorry for the misunderstanding.

In the case of these we need to think about the semantics we want.  It seems to me that MIN(x, null) should return null rather than x, because you don't know the answer, which is what null means.",13/Jul/11 23:22;thejas;Changing the status from patch available until Alan's comment is addressed. (patch-available state is being used for finding jira's that are ready for review. ),25/Nov/13 09:15;rekhajoshm;Submitted patch with review comment from Alan.,25/Nov/13 09:16;rekhajoshm;Attached patch,"27/Nov/13 19:44;cheolsoo;[~rekhajoshm], thank you for the patch. But your patch includes several errors. In fact, it doesn't even compile-
{code}
    [javac] /Users/cheolsoop/workspace/pig-svn/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/evaluation/TestMathUDF.java:528: incompatible types
    [javac] found   : <nulltype>
    [javac] required: double
    [javac]         double expected = null;
    [javac]                           ^
    [javac] /Users/cheolsoop/workspace/pig-svn/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/evaluation/TestMathUDF.java:609: incompatible types
    [javac] found   : <nulltype>
    [javac] required: float
    [javac]         float expected = null;
    [javac]                          ^
{code}
Also, it looks like you manually edited your patch.
{code}
+	-	 public void testnextAfter() throws Exception {
{code}

I am dropping the changes in TestMathUDF.java and will only commit the rest.

Next time when you contribute, please run the unit test before posting a patch. Here is how you run piggybank unit tests-
{code}
ant compile-test
cd contrib/piggybank/java
ant test -Dtestcase=TestMathUDF
{code}",27/Nov/13 19:47;cheolsoo;Here is the patch after clean up. I will commit this.,27/Nov/13 21:21;cheolsoo;Committed to trunk. Thank you Rekha!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Piggybank:MultiStorage is not compressing output files,PIG-2130,12510645,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,vivekp,vivekp,vivekp,17/Jun/11 10:14,26/Apr/12 20:32,14/Mar/19 03:07,12/Jul/11 17:20,0.8.0,0.9.0,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"MultiStorage is not compressing the records while writing the output. Even though it takes a compression param,  when the record is written it ignores the compression.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Jun/11 07:53;vivekp;PIG-2130_1.patch;https://issues.apache.org/jira/secure/attachment/12483120/PIG-2130_1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-11 06:16:00.463,,,no_permission,,,,,,,,,,,,68251,Reviewed,,,Tue Jul 12 17:20:09 UTC 2011,,,,,,,0|i0h15r:,97450,,,,,,,,,,20/Jun/11 07:53;vivekp;Attaching an initial patch,"20/Jun/11 08:02;vivekp;Please note that , if compression is used, then the subfolders and output files will be having the corresponding extension.
For example, if output001.bz2 is output path and f1,f2 are the keys, the files will look like;

/tmp/output001.bz2 
   /tmp/output001.bz2/f1.bz2
      /tmp/output001.bz2/f1.bz2/f1-0.bz2

   /tmp/output001.bz2/f2.bz2
      /tmp/output001.bz2/f2.bz2/f2-0.bz2
","11/Jul/11 06:16;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1063/
-----------------------------------------------------------

Review request for pig.


Summary
-------

MultiStorage is not compressing the records while writing the output. Even though it takes a compression param, when the record is written it ignores the compression.

As a fix enabled compressing the output.

If compression is used, then the sub diretories and output files will be having the corresponding extension.
For example, if output001.bz2 is output path and f1,f2 are the keys, the files will look like;

/tmp/output001.bz2
/tmp/output001.bz2/f1.bz2
/tmp/output001.bz2/f1.bz2/f1-0.bz2

/tmp/output001.bz2/f2.bz2
/tmp/output001.bz2/f2.bz2/f2-0.bz2


This addresses bug PIG-2130.
    https://issues.apache.org/jira/browse/PIG-2130


Diffs
-----


Diff: https://reviews.apache.org/r/1063/diff


Testing
-------


Thanks,

Vivek

",12/Jul/11 04:10;daijy;All tests pass. test-patch all pass. Commit to trunk first.,"12/Jul/11 17:20;daijy;This seems not an urgent issue, I don't think we need to back port to 0.9. Since we already commit the patch to trunk, close the ticket.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NOTICE file needs updates,PIG-2129,12510503,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,alangates,alangates,alangates,15/Jun/11 22:29,04/Aug/11 00:34,14/Mar/19 03:07,15/Jun/11 23:59,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,The NOTICE file has not been updated in a bit and is missing a number of notices about jars we include in our distribution.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Jun/11 22:30;alangates;PIG-2129.patch;https://issues.apache.org/jira/secure/attachment/12482727/PIG-2129.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-15 23:59:01.21,,,no_permission,,,,,,,,,,,,66201,,,,Mon Jun 20 18:34:02 UTC 2011,,,,,,,0|i0h15j:,97449,,,,,,,,,,"15/Jun/11 23:59;olgan;Patch committed. Thanks, Alan!",20/Jun/11 18:34;alangates;Patch applied to trunk as well (original checkin was on 0.9 branch).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigStorageSchema need to deal with missing field,PIG-2127,12510483,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,daijy,daijy,15/Jun/11 18:40,26/Nov/12 02:58,14/Mar/19 03:07,04/Oct/11 22:30,0.10.0,,,,,,,0.10.0,,,,impl,,,0,,,,,,,,,,,,,"Currently, if data contains fewer columns than the schema, PigStorageSchema will throw IndexOutOfBound exception (PigStorageSchema:97). We should padding null in this case as we did in PigStorage.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-10-04 22:30:22.97,,,no_permission,,,,,,,,,,,,46020,,,,Mon Nov 26 02:58:49 UTC 2012,,,,,,,0|i08pyv:,48806,,,,,,,,,,04/Oct/11 22:30;olgan;Looks like PigStorageSchema has been converted to just use PigStorage implementation so it has exactly the same semantics,"06/Jun/12 16:10;vivekp;I think this issue is still present with PigStorage -schema option,


{code}
a = load '2127_withschema' using PigStorage(',','-schema');
b = foreach a generate f1,f2,f3,f4;
dump b;
{code}

input
{code}
d,e,4,1
a,b,1,2
c,b
d,e,4,1
{code}

The above given script and input produces the below exception;
java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.pig.data.DefaultTuple.get(DefaultTuple.java:156)
	at org.apache.pig.builtin.PigStorage.applySchema(PigStorage.java:282)
	at org.apache.pig.builtin.PigStorage.getNext(PigStorage.java:246)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:194)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:532)
	at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
	","06/Jun/12 16:22;vivekp;I am seeing the same issue with PigStorage also for Pig 0.10;
Input;
d,e,4,1
a,b,1,2
c,b
d,e,4,1

Script
a = load '2127_withschema' using PigStorage(',') as (f1,f2,f3,f4);
b = foreach a generate f1,f2,f3,f4;
dump b;

The above script also results in the same IndexOutOfBound exception in Pig 0.10. (works fine with Pig 0.9)

","06/Jun/12 20:31;thejas;Vivek,
Please open a new jira linked to this one.
There does not seem to be an option of reopening this one.","06/Jun/12 22:42;prkommireddi;Is this happening with trunk? I can't reproduce this issue

{code}
cat data
1	3	5
4	123	b
5	12	
10

A = LOAD 'data' as (a:int, b:chararray, c:chararray);
Store A INTO 'out' using PigStorage(',', '-schema');

B = load 'out' using PigStorage(',', '-schema');
describe B;
B: {a: int,b: chararray,c: chararray}

dump B;
(1,3,5)
(4,123,b)
(5,12,)
(10,,)
{code}
dump B;","11/Oct/12 21:29;aperepel;Hi, I've come across this regression in 0.10.0. However, after building PIG from the branch-0.10 (upcoming 0.10.1 as of Oct 11 2012), the issue is still there, nothing was fixed. This is a major problem, as it breaks the very basic promise of Pig being resilient to an evolving schema, please don't let it slip into 0.10.1 and fix it.","11/Oct/12 21:35;aperepel;The reason Prashant couldn't reproduce the issue is that it doesn't happen with the schema declared inline. However, if schema has been loaded from the .pig_schema file (e.g. leave PigStorage defaults, it will load it if available), things break.","19/Nov/12 18:27;knoguchi;bq. Vivek,
bq. Please open a new jira linked to this one.
bq. There does not seem to be an option of reopening this one.

Cloned PIG-3056.
","26/Nov/12 02:58;investinme048;Problem has been resolved 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Script never ending when joining from the same source,PIG-2124,12510437,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,t.croiset,t.croiset,15/Jun/11 13:05,26/Apr/12 20:32,14/Mar/19 03:07,05/Aug/11 16:58,0.8.1,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"Considering the following script, it works perfectly fine or the script never ends depending on the fields used at output.

input (""scores"" file) contains:
------------------
test1;0.1
test2;0.9
test1;0.3
------------------

------------------------------------------------------------------------------
score_list = LOAD  'scores' USING PigStorage(';')
  AS (word: chararray, score: double);

score_list_ = FOREACH score_list GENERATE
  word,
  score,
  0 AS joinField;

group_score = GROUP score_list ALL;
sum_score = FOREACH group_score GENERATE
  0 AS joinField,
  SUM(score_list.score) as scoreTotal;

score_with_sum = JOIN score_list_ BY joinField, sum_score BY joinField;
out = FOREACH score_with_sum GENERATE word, (score / scoreTotal);
DUMP out;
------------------------------------------------------------------------------

This works fine

But if I change ""out"" to : out = FOREACH score_with_sum GENERATE word;

Then the script never ends and the output keeps repeating lines likes:
2011-06-15 15:00:22,536 [SpillThread] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 24
2011-06-15 15:00:22,889 [Thread-13] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output: record full = true
2011-06-15 15:00:22,889 [Thread-13] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 65535810; bufend = 68157240; bufvoid = 99614720
2011-06-15 15:00:22,889 [Thread-13] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 327661; kvend = 262124; length = 327680
2011-06-15 15:00:22,994 [SpillThread] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 25
2011-06-15 15:00:23,345 [Thread-13] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output: record full = true
2011-06-15 15:00:23,345 [Thread-13] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 68157240; bufend = 70778670; bufvoid = 99614720
2011-06-15 15:00:23,345 [Thread-13] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 262124; kvend = 196587; length = 327680
2011-06-15 15:00:23,447 [SpillThread] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 26
2011-06-15 15:00:23,794 [Thread-13] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output: record full = true
2011-06-15 15:00:23,794 [Thread-13] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 70778670; bufend = 73400100; bufvoid = 99614720
2011-06-15 15:00:23,794 [Thread-13] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 196587; kvend = 131050; length = 327680
2011-06-15 15:00:23,896 [SpillThread] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 27
2011-06-15 15:00:24,243 [Thread-13] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output: record full = true
2011-06-15 15:00:24,243 [Thread-13] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 73400100; bufend = 76021530; bufvoid = 99614720
2011-06-15 15:00:24,243 [Thread-13] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 131050; kvend = 65513; length = 327680
2011-06-15 15:00:24,346 [SpillThread] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 28
2011-06-15 15:00:24,692 [Thread-13] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output: record full = true
2011-06-15 15:00:24,692 [Thread-13] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 76021530; bufend = 78642970; bufvoid = 99614720
2011-06-15 15:00:24,693 [Thread-13] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 65513; kvend = 327657; length = 327680
2011-06-15 15:00:24,793 [SpillThread] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 29
2011-06-15 15:00:25,144 [Thread-13] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output: record full = true
2011-06-15 15:00:25,144 [Thread-13] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 78642970; bufend = 81264400; bufvoid = 99614720
2011-06-15 15:00:25,144 [Thread-13] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 327657; kvend = 262120; length = 327680


P.S. I know it's possible to refactor the script using casting to scalar ;)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Jun/11 22:48;daijy;PIG-2124-1.patch;https://issues.apache.org/jira/secure/attachment/12482872/PIG-2124-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-15 21:36:11.689,,,no_permission,,,,,,,,,,,,65993,Reviewed,,,Fri Aug 05 16:58:33 UTC 2011,,,,,,,0|i0h14v:,97446,,,,,,,,,,"15/Jun/11 21:36;daijy;This is related to ColumnMapKeyPrune optimization. If we disable this rule using ""-t ColumnMapKeyPrune"", the error goes away.","16/Jun/11 22:48;daijy;This happens when group by a constant (all), and then we prune all columns except group. This results a CoGroup with no input. Should be a rare case, only fix in trunk.",15/Jul/11 21:35;alangates;Marking as ready for review.,26/Jul/11 19:42;thejas;+1,"05/Aug/11 16:58;daijy;Unit test pass. Test-patch pass:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.


Committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parameter Substitution doesn't work in the Grunt shell,PIG-2122,12510249,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,sandeep.samudrala,gsingers,gsingers,14/Jun/11 00:25,17/Jun/15 18:03,14/Mar/19 03:07,26/Jun/14 04:22,0.12.0,0.8.0,0.8.1,,,,,0.14.0,,,,grunt,,,10,,,,,,,,,,,,,"Simple param substitution and things like %declare (as copied out of the docs) don't work in the grunt shell.

#Start Pig with: Start Pig with: bin/pig -x local -p time=FOO
{quote}
foo = LOAD '/user/grant/foo.txt' AS (a:chararray, b:chararray, c:chararray);
Y = foreach foo generate *, '$time';
dump Y;
{quote}
Output:
{quote}
2011-06-13 20:22:24,197 [main] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1
(1 2 3,,,$time)
(4 5 6,,,$time)
{quote}

Same script, stored in junk.pig, run as: bin/pig -x local -p time=FOO junk.pig
{quote}
2011-06-13 20:23:38,864 [main] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1
(1 2 3,,,FOO)
(4 5 6,,,FOO)
{quote}

Also, things like don't work (nor does %declare):
{quote}
grunt> %default DATE '20090101';    
2011-06-13 20:18:19,943 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Encountered "" <PATH> ""%default """" at line 1, column 1.
Was expecting one of:
    <EOF> 
    ""cat"" ...
    ""fs"" ...
    ""sh"" ...
    ""cd"" ...
    ""cp"" ...
    ""copyFromLocal"" ...
    ""copyToLocal"" ...
    ""dump"" ...
    ""describe"" ...
    ""aliases"" ...
    ""explain"" ...
    ""help"" ...
    ""kill"" ...
    ""ls"" ...
    ""mv"" ...
    ""mkdir"" ...
    ""pwd"" ...
    ""quit"" ...
    ""register"" ...
    ""rm"" ...
    ""rmf"" ...
    ""set"" ...
    ""illustrate"" ...
    ""run"" ...
    ""exec"" ...
    ""scriptDone"" ...
    """" ...
    <EOL> ...
    "";"" ...
    
Details at logfile: /Users/grant.ingersoll/projects/apache/pig/release-0.8.1/pig_1308002917912.log
{quote}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Jun/14 21:57;daijy;PIG-2122-1.patch;https://issues.apache.org/jira/secure/attachment/12651891/PIG-2122-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-15 15:53:39.392,,,no_permission,,,,,,,,,,,,35762,Reviewed,,,Thu Jun 26 04:22:24 UTC 2014,,,,,,,0|i02ipb:,12631,,,,,,,,,,"15/Jun/11 15:53;olgan;This is by design. We did not see any need for parameter substitution in the interactive mode. The current implementation is based on a preprocessor applied to the entire script.

We would need a compelling case to reconsider.","15/Jun/11 16:30;gsingers;Seems to me one of the major use cases of the Grunt shell is to test your scripts line by line.  If you don't have parameter substitution, then you have to futz around with hand editing out the substitutions to run it in Grunt, and then add it back in before you submit it to run in file mode.  

The other use case is, the documentation doesn't work!  I would bet that many people simply try out what is on the website as examples, so when things like declare, etc. as copy and pasted from the website don't work, they lose confidence in the tool.","15/Jun/11 17:37;dvryaboy;This really does diminish usefulness of Grunt significantly.
There isn't an easy way to apply the existing preprocessor here, which is a shame, but it's an issue that comes up often. It would be great if someone took a crack at fixing this.. theoretically having a in-memory variable map and doing line-by-line substitution before handing off to the parser should work, right?",15/Jun/11 17:50;olgan;I think the trick would be to maintain backward compatibility.,"15/Jun/11 17:54;gsingers;I know it would be nice, but is back compat really a requirement for a pre 1.0 release?","15/Jun/11 18:03;olgan;Backward compatibility is a requirement because we have many customers and many of them uset this feature especially in production environment. 

In my opinion design/implementation that does not provide backward comptaibility for batch processing is a non-starter.
","09/Sep/11 16:08;jwartes;I found this JIRA issue because: 
1) I was copying a script into the shell for testing and was confused why it was wasn't working.
2) Reading the documentation and attempting alternative methods to set parameters didn't work either.

I mention this because I hit *both* of cases previously presented as reasons to fix this issue. The second, because of the first.

If this issue cannot be fixed, I'd suggest the grunt shell throw a more helpful error when it encounters preprocessor directives, and warn when it sees likely attempts at dollar-sign interpolation.","23/Apr/12 13:53;jalkanen;Upvoting; this makes debugging Pig scripts *really* hard using Grunt. I mostly run a part of the script simply by cut-n-pasting from the editor into a Grunt shell, then stop to examine the intermediate values. Having to do manual search-replace for variables makes them useless.","23/Apr/12 18:01;azaroth;Hi,
I can agree that the feature is useful.

However to debug a Pig script you can use the -debug and -dryrun options:

{code}
–dryrun
Flag. With this option, the script is not run and a fully 
substituted Pig script produced in the current working 
directory named original_script_name.substituted
{code}

This should help you avoid manual search/replace.

Cheers,","23/Apr/12 18:04;azaroth;On the backward compatibility issue, I must say that I think modifying Grunt and interactive mode is not as bad as modifying the batch mode.
I don't feel changing Grunt should be a big issue as most of the production job that I know are batch scripts which would be unaffected.",27/Sep/12 00:35;jhartman;This irritates me on a weekly basis. I'm often running ad-hoc analysis in grunt with snippets taken from some heavier scripts I've written and I get bitten ALL the time.,"01/Oct/12 03:27;aniket486;There is a workaround to this that you can use-

* Store your script line in a file (with $params included).
* Start grunt interactively
* type- run -param a=b -param c=d myscript.pig 

This should do the trick.","05/Oct/12 19:03;crashedsnow;[~aniket486]

Your workaround is not really much help if you're trying to debug a script (i.e. line by line, DUMPing or ILLUSTRATing as you go.  We want to be able to use the interactive shell with the existing script (including params) but execute it line by line.","10/Jun/14 05:50;phapalegaurav;It would be great if this issue is fixed.
I'm not able to understand how adding parameter substitution will affect back comp.","25/Jun/14 17:11;alangates;+1 for the patch.

[~olgan], I don't see the backwards compatibility issue.  By definition this is for interactive sessions, so users can't have existing scripts that change behavior.  I suppose someone somewhere might regularly use $x in his interactive session and expect it to come out as $x rather than complain that it can't make the substitution, but that seems 1) unlikely, and 2) easy to fix.",26/Jun/14 04:22;daijy;Patch committed to trunk. Thanks Alan for review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e2e test harness should use ant instead of make,PIG-2121,12510241,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,alangates,alangates,alangates,13/Jun/11 22:28,26/Apr/12 20:33,14/Mar/19 03:07,09/Jul/11 23:59,0.10.0,,,,,,,0.10.0,,,,tools,,,0,,,,,,,,,,,,,"It uses make at the moment, but since everything else uses ant, so should it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Jun/11 15:29;alangates;PIG-2121.patch;https://issues.apache.org/jira/secure/attachment/12483720/PIG-2121.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-08 21:53:50.996,,,no_permission,,,,,,,,,,,,68646,,,,Sat Jul 09 23:59:07 UTC 2011,,,,,,,0|i0h14f:,97444,,,,,,,,,,08/Jul/11 21:53;daijy;+1,09/Jul/11 23:59;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDFContext.getClientSystemProps() does not respect pig.properties,PIG-2120,12510107,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,dvryaboy,dvryaboy,13/Jun/11 16:07,23/Jan/12 07:31,14/Mar/19 03:07,20/Jun/11 20:28,0.10.0,0.8.0,0.8.1,0.9.0,,,,0.10.0,0.9.2,,,,,,0,,,,,,,,,,,,,HBaseStorage is supposed to allow changing the default Caster via the pig.hbase.caster property. This does not work.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Jun/11 22:52;dvryaboy;PIG-2120.2.patch;https://issues.apache.org/jira/secure/attachment/12483101/PIG-2120.2.patch,13/Jun/11 16:55;dvryaboy;PIG-2120.patch;https://issues.apache.org/jira/secure/attachment/12482352/PIG-2120.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-06-13 18:13:24.954,,,no_permission,,,,,,,,,,,,41694,,,,Mon Jan 09 22:06:19 UTC 2012,,,,,,,0|i0h147:,97443,,,,,,,,,,"13/Jun/11 16:55;dvryaboy;While fixing this, I discovered a bigger problem that's not HBaseStorage specific -- UDFContext.getUDFContext().getClientSystemProps() does NOT respect properties set in pig.properties.

The attached patch fixes both issues.

Tested manually, not sure how to write a proper unit test for this..","13/Jun/11 18:13;alangates;The UDFContext.setClientSystemProps() should just be changed to setClientSystemProps(Properties), rather than a second method added.  If it's left in there and called with no Properties it would override the properties from the file, causing confusion for the users.  This is not a public part of the API (specifically called out in the comments), so we are free to change it.

You could write an automated test to make sure that properties passed in pig.properties and on the command line still make it to the backend via UDFContext.  This is an important thing to get right.","14/Jun/11 18:01;dvryaboy;I'd have to shell out or something for that test -- the change is in Main.java...

Which probably points out yet another bug. Is it just me or do we need to set UDFContext clientSystemProps in PigServer? Something like 

{code}

Index: src/org/apache/pig/PigServer.java
===================================================================
--- src/org/apache/pig/PigServer.java	(revision 1135694)
+++ src/org/apache/pig/PigServer.java	(working copy)
@@ -1632,6 +1632,8 @@
          */
         private void parseQuery() throws FrontendException {
             UDFContext.getUDFContext().reset();
+            UDFContext.getUDFContext().setClientSystemProps(pigContext.getProperties());
+
             String query = buildQuery();
 
             if( query.isEmpty() ) {
{code}","19/Jun/11 22:52;dvryaboy;Patch attached.
Switched the UDFContext tests to local mode while I was in there.",20/Jun/11 18:14;alangates;+1,20/Jun/11 20:24;dvryaboy;Changing bug title to more accurately reflect the nature of the change,"20/Jun/11 20:28;dvryaboy;Fixed in trunk.
Should I apply this to 0.9?","09/Jan/12 21:41;dvryaboy;Answer to self: yes, you should.",09/Jan/12 22:06;dvryaboy;Applied to 0.9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DuplicateForEachColumnRewrite makes assumptions about the position of LOGGenerate in the plan,PIG-2119,12510051,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,azaroth,azaroth,12/Jun/11 12:17,23/Jan/12 07:31,14/Mar/19 03:07,02/Nov/11 06:19,0.9.0,0.9.1,,,,,,0.10.0,0.11,0.9.2,,,,,0,,,,,,,,,,,,,"The input:
{code}
grunt> cat b.txt
a       11
b       3
c       10
a       12
b       10
c       15
{code}

The script:
{code}
a = load 'b.txt' AS (id:chararray, num:int);
b = group a by id;
c = foreach b { 
  d = order a by num DESC;
  n = COUNT(a);
  e = limit d 1;
  generate n;
}
{code}

The exception:
{code}
Caused by: java.lang.ClassCastException: org.apache.pig.newplan.logical.relational.LOLimit cannot be cast to org.apache.pig.newplan.logical.relational.LOGenerate
        at org.apache.pig.newplan.logical.rules.DuplicateForEachColumnRewrite$DuplicateForEachColumnRewriteTransformer.check(DuplicateForEachColumnRewrite.java:87)
        at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:108)

{code}

I know the script is a bit pointless, but I was just testing and modifying the script bit by bit.
If I remove the limit in any case I get the same exception but with LOSort.

The problem, I think, is that the rule assumes there is only 1 sink in the nested block and that this sink is a LOGenerate.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Oct/11 17:29;daijy;PIG-2119-1.patch;https://issues.apache.org/jira/secure/attachment/12501124/PIG-2119-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-13 20:37:29.604,,,no_permission,,,,,,,,,,,,41033,Reviewed,,,Sat Jan 14 00:10:48 UTC 2012,,,,,,,0|i09zfz:,56183,,,,,,,,,,"13/Jun/11 20:37;daijy;LOGenerate is the only sink in nested plan. This is one of the basic assumption we made throughout logical plan. To solve the issue above, we need to prune the dangling branch before proceed.","30/Sep/11 12:28;vivekp;Faced this issue with the below script;
{code}
A = load '3char_1long_tab' as (f1:chararray, f2:chararray, f3:chararray,ct:long);
B = GROUP A  BY f1;
C =    FOREACH B {
        zip_ordered = ORDER A BY f3 ASC; 
        GENERATE
                FLATTEN(group) AS f1,	
                A.(f3, ct),
		--COUNT(zip_ordered),
                SUM(A.ct) AS total;
  };

dump C;
{code}

The zip_ordered is an accident and not used, but Pig 0.8 silently ignores this while Pig 0.9 throws exception.
I believe the affect version should be 0.9
",01/Nov/11 06:59;thejas;+1,"02/Nov/11 06:19;daijy;Unit tests pass. Test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 458 release audit warnings (more than the trunk's current 447 warnings).

All new files has proper header.

Patch committed to both trunk and 0.10 branch.",14/Jan/12 00:10;daijy;Patch committed to 0.9 branch as per Dmitriy's request (PIG-2474),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig HBaseStorage configuration and setup issues,PIG-2115,12509697,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,gbowyer@fastmail.co.uk,gbowyer@fastmail.co.uk,gbowyer@fastmail.co.uk,09/Jun/11 10:53,27/Sep/12 21:26,14/Mar/19 03:07,05/Sep/11 00:15,0.9.0,,,,,,,0.10.0,,,,,,,0,hbase,load,storage,,,,,,,,,,"HBase storage currently configures child MapR jobs assuming that the HBase jars and cluster configuration are present on the hadoop cluster that will run the pig program. In some circumstances, and for some configurations this causes problems with both the configuration suddenly becoming defaulted during a job run as well as ClassNotFound exceptions occurring due to the child MapReduce job being incorrectly configured.

As such, the hbase built in configuration is merged with the job configuration using the inbuilt pig classes for this functionality, this prevents the jobs configuration from being overwritten by defaults.

At the same time this change has been mirrored in the relevant setup methods called when the HBaseStorage is used as a StoreFunc; this means that the StoreFunc also correctly sets up the relevant classpath preventing ClassNotFound exceptions in clusters that are not adding the HBase classes to the HADOOP_CLASSPATH.  

Attached is a patch that hopefully fixes these issues for others.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Jun/11 10:55;gbowyer@fastmail.co.uk;HBaseStorage-correctly-configure-child-MapR-task-v1.patch;https://issues.apache.org/jira/secure/attachment/12481910/HBaseStorage-correctly-configure-child-MapR-task-v1.patch,27/Jul/11 18:40;gbowyer@fastmail.co.uk;HBaseStorage-correctly-configure-child-MapR-task-v2.patch;https://issues.apache.org/jira/secure/attachment/12488006/HBaseStorage-correctly-configure-child-MapR-task-v2.patch,31/Aug/11 22:28;dvryaboy;PIG-2115.3.patch;https://issues.apache.org/jira/secure/attachment/12492539/PIG-2115.3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-06-09 17:43:08.07,,,no_permission,,,,,,,,,,,,35763,,,,Thu Sep 27 21:26:00 UTC 2012,,,Patch Available,,,,0|i02if3:,12585,,,,,,,,,,"09/Jun/11 17:43;dvryaboy;Great, thanks for doing that. Looks reasonable.
I'll give it a spin and commit if it works.","14/Jul/11 18:56;dvryaboy;Sorry for the long wait.
This looks fine, but stylistically, I don't see a reason for the ""import static"" construct being used. The use of import static is discouraged by its own documentation (http://download.oracle.com/javase/1.5.0/docs/guide/language/static-import.html -- see ""when should I use import static""), because it makes it unclear where a given method is implemented. ","14/Jul/11 23:50;gbowyer@fastmail.co.uk;Fine point, do you want me to change the patch ?","15/Jul/11 00:29;dvryaboy;ifn' you don't mind. I can do it if you are unable to, but I am about to go offline for a week, and have a bunch of other patches in flight :).","27/Jul/11 18:40;gbowyer@fastmail.co.uk;Sorry it took so long to make these changes, I guess my life also went offline :S

Static import removed",12/Aug/11 16:59;alangates;Marking as submit patch so this gets reviewed.,"12/Aug/11 17:42;dvryaboy;It's on my list, I'll take a look this weekend.",30/Aug/11 21:51;gbowyer@fastmail.co.uk;did we get this looked at ? I just found this patch at the end of my todo list :S,"31/Aug/11 22:28;dvryaboy;Code drifted a bit since this was written (my fault), I had to modify the patch slightly.

Tests pass.

Greg can you take a look and let me know if this looks ok?",05/Sep/11 00:15;dvryaboy;Committed to trunk.,27/Sep/12 21:26;dvryaboy;credit where credit's due :),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResourceSchema.toString does not properly handle maps in the schema,PIG-2112,12509636,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,alangates,alangates,alangates,08/Jun/11 20:31,26/Apr/12 20:33,14/Mar/19 03:07,20/Jun/11 17:06,0.9.0,,,,,,,0.10.0,,,,impl,,,0,,,,,,,,,,,,,ResourceSchema.toString prints an invalid schema string when there is a map in the schema.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Jun/11 21:36;alangates;PIG-2112.patch;https://issues.apache.org/jira/secure/attachment/12482099/PIG-2112.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-15 21:21:20.627,,,no_permission,,,,,,,,,,,,70837,,,,Mon Jun 20 17:06:29 UTC 2011,,,,,,,0|i0h133:,97438,,,,,,,,,,"08/Jun/11 20:34;alangates;The offending lines are:

{code}
230             if (DataType.isAtomic(this.type)) {
231                 sb.append(DataType.findTypeName(this.type));
232             } else {
233                 if (this.schema!=null)
234                     stringifyResourceSchema(sb, this.schema, this.type,
235             }
{code}

The map never has a schema, but is not atomic, so it's type name or brackets are never printed.",15/Jun/11 21:21;daijy;+1,20/Jun/11 17:06;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in piggybank.evaluation.util.apachelogparser.SearchTermExtractor,PIG-2110,12509328,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dale_jin,misterbeebee,misterbeebee,06/Jun/11 04:36,26/Apr/12 20:32,14/Mar/19 03:07,07/Jul/11 01:24,0.8.0,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"When processing a large log file, I get an exception in SearchTermExtractor.exec

I don't have a specific log line with a repro yet, but I assume the error occurs when the input URL is null, or maybe just has no query string:

I think a fix would be to be add a guard after creating queryString:

        String queryString = urlObject.getQuery();
        if (queryString == null) { return null; }

Stack Trace:
<code>
Caused by: java.io.IOException: Caught exception processing input row
        at org.apache.pig.piggybank.evaluation.util.apachelogparser.SearchTermExtractor.exec(SearchTermExtractor.java:195)
        at org.apache.pig.piggybank.evaluation.util.apachelogparser.SearchTermExtractor.exec(SearchTermExtractor.java:64)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:229)
Caused by: java.lang.NullPointerException
        at java.util.regex.Matcher.getTextLength(Matcher.java:1140)
        at java.util.regex.Matcher.reset(Matcher.java:291)
        at java.util.regex.Matcher.reset(Matcher.java:311)
        at org.apache.pig.piggybank.evaluation.util.apachelogparser.SearchTermExtractor.exec(SearchTermExtractor.java:170)
</code>",,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,02/Jul/11 08:18;dale_jin;SearchTermExtractor.diff;https://issues.apache.org/jira/secure/attachment/12484957/SearchTermExtractor.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-04 04:13:31.197,,,no_permission,,,,,,,,,,,,68931,Reviewed,,,Thu Jul 07 01:24:38 UTC 2011,,,,,,,0|i0h12n:,97436,,,,,,,,,,04/Jul/11 04:13;daijy;Patch looks good. Will commit if test pass.,07/Jul/11 01:24;daijy;Patch committed to trunk. Thanks Dale for contributing!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ant build continues even if the parser classes fail to be generated.,PIG-2109,12509297,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,zjshen,zjshen,zjshen,05/Jun/11 06:50,26/Apr/12 20:32,14/Mar/19 03:07,06/Jun/11 17:27,0.10.0,,,,,,,0.10.0,,,,build,,,0,,,,,,,,,,,,,"Ant build continues even if the parser classes fail to be generated. Please check the following build log:

'''
Buildfile: /home/zjshen/Workspace/eclipse/pig_svn/build.xml
ivy-download:
      [get] Getting: http://repo2.maven.org/maven2/org/apache/ivy/ivy/2.2.0/ivy-2.2.0.jar
      [get] To: /home/zjshen/Workspace/eclipse/pig_svn/ivy/ivy-2.2.0.jar
      [get] Not modified - so not downloaded
ivy-init-dirs:
ivy-probe-antlib:
ivy-init-antlib:
ivy-init:
[ivy:configure] :: Ivy 2.2.0 - 20100923230623 :: http://ant.apache.org/ivy/ ::
[ivy:configure] :: loading settings :: file = /home/zjshen/Workspace/eclipse/pig_svn/ivy/ivysettings.xml
ivy-compile:
[ivy:resolve] :: resolving dependencies :: org.apache.pig#Pig;0.10.0-SNAPSHOT
[ivy:resolve] 	confs: [compile]
[ivy:resolve] 	found commons-el#commons-el;1.0 in maven2
[ivy:resolve] 	found log4j#log4j;1.2.14 in maven2
[ivy:resolve] 	found org.apache.hadoop#hadoop-core;0.20.2 in maven2
[ivy:resolve] 	found commons-cli#commons-cli;1.2 in maven2
[ivy:resolve] 	found xmlenc#xmlenc;0.52 in maven2
[ivy:resolve] 	found commons-httpclient#commons-httpclient;3.0.1 in maven2
[ivy:resolve] 	found commons-codec#commons-codec;1.3 in maven2
[ivy:resolve] 	found commons-net#commons-net;1.4.1 in maven2
[ivy:resolve] 	found oro#oro;2.0.8 in maven2
[ivy:resolve] 	found org.mortbay.jetty#jetty;6.1.14 in maven2
[ivy:resolve] 	found org.mortbay.jetty#jetty-util;6.1.14 in maven2
[ivy:resolve] 	found org.mortbay.jetty#servlet-api-2.5;6.1.14 in maven2
[ivy:resolve] 	found tomcat#jasper-runtime;5.5.12 in maven2
[ivy:resolve] 	found tomcat#jasper-compiler;5.5.12 in maven2
[ivy:resolve] 	found org.mortbay.jetty#jsp-api-2.1;6.1.14 in maven2
[ivy:resolve] 	found org.mortbay.jetty#jsp-2.1;6.1.14 in maven2
[ivy:resolve] 	found org.eclipse.jdt#core;3.1.1 in maven2
[ivy:resolve] 	found ant#ant;1.6.5 in maven2
[ivy:resolve] 	found net.java.dev.jets3t#jets3t;0.7.1 in maven2
[ivy:resolve] 	found commons-logging#commons-logging;1.1.1 in maven2
[ivy:resolve] 	found net.sf.kosmosfs#kfs;0.3 in maven2
[ivy:resolve] 	found junit#junit;4.5 in maven2
[ivy:resolve] 	found hsqldb#hsqldb;1.8.0.10 in maven2
[ivy:resolve] 	found org.apache.hadoop#hadoop-test;0.20.2 in maven2
[ivy:resolve] 	found org.apache.ftpserver#ftplet-api;1.0.0 in maven2
[ivy:resolve] 	found org.apache.mina#mina-core;2.0.0-M5 in maven2
[ivy:resolve] 	found org.slf4j#slf4j-api;1.5.2 in maven2
[ivy:resolve] 	found org.apache.ftpserver#ftpserver-core;1.0.0 in maven2
[ivy:resolve] 	found org.apache.ftpserver#ftpserver-deprecated;1.0.0-M2 in maven2
[ivy:resolve] 	found org.slf4j#slf4j-log4j12;1.4.3 in maven2
[ivy:resolve] 	found org.apache.avro#avro;1.4.1 in maven2
[ivy:resolve] 	found com.googlecode.json-simple#json-simple;1.1 in maven2
[ivy:resolve] 	found com.jcraft#jsch;0.1.38 in maven2
[ivy:resolve] 	found jline#jline;0.9.94 in maven2
[ivy:resolve] 	found net.java.dev.javacc#javacc;4.2 in maven2
[ivy:resolve] 	found org.codehaus.jackson#jackson-mapper-asl;1.6.0 in maven2
[ivy:resolve] 	found org.codehaus.jackson#jackson-core-asl;1.6.0 in maven2
[ivy:resolve] 	found joda-time#joda-time;1.6 in maven2
[ivy:resolve] 	found commons-lang#commons-lang;2.4 in maven2
[ivy:resolve] 	found com.google.guava#guava;r06 in maven2
[ivy:resolve] 	found org.python#jython;2.5.0 in maven2
[ivy:resolve] 	found rhino#js;1.7R2 in maven2
[ivy:resolve] 	found org.antlr#antlr;3.2 in maven2
[ivy:resolve] 	found org.antlr#antlr-runtime;3.2 in maven2
[ivy:resolve] 	found org.antlr#stringtemplate;3.2 in maven2
[ivy:resolve] 	found antlr#antlr;2.7.7 in maven2
[ivy:resolve] 	found org.apache.zookeeper#zookeeper;3.3.3 in maven2
[ivy:resolve] 	found org.jboss.netty#netty;3.2.2.Final in maven2
[ivy:resolve] 	found org.apache.hbase#hbase;0.90.0 in maven2
[ivy:resolve] :: resolution report :: resolve 1990ms :: artifacts dl 83ms
[ivy:resolve] 	:: evicted modules:
[ivy:resolve] 	junit#junit;3.8.1 by [junit#junit;4.5] in [compile]
[ivy:resolve] 	commons-logging#commons-logging;1.0.3 by [commons-logging#commons-logging;1.1.1] in [compile]
[ivy:resolve] 	commons-codec#commons-codec;1.2 by [commons-codec#commons-codec;1.3] in [compile]
[ivy:resolve] 	commons-httpclient#commons-httpclient;3.1 by [commons-httpclient#commons-httpclient;3.0.1] in [compile]
[ivy:resolve] 	org.apache.mina#mina-core;2.0.0-M4 by [org.apache.mina#mina-core;2.0.0-M5] in [compile]
[ivy:resolve] 	org.apache.ftpserver#ftplet-api;1.0.0-M2 by [org.apache.ftpserver#ftplet-api;1.0.0] in [compile]
[ivy:resolve] 	org.apache.ftpserver#ftpserver-core;1.0.0-M2 by [org.apache.ftpserver#ftpserver-core;1.0.0] in [compile]
[ivy:resolve] 	org.apache.mina#mina-core;2.0.0-M2 by [org.apache.mina#mina-core;2.0.0-M5] in [compile]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      compile     |   57  |   0   |   0   |   8   ||   50  |   0   |
	---------------------------------------------------------------------
[ivy:retrieve] :: retrieving :: org.apache.pig#Pig
[ivy:retrieve] 	confs: [compile]
[ivy:retrieve] 	1 artifacts copied, 49 already retrieved (288kB/40ms)
[ivy:cachepath] DEPRECATED: 'ivy.conf.file' is deprecated, use 'ivy.settings.file' instead
[ivy:cachepath] :: loading settings :: file = /home/zjshen/Workspace/eclipse/pig_svn/ivy/ivysettings.xml
init:
     [move] Moving 1 file to /home/zjshen/Workspace/eclipse/pig_svn/build/ivy/lib/Pig
cc-compile:
prepare:
genLexer:
genParser:
     [java] error(106): /home/zjshen/Workspace/eclipse/pig_svn/src//org/apache/pig/parser/QueryParser.g:571:13: reference to undefined rule: nested_cross
     [java] Java Result: 1
genTreeParser:
gen:
compile:
     [echo] *** Building Main Sources ***
     [echo] *** To compile with all warnings enabled, supply -Dall.warnings=1 on command line ***
     [echo] *** If all.warnings property is supplied, compile-sources-all-warnings target will be executed ***
     [echo] *** Else, compile-sources (which only warns about deprecations) target will be executed ***
compile-sources:
    [javac] Compiling 6 source files to /home/zjshen/Workspace/eclipse/pig_svn/build/classes
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
compile-sources-all-warnings:
jar:
jarWithSvn:
ivy-download:
      [get] Getting: http://repo2.maven.org/maven2/org/apache/ivy/ivy/2.2.0/ivy-2.2.0.jar
      [get] To: /home/zjshen/Workspace/eclipse/pig_svn/ivy/ivy-2.2.0.jar
      [get] Not modified - so not downloaded
ivy-init-dirs:
ivy-probe-antlib:
ivy-init-antlib:
ivy-init:
ivy-buildJar:
[ivy:resolve] :: resolving dependencies :: org.apache.pig#Pig;0.10.0-SNAPSHOT
[ivy:resolve] 	confs: [buildJar]
[ivy:resolve] 	found commons-el#commons-el;1.0 in maven2
[ivy:resolve] 	found log4j#log4j;1.2.14 in maven2
[ivy:resolve] 	found org.apache.hadoop#hadoop-core;0.20.2 in maven2
[ivy:resolve] 	found commons-cli#commons-cli;1.2 in maven2
[ivy:resolve] 	found xmlenc#xmlenc;0.52 in maven2
[ivy:resolve] 	found commons-httpclient#commons-httpclient;3.0.1 in maven2
[ivy:resolve] 	found commons-codec#commons-codec;1.3 in maven2
[ivy:resolve] 	found commons-net#commons-net;1.4.1 in maven2
[ivy:resolve] 	found oro#oro;2.0.8 in maven2
[ivy:resolve] 	found org.mortbay.jetty#jetty;6.1.14 in maven2
[ivy:resolve] 	found org.mortbay.jetty#jetty-util;6.1.14 in maven2
[ivy:resolve] 	found org.mortbay.jetty#servlet-api-2.5;6.1.14 in maven2
[ivy:resolve] 	found tomcat#jasper-runtime;5.5.12 in maven2
[ivy:resolve] 	found tomcat#jasper-compiler;5.5.12 in maven2
[ivy:resolve] 	found org.mortbay.jetty#jsp-api-2.1;6.1.14 in maven2
[ivy:resolve] 	found org.mortbay.jetty#jsp-2.1;6.1.14 in maven2
[ivy:resolve] 	found org.eclipse.jdt#core;3.1.1 in maven2
[ivy:resolve] 	found ant#ant;1.6.5 in maven2
[ivy:resolve] 	found net.java.dev.jets3t#jets3t;0.7.1 in maven2
[ivy:resolve] 	found commons-logging#commons-logging;1.1.1 in maven2
[ivy:resolve] 	found net.sf.kosmosfs#kfs;0.3 in maven2
[ivy:resolve] 	found junit#junit;4.5 in maven2
[ivy:resolve] 	found hsqldb#hsqldb;1.8.0.10 in maven2
[ivy:resolve] 	found org.apache.hadoop#hadoop-test;0.20.2 in maven2
[ivy:resolve] 	found org.apache.ftpserver#ftplet-api;1.0.0 in maven2
[ivy:resolve] 	found org.apache.mina#mina-core;2.0.0-M5 in maven2
[ivy:resolve] 	found org.slf4j#slf4j-api;1.5.2 in maven2
[ivy:resolve] 	found org.apache.ftpserver#ftpserver-core;1.0.0 in maven2
[ivy:resolve] 	found org.apache.ftpserver#ftpserver-deprecated;1.0.0-M2 in maven2
[ivy:resolve] 	found org.slf4j#slf4j-log4j12;1.4.3 in maven2
[ivy:resolve] 	found org.apache.avro#avro;1.4.1 in maven2
[ivy:resolve] 	found com.googlecode.json-simple#json-simple;1.1 in maven2
[ivy:resolve] 	found com.jcraft#jsch;0.1.38 in maven2
[ivy:resolve] 	found jline#jline;0.9.94 in maven2
[ivy:resolve] 	found net.java.dev.javacc#javacc;4.2 in maven2
[ivy:resolve] 	found org.codehaus.jackson#jackson-mapper-asl;1.6.0 in maven2
[ivy:resolve] 	found org.codehaus.jackson#jackson-core-asl;1.6.0 in maven2
[ivy:resolve] 	found joda-time#joda-time;1.6 in maven2
[ivy:resolve] 	found commons-lang#commons-lang;2.4 in maven2
[ivy:resolve] 	found com.google.guava#guava;r06 in maven2
[ivy:resolve] 	found org.python#jython;2.5.0 in maven2
[ivy:resolve] 	found rhino#js;1.7R2 in maven2
[ivy:resolve] 	found org.antlr#antlr;3.2 in maven2
[ivy:resolve] 	found org.antlr#antlr-runtime;3.2 in maven2
[ivy:resolve] 	found org.antlr#stringtemplate;3.2 in maven2
[ivy:resolve] 	found antlr#antlr;2.7.7 in maven2
[ivy:resolve] 	found org.apache.zookeeper#zookeeper;3.3.3 in maven2
[ivy:resolve] 	found org.jboss.netty#netty;3.2.2.Final in maven2
[ivy:resolve] 	found org.apache.hbase#hbase;0.90.0 in maven2
[ivy:resolve] :: resolution report :: resolve 473ms :: artifacts dl 98ms
[ivy:resolve] 	:: evicted modules:
[ivy:resolve] 	junit#junit;3.8.1 by [junit#junit;4.5] in [buildJar]
[ivy:resolve] 	commons-logging#commons-logging;1.0.3 by [commons-logging#commons-logging;1.1.1] in [buildJar]
[ivy:resolve] 	commons-codec#commons-codec;1.2 by [commons-codec#commons-codec;1.3] in [buildJar]
[ivy:resolve] 	commons-httpclient#commons-httpclient;3.1 by [commons-httpclient#commons-httpclient;3.0.1] in [buildJar]
[ivy:resolve] 	org.apache.mina#mina-core;2.0.0-M4 by [org.apache.mina#mina-core;2.0.0-M5] in [buildJar]
[ivy:resolve] 	org.apache.ftpserver#ftplet-api;1.0.0-M2 by [org.apache.ftpserver#ftplet-api;1.0.0] in [buildJar]
[ivy:resolve] 	org.apache.ftpserver#ftpserver-core;1.0.0-M2 by [org.apache.ftpserver#ftpserver-core;1.0.0] in [buildJar]
[ivy:resolve] 	org.apache.mina#mina-core;2.0.0-M2 by [org.apache.mina#mina-core;2.0.0-M5] in [buildJar]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|     buildJar     |   57  |   0   |   0   |   8   ||   50  |   0   |
	---------------------------------------------------------------------
[ivy:retrieve] :: retrieving :: org.apache.pig#Pig
[ivy:retrieve] 	confs: [buildJar]
[ivy:retrieve] 	1 artifacts copied, 49 already retrieved (288kB/35ms)
buildJar:
     [echo] svnString 1131920
      [jar] Building jar: /home/zjshen/Workspace/eclipse/pig_svn/build/pig-0.10.0-SNAPSHOT-core.jar
      [jar] Building jar: /home/zjshen/Workspace/eclipse/pig_svn/build/pig-0.10.0-SNAPSHOT.jar
     [copy] Copying 1 file to /home/zjshen/Workspace/eclipse/pig_svn
jarWithOutSvn:
BUILD SUCCESSFUL
Total time: 29 seconds
'''

It may cause developers overlooking the errors of generating parser classes. The reasonable behavior is terminating the build when the errors are encountered.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Jun/11 06:52;zjshen;build.xml.patch;https://issues.apache.org/jira/secure/attachment/12481490/build.xml.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-06 17:27:28.965,,,no_permission,,,,,,,,,,,,71871,Reviewed,,,Mon Jun 06 17:27:28 UTC 2011,,,Patch Available,,,,0|i0h12f:,97435,,,,,,,,,,06/Jun/11 17:27;daijy;Patch committed. Thanks Zhijie!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix Zebra unit test TestBasicUnion.testNeg3, TestBasicUnion.testNeg4",PIG-2106,12509028,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,01/Jun/11 21:56,04/Aug/11 00:34,14/Mar/19 03:07,01/Jun/11 22:10,0.10.0,0.9.0,,,,,,0.10.0,0.9.0,,,,,,0,,,,,,,,,,,,,Two zebra unit tests are broken after PIG-2084. We need to fix them.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Jun/11 21:57;daijy;PIG-2106-1.patch;https://issues.apache.org/jira/secure/attachment/12481152/PIG-2106-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-01 22:04:45.493,,,no_permission,,,,,,,,,,,,66288,Reviewed,,,Wed Jun 01 22:10:02 UTC 2011,,,,,,,0|i0h11z:,97433,,,,,,,,,,01/Jun/11 22:04;thejas;+1,01/Jun/11 22:10;daijy;Patch committed to both trunk and 0.9.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MonitoredUDF does not work,PIG-2102,12508885,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,alangates,alangates,31/May/11 23:29,05/Oct/11 17:20,14/Mar/19 03:07,13/Aug/11 00:14,0.8.0,0.8.1,0.9.0,,,,,0.10.0,0.9.1,,,impl,,,0,newbie,,,,,,,,,,,,"The MonitoredUDF feature doesn't work.  When a UDF is annotated with it, job setup fails with an internal error.  The stack is long, but the salient line appears to be:

{code}
Caused by: java.io.IOException: Serialization error: org.apache.pig.backend.hadoop.executionengine.physicalLayer.util.MonitoredUDFExecutor
{code}

I think making this class implement Serializable would solve the issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Aug/11 18:12;alangates;PIG-2102-withtests.patch;https://issues.apache.org/jira/secure/attachment/12490275/PIG-2102-withtests.patch,11/Aug/11 02:01;dvryaboy;PIG-2102.patch;https://issues.apache.org/jira/secure/attachment/12490065/PIG-2102.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-08-11 02:02:13.827,,,no_permission,,,,,,,,,,,,41695,,,,Sat Aug 13 00:14:34 UTC 2011,,,,,,,0|i0h11b:,97430,,,,,,,,,,"11/Aug/11 02:02;dvryaboy;Tested by hand. Seems like the sort of thing we don't want to write a unit test for, and perhaps should be tested by the e2e framework.",12/Aug/11 18:12;alangates;+1.  Patch looks good.  The new patch I've uploaded includes end-to-end tests.,"12/Aug/11 23:11;dvryaboy;Committed to trunk.

Alan, looks like the patch applies to 0.9 branch also -- I am not set up to run e2e, were there any changes in 10 that would make it run there but not in 9? Is this safe to commit to the 0.9 branch?",12/Aug/11 23:37;alangates;For 0.9 you should apply your earlier patch.  I haven't ported most of the e2e changes to 0.9.,13/Aug/11 00:14;dvryaboy;Committed to 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Grunt help is missing sh command,PIG-2092,12508251,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,24/May/11 21:57,26/Apr/12 20:33,14/Mar/19 03:07,04/Oct/11 22:21,,,,,,,,0.10.0,,,,,,,0,newbie,,,,,,,,,,,,This is a trivial patch that I am planning to commit asap without review,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Oct/11 21:15;olgan;PIG-2092.patch;https://issues.apache.org/jira/secure/attachment/12497708/PIG-2092.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,45480,,,,Tue Oct 04 22:21:33 UTC 2011,,,,,,,0|i0h0zb:,97421,,,,,,,,,,04/Oct/11 22:21;olgan;Patch committed to trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Javadoc for ResourceFieldSchema.getSchema() is wrong,PIG-2089,12508098,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,23/May/11 20:06,04/Aug/11 00:35,14/Mar/19 03:07,24/May/11 00:12,0.8.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"Javadoc says: ""Only fields of type tuple should have a schema"". Actually bag, map(starting from 0.9) also can have schema.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/May/11 23:42;daijy;PIG-2089-1.patch;https://issues.apache.org/jira/secure/attachment/12480187/PIG-2089-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,66127,,,,Tue May 24 00:12:18 UTC 2011,,,,,,,0|i0h0yn:,97418,,,,,,,,,,24/May/11 00:12;daijy;Patch committed to 0.9 and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Return alias validation failed when there is single line comment in the macro,PIG-2088,12508091,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,23/May/11 18:32,04/Aug/11 00:34,14/Mar/19 03:07,23/May/11 20:19,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"The following script

{code}
define test() returns b { 
   a = load 'data' as (name, age, gpa);
-- message 
   $b = filter a by (int)age > 40; 
};

beta = test();
store beta into 'output';
{code}

results in a validation failure:

{code}
ERROR 1200 ""Macro test missing return alias b""
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/May/11 18:34;rding;PIG-2088.patch;https://issues.apache.org/jira/secure/attachment/12480143/PIG-2088.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-23 19:11:03.541,,,no_permission,,,,,,,,,,,,66293,Reviewed,,,Mon May 23 20:19:33 UTC 2011,,,,,,,0|i0h0yf:,97417,,,,,,,,,,23/May/11 19:11;thejas;+1,23/May/11 20:19;rding;Patch committed to trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"pig is running validation for a statement at a time batch mode, instead of running it for whole script",PIG-2084,12507906,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,20/May/11 17:44,04/Aug/11 00:34,14/Mar/19 03:07,23/May/11 20:37,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"In PIG-2059, a change was made to run validation for each statement instead of running it once for the whole script.
This slows down the validation phase, and it ends up taking tens of seconds.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/May/11 16:26;thejas;PIG-2084.1.patch;https://issues.apache.org/jira/secure/attachment/12480125/PIG-2084.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-23 18:07:46.013,,,no_permission,,,,,,,,,,,,66313,,,,Wed Jun 01 21:43:33 UTC 2011,,,,,,,0|i0h0xj:,97413,,,,,,,,,,"20/May/11 17:44;thejas;To fix this, the per-statement validation should be enabled only for interactive mode and if the -c option (check) is used.
",23/May/11 18:07;rding;+1,23/May/11 20:37;thejas;Unit test and test-patch passed. Patch committed to trunk and 0.9 branch. ,"01/Jun/11 21:43;daijy;PigServer.registerQuery() will not trigger validation after the patch, unless PigServer.setValidateEachStatement(true) is called.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bincond ERROR 1025: Invalid field projection when null is used,PIG-2083,12507903,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,araceli,araceli,20/May/11 17:22,04/Aug/11 00:34,14/Mar/19 03:07,25/May/11 19:53,0.9.0,,,,,,,0.9.0,,,,build,,,0,,,,,,,,,,,,,"This is a regression for 9.

a = load '1.txt' as (a0, a1);
b = foreach a generate (a0==0?null:2);
explain b;

ERROR 1025:
Invalid field projection. Projected field [null] does not exist in schema

","
Linux 2.6.18-53.1.13.el5 #1 SMP Mon Feb 11 13:27:27 EST 2008 x86_64 x86_64 x86_64 GNU/Linux

Hadoop 0.20.203.3.1104011556 -r 96519d04f65e22ffadf89b225d0d44ef1741d126
Compiled on Fri Apr  1 16:29:09 PDT 2011

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/May/11 15:11;thejas;PIG-2083.1.patch;https://issues.apache.org/jira/secure/attachment/12480419/PIG-2083.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-24 00:23:34.371,,,no_permission,,,,,,,,,,,,66189,Reviewed,,,Wed May 25 19:53:02 UTC 2011,,,,,,,0|i0h0xb:,97412,,,,,,,,,,"24/May/11 00:23;thejas;In the above query, the tokeniser is matching NULL as identifier instead of the NULL token. 
The above query works when there is a space after the null. The parser generated by antlr is supposed to match NULL token as that rule comes before the identifier rule. This looks like a bug in antlr.

","24/May/11 06:23;xuefuz;This is probably not caused by Antlr, but by Pig's syntactic rule. 

IDENTIFIER : ( ID DCOLON ) => ( ID DCOLON IDENTIFIER )
           | ID
;

When Antlr say NULL, it cannot make a decision until it says the next character, which is a colon. With the colon, the above rule applies, so it tries to match IDENTIFIER. As a result of such matching, it ends with two tokens, IDENTIFIER and ':'. When there is a space, above rule doesn't apply.

I don't know a good solution yet. A potential solution is to add predicate for all reserved keyword such that matching stops as long as the next character is not a letter. In this way, 'null:' will be match to NULL as a keyword, and ':'.



","25/May/11 16:50;thejas;PIG-2083.1.patch - I have removed the NULL literal from QueryLexer.g and replaced IDENTIFIER in it with IDENTIFIER_L. 
In QueryParser.g IDENTIFIER_L gets replaced by NULL or IDENTIFIER using semantic predicates. I could not find a solution that could get the resolution of NULL vs IDENTIFIER done in QueryLexer itself.
",25/May/11 17:54;rding;+1,"25/May/11 19:53;thejas;Patch committed to trunk and 0.9 branch.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dryrun gives wrong line numbers in error message for scripts containing macro.,PIG-2081,12507799,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,19/May/11 18:58,04/Aug/11 00:35,14/Mar/19 03:07,20/May/11 19:24,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"For following script (test.pig)

{code}
1 DEFINE my_macro (X,key) returns Y
  2         {
  3         tmp1 = foreach  $X generate TOKENIZE((chararray)$key) as tokens;
  4         tmp2 = foreach tmp1 generate flatten(tokens);
  5         tmp3 = order tmp2 by $0;
  6         $Y = distinct tmp3;
  7         }
  8 
  9 A = load 'sometext' using TextLoader() as (row) ;
 10 E = my_macro(A,row);
 11 
 12 A1 = load 'sometext2' using TextLoader() as (row1);
 13 E1 = my_macro(A1,row1);
 14 
 15 A3 = load 'sometext3' using TextLoader() as (row3);
 16 E3 = my_macro(A3,$0);
 17 
 18 F = cogroup E by $0, E1 by $0,E3 by $0;
 19 dump F;
{code}

pig test.pig gives correct line number in error message:

{code}
ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1200: <file test.pig, line 16, column 17>  mismatched input '$0' expecting set null
{code}

while pig -r test.pig gives incorrect line number in error message:

{code}
ERROR org.apache.pig.Main - ERROR 1200: <file test.pig.substituted, line 1, column 17>  mismatched input '$0' expecting set null
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/May/11 21:15;rding;PIG-2081.patch;https://issues.apache.org/jira/secure/attachment/12479834/PIG-2081.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-20 17:57:17.285,,,no_permission,,,,,,,,,,,,66160,Reviewed,,,Fri May 20 19:24:34 UTC 2011,,,,,,,0|i0h0wv:,97410,,,,,,,,,,20/May/11 17:57;thejas;+1,20/May/11 18:59;rding;test-patch and unit tests pass.,20/May/11 19:24;rding;patch committed to trunk and 0.9 branch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
POProject.getNext(DataBag) does not handle null,PIG-2078,12507635,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,18/May/11 17:29,04/Aug/11 00:34,14/Mar/19 03:07,19/May/11 21:28,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"The following script fail with ""-t MergeForEach""
{code}
a = load '1.txt' as (a0:bag{}, a1:int);
b = foreach a generate a0;
dump b;
{code}

1.txt:
{(1)}   2
        3

Error stack:
java.lang.NullPointerException
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.consumeInputBag(POProject.java:310)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:251)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:316)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:332)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:284)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:261)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:256)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:1)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/May/11 17:43;daijy;PIG-2078-1.patch;https://issues.apache.org/jira/secure/attachment/12479804/PIG-2078-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-19 17:46:47.868,,,no_permission,,,,,,,,,,,,66304,Reviewed,,,Thu May 19 21:28:27 UTC 2011,,,,,,,0|i0h0w7:,97407,,,,,,,,,,"19/May/11 17:46;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/763/
-----------------------------------------------------------

Review request for pig and thejas.


Summary
-------

See PIG-2078


This addresses bug PIG-2078.
    https://issues.apache.org/jira/browse/PIG-2078


Diffs
-----

  trunk/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POProject.java 1100118 
  trunk/test/org/apache/pig/test/TestEvalPipeline2.java 1100118 

Diff: https://reviews.apache.org/r/763/diff


Testing
-------

Test-patch:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Unit test:
    all pass

End-to-end test:
    all pass


Thanks,

Daniel

","19/May/11 18:14;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/763/#review687
-----------------------------------------------------------

Ship it!


+1

- thejas


On 2011-05-19 17:46:48, Daniel Dai wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/763/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-05-19 17:46:48)
bq.  
bq.  
bq.  Review request for pig and thejas.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  See PIG-2078
bq.  
bq.  
bq.  This addresses bug PIG-2078.
bq.      https://issues.apache.org/jira/browse/PIG-2078
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    trunk/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POProject.java 1100118 
bq.    trunk/test/org/apache/pig/test/TestEvalPipeline2.java 1100118 
bq.  
bq.  Diff: https://reviews.apache.org/r/763/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  Test-patch:
bq.       [exec] +1 overall.  
bq.       [exec] 
bq.       [exec]     +1 @author.  The patch does not contain any @author tags.
bq.       [exec] 
bq.       [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
bq.       [exec] 
bq.       [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
bq.       [exec] 
bq.       [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
bq.       [exec] 
bq.       [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
bq.       [exec] 
bq.       [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
bq.  
bq.  Unit test:
bq.      all pass
bq.  
bq.  End-to-end test:
bq.      all pass
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Daniel
bq.  
bq.

",19/May/11 21:28;daijy;Committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"update documentation, help command with correct default value of pig.cachedbag.memusage",PIG-2076,12507192,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,13/May/11 23:53,04/Aug/11 00:35,14/Mar/19 03:07,16/May/11 16:13,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"The default value of pig.cachedbag.memusage was changed to 0.2 in pig 0.8, as part of changes in PIG-1447 .
But the help command and documentation shows older default value of 0.1 .
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/May/11 00:00;thejas;PIG-2076.1.patch;https://issues.apache.org/jira/secure/attachment/12479193/PIG-2076.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-14 00:07:35.523,,,no_permission,,,,,,,,,,,,66124,,,,Mon May 16 16:13:55 UTC 2011,,,,,,,0|i0h0vr:,97405,,,,,,,,,,14/May/11 00:07;rding;+1,"16/May/11 16:13;thejas;Test-patch succeeded, ran test-commit as the change is a minor change to documentation and a message string - that succeeded as well.
Patch committed to trunk and 0.9 branch.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bring back TestNewPlanPushUpFilter,PIG-2075,12507190,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,13/May/11 23:24,04/Aug/11 00:34,14/Mar/19 03:07,13/May/11 23:30,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"When I commit PIG-1575, forget the file TestNewPlanPushUpFilter. Bring it back.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/May/11 23:25;daijy;TestNewPlanPushUpFilter.java;https://issues.apache.org/jira/secure/attachment/12479187/TestNewPlanPushUpFilter.java,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,66348,,,,Fri May 13 23:30:28 UTC 2011,,,,,,,0|i0h0vj:,97404,,,,,,,,,,13/May/11 23:30;daijy;New test case committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
update union documentation - numeric types will  not be converted to chararray,PIG-2073,12507175,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,thejas,thejas,13/May/11 20:29,04/Aug/11 00:34,14/Mar/19 03:07,13/May/11 23:10,0.9.0,,,,,,,0.9.0,,,,documentation,,,0,,,,,,,,,,,,,"As a result of changes in PIG-2071, the documentation needs to be updated -
{noformat}
 Union columns of compatible type will produce an ""escalate"" type. The priority is chararray > double > float > long > int > bytearray, tuple|bag|map > bytearray:
{noformat}

should be changed to remove chararray in the numeric type sequence. ie - 
{noformat}
 Union columns of compatible type will produce an ""escalate"" type. The priority is double > float > long > int > bytearray, tuple|bag|map|chararray > bytearray:
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-05-13 23:10:21.17,,,no_permission,,,,,,,,,,,,66185,Reviewed,,,Fri May 13 23:10:21 UTC 2011,,,,,,,0|i0h0vb:,97403,,,,,,,,,,"13/May/11 23:10;chandec;Documentation updated. 
Fix will be included in the GA patch for PIG-1772.
Thanks for review Thejas!
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when udf has project-star argument and input schema is null,PIG-2072,12507164,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,13/May/11 19:05,04/Aug/11 00:34,14/Mar/19 03:07,16/May/11 15:59,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"{code}
grunt> l = load 'x' ;                 
grunt> f = foreach l generate CONCAT(*);
2011-05-13 12:04:48,355 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2999: Unexpected internal error. null
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/May/11 19:19;thejas;PIG-2072.1.patch;https://issues.apache.org/jira/secure/attachment/12479158/PIG-2072.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-13 20:42:03.511,,,no_permission,,,,,,,,,,,,66319,,,,Mon May 16 15:59:39 UTC 2011,,,,,,,0|i0h0v3:,97402,,,,,,,,,,13/May/11 20:42;daijy;+1,"16/May/11 15:59;thejas;Unit test and test-patch passed. Patch committed to trunk and 0.9 branch.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
casting numeric type to chararray during schema merge for union is inconsistent with other schema merge cases,PIG-2071,12507158,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,13/May/11 18:06,04/Aug/11 00:35,14/Mar/19 03:07,16/May/11 18:44,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"In PIG-1536, if a column in output of union has a source column one input of type charraray and in other it is of a numeric type, the column of numeric type will be cast to chararray. But in all other cases, chararray and numeric types are considered incompatible, and as a result such implicit casting is not done.
To be consistent with other cases, union also should consider chararray and numeric types to be incompatible.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/May/11 16:17;thejas;PIG-2071.1.patch;https://issues.apache.org/jira/secure/attachment/12479342/PIG-2071.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-16 17:24:07.757,,,no_permission,,,,,,,,,,,,66104,,,,Mon May 16 18:44:53 UTC 2011,,,,,,,0|i0h0uv:,97401,,,,,,,,,,16/May/11 16:17;thejas;PIG-2071.1.patch - test-patch and unit tests succeeded with this patch.  (except for one test that is currently failing in trunk as well).,16/May/11 17:24;daijy;+1,"16/May/11 18:44;thejas;Patch committed to trunk and 0.9 branch.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Unknown"" appears in error message for an error case",PIG-2070,12507157,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,xuefuz,xuefuz,13/May/11 18:04,04/Aug/11 00:35,14/Mar/19 03:07,16/May/11 18:57,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"For the following query:

a = load '1.txt' as (a0:int, a1:int);
b = load '2.txt' as (a0:int, a1:chararray);
c = cogroup a by (a0,a1), b by (a0,a1);

Pig gives the following message, which includes ""unknown"" word. 

2011-05-13 11:01:18,682 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1051:
<line 3, column 4> Cannot cast to Unknown

The error message should be more meaningful.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/May/11 16:16;thejas;PIG-2070.1.patch;https://issues.apache.org/jira/secure/attachment/12479341/PIG-2070.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-16 16:16:02.533,,,no_permission,,,,,,,,,,,,66147,,,,Mon May 16 18:57:11 UTC 2011,,,,,,,0|i0h0un:,97400,,,,,,,,,,16/May/11 16:16;thejas;PIG-2070.1.patch - test-patch and unit tests have passed with this patch. (except for one test that is currently failing in trunk as well).,16/May/11 17:37;rding;+1,"16/May/11 18:57;thejas;Patch committed to trunk and 0.9 branch.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LoadFunc jar does not ship to backend in MultiQuery case,PIG-2069,12507076,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,daijy,daijy,13/May/11 02:19,04/Aug/11 00:34,14/Mar/19 03:07,16/May/11 17:15,0.8.1,0.9.0,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"Pig is able to automatically figure out the jar containing the LoadFunc and ship them to backend. However, the following script didn't:
{code}
A = load '1.txt' using SomeLoadFunc();
B = filter A by $0==0;
C = filter A by $1==1;
D = join B by $0, C by $0;
dump D;
{code}

The reason is this query is a multiquery (A is reused and thus create an implicit split). When we merge multiquery into one job, we didn't merge udfs list properly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/May/11 21:30;rding;PIG-2069.patch;https://issues.apache.org/jira/secure/attachment/12479175/PIG-2069.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-13 02:39:20.815,,,no_permission,,,,,,,,,,,,66225,Reviewed,,,Mon May 16 17:15:08 UTC 2011,,,,,,,0|i0h0uf:,97399,,,,,,,,,,"13/May/11 02:39;viraj;Daniel is this an issue since Multiquery optimization was introduced (0.6, 0.7), or is this specific to Pig 0.8 and 0.9.",13/May/11 07:37;daijy;This is a general affecting all those versions.,"13/May/11 21:30;rding;This happens when the original MapReduce DAG (before optimization) contains a diamond node.

User can workaround this by explicitly registering the LoadFunc jar in the script.

The attached patch provides a fix. It's verified with manual test.",13/May/11 23:49;daijy;+1 if test pass.,16/May/11 17:15;rding;Unit tests pass. Patch committed to trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
documentation of union onschema restrictions need to to be updated,PIG-2068,12507065,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,thejas,thejas,13/May/11 00:22,04/Aug/11 00:35,14/Mar/19 03:07,13/May/11 23:06,0.9.0,,,,,,,0.9.0,,,,documentation,,,0,,,,,,,,,,,,,"The following requirement mentioned under union-onschema section is no longer applicable -
{code}
The data type for columns with same name in different input schemas should be compatible:

    Numeric types are compatible, and if column having same name in different input schemas have different numeric types, an implicit conversion will happen.
    Bytearray type is considered compatible with all other types, a cast will be added to convert to other type.
    Bags or tuples having different inner schema are considered incompatible.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-05-13 23:06:07.846,,,no_permission,,,,,,,,,,,,66139,,,,Fri May 13 23:06:48 UTC 2011,,,,,,,0|i0h0u7:,97398,,,,,,,,,,"13/May/11 00:24;thejas;I mean, in 0.9 such a restriction is not there for union onschema. This is a result of the changes in PIG-1536.
","13/May/11 23:06;chandec;Documentation updated. 
Fix will be included in the GA patch for PIG-1772.
Thanks for review Thejas!",13/May/11 23:06;chandec;Fixed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FilterLogicExpressionSimplifier removed some branches in some cases,PIG-2067,12507050,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,12/May/11 22:31,21/Sep/11 22:47,14/Mar/19 03:07,13/May/11 20:38,0.8.1,0.9.0,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"The following script produce wrong result:
{code}
A = load 'a.dat' as (cookie);
B = load 'b.dat' as (cookie);
C = cogroup A by cookie, B by cookie;
E = filter C by COUNT(B)>0 AND COUNT(A)>0;
explain E;
{code}

a.dat:
1       1
2       2
3       3
4       4
5       5
6       6
7       7

b.dat:
3       3
4       4
5       5
6       6
7       7
8       8

Expected output:
(3,{(3)},{(3)})
(4,{(4)},{(4)})
(5,{(5)},{(5)})
(6,{(6)},{(6)})
(7,{(7)},{(7)})

We get:
(3,{(3)},{(3)})
(4,{(4)},{(4)})
(5,{(5)},{(5)})
(6,{(6)},{(6)})
(7,{(7)},{(7)})
(8,{},{(8)})
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/May/11 02:27;daijy;PIG-2067-1-0.8.patch;https://issues.apache.org/jira/secure/attachment/12479054/PIG-2067-1-0.8.patch,13/May/11 00:43;daijy;PIG-2067-1.patch;https://issues.apache.org/jira/secure/attachment/12479043/PIG-2067-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-05-13 16:59:47.489,,,no_permission,,,,,,,,,,,,35764,Reviewed,,,Wed Sep 21 22:47:29 UTC 2011,,,,,,,0|i0h0tz:,97397,,,,,,,,,,"12/May/11 22:33;daijy;The logical plan after LogicExpressionSimplifier is wrong:

E: (Name: LOStore Schema: group#29:bytearray,A#30:bag{#240:tuple(cookie#10:bytearray)},B#32:bag{#241:tuple(cookie#11:bytearray)})
|
|---E: (Name: LOFilter Schema: group#29:bytearray,A#30:bag{#240:tuple(cookie#10:bytearray)},B#32:bag{#241:tuple(cookie#11:bytearray)})
    |   |
    |   (Name: And Type: boolean Uid: 41)
    |   |
    |   |---(Name: GreaterThan Type: boolean Uid: 37)
    |   |   |
    |   |   |---(Name: UserFunc(org.apache.pig.builtin.COUNT) Type: long Uid: 34)
    |   |   |   |
    |   |   |   |---B:(Name: Project Type: bag Uid: 32 Input: 0 Column: 2)
    |   |   |
    |   |   |---(Name: Cast Type: long Uid: 35)
    |   |       |
    |   |       |---(Name: Constant Type: int Uid: 35)
    |   |
    |   |---(Name: GreaterThan Type: boolean Uid: 40)
    |       |
    |       |---(Name: Cast Type: int Uid: 29)
    |       |   |
    |       |   |---group:(Name: Project Type: bytearray Uid: 29 Input: 0 Column: 0)
    |       |
    |       |---(Name: Constant Type: int Uid: 39)
    |
    |---C: (Name: LOCogroup Schema: group#29:bytearray,A#30:bag{#240:tuple(cookie#10:bytearray)},B#32:bag{#241:tuple(cookie#11:bytearray)})
        |   |
        |   cookie:(Name: Project Type: bytearray Uid: 10 Input: 0 Column: 0)
        |   |
        |   cookie:(Name: Project Type: bytearray Uid: 11 Input: 1 Column: 0)
        |
        |---A: (Name: LOLoad Schema: cookie#10:bytearray)RequiredFields:null
        |
        |---B: (Name: LOLoad Schema: cookie#11:bytearray)RequiredFields:null

One branch of GreaterThan is on group rather than A.","12/May/11 22:34;daijy;The format mess up:
Here is the logical plan:
{code}
E: (Name: LOStore Schema: group#29:bytearray,A#30:bag{#240:tuple(cookie#10:bytearray)},B#32:bag{#241:tuple(cookie#11:bytearray)})
|
|---E: (Name: LOFilter Schema: group#29:bytearray,A#30:bag{#240:tuple(cookie#10:bytearray)},B#32:bag{#241:tuple(cookie#11:bytearray)})
    |   |
    |   (Name: And Type: boolean Uid: 41)
    |   |
    |   |---(Name: GreaterThan Type: boolean Uid: 37)
    |   |   |
    |   |   |---(Name: UserFunc(org.apache.pig.builtin.COUNT) Type: long Uid: 34)
    |   |   |   |
    |   |   |   |---B:(Name: Project Type: bag Uid: 32 Input: 0 Column: 2)
    |   |   |
    |   |   |---(Name: Cast Type: long Uid: 35)
    |   |       |
    |   |       |---(Name: Constant Type: int Uid: 35)
    |   |
    |   |---(Name: GreaterThan Type: boolean Uid: 40)
    |       |
    |       |---(Name: Cast Type: int Uid: 29)
    |       |   |
    |       |   |---group:(Name: Project Type: bytearray Uid: 29 Input: 0 Column: 0)
    |       |
    |       |---(Name: Constant Type: int Uid: 39)
    |
    |---C: (Name: LOCogroup Schema: group#29:bytearray,A#30:bag{#240:tuple(cookie#10:bytearray)},B#32:bag{#241:tuple(cookie#11:bytearray)})
        |   |
        |   cookie:(Name: Project Type: bytearray Uid: 10 Input: 0 Column: 0)
        |   |
        |   cookie:(Name: Project Type: bytearray Uid: 11 Input: 1 Column: 0)
        |
        |---A: (Name: LOLoad Schema: cookie#10:bytearray)RequiredFields:null
        |
        |---B: (Name: LOLoad Schema: cookie#11:bytearray)RequiredFields:null

{code}

One branch of GreaterThan is on group rather than A.","12/May/11 23:06;daijy;Actually it erroneously remove one branch:
{code}
#-----------------------------------------------
# New Logical Plan:
#-----------------------------------------------
E: (Name: LOStore Schema: group#30:bytearray,A#31:bag{#242:tuple(cookie#10:bytearray)},B#33:bag{#243:tuple(cookie#11:bytearray)})
|
|---E: (Name: LOFilter Schema: group#30:bytearray,A#31:bag{#242:tuple(cookie#10:bytearray)},B#33:bag{#243:tuple(cookie#11:bytearray)})
    |   |
    |   (Name: GreaterThan Type: boolean Uid: 38)
    |   |
    |   |---(Name: UserFunc(org.apache.pig.builtin.COUNT) Type: long Uid: 35)
    |   |   |
    |   |   |---B:(Name: Project Type: bag Uid: 33 Input: 0 Column: 2)
    |   |
    |   |---(Name: Cast Type: long Uid: 36)
    |       |
    |       |---(Name: Constant Type: int Uid: 36)
    |
    |---C: (Name: LOCogroup Schema: group#30:bytearray,A#31:bag{#242:tuple(cookie#10:bytearray)},B#33:bag{#243:tuple(cookie#11:bytearray)})
        |   |
        |   cookie:(Name: Project Type: bytearray Uid: 10 Input: 0 Column: 0)
        |   |
        |   cookie:(Name: Project Type: bytearray Uid: 11 Input: 1 Column: 0)
        |
        |---A: (Name: LOLoad Schema: cookie#10:bytearray)RequiredFields:null
        |
        |---B: (Name: LOLoad Schema: cookie#11:bytearray)RequiredFields:null

{code}","13/May/11 00:48;daijy;This issue happens when:
1. We have AND in filter plan
2. Two branch of AND is the same UDF, but the input for the UDF is different

LogicExpressionSimplifier will erroneously believe two branches are the same and merge them.",13/May/11 02:27;daijy;PIG-2067-1-0.8.patch is for 0.8 branch.,"13/May/11 16:59;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/743/
-----------------------------------------------------------

Review request for pig and Richard Ding.


Summary
-------

See PIG-2067


This addresses bug PIG-2067.
    https://issues.apache.org/jira/browse/PIG-2067


Diffs
-----

  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/expression/ProjectExpression.java 1102169 
  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/expression/UserFuncExpression.java 1102169 
  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/rules/LogicalExpressionSimplifier.java 1102169 
  http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/TestFilterSimplification.java 1102169 

Diff: https://reviews.apache.org/r/743/diff


Testing
-------

Unit test:
    all pass

End-to-end test:
    all pass


Thanks,

Daniel

",13/May/11 17:44;rding;+1,"13/May/11 18:19;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/743/
-----------------------------------------------------------

(Updated 2011-05-13 18:13:38.714903)


Review request for pig and Richard Ding.


Summary
-------

See PIG-2067


This addresses bug PIG-2067.
    https://issues.apache.org/jira/browse/PIG-2067


Diffs
-----

  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/expression/ProjectExpression.java 1102169 
  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/expression/UserFuncExpression.java 1102169 
  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/rules/LogicalExpressionSimplifier.java 1102169 
  http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/TestFilterSimplification.java 1102169 

Diff: https://reviews.apache.org/r/743/diff


Testing (updated)
-------

Test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 562 release audit warnings (more than the trunk's current 561 warnings).
Ignore release audit warning since no new file is added.

Unit test:
    all pass

End-to-end test:
    all pass


Thanks,

Daniel

","13/May/11 20:38;daijy;Patch committed to 0.8, 0.9, trunk.",21/Sep/11 07:35;tlipcon;This fix isn't in the 0.8.1 release - perhaps it's on the 0.8 branch post-0.8.1,"21/Sep/11 22:47;daijy;Yes, this is in 0.8 branch but not 0.8.1 release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Script silently ended,PIG-2062,12506936,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,daijy,daijy,12/May/11 05:38,04/Aug/11 00:34,14/Mar/19 03:07,13/May/11 18:13,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"The following script ended silently without execution.

{code}
a = load '1.txt' as (a0, a1);
b = load '2.txt' as (b0, b1);
all = join a by a0, b by b0;
store all into '1111';
{code}

If change the alias ""all"", it will run. We need to throw exception saying ""all"" is a keyword.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/May/11 17:17;xuefuz;PIG-2062.patch;https://issues.apache.org/jira/secure/attachment/12479141/PIG-2062.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-13 17:17:01.881,,,no_permission,,,,,,,,,,,,66184,,,,Fri May 13 18:13:31 UTC 2011,,,,,,,0|i0h0sv:,97392,,,,,,,,,,"13/May/11 17:17;xuefuz;Related to PIG-2003, which is fixed, but it appears that we need a more general fix.",13/May/11 17:33;thejas;+1,"13/May/11 17:42;daijy;Now it throws exception. The error message is:
ERROR 1200: <file 100.pig, line 3, column 0>  mismatched input 'all' expecting EOF
Seems still not obvious. 

Can you give some hint of how to improve error message?","13/May/11 17:54;xuefuz;Well, while not perfect, but I think it's good enough. If you really want to say something like ""'all' is a reserved keyword', then you will need to customize the error message for each keyword, which I don't think worthwhile. We had such discussions before. ","13/May/11 18:06;xuefuz;Unit test passed. Test-patch run:

     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 12 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
",13/May/11 18:13;xuefuz;Patch is committed to both trunk and 0.9.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PIG doesn't validate incomplete query in batch mode even if -c option is given,PIG-2059,12506785,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,11/May/11 00:06,04/Aug/11 00:34,14/Mar/19 03:07,13/May/11 18:58,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"Given the following in a file to Pig, pig doesn't report any error, even if -c option is given:

A = load 'x' as (u, v);
B = foreach A generate $3;

It's questionable whether to validate the query in batch mode as it doesn't contain any store/dump statement. However, if -c option is given, validation should be nevertheless performed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/May/11 22:12;xuefuz;PIG-2059-2.patch;https://issues.apache.org/jira/secure/attachment/12479017/PIG-2059-2.patch,13/May/11 17:38;xuefuz;PIG-2059-3.patch;https://issues.apache.org/jira/secure/attachment/12479146/PIG-2059-3.patch,11/May/11 17:41;xuefuz;PIG-2059.patch;https://issues.apache.org/jira/secure/attachment/12478848/PIG-2059.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-05-11 21:27:44.968,,,no_permission,,,,,,,,,,,,66344,,,,Fri May 13 18:58:48 UTC 2011,,,,,,,0|i0h0s7:,97389,,,,,,,,,,"11/May/11 18:15;xuefuz;Test-patch run:

     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
","11/May/11 21:27;daijy;Might be better to change the name of the method. validateQuery->parseAndValidate, compile->validate. Other part looks good.",12/May/11 22:12;xuefuz;Previous patch has some undesirable side-effect. Also included in the patch is fixes for some test cases.,12/May/11 22:19;daijy;+1,"13/May/11 17:38;xuefuz;Same code change, but modified additional test cases to make them pass.","13/May/11 18:55;xuefuz;For patch PIG-2059-3.patch, unit test passed. Test-patch run:

     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 24 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
",13/May/11 18:58;xuefuz;patch PIG-2059-3.patch is committed to both trunk and 0.9.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Macro missing returns clause doesn't give a good error message,PIG-2058,12506774,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,xuefuz,xuefuz,10/May/11 22:10,04/Aug/11 00:35,14/Mar/19 03:07,11/May/11 16:54,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"For the following query:

define test( out1,out2 ){
   A  = load 'x' as (u:int, v:int);
   $B  = filter A by u < 3 and v <  20;
}

Pig gives the following error message: Syntax error,unexpected symbol at or near '{'

Previously, it gives: mismatched input '{' expecting RETURNS

The previous message is more meaningful.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11/May/11 00:52;rding;PIG-2058.patch;https://issues.apache.org/jira/secure/attachment/12478751/PIG-2058.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-11 00:52:15.173,,,no_permission,,,,,,,,,,,,66153,Reviewed,,,Wed May 11 16:54:53 UTC 2011,,,,,,,0|i0h0rz:,97388,,,,,,,,,,"10/May/11 23:35;xuefuz;The problem introduced by RETURN VOID support. Changing the grammar as follows will solve the problem.

macro_return_clause : RETURNS ( ( alias ( COMMA alias )* ) | VOID )
                   -> ^( RETURN_VAL alias* )
",11/May/11 00:52;rding;Thanks Xuefu. Attaching a patch with the fix.,11/May/11 16:29;xuefuz;+1,11/May/11 16:54;rding;Patch committed to trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jython error messages should show script name,PIG-2056,12506758,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,rding,rding,rding,10/May/11 19:06,04/Aug/11 00:35,14/Mar/19 03:07,12/May/11 18:12,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"Instead of messages like

{code}
Traceback (most recent call last):
  File ""<iostream>"", line 12, in <module>
{code}

It should display the script file name:

{code}
Traceback (most recent call last):
  File ""test.py"", line 12, in <module>
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/May/11 20:49;rding;PIG-2056.patch;https://issues.apache.org/jira/secure/attachment/12478731/PIG-2056.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-11 22:54:39.18,,,no_permission,,,,,,,,,,,,66144,Reviewed,,,Thu May 12 18:12:41 UTC 2011,,,,,,,0|i0h0rj:,97386,,,,,,,,,,"11/May/11 18:21;rding;Result of test-patch:

{code}
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{code}",11/May/11 22:54;thejas;+1,12/May/11 18:12;rding;Unit tests pass. Patch committed to trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inconsistent behavior in parser generated during build ,PIG-2055,12506737,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,10/May/11 16:22,23/Jan/12 07:31,14/Mar/19 03:07,22/Aug/11 18:41,0.9.0,,,,,,,0.10.0,0.11,0.9.2,,,,,0,,,,,,,,,,,,,"On certain builds, i see that pig fails to support this syntax -

{code}
grunt> l = load 'x' using PigStorage(':');           
2011-05-10 09:21:41,565 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1200: <line 1, column 29>  mismatched input '(' expecting SEMI_COLON
Details at logfile: /Users/tejas/pig_trunk_cp/trunk/pig_1305044484712.log

{code}

I seem to be the only one who has seen this behavior, and I have seen on occassion when I build on mac. It could be problem with antlr and apple jvm interaction. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Nov/11 13:09;miguno;PIG-2055-for-0.9.1.patch;https://issues.apache.org/jira/secure/attachment/12503075/PIG-2055-for-0.9.1.patch,22/Nov/11 21:58;rding;PIG-2055-for-0.9.1_1.patch;https://issues.apache.org/jira/secure/attachment/12504812/PIG-2055-for-0.9.1_1.patch,07/Dec/11 08:27;daijy;PIG-2055-for-0.9.1_2.patch;https://issues.apache.org/jira/secure/attachment/12506417/PIG-2055-for-0.9.1_2.patch,10/Aug/11 21:16;thejas;PIG-2055.1.patch;https://issues.apache.org/jira/secure/attachment/12490036/PIG-2055.1.patch,11/Aug/11 18:36;thejas;PIG-2055.2.patch;https://issues.apache.org/jira/secure/attachment/12490153/PIG-2055.2.patch,09/Nov/11 13:09;miguno;PIG-2055.for-0.9.0.patch;https://issues.apache.org/jira/secure/attachment/12503074/PIG-2055.for-0.9.0.patch,08/Nov/11 10:19;butlermh;QueryParser_broken.java;https://issues.apache.org/jira/secure/attachment/12502901/QueryParser_broken.java,08/Nov/11 14:52;butlermh;QueryParser_broken_other.java;https://issues.apache.org/jira/secure/attachment/12502921/QueryParser_broken_other.java,08/Nov/11 10:19;butlermh;QueryParser_working.java;https://issues.apache.org/jira/secure/attachment/12502902/QueryParser_working.java,08/Nov/11 18:56;butlermh;antlrWatchConversionOutput.txt;https://issues.apache.org/jira/secure/attachment/12502945/antlrWatchConversionOutput.txt,08/Nov/11 10:19;butlermh;diff.txt;https://issues.apache.org/jira/secure/attachment/12502900/diff.txt,11.0,,,,,,,,,,,,,,,,,,,2011-05-10 19:19:46.14,,,no_permission,,,,,,,,,,,,64168,,,,Wed Dec 07 08:27:48 UTC 2011,,,,,,,0|i0h0rb:,97385,,,,,,,,,,"10/May/11 19:19;daijy;I see that before. Once I do ""ant clean"", the message go away.","10/May/11 19:41;thejas;bq. I see that before. Once I do ""ant clean"", the message go away.
I have seen it even after doing 'ant clean', so it does not seem to be caused by unclean build, but by some non deterministic code generation in antlr. 
","13/May/11 01:10;knoguchi;I hit this as well on my macbook.  It drove me crazy.
Using antlr-3.3 (instead of 3.2) seems to have fixed it for me.","10/Aug/11 21:16;thejas;PIG-2055.1.patch - Uses antlr 3.4 . 3.3 seems to have some changes related to determinism. I had to make some changes to the code as well to make it working. All unit tests pass. 
But I am not sure if this actually fixes the problem , because that is not easily reproducible. I will try compile+test in a loop and see if I can reproduce it.","10/Aug/11 21:23;thejas;The official jar of antlr 3.4 that comes will all the binaries uses stringtemplate.jar version 4.0.4 , but this patch is using version 4.0.2 because that is what is available on maven repository. We should move to 4.0.4 once that is available on maven.
",11/Aug/11 18:36;thejas;PIG-2055.2.patch - the stringtemplate.jar 4.0.4 was released under a different artifact name (ST4). This patch uses the new artifact and version 4.0.4.,"22/Aug/11 17:59;daijy;+1 on the patch. We have a hard time to reproduce the issue, I would suggest to check in the patch, and see people's feedback.","22/Aug/11 18:41;thejas;Patch committed to trunk.
","07/Nov/11 10:50;butlermh;I am seeing this issue as well. I have tried building Pig 0.9.0 and Pig 0.9.1 on RHEL 5 and I see nearly 367 test failures as a result, for example

org.apache.pig.parser.TestErrorHandling.tesNegative5

Error Message

<line 1, column 29>  mismatched input '(' expecting SEMI_COLON
Stacktrace

Failed to parse: <line 1, column 29>  mismatched input '(' expecting SEMI_COLON
	at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:222)
	at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:164)
	at org.apache.pig.PigServer$Graph.validateQuery(PigServer.java:1609)
	at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1582)
	at org.apache.pig.PigServer.registerQuery(PigServer.java:584)
	at org.apache.pig.PigServer.registerQuery(PigServer.java:597)
	at org.apache.pig.parser.TestErrorHandling.tesNegative5(TestErrorHandling.java:105)

People have also reported it on the user list - see 

http://www.mail-archive.com/user@pig.apache.org/msg02288.html
http://www.mail-archive.com/user@pig.apache.org/msg02185.html

I do not understand why I am seeing so many failures yet others have had problems reproducing the issue?

This issue is marked as resolved, but it affects 0.9.0 and the patch is only for trunk? 

I am currently in the process of testing trunk to see if it has the same problem. If trunk is ok, it would make sense to backport the patch to 0.9.0 / 0.9.1.","07/Nov/11 21:44;daijy;Hi, Mark,
Let me know your testing with trunk.","08/Nov/11 10:15;butlermh;Hi Daniel!
Trunk works okay on continuous build server. I have also been able to build a working version of 0.9.0 (on Ubuntu 11.10) and a failing version of 0.9.0 (on RHEL 5). I then diff'd the src-gen/org/apache/pig/parser directories of these two versions. For some reason there are differences in QueryParser.java between the two versions. I enclose the diff and also the two versions of this file.",08/Nov/11 10:19;butlermh;The different versions of QueryParser.java in the working and non-working versions. ,"08/Nov/11 10:29;butlermh;I checked through the code, the differences are in order, not in logic. This does not explain the differences I am seeing. ",08/Nov/11 14:52;butlermh;I found a third variant of this file being generated which does have logical differences to the other two versions. ,"08/Nov/11 18:39;butlermh;Here is someone else with the same issue - with ANTLR, not Pig: http://www.antlr.org/pipermail/antlr-interest/2011-August/042456.html","08/Nov/11 18:56;butlermh;I modified build.xml to generate QueryParser.java with the ANTLR -Xwatchconversion option. This shows it is struggling and timing out with the func_clause rule. Therefore the reason for the non-deterministic behavior we are seeing may not be down to differences in JVMs, it might be down to the performance of different machines. ","08/Nov/11 19:11;daijy;Thanks Mark, that's helpful. It does verify the indeterminism nature of antlr 3.2. 

I would suggest to backport the patch to 0.9 branch. Any objection?",09/Nov/11 13:09;miguno;PIG-2055 patch for Pig 0.9.0.  Kudos to our Mark Butler for the backport.,"09/Nov/11 13:09;miguno;PIG-2055 patch, this one is for Pig 0.9.1.  Again kudos to our Mark Butler for the backport.","22/Nov/11 12:15;fang fang chen;For svn release-0.9.1，the following modification is needed besides #PIG-2055-for-0.9.1.patch:
build.xml: 
654c654
--           <zipfileset src=""${ivy.lib.dir}/stringtemplate-${antlr.version}.jar"" />

++           <zipfileset src=""${ivy.lib.dir}/ST4-${stringtemplate.version}.jar"" />

",22/Nov/11 21:58;rding;Upload new patch for 0.9.1 release. Fixing a couple of issues.,"23/Nov/11 02:28;fang fang chen;Thank you Richard. 
Richard included my modification in the new patch.",05/Dec/11 21:27;daijy;+1 for Richard's patch. Will commit once tests pass.,07/Dec/11 08:27;daijy;PIG-2055-for-0.9.1_2.patch resync with 0.9 branch. Unit tests pass. Patch committed to 0.9 branch as well. Thanks guys!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ship guava.jar to backend,PIG-2052,12506641,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,daijy,daijy,09/May/11 18:12,04/Aug/11 00:35,14/Mar/19 03:07,10/May/11 19:18,0.8.1,0.9.0,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,We need to ship guava.jar to backend. GenericInvoker is using it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/May/11 22:51;daijy;PIG-2052-1.patch;https://issues.apache.org/jira/secure/attachment/12478642/PIG-2052-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-09 19:06:20.438,,,no_permission,,,,,,,,,,,,41696,Reviewed,,,Tue May 10 19:18:05 UTC 2011,,,,,,,0|i0h0qn:,97382,,,,,,,,,,"09/May/11 19:06;dvryaboy;I'd like to pick this one up, planning on doing a bunch of Pig related work tonight anyway.",09/May/11 22:51;daijy;Attach an initial patch for reference.,10/May/11 04:15;dvryaboy;+1,10/May/11 19:18;daijy;Patch committed to trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig should display TokenMgrError message consistently across all parsers,PIG-2049,12506443,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,rding,rding,rding,06/May/11 19:04,04/Aug/11 00:34,14/Mar/19 03:07,06/May/11 22:52,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"For example, for org.apache.pig.tools.pigscript.parser.TokenMgrError, Pig logs

{code}
ERROR 1000: Error during parsing. Lexical error at line 5, column 0.
{code}

But for org.apache.pig.tools.parameters.TokenMgrError, Pig logs

{code}
ERROR 2998: Unhandled internal error. Lexical error at line 10, column 0.
{code}

Both should have error code 1000.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/May/11 20:00;rding;PIG-2049.patch;https://issues.apache.org/jira/secure/attachment/12478465/PIG-2049.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-06 22:34:24.139,,,no_permission,,,,,,,,,,,,66341,Reviewed,,,Fri May 06 22:52:40 UTC 2011,,,,,,,0|i0h0pz:,97379,,,,,,,,,,06/May/11 22:34;thejas;+1,06/May/11 22:52;rding;Patch committed to trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add zookeeper to pig jar,PIG-2048,12506435,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,gbowyer@fastmail.co.uk,gbowyer@fastmail.co.uk,gbowyer@fastmail.co.uk,06/May/11 17:41,26/Apr/12 20:32,14/Mar/19 03:07,12/May/11 19:25,0.9.0,,,,,,,0.10.0,,,,build,,,0,hbase,zookeeper,,,,,,,,,,,"Currently the pig jar does not bundle zookeeper in the same fashion as hbase, guava etc. This means that it is unable to run the HBaseStorage on recent versions of HBase. Attached is a patch that fixes this ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/May/11 17:43;gbowyer@fastmail.co.uk;0001-Added-zookeeper-to-the-pig-phat-jar-to-allow-HBaseSt.patch;https://issues.apache.org/jira/secure/attachment/12478451/0001-Added-zookeeper-to-the-pig-phat-jar-to-allow-HBaseSt.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-12 19:25:42.126,,,no_permission,,,,,,,,,,,,165274,,,,Thu May 12 19:25:42 UTC 2011,,,Patch Available,,,,0|i0h0pr:,97378,,,,,,,,,,12/May/11 19:25;alangates;Patch checked in.  Thanks Greg.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Patten match bug in org.apache.pig.newplan.optimizer.Rule,PIG-2044,12506340,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,daijy,daijy,05/May/11 21:29,26/Apr/12 20:32,14/Mar/19 03:07,14/May/11 00:39,0.9.0,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"Koji find that we have a bug org.apache.pig.newplan.optimizer.Rule. The ""break"" in line 179 seems to be wrong. This multiple branch matching is not used in Pig, but could be a problem for the future. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/May/11 00:46;knoguchi;PIG-2044-00.patch;https://issues.apache.org/jira/secure/attachment/12479045/PIG-2044-00.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-13 00:46:44.826,,,no_permission,,,,,,,,,,,,165270,,,,Sat May 14 00:39:00 UTC 2011,,,,,,,0|i0h0ov:,97374,,,,,,,,,,"10/May/11 22:57;daijy;Unlink to 0.9. It is a potential bug, but currently we are not using this ability.",13/May/11 00:46;knoguchi;Taking out 'break' statement which made the for-loop meaningless.  Added one test. ,"14/May/11 00:04;daijy;+1, will commit if tests pass.","14/May/11 00:39;daijy;Test-patch:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Patch commit to trunk. Thanks Koji!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ship antlr-runtime.jar to backend,PIG-2043,12506319,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,05/May/11 18:07,04/Aug/11 00:34,14/Mar/19 03:07,06/May/11 23:56,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"Following the discussion in PIG-2040, we want to make getSchemaFromString work in the backend, so we need to ship antlr-runtime.jar. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/May/11 23:34;daijy;PIG-2043-1.patch;https://issues.apache.org/jira/secure/attachment/12478348/PIG-2043-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,66183,,,,Fri May 06 23:56:50 UTC 2011,,,,,,,0|i0h0on:,97373,,,,,,,,,,"05/May/11 23:34;daijy;Tested manually using PigStorageSchema, which make use of getSchemaFromString.",06/May/11 23:56;daijy;Trivial change. Committed to trunk and 0.9.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minicluster should make each run independent,PIG-2041,12506240,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,05/May/11 01:38,04/Aug/11 00:34,14/Mar/19 03:07,05/May/11 20:11,0.8.0,0.9.0,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"Minicluster will reuse ~/pigtest/conf/hadoop-site.xml. If something wrong in hadoop-site.xml, next test will also be affected. This leads to some mysterious test failures. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/May/11 01:39;daijy;PIG-2041-1.patch;https://issues.apache.org/jira/secure/attachment/12478239/PIG-2041-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-05 18:33:25.543,,,no_permission,,,,,,,,,,,,66347,Reviewed,,,Thu May 05 20:11:16 UTC 2011,,,,,,,0|i05ifj:,30083,,,,,,,,,,05/May/11 01:39;daijy;PIG-2041-1.patch also include a change to fix TestStoreInstances failure.,"05/May/11 18:02;daijy;Test-patch:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Unit test:
    all pass",05/May/11 18:33;rding;+1,05/May/11 20:11;daijy;Patch committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move classloader from QueryParserDriver to PigContext,PIG-2040,12506230,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,04/May/11 21:47,04/Aug/11 00:34,14/Mar/19 03:07,05/May/11 20:09,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"After PIG-1775, mapreduce mode fail. The reason is we move classloader from LogicalPlanBuilder to QueryParserDriver, which will need antlr.jar, however, we don't ship antlr.jar to backend. It is better to move classloader to PigContext.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/May/11 21:48;daijy;PIG-2040-1.patch;https://issues.apache.org/jira/secure/attachment/12478215/PIG-2040-1.patch,05/May/11 20:06;daijy;PIG-2040-2.patch;https://issues.apache.org/jira/secure/attachment/12478320/PIG-2040-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-05-04 23:57:57.424,,,no_permission,,,,,,,,,,,,66190,Reviewed,,,Thu May 05 20:09:31 UTC 2011,,,,,,,0|i0h0o7:,97371,,,,,,,,,,04/May/11 23:57;xuefuz;+1,05/May/11 01:11;sms;I remember a user trying to parse schemas using the Util.parseFromString() method in an UDF. This might require shipping the antlr binaries to the back-end. Will this patch address this issue too?,"05/May/11 01:45;daijy;bq. I remember a user trying to parse schemas using the Util.parseFromString() method in an UDF. This might require shipping the antlr binaries to the back-end. Will this patch address this issue too?

No, it will not. Seems we need to ship antlr.jar as well. But still put classloader into PigContext is more clear. Also I wonder how Util.parseFromString() works now? We don't ship javacc as well.",05/May/11 01:51;sms;It did not work back then because javacc was not shipped. Should we consider shipping the parser binaries to allow users to take advantage of utility methods? This can be tracked as a separate issue.,"05/May/11 02:23;daijy;Yes, definitely. Actually javacc works since javacc does not need runtime support. ","05/May/11 19:51;daijy;Test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
No new unit test added. This only happens when test in real cluster.

Unit-test:
    all pass",05/May/11 20:09;daijy;Patch committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexOutOfBounException for a case,PIG-2039,12506214,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,04/May/11 19:38,04/Aug/11 00:34,14/Mar/19 03:07,10/May/11 20:21,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"The following query gives an exception:

a = load '1.txt' as (a0:int, a1:int, a2:int);
b = group a by a0;
c = foreach b { c1 = limit a 10; c2 =  distinct c1.a1; c3 = distinct c1.a2; generate c2, c3;};
store c into 'output';

2011-05-04 12:36:01,720 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2999: Unexpected internal error. Index: 0, Size: 0

Stack trace:

java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
        at java.util.ArrayList.RangeCheck(ArrayList.java:547)
        at java.util.ArrayList.get(ArrayList.java:322)
        at org.apache.pig.newplan.logical.expression.ProjectExpression.getFieldSchema(ProjectExpression.java:279)
        at org.apache.pig.newplan.logical.relational.LOGenerate.getSchema(LOGenerate.java:88)
        at org.apache.pig.newplan.logical.visitor.SchemaAliasVisitor.validate(SchemaAliasVisitor.java:60)
        at org.apache.pig.newplan.logical.visitor.SchemaAliasVisitor.visit(SchemaAliasVisitor.java:104)
        at org.apache.pig.newplan.logical.relational.LOGenerate.accept(LOGenerate.java:240)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.newplan.logical.visitor.SchemaAliasVisitor.visit(SchemaAliasVisitor.java:99)
        at org.apache.pig.newplan.logical.relational.LOForEach.accept(LOForEach.java:73)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.PigServer$Graph.compile(PigServer.java:1664)
        at org.apache.pig.PigServer$Graph.validateQuery(PigServer.java:1615)
        at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1586)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:580)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:930)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:386)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:176)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:152)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:76)
        at org.apache.pig.Main.run(Main.java:488)
        at org.apache.pig.Main.main(Main.java:109)

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/May/11 18:48;xuefuz;PIG-2039.patch;https://issues.apache.org/jira/secure/attachment/12478630/PIG-2039.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-10 18:50:37.249,,,no_permission,,,,,,,,,,,,66292,,,,Tue May 10 20:21:49 UTC 2011,,,,,,,0|i0h0nz:,97370,,,,,,,,,,09/May/11 18:48;xuefuz;Unit test passed.,"09/May/11 19:58;xuefuz;Test-patch run:

     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
",10/May/11 18:50;daijy;+1,10/May/11 20:21;xuefuz;Patch PIG-2039.patch is committed into both trunk and 0.9.0.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig fails to parse empty tuple/map/bag constant,PIG-2038,12506212,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,04/May/11 19:30,04/Aug/11 00:34,14/Mar/19 03:07,10/May/11 00:10,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"Pig fails to parse the following query:

a = foreach (load 'b') generate ();
store a into 'output';

Error msg: Failed to parse: null

Similar problem occurs for empty bag/map constant.",,,,,,,,,,,,,,,,,,,PIG-1387,,,,,,,,,,,,,06/May/11 17:18;xuefuz;PIG-2038.patch;https://issues.apache.org/jira/secure/attachment/12478449/PIG-2038.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-04 19:34:08.121,,,no_permission,,,,,,,,,,,,66187,,,,Tue May 10 00:05:36 UTC 2011,,,,,,,0|i0h0nr:,97369,,,,,,,,,,"04/May/11 19:34;dvryaboy;This is related to PIG-1387 which proposes {}, (), [] to be sugar for calling toTuple, toBag, ToMap.","09/May/11 20:32;xuefuz;Unit test passed. Test-patch run:

     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
",09/May/11 23:24;thejas;+1,10/May/11 00:05;xuefuz;Patch is committed to both trunk and 0.9.0.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Macro expansion doesn't handle multiple expansions of same macro inside another macro,PIG-2035,12506199,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,04/May/11 17:33,04/Aug/11 00:34,14/Mar/19 03:07,11/May/11 00:34,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"Here is the use case:

{code}
define test ( in, out, x ) returns c { 
    a = load '$in' as (name, age, gpa);
    b = group a by gpa;
    $c = foreach b generate group, COUNT(a.$x);
    store $c into '$out';
};

define test2( in, out ) returns x { 
    $x = test( '$in', '$out', 'name' );
    $x = test( '$in', '$out.1', 'age' );
    $x = test( '$in', '$out.2', 'gpa' );
};

x = test2('studenttab10k', 'myoutput');
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/May/11 19:48;rding;PIG-2035_1.patch;https://issues.apache.org/jira/secure/attachment/12478204/PIG-2035_1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-10 23:58:57.115,,,no_permission,,,,,,,,,,,,66351,Reviewed,,,Wed May 11 00:34:08 UTC 2011,,,,,,,0|i0h0n3:,97366,,,,,,,,,,"04/May/11 23:27;rding;test-patch result:

{code}
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 585 release audit warnings (more than the trunk's current 584 warnings).
{code}

",06/May/11 16:56;rding;Unit tests pass.,10/May/11 23:58;daijy;+1,11/May/11 00:34;rding;Patch committed to trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig returns sucess for the failed Pig script,PIG-2033,12506118,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,03/May/11 23:08,04/Aug/11 00:35,14/Mar/19 03:07,07/May/11 00:14,0.8.0,,,,,,,0.8.1,0.9.0,,,impl,,,0,,,,,,,,,,,,,"Pig returns success when a Pig script fails but the count of failed MR jobs is zero. 



",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/May/11 22:09;rding;PIG-2033.patch;https://issues.apache.org/jira/secure/attachment/12478336/PIG-2033.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-06 23:01:31.391,,,no_permission,,,,,,,,,,,,66157,Reviewed,,,Sat May 07 00:14:31 UTC 2011,,,,,,,0|i0h0mv:,97365,,,,,,,,,,"05/May/11 22:09;rding;We make sure that Pig returns success iff the number of successfully jobs equal the number of compiled jobs.

This patch doesn't include a unit test since it's difficult to simulate the failure case.",06/May/11 23:01;daijy;+1,"07/May/11 00:14;rding;Unit tests pass on 0.8 branch. Patch committed to 0.8 branch, 0.9 branch and trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Merged join/cogroup does not automatically ship loader,PIG-2030,12506003,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,03/May/11 01:25,04/Aug/11 00:35,14/Mar/19 03:07,10/May/11 20:47,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"The following script fail due to TableLoader class not found (If the jar is in classpath):
{code}
a = load '/user/pig/tests/data/zebra/singlefile/studentsortedtab10k' using org.apache.hadoop.zebra.pig.TableLoader('', 'sorted');
b = load '/user/pig/tests/data/zebra/singlefile/votersortedtab10k' using org.apache.hadoop.zebra.pig.TableLoader('', 'sorted');
g = cogroup a by $0, b by $0 using 'merge';
store g into '/user/pig/out/jianyong.1304374720/ZebraMapCogrp_1.out';
{code}

If we use register, the error goes away. However, Pig always ship jars containing LoadFunc automatically. It should be the same for merged cogroup/join.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/May/11 01:25;daijy;PIG-2030-1.patch;https://issues.apache.org/jira/secure/attachment/12478010/PIG-2030-1.patch,05/May/11 22:24;daijy;PIG-2030-2.patch;https://issues.apache.org/jira/secure/attachment/12478340/PIG-2030-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-05-10 20:25:23.397,,,no_permission,,,,,,,,,,,,66133,Reviewed,,,Tue May 10 20:47:29 UTC 2011,,,,,,,0|i0h0m7:,97362,,,,,,,,,,05/May/11 22:24;daijy;PIG-2030-2.patch rebase with current trunk.,"05/May/11 23:15;daijy;Test-patch:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Unit-test:
    all pass","10/May/11 20:25;ashutoshc;+1.

For documentation purposes: After this user need not to register their jars (for loadfuncs/udfs) if they are already in the classpath. But if they have some dependency on which their supplied loadfuncs/udfs depend, they need to register that dependecy. Else, they can bundle all their dependencies in one jar and put it in the classpath.  ",10/May/11 20:47;daijy;Patch committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistency in Pig Stats reports ,PIG-2029,12505996,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,viraj,viraj,03/May/11 00:26,04/Aug/11 00:35,14/Mar/19 03:07,20/May/11 19:24,0.8.1,0.9.0,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"I have a Pig script which reports varying Stats for the same M/R job (same inputs). Sometimes the PigStats reports all the stats (such as Maps,Reduces,MaxMapTime,MinMapTime,AvgMapTime,MaxReduceTime, MinReduceTime and AvgReduceTime) for the M/R job as 0. Sometimes it reports it correctly.

Enclosed are the stderr logs for 2 runs, you can notice that for Run 1 job_201103091134_556600 from Run 1; has 0 against all the columns whereas in Run 2, Hadoop job job_201104272229_75693 has some valid values. 

The actual Job Tracker link shows that they are non empty. This points to a bug in the interaction of the PigStats module with the Jobtracker.

Run 1:
{quote}
Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTIme	AvgMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	Alias	Feature	Outputs
job_201103091134_556458	160	100	552	191	368	1257	371	392	IN,SP10P,SP11P,SP12P,SP13P,SP16P,SP17P,SP18P,SP20P,SP21P,SP22P,SP23P,SP24P,SP26P,SP27P,SP28P,SP29P,SP30P,SP31P,SP32P,SP33P,SP34P,SP4P,SP6P,SP7P,SP8P,SP9P	DISTINCT,MULTI_QUERY	
job_201103091134_556600	0	0	0	0	0	0	0	0	UNION5	    MULTI_QUERY,MAP_ONLY	/user/viraj/dir,,
job_201103091134_556601	7	100	17	8	14	200	15	27	CNJOIN25,GNJOIN25,sampleNJOIN25	GROUP_BY,COMBINER	
job_201103091134_556602	0	0	0	0	0	0	0	0	CNJOIN3,GNJOIN3,sampleNJOIN3	GROUP_BY,COMBINER	
job_201103091134_556603	0	0	0	0	0	0	0	0	CNJOIN15,GNJOIN15,sampleNJOIN15	GROUP_BY,COMBINER	
job_201103091134_556604	2	100	13	7	10	34	13	31	CNJOIN19,GNJOIN19,sampleNJOIN19	GROUP_BY,COMBINER	
job_201103091134_556644	0	0	0	0	0	0	0	0	ONJOIN15	SAMPLER	
job_201103091134_556645	0	0	0	0	0	0	0	0	ONJOIN25	SAMPLER	
job_201103091134_556646	0	0	0	0	0	0	0	0	ONJOIN3	SAMPLER	
job_201103091134_556654	0	0	0	0	0	0	0	0	ONJOIN19	SAMPLER	
job_201103091134_556662	0	0	0	0	0	0	0	0	ONJOIN19	ORDER_BY,COMBINER
..
{quote}


Run 2:
{quote}

Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTIme	AvgMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	Alias	Feature	Outputs
job_201104272229_75503	159	100	484	192	353	396	308	321	IN,SP10P,SP11P,SP12P,SP13P,SP16P,SP17P,SP18P,SP20P,SP21P,SP22P,SP23P,SP24P,SP26P,SP27P,SP28P,SP29P,SP30P,SP31P,SP32P,SP33P,SP34P,SP4P,SP6P,SP7P,SP8P,SP9P	DISTINCT,MULTI_QUERY	
job_201104272229_75693	18	0	31	14	24	0	0	            UNION5	   MULTI_QUERY,MAP_ONLY	/user/viraj/dir,
job_201104272229_75694	7	100	34	13	22	46	20	25	CNJOIN25,GNJOIN25,sampleNJOIN25	GROUP_BY,COMBINER	
job_201104272229_75695	125	100	19	11	15	32	18	26	CNJOIN3,GNJOIN3,sampleNJOIN3	GROUP_BY,COMBINER	
job_201104272229_75698	1	100	12	12	12	13	9	11	CNJOIN15,GNJOIN15,sampleNJOIN15	GROUP_BY,COMBINER	
job_201104272229_75702	2	100	21	5	13	35	22	26	CNJOIN19,GNJOIN19,sampleNJOIN19	GROUP_BY,COMBINER	
job_201104272229_75724	1	1	4	4	4	11	11	11	ONJOIN15	SAMPLER	
job_201104272229_75725	0	0	0	0	0	0	0	            ONJOIN25	SAMPLER	
job_201104272229_75726	6	1	8	6	8	24	24	24	ONJOIN3	SAMPLER	
job_201104272229_75729	0	0	0	0	0	0	0	            ONJOIN19	SAMPLER	
job_201104272229_75752	1	100	5	5	5	12	9	11	ONJOIN19	ORDER_BY,COMBINER
..
{quote}

Viraj",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/May/11 20:08;rding;PIG-2029.patch;https://issues.apache.org/jira/secure/attachment/12479504/PIG-2029.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-03 16:27:21.51,,,no_permission,,,,,,,,,,,,66180,Reviewed,,,Thu May 19 19:45:27 UTC 2011,,,,,,,0|i0h0lz:,97361,,,,,,,,,,"03/May/11 16:27;olgan;I do not believe it is a P1 issue so don't think it belongs on 0.8 branch. Even for 0.9 I do not see it as a blocker. If we can find a quick reproducible case, we will fix it in 0.9. Otherwise will delay till we can reproduce. Also, this could be a potential issue with Hadoop.","17/May/11 19:58;rding;Currently Pig prints out zero (0) if max/min/avg map/reduce time isn't available by querying hadoop using hadoop client API. This is misleading. I propose that we change those values to 'n/a' as following:

{code}
Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTIme	AvgMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	Alias	Feature	Outputs
job_201104272229_434232	2	10	354	220	287	168	149	163	IN,SP10P,SP11P,SP12P,SP13P,SP16P,SP17P,SP18P,SP20P,SP21P,SP22P,SP23P,SP24P,SP26P,SP27P,SP28P,SP29P,SP30P,SP31P,SP32P,SP33P,SP34P,SP4P,SP6P,SP7P,SP8P,SP9P	DISTINCT,MULTI_QUERY	
job_201104272229_434319	2	0	9	3	6	0	0	0	UNION5	MULTI_QUERY,MAP_ONLY	/user/rding/verifypigstats2-UNION5,
job_201104272229_434320	2	10	n/a	n/a	n/a	n/a	n/a	n/a	CNJOIN3,GNJOIN3,sampleNJOIN3	GROUP_BY,COMBINER	
job_201104272229_434321	1	10	5	5	5	23	9	17	CNJOIN25,GNJOIN25,sampleNJOIN25	GROUP_BY,COMBINER	
job_201104272229_434322	2	10	n/a	n/a	n/a	n/a	n/a	n/a	CNJOIN15,GNJOIN15,sampleNJOIN15	GROUP_BY,COMBINER	
job_201104272229_434323	2	10	n/a	n/a	n/a	n/a	n/a	n/a	CNJOIN19,GNJOIN19,sampleNJOIN19	GROUP_BY,COMBINER	
job_201104272229_434331	2	1	n/a	n/a	n/a	n/a	n/a	n/a	ONJOIN15	SAMPLER	
job_201104272229_434332	2	1	n/a	n/a	n/a	n/a	n/a	n/a	ONJOIN3	SAMPLER	
job_201104272229_434333	1	1	2	2	2	13	13	13	ONJOIN25	SAMPLER	
job_201104272229_434334	1	1	1	1	1	12	12	12	ONJOIN19	SAMPLER	
job_201104272229_434342	1	10	2	2	2	16	8	11	ONJOIN25	ORDER_BY,COMBINER	
{code}",19/May/11 17:37;thejas;+1,19/May/11 19:45;rding;Patch committed to trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Speed up multiquery unit tests ,PIG-2028,12505986,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,02/May/11 22:37,04/Aug/11 00:35,14/Mar/19 03:07,04/May/11 00:12,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"Switch TestMultiQueryBasic and TestMultiQuery to use LOCAL mode. The results on my laptop:

Using Mini Cluster:

TestMultiQueryBasic: 17 min 17 sec
TestMultiQuery:      23 min 2 sec

Using LOCAL mode:

TestMultiQueryBasic: 4 min 17 sec
TestMultiQuery:      5 min 51 sec


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/May/11 23:07;rding;PIG-2028.patch;https://issues.apache.org/jira/secure/attachment/12477999/PIG-2028.patch,03/May/11 22:40;rding;PIG-2028_1.patch;https://issues.apache.org/jira/secure/attachment/12478101/PIG-2028_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-05-03 00:08:14.615,,,no_permission,,,,,,,,,,,,66177,Reviewed,,,Wed May 04 00:12:27 UTC 2011,,,,,,,0|i0h0lr:,97360,,,,,,,,,,"03/May/11 00:08;dvryaboy;Nice speed improvement!
A nitpick: if the test is interrupted, it will leave test files in the working directory; that's messy and causes failures on reruns (since the files already exist). It's better to write to and load from a temp directory created inside java.io.tmpdir, and to set deleteOnExit() on them.",03/May/11 22:41;rding;Simplify the test cases. Using Util.createLocalInputFile whenever possible.,03/May/11 23:45;thejas;+1,04/May/11 00:12;rding;Path committed to trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE if Pig don't have permission for log file,PIG-2027,12505974,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,daijy,daijy,daijy,02/May/11 20:18,26/Apr/12 20:32,14/Mar/19 03:07,20/Jul/11 17:14,,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"If specify a log file to Pig, but Pig don't have write permission, if any failure in Pig script, we will get a NPE in addition to Pig script failure:

2011-05-02 13:18:36,493 [main] ERROR org.apache.pig.tools.grunt.Grunt - java.lang.NullPointerException
        at org.apache.pig.impl.util.LogUtils.writeLog(LogUtils.java:172)
        at org.apache.pig.impl.util.LogUtils.writeLog(LogUtils.java:79)
        at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:131)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:180)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:152)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:90)
        at org.apache.pig.Main.run(Main.java:554)
        at org.apache.pig.Main.main(Main.java:109)

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/May/11 20:25;daijy;PIG-2027-1.patch;https://issues.apache.org/jira/secure/attachment/12477983/PIG-2027-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-15 22:09:34.867,,,no_permission,,,,,,,,,,,,67336,Reviewed,,,Wed Jul 20 17:14:25 UTC 2011,,,,,,,0|i0h0lj:,97359,,,,,,,,,,"05/May/11 21:12;daijy;     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Unit-test:
    all pass

Manual test:
    tested when --l point to a file which don't have write permission",15/Jul/11 22:09;alangates;Marking as submitpatch so it can get reviewed and committed.,18/Jul/11 05:13;thejas;+1,20/Jul/11 17:14;daijy;Patch committed to trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e2e tests in eclipse classpath,PIG-2026,12505931,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,azaroth,azaroth,azaroth,02/May/11 13:47,26/Apr/12 20:32,14/Mar/19 03:07,05/May/11 20:16,,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,e2e tests under test/e2e/pig/udfs/java should have their own entry as a source dir in eclipse .classpath,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/May/11 18:50;azaroth;PIG-2026.patch;https://issues.apache.org/jira/secure/attachment/12478309/PIG-2026.patch,02/May/11 13:47;azaroth;PIG-2026.patch;https://issues.apache.org/jira/secure/attachment/12477954/PIG-2026.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-05-05 18:44:45.922,,,no_permission,,,,,,,,,,,,165267,,,,Thu May 05 20:16:25 UTC 2011,,,,,,,0|i0h0lb:,97358,,,,,,,,,,05/May/11 18:44;ashutoshc;Looks like this patch contains PIG-2025 and thus conflicting.,"05/May/11 18:50;azaroth;Yes, I made a mistake generating the patch, sorry.
Here is the correct one.","05/May/11 20:16;ashutoshc;Patch committed. Thanks, Gianmarco!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.pig.test.udf.evalfunc.TOMAP is missing package declaration,PIG-2025,12505927,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,azaroth,azaroth,azaroth,02/May/11 13:30,26/Apr/12 20:32,14/Mar/19 03:07,02/May/11 22:12,,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/May/11 13:37;azaroth;PIG-2025.patch;https://issues.apache.org/jira/secure/attachment/12477949/PIG-2025.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-02 22:12:50.903,,,no_permission,,,,,,,,,,,,165266,,,,Mon May 02 22:12:50 UTC 2011,,,,,,,0|i0h0l3:,97357,,,,,,,,,,"02/May/11 22:12;alangates;Patch checked in, thanks Gianmarco.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect jar paths in .classpath template for eclipse,PIG-2024,12505920,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,azaroth,azaroth,azaroth,02/May/11 11:47,26/Apr/12 20:33,14/Mar/19 03:07,05/May/11 18:43,,,,,,,,0.10.0,,,,,,,0,,,,,,,,,,,,,"The jars listed in .eclipse.templates/.classpath are outdated.
Importing the project in eclipse after using ant eclipse-files generates build path errors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/May/11 11:48;azaroth;PIG-2024.patch;https://issues.apache.org/jira/secure/attachment/12477944/PIG-2024.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-05 18:16:53.978,,,no_permission,,,,,,,,,,,,71420,,,,Thu May 05 18:43:20 UTC 2011,,,Patch Available,,,,0|i0h0kv:,97356,,,,,,,,,,"05/May/11 18:16;ashutoshc;Patch looks good. To make Pig more eclipse friendly, we need to tweak .classpath further to accomodate recently introduced e2e test folder. Something of the effect of the following:
{code}
-<classpathentry kind=""src"" path=""test""/>
+<classpathentry excluding=""e2e/|e2e/pig/udfs/java/|e2e/pig/udfs/java/"" including=""**"" kind=""src"" path=""test""/>
+<classpathentry kind=""src"" path=""test/e2e/pig/udfs/java""/>
{code}
Gianmarco, Can you roll this in your patch.
","05/May/11 18:27;azaroth;Hi Ashutosh,
I noticed the issue with e2e, and I had already opened a different jira for that, PIG-2026.
The patch there does exactly what you suggest.","05/May/11 18:43;ashutoshc;Great. I will take a look at that one. 
Patch committed. Thanks, Gianmarco!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parser error while referring a map nested foreach,PIG-2021,12505755,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,vivekp,vivekp,29/Apr/11 05:27,04/Aug/11 00:35,14/Mar/19 03:07,10/May/11 21:02,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"The below script is throwing parser errors
{code}
register string.jar;
A = load 'test1'  using MapLoader() as ( s, m, l );                       
B = foreach A generate *, string.URLPARSE((chararray) s#'url') as parsedurl;
C = foreach B {
  urlpath = (chararray) parsedurl#'path';
  lc_urlpath = string.TOLOWERCASE((chararray) urlpath);
  generate *;
};
{code}

Error message;
| Failed to generate logical plan.
|Nested exception: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2225: Projection with nothing to reference!



PIG-2002 reports a similar issue, but when i tried with the patch of PIG-2002 i was getting the below exception;
 ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1200: <file repro.pig, line 11, column 33>  mismatched input '(' expecting SEMI_COLON
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-05-08 06:07:44.028,,,no_permission,,,,,,,,,,,,66110,,,,Tue May 10 21:02:23 UTC 2011,,,,,,,0|i0h0k7:,97353,,,,,,,,,,08/May/11 06:07;xuefuz;Could you please attach your string.jar file here so as for me to reproduce the problem? Thanks.,"09/May/11 11:01;vivekp;Attaching a script avoiding all the dependencies;

{code}
A = load 'temp' as ( s, m, l );
B = foreach A generate *, LOWER((chararray) s#'url') as parsedurl;
C = foreach B {
  urlpath = (chararray) parsedurl#'path';
  lc_urlpath = org.apache.pig.piggybank.evaluation.string.Reverse((chararray) urlpath);
  generate *;
};
{code}","09/May/11 20:38;xuefuz;The above test case seems having a few problems:

1. LOWER() returns a string, so parsedurl is a string. However, it's later used as a map. Such conversion is invalid.

2. generate * will output input, so the nested commands in the second foreach is useless.

3. with latest in the trunk, the above query parses without problem.

grunt> A = load 'temp' as ( s, m, l );
grunt> B = foreach A generate *, LOWER((chararray) s#'url') as parsedurl;
2011-05-09 13:37:04,796 [main] WARN  org.apache.pig.PigServer - Encountered Warning IMPLICIT_CAST_TO_MAP 1 time(s).
grunt> C = foreach B {
>>   urlpath = (chararray) parsedurl#'path';
>>   lc_urlpath = org.apache.pig.piggybank.evaluation.string.Reverse((chararray) urlpath);
>>   generate *;
>> };
2011-05-09 13:37:06,315 [main] WARN  org.apache.pig.PigServer - Encountered Warning IMPLICIT_CAST_TO_MAP 1 time(s).
grunt> describe C;
2011-05-09 13:37:10,676 [main] WARN  org.apache.pig.PigServer - Encountered Warning IMPLICIT_CAST_TO_MAP 1 time(s).
C: {s: bytearray,m: bytearray,l: bytearray,parsedurl: chararray}

Please provide a valid case.","10/May/11 10:14;vivekp;Hi Xuefu,
 Extremely sorry about that.I was just trying to remove the dependencies.Please check whether the below is a valid case;

{code}
register mymapudf.jar;
A = load 'temp' as ( s, m, l );
B = foreach A generate *, org.vivek.udfs.mToMapUDF((chararray) s) as mapout;
C = foreach B {
  urlpath = (chararray) mapout#'k1';
  lc_urlpath = org.vivek.udfs.LOWER((chararray) urlpath);
  generate urlpath,lc_urlpath;
};
{code}


Source for org.vivek.udfs.mToMapUDF
{code}
package org.vivek.udfs;
import java.io.IOException;
import java.util.HashMap;
import java.util.Map;
import org.apache.pig.EvalFunc;
import org.apache.pig.data.DataType;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
public class mToMapUDF  extends EvalFunc<Map> {
	public Map<String, Object> exec(Tuple arg0) throws IOException {
		Map <String,Object> myMapTResult =  new HashMap<String, Object>();
		myMapTResult.put(""k1"", ""SomeString"");
		myMapTResult.put(""k3"", ""SomeOtherString"");
		return myMapTResult;
	}
	public Schema outputSchema(Schema input) {
		return new Schema(new Schema.FieldSchema(""mapout"",DataType.MAP));
	}
}
{code}



Source for org.vivek.udfs.LOWER
{code}
package org.vivek.udfs;
import java.io.IOException;
import java.util.List;
import java.util.ArrayList;
import org.apache.pig.EvalFunc;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.DataType;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.FuncSpec;
public class LOWER extends EvalFunc<String> {
    public String exec(Tuple input) throws IOException {
        if (input == null || input.size() == 0)
            return null;
        try {
            String str = (String)input.get(0);
            return str.toLowerCase();
        } catch(Exception e){
            return null;
        }
    }
    public Schema outputSchema(Schema input) {
        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.CHARARRAY));
    }
     public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
        funcList.add(new FuncSpec(this.getClass().getName(), new Schema(new Schema.FieldSchema(null, DataType.CHARARRAY))));
        return funcList;
     }
}
{code}

","10/May/11 20:41;xuefuz;Hi Vivek,

This morning we found that there was a little disparity between 0.9 and trunk regarding this fix. Yes, you would still have this problem in 0.9, but with the latest checkin, the problem should have been addressed. Let me know if you found that this is not the case.

--Xuefu","10/May/11 21:02;olgan;Vivek, please, re-open if the issue still happens with the latest pig 0.9 code.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smoketest-jar target has to depend on pigunit-jar to guarantee inclusion of test classes,PIG-2019,12505422,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cos,cos,cos,28/Apr/11 14:39,26/Apr/12 20:32,14/Mar/19 03:07,02/May/11 20:20,0.9.0,,,,,,,0.10.0,,,,build,,,0,,,,,,,,,,,,,"pigsmoke artifact uses classes from pigunit for real cluster testing. However, in the deployment phase the artifact comes out without the classes added. It happens because of the execution order of targets during deployment.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Apr/11 14:44;cos;PIG-2019.patch;https://issues.apache.org/jira/secure/attachment/12477659/PIG-2019.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-02 18:57:33.299,,,no_permission,,,,,,,,,,,,165263,,,,Mon May 02 20:20:46 UTC 2011,,,,,,,0|i0h0jr:,97351,,,,,,,,,,28/Apr/11 14:43;cos;The easiest way to guarantee the presence of the compiled classes is to add a dependency to smoketests-jar target (see the patch).,"02/May/11 18:57;alangates;When I run ant smoketests-jar now, the resulting smoketests.jar contains:

{code}
META-INF/
META-INF/MANIFEST.MF
org/
org/apache/
org/apache/pig/
org/apache/pig/test/
org/apache/pig/test/pigunit/
org/apache/pig/test/pigunit/pig/
org/apache/pig/test/pigunit/TestPigTest.class
org/apache/pig/test/pigunit/pig/TestGruntParser$1.class
org/apache/pig/test/pigunit/pig/TestGruntParser.class
test/
test/data/
test/data/pigunit/
test/data/pigunit/top_queries.pig
test/data/pigunit/top_queries_expected_top_3.txt
test/data/pigunit/top_queries_input_data.txt
test/data/pigunit/top_queries_params.txt
{code}

Does that look correct? That seems a little thin.  Smoketest just runs the PigUnit tests?","02/May/11 19:13;cos;It seems to be right unless there any other tests which could be run against a real cluster that I missing. I thought that PigUnit was a framework right for this purpose, wasn't it?

Please correct me if I am wrong and I'd fix the patch accordingly.",02/May/11 20:07;alangates;I'm not saying it's the wrong framework.  I just wanted to make sure this wasn't supposed to include some actual tests as well.,"02/May/11 20:19;cos;Right, I understand. I wanted to make sure that I am not missing any tests which can be run against a real cluster (e.g. smoke tests). Thanks.",02/May/11 20:20;alangates;Patch checked in.  Thanks Cos.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE for co-group with group-by column having complex schema and different load functions for each input,PIG-2018,12505365,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,27/Apr/11 23:30,04/Aug/11 00:34,14/Mar/19 03:07,30/Apr/11 00:13,,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"{code}
l1 = load 'x' using PigStorage(':') as (a : (i : int),b,c);
l2 = load 'x' as (a,b,c);
cg = cogroup l1 by a, l2 by a;
explain cg;

Gives -
ERROR 1067: Unable to explain alias cg

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1067: Unable to explain alias cg
        at org.apache.pig.PigServer.explain(PigServer.java:1075)
        at org.apache.pig.tools.grunt.GruntParser.explainCurrentBatch(GruntParser.java:381)
        at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:313)
        at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:276)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.Explain(PigScriptParser.java:665)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:325)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:176)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:152)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:90)
        at org.apache.pig.Main.run(Main.java:554)
        at org.apache.pig.Main.main(Main.java:109)
Caused by: java.lang.NullPointerException
        at org.apache.pig.newplan.logical.visitor.LineageFindRelVisitor.mapMatchLoadFuncToUid(LineageFindRelVisitor.java:528)
        at org.apache.pig.newplan.logical.visitor.LineageFindRelVisitor.visit(LineageFindRelVisitor.java:287)
        at org.apache.pig.newplan.logical.relational.LOCogroup.accept(LOCogroup.java:235)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.newplan.logical.visitor.CastLineageSetter.<init>(CastLineageSetter.java:57)
        at org.apache.pig.PigServer$Graph.compile(PigServer.java:1683)
        at org.apache.pig.PigServer$Graph.compile(PigServer.java:1659)
        at org.apache.pig.PigServer$Graph.access$200(PigServer.java:1389)
        at org.apache.pig.PigServer.buildStorePlan(PigServer.java:1277)
        at org.apache.pig.PigServer.explain(PigServer.java:1038)
        ... 10 more

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Apr/11 00:03;thejas;PIG-2018.1.patch;https://issues.apache.org/jira/secure/attachment/12477596/PIG-2018.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-29 21:29:45.1,,,no_permission,,,,,,,,,,,,66323,,,,Sat Apr 30 00:13:46 UTC 2011,,,,,,,0|i0h0jj:,97350,,,,,,,,,,"29/Apr/11 15:21;thejas;unit tests and test-patch  passed for PIG-2018.1.patch .
",29/Apr/11 21:29;daijy;+1,30/Apr/11 00:13;thejas;Patch committed to 0.9 branch and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
consumeMap() fails with EmptyStackException,PIG-2017,12505265,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thedatachef,thedatachef,thedatachef,27/Apr/11 02:58,04/Aug/11 00:34,14/Mar/19 03:07,29/Apr/11 21:04,,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"If a map is read in its serialized form, eg: [key#value], then the consumeMap() method of Utf8StorageConverter fails for the following maps:

{code:none}
[a#)]
[a#}]
[a#""take a look at my lovely curly brace, }""]
[a#'oh look, a closed parenthesis! )']
{code}

There are a couple of options:

1. Define an escape sequence (ie. quotes or a backslash)
2. Call it a bad record, go get a beer, and move on.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Apr/11 03:00;thedatachef;utf8storagepatch.txt;https://issues.apache.org/jira/secure/attachment/12477472/utf8storagepatch.txt,28/Apr/11 05:01;thedatachef;utf8storagepatch_withtests.txt;https://issues.apache.org/jira/secure/attachment/12477612/utf8storagepatch_withtests.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-27 22:18:56.093,,,no_permission,,,,,,,,,,,,66294,Reviewed,,,Fri Apr 29 21:04:06 UTC 2011,,,Patch Available,,,,0|i0h0jb:,97349,,,,,,,,,,27/Apr/11 03:00;thedatachef;Uses a try...catch block to catch the EmptyStackException and thows an IOException about a 'malformed map',"27/Apr/11 22:18;daijy;Looks good. Can you add couple of test cases (One good place to add is TestConversions.testBytesToComplexTypeMisc), and I can review and commit the patch?",28/Apr/11 05:01;thedatachef;Test cases for map values containing close parenthesis and closed curly braces. Both pass.,"28/Apr/11 21:08;daijy;+1, will commit if tests pass.","29/Apr/11 21:04;daijy;Test patch pass:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Unit test pass

Patch committed to both trunk and 0.9 branch. Thanks Jacob!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
-dot option does not work with explain and new logical plan,PIG-2016,12505246,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,daijy,alangates,alangates,26/Apr/11 20:56,04/Aug/11 00:34,14/Mar/19 03:07,03/May/11 01:42,0.8.0,0.8.1,0.9.0,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"If you specify -dot in explain, it is supposed to produce a file with the graphs in .dot format.  While the physical plan and map reduce plan are correctly output in .dot format, the new logical plan is still output in text format.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30/Apr/11 00:48;daijy;PIG-2016-1.patch;https://issues.apache.org/jira/secure/attachment/12477861/PIG-2016-1.patch,03/May/11 01:28;daijy;PIG-2016-2.patch;https://issues.apache.org/jira/secure/attachment/12478012/PIG-2016-2.patch,05/May/11 17:28;daijy;dot-test.patch;https://issues.apache.org/jira/secure/attachment/12478302/dot-test.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-05-03 01:28:53.665,,,no_permission,,,,,,,,,,,,66202,Reviewed,,,Thu May 05 17:28:30 UTC 2011,,,,,,,0|i0h0j3:,97348,,,,,,,,,,"02/May/11 20:33;alangates;+1, looks good",03/May/11 01:28;daijy;PIG-2016-2.patch fix findbug and javac warnings.,"03/May/11 01:42;daijy;Test-patch result:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 7 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 563 release audit warnings (more than the trunk's current 562 warnings).

Ignore release audit warnings since all new source file contains proper header

Unit test pass.

Patch committed to both trunk and 0.9 branch.","05/May/11 17:28;daijy;dot-test.patch changes the test case cuz we observe indeterministics in old test case, due to node ordering, which out of our control.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Explain writes out logical plan twice,PIG-2015,12505245,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,alangates,alangates,alangates,26/Apr/11 20:53,04/Aug/11 00:34,14/Mar/19 03:07,29/Apr/11 23:05,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"Running explain on a script writes out the logical plan twice, the physical plan once, and the map reduce plan once.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Apr/11 19:58;alangates;PIG-2015.patch;https://issues.apache.org/jira/secure/attachment/12477573/PIG-2015.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-28 21:12:28.053,,,no_permission,,,,,,,,,,,,66223,Reviewed,,,Fri Apr 29 23:05:51 UTC 2011,,,,,,,0|i0h0iv:,97347,,,,,,,,,,28/Apr/11 16:59;alangates;Unit tests pass.  Since I just removed one line I'm not going to bother running test-patch.sh,28/Apr/11 21:12;daijy;+1,29/Apr/11 23:05;daijy;Commit this patch since PIG-2016 need it. Committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SAMPLE shouldn't be pushed up,PIG-2014,12505198,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,thedatachef,thedatachef,26/Apr/11 14:35,04/Aug/11 00:34,14/Mar/19 03:07,12/May/11 10:01,0.10.0,0.9.0,,,,,,0.10.0,0.9.0,,,,,,0,,,,,,,,,,,,,"Consider the following code:

{code:none}
tfidf_all = LOAD '$TFIDF' AS (doc_id:chararray, token:chararray, weight:double);
grouped   = GROUP tfidf_all BY doc_id;
vectors   = FOREACH grouped GENERATE group AS doc_id, tfidf_all.(token, weight) AS vector;
DUMP vectors;
{code}

This, of course, runs just fine. In a real example, tfidf_all contains 1,428,280 records. The reduce output records should be exactly the number of documents, which turn out to be 18,863 in this case. All well and good.

The strangeness comes when you add a SAMPLE command:

{code:none}
sampled = SAMPLE vectors 0.0012;
DUMP sampled;
{code}

Running this results in 1,513 reduce output records. The reduce output records be much much closer to 22 or 23 records (eg. 0.0012*18863).

Evidently, Pig rewrites SAMPLE into filter, and then pushes that filter in front of the group. It shouldn't push that filter  
since the UDF is non-deterministic.  

Quick fix: If you add ""-t PushUpFilter"" to your command line when invoking pig this won't happen.",,,,,,,,,,,,,,,,,,,PIG-2137,,,,,,,,,,,,,11/May/11 15:31;dvryaboy;PIG-2014.2.patch;https://issues.apache.org/jira/secure/attachment/12478831/PIG-2014.2.patch,12/May/11 07:58;daijy;PIG-2014.3.patch;https://issues.apache.org/jira/secure/attachment/12478939/PIG-2014.3.patch,12/May/11 09:47;dvryaboy;PIG-2014.4.patch;https://issues.apache.org/jira/secure/attachment/12478947/PIG-2014.4.patch,12/May/11 09:51;dvryaboy;PIG-2014.5.patch;https://issues.apache.org/jira/secure/attachment/12478948/PIG-2014.5.patch,10/May/11 06:24;dvryaboy;PIG-2014.patch;https://issues.apache.org/jira/secure/attachment/12478672/PIG-2014.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-04-26 17:55:49.221,,,no_permission,,,,,,,,,,,,41697,,,,Thu May 12 10:01:28 UTC 2011,,,,,,,0|i0h0in:,97346,"A new annotation, @Nondeterministic, is introduced to allow UDF authors to mark their UDFs as such. 

A non-deterministic UDF is one that can produce different results when invoked on the same input. Examples of non-deterministic behavior might be, for example, getCurrentTime() or RANDOM.

Certain Pig optimizations depend on UDFs being deterministic. It is therefore very important for correctness that non-deterministic UDFs be annotated as such. 
",,,,,,,,,"26/Apr/11 17:55;dvryaboy;Proposal for a general-case fix:

add a @Nondeterministic annotation for UDFs, have the PushUpFilter check whether the udf is deterministic or not when considering whether pushing up is ok.  Annotate the filter UDF that sample is rewritten to, accordingly.",10/May/11 06:24;dvryaboy;Implemented suggested approach to fixing this. Please review.,"10/May/11 06:26;dvryaboy;Since this is a bug it should go into 0.9.

Then again since it's sort of a new feature maybe not and we should just check for RANDOM specifically in 0.9? 

For my part I don't see the harm in the annotation and would prefer that it goes into 0.9.","10/May/11 22:26;daijy;I think this is more a bug fix, should go into 0.9. 

However, the script will trigger rule FilterAboveForeach not PushUpFilter. So the patch does not fix the problem. The fix should go to all these rules: PushUpFilter, PushDownForEachFlatten, FilterAboveForeach.","10/May/11 22:40;dvryaboy;I'll add to the other rules -- but for the record, I looked at the plan and saw the sample not being pushed up after my patch :-).","10/May/11 22:54;daijy;I think it is because in TestNewPlanFilterAboveForeach, we only invoke some of the rules. If you do an explain, you will see filter still pushed up.","11/May/11 14:26;dvryaboy;Daniel,
So this is interesting. I took my fix out, left the test in, and the test still passed -- because, as you correctly pointed out, TestNewPlanFilterAboveForeach only invokes a few of the rules. If I add PushUpFilter to MyPlanOptimizer within that test, my new test starts failing if the fix is not present, and passes if the fix is present. So the PushUpFilter is definitely at least part of what's causing the movement of Filter in this case.

So I need to fix up PushDownForEachFlatten and FilterAboveForeach, *and* I need to fix my test :).","11/May/11 15:31;dvryaboy;This addresses PushUpFilter and FilterAboveForeach, and fixes the SAMPLE issue.

I didn't tackle PushDownForeachFlatten -- there's a lot going on there and I'm not sure I understand it all. We should open a separate ticket for making sure that optimization does not break on nondeterministic operations.","12/May/11 07:23;daijy;+1 for PIG-2014.2.patch.

I will submit another patch for PushDownForeachFlatten.",12/May/11 07:40;daijy;Also change filterHasNonDeterministicUdf to planHasNonDeterministicUdf. I can reuse it in PushDownForeachFlatten.,12/May/11 07:58;daijy;PIG-2014.3.patch address PushDownFlattenForEach,"12/May/11 09:47;dvryaboy;Attaching combined final version, complete with the renamed method.",12/May/11 09:51;dvryaboy;An unrelated change snuck into the previously attached file; this is what I am actually committing.,12/May/11 10:01;dvryaboy;committed to 0.9 and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Penny gets a null pointer when no properties are set,PIG-2013,12505134,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,breed,breed,breed,25/Apr/11 19:09,05/Oct/11 17:20,14/Mar/19 03:07,02/Aug/11 05:54,,,,,,,,0.10.0,0.9.1,,,tools,,,0,,,,,,,,,,,,,"when you run penny without setting any properties, you get a null pointer exception. unfortunately, we need to set the properties to run in junit, so this bug doesn't get test coverage.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Apr/11 19:10;breed;PIG-2013.patch;https://issues.apache.org/jira/secure/attachment/12477324/PIG-2013.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-02 05:50:50.342,,,no_permission,,,,,,,,,,,,46545,Reviewed,,,Tue Aug 02 05:54:47 UTC 2011,,,,,,,0|i0h0if:,97345,,,,,,,penny,,,02/Aug/11 05:50;daijy;Verified Penny runs with this patch. Committed to both trunk and 0.9 branch.,"02/Aug/11 05:53;daijy;Sorry, the above comment is not mean for this patch.","02/Aug/11 05:54;daijy;Sorry, for the confusion, this is solved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Comments at the begining of the file throws off line numbers in errors,PIG-2012,12505044,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,alangates,alangates,23/Apr/11 14:35,04/Aug/11 00:34,14/Mar/19 03:07,09/May/11 22:52,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"The preprocessor does not appear to be handling leading comments properly when calculating line numbers for error messages.  In the attached script, the error is reported to be on line 7.  It is actually on line 10.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Apr/11 20:26;rding;PIG-2012_1.patch;https://issues.apache.org/jira/secure/attachment/12477688/PIG-2012_1.patch,09/May/11 18:41;rding;PIG-2012_2.patch;https://issues.apache.org/jira/secure/attachment/12478629/PIG-2012_2.patch,23/Apr/11 14:36;alangates;macro.pig;https://issues.apache.org/jira/secure/attachment/12477204/macro.pig,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-04-29 20:43:06.51,,,no_permission,,,,,,,,,,,,66330,Reviewed,,,Tue May 10 04:14:23 UTC 2011,,,,,,,0|i0h0i7:,97344,,,,,,,,,,"29/Apr/11 20:43;rding;test-patch result:

{code}
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     -1 javac.  The applied patch generated 964 javac compiler warnings (more than the trunk's current 963 warnings).
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{code}",04/May/11 17:57;rding;Unit tests pass.,"05/May/11 00:08;xuefuz;1. Macro invocation stack should be per macro, not per node in a macro. Also, stack seems to be a more appropriate data structure for this.

2. To get line numbers correctly, we should be able to do it more naturally in PigParserNode constractor, rather than prepending \n in the text.

3. It might be cleaner and more OO if we have PigParserMacroNode that inherits from PigParserNode. In that case, we only need to override toString() method, but avoid if-else clauses in the code.",09/May/11 18:41;rding;Thanks Xuefu. The new patch addresses the review comments.,09/May/11 22:27;xuefuz;+1 to patch PIG-2012_2.patch,09/May/11 22:52;rding;Patch 2 committed to trunk and 0.9 branch.,10/May/11 04:14;woody.anderson@gmail.com;thanks for this one! this has been a major pain for me.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cache outputFormat in HBaseStorage,PIG-2008,12504910,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,thedatachef,thedatachef,thedatachef,21/Apr/11 18:18,04/Aug/11 00:35,14/Mar/19 03:07,05/May/11 03:51,0.8.0,,,,,,,0.10.0,0.9.0,,,build,,,0,,,,,,,,,,,,,getOutputFormat gets called more than one time in a StoreFunc. Modify HBaseStorage to only create an instance of TableOutputFormat one time (since it creates a new HTable connection each time) as opposed to multiple times like it does now.,,,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,,,,,21/Apr/11 18:18;thedatachef;patch_file.txt;https://issues.apache.org/jira/secure/attachment/12477025/patch_file.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-03 19:35:47.401,,,no_permission,,,,,,,,,,,,66143,,,,Thu May 05 03:51:47 UTC 2011,,,Patch Available,,,,0|i0h0hb:,97340,,,,,,,,,,"03/May/11 19:35;alangates;Dmitriy, could you take a look at this.  It looks ok to me, but I'm not an HBase expert.  I'll run the unit tests and such on it.","03/May/11 19:45;dvryaboy;Yeah looks good. Sorry I missed reviewing this earlier.

Alan, objections to committing this to the 0.9 branch? The HTable connection leak is a bug.",03/May/11 21:43;alangates;No objections.  Unit tests pass.  Running test-patch now.,04/May/11 17:31;alangates;Committed to trunk.  Will commit to 0.9 branch shortly.,05/May/11 03:51;alangates;Patch checked into trunk and 0.9 branch.  Thanks Jacob.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parsing error when map key referred directly from udf in nested foreach ,PIG-2007,12504874,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,anitharaju,anitharaju,21/Apr/11 10:04,04/Aug/11 00:34,14/Mar/19 03:07,22/Apr/11 18:06,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"The below script when executed with version 0.9 fails with parsing error.

{code}
 ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. <line 2, column 15> mismatched input '{' expecting GENERATE
{code}

Script1
{code}
register myudf.jar;
A = load 'test.txt' using PigStorage() as (a:int,b:chararray);
B1 = foreach A {
        C = test.TOMAP('key1',$1)#'key1';
        generate C as C;
}
{code}

The above happens when, in a nested foreach i refer to a map key directly from a udf result

The same would work if one executes without the nested foreach.
{code}
register myudf.jar;
A = load 'test.txt' using PigStorage() as (a:int,b:chararray);
B1 = foreach A generate test.TOMAP('key1',$1)#'key1';
dump B1;
{code}

Script1 works well with 0.8.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/May/11 19:13;xuefuz;PIG-2007-2.patch;https://issues.apache.org/jira/secure/attachment/12478721/PIG-2007-2.patch,22/Apr/11 14:51;xuefuz;PIG-2007.patch;https://issues.apache.org/jira/secure/attachment/12477115/PIG-2007.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-22 14:51:07.893,,,no_permission,,,,,,,,,,,,66219,,,,Tue May 10 20:18:21 UTC 2011,,,,,,,0|i0h0h3:,97339,,,,,,,,,,22/Apr/11 14:51;xuefuz;Unit test passed.,"22/Apr/11 18:04;xuefuz;Test-patch run: (release warnings are ignored as there are no new files introduced.)

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     -1 release audit.  The applied patch generated 568 release audit warnings (more than the trunk's current 566 warnings).
",22/Apr/11 18:06;xuefuz;Patch is committed to trunk.,"22/Apr/11 18:09;xuefuz;Sorry but just realized that the patch hasn't been reviewed yet. I mistaken it for another one which was reviewed.

I have asked Thejas to review this patch.",25/Apr/11 23:04;thejas;+1,26/Apr/11 05:11;xuefuz;Patch is committed to 0.9 as well.,10/May/11 19:13;xuefuz;Patch PIG-2007-2.patch is what was actually committed to the trunk. This needs to be for 0.9 as well.,10/May/11 19:23;thejas;+1 for PIG-2007-2.patch,10/May/11 20:18;xuefuz;PIG-2007-2.patch is committed to 0.9.0.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regression: NPE when Pig processes an empty script file,PIG-2006,12504835,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,21/Apr/11 00:22,04/Aug/11 00:34,14/Mar/19 03:07,26/Apr/11 18:13,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"If a pig script file is empty and supplied as input for Pig (using -f option), an NPE is thrown. Stacktrace:

java.lang.NullPointerException
        at java.util.regex.Matcher.getTextLength(Matcher.java:1140)
        at java.util.regex.Matcher.reset(Matcher.java:291)
        at java.util.regex.Matcher.<init>(Matcher.java:211)
        at java.util.regex.Pattern.matcher(Pattern.java:888)
        at org.apache.pig.scripting.ScriptEngine$SupportedScriptLang.accepts(ScriptEngine.java:89)
        at org.apache.pig.scripting.ScriptEngine.getSupportedScriptLang(ScriptEngine.java:163)
        at org.apache.pig.Main.determineScriptType(Main.java:892)
        at org.apache.pig.Main.run(Main.java:378)
        at org.apache.pig.Main.main(Main.java:108)

This seems related Jython support in 0.9.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Apr/11 17:33;xuefuz;PIG-2006-1.patch;https://issues.apache.org/jira/secure/attachment/12477215/PIG-2006-1.patch,22/Apr/11 16:04;xuefuz;PIG-2006.patch;https://issues.apache.org/jira/secure/attachment/12477116/PIG-2006.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-22 17:03:26.999,,,no_permission,,,,,,,,,,,,66224,,,,Sat Apr 23 17:33:08 UTC 2011,,,,,,,0|i0h0gv:,97338,,,,,,,,,,22/Apr/11 17:03;rding;+1,"22/Apr/11 23:26;xuefuz;Test-patch run:

     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
",22/Apr/11 23:32;xuefuz;Patch is committed to both trunk and 0.9 branch. Issue is resolved.,23/Apr/11 17:31;xuefuz;The newly added test case fails when running as part of unit test set.,23/Apr/11 17:33;xuefuz;Fix the failing test case.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Discrepancy in the way dry run handles semicolon in macro definition,PIG-2005,12504816,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,20/Apr/11 18:33,04/Aug/11 00:34,14/Mar/19 03:07,22/Apr/11 15:56,0.9.0,,,,,,,0.9.0,,,,grunt,,,0,,,,,,,,,,,,,"Macro definition requires a semicolon to mark the end. For example:

{code}
define mymacro(x) returns y {... ...};
{code}

But invoked through command line, the macro definitions without semicolon also work except in the case of dryrun. This discrepancy is due to GruntParser automatic appending a semicolon to Pig statements if semicolon is absent at the end. Dryrun GruntParser should do the same.   ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Apr/11 18:38;rding;PIG-2005_1.patch;https://issues.apache.org/jira/secure/attachment/12476929/PIG-2005_1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-21 21:09:46.853,,,no_permission,,,,,,,,,,,,66358,Reviewed,,,Fri Apr 22 15:56:24 UTC 2011,,,,,,,0|i0h0gn:,97337,,,,,,,,,,21/Apr/11 21:09;thejas;+1,"22/Apr/11 15:49;rding;Patch-test result:

{code}
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{code}

Unit tests pass.",22/Apr/11 15:56;rding;Patch committed to trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect input types passed on to eval function,PIG-2004,12504775,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,vivekp,vivekp,20/Apr/11 10:16,04/Aug/11 00:35,14/Mar/19 03:07,26/Apr/11 19:24,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"The below script fails by throwing a ClassCastException from the MAX udf. The udf expects the value of the bag supplied to be databyte array, but at run time the udf gets the actual type, ie Double in this case.  This causes the script execution to fail with exception;

| Caused by: java.lang.ClassCastException: java.lang.Double cannot be cast to org.apache.pig.data.DataByteArray


The same script runs properly with Pig 0.8.



{code}
A = LOAD 'myinput' as (f1,f2,f3);
B = foreach A generate f1,f2+f3/1000.0 as doub;
C = group B by f1;
D = foreach C generate (long)(MAX(B.doub)) as f4;
dump D;
{code}

myinput
-------
a       1000    12345
b       2000    23456
c       3000    34567
a       1500    54321
b       2500    65432

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Apr/11 20:50;daijy;PIG-2004-0.patch;https://issues.apache.org/jira/secure/attachment/12476939/PIG-2004-0.patch,26/Apr/11 01:21;thejas;PIG-2004.1.patch;https://issues.apache.org/jira/secure/attachment/12477348/PIG-2004.1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-20 18:03:14.338,,,no_permission,,,,,,,,,,,,66173,,,,Tue Apr 26 19:24:46 UTC 2011,,,,,,,0|i0h0gf:,97336,,,,,,,,,,20/Apr/11 18:03;daijy;Seems logical plan pick the wrong MAX implementation (should pick DoubleMax),20/Apr/11 20:50;daijy;TypeCheckVisitor does not update schema before processing LOGenerate. Attach a patch for demonstration. Assign to Thejas for further investigation.,20/Apr/11 21:46;olgan;Thejas - can you treat this as high priority since it can cause failures in pretty basic Pig scripts,"26/Apr/11 01:21;thejas;PIG-2004.1.patch
- Reset fieldschema of all expressions from TypeCheckingExpVisitor constructor, instead of doing it in each visit function.
- Reset target fieldschema in CastExpression, copied LHS fieldschema in BinCondExpression so that uid of inner schema is not re-used.
- Fixed a NPE in LogicalSchema that was seen in test cases after this issue was fixed.",26/Apr/11 19:14;daijy;+1,"26/Apr/11 19:24;thejas;Patch committed to trunk and 0.9 branch.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using keyward as alias doesn't either emit an error or produce a logical plan.,PIG-2003,12504712,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,19/Apr/11 20:57,04/Aug/11 00:34,14/Mar/19 03:07,26/Apr/11 05:08,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"The following is the symptom:

grunt> ship = load 'x';
grunt> describe ship;
2011-04-19 13:52:52,809 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1005: No plan for ship to describe

The correct behavior is to give an error.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Apr/11 16:05;xuefuz;PIG-2003.patch;https://issues.apache.org/jira/secure/attachment/12477117/PIG-2003.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-25 22:54:25.958,,,no_permission,,,,,,,,,,,,66317,,,,Tue Apr 26 05:07:43 UTC 2011,,,,,,,0|i0h0g7:,97335,,,,,,,,,,"23/Apr/11 02:30;xuefuz;Test-patch run: (no new file, ignoring the release audit warnings.)

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     -1 javac.  The applied patch generated 951 javac compiler warnings (more than the trunk's current 947 warnings).
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     -1 release audit.  The applied patch generated 581 release audit warnings (more than the trunk's current 578 warnings).
","25/Apr/11 22:54;thejas;+1 .
The new rule in this patch is logically the same old one, but the new one fixes the problem. This is probably caused by some bug in antlr (3.2).
",26/Apr/11 05:07;xuefuz;unit test passed. patch is committed to both trunk and 0.9.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Regression: Pig gives error ""Projection with nothing to reference!"" for a valid query",PIG-2002,12504705,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,19/Apr/11 19:45,04/Aug/11 00:35,14/Mar/19 03:07,20/Apr/11 19:02,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"Pig has problem parsing the following query:

A = load 'x' as (u:map[], v);
B = foreach A { T = (chararray)u#'hello'#'world'; generate T; };

2011-04-19 12:45:24,486 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2225: Projection with nothing to reference!
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Apr/11 19:52;xuefuz;PIG-2002.patch;https://issues.apache.org/jira/secure/attachment/12476764/PIG-2002.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-19 20:47:24.516,,,no_permission,,,,,,,,,,,,66138,,,,Wed Apr 20 19:03:59 UTC 2011,,,,,,,0|i0h0fz:,97334,,,,,,,,,,"19/Apr/11 20:47;thejas;+1
","19/Apr/11 22:27;xuefuz;Test-patch run: (no new file in the patch, so release audit warnings can be ignored.)

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     -1 release audit.  The applied patch generated 581 release audit warnings (more than the trunk's current 564 warnings).
",20/Apr/11 19:03;xuefuz;Unit test passed. Patch is committed to trunk. Issue is resolved.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig gives incorrect error message dealing with scalar projection,PIG-2000,12504607,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,18/Apr/11 21:59,04/Aug/11 00:34,14/Mar/19 03:07,21/Apr/11 16:29,0.8.0,0.9.0,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"For the following query:

A = load 'x' as (u:tuple(x,y),v);
B = load 'y';
C = foreach B generate $0, A.u.x;

error msg in 0.8:
ERROR 1000: Error during parsing. Invalid alias: x in {u: (x: bytearray,y: bytearray),v: bytearray}

error msg in 0.9:
ERROR 1200: Pig script failed to parse: <line 4, column 27> Invalid scalar projection: A

Both messages are not clear enough. For scalar support, we only support one level, which gives a syntax of R.f format.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Apr/11 00:15;xuefuz;PIG-2000.patch;https://issues.apache.org/jira/secure/attachment/12476953/PIG-2000.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,66206,,,,Thu Apr 21 16:29:38 UTC 2011,,,,,,,0|i0h0fj:,97332,,,,,,,,,,"21/Apr/11 00:15;xuefuz;Test-patch run:

     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
","21/Apr/11 00:19;xuefuz;The expected behavior specified above is not accurate. After discussing with Thejas, it's found that this is valid use case. Unfortunately, we had issues in both 0.8 and 0.9. The patch provided above fixes the issue in 0.9. With it, scalar project in form of ""A.x.y"" is supported.

Fix for 0.8 would require a different patch.",21/Apr/11 16:29;xuefuz;Unit test passed. Patch is committed to trunk. Issue is closed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Macro alias masker should consider schema context ,PIG-1999,12504598,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,18/Apr/11 20:51,04/Aug/11 00:34,14/Mar/19 03:07,05/May/11 17:05,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"Macro alias masker doesn't consider the current schema context. This results errors when deciding with alias to mask. Here is an example:

{code}
define toBytearray(in, intermediate) returns e { 
   a = load '$in' as (name:chararray, age:long, gpa: float);
   b = group a by  name;
   c = foreach b generate a, (1,2,3);
   store c into '$intermediate' using BinStorage();
   d = load '$intermediate' using BinStorage() as (b:bag{t:tuple(x,y,z)}, t2:tuple(a,b,c));
   $e = foreach d generate COUNT(b), t2.a, t2.b, t2.c;
};
 
f = toBytearray ('data', 'output1');
{code} 

Now the alias masker mistakes b in COUNT(b) as an alias instead of b in the current schema.

The workaround is to not use alias as as names in the schema definition. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Apr/11 00:20;rding;PIG-1999_1.patch;https://issues.apache.org/jira/secure/attachment/12477344/PIG-1999_1.patch,27/Apr/11 17:39;rding;PIG-1999_2.patch;https://issues.apache.org/jira/secure/attachment/12477567/PIG-1999_2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-05-03 00:04:17.999,,,no_permission,,,,,,,,,,,,66188,Reviewed,,,Thu May 05 17:05:44 UTC 2011,,,,,,,0|i0h0fb:,97331,,,,,,,,,,"22/Apr/11 23:29;rding;The proposed solution is, in 0.9 release, disallow user defined schema inside a macro to have names that conflict with alias in the macro. Parser will throw an exception when it detects a name conflict in macro.

This is not a perfect solution. But this kind of name-conflict is not common in Pig scripts and any conflict can be easily fixed with clear error messages.

As mentioned before, we need schema context to solve this problem. Now schema resolution happens after parsing. If needed, we'll address this problem in future release.

 ",27/Apr/11 17:39;rding;Rebase the patch.,"02/May/11 17:26;rding;test-patch result:

{code}
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{code}",03/May/11 00:04;xuefuz;+1 Patch PIG-1999_2.patch looks good.,05/May/11 17:05;rding;Unit tests pass. Patch committed to trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig new parser fails to recognize PARALLEL keywords in a case,PIG-1996,12504326,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,15/Apr/11 00:01,04/Aug/11 00:35,14/Mar/19 03:07,19/Apr/11 16:59,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"The following query fails with the new parser:

B = FOREACH (group (load 'a') ALL PARALLEL 16) generate group;

runt> B = FOREACH (group (load 'a') ALL PARALLEL 16) generate group;
2011-04-14 16:57:23,413 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1200: <line 1, column 34>  mismatched input 'PARALLEL' expecting RIGHT_PAREN
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Apr/11 18:33;xuefuz;PIG-1996.patch;https://issues.apache.org/jira/secure/attachment/12476470/PIG-1996.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-18 17:24:44.906,,,no_permission,,,,,,,,,,,,66117,,,,Tue Apr 19 16:58:58 UTC 2011,,,,,,,0|i0h0en:,97328,,,,,,,,,,18/Apr/11 17:24;daijy;+1,"18/Apr/11 21:37;xuefuz;Test-patch run: (no new files. ignore additioanl release audit warnings)

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     -1 release audit.  The applied patch generated 564 release audit warnings (more than the trunk's current 563 warnings).
",19/Apr/11 16:58;xuefuz;Unit test passed. Patch is committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigStorageSchema throw NPE with ColumnPruning,PIG-1993,12504123,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,13/Apr/11 00:47,25/Apr/11 21:27,14/Mar/19 03:07,14/Apr/11 01:48,0.8.0,0.9.0,,,,,,0.8.1,,,,impl,,,0,,,,,,,,,,,,,"The following script fail:
{code}
a = load '1.txt' as (a0:int, a1:int, a2:int);
store a into 'temp' using org.apache.pig.piggybank.storage.PigStorageSchema();
exec
a = LOAD 'temp' using org.apache.pig.piggybank.storage.PigStorageSchema();
b = FOREACH a GENERATE a1;
dump b;
{code}

Error message:
java.lang.ArrayIndexOutOfBoundsException: 2
        at org.apache.pig.piggybank.storage.PigStorageSchema.getNext(PigStorageSchema.java:94)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:187)
        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:423)
        at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Apr/11 00:49;daijy;PIG-1993-0.patch;https://issues.apache.org/jira/secure/attachment/12476196/PIG-1993-0.patch,13/Apr/11 01:34;daijy;PIG-1993-1.patch;https://issues.apache.org/jira/secure/attachment/12476201/PIG-1993-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-13 18:07:36.772,,,no_permission,,,,,,,,,,,,165260,Reviewed,,,Thu Apr 14 01:48:42 UTC 2011,,,,,,,0|i0h0dz:,97325,,,,,,,,,,"13/Apr/11 00:50;daijy;Attach PIG-1993-0.patch for reference. It is not a complete patch, no test case included.",13/Apr/11 01:34;daijy;Attach a complete patch PIG-1993-1.patch,"13/Apr/11 17:53;daijy;Dmitriy, can you review the patch?","13/Apr/11 18:07;dvryaboy;looks good, please commit if tests pass.

Glad to see someone's actually using this! :)",14/Apr/11 01:48;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support casting of complex types with empty inner schema to complex type with non-empty inner schema,PIG-1990,12503993,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,11/Apr/11 23:24,04/Aug/11 00:34,14/Mar/19 03:07,03/May/11 15:57,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"Use case like the following should be supported - 
{code}
a = load '1.txt' as (t:tuple());
b = foreach a generate (tuple(int))t;
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Apr/11 21:16;thejas;PIG-1990.1.patch;https://issues.apache.org/jira/secure/attachment/12477841/PIG-1990.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-03 01:46:49.466,,,no_permission,,,,,,,,,,,,66246,,,,Tue May 03 15:57:44 UTC 2011,,,,,,,0|i0h0db:,97322,,,,,,,,,,"01/May/11 01:26;thejas;test patch passed. Unit test passed except for TestStoreInstances, but that failed for me in trunk without the patch as well.",03/May/11 01:46;daijy;+1,03/May/11 15:57;thejas;Patch committed to trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
complex type casting should return null on casting failure ,PIG-1989,12503960,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,thejas,thejas,11/Apr/11 18:12,04/Aug/11 00:34,14/Mar/19 03:07,29/Apr/11 00:43,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"When casting fails for complex objects, pig is currently returning un-casted object if the cast fails. 
It should return null instead. That is consistent with the behavior when casting to other basic types. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Apr/11 22:15;daijy;PIG-1989-1.patch;https://issues.apache.org/jira/secure/attachment/12477451/PIG-1989-1.patch,28/Apr/11 20:54;daijy;PIG-1989-2.patch;https://issues.apache.org/jira/secure/attachment/12477692/PIG-1989-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-25 23:59:15.829,,,no_permission,,,,,,,,,,,,66342,Reviewed,,,Fri Apr 29 00:43:26 UTC 2011,,,,,,,0|i0h0d3:,97321,,,,,,,,,,"25/Apr/11 23:59;daijy;I tried some test cases, seems currently we set the inner fields which cannot cast to null:

{code}
input:
(a,b,3)

a = load '1.txt' as (t:tuple(i0, i1, i2));
b = foreach a generate (tuple(int,int,int))t;

We get: ((,,3))
{code}

{code}
input:
{(a,1)}

a = load '1.txt' as (a0:bag{t:tuple(i0,i1)});
b = foreach a generate (bag{tuple(int,int)})a0;

We get: ({(,1)})
{code}

{code}
input:
[key#value]

a = load '1.txt' as (m:map[]);
b = foreach a generate (map[int])m;
dump b;

We get: ([key#])
{code}

Sounds Ok to me. Thejas, do you see something else, or you feel this is not proper?","26/Apr/11 01:02;thejas;The problem is seen in this case -
{code}
input:
(a,b,3)

a = load 'inp.txt' as (t:tuple(i0, i1, i2));                          
b = foreach a generate (tuple(tuple(int)))$0;
dump b;


We get: ((,b,3)), instead of ()
{code}",26/Apr/11 22:15;daijy;This happens when the size of tuple inner schema does not match the data. Attach PIG-1989-1.patch.,"28/Apr/11 20:54;daijy;PIG-1989-2.patch include suggestions from Thejas, and address findbug warnings.","28/Apr/11 20:57;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/673/
-----------------------------------------------------------

Review request for pig and thejas.


Summary
-------

See PIG-1989


This addresses bug PIG-1989.
    https://issues.apache.org/jira/browse/PIG-1989


Diffs
-----

  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POCast.java 1097304 
  http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/TestPOCast.java 1097304 

Diff: https://reviews.apache.org/r/673/diff


Testing
-------

Test-patch:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Unit test:
    all pass


Thanks,

Daniel

","28/Apr/11 22:08;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/673/#review606
-----------------------------------------------------------

Ship it!


+1

- thejas


On 2011-04-28 20:56:30, Daniel Dai wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/673/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-04-28 20:56:30)
bq.  
bq.  
bq.  Review request for pig and thejas.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  See PIG-1989
bq.  
bq.  
bq.  This addresses bug PIG-1989.
bq.      https://issues.apache.org/jira/browse/PIG-1989
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POCast.java 1097304 
bq.    http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/TestPOCast.java 1097304 
bq.  
bq.  Diff: https://reviews.apache.org/r/673/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  Test-patch:
bq.       [exec] +1 overall.  
bq.       [exec] 
bq.       [exec]     +1 @author.  The patch does not contain any @author tags.
bq.       [exec] 
bq.       [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
bq.       [exec] 
bq.       [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
bq.       [exec] 
bq.       [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
bq.       [exec] 
bq.       [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
bq.       [exec] 
bq.       [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
bq.  
bq.  Unit test:
bq.      all pass
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Daniel
bq.  
bq.

",29/Apr/11 00:43;daijy;Patch committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Importing an empty macro file causing NPE,PIG-1988,12503951,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,xuefuz,xuefuz,11/Apr/11 16:49,04/Aug/11 00:35,14/Mar/19 03:07,11/Apr/11 22:59,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"The following script, which imports an empty macro file, causes a NPE.

import 'macros.pig';
A = load 'x';
B = foreach A generate flatten($0);

Stack trace:

Caused by: java.lang.NullPointerException
        at org.antlr.runtime.tree.BaseTree.addChildren(BaseTree.java:133)
        at org.apache.pig.parser.QueryParserUtils.replaceNodeWithNodeList(QueryParserUtils.java:194)
        at org.apache.pig.parser.QueryParserDriver.macroImport(QueryParserDriver.java:409)
        at org.apache.pig.parser.QueryParserDriver.expandImport(QueryParserDriver.java:274)
        at org.apache.pig.parser.QueryParserDriver.expandMacro(QueryParserDriver.java:214)
        at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:86)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11/Apr/11 20:11;rding;PIG-1988.patch;https://issues.apache.org/jira/secure/attachment/12476050/PIG-1988.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-11 22:59:33.806,,,no_permission,,,,,,,,,,,,66174,Reviewed,,,Mon Apr 11 22:59:33 UTC 2011,,,,,,,0|i0h0cv:,97320,,,,,,,,,,11/Apr/11 21:31;xuefuz;+1,11/Apr/11 22:59;rding;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
-dryrun does not work with set,PIG-1987,12503949,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,alangates,alangates,11/Apr/11 16:25,04/Aug/11 00:34,14/Mar/19 03:07,19/Apr/11 22:22,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"The following script works

{code}
set default_parallel 10;
import 'scripts/macro1.pig';
import 'scripts/macro2.pig';

wlogs = load 'clicks' as (url, pageid, timestamp);
good  = spam_filter(wlogs, url);
{code}

But if -dryrun is added to the command line it returns the error:

{code}
ERROR 1200: <file scripts/book.pig.substituted, line 1, column 0>  Syntax error, unexpected symbol at or near 'set'
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Apr/11 21:22;rding;PIG-1987.patch;https://issues.apache.org/jira/secure/attachment/12476175/PIG-1987.patch,15/Apr/11 17:31;rding;PIG-1987_1.patch;https://issues.apache.org/jira/secure/attachment/12476463/PIG-1987_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-13 22:34:51.596,,,no_permission,,,,,,,,,,,,66249,Reviewed,,,Tue Apr 19 22:22:55 UTC 2011,,,,,,,0|i0h0cn:,97319,,,,,,,,,,"13/Apr/11 22:34;rding;Test-patch result:

{code}
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 581 release audit warnings (more than the trunk's current 573 warnings).
{code}

The release audit warnings are html related.
","14/Apr/11 19:01;xuefuz;For patch PIG-1987.patch, it appears to me that dryrun() method in QueryParserDriver has nothing to do with the what QueryParserDriver is doing. Right now there is loop in calling stack: Main -> QueryParserDriver -> DryrunParser -> QueryParserDriver, which can be made better by moving dryrun() method from QueryParserDriver to DryrunDriver itself. With that, the class reference should be Main -> DryrunParser ->QueryParserDriver, resulting a cleaner design.

I can review again if the suggested change makes sense.",15/Apr/11 17:31;rding;New patch made changes to address the review comments.,19/Apr/11 22:04;xuefuz;+1 to patch PIG-1987_1.patch,19/Apr/11 22:22;rding;Unit tests pass. Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Utils.getSchemaFromString does not use the new parser, and thus fails to parse valid schema",PIG-1985,12503792,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,woody.anderson@gmail.com,woody.anderson@gmail.com,08/Apr/11 22:14,04/Aug/11 00:35,14/Mar/19 03:07,05/May/11 23:38,0.9.0,,,,,,,0.10.0,0.9.0,,,impl,,,0,,,,,,,,,,,,,"I've been told this is because Utils.getSchemaFromString does not use the new parser to parse the schema, so we should update the impl to use the new parser:

{code}
Utils.getSchemaFromString(""f: map[]"")
{code}
results in: (org.apache.pig.impl.logicalLayer.schema.Schema) {f: map[]}

{code}
Utils.getSchemaFromString(""f: map[int]"")
{code}
results in: An exception occurred: org.apache.pig.impl.logicalLayer.parser.ParseException
..
org.apache.pig.impl.logicalLayer.parser.ParseException: Encountered "" ""map"" ""map """" at line 1, column 4.
Was expecting one of:
    ""int"" ...
    ""long"" ...
    ""float"" ...
    ""double"" ...
    ""chararray"" ...
    ""bytearray"" ...
    ""int"" ...
    ""long"" ...
    ""float"" ...
    ""double"" ...
    ""chararray"" ...
    ""bytearray"" ...	 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Apr/11 23:15;daijy;PIG-1985-1.patch;https://issues.apache.org/jira/secure/attachment/12477053/PIG-1985-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-08 22:38:15.078,,,no_permission,,,,,,,,,,,,66155,,,,Thu May 05 23:38:07 UTC 2011,,,,,,,0|i0h0c7:,97317,,,,,,,,,,08/Apr/11 22:38;olgan;It is too late to make this kind of changes for 9. Moving to 10,"08/Apr/11 22:53;woody.anderson@gmail.com;this is a bug, why are we targeting for .10?","08/Apr/11 23:56;alangates;It seems like we're confusing wrong behavior and the solution here.  The bug is that Utils.getSchemaFromString doesn't support the new syntax.  It isn't a bug that it doesn't use the new parser.  We can fix this in whatever parser it uses.

I agree that this is a bug against 0.9.  It means we didn't fully implement changes to support typed maps.",09/Apr/11 00:16;olgan;Ok. Switched back to 0.9. I am ok with fixing the bug but no ok with switching parser technology so late in the cycle,"21/Apr/11 23:16;daijy;Migrate the typed schema parser back to old logical plan, so that Utils.getSchemaFromString can be used.",05/May/11 23:38;daijy;This is done as part of PIG-1775. There is no need for this patch. Close the ticket.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New logical plan failing with ERROR 2229: Couldn't find matching uid -1 ,PIG-1979,12503705,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,vivekp,vivekp,08/Apr/11 07:24,25/Apr/11 21:27,14/Mar/19 03:07,15/Apr/11 00:25,0.8.0,0.9.0,,,,,,0.8.1,,,,impl,,,0,,,,,,,,,,,,,"The below is my script 
{code}
register myudf.jar;
c01 = LOAD 'input'  USING org.test.MyTableLoader('');
c02 = FILTER c01  BY result == 'OK'  AND formatted IS NOT NULL  AND formatted != '' ;
c03 = FOREACH c02 GENERATE url, formatted, FLATTEN(usage);
c04 = FOREACH c03 GENERATE usage::domain AS domain, url, formatted;
doc_001 = FOREACH c04 GENERATE domain,url, FLATTEN(MyExtractor(formatted)) AS category;
doc_004_1 = GROUP doc_001 BY (domain,url);
doc_005 = FOREACH doc_004_1 GENERATE group.domain as domain, group.url as url, doc_001.category as category;
STORE doc_005 INTO 'out_final' USING PigStorage();

review1 = FOREACH c04 GENERATE domain,url, MyExtractor(formatted) AS rev;
review2 = FILTER review1 BY SIZE(rev)>0;
joinresult = JOIN review2 by (domain,url), doc_005 by (domain,url);
finalresult = FOREACH joinresult GENERATE  doc_005::category;
STORE finalresult INTO 'out_final' using PigStorage();
{code}

The script is failing in building the plan, while applying for logical optimization rule for AddForEach.

ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2229: Couldn't find matching uid -1 for project (Name: Project Type: bytearray Uid: 106 Input: 0 Column: 5)

The problem is happening when I try to include doc_005::category in the projection for relation finalresult. This is field is orginated from the udf org.vivek.udfs.MyExtractor (source given below).

{code}

import java.io.IOException;
import org.apache.pig.EvalFunc;
import org.apache.pig.data.*;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;

public class MyExtractor extends EvalFunc<DataBag>
{
  @Override
	public Schema outputSchema(Schema arg0) {
	  try {
			return Schema.generateNestedSchema(DataType.BAG, DataType.CHARARRAY);
		} catch (FrontendException e) {
			System.err.println(""Error while generating schema. ""+e);
			return new Schema(new FieldSchema(null, DataType.BAG));
		}
	}

  @Override
  public DataBag exec(Tuple inputTuple)
    throws IOException
  {
    try {
      Tuple tp2 = TupleFactory.getInstance().newTuple(1);
      tp2.set(0, (inputTuple.get(0).toString()+inputTuple.hashCode()));
      DataBag retBag = BagFactory.getInstance().newDefaultBag();
      retBag.add(tp2);
      return retBag;
    }
    catch (Exception e) {
      throw new IOException("" Caught exception"", e);
    }
  }
}

{code}

The script goes through fine if I disable AddForEach rule by -t AddForEach",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Apr/11 17:44;daijy;PIG-1979-1-trunk.patch;https://issues.apache.org/jira/secure/attachment/12476266/PIG-1979-1-trunk.patch,11/Apr/11 22:58;daijy;PIG-1979-1.patch;https://issues.apache.org/jira/secure/attachment/12476066/PIG-1979-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-13 00:13:05.829,,,no_permission,,,,,,,,,,,,165258,Reviewed,,,Fri Apr 15 00:25:13 UTC 2011,,,,,,,0|i0h0av:,97311,,,,,,,,,,"13/Apr/11 00:13;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/587/
-----------------------------------------------------------

Review request for pig and thejas.


Summary
-------

See PIG-1979


This addresses bug PIG-1979.
    https://issues.apache.org/jira/browse/PIG-1979


Diffs
-----

  http://svn.apache.org/repos/asf/pig/branches/branch-0.8/src/org/apache/pig/newplan/logical/expression/DereferenceExpression.java 1091122 
  http://svn.apache.org/repos/asf/pig/branches/branch-0.8/test/org/apache/pig/test/TestEvalPipeline2.java 1091122 

Diff: https://reviews.apache.org/r/587/diff


Testing
-------

Test-patch:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Unit-test:
    all pass


Thanks,

Daniel

",13/Apr/11 17:44;daijy;PIG-1979-1-trunk.patch is for trunk.,"14/Apr/11 21:24;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/587/#review461
-----------------------------------------------------------

Ship it!


+1

- thejas


On 2011-04-13 00:11:37, Daniel Dai wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/587/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-04-13 00:11:37)
bq.  
bq.  
bq.  Review request for pig and thejas.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  See PIG-1979
bq.  
bq.  
bq.  This addresses bug PIG-1979.
bq.      https://issues.apache.org/jira/browse/PIG-1979
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    http://svn.apache.org/repos/asf/pig/branches/branch-0.8/src/org/apache/pig/newplan/logical/expression/DereferenceExpression.java 1091122 
bq.    http://svn.apache.org/repos/asf/pig/branches/branch-0.8/test/org/apache/pig/test/TestEvalPipeline2.java 1091122 
bq.  
bq.  Diff: https://reviews.apache.org/r/587/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  Test-patch:
bq.       [exec] +1 overall.  
bq.       [exec] 
bq.       [exec]     +1 @author.  The patch does not contain any @author tags.
bq.       [exec] 
bq.       [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
bq.       [exec] 
bq.       [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
bq.       [exec] 
bq.       [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
bq.       [exec] 
bq.       [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
bq.       [exec] 
bq.       [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
bq.  
bq.  Unit-test:
bq.      all pass
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Daniel
bq.  
bq.

",15/Apr/11 00:25;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Secondary sort fail when dereferencing two fields inside foreach,PIG-1978,12503685,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,07/Apr/11 22:34,04/Aug/11 00:35,14/Mar/19 03:07,20/Apr/11 06:06,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"The following script fail:
{code}
a = load '1.txt' as (a0, a1, a2);
b = group a by (a0, a1);
c = foreach b {
    c1 = a.(a1,a2);
    generate group, c1;
}
explain c;
{code}

Error message:
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 0: POForEach has more than 1 input plans
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.SecondaryKeyOptimizer$SecondaryKeyDiscover.processForEach(SecondaryKeyOptimizer.java:551)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.SecondaryKeyOptimizer$SecondaryKeyDiscover.processRoot(SecondaryKeyOptimizer.java:485)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.SecondaryKeyOptimizer$SecondaryKeyDiscover.process(SecondaryKeyOptimizer.java:470)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.SecondaryKeyOptimizer.visitMROp(SecondaryKeyOptimizer.java:254)

Thanks William reporting.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Apr/11 17:35;daijy;PIG-1978-1.patch;https://issues.apache.org/jira/secure/attachment/12476357/PIG-1978-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-19 00:38:05.788,,,no_permission,,,,,,,,,,,,66136,Reviewed,,,Wed Apr 20 06:06:51 UTC 2011,,,,,,,0|i0h0an:,97310,,,,,,,,,,"19/Apr/11 00:38;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/621/
-----------------------------------------------------------

Review request for pig and thejas.


Summary
-------

See PIG-1978


This addresses bug PIG-1978.
    https://issues.apache.org/jira/browse/PIG-1978


Diffs
-----

  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/SecondaryKeyOptimizer.java 1091982 
  http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/TestSecondarySort.java 1091982 

Diff: https://reviews.apache.org/r/621/diff


Testing
-------

Test-patch:
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Unit test:
    all pass


Thanks,

Daniel

","20/Apr/11 01:38;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/621/#review500
-----------------------------------------------------------

Ship it!


+1

- thejas


On 2011-04-19 00:37:31, Daniel Dai wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/621/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-04-19 00:37:31)
bq.  
bq.  
bq.  Review request for pig and thejas.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  See PIG-1978
bq.  
bq.  
bq.  This addresses bug PIG-1978.
bq.      https://issues.apache.org/jira/browse/PIG-1978
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/SecondaryKeyOptimizer.java 1091982 
bq.    http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/TestSecondarySort.java 1091982 
bq.  
bq.  Diff: https://reviews.apache.org/r/621/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  Test-patch:
bq.       [exec]     +1 @author.  The patch does not contain any @author tags.
bq.       [exec] 
bq.       [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
bq.       [exec] 
bq.       [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
bq.       [exec] 
bq.       [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
bq.       [exec] 
bq.       [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
bq.       [exec] 
bq.       [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
bq.  
bq.  Unit test:
bq.      all pass
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Daniel
bq.  
bq.

",20/Apr/11 06:06;daijy;Patch committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Stream closed"" error while reading Pig temp files (results of intermediate jobs)",PIG-1977,12503684,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,07/Apr/11 22:27,25/Apr/11 21:27,14/Mar/19 03:07,11/Apr/11 16:52,0.8.0,,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"In certain cases when compression of temporary files is on Pig scripts fail with following exception:

{code}
java.io.IOException: Stream closed at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:145) at
java.io.BufferedInputStream.fill(BufferedInputStream.java:189) at
java.io.BufferedInputStream.read(BufferedInputStream.java:237) at
java.io.DataInputStream.readByte(DataInputStream.java:248) at
org.apache.hadoop.io.file.tfile.Utils.readVLong(Utils.java:196) at
org.apache.hadoop.io.file.tfile.Utils.readVInt(Utils.java:168) at
org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder.readLength(Chunk.java:103) at
org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder.checkEOF(Chunk.java:124) at
org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder.close(Chunk.java:190) at
java.io.FilterInputStream.close(FilterInputStream.java:155) at
org.apache.pig.impl.io.TFileRecordReader.nextKeyValue(TFileRecordReader.java:85) at
org.apache.pig.impl.io.TFileStorage.getNext(TFileStorage.java:76) at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:187) at
org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:474) at
org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67) at
org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143) at
org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:676) at
org.apache.hadoop.mapred.MapTask.run(MapTask.java:336) at org.apache.hadoop.mapred.Child$4.run(Child.java:242) at
java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:396) at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059) at
org.apache.hadoop.mapred.Child.main(Child.java:236)
{code}

The workaround is to turn off the compression (pig.tmpfilecompression=false).

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Apr/11 21:34;rding;PIG-1977.patch;https://issues.apache.org/jira/secure/attachment/12475843/PIG-1977.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-08 23:10:07.343,,,no_permission,,,,,,,,,,,,165257,Reviewed,,,Mon Apr 11 16:52:59 UTC 2011,,,,,,,0|i0h0af:,97309,,,,,,,,,,"08/Apr/11 21:34;rding;TFile stores records in chunk encoded format. After reading a record, the cursor must be moved to the end of record.",08/Apr/11 23:10;thejas;Looks good . +1,11/Apr/11 16:52;rding;Unit tests pass. Patch committed to trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
One more TwoLevelAccess to remove,PIG-1976,12503683,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,07/Apr/11 22:06,04/Aug/11 00:35,14/Mar/19 03:07,25/Apr/11 21:10,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"We removed two level access in PIG-847. However, there is another occurrence we miss in ResourceSchema.java:

{code}
            if (type == DataType.BAG && fieldSchema.schema != null
                    && !fieldSchema.schema.isTwoLevelAccessRequired()) { 
                log.info(""Insert two-level access to Resource Schema"");
                FieldSchema fs = new FieldSchema(""t"", fieldSchema.schema);
                inner = new Schema(fs);                
            }
{code}

Though by default schema.isTwoLevelAccessRequired is false, we shall not use this flag in Pig. User could set this flag in legacy UDF.

Thanks Woody uncovered this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Apr/11 21:17;daijy;PIG-1976-1.patch;https://issues.apache.org/jira/secure/attachment/12477040/PIG-1976-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-25 19:47:33.038,,,no_permission,,,,,,,,,,,,66166,Reviewed,,,Mon Apr 25 21:10:04 UTC 2011,,,,,,,0|i0h0a7:,97308,,,,,,,,,,25/Apr/11 19:47;rding;+1,25/Apr/11 21:10;daijy;Patch committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Need to provide backward compatibility for legacy LoadCaster (without bytesToMap(bytes, fieldSchema))",PIG-1975,12503590,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,07/Apr/11 05:47,04/Aug/11 00:34,14/Mar/19 03:07,20/Apr/11 05:52,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"Pig changed LoadCaster interface in 0.9 for typed map (PIG-1876). We change
bytesToMap(byte[] b, ResourceFieldSchema fieldSchema)
to
bytesToMap(byte[] b)

We should provide backward compatibility for old LoadCaster. If we don't find the new bytesToMap, we use the old bytesToMap to convert bytes to map with bytearray value. It is still wrong but at least matching the ability of old behavior.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Apr/11 05:50;daijy;PIG-1975-1.patch;https://issues.apache.org/jira/secure/attachment/12475668/PIG-1975-1.patch,19/Apr/11 00:33;daijy;PIG-1975-2.patch;https://issues.apache.org/jira/secure/attachment/12476676/PIG-1975-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-19 00:40:05.786,,,no_permission,,,,,,,,,,,,66336,Reviewed,,,Wed Apr 20 06:01:03 UTC 2011,,,,,,,0|i0h09z:,97307,,,,,,,,,,07/Apr/11 05:50;daijy;No test added since it is impossible to make legacy LoadCaster compile in new code. Manual test success.,19/Apr/11 00:33;daijy;PIG-1975-2.patch fix javac warnings.,"19/Apr/11 00:40;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/622/
-----------------------------------------------------------

Review request for pig and Richard Ding.


Summary
-------

See PIG-1975


This addresses bug PIG-1975.
    https://issues.apache.org/jira/browse/PIG-1975


Diffs
-----

  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POCast.java 1094799 

Diff: https://reviews.apache.org/r/622/diff


Testing
-------

Test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

No test added since it is impossible to make legacy LoadCaster compile in new code.

Unit-test:
    all pass

Manual test:
    Bring a legacy LoadCaster class file and run it over trunk, it succeed.


Thanks,

Daniel

",19/Apr/11 17:21;rding;+1,20/Apr/11 06:01;daijy;Patch committed to trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lineage need to set for every cast,PIG-1974,12503589,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,daijy,daijy,07/Apr/11 05:37,04/Aug/11 00:34,14/Mar/19 03:07,12/Apr/11 18:44,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"The following script does not cast correctly:
{code}
a = load '1.txt' as (m:map[]);
b = foreach a generate (map[chararray])m;
dump b;
{code}
Pig throw warning that it cannot cast (map[chararray]) because caster is null.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Apr/11 16:01;thejas;PIG-1974.1.patch;https://issues.apache.org/jira/secure/attachment/12476130/PIG-1974.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-12 18:44:34.701,,,no_permission,,,,,,,,,,,,66250,,,,Tue Apr 12 18:44:34 UTC 2011,,,,,,,0|i0h09r:,97306,,,,,,,,,,"07/Apr/11 05:39;daijy;The issue is because lineage only set caster if the input data type is bytearray. However, if it is complex data type, we need to cast inner bytearray to real type, we still rely on caster. So we need to set caster as well.",12/Apr/11 18:15;daijy;+1 if tests pass.,"12/Apr/11 18:44;thejas;unit tests and test-patch passed.
Patch committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typechecking warnings are no longer displayed,PIG-1969,12503569,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,olgan,olgan,06/Apr/11 23:17,04/Aug/11 00:34,14/Mar/19 03:07,08/Apr/11 01:32,,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"Script:

a = load '/user/hadoopqa/pig/tests/data/singlefile/studenttab10k' as (name:chararray, age, gpa);
b = foreach a generate age + 1, gpa + 0.1f;
describe b;

In 0.8, we see the following warnings:

IMPLICIT_CAST_TO_FLOAT 1 time.*)|(Encountered Warning IMPLICIT_CAST_TO_FLOAT 1 time.*\n.*Encountered Warning
IMPLICIT_CAST_TO_INT 1 time.*)"",

They are missing from 0.9",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-04-08 01:32:04.744,,,no_permission,,,,,,,,,,,,66290,,,,Fri Apr 08 01:32:04 UTC 2011,,,,,,,0|i0h08n:,97301,,,,,,,,,,"08/Apr/11 01:32;thejas;The warnings are being printed in trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to document embeding in Java,PIG-1968,12503551,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,olgan,olgan,06/Apr/11 20:30,04/Aug/11 00:35,14/Mar/19 03:07,22/Apr/11 23:01,,,,,,,,0.9.0,,,,documentation,,,0,,,,,,,,,,,,,We have a small snipped in the setup but we should now provide the information in the control structure section,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-04-22 23:01:29.796,,,no_permission,,,,,,,,,,,,66103,,,,Fri Apr 22 23:01:29 UTC 2011,,,,,,,0|i0h08f:,97300,,,,,,,,,,"15/Apr/11 23:56;olgan;There is a section on Embedding in the Getting Started guide. It needs to be moved under Control Structures with 2 changes:

(1) Need to specify that PigServer is the main class that allows to embed Pig in Java and give a link to its javadoc.
(2) Need to mention that PigServer can be instanciated from multiple threads. (This is new in Pig 0.9) In the past, PigServer contained references to static data that prevented multiple instances of the object to be created from different threads within your application. This has been addressed in 0.9. Please note PigServer is not thread safe and the same object can't be shared accross multiple threads. ",22/Apr/11 23:01;chandec;Docs updated. See PIG-1772 and patch pig-1772-beta2-1.patch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigStorageSchema fails if a column value is null,PIG-1964,12503432,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,05/Apr/11 18:37,25/Apr/11 21:27,14/Mar/19 03:07,06/Apr/11 14:04,0.8.0,0.9.0,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"If the data being loaded by PigStorageSchema has a column with null value, the pig query fails with a NullPointerException.

The issue is currently seen in 0.8 branch in svn, not in the 0.8 apache release.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Apr/11 18:44;thejas;PIG-1964.0.patch;https://issues.apache.org/jira/secure/attachment/12475518/PIG-1964.0.patch,05/Apr/11 22:05;thejas;PIG-1964.1.patch;https://issues.apache.org/jira/secure/attachment/12475534/PIG-1964.1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-05 22:20:12.899,,,no_permission,,,,,,,,,,,,165252,Reviewed,,,Wed Apr 06 14:04:39 UTC 2011,,,,,,,0|i0h07j:,97296,,,,,,,,,,"05/Apr/11 18:40;thejas;This bug is related to the change in PIG-1830.

{code}
grunt> cat students2.pigstore
datestamp       vtestid total
{""fields"":[{""name"":""datestamp"",""type"":55,""description"":""autogenerated from Pig Field Schema"",""schema"":null},{""name"":""vtestid"",""type"":55,""description"":""autogenerated from Pig Field Schema"",""schema"":null},{""name"":""total"",""type"":15,""description"":""autogenerated from Pig Field Schema"",""schema"":null}],""version"":0,""sortKeys"":[],""sortKeyOrders"":[]}alice   F       1
bob     M       2
carol   F       3
doug            1
ivan    M       3
jill    F       1
grunt>
grunt> l = load 'students2.pigstore' using  org.apache.pig.piggybank.storage.PigStorageSchema();
grunt> dump l;
...
...
java.lang.NullPointerException
        at org.apache.pig.piggybank.storage.PigStorageSchema.getNext(PigStorageSchema.java:95)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:187)
        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:423)
        at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)


{code}

","05/Apr/11 18:44;thejas;PIG-1964.0.patch - initial patch, need to add unit test.
",05/Apr/11 22:05;thejas;PIG-1964.1.patch - patch with testcase.,05/Apr/11 22:20;dvryaboy;+1,"06/Apr/11 14:04;thejas;piggybank unit tests and test-patch passed. Patch committed to trunk and 0.8 branch.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"in nested foreach, accumutive udf taking input from order-by does not get results in order",PIG-1963,12503350,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,04/Apr/11 22:18,25/Apr/11 21:27,14/Mar/19 03:07,08/Apr/11 19:30,0.8.0,0.9.0,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"This happens only when secondary sort is not being used for the order-by. 
For example -
{code}
a1 = load 'fruits.txt' as (f1:int,f2);
a2 = load 'fruits.txt' as (f1:int,f2);

b = cogroup a1 by f1, a2 by f1;

d = foreach b {
   sort1 = order a1 by f2;
   sort2 = order a2 by f2; -- secondary sort not getting used here, MYCONCATBAG gets results in wrong order
   generate group, MYCONCATBAG(sort1.f1), MYCONCATBAG(sort2.f2);
}

-- explain d;
dump d;
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Apr/11 22:24;thejas;MYCONCATBAG.java;https://issues.apache.org/jira/secure/attachment/12475439/MYCONCATBAG.java,08/Apr/11 19:22;thejas;PIG-1963.1.1.patch;https://issues.apache.org/jira/secure/attachment/12475834/PIG-1963.1.1.patch,07/Apr/11 16:22;thejas;PIG-1963.1.patch;https://issues.apache.org/jira/secure/attachment/12475720/PIG-1963.1.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-04-08 08:17:37.494,,,no_permission,,,,,,,,,,,,165251,,,,Fri Apr 08 19:30:23 UTC 2011,,,,,,,0|i0h07b:,97295,,,,,,,,,,"04/Apr/11 22:22;thejas;MYCONCATBAG udf in the query in description concatenates the entries in the bag, in the order it is recieved.
When the query run with the property - pig.accumulative.batchsize=2 , 
and input -
{code}
100     apple
200     orange
300     strawberry
300     pear
100     apple
300     pear
400     apple
{code}

gives output -
{code}
(100,(100)(100),(apple)(apple))
(200,(200),(orange))
(300,(300)(300)(300),(pear)(strawberry)(pear)) -- this should be (300,(300)(300)(300),(pear)(pear)(strawberry))
(400,(400),(apple))
{code}",04/Apr/11 22:24;thejas;attaching udf used in the example.,"04/Apr/11 22:34;thejas;Note that the issue is seen only when there are more than 20000 in the bag used by the nested order-by statement, or the value of pig.accumulative.batchsize property if it is set.

The is happening because in accumulative mode the nested relational operator is being passed a portion of the bag. That works fine in case of operations such as filter or limit. If secondary sort is used for the ordering, there is no POSort in the plan, so it works fine.

This issue might exist in case of nested distinct as well, because it is also supposed to be a blocking operation.

Another query which demonstrates this issue (when property pig.accumulative.batchsize=2 is set)

{code}
a1 = load 'fruits.txt' as (cid:int,fruit : chararray);

b = group a1 by cid;

d = foreach b {
  sort1 = order a1 by fruit ;
  sort2 = order a1 by fruit desc;
  generate group as cid, MYCONCATBAG(sort1.fruit), MYCONCATBAG(sort2.fruit); -- The second instance of the udf does not get sorted results
}

explain d;
 dump d;
{code}

To fix this, if such blocking relational operators exist in the plan after secondary-sort optimization, accumulative mode should be disabled by the optimizer.
","07/Apr/11 16:22;thejas;PIG-1963.1.patch - unit tests, test-patch with 0.8 branch. Running them for trunk.","08/Apr/11 08:17;daijy;This prevent other nested relational operator, right? From the way checkUDFInput handles PORelationToExprProject, it seems the original code intend to prevent other relational operator (except foreach) as well, but miss the case POProject following PORelationToExprProject. +1 for the patch if this is the case.","08/Apr/11 16:05;thejas;bq. This prevent other nested relational operator, right? From the way checkUDFInput handles PORelationToExprProject, it seems the original code intend to prevent other relational operator (except foreach) as well, but miss the case POProject following PORelationToExprProject. +1 for the patch if this is the case.

Yes, the AccumulatorOptimizer code intends to turn off accumulative mode if it sees any relational operator other than POForEach and POSortedDistinct as input to accumulative udf. The case of POProject should have been handled like PORelationToExprProject. 
","08/Apr/11 16:11;thejas;The AccumulatorOptimizer should allow accumulative mode to be used if the input relation is a non-blocking relation like filter or limit. I have created PIG-1980 to address that.
","08/Apr/11 19:22;thejas;PIG-1963.1.1.patch - patch to remove a test case added in PIG-1911, as that query will no longer run in accumulative mode.","08/Apr/11 19:30;thejas;Patch committed to trunk and 0.8 branch.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong alias assinged to store operator,PIG-1962,12503345,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,rding,rding,04/Apr/11 21:20,04/Aug/11 00:34,14/Mar/19 03:07,20/Apr/11 06:03,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"Given the script

{code}
A = load 'data' as (a0, a1);
B = filter A by a0 > 1;
store B into 'output'; 
{code}

Alias 'A' (instead of 'B') is assigned to the store operator:

{code}
Map Plan
A: Store(file:///Users/trunk/output:org.apache.pig.builtin.PigStorage) - scope-11
|
|---A: New For Each(false,false)[bag] - scope-10
    |   |
    |   Project[bytearray][0] - scope-6
    |   |
    |   Project[bytearray][1] - scope-8
    |
    |---B: Filter[bag] - scope-1
        |   |
        |   Greater Than[boolean] - scope-5
        |   |
        |   |---Cast[int] - scope-3
        |   |   |
        |   |   |---Project[bytearray][0] - scope-2
        |   |
        |   |---Constant(1) - scope-4
        |
        |---A: Load(file:///Users/trunk/data:org.apache.pig.builtin.PigStorage) - scope-0--------
{code}

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Apr/11 23:29;daijy;PIG-1962-1.patch;https://issues.apache.org/jira/secure/attachment/12476187/PIG-1962-1.patch,19/Apr/11 00:32;daijy;PIG-1962-2.patch;https://issues.apache.org/jira/secure/attachment/12476675/PIG-1962-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-19 00:32:20.089,,,no_permission,,,,,,,,,,,,66237,Reviewed,,,Wed Apr 20 06:03:53 UTC 2011,,,,,,,0|i0h073:,97294,,,,,,,,,,19/Apr/11 00:32;daijy;PIG-1962-2.patch fixed unit test failures.,"19/Apr/11 00:36;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/620/
-----------------------------------------------------------

Review request for pig and Richard Ding.


Summary
-------

See PIG-1962


This addresses bug PIG-1962.
    https://issues.apache.org/jira/browse/PIG-1962


Diffs
-----

  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java 1094667 
  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/relational/LogToPhyTranslationVisitor.java 1094667 
  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/visitor/StoreAliasSetter.java PRE-CREATION 
  http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/TestPlanGeneration.java PRE-CREATION 
  http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/Util.java 1094667 

Diff: https://reviews.apache.org/r/620/diff


Testing
-------

Test-patch:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Unit test:
    all pass


Thanks,

Daniel

",19/Apr/11 16:42;rding;+1,20/Apr/11 06:03;daijy;Patch committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Pig prints ""null"" as file name in case of grammar error",PIG-1961,12503335,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,04/Apr/11 20:07,04/Aug/11 00:35,14/Mar/19 03:07,08/Apr/11 16:54,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"Due to macro related check. The following is the symptom:

grunt> A = load ^ 'x';
2011-04-04 13:05:20,336 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 0: <file null, line 6, column 9> Unexpected character '^'
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Apr/11 23:53;xuefuz;PIG-1961.patch;https://issues.apache.org/jira/secure/attachment/12475546/PIG-1961.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-08 08:20:58.124,,,no_permission,,,,,,,,,,,,66109,,,,Fri Apr 08 16:54:56 UTC 2011,,,,,,,0|i0h06v:,97293,,,,,,,,,,"05/Apr/11 23:53;xuefuz;     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
",06/Apr/11 05:31;xuefuz;Unit test also passed.,08/Apr/11 08:20;daijy;+1,08/Apr/11 16:54;xuefuz;Patch is committed to trunk.,08/Apr/11 16:54;xuefuz;Issue is resolved.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Pig CookBook documentation ""Map key should be quoted""",PIG-1960,12503316,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,daijy,daijy,04/Apr/11 17:29,04/Aug/11 00:35,14/Mar/19 03:07,22/Apr/11 22:56,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"There are two places in cookbook refer to a map key, which should be quoted:
B = foreach A generate in#k1 as k1, in#k2 as k2;
==> B = foreach A generate in#'k1' as k1, in#'k2' as k2;

B = foreach A generate CONCAT(in#k1, in#k2);
==> B = foreach A generate CONCAT(in#'k1', in#'k2');
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-04-22 22:56:06.355,,,no_permission,,,,,,,,,,,,66165,,,,Fri Apr 22 22:56:06 UTC 2011,,,,,,,0|i0h06n:,97292,,,,,,,,,,22/Apr/11 22:56;chandec;Docs updated. See PIG-1772 and patch pig-1772-beta2-1.patch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regression: Pig doesn't log type cast warning messages,PIG-1958,12503172,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,01/Apr/11 20:37,04/Aug/11 00:34,14/Mar/19 03:07,05/Apr/11 16:12,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"For a query such as this:

a = load 'x' as (name:chararray, age, gpa);
b = foreach a generate age + 1, gpa + 0.1f;
describe b;

previous Pig logs warning messages as:

2011-04-01 13:30:12,377 [main] WARN  org.apache.pig.PigServer - Encountered Warning IMPLICIT_CAST_TO_INT 1 time(s).
2011-04-01 13:30:12,377 [main] WARN  org.apache.pig.PigServer - Encountered Warning IMPLICIT_CAST_TO_FLOAT 1 time(s).

These warnings are no longer printed in the pig 0.9.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,66346,,,,Tue Apr 05 16:12:44 UTC 2011,,,,,,,0|i0h067:,97290,,,,,,,,,,05/Apr/11 16:12;xuefuz;Fixed by the fix for PIG-1956,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig parser gives misleading error message when the next foreach block has syntactic errors,PIG-1957,12503165,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,01/Apr/11 19:11,04/Aug/11 00:34,14/Mar/19 03:07,05/Apr/11 16:12,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"For the following pig script:

a = load ':INPATH:/singlefile/studenttab10k' as (name, age, gpa);
b = group a by name;
c = foreach b { ba = filter a by age < '25'; bb = foreach ba generate gpa; generate group, flatten(bb);}

parser gives the following error message:

<line 3, column 14> mismatched input '{' expecting GENERATE

The error seems to indicate that we do not support nested foreach, which is misleading. The error message needs to be improved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,66271,,,,Tue Apr 05 16:12:15 UTC 2011,,,,,,,0|i0h05z:,97289,,,,,,,,,,05/Apr/11 16:12;xuefuz;Fixed by the fix for PIG-1956,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig parser shouldn't log error code 0,PIG-1956,12503164,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,xuefuz,xuefuz,xuefuz,01/Apr/11 19:07,04/Aug/11 00:35,14/Mar/19 03:07,05/Apr/11 16:11,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"For the following pig script:

a = load 'x' as (name, age, gpa);
b = group a by name;
c = foreach b { ba = filter a by age < '25'; bb = foreach ba generate gpa; generate group, flatten(bb);}

Parser gives the following error:

ERROR 0: <line 3, column 14> ...

It should give a more meaningful error message.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Apr/11 18:14;xuefuz;PIG-1956.patch;https://issues.apache.org/jira/secure/attachment/12475394/PIG-1956.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-04 22:02:18.371,,,no_permission,,,,,,,,,,,,66152,,,,Tue Apr 05 16:11:37 UTC 2011,,,,,,,0|i0h05r:,97288,,,,,,,,,,"04/Apr/11 18:15;xuefuz;Patch fixing PIG-1956, 1957, and 1958","04/Apr/11 20:11;xuefuz;Test-patch run (additional javac warnings are due to generated code):

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     -1 javac.  The applied patch generated 962 javac compiler warnings (more than the trunk's current 958 warnings).
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
",04/Apr/11 22:02;daijy;+1,04/Apr/11 22:07;thejas;Looks good. +1,"04/Apr/11 22:18;xuefuz;Test-patch run:

     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
","05/Apr/11 16:10;xuefuz;Unit test passed.
Patch is committed to trunk.",05/Apr/11 16:11;xuefuz;Fixed in revision 1089102,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"PhysicalOperator has a member variable (non-static) Log object that is non-transient, this causes serialization errors",PIG-1955,12503058,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,woody.anderson@gmail.com,woody.anderson@gmail.com,woody.anderson@gmail.com,31/Mar/11 18:26,23/Apr/12 17:13,14/Mar/19 03:07,06/Apr/11 17:11,0.8.0,0.9.0,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"I found this while trying to write unit tests. Creating a local PigServer to test my LoadFunc caused a serialization of the PhysicalOperator class, which failed due to:
..
Caused by: java.io.NotSerializableException: org.apache.commons.logging.impl.Log4JCategoryLog
..


this is easily fixed by adding the transient keyword to the definition of log.

e.g.

on trunk:
    private final transient Log log = LogFactory.getLog(getClass());
on the 0.8 tag:
    private transient Log log = LogFactory.getLog(getClass());

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,31/Mar/11 19:20;woody.anderson@gmail.com;1955-po.patch;https://issues.apache.org/jira/secure/attachment/12475143/1955-po.patch,23/Apr/12 17:13;jeromatron;1955-static-0_8_1.patch;https://issues.apache.org/jira/secure/attachment/12523821/1955-static-0_8_1.patch,21/Sep/11 15:32;jeromatron;1955-static-0_8_1.patch;https://issues.apache.org/jira/secure/attachment/12495382/1955-static-0_8_1.patch,04/Apr/11 21:19;woody.anderson@gmail.com;1955-static.patch;https://issues.apache.org/jira/secure/attachment/12475429/1955-static.patch,31/Mar/11 18:27;woody.anderson@gmail.com;1955.patch;https://issues.apache.org/jira/secure/attachment/12475141/1955.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-04-01 20:31:30.841,,,no_permission,,,,,,,,,,,,35765,Reviewed,,,Mon Apr 23 17:13:40 UTC 2012,,,Patch Available,,,,0|i0h05j:,97287,"fix Serialzeable bug in PhysicalOperator, mark log as transient",,,,,,,,,"31/Mar/11 18:27;woody.anderson@gmail.com;this doesn't have all of the unit test trimmings, but is that all really needed to mark a logger as transient?","31/Mar/11 19:20;woody.anderson@gmail.com;Ok. Unfortunately, this issue is more pervasive than i originally thought.

The 'simple' fix that is attached makes  the PO logger transient protected and  removes the loggers from all subclasses which are defined often incorrectly (non-transient members) and inconsistently.

Personally, when i define loggers i always make them private and STATIC so that there is no getClass() call. this makes finding the class where the log line resides in source code much simpler to find. I dislike loggers that define themselves with getClass() b/c logging code in A.java will report as class B in output if class B extends class A.

I did not change this behavior b/c perhaps someone has their reasons for doing what they did. I did however remove some of the static loggers simply to ensure consistency (the majority were done with member variables).

The change to static private is also not such a big deal if anyone agrees we should consistently go that way instead.","01/Apr/11 20:31;rding;I think we should make loggers static (and final). The problem with transient is that it doesn't work the way you expect. As it is transient, the logger would not be serialized, and when the object is deserialized, the logger is initialized to null (default value). ","04/Apr/11 21:19;woody.anderson@gmail.com;Agreed, i prefer the static approach.","04/Apr/11 22:31;dvryaboy;Guys, does this look like a manifestation of the same bug? Email on the pig user list: http://pig.markmail.org/thread/4s76qv3eo6ivszee",04/Apr/11 23:55;rding;It's different. Log4JLogger implements Serializable interface. The error in the email happened during the deserialization. It looks like a class mismatch between client and server.  ,05/Apr/11 00:31;rding;Patch looks good. I'll commit the patch after running test-patch and unit tests.,"06/Apr/11 17:09;rding;Test-patch result:

{code}
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{code}

Unit tests pass (with update of one golden file).",06/Apr/11 17:11;rding;Patch committed to trunk. Thanks Woody!,"20/Sep/11 19:18;jeromatron;Not sure if there are any updates to 0.8.x planned, but if so, would be nice to backport since it's a pretty clean backport.  I can attach an updated patch if so.  (There were just a few places where the hunk fails locally, but applying the changes manually worked)","21/Sep/11 01:21;alangates;I'm not aware of any plans to make a 0.8.2 release, but you can upload a patch here in case others want the same fix.",21/Sep/11 15:32;jeromatron;Attaching a patch that cleanly applies against 0.8.1.  I tested it with what was locally failing and it fixed the problem.,23/Apr/12 17:13;jeromatron;Granting license.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e2e test harness should use bin/pig rather than calling java directly,PIG-1949,12503033,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,alangates,alangates,alangates,31/Mar/11 15:36,26/Apr/12 20:32,14/Mar/19 03:07,09/May/11 20:29,,,,,,,,0.10.0,,,,tools,,,0,,,,,,,,,,,,,Currently TestDriverPig.pm uses java directly to invoke Pig.  It should use the bash shell script bin/pig instead.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30/Apr/11 00:48;alangates;PIG-1949.patch;https://issues.apache.org/jira/secure/attachment/12477860/PIG-1949.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-05 22:43:27.662,,,no_permission,,,,,,,,,,,,165247,,,,Mon May 09 20:29:01 UTC 2011,,,,,,,0|i0h047:,97281,,,,,,,,,,30/Apr/11 00:48;alangates;Fixes to TestDriverPig to make it work with bin/pig instead of using Java directly.  I also yanked the code out of TestDriverPig that mimicked the runTests function in TestDriver.,05/May/11 22:43;daijy;+1,09/May/11 20:29;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
register javascript UDFs does not work ,PIG-1944,12502780,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,julienledem,julienledem,julienledem,30/Mar/11 00:31,04/Aug/11 00:34,14/Mar/19 03:07,06/Apr/11 21:43,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"The following does not work because of a bug in the JSScriptEngine initialization
Register 'test.js' using javascript as myfuncs;
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30/Mar/11 00:33;julienledem;PIG-1944-trunk.patch;https://issues.apache.org/jira/secure/attachment/12474944/PIG-1944-trunk.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-30 18:35:38.158,,,no_permission,,,,,,,,,,,,66357,,,,Wed Apr 06 21:43:59 UTC 2011,,,Patch Available,,,,0|i0h02v:,97275,,,,,,,,,,30/Mar/11 00:33;julienledem;PIG-1944-trunk.patch fixes the bug in trunk,30/Mar/11 18:35;rding;+1. Please commit if test-patch passes.,"01/Apr/11 23:36;julienledem;     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 556 release audit warnings (more than the trunk's current 554 warnings).

the new warnings are related to HTML files
$ cat ~/tmp/releaseAuditDiffWarnings.txt 

142d141
<      [java]  !????? /homes/ledemj/svn/apache/pig/trunk/build/pig-0.9.0-SNAPSHOT/docs/api/overview-frame.html
144d142
<      [java]  !????? /homes/ledemj/svn/apache/pig/trunk/build/pig-0.9.0-SNAPSHOT/docs/api/org/apache/pig/package-tree.html
","06/Apr/11 21:04;olgan;Julien, are you ready to commit this patch?",06/Apr/11 21:43;julienledem;committed in TRUNK,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
order-by statement should support project-range to-end in any position among the sort columns if input schema is known,PIG-1939,12502574,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,28/Mar/11 12:59,04/Aug/11 00:35,14/Mar/19 03:07,20/Apr/11 19:20,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"In order-by statements, if the input schema is known (non-null), then there should not be any restrictions on the use of project-to-end project-range expressions (eg. x ..).
The restriction that it has to be the last sort column should be applicable only when the input schema is null .
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Apr/11 16:24;thejas;PIG-1939.1.patch;https://issues.apache.org/jira/secure/attachment/12476625/PIG-1939.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-20 17:26:07.832,,,no_permission,,,,,,,,,,,,66178,,,,Wed Apr 20 19:20:26 UTC 2011,,,,,,,0|i0h01r:,97270,,,,,,,,,,20/Apr/11 17:26;daijy;+1 if tests pass.,"20/Apr/11 19:20;thejas;Unit test and test-patch passed. Patch committed to trunk and 0.9 branch.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New logical plan: Should not push up filter in front of Bincond,PIG-1935,12502341,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,25/Mar/11 01:33,25/Apr/11 21:27,14/Mar/19 03:07,04/Apr/11 20:46,0.8.0,,,,,,,0.8.1,,,,impl,,,0,,,,,,,,,,,,,"The following script produce wrong result:
{code}
data = LOAD 'data.txt' using PigStorage() as (referrer:chararray, canonical_url:chararray, ip:chararray);
best_url = FOREACH data GENERATE ((canonical_url != '' and canonical_url is not null) ? canonical_url : referrer) AS url, ip;
filtered = FILTER best_url BY url == 'badsite.com';
dump filtered;
{code}
data.txt:
badsite.com             127.0.0.1
goodsite.com/1?foo=true goodsite.com    127.0.0.1

Expected:
(badsite.com,127.0.0.1)

We get nothing.

Thanks Corbin Hoenes for reporting.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Mar/11 01:40;daijy;PIG-1935-1.patch;https://issues.apache.org/jira/secure/attachment/12474584/PIG-1935-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-04 18:12:05.906,,,no_permission,,,,,,,,,,,,165243,Reviewed,,,Mon Apr 04 20:46:38 UTC 2011,,,,,,,0|i0h00v:,97266,,,,,,,,,,"25/Mar/11 01:35;daijy;It fail because we erroneous push filter above foreach. In BinCond case, we shall disable that by setting a new uid for BinCond output.","04/Apr/11 18:12;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/544/
-----------------------------------------------------------

(Updated 2011-04-04 18:10:55.974552)


Review request for pig and thejas.


Summary
-------

The following script produce wrong result:

data = LOAD 'data.txt' using PigStorage() as (referrer:chararray, canonical_url:chararray, ip:chararray);
best_url = FOREACH data GENERATE ((canonical_url != '' and canonical_url is not null) ? canonical_url : referrer) AS url, ip;
filtered = FILTER best_url BY url == 'badsite.com';
dump filtered;

data.txt:
badsite.com 127.0.0.1
goodsite.com/1?foo=true goodsite.com 127.0.0.1

Expected:
(badsite.com,127.0.0.1)

We get nothing.


This addresses bug PIG-1935.
    https://issues.apache.org/jira/browse/PIG-1935


Diffs
-----

  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/expression/BinCondExpression.java 1085215 
  http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/TestNewPlanFilterAboveForeach.java 1085215 

Diff: https://reviews.apache.org/r/544/diff


Testing
-------

test-patch:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Unit test:
    all pass

End-to-end test:
    all pass


Thanks,

Daniel

","04/Apr/11 19:07;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/544/#review382
-----------------------------------------------------------

Ship it!


+1

- thejas


On 2011-04-04 18:10:55, Daniel Dai wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/544/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-04-04 18:10:55)
bq.  
bq.  
bq.  Review request for pig and thejas.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  The following script produce wrong result:
bq.  
bq.  data = LOAD 'data.txt' using PigStorage() as (referrer:chararray, canonical_url:chararray, ip:chararray);
bq.  best_url = FOREACH data GENERATE ((canonical_url != '' and canonical_url is not null) ? canonical_url : referrer) AS url, ip;
bq.  filtered = FILTER best_url BY url == 'badsite.com';
bq.  dump filtered;
bq.  
bq.  data.txt:
bq.  badsite.com 127.0.0.1
bq.  goodsite.com/1?foo=true goodsite.com 127.0.0.1
bq.  
bq.  Expected:
bq.  (badsite.com,127.0.0.1)
bq.  
bq.  We get nothing.
bq.  
bq.  
bq.  This addresses bug PIG-1935.
bq.      https://issues.apache.org/jira/browse/PIG-1935
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/expression/BinCondExpression.java 1085215 
bq.    http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/TestNewPlanFilterAboveForeach.java 1085215 
bq.  
bq.  Diff: https://reviews.apache.org/r/544/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  test-patch:
bq.       [exec] +1 overall.  
bq.       [exec] 
bq.       [exec]     +1 @author.  The patch does not contain any @author tags.
bq.       [exec] 
bq.       [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
bq.       [exec] 
bq.       [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
bq.       [exec] 
bq.       [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
bq.       [exec] 
bq.       [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
bq.       [exec] 
bq.       [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
bq.  
bq.  Unit test:
bq.      all pass
bq.  
bq.  End-to-end test:
bq.      all pass
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Daniel
bq.  
bq.

",04/Apr/11 20:46;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix zebra test TestCheckin1, TestCheckin4",PIG-1934,12502333,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,24/Mar/11 22:58,04/Aug/11 00:34,14/Mar/19 03:07,26/Mar/11 00:38,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"Zebra test TestCheckin1, TestCheckin4 fail after new parser check in. We need to fix them.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Mar/11 22:59;daijy;PIG-1934-1.patch;https://issues.apache.org/jira/secure/attachment/12474565/PIG-1934-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-25 19:47:29.662,,,no_permission,,,,,,,,,,,,66215,Reviewed,,,Sat Mar 26 00:38:36 UTC 2011,,,,,,,0|i0h00n:,97265,,,,,,,,,,25/Mar/11 19:47;xuefuz;+1,26/Mar/11 00:38;daijy;Committed to trunk. No unit test added since it only fix regressions.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Hints such as 'collected' and 'skewed' for ""group by"" or ""join by"" should not be treated as tokens.",PIG-1933,12502208,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,23/Mar/11 17:35,04/Aug/11 00:35,14/Mar/19 03:07,23/Mar/11 21:41,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"The current parser implementation takes hints used for group by or join by such as 'collected' and 'skewed' as tokens, which is different from the old parser, causing some undesirable side-effects. For instance, the following query will fail.

A = load 'skewed';

This needs to be corrected to be consistent with the old parser.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Mar/11 17:37;xuefuz;PIG-1933.patch;https://issues.apache.org/jira/secure/attachment/12474419/PIG-1933.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-23 21:07:01.786,,,no_permission,,,,,,,,,,,,66170,,,,Wed Mar 23 21:40:57 UTC 2011,,,,,,,0|i0h00f:,97264,,,,,,,,,,23/Mar/11 17:37;xuefuz;Unit tests passed. Test-patch run is in progress.,"23/Mar/11 21:00;xuefuz;test-patch run result:

     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
",23/Mar/11 21:07;thejas;+1,23/Mar/11 21:40;thejas;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GFCross should allow the user to set the DEFAULT_PARALLELISM value,PIG-1932,12502122,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,alangates,alangates,alangates,23/Mar/11 00:53,04/Aug/11 00:34,14/Mar/19 03:07,30/Mar/11 21:16,0.8.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"The internal UDF GFCross uses a final static int DEFAULT_PARALLELISM to determine how wide to spread the records in a cross.  It is currently hard wired to 96.  There are no comments in the code on how that value was settled on.  Despite the name, this value is not necessarily related to the reduce parallelism controlled by the parallel clause.  It controls how many artificial join key values are generated and how many times each record is duplicated before going through the join.  The higher it is set the more key values (and thus the less likely the cross will run out of memory) but also the more times each record is duplicated in the map phase before being sent to the reduce.  

We should leave the default value at 96 but allow a property to override this default and change the value.

We cannot use a constructor argument here because the use of the UDF is not exposed to the user, so he has no opportunity to pass a constructor argument to it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Mar/11 17:48;alangates;PIG-1932.patch;https://issues.apache.org/jira/secure/attachment/12474533/PIG-1932.patch,29/Mar/11 17:17;alangates;PIG-1932_2.patch;https://issues.apache.org/jira/secure/attachment/12474904/PIG-1932_2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-03-29 17:51:09.824,,,no_permission,,,,,,,,,,,,66285,,,,Wed Mar 30 21:16:44 UTC 2011,,,,,,,0|i0h007:,97263,,,,,,,,,,"24/Mar/11 17:48;alangates;Unit tests pass.  Results of test-patch:

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     -1 release audit.  The applied patch generated 545 release audit warnings (more than the trunk's current 544 warnings).
     [exec]

the new release audit warning is because I added a file.","24/Mar/11 23:49;alangates;Daniel convinced me I should use the parallelism value from the cross, since what's really important about this is how many join groups it creates.  You want to create enough groups to keep each reducers busy.","29/Mar/11 17:17;alangates;Commit test unit tests pass

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     -1 release audit.  The applied patch generated 552 release audit warnings (more than the trunk's current 550 warnings).
     [exec]
     [exec]
     [exec]

Release audit issues are due to new file and changes to javadocs in GFCross.",29/Mar/11 17:51;daijy;+1,30/Mar/11 21:16;alangates;Patch 2 checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integrate Macro Expansion with New Parser,PIG-1931,12502115,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,22/Mar/11 22:28,04/Aug/11 00:34,14/Mar/19 03:07,06/Apr/11 21:33,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"Currently Macro expansion is implemented as a preprocessor (PIG-1793) so that it can work with the old parser. Now the new parser replaced old parser in trunk and we can integrate macro expansion into the new parser. This has many advantages such as better error reporting, less code and making Macro part of Pig Latin.

To aid debugging, Pig command line option -r (dryrun) will produce a script with expanded macros (in addition to the script with substituted parameters).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Mar/11 22:32;rding;PIG-1931_1.patch;https://issues.apache.org/jira/secure/attachment/12474350/PIG-1931_1.patch,25/Mar/11 21:16;rding;PIG-1931_2.patch;https://issues.apache.org/jira/secure/attachment/12474664/PIG-1931_2.patch,30/Mar/11 22:07;rding;PIG-1931_3.patch;https://issues.apache.org/jira/secure/attachment/12475023/PIG-1931_3.patch,31/Mar/11 22:49;rding;PIG-1931_4.patch;https://issues.apache.org/jira/secure/attachment/12475163/PIG-1931_4.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2011-03-24 19:21:46.982,,,no_permission,,,,,,,,,,,,66261,Reviewed,,,Wed Apr 06 21:33:50 UTC 2011,,,,,,,0|i0gzzz:,97262,,,,,,,,,,"22/Mar/11 22:32;rding;Attaching the initial patch. 

Still need to integrate file name, line number into the parser tree.","24/Mar/11 19:21;xuefuz;I haven't finished reviewing (some code may require discussion in person), but here is my comments so far.

1. QueryParser.g
1) This file is for syntax check and AST generation. It's better to move the validation to one of the tree parsers.
2)For readability, please divide the long rules into sub-rules, each rule having only one root (^). Also, remove unnecessary token renaming such as rets+=alias.
3)Import and returns need to be added to eid (accross all tree parsers as well).

2. AliasMasker.g
   1) Please simplify, be consistent with QueryParser.g, as you don't generate pig script any more. For instance, 
 output_clause 
    : ^( OUTPUT stream_cmd ( stream_cmd)* )
it should be just ^( OUTPUT stream_cmd+ )

3. QueryParserDriver.java
1)Error reporting should be consistent with others (especially line numbers and offsets)
2)I don't see how currentImport is updated anywhere.
3)In inlineMacro(), I don't see how nodes variable is updated. Isn't it always empty?

4. MacroExpansionDebug.g
1) pasted/copied code should be removed. For example, getErrorMessage()
2) I see a few unused member variables/methods.
3) File name is confusing. It seems having nothing to do with macro expansion. It just prints a script from an AST.

5. PigMacro.java
1)We should restrain from throwing runtime exception
","24/Mar/11 20:34;alangates;Please make sure you post any results from your conversation here, so others can follow.","25/Mar/11 18:28;xuefuz;Some additional comments (It can be included in subsequent patches):

1. PigScriptParser.jj : multi assignment should cover the single assignment. No need to have both.


2. Need to add check to make sure that macro definition cannot happen in another macro definition.",25/Mar/11 21:16;rding;This patch added the script file name to the error messages and addressed the review comments. ,"25/Mar/11 23:09;rding;test-patch result:

{code}
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 30 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     -1 javac.  The applied patch generated 941 javac compiler warnings (more than the trunk's current 884 warnings).
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 550 release audit warnings (more than the trunk's current 548 warnings).
{code}

Release audit warnings are html related.",26/Mar/11 00:29;rding;Patch 2 committed to trunk.,"26/Mar/11 04:18;xuefuz;+1 Patch PIG-1931_2.patch looks good. Additional comments:

1) The ""main"" pig script file name should be available from Antlr parser, which can be set in the AST node.

2) If not done already, please test cases where a macro def is called more than one time.","30/Mar/11 19:25;xuefuz;Another observation: current the file name of the main script is set only on AST, so source file name will be there for errors reported by AST visitors or plan visitors. However, for syntax errors, which happen before AST tree is built, the file name is absent, which should be also part of error reporting.",30/Mar/11 22:07;rding;Patch 3 adds source name to the lexer and parser so that lexer/parser error messages contain the script file name. ,"30/Mar/11 23:47;xuefuz;About PIG-1931_3.patch:

1. I think it's a right approach to include the source file name in QueryParserStringStream. Once that happens, we can get the stored file name in every token by calling token.getInputStream().getSourceName(). Then, in the PigParserNodeAdapter, you can set the file name to each PigParserNode instance created. In this way, you set the file name while AST is generated, which eliminates the need of recursively set it as done currently at macro expansion time.

2. For the error header, <file myscript.pig, line 1, column 10> would be more consistent than <At myscript.pig, line 1, column 10>.",31/Mar/11 22:49;rding;Attaching a simplified version. Thanks Xuefu for suggestions.,31/Mar/11 23:19;xuefuz;+1 PIG-1931_4.patch looks good.,"01/Apr/11 16:21;rding;test-patch result:

{code}
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 8 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 556 release audit warnings (more than the trunk's current 555 warnings).
{code}

Release audit warning is html related.

Unit tests pass.",01/Apr/11 16:29;rding;patch 4 committed to trunk.,06/Apr/11 21:14;olgan;Can this ticket be closed?,06/Apr/11 21:33;rding;patches are committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Type checker failed to catch invalid type comparison,PIG-1929,12502068,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,xuefuz,xuefuz,22/Mar/11 17:58,04/Aug/11 00:34,14/Mar/19 03:07,15/Apr/11 15:57,,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"The following script should fail. However, it passed type checking, resulting a backend failure.

A =LOAD 'x' USING PigStorage() AS ( Fint:int, Flong:long, Fdouble:double, Ffloat:float, Fchar:chararray, Fchararray:chararray, Fbytearray:bytearray, Fmap:map[], Fbag:BAG{ t:tuple( name, age, avg ) }, Ftuple:( name:chararray, age:int, avg:float) );
B= FILTER A BY ( Ftuple gte ( 1, 2, 3 ));
STORE B INTO 'y' USING PigStorage();

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Apr/11 15:25;thejas;PIG-1929.1.patch;https://issues.apache.org/jira/secure/attachment/12476249/PIG-1929.1.patch,14/Apr/11 00:40;thejas;PIG-1929.2.patch;https://issues.apache.org/jira/secure/attachment/12476292/PIG-1929.2.patch,14/Apr/11 22:35;thejas;PIG-1929.3.patch;https://issues.apache.org/jira/secure/attachment/12476381/PIG-1929.3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-04-13 15:25:08.627,,,no_permission,,,,,,,,,,,,66265,,,,Fri Apr 15 15:57:45 UTC 2011,,,,,,,0|i0gzzj:,97260,,,,,,,,,,13/Apr/11 15:25;thejas;PIG-1929.1.patch - the patch also has a fix for issue in PIG-1928.,"14/Apr/11 00:40;thejas;PIG-1929.2.patch - Removes code that cast null-constants for == and != expressions, as that will not get used. It will get typecast as bytearray expression. Added test cases for casting of null constants in comparison binary expressions.",14/Apr/11 22:35;thejas;PIG-1929.3.patch - Also casts bytearray objects to Tuple/Map in == and != operations.,14/Apr/11 23:14;xuefuz;+1 for patch PIG-1929.3.patch.,15/Apr/11 15:57;thejas;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Type Checking, incorrect error message",PIG-1928,12502061,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,xuefuz,xuefuz,22/Mar/11 16:30,04/Aug/11 00:35,14/Mar/19 03:07,15/Apr/11 15:58,,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"Pig gives incorrect error message for the following query:

A =LOAD 'x' USING PigStorage() AS ( Fint:int, Flong:long, Fdouble:double, Ffloat:float, Fchar:chararray, Fchararray:chararray, Fbytearray:bytearray, Fmap:map[], Fbag:BAG{ t:tuple( name, age, avg ) }, Ftuple:( name:chararray, age:int, avg:float) );
B =GROUP A ALL; 
X =FOREACH B GENERATE A.Fint, SUM( A.Fint + ( 1 + 0) );
STORE X INTO 'y.out' USING PigStorage();

Error message given:

2011-03-21 16:53:14,602 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1039: incompatible types in Add Operator left hand side:Unknown null#63:tuple(Fint#42:int)  right hand side:Unknown

In 0.8, the following error msg is given:

2011-03-21 22:10:30,945 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1039: In alias X, incompatible types in Add Operator left hand side:bag right hand side:int


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-04-15 15:58:33.279,,,no_permission,,,,,,,,,,,,66175,,,,Fri Apr 15 15:58:33 UTC 2011,,,,,,,0|i0gzzb:,97259,,,,,,,,,,15/Apr/11 15:58;thejas;Fixed as part of patch in PIG-1929.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dereference partial name failed,PIG-1927,12502002,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,22/Mar/11 00:23,02/May/13 02:29,14/Mar/19 03:07,27/Mar/11 05:16,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"The following script fail:
{code}
a = load '1.txt' as (a0:int, a1);
b = group a by a0;
c = foreach b generate flatten(a);
d = cogroup c by (a0);
e = foreach d generate c.a0 as e0;
f = foreach e generate e0;
describe f;
{code}
Error message:
Caused by: Failed to generate logical plan. Nested exception: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 0: Cannot find field a0 in a::a0#17:int,a::a1#18:bytearray
        at org.apache.pig.parser.LogicalPlanGenerator.alias_col_ref(LogicalPlanGenerator.java:12835)
        at org.apache.pig.parser.LogicalPlanGenerator.col_ref(LogicalPlanGenerator.java:12697)
        at org.apache.pig.parser.LogicalPlanGenerator.projectable_expr(LogicalPlanGenerator.java:7715)
        at org.apache.pig.parser.LogicalPlanGenerator.var_expr(LogicalPlanGenerator.java:7491)
        at org.apache.pig.parser.LogicalPlanGenerator.expr(LogicalPlanGenerator.java:6904)
        at org.apache.pig.parser.LogicalPlanGenerator.flatten_generated_item(LogicalPlanGenerator.java:5235)
        at org.apache.pig.parser.LogicalPlanGenerator.generate_clause(LogicalPlanGenerator.java:11022)
        at org.apache.pig.parser.LogicalPlanGenerator.foreach_plan(LogicalPlanGenerator.java:10789)
        at org.apache.pig.parser.LogicalPlanGenerator.foreach_clause(LogicalPlanGenerator.java:10670)
        at org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1280)
        at org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:646)
        at org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:467)
        at org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:365)
        at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:64)

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PIG-1868,22/Mar/11 00:35;daijy;PIG-1927-1.patch;https://issues.apache.org/jira/secure/attachment/12474253/PIG-1927-1.patch,23/Mar/11 22:33;daijy;PIG-1927-2.patch;https://issues.apache.org/jira/secure/attachment/12474443/PIG-1927-2.patch,24/Mar/11 22:18;daijy;PIG-1927-3.patch;https://issues.apache.org/jira/secure/attachment/12474561/PIG-1927-3.patch,25/Mar/11 18:43;daijy;PIG-1927-4.patch;https://issues.apache.org/jira/secure/attachment/12474645/PIG-1927-4.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2011-03-25 19:46:23.488,,,no_permission,,,,,,,,,,,,66238,Reviewed,,,Sun Mar 27 05:58:10 UTC 2011,,,,,,,0|i0gzz3:,97258,,,,,,,,,,"22/Mar/11 00:26;daijy;Schema for d is:
group: int,c: {(a::a0: int,a::a1: bytearray)}

We try to dereference c.a0. However, dereference only handles complete name ""a::a0"", it cannot handle a0.",22/Mar/11 00:38;daijy;PIG-1927-1.patch requires PIG-1868,23/Mar/11 22:33;daijy;This script also reveal that we print null alias for error message. Attach PIG-1927-2.patch to further fix error message issue.,24/Mar/11 22:18;daijy;PIG-1927-3.patch include more error message fixes.,25/Mar/11 18:43;daijy;PIG-1927-4.patch fix unit test failures,25/Mar/11 19:46;xuefuz;+1 Patch looks good.,"27/Mar/11 05:58;daijy;Review note: https://reviews.apache.org/r/525/

Patch committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parser error message doesn't show location of the error or show it as Line 0:0,PIG-1925,12501963,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,21/Mar/11 17:10,04/Aug/11 00:34,14/Mar/19 03:07,22/Mar/11 21:31,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"When Parser emits an error from one of the tree parsers (like AST validation or plan generation), Line number is missing or printed as Line 0:0. Further investigation shows that Antlr 3.2 isn't able to report the correct line number and line offset. We need to address it in Pig code.

The following is an example.

grunt> A = load 'x' as (u, v, u, w);
2011-03-21 10:04:44,486 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 0: Duplicated alias in schema: u


It's desirable to have line number/offset in such kind of error messages.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Mar/11 22:46;xuefuz;PIG-1925-1.patch;https://issues.apache.org/jira/secure/attachment/12474246/PIG-1925-1.patch,21/Mar/11 17:15;xuefuz;PIG-1925.patch;https://issues.apache.org/jira/secure/attachment/12474202/PIG-1925.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-03-21 19:48:40.954,,,no_permission,,,,,,,,,,,,66324,,,,Tue Mar 22 21:31:45 UTC 2011,,,,,,,0|i0gzyn:,97256,,,,,,,,,,"21/Mar/11 17:15;xuefuz;With the patch, the error message is emitted as:

grunt> A = load 'x' as (u, v, u, w);
2011-03-21 10:10:27,067 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 0: Line 1:23 duplicated alias in schema: u

Unit test passed.
Test-patch run result (Additional JAVACC warnings due to generated code):

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     -1 javac.  The applied patch generated 888 javac compiler warnings (more than the trunk's current 877 warnings).
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
","21/Mar/11 17:26;xuefuz;Patch also includes fixes for default error messages generated by Antlr, ""no viable alternative at input ..."". Now the error messages are show as:

1. for lexer

grunt> A = load ^ as (u, v, w);
line 1:9 Unexpected character '^'
2011-03-21 10:21:04,147 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 0: line 1:9 Unexpected character '^'

2. for parser

grunt> A = laod 'x' as (u, v, w);
2011-03-21 10:21:53,517 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 0: line 1:0 Unexpected symbol at or near ['A']

(Please not that in the above case, parser isn't able to identify ""load"" is misspelled, but it's not what this fix is about. That case is tracked in a different JIRA.)

","21/Mar/11 19:48;alangates;A couple of comments.  One, 1:9.  I assume this means line 1 column 9.  It would be better to spell that out.

I think for parser errors it would be good to explicitly say ""Syntax error, unexpected symbol at or near 'A'"".  This way people explicitly know it's in their syntax.",21/Mar/11 22:46;xuefuz;Patch is updated based on Alan's comment above. Unit test and test-patch run are in progress.,"22/Mar/11 16:13;xuefuz;For PIG-1925-1.patch, unit test passed. Test-patch run (additional javacc warnings due to generated code):

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     -1 javac.  The applied patch generated 890 javac compiler warnings (more than the trunk's current 877 warnings).
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
",22/Mar/11 21:31;alangates;Patch 1925-1 checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jython UDFs fail to convert Maps of Integer values back to Pig types,PIG-1923,12501886,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,julienledem,julienledem,julienledem,19/Mar/11 23:05,04/Aug/11 00:35,14/Mar/19 03:07,06/Apr/11 21:44,0.8.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"java.io.IOException: Error executing function: org.apache.pig.backend.executionengine.ExecException: ERROR 0: Cannot convert jython type to pig datatype org.apache.pig.backend.executionengine.ExecException: ERROR 0: Cannot convert jython type to pig datatype java.lang.ClassCastException: java.lang.Integer cannot be cast to org.python.core.PyObject
    at org.apache.pig.scripting.jython.JythonFunction.exec(JythonFunction.java:109)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:229)
    ... 10 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Mar/11 22:19;julienledem;PIG-1923_branch-0.8.patch;https://issues.apache.org/jira/secure/attachment/12474131/PIG-1923_branch-0.8.patch,20/Mar/11 22:21;julienledem;PIG-1923_trunk.patch;https://issues.apache.org/jira/secure/attachment/12474132/PIG-1923_trunk.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-03-25 22:09:03.832,,,no_permission,,,,,,,,,,,,66108,,,,Wed Apr 06 21:44:56 UTC 2011,,,Patch Available,,,,0|i0gzy7:,97254,,,,,,,,,,20/Mar/11 05:40;julienledem;PIG-1923.patch fixes the bug in trunk,"20/Mar/11 22:19;julienledem;PIG-1923_branch-0.8.patch
fix for the branch 0.8","20/Mar/11 22:21;julienledem;PIG-1923_trunk.patch supersedes PIG-1923.patch
I forgot to include the new test case.
fix for trunk",25/Mar/11 22:09;rding;+1,"06/Apr/11 21:04;olgan;Julien, are you ready to commit this patch?",06/Apr/11 21:44;julienledem;committed in TRUNK,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve error messages in new parser,PIG-1921,12501843,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,rding,rding,18/Mar/11 23:13,04/Aug/11 00:34,14/Mar/19 03:07,19/Apr/11 20:34,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"The new parser gives better error messages than the old parser (especially with the line numbers). But there are still some places where the error messages can be improved.

Example:

{code}
ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1025: Invalid field projection. Projected field [E::v] does not exist in schema: A::v#28:long,A::u#29:chararray,A::w#30:bytearray,B::v#31:long,B::u#32:chararray,B::w#33:bytearray.
{code}

In this case the #num should be removed from the schema.

Example:

{code}
ERROR org.apache.pig.tools.grunt.Grunt - ERROR 0: line 5:34 mismatched input [''reg''] expecting set null
{code}

In this case the parser detects the error early and gives the line number, but missing the reason part from the old parser message:

{code}
ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Only COLLECTED, REGULAR or MERGE are valid GROUP modifiers.
{code}

Example:

{code}
grunt> C = join A by v, B by A.t;
2011-03-18 14:04:38,441 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 0: Invalid scalar projection: A
{code}

In this case the real reason is t is not a field of A : {v: int, u:int}. So the message should be: Invalid scalar projection: A.t

Example:

{code}
C = join A by v, B by B::A.v;
2011-03-18 14:40:11,590 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Invalid field reference. Referenced field [v] does not exist in schema: null.
{code}

The message here is not clear and the schema shouldn't be null.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Apr/11 20:31;xuefuz;PIG-1921-1.patch;https://issues.apache.org/jira/secure/attachment/12476775/PIG-1921-1.patch,15/Apr/11 18:08;xuefuz;PIG-1921.patch;https://issues.apache.org/jira/secure/attachment/12476467/PIG-1921.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-15 18:08:38.018,,,no_permission,,,,,,,,,,,,66186,,,,Tue Apr 19 20:35:17 UTC 2011,,,,,,,0|i0gzxr:,97252,Patch PIG-1921-1.patch is committed to trunk,,,,,,,,,"15/Apr/11 18:08;xuefuz;Fix the error message about ""schema: null"".
Scalar error is duplicated with PIG-1788.
Other issues mentioned have been fixed already.",18/Apr/11 17:23;daijy;+1,"18/Apr/11 21:40;xuefuz;Test-patch run: (no new files. Additional release warning seems bogus)

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     -1 release audit.  The applied patch generated 564 release audit warnings (more than the trunk's current 563 warnings).
","19/Apr/11 20:31;xuefuz;Patch is updated to fix a negative test case for which error message is slightly different.

Unit tests all pass now.",19/Apr/11 20:35;xuefuz;Patch PIG-1921-1.patch is committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
An error in new parser,PIG-1920,12501841,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,rding,rding,18/Mar/11 22:54,04/Aug/11 00:35,14/Mar/19 03:07,13/Apr/11 22:46,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"Run following Pig script on trunk:

{code}
A = load 'input' as (v, u);
B = group A by $0;
C = group B by $0;
describe C;
R = foreach C generate B.A.v; 
describe R;
{code}

One gets the this error:

{code}
C: {group: bytearray,B: {(group: bytearray,A: {(v: bytearray,u: bytearray)})}}
[main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Invalid field reference. Referenced field [v] does not exist in schema: A#19:bag{null#20:tuple(v#17:bytearray,u#18:bytearray)}.
{code}

Change the 5th line to 

{code}
R = foreach C generate B.A.$0; 
{code}

One gets this output:

{code}
C: {group: bytearray,B: {(group: bytearray,A: {(v: bytearray,u: bytearray)})}}
R: {{(A: {(v: bytearray,u: bytearray)})}}
{code}

This is different (and wrong) from the corresponding Pig 0.8 output:

{code}
C: {group: bytearray,B: {group: bytearray,A: {v: bytearray,u: bytearray}}}
R: {{v: bytearray}}
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-21 17:33:11.142,,,no_permission,,,,,,,,,,,,66179,,,,Wed Apr 13 22:46:58 UTC 2011,,,,,,,0|i0gzxj:,97251,,,,,,,,,,21/Mar/11 17:33;xuefuz;It seems related to the 'null#u:int' issue.,"13/Apr/11 22:46;daijy;Actually this is the right behavior. 0.8 doing it wrong.

The schema we get for C is C: {group: bytearray,B: {(group: bytearray,A: {(v: bytearray,u: bytearray)})}}

The schema for B is: B: {(group: bytearray,A: {(v: bytearray,u: bytearray)})}

The schema for B.A is: B:{(A: {(v: bytearray,u: bytearray)})}

Note B.A don't flatten a bag, it results a two level bag. v is in the inner bag and cannot be referenced. 

B.A.$0 is valid, but it equals to B.A.

In 0.8, describe B.A.v is valid, however, dump it we get B.A.$0, which is not the right result.

So actually this bug is in 0.8, and we fix it in 0.9.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Line number should be give for logical plan failures,PIG-1918,12501816,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,olgan,olgan,18/Mar/11 19:08,04/Aug/11 00:34,14/Mar/19 03:07,08/Apr/11 16:54,,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"Currently, the line is only given for the cases where we encounter problem in AST but not on the logical plan. It would be much more meaningful to users if it covered both",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30/Mar/11 19:32;xuefuz;PIG-1918-2.patch;https://issues.apache.org/jira/secure/attachment/12475004/PIG-1918-2.patch,05/Apr/11 16:18;xuefuz;PIG-1918-4.patch;https://issues.apache.org/jira/secure/attachment/12475499/PIG-1918-4.patch,29/Mar/11 23:18;xuefuz;PIG-1918.patch;https://issues.apache.org/jira/secure/attachment/12474939/PIG-1918.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-03-19 14:48:27.007,,,no_permission,,,,,,,,,,,,66214,,,,Fri Apr 08 16:54:07 UTC 2011,,,,,,,0|i0gzxb:,97250,,,,,,,,,,19/Mar/11 14:48;xuefuz;Preliminary fix: add line numbers for failures in ast validation and plan generation.,"20/Mar/11 00:53;xuefuz;Unit test passed. Test-patch run results: (javacc warnings are due to generated code.)

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     -1 javac.  The applied patch generated 888 javac compiler warnings (more than the trunk's current 877 warnings).
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec]
","21/Mar/11 17:17;xuefuz;A new JIRA, PIG-1925, was created for the purpose of previously submitted patch.","30/Mar/11 16:16;xuefuz;For PIG-1918.patch, Unit test passed. Test-patch run result (release warning comes because of newly added public methods in a few classes) :

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     -1 release audit.  The applied patch generated 565 release audit warnings (more than the trunk's current 551 warnings).
",30/Mar/11 19:32;xuefuz;patch is updated to handle source file name.,"01/Apr/11 23:45;daijy;Some comments:
1. When I enter ""a = load '1.txt' (a0, a1, a2);"" in grunt, I get ""ERROR 0: <file null, line 1, column 17> mismatched input '(' expecting SEMI_COLON"". It is better to hide ""file null""

2. We introduce PlanValidationException. It inherits VisitorException, which is fine. But we need to clarify when to use PlanValidationException. I saw some place we use PlanValidationException and FrontendException mixed.

3. May relate to 2, seems not all exceptions will include line number in logical plan. How do we decide which one to include, not one does not? 

4. I guess we will not include line number in physical/MR plan in 0.9, right?","02/Apr/11 00:08;xuefuz;Responses for comments above:

1.The message is probably due to Richard's change. If you try with a semantic error case, you will not see the null file name. This issue can be addressed separately.

2. FrontendException is a more general exception and PlanValidationException is more specific. The mixed use of them isn't introduced by this patch. In principle, a as specific as possible exception should be thrown, even the method signature may throw a more general exception.

3. User facing exceptions should have line numbers. This excludes exceptions due to bug (mostly).

4. We will not have line number in physical/MR plans.
","02/Apr/11 01:45;daijy;There are couple of other user facing places might need line number. Eg. LogicalSchema.merge(). As an incremental process, we can commit this patch first, and solve others later.

We can limit the usage of PlanValidationException to what its name suggests. Actually we intentionally make the exception structure simpler. Since PlanValidationException inherits VisitorException, we can limit the usage to Visitor only. In operator class (eg, ProjectExpression), maybe we can keep FrontendException, which means we need to add line number to FrontendException as well.","04/Apr/11 17:04;xuefuz;Current logical schema doesn't have line number information. Schema can be user created, but more likely it's computed, which has no corresponding source location. 

We only have source information for operators (though not all).

If there is anything that prevents us from committing the patch, let me know. Also let me know if a former discussion is needed. Thanks.",04/Apr/11 17:52;daijy;We can commit the patch and open another Jira to capture the line number info thrown by logical schema.,"04/Apr/11 18:38;xuefuz;Patch PIG-1918-2.patch is committed to the trunk.
Will attach a new patch to address error messages handling schema related errors.","05/Apr/11 16:18;xuefuz;Unit test passed.
Test-patch run:

     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
",05/Apr/11 16:20;xuefuz;Above comments is for patch PIG-1918-4.patch. This patch addresses error messages dealing with schema related validations.,"06/Apr/11 21:12;thejas;PIG-1918-4.patch review-
 The following constructor, is ignoring the Operator argument instead of adding the location to the message.
{code}
+    public FrontendException(Operator op, String message, int errCode, byte errSrc,
+            boolean retry, String detailedMsg, Throwable cause) {
+        super(message, errCode, errSrc, retry, detailedMsg, cause);
+    }
{code}
Everything else looks good.
",08/Apr/11 16:53;xuefuz;Patched was updated according to the above comment and checked in trunk.,08/Apr/11 16:54;xuefuz;JIRA is fixed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NativeMapReduce does not Allow Configuration Parameters containing Spaces ,PIG-1917,12501771,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,miteshsjat,miteshsjat,18/Mar/11 09:47,04/Aug/11 00:34,14/Mar/19 03:07,12/Apr/11 22:31,,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"In Pig, NativeMapReduce does not allow Configuration parameters which contain spaces.

Considering a simple pig script with _wordcount_ as Native M-R job.

{code:title=nativeMR.pig|borderStyle=solid}
A = load 'input/WordCountInput.txt';
B = MAPREDUCE 'hadoop-examples.jar' Store A into 'inputDir' Load 'outputDir' as (word:chararray, count:int) `wordcount -Dmapred.job.map.memory.mb=3072 -Dmapred.child.java.opts=""-Xmx1536m -Xss128k"" inputDir outputDir`;
store B into 'output/WordCount' using PigStorage;
{code}

The above pig script fails while launching native MAPREDUCE job. Because space after _-Dmapred.child.java.opts=""-Xmx1536m_ makes
_-Xss128k""_ as the first argument to _wordcount_
Even with other example M-R program, the parameter _mapred.child.java.opts_ was assigned _""-Xmx1536m_ in Jobconf.

Physical plan, Logical plan, and M-R plan of pig show double quotes. 
For example, here is the corresponding M-R Plan for the native M-R ""MAPREDUCE"" statement.
{code}
MapReduce node scope-13
hadoop jar hadoop-examples.jar wordcount -Dmapred.job.map.memory.mb=3072 -Dmapred.child.java.opts=""-Xmx1536m -Xss128k"" inputDir outputDir
--------
{code}

On executing above M-R job after copying  'input/WordCountInput.txt' into 'inputDir/', as Hadoop Job, this ran successfully.
{code}
$ *hadoop jar hadoop-examples.jar wordcount -Dmapred.job.map.memory.mb=3072 -Dmapred.child.java.opts=""-Xmx1536m -Xss128k"" inputDir outputDir*
11/03/18 14:23:29 INFO input.FileInputFormat: Total input paths to process : 1
11/03/18 14:23:29 INFO mapred.JobClient: Running job: job_201103181353_0007
11/03/18 14:23:30 INFO mapred.JobClient:  map 0% reduce 0%
11/03/18 14:23:37 INFO mapred.JobClient:  map 100% reduce 0%
11/03/18 14:23:49 INFO mapred.JobClient:  map 100% reduce 100%
11/03/18 14:23:51 INFO mapred.JobClient: Job complete: job_201103181353_0007
...
...

{code}

Whereas, when  _-Dmapred.child.java.opts=-Xmx1536m_ was used, the *nativeMR.pig* executed successfully.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11/Apr/11 14:10;thejas;PIG-1917.1.patch;https://issues.apache.org/jira/secure/attachment/12475999/PIG-1917.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-07 23:05:41.546,,,no_permission,,,,,,,,,,,,66329,,,,Tue Apr 12 22:31:18 UTC 2011,,,,,,,0|i0gzx3:,97249,,,,,,,,,,"07/Apr/11 23:05;thejas;The pig parser is parsing the argument ""-Dmapred.child.java.opts='-Xmx1536m -Xss128k'"" to native mapreduce as two arguments ""-Dmapred.child.java.opts='-Xmx1536m"" and ""-Xss128k'"" .

To support this use case, we need to parse it as one argument and remove the single quotes, similar to what bash shell does  - ""-Dmapred.child.java.opts=-Xmx1536m -Xss128k"" (note the missing single quotes). 

In general, quoted arguments should be unquoted before being given as argument to native-mapreduce or streaming command, to simulate what the shell does. This would be a change in behavior that is not backward compatible. But it is a problem only if users rely on the delimiter quotes being included in the argument.
","08/Apr/11 00:33;thejas;To avoid breaking backward compatibility while supporting this use case, one option would be to have a way to switch to the mode that supports this use case.

Maybe by adding a "" using 'unquote-mode' "" to the command ?

eg . 
{code}
B = MAPREDUCE 'hadoop-examples.jar' Store A into 'inputDir' Load 'outputDir' as (word:chararray, count:int)  
`wordcount -Dmapred.job.map.memory.mb=3072 -Dmapred.child.java.opts=""-Xmx1536m -Xss128k"" inputDir outputDir` using 'unquote-mode';
{code}

or maybe by setting a system property on commandline - 
-Dpig.unquote.cmdstring='true'
I think adding a system property is better, as it does not pollute the syntax.


In 0.9, maybe we can make the unquote mode the default mode.

","08/Apr/11 21:52;thejas;The change to parse  ""-Dmapred.child.java.opts='-Xmx1536m -Xss128k'"" as one argument is also going to break backward compatibility, if any existing pig script is (accidentally) relying on that behavior. So making this change only for 0.9. 
Also, instead of unquoting all arguments, only the ""-D"" property arguments will be unquoted. This change will only affect Native map-reduce statements (ie, not streaming command).
",12/Apr/11 21:18;rding;+1,"12/Apr/11 22:31;thejas;Unit test and test-patch passed.
Patch committed to trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
non-deterministic output when a file is loaded multiple times,PIG-1912,12501622,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,field.cady,field.cady,16/Mar/11 21:11,02/May/13 20:33,14/Mar/19 03:07,23/Mar/11 00:50,,,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"I have a small demonstration script (actually, a directory with one main script and several other scripts that it calls) where the output (STOREd to a file) is not consistent between runs.  I will paste the files below this message, and I can also email the tarball to anybody who would like it; I wanted to just upload the tarball but I don't see a way to do that.

The problem appears to be that when a dataset X gets LOADed twice, with things other than LOADs occurring between the loads (like a FOREACH GENERATE), a FOREACH GENERATE that is later performed on X doesn't always choose the correct columns.  The correctness of the output was highly variable on my computer, for one of my co-workers it *almost* always failed, and for two other of my co-workers they didn't see any failures, so it's likely to be a race condition or something like that.


-- FILES FOR REPLICATING THE PROBLEM
-- I will paste the name of the file as a comment, with the content of the file beneath it.
-- I will put the contents of the following files:
-- 1) The Pig scripts (main.pig, calc_x_W.pig, calc_x_Y.pig, and load_raw_data.pig)
-- 2) The input data file (data.csv)
-- 3) The correct output file (correct_output.csv)
-- 4) The shell script that runs the pig files and compares their output to what it should be
-- 5) README


-- main.pig
RUN calc_x_W.pig;
RUN calc_x_Y.pig;
STORE x_W INTO 'output/W' USING PigStorage(',');
STORE x_Y INTO 'output/Y' USING PigStorage(',');  -- this is wrong sometimes

-- calc_x_W.pig
RUN load_raw_data.pig;
x_W = FOREACH raw_data GENERATE x, w;

-- calc_x_Y.pig
RUN load_raw_data.pig;
x_Y = FOREACH raw_data GENERATE x, y;

-- load_raw_data.pig
raw_data = LOAD 'data.csv' USING PigStorage(',')
AS (
  x,
  y,
  w
);

-- data.csv
x1,CORRECT  ANSWER,21148.59
x2,CORRECT  OUTPUT,27219.98
x3,RIGHT    ANSWER,10818.15

-- correct_output.csv
x1,CORRECT  ANSWER
x2,CORRECT  OUTPUT
x3,RIGHT    ANSWER

-- testmany.sh
typeset -a results
i=0
while (( i < 10 )); do
  rm -rf output/*
  pig -x local -d WARN -e ""set debug off;run main.pig"" || break
  diff correct_output.csv output/Y/part-m-00000 && echo good
  results[$i]=$?
  i=$((i+1))
done;
echo ${results[*]}

-- README

This directory is intended to show a non-deterministic bug in pig.
Non-deterministic in the sense that the output of the script is not
the same between different times it is run on the same input; usually
the input is right, but sometimes it's wrong for no apparent reason.

The scripts and dataset included in this directory demonstrate the
issue.  The scripts load the file data.csv and write to the output
directory, but the file output/Y/part-m-00000 is sometimes different
between consecutive runs.  In particular, this file SHOULD just be
the first and third columns of data.csv, but it sometimes uses the
second column in place of the third.

The root of the problem appears to be that there is an intermediate
LOAD of data.csv, after some relations have already been defined.
The following things will make the error stop:
* commenting out ""STORE x_W INTO 'output/W' USING PigStorage(',');"" in main.pig
* making a copy of data.csv called data2.csv, and a file load_daw_data2.pig
  that loads data2.csv and having having calc_x_W.pig use that instead.

It's possible that this isn't a bug and I'm just mis-using Pig;
if that is the case I would greatly appreciate hearing about it.
I believe this issue was also discussed here:
http://mail-archives.apache.org/mod_mbox/pig-user/201102.mbox/%3CAANLkTi=2ZtkVGJevKLYSSzSH--KCcX38+Xaw2d2STNiS@mail.gmail.com%3E

I have a shell script testmany.sh which runs my script multiple times
and reports for which runs the output agrreed with the file correct_output.csv.

IMPORTANT NOTE: We have run this code on 4 different laptops, all running
pig 0.8.0.  On one laptop (the one I'm using) the output of this script
was highly non-deterministic, generally giving both the wrong and the right
output several times each during 10 runs.  Another laptop consistently got
the wrong output up until the 28th run, when it finally gave the right output.
The other two computer never actually observed the wrong output.  We suspect
this is likely a race condition.


Thanks!

USAGE
$ cd pigbug
$ bash testmany.sh
$ # the last line of output will be a sequence of 0s and 1s, with 1
$ # meaning that there was disagreement between the output and
$ # correct_output.csv

Field Cady
field.cady@gmail.com
fcady@operasolutions.com
(360)621-4810
",Ubuntu 10.04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Mar/11 01:18;daijy;PIG-1912-1.patch;https://issues.apache.org/jira/secure/attachment/12474143/PIG-1912-1.patch,21/Mar/11 23:11;daijy;PIG-1912-2.patch;https://issues.apache.org/jira/secure/attachment/12474248/PIG-1912-2.patch,22/Mar/11 22:41;daijy;PIG-1912-3.patch;https://issues.apache.org/jira/secure/attachment/12474352/PIG-1912-3.patch,22/Mar/11 22:49;daijy;PIG-1912-3_0.8.patch;https://issues.apache.org/jira/secure/attachment/12474353/PIG-1912-3_0.8.patch,29/Apr/13 19:32;field.cady;pigbug.zip;https://issues.apache.org/jira/secure/attachment/12581020/pigbug.zip,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-03-18 00:01:30.78,,,no_permission,,,,,,,,,,,,165241,Reviewed,,,Thu May 02 20:33:16 UTC 2013,,,,,,,0|i0gzwf:,97246,,,,,,,"non-deterministic, load, store",,,"16/Mar/11 21:12;field.cady;Again, I have a tarball with all the files.  Just email me if you want it.  Thanks!","18/Mar/11 00:01;daijy;I can reproduce. If we collapse all 4 scripts into one, it runs fine. This issue only occurs when we invoking scripts inside script. I will take a look.","19/Mar/11 01:52;daijy;I find the problem. Flatten the script:
{code}
raw_data = LOAD 'data.csv' USING PigStorage(',') AS (x, y, w);
x_W = FOREACH raw_data GENERATE x, w;
raw_data = LOAD 'data.csv' USING PigStorage(',') AS (x, y, w);
x_Y = FOREACH raw_data GENERATE x, y;
STORE x_W INTO 'output/W' USING PigStorage(',');
STORE x_Y INTO 'output/Y' USING PigStorage(',');
{code}

We have two loaders in the script. Internally, every loader has a signature, which consists of alias, input file, parameters. Pig keep track of status of each loader using signature. In this script, all three components are the same. Pig get confused which status belonging to which loader.

There are two ways to get around, either change one load statement slightly, or use only one load statement, feeding it to multiple subsequent statements.

I cannot find a quick way to make a more specific signature. In short turn, we can check conflicts of signature and fail out if it happens.","19/Mar/11 03:16;field.cady;Thank you for the help!  I think my company has figured out some good ways
to work around it - I mostly just wanted to make sure that the Pig
development community was aware of the issue.

Cheers,
Field






-- 
Field Cady
Department of Computer Science
Carnegie Mellon University
(360)621-4810
field.cady@gmail.com
","19/Mar/11 23:24;thejas;bq. We have two loaders in the script. Internally, every loader has a signature, which consists of alias, input file, parameters. Pig keep track of status of each loader using signature. In this script, all three components are the same. Pig get confused which status belonging to which loader.

Does pig need to reconstruct the signature from the parts used to create the signature ? If not, I think pig can just generate a distinct signature each time. Maybe add an signature index to alias + file + params, so that the signature can still be easily to understand for debugging.",21/Mar/11 01:12;daijy;The best way is to add an index to the signature instead of fail out. I will post a patch.,21/Mar/11 23:11;daijy;PIG-1912-2.patch fix findbug warnings.,21/Mar/11 23:16;daijy;Review notes: https://reviews.apache.org/r/519/,22/Mar/11 22:41;daijy;PIG-1912-3.patch address Santhosh's review comments,22/Mar/11 22:49;daijy;PIG-1912-3_0.8.patch is for 0.8 branch. There is slight difference between PIG-1912-3.patch.,"23/Mar/11 00:46;daijy;test-patch result:

     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
",23/Mar/11 00:50;daijy;Patch committed to both trunk and 0.8 branch.,"29/Apr/13 19:32;field.cady;This completely shows the issue, including sample data and a shell script that runs pig in local mode to show the bug.",02/May/13 20:33;daijy;[~field.cady] Did you attach to the wrong Jira?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Infinite loop with accumulator function in nested foreach,PIG-1911,12501621,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,olgan,olgan,16/Mar/11 21:06,25/Apr/11 21:27,14/Mar/19 03:07,08/Apr/11 19:13,0.8.0,,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"Sample script:

register v_udf.jar;
a = load '2records' as (f1:chararray,f2:chararray);
b = group a by f1;
d = foreach b { sort = order a by f1; 
  generate org.udfs.MyCOUNT(sort) as something ; }
dump d;

This causes infinite loop if MyCOUNT implements Accumulator interface.

The workaround is to take the function out of nested foreach into a separate foreach statement.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Apr/11 22:16;thejas;PIG-1911.08.1.patch;https://issues.apache.org/jira/secure/attachment/12475536/PIG-1911.08.1.patch,06/Apr/11 16:07;thejas;PIG-1911.trunk.1.patch;https://issues.apache.org/jira/secure/attachment/12475601/PIG-1911.trunk.1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-03-17 05:09:37.313,,,no_permission,,,,,,,,,,,,165240,,,,Fri Apr 08 19:13:35 UTC 2011,,,,,,,0|i0gzw7:,97245,,,,,,,,,,"17/Mar/11 05:09;vivekp;In this case pig is calling getValue() and cleanup() methods infinitely. The below is the udf source just in case;
{code}
public class MyCOUNT extends EvalFunc<Long> implements  Accumulator<Long>{
    @Override
    public Long exec(Tuple input) throws IOException {
            DataBag bag = (DataBag)input.get(0);
            Iterator it = bag.iterator();
            long cnt = 0;
            while (it.hasNext()){
                    Tuple t = (Tuple)it.next();
                    if (t != null && t.size() > 0 && t.get(0) != null )
                            cnt++;
            }
            return cnt;
    }

    @Override
    public Schema outputSchema(Schema input) {
        return new Schema(new Schema.FieldSchema(null, DataType.LONG)); 
    }
    private long intermediateCount = 0L;
    @Override
    public void accumulate(Tuple b) throws IOException {
            DataBag bag = (DataBag)b.get(0);
            Iterator it = bag.iterator();
            while (it.hasNext()){
                Tuple t = (Tuple)it.next();
                if (t != null && t.size() > 0 && t.get(0) != null) {
                    intermediateCount += 1;
                }
            }
    }
    @Override
    public void cleanup() {
        intermediateCount = 0L;
    }
    @Override
    public Long getValue() {
        return intermediateCount;
    }
}
{code}","05/Apr/11 22:16;thejas;PIG-1911.08.1.patch  - patch for 0.8 branch.
Unit tests and test-patch passed.
","08/Apr/11 07:56;daijy;+1, this is definitely a fix. Accumulator will only be used if there is an accumulator UDF in nested plan. So fix inside UDF should be fine.

Just help me to understand better, I think fix PORelationToExprProject is also possible. Since accumulator only need one extra bag to in order for UDF to invoke getValue(). So after exhaust all batch, send one extra bag, then send EOP, will solve the problem as well. Is that right?","08/Apr/11 15:20;thejas;bq. Just help me to understand better, I think fix PORelationToExprProject is also possible. Since accumulator only need one extra bag to in order for UDF to invoke getValue(). So after exhaust all batch, send one extra bag, then send EOP, will solve the problem as well. Is that right?

I looked at that option first, but the problem is that POUserFunc is expected to be called with isAccumStarted() == false and result.returnStatus == STATUS_OK. In case of a relation like -
F = foreach IN { SBCOL = order BCOL by $1; FBCOL = filter SBCOL by 1 == 2; generate COUNT(FBCOL.$0);}
 FBCOL will have nothing to return.With the approach you mention here - The first call to the plan will be made with isAccumStarted() == true, and PORelationToExprProject will return an empty bag. Another call will be made with isAccumStarted() == false, and this time it will return STATUS_EOP. THis would mean that the udf.cleanup() will not get called. To avoid this, we would need to handle STATUS_EOP differently in POUserFunc.processInput() in accumulative mode. That seemed a little less clean than the approach I finally took.
",08/Apr/11 19:13;thejas;Patch committed to trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incorrect schema shown when project-star is used with other projections,PIG-1910,12501606,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,thejas,thejas,16/Mar/11 17:50,04/Aug/11 00:35,14/Mar/19 03:07,22/Apr/11 17:32,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"{code}
grunt> l = load 'x' ;                                       
grunt> f = foreach l generate $1 as a, *, $2 as b;          
grunt> describe f;
f: {a: bytearray,(null),b: bytearray}  -- The tuple returned by * is automatically flattened, so this schema is not correct. It is more accurate to return a null schema.

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Apr/11 02:14;daijy;PIG-1910-1.patch;https://issues.apache.org/jira/secure/attachment/12476406/PIG-1910-1.patch,19/Apr/11 00:31;daijy;PIG-1910-2.patch;https://issues.apache.org/jira/secure/attachment/12476674/PIG-1910-2.patch,19/Apr/11 21:17;daijy;PIG-1910-3.patch;https://issues.apache.org/jira/secure/attachment/12476782/PIG-1910-3.patch,20/Apr/11 17:27;daijy;PIG-1910-4.patch;https://issues.apache.org/jira/secure/attachment/12476920/PIG-1910-4.patch,20/Apr/11 18:16;daijy;PIG-1910-5.patch;https://issues.apache.org/jira/secure/attachment/12476924/PIG-1910-5.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-04-15 02:17:38.962,,,no_permission,,,,,,,,,,,,66129,Reviewed,,,Fri Apr 22 17:32:07 UTC 2011,,,,,,,0|i0gzvz:,97244,,,,,,,,,,"16/Mar/11 17:53;thejas;This can also lead to incorrect query plan, if an alias that occurs after the position corresponding to * projection in schema is used. -


{code}
grunt> l = load 'x' ;                                       
grunt> f = foreach l generate $1 as a, *, $2 as b;          
grunt> describe f;
f: {a: bytearray,(null),b: bytearray}
grunt> g  = foreach f generate b;
grunt> explain g;
-- NOTE that alias b is getting translated to column index 2, which is incorrect.
...
...
#--------------------------------------------------
# Map Reduce Plan
#--------------------------------------------------
MapReduce node scope-20
Map Plan
g: Store(fakefile:org.apache.pig.builtin.PigStorage) - scope-19
|
|---g: New For Each(false)[bag] - scope-18
    |   |
    |   Project[bytearray][2] - scope-16
    |
    |---f: New For Each(false,true,false)[bag] - scope-15
        |   |
        |   Project[bytearray][1] - scope-9
        |   |
        |   Project[tuple][*] - scope-11
        |   |
        |   Project[bytearray][2] - scope-13
        |
        |---l: Load(file:///Users/tejas/pig_type/trunk/x:org.apache.pig.builtin.PigStorage) - scope-8--------
Global sort: false
----------------

{code}

This problem will also need to be addressed for the range-projection feature in PIG-1693 .","15/Apr/11 02:17;daijy;PIG-1910-1.patch fix the test case in description.

For the other test case in comments, I get:
Invalid field projection. Projected field [b] does not exist in schema:.

Which seems fine. We have no way to translate g into logical plan since we don't know how to translate b into right position.",19/Apr/11 00:31;daijy;PIG-1910-2.patch try to fix unit test failures.,19/Apr/11 21:17;daijy;PIG-1910-3.patch pass unit tests and test-patch,"19/Apr/11 21:21;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/624/
-----------------------------------------------------------

Review request for pig and thejas.


Summary
-------

See PIG-1910


This addresses bug PIG-1910.
    https://issues.apache.org/jira/browse/PIG-1910


Diffs
-----

  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/expression/DereferenceExpression.java 1095145 
  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/expression/ExpToPhyTranslationVisitor.java 1095145 
  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/expression/ProjectExpression.java 1095145 
  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/relational/LOCogroup.java 1095145 
  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/visitor/ColumnAliasConversionVisitor.java 1095145 
  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/visitor/LineageFindRelVisitor.java 1095145 
  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/visitor/UDFFinder.java PRE-CREATION 
  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/parser/QueryParserDriver.java 1095145 
  http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/TestPigServer.java 1095145 
  http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/TestPlanGeneration.java PRE-CREATION 
  http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/TestTypeCheckingValidatorNewLP.java 1095145 
  http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/Util.java 1095145 

Diff: https://reviews.apache.org/r/624/diff


Testing
-------

Test-patch:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 12 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Unit test:
    all pass


Thanks,

Daniel

","20/Apr/11 01:26;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/624/#review499
-----------------------------------------------------------



http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/visitor/LineageFindRelVisitor.java
<https://reviews.apache.org/r/624/#comment1029>

    If there are multiple group-by columns, the group column will be a tuple. This will associate the load function only to the tuple and not the uids of the columns within the tuple.
    Need to associated load function to inner-uids as well like its done in mapMatchLoadFuncToUid


- thejas


On 2011-04-19 21:20:10, Daniel Dai wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/624/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-04-19 21:20:10)
bq.  
bq.  
bq.  Review request for pig and thejas.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  See PIG-1910
bq.  
bq.  
bq.  This addresses bug PIG-1910.
bq.      https://issues.apache.org/jira/browse/PIG-1910
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/expression/DereferenceExpression.java 1095145 
bq.    http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/expression/ExpToPhyTranslationVisitor.java 1095145 
bq.    http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/expression/ProjectExpression.java 1095145 
bq.    http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/relational/LOCogroup.java 1095145 
bq.    http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/visitor/ColumnAliasConversionVisitor.java 1095145 
bq.    http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/visitor/LineageFindRelVisitor.java 1095145 
bq.    http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/visitor/UDFFinder.java PRE-CREATION 
bq.    http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/parser/QueryParserDriver.java 1095145 
bq.    http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/TestPigServer.java 1095145 
bq.    http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/TestPlanGeneration.java PRE-CREATION 
bq.    http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/TestTypeCheckingValidatorNewLP.java 1095145 
bq.    http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/Util.java 1095145 
bq.  
bq.  Diff: https://reviews.apache.org/r/624/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  Test-patch:
bq.       [exec] +1 overall.  
bq.       [exec] 
bq.       [exec]     +1 @author.  The patch does not contain any @author tags.
bq.       [exec] 
bq.       [exec]     +1 tests included.  The patch appears to include 12 new or modified tests.
bq.       [exec] 
bq.       [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
bq.       [exec] 
bq.       [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
bq.       [exec] 
bq.       [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
bq.       [exec] 
bq.       [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
bq.  
bq.  Unit test:
bq.      all pass
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Daniel
bq.  
bq.

",20/Apr/11 17:27;daijy;PIG-1910-4.patch addresses Thejas's review comment.,20/Apr/11 18:29;thejas;PIG-1910-5.patch looks good. +1,22/Apr/11 17:32;daijy;Patch committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
multiple star projection in a statement does not produce the right plan,PIG-1897,12501160,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,11/Mar/11 16:26,04/Aug/11 00:34,14/Mar/19 03:07,14/Apr/11 00:27,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"{code}
grunt> l = load 'x' as (a,b,c);
grunt> f = foreach l generate * ,*;
grunt> describe f;
f: {a: bytearray,b: bytearray,c: bytearray,(a: bytearray,b: bytearray,c: bytearray)}
-- Note that the 2nd project-star contents are going into a tuple
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Apr/11 16:25;thejas;PIG-1897.1.patch;https://issues.apache.org/jira/secure/attachment/12476257/PIG-1897.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-13 17:06:00.223,,,no_permission,,,,,,,,,,,,66256,,,,Thu Apr 14 00:27:17 UTC 2011,,,,,,,0|i0gzt3:,97231,,,,,,,,,,"13/Apr/11 16:24;thejas;Projecting multiple star already works in trunk.
PIG-1277.1.patch has a test case.
",13/Apr/11 16:25;thejas;Changing patch file name from PIG-1277.1.patch to PIG-1897.1.patch .,13/Apr/11 17:06;xuefuz;+1,"14/Apr/11 00:27;thejas;Unit test and test-patch passed. Patch committed to trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Worng stats shown when there are multiple stores but same file names,PIG-1894,12501107,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,vivekp,vivekp,11/Mar/11 04:24,04/Aug/11 00:34,14/Mar/19 03:07,01/Apr/11 23:45,0.8.0,0.9.0,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"Pig 0.8/0.9 shows wrong stats for store counters when I have multiple store but of the same name.

To reproduce the issue please use the below script :
{code}
A = load 'sampledata1' as (f1:chararray,f2:chararray,f3:int);
B = filter A by f3==1;
C = filter A by f3==2;
D = filter A by f3==3;
store B into '/folder/B/out.gz';
store C into '/folder/C/out.gz';
store D into '/folder/D/out.gz';
{code}

Input 
{code}
aaa     a       1
aaa     b       1
bbb     a       2
bbb     b       2
ccc     a       3
ccc     b       3
{code}


For this script Pig shows 
Output(s):
Successfully stored 6 records (32 bytes) in: ""/folder/B/out.gz""
Successfully stored 6 records (32 bytes) in: ""/folder/C/out.gz""
Successfully stored 6 records (32 bytes) in: ""/folder/D/out.gz""

Counters:
Total records written : 18
Total bytes written : 96",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Mar/11 19:23;rding;PIG-1894.patch;https://issues.apache.org/jira/secure/attachment/12474914/PIG-1894.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-13 19:29:42.395,,,no_permission,,,,,,,,,,,,66242,Reviewed,,,Fri Apr 01 23:45:57 UTC 2011,,,,,,,0|i0gzsf:,97228,,,,,,,,,,"13/Mar/11 19:29;olgan;Richard, I remember you already worked on an issue caused by the same name for output files. So this might already be solved but needs to be verified.",14/Mar/11 04:19;vivekp;Just for reference; the issue is PIG-1779 (Worng stats shown when there are multiple loads but same file names),"29/Mar/11 23:13;rding;Test-patch result:

{code}
      [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     -1 release audit.  The applied patch generated 553 release audit warnings (more than the trunk's current 552 warnings).
{code}

The release audit warning is html related.",01/Apr/11 23:23;thejas;Looks good. +1,01/Apr/11 23:45;rding;unit tests pass. patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig report input size -1 for empty input file,PIG-1893,12501100,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,daijy,daijy,11/Mar/11 02:43,04/Aug/11 00:35,14/Mar/19 03:07,29/Mar/11 19:07,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"In the following script:
{code}
a = load '1.txt' as (a0, a1);
b = load '2.txt' as (b0, b1);
c = join a by a0, b by b0;
dump c;
{code}
If 1.txt is empty, Pig will report
Successfully read -1 records from: ""1.txt""

In WebUI, we can see we only have one MultiInputCounters: ""Input records from _0_2.txt"". In this case, we should count inputs ""1.txt"" 0 instead -1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Mar/11 18:28;rding;PIG-1893.patch;https://issues.apache.org/jira/secure/attachment/12474908/PIG-1893.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-29 19:07:35.39,,,no_permission,,,,,,,,,,,,66134,Reviewed,,,Tue Mar 29 19:07:35 UTC 2011,,,,,,,0|i0gzs7:,97227,,,,,,,,,,29/Mar/11 18:57;daijy;+1,29/Mar/11 19:07;rding;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in new logical plan : No output generated even though there are valid records,PIG-1892,12500992,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,vivekp,vivekp,10/Mar/11 08:44,25/Apr/11 21:27,14/Mar/19 03:07,20/Mar/11 23:30,0.8.0,0.9.0,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"I have the below script which provides me no output even though there are valid records in relation B which is used for the left out join.

A0 = load 'input' using Maploader()  as ( map1, map2, map3 );
A = filter A0 by ( (map2#'params'#'prop' == 464)   and (map2#'params'#'query' is not null) );
B0 = filter A by (map1#'type' == 'c');
B = filter B0 by ( map2#'info'#'s' matches 'aaaa|bbb|cccc');
C =  filter A by (map1#'type' == 'p');
D = join B by map2#'params'#'query' LEFT OUTER , C by map2#'params'#'query';
store D into 'output';

This is a bug with the newlogical plan.  From the plan i can see that  map1#'type'  and map2#'info'#'s' is not marked as RequiredKeys ,
but where as all the fields reffered in the firts filter statement is marked as required.


For the script to work I have to turn off the coloumn prune optimizer by -t ColumnMapKeyPrune or rearrange the script such that;
B0 = filter A0 by ( (map2#'params'#'prop' == 464)   and (map2#'params'#'query' is not null) and (map1#'type' == 'c') );
C =  filter A0 by ( (map2#'params'#'prop' == 464)   and (map2#'params'#'query' is not null) and (map1#'type' == 'p') );

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Mar/11 21:35;daijy;PIG-1892-1.patch;https://issues.apache.org/jira/secure/attachment/12473948/PIG-1892-1.patch,19/Mar/11 01:16;daijy;PIG-1892-2.patch;https://issues.apache.org/jira/secure/attachment/12474052/PIG-1892-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-03-18 22:29:00.115,,,no_permission,,,,,,,,,,,,165231,Reviewed,,,Sun Mar 20 23:30:06 UTC 2011,,,,,,,0|i0gzrz:,97226,,,,,,,,,,18/Mar/11 22:29;thejas;+1,"19/Mar/11 01:16;daijy;Discuss with Thejas, and we find another problem. Attach PIG-1892-2.patch.",19/Mar/11 23:49;thejas;Looks good. +1,"20/Mar/11 23:30;daijy;Test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 543 release audit warnings (more than the trunk's current 540 warnings).

All tests pass.

Patch committed to both trunk and 0.8 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix piggybank unit test TestAvroStorage,PIG-1890,12500949,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kengoodhope,daijy,daijy,09/Mar/11 21:57,04/Aug/11 00:35,14/Mar/19 03:07,12/Jul/11 01:15,0.9.0,,,,,,,0.10.0,0.9.0,,,impl,,,1,patch,,,,,,,,,,,,"TestAvroStorage fail on trunk. There are two reasons:
1. After PIG-1680, we call LoadFunc.setLocation one more time.
2. The schema for AvroStorage seems to be wrong. For example, in first test case testArrayDefault, the schema for ""in"" is set to ""PIG_WRAPPER: (FIELD: {PIG_WRAPPER: (ARRAY_ELEM: float)})"". It seems PIG_WRAPPER is redundant. This issue is hidden until PIG-1188 checked in.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Mar/11 00:53;daijy;PIG-1890-1.patch;https://issues.apache.org/jira/secure/attachment/12473226/PIG-1890-1.patch,04/Jul/11 22:01;kengoodhope;PIG-1890-2.patch;https://issues.apache.org/jira/secure/attachment/12485185/PIG-1890-2.patch,06/Jul/11 02:28;kengoodhope;PIG-1890-3.patch;https://issues.apache.org/jira/secure/attachment/12485361/PIG-1890-3.patch,10/Jul/11 02:20;kengoodhope;PIG-1890-4.patch;https://issues.apache.org/jira/secure/attachment/12485978/PIG-1890-4.patch,06/Jul/11 17:22;phunt;pig_setloc_avro.txt;https://issues.apache.org/jira/secure/attachment/12485454/pig_setloc_avro.txt,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-05-02 21:21:41.931,,,no_permission,,,,,,,,,,,,37462,Reviewed,,,Tue Jul 12 17:12:34 UTC 2011,,,,,,,0|i05ign:,30088,Fixed AvroStorage unit tests.,,,,,,,,,10/Mar/11 00:54;daijy;PIG-1890-1.patch fix the first issue. I temporary comment out all test cases in TestAvroStorage.,"02/May/11 21:21;olgan;Hi Jacob, 

Are you planning to address the additional issue for 0.9 or should we delay this?",02/May/11 21:40;kengoodhope;I have been working on some fixes to AvroStorage already.  I should be able to make sure this issue gets addressed in those fixes as will.  Will have it done sometime this week.,09/May/11 18:29;jghoman;@Ken - any update now that we're in a new week?,"15/May/11 21:35;kengoodhope;For testArrayDefault, we are attempting to return an entire avro array, which is consistent with the schema.  The result is tuple with one column, a bag of floats"".  In POProject.getNext(Tuple), tuples with one column have their single column extracted, cast to a tuple, and then returned.  Obviously in this case, this results in trying to cast the bag of floats into a tuple and an exception being thrown.

Does anyone know why this is being done in POProject?","17/May/11 18:04;daijy;Seems it should call POProject.getNext(DataBag) instead. Project one item assumes this item already has the correct type and need not convert. The issue should be caused by plan generation, which results a wrong result type for POProject.","23/May/11 18:09;kengoodhope;Right now, in this test, AvroStorage is attempting to pass back a single array of floats with one call to next. To be consistent with intent of how the data is stored we want this array returned as a single unit(databag) with each foreach call. In other words we don't want foreach to return each element of that array one at a time. If I am understanding the code right, it appears that is what it is trying to do. Am I missing something? Is there a way to control this behavior?

","01/Jun/11 00:37;kengoodhope;I need some clarification on the contract for POProject.getNext(Tuple).  Right now, if it receives a tuple with a single element, it extracts that element and attempts to cast it as a tuple and return it.  This breaks with any single element tuple that where the single element is not a tuple.  The code could be modified to not extract non-tuple elements.",15/Jun/11 21:12;olgan;About to cut the release,"01/Jul/11 21:24;kengoodhope;The fix for this jira involves two parts, making setLocation idempotent, and a fix in POProject.  I have added a jira for POProject issue PIG-2153.  I will try and get a patch for the setLocation issue added this weekend.  I have made some other changes to the version of AvroStorage we are using at LinkedIn and want to seperate those changes from any patch I submit for this.",04/Jul/11 22:01;kengoodhope;Attached patch.  Only works if PIG-2153 is fixed.  Until then the unit tests still break.  This patch fixes setLocation.,"05/Jul/11 15:37;dvryaboy;Marked PIG-2153 as a blocker to this.

I have a feeling that ticket is also blocking EB issue 60 https://github.com/kevinweil/elephant-bird/issues/60 ","05/Jul/11 20:26;phunt;Hi, I'm seeing an issue with both versions of the attached patches when I run the following:

{noformat}
REGISTER avro-1.4.1.jar; 
REGISTER json-simple-1.1.jar; 
REGISTER piggybank.jar;

A = LOAD 'input_123.avro' USING 
org.apache.pig.piggybank.storage.avro.AvroStorage();

B = LOAD 'input_789.avro' USING 
org.apache.pig.piggybank.storage.avro.AvroStorage();

C = UNION A, B; 
DUMP C;
{noformat}

where each file contains a single tuple; input_123.avro contains ""1,2,3"" (ints) and input_789.avro contains ""7,8,9""
Dump C should be returning 2 tuples; 1 tuple 1,2,3 and 1 tuple 7,8,9.

Without the patch I see 6 tuples output (3 1,2,3 and 3 7,8,9)
With either of the proposed patches applied I see 4 tuples output (2 1,2,3 and 2 7,8,9)

From looking at other pig loader functions it seems like the following would address the setLocation issue:

{noformat}
     public void setLocation(String location, Job job) throws IOException {
-        if(AvroStorageUtils.addInputPaths(location, job) && inputAvroSchema == null) {
-            inputAvroSchema = getAvroSchema(location, job);
-        }
+        FileInputFormat.setInputPaths(job, location);
+        inputAvroSchema = getAvroSchema(location, job);
     }
{noformat}

This does resolve the issue for the script I described. However the ""addInputPaths"" functionality of AvroStorageUtils is lost - but I'm wondering why this was added rather than just rely on the std capabilities of LOAD? (such as globbing).


I'd be happy to package up my suggestion as a patch if there's interest.
","05/Jul/11 22:11;kengoodhope;Hi Patrick, for our purposes we need setLocation to add all sub-directories, including directories more than 2 levels deep.  A common use case for us to to have directories organized by time, yyyy/MM/dd/hh/mm.  In that case if you want to load all the data from a particular month, then you need to add all the subdirs.  Your right that a UNION can accomplish this, but it can be painful to add the directories that way.  I will take a look at why this is still breaking in your case.

","05/Jul/11 22:43;mmoeller;Hi Ken,

I am have the same use case as you and encountering the same behavior as Patrick. I made a few modifications to the methods ""addInputPaths"" and ""addAllSubDirs"" from your patch, which seems to solve the UNION issue. 

{code}
    public static boolean addInputPaths(String pathString, Job job)
        throws IOException {

        Set<Path> pathSet = new HashSet<Path>();
        
        if (addAllSubDirs(new Path(pathString), job, pathSet)) {
            Path[] paths = pathSet.toArray(new Path[pathSet.size()]);
 
            return true;
        }
        return false;
    }

    /**
     * Adds all non-hidden directories and subdirectories to the paths set
     * 
     * @throws IOException
     */
  	private static boolean addAllSubDirs(Path path, Job job, Set<Path> paths) throws IOException {
  		FileSystem fs = FileSystem.get(job.getConfiguration());

  		if (PATH_FILTER.accept(path)) {
  			try {
  				FileStatus file = fs.getFileStatus(path);
  				if (file.isDir()) {
  					for (FileStatus sub : fs.listStatus(path)) {
  						addAllSubDirs(sub.getPath(), job, paths);
  					}
  				} else {
  					AvroStorageLog.details(""Add input file:"" + file);
  					paths.add(file.getPath());
  				}
  			} catch (FileNotFoundException e) {
  				AvroStorageLog.details(""Input path does not exist: "" + path);
  				return false;
  			}
  			return true;
  		}
  		return false;
  	}
{code}","05/Jul/11 23:12;mmoeller;Re-pasting addInputPaths. 

{code}
    /**
     * get input paths to job config
     */
    public static boolean addInputPaths(String pathString, Job job)
        throws IOException {

        Set<Path> pathSet = new HashSet<Path>();
        
        if (addAllSubDirs(new Path(pathString), job, pathSet)) {
            Path[] paths = pathSet.toArray(new Path[pathSet.size()]);

            FileInputFormat.setInputPaths(job, paths);  
            return true;
        }
        return false;
    }
{code}","06/Jul/11 00:21;phunt;@ken (and @mads) thanks, I figured something like that. Could this possibly be an issue in pig itself? I do see this

{noformat}
LoadFunc.setLocation:
     * This method will be called in the backend multiple times. Implementations
     * should bear in mind that this method is called multiple times and should
     * ensure there are no inconsistent side effects due to the multiple calls.
{noformat}

But what I'm seeing in this UNION case is that setLocation is being called multiple times on the same AvroStorage instance, for the same job, with different files. This results (current avrostorage code with pig-1890-2.patch applied) in the duplication - 2 files are added rather than one (my patch fixes this by only taking the most recent argument to setLocation, which is consistent with existing loader funcs, whereas avrostorage keeps adding). If you check the debugging output you'll see this (I might have added a bit more debugging to setLocation to capture this event...)

Regards.","06/Jul/11 00:38;dvryaboy;I've been a bit out of the loop on this -- you are doing your own directory traversal? You shouldn't need to do that in the Pig layer, this should be done in your InputFormat. I had to write a wrapper to emulate what MAPREDUCE-1501 does in Elephant-Bird, and I believe Pig does the same thing (but without caring about the mapred.input.dir.recursive config).

As for setLocation, yes. Making it idempotent is ""fun"".  

I am curious about this business with calling it with different files for the same instance for the same job. Patrick, can you show some debug output that has the sequence of calls? ","06/Jul/11 02:28;kengoodhope;There are places where we use addInputDir as a true add, not set.  Otherwise your solution would work.  I did incorporate the use in a set for addAllSubDirs.  Since the method name was no longer descriptive, I changed it to getAllSubDirs.  This new patch passed unit tests, but currently there isn't a test for UNION.  Let me know if this works.","06/Jul/11 02:40;kengoodhope;Dmitry, when I inherited the code it was already doing the traversal in setLocation, and I didn't consider doing in the InputFormat.  To be honest, I am not crazy about adding all the subdirs by default, since this is inconsistent with the way a standard map-reduce job works.  But, our users expect this behavior, and have pig jobs that depend on it.

If the current patch works, I am inclined to leave it, until I get time to do a better re-factoring.","06/Jul/11 02:56;dvryaboy;Ken, adding all subdirs is how Hadoop + whatever patchset works, given the right value for mapred.input.dir.recursive

Now, what version of Hadoop, I have no idea, but it's in there somewhere :). And since that's what people decided on it probably behooves us to respect it. But fixing that issue is a separate concern from what this ticket tries to address. We should open a ticket, though.","06/Jul/11 17:21;phunt;@Dmitriy thanks.

bq. Patrick, can you show some debug output that has the sequence of calls?

Sure, I didn't save the original so I re-ran it, see attached (pig_setloc_avro.txt) for full details using the UNION example (this is with current trunk - notice that there are 6 tuples output rather than 2). I mis-remembered one detail - it's calling setLoc for the same job, with different files, but _different_ AvroStorage objects. (see first two lines of setLocation debug message). 

Why are there 8 AvroStorage objects being created, shouldn't there just be 2, one for loading each of the two input files?",06/Jul/11 17:22;phunt;demonstrate setLocation calls on AvroStorage.,"06/Jul/11 18:55;kengoodhope;A recent change in Pig causes setLocation to be called twice, and if setLocation isn't idempotent, then you get twice the output.  My suspicion is UNION is further exasperating the problem leading to the input being added 4X.  Did you still see the problem with the last patch I added?","06/Jul/11 19:01;mmoeller;Hi Ken,

With the latest patch the UNION behaves as expected for me.


Thanks,
Mads",10/Jul/11 02:20;kengoodhope;Uploading new patch that contains the same fixes to setLocation contained in the previous patch.  New patch adds fixes to the schema that resolve the issues around the unit tests.,"10/Jul/11 02:26;kengoodhope;Removing the blocker for PIG-2153.  Turns out the problem, as first asserted, was in AvroStorage.  The new logical plan must handle implicit wrapping tuples differently than used to be the case.  In order to make this work, I removed the wrapping tuple from the schema produced by getSchema.  getNext still returns its result in the wrapping tuple.  I also had to modify putNext, to expect a piq schema without the implicit wrapping tuple.",10/Jul/11 02:29;kengoodhope;PIG-1890-4.patch ready for review.  All unit test now working against trunk.,12/Jul/11 01:00;phunt;I tested PIG-1890-4.patch against trunk using the UNION example and it generated expected (i.e. correct) results.,"12/Jul/11 01:10;daijy;All Avro unit tests pass now, and test-patch returns all +1. Now we don't get a double PIG_WRAPPER, the schema generated by ArvoStorage looks good to me. Thanks guys for your hard working!",12/Jul/11 01:15;daijy;Patch committed to trunk.,12/Jul/11 17:12;daijy;Also committed to 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix TestLogicalPlanGenerator not use hardcoded path,PIG-1888,12500669,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,daijy,daijy,daijy,07/Mar/11 19:56,04/Aug/11 00:34,14/Mar/19 03:07,08/Mar/11 01:37,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,We have hardcoded path in TestLogicalPlanGenerator. It fails when it runs on a machine without these files. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Mar/11 19:57;daijy;PIG-1888-1.patch;https://issues.apache.org/jira/secure/attachment/12472853/PIG-1888-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-08 00:54:49.759,,,no_permission,,,,,,,,,,,,66263,Reviewed,,,Tue Mar 08 01:37:55 UTC 2011,,,,,,,0|i0gzrr:,97225,,,,,,,,,,08/Mar/11 00:54;xuefuz;+1,08/Mar/11 01:37;daijy;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix pig-withouthadoop.jar to contains proper jars,PIG-1887,12500664,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,07/Mar/11 19:23,04/Aug/11 00:34,14/Mar/19 03:07,07/Mar/11 19:35,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,pig-withouthadoop.jar miss some libraries after new parser check in. It is not runnable now. We should fix it with reference to how we build pig.jar.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Mar/11 19:24;daijy;PIG-1887-1.patch;https://issues.apache.org/jira/secure/attachment/12472849/PIG-1887-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-07 19:27:52.969,,,no_permission,,,,,,,,,,,,66356,Reviewed,,,Mon Mar 07 23:15:06 UTC 2011,,,,,,,0|i0gzrj:,97224,,,,,,,,,,07/Mar/11 19:27;thejas;+1,07/Mar/11 19:35;daijy;Patch committed to trunk.,07/Mar/11 20:56;eyang;-1 Bundling HBase in withouthadoop.jar is not right.  HBase should be treated the same as Hadoop to avoid problem where bundled HBase is incompatible with system provided hadoop.,"07/Mar/11 21:06;dvryaboy;Agreed with Eric. 
Not only that, but users may want to use other hbase jars that are api-compatible (like 0.90.1 and 0.90.2..)
Plus this won't make hbase runnable anyway -- it needs zookeeper and guava, too.
","07/Mar/11 21:30;daijy;Yes, actually I take out the hbase part when I commit.","07/Mar/11 23:15;eyang;Great, thanks Daniel.  :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add zookeeper jar to list of jars shipped when HBaseStorage used,PIG-1886,12500530,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,dvryaboy,dvryaboy,dvryaboy,05/Mar/11 23:49,24/Aug/11 19:59,14/Mar/19 03:07,13/Mar/11 03:42,,,,,,,,0.9.0,,,,,,,0,hbase,,,,,,,,,,,,HBaseStorage currently asks Hadoop to include hbase and guava among the shipped jars. It should do the same for ZooKeeper.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Mar/11 03:15;dvryaboy;PIG_1886.patch;https://issues.apache.org/jira/secure/attachment/12473486/PIG_1886.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,41699,,,,Sun Mar 13 03:41:55 UTC 2011,,,,,,,0|i0gzrb:,97223,,,,,,,,,,"13/Mar/11 03:15;dvryaboy;Attaching patch. Also noticed that the docs didn't mention -limit option, threw it in there. This is a trivial patch, I'll just commit it.",13/Mar/11 03:41;dvryaboy;Committed to trunk and 0.8. Deleted the old ZK jar while I was in there.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SUBSTRING fails when input length less than start,PIG-1885,12500520,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,deepujain,alangates,alangates,05/Mar/11 17:55,04/Aug/11 00:35,14/Mar/19 03:07,14/Mar/11 18:42,0.8.0,0.9.0,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"SUBSTRING throws an error if it gets a string which has a length less than its start value.  For example, SUBSTRING(x, 100, 120) will fail with any chararray of length less than 100.  It should return null instead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11/Mar/11 19:03;deepujain;PIG-1885.txt;https://issues.apache.org/jira/secure/attachment/12473420/PIG-1885.txt,08/Mar/11 18:15;deepujain;PIG-1885.txt;https://issues.apache.org/jira/secure/attachment/12473020/PIG-1885.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-03-08 14:39:37.775,,,no_permission,,,,,,,,,,,,66122,,,,Tue Mar 15 04:56:32 UTC 2011,,,,,,,0|i0gzr3:,97222,"Fix has been tested with 0.9.0 source. (http://svn.apache.org/repos/asf/pig/trunk/)
JUnit test case included.
Created a individual test cases for each scenario. This way its clear that SUBSTRING has been tested with different scenarios , also the same is reflected in test reports. Otherwise having all the scenarios within a single test case, does not reflect the various scenarios that a given api has been tested with.",,,,,,,,,"08/Mar/11 14:39;deepujain;There are few more cases that needs to be handled.

a) beginindex is -ve
Current Behavior: SUBSTRING throws ExceException because of java.lang.StringIndexOutOfBoundsException

b) beginindex > endindex
Current Behavior: SUBSTRING throws ExceException because of java.lang.StringIndexOutOfBoundsException

c) beginindex > string length -- bug reported
Current Behavior: SUBSTRING throws ExceException because of java.lang.StringIndexOutOfBoundsException

d) endindex is -ve
Current Behavior: SUBSTRING throws ExceException because of java.lang.StringIndexOutOfBoundsException


SUBSTRING.java can be modified to return null in all above cases as follows
            if(beginindex < 0 || beginindex > source.length() || beginindex > endindex) {
                return null;
            }else {
                return source.substring(beginindex, Math.min(source.length(), endindex));
            }


In case no one is working on this defect, reassign to me.


","08/Mar/11 18:15;deepujain;Following scenarios are tested

grunt> a = load 'substrinput.txt' as (data:chararray);   
grunt> dump a;                                        
(abcde)
()
(fghi)

1) beginindex == endindex
grunt> a = load 'substrinput.txt' as (data:chararray);
grunt> b = foreach a generate SUBSTRING(data,0,0);    
grunt> dump b;
Output(s):
Successfully stored records in: ""file:/tmp/temp1752997904/tmp55103794""
()
()
()

2) beginindex < endindex  - normal scenario 
grunt> b = foreach a generate SUBSTRING(data,0,3);
grunt> dump b;                                    
Output(s):
Successfully stored records in: ""file:/tmp/temp1752997904/tmp-314465179""
(abc)
()
(fgh)

3) beginindex is -ve
grunt> b = foreach a generate SUBSTRING(data,-1,3);
grunt> dump b;                                     
Output(s):
Successfully stored records in: ""file:/tmp/temp1752997904/tmp286819157""
()
()
()

4) beginindex > String length.
grunt> b = foreach a generate SUBSTRING(data,8,3); 
grunt> dump b;                                    
Output(s):
Successfully stored records in: ""file:/tmp/temp1752997904/tmp-900213360""
()
()
()

5) beginindex > endindex
grunt> b = foreach a generate SUBSTRING(data,2,0);
grunt> dump b;                                    
Output(s):
Successfully stored records in: ""file:/tmp/temp1752997904/tmp-834743686""
()
()
()

5) beginindex is correct endindex > string length.
grunt> b = foreach a generate SUBSTRING(data,0,9);
grunt> dump b;                                    
Output(s):
Successfully stored records in: ""file:/tmp/temp1752997904/tmp243973324""
(abcde)
()
(fghi)

6) beginindex is correct and endindex is -ve
grunt> b = foreach a generate SUBSTRING(data,0,-2);   
grunt> dump b;
Output(s):
Successfully stored records in: ""file:/tmp/temp970048830/tmp1979923245""
()
()
()
grunt> quit
","08/Mar/11 18:51;alangates;Changes look good.  One thought I had is rather than paying the cost of three ifs up front we could still call String.substring as we do today and catch the StringIndexOutofBoundsException, and only then figure out what was and wrong and give an error message.  That way you are only paying the cost of the checks once (since String.substring will do it anyway).  Up to you.

You will need to add units tests before we can commit it.  Add them to the existing TestStringUDFs class.  A test should be added for each of the error conditions you check for.",11/Mar/11 19:07;deepujain;Please review.,11/Mar/11 23:49;alangates;Patch looks good.  I'll run the unit tests and test_patch.,"14/Mar/11 18:38;alangates;  [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec]
     [exec]
     [exec]

Unit tests pass",14/Mar/11 18:42;alangates;Patch committed.  Thanks Deepak.,"15/Mar/11 04:56;deepujain;Yee-Haw
1st Commit.

@Alan
Can you review https://issues.apache.org/jira/browse/PIG-671 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change ReadToEndLoader.setLocation not throw UnsupportedOperationException,PIG-1884,12500487,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,daijy,daijy,04/Mar/11 23:53,25/Apr/11 21:27,14/Mar/19 03:07,08/Mar/11 18:58,0.8.0,0.9.0,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"After PIG-1680, we will call LoadFunc.setLocation() before launching hadoop job. This change cause TestExampleGenerator fail. The reason is ReadToEndLoader.setLocation() will throw UnsupportedOperationException. We never get to this point before PIG-1680.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Mar/11 17:09;thejas;PIG-1884.08.1.patch;https://issues.apache.org/jira/secure/attachment/12473008/PIG-1884.08.1.patch,05/Mar/11 00:24;thejas;PIG-1884.1.patch;https://issues.apache.org/jira/secure/attachment/12472725/PIG-1884.1.patch,07/Mar/11 18:44;thejas;PIG-1884.2.patch;https://issues.apache.org/jira/secure/attachment/12472841/PIG-1884.2.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-03-07 18:44:03.503,,,no_permission,,,,,,,,,,,,165229,,,,Tue Mar 08 18:58:13 UTC 2011,,,,,,,0|i0gzqv:,97221,,,,,,,,,,"04/Mar/11 23:53;daijy;In general, I think calling LoadFunc.setLocation before launching hadoop job is beneficial. This gives user a hook to setup UDFContext properly. There could be some minor backward compatibility issues, but our document clearly state it will be called multiple times in both frontend and backend. So I think it is Ok.

To fix this issue, we can simply remove ""throw UnsupportedOperationException"" in ReadToEndLoader, which seems unnecessary. ","07/Mar/11 18:44;thejas;With PIG-1884.1.patch all (non-contrib) unit tests passed, except for TestJobSubmission.

PIG-1884.2.patch has additional change to start mini hbase cluster. It also fixes the code indentation in TestHBaseStorage to comply with the pig coding standard. ",07/Mar/11 18:48;daijy;+1,07/Mar/11 20:08;thejas;Patch committed to trunk.,"07/Mar/11 20:28;dvryaboy;Alan specifically emailed me asking to stop reindenting things cause it made patches hard to read:).

Sorry about breaking the test. 

What's the change to start mini hbase cluster?","07/Mar/11 20:41;thejas;bq. Alan specifically emailed me asking to stop reindenting things cause it made patches hard to read
Because of the indentation issues, the code was very hard to read ! There was only a one line change other than indentation change in TestHBaseStorage, so I guess it was not that hard to review the patch this time.


bq. What's the change to start mini hbase cluster?
Sorry, my comment was incomplete . The change to start mini hbase cluster was only in TestJobSubmission.java .

I am creating a new patch to fix the related unit test failures in 0.8, as this patch has merge issues in 0.8. I will submit that after running unit tests on 0.8.

","08/Mar/11 17:09;thejas;PIG-1884.08.1.patch - patch generated for 0.8 which includes changes in 0.9 patch, except for ones in TestHBaseStorage.",08/Mar/11 18:58;thejas;Unit tests and test-patch passed with PIG-1884.08.1.patch.  Patch committed to 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig try to interpret UDF as Macro,PIG-1873,12499968,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,daijy,daijy,28/Feb/11 20:13,04/Aug/11 00:34,14/Mar/19 03:07,22/Apr/11 22:54,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"The following script fail:
{code}
A = load '1.txt';
B = group A by $0;
C = foreach B {
    C1 = filter A by $0 > -1;
    C2 = distinct C1;
    C3 = distinct A;
    C4 = org.apache.pig.test.utils.Identity(*);
    C5 = COUNT(C2);
    C6 = SUM(C2.$1);
    C7 = org.apache.pig.test.TestEvalPipeline\$TitleNGrams(C3);
    C8 = MAX(C3.$1);
    generate (int)group, C4, C5, C6, C7, C8, C2;
};
dump C;
{code}
Stack:
ERROR 2999: Unexpected internal error. Pig macro 'COUNT' must be defined before being invoked

java.lang.RuntimeException: Pig macro 'COUNT' must be defined before being invoked
        at org.apache.pig.parser.MacroExpansion.mINLINE(MacroExpansion.java:509)
        at org.apache.pig.parser.MacroExpansion.mTokens(MacroExpansion.java:1240)
        at org.apache.pig.parser.MacroExpansion.nextToken(MacroExpansion.java:68)
        at org.apache.pig.parser.ParserUtil.expandMacros(ParserUtil.java:53)
        at org.apache.pig.parser.ParserUtil.getExpandedMacroAsBufferedReader(ParserUtil.java:89)
        at org.apache.pig.Main.run(Main.java:505)
        at org.apache.pig.Main.main(Main.java:108)

Seems Pig try to interpret UDF COUNT as a Macro.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Mar/11 23:56;rding;PIG-1873.patch;https://issues.apache.org/jira/secure/attachment/12472723/PIG-1873.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-04 23:56:16.914,,,no_permission,,,,,,,,,,,,66245,,,,Wed Apr 06 23:00:45 UTC 2011,,,,,,,0|i0gzof:,97210,Docs updated. See PIG-1772 and patch pig-1772-beta2-1.patch,,,,,,,,,"04/Mar/11 23:56;rding;test-patch results:

{code}
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec] 
{code}","29/Mar/11 21:28;rding;This is addressed by PIG-1931 where Macro inline is allowed only as top-level statements. One cannot invoke/inline a macro in a nested block (it'll cause a parser error). 

By disallowing macro invocation in nested blocks we avoid the problem raised by this jira, i.e., parser ambiguity between macro invocation and assignment with UDF.



",29/Mar/11 23:00;olgan;We need to make sure a reasonable error message is given in this case and also we need to make sure this limitation is documented.,"06/Apr/11 23:00;olgan;Corinne, we need to add a note to both MACRO documentation as well as documentation of nested foreach stating that macros are not supported within the nested block",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix bug in AvroStorage,PIG-1872,12499781,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,guolin2001,guolin2001,guolin2001,26/Feb/11 01:09,04/Aug/11 00:34,14/Mar/19 03:07,28/Feb/11 19:32,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"AvroStorageUtils.containsRecursiveRecord() has a bug and returns true for a record with multiple fields of the same type, e.g.

 { ""type"":""record"", ""name"":""Event"", "" +
    ""fields"":[{""name"":""f1"", ""type"":{ ""type"":""record"",""name"":""EntityID"", ....}}
              {""name"":""f2"",""type"":""EntityID""}, "" +
              {""name"":""f3"",""type"":""EntityID""} ]}
               

Patch contains bug fix and unit tests.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Feb/11 01:11;guolin2001;my.diff;https://issues.apache.org/jira/secure/attachment/12472005/my.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-02-26 01:16:55.202,,,no_permission,,,,,,,,,,,,66335,Reviewed,,,Mon Feb 28 19:32:56 UTC 2011,,,Patch Available,,,,0|i0gzo7:,97209,,,,,,,,,,26/Feb/11 01:16;jghoman;+1.  Looks good to me.,"28/Feb/11 19:32;daijy;test-patch:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Piggybank test pass.

Patch committed to trunk. Thanks Lin, Jakob!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dont throw exception if partition filters  cannot be pushed up. ,PIG-1871,12499773,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,rding,ashutoshc,ashutoshc,25/Feb/11 22:43,04/Aug/11 00:34,14/Mar/19 03:07,18/Apr/11 17:38,,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,Instead don't try to push partition filters up and continue execution.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Apr/11 21:31;rding;PIG-1871_1.patch;https://issues.apache.org/jira/secure/attachment/12476377/PIG-1871_1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-02-25 23:33:44.51,,,no_permission,,,,,,,,,,,,66322,Reviewed,,,Mon Apr 18 17:38:31 UTC 2011,,,,,,,0|i0gznz:,97208,,,,,,,,,,"25/Feb/11 23:14;ashutoshc;a = load 'mydata' USING MyLoader();
b0 = filter a BY
    (   (ds < '20091103' )
     or (action == 'continue' )
    );
b = filter b0 BY
    (   (ds < '20091103' and age < 50)
     or (action == 'continue' and age > 100)
    );
c = foreach b generate ds, action, age, data;
store c into ':OUTPATH:';
...

fails with message:

Grunt - ERROR 1112: 
Unsupported query: 
You have an partition column (datestamp ) in a construction like: (pcond  and ...) or (pcond and ...) where pcond is a condition on a partition column.

In this testcase ds and action are partition columns while age is not. So, pig decided it cannot split the filter and can't push the partition filter to the loader. In such a condition instead of dying, Pig should disable partition filter push up and continue.","25/Feb/11 23:33;dvryaboy;Agreed with the spirit of the thing, but for the particular example -- why not push up the ds and action filters as (ds < '20091103' or action == 'continue') and then apply the original filters in Pig? You still get the benefits of whatever partition pushdown can do, and don't sacrifice correctness.","25/Feb/11 23:43;ashutoshc;Yes, that is the most appropriate thing to do in this case.
Additionally, The query works as is when using older version of Pig:

Apache Pig version 0.7.0.20.10.0.1006041903 (r951530)
compiled Jun 04 2010, 19:03:37","25/Feb/11 23:54;ashutoshc;In Pig 0.8 following succeeds:
a = load 'mydata' USING MyLoader();
b0 = filter a BY
( (ds < '20091103' )
or (action == 'continue' )
);
c = foreach b0 generate ds, action, age, data;
store c into ':OUTPATH:';

This implies that in 0.8, b0 and b got combined first. Then, Pig tried to weed out partitioning columns from the combined expression and bailed out. But clearly, b0 can be pushed up and is separated by 'AND' in combined expression so should have been pushed up. There will still be the cases where partition columns can't be weeded out easily from an expression. In those cases, don't try to push up anything and continue.",14/Apr/11 21:31;rding;Attaching patch that wouldn't throw exceptions if partition filter can't be pushed down. Now warnings are logged instead.,18/Apr/11 17:28;daijy;+1,18/Apr/11 17:38;rding;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBaseStorage doesn't project correctly,PIG-1870,12499764,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,billgraham,billgraham,25/Feb/11 21:34,04/Aug/11 00:34,14/Mar/19 03:07,16/Apr/11 21:31,0.8.1,,,,,,,0.8.1,0.9.0,,,,,,0,,,,,,,,,,,,,"Projecting columns after {{LOAD}} via {{HBaseStorage}} produces unexpected results. This is related to the {{loadKey}} functionality and how the {{pushProjection}} method in {{HBaseStorage}} has to offset to build a column list that aligns with the tuple (the column list doesn't contain the row key).

This shift appears to create an inconsistency with the FieldSchema for the tuple which results in the wrong tuple value being fetched for a given column. I'll attach a patch with unit tests that illustrate the problem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11/Apr/11 23:59;dvryaboy;PIG_1870.2.patch;https://issues.apache.org/jira/secure/attachment/12476069/PIG_1870.2.patch,12/Apr/11 04:43;dvryaboy;PIG_1870.3.patch;https://issues.apache.org/jira/secure/attachment/12476096/PIG_1870.3.patch,13/Apr/11 05:51;dvryaboy;PIG_1870.4.patch;https://issues.apache.org/jira/secure/attachment/12476215/PIG_1870.4.patch,11/Apr/11 07:03;dvryaboy;PIG_1870.patch;https://issues.apache.org/jira/secure/attachment/12475979/PIG_1870.patch,25/Feb/11 21:45;billgraham;PIG_1870_1.patch;https://issues.apache.org/jira/secure/attachment/12471977/PIG_1870_1.patch,12/Apr/11 04:43;dvryaboy;PIG_1870_for0.8.2.patch;https://issues.apache.org/jira/secure/attachment/12476095/PIG_1870_for0.8.2.patch,16/Apr/11 21:26;dvryaboy;PIG_1870_for0.8.final.patch;https://issues.apache.org/jira/secure/attachment/12476537/PIG_1870_for0.8.final.patch,11/Apr/11 03:06;dvryaboy;PIG_1870_for0.8.patch;https://issues.apache.org/jira/secure/attachment/12475955/PIG_1870_for0.8.patch,,,,8.0,,,,,,,,,,,,,,,,,,,2011-04-05 16:36:54.57,,,no_permission,,,,,,,,,,,,41700,,,,Fri Jun 17 19:58:58 UTC 2011,,,,,,,0|i0gznr:,97207,,,,,,,,,,"25/Feb/11 21:45;billgraham;Here's a patch with 5 new projection tests added to {{TestHBaseStorage}}, of which 4 fail. It's intended to be applied over PIG_1680 and it was generated from the git branch show here fyi:

https://github.com/billonahill/pig/commit/55561c23f209ca2f27e13ddd93146d7b2a2492e3","05/Apr/11 16:36;dvryaboy;The problem isn't loadKey, it's that we set up a (static) TableInputFormat.SCAN in setLocation, which gets called multiple times, and not always after pushProjection; we wind up overwriting SCAN and ""forgetting"" we are pushing things. I am working on a patch.",11/Apr/11 03:06;dvryaboy;Attached patch for 0.8,11/Apr/11 07:03;dvryaboy;Attaching patch for trunk.,11/Apr/11 17:29;dvryaboy;This is ready for review.,"11/Apr/11 20:54;billgraham;Dmitriy, I was able to build with the new patches and the TestHBaseStorage test suite ran successfully with both trunk and 0.8.0. I'm getting failures when trying to run an HBase job against a distributed cluster though (version 0.90.0). This is similar to the issue I ran into in PIG-1782 that caused me to mess with how configs were initialized at one point.

These are the only values that I've overriden in {{$HBASE_CONF_DIR/hbase-site.xml}}: 

{noformat}
hbase.rootdir
hbase.cluster.distributed
hbase.tmp.dir
hbase.zookeeper.quorum
hbase.zookeeper.property.dataDir
{noformat}

And this is the error I get trying to run any Pig job against HBase:
{noformat}
2011-04-11 13:36:58,659 [main] ERROR org.apache.hadoop.hbase.zookeeper.ZKConfig - no clientPort found in zoo.cfg
2011-04-11 13:36:58,665 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2017: Internal error creating job configuration.
{noformat}

I don't get this error running the PIG-1782 patch.
","11/Apr/11 23:59;dvryaboy;Thanks, Bill.
Attaching a patch that fixes the config confusion.","12/Apr/11 00:00;dvryaboy;Forgot to click the apache license button -- i hereby submit this patch under the apache license, etc etc etc.
","12/Apr/11 04:02;billgraham;Dymitriy, can you check that latest patch? PIG_1870.patch and PIG_1870.2.patch are the same. ",12/Apr/11 04:42;dvryaboy;whoops :),12/Apr/11 04:43;dvryaboy;patch for 0.8,12/Apr/11 04:43;dvryaboy;Patch for trunk,"12/Apr/11 05:01;billgraham;Either someone ran {{alias diff='echo -n """"'}} on my machine on April 1st, or all these patches are still the same. I think it's the latter. :)",12/Apr/11 05:08;dvryaboy;Are not.,"12/Apr/11 06:13;billgraham;Ok, my bad (embarrassed). Lesson learned: don't wget a patch from JIRA, then manually change the patch name to get another. You'll get something totally unexpected.

Verified that trunk and 0.8.0 branch tests pass for PIG_1870.3.patch and PIG_1870_for0.8.2.patch, respectively. Also verified ad-hoc Pig HBase jobs with projections now work against a cluster for both.",13/Apr/11 05:51;dvryaboy;Massively sped up tests for HBaseStorage by using local mode instead of mapreduce. About 10 secs / test now on my laptop (down from 50).,"15/Apr/11 00:38;daijy;Hi, Dmitriy, do you still plan to commit this patch to 0.8?","15/Apr/11 00:51;dvryaboy;Daniel, yeah, it's ready to go -- just waiting on another committer to +1 it.",15/Apr/11 18:39;daijy;+1. Please commit if test pass.,16/Apr/11 21:31;dvryaboy;Committed to 0.8 branch and trunk.,"17/Jun/11 16:33;qwertymaniac;Could someone please add the appropriate 0.8.x fix version here? Or if that's done when a release is tagged, np. Just thought it might help those following tickets here :)","17/Jun/11 19:58;dvryaboy;it's in 8.1, I updated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New logical plan fails when I have complex data types from udf,PIG-1868,12499556,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,vivekp,vivekp,24/Feb/11 06:42,02/May/13 02:29,14/Mar/19 03:07,27/Mar/11 05:57,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"The new logical plan fails when I have complex data types returning from my eval function.

The below is my script :

{code}
register myudf.jar;   
B1 = load 'myinput' as (id:chararray,ts:int,url:chararray);
B2 = group B1 by id;
B = foreach B2 {
 Tuples = order B1 by ts;
 generate Tuples;
};
C1 = foreach B generate TransformToMyDataType(Tuples,-1,0,1) as seq: { t: ( previous, current, next ) };
C2 = foreach C1 generate FLATTEN(seq);
C3 = foreach C2 generate  current.id as id;
dump C3;
{code}

On C3 it fails with below message :
{code}
Couldn't find matching uid -1 for project (Name: Project Type: bytearray Uid: 45 Input: 0 Column: 1)
{code}

The below is the describe on C1 ;
{code}
C1: {seq: {t: (previous: (id: chararray,ts: int,url: chararray),current: (id: chararray,ts: int,url: chararray),next: (id: chararray,ts: int,url: chararray))}}
{code}

The script works if I turn off new logical plan or use Pig 0.7.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Mar/11 20:13;daijy;PIG-1868-1.patch;https://issues.apache.org/jira/secure/attachment/12473844/PIG-1868-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-15 01:39:43.265,,,no_permission,,,,,,,,,,,,66343,Reviewed,,,Sun Mar 27 05:57:44 UTC 2011,,,,,,,0|i0gznj:,97206,,,,,,,,,,"15/Mar/11 01:39;daijy;Get actually implmenentation of TransformToMyDataType from the user. The problem is TransformToMyDataType forget to set twolevelaccess flag for the bag. Pig 0.9 does not require twolevelaccess flag. However, we saw Pig 0.9 fail with a different stack:

ERROR 1000: Invalid field reference. Referenced field [guid] does not exist in schema: null.

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1067: Unable to explain alias C3
at org.apache.pig.PigServer.explain(PigServer.java:993)
at org.apache.pig.tools.grunt.GruntParser.explainCurrentBatch(GruntParser.java:368)
at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:300)
at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:263)
at org.apache.pig.tools.pigscript.parser.PigScriptParser.Explain(PigScriptParser.java:665)
at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:325)
at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:176)
at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:152)
at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:90)
at org.apache.pig.Main.run(Main.java:537)
at org.apache.pig.Main.main(Main.java:108)
Caused by: org.apache.pig.impl.plan.PlanValidationException: ERROR 1000: Invalid field reference. Referenced field [guid] does not exist in schema: null.
at org.apache.pig.newplan.logical.visitor.ColumnAliasConversionVisitor$1.visit(ColumnAliasConversionVisitor.java:114)
at org.apache.pig.newplan.logical.expression.DereferenceExpression.accept(DereferenceExpression.java:83)
at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
at org.apache.pig.newplan.logical.optimizer.AllExpressionVisitor.visit(AllExpressionVisitor.java:114)
at org.apache.pig.newplan.logical.relational.LOGenerate.accept(LOGenerate.java:240)
at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
at org.apache.pig.newplan.logical.optimizer.AllExpressionVisitor.visit(AllExpressionVisitor.java:104)
at org.apache.pig.newplan.logical.relational.LOForEach.accept(LOForEach.java:73)
at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
at org.apache.pig.PigServer$Graph.compile(PigServer.java:1538)
at org.apache.pig.PigServer$Graph.compile(PigServer.java:1533)
at org.apache.pig.PigServer$Graph.access$200(PigServer.java:1295)
at org.apache.pig.PigServer.buildStorePlan(PigServer.java:1195)
at org.apache.pig.PigServer.explain(PigServer.java:956)
","16/Mar/11 19:55;daijy;The reason for the failure on trunk is because in this query, TransformToMyDataType returns more specific error message than user declared schema ""seq: { t: ( previous, current, next ) }"", and now we only take user declared schema. This is a regression, I will attach a patch to fix it.","25/Mar/11 19:39;thejas;+1
","27/Mar/11 05:57;daijy;Review notes: https://reviews.apache.org/r/526/

Patch committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dereference a bag within a tuple does not work,PIG-1866,12499522,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,23/Feb/11 20:01,04/Aug/11 00:35,14/Mar/19 03:07,21/Apr/11 00:50,0.8.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"The following script does not work (both in new and old logical plan):
{code}
a = load '1.txt' as (t : tuple(i: int, b1: bag { b_tuple : tuple ( b_str: chararray) }));
b = foreach a generate t.b1;
dump b;
{code}
1.txt:
(1,{(one),(two)})

Error from old logical plan:
java.lang.ClassCastException: org.apache.pig.data.BinSedesTuple cannot be cast to org.apache.pig.data.DataBag
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.processInputBag(POProject.java:482)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:197)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.processInputBag(POProject.java:480)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:197)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:339)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:291)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:237)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:232)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)

Error from new logical plan:
java.lang.NullPointerException
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.consumeInputBag(POProject.java:246)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:200)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:339)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:291)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:237)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:232)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)

If we change ""b = foreach a generate t.b1;"" to ""b = foreach a generate t.i;"", it works fine, only refer to a bag does not work.",,,,,,,,,,,,,,,,,,,PIG-1849,,,,,,,,,,,,,22/Mar/11 04:08;daijy;PIG-1866-1.patch;https://issues.apache.org/jira/secure/attachment/12474269/PIG-1866-1.patch,23/Mar/11 03:43;daijy;PIG-1866-2.patch;https://issues.apache.org/jira/secure/attachment/12474365/PIG-1866-2.patch,24/Mar/11 19:18;daijy;PIG-1866-3.patch;https://issues.apache.org/jira/secure/attachment/12474548/PIG-1866-3.patch,16/Jul/11 04:18;kimballa;PIG-1866-4-cdh3-0.8.0.patch;https://issues.apache.org/jira/secure/attachment/12486710/PIG-1866-4-cdh3-0.8.0.patch,19/May/11 21:48;daijy;PIG-1866-4.patch;https://issues.apache.org/jira/secure/attachment/12479846/PIG-1866-4.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-04-02 00:36:25.966,,,no_permission,,,,,,,,,,,,41801,Reviewed,,,Sat Jul 16 04:18:56 UTC 2011,,,,,,,0|i05ifr:,30084,,,,,,,,,,22/Mar/11 04:08;daijy;PIG-1866-1.patch is for 0.8 branch. 0.9 has typed map entailing more changes.,23/Mar/11 03:43;daijy;PIG-1866-2.patch try to fix test failures of PIG-1866-1.patch,"24/Mar/11 19:18;daijy;PIG-1866-3.patch fix all test failures. Also this only fix bag in tuple, which is applicable to both 0.8 and trunk. For trunk there will be one additional patch for bag in map.",02/Apr/11 00:36;thejas;Looks good. +1,"02/Apr/11 00:37;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/524/#review375
-----------------------------------------------------------

Ship it!


+1

- thejas


On 2011-03-24 12:22:48, Daniel Dai wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/524/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-03-24 12:22:48)
bq.  
bq.  
bq.  Review request for pig and thejas.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  The following script does not work (both in new and old logical plan):
bq.  
bq.  a = load '1.txt' as (t : tuple(i: int, b1: bag { b_tuple : tuple ( b_str: chararray) }));
bq.  b = foreach a generate t.b1;
bq.  dump b;
bq.  
bq.  1.txt:
bq.  (1,{(one),(two)})
bq.  
bq.  Error from old logical plan:
bq.  java.lang.ClassCastException: org.apache.pig.data.BinSedesTuple cannot be cast to org.apache.pig.data.DataBag
bq.  at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.processInputBag(POProject.java:482)
bq.  at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:197)
bq.  at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.processInputBag(POProject.java:480)
bq.  at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:197)
bq.  at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:339)
bq.  at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:291)
bq.  at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:237)
bq.  at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:232)
bq.  at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
bq.  at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
bq.  at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
bq.  at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
bq.  at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
bq.  
bq.  Error from new logical plan:
bq.  java.lang.NullPointerException
bq.  at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.consumeInputBag(POProject.java:246)
bq.  at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:200)
bq.  at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:339)
bq.  at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:291)
bq.  at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:237)
bq.  at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:232)
bq.  at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
bq.  at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
bq.  at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
bq.  at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
bq.  at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
bq.  
bq.  If we change ""b = foreach a generate t.b1;"" to ""b = foreach a generate t.i;"", it works fine, only refer to a bag does not work.
bq.  
bq.  
bq.  This addresses bug PIG-1866.
bq.      https://issues.apache.org/jira/browse/PIG-1866
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java 1084415 
bq.    http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POProject.java 1084415 
bq.    http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/newplan/logical/relational/LogToPhyTranslationVisitor.java 1084415 
bq.    http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/TestEvalPipeline2.java 1084415 
bq.    http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/data/GoldenFiles/MRC15.gld 1084415 
bq.  
bq.  Diff: https://reviews.apache.org/r/524/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  test-patch:
bq.       [exec] +1 overall.  
bq.       [exec] 
bq.       [exec]     +1 @author.  The patch does not contain any @author tags.
bq.       [exec] 
bq.       [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
bq.       [exec] 
bq.       [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
bq.       [exec] 
bq.       [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
bq.       [exec] 
bq.       [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
bq.       [exec] 
bq.       [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
bq.  
bq.  Unit test:
bq.      all pass
bq.  
bq.  End-to-end test:
bq.      all pass
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Daniel
bq.  
bq.

","04/Apr/11 19:32;daijy;PIG-1866-3.patch committed to trunk only. We will decide whether to backport it to 0.8 later. 

Also we need to do another patch for trunk for dereference bag inside a map.","21/Apr/11 00:50;daijy;Actually bag inside map is done by POMapLookup, not POProject. It should work now, no need another patch.","04/May/11 02:15;dvryaboy;Daniel, is there any way to get a bag inside a tuple in 0.8? This ticket seems to indicate that this is not possible without your patch.","04/May/11 22:23;daijy;Yes, it only goes to 0.9. For 0.8, you need to apply PIG-1866-3.patch manually.",19/May/11 21:37;daijy;PIG-1866-4.patch contains additional fixes for those who need this patch in 0.8.,16/Jul/11 04:18;kimballa;Here is a version of patch #4 that applies cleanly to CDH3u0 (pig 0.8.0),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BinStorage/PigStorageSchema cannot load data from a different namenode,PIG-1865,12499336,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,vivekp,vivekp,22/Feb/11 12:44,04/Aug/11 00:34,14/Mar/19 03:07,25/Apr/11 21:08,0.7.0,0.8.0,0.9.0,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"BinStorage/PigStorageSchema cannot load data from a different namenode. The main reason for this is that, in the getSchema method , they use org.apache.pig.impl.io.FileLocalizer to check whether the exists, but the filesystem in HDataStorage refers to the natively configured dfs.

The test case is simple :
a = load 'hdfs://<nn2>/input' using BinStorage();
dump a;

Here if I specify -Dmapreduce.job.hdfs-servers, it should have worked , by pig still takes the fs from fs.default.name so to make it work i had to override  fs.default.name in pig command line.

Raising this as a bug since the same scenario works with PigStorage.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Apr/11 23:48;daijy;PIG-1865-1.patch;https://issues.apache.org/jira/secure/attachment/12476950/PIG-1865-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-20 23:48:32.882,,,no_permission,,,,,,,,,,,,66321,Reviewed,,,Mon Apr 25 21:08:00 UTC 2011,,,,,,,0|i0gzn3:,97204,,,,,,,,,,"20/Apr/11 23:48;daijy;The problem is in BinStorage, we use wrong API to check the existence of input file. I didn't see the same issue in other loader (like Zebra).",20/Apr/11 23:49;daijy;There is no test case added. It requires two clusters and is hard to do in unit test. Manually tested and it works.,"25/Apr/11 17:09;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/658/
-----------------------------------------------------------

Review request for pig.


Summary
-------

See PIG-1865


This addresses bug PIG-1865.
    https://issues.apache.org/jira/browse/PIG-1865


Diffs
-----

  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/builtin/BinStorage.java 1095143 

Diff: https://reviews.apache.org/r/658/diff


Testing
-------

Test-patch:
     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

There is no test case added. It requires two clusters and is hard to do in unit test.

Unit-test:
    all pass

Manual-test:
    Tested using two clusters and BinStorage can access remote hdfs


Thanks,

Daniel

",25/Apr/11 19:45;rding;+1,25/Apr/11 21:08;daijy;Patch committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig 0.8 Documentation : non-ascii characters present in sample udf scripts ,PIG-1864,12499194,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,chandec,vivekp,vivekp,21/Feb/11 05:29,04/Aug/11 00:34,14/Mar/19 03:07,25/Mar/11 19:24,0.8.0,,,,,,,0.9.0,,,,documentation,,,0,,,,,,,,,,,,,"In documentation ;
http://pig.apache.org/docs/r0.8.0/udf.html#Python+UDFs

For Sample Script UDFs , there are some non -ascii charaters present. Because of this when we try to execute the sample scripts it fails with error 
ERROR 2999: Unexpected internal error. null

SyntaxError: Non-ASCII character in file '<iostream>', but no encoding declared; see http://www.python.org/peps/pep-0263.html for details
================================================================================


In the sample scripts provided in some of the line wrong characters are prsent . For example :
{code}
@outputSchema(""onestring:chararray"")
{code}


{code}
@outputSchema(""y:bag{t:tuple(len:int,word:chararray)}"") 
{code}

Requesting to have a look at all the udf examples present , since its a common practice to copy the examples directly and do a run .
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-25 19:24:20.532,,,no_permission,,,,,,,,,,,,66315,,,,Fri Mar 25 19:24:20 UTC 2011,,,,,,,0|i0gzmv:,97203,,,,,,,,,,"25/Mar/11 19:24;chandec;Fix for Pig 080 included in Pig-1936 patch-1 patch

Fix for Pig 090 included in Pig-1772 beta-1 patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig returns exit code 0 for the failed Pig script due to non-existing input directory,PIG-1862,12499090,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,18/Feb/11 22:05,25/Apr/11 21:27,14/Mar/19 03:07,10/Mar/11 22:51,0.8.0,,,,,,,0.8.1,,,,impl,,,0,,,,,,,,,,,,,"Test case:

{code}
a = load 'nonexisting.file';
b = filter a by $0 > 0;
c = group b by $0;
d = join c by $0, b by $0;
store d into 'output';
{code}

The script fails, but the return code is 0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Feb/11 22:06;rding;PIG-1862.patch;https://issues.apache.org/jira/secure/attachment/12471441/PIG-1862.patch,08/Mar/11 00:39;rding;PIG-1862_1.patch;https://issues.apache.org/jira/secure/attachment/12472884/PIG-1862_1.patch,25/Feb/11 18:41;rding;PIG-1862_1.patch;https://issues.apache.org/jira/secure/attachment/12471950/PIG-1862_1.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-02-18 23:20:08.605,,,no_permission,,,,,,,,,,,,165223,Reviewed,,,Thu Mar 10 22:51:27 UTC 2011,,,,,,,0|i0gzmf:,97201,,,,,,,,,,"18/Feb/11 23:20;olgan;I am ok with this patch as the short-term fix for 0.8 However, we need to revisit this for 0.9 there are a couple of things in our current approach that seems strange to me:

(1) That now we log temp jobs that failed but not the ones that succeed
(2) That reporting an error is so tightly connected with reporting stats for the store
",19/Feb/11 01:21;rding;committed to branch-0.8,"25/Feb/11 18:41;rding;New patch for trunk that addresses review comments.

Output of test-patch:

{code}
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 12 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{code}",08/Mar/11 00:39;rding;Rebase the patch against trunk.,10/Mar/11 19:55;daijy;+1,10/Mar/11 22:51;rding;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The pig script stored in the Hadoop History logs is stored as a concatenated string without whitespace this causes problems when attempting to extract and execute the script,PIG-1861,12499079,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,araceli,araceli,18/Feb/11 18:39,25/Apr/11 21:27,14/Mar/19 03:07,07/Mar/11 18:09,0.8.0,0.9.0,,,,,,0.8.1,,,,build,,,0,,,,,,,,,,,,,"a = load '$in' using com.yahoo.grid.sath.JobHistoryLoader() as
(job:map[],maps:bag{},reducers:bag{},other:bag{},conf:map[]);

The pig script stored in: conf#'pig.script' has the whitespace removed, this makes it difficult to extract and run the
script. In particular, statements that terminate in "";"" work correctly as
""statement1;statement2;statement99""  but statements that do not end in "";"" result in
""statement1statement2statement3"" and it's difficult to parse the pig script and fix the concatenated string.

There's also a problem with comments as in:

/*mycomment*//*more comments*/ PIG_CODE //comment PIG_CODE
On a side note, I also noticed that in many of the scripts the last statement is missing "";""

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Feb/11 21:50;rding;PIG-1861.patch;https://issues.apache.org/jira/secure/attachment/12471979/PIG-1861.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-02-25 21:50:57.613,,,no_permission,,,,,,,,,,,,165222,Reviewed,,,Wed Mar 23 22:00:45 UTC 2011,,,,,,,0|i0gzm7:,97200,,,,,,,,,,"25/Feb/11 21:50;rding;Output of test-patch:

{code}
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{code}",02/Mar/11 18:15;daijy;+1,07/Mar/11 18:09;rding;Patch committed to trunk.,23/Mar/11 22:00;rding;Patch committed to 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unexpected results from a projection after a union and join,PIG-1859,12498932,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,jkebinger,jkebinger,17/Feb/11 19:00,25/Apr/11 21:27,14/Mar/19 03:07,17/Feb/11 19:39,0.8.0,,,,,,,0.8.1,,,,data,,,0,,,,,,,,,,,,,"Posted this to the pig-users list, and another user indicated he had seen it too, so I thought I'd open a ticket. Adding as major because I can't workaround the projection issue even with numbered $n column names.

I have a log file with events on pages, and the id of the event can be a users login or a users numeric id:

2010-05-14,james
2010-05-15,123
2010-05-15,23
2010-05-15,456
2010-05-15,notjames

So i want to join a set of users on either the login or user id.

Here's my users:

123,james,11
234,notjames,11
456,someoneelse,11


So I thought I would be clever and load the user list, union it with itself to generate a relation where each user is represented twice, once by login, once by id:

logins = FOREACH users GENERATE LOWER(login) as matching_id, user_id as actual_user_id; 
user_ids = FOREACH users GENERATE user_id as matching_id, user_id as actual_user_id;
user_id_or_login_lookup = UNION logins, user_ids;



user_id_or_login_lookup: {matching_id: chararray,actual_user_id: chararray}
(123,123)
(234,234)
(456,456)
(james,123)
(notjames,234)
(someoneelse,456)

Then join on that, by the first column, and project that away, leaving just the event info and the numeric id.

views_with_id = JOIN profile_views by viewed_user_id, user_id_or_login_lookup by matching_id;

That is not working however. My joined relation looks like this (which is what I expect)

views_with_id: {profile_views::date: chararray,profile_views::viewed_user_id: chararray,user_id_or_login_lookup::matching_id: chararray,user_id_or_login_lookup::actual_user_id: chararray}

(2010-05-15,123,123,123)
(2010-05-15,456,456,456)
(2010-05-14,james,james,123)
(2010-05-15,notjames,notjames,234)

But when I project as follows: views_with_id_projected = FOREACH views_with_id GENERATE date, viewed_user_id, user_id_or_login_lookup::actual_user_id;

The result is not what I expect

(2010-05-15,123,123)
(2010-05-15,456,456)
(2010-05-14,james,james)
(2010-05-15,notjames,notjames)

To be clear, I expect

(2010-05-15,123,123)
(2010-05-15,456,456)
(2010-05-14,james,123)
(2010-05-15,notjames,456)

Here's my full pig script:

users = LOAD 'patients-test.txt' USING PigStorage(',') AS (user_id:chararray, login:chararray, disease_id: chararray);
profile_views = LOAD 'patient-views-test.txt' USING PigStorage(',') AS(date: chararray, viewed_user_id:chararray);

dump users;
dump profile_views;

-- build a relation so that users are present to join by login or user_id
logins = FOREACH users GENERATE LOWER(login) as matching_id, user_id as actual_user_id; 
user_ids = FOREACH users GENERATE user_id as matching_id, user_id as actual_user_id;
user_id_or_login_lookup = UNION logins, user_ids;

dump user_id_or_login_lookup;
describe user_id_or_login_lookup;

views_with_id = JOIN profile_views by viewed_user_id, user_id_or_login_lookup by matching_id;

describe views_with_id;
--STORE views_with_id into 'ep-views.txt';

dump views_with_id;

views_with_id_projected = FOREACH views_with_id GENERATE date, viewed_user_id, user_id_or_login_lookup::actual_user_id;


dump views_with_id_projected;
","Mac OS 10.6.6
java version ""1.6.0_22""
Java(TM) SE Runtime Environment (build 1.6.0_22-b04-307-10M3261)
Java HotSpot(TM) 64-Bit Server VM (build 17.1-b03-307, mixed mode)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-02-17 19:46:39.683,,,no_permission,,,,,,,,,,,,165220,,,,Thu Feb 17 19:46:39 UTC 2011,,,,,,,0|i0gzlr:,97198,,,,,,,,,,"17/Feb/11 19:46;daijy;This is already fixed on 0.8 branch. Please check out 0.8 branch to get the fix:
svn co http://svn.apache.org/repos/asf/pig/branches/branch-0.8",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDF in nested plan results frontend exception,PIG-1858,12498845,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,vivekp,vivekp,17/Feb/11 08:27,04/Aug/11 00:34,14/Mar/19 03:07,12/Apr/11 00:45,0.8.0,0.9.0,,,,,,0.9.0,,,,,,,1,,,,,,,,,,,,,"The below is my script :
{code}
register myanotherudf.jar;
A = load 'myinput' using PigStorage() as ( date:chararray,bcookie:chararray,count:int,avg:double,pvs:int);
B = foreach A generate (int)(avg / 100.0) * 100   as avg, pvs;
C = group B by ( avg );
D = foreach C {
        Pvs = order B by pvs;
        Const = org.vivek.MyAnotherUDF(Pvs.pvs).(count,sum);
        generate Const.sum as sum;
        };
store D into 'out_D';
{code}

The script is failing during compilation of the plan. The usage of the udf inside the foreach is causing the problem. The udf implements algebraic and the 
output schema is also defined.
The below is the exception that I get :

ERROR 2042: Error in new logical plan. Try -Dpig.usenewlogicalplan=false.

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2042: Error in new logical plan. Try -Dpig.usenewlogicalplan=false.
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:309)
        at org.apache.pig.PigServer.compilePp(PigServer.java:1364)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1206)
        at org.apache.pig.PigServer.execute(PigServer.java:1200)
        at org.apache.pig.PigServer.access$100(PigServer.java:128)
        at org.apache.pig.PigServer$Graph.execute(PigServer.java:1527)
        at org.apache.pig.PigServer.executeBatchEx(PigServer.java:372)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:339)
        at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:112)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:169)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:141)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:90)
        at org.apache.pig.Main.run(Main.java:500)
        at org.apache.pig.Main.main(Main.java:107)
Caused by: java.lang.NullPointerException
        at org.apache.pig.newplan.ReverseDependencyOrderWalker.walk(ReverseDependencyOrderWalker.java:70)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:105)
        at org.apache.pig.newplan.logical.relational.LOGenerate.accept(LOGenerate.java:229)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:94)
        at org.apache.pig.newplan.logical.relational.LOForEach.accept(LOForEach.java:71)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:261)
        ... 13 more
 

When i trun off new logical plan the script executes successfully. The issue is observed in both 0.8 and 0.9
",,,,,,,,,,,,,,,PIG-1798,,,,,,,,,,,,,,,,,17/Feb/11 08:37;vivekp;MyAnotherUDF.java;https://issues.apache.org/jira/secure/attachment/12471258/MyAnotherUDF.java,22/Feb/11 06:44;daijy;PIG-1858-1.patch;https://issues.apache.org/jira/secure/attachment/12471600/PIG-1858-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-02-17 18:39:55.432,,,no_permission,,,,,,,,,,,,66287,Reviewed,,,Tue Apr 12 00:45:13 UTC 2011,,,,,,,0|i0gzlj:,97197,,,,,,,,,,17/Feb/11 08:37;vivekp;Attaching the udf source,"17/Feb/11 18:39;daijy;Please rewrite the query like the following currently. I will provide a fix later.

{code}
register myanotherudf.jar;
A = load 'myinput' using PigStorage() as ( date:chararray,bcookie:chararray,count:int,avg:double,pvs:int);
B = foreach A generate (int)(avg / 100.0) * 100   as avg, pvs;
C = group B by ( avg );
D = foreach C {
        Pvs = order B by pvs;
        generate MyAnotherUDF(Pvs.pvs).sum as sum;
        };
store D into 'out_D';
{code}
","17/Feb/11 18:42;daijy;Also the fix may involve a non-trivial change in parser, we probably want to fix it in the new parser (Pig 0.9)","19/Feb/11 00:01;daijy;PIG-1860 provide another script share the same problem:
{code}
register myanotherudf.jar;
A = load 'myinput' using PigStorage() as ( date:chararray,bcookie:chararray,count:int,avg:double,pvs:int);
B = foreach A generate (int)(avg / 100.0) * 100   as avg, pvs;
C = group B by ( avg );
D = foreach C {
        Pvs = order B by pvs;
        Const = org.vivek.MyAnotherUDF(Pvs.pvs).(count,sum);
        generate Const.sum as sum;
        };
store D into 'out_D';
{code}

New logical plan give the same frontend exception. Old logical plan give wrong result.","19/Feb/11 00:30;daijy;Actually they are the same query. Checked the old logical plan, nested sort is not even in the plan. The plan generated is completely wrong. We feed bag B directly to MyAnotherUDF without sort, projection.

The first question is whether MyAnotherUDF mean to take Pvs.vs as bag or tuple. 

If it takes a bag, move MyAnotherUDF to generate will work. The meaning for this query is sort B first, get a sorted bag, then feed to MyAnotherUDF.

If it takes a tuple, which means MyAnotherUDF take individual tuple of B, then it is similar to a nested foreach. We do not currently support it (Unfortunately old logical plan does not complain and give wrong result). In nested plan, we can only transform tuple coming from input bag using sort/filter/limit/distinct/simple projection.

In sum, no matter MyAnotherUDF takes tuple/bag, old plan generates wrong plan, new plan fail on frontend. If bag, the right syntax is move MyAnotherUDF into generate. If tuple, it is not currently supported. 

To fix it, currently we can provide meaningful message. In the future, we can support nested foreach to address this use case.","20/Feb/11 00:58;ciemo;Daniel, the original code for ""MyAnotherUDF"" tags a bag as the input. I'm pretty sure that this worked in some older versions of Pig (like Pig 0.2) and so this is a regression (I think).","22/Feb/11 06:44;daijy;Hi, Ciemiewicz,
Yes, UDF takes a bag should work even not in generate statement. I will consider it a bug. PIG-1858-1.patch fix it. 

More naturally, people may want UDF take tuples, which is not currently supported. We plan to support nested foreach in future release to solve the problem.","22/Feb/11 08:51;daijy;Review request: https://reviews.apache.org/r/439/

Note this patch is only for 0.8 branch since it only changes old parser. For 0.9, we need to verify in new parser.",22/Feb/11 23:11;rding;+1,22/Feb/11 23:43;daijy;PIG-1858-1.patch committed to 0.8 branch. Leave the Jira open cuz we also want to verify it in the new parser.,12/Apr/11 00:45;daijy;Verify new parser works as well.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Custom jar is not packaged with the new job created by LimitAdjuster ,PIG-1856,12498700,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,15/Feb/11 23:26,04/Aug/11 00:34,14/Mar/19 03:07,01/Mar/11 19:52,0.8.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"The script:

{code}
A = load 'data' as (s, m);
B = order A by s parallel 2;
C = limit B 20;
store C into 'output' using org.apache.pig.piggybank.storage.PigStorageSchema('\t')
{code}

where piggybank jar is in the classpath.

The script, however,  fails since the piggybank jar isn't shipped to the backend with the additional job created by the LimitAdjuster.

The workaround is to explicitly register the piggybank jar in the script. 

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Feb/11 18:02;rding;PIG-1856.patch;https://issues.apache.org/jira/secure/attachment/12471300/PIG-1856.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-01 19:41:26.771,,,no_permission,,,,,,,,,,,,66266,Reviewed,,,Tue Mar 01 19:52:14 UTC 2011,,,,,,,0|i0gzl3:,97195,,,,,,,,,,"17/Feb/11 18:02;rding;Test-patch output:

{code}
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

{code}",18/Feb/11 00:42;rding;Unit tests pass.,01/Mar/11 19:41;daijy;+1,01/Mar/11 19:52;rding;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig returns exit code 0 for the failed Pig script .,PIG-1854,12498509,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,peeyushb,peeyushb,14/Feb/11 09:03,25/Apr/11 21:27,14/Mar/19 03:07,18/Feb/11 22:08,0.7.0,0.8.0,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"Pig returns exit code 0 for the some of the failed Pig scripts. Due to this workflow system like Oozie showing the ""succeeded"" status instead of ""killed"" status.

Illustrative example
---------------------

Suppose in Pig script we have following statement:

$ cat move.pig
fs -mv /user/jerry/a.txt /user/tom/

On executing the pig script, script failed with following exception

[tom@abc ]$ pig move.pig 
2011-02-14 06:48:08,354 [main] INFO  org.apache.pig.Main - Logging error messages to: /home/tom/pig_1297666088350.log
2011-02-14 06:48:08,652 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://abc-nn:8020
2011-02-14 06:48:09,542 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to map-reduce job tracker at: abc-jt:50300
mv: org.apache.hadoop.security.AccessControlException: Permission denied: user=tom, access=WRITE, inode=""jobs"":jerry:users:rwxr-xr-x
[tom@abc ]$


But when executed the similar Pig script through Oozie workflow, Pig script failed but Oozie show the ""succeeded"" status message instead of ""killed"" status as return exit code is 0.

From logs of workflow launcher job:

stdout logs
-----------
>>> Invoking Pig command line now >>>

Apache Pig version 0.7.0.20.100.1.1007220309 (r966485) 
compiled Jul 22 2010, 03:09:21

807  [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine  - Connecting to hadoop file system at: hdfs://abc-nn:8020/
853  [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine  - Connecting to map-reduce job tracker at: abc-jt:50300

<<< Invocation of Pig command completed <<<

 Hadoop Job IDs executed by Pig: 

<<< Invocation of Main class completed <<<


Oozie Launcher, capturing output data:
=======================
#
#Mon Feb 14 06:46:00 UTC 2011
hadoopJobs=

=======================

Oozie Launcher ends


stderr logs
-----------
mv: org.apache.hadoop.security.AccessControlException: Permission denied: user=tom, access=WRITE, inode=""jobs"":jerry:users:rwxr-xr-x


One more scenario where Pig script failed but exit code is 0 and no exception occurred for this failure.


stdout logs
-----------
>>> Invoking Pig command line now >>>


Apache Pig version 0.7.0.20.100.1.1007220309 (r966485) 
compiled Jul 22 2010, 03:09:21
...
...
48247 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher  - 100% complete
48248 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher  - 1 map reduce job(s) failed!
48306 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher  - Failed to produce result in: ""hdfs://abc-nn/user/jerry/feature-keys""
48307 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher  - Failed!

<<< Invocation of Pig command completed <<<

 Hadoop Job IDs executed by Pig: job_201102080811_28852

<<< Invocation of Main class completed <<<

Oozie Launcher, capturing output data:
=======================
#
#Sat Feb 12 04:13:53 UTC 2011
hadoopJobs=job_201102080811_28852

=======================

Oozie Launcher ends

---
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Feb/11 00:44;rding;PIG-1854.patch;https://issues.apache.org/jira/secure/attachment/12471045/PIG-1854.patch,15/Feb/11 00:10;rding;PIG-1854.patch;https://issues.apache.org/jira/secure/attachment/12471039/PIG-1854.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-02-15 00:10:03.165,,,no_permission,,,,,,,,,,,,165218,,,,Fri Feb 18 22:08:37 UTC 2011,,,,,,,0|i0gzkn:,97193,,,,,,,,,,15/Feb/11 00:10;rding;The problem is that Pig doesn't check the return code while calling method FsShell.run. This works fine in interactive mode since the error message is written to the stdout. The fix checks the return code and throws an IOException in batch mode.,15/Feb/11 00:18;olgan;changes look good. One question - are we guaranteed -1 return from FSShell for all errors or should we be testing for non-0 return instead?,15/Feb/11 00:44;rding;You're right. Change the checking of return code from -1 to none-zero.,15/Feb/11 01:11;olgan;+1,15/Feb/11 18:34;rding;patch committed to both trunk and 0.8 branch.,"18/Feb/11 21:45;rding;Here is another case: 

{code}
a = load 'nonexisting.file';
b = filter a by $0 > 0;
c = group b by $0;
d = join c by $0, b by $0;
store d into 'output';
{code}

The script fails due to non-existing input file, but the return code is 0.","18/Feb/11 21:54;olgan;Unless this two issues are related, lets use a separate JIRA to track.",18/Feb/11 22:08;rding;Opened PIG-1862 to trunk the issue.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Packaging antlr jar with pig.jar,PIG-1852,12498412,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,11/Feb/11 18:51,04/Aug/11 00:35,14/Mar/19 03:07,11/Feb/11 23:47,,,,,,,,0.9.0,,,,build,,,0,,,,,,,,,,,,,Pig 0.9 uses ANTLR as default parser. Pig needs to add antlr jar to pig.jar and pig-withouthadoop.jar.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11/Feb/11 20:19;rding;PIG-1852.patch;https://issues.apache.org/jira/secure/attachment/12470891/PIG-1852.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-02-11 20:47:48.81,,,no_permission,,,,,,,,,,,,66159,Reviewed,,,Fri Feb 11 23:47:55 UTC 2011,,,,,,,0|i0gzk7:,97191,,,,,,,,,,"11/Feb/11 20:47;ashutoshc;antlr shouldn't be pig-withouthadoop.jar since its a client side dependency. pig-withouthadoop.jar gets shipped to task nodes, we should not add anything in it which is not needed in backend.","11/Feb/11 20:51;ashutoshc;Also, antlr.jar is required only while compiling pig, antlr-runtime.jar should be used instead in pig.jar","11/Feb/11 22:17;rding;Ashutosh, you're right. But the antlr-3.2.jar Pig uses now is actually antlr[all].jar. What Pig needs is to pull the various antlr jars from apache maven repository (PIG-1583). ","11/Feb/11 22:25;daijy;Ideally we should pack antlr-runtime.jar only. However, it seems antlr-runtime.jar is not enough, we have problem in running pig if we only pack antlr-runtime.jar. For now, we can pack antlr.jar so Pig can run. Later we can figure out how to optimize antlr.jar.",11/Feb/11 23:47;rding;patch committed to trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Order by is failing with ClassCastException if schema is undefined for new logical plan in 0.8,PIG-1850,12498365,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,vivekp,vivekp,11/Feb/11 13:50,25/Apr/11 21:27,14/Mar/19 03:07,18/Feb/11 06:16,0.8.0,0.9.0,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"The below is the script :

A = load 'input' ;
B = group A all;
C = foreach B generate SUM($1.$0);
C1 = CROSS A,C;
D = foreach C1 generate ROUND($0*10000.0/$2)/100.0, $1;
E = order D by $0 desc; 
store E  into 'out1';

input (tab separated fields)
26      AAAAA
1349595 BBBBB
235693  CCCCC


Exception
java.lang.ClassCastException: org.apache.pig.impl.io.NullableDoubleWritable cannot be cast to org.apache.pig.impl.io.NullableBytesWritable
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigBytesRawComparator.compare(PigBytesRawComparator.java:94)
	at java.util.Arrays.binarySearch0(Arrays.java:2105)
	at java.util.Arrays.binarySearch(Arrays.java:2043)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.getPartition(WeightedRangePartitioner.java:72)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.getPartition(WeightedRangePartitioner.java:52)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:602)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:116)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:238)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:231)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:676)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:336)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:242)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
	at org.apache.hadoop.mapred.Child.main(Child.java:236)


The script is failing while doing order by in WeightedRangePartitioner since it considers the quantiles to be NullableBytesWritable but at run time this is NullableDoubleWritable . This is happening because there is no schema defined in the load statement.
But the same works fine when the  multiquery is turned off.

One more issue worth noting is that if i have a filter statement after relation E, then the above exception is swallowed by Pig. This make debugging really hard. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Feb/11 19:56;daijy;PIG-1850-1.patch;https://issues.apache.org/jira/secure/attachment/12471105/PIG-1850-1.patch,15/Feb/11 22:55;daijy;PIG-1850-2.patch;https://issues.apache.org/jira/secure/attachment/12471127/PIG-1850-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-02-15 22:55:59.328,,,no_permission,,,,,,,,,,,,165216,Reviewed,,,Fri Feb 18 06:16:43 UTC 2011,,,,,,,0|i0gzjr:,97189,,,,,,,,,,15/Feb/11 22:55;daijy;Attach another patch (PIG-1850-2.patch) to solve potential test fail.,"15/Feb/11 23:18;thejas;+1 Please commit after test-patch and unit tests pass.
","18/Feb/11 06:16;daijy;Review notes:
https://reviews.apache.org/r/424/

Patch committed to both trunk and 0.8 branch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in schema generation,PIG-1843,12497735,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,05/Feb/11 02:42,25/Apr/11 21:27,14/Mar/19 03:07,18/Feb/11 01:48,0.8.0,,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"Hit NPE in following script:
{code}
a = load 'table_testBagDereferenceInMiddle2' as (a0:chararray);
b = foreach a generate MapGenerate(STRSPLIT(a0).$0));
{code}
{code}
public class MapGenerate extends EvalFunc<Map> {
    @Override
    public Map exec(Tuple input) throws IOException {
        Map m = new HashMap();
        m.put(""key"", new Integer(input.size()));
        return m;
    }
    
    @Override
    public Schema outputSchema(Schema input) {
        return new Schema(new Schema.FieldSchema(getSchemaName(""parselong"", input), DataType.MAP));
    }
}
{code}

Error message:
Caused by: java.lang.NullPointerException
        at org.apache.pig.EvalFunc.getSchemaName(EvalFunc.java:76)
        at string.PARSELONG.outputSchema(PARSELONG.java:63)
        at org.apache.pig.newplan.logical.expression.UserFuncExpression.getFieldSchema(UserFuncExpression.java:154)
        at org.apache.pig.newplan.logical.optimizer.FieldSchemaResetter.execute(SchemaResetter.java:192)
        at org.apache.pig.newplan.logical.expression.AllSameExpressionVisitor.visit(AllSameExpressionVisitor.java:143)
        at org.apache.pig.newplan.logical.expression.UserFuncExpression.accept(UserFuncExpression.java:71)
        at org.apache.pig.newplan.ReverseDependencyOrderWalker.walk(ReverseDependencyOrderWalker.java:70)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:104)
        at org.apache.pig.newplan.logical.relational.LOGenerate.accept(LOGenerate.java:240)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:93)
        at org.apache.pig.newplan.logical.relational.LOForEach.accept(LOForEach.java:73)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:279)
        at org.apache.pig.PigServer.compilePp(PigServer.java:1480)
        at org.apache.pig.PigServer.explain(PigServer.java:1042)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Feb/11 02:48;daijy;PIG-1843-1.patch;https://issues.apache.org/jira/secure/attachment/12470337/PIG-1843-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-02-14 23:45:40.729,,,no_permission,,,,,,,,,,,,165213,Reviewed,,,Fri Feb 18 01:48:25 UTC 2011,,,,,,,0|i0gzi7:,97182,,,,,,,,,,"05/Feb/11 02:44;daijy;The problem happens when we have nested UDF:
1. Inner UDF does not define complete outputSchema
2. Outer UDF does not define getArgToFuncMapping
3. outputSchema in outer UDF uses inner schema to infer alias",14/Feb/11 23:45;thejas;+1,18/Feb/11 01:46;daijy;Review notes: NPE in schema generation,18/Feb/11 01:46;daijy;Review notes: https://reviews.apache.org/r/423/,18/Feb/11 01:48;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TupleSize implemented incorrectly,PIG-1841,12497593,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,laukik,cheddar,cheddar,03/Feb/11 17:31,25/Apr/11 21:27,14/Mar/19 03:07,18/Feb/11 01:53,0.8.0,,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"I sent this to the list:

I'm looking at Pig's TupleSize implementation and wondering if it's
implemented correctly:

   @Override
   public Long exec(Tuple input) throws IOException {
       try{
           if (input == null) return null;
           return Long.valueOf(input.size());
       }catch(Exception e){
           int errCode = 2106;
           String msg = ""Error while computing size in "" +
this.getClass().getSimpleName();
           throw new ExecException(msg, errCode, PigException.BUG,
e);
       }
   }


I have a script that looks like

A = FOREACH A GENERATE STRSPLIT(value, '\u0001') AS values;
B = FOREACH B GENERATE values, SIZE(values) AS cnt;

and cnt always ends up as 1.  From the code, it looks like TupleSize
is intended to only return the number of arguments into the SIZE()
UDF?  Is that really the intention and I'm using the SIZE() UDF wrong?
 Or, is it just a bug and it's supposed to be written as ""return
Long.valueOf(((Tuple) input.get(0)).size()))""?


I got this response back:

This is definitely a bug. Can you open a Jira ticket?

Done!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Feb/11 01:35;laukik;PIG-1841.patch;https://issues.apache.org/jira/secure/attachment/12470751/PIG-1841.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-02-10 01:35:09.465,,,no_permission,,,,,,,,,,,,165211,Reviewed,,,Fri Feb 18 01:53:17 UTC 2011,,,,,,,0|i0gzhr:,97180,,,,,,,,,,"10/Feb/11 01:35;laukik;The TupleSize.exec(Tuple input) method needed to unwrap the actual Tuple whose size is to be determined from the input Tuple. Fixed the unit tests which were assuming that there would be no wrapping of the object of interest. Also added a new end-to-end test case.

Results from the test-patch run:

{noformat}
     [exec]
     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec]
{noformat}
",10/Feb/11 02:38;daijy;+1,18/Feb/11 01:53;daijy;All tests pass. Patch committed to both trunk and 0.8 branch. Thanks Laukik!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
piggybank: XMLLoader will always add an extra empty tuple even if no tags are matched,PIG-1839,12497461,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,vivekp,vivekp,vivekp,02/Feb/11 13:07,04/Aug/11 00:34,14/Mar/19 03:07,03/Feb/11 22:44,0.7.0,0.8.0,0.9.0,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"The XMLLoader in piggy bank always add an empty tuple. Everytime this has to be filtered out. Instead the same could be done by the loader itself.
Consider the below script :
a= load 'a.xml' using org.apache.pig.piggybank.storage.XMLLoader('name');
dump a;
b= filter a by $0  is not null;
dump b;


The output of first dump is :
(<name> foobar </name>)
(<name> foo </name>)
(<name> justname </name>)
()

The output of second dump is :
(<name> foobar </name>)
(<name> foo </name>)
(<name> justname </name>)

Again another case is if I dont have a matching tag , still the loader will generate the empty tuple.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Feb/11 14:12;vivekp;PIG-1839-1.patch;https://issues.apache.org/jira/secure/attachment/12470046/PIG-1839-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-03 19:21:59.978,,,no_permission,,,,,,,,,,,,66264,,,,Thu Mar 03 19:21:59 UTC 2011,,,Patch Available,,,,0|i0gzhb:,97178,contrib unit tests all pass.  Fix checked in.  Thanks Vivek for the contribution.,,,,,,,,,"02/Feb/11 14:12;vivekp;Attaching the initial patch. 
Please note that I have modified the existing test case to assert for the correct number of tuples .",03/Mar/11 19:21;alangates;Checked into the 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error while using IsEmpty function,PIG-1837,12497399,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,olgan,olgan,01/Feb/11 23:57,04/Aug/11 00:34,14/Mar/19 03:07,07/Mar/11 20:06,0.9.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"I have the following 2 inputs:

data1: 
1       11
2       15

data2:
1       10
4       11
5       10

And the following script to work on the data:

grunt> A = load 'data1' as (x, y: int);
grunt> B = load 'data2' as (x, z: int);
grunt> C = cogroup A by x, B by x;
grunt> D = foreach C generate group, SUM(((IsEmpty(A.y))? {(0)} : A.y)) + SUM (((IsEmpty(B.z))? {(0)} : B.z));
grunt> dump D;

After running for a while, I get the following error:

Backend error : org.apache.pig.builtin.IsEmpty cannot be cast to org.apache.pig.Accumulator

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Mar/11 21:52;rding;PIG-1837.patch;https://issues.apache.org/jira/secure/attachment/12472713/PIG-1837.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-04 21:52:58.515,,,no_permission,,,,,,,,,,,,66301,Reviewed,,,Mon Mar 07 20:06:58 UTC 2011,,,,,,,0|i0gzh3:,97177,,,,,,,,,,"04/Mar/11 21:52;rding;test-patch results:

{code}
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec] 
{code}",07/Mar/11 19:57;daijy;+1,07/Mar/11 20:06;rding;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig 0.9 new logical plan throws class cast exception,PIG-1835,12497302,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,vivekp,vivekp,01/Feb/11 09:42,04/Aug/11 00:35,14/Mar/19 03:07,02/Feb/11 18:51,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"I have the below script which is throwing class cast exception while doing SUM. Even though all the fields are properly typed, while computing sum in m_agg0 and m_agg02 the record from tuple is coming as java.lang.Long instead of Double.

The problem is happening in Pig 0.9. It works fine with 0.9 if I flag off new logical plan by -Dpig.usenewlogicalplan=false. 

{code}
A0 = load 'inputA' using PigStorage('\t') as ( group_id, r_id:long, is_phase2:int, roi_value:double,roi_cost:double,ecpm, prob:double,pixel_id, pixel_type,
val:long,f3, f4,type:long, amount:double,item_id:long);

A0 = foreach A0 generate r_id, is_phase2, ((val==257 or val==258)? 1: 0) as imps,
        ((val==257 or val==258)? amount: 0.0) as a_out, ((val==257 or val==258)? item_id: 0) as a_item_id,
        ((val==257 or val==258)? roi_value: 0.0) as roi_value,((val==257 or val==258)? roi_cost: 0.0) as roi_cost,
        ((val==257 or val==513)? ecpm: 0.0) as ecpm, ((val==257 or val==513)? prob: 0.0) as prob,
        ((val==257 or val==513)? amount: 0.0) as pub_rev, ((val==257 or val==513)? item_id: 0) as pub_line_id,((val==257 or val==513)? type: 0) as pub_pt;
-------------------------------------------------------------------------------------------------------------------------------------------------
B0 = load 'inputB' using PigStorage('\t') as ( group_id:long, r_id:long, roi_value:double,roi_cost:double,receive_time, host_name,site_id,rm_has_cookies,rm_pearl_id, f1,f2,pixel_id:long,pixel_type:int, xcookie,val:long,f3, f4,type:long,amount:double,item_id:long);

B0 = foreach B0 generate r_id, ((val==257 or val==258)? 1: 0) as B0,((val==257 or val==258)? amount: 0.0) as a_out,
        ((val==257 or val==258)? item_id: 0) as a_item_id,((val==257 or val==258)? roi_value: 0.0) as roi_value,
        ((val==257 or val==258)? roi_cost: 0.0) as roi_cost, ((val==257 or val==513)? amount: 0.0) as pub_rev,
        ((val==257 or val==513)? item_id: 0) as pub_line_id, ((val==257 or val==513)? type: 0) as pub_pt;
------------------------------------------------------------------------------------------------------------------------------------------------
C0 = load 'inputC' using PigStorage('\t') as (  group_id:long, r_id:long, roi_value:double, roi_cost:double, receive_time:long, host_name:chararray, site_id:long, rm_has_cookies:int,rm_pearl_id:long,f1,f2, pixel_id:long, pixel_type:int,rm_is_post_click:int, rm_conversion_id,xcookie:chararray,val:long,f3:long,f4:long,type:long,amount:double,item_id:long);

C0 = foreach C0 generate   r_id,((val==257 or val==258)? 1: 0) as C0, ((val==257 or val==258)? amount: 0.0) as a_out,
        ((val==257 or val==258)? item_id: 0) as a_item_id,((val==257 or val==513)? amount: 0.0) as pub_rev,
        ((val==257 or val==513)? item_id: 0) as pub_line_id, ((val==257 or val==513)? type: 0) as pub_pt;
------------------------------------------------------------------------------------------------------------------------------------------------
m_all = cogroup   A0 by (r_id) outer, B0 by (r_id) outer, C0  by (r_id) outer ;
m_agg01 = foreach m_all generate (double)(IsEmpty(C0) ? 0.0 : SUM(C0.pub_rev)) as conv_pub_rev;
store m_agg01 into 'out1' USING PigStorage(',');

m_all = cogroup   A0 by (r_id) outer, B0 by (r_id) outer, C0  by (r_id) outer ;
m_agg02 = foreach m_all generate (double)(IsEmpty(C0) ? 0.0 : SUM(C0.pub_rev)) as conv_pub_rev;
store m_agg02 into 'out2' USING PigStorage(',');
{code}



The below are the inputs to the script (all single record and tab seperated)

{code}
inputA
------
1111    1111    1       1.1     1.1     1.1     1.1     1111    1       1111    1111    1111    1111    1.1     1111

inputB
------
1111    1111    1.1     1.1     1111    a1      1111    1       1111    b1      1       1111    1       c1      1111    1111    1111    1111    1.1     1111

inputC
------
1111    1111    1.1     1.1     1111    a1      1111    1       1111    b1      1       1111    1       1       1111    c1      1111    1111    1111    1111    1.1     1111
{code}

Exception from reducers 
_______________________
org.apache.pig.backend.executionengine.ExecException: ERROR 2103: Problem while computing sum of doubles.
	at org.apache.pig.builtin.DoubleSum.sum(DoubleSum.java:147)
	at org.apache.pig.builtin.DoubleSum.exec(DoubleSum.java:46)
	at org.apache.pig.builtin.DoubleSum.exec(DoubleSum.java:41)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:230)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:302)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POBinCond.getNext(POBinCond.java:140)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(POCast.java:446)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:346)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:289)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.runPipeline(PigMapReduce.java:455)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.processOnePackageOutput(PigMapReduce.java:423)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:403)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:258)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:572)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:414)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:242)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1062)
	at org.apache.hadoop.mapred.Child.main(Child.java:236)
Caused by: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Double
	at org.apache.pig.builtin.DoubleSum.sum(DoubleSum.java:140)
	... 20 more

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-02-02 18:51:36.285,,,no_permission,,,,,,,,,,,,66111,,,,Wed Feb 02 18:51:36 UTC 2011,,,,,,,0|i0gzgn:,97175,,,,,,,,,,02/Feb/11 18:51;daijy;The issue only happen in older version of trunk. Currently trunk and all the releases does not have this issue.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
relation-as-scalar - uses the last statement associated with the scalar alias,PIG-1834,12497103,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,thejas,thejas,28/Jan/11 21:42,04/Aug/11 00:34,14/Mar/19 03:07,04/Mar/11 22:13,0.8.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"Pig allows relation alias to be re-used , ie refer to different relations(/statements) . I have not seen this in documentation, but I have seen people writing such queries.

For example -
{code}
l = load 'x' as (a,b);
l = filter l by a > 1;
l = foreach ...
store l into  'y'
{code}

At any part of the query, the alias ""l' always represents the relation it last associated with the portion of pig-query above it.

But in case of relation-as-scalar feature the association is happening with the last relation associated with the alias in entire script.

For example -
{code}
 l = load 'x' as (a,b);
 A = load 'x' as (a,b); 
 B = foreach A generate a, l.a as la;
 l = foreach l generate a+1 as a;
store B into 'b';
{code}

The alias l in relation with alias B should refer to the load, but it refers to the foreach statement -
{code}

#--------------------------------------------------
# Map Reduce Plan
#--------------------------------------------------
MapReduce node scope-16
Map Plan
l: Store(file:/tmp/temp-953430379/tmp2006282146:org.apache.pig.impl.io.InterStorage) - scope-8
|
|---l: New For Each(false)[bag] - scope-7
    |   |
    |   Add[int] - scope-5
    |   |
    |   |---Cast[int] - scope-3
    |   |   |  
    |   |   |---Project[bytearray][0] - scope-2
    |   |
    |   |---Constant(1) - scope-4
    |
    |---l: Load(file:///Users/tejas/pig_type/trunk/x:org.apache.pig.builtin.PigStorage) - scope-1--------
Global sort: false
----------------

MapReduce node scope-17
Map Plan
B: Store(file:///Users/tejas/pig_type/trunk/b:org.apache.pig.builtin.PigStorage) - scope-15
|
|---B: New For Each(false,false)[bag] - scope-14
    |   |
    |   Project[bytearray][0] - scope-9
    |   |
    |   POUserFunc(org.apache.pig.impl.builtin.ReadScalars)[int] - scope-13
    |   |
    |   |---Constant(0) - scope-11
    |   |
    |   |---Constant(file:/tmp/temp-953430379/tmp2006282146) - scope-12
    |
    |---A: Load(file:///Users/tejas/pig_type/trunk/x:org.apache.pig.builtin.PigStorage) - scope-0--------
Global sort: false
----------------
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-04 21:54:21.654,,,no_permission,,,,,,,,,,,,66310,,,,Fri Mar 04 21:54:21 UTC 2011,,,,,,,0|i0gzgf:,97174,,,,,,,,,,04/Mar/11 21:54;rding;This is fixed with the new parser changes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Indeterministic behavior in local mode due to static variable PigMapReduce.sJobConf,PIG-1831,12497038,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,vivekp,vivekp,28/Jan/11 10:02,25/Apr/11 21:27,14/Mar/19 03:07,18/Feb/11 02:16,0.8.0,,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"The below script when run in local mode gives me a different output. It looks like in local mode I have to store a relation obtained through streaming in order to use it afterwards.

 For example consider the below script : 

DEFINE MySTREAMUDF `test.sh`;
A  = LOAD 'myinput' USING PigStorage() AS (myId:chararray, data2, data3,data4 );
B = STREAM A THROUGH MySTREAMUDF AS (wId:chararray, num:int);
--STORE B into 'output.B';
C = JOIN B by wId LEFT OUTER, A by myId;
D = FOREACH C GENERATE B::wId,B::num,data4 ;
D = STREAM D THROUGH MySTREAMUDF AS (f1:chararray,f2:int);
--STORE D into 'output.D';
E = foreach B GENERATE wId,num;
F = DISTINCT E;
G = GROUP F ALL;
H = FOREACH G GENERATE COUNT_STAR(F) as TotalCount;
I = CROSS D,H;
STORE I  into 'output.I';


test.sh
---------
#/bin/bash
cut -f1,3


And input is 
abcd    label1  11      feature1
acbd    label2  22      feature2
adbc    label3  33      feature3


Here if I store relation B and D then everytime i get the result  :
acbd            3
abcd            3
adbc            3

But if i dont store relations B and D then I get an empty output.  Here again I have observed that this behaviour is random ie sometimes like 1out of 5 runs there will be output. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,31/Jan/11 22:07;daijy;PIG-1831-0.patch;https://issues.apache.org/jira/secure/attachment/12469866/PIG-1831-0.patch,01/Feb/11 19:05;daijy;PIG-1831-1.patch;https://issues.apache.org/jira/secure/attachment/12469969/PIG-1831-1.patch,15/Feb/11 02:54;daijy;PIG-1831-2.patch;https://issues.apache.org/jira/secure/attachment/12471052/PIG-1831-2.patch,18/Feb/11 01:54;daijy;PIG-1831-3.patch;https://issues.apache.org/jira/secure/attachment/12471356/PIG-1831-3.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2011-01-31 21:47:10.762,,,no_permission,,,,,,,,,,,,165208,Reviewed,,,Fri Feb 18 02:16:58 UTC 2011,,,,,,,0|i0gzfr:,97171,,,,,,,,,,"31/Jan/11 21:47;daijy;This issue is caused by race condition in using static variable PigMapReduce.sJobConf in local mode. In local mode, all mapreduce job share a single VM. Pig keep on overwriting static variable PigMapReduce.sJobConf each time we launch a new mapreduce job. When multiple mapreduce jobs launching simultaneously, one mapreduce job may use config for other mapreduce job, and cause indeterministic behavior. Options to fix this issue are:
1. force local mode run mapreduce job sequentially, if there is a way
2. Make sJobConf an array keyed by mapreduce jobid. However, some UDFs is using sJobConf, we could break backward compatibility","31/Jan/11 22:07;daijy;PIG-1831-0.patch illustrates how to fix the specific case in the Jira. However, a complete patch should address for all usage of PigMapReduce.sJobConf.","01/Feb/11 19:05;daijy;Richard suggest a better fix using ThreadLocal variable instead of static variable. Still keep static sJobConf for backward compatibility though it is already marked as deprecate in 0.7. In theory, if UDF still use deprecated sJobConf, they might see the same issue. But the chance of it should be very low.",02/Feb/11 23:05;rding;+1,15/Feb/11 02:54;daijy;PIG-1831-2.patch fix unit test failures.,15/Feb/11 02:55;daijy;Review notes: https://reviews.apache.org/r/376/,18/Feb/11 01:54;daijy;PIG-1831-3.patch fix findbug warnings.,18/Feb/11 02:16;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Type mismatch error in key from map, when doing GROUP on PigStorageSchema() variable",PIG-1830,12497017,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,miteshsjat,miteshsjat,28/Jan/11 06:18,25/Apr/11 21:27,14/Mar/19 03:07,18/Mar/11 08:02,,,,,,,,0.8.1,,,,,,,2,,,,,,,,,,,,,"Pig fails when we try to GROUP data loaded via PigStorageSchema.

{code}
Events = LOAD 'input/PigStorageSchema' USING org.apache.pig.piggybank.storage.PigStorageSchema();

Sessions = GROUP Events BY name;

DUMP Sessions;
{code}

Schema file '''input/PigStorageSchema/.pig_schema'''
{code}
{""fields"":[{""name"":""name"",""type"":55,""schema"":null,""description"":""autogenerated from Pig Field Schema""},{""name"":""val"",""type"":10,""schema"":null,""description"":""autogenerated from Pig Field Schema""}],""version"":0,""sortKeys"":[],""sortKeyOrders"":[]}
{code}

Header file '''input/PigStorageSchema/.pig_header'''
{code}
name    val
{code}

Sample input file '''input/PigStorageSchema/pss.in'''
{code}
peter   1
samir   3
michael 4
peter   2
peter   4
samir   1
{code}

On running the above pig script, the following error is received.

{code}
2010-12-15 08:07:58,367 WARN org.apache.hadoop.mapred.Child: Error running child
java.io.IOException: Type mismatch in key from map: expected org.apache.pig.impl.io.NullableText, recieved
org.apache.pig.impl.io.NullableBytesWritable
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:898)
        at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:600)
        at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:116)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:238)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:231)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:674)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:335)
        at org.apache.hadoop.mapred.Child$4.run(Child.java:242)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1062)
        at org.apache.hadoop.mapred.Child.main(Child.java:236)
{code}

On changing ""type"" of ""name"" from 55(chararray) to 50(bytearray), the
GROUP-BY worked.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Mar/11 21:05;alangates;1830-testpatch.tgz;https://issues.apache.org/jira/secure/attachment/12473615/1830-testpatch.tgz,07/Mar/11 03:56;dvryaboy;PIG_1830.patch;https://issues.apache.org/jira/secure/attachment/12472801/PIG_1830.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-01-28 17:03:35.031,,,no_permission,,,,,,,,,,,,41702,,,,Tue Apr 05 16:43:16 UTC 2011,,,,,,,0|i0gzfj:,97170,,,,,,,,,,"28/Jan/11 17:03;ciemo;I certainly hope the solution is not to require users to cast chararray to bytearray.

Also, why are the .pig_schema type values numeric ids and not name strings (e.g. chararray, int).","28/Jan/11 19:25;cegner;The mapping from numeric values to data types can be found in [DataType.java|http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/data/DataType.java?view=markup]

Excerpt:
{code}
    public static final byte UNKNOWN   =   0;
    public static final byte NULL      =   1;
    public static final byte BOOLEAN   =   5; // internal use only
    public static final byte BYTE      =   6; // internal use only
    public static final byte INTEGER   =  10;
    public static final byte LONG      =  15;
    public static final byte FLOAT     =  20;
    public static final byte DOUBLE    =  25;
    public static final byte BYTEARRAY =  50;
    public static final byte CHARARRAY =  55;
    /**
     * Internal use only.
     */
    public static final byte BIGCHARARRAY =  60; //internal use only; for storing/loading chararray bigger than 64K characters in BinStorage
    public static final byte MAP       = 100;
    public static final byte TUPLE     = 110;
    public static final byte BAG       = 120;
{code}
","28/Jan/11 20:11;olgan;The problem is with PigStorageSchema implementation. The class extends PigStorage without overwriting getNext.
So, while the schema tells Pig that the data is coming as chararray, the data is actually created (by PigStorage)
as bytearray.

The owner of the PigStorageSchema function needs to make sure that the data and schema types match. ","28/Jan/11 21:13;dvryaboy;That'd be me. I'll fix. Thanks for pointing the way, Olga.

The reason it's numbers instead of strings is that the schema is automatically (de)serialized and that's what the values are in a schema.. I would've preferred an enum but that's not what it is in DataType.java

","07/Mar/11 03:56;dvryaboy;Version of the patch attached.

Note that I had to change stringify() in ResourceSchema, since it produced a version of the schema that was not paresable by the Pig parser. 

Also note that somehow this breaks AllLoader, still looking at why that happens.",07/Mar/11 16:29;olgan;I think it would be good to add a test that verifies the use case that was failing with the prior version,"07/Mar/11 17:16;dvryaboy;Olga, it's there:


+    /**
+     * See PIG-1830
+     * @throws IOException
+     */
+    @Test
+    public void testByteArrayConversion() throws IOException {
....

","07/Mar/11 17:29;olgan;sorry, Dmitriy, I missed it","13/Mar/11 01:57;dvryaboy;Turns out the AllLoader failures were unrelated, after all. Fixed in a different ticket. 
This is ready for review.","14/Mar/11 18:57;alangates;+1 to everything but the build.xml changes.  Not that I disapprove of those.  But I'm unsure what affect they'll have.  They seem to mess with debug and warnings levels of javac, which may cause confusion in our test_patch program.  

Also, why are there changes to hbase libraries in the patch?","14/Mar/11 19:45;dvryaboy;The hbase jar changes are uncommitted leftovers.. I believe those jars are gone already.

The changes to build.xml are me trying to get line numbers to show up in stack traces. With build.xml as it was, line numbers were not included; now they are. It's the same settings -- I think -- as what the main build.xml uses for ant test. But to be honest, with ivy I just copy and paste until things work the way I want them to, so if someone who knows this stuff can take a look and fix any deficiencies that would be cool.

D","14/Mar/11 21:03;alangates;     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     -1 javac.  The applied patch generated 870 javac compiler warnings (more than the trunk's current 869 warnings).
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     -1 release audit.  The applied patch generated 540 release audit warnings (more than the trunk's current 538 warnings).
     [exec]
     [exec]

I'll attach the patch and trunk versions of release audit.",14/Mar/11 21:05;alangates;release audit output from test-patch,"15/Mar/11 09:15;dvryaboy;Alan I am not sure what the release audit warnings are.. 

Tried finding the new compiler warning, but turning on all warnings for both pig and piggybank didn't give me anything in the files I touched, mostly the generated code for Pig and some of the other loaders in the piggybank. ",15/Mar/11 15:14;alangates;If the release audit warnings are all in generated code then don't worry about them.,16/Mar/11 19:41;alangates;Unit tests pass.  All looks good.,18/Mar/11 08:02;dvryaboy;Committed to trunk and 0.8 branch.,"05/Apr/11 16:43;thejas;Adding 0.8 to 'fix versions' as the patch has been committed to 0.8 as well.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBaseStorage has problems with processing multiregion tables,PIG-1828,12496912,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,mr_luk,mr_luk,27/Jan/11 09:29,05/Mar/11 23:45,14/Mar/19 03:07,05/Mar/11 23:45,0.8.0,,,,,,,,,,,,,,0,,,,,,,,,,,,,"As brought up in the pig user mailing list (http://www.mail-archive.com/user%40pig.apache.org/msg00606.html) Pig does sometime not scan the full HBase table.
It seems that HBaseStorage has problems scanning large tables. It issues just one mapper job instead of one mapper job per table region.
Ian Stevens, who brought this issue up in the mailing list, attached a script to reproduce the problem (https://gist.github.com/766929).
However, in my case, the problem only occurred, after the table was split into more than one regions.

","Hadoop 0.20.2, Hbase 0.20.6, Distributed mode",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-01-28 08:08:14.799,,,no_permission,,,,,,,,,,,,41703,,,,Sat Mar 05 23:45:37 UTC 2011,,,,,,,0|i0gzf3:,97168,,,,,,,"pig, hbase, hbasestorage",,,"28/Jan/11 08:08;dvryaboy;Found the issue!

Turns out HBaseStorage is doing the right thing and returning the correct set of splits; but PIG-1518 is merging the splits back into a single split! No wonder I wasn't seeing it, i was running with combinations turned off.

Short term fix: set pig.splitCombination to false.

Long term fix: I added OrderedLoadFunc implementation to the loader, so that PIG-1518 doesn't apply. I think this is correct, since TableSplits are in fact comparable, but I am not sure what exact consequences implementing this interface will have with regards to merge joins and such.  Ashutosh, can you comment?

For the folks using the EB version -- you are not affected, since this is only a 0.8 problem.","28/Jan/11 10:22;ashutoshc;I don't think we have sufficient evidence yet to point finger at split combination for this bug. Theoretically, combination of multiple TableSplits into one Split within Pig should not result in any problem, if you honor the semantics of InputFormat imposed by MR framework, which is each split is stateless in a sense it doesn't maintain any state. One TableSplit should know nothing about another one. I don't know enough about TableSplit, but I would assume they are indeed stateless. 

OrderedLoadFunc tries to impose this restriction by defining an order on Splits. It dictates that all keys in one split are smaller then another one. Thus, ideally Pig should *not* combine the loaders implementing it. But for reasons discussed in PIG-1518 it was eventually decided that for feature to be useful, Pig wouldn't  combine OrderedLoadFunc loaders *only* if loader is also used for MergeJoin or map-side cogroups in scripts. So, adding OLF won't turn off the combination in all cases. If you suspect combination is causing a bug (potentially because TableSplits are stateful w.r.t each other) then only setting the flag to false will ensure no-combination. But, I doubt that TableSplits have state and the split combination is causing the bug. Ian, Lukas can you confirm if setting pig.splitCombination to false results in bug going away?  
","28/Jan/11 10:27;ashutoshc;Oh, I just saw that you stated TableSplits are comparable. Can you explain a bit how are 2 TableSplits  compared? Do they define any property on keys? If TableSplit can faithfully implement OLF, then split combination may not be safe. Fix then is to stop combining  straight away when a loader implements OLF and not check further whether loader is used for merge join later on or not. ","28/Jan/11 11:30;mr_luk;Hi there,
I set pig.splitCombination to false in the pig.properties and now the table is fully processed/the bug went away. Pig issued one map job for each region.","28/Jan/11 17:39;ashutoshc;Thanks Lukas for checking. This indicates that TableSplits are rather not combinable. Thinking more about it, I think basic Pig's assumption that splits can be combined in general and only for special cases we won't combine (which Pig checks itself) is not correct. Question of combination should really be asked from Loader and not assumed. Also, this OLF thing is too complicated.  Condition imposed by OLF is one possibility, but I assume there exists other scenarios where loader is not OLF but is still not combinable. I would propose to add a new method in LoadFunc and ask directly from loader and drop all the logic of determining whether splits are combinable or not.
{java}
// By default, splits generated by a loader is considered combinable to preserve current behavior
public boolean isCombinable() {
return true;
}
{java}

Good thing is LoadFunc is abstract class, so this won't break backward compatibility.

@Dmitiry,
As I pointed above adding OLF to HBaseStorage will not help. Though it won't hurt either. A quick fix for HBaseStorage loader for now is to set the key to false, somewhere early. I think setLocation() or setSchema() is one of the first methods called on LoadFunc and since checks for determining combination happen much later,  loader setting that key to false will be seen and combination won't happen. That will avoid the need of telling the users of HbaseStorage to set the key themselves. 
","28/Jan/11 17:40;dvryaboy;Ashutosh,
HBase stores records ordered by their keys, and splits the keyspace into regions as needed (unlike something like Cassandra, which by default uses hash partitioning and can be *made* to use total order partitions, total order is the *only* thing HBase does).

Indeed, implementing OLF didn't solve my problem as the splits were still combined. I don't know if TableSplits are stateful.","28/Jan/11 18:03;dvryaboy;Ashutosh,
Wouldn't setting the key affect all loaders, not just HBase?
Are you saying that if I add something like 
<code>
job.getConfiguration().setBoolean(""pig.splitCombination"", false);
<code>
at the top of setLocation, the setting will be honored? I thought there was some copying going on under the covers that made this not work.. but I am very hazy on what exactly happens during initialization, you have been far deeper in there.","28/Jan/11 19:05;ashutoshc;Yes, setting it the way you had here. But not in Pig code but in loader. That way that change is only in HBaseStorage not in Pig and Pig's default behavior is modified.  All the loader methods are passed a job object. So, just set the key in that job object. Trick is in which of loader's method. Job confs in few of those methods are read-only. I need to check in which of loader's method it is appropriate to do so. ","14/Feb/11 11:41;dvryaboy;hunted about for that this weekend, and it looks like Job is only passed in setLocation, at which point it's too late -- the modified job conf won't be the one used for the actual job! (see http://www.apacheserver.net/UDFContext-in-0-8-LoadFunc-at1098923.htm).

The suggestion in the linked thread was to use relativeToAbsolutePath to stick things into UDFContext, which worked there since the task was just to pass something along to the job-side loadfuncs, but does not work in this case, where we actually want to fix up the Conf.

Various LoadMetadata functions also get called with the Job param, I'll try that next.
I am not sure where those are getting called in regards to job creation, but regardless, it's silly to make people implement LoadMetadata just to be able to muck with the job config. We can add something like this to JobControlCompiler.getJob:


                    // Call setLocation as a hacky way of letting a LoadFunc fix up the Job.
                    lf.setLocation(ld.getLFile().getFileName(), nwJob);

(inside its POLoad loop). 

Thoughts?","14/Feb/11 11:42;dvryaboy;Sorry, for completeness, I meant:


// Call setLocation as a hacky way of letting a LoadFunc fix up the Job.
LoadFunc lf = ld.getLoadFunc();
lf.setLocation(ld.getLFile().getFileName(), nwJob);","15/Feb/11 09:00;dvryaboy;My patch to PIG-1680 addresses this, please review there.",05/Mar/11 23:45;dvryaboy;PIG-1680 has been committed to both trunk and the 0.8 branch. Closing this.,05/Mar/11 23:45;dvryaboy;Fixed as part of PIG-1680.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When passing a parameter to Pig, if the value contains $ it has to be escaped for no apparent reason",PIG-1827,12496871,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,julienledem,julienledem,26/Jan/11 21:53,04/Aug/11 00:34,14/Mar/19 03:07,13/May/11 17:24,0.8.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,,,,,,,,,,,,PIG-1819,,,,,,,,,,,,,,,,,,,,,21/Apr/11 23:50;rding;PIG-1827-1.patch;https://issues.apache.org/jira/secure/attachment/12477057/PIG-1827-1.patch,27/Apr/11 00:26;rding;PIG-1827_2.patch;https://issues.apache.org/jira/secure/attachment/12477461/PIG-1827_2.patch,10/May/11 00:33;rding;PIG-1827_3.patch;https://issues.apache.org/jira/secure/attachment/12478659/PIG-1827_3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-03-29 21:34:16.065,,,no_permission,,,,,,,,,,,,66195,Reviewed,,,Fri May 13 20:58:46 UTC 2011,,,,,,,0|i0gzev:,97167,,,,,,,,,,"29/Mar/11 21:34;rding;The reason that one has to escape $ in this case is that Pig, by design, allows recursive parameter substitution. That is, a parameter value can contain another parameter key (prefixed by $). So if the value contains literal $, it must be escaped. ","21/Apr/11 23:37;rding;Looked into it a little more. Actually, embedded Pig just use parameter substitution to perform parameter binding and it doesn't support recursive substitution. So it should be ok to remove the requirement that $ in parameter be escaped.","22/Apr/11 21:04;rding;Test-patch results:

{code}
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{code}",25/Apr/11 16:41;rding;Unit tests pass.,"25/Apr/11 21:35;julienledem;Hi Richard,
thanks for taking care of this.

* in test/org/apache/pig/test/TestScriptLanguage.java
the test should verify that the value are properly passed to the script
{code}
""testvar = 'abcd$py'"",
""testvar2 = '$'"",
""testvar3 = '\\\\\\\\$'"",
""testvar4 = 'abcd\\$py$'"",
""testvar5 = 'abcd\\$py'"",
{code}
what about this?
{code}
""P = Pig.compile(\""\""\""a = load '$input'; b = foreach a generate $0,$1,$testvar,$testvar2,$testvar3,$testvar4,$testvar5; store b into '$output';\""\""\"")"", 
{code}
then you can check that you get the values in the output.

we should check that the transformation is bijective.
","27/Apr/11 00:26;rding;The new patch added a test case that supports $ as part of parameter:

{code}
separator = '$'
P = Pig.compile(\""\""\""a = load 'input' using PigStorage('$separator');store a into 'output';\""\""\"")
Q = P.bind()
{code}

On the other hand, Pig Latin doesn't support '\$' in its variables, string literals. ","09/May/11 18:13;julienledem;right.
Please add a unit test to verify that fixNonEscapedDollarSign returns what we expect.
","09/May/11 18:34;julienledem;After discussing with Richard and looking into the code of PreprocessorContext
http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/tools/parameters/PreprocessorContext.java?view=markup
There seems to be a bug here:
{code}
235 	//String litVal = Matcher.quoteReplacement(val);
236 	replaced_line = replaced_line.replaceFirst(""\\$""+key, val); 
{code}
the replacement (2nd) parameter of replaceFirst is not a plain string, it can contain references to the matched pattern like ""$0"" so $ in val must be escaped.
Does someone know why line 235 is commented out ?
",10/May/11 00:33;rding;We should limit this jira to fix the issue in embedded Pig (i.e. workaround the general parameter substitution) and visit parameter substitution parser and related code in a separate jira.,10/May/11 00:34;rding;New patch added a unit test case as suggested.,13/May/11 07:35;daijy;+1,13/May/11 17:24;rding;Patch committed to trunk and 0.9 branch.,13/May/11 20:58;julienledem;I created PIG-2074 to discuss the PreprocessorContext,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unexpected data type -1 found in stream error,PIG-1826,12496867,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,jcoveney,jcoveney,26/Jan/11 21:19,04/Aug/11 00:34,14/Mar/19 03:07,29/Apr/11 00:40,0.8.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"When running the attached udf I get the title error. By inserting printlns extensively, the script is functioning properly and returning a DataBag, but for whatever reason, pig does not detect it as such.",This is pig 0.8.0 on a linux box,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Apr/11 00:40;daijy;PIG-1826-1.patch;https://issues.apache.org/jira/secure/attachment/12477347/PIG-1826-1.patch,14/Feb/11 17:54;charles.fg;PIG-1826.tar.gz;https://issues.apache.org/jira/secure/attachment/12471004/PIG-1826.tar.gz,26/Jan/11 21:20;jcoveney;numgraph.java;https://issues.apache.org/jira/secure/attachment/12469477/numgraph.java,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-02-01 02:39:21.007,,,no_permission,,,,,,,,,,,,66221,Reviewed,,,Fri Apr 29 00:40:23 UTC 2011,,,,,,,0|i0gzen:,97166,,,,,,,,,,"26/Jan/11 21:20;jcoveney;This is the UDF I made that it fails on

The form of the script is

register /path/to/myudf.jar;
A = LOAD 'test.txt' as (a:chararray, b:chararray);
B = GROUP A BY a;
C = FOREACH B GENERATE A.b;
D = GROUP C ALL;
E = FOREACH D GENERATE myudf.fun.udf(C.b);",01/Feb/11 02:39;daijy;What is MutableInt in UDF? What is your input file?,"02/Feb/11 20:50;jcoveney;MutableInt is defined as such:


package squeal.com;

public class MutableInt {
        int value;

        public MutableInt() { value = 1; }
        public MutableInt(int val) { value = val; }
        public void inc() { ++value; }
        public void inc(int val) { value += val; }
        public int get() { return value; }
        public String toString() { return (new Integer(value)).toString(); }
}

Just a dumb wrapper class to avoid having to do another put after getting from a Map.

it is failing on a trivial example:


1       a
1       b
1       c
2       a
2       b
2       c
3       a
3       b
3       c","02/Feb/11 20:51;jcoveney;(note that it should be 1 tab a, 1 tab b, etc...I hit enter hastily)","14/Feb/11 17:53;charles.fg;Hi Guys, 

Adding other sample, in fact is the code, the script and the  input data.
If any help is needed from my part count on me ok!?

Thanks!",14/Feb/11 17:54;charles.fg;The files to be tested to see to help with the issue!,19/Mar/11 08:15;daijy;The problem is because Pig don't understand MutableInt. All data feeding by UDF should be Pig compatible data types. ,"21/Mar/11 17:41;daijy;The error message can be improved. We can be more specific to say something like ""MutableInt is not supported. Only Pig data type is allowed"".",26/Apr/11 00:40;daijy;PIG-1826-1.patch fix the error message. It also piggyback a change in number of retries in hadoop. I decrease this number from 4 to 1 to accelerate the unit tests.,"27/Apr/11 21:29;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/670/
-----------------------------------------------------------

Review request for pig and thejas.


Summary
-------

See PIG-1826


This addresses bug PIG-1826.
    https://issues.apache.org/jira/browse/PIG-1826


Diffs
-----

  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/data/BinInterSedes.java 1096629 
  http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/MiniCluster.java 1096629 
  http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/TestEvalPipeline2.java 1096629 

Diff: https://reviews.apache.org/r/670/diff


Testing
-------

Test-patch:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Unit-test:
    all pass.


Thanks,

Daniel

","28/Apr/11 00:07;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/670/#review594
-----------------------------------------------------------

Ship it!


+1

- thejas


On 2011-04-27 21:28:47, Daniel Dai wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/670/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-04-27 21:28:47)
bq.  
bq.  
bq.  Review request for pig and thejas.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  See PIG-1826
bq.  
bq.  
bq.  This addresses bug PIG-1826.
bq.      https://issues.apache.org/jira/browse/PIG-1826
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/data/BinInterSedes.java 1096629 
bq.    http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/MiniCluster.java 1096629 
bq.    http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/TestEvalPipeline2.java 1096629 
bq.  
bq.  Diff: https://reviews.apache.org/r/670/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  Test-patch:
bq.       [exec] +1 overall.  
bq.       [exec] 
bq.       [exec]     +1 @author.  The patch does not contain any @author tags.
bq.       [exec] 
bq.       [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
bq.       [exec] 
bq.       [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
bq.       [exec] 
bq.       [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
bq.       [exec] 
bq.       [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
bq.       [exec] 
bq.       [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
bq.  
bq.  Unit-test:
bq.      all pass.
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Daniel
bq.  
bq.

",29/Apr/11 00:40;daijy;Patch committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDFContext.getUDFProperties does not handle collisions in hashcode of udf classname (+ arg hashcodes),PIG-1821,12496721,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,25/Jan/11 20:32,04/Aug/11 00:34,14/Mar/19 03:07,08/May/11 17:36,0.8.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"In code below, if generateKey() returns same value for two udfs, the udfs would end up sharing the properties object. 

{code}
private HashMap<Integer, Properties> udfConfs = new HashMap<Integer, Properties>();

    public Properties getUDFProperties(Class c) {
        Integer k = generateKey(c);
        Properties p = udfConfs.get(k);
        if (p == null) {
            p = new Properties();
            udfConfs.put(k, p);
        }
        return p;
    }

    private int generateKey(Class c) {
        return c.getName().hashCode();
    }

    public Properties getUDFProperties(Class c, String[] args) {
        Integer k = generateKey(c, args);
        Properties p = udfConfs.get(k);
        if (p == null) {
            p = new Properties();
            udfConfs.put(k, p);
        }
        return p;
    }

    private int generateKey(Class c, String[] args) {
        int hc = c.getName().hashCode();
        for (int i = 0; i < args.length; i++) {
            hc <<= 1;
            hc ^= args[i].hashCode();
        }
        return hc;
    }

{code}


To prevent this, a new class (say X) that can hold the classname and args should be created, and instead of HashMap<Integer, Properties>,  HashMap<X, Properties> should be used. Then HahsMap will deal with the collisions. 


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/May/11 14:36;thejas;PIG-1821.1.patch;https://issues.apache.org/jira/secure/attachment/12477958/PIG-1821.1.patch,03/May/11 21:10;thejas;PIG-1821.2.patch;https://issues.apache.org/jira/secure/attachment/12478095/PIG-1821.2.patch,07/May/11 00:30;thejas;PIG-1821.3.patch;https://issues.apache.org/jira/secure/attachment/12478484/PIG-1821.3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-01-25 21:01:16.614,,,no_permission,,,,,,,,,,,,66338,,,,Sun May 08 17:36:53 UTC 2011,,,,,,,0|i0gzdj:,97161,,,,,,,,,,"25/Jan/11 21:01;sms;Since Pig does not allow function name overloading, can we use fully qualified class names as the key, i..e., HashMap <String, Properties> ?","25/Jan/11 21:51;thejas;bq. Since Pig does not allow function name overloading, can we use fully qualified class names as the key, i..e., HashMap <String, Properties> ? 

Yes, but getUDFProperties(Class c, String[] args) also needs to be supported. So List<String> is more appropriate. ","25/Jan/11 21:53;thejas;I meant to say that List<String> can be used as key, ie  HashMap <List<String>, Properties>","02/May/11 14:39;thejas;PIG-1821.1.patch - unit tests passed (except TestStoreInstances, which is failing in trunk). test-patch failed because of no new unit tests. There are no new unit tests because it is not easy to create a test case to produce the problem this could have caused.
","03/May/11 21:10;thejas;PIG-1821.2.patch - Changes to incorporate comments from Richard. Passes unit tests and test-patch.
",03/May/11 21:27;rding;+1,"05/May/11 16:12;thejas;Patch committed to 0.9 branch and trunk.
","06/May/11 23:14;woody.anderson@gmail.com;this checkin has caused a classloader failure when using my loader UDF.

i've narrowed it the the checking that references this bug:

r1099860 | thejas | 2011-05-05 09:10:26 -0700 (Thu, 05 May 2011) | 3 lines
PIG-1821: UDFContext.getUDFProperties does not handle collisions
  in hashcode of udf classname (+ arg hashcodes) (thejas)

I rebuilt my loader against the new pig jar, still fails.

here's the output of my code (it works if i run using pig built with the previous revision):
Backend error message during job submission
-------------------------------------------
java.io.IOException: Deserialization error: com.yahoo.ymail.pigfunctions.AsStorage
        at org.apache.pig.impl.util.ObjectSerializer.deserialize(ObjectSerializer.java:55)
        at org.apache.pig.impl.util.UDFContext.deserialize(UDFContext.java:183)
        at org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil.setupUDFContext(MapRedUtil.java:155)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.setupUdfEnvAndStores(PigOutputFormat.java:228)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.checkOutputSpecs(PigOutputFormat.java:185)
        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:770)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:730)
        at org.apache.hadoop.mapred.jobcontrol.Job.submit(Job.java:378)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs(JobControl.java:247)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.run(JobControl.java:279)
        at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.ClassNotFoundException: com.yahoo.ymail.pigfunctions.AsStorage
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:247)
        at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:603)
        at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1574)
        at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1495)
        at java.io.ObjectInputStream.readClass(ObjectInputStream.java:1461)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1311)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:350)
        at java.util.HashMap.readObject(HashMap.java:1029)
        at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:974)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1848)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:350)
        at org.apache.pig.impl.util.ObjectSerializer.deserialize(ObjectSerializer.java:53)
        ... 10 more
",06/May/11 23:25;thejas;Reopening to address the issue Woody reported.,"07/May/11 00:30;thejas;PIG-1821.3.patch - fixes problem reported by Woody. Replaced Class in UDFContext key with class name (String).
",07/May/11 01:12;daijy;+1,"08/May/11 17:36;thejas;PIG-1821.3.patch - tests passed, patch committed to trunk and 0.9 branch.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New logical plan: FilterLogicExpressionSimplifier fail to deal with UDF,PIG-1820,12496616,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,24/Jan/11 23:31,25/Apr/11 21:27,14/Mar/19 03:07,18/Feb/11 01:32,0.8.0,,,,,,,0.8.1,,,,impl,,,0,,,,,,,,,,,,,"The following script fail:
{code}
a = load '1.txt' as (a0, a1);
b = filter a by (a0 is not null or a1 is not null) and IsEmpty(a0);
explain b;
{code}

Error message:
Caused by: java.lang.ClassCastException: org.apache.pig.newplan.logical.expression.UserFuncExpression cannot be cast to org.apache.pig.newplan.logical.expression.BinaryExpression
        at org.apache.pig.newplan.logical.rules.LogicalExpressionSimplifier$LogicalExpressionSimplifierTransformer.handleBinary(LogicalExpressionSimplifier.java:561)
        at org.apache.pig.newplan.logical.rules.LogicalExpressionSimplifier$LogicalExpressionSimplifierTransformer.handleAnd(LogicalExpressionSimplifier.java:429)
        at org.apache.pig.newplan.logical.rules.LogicalExpressionSimplifier$LogicalExpressionSimplifierTransformer.inferRelationship(LogicalExpressionSimplifier.java:397)
        at org.apache.pig.newplan.logical.rules.LogicalExpressionSimplifier$LogicalExpressionSimplifierTransformer.handleDNFOr(LogicalExpressionSimplifier.java:281)
        at org.apache.pig.newplan.logical.rules.LogicalExpressionSimplifier$LogicalExpressionSimplifierTransformer.checkDNFLeaves(LogicalExpressionSimplifier.java:192)
        at org.apache.pig.newplan.logical.rules.LogicalExpressionSimplifier$LogicalExpressionSimplifierTransformer.transform(LogicalExpressionSimplifier.java:108)
        at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:110)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Jan/11 23:32;daijy;PIG-1820-1.patch;https://issues.apache.org/jira/secure/attachment/12469216/PIG-1820-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-02-15 01:09:02.158,,,no_permission,,,,,,,,,,,,165206,Reviewed,,,Fri Feb 18 01:32:09 UTC 2011,,,,,,,0|i0gzdb:,97160,,,,,,,,,,"26/Jan/11 17:58;daijy;Review request:
https://reviews.apache.org/r/356/",15/Feb/11 01:09;thejas;+1,18/Feb/11 01:32;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"For implicit binding, Jython embedded Pig should skip any variable/value that contains $. ",PIG-1819,12496615,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,24/Jan/11 23:30,04/Aug/11 00:34,14/Mar/19 03:07,13/May/11 17:25,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,We use the Pig parameter substitution for the bindings so variable/value that contains $ cannot be used.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Jan/11 19:52;rding;PIG-1819.patch;https://issues.apache.org/jira/secure/attachment/12469320/PIG-1819.patch,26/Jan/11 19:27;rding;PIG-1819_1.patch;https://issues.apache.org/jira/secure/attachment/12469457/PIG-1819_1.patch,27/Jan/11 00:39;rding;PIG-1819_2.patch;https://issues.apache.org/jira/secure/attachment/12469493/PIG-1819_2.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-01-25 22:48:53.126,,,no_permission,,,,,,,,,,,,66355,,,,Fri May 13 17:25:12 UTC 2011,,,,,,,0|i0gzd3:,97159,,,,,,,,,,"25/Jan/11 21:30;rding;
Result of test-patch:

{code}     
[exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{code}","25/Jan/11 22:48;julienledem;Hi Richard,
I don't understand why values that contain $ can not be used.
Could you explain this ?
Also, is there a way to know the variables expected by a script ? That would enable returning an error instead of silently skipping which can be very confusing to the user.
Julien","26/Jan/11 02:03;rding;We're using Pig's parameter substitution parser to bind variables to the parameters (which are marked by $). The parser doesn't allow $ in the variable values (resulting in a parser exception). Basically, you can't substitute a $param with a $var.","26/Jan/11 02:35;julienledem;In the following example:
{code}
from org.apache.pig.scripting import Pig
... ...
separator = ""$""
results = Pig.compile(""""""
A = LOAD 'data' USING PigStorage('$separator') AS (a,b,c);
....
"""""").bind().run()
{code}
Let's assume I want to use $ as a separator, how can we do that? It seems to be a reasonable use case and silently skipping the variable does not seem like a good idea.","26/Jan/11 18:23;rding;
In this case, you need to escape the $:

{code}
from org.apache.pig.scripting import Pig
... ...
separator = ""\\\\$""
results = Pig.compile(""""""
A = LOAD 'data' USING PigStorage('$separator') AS (a,b,c);
....
"""""").bind().run()
{code}",26/Jan/11 19:27;rding;Thanks Julien for reviewing the patch. I shouldn't skip the escaped $ in the variables. The new patch corrects the problem.,"26/Jan/11 21:57;julienledem;As discussed:
* I've created a new Jira PIG-1827 regarding the need to escape $ in value.
* The latest patch does not handle the following string correctly: {code}""\\$""{code} If PIG-1827 is resolved then we don't need to worry about it.

Julien",27/Jan/11 00:39;rding;Attaching a new patch. Thanks Julien for the suggestions. ,13/May/11 17:25;rding;This is fixed per PIG-1827.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation: streaming example uses single quotes and not backtick,PIG-1817,12496356,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,thejas,thejas,22/Jan/11 00:00,04/Aug/11 00:35,14/Mar/19 03:07,11/Feb/11 21:35,0.8.0,,,,,,,0.9.0,,,,documentation,,,0,,,,,,,,,,,,,"In http://pig.apache.org/docs/r0.8.0/piglatin_ref2.html#STREAM, the examples use single quote instead of backtick.
Mapreduce operator example is correct in its use of backtick.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-02-11 21:35:29.369,,,no_permission,,,,,,,,,,,,66158,,,,Fri Feb 11 21:35:29 UTC 2011,,,,,,,0|i0gzcn:,97157,,,,,,,,,,"22/Jan/11 00:04;thejas;In streaming example with schema, semi-colon is used instead of colon -

(f1:int, f2{color:red} ;{color} int, f3:int);
","11/Feb/11 21:35;chandec;Pig Latin Basics doc updated. 

Stream section - examples corrected. 

Patch will be submitted under Pig-1772.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig task retains used instances of PhysicalPlan,PIG-1815,12496202,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,20/Jan/11 23:53,25/Apr/11 21:27,14/Mar/19 03:07,21/Jan/11 21:10,0.8.0,,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"map tasks of a pig query ran out of memory because there were too many (thousands)  instances of combiner PhysicalPlan in memory. Each physical plan (except the last?) was linked to older one as shown in the yourkit snapshot that I am attaching.

This problem was noticed with 0.8 because of the split combination feature, that resulted in each map having larger inputs. The query also had large physical plan because of multi-query, it had 17 MR jobs merged into one during the multi-query optimization phase.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Jan/11 17:29;thejas;PIG-1815.1.patch;https://issues.apache.org/jira/secure/attachment/12468991/PIG-1815.1.patch,20/Jan/11 23:55;thejas;yourkit_combiner_hprof.jpg;https://issues.apache.org/jira/secure/attachment/12468925/yourkit_combiner_hprof.jpg,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-01-21 18:21:15.155,,,no_permission,,,,,,,,,,,,165204,,,,Fri Jan 21 21:28:03 UTC 2011,,,,,,,0|i0gzc7:,97155,,,,,,,,,,"21/Jan/11 17:29;thejas;I have tested the patch using the query that was running out of memory. Patch does not have any unit tests, as I can't think of a good/easy way to test the leak in unit test.
All unit tests except TestScriptLanguage passed. TestScriptLanguage is failing even without the patch.

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
","21/Jan/11 18:21;ashutoshc;Great find.  
I couldn't follow when and what may cause this. If its easy enough for you, can you give a small pig query snippet which may result in this and why. And also how patch fixes the issue.
","21/Jan/11 19:27;thejas; The references in the heap dump show that the POUserFunc a plan (except the oldest one?) has reference to Reducer$Context ( POUserFunc -> udf object -> ProgressableReporter -> Reducer$Context). But the Reducer$Context object has reference to PigCombiner$Combine which has reference to another (previously created?) PhysicalPlan. So any Combiner PhysicalPlan instance that has been created in the map task has a reference to it and can't be freed by GC .

I haven't followed the exact call sequence that leads to it, but it looks like a PhysicalPlan instance is created with reference to a copy of the previous Reducer$Context, and since this is a inner class of PigCombiner$Combine (a subclass of Reducer) it has a reference (this$0) to it. And this older  PigCombiner$Combine  has references to the old physical plan. The old physical plan has references to the older Reducer$Context and so on. 
To break this chain, in this patch I clean the references to the PhysicalPlan in PigCombiner$Combine when the cleanup method is called.

I had a look at the hadoop mapreduce code that does the sort-and-spill of map output (org.apache.hadoop.mapred$MapOutputBuffer$SpillThread.sortAndSpill() ), and it looks like one combiner class instance is created for every partition (ie reducer). 
In case of the query whose map tasks ran out of memory, mapred.reduce.tasks was set to 300, ie 300 instances of combiner class , and therefore 300 instances of physical plan will be created for every spill. The query in 0.8 also had several spills ( 10+) , which means that there will be more than 3000 instances of PhysicalPlan lying around.  The Physical plans in this case were also large because it was a 'multi-query' , and 17 MR jobs were merged into 1.

ie, The failure can happen in any query which uses combiner. There just needs to be large number of instances of physical plan , and number of physical plan instances = number-of-reducers * number-of-spills. If the PhysicalPlan is large, you need fewer instances of it for failure. 
","21/Jan/11 19:52;ashutoshc;Thanks for the description, Thejas. 
+1","21/Jan/11 21:10;thejas;Patch committed to 0.8 branch and trunk.
","21/Jan/11 21:28;scott_carey;{quote}
I haven't followed the exact call sequence that leads to it, but it looks like a PhysicalPlan instance is created with reference to a copy of the previous Reducer$Context, and since this is a inner class of PigCombiner$Combine (a subclass of Reducer) it has a reference (this$0) to it.
{quote}

I don't know if this is new info to anyone reading this, but there are two types of inner classes, those with the 'this' reference to the enclosing class and those without it.   Those that don't hold references to their enclosing class are those marked 'static'.    If this inner class can be made static, it would remove that reference (and make the object a little smaller).

In general, you can help the GC and memory footprint by making sure inner classes are 'static' whenever they don't need to have the implicit 'this' reference.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mapred.output.compress in SET statement does not work,PIG-1814,12496190,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,20/Jan/11 22:22,04/Aug/11 00:34,14/Mar/19 03:07,25/Apr/11 21:42,0.8.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"Setting output compression using ""SET"" in the script does not work:
SET mapred.output.compress true;
SET mapred.output.compression.codec org.apache.hadoop.io.compress.GzipCodec;

We did some trick to make individual compression setting for multistore work. Instead of the above parameter, using the following works:
SET output.compression.enabled true;
SET output.compression.codec org.apache.hadoop.io.compress.GzipCodec;

However, this is against intuition. We should use mapred.output.compress/mapred.output.compression.codec.",,,,,,,,,,,,,,,,,,,PIG-1714,,,,,,,,,,,,,21/Apr/11 20:37;daijy;PIG-1814-1.patch;https://issues.apache.org/jira/secure/attachment/12477034/PIG-1814-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-23 15:19:22.138,,,no_permission,,,,,,,,,,,,66229,Reviewed,,,Mon Apr 25 21:42:54 UTC 2011,,,,,,,0|i0gzbz:,97154,,,,,,,,,,23/Mar/11 15:19;olgan;This issue has a very reasonable workaround - delaying till 0.9,"25/Apr/11 17:11;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/661/
-----------------------------------------------------------

Review request for pig.


Summary
-------

See PIG-1814


This addresses bug PIG-1814.
    https://issues.apache.org/jira/browse/PIG-1814


Diffs
-----

  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/PigServer.java 1095577 
  http://svn.apache.org/repos/asf/pig/trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java 1095577 
  http://svn.apache.org/repos/asf/pig/trunk/test/org/apache/pig/test/TestBZip.java 1095577 

Diff: https://reviews.apache.org/r/661/diff


Testing
-------

Test-patch:
     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Unit test:
    all pass


Thanks,

Daniel

",25/Apr/11 21:35;xuefuz;+1 patch looks good.,25/Apr/11 21:42;daijy;Patch committed to both trunk and 0.9 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig 0.8 throws ERROR 1075 while trying to refer a map in the result of  eval udf.Works with 0.7,PIG-1813,12496143,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,vivekp,vivekp,20/Jan/11 16:25,25/Apr/11 21:27,14/Mar/19 03:07,27/Jan/11 07:12,0.8.0,,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"register myudf.jar;
A = load 'input' MyZippedStorage('\u0001') as ($inputSchema);
B = foreach A generate id , value  ;
C = foreach B generate id , org.myudf.ExplodeHashList( (chararray)value, '\u0002', '\u0004', '\u0003') as value;
D = FILTER C by value is not null;
E = foreach D generate id , flatten(org.myudf.GETFIRST(value)) as hop;
F = foreach E generate id , hop#'rmli' as rmli:bytearray ;
store F into 'output.bz2' using PigStorage();

The above script fails when run with Pig 0.8 but runs fine with Pig 0.7 or if pig.usenewlogicalplan=false.
The below is the exception thrown in 0.8 :

org.apache.pig.backend.executionengine.ExecException: ERROR 1075: Received a bytearray from the UDF. Cannot determine how to convert the bytearray to map.
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(POCast.java:952)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POMapLookUp.processInput(POMapLookUp.java:87)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POMapLookUp.getNext(POMapLookUp.java:98)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POMapLookUp.getNext(POMapLookUp.java:117)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:346)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:291)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:236)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:231)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:638)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:314)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:217)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1062)
	at org.apache.hadoop.mapred.Child.main(Child.java:211)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Jan/11 22:40;daijy;PIG-1813-0.patch;https://issues.apache.org/jira/secure/attachment/12468913/PIG-1813-0.patch,23/Jan/11 07:11;daijy;PIG-1813-1.patch;https://issues.apache.org/jira/secure/attachment/12469065/PIG-1813-1.patch,24/Jan/11 23:45;daijy;PIG-1813-2.patch;https://issues.apache.org/jira/secure/attachment/12469218/PIG-1813-2.patch,27/Jan/11 07:09;daijy;PIG-1813-3.patch;https://issues.apache.org/jira/secure/attachment/12469523/PIG-1813-3.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2011-01-20 22:40:04.529,,,no_permission,,,,,,,,,,,,165203,Reviewed,,,Thu Jan 27 07:12:03 UTC 2011,,,,,,,0|i0gzbr:,97153,,,,,,,,,,20/Jan/11 22:40;daijy;Attach the initial patch. Will add test case later.,"23/Jan/11 07:07;daijy;Script to reproduce the issue:
{code}
public static class BagGenerateNoSchema extends EvalFunc<DataBag> {
        @Override
        public DataBag exec(Tuple input) throws IOException {
            DataBag bg = DefaultBagFactory.getInstance().newDefaultBag();
            bg.add(input);
            return bg;
        }
}
{code}
{code}
a = load '1.txt' as (a0:map[]);
b = foreach a generate BagGenerateNoSchema(*) as b0;
c = foreach b generate flatten(IdentityColumn(b0));
d = foreach c generate $0#'key';
dump d;
{code}

Analysis:
1. BagGenerateNoSchema does not define outputSchema, the output schema should be bag{} (since BagGenerateNoSchema extends EvalFunc<DataBag>
2. LOGenerate erroneously generate null as schema for BagGenerateNoSchema. However, when translate LOUserFunc into physical operator, we only use the LOUserFunc's output schema bag{}, which is correct. So BagGenerateNoSchema alone will not cause the problem
3. IdentityColumn defines outputSchema:
        public Schema outputSchema(Schema input) {
            return input; 
        }
It will use the input schema passed by logical plan, which is null
4. When we translate IdentityColumn into physical operator, we get the wrong schema ""null"" as the expression's schema. And null will be translate into bytearray eventually
5. In POUserFunc, we will convert everything into bytearray, if the declared type for POUserFunc is bytearray
6. In statement d, we try to convert bytearray back into map; However, the lineage for $0 trace to UDF IdentityBag, rather than a loader, so Pig complain it cannot convert bytearray to map; In realty, $0 is a map, such a conversion should not happen

There are two problems:
1. LOGenerate generate wrong schema in the place of BagGenerateNoSchema
2. #1 will fix the above script, however, it still fail the following script:
{code}
a = load '1.txt' as (a0:map[]);
b = foreach a generate BagGenerateNoSchema(*) as b0;
c = foreach b generate flatten(IdentityColumn(b0));
d = foreach c generate $0#'key';
{code}
The reason is the schema for BagGenerateNoSchema is an empty bag, after flatten (in c), the schema is null. So POUserFunc for IdentityColumn will carry type bytearray anyway. Pig will convert data into bytearray in POUserFunc, then try to conver back to map in statement d, which will cause lineage error again. 

The script works in 0.7. The reason is in 0.7, we have the code in POUserFunc:
{code}
if(resultType == DataType.BYTEARRAY) {
    if(res.result != null && DataType.findType(result.result) != DataType.BYTEARRAY) {
        result.result = new DataByteArray(result.result.toString().getBytes());
    }
}
{code}
Which is apparently wrong since res is always empty. In 0.8, we change this code into:
{code}
if(resultType == DataType.BYTEARRAY) {
    if(result.result != null && DataType.findType(result.result) != DataType.BYTEARRAY) {
        result.result = new DataByteArray(result.result.toString().getBytes());
    }
}
{code}
It checks if the result type for UDF is bytearray, if it is, convert data to bytearray. However, this conversion should not happen. All udf with output schema bytearray should be treated as unknown and leave the data as is.","24/Jan/11 23:45;daijy;Resync with 0.8 branch. For trunk, a different patch needed.","25/Jan/11 02:23;daijy;Patch is ready for review:
https://reviews.apache.org/r/353/","26/Jan/11 00:57;thejas;+1
One minor comment about a comment in org/apache/pig/newplan/logical/relational/LOGenerate.java
{code}
// schema of the expression after flatten
LogicalSchema expSchema = null;
{code}

I think it will be more accurate to say something like - // schema of expression, or inner schema if expression is flattenned ",27/Jan/11 07:09;daijy;PIG-1813-2.patch is for trunk.,27/Jan/11 07:12;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problem with DID_NOT_FIND_LOAD_ONLY_MAP_PLAN,PIG-1812,12496088,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,xianyu.zhao,xianyu.zhao,20/Jan/11 02:06,25/Apr/11 21:27,14/Mar/19 03:07,31/Jan/11 22:01,0.7.0,0.8.0,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"Hi, 
I have the following input files:

pkg.txt

a       3       {(123,1.0),(236,2.0)}
a       3       {(236,1.0)}

model.txt

a       123     2       0.33
a       236     2       0.5

My script is listed below:

A = load 'pkg.txt' using PigStorage('\t') as (pkg:chararray, ts:int, cat_bag:{t:(id:chararray, wht:float)});

M = load 'model.txt' using PigStorage('\t') as (pkg:chararray, cat_id:chararray, ts:int, score:double);

B = foreach A generate ts, pkg, flatten(cat_bag.id) as (cat_id:chararray);

B = distinct B;

H1 = cogroup M by (pkg, cat_id) inner, B by (pkg, cat_id);

H2 = foreach H1 {
        I = order M by ts;
        J = order B by ts;
        generate flatten(group) as (pkg:chararray, cat_id:chararray), J.ts as tsorig, I.ts as tsmap;
}

dump H2;

When running this script, I got a warning about ""Encountered Warning DID_NOT_FIND_LOAD_ONLY_MAP_PLAN 1 time(s)"" and pig error log as below:

Pig Stack Trace

---------------

ERROR 2043: Unexpected error during execution.
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias H2
        at org.apache.pig.PigServer.openIterator(PigServer.java:764)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:612)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:303)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:165)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:141)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:90)
        at org.apache.pig.Main.run(Main.java:500)
        at org.apache.pig.Main.main(Main.java:107)
Caused by: org.apache.pig.PigException: ERROR 1002: Unable to store alias H2
        at org.apache.pig.PigServer.storeEx(PigServer.java:888)
        at org.apache.pig.PigServer.store(PigServer.java:826)
        at org.apache.pig.PigServer.openIterator(PigServer.java:738)
        ... 7 more
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 2043: Unexpected error during execution.
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:403)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1208)
        at org.apache.pig.PigServer.storeEx(PigServer.java:884)
        ... 9 more
Caused by: java.lang.ClassCastException: org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad cannot be cast to org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.SecondaryKeyOptimizer.visitMROp(SecondaryKeyOptimizer.java:352)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper.visit(MapReduceOper.java:246)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper.visit(MapReduceOper.java:41)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:69)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:71)
        at org.apache.pig.impl.plan.DepthFirstWalker.walk(DepthFirstWalker.java:52)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.compile(MapReduceLauncher.java:498)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:117)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:378)
        ... 11 more

But, when I removed the DISTINCT statement before COGROUP, i.e. ""B = distinct B;""  this script can run smoothly. I have also tried other reducer side operations like ORDER, it seems that they will also trigger above error. This is really very confusing.

","RHEL, Pig 0.8.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Jan/11 01:55;daijy;PIG-1812-1.patch;https://issues.apache.org/jira/secure/attachment/12469229/PIG-1812-1.patch,27/Jan/11 06:16;daijy;PIG-1812-2.patch;https://issues.apache.org/jira/secure/attachment/12469517/PIG-1812-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-01-26 18:59:02.987,,,no_permission,,,,,,,,,,,,165202,Reviewed,,,Mon Jan 31 22:01:10 UTC 2011,,,,,,,0|i0gzbj:,97152,,,,,,,,,,"26/Jan/11 18:59;daijy;Review request:
https://reviews.apache.org/r/357/","28/Jan/11 01:49;alangates;+1
",28/Jan/11 22:06;thejas;+1,31/Jan/11 22:01;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error message in 0.8 not much helpful as compared to 0.7,PIG-1808,12495790,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,daijy,vivekp,vivekp,17/Jan/11 14:50,25/Apr/11 21:27,14/Mar/19 03:07,15/Mar/11 02:00,0.8.0,,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"A = LOAD 'i1' ;
B = LOAD 'i2' ;
C = JOIN A by $92 left outer,B by $92  ;
D =  filter C by $100 is null;
DUMP D;

The below script fails both in 0.7 and 0.8 since A requires a valid schema to be defined. But the error message in 0.8 is not helpful.

Error message in 0.8 
-----------------------------
ERROR 2000: Error processing rule PushUpFilter. Try -t PushUpFilter
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias D
        ....
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2042: Error in new logical plan. Try -Dpig.usenewlogicalplan=false.
        ....
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2000: Error processing rule PushUpFilter. Try -t PushUpFilter
        ....
Caused by: java.lang.NullPointerException
        at org.apache.pig.newplan.logical.rules.PushUpFilter$PushUpFilterTransformer.hasAll(PushUpFilter.java:308)
        at org.apache.pig.newplan.logical.rules.PushUpFilter$PushUpFilterTransformer.check(PushUpFilter.java:141)
        at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:108)
        ... 13 more



Error message in 0.7
-----------------------------
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias D
        ....
        ....
Caused by: org.apache.pig.backend.hadoop.executionengine.physicalLayer.LogicalToPhysicalTranslatorException: 
ERROR 1109: Input (B) on which outer join is desired should have a valid schema


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Jan/11 23:59;daijy;PIG-1808-1.patch;https://issues.apache.org/jira/secure/attachment/12468704/PIG-1808-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-01-18 23:59:23.802,,,no_permission,,,,,,,,,,,,165200,Reviewed,,,Tue Mar 15 02:00:51 UTC 2011,,,,,,,0|i0gzan:,97148,,,,,,,,,,18/Jan/11 23:59;daijy;This happens when we don't have a schema for join.,23/Jan/11 03:54;olgan;+1,15/Mar/11 02:00;daijy;Patch committed to both 0.8 and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need better error message for Jython errors,PIG-1801,12495431,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,12/Jan/11 22:26,04/Aug/11 00:34,14/Mar/19 03:07,18/Jan/11 18:17,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,Current Pig exception handling does not treat Jython exceptions differently from general RuntimeExceptions. We need to put Jython exceptions in a separate class and output better error messages.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Jan/11 01:40;rding;PIG-1801.patch;https://issues.apache.org/jira/secure/attachment/12468331/PIG-1801.patch,14/Jan/11 22:38;rding;PIG-1801_1.patch;https://issues.apache.org/jira/secure/attachment/12468418/PIG-1801_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-01-12 22:44:51.247,,,no_permission,,,,,,,,,,,,66314,Reviewed,,,Tue Jan 18 18:17:34 UTC 2011,,,,,,,0|i0gz93:,97141,,,,,,,,,,"12/Jan/11 22:44;julienledem;Exception handling should be for Scripting in general (whether it is Jython, Javascript, etc)","14/Jan/11 01:40;rding;Attaching patch that captures Jython runtime exceptions. Also added is that one doesn't need to use ""-"" before a fs command when using Pig.fs(...) method.",14/Jan/11 22:38;rding;Review board comments: https://reviews.apache.org/r/320/,18/Jan/11 18:17;rding;test-patch and unit test passed. Patch committed to the trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing Signature for maven staging release,PIG-1800,12495428,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,12/Jan/11 22:19,25/Apr/11 21:27,14/Mar/19 03:07,22/Jan/11 00:21,0.8.0,,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"Maven Staging Signature Validation failed with following message:

    -Missing Signature: '/org/apache/pig/pig/0.8.0/pig-0.8.0-sources.jar.asc' does not exist for 'pig-0.8.0-sources.jar'.
    -Missing Signature: '/org/apache/pig/pig/0.8.0/pig-0.8.0-javadoc.jar.asc' does not exist for 'pig-0.8.0-javadoc.jar'.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Jan/11 00:27;rding;PIG-1800.patch;https://issues.apache.org/jira/secure/attachment/12468706/PIG-1800.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,165196,,,,Sat Jan 22 00:21:03 UTC 2011,,,,,,,0|i0gz8v:,97140,,,,,,,,,,22/Jan/11 00:21;rding;Patch committed to both 0.8 branch and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"System property mapred.output.compress, but pig-cluster-hadoop-site.xml doesn't",PIG-1791,12494906,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,07/Jan/11 02:14,25/Apr/11 21:27,14/Mar/19 03:07,11/Jan/11 19:36,0.8.0,,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"In PIG-1714, we allow user to set system property mapred.output.compress. It also works in pig.properties. However, the same entry in pig-cluster-hadoop-site.xml is ignored.",,,,,,,,,,,,,,,,,,,PIG-1714,,,,,,,,,,,,,07/Jan/11 02:21;daijy;PIG-1791-1.patch;https://issues.apache.org/jira/secure/attachment/12467695/PIG-1791-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-01-07 18:44:34.177,,,no_permission,,,,,,,,,,,,165195,Reviewed,,,Tue Jan 11 19:43:56 UTC 2011,,,,,,,0|i0gz73:,97132,,,,,,,,,,07/Jan/11 02:21;daijy;It's tricky to test it automatically. Manual test pass.,07/Jan/11 18:44;olgan;+1,"11/Jan/11 19:36;daijy;Test-patch result:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Unit-test:
    all pass.

Manual test:
    manually tested put mapred.output.compress in pig-cluster-hadoop-site.xml, works",11/Jan/11 19:43;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
relation-as-scalar error messages should indicate the field being used as scalar,PIG-1788,12494768,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,laukik,thejas,thejas,05/Jan/11 20:51,04/Aug/11 00:35,14/Mar/19 03:07,15/Apr/11 23:55,,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"Issue 1

{code}
grunt> l = load 'x' as (a,b,c);        
grunt> g = group l by a;
grunt> f = foreach g generate COUNT(g);       -- g is unintentionally being used as scalar , the user intends it to be COUNT(l) . 
2011-01-05 12:44:53,098 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Scalars can be only used with projections
{code}

Since the user did not intend to use a scalar, the scalar error messages are hard to understand. 
It will be useful to have the relation being used in scalar context in the error message. Something like - ""ERROR 1000: Error during parsing. Relation g being used in scalar context at ... .Scalars can be only used with projections""

Issue 2
The error message ""Scalars can be only used with projections"" is not easy to understand. A better error message is needed, something like ""A column needs to be projected from relation used in scalar context.""


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Apr/11 21:58;laukik;PIG-1788-3.patch;https://issues.apache.org/jira/secure/attachment/12476487/PIG-1788-3.patch,14/Apr/11 16:02;laukik;PIG-1788.patch;https://issues.apache.org/jira/secure/attachment/12476349/PIG-1788.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-13 06:33:51.834,,,no_permission,,,,,,,,,,,,66106,,,,Fri Apr 15 23:55:33 UTC 2011,,,,,,,0|i0gz6f:,97129,,,,,,,,,,"05/Jan/11 20:57;thejas;It would be better to say 'relation used as scalar' instead of 'relation used in scalar context' in above error messages.
","13/Apr/11 06:33;laukik;This issue seems to have been resolved as part of the parser overhaul in 0.9:

{noformat}
grunt> a = load 'a' as (x,y,z);
grunt> g = group a by y;
grunt> f = foreach g generate COUNT(g);
2011-04-13 06:25:15,619 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 0: <line 3, column 29> Invalid scalar projection: g
{noformat}
","13/Apr/11 15:21;thejas;The new error message still does not address 'issue 2' in description. It would be good to have an explanation  in the error message, of why the scalar projection is invalid.
","14/Apr/11 16:02;laukik;This is a trivial change in error message; no additional test cases are required.

{noformat}
     [exec]
     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec]
{noformat}

","14/Apr/11 17:06;thejas;Review of PIG-1788.patch - 
- InvalidScalarProjectionException is used for several scalar errors, and the reasons for those errors are not same as this one. So this same error message is not applicable to all cases. I think the error message should be part of the constructor of this exception so that the error message is correct.
- I think it makes sense to have a test case as well. Error messages are important, and it is very easy to add test cases that test for error messages such as this one. (for example see TestProjectRange.testRangeCoGroupNegNoSchema() ) 
","15/Apr/11 21:58;laukik;Added a new constructor that takes an additional string as a parameter to be appended to the error message. Also added a test case.
","15/Apr/11 23:55;thejas;+1
Patch committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error in logical plan generated,PIG-1787,12494679,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,anitharaju,anitharaju,05/Jan/11 05:50,25/Apr/11 21:27,14/Mar/19 03:07,11/Jan/11 19:43,0.8.0,,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"Here is a sample pig script:

set default_parallel 2
ALLDATA = load 'sample.txt' using PigStorage() as (id, spaceid, type, pcid);
C1 = filter ALLDATA by (type == 'p' and
                   (spaceid == '1196250013'
                    or spaceid == '1196250024'
                    or spaceid == '1196250011'));
C2 = group C1 by pcid;
C3 = foreach C2 generate flatten(group) as (pc_id), COUNT(C1) as tot;
C4 = order C3 by tot desc;
C5 = limit C4 3;
C6 = join C5 by pc_id, C1 by pcid;
dump C6;


sample.txt:
1       1196250013      p       1234
2       1196250024      p       2314
3       1196250011      t       1111
4       1111111111      p       1231
5       1196250013      p       1254
6       1196250024      p       9007


This fails with the error 
java.io.IOException: Type mismatch in key from map: expected org.apache.pig.impl.io.NullableLongWritable, recieved
org.apache.pig.impl.io.NullableBytesWritable
when both pc_id and pcid are of type bytearray.

The script seems to work when 
	a) replicated join is substituted in the place of the regular join 
	b) pcid is cast to long in the loader 
	c) doing a dump of any statement before C6
	d) setting default_parallel to 1 or removing it.
	
One possible cause seems to be with the logical plan generation during the projection operation in C4 as can be observed from the describe statement. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Jan/11 22:53;daijy;PIG-1787-1.patch;https://issues.apache.org/jira/secure/attachment/12467682/PIG-1787-1.patch,10/Jan/11 18:13;daijy;PIG-1787-2.patch;https://issues.apache.org/jira/secure/attachment/12467904/PIG-1787-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-01-06 22:56:46.938,,,no_permission,,,,,,,,,,,,165194,Reviewed,,,Tue Jan 11 19:43:16 UTC 2011,,,,,,,0|i0gz67:,97128,,,,,,,,,,"06/Jan/11 22:56;daijy;Simplified test case:
{code}
a = load '1.txt' as (a0, a1);
b = group a by a0;
c = foreach b generate group as c0, COUNT(a) as c1;
d = order c by c1 parallel 2;
e = limit d 10;
f = join e by c0, a by a0;
dump f;
{code}
1.txt:
1       1
1       2

Error message:
Caused by: java.lang.ClassCastException: org.apache.pig.data.DataByteArray cannot be cast to java.lang.Long
        at org.apache.pig.backend.hadoop.HDataType.getWritableComparableTypes(HDataType.java:84)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:113)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:262)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:255)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:58)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)",10/Jan/11 18:13;daijy;PIG-1787-2.patch fix unit test failures.,10/Jan/11 18:58;daijy;Review request: https://reviews.apache.org/r/265/,"10/Jan/11 23:42;daijy;Note the test case only works in mapreduce mode. In local mode, parallel 2 is not grantted. ",11/Jan/11 01:25;rding;+1.,11/Jan/11 19:43;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New logical plan: uid conflict in flattened fields,PIG-1785,12494654,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,04/Jan/11 23:20,25/Apr/11 21:27,14/Mar/19 03:07,12/Jan/11 02:30,0.8.0,,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"The following script produce wrong result:
{code}
a = load '1.txt' as (a0:bag{t:tuple(i0:int, i1:int)});
b = foreach a generate flatten(a0) as (b0, b1), flatten(a0) as (b2, b3);
c = filter b by b0>b2;
dump c;
{code}

1.txt:
{(1,2),(2,3)}

Expected result:
(2,3,1,2)

We get nothing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Jan/11 02:14;daijy;PIG-1785-1.patch;https://issues.apache.org/jira/secure/attachment/12467505/PIG-1785-1.patch,06/Jan/11 00:16;daijy;PIG-1785-2.patch;https://issues.apache.org/jira/secure/attachment/12467599/PIG-1785-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-01-12 02:08:41.231,,,no_permission,,,,,,,,,,,,165193,Reviewed,,,Wed Jan 12 02:30:59 UTC 2011,,,,,,,0|i0gz5r:,97126,,,,,,,,,,06/Jan/11 00:16;daijy;PIG-1785-2.patch fix unit test failures.,10/Jan/11 19:48;daijy;Review request: https://reviews.apache.org/r/269/,"12/Jan/11 02:08;xuefuz;+1
Please commit when test passes.",12/Jan/11 02:30;daijy;All tests pass as in review notes. Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ERROR 1081: Cannot cast to map. Expected bytearray but received: bag,PIG-1784,12494587,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,vivekp,vivekp,04/Jan/11 12:41,25/Apr/11 21:27,14/Mar/19 03:07,04/Mar/11 16:44,0.8.0,,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"The below script was working fine with Pig 0.7. But in Pig 0.8 it is throwing exception.

A = LOAD 'small_input' USING PigStorage(',') AS (spaceid);
B = foreach A generate spaceid;
X = load 'main_data/part-00149' using MapLoader() as (map1, map2, map3);
X = LIMIT X 10;
Raw = foreach X generate map1#'yuid' as type, flatten(map3#'timespent_sessions') as timespent;
Mapping = join Raw by timespent#'p', B by spaceid USING 'replicated' parallel 50;
store Mapping into 'out';
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total 3 Mr jobs ar e planned for this script.The reducers of the second MR job is failing with the below exception :

WARN org.apache.hadoop.mapred.Child: Error running child
org.apache.pig.backend.executionengine.ExecException: ERROR 1081: Cannot cast to map. Expected bytearray but received: bag
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(POCast.java:942)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POMapLookUp.processInput(POMapLookUp.java:87)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POMapLookUp.getNext(POMapLookUp.java:98)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POMapLookUp.getNext(POMapLookUp.java:117)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNext(POLocalRearrange.java:288)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin.getNext(POFRJoin.java:252)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:276)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:240)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.runPipeline(PigMapReduce.java:433)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.processOnePackageOutput(PigMapReduce.java:401)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:381)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:251)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:572)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:414)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:242)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1062)
	at org.apache.hadoop.mapred.Child.main(Child.java:236)
Caused by: java.lang.ClassCastException: org.apache.pig.data.DefaultDataBag cannot be cast to org.apache.pig.data.DataByteArray
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(POCast.java:928)


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,165192,,,,2011-01-04 12:41:41.0,,,,,,,0|i0gz5j:,97125,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Worng stats shown when there are multiple loads but same file names,PIG-1779,12493836,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,vivekp,vivekp,22/Dec/10 07:00,04/Aug/11 00:34,14/Mar/19 03:07,07/Mar/11 18:17,0.8.0,,,,,,,0.9.0,,,,tools,,,0,,,,,,,,,,,,,"In Pig 0.8 , the stats is showing wrong information when ever I have multiple loads and the the file names are similar .

a) Problem 1
Sample Script : 
A = LOAD 'myfolder/tryme' AS (f1);
B = LOAD 'myfolder/anotherfolder/tryme' AS (f2);
C = JOIN A BY f1, B BY f2;
DUMP C;

Here I have 10 records for A and 3 records for B , but pig says 
Successfully read 6 records from: ""<nn>/myfolder/anotherfolder/tryme""
Successfully read 6 records from: ""<nn>myfolder/tryme""

b) Problem 2
A = LOAD 'myfolder/tryme' AS (f1);
B = LOAD 'myfolder/an1111otherfolder/tryme' AS (f2);
C = JOIN A BY f1, B BY f2;
DUMP C;

Here there is no folder named an1111otherfolder while ""myfolder/tryme"" exists . But pig says
Failed to read data from ""<nn>/myfolder/an1111otherfolder/tryme""
Failed to read data from ""<nn>/myfolder/tryme""
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Feb/11 20:58;rding;PIG-1779.patch;https://issues.apache.org/jira/secure/attachment/12471330/PIG-1779.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-02-17 20:58:25.604,,,no_permission,,,,,,,,,,,,66299,Reviewed,,,Mon Mar 07 18:17:12 UTC 2011,,,,,,,0|i0gz4f:,97120,,,,,,,,,,"17/Feb/11 20:58;rding;Output of test-patch:

{code}
    [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{code}",05/Mar/11 00:51;thejas;+1,07/Mar/11 18:17;rding;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"changing statement corresponding to alias after explain , then doing dump gives incorrect result",PIG-1776,12493556,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,17/Dec/10 22:28,25/Apr/11 21:27,14/Mar/19 03:07,28/Jan/11 00:47,0.8.0,,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"{code}
grunt> a = load '/tmp/t2.txt' as (str:chararray, num1:int, alph : chararray);
grunt> dump a;
(ABC,1,a)
(ABC,1,b)
(ABC,1,a)
(ABC,2,b)
(DEF,1,d)
(XYZ,1,x)

grunt> c = foreach b  generate group.str, group.$1, COUNT(a.alph) ;          
grunt> dump c; -- gives correct results
(ABC,1,3)
(ABC,2,1)
(DEF,1,1)
(XYZ,1,1)

/* but dumping c after following steps gives incorrect results */

grunt> c = foreach b  generate group.$0 , (CHARARRAY)group.$1;                                                                                 
grunt> explain c;
...
...
grunt> c = foreach b  generate group.str, group.$1, COUNT(a.alph) ;
grunt> dump c;             
(ABC,1,0)
(ABC,2,0)
(DEF,1,0)
(XYZ,1,0)

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Jan/11 21:28;thejas;PIG-1776.1.patch;https://issues.apache.org/jira/secure/attachment/12469480/PIG-1776.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-01-26 23:57:24.49,,,no_permission,,,,,,,,,,,,165188,,,,Fri Jan 28 00:47:37 UTC 2011,,,,,,,0|i0gz3r:,97117,,,,,,,,,,"26/Jan/11 21:28;thejas;The root cause of the problem was that the UDFContext objects were not reset between plan regenerations. The explain command in the query set the requiredfields property of the load function to require only first two fields. When the plan was regenerated during the dump command, the optimizer rules figured that all columns in load statement are required, and it did not set the requiredfields property. 
To fix this bug, each time a clone of the logical plan is created for regenerating the plan, the UDFContext is being reset.
","26/Jan/11 21:32;thejas;(adding more explanation to previous comment)
The root cause of the problem was that the UDFContext objects were not reset between plan regenerations. 
The explain command in the query set the requiredfields property of the load function to require only first two fields. When the plan was regenerated during the dump command, the optimizer rules figured that all columns in load statement are required, and it did not set the requiredfields property. As a result, the load projected only the first two columns and the 3rd column was null.
To fix this bug, each time a clone of the logical plan is created for regenerating the plan, the UDFContext is being reset. ","26/Jan/11 21:57;thejas;test-patch and all unit tests pass. 
     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
",26/Jan/11 23:57;rding;+1,28/Jan/11 00:47;thejas;Patch committed to 0.8 branch and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Casting docs do not cover casting chararrays,PIG-1773,12493468,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,dvryaboy,dvryaboy,16/Dec/10 22:58,25/Apr/11 21:27,14/Mar/19 03:07,10/Feb/11 22:51,,,,,,,,0.8.1,,,,,,,0,,,,,,,,,,,,,"The documentation appears to be out of date in regards to casting chararrays.
http://pig.apache.org/docs/r0.7.0/piglatin_ref2.html#Cast+Operators lists chararray as being uncastable to anything else.

This isn't the case since PIG-893 


grunt> x = load 'tmp/numbers' as (a:chararray, b:chararray);
grunt> y = foreach x generate ((long) b) + 1;               
grunt> z = foreach y generate (chararray) $0;               
grunt> dump y
(2L)
(4L)
(6L)
(3L)
(8L)
grunt> dump z
(2)
(4)
(6)
(3)
(8)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-02-10 22:51:14.797,,,no_permission,,,,,,,,,,,,165187,,,,Thu Feb 10 23:04:05 UTC 2011,,,,,,,0|i0gz33:,97114,,,,,,,,,,"10/Feb/11 22:51;olgan;Dmitry, the table was updated for 0.8. Could you please review to make sure that what you wanted to cover is there. If not, please, re-open with additional details.",10/Feb/11 23:04;dvryaboy;Looks good.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"New logical plan: Merge schema fail if LoadFunc.getSchema return different schema with ""Load...AS""",PIG-1771,12493456,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,16/Dec/10 20:45,25/Apr/11 21:27,14/Mar/19 03:07,20/Dec/10 18:41,0.8.0,,,,,,,0.8.1,,,,impl,,,0,,,,,,,,,,,,,"The following script fail:

{code}
a = load '1.txt' as (a0:chararray, a1:chararray, a3, a4:map[]);
store a into '1.bin' using BinStorage();

auxData = LOAD '1.bin' USING BinStorage('Utf8StorageConverter') AS (cookieId:chararray, type:chararray, record:tuple(), state:map[]);
dump auxData;
{code}

Error message:
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2246: Error merging schema record#-1:tuple{} and null#-1:bytearray
        at org.apache.pig.newplan.logical.relational.LogicalSchema.merge(LogicalSchema.java:337)
        at org.apache.pig.newplan.logical.relational.LOLoad.getSchema(LOLoad.java:103)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:59)
        at org.apache.pig.newplan.logical.relational.LOLoad.accept(LOLoad.java:159)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:261)
        ... 12 more
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Dec/10 21:24;daijy;PIG-1771-1.patch;https://issues.apache.org/jira/secure/attachment/12466406/PIG-1771-1.patch,17/Dec/10 00:29;daijy;PIG-1771-2.patch;https://issues.apache.org/jira/secure/attachment/12466429/PIG-1771-2.patch,17/Dec/10 19:02;daijy;PIG-1771-3.patch;https://issues.apache.org/jira/secure/attachment/12466483/PIG-1771-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-12-18 00:21:57.137,,,no_permission,,,,,,,,,,,,165186,Reviewed,,,Mon Dec 20 18:42:16 UTC 2010,,,,,,,0|i0gz2n:,97112,,,,,,,,,,17/Dec/10 00:29;daijy;There is an error in PIG-1771-1.patch. Attach PIG-1771-2.patch.,17/Dec/10 19:02;daijy;PIG-1771-3.patch address unit test failure.,"18/Dec/10 00:21;xuefuz;+1

Patches looks good.","20/Dec/10 18:41;daijy;test-patch:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Unit test:
    all pass

End-to-end test:
    all pass",20/Dec/10 18:42;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"matches clause problem with chars that have special meaning in dk.brics - #, @ ..",PIG-1770,12493454,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,16/Dec/10 20:10,25/Apr/11 21:27,14/Mar/19 03:07,11/Mar/11 20:21,0.8.0,,,,,,,0.8.1,,,,impl,,,0,,,,,,,,,,,,,"When special chars #, @ , and the 'optional' patterns described here - http://www.brics.dk/automaton/doc/dk/brics/automaton/RegExp.html#RegExp%28java.lang.String%29 are used , the regex match fails to work. 

This is related to  PIG-965.

Example and workaround are as follows -

{code}
grunt> cat t.txt                           
asd#asdf
zxcasdf
2#asdf

grunt> l = load 't.txt' as (a : chararray);
grunt> f = filter l by (a matches '.*#.*');
grunt> dump f; 
-- No output, though two rows are expected.

--As a workaround, add a \ to escape the # . This regex is valid even in 0.7 , and it will be even after this bug is fixed (its valid java regex, which has same meaning as above regex).
grunt> f = filter l by (a matches '.*\\#.*');
grunt> dump f; 
asd#asdf
2#asdf
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Mar/11 16:43;thejas;PIG-1770.1.patch;https://issues.apache.org/jira/secure/attachment/12472432/PIG-1770.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-11 19:53:08.225,,,no_permission,,,,,,,,,,,,165185,,,,Fri Mar 11 20:21:30 UTC 2011,,,,,,,0|i0gz2f:,97111,,,,,,,,,,"02/Mar/11 16:43;thejas;PIG-1770.1.patch - With this patch the regex optional patterns in the automaton library are disabled so that pattern matching works as expected. 
Unit tests pass. Output of test-patch -
     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
",11/Mar/11 19:53;rding;+1,"11/Mar/11 20:21;thejas;Patch committed to 0.8 branch and trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New logical plan: ImplicitSplitInserter should before DuplicateForEachColumnRewrite,PIG-1766,12493154,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,14/Dec/10 03:51,17/Dec/10 22:47,14/Mar/19 03:07,14/Dec/10 23:21,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"The following script produce wrong result:
{code}
A = load '1.txt' AS (a0:int, a1:int);
B = load '2.txt' AS (b0:int, b1:chararray);
C = join A by a0, B by b0;
D = foreach B generate b0 as d0, b1 as d1;
E = join C by a1, D by d0;
F = foreach E generate b1, d1;
dump F;
{code}

1.txt:
1       2
1       3
2       4
2       5

2.txt:
1       one
2       two

Expected:
(one,two)

We get:
(one,one)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Dec/10 04:01;daijy;PIG-1766-1.patch;https://issues.apache.org/jira/secure/attachment/12466193/PIG-1766-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-12-14 22:56:27.059,,,no_permission,,,,,,,,,,,,165183,Reviewed,,,Tue Dec 14 23:21:59 UTC 2010,,,,,,,0|i0gz1r:,97108,,,,,,,,,,"14/Dec/10 07:17;daijy;The error is due to the conflict of two different uid conflict resolving procedure:
1. If the conflicting uids come from two different relations (PIG-1705)
2. If the conflicting uids come from the same relations (PIG-1732)

Currently, we solve 2 first by checking if ForEach generates the same uid, if so, we convert the subsequent fields into a udf. Then we solve 1 by introducing new uids for every SplitOutput. 

However, if we have a ForEach statement contains conflicting uids due to 1, we erroneously using approach 2 to solve it (since we check 2 first). Although 2 generate different uid for ForEach output, ForEach itself references to the wrong inputs. 

To correct it, we should check 1 first. Procedure 2 will not find the conflict so it will not trigger. 

On the other hand, if we have conflicting uids due to 2, we will never erroneously trigger approach 1 even if we check 1 first. This is because the condition triggering procedure 1 (search for split) will not be affected by the action of procedure 2 (adding udf).","14/Dec/10 22:56;xuefuz;+1 
Please commit after test-patch and unit tests pass. ","14/Dec/10 23:21;daijy;Review notes:
https://reviews.apache.org/r/167/

Patch committed to both trunk and 0.8 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Logic simplification does not work on map key referenced values,PIG-1762,12492962,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yanz,yanz,yanz,10/Dec/10 17:30,17/Dec/10 22:47,14/Mar/19 03:07,11/Dec/10 04:00,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"Logical expression simplification, introduced in PIG-1399, can not handle the map key referenced value in such expressions. For instance, the following statement causes exception  thrown:

b = filter (load 'd.txt' as (k1, k2, k3, v1, v2, v3)) by k2#'f1'#'f' is not null and (v2#'f'#'f1' is not null or v2#'f'#'f2' is not null;

The problem is that if the operand of AND/OR is not a terminal, the current use of stack to store the operand's results won't work because the the used AllSameExpressionVisitor is inadequate in that it visits all expressions while we only need to visit both children of AND/OR.

Another problem is that MapLookupExpression.isEqual method only checks for the top level map references. For instance, m#k1 and m#k2 will be regarded equal. We need to check recursively if nested map key reference is present.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Dec/10 18:08;yanz;PIG-1762.patch;https://issues.apache.org/jira/secure/attachment/12466007/PIG-1762.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-12-10 18:47:16.472,,,no_permission,,,,,,,,,,,,165180,,,,Sat Dec 11 04:00:43 UTC 2010,,,,,,,0|i0gz0v:,97104,,,,,,,,,,10/Dec/10 18:47;daijy;+1. Please commit if tests pass.,11/Dec/10 04:00;yanz;test-patch and test-core pass cleanly. Committed to both trunk and the 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New logical plan: Exception when bag dereference in the middle of expression,PIG-1761,12492889,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,09/Dec/10 23:19,17/Dec/10 22:47,14/Mar/19 03:07,10/Dec/10 20:01,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"The following script fail:

{code}
A = load '1.txt' as (a0:chararray);
B = foreach A generate UPPER(REGEX_EXTRACT_ALL(a0, '.*@(.*)#.*').$0);
explain B;
{code}

Error message:
Caused by: java.lang.NullPointerException
        at org.apache.pig.newplan.ReverseDependencyOrderWalker.walk(ReverseDependencyOrderWalker.java:70)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:104)
        at org.apache.pig.newplan.logical.relational.LOGenerate.accept(LOGenerate.java:229)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:93)
        at org.apache.pig.newplan.logical.relational.LOForEach.accept(LOForEach.java:71)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:261)
        ... 12 more
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Dec/10 23:43;daijy;PIG-1761-1.patch;https://issues.apache.org/jira/secure/attachment/12465954/PIG-1761-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-12-10 19:52:38.713,,,no_permission,,,,,,,,,,,,165179,Reviewed,,,Fri Dec 10 20:01:51 UTC 2010,,,,,,,0|i0gz0n:,97103,,,,,,,,,,"10/Dec/10 19:52;xuefuz;+1
Patch looks good.","10/Dec/10 20:01;daijy;Review notes:
https://reviews.apache.org/r/163/

Patch committed to both trunk and 0.8 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to report progress in all databags,PIG-1760,12492785,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,09/Dec/10 01:15,17/Dec/10 22:47,14/Mar/19 03:07,09/Dec/10 18:10,0.7.0,,,,,,,0.8.0,0.9.0,,,,,,0,,,,,,,,,,,,,Pig needs to report progress periodically in its DataBag implementation to avoid the tasks being killed by Hadoop task tracker because timeout. The progress reporting isn't implemented in the  InternalCacheBag class.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Dec/10 01:17;rding;PIG-1760.patch;https://issues.apache.org/jira/secure/attachment/12465869/PIG-1760.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-12-09 01:23:10.832,,,no_permission,,,,,,,,,,,,165178,Reviewed,,,Thu Dec 09 18:10:38 UTC 2010,,,,,,,0|i0gz0f:,97102,,,,,,,,,,09/Dec/10 01:23;daijy;+1. Please commit if tests pass.,09/Dec/10 18:10;rding;Unit tests passed. The patch is committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New logical plan: PushDownForEachFlatten fail in UDF with unknown output schema,PIG-1751,12491917,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,02/Dec/10 19:24,17/Dec/10 22:47,14/Mar/19 03:07,06/Dec/10 01:02,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"The following script fail:

{code}
a = load '1.txt' as (a0:chararray);
b = load '2.txt' as (b0:chararray);
c = foreach b generate flatten(STRSPLIT(b0)) as c0;
d = join c by c0, a by a0;
dump d;
{code}

Error message:
Caused by: java.lang.NullPointerException
        at org.apache.pig.newplan.logical.rules.PushDownForEachFlatten$PushDownForEachFlattenTransformer.getNonFlattenFieldUids(PushDownForEachFlatten.java:186)
        at org.apache.pig.newplan.logical.rules.PushDownForEachFlatten$PushDownForEachFlattenTransformer.check(PushDownForEachFlatten.java:101)
        at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:108)
        ... 13 more
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Dec/10 20:59;daijy;PIG-1751-1.patch;https://issues.apache.org/jira/secure/attachment/12465167/PIG-1751-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-12-03 19:55:53.211,,,no_permission,,,,,,,,,,,,165174,Reviewed,,,Mon Dec 06 01:02:30 UTC 2010,,,,,,,0|i0gyyf:,97093,,,,,,,,,,"03/Dec/10 19:55;xuefuz;+1
","06/Dec/10 01:02;daijy;test-patch:

     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Unit test:
pass

end-to-end test:
pass",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable converting bytes loading from BinStorage,PIG-1745,12480724,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,23/Nov/10 19:58,04/Aug/11 00:34,14/Mar/19 03:07,06/Dec/10 06:56,0.8.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"If we load bytes from BinStorage, we don't actually know how we get these bytes originally, and we will not have a way to cast those bytes. Ideally we shall encode caster into BinStorage data file, but we are not there yet. Currrently bytesToXXX methods for BinStorage is wrong and it results unexpected errors. Eg.

{code}
a = load '1.txt' as (a0, a1, a2);
store a into '1.bin' as BinStorage();

a = load '1.bin' using BinStorage as (a0, a1, a2);
b = foreach a generate (long)a0;
dump b;
{code}

The code will run but produce wrong data. It's less confusing if we throw an exception in this case.

Release Notes:
Pig will throw exception in the case we want to convert bytes loading from BinStorage",,,,,,,,,,,,,,,,,PIG-1222,PIG-1341,,,,,,,,,,,,,,23/Nov/10 20:01;daijy;PIG-1745-1.patch;https://issues.apache.org/jira/secure/attachment/12460300/PIG-1745-1.patch,30/Nov/10 22:40;daijy;PIG-1745-2.patch;https://issues.apache.org/jira/secure/attachment/12465001/PIG-1745-2.patch,02/Dec/10 23:07;daijy;PIG-1745-3.patch;https://issues.apache.org/jira/secure/attachment/12465178/PIG-1745-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-12-02 21:33:30.966,,,no_permission,,,,,,,,,,,,66255,Reviewed,,,Mon Apr 11 08:46:46 UTC 2011,,,,,,,0|i0gyxj:,97089,"1. Converting bytes loading from BinStorage() will now result an error.
2. If user clearly knows the caster for the BinStorage, he/she can pass the caster class name to the construct of BinStorage. For example, if data is load from PigStorage (or other LoadFunc using Utf8StorageConverter) and store using BinStorage, user can pass ""org.apache.pig.builtin.Utf8StorageConverter"" to  the construct of BinStorage (or pass ""Utf8StorageConverter"", since we will search for ""org.apache.pig.builtin"" package by default). By doing this, converting bytes to other type will still work.",,,,,,,,,"02/Dec/10 21:33;dvryaboy;Daniel,
check out how I addressed a very similar problem in the HBase loader -- I have a default caster, and allow a user to specify one using a constructor if necessary. I think that's cleaner than adding and extra storage class.

https://github.com/apache/pig/blob/trunk/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java","02/Dec/10 22:12;daijy;Yes, I think it is good, in that we can also allow other caster. But different from HBaseStorage, I don't want to make UTF8StorageConverter special, we require full qualified caster class name in the constructor parameter, unless this class is in org.apache.pig.builtin. ",02/Dec/10 23:04;daijy;Attach another patch addressing comments from Dmitriy and Thejas.,"03/Dec/10 00:50;thejas;+1 
please commit after test-patch and unit tests succeed.
","06/Dec/10 06:56;daijy;test-patch:
[exec] +1 overall. 
[exec] 
[exec] +1 @author. The patch does not contain any @author tags.
[exec] 
[exec] +1 tests included. The patch appears to include 2 new or modified tests.
[exec] 
[exec] +1 javadoc. The javadoc tool did not generate any warning messages.
[exec] 
[exec] +1 javac. The applied patch does not increase the total number of javac compiler warnings.
[exec] 
[exec] +1 findbugs. The patch does not introduce any new Findbugs warnings.
[exec] 
[exec] +1 release audit. The applied patch does not increase the total number of release audit warnings.

Unit test:
pass

end-to-end test:
pass","08/Apr/11 08:29;mridulm@yahoo-inc.com;Since this is a backwardly incompatible change, some release notes indicating how users can continue to rely on earlier behavior (when the data being casted is indeed a long in the example in desc) would be good - assuming it is still possible to do so.
If it is totally removed as an option, it is very unfortunate and drastically diminishes the value of BinStorage.","08/Apr/11 21:33;daijy;bq. Since this is a backwardly incompatible change, some release notes indicating how users can continue to rely on earlier behavior (when the data being casted is indeed a long in the example in desc) would be good - assuming it is still possible to do so.
Yes, I moved these Jiras to the right section in CHANGES.txt

bq. If it is totally removed as an option, it is very unfortunate and drastically diminishes the value of BinStorage.
What do you mean ""removed as an option""? This option will not be removed.","08/Apr/11 22:35;olgan;Just to clarify what is happenning (and we are adding information to the 0.9 documentation)

The way to get previous functionality is to specify a converter for BinStorage to use to do the casts:

a = load 'g/part*' using BinStorage('Utf8StorageConverter') as (id, d:bag{t:(v, s)});
b = foreach a generate (double)id, flatten(d);
dump b;

The UTF8StorageConverter is provided by default in Pig.

We are require users to specify the converter explicitely to make sure that wrong results are not returned in case the data is not in the format UTF8Converter can understand.

","09/Apr/11 11:26;mridulm@yahoo-inc.com;
@Daniel Dai: By ""removed as an option"", I meant whether the ability to cast to arbitrary schema was removed or not. Olga clarified that.

@Olga : Glad to see there is a way to achieve the functionality I was looking for.
But passing classname as a parameter seems fragile behavior if the intention is to get an instance of LoadCaster.
How would we handle cases for other loaders which require similar functionality (just as with BinStorage, it is not practical to assume all loaders can implement LoadCaster 'well') ? Mirror similar idiom ?
Exposing implementation dependent constructor arguments for things like this is not extensible in long run if the deficiency is due to pig interface requirements. A more general way to plug it in would be better (and more easily verifiable by pig).
","10/Apr/11 00:02;dvryaboy;Mridul: the same idiom is used in HBaseStorage. 

Can you suggest a better idiom or interface?","11/Apr/11 08:46;mridulm@yahoo-inc.com;
As of now, as we have more loaders - sherpa, fish, hive, etc - which require similar functionality which is already duplicated by BinStorage, hbase, etc : all of them will end up exposing details of this nature directly to the user : that too in ways which are validated differently by each (not to mention duplication of code).

load and store func's are getting overloaded with functionalitywhich is not directly (but cosmetically) related to their operation : like in this case, where we tie casting to the loader - but configure it ""externally"" with the loader acting as a delegate for it.
Instead of each loader reinventing and exposing similar (or different) idioms, would be better to abstract this away into pig : either as a grammar construct or as something else with reasonable smart default provided by the loader (getDefaultLoadCaster instead of getLoadCaster ?).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.pig.newplan.optimizer.Rule.java does not work with plan patterns where leaves/sinks are not siblings,PIG-1742,12480614,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,22/Nov/10 21:42,04/Aug/11 00:35,14/Mar/19 03:07,24/Nov/10 19:25,0.8.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"Rule.java which is used for finding patterns in the logical plan during logical plan optimization does not work with patterns where the leaves/sinks in the patter are not siblings.

For eg, it will not work with a pattern such as  -
{code}
A->B
|-> C -> D
In this example, B and D are leaves, but not siblings.
{code}

In org.apache.pig.newplan.optimizer.Rule.java, the code in the for loop at line 138 (in this revision - http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/newplan/optimizer/Rule.java?annotate=1005230) checks if the leaves in the pattern are are sibling of the node that matched. This is an undocumented limitation. Note that none of the existing logical plan optimization rules have such a patter, so this issue does not have any impact on the user.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Nov/10 18:55;thejas;PIG-1742.1.patch;https://issues.apache.org/jira/secure/attachment/12460389/PIG-1742.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-11-24 18:57:28.888,,,no_permission,,,,,,,,,,,,66167,Reviewed,,,Wed Nov 24 19:25:48 UTC 2010,,,,,,,0|i0gywv:,97086,,,,,,,,,,"24/Nov/10 18:55;thejas;PIG-1742.1.patch - documents this limitation in patterns
No new tests as it has only documentation change.

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

",24/Nov/10 18:57;daijy;+1,"24/Nov/10 19:25;thejas;Patch committed to trunk.
The limitation described in the jira can be fixed if the need arises from  new logical plan optimization rules.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lineage fail when flatten a bag,PIG-1741,12480525,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,21/Nov/10 09:01,17/Dec/10 22:47,14/Mar/19 03:07,23/Nov/10 00:40,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"The following script fail:
{code}
a = load '1.txt' as (a0);
b = foreach a generate flatten((bag{tuple(map[])})a0) as b0:map[];
c = foreach b generate (long)b0#'key1';
dump c;
{code}

1.txt:
{([key1#1])}

Error message:
org.apache.pig.backend.executionengine.ExecException: ERROR 1075: Received a bytearray from the UDF. Cannot determine how to convert the bytearray to long.
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(POCast.java:286)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:361)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:291)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:236)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:231)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)

In logical plan:
{code}
fake: Store 1-30 Schema: {long} Type: Unknown
|
|---c: ForEach 1-29 Schema: {long} Type: bag
    |   |
    |   Cast 1-28--null FieldSchema: long Type: long
    |   |
    |   |---MapLookup 1-26 FieldSchema: bytearray Type: bytearray
    |       |
    |       |---Project 1-27 Projections: [0] Overloaded: false FieldSchema: b0: map Type: map
    |           Input: b: ForEach 1-21
    |
    |---b: ForEach 1-21 Schema: {b0: map[ ]} Type: bag
        |   |
        |   Cast 1-20--org.apache.pig.builtin.PigStorage FieldSchema: a0: bag({(map[ ])}) Type: bag
        |   |
        |   |---Project 1-19 Projections: [0] Overloaded: false FieldSchema: a0: bytearray Type: bytearray
        |       Input: a: Load 1-15
        |
        |---a: Load 1-15 Schema: {a0: bytearray} Type: bag
{code}

Cast 1-28 get null caster. This error cannot be fixed by -Dusenewlogicalplan=false flag.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Nov/10 19:30;daijy;PIG-1741-0.patch;https://issues.apache.org/jira/secure/attachment/12460137/PIG-1741-0.patch,22/Nov/10 18:04;daijy;PIG-1741-1.patch;https://issues.apache.org/jira/secure/attachment/12460191/PIG-1741-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-11-22 22:19:35.982,,,no_permission,,,,,,,,,,,,165170,Reviewed,,,Tue Nov 23 00:40:28 UTC 2010,,,,,,,0|i0gywn:,97085,,,,,,,,,,21/Nov/10 19:30;daijy;Attach a preliminary fix.,22/Nov/10 19:40;daijy;All tests pass. Patch is ready for review.,"22/Nov/10 22:19;xuefuz;+1
Patch looks fine as a temporary fix, as we are moving to the new logical plan, where lineage is done different. Per the issue, the right fix should be to find why the canonical map is empty and create the map, instead of checking if the map is empty as done in the fix.",22/Nov/10 22:20;xuefuz;Please document the nature of the fix in the code. Thanks.,"23/Nov/10 00:40;daijy;test-patch result: 

     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Patch committed to both trunk and 0.8 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig 0.8 zero return code when pig script fails; also error is dumped on screen instead of logfile,PIG-1739,12480403,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yanz,viraj,viraj,19/Nov/10 07:12,17/Dec/10 22:47,14/Mar/19 03:07,22/Nov/10 17:30,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"I have pig script where one input directory cannot be accessed. The pig script obviously fails but the return code is zero.

{code}
A = LOAD '/user/viraj/testdata1' USING PigStorage(':') AS (ia, na);
B = FOREACH A GENERATE $0 AS id;
C = LOAD '/user/tstusr/test/' USING PigStorage(':') AS (ib, nb);
D = FOREACH C GENERATE $0 AS id;
--dump B;
E = JOIN A by ia, C by ib USING 'replicated';
store E into 'id.out';
{code}

Here is the console output:
{quote}
$ java -cp $PIG_HOME/pig.jar org.apache.pig.Main script.pig
2010-11-19 06:51:32,780 [main] INFO  org.apache.pig.Main - Logging error messages to: /home/viraj/pigscripts/pig_1290149492775.log
...
2010-11-19 06:51:39,136 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: REPLICATED_JOIN
2010-11-19 06:51:39,187 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - (Name: E: Store(hdfs://mynamenode/user/viraj/id.out:org.apache.pig.builtin.PigStorage) - 1-38 Operator Key: 1-38)
2010-11-19 06:51:39,198 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false
2010-11-19 06:51:39,344 [main] WARN  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - failed to get number of input files
org.apache.hadoop.security.AccessControlException: org.apache.hadoop.security.AccessControlException: Permission denied: user=viraj, access=EXECUTE, inode=""tstusr"":tstusr:users:rwx------
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95)
        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:57)
        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:678)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:521)
        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:692)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.hasTooManyInputFiles(MRCompiler.java:1302)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.visitFRJoin(MRCompiler.java:1210)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin.visit(POFRJoin.java:188)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.compile(MRCompiler.java:472)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.compile(MRCompiler.java:451)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.compile(MRCompiler.java:333)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.compile(MapReduceLauncher.java:469)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:117)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:378)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1198)
        at org.apache.pig.PigServer.execute(PigServer.java:1190)
        at org.apache.pig.PigServer.access$100(PigServer.java:128)
        at org.apache.pig.PigServer$Graph.execute(PigServer.java:1517)
        at org.apache.pig.PigServer.executeBatchEx(PigServer.java:362)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:329)
        at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:112)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:169)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:141)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:90)
        at org.apache.pig.Main.run(Main.java:498)
        at org.apache.pig.Main.main(Main.java:107)
2010-11-19 06:51:56,712 [main] ERROR org.apache.pig.tools.pigstats.PigStats - ERROR 2997: Unable to recreate exception from backend error: org.apache.pig.backend.executionengine.ExecException: ERROR 2118: org.apache.hadoop.security.AccessControlException: Permission denied: user=viraj, access=EXECUTE, inode=""tstusr"":tstusr:users:rwx------
2010-11-19 06:51:56,712 [main] ERROR org.apache.pig.tools.pigstats.PigStatsUtil - 1 map reduce job(s) failed!
2010-11-19 06:51:56,714 [main] INFO  org.apache.pig.tools.pigstats.PigStats - Script Statistics:
...
HadoopVersion   PigVersion      UserId  StartedAt       FinishedAt      Features
0.20.1   0.8.0..1011012300       viraj   2010-11-19 06:51:41     2010-11-19 06:51:56     REPLICATED_JOIN

Failed!

Failed Jobs:
JobId   Alias   Feature Message Outputs
N/A     C       MAP_ONLY        Message: org.apache.pig.backend.executionengine.ExecException: ERROR 2118: org.apache.hadoop.security.AccessControlException: Permission denied: user=viraj, access=EXECUTE, inode=""tstusr"":tstusr:users:rwx------
Input(s):
Failed to read data from ""/user/tstusr/test/""

Output(s):

Counters:
Total records written : 0
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
null    ->      null,
null

2010-11-19 06:51:56,714 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Failed!

$echo $?
0
{quote}

Clearly users depending on this return code to run their workflows are affected.

Viraj",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Nov/10 01:49;yanz;PIG-1739.patch;https://issues.apache.org/jira/secure/attachment/12460081/PIG-1739.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-11-20 01:49:27.922,,,no_permission,,,,,,,,,,,,165168,,,,Mon Nov 22 17:30:27 UTC 2010,,,,,,,0|i0gyw7:,97083,,,,,,,,,,"20/Nov/10 01:49;yanz;The problem is that when a job status is checked, its output status is checked. But the temporary file output status is never checked, which makes sense in case of successful jobs since nobody is interested in temporary file stats. But if a job producing temporary output fails, the output status need to be added otherwise the job status check won't find the failed job.","20/Nov/10 02:14;yanz;With the fix, the log files contains the following, which I believe is good enough for informational and debugging purposes as well. The ""failed to get number of input files"" message is just a warning,  by itself does not cause the failure,  and only sent to the console.

Backend error message during job submission
-------------------------------------------
org.apache.pig.backend.executionengine.ExecException: ERROR 2118: org.apache.hadoop.security.AccessControlException: Permission denied: user=yanz, access=EXECUTE, inode=""test1"":yanz:hdfs:---------
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:280)
	at org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:907)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:801)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:752)
	at org.apache.hadoop.mapred.jobcontrol.Job.submit(Job.java:378)
	at org.apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs(JobControl.java:247)
	at org.apache.hadoop.mapred.jobcontrol.JobControl.run(JobControl.java:279)
	at java.lang.Thread.run(Thread.java:619)
Caused by: org.apache.hadoop.security.AccessControlException: org.apache.hadoop.security.AccessControlException: Permission denied: user=yanz, access=EXECUTE, inode=""test1"":yanz:hdfs:---------
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:96)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:58)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:617)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:453)
	at org.apache.hadoop.fs.FileSystem.getFileStatus(FileSystem.java:1330)
	at org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:924)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:866)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:209)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigTextInputFormat.listStatus(PigTextInputFormat.java:36)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:246)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:268)
	... 7 more
Caused by: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.security.AccessControlException: Permission denied: user=yanz, access=EXECUTE, inode=""test1"":yanz:hdfs:---------
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:159)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:115)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:85)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:4575)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkTraverse(FSNamesystem.java:4554)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:1751)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.getFileInfo(NameNode.java:572)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:963)

	at org.apache.hadoop.ipc.Client.call(Client.java:740)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
	at $Proxy0.getFileInfo(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
	at $Proxy0.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:615)
	... 15 more

Pig Stack Trace
---------------
ERROR 2244: Job failed, hadoop does not return any error message

org.apache.pig.backend.executionengine.ExecException: ERROR 2244: Job failed, hadoop does not return any error message
	at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:117)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:169)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:141)
	at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:90)
	at org.apache.pig.Main.run(Main.java:509)
	at org.apache.pig.Main.main(Main.java:107)
================================================================================","21/Nov/10 02:33;yanz;test-core passes ok. test-patch has the following output:

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.


The test case is hard to automate, I manually verified the fix.",22/Nov/10 17:14;rding;+1,22/Nov/10 17:30;yanz;Committed to both the trunk and the 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New logical plan: Optimized UserFuncExpression.getFieldSchema,PIG-1738,12480285,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,18/Nov/10 02:23,17/Dec/10 22:47,14/Mar/19 03:07,22/Nov/10 08:32,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"Currently every time we invoke UserFuncExpression.getFieldSchema, Pig will instantiate UserFunc. This is not efficient and should change.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Nov/10 02:26;daijy;PIG-1738-1.patch;https://issues.apache.org/jira/secure/attachment/12459868/PIG-1738-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-11-18 18:52:10.969,,,no_permission,,,,,,,,,,,,165167,Reviewed,,,Mon Nov 22 08:32:20 UTC 2010,,,,,,,0|i0gyvz:,97082,,,,,,,,,,"18/Nov/10 02:26;daijy;Patch is performance improvement, no obvious way to write a unit test.","18/Nov/10 18:52;yanz;+1. This solution is to use caching of the often-used object. It is based on the assumption that once mFuncSpec is set, itself and its contents won't be modified, at least after the getFieldSchema method is called. But this invariant is nowhere to find in code. We may need to comment somehow. Otherwise patch looks ok.

","22/Nov/10 08:32;daijy;test-patch result:

[exec] -1 overall.
[exec]
[exec] +1 @author. The patch does not contain any @author tags.
[exec]
[exec] -1 tests included. The patch doesn't appear to include any new or modified tests.
[exec] Please justify why no tests are needed for this patch.
[exec]
[exec] +1 javadoc. The javadoc tool did not generate any warning messages.
[exec]
[exec] +1 javac. The applied patch does not increase the total number of javac compiler warnings.
[exec]
[exec] +1 findbugs. The patch does not introduce any new Findbugs warnings.
[exec]
[exec] +1 release audit. The applied patch does not increase the total number of release audit warnings.

All tests pass. Add comments as per Yan's suggestion.

Patch committed to both trunk and 0.8 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New logical plan: Improve error messages when merge schema fail,PIG-1737,12480280,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,18/Nov/10 01:06,17/Dec/10 22:46,14/Mar/19 03:07,20/Nov/10 02:24,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"Pig fail when foreach schema does not match user defined schema with the NPE. The error message should be enhanced. Eg:
{code}
a = load '1.txt' as (a0, a1, a2);
b = group a by (a0, a1);
c = foreach b generate flatten(group) as c0;
dump c;
{code}
flatten(group) contains 2 items, user cannot name it into c0 alone. However, Pig die with NPE, which is clueless to the user.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Nov/10 01:13;daijy;PIG-1737-1.patch;https://issues.apache.org/jira/secure/attachment/12459863/PIG-1737-1.patch,19/Nov/10 20:28;daijy;PIG-1737-2.patch;https://issues.apache.org/jira/secure/attachment/12460051/PIG-1737-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-11-19 20:49:05.541,,,no_permission,,,,,,,,,,,,165166,Reviewed,,,Sat Nov 20 02:24:34 UTC 2010,,,,,,,0|i0gyvr:,97081,,,,,,,,,,18/Nov/10 01:13;daijy;It is not obvious to write a unit test. Tested it manually.,19/Nov/10 20:28;daijy;Changed the patch as per Thejas's suggestion.,"19/Nov/10 20:49;thejas;+1
Please commit once test-patch and unit tests pass.
","20/Nov/10 02:24;daijy;test-patch result:

     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

All tests pass.

Patch committed to both trunk and 0.8 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig trunk-commit build fails with the mvn-install ant target,PIG-1736,12480263,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,gkesavan,gkesavan,gkesavan,17/Nov/10 22:24,04/Aug/11 00:34,14/Mar/19 03:07,18/Nov/10 08:58,,,,,,,,0.9.0,,,,build,,,0,,,,,,,,,,,,,"link : https://hudson.apache.org/hudson/job/Pig-trunk-commit/593/console

simpledeploy:
[artifact:install-provider] Installing provider: org.apache.maven.wagon:wagon-http:jar:1.0-beta-2:runtime
[artifact:deploy] Deploying to https://repository.apache.org/content/repositories/snapshots
[artifact:deploy] Uploading: org/apache/pig/pig/2010-11-17_22-01-11/pig-2010-11-17_22-01-11.jar to apache.snapshots.https
[artifact:deploy] Uploaded 2341K
[artifact:deploy] An error has occurred while processing the Maven artifact tasks.
[artifact:deploy]  Diagnosis:
[artifact:deploy] 
[artifact:deploy] Error deploying artifact 'org.apache.pig:pig:jar': Error deploying artifact: Failed to transfer file: https://repository.apache.org/content/repositories/snapshots/org/apache/pig/pig/2010-11-17_22-01-11/pig-2010-11-17_22-01-11.jar. Return code is: 400

BUILD FAILED
/grid/0/hudson/hudson-slave/workspace/Pig-trunk-commit/trunk/build.xml:915: Error deploying artifact 'org.apache.pig:pig:jar': Error deploying artifact: Failed to transfer file: https://repository.apache.org/content/repositories/snapshots/org/apache/pig/pig/2010-11-17_22-01-11/pig-2010-11-17_22-01-11.jar. Return code is: 400
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-11-17 23:54:12.385,,,no_permission,,,,,,,,,,,,66320,,,,Thu Nov 18 08:58:09 UTC 2010,,,,,,,0|i0gyvj:,97080,,,,,,,,,,"17/Nov/10 23:54;olgan;Niraj, could you, please, take a look","18/Nov/10 00:44;gkesavan;It looks like something to do with the verisioning of the maven artifact, let me look into it.","18/Nov/10 08:58;gkesavan;Fixed the hudson builds failures. 

https://hudson.apache.org/hudson/view/M-R/view/Pig/job/Pig-trunk-commit/594",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New logical plan: logical plan get confused if we generate the same field twice in ForEach,PIG-1732,12480166,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,17/Nov/10 00:32,17/Dec/10 22:46,14/Mar/19 03:07,21/Nov/10 08:55,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"The following script fail:

{code}
a = load '1.txt' as (a0, a1);
b = load '2.txt' as (b0, b1, b2);
c = foreach a generate a0, a1, a1 as a2;
d = union b, c;
e = foreach d generate $1;
explain e;
{code}

Error message:
ERROR 2000: Error processing rule ColumnMapKeyPrune. Try -t ColumnMapKeyPrune

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1067: Unable to explain alias e
        at org.apache.pig.PigServer.explain(PigServer.java:958)
        at org.apache.pig.tools.grunt.GruntParser.explainCurrentBatch(GruntParser.java:353)
        at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:285)
        at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:248)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.Explain(PigScriptParser.java:605)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:327)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:165)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:141)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:90)
        at org.apache.pig.Main.run(Main.java:498)
        at org.apache.pig.Main.main(Main.java:107)
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2042: Error in new logical plan. Try -Dpig.usenewlogicalplan=false.
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:309)
        at org.apache.pig.PigServer.compilePp(PigServer.java:1354)
        at org.apache.pig.PigServer.explain(PigServer.java:927)
        ... 10 more
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2000: Error processing rule ColumnMapKeyPrune. Try -t ColumnMapKeyPrune
        at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:120)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:277)
        ... 12 more
Caused by: java.lang.NullPointerException
        at org.apache.pig.newplan.logical.relational.LOUnion.getSchema(LOUnion.java:75)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:127)
        at org.apache.pig.newplan.logical.relational.LOUnion.accept(LOUnion.java:102)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.newplan.logical.optimizer.SchemaPatcher.transformed(SchemaPatcher.java:43)
        at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:112)

The problem is caused by c = foreach a generate a0, a1, a1 as a2, in which we will generate two fields share one uid.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Nov/10 03:09;daijy;PIG-1732-1.patch;https://issues.apache.org/jira/secure/attachment/12459965/PIG-1732-1.patch,19/Nov/10 19:47;daijy;PIG-1732-2.patch;https://issues.apache.org/jira/secure/attachment/12460038/PIG-1732-2.patch,20/Nov/10 19:39;daijy;PIG-1732-3.patch;https://issues.apache.org/jira/secure/attachment/12460107/PIG-1732-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-11-19 20:37:09.181,,,no_permission,,,,,,,,,,,,165164,Reviewed,,,Sun Nov 21 08:55:25 UTC 2010,,,,,,,0|i0gyun:,97076,,,,,,,,,,19/Nov/10 19:47;daijy;PIG-1732-2.patch address test failures.,"19/Nov/10 20:37;thejas;+1 
Please commit after test-patch and unit tests pass.
",20/Nov/10 19:39;daijy;PIG-1732-3.patch address some additional test failures.,"21/Nov/10 08:55;daijy;test-patch result:

     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

All tests pass.

Patch committed to both trunk and 0.8 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New Logical Plan: FILTER fails when there are multiple conditions,PIG-1731,12480152,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yanz,sherri_chen,sherri_chen,16/Nov/10 22:23,17/Dec/10 22:46,14/Mar/19 03:07,17/Nov/10 16:34,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"Following pig script fails:
===========
REGISTER string.jar;

A = LOAD 'data' USING PigStorage() AS (a0:chararray, a1, a2);
B = FOREACH A GENERATE a0;
C = DISTINCT B;
D = FILTER C BY string.LENGTH(a0) <= 50 AND string.LENGTH(a0) > 1; 
STORE D INTO 'D';
===========
2010-11-16 22:11:38,097 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - pig.usenewlogicalplan is set to true. New logical plan will be used.
2010-11-16 22:11:38,202 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2998: Unhandled internal error. null

But this one works:
===========
REGISTER string.jar;

A = LOAD 'data' USING PigStorage() AS (a0:chararray, a1, a2);
B = FOREACH A GENERATE a0;
C = DISTINCT B;
D = FILTER C BY string.LENGTH(a0) <= 50;
E = FILTER D BY string.LENGTH(a0) > 1; 
STORE E INTO 'D';
===========

Input data:
===========
Jerry   Jerry   0.55
Dave    David   0.15
Danny   Dan     0.015
S       Smith   0.2
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA        Amy     0.8
===========",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Nov/10 00:18;yanz;PIG-1731.patch;https://issues.apache.org/jira/secure/attachment/12459747/PIG-1731.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-11-17 00:18:54.877,,,no_permission,,,,,,,,,,,,165163,,,,Wed Nov 17 16:34:31 UTC 2010,,,,,,,0|i0gyuf:,97075,,,,,,,,,,"17/Nov/10 00:18;yanz;The problem is that UserFuncExpression.isEqual checks for the logical plan equality before check for the the UDF's equality. This causes an infinite loop as the logical plan equality check
requires the equality checks of all its operators of which the UserFuncExpression is part.

We had the similar issues on other expression operators before. NotExpression, e.g, had the problem fixed in PIG-1510. 

The  UserFuncExpression.isEqual is called by the logicl expression simplifier (PIG-1399)  in the new logical plan. At least in this test it is not called if the old logical plan is used. That's why use of the old logical plan is ok in this test case.",17/Nov/10 00:23;yanz;UserFuncExpression seems to be the only logical expression operator that still has this problem as I just checked.,17/Nov/10 00:34;olgan;can we solve both issues with the same patch?,"17/Nov/10 00:36;yanz;If there is no AND in this test, the simplifier won't check for the equality of the two UDF calls so it is ok too.",17/Nov/10 00:47;daijy;+1,17/Nov/10 01:04;yanz;test-patch runs clean.,"17/Nov/10 16:34;yanz;test-core passed cleanly.

Committed to both trunk and the 0.8 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.ArrayIndexOutOfBoundsException druing accessing relation,PIG-1730,12480141,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,sherri_chen,sherri_chen,16/Nov/10 20:18,17/Dec/10 22:46,14/Mar/19 03:07,17/Nov/10 02:25,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"New logical plan is failing following code:
<pre>
A = load 'data' AS (query:chararray, type:chararray, freq:int);
B = group A by query;

C = foreach B {

    click = filter A by (type == 'c');
    pv = filter A by (type == 'p');
    click_sum = (IsEmpty(click.freq)? 0 : SUM(click.freq)); 

    generate
        COUNT(click),
        COUNT(pv),
        click_sum;
}
store C into 'C';
</pre>

java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.ArrayList.get(ArrayList.java:324)
        at org.apache.pig.data.DefaultTuple.get(DefaultTuple.java:158)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.processInputBag(POProject.java:482)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PORelationToExprProject.getNext(PORelationToExprProject.java:107)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.processInputBag(POProject.java:480)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:197)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.processInput(POUserFunc.java:160)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:212)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:289)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POBinCond.getNext(POBinCond.java:193)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:361)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:291)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.runPipeline(PigMapReduce.java:433)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.processOnePackageOutput(PigMapReduce.java:401)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:381)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:251)
        at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
        at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:570)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:412)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:258)
2010-11-16 19:53:51,816 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - job job_local_0002 has failed! Stop running all dependent jobs
2010-11-16 19:53:51,818 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete
2010-11-16 19:53:51,818 [main] ERROR org.apache.pig.tools.pigstats.PigStatsUtil - 1 map reduce job(s) failed!
2010-11-16 19:53:51,818 [main] INFO  org.apache.pig.tools.pigstats.PigStats - Detected Local mode. Stats reported below may be incomplete
2010-11-16 19:53:51,819 [main] INFO  org.apache.pig.tools.pigstats.PigStats - Script Statistics: 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-11-17 02:25:35.1,,,no_permission,,,,,,,,,,,,165162,,,,Wed Nov 17 02:25:35 UTC 2010,,,,,,,0|i0gyu7:,97074,,,,,,,,,,17/Nov/10 02:25;daijy;It has the same nature of [PIG-1721|https://issues.apache.org/jira/browse/PIG-1721]. Close it and move discussion to PIG-1721.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New logical plan: Dereference does not add into plan after deepCopy,PIG-1729,12480139,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,16/Nov/10 20:04,17/Dec/10 22:46,14/Mar/19 03:07,20/Nov/10 00:04,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"The following script fail:
{code}
a = load '1.txt' as (a0:int, a1:int, a2:int);
b = load '2.txt' as (b0:int, b1:int);
c = cogroup a by a0, b by b0;
d = foreach c generate ((COUNT(a)==0L)?null : a.a0) as d0;
e = foreach d generate flatten(d0);
f = group e all;
explain f;
{code}

Error message:
ERROR 2000: Error processing rule GroupByConstParallelSetter. Try -t GroupByConstParallelSetter

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1067: Unable to explain alias f
        at org.apache.pig.PigServer.explain(PigServer.java:958)
        at org.apache.pig.tools.grunt.GruntParser.explainCurrentBatch(GruntParser.java:353)
        at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:285)
        at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:248)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.Explain(PigScriptParser.java:605)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:327)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:165)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:141)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:90)
        at org.apache.pig.Main.run(Main.java:498)
        at org.apache.pig.Main.main(Main.java:107)
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2042: Error in new logical plan. Try -Dpig.usenewlogicalplan=false.
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:309)
        at org.apache.pig.PigServer.compilePp(PigServer.java:1354)
        at org.apache.pig.PigServer.explain(PigServer.java:927)
        ... 10 more
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2000: Error processing rule GroupByConstParallelSetter. Try -t GroupByConstParallelSetter
        at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:120)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:277)
        ... 12 more
Caused by: java.lang.NullPointerException
        at org.apache.pig.newplan.logical.relational.LogicalSchema$LogicalFieldSchema.compatible(LogicalSchema.java:106)
        at org.apache.pig.newplan.logical.relational.LogicalSchema$LogicalFieldSchema.mergeUid(LogicalSchema.java:116)
        at org.apache.pig.newplan.logical.expression.ProjectExpression.getFieldSchema(ProjectExpression.java:153)
        at org.apache.pig.newplan.logical.optimizer.FieldSchemaResetter.execute(SchemaResetter.java:175)
        at org.apache.pig.newplan.logical.expression.AllSameExpressionVisitor.visit(AllSameExpressionVisitor.java:53)
        at org.apache.pig.newplan.logical.expression.ProjectExpression.accept(ProjectExpression.java:75)
        at org.apache.pig.newplan.ReverseDependencyOrderWalker.walk(ReverseDependencyOrderWalker.java:70)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:87)
        at org.apache.pig.newplan.logical.relational.LOGenerate.accept(LOGenerate.java:225)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:76)
        at org.apache.pig.newplan.logical.relational.LOForEach.accept(LOForEach.java:71)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.newplan.logical.optimizer.SchemaPatcher.transformed(SchemaPatcher.java:43)
        at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:112)
        ... 13 more

The reason is in MergeForEach rule, Pig does not add Dereference operator after deepCopy the expression plan of the second foreach. So either disable Column pruning (so we do not have extra foreach after cogroup), MergeForEach, GroupByConstParallelSetter (so we don't do a global schema regeneration) will suppress the error message. One minor issue is GroupByConstParallelSetter should not regenerate schema, since schema will not change after this rule.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Nov/10 20:05;daijy;PIG-1729-0.patch;https://issues.apache.org/jira/secure/attachment/12459726/PIG-1729-0.patch,18/Nov/10 00:57;daijy;PIG-1729-1.patch;https://issues.apache.org/jira/secure/attachment/12459861/PIG-1729-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-11-18 18:44:17.418,,,no_permission,,,,,,,,,,,,165161,Reviewed,,,Sat Nov 20 00:04:17 UTC 2010,,,,,,,0|i0gytz:,97073,,,,,,,,,,16/Nov/10 20:05;daijy;PIG-1729-0.patch is a preliminary fix.,"18/Nov/10 18:44;xuefuz;+1
Patch looks good.","20/Nov/10 00:04;daijy;test-patch result:

     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

All tests pass. 

Patch committed to both trunk and 0.8 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document UDF.html and xml SVN repository address incorrect,PIG-1728,12480115,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,ymstsn1,ymstsn1,16/Nov/10 14:09,17/Dec/10 22:46,14/Mar/19 03:07,09/Dec/10 01:07,0.7.0,0.8.0,,,,,,0.8.0,0.9.0,,,documentation,,,0,,,,,,,,,,,,,"I was looking at udf.html
And I found that SVN repository address was not right.
I think that it is better to correct an address. (maybe udf.xml , too)

[before]
svn co http://svn.apache.org/repos/asf/hadoop/pig/trunk

[after]
svn co http://svn.apache.org/repos/asf/pig/trunk",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Dec/10 23:35;chandec;pig-1728-2.patch;https://issues.apache.org/jira/secure/attachment/12465859/pig-1728-2.patch,08/Dec/10 18:58;chandec;pig-1728.patch;https://issues.apache.org/jira/secure/attachment/12465821/pig-1728.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-12-08 18:58:59.024,,,no_permission,,,,,,,,,,,,165160,,,,Thu Dec 09 01:07:35 UTC 2010,,,,,,,0|i0gytr:,97072,,,,,,,,,,"08/Dec/10 18:58;chandec;Patch file ...

Updates SVN references in udf.xml and zebra_overview.xml","08/Dec/10 18:59;chandec;Apply patch to both:
> pig 080 branch
> pig trunk (pig 090)","08/Dec/10 23:35;chandec;Path file #2 - now includes updates for tutorial.
> udf.xml 
> zebra_overview.xml
> tutorial.xml",09/Dec/10 01:07;olgan;patch committed. Thanks Corinne!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop default config override pig.properties,PIG-1727,12480012,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,daijy,daijy,15/Nov/10 23:07,17/Dec/10 22:46,14/Mar/19 03:07,17/Nov/10 18:50,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"In GenericOptionsParser work, we have these lines:

{code}
GenericOptionsParser parser = new GenericOptionsParser(args);
Configuration conf = parser.getConfiguration();    

Properties properties = new Properties();
properties.putAll(ConfigurationUtil.toProperties(conf));
PropertiesUtil.loadDefaultProperties(properties);
{code}

conf contains two sources: one from hadoop default config files (core-site.xml, hdfs-site.xml, mapred-site.xml), the other is from hadoop related command line options. Override conf over pig property files is wrong for the default hadoop configuration part. We shall not bring default hadoop configuration in. The following code illustrate how to do that:

{code}
Configuration conf = new Configuration(false);
GenericOptionsParser parser = new GenericOptionsParser(conf, args);
conf = parser.getConfiguration();
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Nov/10 01:16;rding;PIG-1727.patch;https://issues.apache.org/jira/secure/attachment/12459668/PIG-1727.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-11-16 01:09:05.964,,,no_permission,,,,,,,,,,,,165159,Reviewed,,,Wed Nov 17 18:50:13 UTC 2010,,,,,,,0|i0gytj:,97071,,,,,,,,,,16/Nov/10 01:09;rding;The workaround is to use the command line option -P to pass in additional property files. ,16/Nov/10 01:16;rding;Attaching the patch.,"16/Nov/10 03:27;chaitk;Richard,

  I see that this has more to do with the order in which properties are loaded. If we make sure that hadoop-specific default properties are loaded first, followed by the pig-specific ones, this problem should not arise. Looking at the following code snippet in Main.java :

{noformat}
        PropertiesUtil.loadDefaultProperties(properties);
        properties.putAll(ConfigurationUtil.toProperties(conf));
{noformat}

The order of loading the properties object in this case seems to be such that pig-specific default properties (i.e., properties in pig-default.properties, .pigrc and pig.properties) get loaded first, followed by the ones in hadoop-default configuration. Due to this order, some of the pig properties might get overriden. Therefore, a simple swap of the above two lines to change the order of loading of properties should solve the problem.

Thoughts?","16/Nov/10 17:33;rding;Thanks for the comment. The purpose of using GenericOptionsParser is to parse command line arguments generic to the Hadoop framework so that users can override system properties (including pig properties using -D option) from the command line.  This is the reason for the current load ordering. As Daniel pointed out, Pig doesn't need to load the hadoop-default properties at this point. The patch now only add the command line arguments to the properties.",17/Nov/10 18:32;daijy;+1 for the patch.,"17/Nov/10 18:35;daijy;To V.V.Chaitanya,
We will merge with hadoop default configurations anyway in HExecutionEngine.init(). ",17/Nov/10 18:50;rding;patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New logical plan: uidOnlySchema bug in LOGenerate,PIG-1725,12479854,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,13/Nov/10 07:07,17/Dec/10 22:46,14/Mar/19 03:07,20/Nov/10 01:08,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"The following script fail:

{code}
a = load '1.txt' as (a0:int, a1, a2:bag{});
b = foreach a generate a0 as b0, a1 as b1, flatten(a2) as b2:int;
c = filter b by b0==1;
d = foreach c generate b0+1, b2;
dump d;
{code}

Error message:
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2239: Structure of schema change. Original: null#1:int,null#2:bytearray,null#5:int Now: b0#1:int,b1#114:NULL
        at org.apache.pig.newplan.logical.relational.LogicalSchema.mergeUid(LogicalSchema.java:364)
        at org.apache.pig.newplan.logical.relational.LOGenerate.getSchema(LOGenerate.java:170)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:89)
        at org.apache.pig.newplan.logical.relational.LOGenerate.accept(LOGenerate.java:225)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:76)
        at org.apache.pig.newplan.logical.relational.LOForEach.accept(LOForEach.java:71)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.newplan.logical.optimizer.SchemaPatcher.transformed(SchemaPatcher.java:43)
        at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:112)
        ... 13 more

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Nov/10 07:20;daijy;PIG-1725-0.patch;https://issues.apache.org/jira/secure/attachment/12459521/PIG-1725-0.patch,18/Nov/10 00:48;daijy;PIG-1725-1.patch;https://issues.apache.org/jira/secure/attachment/12459858/PIG-1725-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-11-19 23:22:35.056,,,no_permission,,,,,,,,,,,,165157,Reviewed,,,Sat Nov 20 01:08:28 UTC 2010,,,,,,,0|i0gyt3:,97069,,,,,,,,,,13/Nov/10 07:20;daijy;PIG-1725-0.patch is a preliminary patch. ,19/Nov/10 23:22;rding;+1. Looks good.,"20/Nov/10 01:08;daijy;test-patch result:

     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

All tests pass.

Patch committed to both trunk and 0.8 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to limit the length of Pig counter names,PIG-1723,12479845,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,13/Nov/10 01:14,17/Dec/10 22:46,14/Mar/19 03:07,16/Nov/10 20:54,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,Some version of Hadoop now restrict the length of the names for the counters. Pig counters (e.g. multi-input/multi-output counters) needs to take this into account so that Pig can correctly retrieve its counters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Nov/10 01:20;rding;PIG-1723.patch;https://issues.apache.org/jira/secure/attachment/12459669/PIG-1723.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-11-16 18:23:28.8,,,no_permission,,,,,,,,,,,,165155,Reviewed,,,Tue Nov 16 20:54:30 UTC 2010,,,,,,,0|i0gysn:,97067,,,,,,,,,,16/Nov/10 01:20;rding;Attaching patch that limits the string size of the Pig counter name to less than 64 characters.,16/Nov/10 17:24;rding;Patch passed core test and patch test.,16/Nov/10 18:23;yanz;Patch looks good.,16/Nov/10 18:23;yanz;+1,16/Nov/10 20:54;rding;patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New logical plan: script fail when reuse foreach inner alias,PIG-1721,12479779,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,12/Nov/10 09:12,17/Dec/10 22:46,14/Mar/19 03:07,19/Nov/10 22:30,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"The following script fail:

{code}
a = load '1.txt' as (a0:int, a1:map[]);

b = filter a by a0==1;

c = FOREACH b {
        b0 = a1#'key1';
        generate ((b0 is null or b0 == '')?1:0);
}

dump c;
{code}

In the foreach inner plan, b0 is used twice. 

Error message:
java.lang.ClassCastException: org.apache.pig.data.BinSedesTuple cannot be cast to java.util.Map
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POMapLookUp.getNext(POMapLookUp.java:100)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POMapLookUp.getNext(POMapLookUp.java:117)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POIsNull.getNext(POIsNull.java:72)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POOr.getNext(POOr.java:67)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POBinCond.getNext(POBinCond.java:172)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:355)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:291)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:236)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:231)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Nov/10 18:34;daijy;PIG-1721-0.patch;https://issues.apache.org/jira/secure/attachment/12459474/PIG-1721-0.patch,18/Nov/10 00:38;daijy;PIG-1721-1.patch;https://issues.apache.org/jira/secure/attachment/12459856/PIG-1721-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-11-18 20:26:19.111,,,no_permission,,,,,,,,,,,,165154,Reviewed,,,Fri Nov 19 22:30:03 UTC 2010,,,,,,,0|i0gys7:,97065,,,,,,,,,,"12/Nov/10 18:34;daijy;PIG-1721-0.patch give the idea how to fix it. It is not a full patch. Also notice this patch is only for 0.8, cuz LogicalExpPlanMigrationVistor is gone in 0.9, I will coordinate with Xuefu to see whether to solve it in parser or LogToPhyTranslationVisitor.","17/Nov/10 02:26;daijy;Here is another test case of the same nature:
{code}
A = load 'data' AS (query:chararray, type:chararray, freq:int);
B = group A by query;

C = foreach B {

    click = filter A by (type == 'c');
    pv = filter A by (type == 'p');
    click_sum = (IsEmpty(click.freq)? 0 : SUM(click.freq));

    generate
        COUNT(click),
        COUNT(pv),
        click_sum;
}

dump C;
{code}

Error message:
java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.ArrayList.get(ArrayList.java:324)
        at org.apache.pig.data.DefaultTuple.get(DefaultTuple.java:158)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.processInputBag(POProject.java:482)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PORelationToExprProject.getNext(PORelationToExprProject.java:107)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.processInputBag(POProject.java:480)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:197)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.processInput(POUserFunc.java:160)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:212)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:289)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POBinCond.getNext(POBinCond.java:193)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:361)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:291)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.runPipeline(PigMapReduce.java:433)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.processOnePackageOutput(PigMapReduce.java:401)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:381)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:251)
        at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
        at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:566)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216)
",18/Nov/10 20:26;rding;+1,"19/Nov/10 22:30;daijy;test-patch result:

 +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

All tests pass. Patch committed to both trunk and 0.8 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.NegativeArraySizeException during Quicksort,PIG-1720,12479765,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,viraj,viraj,12/Nov/10 01:20,17/Dec/10 22:46,14/Mar/19 03:07,17/Nov/10 21:26,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"I have a simple script, which I attempt to run with Pig 0.8

The script looks like this:
{code}
clicks = load 'pairs.out' using PigStorage('\u0001') as (user,query,id);
query0 = group clicks by query;

query1 = foreach query0 {
        distinct_id = DISTINCT clicks.id; 
        distinct_user = DISTINCT clicks.user;
        count = COUNT(distinct_user);
        generate count as count,group,distinct_id; }

store query1 into 'query.out' using PigStorage('|');
{code}

The Mapper's fail in the quick sort phase with the following error message:
{quote}
java.lang.NegativeArraySizeException at org.apache.pig.data.BinInterSedes$BinInterSedesTupleRawComparator.compareBinInterSedesDatum(BinInterSedes.java:782) at org.apache.pig.data.BinInterSedes$BinInterSedesTupleRawComparator.compareBinSedesTuple(BinInterSedes.java:662) at org.apache.pig.data.BinInterSedes$BinInterSedesTupleRawComparator.compare(BinInterSedes.java:623) at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSecondaryKeyComparator.compare(PigSecondaryKeyComparator.java:78) at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.compare(MapTask.java:995) at org.apache.hadoop.util.QuickSort.sortInternal(QuickSort.java:95) at org.apache.hadoop.util.QuickSort.sort(QuickSort.java:59) at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1281) at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1182) at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:608) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:676) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:335) at org.apache.hadoop.mapred.Child$4.run(Child.java:242) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:396) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1045) at org.apache.hadoop.mapred.Child.main(Child.java:236) 
{quote}
Viraj
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Nov/10 21:41;thejas;PIG-1720.1.patch;https://issues.apache.org/jira/secure/attachment/12459731/PIG-1720.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-11-16 21:41:33.744,,,no_permission,,,,,,,,,,,,165153,Reviewed,,,Wed Nov 17 21:26:51 UTC 2010,,,,,,,0|i0gyrz:,97064,,,,,,,,,,"16/Nov/10 21:41;thejas;Fixing bug in handling of size information of serialized columns. Size is stored as unsigned byte/short when size is small enough, but the raw comparator code was reading it as signed byte/short.
The unit tests are still running.
","17/Nov/10 17:15;thejas;Unit tests have completed successfully.
",17/Nov/10 19:36;yanz;+1,"17/Nov/10 21:26;thejas;Patch committed to 0.8 branch and trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New logical plan: FieldSchema generation for BinCond is wrong,PIG-1719,12479763,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,12/Nov/10 00:39,17/Dec/10 22:46,14/Mar/19 03:07,19/Nov/10 19:54,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"New logical plan generate incomplete schema for BinCond, if the expression for BinCond is of complex type. Eg,

{code}
a = load '1.txt' as (a0, a1:int);
b = group a by a0;
c = foreach b generate FLATTEN((IsEmpty(a)?{(65535)}:sequence.FIRST(a.a1)));
{code}

The right schema for c is (int), however, in current code, we get null schema. This is because BinCond should get the schema {(int)}, but we get empty bag {}.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Nov/10 00:40;daijy;PIG-1719-0.patch;https://issues.apache.org/jira/secure/attachment/12459412/PIG-1719-0.patch,18/Nov/10 00:26;daijy;PIG-1719-1.patch;https://issues.apache.org/jira/secure/attachment/12459854/PIG-1719-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-11-18 19:29:28.728,,,no_permission,,,,,,,,,,,,165152,Reviewed,,,Fri Nov 19 19:54:21 UTC 2010,,,,,,,0|i0gyrr:,97063,,,,,,,,,,12/Nov/10 00:40;daijy;PIG-1719-0.patch demonstrate the idea to fix it. It is not a full patch yet.,18/Nov/10 19:29;rding;+1. ,"19/Nov/10 19:54;daijy;test-patch result:

     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

All tests pass.

Patch committed to both trunk and 0.8 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New logical plan: LogToPhyTranslationVisitor should translate the structure for regex optimization,PIG-1716,12479674,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,11/Nov/10 02:39,17/Dec/10 22:46,14/Mar/19 03:07,03/Dec/10 23:41,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"In [PIG-965|https://issues.apache.org/jira/browse/PIG-965], we will use an optimized regex implementation. However, in new logical plan, we didn't translate the necessary structure to support this, so we never use this optimization in new logical plan.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Nov/10 20:28;daijy;PIG-1716-1.patch;https://issues.apache.org/jira/secure/attachment/12459828/PIG-1716-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-11-18 18:22:03.411,,,no_permission,,,,,,,,,,,,165150,Reviewed,,,Fri Nov 19 22:26:35 UTC 2010,,,,,,,0|i0gyr3:,97060,,,,,,,,,,17/Nov/10 20:28;daijy;There is no obvious way to write a unit test for it. Test it manually and it does use automaton.,"18/Nov/10 18:22;thejas;+1 
Please commit once test-patch and unit tests succeed.
","19/Nov/10 22:26;daijy;test-patch result:

     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

All tests pass.

Patch committed to both trunk and 0.8 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig-withouthadoop.jar missing automaton.jar,PIG-1715,12479673,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,11/Nov/10 02:38,17/Dec/10 22:46,14/Mar/19 03:07,11/Nov/10 19:40,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"automaton.jar is used for matching  regex patterns (PIG-965) . It is not bundled in pig-withouthadoop.jar .
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11/Nov/10 16:12;thejas;PIG-1715.1.08.patch;https://issues.apache.org/jira/secure/attachment/12459362/PIG-1715.1.08.patch,11/Nov/10 16:12;thejas;PIG-1715.1.trunk.patch;https://issues.apache.org/jira/secure/attachment/12459363/PIG-1715.1.trunk.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-11-11 19:14:25.22,,,no_permission,,,,,,,,,,,,165149,Reviewed,,,Thu Nov 11 19:40:04 UTC 2010,,,,,,,0|i0gyqv:,97059,,,,,,,,,,"11/Nov/10 16:12;thejas;Patch for 0.8 branch and trunk. 
",11/Nov/10 19:14;daijy;+1,"11/Nov/10 19:40;thejas;test-patch succeeded except for the check for new test cases. No new unit test cases have been added since this is only a packaging change. Tested that matches clause works with 0.8 when old logical plan is used.  
Because of issue in PIG-1716, failures with matches clause happens only if new logical plan is turned off.

Patch committed to 0.8 branch and trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Option mapred.output.compress doesn't work in Pig 0.8 but worked in 0.7,PIG-1714,12479624,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,10/Nov/10 18:40,21/Apr/11 20:38,14/Mar/19 03:07,13/Nov/10 00:07,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"Command line options -Dmapred.output.compress and -Dmapred.output.compression.codec worked in Pig 0.7, which, when set, would compress the output, whether or not the output has an extension .gz, .bz, or .bz2. This behavior changed in 0.8 in that compression is on only if the output has such extensions. In other words, the command line options have no effect.

Pig needs to clarify the right way to enable/disable compression and implement it accordingly.

The behavior change is probably related to PIg-1533.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Nov/10 03:52;xuefuz;jira-1714-0.patch;https://issues.apache.org/jira/secure/attachment/12459421/jira-1714-0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-11-12 22:26:41.78,,,no_permission,,,,,,,,,,,,165148,Reviewed,,,Sat Nov 13 00:07:49 UTC 2010,,,,,,,0|i0gyqn:,97058,,,,,,,,,,"12/Nov/10 22:16;xuefuz;Here is the behavior that Pig is taking:

1. If JVM property ""mapred.output.compress"" is set to ""true"", then the output is always compressed (regardless of the output file extension).

2. If the JVM property ""mapred.output.compress"" is not set or is set to ""false"", then whether pig output is compressed depends on the given file extension: if the extension is .bz or .bz2, then bzip compression will be used. If the extension is gz, then gzip compression will be used. In all other cases, no compression will be performed.

3. When JVM property ""mapred.output.compress"" is set to ""true"", then another property, ""mapred.output.compress.codec"" must also be set. Otherwise, exception will be thrown.",12/Nov/10 22:26;rding;+1. Please commit when all tests pass.,12/Nov/10 22:53;xuefuz;All nightly unit test passes. Verified the fix on a real cluster and it fixes the problem as expected.,"12/Nov/10 23:46;xuefuz;     [exec] There appear to be 463 release audit warnings before the patch and 463 release audit warnings after applying the patch.
     [exec]
     [exec]
     [exec]
     [exec]
     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec]
     [exec]
     [exec]
     [exec]
     [exec] ======================================================================
     [exec] ======================================================================
     [exec]     Finished build.
     [exec] ======================================================================
     [exec] ======================================================================
     [exec]
     [exec]

BUILD SUCCESSFUL
",13/Nov/10 00:07;rding;patch committed to both trunk and 0.8 branch. Thanks Xuefu!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig Illustrate rework,PIG-1712,12479397,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yanz,yanz,yanz,08/Nov/10 18:37,24/Aug/11 03:50,14/Mar/19 03:07,18/Jan/11 02:26,0.9.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"PigPen has been found to be a usable feature. The underlying PIG functionality, ILLUSTRATE, however, has not been stable and complete. It also has unique access paths that  are not shared by the mainstream PIG call paths, which makes it hard if not impossible to maintain as the PIG evolves along.

The purpose of this work is to use the common access paths yet still follow the performance-sensitive criteria for ILLUSTRATE, complete support for as many logical operators as theoretically possible,
plus algorithm polishes and bug fixes as necessary. Details can be found in http://wiki.apache.org/pig/PigIllustrate

Pig-366 has been for the PigPen in general, and was used for the previous ILLUSTRATE work too. With this JIRA, the work on ILLUSTRATE will be separated.

This JIRA also serves as an umbrella for existing issues in ILLUSTRATE. Specifically, PIG-502, PIG-903, PIG-1066 should all fold into this issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Nov/10 01:06;yanz;illustrator_1.patch;https://issues.apache.org/jira/secure/attachment/12459122/illustrator_1.patch,10/Nov/10 19:22;yanz;illustrator_2.patch;https://issues.apache.org/jira/secure/attachment/12459267/illustrator_2.patch,16/Nov/10 16:17;yanz;illustrator_3.patch;https://issues.apache.org/jira/secure/attachment/12459709/illustrator_3.patch,29/Nov/10 18:32;yanz;illustrator_4.patch;https://issues.apache.org/jira/secure/attachment/12464888/illustrator_4.patch,06/Dec/10 17:19;yanz;illustrator_5.patch;https://issues.apache.org/jira/secure/attachment/12465392/illustrator_5.patch,13/Dec/10 17:32;yanz;illustrator_6.patch;https://issues.apache.org/jira/secure/attachment/12466152/illustrator_6.patch,17/Dec/10 17:24;yanz;illustrator_7.patch;https://issues.apache.org/jira/secure/attachment/12466471/illustrator_7.patch,17/Dec/10 20:33;yanz;illustrator_8.patch;https://issues.apache.org/jira/secure/attachment/12466496/illustrator_8.patch,23/Dec/10 17:24;yanz;illustrator_9.patch;https://issues.apache.org/jira/secure/attachment/12466903/illustrator_9.patch,,,9.0,,,,,,,,,,,,,,,,,,,2010-12-13 18:47:41.09,,,no_permission,,,,,,,,,,,,63959,,,,Tue Jan 18 02:26:18 UTC 2011,,,,,,,0|i0gyq7:,97056,,,,,,,,,,"09/Nov/10 01:06;yanz;illustrator_1 has the local Map/Reduce simulator and all illustrator-specific method in Physical Operators.

Currently, LOAD, FILTER, UNION, SORT, COGROUP are working.

JOIN is not quite working yet as there are more than necessary rows for the illustration purpose.

LIMIT, FOREACH, DISTINCT,  nested generate block in FOREACH, CROSS, script illustration are not working yet.

The manual Hudson results are as follows:

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 21 new or modified tests.
     [exec]
     [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     -1 release audit.  The applied patch generated 495 release audit warnings (more than the trunk's current 461 warnings).


The release audit warnings are all on html files.

The javadoc warnings seem to be unrelated to this patch.","10/Nov/10 19:22;yanz;illustrator_2.patch:

Addition of JOIN input pruning, plus some bug fixes.","16/Nov/10 16:17;yanz;Support for operators of logic subexpression, foreach, load, sort, plus bug fixes.","29/Nov/10 18:32;yanz;LIMIT, DISTINCT, Aggregation, SPLIT support, plus bug fixes.","06/Dec/10 17:19;yanz;augmentation for DISTINCT, script support, code cleanups;","13/Dec/10 17:32;yanz;code cleanups, scalar fix, multi-query fix.","13/Dec/10 18:47;rding;
+1. The patch looks good. It fixed various issues about ILLUSTRATE command. It also added new feature that allows ILLUSTRATE of Pig Latin scripts:

{code}
illustrate -script <script name>
{code}

It still needs to remove the dependency on the old logical operators in a late date since those operators will be deprecated in the 0.9 release.

Please commit once all tests pass.
 ",13/Dec/10 19:27;yanz;illustrator_6.patch committed to the trunk. It passes test-patch and test-core.,"17/Dec/10 17:24;yanz;Use of new logical operator, plus bug fixes.",17/Dec/10 20:33;yanz;some code cleanup.,17/Dec/10 20:33;yanz;test-patch and test-core pass cleanly.,17/Dec/10 20:58;rding;+1,17/Dec/10 22:42;yanz;illustrator_8 committed to the trunk.,"23/Dec/10 17:24;yanz;Join augmentation added, code cleanups.",23/Dec/10 17:56;rding;+1,23/Dec/10 18:16;yanz;illustrator_9 committed to the trunk.,18/Jan/11 02:26;olgan;major work is done here. We will track discovered bugs separately,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document BinStorage behaviour ,PIG-1711,12479280,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,viraj,viraj,06/Nov/10 00:53,04/Aug/11 00:34,14/Mar/19 03:07,11/Feb/11 21:33,0.6.0,0.7.0,,,,,,0.9.0,,,,documentation,,,0,,,,,,,,,,,,,"We need to document some features of BinStorage that can cause indeterminate results.

I have a Pig script of this type:

{code}
raw = load 'sampledata' using BinStorage() as (col1,col2, col3);
--filter out null columns
A = filter raw by col1#'bcookie' is not null;

B = foreach A generate col1#'bcookie'  as reqcolumn;
describe B;
--B: {regcolumn: bytearray}
X = limit B 5;
dump X;

B = foreach A generate (chararray)col1#'bcookie'  as convertedcol;
describe B;
--B: {convertedcol: chararray}
X = limit B 5;
dump X;

{code}

The first dump produces:

(36co9b55onr8s)
(36co9b55onr8s)
(36hilul5oo1q1)
(36hilul5oo1q1)
(36l4cj15ooa8a)

The second dump produces:
()
()
()
()
()


So we need to write correct documentation on why this happens. One good explanation seems to be:

According to Alan:

BinStorage should not track data lineage. In the case where Pig is using BinStorage (or whatever) for moving data between MR jobs then Pig can figure out the correct cast function to use and apply it. For cases such as the one here where users are storing data using BinStorage and then in a separate Pig Latin script reading it (and thus loosing the type information) it is the users responsibility to correctly cast the data before storing it in BinStorage.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-01-18 16:59:13.289,,,no_permission,,,,,,,,,,,,66197,,,,Fri Feb 11 21:33:21 UTC 2011,,,,,,,0|i0gypz:,97055,,,,,,,,,,"18/Jan/11 16:59;olgan;Here is what we need to document:

Pig uses BinStorage? to store/load data generated between Map-Reduce jobs. Also, occasionally, users store their data using BinStorage?. Because this is a proprietory binary format, the original data is never in BinStorage? - it is always a derivation of some other data.

We have seen several examples of users doing something like this:

a = load 'b.txt' as (id, f);
b = group a by id;
store b into 'g' using BinStorage();

And then later:

a = load 'g/part*' using BinStorage() as (id, d:bag{t:(v, s)});
b = foreach a generate (double)id, flatten(d);
dump b;

There is a problem with this sequence of events. The first script does not define data types and, as the result, the data is stored as a bytearray and a bug with tuple with two bytearrays. The second script attempts to cast the bytearray to double; however, since the data originated from a different loader, it has no way to know the format of the bytearray or how to cast it to a different type. Pig 0.9 addresses this issue in 2 different ways:

    * By giving a meaningful error message when the second script is executed: ""ERROR 1118: Cannot convert bytes load from BinStorage?""
    * By allowing the user to provide a converter to use during casting. 

a = load 'g/part*' using BinStorage('Utf8StorageConverter') as (id, d:bag{t:(v, s)});
b = foreach a generate (double)id, flatten(d);
dump b;
","11/Feb/11 21:33;chandec;Built In Functions doc updated. 

BinStorage section updated with new information.

Patch will be submitted under Pig-1772.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document limitation on how many aliases cogroup can handle and why users should not cogroup more than 127 aliases,PIG-1710,12479274,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,viraj,viraj,05/Nov/10 23:45,04/Aug/11 00:34,14/Mar/19 03:07,16/Dec/10 22:44,,,,,,,,0.9.0,,,,documentation,,,0,,,,,,,,,,,,,"We have a usecase in Pig where we cogroup on more than 2000 aliases.

{code}
cogroup_1 = foreach(cogroup A, B, C ... more than 2000 aliases ) generate flatten (udf(....));
{code}

But we found out that there is a limitation on how many aliases a cogroup can handle which is aound 127 

So we had to workaround this by using 10-15 batches of 127 cogroups.

{code}
cogroup_1 = foreach(cogroup A, B, C ... 127 aliases ) generate flatten (udf(....));
cogroup_2 = foreach(cogroup A, B, C ... 127 aliases ) generate flatten (udf(....));
...
cogroup_15 = foreach(cogroup A, B, C ... 127 aliases ) generate flatten (udf(....));
{code}

Is there some documentation on this?

Viraj",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-12-16 22:44:14.749,,,no_permission,,,,,,,,,,,,66332,,,,Thu Dec 16 22:44:44 UTC 2010,,,,,,,0|i0gypr:,97054,,,,,,,,,,"16/Dec/10 22:44;chandec;Updated GROUP operator (COGROUP operator points to GROUP operator).
Fix submitted via PIG-1768.",16/Dec/10 22:44;chandec;Marking resolved.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skewed join use fewer reducer for extreme large key,PIG-1709,12479259,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,05/Nov/10 21:11,17/Dec/10 22:46,14/Mar/19 03:07,09/Dec/10 00:17,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"In skewed join, we use PartitionSkewedKeys to calculate number of reducers needed for a single key. If the result if larger than the number of total reducers, we will round it with reducer#. Eg, if Pig calculates that we need 12 reducers to hold a key in memory, and total reducers for this job is 10, we then allocate 2 reducers to this key; We shall use all 10 reducers in this case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Nov/10 19:51;daijy;PIG-1709-1.patch;https://issues.apache.org/jira/secure/attachment/12459273/PIG-1709-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-12-07 18:40:26.449,,,no_permission,,,,,,,,,,,,165147,Reviewed,,,Thu Dec 09 00:17:04 UTC 2010,,,,,,,0|i0gypj:,97053,"In skewed join, if one large key requires more reducers than available, we give it all the available reducers.",,,,,,,,,"05/Nov/10 23:10;daijy;As Thejas point out, the right approach and original design is to fail over in such case. This is broken and we should fix it.","10/Nov/10 19:51;daijy;Current code will issue a warning in case we don't have enough reducers. However, the warning message is easily get ignored, and skewed join will continue with something very wrong. The attached patch will use all available reducers in this case, and we will still see warning message.",07/Dec/10 18:40;rding;+1,"09/Dec/10 00:17;daijy;test-patch:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

unit test:
    all pass

end-to-end test:
    all pass

Patch committed to both trunk and 0.8 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New logical plan: PushDownFlattenForEach fail if flattened field has user defined schema,PIG-1706,12478748,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,31/Oct/10 02:29,17/Dec/10 22:46,14/Mar/19 03:07,01/Nov/10 22:57,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"The following script fail:

{code}
a = load '1.txt' as (a0:int, a1, a2:bag{t:(i1:int, i2:int)});
b = load '2.txt' as (b0:int, b1);
c = foreach a generate a0, flatten(a2) as (q1, q2);
d = join c by a0, b by b0;
explain d;
{code}

Error message:
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1067: Unable to explain alias d
        at org.apache.pig.PigServer.explain(PigServer.java:958)
        at org.apache.pig.tools.grunt.GruntParser.explainCurrentBatch(GruntParser.java:353)
        at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:285)
        at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:248)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.Explain(PigScriptParser.java:605)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:327)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:165)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:141)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:90)
        at org.apache.pig.Main.run(Main.java:498)
        at org.apache.pig.Main.main(Main.java:107)
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2042: Error in new logical plan. Try -Dpig.usenewlogicalplan=false.
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:308)
        at org.apache.pig.PigServer.compilePp(PigServer.java:1354)
        at org.apache.pig.PigServer.explain(PigServer.java:927)
        ... 10 more
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2000: Error processing rule PushDownForEachFlatten. Try -t PushDownForEachFlatten
        at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:120)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:276)
        ... 12 more
Caused by: java.lang.NullPointerException
        at org.apache.pig.newplan.logical.relational.LOGenerate.getSchema(LOGenerate.java:145)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:89)
        at org.apache.pig.newplan.logical.relational.LOGenerate.accept(LOGenerate.java:225)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:76)
        at org.apache.pig.newplan.logical.relational.LOForEach.accept(LOForEach.java:71)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.newplan.logical.optimizer.SchemaPatcher.transformed(SchemaPatcher.java:43)
        at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:112)
        ... 13 more
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,31/Oct/10 02:30;daijy;PIG-1706-1.patch;https://issues.apache.org/jira/secure/attachment/12458500/PIG-1706-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-11-01 22:27:20.214,,,no_permission,,,,,,,,,,,,165144,Reviewed,,,Mon Nov 01 22:57:55 UTC 2010,,,,,,,0|i0gyov:,97050,,,,,,,,,,"01/Nov/10 22:27;thejas;+1
",01/Nov/10 22:57;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New logical plan: self-join fail for some queries,PIG-1705,12478747,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,31/Oct/10 02:06,17/Dec/10 22:46,14/Mar/19 03:07,01/Nov/10 22:53,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"The following script fail: 

{code}
A = load '1.txt' AS (a0:int, a1:int, a2:int);
B = foreach A generate a0, a1;
C = join A by a0, B by a0;
D = filter C by A::a1>=B::a1;
dump D;
{code}

Error message:
org.apache.pig.backend.executionengine.ExecException: ERROR 2067: GTOrEqualToExpr does not know how to handle type: tuple
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.GTOrEqualToExpr.getNext(GTOrEqualToExpr.java:132)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter.getNext(POFilter.java:148)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:276)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:240)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:276)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNext(POLocalRearrange.java:256)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POUnion.getNext(POUnion.java:163)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:236)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:231)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,31/Oct/10 02:10;daijy;PIG-1705-1.patch;https://issues.apache.org/jira/secure/attachment/12458499/PIG-1705-1.patch,01/Nov/10 16:39;daijy;PIG-1705-2.patch;https://issues.apache.org/jira/secure/attachment/12458553/PIG-1705-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-11-01 22:44:28.737,,,no_permission,,,,,,,,,,,,165143,Reviewed,,,Mon Nov 01 22:53:15 UTC 2010,,,,,,,0|i0gyon:,97049,,,,,,,,,,"31/Oct/10 02:08;daijy;The problem is because two branches share the same uid set. When we do join, uid get conflict. To solve it, we force split (include implicit split) generate a new set of uids for each branch.",01/Nov/10 16:39;daijy;Fix release audit warning.,01/Nov/10 22:44;thejas;+1,"01/Nov/10 22:48;daijy;test-patch result:
     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

javadoc warning is not related to the patch. Maybe a bug in test-patch. All tests pass.",01/Nov/10 22:53;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Output Compression is not at work if the output path is absolute and there is a trailing / afte the compression suffix,PIG-1704,12478690,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yanz,yanz,yanz,29/Oct/10 19:04,17/Dec/10 22:46,14/Mar/19 03:07,01/Nov/10 17:52,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"For example, if the output path is /path/to/output.bz2/, the files generated are not compressed at all. This is new in 0.8 as the patch of PIG-1378 changes the call path.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,31/Oct/10 21:19;yanz;PIG-1704.patch;https://issues.apache.org/jira/secure/attachment/12458526/PIG-1704.patch,29/Oct/10 19:33;yanz;PIG-1704.patch;https://issues.apache.org/jira/secure/attachment/12458442/PIG-1704.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-11-01 16:37:24.557,,,no_permission,,,,,,,,,,,,165142,,,,Mon Nov 01 17:52:21 UTC 2010,,,,,,,0|i0gyof:,97048,,,,,,,,,,31/Oct/10 21:19;yanz;Addition of a few existing test fixes.,"01/Nov/10 03:58;yanz;test-core passes. 

test-patch results: 

 [exec] -1 overall. 
     [exec] 
     [exec] +1 @author. The patch does not contain any @author tags. 
     [exec] 
     [exec] +1 tests included. The patch appears to include 9 new or modified tests. 
     [exec] 
     [exec] -1 javadoc. The javadoc tool appears to have generated 1 warning messages. 
     [exec] 
     [exec] +1 javac. The applied patch does not increase the total number of javac compiler warnings. 
     [exec] 
     [exec] +1 findbugs. The patch does not introduce any new Findbugs warnings. 
     [exec] 
     [exec] +1 release audit. The applied patch does not increase the total number of release audit warning 

The javadoc warning is : 

/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigOutputCommitter.java:63: warning - @param argument ""list2"" is not a parameter name. 
/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigOutputCommitter.java:63: warning - @param argument ""list"" is not a parameter name. 

Neither is from this patch. ","01/Nov/10 16:37;daijy;+1. javadoc warning is not related to the patch. Actually test-patch complains about every patch now, seems a bug in test-patch javadoc comparison. Please commit to both 0.8 branch and trunk.",01/Nov/10 17:52;yanz;Committed to the trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming debug output outputs null input-split information,PIG-1702,12478288,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,awarring,awarring,awarring,26/Oct/10 02:04,26/Apr/12 20:32,14/Mar/19 03:07,17/Jun/11 00:24,0.7.0,,,,,,,0.10.0,,,,impl,,,0,,,,,,,,,,,,,"Within the Pig streaming command execution, debug information is printed out to stderr which specified the input file, as well as split information. The function is org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager.writeDebugHeader(). Pig 0.7 outputs null for the split file, and -1 for the split start-offset and split length. Example output:

===== Task Information Header =====
Command: test.pl (stdin-org.apache.pig.builtin.PigStreaming/stdout-org.apache.pig.builtin.PigStreaming)
Start time: Mon Oct 25 21:24:45 EDT 2010
Input-split file: null
Input-split start-offset: -1
Input-split length: -1

Within the writeDebugHeader() function, the input file information is obtained by querying for the ""map.input.file"" configuration variable. This configuration variable was set by the old hadoop m/r api, but not by the 0.20 api, which Pig 0.7 now uses. The new way to get this information is with something like: ((FileSplit) context.getInputSplit).getPath(). See HADOOP-5973.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Apr/11 18:27;awarring;PIG-1702-0.patch;https://issues.apache.org/jira/secure/attachment/12475395/PIG-1702-0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-10-26 03:56:12.102,,,no_permission,,,,,,,,,,,,71082,Reviewed,,,Fri Jun 17 00:24:01 UTC 2011,,,,,,,0|i0gynz:,97046,,,,,,,,,,"26/Oct/10 03:56;ashutoshc;@Adam,

Nice catch. Would you like to contribute a patch for it?","26/Oct/10 14:06;awarring;Yea, I'd like to do that. I'll try to get one up very soon.",15/Mar/11 20:00;alexvk;Any update on this?,"04/Apr/11 18:27;awarring;Here is a patch that fixes the Header output by retrieving the information (the path, start offset, and length) from the FileSplit. 

One potential issue with this code is that it has to gain a reference to the current MapContext, which it does from PigMapReduce.sJobContext, and if PIG is running in local mode, there may be a race condition. PIG-1831 solved a similar issue with the configuration. Would it be wise to use a thread local variable in PigMapReduce for the context as well?","04/Apr/11 19:42;awarring;Review request can be found here:

https://reviews.apache.org/r/547/","15/Jun/11 21:17;olgan;Delaying till 10 since we are about to spin the release.

Can one of the committers review post 0.9? thanks",17/Jun/11 00:19;daijy;Patch looks fine. Will commit it shortly.,17/Jun/11 00:24;daijy;Patch committed to trunk. Thanks Adam!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException if log4j.properties is Used,PIG-1697,12478225,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,laukik,ranjit,ranjit,25/Oct/10 11:26,04/Aug/11 00:35,14/Mar/19 03:07,15/Apr/11 18:17,0.8.0,,,,,,,0.9.0,,,,impl,,,0,logging,,,,,,,,,,,,"If I use a {{log4j.properties}} _without_ the magical line:
bq. log4j.logger.org.apache.pig=WARN, MyAppender
Pig 0.8.0 crashes on me and I get:
bq. Details at logfile: /home/ranjit/src/Pig/stage/pig-0.8.0-SNAPSHOT/pig_1288005234464.log
This file contains:
{quote}
Error before Pig is launched
----------------------------
ERROR 2999: Unexpected internal error. null

java.lang.NullPointerException
	at org.apache.pig.Main.configureLog4J(Main.java:605)
	at org.apache.pig.Main.run(Main.java:337)
	at org.apache.pig.Main.main(Main.java:107)
================================================================================
{quote}

Line #605 in {{Main.java}} is:
bq. backendProps.setProperty(""log4j.logger.org.apache.pig.level"", logLevel.toString());
and it turns out that {{logLevel}} is NULL in this case. That in turn is because line #603 contains:
bq. logLevel = Logger.getLogger(""org.apache.pig"").getLevel();

I believe we should use {{Logger.getEffectiveLevel()}} instead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Feb/11 01:48;laukik;PIG-1697.patch;https://issues.apache.org/jira/secure/attachment/12471452/PIG-1697.patch,09/Feb/11 23:09;charles.fg;pig-issue-1697.tar.gz;https://issues.apache.org/jira/secure/attachment/12470735/pig-issue-1697.tar.gz,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-02-09 22:56:48.934,,,no_permission,,,,,,,,,,,,66113,Reviewed,,,Fri Apr 15 18:17:40 UTC 2011,,,,,,,0|i0gymf:,97039,,,,,,,,,,26/Nov/10 05:44;ranjit;I can't reproduce this issue any more on the Pig 0.8.0 branch.,"09/Feb/11 22:56;charles.fg;Guys I'm gotting the exactly same issue.

I will attach some files and scripts that could help reproduce those errors.
I don't know the best approach to reopen the issue. 
If someone could help in this.
Thanks!!!

","09/Feb/11 23:09;charles.fg;I attached anything that you will need to reproduce this error.

",11/Feb/11 02:52;daijy;I reopen the ticket. Thanks Charles!,"19/Feb/11 01:48;laukik;I have tested this manually by passing the log4j properties file at command line. I did not add a test case due to the effort required in putting a framework in place to simulate files passed at command line (and hence the overall -1)

I have verified that all unit tests pass.

{noformat}

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec]
{noformat}
",22/Feb/11 18:47;daijy;+1. Hold committing after we check in new parser changes.,15/Apr/11 18:17;daijy;Patch committed to trunk. Thanks Laukik!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MergeForEach does not carry user defined schema if any one of the merged ForEach has user defined schema,PIG-1695,12478124,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,22/Oct/10 23:01,17/Dec/10 22:46,14/Mar/19 03:07,01/Nov/10 16:46,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"The following script missing the column name i after merge.

{code}
a = load 'num.txt' as (i);
b = foreach a generate (int)i;
c = foreach b generate i + 60 as i;
store c into 'sectest';
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Oct/10 23:01;daijy;PIG-1695-1.patch;https://issues.apache.org/jira/secure/attachment/12457882/PIG-1695-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-10-29 23:14:20.701,,,no_permission,,,,,,,,,,,,165138,Reviewed,,,Mon Nov 01 16:46:23 UTC 2010,,,,,,,0|i0gylj:,97035,,,,,,,,,,"24/Oct/10 20:57;daijy;test-patch result:

     [exec] 
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

All tests pass.","29/Oct/10 23:14;thejas;+1

With this fix, pig does not merge the foreach statements if first foreach has a user defined schema. We should consider supporting that case as well (in a separate jira), because users sometimes add a foreach statement just to give convenient names for columns . 
eg- 
{code}
J = join A by col1, B by col1
F1 = foreach J generate A::col1 as Acol1 , A::col2 as col2, B::col1 as Bcol1; -- foreach that has been added just to give convenient names for expressions.
F2 = foreach F1 generate Acol1+col2, Bcol1 + col2 ;
{code}",01/Nov/10 16:46;daijy;Patch committed to both trunk and 0.8 branch. Future improvement is possible as per Thejas's comment. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
union-onschema projects null schema at parsing stage for some queries,PIG-1694,12478121,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,22/Oct/10 21:39,17/Dec/10 22:46,14/Mar/19 03:07,28/Oct/10 00:50,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"If the input relations of union-onschema have different aliases in the schemas, a subsequent statement that refers to a column alias in the union fails.

eg
{code}
grunt> l1 = load 'x' as (a);   
grunt> l2 = load 'x' as (a,b); 
grunt> u = union onschema l1,l2;
grunt> o = order u by a;        
2010-10-22 14:33:56,086 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Invalid alias: a in null
Details at logfile: /Users/tejas/pig-0.8/branch-0.8/pig_1287783217811.log
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Oct/10 21:42;thejas;PIG-1694.1.patch;https://issues.apache.org/jira/secure/attachment/12458006/PIG-1694.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-10-28 00:17:40.218,,,no_permission,,,,,,,,,,,,165137,Reviewed,,,Thu Oct 28 00:50:11 UTC 2010,,,,,,,0|i0gykv:,97032,,,,,,,,,,"25/Oct/10 21:42;thejas;Patch passes unit tests and test-patch .
The merging of schema for union-onschema has been moved to LOUnion.getSchema(). This way a schema is always available for the union-onschema during the parsing stage. The correct datatypes will be available in the schema returned by LOUnion.getSchema() after TypeChecking is complete.
",28/Oct/10 00:17;daijy;+1,"28/Oct/10 00:50;thejas;Patch committed to trunk and 0.8 branch.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig is unable to handle counters for glob paths ?,PIG-1685,12477657,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,mridulm@yahoo-inc.com,mridulm@yahoo-inc.com,18/Oct/10 18:21,17/Dec/10 22:46,14/Mar/19 03:07,20/Oct/10 18:12,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"We get the following exception, which seems to be related to processing counters per path :


java.net.URISyntaxException: Illegal character in path at index 71: $path_prefix/20100830_cdxcore_10.7_{00,01,02,03,04,05,06,07,08}.bz2
        at java.net.URI$Parser.fail(URI.java:2809)
        at java.net.URI$Parser.checkChars(URI.java:2982)
        at java.net.URI$Parser.parseHierarchical(URI.java:3066)
        at java.net.URI$Parser.parse(URI.java:3024)
        at java.net.URI.<init>(URI.java:578)
        at org.apache.pig.tools.pigstats.PigStatsUtil.getMultiInputsCounterName(PigStatsUtil.java:128)
        at org.apache.pig.tools.pigstats.JobStats.addInputStatistics(JobStats.java:523)
        at org.apache.pig.tools.pigstats.PigStatsUtil.accumulateSuccessStatistics(PigStatsUtil.java:340)
        at org.apache.pig.tools.pigstats.PigStatsUtil.accumulateStats(PigStatsUtil.java:249)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:315)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:301)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1197)
        at org.apache.pig.PigServer.execute(PigServer.java:1189)
        at org.apache.pig.PigServer.access$100(PigServer.java:127)
        at org.apache.pig.PigServer$Graph.execute(PigServer.java:1513)
        at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1560)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:522)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:868)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:386)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:165)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:141)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:90)
        at org.apache.pig.Main.run(Main.java:498)
        at org.apache.pig.Main.main(Main.java:107)




It results is (notice -1) among others:

Input(s):
Successfully read -1 records from: ""$path_prefix/20100830_cdxcore_10.7_{00,01,02,03,04,05,06,07,08}.bz2""



Thankfully, there is no direct functional impact except for inability to depend on counters.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Oct/10 21:03;daijy;PIG-1685-1.patch;https://issues.apache.org/jira/secure/attachment/12457594/PIG-1685-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-10-20 17:06:05.639,,,no_permission,,,,,,,,,,,,165131,Reviewed,,,Tue Oct 26 07:55:07 UTC 2010,,,,,,,0|i0gyi7:,97020,,,,,,,,,,20/Oct/10 17:06;thejas;+1,"20/Oct/10 18:08;daijy;test-patch result:

     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

All unit tests pass.",20/Oct/10 18:12;daijy;Patch committed to both trunk and 0.8 branch.,"20/Oct/10 20:25;mridulm@yahoo-inc.com;Thanks guys, that was real quick !",26/Oct/10 07:55;mridulm@yahoo-inc.com;Modified description to remove internal path ... my mistake.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent usage of store func.,PIG-1684,12477552,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,mridulm@yahoo-inc.com,mridulm@yahoo-inc.com,17/Oct/10 01:58,17/Dec/10 22:46,14/Mar/19 03:07,29/Oct/10 03:02,0.7.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"

Pig seems to be using multiple instances of StoreFuncInterface in the reducer inconsistently.
Some hadoop api calls are made to one instance and others made to other : which makes state management very inconsistent and is requiring hacks on our part to deal with it.


The call snippet below should hopefully indicate the issue.
The format is :

Instance.toString()   method_call.


com.yahoo.psox.fish.pig.IndexJoinStore@1be4777 getOutputFormat()
com.yahoo.psox.fish.pig.IndexJoinStore@1be4777 getOutputCommitter
com.yahoo.psox.fish.pig.IndexJoinStore@1be4777 setupTask
com.yahoo.psox.fish.pig.IndexJoinStore@1be4777 init
com.yahoo.psox.fish.pig.IndexJoinStore@1429cb2 getOutputFormat()
com.yahoo.psox.fish.pig.IndexJoinStore@1429cb2 getRecordWriter
com.yahoo.psox.fish.pig.IndexJoinStore@1429cb2 init
com.yahoo.psox.fish.pig.IndexJoinStore@1429cb2 putNext()
... 
com.yahoo.psox.fish.pig.IndexJoinStore@1be4777 needsTaskCommit
com.yahoo.psox.fish.pig.IndexJoinStore@1be4777 commitTask
com.yahoo.psox.fish.pig.IndexJoinStore@1be4777 finish()



As is obvious, two instances are used for different purposes - one to get the record writer and do the actual write, and another to call the OutputCommitter and its methods.
Since they are from different instances (StoreFuncInterface), the output committer is unable to gracefully commit and cleanup.


I am not attaching the StoreFunc, but any user defined StoreFunc will exhibit this behavior.","
A custom StoreFuncInterface used to store data at the reducer.
(Output of a group )

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Oct/10 21:51;thejas;PIG-1684.1.patch;https://issues.apache.org/jira/secure/attachment/12457876/PIG-1684.1.patch,01/Nov/10 22:48;thejas;javadoc.patch;https://issues.apache.org/jira/secure/attachment/12458589/javadoc.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-10-17 09:49:05.41,,,no_permission,,,,,,,,,,,,165130,Reviewed,,,Mon Nov 01 22:53:44 UTC 2010,,,,,,,0|i0gyhz:,97019,,,,,,,,,,"17/Oct/10 09:49;ashutoshc; I fixed  this multiple instantiations problem for loadfuncs in PIG-1363.  It needs to be fixed for storefuncs as well. I see no reason why same instance of storeFunc cant be shared across PigOutputCommitter and PigOutputFormat. Ideal solution will be to instantiate both loadfunc and storefunc exactly once on client side (during logical planning) and then ship it to the backend where this same instance is continued to be used. But that will require these interfaces to implement Serializable which will break backward compatibility. But atleast we need to make sure that they are instantiated exactly twice once in frontend and once in backend. As evident here, storefunc is getting instantiated multiple times in backend. 

Mridul, 
It will be great if you can provide a strip down version of your storeFunc, that will make it easier to write a unit test for it.","17/Oct/10 17:11;ashutoshc;Actually what I said above is not entirely correct. OutputCommitter runs as a separate task at the end of job. This separate task can run on any node of cluster so there needs to be a separate instantiation of storefunc in output committer then from outputformat. As mandated by mapreduce framework Pig (and thus user's storefunc) should not maintain state between  format and committer. Mridul, if I understand your use case correctly that is what you are trying to do. I dont see any straight forward workaround if thats the usecase. It will help if you can briefly explain what state you are wanting to maintain between different storefunc functions. ","17/Oct/10 22:07;mridulm@yahoo-inc.com;I am not sure if I understand the comments properly. To elaborate, the usecase is fairly straightforward use of the interfaces.


Our store func does task specific initializations (like create ""$OUTPUT/_temporary/attempt_id"" directory, etc). creates side files based on the data to be stored (task specific) , etc.
The output committer flushes the various streams, ensures graceful close of resources, reconciles the final output, moves the appropriate files/directories. etc - once the task is done, or cleans up if there is an abort.


This information is, as is obvious, task specific for a store func : and tightly coupled to the instance of store func used (since only the store func has visibility to this data).


Please note that, in the bug description above, different instances of the Store func's are getting called on the SAME task - not across tasks or at frontend/backend.


We are ok with pig serializing/deserializing the objects, managing its split lifecycle, etc - but at a given task, if it used a single instance of StoreFunc for all invocations consistently : to get Output format, to get record reader, to initialize, to write tuples and to commit : it would be consistent with the way we would expect the invocations (IIRC, hadoop does it this way, but I will need to recheck if there are concerns there).

Hopefully this clarifies things, please let me know in case there are issues with the way we are making use of the interfaces/our expectation of the behavior.","22/Oct/10 21:51;thejas;In patch, POStore creates only one instance of StoreFunc. PigOutputFormat, PigOutputCommitter classes create only one instance of POStore for each store.

","26/Oct/10 17:24;thejas;Patch passes unit tests and test-patch .
",28/Oct/10 22:05;daijy;+1,"29/Oct/10 03:02;thejas;Patch committed to 0.8 branch and trunk.
","01/Nov/10 22:48;thejas;The original patch introduced javadoc warnings. (I thought I had run test-patch successfully)
 javadoc.patch fixes it.
",01/Nov/10 22:53;daijy;+1 for javadoc changes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New logical plan: Nested foreach plan fail if one inner alias is refered more than once,PIG-1683,12477529,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,16/Oct/10 05:47,11/May/11 17:26,14/Mar/19 03:07,19/Oct/10 19:55,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"The following script fail:

{code}
a = load '1.txt' as (a0, a1, a2);
b = load '2.txt' as (b0, b1);
c = join a by a0, b by b0;
d = foreach c {
    d0 = a::a0;
    d1 = a::a1;
    generate ((d0 is not null)? d0 : d1);
}
explain d;
{code}

Stack:
ERROR 2015: Invalid physical operators in the physical plan

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1067: Unable to explain alias d
        at org.apache.pig.PigServer.explain(PigServer.java:957)
        at org.apache.pig.tools.grunt.GruntParser.explainCurrentBatch(GruntParser.java:353)
        at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:285)
        at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:248)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.Explain(PigScriptParser.java:605)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:327)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:165)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:141)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:90)
        at org.apache.pig.Main.run(Main.java:498)
        at org.apache.pig.Main.main(Main.java:107)
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2042: Error in new logical plan. Try -Dpig.usenewlogicalplan=false.
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:308)
        at org.apache.pig.PigServer.compilePp(PigServer.java:1350)
        at org.apache.pig.PigServer.explain(PigServer.java:926)
        ... 10 more
Caused by: org.apache.pig.backend.hadoop.executionengine.physicalLayer.LogicalToPhysicalTranslatorException: ERROR 2015: Invalid physical operators in the physical plan
        at org.apache.pig.newplan.logical.expression.ExpToPhyTranslationVisitor.visit(ExpToPhyTranslationVisitor.java:474)
        at org.apache.pig.newplan.logical.expression.BinCondExpression.accept(BinCondExpression.java:82)
        at org.apache.pig.newplan.ReverseDependencyOrderWalker.walk(ReverseDependencyOrderWalker.java:70)
        at org.apache.pig.newplan.logical.relational.LogToPhyTranslationVisitor.visit(LogToPhyTranslationVisitor.java:519)
        at org.apache.pig.newplan.logical.relational.LOForEach.accept(LOForEach.java:71)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:295)
        ... 12 more
Caused by: org.apache.pig.impl.plan.PlanException: ERROR 0: Attempt to give operator of type org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject multiple outputs.  This operator does not support multiple outputs.
        at org.apache.pig.impl.plan.OperatorPlan.connect(OperatorPlan.java:180)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan.connect(PhysicalPlan.java:133)
        at org.apache.pig.newplan.logical.expression.ExpToPhyTranslationVisitor.visit(ExpToPhyTranslationVisitor.java:470)
        ... 19 more
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Oct/10 08:06;daijy;PIG-1683-1.patch;https://issues.apache.org/jira/secure/attachment/12457427/PIG-1683-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-10-19 18:05:15.783,,,no_permission,,,,,,,,,,,,165129,Reviewed,,,Wed May 11 17:26:55 UTC 2011,,,,,,,0|i0gyhj:,97017,,,,,,,,,,18/Oct/10 18:11;daijy;All unit tests and end-to-end tests pass.,"18/Oct/10 19:09;daijy;test-patch result:

     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.","19/Oct/10 18:05;thejas;Comment on the patch - The unused 'Set<Operator> seen' in ReverseDependencyOrderWalkerWOSeenChk should be removed . Otherwise +1.
",19/Oct/10 19:55;daijy;Patch committed to both trunk and 0.8 branch.,"11/May/11 15:07;thomas11;I found a strange problem that looks like a special case of this issue. Apologies if it isn't.

I wanted to use REGEX_EXTRACT in a nested generate block where I clean up some strings. Pig accepts or rejects the block depending on the order of the ""is null"" condition. The simplest example I could come up with that shows the problem is this:

{noformat} 
a = load '1.txt' using PigStorage(',') as (a0:chararray, a1:chararray);
b = foreach a {
    b0 = TRIM(a0);
    b1 = REGEX_EXTRACT(b0, '^\\((.+)\\)$', 1);
    generate ((b1 is null) ? b0 : b1) as cleaned_name; -- FAILS
    -- generate ((b1 is not null) ? b1 : b0) as cleaned_name; -- SUCCEEDS
    -- generate ((b1 is null) ? b0 : b1); -- FAILS
}
store b into 'out';
{noformat}

1.txt is

{noformat}
foo1,bar1
 (foo2),bar2
{noformat}

The ""b is null"" variant fails with the original error message of this issue: ""Attempt to give operator of type org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject multiple outputs. This operator does not support multiple outputs.""

The inverted, logically equivalent ""b is not null"" variant succeeds.

If I replace the REGEX_EXTRACT call with a simple expression like ""b1 = a0"", it works. But the way I read the Pig Latin reference, it should be allowed at this point since it's not a relational operator?","11/May/11 15:09;thomas11;Sorry, I forgot: this is with Pig 0.8.1 and the included Hadoop.","11/May/11 17:26;daijy;Tried it with 0.8.1, even use old logical plan (-Dpig.usenewlogicalplan=true), the issue is the same. However, 0.9 fixed this issue. Seems to be a problem in the old parser and 0.9 new parser fix the issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
modify the repository path of pig artifacts to org/apache/pig in stead or org/apache/hadoop/pig,PIG-1677,12477077,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nrai,nrai,nrai,11/Oct/10 21:53,17/Dec/10 22:46,14/Mar/19 03:07,26/Oct/10 21:00,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"With pig becoming the TLP, we need to modify the pig artifacts directory path to org/apache/pig from org/apache/hadoop/pig in the maven respository.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11/Oct/10 22:49;nrai;PIG-1677_0.patch;https://issues.apache.org/jira/secure/attachment/12456907/PIG-1677_0.patch,22/Oct/10 17:12;nrai;PIG-1677_1.patch;https://issues.apache.org/jira/secure/attachment/12457850/PIG-1677_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-10-20 22:51:35.759,,,no_permission,,,,,,,,,,,,165125,,,,Fri Oct 22 22:46:16 UTC 2010,,,,,,,0|i0gyfj:,97008,,,,,,,,,,"20/Oct/10 22:51;gkesavan;looked at the maven naming conventions for group-id 
http://maven.apache.org/guides/mini/guide-naming-conventions.html

and it looks like we should have the groupd id as org.apache.pig.  ","22/Oct/10 21:54;gkesavan;PIG-1677_1.patch ; tested the path by uploading artifacts to the staging and the snapshot repos.

+1",22/Oct/10 22:46;olgan;patch committed to trunk and branch 0.8. Thanks Niraj and Giri.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
query with consecutive union-onschema statement errors out,PIG-1673,12476803,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,07/Oct/10 20:29,17/Dec/10 22:46,14/Mar/19 03:07,18/Oct/10 22:47,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"grunt> l = load 't.txt' as (a,b);
grunt> f1 = foreach l generate (chararray)a;
grunt> u = union onschema f1, l;
grunt> u2 = union onschema u, f1;
2010-10-07 11:13:17,348 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. null
Details at logfile: /Users/tejas/pig-0.8/branch-0.8/pig_1286475169418.log

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Oct/10 15:53;thejas;PIG-1673.1.patch;https://issues.apache.org/jira/secure/attachment/12457267/PIG-1673.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-10-18 18:11:04.271,,,no_permission,,,,,,,,,,,,165122,Reviewed,,,Mon Oct 18 22:47:45 UTC 2010,,,,,,,0|i0gye7:,97002,,,,,,,,,,"15/Oct/10 15:53;thejas;The root cause for the issue was that the logical plan during parsing of a statement does not have the entire plan that has been generated at that point, and in case of union-onschema LogicalPlanValidator was being called with this partial plan.
When there were two consecutive union-onschema in the plan, code in TypeCheckingVisitor.insertCastForEachInBetweenIfNecessary assumes that predecessors of the firs LOUnion are available in the logical plan and it ends up throwing a NullPointerException .
Another issue caused by same root cause, was that the correct types would not be available in the parser at the point where union-onschema adds foreach statements and casts to the plan. 
To address this problem, the patch moves most of the code for union-onschema in the parser to a visitor which is run from PigServer.compileLp(). The visitor has access to the the complete logical plan.
","15/Oct/10 17:17;thejas;Unit tests and test-patch have successfully completed.
",18/Oct/10 18:11;daijy;+1,"18/Oct/10 22:47;thejas;Patch committed to 0.8 branch and trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
order of relations in replicated join gets switched in a query where first relation has two mergeable foreach statements,PIG-1672,12476796,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,07/Oct/10 18:51,17/Dec/10 22:46,14/Mar/19 03:07,09/Oct/10 00:19,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"The replicated join query was running out of memory because the order of relations got switched during logical plan optimization and it was attempting to load the larger (left) relation into memory.

{code}
cat replj.pig
l1 = load 'x' as (a);
l2 = load 'y' as (b);
l3 = load 'z' as (a1,b1,c1,d1);
f1 = foreach l3 generate a1 as a, b1 as b, c1 as c, d1 as d;
f2 = foreach f1 generate a,b,c; 
j1 = join f2 by a, l1 by a using 'replicated';
j2 = join j1 by b, l2 by b using 'replicated';
explain j2;


Note that in the MR plan printed below, the Load in the MR job with join operations has 'x' as the input instead of 'z' .

#--------------------------------------------------
# Map Reduce Plan                                  
#--------------------------------------------------
MapReduce node scope-30
Map Plan
Store(file:/tmp/temp101387354/tmp-125684214:org.apache.pig.impl.io.InterStorage) - scope-31
|
|---l2: Load(file:///Users/tejas/pig-0.8/branch-0.8/y:org.apache.pig.builtin.PigStorage) - scope-17--------
Global sort: false
----------------

MapReduce node scope-27
Map Plan
j2: Store(fakefile:org.apache.pig.builtin.PigStorage) - scope-26
|
|---j2: FRJoin[tuple] - scope-20
    |   |
    |   Project[bytearray][1] - scope-18
    |   |
    |   Project[bytearray][0] - scope-19
    |
    |---j1: FRJoin[tuple] - scope-11
        |   |
        |   Project[bytearray][0] - scope-9
        |   |
        |   Project[bytearray][0] - scope-10
        |
        |---l1: Load(file:///Users/tejas/pig-0.8/branch-0.8/x:org.apache.pig.builtin.PigStorage) - scope-0--------
Global sort: false
----------------

MapReduce node scope-28
Map Plan
Store(file:/tmp/temp101387354/tmp-890864787:org.apache.pig.impl.io.InterStorage) - scope-29
|
|---f2: New For Each(false,false,false)[bag] - scope-8
    |   |
    |   Project[bytearray][0] - scope-2
    |   |
    |   Project[bytearray][1] - scope-4
    |   |
    |   Project[bytearray][2] - scope-6
    |
    |---l3: Load(file:///Users/tejas/pig-0.8/branch-0.8/z:org.apache.pig.builtin.PigStorage) - scope-1--------
Global sort: false
----------------

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Oct/10 18:54;thejas;PIG-1672.1.patch;https://issues.apache.org/jira/secure/attachment/12456619/PIG-1672.1.patch,08/Oct/10 23:58;thejas;PIG-1672.2.patch;https://issues.apache.org/jira/secure/attachment/12456751/PIG-1672.2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-10-09 00:03:38.088,,,no_permission,,,,,,,,,,,,165121,Reviewed,,,Sat Oct 09 00:19:56 UTC 2010,,,,,,,0|i0gydz:,97001,,,,,,,,,,"07/Oct/10 18:54;thejas;PIG-1672.1.patch
Fixed the way new merged foreach gets added after two foreach statements get merged. I will upload another patch with test cases.
","08/Oct/10 23:58;thejas;PIG-1672.2.patch
Added test case.
I have also made minor changes to reduce the memory footprint of the right table, which should reduce the overhead for every record in the table by up to 80 bytes. In POFRJoin.java, now the ArrayList that stores multiple values for a key is initialized with 1, the 'value' tuple in the hashmap is initialized with expected number of fields.


","08/Oct/10 23:58;thejas;Unit tests and test-patch have passed with PIG-1672.2.patch .
",09/Oct/10 00:03;daijy;+1,"09/Oct/10 00:19;thejas;Patch committed to 0.8 branch and trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig throws ExecException in stead of FrontEnd exception when the plan validation fails,PIG-1670,12476733,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nrai,nrai,nrai,07/Oct/10 01:38,17/Dec/10 22:46,14/Mar/19 03:07,07/Oct/10 18:09,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"If the plan validation fails, pig throws the ExecException in stead of FrontendException.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Oct/10 01:41;nrai;PIG-1670_0.patch;https://issues.apache.org/jira/secure/attachment/12456564/PIG-1670_0.patch,07/Oct/10 17:54;nrai;PIG-1670_1.patch;https://issues.apache.org/jira/secure/attachment/12456613/PIG-1670_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-10-07 17:19:46.409,,,no_permission,,,,,,,,,,,,38619,Reviewed,,,Thu Oct 07 18:09:07 UTC 2010,,,Patch Available,,,,0|i0gydj:,96999,,,,,,,,,,"07/Oct/10 01:41;nrai;Fixed the code to throw right kind of exception.
Modifies the testcase.",07/Oct/10 02:11;nrai;The unit test and patch test have been successful.,07/Oct/10 17:19;daijy;Is that PigServer.compilePp only throw FrontendException? If so we shall remove ExecException in compilePp signature.,07/Oct/10 17:54;nrai;Removed the ExecException ,07/Oct/10 18:09;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PushUpFilter fail when filter condition contains scalar,PIG-1669,12475894,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,05/Oct/10 21:05,30/Jun/17 21:26,14/Mar/19 03:07,18/Oct/10 18:18,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"The following script fail:
{code}
a = load 'studenttab10k' as (name, age, gpa);
b = group a all;
c = foreach b generate AVG(a.age) as age;
d = foreach a generate name, age;
e = filter d by age > c.age;
dump e;
{code}

Stack:
ERROR 2243: Attempt to remove operator LOFilter that is still softly connected in the plan

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias e
        at org.apache.pig.PigServer.openIterator(PigServer.java:753)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:612)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:303)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:165)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:141)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:90)
        at org.apache.pig.Main.run(Main.java:498)
        at org.apache.pig.Main.main(Main.java:107)
Caused by: org.apache.pig.PigException: ERROR 1002: Unable to store alias e
        at org.apache.pig.PigServer.storeEx(PigServer.java:877)
        at org.apache.pig.PigServer.store(PigServer.java:815)
        at org.apache.pig.PigServer.openIterator(PigServer.java:727)
        ... 7 more
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 2042: Error in new logical plan. Try -Dpig.usenewlogicalplan=false.
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:308)
        at org.apache.pig.PigServer.compilePp(PigServer.java:1350)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1195)
        at org.apache.pig.PigServer.storeEx(PigServer.java:873)
        ... 9 more
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2000: Error processing rule PushUpFilter. Try -t PushUpFilter
        at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:120)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:276)
        ... 12 more
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2243: Attempt to remove operator LOFilter that is still softly connected in the plan
        at org.apache.pig.newplan.BaseOperatorPlan.remove(BaseOperatorPlan.java:161)
        at org.apache.pig.newplan.BaseOperatorPlan.removeAndReconnect(BaseOperatorPlan.java:423)
        at org.apache.pig.newplan.logical.rules.FilterAboveForeach$FilterAboveForEachTransformer.transform(FilterAboveForeach.java:263)
        at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:110)
",,,,,,,,,,,,,,,,,,,PIG-4767,,,,,,,,,,,,,08/Oct/10 23:17;daijy;PIG-1669-1.patch;https://issues.apache.org/jira/secure/attachment/12456748/PIG-1669-1.patch,11/Oct/10 17:07;daijy;PIG-1669-2.patch;https://issues.apache.org/jira/secure/attachment/12456874/PIG-1669-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-10-14 22:26:00.282,,,no_permission,,,,,,,,,,,,165119,Reviewed,,,Mon Oct 18 18:18:34 UTC 2010,,,,,,,0|i0gyd3:,96997,,,,,,,,,,11/Oct/10 17:07;daijy;Fix unit test failures.,"11/Oct/10 23:43;daijy;test-patch result.

     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
",14/Oct/10 22:26;thejas;Looks good +1.,18/Oct/10 18:18;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Order by failed with RuntimeException,PIG-1668,12475886,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,05/Oct/10 19:55,17/Dec/10 22:46,14/Mar/19 03:07,06/Oct/10 23:46,0.7.0,0.8.0,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"
This is data-dependent. We need to fix the underlining algorithm.

{code}
Backend error message
---------------------
java.lang.RuntimeException: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.MalFormedProbVecException: ERROR 2122: Sum of probabilities should be one
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.setConf(WeightedRangePartitioner.java:139)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.<init>(MapTask.java:578)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:658)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:327)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:219)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1021)
	at org.apache.hadoop.mapred.Child.main(Child.java:213)
Caused by: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.MalFormedProbVecException: ERROR 2122: Sum of probabilities should be one
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.DiscreteProbabilitySampleGenerator.<init>(DiscreteProbabilitySampleGenerator.java:56)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.setConf(WeightedRangePartitioner.java:128)
	... 10 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Oct/10 20:35;rding;PIG-1668.patch;https://issues.apache.org/jira/secure/attachment/12456540/PIG-1668.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-10-06 23:36:12.111,,,no_permission,,,,,,,,,,,,165118,Reviewed,,,Wed Oct 06 23:46:49 UTC 2010,,,,,,,0|i0gycv:,96996,,,,,,,,,,06/Oct/10 23:36;pradeepkth;+1,06/Oct/10 23:46;rding;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
union onschema fails when the input relation has cast from bytearray to another type,PIG-1666,12475869,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,05/Oct/10 17:54,17/Dec/10 22:46,14/Mar/19 03:07,08/Oct/10 18:27,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"Script- 
{code}
l = load 't.txt' as (a,b);
f1 = foreach l generate (chararray)a;
u = union onschema f1, l;
dump u;
{code}

exception -
{code}
org.apache.pig.backend.executionengine.ExecException: ERROR 1075: Received a bytearray from the UDF. Cannot determine how to convert the bytearray to string.
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(POCast.java:652)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:367)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:291)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POUnion.getNext(POUnion.java:163)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:236)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:231)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:1)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Oct/10 16:17;thejas;PIG-1666.1.patch;https://issues.apache.org/jira/secure/attachment/12456605/PIG-1666.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-10-07 20:51:40.263,,,no_permission,,,,,,,,,,,,165116,Reviewed,,,Fri Oct 08 18:27:54 UTC 2010,,,,,,,0|i0gyc7:,96993,,,,,,,,,,"07/Oct/10 16:17;thejas;PIG-1666.1.patch 
When union onschema is used, TypeCheckingValidator is called more than once. But lineage information set  in fieldschema of LOCast during first invocation of TypeCheckingValidator was still there, causing issues with the lineage logic. The patch fixes it. I have also added checks to TypeCheckingVisitor to check for any conflicting lineage information for LOCast . 

Unit tests and test-patch have passed.
",07/Oct/10 20:51;rding;+1,"08/Oct/10 18:27;thejas;Patch committed to 0.8 branch and trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[zebra] leading '_' in directory/file names should be ignored; the ""pigtest"" build target should include all pig-related zebra tests.",PIG-1664,12475797,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,yanz,yanz,yanz,04/Oct/10 19:56,17/Dec/10 22:46,14/Mar/19 03:07,04/Oct/10 22:10,0.6.0,0.7.0,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"Disk entries of names prefixed with '_' is excluded during split generation, and should be excluded from CG's data file processing too. Furthermore, FileInputFormat treats those files as hidden files. Zebra should ignore them to be safe.

At writing, zebra creates _temporary directory under the table (PIG-1115). The directory may not be totally cleaned up due to lagging speculative executions. To avoid  any confusions resulting from attempt to access the _temporary directory, the disk entries of names with the '_' prefix should also be excluded by Zebra processing.

On the other hand, now that Zebra's pig tests run much faster using PIG's local mode, all pig-related should be included in the ""pigtest"" target to complete PIG's test coverage.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Oct/10 21:10;yanz;PIG-1664.patch;https://issues.apache.org/jira/secure/attachment/12456322/PIG-1664.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-10-04 21:35:02.219,,,no_permission,,,,,,,,,,,,165114,,,,Mon Oct 04 22:10:47 UTC 2010,,,,,,,0|i0gybr:,96991,,,,,,,,,,"04/Oct/10 21:35;xuefuz;+1
Patch looks good.","04/Oct/10 22:10;yanz;zebra nightly tests pass ok.

Patch committed to the trunk and the 0.8 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need better error message for MalFormedProbVecException,PIG-1662,12475650,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,01/Oct/10 18:04,17/Dec/10 22:46,14/Mar/19 03:07,04/Oct/10 21:40,0.7.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"Instead the generic error message:

Backend error message
---------------------

Caused by: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.MalFormedProbVecException: ERROR 2122: Sum of probabilities should be one
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.DiscreteProbabilitySampleGenerator.<init>(DiscreteProbabilitySampleGenerator.java:56)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.setConf(WeightedRangePartitioner.java:128)
	... 10 more

it can easily print out the content of the malformed probability vector.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Oct/10 18:11;rding;PIG-1662.patch;https://issues.apache.org/jira/secure/attachment/12456149/PIG-1662.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-10-04 21:27:49.56,,,no_permission,,,,,,,,,,,,165113,Reviewed,,,Mon Oct 04 21:27:49 UTC 2010,,,,,,,0|i0gyb3:,96988,,,,,,,,,,04/Oct/10 21:27;thejas;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sortinfo is not set for store if there is a filter after ORDER BY,PIG-1659,12475534,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,yanz,yanz,30/Sep/10 18:59,17/Dec/10 22:46,14/Mar/19 03:07,05/Oct/10 00:53,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,This has caused 6 (of 7) failures in the Zebra test TestOrderPreserveVariableTable.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Oct/10 21:28;daijy;PIG-1659-1.patch;https://issues.apache.org/jira/secure/attachment/12456159/PIG-1659-1.patch,04/Oct/10 17:16;daijy;PIG-1659-2.patch;https://issues.apache.org/jira/secure/attachment/12456297/PIG-1659-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-10-01 18:45:33.391,,,no_permission,,,,,,,,,,,,165112,Reviewed,,,Tue Oct 05 00:53:21 UTC 2010,,,,,,,0|i0gy9z:,96983,,,,,,,,,,01/Oct/10 18:45;daijy;We should set sortInfo after optimization. So we should add SetSortInfo after the optimization of new logical plan. This code is missing.,01/Oct/10 19:26;yanz;Need to make sure it is invoked after optimization in both old and new logical plans.,04/Oct/10 17:16;daijy;PIG-1659-2.patch address Yan's comments. All Pig tests pass.,04/Oct/10 19:02;yanz;+1. All zebra nightly tests pass.,"05/Oct/10 00:53;daijy;test-patch result:

     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Patch committed to both trunk and 0.8 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ORDER BY does not work properly on integer/short keys that are -1,PIG-1658,12475528,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yanz,yanz,yanz,30/Sep/10 18:25,17/Dec/10 22:46,14/Mar/19 03:07,01/Oct/10 20:30,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"In fact, all these types of keys of values that are negative but within the byte or short's range would have the problem.

Basic cally, a byte value of -1 & 0xff will return 255 not -1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Oct/10 19:14;yanz;PIG-1658.patch;https://issues.apache.org/jira/secure/attachment/12456153/PIG-1658.patch,01/Oct/10 05:14;yanz;PIG-1658.patch;https://issues.apache.org/jira/secure/attachment/12456086/PIG-1658.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-10-01 19:33:45.243,,,no_permission,,,,,,,,,,,,165111,,,,Fri Oct 01 20:30:27 UTC 2010,,,,,,,0|i0gy9j:,96981,,,,,,,,,,"01/Oct/10 05:14;yanz;This problem is caused by the PIG-1295 patch.

test-core pass. Zebra's nightly pass too.

test-patch output:

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Zebra's TestMergeJoinPartial is used to verify the fix.","01/Oct/10 19:14;yanz;Add Zebra test TestMergeJoinPartial to the ""pigtest"" target.","01/Oct/10 19:33;thejas;Looks good . +1
",01/Oct/10 20:30;yanz;Committed to both trunk and the 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TOBAG  udfs ignores columns with null value;  it does not use input type to determine output schema,PIG-1656,12475366,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,28/Sep/10 23:46,17/Dec/10 22:46,14/Mar/19 03:07,01/Oct/10 23:56,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"TOBAG udf ignores columns with null value
{code}
R4= foreach B generate $0,  TOBAG( id, null, id,null );
grunt> dump R4;
1000    {(1),(1)}
1000    {(2),(2)}
1000    {(3),(3)}
1000    {(4),(4)}
{code}


 TOBAG does not use input type to determine output schema
{code}
grunt> B1 = foreach B generate TOBAG( 1, 2, 3);         
grunt> describe B1;
B1: {{null}}
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Oct/10 20:35;thejas;PIG-1656.1.patch;https://issues.apache.org/jira/secure/attachment/12456157/PIG-1656.1.patch,01/Oct/10 23:16;thejas;PIG-1656.2.patch;https://issues.apache.org/jira/secure/attachment/12456168/PIG-1656.2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-10-01 21:52:08.804,,,no_permission,,,,,,,,,,,,165109,Reviewed,,,Fri Oct 01 23:56:04 UTC 2010,,,,,,,0|i0gy8v:,96978,,,,,,,,,,"01/Oct/10 20:36;thejas;Patch passes unit tests and test-patch .
","01/Oct/10 21:52;rding;
We need to make it clear how the output schema of TOBAG is generated. For example, in the first case, the type is preserved in the inner schema:

{code}
grunt> a = load 'input' as (a0:int, a1:int);
grunt> b = foreach a generate TOBAG(a0, a1);
grunt> describe b;
b: {{int}}
{code}

but not in the second case:

{code}
grunt> a = load 'input' as (a0:int, a1:int);
grunt> c = group a by a0 ;
grunt> b = foreach c generate TOBAG(a.a0, a.a1);
grunt> describe b;
b: {{NULL}}
{code}","01/Oct/10 23:16;thejas;PIG-1656.2.patch
Updated patch to include documentation of details of output schema generation.
",01/Oct/10 23:43;rding;+1,"01/Oct/10 23:56;thejas;Patch committed to 0.8 branch and trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
code duplicated for udfs that were moved from piggybank to builtin,PIG-1655,12475362,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nrai,thejas,thejas,28/Sep/10 23:27,17/Dec/10 22:46,14/Mar/19 03:07,07/Oct/10 18:19,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"As part of PIG-1405, some udfs from piggybank were made standard udfs. But now the code is duplicated in piggybank and org.apache.pig.builtin. . This can cause confusion.
I am planning to make these udfs in piggybank subclasses of those in org.apache.pig.builtin. so that users don't have to change their scripts.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Oct/10 02:23;nrai;PIG-1655_0.patch;https://issues.apache.org/jira/secure/attachment/12456357/PIG-1655_0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-10-05 02:23:16.544,,,no_permission,,,,,,,,,,,,165108,Reviewed,,,Thu Oct 07 18:19:35 UTC 2010,,,,,,,0|i0gy8f:,96976,,,,,,,,,,"05/Oct/10 02:23;nrai;I stead of removing the duplicate code, I have marked them deprecated. Making these classes a subclass of the builtin class needs change in the signature of the constructor, which we don't want at this stage.",07/Oct/10 18:15;daijy;+1,07/Oct/10 18:19;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig should check schema alias duplication at any levels.,PIG-1654,12475359,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,28/Sep/10 23:02,26/Jul/13 08:26,14/Mar/19 03:07,15/Mar/11 18:35,,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"The following script appears valid to Pig but it shouldn't:

A = load 'file' as (a:tuple( u:int, u:bytearray, w:long), b:int, c:chararray);
dump A;

Pig tries to launch map/reduce jobs for this.

However, for the following script, Pig correctly reports error message:

A = load 'file' as (a:int, a:long, c:bytearray);
dump A;

Error message is:
2010-09-28 15:53:37,390 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1108: Duplicate schema alias: b in ""A""

Thus, Pig only checks alias duplication at the top level, which is confirmed by looking at the code. The right behavior is that the same check should be applied at all levels. 

This should be addressed in the new parser.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-07-25 20:44:56.586,,,no_permission,,,,,,,,,,,,66233,,,,Fri Jul 26 08:26:39 UTC 2013,,,,,,,0|i0gy7z:,96974,,,,,,,,,,15/Mar/11 18:35;xuefuz;This problem is correctly addressed in the new parser.,"25/Jul/13 20:44;serega_sheypak;Hi, I'm developing and debugging my pig script locally. Suddenly I've got such exception.

The partial code is:

{code}
routePivotsGroupedByMsisdn = GROUP routePivots BY msisdn;
markedPivots = FOREACH routePivotsGroupedByMsisdn {
                ordered = ORDER routePivots BY ts;
                GENERATE FLATTEN(udf.filter_route_pivots(ordered))
                        as  (msisdn: long,        --0
                             ts: long,            --1

                             lac: int,            --2
                             cid: int,            --3
                             lon,                 --4
                             lat,                 --5
                             azimuth,             --6
                             hpbw,                --7
                             max_dist,            --8

                             cell_type: chararray, --9
                             branch_id,           --10
                             center_lon: double,  --11
                             center_lat: double,  --12
                             tile_id:    int,     --13
                             zone_col:   int,     --14
                             zone_row:   int,     --15
                             is_active,           --16
                             not_valid);

               }
SPLIT markedPivots INTO corruptedPivots if not_valid is not null, validPivots if not_valid is null;

groupedValidPivots = GROUP validPivots BY msisdn;
pivotsWithEndPoints = FOREACH groupedValidPivots {
                ordered = ORDER validPivots BY ts;
                GENERATE FLATTEN(udf.mark_end_points(validPivots))
                        as  (msisdn: long,        --0
                             ts: long,            --1

                             lac: int,            --2
                             cid: int,            --3
                             lon,                 --4
                             lat,                 --5
                             azimuth,             --6
                             hpbw,                --7
                             max_dist,            --8

                             cell_type: chararray, --9
                             branch_id,           --10
                             center_lon: double,  --11
                             center_lat: double,  --12
                             tile_id:    int,     --13
                             zone_col:   int,     --14
                             zone_row:   int,     --15
                             is_active,           --16

                             is_end_point: boolean,
                             end_point_type: chararray);

               }
--complains on msisdn..
projPivotsWithEndPoints = FOREACH pivotsWithEndPoints GENERATE msisdn, ts,
                                                               center_lon, center_lat,
                                                               lac, cid, cell_type, is_active,
                                                               is_end_point, end_point_type;
STORE projPivotsWithEndPoints INTO '$validPivots' USING
 org.apache.pig.piggybank.storage.avro.AvroStorage('index', '3', 'schema', '{""name"": ""valid_pivots"", ""doc"": ""version 0.0.1"", ""type"": ""record"", ""fields"": [
   {""name"": ""msisdn"",        ""type"": ""long""},
   {""name"": ""ts"",            ""type"": ""long""},

   {""name"": ""center_lon"",    ""type"": ""double""},
   {""name"": ""center_lat"",    ""type"": ""double""},

   {""name"": ""lac"",           ""type"": ""int""},
   {""name"": ""cid"",           ""type"": ""int""},
   {""name"": ""cell_type"",     ""type"": ""string""},
   {""name"": ""is_active"",     ""type"": ""boolean""},

   {""name"": ""is_end_point"",  ""type"": ""boolean""},
   {""name"": ""end_point_type"",""type"": ""string""}
 ]}');
{code}
It complains on filed with comment ""--complains on msisdn..""
If I use default 
STORE pivotsWithEndPoints INTO '$validPivots';

everything works fine.
What do I do wrong?",26/Jul/13 08:26;serega_sheypak;Not a problem.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scripting UDF fails if the path to script is an absolute path,PIG-1653,12475331,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,28/Sep/10 18:50,17/Dec/10 22:46,14/Mar/19 03:07,18/Oct/10 21:05,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"The following script fail:
{code}
register '/homes/jianyong/pig/aaa/scriptingudf.py' using jython as myfuncs;
a = load '/user/pig/tests/data/singlefile/studenttab10k' using PigStorage() as (name, age, gpa:double);
b = foreach a generate myfuncs.square(gpa);
dump b;
{code}

If we change the register to use relative path (such as ""aaa/scriptingudf.py""), it success.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Oct/10 05:54;daijy;PIG-1653-1.patch;https://issues.apache.org/jira/secure/attachment/12456938/PIG-1653-1.patch,12/Oct/10 18:56;daijy;PIG-1653-2.patch;https://issues.apache.org/jira/secure/attachment/12456995/PIG-1653-2.patch,15/Oct/10 21:33;daijy;PIG-1653-3.patch;https://issues.apache.org/jira/secure/attachment/12457301/PIG-1653-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-10-18 18:43:07.083,,,no_permission,,,,,,,,,,,,165107,Reviewed,,,Mon Oct 18 21:05:54 UTC 2010,,,,,,,0|i0gy7r:,96973,,,,,,,,,,12/Oct/10 18:56;daijy;Did some refinement (PIG-1653-2.patch). All tests pass.,15/Oct/10 21:33;daijy;Richard suggest a better fix. Attach PIG-1653-3.patch for that.,18/Oct/10 18:43;thejas;+1,18/Oct/10 21:05;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PIG class loading mishandled,PIG-1651,12475265,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,yanz,yanz,28/Sep/10 01:09,17/Dec/10 22:46,14/Mar/19 03:07,30/Sep/10 20:27,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"If just having zebra.jar as being registered in a PIG script but not in the CLASSPATH, the query using zebra fails since there appear to be multiple classes loaded into JVM, causing static variable set previously not seen after one instance of the class is created through reflection. (After the zebra.jar is specified in CLASSPATH, it works fine.) The exception stack is as follows:

ackend error message during job submission
-------------------------------------------
org.apache.pig.backend.executionengine.ExecException: ERROR 2118: Unable to create input splits for: hdfs://hostname/pathto/zebra_dir :: null
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:284)
        at org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:907)
        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:801)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:752)
        at org.apache.hadoop.mapred.jobcontrol.Job.submit(Job.java:378)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs(JobControl.java:247)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.run(JobControl.java:279)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.zebra.io.ColumnGroup.getNonDataFilePrefix(ColumnGroup.java:123)
        at org.apache.hadoop.zebra.io.ColumnGroup$CGPathFilter.accept(ColumnGroup.java:2413)
        at org.apache.hadoop.zebra.mapreduce.TableInputFormat$DummyFileInputFormat$MultiPathFilter.accept(TableInputFormat.java:718)
        at org.apache.hadoop.fs.FileSystem$GlobFilter.accept(FileSystem.java:1084)
        at org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:919)
        at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:866)
        at org.apache.hadoop.zebra.mapreduce.TableInputFormat$DummyFileInputFormat.listStatus(TableInputFormat.java:780)
        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:246)
        at org.apache.hadoop.zebra.mapreduce.TableInputFormat.getRowSplits(TableInputFormat.java:863)
        at org.apache.hadoop.zebra.mapreduce.TableInputFormat.getSplits(TableInputFormat.java:1017)
        at org.apache.hadoop.zebra.mapreduce.TableInputFormat.getSplits(TableInputFormat.java:961)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:269)
        ... 7 more

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Sep/10 22:51;rding;PIG-1651.patch;https://issues.apache.org/jira/secure/attachment/12455884/PIG-1651.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-09-28 22:35:16.806,,,no_permission,,,,,,,,,,,,165105,Reviewed,,,Tue Sep 28 22:56:36 UTC 2010,,,,,,,0|i0gy6v:,96969,,,,,,,,,,"28/Sep/10 22:35;rding;The problem here is that PigContext uses LogicalPlanBuilder.classloader to instantiate the LoadFuncs, but the context ClassLoader for the Thread uses a different class loader, and hence the static variable set for the class loaded by one loader is not visible by the class loaded by the other loader. The solution is to use the same LogicalPlanBuilder.classloader as the context ClassLoader for the Thread.",28/Sep/10 22:56;daijy;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FRJoin fails to compute number of input files for replicated input,PIG-1649,12475248,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,27/Sep/10 22:04,17/Dec/10 22:46,14/Mar/19 03:07,30/Sep/10 15:49,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"In FRJoin, if input path has curly braces, it fails to compute number of input files and logs the following exception in the log -

10/09/27 14:31:13 WARN mapReduceLayer.MRCompiler: failed to get number of input files
java.net.URISyntaxException: Illegal character in path at index 12: /user/tejas/{std*txt}
        at java.net.URI$Parser.fail(URI.java:2809)
        at java.net.URI$Parser.checkChars(URI.java:2982)
        at java.net.URI$Parser.parseHierarchical(URI.java:3066)
        at java.net.URI$Parser.parse(URI.java:3024)
        at java.net.URI.<init>(URI.java:578)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.hasTooManyInputFiles(MRCompiler.java:1283)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.visitFRJoin(MRCompiler.java:1203)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin.visit(POFRJoin.java:188)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.compile(MRCompiler.java:475)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.compile(MRCompiler.java:454)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.compile(MRCompiler.java:336)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.compile(MapReduceLauncher.java:468)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:116)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:301)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1197)
        at org.apache.pig.PigServer.storeEx(PigServer.java:873)
        at org.apache.pig.PigServer.store(PigServer.java:815)
        at org.apache.pig.PigServer.openIterator(PigServer.java:727)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:612)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:301)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:165)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:141)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:76)
        at org.apache.pig.Main.run(Main.java:453)
        at org.apache.pig.Main.main(Main.java:107)

This does not cause a query to fail. But since the number of input files don't get calculated, the optimizations added in PIG-1458 to reduce load on name node will not get used.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Sep/10 16:30;thejas;PIG-1649.1.patch;https://issues.apache.org/jira/secure/attachment/12455841/PIG-1649.1.patch,28/Sep/10 22:40;thejas;PIG-1649.2.patch;https://issues.apache.org/jira/secure/attachment/12455882/PIG-1649.2.patch,28/Sep/10 23:14;thejas;PIG-1649.3.patch;https://issues.apache.org/jira/secure/attachment/12455886/PIG-1649.3.patch,29/Sep/10 00:40;thejas;PIG-1649.4.patch;https://issues.apache.org/jira/secure/attachment/12455893/PIG-1649.4.patch,29/Sep/10 22:52;thejas;PIG-1649.5.patch;https://issues.apache.org/jira/secure/attachment/12455952/PIG-1649.5.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2010-09-29 00:45:44.903,,,no_permission,,,,,,,,,,,,165103,Reviewed,,,Thu Sep 30 15:49:11 UTC 2010,,,,,,,0|i0gy67:,96966,,,,,,,,,,"28/Sep/10 16:30;thejas;Patch passes unit tests and test-patch .
","28/Sep/10 22:40;thejas;PIG-1649.2.patch
Addressing review comments from Richard 
-  pointed out that that hdfs Path class constructor can fail on valid Uri like the format used for jdbc. So this patch checks if the input location uri has a hdfs scheme before using the hdfs Path constructor.
- The code here can run into same problem as one in PIG-1652. The patch also includes changes to handle comma separated file names.

A better long term solution would be to have support in LoadFunc or related interfaces to check the input size and to check if parts of the file should be consolidated.

","28/Sep/10 22:41;thejas;The patch also includes changes to fix the issue in PIG-1652 , since FRJoin code path also faces similar issue.

","28/Sep/10 23:14;thejas;New patch that includes apache license header for UriUtil.java. Passes test-patch, waiting for unit tests to finish.
","29/Sep/10 00:40;thejas;New patch addressing comments from Richard
- In UriUtil.isHDFSFile(String uri) return false if uri is null
- Modified a test in TestFRJoin2 to use comma separated file name.",29/Sep/10 00:45;rding;+1. Looks good.,"29/Sep/10 22:52;thejas;
I committed PIG-1649.4.patch to 0.8 and trunk after unit tests completed, but only after that I realized that I had run unit tests against a older patch (PIG-1649.2.patch). While running unit tests again I found 2 failure in TestJobSubmission. Patch PIG-1649.5.patch has the fix. I am waiting for unit tests to complete.","30/Sep/10 15:49;thejas;unit tests passed. PIG-1649.5.patch committed to trunk and 0.8 branch.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split combination may return too many block locations to map/reduce framework,PIG-1648,12475091,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yanz,yanz,yanz,25/Sep/10 00:20,17/Dec/10 22:46,14/Mar/19 03:07,28/Sep/10 19:28,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"For instance, if a small split has block locations h1, h2 and h3; another small split has h1, h3, h4. After combination, the composite split contains 4 block locations. If the number of component splits is big, then the number of block locations could be big too. In fact, the  number of block locations serves as a hint to M/R as the best hosts this composite split should be run on so the list should contain a short list, say 5, of the hosts that contain the most data in this composite split.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Sep/10 17:21;yanz;PIG-1648.patch;https://issues.apache.org/jira/secure/attachment/12455851/PIG-1648.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-09-28 19:21:40.616,,,no_permission,,,,,,,,,,,,165102,,,,Mon Dec 13 19:49:20 UTC 2010,,,,,,,0|i0gy5r:,96964,,,,,,,,,,28/Sep/10 16:26;yanz;Top 5 locations with most data will be used. This has been agreed upon by the M/R dev.,"28/Sep/10 17:44;yanz;test-patch results:

     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

test-core tests pass too.
",28/Sep/10 19:21;rding;+1,28/Sep/10 19:28;yanz;Patch committed to both trunk and the 0.8 branch.,"13/Dec/10 19:49;thejas;(Some clarification about what this patch does, after discussing with Yan -)
This change does not alter the number of splits combined together, it just alters the number of block locations sent to MR . As the description says, this information is used by MR framework only for the deciding where to schedule the map job.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Logical simplifier throws a NPE,PIG-1647,12475090,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yanz,yanz,yanz,25/Sep/10 00:11,17/Dec/10 22:46,14/Mar/19 03:07,27/Sep/10 17:58,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"A query like:

A = load 'd.txt' as (a:chararray, b:long, c:map[], d:chararray, e:chararray);
B = filter A by a == 'v' and b == 117L and c#'p1' == 'h' and c#'p2' == 'to' and ((d is not null and d != '') or (e is not null and e != ''));

will cause the logical expression simplifier to throw a NPE.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Sep/10 15:14;yanz;PIG-1647.patch;https://issues.apache.org/jira/secure/attachment/12455609/PIG-1647.patch,25/Sep/10 01:33;yanz;PIG-1647.patch;https://issues.apache.org/jira/secure/attachment/12455543/PIG-1647.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-09-27 17:10:57.3,,,no_permission,,,,,,,,,,,,165101,,,,Mon Sep 27 17:58:11 UTC 2010,,,,,,,0|i0gy5j:,96963,,,,,,,,,,"26/Sep/10 15:14;yanz;passes test-core.

test-patch results:

     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
",27/Sep/10 17:10;daijy;+1. Please commit.,27/Sep/10 17:58;yanz;Patch committed to both trunk and the 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using both small split combination and temporary file compression on a query of ORDER BY may cause crash,PIG-1645,12474893,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yanz,yanz,yanz,23/Sep/10 01:50,17/Dec/10 22:46,14/Mar/19 03:07,25/Sep/10 03:58,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"The stack looks like the following:

java.lang.NullPointerException at java.util.Arrays.binarySearch(Arrays.java:2043) at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.getPartition(WeightedRangePartitioner.java:72) at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.getPartition(WeightedRangePartitioner.java:52) at 
org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:565) at
org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80) at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:116) at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:238) at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:231) at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53) at
org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144) at
org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:638) at
org.apache.hadoop.mapred.MapTask.run(MapTask.java:314) at org.apache.hadoop.mapred.Child$4.run(Child.java:217) at
java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:396) at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1062) at
org.apache.hadoop.mapred.Child.main(Child.java:211) ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Sep/10 16:08;yanz;PIG-1645.patch;https://issues.apache.org/jira/secure/attachment/12455500/PIG-1645.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-09-24 17:47:55.464,,,no_permission,,,,,,,,,,,,165099,,,,Sat Sep 25 03:58:42 UTC 2010,,,,,,,0|i0gy4n:,96959,,,,,,,,,,"23/Sep/10 17:56;yanz;The problem is that both RandomSampleLoader and PossionSampleLoader have internal states from the previous invocations that should be reset when a different underlying split is worked on under the same umbrella split when the split combination (PIG-1518) is on.

When temporary file compression is disabled, Pig internal storage will create empty files which will be discarded by split combiner, making the only non-empty split as the only split to be worked on, so it is ok in this case.","24/Sep/10 16:08;yanz;test-core passed.

test-patch results:

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     -1 release audit.  The applied patch generated 459 release audit warnings (more than the trunk's current 457 warnings).

The scenario is trully a corner case. The following query *might* have caused the problem:

A = load '/tmp/test/jsTst2.txt' as (fn, age:int);
B = load '/tmp/test/sample.txt' as (fn, age:int);
C = join A by fn, B by fn USING 'replicated';
D = ORDER C BY B::age;
dump D;

where sample.txt has only one row that contains one record that has the same join key as a single record in jsTst2.txt which should have size of several HDFS blocks. Even so, it is random to see a failure, as it depends upon whether any of the logically empty files is placed in the first underlying split of the list of splits combined. Compute nodes' host names seem to play a role too.  Running in local mode seems to see no failure.

The 2 release audit warnings are due to jdiff. No new file added.",24/Sep/10 17:07;yanz;The possibility of failure also depends upon the block distribution since the split combination makes use of that info.,24/Sep/10 17:47;thejas;+1,25/Sep/10 03:58;yanz;Patch committed to both trunk and the 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New logical plan: Plan.connect with position is misused in some places,PIG-1644,12474887,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,23/Sep/10 00:28,17/Dec/10 22:46,14/Mar/19 03:07,26/Sep/10 21:23,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"When we replace/remove/insert a node, we will use disconnect/connect methods of OperatorPlan. When we disconnect an edge, we shall save the position of the edge in origination and destination, and use this position when connect to the new predecessor/successor. Some of the pattens are:

Insert a new node:
{code}
Pair<Integer, Integer> pos = plan.disconnect(pred, succ);
plan.connect(pred, pos.first, newnode, 0);
plan.connect(newnode, 0, succ, pos.second);
{code}

Remove a node:
{code}
Pair<Integer, Integer> pos1 = plan.disconnect(pred, nodeToRemove);
Pair<Integer, Integer> pos2 = plan.disconnect(nodeToRemove, succ);
plan.connect(pred, pos1.first, succ, pos2.second);
{code}

Replace a node:
{code}
Pair<Integer, Integer> pos1 = plan.disconnect(pred, nodeToReplace);
Pair<Integer, Integer> pos2 = plan.disconnect(nodeToReplace, succ);
plan.connect(pred, pos1.first, newNode, pos1.second);
plan.connect(newNode, pos2.first, succ, pos2.second);
{code}

There are couple of places of we does not follow this pattern, that results some error. For example, the following script fail:
{code}
a = load '1.txt' as (a0, a1, a2, a3);
b = foreach a generate a0, a1, a2;
store b into 'aaa';
c = order b by a2;
d = foreach c generate a2;
store d into 'bbb';
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Sep/10 01:01;daijy;PIG-1644-1.patch;https://issues.apache.org/jira/secure/attachment/12455334/PIG-1644-1.patch,24/Sep/10 02:12;daijy;PIG-1644-2.patch;https://issues.apache.org/jira/secure/attachment/12455458/PIG-1644-2.patch,24/Sep/10 21:46;daijy;PIG-1644-3.patch;https://issues.apache.org/jira/secure/attachment/12455526/PIG-1644-3.patch,26/Sep/10 21:21;daijy;PIG-1644-4.patch;https://issues.apache.org/jira/secure/attachment/12455623/PIG-1644-4.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2010-09-23 18:00:19.451,,,no_permission,,,,,,,,,,,,165098,Reviewed,,,Sun Sep 26 21:23:46 UTC 2010,,,,,,,0|i0gy47:,96957,,,,,,,,,,"23/Sep/10 01:00;daijy;Attach the patch to address all such places in new logical plan, except for ExpressionSimplifier. There is some work underway for ExpressionSimplifier ([PIG-1635|https://issues.apache.org/jira/browse/PIG-1635]) include some of these changes, I don't want to conflict with that patch. So after PIG-1635, we may also review the connect/disconnect usage of ExpressionSimplifier.","23/Sep/10 18:00;thejas;These operations will be fairly common in the optimizer. I think it would be good to have functions in the OperatorPlan that support these operations, that will reduce the chances of bugs and also make the code more readable.
","23/Sep/10 18:33;daijy;Yes, I think we can do replace/remove/insert. They should be simple and clear enough to use. Here is the new methods adding to OperatorPlan:
{code}
replace(Operator oldOperator, Operator newOperator)
remove(Operator operatorToRemove) // Connect all its successors to predecessor/connect all it's predecessors to successor
insertBefore(Operator operatorToInsert, Operator pos) // Insert operatorToInsert before pos, connect all pos's predecessors to operatorToInsert
insertAfter(Operator operatorToInsert, Operator pos) // Insert operatorToInsert after pos, connect operatorToInsert to all pos's successor
{code}

How does it sounds?","23/Sep/10 18:59;thejas;I think insertAsPredecessor and insertAsSuccessor (instead of  insertBefore and insertAfter) will convey the idea of what it does a little better. 
","24/Sep/10 02:11;daijy;After looking into the existing code, seems insertBetween is a more useful method. So I want to drop insertBefore/insertAfter, and add insertBetween
{code}
insertBetween(Operator pred, Operator operatorToInsert, Operator succ)
{code}",24/Sep/10 02:12;daijy;Attach the patch with new methods and refactory of existing code.,"24/Sep/10 20:25;thejas;Looks good. +1
Please commit after test-patch and unit tests pass.
","24/Sep/10 21:46;daijy;Find one bug introduced by refactory. Attach PIG-1644-3.patch with the fix, and running the tests again.",26/Sep/10 21:21;daijy;PIG-1644-4.patch fix findbug warnings and additional unit failures.,"26/Sep/10 21:23;daijy;     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

All tests pass. 

Patch committed to both trunk and 0.8 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
join fails for a query with input having 'load using pigstorage without schema' + 'foreach',PIG-1643,12474885,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,22/Sep/10 23:28,17/Dec/10 22:45,14/Mar/19 03:07,26/Sep/10 21:17,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"{code}
l1 = load 'std.txt';
l2 = load 'std.txt'; 
f1 = foreach l1 generate $0 as abc, $1 as  def;
-- j =  join f1 by $0, l2 by $0 using 'replicated';    
-- j =  join l2 by $0, f1 by $0 using 'replicated';    
j =  join l2 by $0, f1 by $0 ;    
dump j;
{code}

the error -
{code}
2010-09-22 16:24:48,584 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2044: The type null cannot be collected as a Key type
{code}

The MR plan from explain  -
{code}
#--------------------------------------------------
# Map Reduce Plan                                  
#--------------------------------------------------
MapReduce node scope-21
Map Plan
Union[tuple] - scope-22
|
|---j: Local Rearrange[tuple]{bytearray}(false) - scope-11
|   |   |
|   |   Project[bytearray][0] - scope-12
|   |
|   |---l2: Load(file:///Users/tejas/pig_obyfail/trunk/std.txt:org.apache.pig.builtin.PigStorage) - scope-0
|
|---j: Local Rearrange[tuple]{NULL}(false) - scope-13
    |   |
    |   Project[NULL][0] - scope-14
    |
    |---f1: New For Each(false,false)[bag] - scope-6
        |   |
        |   Project[bytearray][0] - scope-2
        |   |
        |   Project[bytearray][1] - scope-4
        |
        |---l1: Load(file:///Users/tejas/pig_obyfail/trunk/std.txt:org.apache.pig.builtin.PigStorage) - scope-1--------
Reduce Plan
j: Store(/tmp/x:org.apache.pig.builtin.PigStorage) - scope-18
|
|---POJoinPackage(true,true)[tuple] - scope-23--------
Global sort: false
----------------

{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Sep/10 17:45;thejas;PIG-1643.1.patch;https://issues.apache.org/jira/secure/attachment/12455391/PIG-1643.1.patch,24/Sep/10 22:50;daijy;PIG-1643.2.patch;https://issues.apache.org/jira/secure/attachment/12455531/PIG-1643.2.patch,25/Sep/10 00:04;daijy;PIG-1643.3.patch;https://issues.apache.org/jira/secure/attachment/12455536/PIG-1643.3.patch,25/Sep/10 00:33;thejas;PIG-1643.4.patch;https://issues.apache.org/jira/secure/attachment/12455538/PIG-1643.4.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2010-09-23 17:54:21.656,,,no_permission,,,,,,,,,,,,165097,Reviewed,,,Sun Sep 26 21:12:25 UTC 2010,,,,,,,0|i0gy3r:,96955,PIG-1643.4.patch committed to both trunk and 0.8 branch.,,,,,,,,,"22/Sep/10 23:28;thejas;In case of replicated join, the error was - 
java.lang.NullPointerException
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin.setUpHashMap(POFRJoin.java:343)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin.getNext(POFRJoin.java:212)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:236)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:231)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:1)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
","23/Sep/10 17:45;thejas;PIG-1643.1.patch
There was a code path that lead to fields having NULL datatype instead of the default datatype of BYTEARRAY. That was causing these failures. 
Test-patch has succeeded, unit tests are running.
",23/Sep/10 17:54;daijy;+1 if tests pass.,"24/Sep/10 00:07;thejas;Tests passed.
Patch committed to 0.8 branch and trunk.
","24/Sep/10 22:49;daijy;The following script does not produce the right result after patch:
{code}
a = load '/grid/2/dev/pigqa/in/singlefile/studenttab10k';
b = foreach a generate *;
store b into '/grid/2/dev/pigqa/out/log/hadoopqa.1285338379/Foreach_2.out';
{code}",24/Sep/10 22:50;daijy;Attach a fix.,25/Sep/10 00:04;daijy;PIG-1643.3.patch is more general than PIG-1643.2.patch. It solves this null schema issue for all expressions.,25/Sep/10 00:33;thejas;PIG-1643.4.patch  is PIG-1643.3.patch + test case,"26/Sep/10 21:12;daijy;     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

All tests pass.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Order by doesn't use estimation to determine the parallelism,PIG-1642,12474877,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,22/Sep/10 22:02,17/Dec/10 22:45,14/Mar/19 03:07,27/Sep/10 16:12,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"With PIG-1249, a simple heuristic is used to determine the number of reducers if it isn't specified (via PARALLEL or default_parallel). For order by statement, however, it still defaults to 1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Sep/10 18:17;rding;PIG-1642.patch;https://issues.apache.org/jira/secure/attachment/12455507/PIG-1642.patch,24/Sep/10 21:39;rding;PIG-1642_1.patch;https://issues.apache.org/jira/secure/attachment/12455525/PIG-1642_1.patch,24/Sep/10 19:56;rding;PIG-1642_1.patch;https://issues.apache.org/jira/secure/attachment/12455521/PIG-1642_1.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-09-24 21:43:27.68,,,no_permission,,,,,,,,,,,,165096,Reviewed,,,Mon Sep 27 16:12:11 UTC 2010,,,,,,,0|i0gy3b:,96953,,,,,,,,,,"24/Sep/10 18:17;rding;The patch passed test-core.

The results of test-patch:

{code}
    [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 8 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{code}","24/Sep/10 21:43;thejas;Comments on the patch -
- In SampleOptimizer.java It expects the sampling MR plan to have only one integer argument which has information about the number of reducers that will be used in the successor of sampling job (order-by/skewed-join). We might not remember this assumption if we make changes to the sampling plan, so it will be safer to throw an error if more than one integer constant is seen in the plan.
- In test case, the expected number of reducers is being computed dynamically and used for checking in first scenario, it can be used it in last scenario as well.
",24/Sep/10 21:50;rding;New patch to address the review comments.,"24/Sep/10 23:18;thejas;Looks good. +1 
",27/Sep/10 16:12;rding;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect counters in local mode,PIG-1641,12474833,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,ashutoshc,ashutoshc,22/Sep/10 15:25,17/Dec/10 22:45,14/Mar/19 03:07,27/Sep/10 19:57,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"User report, not verified.

<email>

HadoopVersion    PigVersion    UserId    StartedAt    FinishedAt    Features
0.20.2    0.8.0-SNAPSHOT    user    2010-09-21 19:25:58    2010-09-21 21:58:42    ORDER_BY

Success!

Job Stats (time in seconds):
JobId    Maps    Reduces    MaxMapTime    MinMapTIme    AvgMapTime    MaxReduceTime    MinReduceTime    AvgReduceTime    Alias    Feature    Outputs
job_local_0001    0    0    0    0    0    0    0    0    raw    MAP_ONLY    
job_local_0002    0    0    0    0    0    0    0    0    rank_sort    SAMPLER    
job_local_0003    0    0    0    0    0    0    0    0    rank_sort    ORDER_BY    Processed/user_visits_table,

Input(s):
Successfully read 0 records from: ""Data/Raw/UserVisits.dat""

Output(s):
Successfully stored 0 records in: ""Processed/user_visits_table""


However, when I look in the output:

$ ls -lh Processed/user_visits_table/CG0/
total 15250760
-rwxrwxrwx  1 user  _lpoperator   7.3G Sep 21 21:58 part-0*

It read a 20G input file and generated some output...

</email>

Is it that in local mode counters are not available? If so, instead of printing zeros we should print ""Information Unavailable"" or some such.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Sep/10 16:56;rding;PIG-1641.patch;https://issues.apache.org/jira/secure/attachment/12455387/PIG-1641.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-09-22 19:39:00.818,,,no_permission,,,,,,,,,,,,165095,Reviewed,,,Mon Sep 27 19:57:07 UTC 2010,,,,,,,0|i0gy33:,96952,,,,,,,,,,"22/Sep/10 19:39;rding;Hadoop counters are not available in local mode (PIG-1286).

So for now I propose that, in local mode,  Pig stats output is changed to something like the following:

{code} 
Job Stats (time in seconds):
JobId  Alias Feature Outputs
job_local_0001 raw MAP_ONLY
job_local_0002 rank_sort SAMPLER
job_local_0003 rank_sort ORDER_BY Processed/user_visits_table,

Input(s):
Successfully read records from: ""Data/Raw/UserVisits.dat""

Output(s):
Successfully stored records in: ""Processed/user_visits_table""
{code}","27/Sep/10 18:15;ashutoshc;Tested manually for local mode. Messages were same as proposed above. +1 for the commit. One minor suggestion is to put a line at the start saying something like: ""Detected Local mode. Stats reported below may be incomplete."" This will reinforce the message to users that stats reporting is not transparent across different modes (local Vs map-reduce).",27/Sep/10 19:57;rding;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bin/pig does not run in local mode due to classes missing from classpath,PIG-1640,12474778,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,22/Sep/10 00:13,17/Dec/10 22:45,14/Mar/19 03:07,17/Dec/10 22:35,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"This issue was reported by one of Yahoo users. I have not verified the problem. Here is the report

""when do bin/pig -x local, the shell doesn't come up.  It complained about jline not being found.  Here is a patch to bin/pig:

+for f in $PIG_HOME/build/ivy/lib/Pig/*.jar; do
+    CLASSPATH=${CLASSPATH}:$f;
+done
+""
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,165094,,,,Tue Dec 07 19:26:42 UTC 2010,,,,,,,0|i0gy2f:,96949,,,,,,,,,,07/Dec/10 19:26;olgan;I looked into this issue and was able to reproduce the behavior. The problem is that we are bundling and using wrong pig.jar. I will be fixing that as part of the release process,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New logical plan: PushUpFilter should not push before group/cogroup if filter condition contains UDF,PIG-1639,12474777,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,daijy,daijy,22/Sep/10 00:10,17/Dec/10 22:45,14/Mar/19 03:07,24/Sep/10 23:46,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"The following script fail:
{code}
a = load 'file' AS (f1, f2, f3);
b = group a by f1;
c = filter b by COUNT(a) > 1;
dump c;
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Sep/10 00:11;xuefuz;jira-1639-1.patch;https://issues.apache.org/jira/secure/attachment/12455332/jira-1639-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-09-24 18:01:33.408,,,no_permission,,,,,,,,,,,,165093,Reviewed,,,Fri Sep 24 23:46:25 UTC 2010,,,,,,,0|i0gy27:,96948,,,,,,,,,,23/Sep/10 18:46;daijy;+1 if all tests pass.,"24/Sep/10 18:01;xuefuz;     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modifi
ed tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messa
ges.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number 
of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warn
ings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total
 number of release audit warnings.


Unit tests all passed.
",24/Sep/10 23:46;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sh output gets mixed up with the grunt prompt,PIG-1638,12474761,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,nrai,nrai,nrai,21/Sep/10 22:13,17/Dec/10 22:45,14/Mar/19 03:07,30/Sep/10 23:30,0.8.0,,,,,,,0.8.0,,,,grunt,,,0,,,,,,,,,,,,,"Many times, the grunt prompt gets mixed up with the sh output.e.g.
grunt> sh ls
000
autocomplete
bin
build
build.xml
grunt> CHANGES.txt
conf
contrib

In the above case,  grunt> is mixed up with the output.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Sep/10 22:24;nrai;PIG-1638_0.patch;https://issues.apache.org/jira/secure/attachment/12455198/PIG-1638_0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-09-30 22:59:07.376,,,no_permission,,,,,,,,,,,,165092,Reviewed,,,Thu Sep 30 23:30:25 UTC 2010,,,,,,,0|i0gy1r:,96946,,,,,,,,,,30/Sep/10 22:59;daijy;+1,30/Sep/10 23:30;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Combiner not use because optimizor inserts a foreach between group and algebric function,PIG-1637,12474739,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,21/Sep/10 18:50,17/Dec/10 22:45,14/Mar/19 03:07,29/Sep/10 05:28,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"The following script does not use combiner after new optimization change.

{code}
A = load ':INPATH:/pigmix/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
    as (user, action, timespent, query_term, ip_addr, timestamp, estimated_revenue, page_info, page_links);
B = foreach A generate user, (int)timespent as timespent, (double)estimated_revenue as estimated_revenue;
C = group B all; 
D = foreach C generate SUM(B.timespent), AVG(B.estimated_revenue);
store D into ':OUTPATH:';
{code}

This is because after group, optimizer detect group key is not used afterward, it add a foreach statement after C. This is how it looks like after optimization:
{code}
A = load ':INPATH:/pigmix/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
    as (user, action, timespent, query_term, ip_addr, timestamp, estimated_revenue, page_info, page_links);
B = foreach A generate user, (int)timespent as timespent, (double)estimated_revenue as estimated_revenue;
C = group B all; 
C1 = foreach C generate B;
D = foreach C1 generate SUM(B.timespent), AVG(B.estimated_revenue);
store D into ':OUTPATH:';
{code}

That cancel the combiner optimization for D. 

The way to solve the issue is to merge the C1 we inserted and D. Currently, we do not merge these two foreach. The reason is that one output of the first foreach (B) is referred twice in D, and currently rule assume after merge, we need to calculate B twice in D. Actually, C1 is only doing projection, no calculation of B. Merging C1 and D will not result calculating B twice. So C1 and D should be merged.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Sep/10 22:23;daijy;PIG-1637-1.patch;https://issues.apache.org/jira/secure/attachment/12455767/PIG-1637-1.patch,28/Sep/10 18:23;daijy;PIG-1637-2.patch;https://issues.apache.org/jira/secure/attachment/12455858/PIG-1637-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-09-28 22:25:12.728,,,no_permission,,,,,,,,,,,,165091,Reviewed,,,Wed Sep 29 05:28:54 UTC 2010,,,,,,,0|i0gy1j:,96945,,,,,,,,,,28/Sep/10 18:23;daijy;A bug caught by Xuefu. Reattach the patch.,"28/Sep/10 19:00;daijy;test-patch result for PIG-1637-2.patch:

     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
","28/Sep/10 22:25;xuefuz;+1

Patch looks good, except that we don't have to require that all output expressions in the first foreach contain only simple projection. As long as the output expression in the first foreach that is referenced multiple times in the second foreach contains only simple projection, the merge can proceed. Doing this, the following two loops may be better merged to one.

@@ -93,14 +93,17 @@
             // Otherwise, we may do expression calculation more than once, defeat the benefit of this
             // optimization
             Set<Integer> inputs = new HashSet<Integer>();
+            boolean duplicateInputs = false;
             for (Operator op : foreach2.getInnerPlan().getSources()) {
                 // If the source is not LOInnerLoad, then it must be LOGenerate. This happens when 
                 // the 1st ForEach does not rely on any input of 2nd ForEach
                 if (op instanceof LOInnerLoad) {
                     LOInnerLoad innerLoad = (LOInnerLoad)op;
                     int input = innerLoad.getProjection().getColNum();
-                    if (inputs.contains(input))
-                        return false;
+                    if (inputs.contains(input)) {
+                        duplicateInputs = true;
+                        break;
+                    }
                     else
                         inputs.add(input);
                     
@@ -109,6 +112,27 @@
                 }
             }
             
+            // Duplicate inputs in the case first foreach only containing LOInnerLoad and
+            // LOGenerate is allowed, and output plan is simple projection
+            if (duplicateInputs) {
+                Iterator<Operator> it1 = foreach1.getInnerPlan().getOperators();
+                while( it1.hasNext() ) {
+                    Operator op = it1.next();
+                    if(!(op instanceof LOGenerate) && !(op instanceof LOInnerLoad))
+                        return false;
+                    if (op instanceof LOGenerate) {
+                        List<LogicalExpressionPlan> outputPlans = ((LOGenerate)op).getOutputPlans();
+                        for (LogicalExpressionPlan outputPlan : outputPlans) {
+                            Iterator<Operator> iter = outputPlan.getOperators();
+                            while (iter.hasNext()) {
+                                if (!(iter.next() instanceof ProjectExpression))
+                                    return false;
+                            }
+                        }
+                    }
+                }
+            }
","28/Sep/10 22:38;daijy;Yes, it could be improved as per Xuefu's suggestion. Anyway, current patch solve the ""combiner not used"" issue, will commit this part first. I will open another Jira to improve it. Also, MergeForEach is a best example to practice cloning framework [PIG-1587|https://issues.apache.org/jira/browse/PIG-1587], so it is better to improve it once PIG-1587 is available.","29/Sep/10 05:28;daijy;All tests pass except for TestSortedTableUnion / TestSortedTableUnionMergeJoin for zebra, which are already fail and will be addressed by [PIG-1649|https://issues.apache.org/jira/browse/PIG-1649].

Patch committed to both trunk and 0.8 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scalar fail if the scalar variable is generated by limit,PIG-1636,12474736,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,21/Sep/10 18:38,17/Dec/10 22:45,14/Mar/19 03:07,22/Sep/10 18:49,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"The following script fail:
{code}
a = load 'studenttab10k' as (name: chararray, age: int, gpa: float);
b = group a all;
c = foreach b generate SUM(a.age) as total;
c1= limit c 1;
d = foreach a generate name, age/(double)c1.total as d_sum;
store d into '111';
{code}

The problem is we have a reference to c1 in d. In the optimizer, we push limit before foreach, d still reference to limit, and we get the wrong schema for the scalar.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Sep/10 23:34;daijy;PIG-1636-1.patch;https://issues.apache.org/jira/secure/attachment/12455209/PIG-1636-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-09-22 01:11:35.986,,,no_permission,,,,,,,,,,,,165090,Reviewed,,,Wed Sep 22 18:49:56 UTC 2010,,,,,,,0|i0gy13:,96943,,,,,,,,,,21/Sep/10 23:34;daijy;This patch depends on PIG-1605.,22/Sep/10 01:11;thejas;+1 ,"22/Sep/10 18:45;daijy;test-patch result:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

All tests pass.",22/Sep/10 18:49;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Logical simplifier does not simplify away constants under AND and OR; after simplificaion the ordering of operands of AND and OR may get changed,PIG-1635,12474709,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,yanz,yanz,yanz,21/Sep/10 15:35,17/Dec/10 22:45,14/Mar/19 03:07,25/Sep/10 01:09,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"b = FILTER a by (( f1 > 1) AND (1 == 1))

or 

b = FILTER a by ((f1 > 1) OR ( 1==0))

should be simplified to

b = FILTER a by f1 > 1;

Regarding ordering change, an example is that 

b = filter a by ((f1 is not null) AND (f2 is not null));

Even without possible simplification, the expression is changed to

b = filter a by ((f2 is not null) AND (f1 is not null));

Even though the ordering change in this case, and probably in most other cases, does not create any difference, but for two reasons some users might care about the ordering: if stateful UDFs are used as operands of AND or OR; and if the ordering is intended by the application designer to maximize the chances to shortcut the composite boolean evaluation. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Sep/10 04:33;yanz;PIG-1635.patch;https://issues.apache.org/jira/secure/attachment/12455231/PIG-1635.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-09-24 21:42:44.566,,,no_permission,,,,,,,,,,,,165089,,,,Sat Sep 25 01:09:47 UTC 2010,,,,,,,0|i0gy0n:,96941,,,,,,,,,,21/Sep/10 15:38;yanz;This is regarding a new feature (PIG-1399) added for 0.8.,"23/Sep/10 18:30;yanz;test-patch results:

     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.",23/Sep/10 18:36;yanz;All test-core tests also run clean.,"24/Sep/10 21:42;daijy;+1, patch looks good. Also can you have a review of all connect/disconnect usage in ExpressionSimplifer, according to [PIG-1644|https://issues.apache.org/jira/browse/PIG-1644]? I see lots of misuse in other rules.",24/Sep/10 22:02;yanz;I did a thorough check for this patch. Actually some of the ordering changes were caused by the mentioned misuse. Thanks.,24/Sep/10 22:11;daijy;+1 for commit.,25/Sep/10 01:09;yanz;Patch committed to both trunk and the 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The core jar in the tarball contains the kitchen sink ,PIG-1632,12474638,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,eli,eli,eli,20/Sep/10 23:26,04/Aug/11 00:34,14/Mar/19 03:07,23/Sep/10 20:10,0.8.0,0.9.0,,,,,,0.9.0,site,,,build,,,0,,,,,,,,,,,,,"The core jar in the tarball contains the kitchen sink, it's not the same core jar built by ant jar. This is problematic since other projects that want to depend on the pig core jar just want pig core, but pig-0.8.0-SNAPSHOT-core.jar in the tarball contains a bunch of other stuff (hadoop, com.google, commons, etc) that may conflict with the packages also on a user's classpath.

{noformat}
pig1 (trunk)$ jar tvf build/pig-0.8.0-SNAPSHOT-core.jar |grep -v pig|wc -l
12
pig1 (trunk)$ tar xvzf build/pig-0.8.0-SNAPSHOT.tar.gz
...
pig1 (trunk)$ jar tvf pig-0.8.0-SNAPSHOT/pig-0.8.0-SNAPSHOT-core.jar |grep -v pig|wc -l
4819
{noformat}

How about restricting the core jar to just Pig classes?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Sep/10 23:29;eli;pig-1632-1.patch;https://issues.apache.org/jira/secure/attachment/12455096/pig-1632-1.patch,22/Sep/10 20:05;eli;pig-1632-2.patch;https://issues.apache.org/jira/secure/attachment/12455296/pig-1632-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-09-22 19:30:02.273,,,no_permission,,,,,,,,,,,,66212,,,,Wed Sep 22 21:01:31 UTC 2010,,,,,,,0|i0gxzb:,96935,,,,,,,,,,"20/Sep/10 23:29;eli;Attached patch updates the package target so that the tarball, and therefore Pig release, just contain the Pig core jar.   If a Pig release needs to bundle Hadoop and a bunch of other stuff perhaps we could put those jars in lib instead of the core jar.

Running things like the tests out of a tarball that just includes the core jar works as these come in via ivy, anything else that needs to be tested? ","22/Sep/10 19:30;olgan;Hi Eli, thanks for the patch.

I don't think this is the approach we want to take. I think we should publish just core pig jar in maven since users have a way to pull the dependencies. However, as part of our release package we should include bundled pig.jar so that it works for users out of the box and they get exactly the version we have been testing for. I am fine if additionally we include the core jar as well if we do not do this already.","22/Sep/10 19:39;eli;Hey Olga,

Thanks for the feedback.    Agree that we want the out of box experience to use the same versions of other jars we've been testing with, but shouldn't that happen by bundling the necessary jars in eg the lib directory rather than embedding all the jars inside the core pig jar?

If people want all the dependencies bundled into a single jar, how about I update the patch so the release has two jars: a pig.jar which is like the current one (has all the other jars bundled in) and a pig-core.jar which just has pig?

Thanks,
Eli ",22/Sep/10 19:50;olgan;I am fine with your second proposal which is what I also suggested in my last comment. The first one makes it harder for the users to compile their UDFs,22/Sep/10 20:05;eli;Great. Patch attached. I verified the tarball produced by ant tar includes both a core jar that is just pig core and a pig jar that has everything. ,"22/Sep/10 20:12;olgan;+ 1, patch looks good. I will commit it to trunk and 0.8 branch shortly","22/Sep/10 21:01;olgan;patch committed to both 0.8 branch and trunk. Thanks, Eli for contributing!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
log this message at debug level : 'Pig Internal storage in use',PIG-1628,12474446,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,17/Sep/10 21:42,17/Dec/10 22:45,14/Mar/19 03:07,23/Sep/10 00:14,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"The temporary storage functions used are logging at the INFO level. This should change to debug level, they are reducing the visibility of more useful INFO messages. The messages include  'Pig Internal storage in use' from InterStorage and  'TFile storage in use' from TFileStorage.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Sep/10 12:43;thejas;PIG-1628.1.patch;https://issues.apache.org/jira/secure/attachment/12455137/PIG-1628.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-09-21 15:22:23.275,,,no_permission,,,,,,,,,,,,165085,Reviewed,,,Thu Sep 23 00:14:34 UTC 2010,,,,,,,0|i0gxxz:,96929,,,,,,,,,,"21/Sep/10 12:43;thejas;Patch passes unit tests and test-patch. Ready for review.
",21/Sep/10 15:22;yanz;+1. Patch looks good.,23/Sep/10 00:14;thejas;Patch committed to 0.8 branch and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flattening of bags with unknown schemas produces wrong schema,PIG-1627,12474436,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,alangates,alangates,17/Sep/10 20:01,04/Aug/11 00:34,14/Mar/19 03:07,24/Jan/11 18:53,0.7.0,,,,,,,0.9.0,,,,impl,,,0,,,,,,,,,,,,,"The following should produce an unknown schema:

{code}
A = load '/Users/gates/test/data/studenttab10';
B = group A by $0;
C = foreach B generate flatten(A);
describe C;
{code}

Instead it gives
{code}
C: {bytearray}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-12-24 02:36:29.77,,,no_permission,,,,,,,,,,,,66340,,,,Fri Apr 08 21:30:25 UTC 2011,,,,,,,0|i0gxxj:,96927,,,,,,,,,,"17/Sep/10 20:04;alangates;The problem is in the flatten, not the group.  The group has the proper schema (bytearray, bag{}).  Loading a bag of unknown schema and flattening it produces the same result.

Flattening a tuple of unknown content has the same problem as well.","24/Dec/10 02:36;daijy;In new logical plan, explain C we get:
C: (Name: LOForEach Schema: null)

Which does the right thing. Once we migrate describe to new logical plan, this should be fixed automatically.","24/Jan/11 18:53;daijy;PIG-1786 checked in. Retest and now we get:
Schema for C unknown.

Close the Jira.","08/Apr/11 08:35;mridulm@yahoo-inc.com;bytearray vs unknown schema use is always confusing.
The description in https://issues.apache.org/jira/browse/PIG-1876, for example, indicates that unknown schema implies it should be bytearray (desc starts with : ""Currently Pig map type is untyped, which means map value is always of bytearray(ie. unknown) type."" ..), while this JIRA seems to indicate it is not the case !

I have seen varying interpretations of what bytearray is supposed to mean in the jira's, pig docs and pig source code over the last 3+ years, not to mention in the various ilist's and user source codebass - some clarity in this regard would be good and less confusing.","08/Apr/11 21:30;daijy;We use bytearray for a field with unknown type. In the case we don't even know the number of fields, we use null schema (unknown schema). Yes, some clearance in the document is needed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to clarify how COUNT handles nulls,PIG-1626,12474433,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,olgan,olgan,17/Sep/10 19:47,17/Dec/10 22:45,14/Mar/19 03:07,25/Sep/10 00:00,,,,,,,,0.8.0,,,,documentation,,,0,,,,,,,,,,,,,"The current documentation just states: ""The COUNT function ignores NULL values. If you want to include NULL values in the count computation, use COUNT_STAR. ""

The new text should be something like

""The COUNT function follows syntax semantics and ignores nulls. What this means is that a tuple in the bag will not be counted if the first field in this tuple is NULL. If you want to include NULL values in the count computation, use COUNT_STAR. ""
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-09-25 00:00:12.133,,,no_permission,,,,,,,,,,,,165084,,,,Sat Sep 25 00:00:12 UTC 2010,,,,,,,0|i0gxx3:,96925,,,,,,,,,,25/Sep/10 00:00;chandec;Updates included in Pig-1600 -- See pig080-3.patch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Docs incorreclty say SAMPLE can be used in a nested FOREACH and do not mention projections in nested foreach,PIG-1625,12474421,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,alangates,alangates,17/Sep/10 18:15,17/Dec/10 22:45,14/Mar/19 03:07,08/Oct/10 22:02,0.7.0,,,,,,,0.8.0,,,,documentation,,,0,,,,,,,,,,,,,"Currently the docs in http://hadoop.apache.org/pig/docs/r0.7.0/piglatin_ref2.html#FOREACH say that SAMPLE can be used as an operator in nested foreach.  It cannot.

Also, they do not mention the ability to do projections inside nested foreach, such as the following:

{code}
A = load '/Users/gates/test/data/studenttab10';
B = group A all;
C = foreach B {
    C1 = A.$0;
    C2 = distinct C1;
    generate C2;
}
dump C;
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-10-08 22:02:15.584,,,no_permission,,,,,,,,,,,,165083,,,,Fri Oct 08 22:02:15 UTC 2010,,,,,,,0|i0gxwn:,96923,,,,,,,,,,"08/Oct/10 22:02;chandec;(1) Removed SAMPLE from nested_op description

(2) Examples using projection already in doc - changed label to read Example: Projection 

Thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FOREACH AS documentation is incorrect,PIG-1624,12474419,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,alangates,alangates,17/Sep/10 18:03,17/Dec/10 22:45,14/Mar/19 03:07,25/Sep/10 00:02,0.7.0,,,,,,,0.8.0,,,,documentation,,,0,,,,,,,,,,,,,"According to the Pig Latin manual (http://hadoop.apache.org/pig/docs/r0.7.0/piglatin_ref2.html#FOREACH) the correct usage of AS in a FOREACH clause is:

{code}
B = foreach A generate $0, $1, $2 as (user, age, gpa);
{code}

However, this is incorrect, and produce a syntax error.  The correct syntax for AS for FOREACH is:

{code}
B = foreach A generate $0 as user, $1 as age, $2 as gpa;
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-09-17 18:10:37.15,,,no_permission,,,,,,,,,,,,165082,,,,Sat Sep 25 00:02:53 UTC 2010,,,,,,,0|i0gxw7:,96921,,,,,,,,,,"17/Sep/10 18:08;alangates;I should note as well that when a flatten is involved, the proper syntax is:

{code}
A = load '/Users/gates/test/data/studenttab10' as (name:chararray, b{}, gpa:double);
B = foreach A generate name, flatten(b) as (fred, bob, joe), gpa;
dump B;
{code}

Note the use of parenthesis to enclose the list of fields coming from the flattened bag.",17/Sep/10 18:10;olgan;We are still updating docs so we should be able to get this in for 0.8,25/Sep/10 00:02;chandec;Updates included in Pig-1600 -- See pig080-3.patch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DEFINE streaming options are ill defined and not properly documented,PIG-1622,12474413,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,xuefuz,alangates,alangates,17/Sep/10 17:40,04/Aug/11 00:35,14/Mar/19 03:07,02/May/11 20:43,0.7.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"According to the documentation (http://hadoop.apache.org/pig/docs/r0.7.0/piglatin_ref2.html#DEFINE) the syntax for DEFINE when used to define a streaming command is:

DEFINE cmd INPUT(stdin|path) OUTPUT(stdout|stderr|path) SHIP(path [, path, ...]) CACHE (path [, path, ...])

However, the actual parser accepts something pretty different.  Consider the following script:

{code}
define strm `wc -l` INPUT(stdin) 
                    CACHE('/Users/gates/.vimrc#myvim') 
                    OUTPUT(stdin)
                    INPUT('/tmp/fred') 
                    OUTPUT('/tmp/bob')
                    SHIP('/Users/gates/.bashrc') 
                    SHIP('/Users/gates/.vimrc') 
                    CACHE('/Users/gates/.bashrc#mybash')
                    stderr('/tmp/errors' limit 10);

A = load '/Users/gates/test/data/studenttab10';
B = stream A through strm;
dump B;
{code}

The above actually parsers.  I see several issues here:

# What do multiple INPUT and OUTPUT statements mean in the context of streaming?  These should not be allowed.
# The documentation implies an order (INPUT, OUTPUT, SHIP, CACHE) that is not enforced by the parser.  We should either enforce the order in the parser or update the documentation.  Most likely the latter to avoid breaking existing scripts.
# Why are multiple SHIP and CACHE clauses allowed when each can take multiple paths?  It seems we should only allow one of each.
# The error clause is completely different that what is given in the documentation.  I suspect this is a documentation error and the grammar supported by the parser here is what we want.
",,,,,,,,,,,PIG-1618,,,,,,,,,,,,,,,,,,,,,25/Apr/11 16:30;xuefuz;PIG-1622-1.patch;https://issues.apache.org/jira/secure/attachment/12477303/PIG-1622-1.patch,26/Apr/11 17:10;xuefuz;PIG-1622-2.patch;https://issues.apache.org/jira/secure/attachment/12477424/PIG-1622-2.patch,23/Apr/11 17:40;xuefuz;PIG-1622.patch;https://issues.apache.org/jira/secure/attachment/12477217/PIG-1622.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-04-14 17:22:51.098,,,no_permission,,,,,,,,,,,,66149,Incompatible change,,,Tue Apr 26 17:15:26 UTC 2011,,,,,,,0|i0gxvb:,96917,,,,,,,,,,14/Apr/11 17:22;xuefuz;Discussed this with Alan and agreed that this needs to be correctly documented.,"22/Apr/11 15:27;xuefuz;Based on discussion with Olga, we need to limit that each command option can be specified at most once. Multiple occurrence of the same option results a semantic error when script is parsed. Patch enforcing this restriction will be provided soon.","23/Apr/11 17:40;xuefuz;Test-patch run:

     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
",25/Apr/11 16:30;xuefuz;Added a minor change for a test case.,25/Apr/11 23:27;thejas;+1,26/Apr/11 17:10;xuefuz;Update the patch with minor fix for a test case.,"26/Apr/11 17:15;xuefuz;Patch PIG-1622-2.patch is checked in for both trunk and 0.9.0. With this change, a command doesn't allow multiple occurrence of the same option. And this is backward incompatible change.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
What does EVAL keyword do?,PIG-1621,12474409,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,xuefuz,alangates,alangates,17/Sep/10 17:17,04/Aug/11 00:34,14/Mar/19 03:07,10/Mar/11 18:36,0.7.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"EVAL is listed as a keyword in Pig Latin, in both the documentation and the QueryParser.jjt file.  However, it has no productions in the grammar and no further mention in the documentation.  We need to either clarify what it does, or why we are reserving it as a keyword, or remove it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PIG-1618,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-10 18:36:52.598,,,no_permission,,,,,,,,,,,,66326,,,,Thu Mar 10 18:36:52 UTC 2011,,,,,,,0|i0gxuv:,96915,EVAL is no longer reserved as a keyword.,,,,,,,,,10/Mar/11 18:36;xuefuz;EVAL is no longer reserved as a keyword.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ARRANGE keyword should be deprecated,PIG-1620,12474405,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,xuefuz,alangates,alangates,17/Sep/10 17:08,04/Aug/11 00:34,14/Mar/19 03:07,10/Mar/11 18:00,0.7.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,ARRANGE is a synonym for ORDER in Pig Latin.  As far as I know no one uses it.  Its use is not documented.  And I am a strong fan of having one way to do things in programming languages.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PIG-1618,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-10 18:00:41.19,,,no_permission,,,,,,,,,,,,66331,,,,Thu Mar 10 18:00:41 UTC 2011,,,,,,,0|i0gxuf:,96913,ARRANGE is no longer reserved as a keyword.,,,,,,,,,10/Mar/11 18:00;xuefuz;Fixed as part of PIG-1618.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad error message when a double constant is incorrectly specified,PIG-1619,12474404,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,xuefuz,alangates,alangates,17/Sep/10 17:04,04/Aug/11 00:35,14/Mar/19 03:07,10/Mar/11 18:40,0.7.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"Given the following Pig Latin script (notice that the exponent for the floating point is a floating point when it should be a integer)

{code}
A = load '/Users/gates/test/data/studenttab10';
B = foreach A generate $0, 3.0e10.1;
dump B;
{code}

Pig returns
{code}
 ERROR 2999: Unexpected internal error. For input string: ""3.0e10.1""
{code}

This should be a syntax error caught by the parser.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PIG-1618,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-10 18:40:02.221,,,no_permission,,,,,,,,,,,,66140,,,,Thu Mar 10 18:40:02 UTC 2011,,,,,,,0|i0gxtz:,96911,,,,,,,,,,"10/Mar/11 18:40;xuefuz;Better error message is given now:

grunt> B = foreach A generate $0, 3.0e10.1;
2011-03-10 10:38:47,810 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 0: line 4:33 mismatched input [@58,161:162='.1',<86>,4:33] expecting SEMI_COLON
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'union onschema' does not use create output with correct schema when udfs are involved,PIG-1616,12474343,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,17/Sep/10 00:25,17/Dec/10 22:45,14/Mar/19 03:07,20/Sep/10 20:40,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"'union onshcema' creates a merged schema based on the input schemas. It does that in the queryparser, and at that stage the udf return type used is the default return type.  The actual return type for the udf is determined later in the TypeCheckingVisitor using EvalFunc.getArgsToFuncMapping().
'union onschema' should use the final type for its input relation to create the merged schema.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Sep/10 11:55;thejas;PIG-1616.1.patch;https://issues.apache.org/jira/secure/attachment/12455036/PIG-1616.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-09-20 20:30:13.337,,,no_permission,,,,,,,,,,,,165080,Reviewed,,,Mon Sep 20 20:40:01 UTC 2010,,,,,,,0|i0gxsn:,96905,,,,,,,,,,"17/Sep/10 16:09;thejas;Example to demonstrate the problem -
{code}
grunt> run /Users/tejas/pig_unions_udf/trunk/uudf.pig
grunt> l1 = load '/tmp/bag.txt' as (a, b : bag { t : tuple (i : int) } );   
grunt> f1 = foreach l1 generate a, MAX(b.i) as mx;  
grunt> describe f1;
f1: {a: bytearray,mx: int}

grunt> l2 = load '/tmp/bag.txt' as (a, b : bag { t : tuple (i : float) } );
grunt> f2 = foreach l2 generate a, COUNT(b.i) as mx;
grunt> describe f2;
f2: {a: bytearray,mx: long}

grunt> u = union onschema f1, f2;
grunt> describe u;
u: {a: bytearray,mx: double}
-- it should be u: {a: bytearray,mx: long}
{code}","20/Sep/10 11:55;thejas;PIG-1616.1.patch - calls LogicalPlanValidationExecutor.validate() to set the actual types, before the merged schema for 'union onschema' is created.
Passes unit tests and test-patch. Ready for review.
",20/Sep/10 20:30;rding;+1,20/Sep/10 20:40;thejas;Patch committed to trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Return code from Pig is 0 even if the job fails when using -M flag,PIG-1615,12474341,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,viraj,viraj,16/Sep/10 23:56,17/Dec/10 22:45,14/Mar/19 03:07,17/Sep/10 00:28,0.6.0,0.7.0,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"I have a Pig script of this form, which I used inside a workflow system such as Oozie.
{code}
A = load  '$INPUT' using PigStorage();
store A into '$OUTPUT';
{code}

I run this as with Multi-query optimization turned off :
{quote}
$java -cp ~/pig-svn/trunk/pig.jar:$HADOOP_CONF_DIR org.apache.pig.Main -p INPUT=/user/viraj/junk1 -M -p OUTPUT=/user/viraj/junk2 loadpigstorage.pig
{quote}

The directory ""/user/viraj/junk1"" is not present

I get the following results:
{quote}
Input(s):
Failed to read data from ""/user/viraj/junk1""
Output(s):
Failed to produce result in ""/user/viraj/junk2""
{quote}

This is expected, but the return code is still 0
{code}
$ echo $?
0
{code}

If I run this script with Multi-query optimization turned on, it gives, a return code of 2, which is correct.

{code}
$ java -cp ~/pig-svn/trunk/pig.jar:$HADOOP_CONF_DIR org.apache.pig.Main -p INPUT=/user/viraj/junk1 -p OUTPUT=/user/viraj/junk2 loadpigstorage.pig
...
$ echo $?
2
{code}

I believe a wrong return code from Pig, is causing Oozie to believe that Pig script succeeded.

Viraj",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-09-17 00:22:08.361,,,no_permission,,,,,,,,,,,,165079,,,,Fri Sep 17 01:06:08 UTC 2010,,,,,,,0|i0gxsf:,96904,,,,,,,,,,17/Sep/10 00:22;rding;This problem exists in Pig 0.7 and fixed in Pig 0.8.,"17/Sep/10 01:06;viraj;I tested this on Pig 0.8, but with a downloaded version, which was little old. 

I re-downloaded the latest source, seems to be fixed.

Viraj",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'union onschema' does handle some cases involving 'namespaced' column names in schema,PIG-1610,12474011,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,13/Sep/10 22:15,17/Dec/10 22:45,14/Mar/19 03:07,17/Sep/10 00:36,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"case 1:

grunt> describe f;              
f: {l1::a: bytearray,l1::b: bytearray}
grunt> describe l1;
l1: {a: bytearray,b: bytearray}
grunt> dump f;
(1,11)
(2,22)
(3,33)

grunt> dump l1;
(1,11)
(2,22)
(3,33)

grunt> u = union onschema f, l1;
grunt> describe u;
u: {l1::a: bytearray,l1::b: bytearray}

-- the dump u gives incorrect results
grunt> dump u;     
(,)
(,)
(,)
(1,11)
(2,22)
(3,33)



case 2:
grunt> u = union onschema l1, f;
grunt> describe u;
2010-09-13 15:11:13,877 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1108: Duplicate schema alias: l1::a
Details at logfile: /Users/tejas/pig_unions_err2/trunk/pig_1284410413970.log

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Sep/10 15:51;thejas;PIG-1610.1.patch;https://issues.apache.org/jira/secure/attachment/12454665/PIG-1610.1.patch,16/Sep/10 18:17;thejas;PIG-1610.2.patch;https://issues.apache.org/jira/secure/attachment/12454786/PIG-1610.2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-09-17 00:27:46.225,,,no_permission,,,,,,,,,,,,165075,Reviewed,,,Fri Sep 17 00:36:59 UTC 2010,,,,,,,0|i0gxqn:,96896,"This fixes the behavior for merging of column alias name that have a 'namespace' portion in them.

- Alias such as 'nm::c1' and 'c1' in two separate relations specified in 'union onschema' are considered mergeable and in the schema of the union, the merged column alias will be 'c1'. 
- Alias such as 'nm1::c1' and 'nm2::c1' in two separate relations specified in 'union onschema'  will not be merged together, in schema of the union there will be two columns with these names.

Example -

> describe f;
f: {l1::a: int, l1::b: int, l1::c: int}
> describe l1;
l1: {a: int, b: int}

> u = union onschema f,l1;
> desc u;
u: {a: int, b: int, l1::c: int}",,,,,,,,,"15/Sep/10 15:59;thejas;Test-patch and unit test cases have succeeded.
","15/Sep/10 19:51;thejas;Richard pointed out an issue with the patch where the schema of 'union onschema' differs with different order of relation in the statement. The case is like -
{code}
l = load 'x' as (c, nm::c);
f = load 'y' as (i,j);

u = union onschema f,l;
describe u;
u: {i: bytearray,j: bytearray,c: bytearray}

u = union onschema l,f;
describe u;
u: {c: bytearray,nm::c: bytearray,i: bytearray,j: bytearray}
{code}

Another issue found with the feature is that the schema of union is null when a column in one of the relations has a complex type with null inner schema.

I will submit another patch with fix for these issues.

","16/Sep/10 18:17;thejas;PIG-1610.2.patch fixes the issues mentioned in previous comment.
 passes unit tests and test-patch.
","17/Sep/10 00:26;thejas;There is a problem with 'union onschema' implementation that is not specific to this jira, I have created a new jira to address that - PIG-1616.
",17/Sep/10 00:27;rding;+1,"17/Sep/10 00:36;thejas;Patch committed to trunk and 0.8 branch.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'union onschema' should give a more useful error message when schema of one of the relations has null column name,PIG-1609,12473888,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,11/Sep/10 01:17,17/Dec/10 22:45,14/Mar/19 03:07,14/Sep/10 20:09,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"A better error message needs to be given in this case -
{code}
grunt> l = load '/tmp/empty.bag' as (i : int);
grunt> f = foreach l generate i+1;
grunt> describe f;
f: {int}
grunt> u = union onschema l , f;
2010-09-10 18:08:13,000 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Error merging
schemas for union operator
Details at logfile: /Users/tejas/pig_nmr_syn/trunk/pig_1284167020897.log

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Sep/10 19:54;thejas;PIG-1609.1.patch;https://issues.apache.org/jira/secure/attachment/12454477/PIG-1609.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-09-14 19:50:44.393,,,no_permission,,,,,,,,,,,,165074,Reviewed,,,Tue Sep 14 20:09:20 UTC 2010,,,,,,,0|i0gxq7:,96894,,,,,,,,,,"13/Sep/10 19:54;thejas;Pasting result of test patch for PIG-1609.1.patch
     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
","14/Sep/10 18:48;thejas;All unit tests passed in my run. Patch is ready for review.

  ",14/Sep/10 19:50;rding;+1,"14/Sep/10 20:09;thejas;Patch committed to 0.8 branch and trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig should always include pig-default.properties and pig.properties in the pig.jar,PIG-1608,12473880,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nrai,nrai,nrai,10/Sep/10 23:52,04/Aug/11 00:34,14/Mar/19 03:07,16/Sep/10 00:28,0.8.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,pig should always include pig-default.properties and pig.properties as a part of the pig.jar file,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Sep/10 22:19;nrai;PIG-1608_0.patch;https://issues.apache.org/jira/secure/attachment/12454608/PIG-1608_0.patch,15/Sep/10 19:00;nrai;PIG-1608_1.patch;https://issues.apache.org/jira/secure/attachment/12454688/PIG-1608_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-09-11 00:10:41.227,,,no_permission,,,,,,,,,,,,66234,Reviewed,,,Thu Sep 16 00:28:58 UTC 2010,,,,,,,0|i0gxpr:,96892,,,,,,,,,,11/Sep/10 00:10;olgan;pig-default is the only one we include. The other one is for users.,"13/Sep/10 18:02;daijy;pig should include pig-default.properties into pig.jar, but not pig.properties, just like hadoop does for core-default.xml, core-site.xml.","14/Sep/10 22:19;nrai;This patch will include pig-default.properties with each pig jar file, by default.",15/Sep/10 10:08;gkesavan;re-submiting patch to hudson ..,"15/Sep/10 17:51;daijy;Two comments:
1. target ""buildJar-withouthadoop"" should also include this change
2. format comment: use space instead of tab

Target ""jar"", ""package"" looks good.",15/Sep/10 19:00;nrai;updated patch to accommodate the review comments.,16/Sep/10 00:28;daijy;Patch committed to trunk. Thanks Niraj!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig should have separate javadoc.jar in the maven repository,PIG-1607,12473879,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nrai,nrai,nrai,10/Sep/10 23:50,17/Dec/10 22:45,14/Mar/19 03:07,30/Sep/10 22:50,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"At this moment, javadoc is part of the source.jar but pig should have separate javadoc.jar in the maven repository.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Sep/10 17:07;nrai;PIG-1607_0.patch;https://issues.apache.org/jira/secure/attachment/12454671/PIG-1607_0.patch,15/Sep/10 17:21;nrai;PIG-1607_1.patch;https://issues.apache.org/jira/secure/attachment/12454673/PIG-1607_1.patch,15/Sep/10 17:31;nrai;PIG-1607_2.patch;https://issues.apache.org/jira/secure/attachment/12454676/PIG-1607_2.patch,15/Sep/10 18:50;nrai;PIG-1607_3.patch;https://issues.apache.org/jira/secure/attachment/12454687/PIG-1607_3.patch,15/Sep/10 21:38;nrai;PIG-1607_4.patch;https://issues.apache.org/jira/secure/attachment/12454709/PIG-1607_4.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2010-09-15 17:42:07.768,,,no_permission,,,,,,,,,,,,165073,Reviewed,,,Thu Sep 30 22:50:05 UTC 2010,,,,,,,0|i0gxpb:,96890,,,,,,,,,,"15/Sep/10 17:07;nrai;Things fixed with this patch:
1. Created javadoc.jar 
2. Cleaned sources.jar . Removed the generated source and the zebra related files.
2. changed build.xml to upload the javadoc.jar to maven",15/Sep/10 17:21;nrai;fixed the javadoc-jar dependency,"15/Sep/10 17:42;rding;
The test result can be viewed here:

{code}
https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/pig/0.8.0-SNAPSHOT/
{code}","15/Sep/10 18:50;nrai;fixed the directory structure for javadoc.jar 

",15/Sep/10 21:38;nrai;fixed the package structure of the javadoc.,"30/Sep/10 18:27;gkesavan;looks good +1 

able to do mvn-install and mvn-deploy to install/deploy javadoc jar to the fs and apache mvn repo.

","30/Sep/10 22:50;thejas;Patch committed to 0.8 branch and trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flatten documentation does not discuss flatten of empty bag,PIG-1606,12473877,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,thejas,thejas,10/Sep/10 23:17,17/Dec/10 22:45,14/Mar/19 03:07,24/Sep/10 23:58,,,,,,,,0.8.0,,,,documentation,,,0,,,,,,,,,,,,,"From the existing flatten documentation, it is not clear that flatten of an empty bag results in that row being discarded .

For example the following query gives no output -
{code}
grunt> cat /tmp/empty.bag
{}      1
grunt> l = load '/tmp/empty.bag' as (b : bag{}, i : int);
grunt> f = foreach l generate flatten(b), i;
grunt> dump f;
grunt>
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-09-11 00:11:46.443,,,no_permission,,,,,,,,,,,,165072,,,,Fri Sep 24 23:58:56 UTC 2010,,,,,,,0|i0gxov:,96888,,,,,,,,,,11/Sep/10 00:11;olgan;Is this even the semantics we want. I would expect a single row with an empty field.,"11/Sep/10 01:08;alangates;flatten of an empty bag has to be a black hole else cogroup + foreach != inner join, which we claim it does.",15/Sep/10 17:31;olgan;If we are not planning to change the semantics I will ask Corinne to document for 0.8,24/Sep/10 23:58;chandec;Updates included in Pig-1600 -- See pig080-3.patch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding soft link to plan to solve input file dependency,PIG-1605,12473836,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,10/Sep/10 18:01,17/Dec/10 22:45,14/Mar/19 03:07,22/Sep/10 05:46,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"In scalar implementation, we need to deal with implicit dependencies. [PIG-1603|https://issues.apache.org/jira/browse/PIG-1603] is trying to solve the problem by adding a LOScalar operator. Here is a different approach. We will add a soft link to the plan, and soft link is only visible to the walkers. By doing this, we can make sure we visit LOStore which generate scalar first, and then LOForEach which use the scalar. All other part of the logical plan does not know the existence of the soft link. The benefits are:

1. Logical plan do not need to deal with LOScalar, this makes logical plan cleaner
2. Conceptually scalar dependency is different. Regular link represent a data flow in pipeline. In scalar, the dependency means an operator depends on a file generated by the other operator. It's different type of data dependency.
3. Soft link can solve other dependency problem in the future. If we introduce another UDF dependent on a file generated by another operator, we can use this mechanism to solve it. 
4. With soft link, we can use scalar come from different sources in the same statement, which in my mind is not a rare use case. (eg: D = foreach C generate c0/A.total, c1/B.count; )

Currently, there are two cases we can use soft link:
1. scalar dependency, where ReadScalar UDF will use a file generate by a LOStore
2. store-load dependency, where we will load a file which is generated by a store in the same script. This happens in multi-store case. Currently we solve it by regular link. It is better to use a soft link.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Sep/10 18:09;daijy;PIG-1605-1.patch;https://issues.apache.org/jira/secure/attachment/12455166/PIG-1605-1.patch,22/Sep/10 05:40;daijy;PIG-1605-2.patch;https://issues.apache.org/jira/secure/attachment/12455236/PIG-1605-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-09-13 18:28:03.888,,,no_permission,,,,,,,,,,,,165071,Reviewed,,,Wed Sep 22 05:46:25 UTC 2010,,,,,,,0|i0gxof:,96886,,,,,,,,,,"13/Sep/10 18:28;thejas;bq. 4. With soft link, we can use scalar come from different sources in the same statement, which in my mind is not a rare use case. (eg: D = foreach C generate c0/A.total, c1/B.count; )
This works with LOScalar as well. For above example, there will be two LOScalar operators preceding the LogicalOperator for D. It will be like - 

{code}
C -> LOScalar -> LOScalar -> D .
     A _^         B_^
{code}
","13/Sep/10 18:33;thejas;I think the first three benefits mentioned here are good reason to go for this approach instead of LOScalar.  
",13/Sep/10 19:10;alangates;How in depth are the changes to the graphing package and the walkers to handle this different type of edge in the graph?,"13/Sep/10 21:27;daijy;Changes are reasonably small. Here is a summary:
1. Add the following methods to the plan (both old and new):
{code}
public void createSoftLink(E from, E to)
public List<E> getSoftLinkPredecessors(E op)
public List<E> getSoftLinkSuccessors(E op)
{code}

2. All walkers need to change. When walker get predecessors/successors, it need to get both soft/regular link predecessors. The changes are straight forward, eg
from:
{code}
Collection<O> newSuccessors = mPlan.getSuccessors(suc);
{code}
to:
{code}
Collection<O> newSuccessors = mPlan.getSuccessors(suc);
newSuccessors.addAll(mPlan.getSoftLinkSuccessors(suc));
{code}

3. Change plan utility functions, such as replace, replaceAndAddSucessors, replaceAndAddPredecessors, etc
In new logical plan, there is no change since we only have minimum utility functions. In old logical plan, there should be some change to make those utility functions aware of soft link, but if we decide not support old logical plan going forward, no change needed, only need to note those utility functions does not deal with soft link within the function.

4. Change scalar to use soft link
This include creating soft link, maintaining soft link when doing transform (migrating to new plan, translating to physical plan). 

5. Change store-load to use soft link
This is an optional step. Currently we use regular link, conceptually we shall use soft link. It is Ok if we don't do this for now.

Also note in most cases, there is no soft link, the plan will behave just like before, so this change should be safe enough.","13/Sep/10 21:27;daijy;Yes, Thejas is right. The first 3 are the main reasons for the change.","22/Sep/10 01:04;thejas;Looks good. +1
Possible optimizations - (can be done in future )-
1. If column-pruning rule removes the relation-as-scalar column, then the soft-link can be removed.
2. split-filter rule will be disabled if it has a relation-as-scalar in the filter expression. If we filter expressions has the relation-as-scalar and update soft-links accordingly, we don't need to disable this rule.
","22/Sep/10 05:40;daijy;PIG-1605-2.patch fix findbug warnings.

test-patch result:
     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     -1 release audit.  The applied patch generated 455 release audit warnings (more than the trunk's current 453 warning
s).",22/Sep/10 05:46;daijy;Release audit warning is due to jdiff. No new file added. Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'relation as scalar' does not work with complex types ,PIG-1604,12473819,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,10/Sep/10 16:06,17/Dec/10 22:45,14/Mar/19 03:07,10/Sep/10 17:19,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"Statement such as 
sclr = limit b 1;
d = foreach a generate name, age/(double)sclr.mapcol#'it' as some_sum;

Results in the following parse error:
 ERROR 1000: Error during parsing. Non-atomic field expected but found atomic field
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Sep/10 16:08;thejas;PIG-1604.1.patch;https://issues.apache.org/jira/secure/attachment/12454307/PIG-1604.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-09-10 17:12:28.501,,,no_permission,,,,,,,,,,,,165070,Reviewed,,,Fri Sep 10 17:19:39 UTC 2010,,,,,,,0|i0gxnz:,96884,,,,,,,,,,"10/Sep/10 16:08;thejas;Patch passes unit tests. Pasting result of test-patch -
     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
","10/Sep/10 17:12;daijy;+1, patch looks good.","10/Sep/10 17:19;thejas;Patch committed to 0.8 branch and trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The .classpath of eclipse template still use hbase-0.20.0,PIG-1602,12473359,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,zjffdu,zjffdu,zjffdu,06/Sep/10 02:53,17/Dec/10 22:45,14/Mar/19 03:07,06/Sep/10 09:18,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"The .classpath of eclipse template still use hbase-0.20.0, it should be updated to hbase-0.20.6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Sep/10 02:56;zjffdu;PIG_1602.patch;https://issues.apache.org/jira/secure/attachment/12453913/PIG_1602.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-09-06 08:14:27.583,,,no_permission,,,,,,,,,,,,165068,,,,Mon Sep 06 09:18:00 UTC 2010,,,,,,,0|i0gxnj:,96882,,,,,,,,,,06/Sep/10 08:14;dvryaboy;+1,"06/Sep/10 09:18;zjffdu;Patch committed to both trunk and branch-0.8

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make scalar work for secure hadoop,PIG-1601,12473295,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,04/Sep/10 00:44,17/Dec/10 22:45,14/Mar/19 03:07,07/Sep/10 17:16,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"Error message:
open file
'hdfs://gsbl90890.blue.ygrid.yahoo.com/tmp/temp851711738/tmp727366271'; error =
java.io.IOException: Delegation Token can be issued only with kerberos or web
authentication at
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDelegationToken(FSNamesystem.java:4975)
at
org.apache.hadoop.hdfs.server.namenode.NameNode.getDelegationToken(NameNode.java:432)
at sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source) at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597) at
org.apache.hadoop.ipc.RPC$Server.call(RPC.java:523) at
org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1301) at
org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1297) at
java.security.AccessController.doPrivileged(Native Method) at
javax.security.auth.Subject.doAs(Subject.java:396) at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1062)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1295) at
org.apache.pig.impl.builtin.ReadScalars.exec(ReadScalars.java:66) at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:229)
at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:313)
at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(POCast.java:448)
at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(POCast.java:441)
at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Divide.getNext(Divide.java:72)
at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:358)
at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:291)
at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:236)
at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:231)
at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144) at
org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:638) at
org.apache.hadoop.mapred.MapTask.run(MapTask.java:314) at
org.apache.hadoop.mapred.Child$4.run(Child.java:217) at
java.security.AccessController.doPrivileged(Native Method) at
javax.security.auth.Subject.doAs(Subject.java:396) at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1062)
at org.apache.hadoop.mapred.Child.main(Child.java:211) ",,,,,,,,,,,,,,,,,,,PIG-1312,,,,,,,,,,,,,04/Sep/10 00:45;daijy;PIG-1601-1.patch;https://issues.apache.org/jira/secure/attachment/12453851/PIG-1601-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-09-07 16:58:17.586,,,no_permission,,,,,,,,,,,,165067,Reviewed,,,Tue Sep 07 17:16:03 UTC 2010,,,,,,,0|i0gxnb:,96881,,,,,,,,,,07/Sep/10 16:58;thejas;+1 ,07/Sep/10 17:16;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Development snapshot jar no longer picked up by bin/pig,PIG-1597,12473106,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,dvryaboy,dvryaboy,02/Sep/10 05:05,17/Dec/10 22:45,14/Mar/19 03:07,04/Sep/10 22:34,0.8.0,,,,,,,0.8.0,0.9.0,,,grunt,,,0,,,,,,,,,,,,,"As George Stathis poined out in PIG-1596, bin/pig no longer picks up development pig jars. This appears to have been introduced in PIG-1334, as the jar was renamed from -dev- to -SNAPSHOT-",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Sep/10 05:06;dvryaboy;PIG_1597.patch;https://issues.apache.org/jira/secure/attachment/12453652/PIG_1597.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-09-03 08:18:47.22,,,no_permission,,,,,,,,,,,,41707,,,,Sat Sep 04 22:34:12 UTC 2010,,,,,,,0|i0gxlz:,96875,,,,,,,,,,02/Sep/10 05:06;dvryaboy;Attached patch is trivial and does not require tests.,03/Sep/10 07:19;dvryaboy;Does anyone object to me just committing this to 0.8 and trunk?,03/Sep/10 08:18;zjffdu;+1,04/Sep/10 22:34;dvryaboy;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE's thrown when attempting to load hbase columns containing null values,PIG-1596,12473101,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,gstathis,gstathis,02/Sep/10 01:49,17/Dec/10 22:45,14/Mar/19 03:07,06/Sep/10 02:07,0.7.0,,,,,,,0.8.0,,,,data,,,0,,,,,,,,,,,,,"I'm not a committer, but I'd like to suggest the attached patch to handle loading hbase rows containing null cell values (since hbase is all about sparsly populated data rows). As it stands, a DataByteArray can be created with a null mData if a cell has no value, which causes NPEs by simply attempting to load a row containing the null cell in question.

PS: the attached patch also contains a slight change to the bin/pig executable to point to the build/pig\-\*\-SNAPSHOT.jar and not the build/pig\-\*\-dev.jar (the latter no longer seems to exist). If you prefer a separate patch for this, I'll be happy to submit it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Sep/10 03:40;zjffdu;PIG_1596.patch;https://issues.apache.org/jira/secure/attachment/12453651/PIG_1596.patch,02/Sep/10 05:45;zjffdu;PIG_1596_2.patch;https://issues.apache.org/jira/secure/attachment/12453654/PIG_1596_2.patch,02/Sep/10 01:49;gstathis;null_hbase_records.patch;https://issues.apache.org/jira/secure/attachment/12453647/null_hbase_records.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-09-02 03:38:06.347,,,no_permission,,,,,,,,,,,,165063,,,,Mon Sep 06 02:06:40 UTC 2010,,,Patch Available,,,,0|i0gxlj:,96873,,,,,,,,,,"02/Sep/10 03:38;zjffdu;George, thanks for your suggestion. And I believe you are using latest HBaseStorage in trunk. What you pointed at is really a problem, and I have another solution for this. If the cell is null,  we put an empty byte array in DataByteArray, I think it should been the LoadFunc's reponponslibity to handle null cell.


",02/Sep/10 03:40;zjffdu;Attach patch (modify HBaseStorage and add new TestCase),"02/Sep/10 04:17;gstathis;Works just as well with less files modified. Thanks Jeff. How about the bin/pig modification? Is it ok or am I missing a step in my build process? Without it, when I download trunk and build, I get NoClassDefFoundError errors out of the box when I attempt to rum in grunt mode.","02/Sep/10 05:01;dvryaboy;Jeff,
I think it's clearer if you insert null into the tuple, not an empty DataByteArray (and assertNull in the test)

George, the SNAPSHOT thing is a real bug, thanks for catching that, this happened when pig was made available through maven in PIG-1334.

I'll create a separate ticket for that.","02/Sep/10 05:45;zjffdu;Dmitriy, you are right. I updated the patch according your suggestion.

",03/Sep/10 07:18;dvryaboy;I will review on Friday.,"04/Sep/10 23:12;dvryaboy;+1

before committing, do you mind combining the new test with one of the existing ones? Trying to keep the test suite at under 24 hours :)","06/Sep/10 02:06;zjffdu;Combine the new test case with existing ones. And commit the patch to both trunk and branch-0.8

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
casting relation to scalar- problem with handling of data from non PigStorage loaders,PIG-1595,12473097,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,02/Sep/10 00:37,17/Dec/10 22:45,14/Mar/19 03:07,03/Sep/10 22:39,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"If load functions that don't follow the same bytearray format as PigStorage for other supported datatypes, or those that don't implement the LoadCaster interface are used in 'casting relation to scalar' (PIG-1434), it can cause the query to fail or create incorrect results.

The root cause of the problem is that there is a real dependency between the ReadScalars udf that returns the scalar value and the LogicalOperator that acts as its input. But the logicalplan does not capture this dependency. So in SchemaResetter visitor used by the optimizer, the order in which schema is reset and evaluated does not take this into consideration. If the schema of the input LogicalOperator does not get evaluated before the ReadScalar udf, the resutltype of ReadScalar udf becomes bytearray. POUserFunc will convert the input to bytearray using ' new DataByteArray(inp.toString().getBytes())'. But this bytearray encoding of other supported types might not be same for the LoadFunction associated with the column, and that can result in problems.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Sep/10 17:58;thejas;PIG-1595.1.patch;https://issues.apache.org/jira/secure/attachment/12453796/PIG-1595.1.patch,07/Sep/10 18:42;thejas;PIG-1595.2.patch;https://issues.apache.org/jira/secure/attachment/12454037/PIG-1595.2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-09-03 21:35:53.098,,,no_permission,,,,,,,,,,,,165062,Reviewed,,,Tue Sep 07 20:09:55 UTC 2010,,,,,,,0|i0gxl3:,96871,,,,,,,,,,"03/Sep/10 17:58;thejas;PIG-1595.1.patch 
- In this patch a new sublcass of DependencyOrderWalker has been created (DependencyOrderWalkerLPScalar) , when it chooses the sink nodes of the plan to start the walk, it chooses them in the order as determined by the dependency order resulting from the ReadScalars dependencies.
- The LOCast that was being added after ReadScalars to get expected type is no longer necessary and has been removed. 
- There is also a check in PigServer.mergeScalars() to see if the LOStore that the code attempts to re-use has the same store function - InterStorage which is used by ReadScalar udf to read the input .
- No new unit test case has been added as the test TestScalarAliases.testFilteredScalarDollarProj is a test case that was failing without the additional cast now succeeds without the cast.

Unit tests have passed. Test-patch result results are pasted below. Patch is ready for review.
     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.



","03/Sep/10 21:35;daijy;Patch looks good. This patch is to address the problem that we cannot get output schema of the scalar UDF at compile time. Another approach is write ReadScalars.outputSchema(), and use the input schema to figure out the output schema. But again we need to address the dependency to make sure input schema is correctly set before calling outputSchema(). So both approach should be equivalent.",03/Sep/10 22:39;thejas;Patch committed to trunk and 0.8 branch.,05/Sep/10 05:29;daijy;Patch break TestScalarAliases.testScalarErrMultipleRowsInInput. Comment out TestScalarAliases.testScalarErrMultipleRowsInInput temporarily.,"07/Sep/10 18:42;thejas; With changes in PIG-1595.1.patch, the column name gets propagated in the schema , so I have updated the test case to use different column names in the relation used as scalar so that it does not conflict with other column being projected.

All testScalarAlias unit tests pass. 

test-patch results -
     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.


",07/Sep/10 18:57;daijy;+1 for the test failure fix.,"07/Sep/10 20:09;thejas; 	PIG-1595.2.patch committed to trunk and 0.8 branch.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in new logical planner,PIG-1594,12473086,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,anhi,anhi,01/Sep/10 21:41,17/Dec/10 22:45,14/Mar/19 03:07,06/Sep/10 21:17,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"I've been testing the trunk version of Pig on Elastic MapReduce against our log processing sample application(1). When I try to run the query it throws a NullPointerException and suggests I disable the new logical plan. Disabling it works and the script succeeds. Here is the query I'm trying to run:

{code}
register file:/home/hadoop/lib/pig/piggybank.jar
  DEFINE EXTRACT org.apache.pig.piggybank.evaluation.string.EXTRACT();
  RAW_LOGS = LOAD '$INPUT' USING TextLoader as (line:chararray);
  LOGS_BASE= foreach RAW_LOGS generate FLATTEN(EXTRACT(line, '^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] ""(.+?)"" (\\S+) (\\S+) ""([^""]*)"" ""([^""]*)""')) as (remoteAddr:chararray, remoteLogname:chararray, user:chararray, time:chararray, request:chararray, status:int, bytes_string:chararray, referrer:chararray, browser:chararray);
  REFERRER_ONLY = FOREACH LOGS_BASE GENERATE referrer;
  FILTERED = FILTER REFERRER_ONLY BY referrer matches '.*bing.*' OR referrer matches '.*google.*';
  SEARCH_TERMS = FOREACH FILTERED GENERATE FLATTEN(EXTRACT(referrer, '.*[&\\?]q=([^&]+).*')) as terms:chararray;
  SEARCH_TERMS_FILTERED = FILTER SEARCH_TERMS BY NOT $0 IS NULL;
  SEARCH_TERMS_COUNT = FOREACH (GROUP SEARCH_TERMS_FILTERED BY $0) GENERATE $0, COUNT($1) as num;
  SEARCH_TERMS_COUNT_SORTED = LIMIT(ORDER SEARCH_TERMS_COUNT BY num DESC) 50;
  STORE SEARCH_TERMS_COUNT_SORTED into '$OUTPUT';
{code}

And here is the stack trace that results:

{code}
ERROR 2042: Error in new logical plan. Try -Dpig.usenewlogicalplan=false.

org.apache.pig.backend.executionengine.ExecException: ERROR 2042: Error in new logical plan. Try -Dpig.usenewlogicalplan=false.
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:285)
        at org.apache.pig.PigServer.compilePp(PigServer.java:1301)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1154)
        at org.apache.pig.PigServer.execute(PigServer.java:1148)
        at org.apache.pig.PigServer.access$100(PigServer.java:123)
        at org.apache.pig.PigServer$Graph.execute(PigServer.java:1464)
        at org.apache.pig.PigServer.executeBatchEx(PigServer.java:350)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:324)
        at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:111)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:168)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:140)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:90)
        at org.apache.pig.Main.run(Main.java:491)
        at org.apache.pig.Main.main(Main.java:107)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
Caused by: java.lang.NullPointerException
        at org.apache.pig.EvalFunc.getSchemaName(EvalFunc.java:76)
        at org.apache.pig.piggybank.impl.ErrorCatchingBase.outputSchema(ErrorCatchingBase.java:76)
        at org.apache.pig.newplan.logical.expression.UserFuncExpression.getFieldSchema(UserFuncExpression.java:111)
        at org.apache.pig.newplan.logical.optimizer.FieldSchemaResetter.execute(SchemaResetter.java:175)
        at org.apache.pig.newplan.logical.expression.AllSameExpressionVisitor.visit(AllSameExpressionVisitor.java:143)
        at org.apache.pig.newplan.logical.expression.UserFuncExpression.accept(UserFuncExpression.java:55)
        at org.apache.pig.newplan.ReverseDependencyOrderWalker.walk(ReverseDependencyOrderWalker.java:69)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:87)
        at org.apache.pig.newplan.logical.relational.LOGenerate.accept(LOGenerate.java:149)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:74)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:76)
        at org.apache.pig.newplan.logical.relational.LOForEach.accept(LOForEach.java:71)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:74)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:247)
        ... 18 more
================================================================================
{code}




1. http://developer.amazonwebservices.com/connect/entry.jspa?externalID=2729",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-09-06 21:17:55.789,,,no_permission,,,,,,,,,,,,165061,,,,Mon Sep 06 21:17:55 UTC 2010,,,,,,,0|i0gxkv:,96870,,,,,,,,,,06/Sep/10 21:17;daijy;This issue is fixed by PIG-1178-10.patch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"pig does not create a log file, if tje MR job succeeds but front end fails.",PIG-1591,12473076,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nrai,nrai,nrai,01/Sep/10 20:42,17/Dec/10 22:45,14/Mar/19 03:07,03/Sep/10 21:48,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"When I run this script:
A = load 'limit_empty.input_a' as (a1:int);
B = load 'limit_empty.input_b' as (b1:int);
C =COGROUP A by a1, B by b1;
C1 = foreach C { Alim = limit A 1; Blim = limit B 1; generate Alim, Blim; }
D1 = FOREACH C1 generate Alim,Blim, (IsEmpty(Alim)? 0:1), (IsEmpty(Blim)? 0:1), COUNT(Alim), COUNT(Blim);
dump D1;

The MR job succeeds but the pig job fails with the fillowing error:
2010-08-31 13:33:09,960 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
2010-08-31 13:33:09,962 [main] INFO  org.apache.pig.impl.io.InterStorage - Pig Internal storage in use
2010-08-31 13:33:09,963 [main] INFO  org.apache.pig.impl.io.InterStorage - Pig Internal storage in use
2010-08-31 13:33:09,963 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!
2010-08-31 13:33:09,964 [main] INFO  org.apache.pig.impl.io.InterStorage - Pig Internal storage in use
2010-08-31 13:33:09,965 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
2010-08-31 13:33:09,969 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1
2010-08-31 13:33:09,969 [main] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1
2010-08-31 13:33:09,973 [main] ERROR org.apache.pig.backend.hadoop.executionengine.HJob - java.lang.ClassCastException: java.lang.Integer cannot be cast to org.apache.pig.data.Tuple


since MR job is succeeded, so the pig does not create any log file, but it should still create a log file, giving the cause of failure in the pig.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Sep/10 20:46;nrai;pig_1591.patch;https://issues.apache.org/jira/secure/attachment/12453621/pig_1591.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-09-03 17:13:43.391,,,no_permission,,,,,,,,,,,,165059,Reviewed,,,Fri Sep 03 21:48:33 UTC 2010,,,,,,,0|i0gxk7:,96867,,,,,,,,,,03/Sep/10 17:13;daijy;+1. No unit test needed since it is about error message. Manually tested and it works. Will commit it shortly.,03/Sep/10 21:48;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add new properties to help and documentation,PIG-1585,12472983,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,01/Sep/10 00:01,17/Dec/10 22:45,14/Mar/19 03:07,02/Sep/10 00:03,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"New properties:

Compression:

pig.tmpfilecompression, default to false, tells if the temporary files should be compressed or not. If true, then 
pig.tmpfilecompression.codec specifies which compression codec to use. Currently, PIG only accepts ""gz"" and ""lzo"" as possible values. Since LZO is under GPL license, Hadoop may need to be configured to use LZO codec. Please refer to http://code.google.com/p/hadoop-gpl-compression/wiki/FAQ for details. 

Combining small files:

pig.noSplitCombination - disables combining multiple small files to the block size
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Sep/10 23:57;olgan;PIG-1585.patch;https://issues.apache.org/jira/secure/attachment/12453641/PIG-1585.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,165053,,,,Thu Sep 02 00:03:34 UTC 2010,,,,,,,0|i0gxhr:,96856,,,,,,,,,,"01/Sep/10 23:57;olgan;Since this is just a minor cosmetic patch, I am just planning to commit the changes to both the branch and the trunk without tests and review.",02/Sep/10 00:03;olgan;patch committed to both trunk and 0.8 branch. I also added LogicalExpressionSimplifier to the help,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deal with inner cogroup,PIG-1584,12472980,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,olgan,olgan,31/Aug/10 23:21,04/Aug/11 00:34,14/Mar/19 03:07,22/Apr/11 22:52,,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,The current implementation of inner in case of cogroup is in conflict with join. We need to decide of whether to fix inner cogroup or just remove the functionality if it is not widely used,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,66270,,,,Thu Apr 07 01:04:16 UTC 2011,,,,,,,0|i0gxhb:,96854,Docs updated. See PIG-1772 and patch pig-1772-beta2-1.patch,,,,,,,,,02/Mar/11 21:21;olgan;We need to document that this functionality is deprecated,"07/Apr/11 01:04;olgan;It looks like we already do not mention inner or outer keywords neither in the syntax of group/cogroup, nor in any discussions.

The only place I found the reference is in examples where I suggest to do the following:

- remove all examples from GROUP section that refer to INNER cogroup
- There is also example in the description of IsEmpty function that uses inner cogroup, it should be changed to an outer join as follows:

SSN = load 'ssn.txt' using PigStorage() as (ssn:long);

SSN_NAME = load 'students.txt' using PigStorage() as (ssn:long, name:chararray);

-- do a left out join of SSN with SSN_Name
X = JOIN SSN by ssn LEFT OUTER, SSN_NAME by ssn;

-- only keep those ssn's for which there is no name
Y = filter X by IsEmpty(SSN_NAME);
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
piggybank unit test TestLookupInFiles is broken,PIG-1583,12472967,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,31/Aug/10 20:33,17/Dec/10 22:45,14/Mar/19 03:07,01/Sep/10 23:00,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"Error message:
10/08/31 09:32:12 INFO mapred.TaskInProgress: Error from 
attempt_20100831093139211_0001_m_000000_3: 
org.apache.pig.backend.executionengine.ExecException: ERROR 2078: Caught 
error from UDF: org.apache.pig.piggybank.evaluation.string.LookupInFiles 
[LookupInFiles : Cannot open file one]
        at 
org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:262)
        at 
org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:283)
        at 
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:355)
        at 
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:291)
        at 
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:236)
        at 
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:231)
        at 
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
        at org.apache.hadoop.mapred.Child.main(Child.java:170)
Caused by: java.io.IOException: LookupInFiles : Cannot open file one
        at 
org.apache.pig.piggybank.evaluation.string.LookupInFiles.init(LookupInFiles.java:92)
        at 
org.apache.pig.piggybank.evaluation.string.LookupInFiles.exec(LookupInFiles.java:115)
        at 
org.apache.pig.piggybank.evaluation.string.LookupInFiles.exec(LookupInFiles.java:49)
        at 
org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:229)
        ... 10 more
Caused by: java.io.IOException: hdfs://localhost:47453/user/hadoopqa/one 
does not exist
        at 
org.apache.pig.impl.io.FileLocalizer.openDFSFile(FileLocalizer.java:224)
        at 
org.apache.pig.impl.io.FileLocalizer.openDFSFile(FileLocalizer.java:172)
        at 
org.apache.pig.piggybank.evaluation.string.LookupInFiles.init(LookupInFiles.java:89)
        ... 13 more
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,31/Aug/10 20:35;daijy;PIG-1583-1.patch;https://issues.apache.org/jira/secure/attachment/12453540/PIG-1583-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-08-31 21:41:45.45,,,no_permission,,,,,,,,,,,,165052,Reviewed,,,Wed Sep 01 23:00:54 UTC 2010,,,,,,,0|i0gxgv:,96852,,,,,,,,,,31/Aug/10 21:41;xuefuz;+1 Patch Looks Good.,31/Aug/10 23:26;gkesavan;submitting to hudson ,01/Sep/10 23:00;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Intermittent unit test failure for TestScriptUDF.testPythonScriptUDFNullInputOutput,PIG-1579,12472833,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,30/Aug/10 08:02,17/Dec/10 22:45,14/Mar/19 03:07,04/Oct/10 18:41,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"Error message:
org.apache.pig.backend.executionengine.ExecException: ERROR 0: Error executing function: Traceback (most recent call last):
  File ""<iostream>"", line 5, in multStr
TypeError: can't multiply sequence by non-int of type 'NoneType'

        at org.apache.pig.scripting.jython.JythonFunction.exec(JythonFunction.java:107)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:229)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:295)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:346)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:291)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:236)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:231)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
        at org.apache.hadoop.mapred.Child.main(Child.java:170)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30/Aug/10 08:05;daijy;PIG-1579-1.patch;https://issues.apache.org/jira/secure/attachment/12453407/PIG-1579-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,165049,,,,Tue Sep 28 22:13:34 UTC 2010,,,,,,,0|i0gxev:,96843,,,,,,,,,,"30/Aug/10 08:05;daijy;Attach a fix. However, this fix is shallow and may need an in-depth look. Commit the temporary fix and leave the Jira open.","28/Sep/10 22:13;daijy;Rollback the change and run test many times, all tests pass. Seems some change between r990721 and now (r1002348) fix this issue. Will rollback the change and close the Jira.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support to variable number of arguments in UDF,PIG-1577,12472763,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,olgan,olgan,28/Aug/10 00:36,07/Jul/14 18:08,14/Mar/19 03:07,25/Oct/13 05:17,0.6.0,,,,,,,0.13.0,,,,,,,0,,,,,,,,,,,,,"In the current implementation, functionality that allows to map arguments to classes does not support functions with variable number of arguments. Also it does not support funtions that can have variable (but fixed in number) number of arguments. 

This causes problems for string UDFs such as CONCAT that can take an arbitrary number of arguments or TRIM that can take 1,2, or 3 arguments",,,,,,,,,,,PIG-2057,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-10-16 19:32:44.994,,,no_permission,,,,,,,,,,,,46093,,,,Mon Oct 18 19:55:17 UTC 2010,,,,,,,0|i0gxe7:,96840,Fixed as part of PIG-3444.,,,,,,,,,"16/Oct/10 19:32;ciemo;Olga, can you clarify the issue?

I've been writing and using UDFs with multiple parameters for years so there must be some subtlety that is escaping me in this JIRA problem description.

Thanks.","18/Oct/10 17:27;daijy;The issue in this Jira is that Pig has no ability to declare input schema of the UDF with variable number of arguments (in getArgToFuncMapping). You can interpret input tuple arbitrarily and use that for variable number of arguments. But with getArgToFuncMapping, you can get UDF signature matching and early input schema checking.",18/Oct/10 19:55;olgan;The issue is that if your function takes sometimes two and sometimes three parameters and has getArgToFuncMapping it is likely to result in an error. If you don't use getArgToFuncMapping things work just fine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Complete the migration of optimization rule PushUpFilter including missing test cases,PIG-1575,12472727,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,27/Aug/10 19:17,17/Dec/10 22:45,14/Mar/19 03:07,06/Sep/10 05:39,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"The Optimization rule under the new logical plan, PushUpFilter, only does a subset of optimization scenarios compared to the same rule under the old logical plan. For instance, it only considers filter after join, but the old optimization also considers other operators such as CoGroup, Union, Cross, etc. The migration of the rule should be complete.

Also, the test cases created for testing the old PushUpFilter wasn't migrated to the new logical plan code base. It should be also migrated. (A few has been migrated in JIRA-1574.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Sep/10 00:38;xuefuz;jira-1575-1.patch;https://issues.apache.org/jira/secure/attachment/12453741/jira-1575-1.patch,03/Sep/10 18:29;xuefuz;jira-1575-2.patch;https://issues.apache.org/jira/secure/attachment/12453800/jira-1575-2.patch,03/Sep/10 23:23;xuefuz;jira-1575-3.patch;https://issues.apache.org/jira/secure/attachment/12453841/jira-1575-3.patch,04/Sep/10 06:07;xuefuz;jira-1575-4.patch;https://issues.apache.org/jira/secure/attachment/12453862/jira-1575-4.patch,06/Sep/10 05:39;daijy;jira-1575-5.patch;https://issues.apache.org/jira/secure/attachment/12453920/jira-1575-5.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2010-09-06 05:39:23.023,,,no_permission,,,,,,,,,,,,165048,Reviewed,,,Mon Sep 06 05:39:23 UTC 2010,,,,,,,0|i0gxdj:,96837,,,,,,,,,,03/Sep/10 18:29;xuefuz;Updated after fixing unit test failure.,03/Sep/10 23:24;xuefuz;regenerate the patch to include fixes for UDF and Dereference deep copy.,04/Sep/10 06:07;xuefuz;Add Fix for a failed test case. Only the test case itself gets changed.,"06/Sep/10 05:39;daijy;Patch looks good. Attach the final patch. 

test patch result:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

All tests pass.

Patch committed to both trunk and 0.8 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimization rule PushUpFilter causes filter to be pushed up out joins,PIG-1574,12472723,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,27/Aug/10 18:44,17/Dec/10 22:45,14/Mar/19 03:07,30/Aug/10 07:46,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"The PushUpFilter optimization rule in the new logical plan moves the filter up to one of the join branch. It does this aggressively by find an operator that has all the projection UIDs. However, it didn't consider that the found operator might be another join. If that join is outer, then we cannot simply move the filter to one of its branches.

As an example, the following script will be erroneously optimized:

        A = load 'myfile' as (d1:int);
        B = load 'anotherfile' as (d2:int);
        C = join A by d1 full outer, B by d2;        
        D = load 'xxx' as (d3:int);
        E = join C by d1, D by d3;        
        F = filter E by d1 > 5;
        G = store F into 'dummy';
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Aug/10 19:09;xuefuz;jira-1574-1.patch;https://issues.apache.org/jira/secure/attachment/12453262/jira-1574-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-08-30 07:46:30.234,,,no_permission,,,,,,,,,,,,165047,Reviewed,,,Mon Aug 30 07:46:30 UTC 2010,,,,,,,0|i0gxd3:,96835,,,,,,,,,,"30/Aug/10 07:46;daijy;test-patch result:
jira-1574-1.patch

     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

This patch does not push filter before join if the join is outer join. Actually we can push filter to the outer side of the join. I assume it will be addressed in PIG-1575.

Patch jira-1574-1.patch committed. Thanks Xuefu!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
change default datatype when relations are used as scalar to bytearray,PIG-1572,12472627,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,26/Aug/10 17:21,17/Dec/10 22:45,14/Mar/19 03:07,02/Sep/10 00:08,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"When relations are cast to scalar, the current default type is chararray. This is inconsistent with the behavior in rest of pig-latin.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30/Aug/10 23:58;thejas;PIG-1572.1.patch;https://issues.apache.org/jira/secure/attachment/12453477/PIG-1572.1.patch,01/Sep/10 00:19;thejas;PIG-1572.2.patch;https://issues.apache.org/jira/secure/attachment/12453562/PIG-1572.2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-09-01 22:52:58.768,,,no_permission,,,,,,,,,,,,165046,Reviewed,,,Thu Sep 02 00:38:24 UTC 2010,,,,,,,0|i0gxbz:,96830,"This changes the release note in PIG-1434, the part  ""Also, please, note that when the schema can't be inferred chararray rather than bytearray is used.""

The datatype of byetarray is used when schema can't be inferred.

",,,,,,,,,"30/Aug/10 23:58;thejas;Summary of changes
- Changed default type (ie type when input relation to scalar has not type) to bytearray.
- Replaced PigStorage with InterStorage for load/store of scalar data, so typed data is stored.
- Changes to track lineage of the ReadScalars udf to the load function(s).
- Removed unnecessary casts on output of ReadScalars
- ""describe alias;"" PigServer code now checks the alias of the leaf logical operators 
- Changed test cases - explicit cast no longer required when bytearray is used in arithmetic operations. Moved some of the tests to local mode to reduce test run time.
","01/Sep/10 00:19;thejas;PIG-1572.2.patch 
- Fixed loss of lineage information in translation during explain call
- Added cast on output of ReadScalars so that type information is not lost during schema reset from optimizer.

Unit tests and test-patch has passed. Patch is ready for review.

     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
","01/Sep/10 22:52;daijy;Patch looks good. One minor doubt is when we migrate to new logical plan, UserFuncExpression already have necessary cast inserted, seems we do not need to change new logical plan's UserFuncExpression.getFieldSchema(), am I right?","01/Sep/10 23:58;thejas;Yes, the changes to UserFuncExpression.getFieldSchema() are no longer required because the cast inserted to appropriate type. But while thinking about that I believe I have found an issue with the handling of non PigStorage load functions.
Since this patch address a bunch of issues I will commit it and create a new jira to address that, and also look at the utility of this change to UserFuncExpression.getFieldSchema().

","02/Sep/10 00:08;thejas;Patch committed to trunk.
",02/Sep/10 00:14;thejas;Patch committed to 0.8 branch as well .,"02/Sep/10 00:38;thejas;bq. Yes, the changes to UserFuncExpression.getFieldSchema() are no longer required because the cast inserted to appropriate type. But while thinking about that I believe I have found an issue with the handling of non PigStorage load functions.
Since this patch address a bunch of issues I will commit it and create a new jira to address that, and also look at the utility of this change to UserFuncExpression.getFieldSchema().

Created  PIG-1595 to address the issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
native mapreduce operator MR job does not follow same failure handling logic as other pig MR jobs,PIG-1570,12472613,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,26/Aug/10 15:50,17/Dec/10 22:45,14/Mar/19 03:07,31/Aug/10 00:09,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"The code path for handling failure in MR job corresponding to native MR is different and does not have the same behavior.
For example, even if the MR job for mapreduce operator fails, the number of jobs that failed is being reported as 0 in PigStats log.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30/Aug/10 17:24;thejas;PIG-1570.1.patch;https://issues.apache.org/jira/secure/attachment/12453442/PIG-1570.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-08-30 20:22:54.77,,,no_permission,,,,,,,,,,,,165044,Reviewed,,,Tue Aug 31 00:09:52 UTC 2010,,,,,,,0|i0gxbb:,96827,,,,,,,,,,"26/Aug/10 16:30;thejas;Another thing to investigate (somewhat related) - there seems to be a problem when PigServer is used to execute query having native mr operator -  i was unable to run the tests in local mode . But i am able to run query in local mode from commandline.
","30/Aug/10 17:23;thejas;Regarding 
bq. Another thing to investigate (somewhat related) - there seems to be a problem when PigServer is used to execute query having native mr operator - i was unable to run the tests in local mode . But i am able to run query in local mode from commandline.

The problem was that in test setup, the MiniCluster hadoop-site.xml (~/pigtest/conf/hadoop-site.xml) is in classpath. The WordCount.jar would end up trying to run the MR job using minicluster and fail, if rest of the test is using local mode.
","30/Aug/10 17:24;thejas;Patch passed test-patch and core tests. Patch is ready for review.
     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 5 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec]
","30/Aug/10 17:28;thejas;The code path that is followed in case of the native MR job is still different because the jar is a black box, and pig just calls the main function, pig doesn't even know if it is a MR job that is actually being run.
This fixes the pig stats reporting (log messages) for failed native MR job and also the feature list in the native MR job.
",30/Aug/10 20:22;rding;+1.,31/Aug/10 00:09;thejas;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java properties not honored in case of properties such as stop.on.failure,PIG-1569,12472605,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,thejas,thejas,26/Aug/10 14:52,17/Dec/10 22:45,14/Mar/19 03:07,30/Aug/10 23:08,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"In org.apache.pig.Main , properties are being set to default value without checking if the java system properties have been set to something else.
stop.on.failure, opt.multiquery, aggregate.warning are some properties that have this problem.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30/Aug/10 21:39;rding;PIG-1569.patch;https://issues.apache.org/jira/secure/attachment/12453464/PIG-1569.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-08-31 00:38:12.195,,,no_permission,,,,,,,,,,,,165043,Reviewed,,,Tue Aug 31 00:38:12 UTC 2010,,,,,,,0|i0gxav:,96825,,,,,,,,,,30/Aug/10 22:12;thejas;looks good. +1 ,31/Aug/10 00:38;rding;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimization rule FilterAboveForeach is too restrictive and doesn't handle project * correctly,PIG-1568,12472525,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,25/Aug/10 21:36,17/Dec/10 22:45,14/Mar/19 03:07,30/Aug/10 07:58,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"FilterAboveForeach rule is to optimize the plan by pushing up filter above previous foreach operator. However, during code review, two major problems were found:

1. Current implementation assumes that if no projection is found in the filter condition then all columns from foreach are projected. This issue prevents the following optimization:
    	A = LOAD 'file.txt' AS (a(u,v), b, c);
    	B = FOREACH A GENERATE $0, b;
    	C = FILTER B BY 8 > 5;
    	STORE C INTO 'empty';

2. Current implementation doesn't handle * probjection, which means project all columns. As a result, it wasn't able to optimize the following:
    	A = LOAD 'file.txt' AS (a(u,v), b, c);
    	B = FOREACH A GENERATE $0, b;
    	C = FILTER B BY Identity.class.getName(*) > 5;
    	STORE C INTO 'empty';",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Aug/10 17:39;xuefuz;jira-1568-1.patch;https://issues.apache.org/jira/secure/attachment/12453151/jira-1568-1.patch,26/Aug/10 00:26;xuefuz;jira-1568-1.patch;https://issues.apache.org/jira/secure/attachment/12453093/jira-1568-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-08-30 07:58:01.026,,,no_permission,,,,,,,,,,,,165042,Reviewed,,,Mon Aug 30 07:58:01 UTC 2010,,,,,,,0|i0gxaf:,96823,,,,,,,,,,"26/Aug/10 17:43;xuefuz;Regenerate the patch after fixing failed test case. The test case itself was changed as it uses an internal bug. When a UDF takes no argument, PIG backend passes the whole input to the UDF. This needs to be corrected. In another word, if a UDF doesn't specify any argument, we assume that it doesn't need any input. If a UDF needs all input, it can either specify a star (*). It can also list whatever it requires in the argument list.

A Jira tracking Pig backend changes will be created.
","30/Aug/10 07:58;daijy;test-patch result:

     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Patch committed. Thanks Xuefu!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some string functions don't work with bytearray arguments,PIG-1563,12472425,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,olgan,olgan,24/Aug/10 23:02,17/Dec/10 22:45,14/Mar/19 03:07,31/Aug/10 00:56,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"Script:

A = load 'studenttab10k' as (name, age, gpa);
C = foreach A generate SUBSTRING(name, 0,5);
E = limit C 10;
dump E;

Output is always empty:

()
()
()
()
()
()
()
()
()
()
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Aug/10 08:34;dvryaboy;PIG_1563.patch;https://issues.apache.org/jira/secure/attachment/12453020/PIG_1563.patch,28/Aug/10 00:57;olgan;PIG_1563_v2.patch;https://issues.apache.org/jira/secure/attachment/12453326/PIG_1563_v2.patch,31/Aug/10 00:53;olgan;PIG_1563_v3.patch;https://issues.apache.org/jira/secure/attachment/12453485/PIG_1563_v3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-08-25 00:21:19.267,,,no_permission,,,,,,,,,,,,41708,,,,Tue Aug 31 00:56:01 UTC 2010,,,,,,,0|i0gx8n:,96815,,,,,,,,,,"25/Aug/10 00:21;dvryaboy;Interesting -- this is a casting issue. If you specify name:chararray, it works. The log should be full of class cast exceptions..
Two ways to fix -- take out the classCast catch clause, or (better) write a BinSubstring EvalFunc that can do the conversions appropriately.","25/Aug/10 00:27;olgan;I think you just need to add the arg mapping function and the pig will insert the casts.
","25/Aug/10 00:34;olgan;The same needs to be done (and we need unit tests) for the following string manipulation functions:

INDEXOF
LAST_INDEX_OF
REPLACE
SPLIT
TRIM","25/Aug/10 05:32;dvryaboy;I wrote a unit test and it fails (due to classcastexceptions) even after I add 

{code}
    @Override
    public List<FuncSpec> getArgToFuncMapping() {
        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
        funcList.add(new FuncSpec(this.getClass().getName(), 
                new Schema(Lists.newArrayList(new Schema.FieldSchema(null, DataType.CHARARRAY),
                        new Schema.FieldSchema(null, DataType.INTEGER),
                        new Schema.FieldSchema(null, DataType.INTEGER)))));

        return funcList;
    }
{code}

to builtin.SUBSTRING","25/Aug/10 08:34;dvryaboy;Updated the functions with the appropriate argToFunc mappings, added tests.
Also tried on current build of trunk to make sure all the up- and down- casting magic works, and it does.

Note the new methods in test.Utils and pig.impl.util.Utils -- pretty handy.","27/Aug/10 19:15;olgan;I am looking into this to see if I can make it work without double wrapping. So far I got the easy case of trim to work. Will update the JIRA once I have more results
","27/Aug/10 20:03;olgan;I was able to make it successfully working (without wrapping) for the functions that have fixed number of arguments:

LAST_INDEX_OF
REPLACE
TRIM

I don't believe there is currently a way to make it work with variable number of args (even if the number of combinations is fixed.) Moreover, if we add the mapping table in this case, it breaks the case of typed data which is bad. This is the case with the remaining functions - INDEXOF and SPLIT.

So my suggestion is only to fix the first set of function and delay the rest to 0.9 when we fix the mapping code.

Dmitry and others, are you ok with this? If so, I can update the patch to reflect this.


",27/Aug/10 21:04;dvryaboy;Sounds good.  Should we just merge in the amazon contrib for some of these?,"27/Aug/10 21:08;olgan;which JIRA is that?

I will just get this in - I think that's all I have time today but I can look at the other one as well next week","27/Aug/10 21:16;dvryaboy;Olga, the amazon contrib is PIG-1565","28/Aug/10 01:00;olgan;Uploaded new patch which does the following:

(1) Adds mapping function for functions with fixed number of arguments: SUBSTRING, LAST_INDEX_OF, REPLACE,TRIM
(2) Left the rest of the functions alone which means that until 0.9 they will only work on typed data. CONCAT is in the same category
(3) Re-used applicable tests that Dmitry create, thanks!
(3) Added a couple of e2e tests to make sure that we test the mapping function as well

Please, review. 

We will keep the open till we address (2) in 0.9.

","28/Aug/10 01:25;dvryaboy;+1

question/comment -- any reason you discarded the new buildSimpleFuncSpec I wrote in the first iteration of this patch? I think it simplifies the code:

{code}
funcList.add(Utils.buildSimpleFuncSpec(
  this.getClass().getName(), DataType.CHARARRAY, DataType.CHARARRAY));
{code}

vs
{code}
Schema s = new Schema();
s.add(new Schema.FieldSchema(null, DataType.CHARARRAY));
s.add(new Schema.FieldSchema(null, DataType.CHARARRAY));
funcList.add(new FuncSpec(this.getClass().getName(), s));
{code}","31/Aug/10 00:35;olgan;Dmitry, thanks for the review. I did not discard your function - it was part of the patch. I did not change the code to use it just because I already finished testing the changes and did not have time to redo the code.

I am fixing some javadoc and release audit failures and will commit the code shortly.","31/Aug/10 00:49;olgan; +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 13 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec]
",31/Aug/10 00:52;olgan;I made one additional change and renamed SPLIT into STRSPLIT to avoid conflict with SPLIT operator,31/Aug/10 00:53;olgan;latest patch,31/Aug/10 00:56;olgan;patch committed. Thanks Dmitry for the help and review,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the version for the dependent packages for the maven ,PIG-1562,12472391,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nrai,nrai,nrai,24/Aug/10 16:49,17/Dec/10 22:45,14/Mar/19 03:07,13/Sep/10 17:53,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"We need to fix the set version so that, version is properly set for the dependent packages in the maven repository.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Sep/10 18:38;nrai;PIG-1562_1.patch;https://issues.apache.org/jira/secure/attachment/12454223/PIG-1562_1.patch,09/Sep/10 19:06;nrai;PIG-1562_2.patch;https://issues.apache.org/jira/secure/attachment/12454224/PIG-1562_2.patch,27/Aug/10 02:25;nrai;PIG_1562_0.patch;https://issues.apache.org/jira/secure/attachment/12453200/PIG_1562_0.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-09-09 20:45:50.502,,,no_permission,,,,,,,,,,,,165038,Reviewed,,,Mon Sep 13 17:53:22 UTC 2010,,,,,,,0|i0gx87:,96813,,,,,,,,,,27/Aug/10 02:25;nrai;This patch has fix for the version issue of the required packages.,09/Sep/10 19:06;nrai;fixed the path,"09/Sep/10 20:45;gkesavan;I verified this patch by uploading artifacts to the staging repo by using my gpg keys. This works fine.. 

But,
It publishes artifacts to org/apache/hadoop/pig-core/jar and pom artifacts.

i guess it should be pig and not pig-core

","09/Sep/10 21:38;gkesavan;It looks like the latest patch publishes as org/apache/hadoop/pig/jar's.
it looks good except for javadoc artifact as mentioned in pig-1334.","09/Sep/10 21:50;nrai;Thanks Giri. We will have separate Jira to track the javadoc jar.
",13/Sep/10 17:53;rding;Patch committed to both trunk and 0.8 branch. Thanks Niraj!.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XMLLoader in Piggybank does not support bz2 or gzip compressed XML files,PIG-1561,12472317,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,vivekp,viraj,viraj,24/Aug/10 00:50,25/Apr/11 21:27,14/Mar/19 03:07,20/Jan/11 01:17,0.7.0,0.8.0,,,,,,0.8.1,,,,impl,,,0,,,,,,,,,,,,,"I have a simple Pig script which uses the XMLLoader after the Piggybank is built.

{code}
register piggybank.jar;
A = load '/user/viraj/capacity-scheduler.xml.gz' using org.apache.pig.piggybank.storage.XMLLoader('property') as (docs:chararray);
B = limit A 1;
dump B;
--store B into '/user/viraj/handlegz' using PigStorage();
{code}


returns empty tuple
{code}
()
{code}

If you supply the uncompressed XML file, you get
{code}
(<property>
    <name>mapred.capacity-scheduler.queue.my.capacity</name>
    <value>10</value>
    <description>Percentage of the number of slots in the cluster that are
      guaranteed to be available for jobs in this queue.
    </description>    
  </property>)
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Jan/11 12:43;vivekp;PIG-1561-1.patch;https://issues.apache.org/jira/secure/attachment/12468366/PIG-1561-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-01-14 12:43:16.237,,,no_permission,,,,,,,,,,,,165037,Reviewed,,,Thu Jan 20 01:59:00 UTC 2011,,,,,,,0|i0gx7r:,96811,,,,,,,,,,14/Jan/11 12:43;vivekp;Attaching an initial patch for the issue. Please review. ,14/Jan/11 20:25;daijy;Patch looks good. The only concern is we mark it unsplitable. Did you find out why we cannot split? Neither bz2 and gz is splittable?,"19/Jan/11 15:35;vivekp;In the current XML loader, the behavior is that, the XMLLoaderBufferedPositionedInputStream reads the entire XML file without considering the split start and end locations.
 Hence if there is an XML > block size, the MR will execute multiple mappers but in all the mappers the loaders will load the entire XML file. 
 ie If i have an XML of size 256mb and the block size is 128mb there will be two mappers , but because of the loader, both the mappers will read the entire file regardless of the split boundaries . This is functionally wrong. This is the reason why I marked it as unsplitable.",20/Jan/11 01:17;daijy;All tests pass. Patch committed to trunk. Thanks Vivek!,20/Jan/11 01:59;daijy;Also commit to 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build target 'checkstyle' fails,PIG-1560,12472310,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,gkesavan,rding,rding,23/Aug/10 23:39,17/Dec/10 22:45,14/Mar/19 03:07,25/Aug/10 00:53,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"Stack trace:

{code}
/trunk/build.xml:894: java.lang.NoClassDefFoundError: org/apache/commons/logging/LogFactory
        at org.apache.commons.beanutils.ConvertUtilsBean.<init>(ConvertUtilsBean.java:130)
        at com.puppycrawl.tools.checkstyle.api.AutomaticBean.createBeanUtilsBean(AutomaticBean.java:73)
        at com.puppycrawl.tools.checkstyle.api.AutomaticBean.contextualize(AutomaticBean.java:222)
        at com.puppycrawl.tools.checkstyle.CheckStyleTask.createChecker(CheckStyleTask.java:372)
        at com.puppycrawl.tools.checkstyle.CheckStyleTask.realExecute(CheckStyleTask.java:304)
        at com.puppycrawl.tools.checkstyle.CheckStyleTask.execute(CheckStyleTask.java:265)
        at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)
        at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)
        at org.apache.tools.ant.Task.perform(Task.java:348)
        at org.apache.tools.ant.Target.execute(Target.java:390)
        at org.apache.tools.ant.Target.performTasks(Target.java:411)
        at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1360)
        at org.apache.tools.ant.Project.executeTarget(Project.java:1329)
        at org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)
        at org.apache.tools.ant.Project.executeTargets(Project.java:1212)
        at org.apache.tools.ant.Main.runBuild(Main.java:801)
        at org.apache.tools.ant.Main.startAnt(Main.java:218)
        at org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)
        at org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)
Caused by: java.lang.ClassNotFoundException: org.apache.commons.logging.LogFactory
        at org.apache.tools.ant.AntClassLoader.findClassInComponents(AntClassLoader.java:1386)
        at org.apache.tools.ant.AntClassLoader.findClass(AntClassLoader.java:1336)
        at org.apache.tools.ant.AntClassLoader.loadClass(AntClassLoader.java:1074)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
        ... 22 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Aug/10 16:26;gkesavan;pig-1560.patch;https://issues.apache.org/jira/secure/attachment/12452945/pig-1560.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-08-24 16:26:09.51,,,no_permission,,,,,,,,,,,,165036,,,,Tue Aug 24 17:26:52 UTC 2010,,,,,,,0|i0gx7b:,96809,,,,,,,,,,24/Aug/10 16:26;gkesavan;This patch fixes the checkstyle target build failure.,"24/Aug/10 17:26;olgan;please, commit",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Several things stated in Pig philosophy page are out of date,PIG-1559,12472304,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,alangates,alangates,alangates,23/Aug/10 21:49,17/Dec/10 22:45,14/Mar/19 03:07,24/Aug/10 17:15,0.7.0,,,,,,,0.8.0,,,,documentation,,,0,,,,,,,,,,,,,"The Pig philosophy page says several things that are no longer true (such as that Pig does not have an optimizer (it does now), that we someday hope to support streaming (we already do), that we some day hope to control splits (we don't, we just use what Hadoop gives us now)).  These need to be updated to reflect the current situation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Aug/10 22:17;alangates;PIG-1559.patch;https://issues.apache.org/jira/secure/attachment/12452867/PIG-1559.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-08-23 23:57:02.547,,,no_permission,,,,,,,,,,,,165035,,,,Tue Aug 24 17:47:31 UTC 2010,,,,,,,0|i0gx6v:,96807,,,,,,,,,,"23/Aug/10 23:57;olgan;+1, looks good",24/Aug/10 17:15;alangates;Patch checked in.,"24/Aug/10 17:33;olgan;Looks like limit issue I was seeing has been addressed in the latest trunk. 

I think we need to add unit tests to catch this things in the future.","24/Aug/10 17:47;olgan;sorry, wrong JIRA",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
build.xml for site directory does not work,PIG-1558,12472298,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,alangates,alangates,alangates,23/Aug/10 21:05,17/Dec/10 22:45,14/Mar/19 03:07,24/Aug/10 17:03,0.8.0,,,,,,,0.8.0,,,,build,,,0,,,,,,,,,,,,,"Going to the site directory and running ant produces:  

{code}
ant 
Buildfile: build.xml

clean:
   [delete] Deleting directory /Users/gates/src/pig/apache/site/author/build

update:

BUILD FAILED
/Users/gates/src/pig/apache/site/build.xml:6: Execute failed: java.io.IOException: Cannot run program ""forrest"" (in directory ""/Users/gates/src/pig/apache/site/author""): error=2, No such file or directory
{code}

Also, forrest here still requires Java 1.5, which can be fixed (see PIG-1508).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Aug/10 21:43;alangates;PIG-1558.patch;https://issues.apache.org/jira/secure/attachment/12452860/PIG-1558.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-08-23 21:50:13.236,,,no_permission,,,,,,,,,,,,165034,,,,Tue Aug 24 17:03:33 UTC 2010,,,,,,,0|i0gx6f:,96805,,,,,,,,,,"23/Aug/10 21:43;alangates;Attached patch makes it so that the ant invocation requires the user to specify the location of forrest.  Also, the validation phase of forrest is disabled so that Java 1.6 can be used.

Removal of the validation phase does not seem to impact creation of the web pages.",23/Aug/10 21:50;olgan;+1,24/Aug/10 17:03;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
couple of issue mapping aliases to jobs,PIG-1557,12472294,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,olgan,olgan,23/Aug/10 20:27,17/Dec/10 22:45,14/Mar/19 03:07,25/Aug/10 00:32,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"I have a simple script:

A = load '/user/pig/tests/data/singlefile/studenttab10k' as (name, age, gpa);
B = group A by name;
C = foreach B generate group, COUNT(A);
D = order C by $1;
E = limit D 10;
dump E;

I noticed a couple of issues with alias to job mapping: neither load(A) nor limit(E) shows in the output
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Aug/10 00:32;rding;PIG-1557.patch;https://issues.apache.org/jira/secure/attachment/12452881/PIG-1557.patch,25/Aug/10 00:13;rding;PIG-1557_1.patch;https://issues.apache.org/jira/secure/attachment/12452994/PIG-1557_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-08-24 00:32:35.51,,,no_permission,,,,,,,,,,,,165033,Reviewed,,,Tue Aug 31 00:41:55 UTC 2010,,,,,,,0|i0gx5z:,96803,,,,,,,,,,24/Aug/10 00:32;rding;The alias for load statement is missing. Add load alias to the alias list.,24/Aug/10 00:49;dvryaboy;+1.,"24/Aug/10 17:47;olgan;Looks like limit issue I was seeing has been addressed in the latest trunk. 

I think we need to add unit tests to catch this things in the future.

",25/Aug/10 00:13;rding;New patch adds a unit test.,"25/Aug/10 00:21;olgan;Looks good. Please, commit",31/Aug/10 00:41;rding;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nested describe failed when the alias is not referred in the first foreach inner plan,PIG-1552,12472130,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,20/Aug/10 22:11,17/Dec/10 22:45,14/Mar/19 03:07,23/Aug/10 21:32,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"The following script fail:

{code}
A = load 'studentab10k' as (name, age, gpa);
B = group A by name;
C = foreach B {
    D = distinct A.age;
    generate group, COUNT(D);
}
describe C::D;
{code}

If we remove group from generate statement, then it works",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Aug/10 22:27;daijy;PIG-1552-1.patch;https://issues.apache.org/jira/secure/attachment/12452677/PIG-1552-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-08-20 23:01:33.478,,,no_permission,,,,,,,,,,,,165030,Reviewed,,,Mon Aug 23 21:32:36 UTC 2010,,,,,,,0|i0gx3r:,96793,,,,,,,,,,20/Aug/10 23:01;aniket486;+1,"23/Aug/10 21:29;daijy;Unit test pass.

test-patch result:

     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
",23/Aug/10 21:32;daijy;Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
better error handling in casting relations to scalars,PIG-1550,12472051,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,olgan,olgan,20/Aug/10 00:29,17/Dec/10 22:45,14/Mar/19 03:07,02/Sep/10 23:28,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"I ran the following script:

Input data:

joe     100
sam     20
bob     134

Script:

A = load 'user_clicks' as (user: chararray, clicks: int);
B = group A by user;
C = foreach A generate group, SUM(A.clicks);
D = foreach A generate clicks/(double)C.$1;
dump C;

Since C contains more than 1 tuple, I expected to get an error which I did. However, the error was not very clear. When the job failed, I did see a valid error (however it lacked the error code): 210630 [main] ERROR org.apache.pig.tools.pigstats.PigStats  - ERROR 0: Scalar has more than one row in the output
 However at the end of processing, I saw a misleading error:

210709 [main] ERROR org.apache.pig.tools.grunt.Grunt  - ERROR 2088: Unable to get results for: hdfs://wilbur20.labs.corp.sp1.yahoo.com:9020/tmp/temp818551960/tmp1063730945:org.apache.pig.impl.io.InterStorage
10/08/19 17:16:22 ERROR grunt.Grunt: ERROR 2088: Unable to get results for: hdfs://wilbur20.labs.corp.sp1.yahoo.com:9020/tmp/temp818551960/tmp1063730945:org.apache.pig.impl.io.InterStorage
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Sep/10 17:17;thejas;PIG-1550.1.patch;https://issues.apache.org/jira/secure/attachment/12453697/PIG-1550.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-09-02 17:17:21.182,,,no_permission,,,,,,,,,,,,165029,,,,Thu Sep 02 23:28:06 UTC 2010,,,,,,,0|i0gx2v:,96789,,,,,,,,,,"02/Sep/10 17:17;thejas;PIG-1550.1.patch
test-patch has succeeded . unit tests are still running.
     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
","02/Sep/10 22:48;thejas;Unit tests have succeeded. Patch is ready for review.
","02/Sep/10 22:50;olgan;I will review the patch
","02/Sep/10 23:23;olgan;+1, looks good","02/Sep/10 23:28;thejas;Patch committed to trunk and 0.8 branch.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect assert statements in operator evaluation,PIG-1546,12471755,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,ajaykidave,ajaykidave,ajaykidave,16/Aug/10 22:29,17/Dec/10 22:45,14/Mar/19 03:07,17/Aug/10 20:43,0.7.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"The physical operator evaluation code path for <, <=, > and >= have incorrect assert statements. These asserts fail if the jvm have asserts enabled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Aug/10 22:40;ajaykidave;pig_1546.patch;https://issues.apache.org/jira/secure/attachment/12452222/pig_1546.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-08-17 20:35:33.59,,,no_permission,,,,,,,,,,,,165025,Reviewed,,,Tue Aug 17 20:43:12 UTC 2010,,,,,,,0|i0gx1j:,96783,,,,,,,,,,"16/Aug/10 22:29;ajaykidave;The failure stack trace is 

junit] java.lang.AssertionError [junit] at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.GreaterThanExpr.doComparison(GreaterThanExpr.java:150)
[junit] at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.GreaterThanExpr.getNext(GreaterThanExpr.java:104)
[junit] at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter.getNext(POFilter.java:148)
[junit] at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:232)
[junit] at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:227)
[junit] at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer
",16/Aug/10 22:40;ajaykidave;Patch fixing the incorrect asserts and adding a test case. The test case is valid only if hadoop processes are run under a jvm having asserts enabled. So enabled asserts for all test cases as part of this patch.,16/Aug/10 22:41;ajaykidave;Patch attached.,"17/Aug/10 20:35;pradeepkth;Results from running the ""test-patch"" ant target
    [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
",17/Aug/10 20:38;ajaykidave;All the units tests were run and passed.,17/Aug/10 20:43;pradeepkth;Patch committed - thanks Ajay!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IsEmpty returns the wrong value after using LIMIT,PIG-1543,12471460,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,zhijin,zhijin,12/Aug/10 20:59,17/Dec/10 22:45,14/Mar/19 03:07,03/Sep/10 21:49,0.7.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"1. Two input files:

1a: limit_empty.input_a
1
1
1

1b: limit_empty.input_b
2
2

2.
The pig script: limit_empty.pig

-- A contains only 1's & B contains only 2's
A = load 'limit_empty.input_a' as (a1:int);
B = load 'limit_empty.input_a' as (b1:int);

C =COGROUP A by a1, B by b1;
D = FOREACH C generate A, B, (IsEmpty(A)? 0:1), (IsEmpty(B)? 0:1), COUNT(A), COUNT(B);
store D into 'limit_empty.output/d';
-- After the script done, we see the right results:
-- {(1),(1),(1)}   {}      1       0       3       0
-- {}         {(2),(2)}      0       1       0       2

C1 = foreach C { Alim = limit A 1; Blim = limit B 1; generate Alim, Blim; }
D1 = FOREACH C1 generate Alim,Blim, (IsEmpty(Alim)? 0:1), (IsEmpty(Blim)? 0:1), COUNT(Alim), COUNT(Blim);
store D1 into 'limit_empty.output/d1';
-- After the script done, we see the unexpected results:
-- {(1)}   {}        1       1       1       0
-- {}      {(2)}     1       1       0       1

dump D;
dump D1;

3. Run the scrip and redirect the stdout (2 dumps) file. There are two issues:

The major one:

IsEmpty() returns FALSE for empty bag in limit_empty.output/d1/*, while IsEmpty() returns correctly in limit_empty.output/d/*.

The difference is that one has been applied with ""LIMIT"" before using IsEmpty().

The minor one:

The redirected output only contains the first dump:

({(1),(1),(1)},{},1,0,3L,0L)
({},{(2),(2)},0,1,0L,2L)

We expect two more lines like:
({(1)},{},1,1,1L,0L)
({},{(2)},1,1,0L,1L)

Besides, there is error says:

[main] ERROR org.apache.pig.backend.hadoop.executionengine.HJob - java.lang.ClassCastException: java.lang.Integer cannot be cast to org.apache.pig.data.Tuple
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Sep/10 19:28;daijy;PIG-1543-1.patch;https://issues.apache.org/jira/secure/attachment/12453611/PIG-1543-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-08-27 20:29:35.229,,,no_permission,,,,,,,,,,,,165024,Reviewed,,,Fri Sep 03 21:49:54 UTC 2010,,,,,,,0|i0gx0f:,96778,,,,,,,,,,27/Aug/10 20:29;olgan;Daniel can you check if this is related to limit optimizer and if it was addressed with new optimizer. (This can be done post branch since it is a bug split.),"30/Aug/10 19:06;daijy;This seems not a logical layer problem and new optimizer does not address it. It might related to [PIG-747|https://issues.apache.org/jira/browse/PIG-747], need further investigation.","01/Sep/10 19:28;daijy;This patch fix the first issue. The problem is we erroneously put a null in the bag when we expect an empty bag

The second issue is a side effect of first issue. BinInterSedes has the assumption that bag only contains tuple, so it does not expect a null inside bag. This issue is fixed automatically once first issue is in.","02/Sep/10 17:04;daijy;test-patch result:

     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

All tests pass",03/Sep/10 18:37;rding;+1. Looks good.,03/Sep/10 21:49;daijy;Patch committed to both trunk and 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
log level not propogated to MR task loggers,PIG-1542,12471360,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nrai,thejas,thejas,11/Aug/10 23:44,17/Dec/10 22:45,14/Mar/19 03:07,19/Oct/10 05:12,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"Specifying ""-d DEBUG"" does not affect the logging of the MR tasks .
This was fixed earlier in PIG-882 .",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Sep/10 22:20;nrai;PIG-1542.patch;https://issues.apache.org/jira/secure/attachment/12454497/PIG-1542.patch,14/Sep/10 00:19;nrai;PIG-1542_1.patch;https://issues.apache.org/jira/secure/attachment/12454502/PIG-1542_1.patch,29/Sep/10 00:23;nrai;PIG-1542_2.patch;https://issues.apache.org/jira/secure/attachment/12455890/PIG-1542_2.patch,13/Oct/10 01:00;nrai;PIG-1542_3.patch;https://issues.apache.org/jira/secure/attachment/12457034/PIG-1542_3.patch,18/Oct/10 23:13;daijy;PIG-1542_4.patch;https://issues.apache.org/jira/secure/attachment/12457501/PIG-1542_4.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2010-08-27 20:27:38.389,,,no_permission,,,,,,,,,,,,165023,Reviewed,,,Mon Oct 18 23:13:02 UTC 2010,,,,,,,0|i0gx07:,96777,Patch committed to both trunk and 0.8 branch.,,,,,,,,,27/Aug/10 20:27;olgan;This will be looked at after the branch since this is a regression and we don't have time to do it now.,"13/Sep/10 22:28;nrai;I have also changes the logic of setting the right log level, in case, the pig.logfile.level is passed from command line.
",14/Sep/10 00:19;nrai;Unset the hadoop debug messages and only printing the pig debug messages.,"01/Oct/10 17:55;nrai;All the tests: end to end, unit and test-patch tests passed. If Thejas does not have any feedback, please commit the patch.
Thanks
Niraj","01/Oct/10 22:07;thejas;Comment on the patch -
In case of log level settings, it is not possible to override the config file setting using command line options. In other cases, the command line values usually override what is specified in configuration file. For example in case of hadoop properties, this is what happens. This is also very convenient, because you can easily change the setting for a particular invocation of pig. You don't have to change config file which you might potentially share with other users.
","01/Oct/10 22:32;daijy;Yes, -d xxx should treat as -Ddebug=xxx. And system properties already have higher priority in the current code. (And in my mind, we should deprecate -d in favor of -Ddebug)","12/Oct/10 06:16;daijy;Is it possible to move ""pigContext.getLog4jProperties().setProperty( ""log4j.logger.org.apache.pig"", pigContext.getLog4jProperties().getProperty(""log4j.logger.org.apache.pig.level""));"" from PigMapBase to Main? It might be better to set things up in the frontend and just apply in the backend. ",13/Oct/10 01:00;nrai;Moved the log4j property setting to Main.java,18/Oct/10 23:13;daijy;I attach a simpler fix PIG-1542_4.patch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FR Join shouldn't match null values,PIG-1541,12471228,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,10/Aug/10 20:15,17/Dec/10 22:45,14/Mar/19 03:07,16/Aug/10 17:19,0.7.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"
Here is an example:

Data input:

{code}
1       1
        2
{code}

the script 

{code}
a = load 'input';
b = load 'input';
c = join a by $0, b by $0 using 'repl';
dump c; 
{code}

generates results that matches null values:

{code}
(1,1,1,1)
(,2,,2)
{code}

The regular join, on the other hand, gives the correct results.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Aug/10 01:11;rding;PIG-1541.patch;https://issues.apache.org/jira/secure/attachment/12451851/PIG-1541.patch,13/Aug/10 19:37;rding;PIG-1541_1.patch;https://issues.apache.org/jira/secure/attachment/12452047/PIG-1541_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-08-13 20:16:19.58,,,no_permission,,,,,,,,,,,,165022,Reviewed,,,Mon Aug 16 17:19:20 UTC 2010,,,,,,,0|i0gwzr:,96775,,,,,,,,,,"12/Aug/10 18:11;rding;
Results of test-patch:

{code}
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to i
     [exec] nclude 6 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{code}",13/Aug/10 19:37;rding;New patch to address the general case where the join key is tuple.,"13/Aug/10 20:16;thejas;+1 , please commit if tests are successful.
",16/Aug/10 17:19;rding;Tests are successful. The patch is committed to the trunk. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Column pruner causes wrong results when using both Custom Store UDF and PigStorage,PIG-1537,12470853,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,viraj,viraj,05/Aug/10 00:59,17/Dec/10 22:45,14/Mar/19 03:07,01/Sep/10 01:29,0.7.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"I have script which is of this pattern and it uses 2 StoreFunc's:

{code}
register loader.jar
register piggy-bank/java/build/storage.jar;
%DEFAULT OUTPUTDIR /user/viraj/prunecol/

ss_sc_0 = LOAD '/data/click/20100707/0' USING Loader() AS (a, b, c);

ss_sc_filtered_0 = FILTER ss_sc_0 BY
                        a#'id' matches '1.*' OR
                        a#'id' matches '2.*' OR
                        a#'id' matches '3.*' OR
                        a#'id' matches '4.*';

ss_sc_1 = LOAD '/data/click/20100707/1' USING Loader() AS (a, b, c);

ss_sc_filtered_1 = FILTER ss_sc_1 BY
                        a#'id' matches '65.*' OR
                        a#'id' matches '466.*' OR
                        a#'id' matches '043.*' OR
                        a#'id' matches '044.*' OR
                        a#'id' matches '0650.*' OR
                        a#'id' matches '001.*';

ss_sc_all = UNION ss_sc_filtered_0,ss_sc_filtered_1;

ss_sc_all_proj = FOREACH ss_sc_all GENERATE
                        a#'query' as query,
                        a#'testid' as testid,
                        a#'timestamp' as timestamp,
                        a,
                        b,
                        c;

ss_sc_all_ord = ORDER ss_sc_all_proj BY query,testid,timestamp PARALLEL 10;

ss_sc_all_map = FOREACH ss_sc_all_ord  GENERATE a, b, c;

STORE ss_sc_all_map INTO '$OUTPUTDIR/data/20100707' using Storage();

ss_sc_all_map_count = group ss_sc_all_map all;

count = FOREACH ss_sc_all_map_count GENERATE 'record_count' as record_count,COUNT($1);

STORE count INTO '$OUTPUTDIR/count/20100707' using PigStorage('\u0009');
{code}

I run this script using:

a) java -cp pig0.7.jar script.pig
b) java -cp pig0.7.jar -t PruneColumns script.pig

What I observe is that the alias ""count"" produces the same number of records but ""ss_sc_all_map"" have different sizes when run with above 2 options.

Is due to the fact that there are 2 store func's used?

Viraj",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-08-05 16:56:56.882,,,no_permission,,,,,,,,,,,,165019,,,,Thu Aug 05 22:53:18 UTC 2010,,,,,,,0|i0gwxr:,96766,,,,,,,,,,"05/Aug/10 16:56;olgan;Daniel, can we test if this is a problem with 0.8

Viraj, is this data specific and if so can you provide data tp reproduce. Also, do you know which one produces correct results.","05/Aug/10 22:53;viraj;Hi Olga, I have given the specific script with UDF's for Daniel to test.  Thanks Daniel for your help.
The script which does not use Column Pruner optimization or disables it using -t gives correct results.
Viraj",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Code discovering UDFs in the script has a bug in a order by case,PIG-1534,12470818,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pradeepkth,pradeepkth,pradeepkth,04/Aug/10 17:28,17/Dec/10 22:45,14/Mar/19 03:07,05/Aug/10 19:25,0.7.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"Consider the following commandline:
{noformat}
java -cp /tmp/svncheckout/pig.jar:udf.jar:clusterdir org.apache.pig.Main -e ""a = load 'studenttab' using udf.MyPigStorage(); b = order a by $0; dump b;""
{noformat}

Notice there is no ""register udf.jar"", instead udf.jar (which contains udf.MyPigStorage) is in the classpath. Pig handles this case by shipping udf.jar to the backend. However the above script with order by triggers the bug with the following error message:
 ERROR 2997: Unable to recreate exception from backed error: java.lang.RuntimeException: could not instantiate 'org.apache.pig.impl.builtin.RandomSampleLoader' with arguments '[udf.MyPigStorage, 100]'
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Aug/10 19:11;pradeepkth;PIG-1534.patch;https://issues.apache.org/jira/secure/attachment/12451252/PIG-1534.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-08-05 02:43:34.525,,,no_permission,,,,,,,,,,,,165017,Reviewed,,,Thu Aug 05 19:25:44 UTC 2010,,,,,,,0|i0gwwn:,96761,,,,,,,,,,"04/Aug/10 19:11;pradeepkth;Patch fixes SampleOptimizer to add the loadFunc funcspecs into the Mapreduce operators after optimization - this fixes the above order by error.

Here are results from running the test-patch target locally
[exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec]

The javadoc warning is present on trunk and not related to this patch:
{noformat}
...
 [javadoc] Standard Doclet version 1.6.0_01
  [javadoc] Building tree for all the packages and classes...
  [javadoc] /tmp/svncheckout/src/org/apache/pig/newplan/logical/expression/ProjectExpression.java:192: warning - @param argument ""currentOp"" is not a parameter name.
  [javadoc] Building index for all the packages and classes...
...
{noformat}
Will run unit tests locally and update with results.","05/Aug/10 02:43;pkamath;Ran all unit tests - TestScriptUDF fails but the failure is unrelated to the change in this patch and the failure occurs even with a fresh svn checkout.

Patch is ready for review.",05/Aug/10 17:20;daijy;+1. Verified that also solve skewed join case.,"05/Aug/10 19:25;pradeepkth;Thanks for the review Daniel, patch committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compression codec should be a per-store property,PIG-1533,12470688,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,03/Aug/10 00:35,17/Dec/10 22:45,14/Mar/19 03:07,05/Aug/10 17:34,0.7.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"
The following script with multi-query optimization

{code}
a = load 'input';
store a into 'outout.bz2';
store a into 'outout2'
{code}

generates two .bz files, while only one of them should be compressed.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Aug/10 18:17;rding;PIG-1533.patch;https://issues.apache.org/jira/secure/attachment/12451140/PIG-1533.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-08-04 04:43:58.213,,,no_permission,,,,,,,,,,,,165016,Reviewed,,,Thu Aug 05 17:21:47 UTC 2010,,,,,,,0|i0gww7:,96759,,,,,,,,,,"04/Aug/10 04:43;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12451140/PIG-1533.patch
  against trunk revision 981984.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/371/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/371/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/371/console

This message is automatically generated.",05/Aug/10 00:51;rding;Locally ran and passed core tests. ,05/Aug/10 17:21;daijy;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig gobbles up error messages,PIG-1531,12470576,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nrai,ashutoshc,ashutoshc,31/Jul/10 21:54,17/Dec/10 22:45,14/Mar/19 03:07,01/Oct/10 22:24,0.7.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"Consider the following. I have my own Storer implementing StoreFunc and I am throwing FrontEndException (and other Exceptions derived from PigException) in its various methods. I expect those error messages to be shown in error scenarios. Instead Pig gobbles up my error messages and shows its own generic error message like: 
{code}
010-07-31 14:14:25,414 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2116: Unexpected error. Could not validate the output specification for: default.partitoned
Details at logfile: /Users/ashutosh/workspace/pig/pig_1280610650690.log

{code}
Instead I expect it to display my error messages which it stores away in that log file.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Oct/10 17:51;nrai;PIG-1531_5.patch;https://issues.apache.org/jira/secure/attachment/12456145/PIG-1531_5.patch,24/Aug/10 22:54;nrai;PIG_1531.patch;https://issues.apache.org/jira/secure/attachment/12452983/PIG_1531.patch,28/Aug/10 00:57;nrai;PIG_1531_2.patch;https://issues.apache.org/jira/secure/attachment/12453327/PIG_1531_2.patch,30/Aug/10 00:33;ashutoshc;pig-1531_3.patch;https://issues.apache.org/jira/secure/attachment/12453396/pig-1531_3.patch,19/Sep/10 08:58;ashutoshc;pig-1531_4.patch;https://issues.apache.org/jira/secure/attachment/12454968/pig-1531_4.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2010-08-24 22:54:18.194,,,no_permission,,,,,,,,,,,,165014,Reviewed,,,Fri Oct 01 22:24:29 UTC 2010,,,,,,,0|i0gwvb:,96755,,,,,,,,,,"31/Jul/10 22:14;ashutoshc;This is because in InputOutputFileVisitor#visit() Pig defines its own {{errMsg}} String and uses that to throw PlanValidationException. It should use the error String of the Exception it has caught. 
I have not checked at other places. But I have a hunch that it happens at few other places in Pig  as well.  This is a real usability issue since generic message is usually useless and Pig misses an opportunity to provide an useful bit of information in error scenarios. From that point on, user has to go open the log file and scroll among tens of lines of stack trace and only if she is familiar with Pig will spot that error String. ","03/Aug/10 16:55;ashutoshc;Another instance where it happens is when input location doesnt exists, error message shown is 
{code}
org.apache.pig.backend.executionengine.ExecException: ERROR 2118: Unable to create input splits for tmp_emtpy_1280539088
{code}
Whereas underlying exception did have more useful String which gets lost in log file
{code}
org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist:
hdfs://machine.server.edu/tmp/pig/tmp_tables/tmp_empty_1280539088
{code}","24/Aug/10 22:54;nrai;Fixed the code where it gives the relevent error message rather than giving the hard coded one.
  ","25/Aug/10 18:55;ashutoshc;* In addition to error Msg, you also need to set error code on the exception you are throwing.
* Since you are catching exceptions thrown by user code (StoreFunc Interface) it is not safe to assume that e.getMessage() will be non-null or non-empty string. This will result in NPE. You need to check for it and provide a generic error Msg in those cases.
* Generic error msg should also contain output location String. Since if user didnt provide it, that wont get printed. So, you can reword the message as ""Output location validation failed for: <location>. More Information to follow:"" 
* Since, PigException extends from IOException. The IOException you are catching can also be a PigException, you need to test if it is and then set the message and error code.
* In case of non-existent input location I am still seeing the generic message ""ERROR 2997: Unable to recreate exception from backend error: org.apache.pig.backend.executionengine.ExecException: ERROR 2118: Unable to create input splits for: file:///Users/chauhana/workspace/pig-1531/a"" Though the full stack trace is printed at the end which contains the underlying error String. Its more confusing because now there are three different error messages amid a java stack trace.
* This warrants a testcase for regression purposes. (Infact error reporting behavior already changed since the time I opened this bug.)","28/Aug/10 00:57;nrai;I have tried to accommodate all the recommendations from Ashutosh. I have changed the existing test case to validate the error message, in case the store directory exist. Writing test case for the case, when input file deos not exist was  more effort than fixing the actual fix. So, I verified it manually and they looked good.
Thanks
Niraj","30/Aug/10 00:33;ashutoshc;I took a look of the latest patch. There are two minor problems. Firstly, pigExec was always null and never assigned a value, so it resulted in NPE in certain code path. Second, the boolean logic in PigInputFormat needs && instead of ||. I thought of correcting it and committing. But then realized hudson hasnt come back with results yet. So, I am uploading a new patch with those corrections and submitting to Hudson again. In this patch, I also refactored a code a bit, so its easier to read. Have a look and if its look fine to you. Can you run test-patch and unit tests and paste results here, so I can commit it.","31/Aug/10 01:57;ashutoshc;Niraj ran all the unit tests. All passed. No complaints from test-patch either. Committed to the trunk.
Thanks, Niraj !","19/Sep/10 08:44;ashutoshc;Peril of not writing unit test : Resurrection of bug. Argh..
",19/Sep/10 08:58;ashutoshc;Added a test-case which fails on trunk. Pig still gobbles up error messages. Fix is to rethrow the message in the hierarchy. Attached patch containis the test case and the fix.,"21/Sep/10 15:58;ashutoshc;Oh Hudson, oh well...

Ran the full suite of 400 minutes of unit tests; all passed. Patch is ready for review.","01/Oct/10 17:51;nrai;reviewed the patch and made the required changes after discussion with Ashutosh. Ran test-patch and unit test and everything looks fine.
Ashutosh, please commit the patch, if you don't have any further comment.
Thanks
Niraj","01/Oct/10 22:24;ashutoshc;Committed to both trunk and 0.8. Thanks, Niraj!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No need to deserialize UDFContext on the client side,PIG-1527,12470533,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,30/Jul/10 17:18,17/Dec/10 22:45,14/Mar/19 03:07,04/Aug/10 17:29,0.7.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Aug/10 00:39;rding;PIG-1527.patch;https://issues.apache.org/jira/secure/attachment/12451181/PIG-1527.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-08-04 16:41:02.164,,,no_permission,,,,,,,,,,,,165010,Reviewed,,,Wed Aug 04 17:11:48 UTC 2010,,,,,,,0|i0gwtr:,96748,,,,,,,,,,"04/Aug/10 16:41;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12451181/PIG-1527.patch
  against trunk revision 981984.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 406 release audit warnings (more than the trunk's current 405 warnings).

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/373/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/373/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/373/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/373/console

This message is automatically generated.",04/Aug/10 17:11;daijy;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect data generated by diff of SUM,PIG-1525,12470472,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,29/Jul/10 21:59,17/Dec/10 22:45,14/Mar/19 03:07,09/Aug/10 17:48,0.7.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"Given data;

input1:

{code}
id9     0
{code}

input2:

{code}
id8     1
id9     1
{code}

Pig script

{code}
A = LOAD 'input1' AS (id:chararray, val:long);
B = LOAD 'input2' AS (id:chararray, val:long);
C = COGROUP A BY id, B BY id;
D = FOREACH C GENERATE group, SUM(B.val), SUM(A.val), (SUM(A.val) - SUM(B.val));
dump D;
{code}

generates incorrect data:

{code}
(id8,1L,,)
(id9,1L,0L,-2L)
{code}

The workaround is to replace the FOREACH statement with

{code}
D = FOREACH C GENERATE group, SUM(B.val) as b, SUM(A.val) as a;
E = FOREACH D GENERATE $0, b, a, (a-b);
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Aug/10 17:49;rding;PIG-1525.patch;https://issues.apache.org/jira/secure/attachment/12451349/PIG-1525.patch,09/Aug/10 17:08;rding;PIG-1525_1.patch;https://issues.apache.org/jira/secure/attachment/12451607/PIG-1525_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-08-09 17:41:02.226,,,no_permission,,,,,,,,,,,,165008,Reviewed,,,Mon Aug 09 17:41:02 UTC 2010,,,,,,,0|i0gwsv:,96744,,,,,,,,,,30/Jul/10 18:56;rding;This problem affects other BinaryExpressionOperators as well.,"06/Aug/10 17:49;rding;

Results of running test-patch:

{code}
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{code}",06/Aug/10 18:55;rding;It turns out that the problem also affects the conditional operator (BinCond). ,"06/Aug/10 19:02;rding;The cause is the interaction between Accumulator UDF and binary operators. In the failure cases, the state kept by Accumulator is not reset cross record boundaries. ",09/Aug/10 17:08;rding;Thanks Thejas for suggesting a simple fix. The new patch passed core tests.,09/Aug/10 17:41;thejas;+1 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'Proactive spill count' is misleading,PIG-1524,12470388,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,28/Jul/10 23:14,17/Dec/10 22:44,14/Mar/19 03:07,19/Aug/10 15:56,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"InternalCacheBag, InternalSortedBag, InternalDistinctBag increment this counter for every record that it writes to disk, once it exceeds the memory limit. This number is misleading.

Instead, this counter should be increment it by 1 for each instance of these bags that has spilled to disk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Aug/10 16:07;thejas;PIG-1524.2.patch;https://issues.apache.org/jira/secure/attachment/12452410/PIG-1524.2.patch,18/Aug/10 20:02;thejas;PIG-1524.3.patch;https://issues.apache.org/jira/secure/attachment/12452448/PIG-1524.3.patch,17/Aug/10 00:49;thejas;PIG-1524.patch;https://issues.apache.org/jira/secure/attachment/12452237/PIG-1524.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-08-18 22:54:43.526,,,no_permission,,,,,,,,,,,,165007,Reviewed,,,Thu Aug 19 15:56:50 UTC 2010,,,,,,,0|i0gwsf:,96742,,,,,,,,,,"28/Jul/10 23:52;thejas;It will be useful to have a measure of number of records being spilled as well as the number of bags that spilled.
ie have two counters - 'proactive spill count' and 'proactively spilled record count' .
","17/Aug/10 00:49;thejas;In this patch (PIG-1524.patch) I have also re-factored the spill code in case of InternalSortedBag and InternalDistinctBag into a common super class SortedSpillBag.
I don't have any new test cases because the counter values will vary depending on current max memory.
","18/Aug/10 16:07;thejas;New patch with fix for issues found after more tests. 
","18/Aug/10 20:02;thejas;Patch with fix for a javadoc warning.
","18/Aug/10 22:19;thejas;All core, contrib tests pass .
Result of test-patch
     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     -1 release audit.  The applied patch generated 418 release audit warnings (more than the trunk's current 415 warnings).

As mentioned earlier, I don't have any new test cases because the counter values will vary depending on current max memory. 
The release audit -1 is caused by java doc jdiff changes.

Patch is ready for review.
",18/Aug/10 22:54;olgan;I am reviewing this patch,"18/Aug/10 23:30;olgan;+1 with a couple comment cleanups:

(1) Locking comment is misleading because we don't actually lock anything :)
(2) Comment regarding moving data from list to array for sorting needs to be also clarified.

Other than that, looks good. Please, commit",19/Aug/10 15:56;thejas;Patch with modified comments as per Olga's recommendation committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"explain plan does not show correct Physical operator in MR plan when POSortedDistinct, POPackageLite are used",PIG-1521,12470284,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,thejas,thejas,thejas,27/Jul/10 18:34,17/Dec/10 22:44,14/Mar/19 03:07,30/Jul/10 19:16,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"MR plan in explain shows PODistinct and Package (POPackage), when the operators POSortedDistinct and PackageLite (POPackageLite) are actually being used.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Jul/10 01:20;thejas;PIG-1521.patch;https://issues.apache.org/jira/secure/attachment/12450784/PIG-1521.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-07-29 15:35:42.654,,,no_permission,,,,,,,,,,,,165004,Reviewed,,,Fri Jul 30 19:16:07 UTC 2010,,,,,,,0|i0gwr3:,96736,,,,,,,,,,"29/Jul/10 15:35;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12450784/PIG-1521.patch
  against trunk revision 980276.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 11 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 409 release audit warnings (more than the trunk's current 406 warnings).

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/385/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/385/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/385/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/385/console

This message is automatically generated.","30/Jul/10 18:24;thejas;Core and contrib tests pass when run manually.
The release audit has 3 warnings, one is about the new golden file for test case which does not have license header, other two are about javadoc html file .
Patch is ready for review.

",30/Jul/10 18:53;rding;+1,"30/Jul/10 19:16;thejas;Patch committed to trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig needs to support keywords in the package name,PIG-1517,12470178,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,aniket486,aniket486,aniket486,26/Jul/10 16:34,17/Dec/10 22:44,14/Mar/19 03:07,29/Jul/10 00:36,,,,,,,,0.8.0,,,,grunt,,30/Sep/10 00:00,0,,,,,,,,,,,,,"Pig needs to support keywords in the package name. Pig supports most of the keywords as this was fixed in https://issues.apache.org/jira/browse/PIG-656. There are a few missing tokens like ""eq"",""gt"",""lt"",""gte"",""lte"",""neq"" that need to be supported.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Jul/10 23:22;aniket486;KeywordSupportName.patch;https://issues.apache.org/jira/secure/attachment/12450768/KeywordSupportName.patch,26/Jul/10 16:36;aniket486;pigusergroup656.patch;https://issues.apache.org/jira/secure/attachment/12450481/pigusergroup656.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-07-27 12:23:09.792,,,no_permission,,,,,,,,,,,,165000,,,,Thu Jul 29 05:59:43 UTC 2010,,,,,,,0|i0gwpj:,96729,,,,,,,,,,"27/Jul/10 12:23;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12450481/pigusergroup656.patch
  against trunk revision 979503.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/358/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/358/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/358/console

This message is automatically generated.","27/Jul/10 17:29;aniket486;This bug is an extension of https://issues.apache.org/jira/browse/PIG-656, does not need extra test cases.
Other tests pass manually.","27/Jul/10 17:38;olgan;Thanks, Aniket. I will review and commit the patch","27/Jul/10 23:17;olgan;I asked Aniket to add a test to make sure that we don't regress when we switch parsers. Once the new patch os submitted, I will review and commit it","29/Jul/10 00:28;olgan;+1, changes look good. Will be committing the patch shortly","29/Jul/10 00:36;olgan;patch committed; thanks, Aniket!","29/Jul/10 05:59;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12450768/KeywordSupportName.patch
  against trunk revision 980148.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/363/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/363/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/363/console

This message is automatically generated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
finalize in bag implementations causes pig to run out of memory in reduce ,PIG-1516,12470043,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,23/Jul/10 20:12,17/Dec/10 22:44,14/Mar/19 03:07,03/Aug/10 17:44,0.7.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"*Problem:*
pig bag implementations that are subclasses of DefaultAbstractBag, have finalize methods implemented. As a result, the garbage collector moves them to a finalization queue, and the memory used is freed only after the finalization happens on it.
If the bags are not finalized fast enough, a lot of memory is consumed by the finalization queue, and pig runs out of memory. This can happen if large number of small bags are being created.

*Solution:*
The finalize function exists for the purpose of deleting the spill files that are created when the bag is too large. But if the bags are small enough, no spill files are created, and there is no use of the finalize function.
 A new class that holds a list of files will be introduced (FileList). This class will have a finalize method that deletes the files. The bags will no longer have finalize methods, and the bags will use FileList instead of ArrayList<File>.

*Possible workaround for earlier releases:*
Since the fix is going into 0.8, here is a workaround -
Disabling the combiner will reduce the number of bags getting created, as there will not be the stage of combining intermediate merge results. But I would recommend disabling it only if you have this problem as it is likely to slow down the query .
To disable combiner, set the property: -Dpig.exec.nocombiner=true
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Jul/10 00:17;thejas;PIG-1516.2.patch;https://issues.apache.org/jira/secure/attachment/12450778/PIG-1516.2.patch,27/Jul/10 00:13;thejas;PIG-1516.patch;https://issues.apache.org/jira/secure/attachment/12450540/PIG-1516.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-07-26 05:25:35.831,,,no_permission,,,,,,,,,,,,164999,,,,Tue Aug 03 17:44:46 UTC 2010,,,,,,,0|i0gwpb:,96728,,,,,,,,,,23/Jul/10 20:45;thejas;Regarding the workaround - I would recommend disabling the combiner only if other steps such as increasing the heap size or increasing the number of reducers do not help.,"26/Jul/10 05:25;ankur;The solution to have the finalize method AT ALL for the purpose of deleting files when object is garbage collected is NOT a good one. Generally speaking using finalizers to release non-memory resources like file handles should be avoided as it has an insidious bug. From the article on ""Object finalization and Cleanup"" - http://www.javaworld.com/jw-06-1998/jw-06-techniques.html

""Don't rely on finalizers to release non-memory resources""

An example of an object that breaks this rule is one that opens a file in its constructor and closes the file in its finalize() method. Although this design seems neat, tidy, and symmetrical, it potentially creates an insidious bug. A Java program generally will have only a finite number of file handles at its disposal. When all those handles are in use, the program won't be able to open any more files.  ","26/Jul/10 17:17;dvryaboy;Another workaround for the meantime:

One can introduce a SmallBagFactory that inherits from BagFactory and produces SmallBags which implement DataBag() without a finalize, and does not implement the file spilling behavior.  SmallBagFactory would return SmallBags when bagFactory.newDefaultBag() is called. Then, provide the system properties pig.data.bag.factory.name and pig.data.bag.factory.jar in pig.properties to point to the new classes. 

Naturally, one has to be certain that databags won't need to spill to disk when doing this...


Ankur -- so what are you suggesting as a fix that avoids finalize? ","26/Jul/10 19:24;scott_carey;You avoid finalize() by using a WeakReference. There is no situation that you can't substitute a weak reference for a finalizer, other than object resurrection which is a really bad idea.

finalize() should be avoided for any case that might create many objects.  Its OK to ask the GC to deal with a small number of objects that themselves don't hold many resources.  Its bad form to use finalize() in any case where throughput is high or the object is potentially large.  It will kill performance and  thrash GC.

Extend WeakReference and put the things you need to clean up in it as member variables. Those should also have a strong reference from the bag  the WeakReference should be strongly referenced from the Bag too.   When the Bag is GC'd the objects of interest will no longer have any reference to them other than the WeakReference, and the WeakReference will no longer be strongly referenced.  The WeakReference will be placed onto a Queue of your choosing, and you can then process the queue and access the data required to do any cleanup. 
 
Unlike a finalizer, the actual object is released when GC happens and does not linger.  Only the WeakReference and what it holds onto remains, and you get notified (via the queue) when the object is gone.  Therefore, you have control over your resources and do not rely on the JVM to run the finalizer. 

I have seen performance improvements of ~10x due to moving high volume finalizers to a weak reference queue implementation, along with significantly lower memory consumption.","27/Jul/10 00:13;thejas;I haven't removed the use of finalize in this patch,  but with the patch the number of objects with finalize() method that get created should be much smaller, and if all the bags used in a query are small enough, there will not be any such objects created. 
Even in case of large bags that spill to disk, since finalize() is not a method of the bags, the tuples in the bags can be freed by GC without waiting on finalization.
This should stop queries from running out of memory because of of the wait on finalize().

The creation of spill files happens only if the bag is very large, and the processing of the tuples in those bags is likely to give enough time for the finalization thread to catch up. 

The changes - 
1. As I proposed in the solution, the finalize has been removed from DefaultAbstractBag and its subclasses, and a FileList class with a finalize is used as container for the list of spill files.
2. Removed the finalize() method in InternalCachedBag.CachedBagIterator . It was used to call close on DataInputStream. The DataInputStream contains a FileInputStream which would have non-memory resources to be freed. But the FileInputStream already has a finalize() method, so the finalize() method in InternalCachedBag.CachedBagIterator is unnecessary.
3. In the bags that have code to pre-merge files when there are large number of spill files, the files that have been merged into larger files are deleted.


Using WeakReferences as Scott suggested, we can get rid of the finalization completely. I have created a separate jira for that - PIG-1519 .
","29/Jul/10 00:17;thejas;New patch with fix for findbugs warnings.
I have also run large queries that spill to disk to test the changes in handling of mSpillFiles. 
","29/Jul/10 11:45;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12450778/PIG-1516.2.patch
  against trunk revision 980276.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 402 release audit warnings (more than the trunk's current 400 warnings).

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/364/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/364/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/364/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/364/console

This message is automatically generated.","02/Aug/10 16:22;thejas;The core and contrib tests pass on my machine.
The release audit warning is about javadoc html files.
Patch is ready for review.
",03/Aug/10 17:16;ashutoshc;+1. Changes look good.,03/Aug/10 17:44;thejas;Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig doesn't handle empty input directory,PIG-1513,12469941,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,22/Jul/10 18:49,24/Feb/17 16:01,14/Mar/19 03:07,30/Jul/10 16:56,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"
The following script

{code}
A = load 'input';
B = load 'emptydir';
C = join B by $0, A by $0 using 'skewed';
store C into 'output';
{code}

fails with ""ERROR: java.lang.RuntimeException: Empty samples file';

In this case, the sample job has 0 maps.  Pig doesn't expect this and fails . 

For merge join the script

The merge join script

{code}
A = load 'input';
B = load 'emptydir';
C = join A by $0, B by $0 using 'merge';
store C into 'output';
{code}

the sample job again has 0 maps and the script  fails with "" ERROR 2176: Error processing right input during merge join"".

But if we change the join order: 

{code}
A = load 'input';
B = load 'emptydir';
C = join B by $0, A by $0 using 'merge';
store C into 'output';
{code}

The second job (merge) now has 0 maps and 0 reduces. And it generates an empty 'output' directory.

Order by on empty directory works fine and generates empty part files.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Jul/10 17:05;rding;PIG-1513.patch;https://issues.apache.org/jira/secure/attachment/12450727/PIG-1513.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-07-22 18:58:17.67,,,no_permission,,,,,,,,,,,,164996,Reviewed,,,Thu Jul 29 22:01:29 UTC 2010,,,,,,,0|i0gwof:,96724,,,,,,,,,,22/Jul/10 18:58;olgan;Are we sure that the problem only occurs with skewed join? I would like to make this JIRA more generic and to make sure that pig returns empty results given empty input and short circuits the processing as early as possible,22/Jul/10 22:27;rding;Changed the JIRA title to deal with general problem of empty input directory handling.,"29/Jul/10 00:05;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12450727/PIG-1513.patch
  against trunk revision 979918.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/383/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/383/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/383/console

This message is automatically generated.",29/Jul/10 18:00;rding;Manually ran and passed all core tests.,"29/Jul/10 22:01;thejas;+1.  please commit.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PlanPrinter does not print LOJoin operator in the new logical optimization framework,PIG-1512,12469893,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,swati.j,swati.j,swati.j,22/Jul/10 08:10,17/Dec/10 22:44,14/Mar/19 03:07,27/Aug/10 21:37,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"PlanPrinter does not print LOJoin relational operator. As such, the LOJoin operator would not get printed when we do an explain.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Jul/10 08:16;swati.j;printJoin.patch;https://issues.apache.org/jira/secure/attachment/12450145/printJoin.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-07-27 12:14:11.893,,,no_permission,,,,,,,,,,,,164995,,,,Fri Aug 27 21:37:54 UTC 2010,,,Patch Available,,,,0|i0gwnz:,96722,,,,,,,,,,22/Jul/10 08:13;swati.j;Fix tab character,"22/Jul/10 08:16;swati.j;Attach the right file, final upload.","27/Jul/10 12:14;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12450145/printJoin.patch
  against trunk revision 979503.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 407 release audit warnings (more than the trunk's current 405 warnings).

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/380/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/380/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/380/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/380/console

This message is automatically generated.",27/Aug/10 21:37;daijy;This is already fixed in the latest code. Thanks Swati!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make 'docs' target (forrest) work with Java 1.6,PIG-1508,12469750,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cwsteinbach,cwsteinbach,cwsteinbach,20/Jul/10 21:07,18/Feb/11 22:16,14/Mar/19 03:07,17/Sep/10 23:02,0.7.0,,,,,,,0.8.0,,,,documentation,,,0,,,,,,,,,,,,,"FOR-984 covers the very inconvenient fact that Forrest 0.8 does not work with Java 1.6
The same ticket also suggests a workaround: disabling sitemap and stylesheet validation
by setting the forrest.validate.sitemap and forrest.validate.stylesheets properties to false.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Jul/10 21:32;cwsteinbach;PIG-1508.patch.txt;https://issues.apache.org/jira/secure/attachment/12449977/PIG-1508.patch.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-07-21 07:29:02.898,,,no_permission,,,,,,,,,,,,37469,,,,Fri Feb 18 22:16:02 UTC 2011,,,,,,,0|i05ii7:,30095,,,,,,,forrest,,,"20/Jul/10 21:32;cwsteinbach;PIG-1508.patch.txt:
* set forrest.validate.sitemap=false in forrest.properties
* Remove java5 specific settings in build.xml
* Remove java5 specific settings in test-patch.sh
","21/Jul/10 07:29;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12449977/PIG-1508.patch.txt
  against trunk revision 965559.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/349/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/349/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/349/console

This message is automatically generated.","21/Jul/10 20:52;alangates;Carl,

Are there any other side effects of turning off sitemap validation?","22/Aug/10 13:06;dvryaboy;In http://comments.gmane.org/gmane.text.xml.forrest.user/4899 a forrest committer says ""This ""validate sitemap"" task doesn't really do much anyway.
Its main purpose is to demonstrate the power of using Jing to do xml validation during the build phase. There are other better demonstrations of that.""

Sounds like this is safe to do. 
+1
","23/Aug/10 19:39;alangates;Alright, I'll get this checked in before we branch for 0.8 then.","23/Aug/10 21:02;alangates;I can't figure out a way to test test-patch.sh without checking it in.  And, if this does break something it will make life hard for developers who are trying to get their patches in before the 0.8 branch is cut.  So, I propose that I hold off checking this in until we have all other pre-0.8 patches checked in.  Then I'll check it in and do extensive testing with test-patch.  That way I can quickly fix any issues I find and not disrupt others.  Than we can branch for 0.8.  Seem reasonable?

As a side note, we still need Java 1.5 for forrest in the site docs.  This patch only claims to fix it for the docs target, which it does.  I'll open a separate JIRA to fix it on the site side, as it would be really nice to not force people to have 2 versions of Java to build Pig stuff.","23/Aug/10 21:48;cwsteinbach;The patch I posted caused failures in the contrib tests. I think there may be a Java5 dependency lurking there. I'm rerunning the tests and will dig into any failures that I find.

bq. As a side note, we still need Java 1.5 for forrest in the site docs.

Is there an ant target for this? Otherwise, I'm not quite sure what you're talking about.","23/Aug/10 21:58;alangates;I'm guessing the contrib failures are just because Hudson isn't working properly.  I run contrib tests only with 1.6 all the time and don't see issues.

The site issues I'm talking about are under pig/site (not pig/trunk).  I've already posted another patch (see PIG-1558) to deal with it. ","26/Aug/10 07:53;hbasereviewboard;Message from: ""Carl Steinbach"" <carl@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/725/
-----------------------------------------------------------

Review request for Pig Developers.


Summary
-------

Remove Pig's dependency on Java5.


This addresses bug PIG-1508.
    http://issues.apache.org/jira/browse/PIG-1508


Diffs
-----

  build.xml b0a2ada 
  src/docs/forrest.properties 51f1af7 
  test/bin/test-patch.sh 55c449e 

Diff: http://review.cloudera.org/r/725/diff


Testing
-------


Thanks,

Carl


",17/Sep/10 23:02;alangates;Patch checked in.  Thanks Carl.,"18/Feb/11 22:02;cos;Looks like this patch has never make to branch-0.8 despite the fact that that branch has been cut later then the commit and ""Fix version"" is set to 0.8.0. So, somehow magically this patch has been removed from the branch or something.","18/Feb/11 22:16;olgan;yes, I think it only made it to the trunk. Don't think there is much point to porting it back now",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Full outer join fails while doing a filter on joined data,PIG-1507,12469736,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,20/Jul/10 17:36,17/Dec/10 22:44,14/Mar/19 03:07,22/Jul/10 01:14,0.8.0,,,,,,,0.7.0,0.8.0,,,impl,,,0,,,,,,,,,,,,,"The following script produce wrong result:

test1.dat:
1
2
3

test2.dat:
1
2

pig script:
{code}
a = LOAD 'test1.dat' USING PigStorage() AS (d1:int);
b = LOAD 'test2.dat' USING PigStorage() AS (d2:int);
c = JOIN a BY d1 FULL OUTER, b BY d2;
d = FILTER c BY d2 IS NULL;
STORE d INTO 'test.out' USING PigStorage();
{code}

expected:
3

We get:
1
2
3

This is because we erroneously push the filter before full outer join. Similar issue is addressed in [PIG-1289|https://issues.apache.org/jira/browse/PIG-1289], but we only fix left/right outer join.",,,,,,,,,,,,,,,,,,,PIG-1289,,,,,,,,,,,,,20/Jul/10 19:29;daijy;PIG-1507-1.patch;https://issues.apache.org/jira/secure/attachment/12449962/PIG-1507-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-07-21 01:25:29.179,,,no_permission,,,,,,,,,,,,164991,Reviewed,,,Thu Jul 22 01:14:08 UTC 2010,,,,,,,0|i0gwmn:,96716,,,,,,,,,,"21/Jul/10 01:25;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12449962/PIG-1507-1.patch
  against trunk revision 965559.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/348/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/348/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/348/console

This message is automatically generated.",21/Jul/10 21:01;rding;+1,22/Jul/10 01:14;daijy;Patch committed to both trunk and 0.7 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
guava.jar should be removed from the lib folder,PIG-1500,12469179,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nrai,gkesavan,gkesavan,13/Jul/10 17:36,17/Dec/10 22:44,14/Mar/19 03:07,28/Jul/10 18:38,,,,,,,,0.8.0,,,,build,,,0,,,,,,,,,,,,,"guava jar is available in the maven repository but still its is checked into the pig trunk's lib folder.

I ve checked the availability of guava jar in the maven repository.
http://mvnrepository.com/artifact/com.google.guava/guava",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Jul/10 17:00;nrai;guava.jar.06.afterjython.patch;https://issues.apache.org/jira/secure/attachment/12450607/guava.jar.06.afterjython.patch,24/Jul/10 00:52;nrai;guava.jar.r06.patch;https://issues.apache.org/jira/secure/attachment/12450378/guava.jar.r06.patch,28/Jul/10 18:16;nrai;guava.jar.r06_4.patch;https://issues.apache.org/jira/secure/attachment/12450736/guava.jar.r06_4.patch,21/Jul/10 06:50;nrai;removeGuavaJar.patch;https://issues.apache.org/jira/secure/attachment/12450029/removeGuavaJar.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2010-07-13 17:57:54.979,,,no_permission,,,,,,,,,,,,164984,Reviewed,,,Thu Jul 29 00:15:10 UTC 2010,,,,,,,0|i0gwjr:,96703,,,,,,,,,,"13/Jul/10 17:57;olgan;Niraj, could you take care of this as well since you already working in this area",21/Jul/10 06:50;nrai;Submitting this patch for review.,"21/Jul/10 08:04;dvryaboy;Have you tried actually building with this? The reason I put guava r3 into lib was that the public maven deploy for it is broken.

Here's what happens when I apply this patch and try to build:

{code}
[ivy:resolve] :::: WARNINGS
[ivy:resolve] 	problem while downloading module descriptor: http://repo1.maven.org/maven2/com/google/guava/guava/r03/guava-r03.pom: invalid sha1: expected=1cbd6fab2460050ff7147b6d8536f39c8f535067 computed=7a37041386ee39a1fbb3efd3c4c6932809cb5887 (1304ms)
{code}

Now, we can probably still get away with removing guava from lib/ -- they just release guava-r6, which should be compatible with the guava-dependent code in Pig, and is supposed to have a proper maven deploy. But the patch as is should not be applied.","21/Jul/10 17:18;nrai;Hi Dmitriy,  Yes, this problem was there but when I upgraded ivy then this problem was fixed. Yes, I can build with this change.
","22/Jul/10 17:06;nrai;I  ran test with guava-r06.jar and all test passed. If everyone is fine, we can move to r06
","24/Jul/10 00:52;nrai;Attaching the patch with guava.jar r06 version as no one had problem in migrating to that version.
","26/Jul/10 18:40;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12450378/guava.jar.r06.patch
  against trunk revision 979362.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/376/console

This message is automatically generated.",27/Jul/10 17:00;nrai;Merged with the Daniel code of Jython,"28/Jul/10 05:53;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12450607/guava.jar.06.afterjython.patch
  against trunk revision 979781.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/361/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/361/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/361/console

This message is automatically generated.",28/Jul/10 18:15;nrai;conflict with richard patch,28/Jul/10 18:16;nrai;merged with richard change,28/Jul/10 18:38;rding;+1. The patch is committed to trunk. Thanks Niraj.,"29/Jul/10 00:15;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12450736/guava.jar.r06_4.patch
  against trunk revision 979918.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/362/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/362/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/362/console

This message is automatically generated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Column Pruner throw exception ""inconsistent pruning""",PIG-1493,12469020,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,12/Jul/10 05:14,17/Dec/10 22:44,14/Mar/19 03:07,19/Jul/10 17:11,0.7.0,,,,,,,0.7.0,0.8.0,,,impl,,,0,,,,,,,,,,,,,"The following script fail:
{code}
a = load '1.txt' as (a0:chararray, a1:chararray, a2);
b = foreach a generate CONCAT(a0,a1) as b0, a0, a2;
c = foreach b generate a0, a2;
dump c;
{code}

Error message:
ERROR 2185: Column $0 of (Name: b: ForEach 1-50 Operator Key: 1-50) inconsistent pruning

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias c
        at org.apache.pig.PigServer.openIterator(PigServer.java:698)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:595)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:291)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:162)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:138)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:90)
        at org.apache.pig.Main.run(Main.java:451)
        at org.apache.pig.Main.main(Main.java:103)
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1002: Unable to store alias c
        at org.apache.pig.PigServer.storeEx(PigServer.java:804)
        at org.apache.pig.PigServer.store(PigServer.java:760)
        at org.apache.pig.PigServer.openIterator(PigServer.java:680)
        ... 7 more
Caused by: org.apache.pig.impl.plan.optimizer.OptimizerException: ERROR 2212: Unable to prune plan
        at org.apache.pig.impl.logicalLayer.optimizer.PruneColumns.prune(PruneColumns.java:826)
        at org.apache.pig.impl.logicalLayer.optimizer.LogicalOptimizer.optimize(LogicalOptimizer.java:240)
        at org.apache.pig.PigServer.compileLp(PigServer.java:1180)
        at org.apache.pig.PigServer.storeEx(PigServer.java:799)
        ... 9 more
Caused by: org.apache.pig.impl.plan.VisitorException: ERROR 2188: Cannot prune columns for (Name: b: ForEach 1-50 Operator Key: 1-50)
        at org.apache.pig.impl.logicalLayer.ColumnPruner.prune(ColumnPruner.java:177)
        at org.apache.pig.impl.logicalLayer.ColumnPruner.visit(ColumnPruner.java:202)
        at org.apache.pig.impl.logicalLayer.LOForEach.visit(LOForEach.java:132)
        at org.apache.pig.impl.logicalLayer.LOForEach.visit(LOForEach.java:47)
        at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:69)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.impl.logicalLayer.optimizer.PruneColumns.prune(PruneColumns.java:821)
        ... 12 more
Caused by: org.apache.pig.impl.plan.optimizer.OptimizerException: ERROR 2185: Column $0 of (Name: b: ForEach 1-50 Operator Key: 1-50) inconsistent pruning
        at org.apache.pig.impl.logicalLayer.ColumnPruner.prune(ColumnPruner.java:148)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Jul/10 05:17;daijy;PIG-1493-1.patch;https://issues.apache.org/jira/secure/attachment/12449203/PIG-1493-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-07-13 22:38:00.862,,,no_permission,,,,,,,,,,,,164978,Reviewed,,,Mon Jul 19 17:11:57 UTC 2010,,,,,,,0|i0gwhj:,96693,,,,,,,,,,"13/Jul/10 22:38;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12449203/PIG-1493-1.patch
  against trunk revision 963504.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/367/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/367/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/367/console

This message is automatically generated.",15/Jul/10 17:11;daijy;Manually run test successfully.,15/Jul/10 18:19;rding;+1,19/Jul/10 17:11;daijy;Patch committed to both trunk and 0.7 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DefaultTuple and DefaultMemory understimate their memory footprint,PIG-1492,12468965,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,10/Jul/10 01:39,17/Dec/10 22:44,14/Mar/19 03:07,21/Jul/10 18:30,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"There are several places where we highly underestimate the memory footprint . For example, for map datatypes, we don't account for the per entry cost for the map container data structures. The estimated size of a tuple having map with 100 integer key-value entries , as per current version of code is 3260 bytes, while what is observed is around 6775 bytes .  To verify the memory footprint, i checked free memory before and after creating multiple instances of the object , using code on the lines of http://www.javaspecialists.eu/archive/Issue029.html . 

In PIG-1443 similar change was done to fix this for CHARARRAY .
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Jul/10 03:12;thejas;PIG-1492.1.patch;https://issues.apache.org/jira/secure/attachment/12449531/PIG-1492.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-07-15 20:20:32.893,,,no_permission,,,,,,,,,,,,164977,,,,Wed Jul 21 17:47:15 UTC 2010,,,,,,,0|i0gwh3:,96691,,,,,,,,,,"15/Jul/10 03:12;thejas;This patch updates the memory size calculations . This changes were made so that the estimated sizes are closer to what is seen in 32 bit Java HotSpot(TM) Server VM (build 10.0-b19, mixed mode). 

It is based on some of the observations in http://www.javamex.com/tutorials/memory/string_memory_usage.shtm .  The header sizes of objects has been taken to be 8 bytes. The objects size is rounded to multiple of 8 bytes. Some other adjustments for minimum size of array in a ArrayList were made based on observed size values.

The follow tables shows the tuple estimated sizes before/after the patch and what is actually observed, for the types whose calculation logic changed -

|| type || num of columns of this type in the tuple || before || patched || observed ||
| BYTEARRAY with 5 bytes| 10|254 | 504|495 |
| BYTEARRAY with 5 bytes| 1000| 21044| 44064|44127 |
| DOUBLE| 10|364 | 264|255 |
| DOUBLE| 1000|32044 | 20064| 20127 |
| LONG |10 | 284|264 |255 |
| LONG | 1000 | 24044 | 20064 | 20127 |




|| Tuple containing a single - || patched || observed ||
|  BAG with 10 empty tuples|524| 1092|1159 |
|  BAG with 1000 empty tuples| 48044| 100092| 100211|
|  map with 10 integer key-value pairs| 380| 824| 775|
|  map with 1000 integer key-value pairs| 32060| 64184| 64346|","15/Jul/10 20:20;daijy;Talked with Tejas, he get more observations than listed above. So I believe the formula should be good. +1 for commit once hudson pass.","15/Jul/10 23:32;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12449531/PIG-1492.1.patch
  against trunk revision 964182.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/370/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/370/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/370/console

This message is automatically generated.","21/Jul/10 17:47;thejas;Committed to trunk.
The contrib tests pass in my machine. The errors in hudson run seem to be caused by some environment specific issues.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Failure planning nested FOREACH with DISTINCT, POLoad cannot be cast to POLocalRearrange",PIG-1491,12468957,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,laukik,scott_carey,scott_carey,09/Jul/10 21:39,04/Aug/11 00:34,14/Mar/19 03:07,18/Feb/11 01:55,0.7.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"I have a failure that occurs during planning while using DISTINCT in a nested FOREACH. 

Caused by: java.lang.ClassCastException: org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad cannot be cast to org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.SecondaryKeyOptimizer.visitMROp(SecondaryKeyOptimizer.java:352)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper.visit(MapReduceOper.java:218)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper.visit(MapReduceOper.java:40)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:67)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-07-09 21:48:32.9,,,no_permission,,,,,,,,,,,,66217,,,,Fri Feb 18 01:55:35 UTC 2011,,,,,,,0|i0gwgn:,96689,,,,,,,,,,"09/Jul/10 21:48;ashutoshc;Scott,

It will be useful if you can also paste the Pig script which produced this exception.","09/Jul/10 21:48;scott_carey;Full stack trace at the end of this comment.  

The pig script used is a couple hundred lines long.  But the individual chunk that I can change to trigger the issue is the following:

DATA_G = COGROUP A by (a, b, c, d) OUTER, B by (a, b, c, d) OUTER;
DATA = FOREACH DATA_G {
    a_items = DISTINCT A.x;
    b_items = DISTINCT B.x;
    GENERATE
    FLATTEN(group) as (a,b,c,d),
    SUM(A.m) as m,
    SUM(A.n) as n,
    COUNT(a_items) as a_item_count,
    (long)(SUM(B.u) + (double)0.5) as u,
    (long)(SUM(B.v) + (double)0.5) as v,
    COUNT(b_items) as b_item_count;
}

Removing both of the DISTINCT temporary aliases and not generating those counts works fine.  Adding either one of them causes it to fail.





ERROR 2043: Unexpected error during execution.

org.apache.pig.backend.executionengine.ExecException: ERROR 2043: Unexpected error during execution.
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:318)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1016)
        at org.apache.pig.PigServer.execute(PigServer.java:1009)
        at org.apache.pig.PigServer.access$100(PigServer.java:114)
        at org.apache.pig.PigServer$Graph.execute(PigServer.java:1261)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:326)
        at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:110)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:167)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:139)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:90)
        at org.apache.pig.Main.main(Main.java:335)
Caused by: java.lang.ClassCastException: org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad cannot be cast to org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.SecondaryKeyOptimizer.visitMROp(SecondaryKeyOptimizer.java:352)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper.visit(MapReduceOper.java:218)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper.visit(MapReduceOper.java:40)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:67)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:69)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:69)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:69)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:69)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:69)
        at org.apache.pig.impl.plan.DepthFirstWalker.walk(DepthFirstWalker.java:50)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.compile(MapReduceLauncher.java:446)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:108)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:294)
        ... 10 more
","09/Jul/10 21:51;scott_carey;more readable:

{code}
DATA_G = COGROUP A by (a, b, c, d) OUTER, B by (a, b, c, d) OUTER;
DATA = FOREACH DATA_G { 
  a_items = DISTINCT A.x;
  b_items = DISTINCT B.x;
  GENERATE 
  FLATTEN(group) as (a,b,c,d),
  SUM(A.m) as m, SUM(A.n) as n,
  COUNT(a_items) as a_item_count,
  (long)(SUM(B.u) + (double)0.5) as u,
  (long)(SUM(B.v) + (double)0.5) as v,
  COUNT(b_items) as b_item_count;
}
{code}","15/Feb/11 22:36;laukik;Latest trunk does not throw the exception reported in this jira, and produces result as expected.","15/Feb/11 23:04;laukik;This is the script I executed:

{noformat}
A = load '$input1' as (a, b, c, d, x, m, n, u, v);
B = load '$input2' as (a, b, c, d, x, m, n, u, v);
DATA_G = COGROUP A by (a, b, c, d) OUTER, B by (a, b, c, d) OUTER;
DATA = FOREACH DATA_G { 
  a_items = DISTINCT A.x;
  b_items = DISTINCT B.x;
  GENERATE 
  FLATTEN(group) as (a,b,c,d),
  SUM(A.m) as m, SUM(A.n) as n,
  COUNT(a_items) as a_item_count,
  (long)(SUM(B.u) + (double)0.5) as u,
  (long)(SUM(B.v) + (double)0.5) as v,
  COUNT(b_items) as b_item_count;
}
store DATA into '$output';
{noformat}


My sample inputs:

{noformat}
1	1	1	1	10	10	10	10	10
2	2	2	2	20	20	20	20	20
1	1	1	1	10	10	10	10	10
{noformat}

{noformat}
1	1	1	1	100	100	100	100	100
2	2	2	2	200	200	200	200	200
2	2	2	2	202	202	202	202	202
1	1	1	1	100	100	100	100	100
{noformat}

And the output:

{noformat}
1	1	1	1	20.0	20.0	1	200	200	1
2	2	2	2	20.0	20.0	1	402	402	2
{noformat}

This was tested on svn versions 1068769 (recent trunk) and 966485 (Pig version 0.7.0 compiled on Jul 22 2010)

@Scott
Can you try the latest trunk, and close the issue if it cannot be reproduced? Thanks!
","18/Feb/11 01:55;olgan;Looks like this issue is fixed in the latest code. Please, re-open if you still experiencing the problem",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Pig storers work with remote HDFS in secure mode,PIG-1490,12468956,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,09/Jul/10 21:25,17/Dec/10 22:44,14/Mar/19 03:07,12/Jul/10 17:37,,,,,,,,0.7.0,0.8.0,,,,,,0,,,,,,,,,,,,,PIG-1403 fixed the problem for Pig loaders. We need to do the same for Pig storers. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Jul/10 23:56;rding;PIG-1490.patch;https://issues.apache.org/jira/secure/attachment/12449139/PIG-1490.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-07-10 09:20:50.289,,,no_permission,,,,,,,,,,,,164976,Reviewed,,,Mon Jul 12 17:10:22 UTC 2010,,,,,,,0|i0gwg7:,96687,Committed to both trunk and 0.7 branch,,,,,,,,,"10/Jul/10 09:20;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12449139/PIG-1490.patch
  against trunk revision 962722.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/366/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/366/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/366/console

This message is automatically generated.",12/Jul/10 17:10;daijy;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Pig MapReduceLauncher does not use jars in register statement ,PIG-1489,12468863,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,olgan,olgan,08/Jul/10 22:03,17/Dec/10 22:44,14/Mar/19 03:07,26/Jul/10 17:03,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"If my Pig StorFunc has its own OutputFormat class then Pig MapReducelauncher will try to instantiate it before
launching the mapreduce job and fail with ClassNotFoundException.

This happens because Pig MapReduce launcher uses its own classloader and ignores the classes in the jars in the
register statement.

The effect is that the jars not only have to be in ""register "" statement in the script but also in the pig
classpath with the -classpath tag. 

This can be remedied by making the Pig MapReduceLauncher constructing a classloader that includes the registered jars
and using that to instantiate the OutputFormat class.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Jul/10 19:10;rding;PIG-1489.patch;https://issues.apache.org/jira/secure/attachment/12450082/PIG-1489.patch,20/Jul/10 00:31;rding;PIG-1489.patch;https://issues.apache.org/jira/secure/attachment/12449902/PIG-1489.patch,23/Jul/10 23:09;rding;PIG-1489_1.patch;https://issues.apache.org/jira/secure/attachment/12450367/PIG-1489_1.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-07-20 00:31:39.126,,,no_permission,,,,,,,,,,,,164975,Reviewed,,,Mon Jul 26 16:57:09 UTC 2010,,,,,,,0|i0gwfj:,96684,,,,,,,,,,20/Jul/10 00:31;rding;This patch adds registered jars to the class loader.,21/Jul/10 19:10;rding;re-sync with trunk.,23/Jul/10 23:09;rding;New patch adding the source code of the test jar.,"23/Jul/10 23:17;thejas;+1 
You can commit after verifying that tests & checks are passing.
","23/Jul/10 23:33;rding;
test-patch results:

{code}
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 10 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{code}",26/Jul/10 16:57;rding;I ran core tests manually and they passed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Replace ""bz"" with "".bz""  in all the LoadFunc",PIG-1487,12468777,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,zjffdu,zjffdu,zjffdu,08/Jul/10 01:43,17/Dec/10 22:44,14/Mar/19 03:07,27/Jul/10 02:51,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,This issue relates with PIG-1463. Thank Ashutosh find another place in PigStorage should be corrected. I check all the LoadFunc and found that TextLoader also has same problem. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Jul/10 01:51;zjffdu;PIG_1487.patch;https://issues.apache.org/jira/secure/attachment/12448940/PIG_1487.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-07-14 03:52:34.81,,,no_permission,,,,,,,,,,,,164973,,,,Tue Jul 27 02:51:38 UTC 2010,,,,,,,0|i0gwev:,96681,,,,,,,,,,08/Jul/10 01:51;zjffdu;Attach the patch,14/Jul/10 03:52;ashutoshc;+1 ,"27/Jul/10 00:40;olgan;Jeff, please, go ahead and commit the patch, thanks",27/Jul/10 02:51;zjffdu;Commit the patch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
update ant eclipse-files target to include new jar and remove contrib dirs from build path,PIG-1486,12468776,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,thejas,thejas,thejas,08/Jul/10 00:55,17/Dec/10 22:44,14/Mar/19 03:07,20/Aug/10 17:11,0.8.0,,,,,,,0.8.0,,,,tools,,,0,,,,,,,,,,,,," .eclipse.templates/.classpath needs to be updated to address following -
1. There is a new jar that is used by the code - guava-r03.jar
2. The jar ""ANT_HOME/lib/ant.jar"" gives an 'unbounded jar' error in eclipse.
3. Removing the contrib projects from class path as discussed in PIG-1390, until all libs necessary for the contribs are included in classpath.
",,,,,,,,,,,,,,,,,,,PIG-1390,,,,,,,,,,,,,12/Aug/10 18:18;thejas;PIG-1486.1.patch;https://issues.apache.org/jira/secure/attachment/12451931/PIG-1486.1.patch,19/Aug/10 19:52;thejas;PIG-1486.2.patch;https://issues.apache.org/jira/secure/attachment/12452562/PIG-1486.2.patch,08/Jul/10 00:56;thejas;PIG-1486.patch;https://issues.apache.org/jira/secure/attachment/12448935/PIG-1486.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-07-08 01:39:59.333,,,no_permission,,,,,,,,,,,,164972,Reviewed,,,Fri Aug 20 17:11:26 UTC 2010,,,,,,,0|i0gwen:,96680,,,,,,,,,,"08/Jul/10 01:39;chaitk;Thejas,

 Had a look at the patch and found that the ANT_HOME path is removed. This means that the variable has to be manually setup in order to get it running on eclipse, right?
 Am I missing something?","08/Jul/10 02:12;thejas;When ANT_HOME path is present, i see an error ""Unbounded classpath variable :.."" error in the problems window of eclipse. But without that things are working fine for me. I am also able to run the ant targets from the ant view.
I don't have any idea why that path is needed. Are you having to setup any variable when you use this patch ?
","08/Jul/10 07:43;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12448935/PIG-1486.patch
  against trunk revision 960062.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/341/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/341/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/341/console

This message is automatically generated.","13/Jul/10 17:50;ashutoshc;Took a look at the patch. Changes look good. But, because of PIG-1452 some additional changes are required. Need to remove lib/hadoop20.jar from eclipse build-path and hadoop-core.jar, hadoop-test.jar, apache-commons-* and few other jars needed to be added in, which now are pulled in from maven repos and put in build/ivy/lib/Pig","12/Aug/10 18:18;thejas;Updated patch which includes guava and jython. It might need more changes after PIG-1452 is committed.
","19/Aug/10 19:52;thejas;Patch updated after changes from PIG-1524 .
","20/Aug/10 04:09;ashutoshc;I did 
svn co https://svn.apache.org/repos/asf/hadoop/pig/trunk/ pig-1486
ant eclipse-files

and then imported pig-1486 as existing project in eclipse. I presume thats all I need to do.
Patch needs more updates after PIG-1520 . Essentially needs to remove owl from eclipse's build path. Further, eclipse also reported
* Unbound classpath variable: 'ANT_HOME/lib/ant.jar' in project 'pig-1486'
* Project 'pig-1486' is missing required library: 'lib/hadoop20.jar'

","20/Aug/10 16:48;ashutoshc;meh.. Before testing thou shalt apply the patch !
After I applied the patch, works like a charm. +1",20/Aug/10 17:11;thejas;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BinStorage should support comma seperated path,PIG-1484,12468747,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,07/Jul/10 19:00,17/Dec/10 22:44,14/Mar/19 03:07,09/Jul/10 18:34,0.7.0,,,,,,,0.7.0,0.8.0,,,impl,,,0,,,,,,,,,,,,,"BinStorage does not take comma seperated path. The following script fail:

a = load '1.bin,2.bin' using BinStorage();
dump a;
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Jul/10 19:07;daijy;PIG-1484-1.patch;https://issues.apache.org/jira/secure/attachment/12448904/PIG-1484-1.patch,08/Jul/10 16:51;daijy;PIG-1484-2.patch;https://issues.apache.org/jira/secure/attachment/12448988/PIG-1484-2.patch,08/Jul/10 20:41;daijy;PIG-1484-3.patch;https://issues.apache.org/jira/secure/attachment/12449001/PIG-1484-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-07-08 01:09:50.689,,,no_permission,,,,,,,,,,,,164970,Reviewed,,,Fri Jul 09 18:34:58 UTC 2010,,,,,,,0|i0gwdz:,96677,"In Pig 0.7.0 only a single location is supported as input to BinStorage. (This location can be a file, a directory or a glob). With Pig 0.8.0 we are making BinSTorage  (similar to PigStorage) support a list of locations.

Example:

a = load '1.bin,2.bin' using BinStorage();

",,,,,,,,,"08/Jul/10 01:09;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12448904/PIG-1484-1.patch
  against trunk revision 960062.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/361/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/361/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/361/console

This message is automatically generated.","08/Jul/10 04:19;olgan;Daniel,

Should we only return non-null value if all files exist?","08/Jul/10 04:45;daijy;I follow globbing. In globbing, any file exist we say this globbing exist. ",08/Jul/10 14:25;olgan;I think that's different. Globbing means - give me any data that matches the globe. I think semantics of list is that all elements must exist. What does PigStorage do?,08/Jul/10 16:26;daijy;That's a good point. We shall follow what PigStorage does. PigStorage needs all file exist. Will change the patch.,08/Jul/10 17:00;olgan;+1 to the code changes. I think it will be good if the test case actually verified that it got all the data it expects not just that it can get to the data.,"08/Jul/10 20:41;daijy;Sure, reattach the patch.",08/Jul/10 21:17;olgan;+1,"08/Jul/10 22:47;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12448988/PIG-1484-2.patch
  against trunk revision 960062.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/363/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/363/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/363/console

This message is automatically generated.","09/Jul/10 02:55;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12449001/PIG-1484-3.patch
  against trunk revision 960062.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/342/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/342/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/342/console

This message is automatically generated.",09/Jul/10 18:34;daijy;Patch committed to both trunk and 0.7 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig gets confused when more than one loader is involved,PIG-1482,12468693,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,ankur,ankur,07/Jul/10 05:56,17/Dec/10 22:44,14/Mar/19 03:07,30/Aug/10 17:17,0.7.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"In case of two relations being loaded using different loader, joined, grouped and projected, pig gets confused in trying to find appropriate loader for the requested cast. Consider the following script :-

A = LOAD 'data1' USING PigStorage() AS (s, m, l);
B = FOREACH A GENERATE s#'k1' as v1, m#'k2' as v2, l#'k3' as v3;
C = FOREACH B GENERATE v1, (v2 == 'v2' ? 1L : 0L) as v2:long, (v3 == 'v3' ? 1 :0) as v3:int;

D = LOAD 'data2' USING TextLoader() AS (a);
E = JOIN C BY v1, D BY a USING 'replicated';

F = GROUP E BY (v1, a);
G = FOREACH F GENERATE (chararray)group.v1, group.a;
        
dump G;

This throws the error, stack trace of which is in the next comment
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Aug/10 00:17;xuefuz;jira-1482-final-1.patch;https://issues.apache.org/jira/secure/attachment/12453324/jira-1482-final-1.patch,30/Aug/10 04:16;xuefuz;jira-1482-final-2.patch;https://issues.apache.org/jira/secure/attachment/12453399/jira-1482-final-2.patch,18/Aug/10 16:03;xuefuz;jira-1482-final.patch;https://issues.apache.org/jira/secure/attachment/12452409/jira-1482-final.patch,10/Aug/10 21:34;xuefuz;jira-1482-final.patch;https://issues.apache.org/jira/secure/attachment/12451713/jira-1482-final.patch,09/Aug/10 23:38;xuefuz;jira-1482-final.patch;https://issues.apache.org/jira/secure/attachment/12451633/jira-1482-final.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2010-08-10 21:52:37.461,,,no_permission,,,,,,,,,,,,164968,Reviewed,,,Mon Aug 30 17:17:54 UTC 2010,,,,,,,0|i0gwd3:,96673,,,,,,,,,,"07/Jul/10 05:56;ankur;ERROR 1065: Found more than one load function to use: [PigStorage, TextLoader]

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias K
        at org.apache.pig.PigServer.openIterator(PigServer.java:521)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:544)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:241)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:162)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:138)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:89)
        at org.apache.pig.Main.main(Main.java:391)
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1002: Unable to store alias K
        at org.apache.pig.PigServer.store(PigServer.java:577)
        at org.apache.pig.PigServer.openIterator(PigServer.java:504)
        ... 6 more
Caused by: org.apache.pig.impl.plan.PlanValidationException: ERROR 0: An unexpected exception caused the validation to stop
        at org.apache.pig.impl.plan.PlanValidator.validateSkipCollectException(PlanValidator.java:104)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingValidator.validate(TypeCheckingValidator.java:40)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingValidator.validate(TypeCheckingValidator.java:30)
        at org.apache.pig.impl.logicalLayer.validators.LogicalPlanValidationExecutor.validate(LogicalPlanValidationExecutor.java:89)
        at org.apache.pig.PigServer.validate(PigServer.java:930)
        at org.apache.pig.PigServer.compileLp(PigServer.java:884)
        at org.apache.pig.PigServer.store(PigServer.java:568)
        ... 7 more
Caused by: org.apache.pig.impl.logicalLayer.validators.TypeCheckerException: ERROR 1053: Cannot resolve load function to use for casting from bytearray to chararray.
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.visit(TypeCheckingVisitor.java:1775)
        at org.apache.pig.impl.logicalLayer.LOCast.visit(LOCast.java:67)
        at org.apache.pig.impl.logicalLayer.LOCast.visit(LOCast.java:32)
        at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:69)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.checkInnerPlan(TypeCheckingVisitor.java:2819)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.visit(TypeCheckingVisitor.java:2723)
        at org.apache.pig.impl.logicalLayer.LOForEach.visit(LOForEach.java:130)
        at org.apache.pig.impl.logicalLayer.LOForEach.visit(LOForEach.java:45)
        at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:69)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.impl.plan.PlanValidator.validateSkipCollectException(PlanValidator.java:101)
        ... 13 more
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1065: Found more than one load function to use: [PigStorage, TextLoader]
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.getLoadFuncSpec(TypeCheckingVisitor.java:3161)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.getLoadFuncSpec(TypeCheckingVisitor.java:3176)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.getLoadFuncSpec(TypeCheckingVisitor.java:3103)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.getLoadFuncSpec(TypeCheckingVisitor.java:3176)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.getLoadFuncSpec(TypeCheckingVisitor.java:3103)
","07/Jul/10 06:00;ankur;Casting early alleviates the problem. So this makes the above script work

C = FOREACH B GENERATE (chararray) v1, (v2 == 'v2' ? 1L : 0L) as v2:long, (v3 == 'v3' ? 1 :0) as v3:int;","07/Jul/10 06:01;ankur;forgot to add

Include this change as well for the above script to work

G = FOREACH F GENERATE group.v1, group.a;","10/Aug/10 21:52;xuefuz;Manual Hudson run result:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 424 release audit warnings (more than the trunk's current 423 warnings).

The release audit diff was due to new public methods  introduced in Schema.java.",18/Aug/10 16:04;xuefuz;minor changes added based on review feedback.,27/Aug/10 18:12;xuefuz;The patch passed all unit tests and end-to-end tests.,"27/Aug/10 23:35;thejas;Patch review comments -
- Schema.java
{code}
        public FieldSchema(String a, Schema s, byte t)  throws FrontendException {
            alias = a;
            schema = s;
            log.debug(""t: "" + t + "" Bag: "" + DataType.BAG + "" tuple: "" + DataType.TUPLE);
            
            /*
             * The following check is removed because it may not be always true. As a matter of 
             * fact, the condition can be produced using other constructors anyway.
             *
            if ((null != s) && !(DataType.isSchemaType(t))) {
                int errCode = 1020;
                throw new FrontendException(""Only a BAG or TUPLE can have schemas. Got ""
                        + DataType.findTypeName(t), errCode, PigException.INPUT);
            }
            */
{code}
I think some other code paths might be relying on this constructor for error checking. It would be safer to create another constructor with a check boolean argument 
{code}
        public FieldSchema(String a, Schema s, byte t, boolean innerTypeCheck)  
{code}
 and call that from above constructor and from FieldSchema.copyAndLink(..)


- In LOStream.java.getSchema() mIsSchemaComputed is used to keep track of whether the fieldschema parents have been set.
I think it will be better to use a different variable for the purpose - it will be more readable, and also not likely to break any assumptions people are likely to make about this variable that is from the LogicalOperator class.

- TypeCheckingVisitor.java insertCastForUDF is called on input of udf , it seems like same logic should be used for other expressions as well (instead of insertCast(.) ). Also, insertCastForUDF(..) and insertCast(..) have only two lines different, we can share rest of the code.
{code}
private void insertCastForUDF(LOUserFunc udf,
    		FieldSchema fromFS, FieldSchema toFs, ExpressionOperator predecessor){

toFs.setParent( fromFS.canonicalName, predecessor );
insertCast(udf, toFs.type, toFs, predecessor);
}
{code}

- TypeCheckingVisitor.java In visit(LOCast), it seems like we can just pick any of the matching predecessor load functions, shouldn't we check if all the FuncSpec returned are the same ?
{code}
            	for( Map.Entry<String, LogicalOperator> entry : canonicalMap.entrySet() ) {
                    FuncSpec loadFuncSpec = getLoadFuncSpec( entry.getValue(), entry.getKey() );
                    cast.setLoadFuncSpec( loadFuncSpec );
            	}
{code}

- LOProject.java
the commented line can be removed -
{code}
+//                         mFieldSchema.setParent(fs.canonicalName, expressionOperator);
{code}




","28/Aug/10 00:19;xuefuz;Updated the patch based on the review comments.

For comments above, the one next to the last, the map should only contain one entry. Before the result is obtained, exception is thrown anytime two  different loadfunspec's are found. It was done that way before.
","28/Aug/10 00:42;thejas;Regarding jira-1482-final-1.patch
+1 , just one comment from review of previous patch needs to be addressed -
- As mentioned in 3rd comment, the code in insertCast() can be reused from insertCastForUdf()
{code}
private void insertCastForUDF(LOUserFunc udf,
    		FieldSchema fromFS, FieldSchema toFs, ExpressionOperator predecessor){

     toFs.setParent( fromFS.canonicalName, predecessor );
     insertCast(udf, toFs.type, toFs, predecessor);
}
{code}

",30/Aug/10 04:16;xuefuz;Updated  based on review comments above.,"30/Aug/10 17:17;thejas;Patch committed to trunk.
Xuefu, thanks for the fix.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Syntax error in tutorial Pig Script 1: Query Phrase Popularity (ORDER operator),PIG-1477,12468323,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,chandec,bmansell,bmansell,01/Jul/10 05:23,17/Dec/10 22:44,14/Mar/19 03:07,03/Sep/10 22:55,0.7.0,,,,,,,0.8.0,,,,documentation,,,0,,,,,,,,,,,,,"Documentation syntax should reflect the correct code indicated in the tutorial script.

Documentation syntax 
{code}
ordered_uniq_frequency = ORDER filtered_uniq_frequency BY (hour, score);
{code}

Above syntax results in this error:
{code}
2010-06-30 22:12:16,412 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Encountered "" "","" "", """" at line 1, column 64.
Was expecting:
    "")"" ..
{code}

(Correct) Tutorial script syntax
{code}
ordered_uniq_frequency = ORDER filtered_uniq_frequency BY hour, score;
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-09-03 22:55:06.461,,,no_permission,,,,,,,,,,,,164964,,,,Fri Sep 03 22:55:06 UTC 2010,,,,,,,0|i0gwbb:,96665,,,,,,,,,,03/Sep/10 22:55;chandec;Pig Tutorial updated - patch submitted via pig-1600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DefaultDataBag assumes ArrayList as default List type,PIG-1469,12467990,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,azaroth,azaroth,azaroth,27/Jun/10 13:17,02/Jul/10 17:18,14/Mar/19 03:07,02/Jul/10 17:17,0.8.0,,,,,,,0.8.0,,,,data,,,0,,,,,,,,,,,,,"In org.apache.pig.data.DefaultDataBag, the field mContents is assumed to be of type ArrayList but the user can actually pass a different List to the constructor.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Jun/10 13:18;azaroth;PIG-1469.patch;https://issues.apache.org/jira/secure/attachment/12448156/PIG-1469.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-06-27 19:40:11.283,,,no_permission,,,,,,,,,,,,164956,,,,Fri Jul 02 17:17:06 UTC 2010,,,,,,,0|i0gw87:,96651,,,,,,,,,,"27/Jun/10 19:40;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12448156/PIG-1469.patch
  against trunk revision 958053.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/334/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/334/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/334/console

This message is automatically generated.","27/Jun/10 23:14;azaroth;The failures seem related to hudson.
There is no unit test because it is a trivial change.
If needed, I can provide some simple test case.",28/Jun/10 04:54;daijy;+1. I am fine commit it as is since the change is trivial. ,02/Jul/10 17:17;dvryaboy;I committed this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"order by fail when set ""fs.file.impl.disable.cache"" to true",PIG-1467,12467950,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,25/Jun/10 22:24,17/Dec/10 22:44,14/Mar/19 03:07,28/Jun/10 18:09,0.7.0,,,,,,,0.7.0,0.8.0,,,impl,,,0,,,,,,,,,,,,,"Order by fail with the message:
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.setConf(WeightedRangePartitioner.java:135)
    at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)
    at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
    at org.apache.hadoop.mapred.MapTask$NewOutputCollector.<init>(MapTask.java:551)
    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:630)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:314)
    at org.apache.hadoop.mapred.Child$4.run(Child.java:217)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:396)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1062)
    at org.apache.hadoop.mapred.Child.main(Child.java:211)

This happens with the following hadoop settings:
fs.file.impl.disable.cache=true
fs.hdfs.impl.disable.cache=true
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Jun/10 22:57;daijy;PIG-1467-1.patch;https://issues.apache.org/jira/secure/attachment/12448103/PIG-1467-1.patch,25/Jun/10 23:28;daijy;PIG-1467-2.patch;https://issues.apache.org/jira/secure/attachment/12448105/PIG-1467-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-06-25 23:18:50.326,,,no_permission,,,,,,,,,,,,164954,Reviewed,,,Mon Jun 28 18:09:25 UTC 2010,,,,,,,0|i0gw7b:,96647,,,,,,,,,,25/Jun/10 22:57;daijy;This error also apply to skewed join,"25/Jun/10 23:18;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12448103/PIG-1467-1.patch
  against trunk revision 958053.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/352/console

This message is automatically generated.",25/Jun/10 23:28;daijy;Resync with trunk. Patch 1 is still valid for branch 0.7.,"26/Jun/10 05:21;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12448105/PIG-1467-2.patch
  against trunk revision 958053.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 145 javac compiler warnings (more than the trunk's current 140 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/353/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/353/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/353/console

This message is automatically generated.",28/Jun/10 17:59;rding;+1,28/Jun/10 18:09;daijy;Patch committed to both trunk and 0.7 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Replace ""bz"" with "".bz"" in setStoreLocation in PigStorage ",PIG-1463,12467766,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,zjffdu,zjffdu,zjffdu,24/Jun/10 02:48,17/Dec/10 22:44,14/Mar/19 03:07,24/Jun/10 03:38,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Jun/10 02:50;zjffdu;PIG_1463.patch;https://issues.apache.org/jira/secure/attachment/12447918/PIG_1463.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-06-24 03:17:55.276,,,no_permission,,,,,,,,,,,,164950,,,,Thu Jul 08 01:44:27 UTC 2010,,,,,,,0|i0gw5z:,96641,,,,,,,,,,24/Jun/10 02:50;zjffdu;Attach the patch for this small issue.,24/Jun/10 03:17;ashutoshc;+1,24/Jun/10 03:38;zjffdu;Patch committed,"07/Jul/10 17:36;ashutoshc;Jeff,

Similar problem exists in getInputFormat() of PigStorage. There ain't no leading '.' before bz2 or bz. As a result Pig may attempt to load filenames ending with bz (such as myfilebz) as compressed bzip file. Would you like to take a look?","08/Jul/10 01:44;zjffdu;Thanks, Ashutosh. I check other places and find that TextLoader has the same problem. I create another jira PIG-1487 to track this problem",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No informative error message on parse problem,PIG-1462,12467658,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,ankur,ankur,23/Jun/10 04:30,04/Aug/11 00:34,14/Mar/19 03:07,10/Mar/11 19:12,0.7.0,,,,,,,0.9.0,,,,,,,0,,,,,,,,,,,,,"Consider the following script

in = load 'data' using PigStorage() as (m:map[]);
tags = foreach in generate m#'k1' as (tagtuple: tuple(chararray));
dump tags;

This throws the following error message that does not really say that this is a bad declaration

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1000: Error during parsing. Encountered """" at line 2, column 38.
Was expecting one of:
    
	at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1170)
	at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1114)
	at org.apache.pig.PigServer.registerQuery(PigServer.java:425)
	at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:737)
	at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:324)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:162)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:138)
	at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:89)
	at org.apache.pig.Main.main(Main.java:391)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PIG-1618,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-06-23 05:19:39.044,,,no_permission,,,,,,,,,,,,66253,,,,Thu Mar 10 19:12:15 UTC 2011,,,,,,,0|i0gw5r:,96640,,,,,,,,,,"23/Jun/10 05:19;ashutoshc;This has come up before. As noted on PIG-798 correct way to achieve this is
{code}
grunt> in = load 'data' using PigStorage() as (m:map[]);                 
grunt> tags = foreach in generate (tuple(chararray)) m#'k1' as tagtuple;                    
grunt> dump tags;

{code}
 
We probably need to add a note about casting in cookbook. Also, need to generate better error message.","23/Jun/10 05:22;ankur;Right, the JIRA is for adding a better error message that doesn't leave a user guessing",26/Jul/10 22:00;olgan;Pig 0.9 release will focus on error handling,"10/Mar/11 19:12;xuefuz;Better error message is given now:

runt> in = load 'data' using PigStorage() as (m:map[]);
grunt> tags = foreach in generate m#'k1' as (tagtuple: tuple(chararray));
2011-03-10 11:11:12,479 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 0: line 13:37 mismatched input [@228,503:503='(',<104>,13:37] expecting IDENTIFIER
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDF manual and javadocs should make clear how to use RequiredFieldList,PIG-1460,12467371,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,chandec,alangates,alangates,18/Jun/10 19:43,17/Dec/10 22:44,14/Mar/19 03:07,03/Sep/10 22:58,0.7.0,,,,,,,0.8.0,,,,documentation,,,0,,,,,,,,,,,,,"The UDF manual mentions that load function writers need to handle RequiredFieldList passed to LoadPushDown.pushProjection, but it does not specify how the writer should interpret the contents of that list.  The javadoc is similarly vague. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Jul/10 23:17;daijy;PIG-1460-1.patch;https://issues.apache.org/jira/secure/attachment/12449401/PIG-1460-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-07-12 23:00:24.876,,,no_permission,,,,,,,,,,,,164948,,,,Fri Sep 03 22:58:08 UTC 2010,,,,,,,0|i0gw53:,96637,,,,,,,,,,"18/Jun/10 19:45;alangates;From email thread on the pig-user list:

{quote}
The documentation is also poor when it comes to describing what the
RequiredFieldList even is.

It has a name and an index field.   The code itself seems to allow for
either of these to be filled.  What do they mean?

Is it:
the schema returned by the loader is:
(id: int, name: chararray, department: chararray)

The RequiredFieldList is [ (""department"", 1) , (""id"", 0) ]

What does that mean?
* The name is the field name requested, and the index is the location it
should be in the result?  so return (id: int, department: chararray)?
* The index is the index in the source schema, and the name is for
renaming, so return (department: chararray, id: int) (where the data in
department is actualy that from the original's name field)?
* The location in the RequiredFieldList array is the 'destination'
requested, the name is optional (if the schema had one) and the index is the
location in the original schema.  so the above RequiredFieldList is actually
impossible, since ""department"" is always index 2.
{quote}

The last is the correct answer.

","12/Jul/10 23:00;olgan;Pradeep, could you provide the information needed and also update the javadoc. Then, please, re-assign to Corinne so that she can update the UDF manual, thanks.",13/Jul/10 22:42;daijy;Patch for javadoc for RequiredFieldList,"13/Jul/10 23:11;daijy;Hi, Corinne,
Can you update Pig UDF Manual? After section ""LoadPushDown"", add a subsection (pushProjection() is a method of LoadPushDown (also help to polish my writing):

pushProjection(): This method tells LoadFunc which fields is used in Pig, so LoadFunc is able to optimize not to load unneeded fields. pushProjection takes a RequiredFieldList. Each item inside RequiredFieldList indicates a required field Pig needs. Pig will use column index RequiredField.index to communicate with the LoadFunc about the fields Pig needs. If the field is a map, Pig will optionally pass RequiredField.subFields which contains a list of keys Pig needs for that map. For example, if we need two keys ""key1"", ""key2"" for the map, we will have a subFields for that map which contains two RequiredField. The alias field for 1st RequiredField is ""key1"", alias field for 2nd RequiredField is ""key2"". RequiredField.type is reserved for future use. LoadFunc will use RequiredFieldResponse.requiredFieldRequestHonored to indicate whether the pushProjection request is honored.","13/Jul/10 23:18;olgan;Pradeep, could you please review before Daniel commits the text and before Corinne updates the docs. Thanks.

Daniel, please, assign the ticket to Corinne once your patch is committed, thanks.",03/Sep/10 22:58;chandec;UDF manual updated - patch submitted via pig-1600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[zebra] Intermittent failure for TestOrderPreserveUnionHDFS,PIG-1453,12467101,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yanz,daijy,daijy,16/Jun/10 17:28,17/Dec/10 22:44,14/Mar/19 03:07,23/Jul/10 21:11,0.8.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Jun/10 21:30;yanz;PIG-1453.patch;https://issues.apache.org/jira/secure/attachment/12447494/PIG-1453.patch,17/Jun/10 19:07;yanz;PIG-1453.patch;https://issues.apache.org/jira/secure/attachment/12447373/PIG-1453.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-06-16 19:09:55.49,,,no_permission,,,,,,,,,,,,164942,,,,Fri Jul 23 21:11:40 UTC 2010,,,,,,,0|i0gw1r:,96622,,,,,,,,,,"16/Jun/10 19:09;yanz;There are two issues that generally make some test cases (not just TestOrderPreserveUnionHDFS) in Zebra's ""pigtest"" fail intermittently.

1) There is some randomness when multiple tables are unioned. The correctness check relies on the ordering of tables in output rows, which is incorrect. Instead the table a particular row belongs to can only be associated with the table index in output;

2) There are some failures in PIG STORE calls as the destination directory are not cleaned up properly before store.","18/Jun/10 00:54;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12447373/PIG-1453.patch
  against trunk revision 955701.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 36 new or modified tests.

    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/341/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/341/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/341/console

This message is automatically generated.","19/Jun/10 04:52;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12447494/PIG-1453.patch
  against trunk revision 955763.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 36 new or modified tests.

    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/331/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/331/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/331/console

This message is automatically generated.","24/Jun/10 01:28;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12447494/PIG-1453.patch
  against trunk revision 957277.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 36 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/348/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/348/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/348/console

This message is automatically generated.",23/Jul/10 21:11;yanz;Committed to the trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RegExLoader hangs on lines that don't match the regular expression,PIG-1449,12466915,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,,justinjas,justinjas,14/Jun/10 17:37,02/Jul/10 17:07,14/Mar/19 03:07,02/Jul/10 06:11,0.7.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"In the 0.7.0 changes to RegExLoader there was a bug introduced where the code will stay in the while loop if the line isn't matched.  Before 0.7.0 these lines would be skipped if they didn't match the regular expression.  The result is the mapper will not respond and will time out with ""Task attempt_X failed to report status for 600 seconds. Killing!"".

Here are the steps to recreate the bug:

Create a text file in HDFS with the following lines:

test1
testA
test2

Run the following pig script:

REGISTER /usr/local/pig/contrib/piggybank/java/piggybank.jar;
test = LOAD '/path/to/test.txt' using org.apache.pig.piggybank.storage.MyRegExLoader('(test\\d)') AS (line);
dump test;

Expected result:

(test1)
(test3)

Actual result:

Job fails to complete after 600 second timeout waiting on the mapper to complete.  The mapper hangs at 33% since it can process the first line but gets stuck into the while loop on the second line.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Jul/10 19:34;engrean;PIG-1449-RegExLoaderInfiniteLoopFix.patch;https://issues.apache.org/jira/secure/attachment/12448516/PIG-1449-RegExLoaderInfiniteLoopFix.patch,14/Jun/10 17:44;justinjas;RegExLoader.patch;https://issues.apache.org/jira/secure/attachment/12447045/RegExLoader.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-06-14 17:54:59.517,,,no_permission,,,,,,,,,,,,164938,,,,Fri Jul 02 06:10:19 UTC 2010,,,Patch Available,,,,0|i0gw07:,96615,Fixed hanging in RegExLoader if line didn't match regular expression.,,,,,,RegExLoader MyRegExLoader piggybank LoadFunc,,,"14/Jun/10 17:54;ashutoshc;Justin,

Good catch. Can you assimilate your test case in junit in one of piggybank/test/storage/TestRegExLoader or TestMyRegExLoader. That way we'll have a regression test for the issue.","15/Jun/10 00:45;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12447045/RegExLoader.patch
  against trunk revision 953798.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/328/console

This message is automatically generated.","01/Jul/10 19:32;engrean;I ran into this issue last night and before seeing this bug, I fixed it. My fix is similar to the previous, but it includes a unit test. Hopefully, the test will help move this in more quickly. I notice that it takes over 4 minutes to run the unit tests. Would be any added value in trying to reduce the execution time in these tests? If there's any interest, I might be able to help.",01/Jul/10 19:34;engrean;This should fix the problem by adding a call to nextKeyValue on each iteration. ,01/Jul/10 21:22;ashutoshc;Running through Hudson.,"02/Jul/10 04:07;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12448516/PIG-1449-RegExLoaderInfiniteLoopFix.patch
  against trunk revision 958666.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/357/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/357/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/357/console

This message is automatically generated.","02/Jul/10 06:07;ashutoshc;Reran the contrib tests. All passed. Patch committed. Thanks, Christian and Justin for working on this !","02/Jul/10 06:10;ashutoshc;@Christian,

It would definitely be useful to get the execution time for running the tests down. It takes a while currently to run all Pig tests.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Detach tuple from inner plans of physical operator ,PIG-1448,12466821,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,ashutoshc,ashutoshc,12/Jun/10 17:01,17/Dec/10 22:44,14/Mar/19 03:07,13/Aug/10 23:57,0.1.0,0.2.0,0.3.0,0.4.0,0.5.0,0.6.0,0.7.0,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"This is a follow-up on PIG-1446 which only addresses this general problem for a specific instance of For Each. In general, all the physical operators which can have inner plans are vulnerable to this. Few of them include POLocalRearrange, POFilter, POCollectedGroup etc.  Need to fix all of these.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Aug/10 00:35;thejas;PIG-1448.1.patch;https://issues.apache.org/jira/secure/attachment/12451847/PIG-1448.1.patch,12/Aug/10 00:34;thejas;multi_oom_filt.pig;https://issues.apache.org/jira/secure/attachment/12451846/multi_oom_filt.pig,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-08-12 00:34:56.918,,,no_permission,,,,,,,,,,,,164937,,,,Fri Aug 13 23:57:13 UTC 2010,,,,,,,0|i0gvzr:,96613,,,,,,,,,,"12/Jun/10 17:57;ashutoshc;Problem here is not as bad as it may sound. All the physical operator already detaches the input tuple after they are done with it. In the getNext() phy op first calls processInput() which first attaches the input tuple and then detaches it at the end. So, physical operators contained within inner plans will also do that. Problem is when there is a Bin Cond, Pig short circuits one of the branches of the inner plan, in which case getNext() of the operator is never called and thus tuple is never detached. Note in these cases, tuple was already attached by the operator which had this inner plan to all the roots of the plan. So, in this particular use case tuple got attached but was never detached and thus had the stray reference which cannot be GC'ed. This still will not be a problem if there is only a single pipeline in mapper or reducer since the next time new key/value pair is read and is run through pipeline, the reference will be overwritten and thus tuple which was not detached in previous run can now be GC'ed. Only if you have Multi Query optimized script the same pipeline may not be run when the next key/value pair is read in map() or reduce() and then stray reference will not be overwritten. If all of these conditions are met and if tuple  itself is large or contains large bags, we may end up with OOME. ","12/Aug/10 00:34;thejas;multi_oom_filt.pig is a query that reproduces this problem for a filter query. There is a bincond in the filtercondition, and one side of the bincond does not get evaluated and detached. The query runs out of memory in reduce. With fix to call detach from POFilter, the query succeeds.
","13/Aug/10 17:25;thejas;Pasting result of test-patch . No new tests are included because the patch only changes the time at which memory is freed.

     [exec]
     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec]
","13/Aug/10 20:18;thejas;All tests are successful. Patch is ready for review.
",13/Aug/10 22:38;rding;+1. Looks good.,"13/Aug/10 23:57;thejas;Patch committed to trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OOME in a query having a bincond in the inner plan of a Foreach.,PIG-1446,12466671,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,10/Jun/10 17:23,17/Dec/10 22:44,14/Mar/19 03:07,11/Jun/10 17:55,0.7.0,,,,,,,0.7.0,0.8.0,,,,,,0,,,,,,,,,,,,,This is seen when For Each is following a group-by and there is a bin cond as an inner plan of For Each.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Jun/10 17:33;ashutoshc;pig-1446.patch;https://issues.apache.org/jira/secure/attachment/12446776/pig-1446.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-06-10 19:30:31.79,,,no_permission,,,,,,,,,,,,164935,,,,Fri Jun 11 17:55:59 UTC 2010,,,,,,,0|i0gvyv:,96609,,,,,,,,,,"10/Jun/10 17:33;ashutoshc;Sequence of event is as follows:
1) MultiQuery optimizer combined 30 group-bys in one reducer. So, there are 30 pipelines in a reducer.
2) Each of these group-by has a ForEach after them.
3) ForEach has a bincond in its own plan.
4) Group-by resulted in large bags (10s of million of records).
5) Tuple containing group and bag is attached to the roots of inner plan of FE.
6) FE pulled the tuples through its leaves.
7) Due to short-circuiting in bin-cond, one branch of the plan is never pulled resulting in stray reference of bag which actually was not needed.
8) Due to MQ optimized 30 group-bys, we had many such bags now hanging in there, eating up all the memory.

Fix: Detach tuples from the roots once you are done in FE.",10/Jun/10 19:30;daijy;+1. Please commit after test and hudson pass.,"11/Jun/10 17:55;ashutoshc;As usual, hudson is not responding. I manually ran all the unit tests, all of them passed. Committed to both trunk and 0.7",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig error: ERROR 2013: Moving LOLimit in front of LOStream is not implemented ,PIG-1445,12466592,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,09/Jun/10 18:11,17/Dec/10 22:44,14/Mar/19 03:07,27/Jul/10 16:51,0.7.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"The following script fail due to ""ERROR 2013: Moving LOLimit in front of LOStream is not implemented"".

{code}
A = LOAD 'data';
B = STREAM A THROUGH `stream.pl`;
C = LIMIT B 10;
explain C;
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Jun/10 19:11;daijy;PIG-1445-1.patch;https://issues.apache.org/jira/secure/attachment/12446718/PIG-1445-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-06-10 04:34:36.433,,,no_permission,,,,,,,,,,,,164934,Reviewed,,,Tue Jul 27 16:51:15 UTC 2010,,,,,,,0|i0gvyn:,96608,,,,,,,,,,09/Jun/10 19:11;daijy;We should not push LOLimit in front of LOStream. Attach patch.,"10/Jun/10 04:34;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12446718/PIG-1445-1.patch
  against trunk revision 953109.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 9 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 383 release audit warnings (more than the trunk's current 382 warnings).

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/322/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/322/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/322/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/322/console

This message is automatically generated.",26/Jul/10 23:03;rding;+1,27/Jul/10 16:51;daijy;Run unit test manually and all pass. Patch committed. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make sure dependent jobs fail when a jon in multiquery fails,PIG-1435,12466144,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nrai,olgan,olgan,03/Jun/10 23:53,17/Dec/10 22:44,14/Mar/19 03:07,23/Jul/10 18:24,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"Currently if one of the MQ jobs fails, Pig tries to run all remainin jobs. As the result, if data was partially generated by the failed job, you might get incorrect results from dependent jobs. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Jul/10 21:12;nrai;depJobs.patch;https://issues.apache.org/jira/secure/attachment/12449486/depJobs.patch,16/Jul/10 01:22;nrai;depJobsFailure.patch;https://issues.apache.org/jira/secure/attachment/12449609/depJobsFailure.patch,21/Jul/10 06:36;nrai;depJobsFailure2.patch;https://issues.apache.org/jira/secure/attachment/12450023/depJobsFailure2.patch,21/Jul/10 18:39;nrai;depJobsFailure3.patch;https://issues.apache.org/jira/secure/attachment/12450075/depJobsFailure3.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2010-07-14 21:12:20.879,,,no_permission,,,,,,,,,,,,164924,Reviewed,,,Fri Jul 23 18:24:17 UTC 2010,,,,,,,0|i0gvun:,96590,,,,,,,,,,14/Jul/10 21:12;nrai;Attaching patch for the review,"15/Jul/10 04:24;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12449486/depJobs.patch
  against trunk revision 964182.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 405 release audit warnings (more than the trunk's current 404 warnings).

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/369/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/369/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/369/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/369/console

This message is automatically generated.",16/Jul/10 01:22;nrai;Fixed the hudson failure issues,"17/Jul/10 04:54;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12449609/depJobsFailure.patch
  against trunk revision 964182.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 405 release audit warnings (more than the trunk's current 404 warnings).

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/371/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/371/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/371/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/371/console

This message is automatically generated.","19/Jul/10 20:29;rding;Looks good. A few comments:

* The new test class _test/org/apache/pig/test/utils/UPPER.java_ is missing Apache header and this results in the release audit warning.
* In the test case, the line

{code}
w.println(""B = FOREACH A GENERATE UPPER(name);"");
{code}

should be replaced by 

{code}
w.println(""B = FOREACH A GENERATE org.apache.pig.test.utils.UPPER(name);"");
{code}

otherwise the builtin UDF UPPER is used and job will succeed.

* Also in the test case, should replace

{code}
if(js.getState().equals(""FAILED"")){
{code}

with 

{code}
if(js.getState().name().equals(""FAILED"")){
{code}

to perform the string comparison.

* The logging in _MapReduceLauncher_

{code}
log.info(""jobs"" + job.getAssignedJobID() + "" have failed!
{code}
 
should be written as 

{code}
log.info(""job "" + job.getAssignedJobID() + "" has failed!
{code}

",21/Jul/10 06:36;nrai;review recommendations implemented,21/Jul/10 18:39;nrai;Made changes to make it compatible with PIG-1478,23/Jul/10 18:24;rding;Patch committed to trunk. Thanks Niraj.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig should create success file if mapreduce.fileoutputcommitter.marksuccessfuljobs is true,PIG-1433,12466044,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pradeepkth,pradeepkth,pradeepkth,03/Jun/10 04:29,17/Dec/10 22:44,14/Mar/19 03:07,04/Jun/10 16:08,0.8.0,,,,,,,0.7.0,0.8.0,,,,,,0,,,,,,,,,,,,,pig should create success file if mapreduce.fileoutputcommitter.marksuccessfuljobs is true,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Jun/10 16:07;pradeepkth;PIG-1433-for-branch-0.7.patch;https://issues.apache.org/jira/secure/attachment/12446351/PIG-1433-for-branch-0.7.patch,03/Jun/10 04:34;pradeepkth;PIG-1433.patch;https://issues.apache.org/jira/secure/attachment/12446222/PIG-1433.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-06-03 22:01:10.233,,,no_permission,,,,,,,,,,,,164922,Reviewed,,,Fri Jun 04 16:07:32 UTC 2010,,,,,,,0|i0gvtr:,96586,,,,,,,,,,03/Jun/10 04:34;pradeepkth;Attached patch addresses the issue in MapReduceLauncher by creating an _SUCCESS file for stores which are part of successful jobs if the property is set in the job.,"03/Jun/10 21:03;pradeepkth;Hudson seems to be unresponsive - I ran unit tests locally and they completed successfully. The ""test-patch"" ant target also came back successfully except for a html page change in the release audit warnings which can be ignored.

Patch is ready for review.","03/Jun/10 22:01;ashutoshc;+1 for the commit. couple of notes for future:
* Since this is related to Hadoop property. We should consider this removing from Pig codebase when MAPREDUCE-1447 and MAPREDUCE-947 are fixed.
* We have lot of constant strings in our codebase. For the sake of clean code, we shall put all of those public static final string in one top level interface called Constants. This should be part of seperate clean-up code jira.","03/Jun/10 22:06;azaroth;Just for the sake of clean code, constant interface is an anti-pattern.

http://en.wikipedia.org/wiki/Constant_interface

Public final instance controlled (no instance) classes are better for this purpose.","03/Jun/10 22:22;ashutoshc;My point was to have all constant strings in one place instead of each class having some of them It could be either interface or class. If interface is considered anti-pattern, doing it in class is fine too.","04/Jun/10 15:53;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12446222/PIG-1433.patch
  against trunk revision 951229.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/330/console

This message is automatically generated.",04/Jun/10 16:07;pradeepkth;The original patch was committed to trunk. It did not apply for branch-0.7 - so I have attached a new patch with minor modifications for branch-0.7. This latter patch was committed to branch-0.7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[zebra] There are some debuging info output to STDOUT in PIG's TableStorer call path,PIG-1432,12465917,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,yanz,yanz,yanz,01/Jun/10 23:54,17/Dec/10 22:44,14/Mar/19 03:07,03/Jun/10 00:15,0.7.0,,,,,,,0.7.0,0.8.0,,,,,,0,,,,,,,,,,,,,"Users redirecting STDOUT to disk file got ""disk full"" errors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Jun/10 23:59;yanz;PIG-1432.patch;https://issues.apache.org/jira/secure/attachment/12446078/PIG-1432.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-06-02 03:13:18.943,,,no_permission,,,,,,,,,,,,164921,,,,Thu Jun 03 00:15:26 UTC 2010,,,,,,,0|i0gvtj:,96585,,,,,,,,,,"02/Jun/10 03:13;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12446078/PIG-1432.patch
  against trunk revision 949057.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h1.grid.sp2.yahoo.net/19/console

This message is automatically generated.",02/Jun/10 16:02;yanz;The patch is based on the 0.7 branch. No test is necessary as athis is a trivial fix.,"02/Jun/10 18:34;yanz;Internal Hudson results:

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
","03/Jun/10 00:04;gauravj;
+1",03/Jun/10 00:15;yanz;Committed to both 0.7 branch and trunk where TableStorer does not output to STDOUT in itself but the other two occurrences in key generator called by TableStorer are still present.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make a StatusReporter singleton available for incrementing counters,PIG-1428,12465612,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,ashutoshc,ashutoshc,27/May/10 23:44,14/Jul/10 22:55,14/Mar/19 03:07,17/Jun/10 18:25,0.7.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"Without this getter method, its not possible to get counters, report progress etc. from UDFs. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Jun/10 20:30;dvryaboy;PIG-1428.patch;https://issues.apache.org/jira/secure/attachment/12446958/PIG-1428.patch,02/Jun/10 03:30;dvryaboy;PIG-1428.patch;https://issues.apache.org/jira/secure/attachment/12446095/PIG-1428.patch,01/Jun/10 01:54;dvryaboy;PIG-1428.patch;https://issues.apache.org/jira/secure/attachment/12445985/PIG-1428.patch,14/Jul/10 22:52;daijy;npe.patch;https://issues.apache.org/jira/secure/attachment/12449498/npe.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2010-06-01 01:54:24.942,,,no_permission,,,,,,,,,,,,41711,,,,Wed Jul 14 22:55:24 UTC 2010,,,Patch Available,,,,0|i0gvrr:,96577,,,,,,,pig-0.7.1,,,"01/Jun/10 01:54;dvryaboy;No tests, as this is trivial.",01/Jun/10 01:55;dvryaboy;please review if this gets no -1s other than lack of tests.,"01/Jun/10 06:59;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12445985/PIG-1428.patch
  against trunk revision 949057.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.

    -1 release audit.  The applied patch generated 386 release audit warnings (more than the trunk's current 385 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h1.grid.sp2.yahoo.net/17/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h1.grid.sp2.yahoo.net/17/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h1.grid.sp2.yahoo.net/17/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h1.grid.sp2.yahoo.net/17/console

This message is automatically generated.","01/Jun/10 07:23;dvryaboy;Findbugs is quite right to call me out on the synchronization thing. I am not sure why the setter needs to by synchronized; I am even less sure the getter should be.  Seems like this would add one more lock every time we want to increment a counter or write a log line, which is unfortunate (I assume those objects handle their own concurrency issues). Can Richard or Pradeep comment on that?",01/Jun/10 20:26;rding;Remove the keyword synchronized from the getter will fix this findbugs warning. Setters are called only once per map/reduce task (during the setup). ,02/Jun/10 03:30;dvryaboy;removed the synchronized keyword,"02/Jun/10 23:50;dvryaboy;I notice that the issue has been discussed before in PIG-889, and Santosh argued (convincingly) that adding this method to PigLogger might not make sense. Santosh, would you like to suggest a different place to put this functionality? I am not married to using this method, it's just the path of least resistance.",06/Jun/10 22:09;dvryaboy;trying to tickle hudson,"08/Jun/10 16:09;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12446095/PIG-1428.patch
  against trunk revision 952098.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 383 release audit warnings (more than the trunk's current 382 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/332/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/332/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/332/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/332/console

This message is automatically generated.","10/Jun/10 05:31;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12446095/PIG-1428.patch
  against trunk revision 949057.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 379 release audit warnings (more than the trunk's current 378 warnings).

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/318/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/318/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/318/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/318/console

This message is automatically generated.","10/Jun/10 22:00;ashutoshc;So, I read through PIG-889. It seems that there never was a documented way to use counters, reporters etc from UDFs, Load/Store Funcs. Actually, there is a hacky way to do it, which exists in DefaultAbstractBag.java 
{code}
    protected void incSpillCount(Enum counter) {
        // Increment the spill count
        // warn is a misnomer. The function updates the counter. If the update
        // fails, it dumps a warning
        PigHadoopLogger.getInstance().warn(this, ""Spill counter incremented"", counter);
    }
{code}
But in PIG-889 Santhosh has argued against for this (mis)use of PigLogger. I think we need to provide a formal way to Pig users to access counters, reporters from our interfaces (UDFs, L/S) as PigHadoopLogger is designed for error-handling (warning aggregation in particular) and not for this purpose. And we shall mark this class as Internal only, before some one starts using it. With the same argument, above method where Pig is internally making use of its own Counters is flawed and needs to be corrected.","10/Jun/10 22:55;ashutoshc;I propose a slightly different approach here. Instead of adding getPigStatusReporter() to PigLogger() interface and the corresponding implementation in PigHadoopLogger, we can add a static singleton method in PigStatusReporter and also add a setContext( TaskInputOutputContext context) We can then set the context in map() and reduce() functions and users will have full access of the reporter object through the static method. This will allow us to keep error logging different then status reporting. ","12/Jun/10 20:30;dvryaboy;Once more, with feeling.
This implements Ashutosh's suggestion of making PigStatusReporter maintain a singleton and expose a public getInstance() method.",14/Jun/10 18:13;rding;+1,"17/Jun/10 18:25;dvryaboy;Committed to trunk.
We may want to consider this for a 0.7.1, if such a thing comes about, as in a sense it's addressing a regression.

I tagged this issue with ""pig-0.7.1"" so we can find it later if we decide a dot-release is warranted.",14/Jul/10 06:15;dvryaboy;Committed to 0.7. ,14/Jul/10 20:34;olgan;Looks like the change broke unit tests. Hudson only runs tests against the trunk so we need to run the manually against other branches,"14/Jul/10 21:19;dvryaboy;I ran the new and changed tests manually before committing, but not the whole set (didn't have 12 hours to spare). Which tests are failing for you?",14/Jul/10 22:33;olgan;Daniel will publish tests that failed. We do need to make sure that we run all the tests especially on branches before committing because branches need to stay stable.,"14/Jul/10 22:43;dvryaboy;Found the culprit, will commit fix within ~ 20 mins assuming tests pass.
",14/Jul/10 22:52;daijy;TestDataBag for 0.7 branch fail after check in. Attach the patch. This fix is already checked in.,"14/Jul/10 22:55;dvryaboy;yeah that's the patch I have, verbatim. Sorry about breaking the build again. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Remove ""user.name"" from JobConf",PIG-1419,12464567,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,14/May/10 22:29,17/Dec/10 22:44,14/Mar/19 03:07,26/May/10 17:22,0.7.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"In hadoop security, hadoop will use kerberos id instead of unix id. Pig should not set ""user.name"" entry in jobconf. This should be decided by hadoop.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/May/10 22:31;daijy;PIG-1419-1.patch;https://issues.apache.org/jira/secure/attachment/12444536/PIG-1419-1.patch,26/May/10 02:25;daijy;PIG-1419-2.patch;https://issues.apache.org/jira/secure/attachment/12445522/PIG-1419-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-05-25 22:20:27.767,,,no_permission,,,,,,,,,,,,164911,Reviewed,,,Wed May 26 17:42:36 UTC 2010,,,,,,,0|i0gvnz:,96560,,,,,,,,,,"25/May/10 22:20;pradeepkth;+1

Minor observation in GruntParser.java:
{noformat}
565                 if (path == null) {                                                                                                                                                                                                  
  566                     if (mDfs instanceof HDataStorage) {                                                                                                                                                                              
  567                         container = mDfs.asContainer(((HDataStorage)mDfs).                                                                                                                                                           
  568                                 getHFS().getHomeDirectory().toString());                                                                                                                                                             
  569                     } else                                                                                                                                                                                                           
  570                         container = mDfs.asContainer(""/user/"" + System.getProperty(""user.name""));        
{noformat}

Would the else ever get executed? (I think currently mDfs is always an instance of HDataStorage right?) If this is just to make it future proof, then I am fine keeping it. Minor style comment - would be good to enclose the else in {} even though it is a single statement - there is another statement right below the container = ... statement - so it would be more readable with {} block.","26/May/10 02:21;daijy;For Pradeep's review comment, I think we can safely assume HDataStorage is the only data storage, so we can remove this check and make it simpler. Reattach the patch.",26/May/10 17:22;daijy;Manual test pass. Patch committed.,26/May/10 17:42;pradeepkth;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problem with parameter substitution,PIG-1414,12464386,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,12/May/10 20:27,17/Dec/10 22:44,14/Mar/19 03:07,13/May/10 21:02,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"
The following script:

{code}
L = load 'input';
store L into 'output' using MyClass$StorerAsInnerClass();
{code}

causes Pig to fail with this error message:

{code}
ERROR org.apache.pig.Main - ERROR 2999: Unexpected internal error. Undefined parameter : StorerAsInnerClass

java.lang.RuntimeException: Undefined parameter : StorerAsInnerClass
        at org.apache.pig.tools.parameters.PreprocessorContext.substitute(PreprocessorContext.java:232)
        at org.apache.pig.tools.parameters.PigFileParser.input(PigFileParser.java:60)
        at org.apache.pig.tools.parameters.PigFileParser.Parse(PigFileParser.java:42)
        at org.apache.pig.tools.parameters.ParameterSubstitutionPreprocessor.parsePigFile(ParameterSubstitutionPreprocessor.java:105)
        at org.apache.pig.tools.parameters.ParameterSubstitutionPreprocessor.genSubstitutedFile(ParameterSubstitutionPreprocessor.java:98)
        at org.apache.pig.Main.runParamPreprocessor(Main.java:576)
        at org.apache.pig.Main.main(Main.java:418)
{code} 

even though no parameter substitution is specified from the command line. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/May/10 22:45;rding;PIG-1414.patch;https://issues.apache.org/jira/secure/attachment/12444353/PIG-1414.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-05-12 20:58:56.029,,,no_permission,,,,,,,,,,,,164906,Reviewed,,,Thu May 13 21:02:53 UTC 2010,,,,,,,0|i0gvm7:,96552,,,,,,,,,,"12/May/10 20:58;olgan;The issue is with ""$"". I think you just need to escape it.",12/May/10 22:45;rding;Patch to fix unit test failures due to problem with parameter substitution.,"13/May/10 04:00;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12444353/PIG-1414.patch
  against trunk revision 943578.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/316/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/316/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/316/console

This message is automatically generated.",13/May/10 20:57;olgan;+1,13/May/10 21:02;rding;This patch fixed the failed unit tests due to parameter substitution.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Logging starts before being configured,PIG-1407,12463961,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,azaroth,azaroth,azaroth,07/May/10 09:00,17/Dec/10 22:44,14/Mar/19 03:07,12/May/10 17:34,0.7.0,0.8.0,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"Pig's Main starts logging before log4j has been configured.

This way logging messages are mixed with pig's output.


$ cat script.pig 
A = LOAD 'input' AS (num:long, fruit:chararray);
DUMP A;


$ cat input 
1	orange
2	apple
3	coconut
4	mango
5	grape
6	pear


$ pig -x local prova.pig > dump


$ cat dump
0    [main] INFO  org.apache.pig.Main  - Logging error messages to: /home/gianmarcodfm/pig-sbox/pig_1273222206353.log
(1,orange)
(2,apple)
(3,coconut)
(4,mango)
(5,grape)
(6,pear)

",,;07/May/10 09:14;azaroth;60,60,0,60,100%,60,0,60,,,,,,,,,,,,,,,,,,,,,,,07/May/10 09:02;azaroth;PIG-1407.patch;https://issues.apache.org/jira/secure/attachment/12443944/PIG-1407.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-05-07 14:25:24.283,,,no_permission,,,,,,,,,,,,164899,Reviewed,,,Wed May 12 17:34:41 UTC 2010,,,,,,,0|i0gvjj:,96540,,,,,,,,,,07/May/10 09:02;azaroth;The patch simply moves the first log statement after log4j has been initialized,"07/May/10 14:25;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12443944/PIG-1407.patch
  against trunk revision 941976.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/316/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/316/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/316/console

This message is automatically generated.","07/May/10 15:31;azaroth;As I only modified the order of two lines in Main.java and the bug is related to log4j configuration, there is no unit test available.

The patch is ready for review, I think.",11/May/10 05:28;daijy;+1. Log should happen after setting up. ,12/May/10 17:34;daijy;Patch committed. Thanks Gianmarco!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""explain -script <script file>"" executes grunt commands like run/dump/copy etc - explain -script should not execute any grunt command and only explain the query plans.",PIG-1401,12463571,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pradeepkth,pradeepkth,pradeepkth,03/May/10 16:15,17/Dec/10 22:44,14/Mar/19 03:07,04/May/10 18:42,0.6.0,0.7.0,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"""explain -script <script file>"" executes grunt commands like run/dump/copy etc - explain -script should not execute any grunt command and only explain the query plans.

Note: ""explain <alias>"" statement in the script will still cause all grunt commands upto the explain to be executed. This issue only fixes the behavior of ""explain -script <script file>"" wherein any grunt commands like ""run"", ""dump"", ""copy"", ""fs .."" present in the supplied <script file> will need to be ignored.

This should be documented in the release in which this jira will be resolved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/May/10 20:40;pradeepkth;PIG-1401-2.patch;https://issues.apache.org/jira/secure/attachment/12443498/PIG-1401-2.patch,04/May/10 06:01;pradeepkth;PIG-1401-3.patch;https://issues.apache.org/jira/secure/attachment/12443548/PIG-1401-3.patch,03/May/10 18:09;pradeepkth;PIG-1401.patch;https://issues.apache.org/jira/secure/attachment/12443481/PIG-1401.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-05-03 23:15:32.395,,,no_permission,,,,,,,,,,,,164893,Reviewed,,,Tue May 04 18:42:54 UTC 2010,,,,,,,0|i0gvh3:,96529,,,,,,,,,,"03/May/10 18:09;pradeepkth;Attached patch addresses the issue by checking internal state in GruntParser to check if the current execution is in ""explain -script"" mode and if so, ignores grunt commands like ""run"", ""copy"" etc.",03/May/10 20:38;pradeepkth;The patch did not contain the test script file- will attach new patch shortly,"03/May/10 20:40;pradeepkth;New patch includes test script file needed for the unit test. It also has some changes in code to not call executeBatch() in explain -script mode. Also ""fs .."" commands also invoke executeBatch() now - this was missing but is required since the fs command could be a delete/move/copy command which should result in an execution of the current batch just like the ""rm, mv and cp"" grunt statements do.","03/May/10 23:15;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12443481/PIG-1401.patch
  against trunk revision 940502.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/312/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/312/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/312/console

This message is automatically generated.","04/May/10 01:58;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12443498/PIG-1401-2.patch
  against trunk revision 940601.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 529 release audit warnings (more than the trunk's current 528 warnings).

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/307/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/307/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/307/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/307/console

This message is automatically generated.",04/May/10 06:01;pradeepkth;New patch with a fix in the test script used in TestGrunt unit test,"04/May/10 11:21;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12443548/PIG-1401-3.patch
  against trunk revision 940601.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 536 release audit warnings (more than the trunk's current 535 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/313/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/313/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/313/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/313/console

This message is automatically generated.",04/May/10 16:05;pradeepkth;The release audit warning is due to the new test script file added in the patch and can be ignored - the patch is ready for review.,04/May/10 17:57;olgan;+1,04/May/10 18:42;pradeepkth;Patch committed to trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GruntParser should invoke executeBatch() first in processFsCommand(),PIG-1397,12463376,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,pradeepkth,pradeepkth,29/Apr/10 21:35,17/Dec/10 22:44,14/Mar/19 03:07,16/Jul/10 20:36,0.6.0,0.7.0,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"If a script has multiple stores which can be optimized using multiquery optimization and if the script also has some file system state modifiying commands like ""cp, mv, rm"" then currently Gruntparser executes the plan until the filesystem command so that multiquery optimization will work on the file system after it has been modified (for example some portion of the multi query optimized script might be depending on the cp/mv or rm command to have run first). This is not done for ""fs ..."" commands - GruntParser should do the same even for ""fs .."" commands in processFsCommand()",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-07-16 20:27:54.041,,,no_permission,,,,,,,,,,,,164889,,,,Fri Jul 16 20:36:04 UTC 2010,,,,,,,0|i0gvfb:,96521,,,,,,,,,,16/Jul/10 20:27;rding;This seems already being fixed.,16/Jul/10 20:36;rding;Pradeep confirmed he fixed this in an earlier jira. Mark this as fixed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
eclipse-files target in build.xml fails to generate necessary classes in src-gen,PIG-1396,12463284,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chaitk,chaitk,chaitk,29/Apr/10 03:03,17/Dec/10 22:44,14/Mar/19 03:07,30/Apr/10 20:50,0.8.0,,,,,,,0.8.0,,,,build,,,0,,,,,,,,,,,,,"With PIG-1390 checked in, ant eclipse-files does only the ivy-download part but does not compile and generate the necessary classes in src-gen.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Apr/10 05:08;chaitk;PIG-1396.patch;https://issues.apache.org/jira/secure/attachment/12443165/PIG-1396.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-04-29 21:20:05.28,,,no_permission,,,,,,,,,,,,164888,Reviewed,,,Fri Apr 30 20:50:43 UTC 2010,,,,,,,0|i0gvev:,96519,,,,,,,,,,"29/Apr/10 05:08;chaitk;Uploading patch with single-line change to fix this.
Thejas, can you please check if this works on your mac machine?

Thanks.","29/Apr/10 21:20;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12443165/PIG-1396.patch
  against trunk revision 938733.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/304/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/304/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/304/console

This message is automatically generated.","30/Apr/10 02:54;chaitk;bq. -1 tests included. The patch doesn't appear to include any new or modified tests.
Please justify why no tests are needed for this patch.

Since this is a build change and more specifically, it's a change that is in action only while importing to eclipse. So, it cannot be unit-tested, unlike other patches.

bq. -1 core tests. The patch failed core unit tests.

I don't think this failure is related to this patch because as mentioned before, it's a build change that is used only for eclipse environment issues and this should not hinder the running of ant targets that are responsible for running testcases.","30/Apr/10 20:50;thejas;Thanks Chaitanya.
I have committed the patch along with some changes to the indentation, and also updated the instructions in http://wiki.apache.org/pig/Eclipse_Environment .",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
POCombinerPackage hold too much memory for InternalCachedBag,PIG-1394,12463040,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,26/Apr/10 17:58,14/May/10 06:47,14/Mar/19 03:07,03/May/10 20:23,0.7.0,,,,,,,0.7.0,,,,impl,,,0,,,,,,,,,,,,,"In POCombinerPackage, we create bunch of InternalCachedBag, the number of which is the number of algebraic UDFs we use. However, when we create InternalCachedBag, we use the default construct which assume we only create 1 InternalCachedBag in the system. It turns out we reserve way to much memory to InternalCachedBag.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Apr/10 17:59;daijy;PIG-1394-1.patch;https://issues.apache.org/jira/secure/attachment/12442875/PIG-1394-1.patch,26/Apr/10 21:02;daijy;PIG-1394-2.patch;https://issues.apache.org/jira/secure/attachment/12442897/PIG-1394-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-04-26 23:10:45.345,,,no_permission,,,,,,,,,,,,164886,Reviewed,,,Mon May 03 20:23:08 UTC 2010,,,,,,,0|i0gvdz:,96515,,,,,,,,,,"26/Apr/10 20:55;daijy;Previous patch miss something, attach a new one.","26/Apr/10 23:10;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12442875/PIG-1394-1.patch
  against trunk revision 937570.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/303/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/303/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/303/console

This message is automatically generated.",29/Apr/10 21:41;pradeepkth;+1,03/May/10 20:23;daijy;Committed to both 0.7 branch and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parser fails to recognize valid field,PIG-1392,12462857,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nrai,ankur,ankur,23/Apr/10 11:12,17/Dec/10 22:44,14/Mar/19 03:07,16/Aug/10 17:39,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"Using this script below, parser fails to recognize a valid field in the relation and throws error

A = LOAD '/tmp' as (a:int, b:chararray, c:int);
B = GROUP A BY (a, b);
C = FOREACH B { bg = A.(b,c); GENERATE group, bg; } ;

The error thrown is

2010-04-23 10:16:20,610 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Invalid alias: c in {group: (a: int,b: chararray),A: {a: int,b: chararray,c: int}}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Aug/10 22:10;nrai;nested_parser.patch;https://issues.apache.org/jira/secure/attachment/12452061/nested_parser.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-04-23 18:31:33.759,,,no_permission,,,,,,,,,,,,164884,Reviewed,,,Mon Aug 16 17:41:10 UTC 2010,,,,,,,0|i0gvdb:,96512,"The issue is fixed but while fixing this, I encountered another problem of can't open iterator for C. Created jira# PIG-1545. There was also issue in the secondary optimizer, where it calls system.setProperties to set the pig.exec.nosecondarykey . I changed to use pigContext properties.
",,,,,,,,,"23/Apr/10 11:14;ankur;This script works

A = LOAD '/tmp' as (a:int, b:chararray, c:int);
B = GROUP A BY (a, b);
C = FOREACH B {  GENERATE group, A.(b,c); } ;",23/Apr/10 18:31;pradeepkth;Unlinking this from 0.7 release and moving to 0.8 since there is a workaround.,"16/Aug/10 17:26;nrai;I have run the test manually and all the tests have been successful.
","16/Aug/10 17:39;rding;The parser bug is fixed, but encounters another problem which is tracked by PIG-1545. The work around is to disable the secondary key optimization.

The patch is committed to the trunk.",16/Aug/10 17:41;rding;Thanks Niraj for fixing this issue.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig unit tests leave behind files in temp directory because MiniCluster files don't get deleted,PIG-1391,12462831,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,23/Apr/10 00:08,14/May/10 06:47,14/Mar/19 03:07,14/May/10 06:29,0.7.0,0.8.0,,,,,,0.6.0,0.7.0,0.8.0,,,,,0,,,,,,,,,,,,,"Pig unit test runs leave behind files in temp dir (/tmp) and there are too many files in the directory over time.
Most of the files are left behind by MiniCluster . It closes/shutsdown MiniDFSCluster, MiniMRCluster and the FileSystem that it has created when the constructor is called, only in finalize(). And java does not guarantee that finalize() will be called. 

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Apr/10 13:49;thejas;PIG-1391.06.2.patch;https://issues.apache.org/jira/secure/attachment/12443197/PIG-1391.06.2.patch,27/Apr/10 19:47;thejas;PIG-1391.06.patch;https://issues.apache.org/jira/secure/attachment/12442990/PIG-1391.06.patch,30/Apr/10 16:24;thejas;PIG-1391.07.patch;https://issues.apache.org/jira/secure/attachment/12443297/PIG-1391.07.patch,04/May/10 18:07;thejas;PIG-1391.trunk.patch;https://issues.apache.org/jira/secure/attachment/12443607/PIG-1391.trunk.patch,23/Apr/10 00:18;thejas;minicluster.patch;https://issues.apache.org/jira/secure/attachment/12442636/minicluster.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2010-04-25 17:27:28.014,,,no_permission,,,,,,,,,,,,164883,Reviewed,,,Wed May 05 23:16:14 UTC 2010,,,,,,,0|i0gvcv:,96510,,,,,,,,,,"23/Apr/10 00:18;thejas;This is a path for review before I go ahead and change around 70+ test files. 

It adds a shutdown() function in MiniCluster, which can be called from test cases to free the resources and delete the temp files.

In the patch, I have also made corresponding changes in one of the tests - TestPigContext to demonstrate the kind of changes that will be needed in other tests that use MiniCluster . The required changes are -
1. Add function annotated by @AfterClass which will call Minicluster.shutdown(). This will be called after all the unit test cases in the class have been run.
2. Annotate the class with @RunWith(JUnit4.class) so that junit4 is used . Without that it will run in junit3.x mode, which ignores @AfterClass annotation. 
","25/Apr/10 17:27;chaitk;bq. 2. Annotate the class with @RunWith(JUnit4.class) so that junit4 is used . Without that it will run in junit3.x mode, which ignores @AfterClass annotation.

Since ivy cache currently gets junit-4.5.jar, the tests run with junit4 mode, no?","25/Apr/10 21:28;thejas;bq. Since ivy cache currently gets junit-4.5.jar, the tests run with junit4 mode, no?
No. If a test class is a subclass of junit.framework.TestCase, it runs in the junit 3 compatible mode, and does not recognize the annotations such as @AfterClass . So it is necessary to annotate the class with @RunWith(JUnit4.class).
","26/Apr/10 04:00;chaitk;@Thejas : Ok, I get it now. It makes sense in this case to use the annotation.
But there was one more thing that I observed in most of the test classes that extend junit.framework.TestCase. These classes are also using either of @BeforeClass and @Test annotations. This would mean that they are actually junit4.x compatible and need not extend junit.framework.TestCase. I'm not sure if this should be discussed as another JIRA or can it be sorted out while working on this issue itself?
","26/Apr/10 04:02;chaitk;@Thejas : Ok, I get it now. It makes sense in this case to use the annotation.
But there was one more thing that I observed in most of the test classes that extend junit.framework.TestCase. These classes are also using either of @BeforeClass and @Test annotations. This would mean that they are actually junit4.x compatible and need not extend junit.framework.TestCase. I'm not sure if this should be discussed as another JIRA or can it be sorted out while working on this issue itself?
","26/Apr/10 15:59;thejas;bq. @Thejas : Ok, I get it now. It makes sense in this case to use the annotation.
bq. But there was one more thing that I observed in most of the test classes that extend junit.framework.TestCase. These classes are also using either of @BeforeClass and @Test annotations. This would mean that they are actually junit4.x compatible and need not extend junit.framework.TestCase. I'm not sure if this should be discussed as another JIRA or can it be sorted out while working on this issue itself?

Yes, the classes that use junit4 annotations need not extend  junit.framework.TestCase. But if they don't extend  junit.framework.TestCase, more changes are required such as removing the @Override for all the functions. Adding @RunWith(JUnit4.class)  to those classes seemed to be easier. 
","27/Apr/10 19:47;thejas;Patch for 0.6 branch. It reduces the number of temp files being left behind from around 1767 to 135 .  It changes only contents of test/ dir. 

As the patch does not apply to trunk, I have manually run the unit tests and test-patch . All unit tests succeeded, pasting result of test-patch - 
     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 208 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
","29/Apr/10 13:49;thejas;New patch for 0.6 branch. Made a change to build.xml - A new dir will be created within then temp dir and this dir will be used as the temp dir for tests. After the tests are done, this dir will be deleted, so that no temp files are left behind.
","04/May/10 23:31;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12443607/PIG-1391.trunk.patch
  against trunk revision 940601.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 251 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/314/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/314/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/314/console

This message is automatically generated.","05/May/10 00:04;thejas;The test case that is shown to fail in above report - TestSampleOptimizer , passes when run locally.
","05/May/10 23:16;daijy;Recommend to change two things:
1. remove ${junit.tmp.dir} after unit test targets in build.xml
2. remove fixes for zebra test case in this patch, it seems to be an irrelevant change.

Other part are good. Please commit after the above two changes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hadoop18 jar should be removed from Pig trunk,PIG-1388,12462789,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,olgan,alangates,alangates,22/Apr/10 17:10,17/Dec/10 22:44,14/Mar/19 03:07,06/May/10 22:55,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,Pig has used Hadoop 0.20 since 0.5.  hadoop18.jar is no longer needed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,164880,,,,2010-04-22 17:10:39.0,,,,,,,0|i0gvbj:,96504,I removed hadoop18.jar from the trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove empty svn directories from source tree,PIG-1383,12462578,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,20/Apr/10 20:48,17/Dec/10 22:44,14/Mar/19 03:07,21/Apr/10 17:22,0.7.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,Directories such as src/org/apache/pig/backend/local/ and its sub directories are empty and should be removed from the svn repository.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-04-20 22:26:16.438,,,no_permission,,,,,,,,,,,,164876,,,,Wed Apr 21 17:22:46 UTC 2010,,,,,,,0|i0gv9j:,96495,,,,,,,,,,"20/Apr/10 22:26;daijy;+1, go ahead and remove them.","21/Apr/10 17:22;rding;
The following empty directories are removed from svn:
{code}
src/org/apache/pig/backend/local
src/org/apache/pig/backend/hadoop/executionengine/mapreduceExec 
src/org/apache/pig/impl/eval
src/org/apache/pig/impl/mapreduceExec
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Command line option -c doesn't work,PIG-1382,12462577,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pradeepkth,rding,rding,20/Apr/10 20:43,17/Dec/10 22:44,14/Mar/19 03:07,04/May/10 21:20,0.6.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"Currently this option is not used, but it's documented:

""-c, -cluster clustername, kryptonite is default""

We should either remove it from documentation or find someway to use it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-05-04 21:20:27.987,,,no_permission,,,,,,,,,,,,164875,Incompatible change,,,Tue May 04 21:20:27 UTC 2010,,,,,,,0|i0gv93:,96493,"-c (-cluster) was earlier documented as the option to provide cluster information - this was not being used in the Pig code though - with PIG-1211, ""-c"" is being reused as the option to check syntax of the pig script ",,,,,,,,,04/May/10 21:20;pradeepkth;Fixed through https://issues.apache.org/jira/browse/PIG-1211?focusedCommentId=12864002&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12864002,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
har url of the form har:///<path> not usable in Pig scripts (har://hdfs-namenode:port/<path> works),PIG-1378,12462089,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pradeepkth,viraj,viraj,14/Apr/10 22:20,18/Jun/15 23:58,14/Mar/19 03:07,05/May/10 17:27,0.7.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"I am trying to use har (Hadoop Archives) in my Pig script.

I can use them through the HDFS shell
{noformat}
$hadoop fs -ls 'har:///user/viraj/project/subproject/files/size/data'
Found 1 items
-rw-------   5 viraj users    1537234 2010-04-14 09:49 user/viraj/project/subproject/files/size/data/part-00001
{noformat}

Using similar URL's in grunt yields
{noformat}
grunt> a = load 'har:///user/viraj/project/subproject/files/size/data'; 
grunt> dump a;
{noformat}


{noformat}
2010-04-14 22:08:48,814 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2998: Unhandled internal error. org.apache.pig.impl.logicalLayer.FrontendException: ERROR 0: Incompatible file URI scheme: har : hdfs
2010-04-14 22:08:48,814 [main] WARN  org.apache.pig.tools.grunt.Grunt - There is no log file to write to.
2010-04-14 22:08:48,814 [main] ERROR org.apache.pig.tools.grunt.Grunt - java.lang.Error: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 0: Incompatible file URI scheme: har : hdfs
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.LoadClause(QueryParser.java:1483)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:1245)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:911)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:700)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:63)
        at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1164)
        at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1114)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:425)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:737)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:324)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:162)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:138)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:75)
        at org.apache.pig.Main.main(Main.java:357)
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 0: Incompatible file URI scheme: har : hdfs
        at org.apache.pig.LoadFunc.getAbsolutePath(LoadFunc.java:249)
        at org.apache.pig.LoadFunc.relativeToAbsolutePath(LoadFunc.java:62)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.LoadClause(QueryParser.java:1472)
        ... 13 more
{noformat}

According to Jira http://issues.apache.org/jira/browse/PIG-1234 I try the following as stated in the original description

{noformat}
grunt> a = load 'har://namenode-location/user/viraj/project/subproject/files/size/data'; 
grunt> dump a;
{noformat}

{noformat}
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 2118: Unable to create input splits for: har://namenode-location/user/viraj/project/subproject/files/size/data'; 
        ... 8 more
Caused by: java.io.IOException: No FileSystem for scheme: namenode-location
        at .apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1375)
        at .apache.hadoop.fs.FileSystem.access(200(FileSystem.java:66)
        at .apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1390)
        at .apache.hadoop.fs.FileSystem.get(FileSystem.java:196)
        at .apache.hadoop.fs.HarFileSystem.initialize(HarFileSystem.java:104)
        at .apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1378)
        at .apache.hadoop.fs.FileSystem.get(FileSystem.java:193)
        at .apache.hadoop.fs.Path.getFileSystem(Path.java:175)
        at .apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:208)
        at .apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigTextInputFormat.listStatus(PigTextInputFormat.java:36)
        at .apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:246)
        at .apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:245)
{noformat}

Viraj",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-1522,28/Apr/10 22:07;pradeepkth;PIG-1378-2.patch;https://issues.apache.org/jira/secure/attachment/12443128/PIG-1378-2.patch,29/Apr/10 16:03;pradeepkth;PIG-1378-3.patch;https://issues.apache.org/jira/secure/attachment/12443202/PIG-1378-3.patch,29/Apr/10 20:51;pradeepkth;PIG-1378-4.patch;https://issues.apache.org/jira/secure/attachment/12443232/PIG-1378-4.patch,27/Apr/10 22:13;pradeepkth;PIG-1378.patch;https://issues.apache.org/jira/secure/attachment/12443013/PIG-1378.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2010-04-19 22:35:54.576,,,no_permission,,,,,,,,,,,,164871,Reviewed,,,Wed May 05 18:29:11 UTC 2010,,,,,,,0|i0gv7j:,96486,"The fix for this issue described in this jira depends on a issue with Hadoop code which was fixed on the hadoop trunk ( https://issues.apache.org/jira/browse/MAPREDUCE-1522 ). Until that goes into a hadoop release which is used by pig, this will remain an issue ",,,,,,,,,"19/Apr/10 22:35;ashutoshc;{noformat}
grunt> a = load 'har://namenode-location/user/viraj/project/subproject/files/size/data'; 
grunt> dump a;
{noformat}

 This is incorrect. You need to do the following:
{noformat}
grunt> a = load 'har://hdfs-namenode.foo.com:8020/user/viraj/project/subproject/files/size/data'; 
grunt> dump a;
{noformat}

Note that scheme is hdfs. Then a -(dash), followed by namenode url, followed by semi-colon, followed by port number(8020) and then location of your har archive. 
",21/Apr/10 15:10;viraj;har:// currently works in Pig 0.7 when the hdfs location is specified.,"21/Apr/10 17:55;pradeepkth;Adding to previous comment the har url has to be of the form (note the hdfs- prefix in the authority part):
har://hdfs-<namenodehost>:<namenodeport>/<datalocation>",27/Apr/10 22:13;pradeepkth;Attached patch addresses the issue in the description by changing LoadFunc.relativeToAbsolutePath() implementation to only convert input locations if the location does not have a scheme or the path in the location is not absolute.,"28/Apr/10 03:22;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12443013/PIG-1378.patch
  against trunk revision 937570.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 42 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/306/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/306/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/306/console

This message is automatically generated.",28/Apr/10 22:07;pradeepkth;Attached new patch addressing unit test failures - mostly due to the fact that the new patch no longer converts locations which are already absolute like '/foo/bar',"29/Apr/10 03:17;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12443128/PIG-1378-2.patch
  against trunk revision 938733.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 61 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/307/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/307/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/307/console

This message is automatically generated.",29/Apr/10 16:03;pradeepkth;Looks like the golden file change from the last patch was not correct - updated patch with just that change attached - all unit tests ran successfully locally with this new patch - patch is ready for review,29/Apr/10 20:51;pradeepkth;Realized that a stray change in TestMRCompiler got into my previous patch - attaching new patch with just that change removed.,29/Apr/10 20:54;rding;+1,"29/Apr/10 21:19;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12443202/PIG-1378-3.patch
  against trunk revision 938733.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 64 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/308/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/308/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/308/console

This message is automatically generated.","30/Apr/10 13:44;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12443232/PIG-1378-4.patch
  against trunk revision 938733.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 61 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/309/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/309/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/309/console

This message is automatically generated.",30/Apr/10 16:07;pradeepkth;The hudson test failures seem to be due to some temporary env. issue - I ran all unit tests locally and the run was successful - patch committed to trunk.,"04/May/10 02:19;viraj;Pradeep, 
 After rerunning with patch the following revision

Apache Pig version 0.8.0-dev (r940560) 
compiled May 03 2010, 12:22:35

{code}
grunt> a = load 'har:///user/viraj/project/dev/subproject/5m/data/201003042355/0/0_1/part-00000' using PigStorage('\u0001');
grunt> alimit = limit a 10;
grunt> dump alimit;
{code}

{noformat}
2010-05-04 02:17:22,196 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2118: Unable to create input splits for: har:///user/viraj/project/dev/subproject/5m/data/201003042355/0/0_1/part-00000
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:269)
        at org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:907)
        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:801)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:752)
        at org.apache.hadoop.mapred.jobcontrol.Job.submit(Job.java:378)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs(JobControl.java:247)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.run(JobControl.java:279)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: No FileSystem for scheme: myhdfs
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1375)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1390)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:196)
        at org.apache.hadoop.fs.HarFileSystem.initialize(HarFileSystem.java:104)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1378)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:193)
        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)
        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:208)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigTextInputFormat.listStatus(PigTextInputFormat.java:36)
        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:246)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:258)
        ... 7 more
{noformat}

Is this a problem with Hadoop/Pig?","04/May/10 21:09;pradeepkth;Spoke with a developer on the hadoop team to confirm that this is an issue with Hadoop code fixed on the hadoop trunk ( https://issues.apache.org/jira/browse/MAPREDUCE-1522). Until that goes into a hadoop release which is used by pig, this will remain an issue - not sure if we should keep this jira open until that point - am fine if we should.",05/May/10 17:27;pradeepkth;Am closing this bug since the pig changes are in and hadoop changes are in trunk - this should work once we use the appropriate hadoop release.,05/May/10 18:29;viraj;Linking it to MAPREDUCE issue for quick reference,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PushDownForeachFlatten shall not push ForEach below Join if the flattened fields is used in the next statement,PIG-1374,12461914,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,viraj,viraj,13/Apr/10 01:16,14/May/10 06:47,14/Mar/19 03:07,16/Apr/10 17:20,0.6.0,0.7.0,,,,,,0.7.0,,,,impl,,,0,,,,,,,,,,,,,"A reproducible sample:
{code}
a = load '2.txt' as (b{t(a0:chararray,a1:int)});
b = foreach a generate flatten($0);
c = order b by $1 desc;
dump c;
{code}
2.txt
{code}
{(a,1),(b,2)}
{code}
Error message:
java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.pig.data.DataBag
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.processInputBag(POProject.java:479)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:197)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNext(POLocalRearrange.java:332)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:233)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:228)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)

The problem is we push foreach below order by, but the foreach generated field is used by order by.

Original report:
Subject: Order by fails with java.lang.String cannot be cast to org.apache.pig.data.DataBag
Script loads data from BinStorage(), then flattens columns and then sorts on the second column with order descending. The order by fails with the ClassCastException

{code}
register loader.jar;
a = load 'c2' using BinStorage();
b = foreach a generate org.apache.pig.CCMLoader(*);
describe b;
c = foreach b generate flatten($0);
describe c;
d = order c by $1 desc;
dump d;
{code}

The sampling job fails with the following error:
===============================================================================================================
java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.pig.data.DataBag
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.processInputBag(POProject.java:407)
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:188)
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNext(POLocalRearrange.java:329)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:232)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:227)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:52)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
        at org.apache.hadoop.mapred.Child.main(Child.java:159)
===============================================================================================================

The schema for b, c and d are as follows:

b: {bag_of_tuples: {tuple: (uuid: chararray,velocity: double)}}

c: {bag_of_tuples::uuid: chararray,bag_of_tuples::velocity: double}

d: {bag_of_tuples::uuid: chararray,bag_of_tuples::velocity: double}

If we modify this script to order on the first column it seems to work

{code}
register loader.jar;
a = load 'c2' using BinStorage();
b = foreach a generate org.apache.pig.CCMLoader(*);
describe b;
c = foreach b generate flatten($0);
describe c;
d = order c by $0 desc;
dump d;
{code}

(gc639c60-4267-11df-9879-0800200c9a66,2.4227339503478493)
(ec639c60-4267-11df-9879-0800200c9a66,1.140175425099138)


There is a workaround to do a projection before ORDER

{code}
register loader.jar;
a = load 'c2' using BinStorage();
b = foreach a generate org.apache.pig.CCMLoader(*);
describe b;
c = foreach b generate flatten($0);
describe c;
newc = foreach c generate $0 as uuid, $1 as velocity;
newd = order newc by velocity desc;
dump newd;
{code}

(gc639c60-4267-11df-9879-0800200c9a66,2.4227339503478493)
(ec639c60-4267-11df-9879-0800200c9a66,1.140175425099138)


The schema for the Loader is as follows:

{code}
  public Schema outputSchema(Schema input) {
                 try{          
                        List<Schema.FieldSchema> list = new ArrayList<Schema.FieldSchema>();
                        list.add(new Schema.FieldSchema(""uuid"", DataType.CHARARRAY));
                        list.add(new Schema.FieldSchema(""velocity"", DataType.DOUBLE));
                        Schema tupleSchema = new Schema(list);
                        Schema.FieldSchema tupleFs = new Schema.FieldSchema(""tuple"", tupleSchema, DataType.TUPLE);
                        Schema bagSchema = new Schema(tupleFs);
                        bagSchema.setTwoLevelAccessRequired(true);
                        Schema.FieldSchema bagFs = new Schema.FieldSchema(""bag_of_tuples"",bagSchema, DataType.BAG);
                        return new Schema(bagFs);
                }catch (Exception e){
                        return null;
                }
    }
{code}",,,,,,,,,,,,,,,,,,,PIG-1172,,,,,,,,,,,,,14/Apr/10 01:25;daijy;ASF.LICENSE.NOT.GRANTED--PIG-1374-1.patch;https://issues.apache.org/jira/secure/attachment/12441671/ASF.LICENSE.NOT.GRANTED--PIG-1374-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-04-16 17:08:40.303,,,no_permission,,,,,,,,,,,,164867,Reviewed,,,Fri Apr 16 17:20:32 UTC 2010,,,,,,,0|i0gv67:,96480,,,,,,,,,,16/Apr/10 17:08;alangates;+1,16/Apr/10 17:20;daijy;Patch committed to both 0.7 branch and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
We need to add jdiff output to docs on the website,PIG-1373,12461913,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,daijy,alangates,alangates,13/Apr/10 01:07,17/Dec/10 22:44,14/Mar/19 03:07,01/Sep/10 01:35,,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"Our build process constructs a jdiff between APIs for different versions.  But we don't post the results of that to the website when we deploy the docs.  We should, in order to help users understand changes across versions of pig.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/May/10 01:34;daijy;PIG-1373-1.patch;https://issues.apache.org/jira/secure/attachment/12444555/PIG-1373-1.patch,17/May/10 19:18;daijy;PIG-1373-2.patch;https://issues.apache.org/jira/secure/attachment/12444728/PIG-1373-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-05-15 01:34:51.336,,,no_permission,,,,,,,,,,,,164866,,,,Mon Jul 12 23:05:08 UTC 2010,,,,,,,0|i0gv5r:,96478,,,,,,,,,,"15/May/10 01:34;daijy;PIG-1373-1.patch is to do jdiff against pig 0.7.0. Still need to change site once 0.8.0 release to make ""API changes"" available. ",17/May/10 19:18;daijy;PIG-1373-2.patch include site change as well. I borrow some forrest and cocoon settings from hadoop.,"19/May/10 23:30;alangates;+1, looks good.","27/May/10 12:18;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12444728/PIG-1373-2.patch
  against trunk revision 948526.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h1.grid.sp2.yahoo.net/4/console

This message is automatically generated.","12/Jul/10 23:05;daijy;All the changes are made, need to verify ""API changes"" link when 0.8 release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Restore PigInputFormat.sJob for backward compatibility,PIG-1372,12461904,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,12/Apr/10 22:19,14/May/10 06:47,14/Mar/19 03:07,15/Apr/10 17:23,0.7.0,,,,,,,0.7.0,,,,,,,1,,,,,,,,,,,,,The preferred method to get the job's Configuration object would be to use UDFContext.getJobConf(). This jira is to restore PigInputFormat.sJob  (but we will be marking it deprecated and indicating to use UDFContext.getJobConf() instead) to be backward compatible - we can remove it from pig in a future release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Apr/10 23:13;pradeepkth;ASF.LICENSE.NOT.GRANTED--PIG-1372-2.patch;https://issues.apache.org/jira/secure/attachment/12441779/ASF.LICENSE.NOT.GRANTED--PIG-1372-2.patch,13/Apr/10 00:36;pkamath;ASF.LICENSE.NOT.GRANTED--PIG-1372.patch;https://issues.apache.org/jira/secure/attachment/12441568/ASF.LICENSE.NOT.GRANTED--PIG-1372.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-04-14 22:57:31.508,,,no_permission,,,,,,,,,,,,164865,Reviewed,,,Thu Apr 15 17:23:23 UTC 2010,,,,,,,0|i0gv53:,96475,,,,,,,,,,13/Apr/10 00:36;pkamath;Attached patch restores PigInputFormat.sJob - however it is deprecated (and so also PigMapReduce.sJobConf for user code) and the javadoc comment indicates to use UDFContext.getUDFContext().getJobConf() instead. No tests are included since this simply restores a static variable for backward compatibility and is not used in pig code.,14/Apr/10 22:57;olgan;+1,"14/Apr/10 23:13;pradeepkth;Regenerated patch against latest trunk (same changes).

Here are the results of running ""test-patch"" ant target:
[exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec] 

",15/Apr/10 17:23;pradeepkth;All tests pass on my local machine - patch committed to 0.7 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
POProject does not handle null tuples and non existent fields in some cases,PIG-1369,12461574,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,08/Apr/10 19:26,14/May/10 06:47,14/Mar/19 03:07,13/Apr/10 18:36,0.7.0,,,,,,,0.7.0,,,,,,,0,,,,,,,,,,,,,"If a field (which is of type Tuple) in the data in null, POProject throws a NullPointerException. Also while projecting fields form a bag if a certain tuple in the bag does not contain a field being projected, an IndexOutofBoundsException is thrown. Since in a similar situation (accessing a non exisiting field in input tuple), POProject catches the IndexOutOfBoundsException and returns null, it should do the same for the above two cases and other cases where similar situations occur.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Apr/10 21:35;pkamath;PIG-1369.patch;https://issues.apache.org/jira/secure/attachment/12441210/PIG-1369.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-04-09 03:21:35.237,,,no_permission,,,,,,,,,,,,164862,Reviewed,,,Tue Apr 13 18:36:06 UTC 2010,,,,,,,0|i0gv3r:,96469,,,,,,,,,,08/Apr/10 21:35;pkamath;Attached patch addresses the issues mentioned in the description by catching NullPointerException and IndexOutofBoundsException at appropriate places.,"09/Apr/10 03:21;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12441210/PIG-1369.patch
  against trunk revision 932144.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/290/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/290/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/290/console

This message is automatically generated.",12/Apr/10 18:00;pkamath;The unit tests all run successfully on my local machine - the hudson QA failure was due to a temporal port conflict issue - will resubmit - meantime the patch is ready for review.,12/Apr/10 18:14;daijy;+1,13/Apr/10 18:36;pkamath;Patch committed to trunk and branch-0.7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigStorage's pushProjection implementation results in NPE under certain data conditions,PIG-1366,12461488,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,08/Apr/10 03:43,14/May/10 06:47,14/Mar/19 03:07,09/Apr/10 16:16,0.6.0,0.7.0,,,,,,0.7.0,,,,,,,0,,,,,,,,,,,,,"Under the following conditions, a NullPointerException is caused when PigStorage is used:
If in the script, only the 2nd and 3rd column of the data (say) are used, the PruneColumns optimization passes this information to PigStorage through the pushProjection() method. If the data contains a row with only one column (malformed data due to missing cols in certain rows), PigStorage returns a Tuple backed by a null ArrayList. Subsequent projection operations on this tuple result in the NPE.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Apr/10 03:50;pkamath;PIG-1366.patch;https://issues.apache.org/jira/secure/attachment/12441109/PIG-1366.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-04-08 09:24:30.649,,,no_permission,,,,,,,,,,,,164859,Reviewed,,,Fri Apr 09 16:16:36 UTC 2010,,,,,,,0|i0gv2f:,96463,,,,,,,,,,"08/Apr/10 03:50;pkamath;Currently in PigStorage the ArrayList backing the Tuple returned in getNext() is created in readField(). Under the data conditions explained in the description, readField() never gets called and the ArrayList (mProtoTuple) remains null causing the eventual NPE. The patch fixes the issue by initializing mProtoTuple to a new ArrayList at the beginning of getNext().","08/Apr/10 09:24;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12441109/PIG-1366.patch
  against trunk revision 931764.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/277/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/277/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/277/console

This message is automatically generated.",08/Apr/10 21:28;daijy;+1,09/Apr/10 16:16;pkamath;Patch committed to trunk and branch-0.7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WrappedIOException is missing from Pig.jar,PIG-1365,12461474,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,pradeepkth,olgan,olgan,07/Apr/10 23:54,14/May/10 06:47,14/Mar/19 03:07,08/Apr/10 17:29,,,,,,,,0.7.0,,,,,,,0,,,,,,,,,,,,,We need to put it back since UDFs rely on it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Apr/10 04:47;pkamath;PIG-1365.patch;https://issues.apache.org/jira/secure/attachment/12441113/PIG-1365.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-04-08 04:47:41.482,,,no_permission,,,,,,,,,,,,164858,Reviewed,,,Thu Apr 08 17:29:51 UTC 2010,,,,,,,0|i0gv1z:,96461,,,,,,,,,,"08/Apr/10 04:47;pkamath;Attached patch restores WrappedIOException - this is not used in Pig Code and only provided for use by UDFs to maintain backward compatibility. I have marked the class as deprecated so that it can be removed from pig code base in a later release.

No unit tests have been added since this is just restoring an old class which is no longer used in the pig code.","08/Apr/10 13:24;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12441113/PIG-1365.patch
  against trunk revision 931764.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 524 release audit warnings (more than the trunk's current 523 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/287/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/287/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/287/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/287/console

This message is automatically generated.",08/Apr/10 16:10;pkamath;No unit tests have been added since this is just restoring an old class for backward compatibility for users and is no longer used in the pig code. The release audit warning is about a html file and can be ignored.,"08/Apr/10 16:21;olgan;+1.  Please, commit to both trunk and 0.7.0 branch",08/Apr/10 17:29;pkamath;Patch committed to trunk and branch-0.7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Public javadoc on apache site still on 0.2, needs to be updated for each version release",PIG-1364,12461470,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,alangates,alangates,alangates,07/Apr/10 23:28,14/May/10 06:47,14/Mar/19 03:07,13/Apr/10 01:03,0.4.0,0.5.0,0.6.0,,,,,0.4.0,0.5.0,0.6.0,0.7.0,documentation,,,0,,,,,,,,,,,,,"See http://hadoop.apache.org/pig/javadoc/docs/api/.  This currently contains javadocs for 0.2.  It is also versionless.

It needs to be changed so that javadocs for recent versions are posted.  It also needs to change so that the version is in the api so that multiple versions of the API can be posted.

It's probably too late to do this for 0.6 and before, but it needs to happen for 0.7.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Apr/10 22:25;alangates;PIG-1364-0.4.patch;https://issues.apache.org/jira/secure/attachment/12441220/PIG-1364-0.4.patch,08/Apr/10 22:25;alangates;PIG-1364-0.5.patch;https://issues.apache.org/jira/secure/attachment/12441219/PIG-1364-0.5.patch,08/Apr/10 22:25;alangates;PIG-1364-0.6.patch;https://issues.apache.org/jira/secure/attachment/12441218/PIG-1364-0.6.patch,08/Apr/10 22:52;alangates;PIG-1364-0.7.patch;https://issues.apache.org/jira/secure/attachment/12441226/PIG-1364-0.7.patch,08/Apr/10 22:52;alangates;PIG-1364-trunk.patch;https://issues.apache.org/jira/secure/attachment/12441227/PIG-1364-trunk.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2010-04-09 13:53:26.174,,,no_permission,,,,,,,,,,,,164857,,,,Tue Apr 13 01:03:33 UTC 2010,,,,,,,0|i0gv1r:,96460,,,,,,,,,,"08/Apr/10 21:44;alangates;The javadoc is actually already loaded to the site.  The link just points to the old 0.2 docs.  Since documentation for 0.4 through 0.6 is on our site, I'll upload patches for each of those as well as a patches for 0.7 and for the trunk.","09/Apr/10 13:53;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12441227/PIG-1364-trunk.patch
  against trunk revision 932161.

    +1 @author.  The patch does not contain any @author tags.

    +0 tests included.  The patch appears to be a documentation patch that doesn't require tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/280/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/280/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/280/console

This message is automatically generated.",12/Apr/10 18:05;olgan;+1 for all the branches,"13/Apr/10 01:03;alangates;All of the patches checked in.  Confirmed that for 0.4, 0.5, and 0.6 on the website the link now points to the version specific docs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unnecessary loadFunc instantiations,PIG-1363,12461444,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,07/Apr/10 19:08,17/Dec/10 22:44,14/Mar/19 03:07,15/Apr/10 20:57,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"In MRCompiler loadfuncs are instantiated at multiple locations in different visit methods. This is inconsistent and confusing. LoadFunc should be instantiated at only one place, ideally in LogToPhyTanslation#visit(LOLoad). A getter should be added to POLoad to retrieve this instantiated loadFunc wherever it is needed in later stages of compilation. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Apr/10 00:12;ashutoshc;ASF.LICENSE.NOT.GRANTED--pig-1363.patch;https://issues.apache.org/jira/secure/attachment/12441566/ASF.LICENSE.NOT.GRANTED--pig-1363.patch,15/Apr/10 18:24;ashutoshc;ASF.LICENSE.NOT.GRANTED--pig-1363_1.patch;https://issues.apache.org/jira/secure/attachment/12441862/ASF.LICENSE.NOT.GRANTED--pig-1363_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-04-14 23:06:32.644,,,no_permission,,,,,,,,,,,,164856,Reviewed,,,Thu Apr 15 23:23:25 UTC 2010,,,,,,,0|i0gv13:,96457,,,,,,,,,,"13/Apr/10 00:12;ashutoshc;Ideal solution of this problem is to have {{{LoadFunc}}} implements {{{Serializable}}}. Then LoadFunc will be instantiated once first time its needed (in LoLoad) and then everywhere this one object is used. But this will be backward incompatible as all the load func implementation then have to be necessarily implement Serializable. So, for now we will live with this. 
This patch gets rid of the multiple load func instantiation in front end where it could be avoided without the need of making it Serializable. No test cases are needed since this is purely code cleanup and doesn't add/delete/modify any existing functionality, so current regression tests suffice. ","14/Apr/10 17:50;ashutoshc;Hudson is flaky (again). Result of test-patch:
{noformat}
     [exec] 
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec] 

{noformat} 
Patch is ready for review.",14/Apr/10 23:06;pradeepkth;+1,14/Apr/10 23:54;ashutoshc;Patch checked-in.,"15/Apr/10 18:23;ashutoshc;Issue patch is supposed to fix at one place, breaks it at another place. Need to Re-fix.","15/Apr/10 18:24;ashutoshc;Can't get away without pasing loader signature to backend for Merge Join. So, set it.",15/Apr/10 20:48;daijy;+1,15/Apr/10 20:57;daijy;Patch committed.,"15/Apr/10 23:23;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12441862/pig-1363_1.patch
  against trunk revision 934488.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/298/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/298/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/298/console

This message is automatically generated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide udf context signature in ensureAllKeysInSameSplit() method of loader,PIG-1362,12461435,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,ashutoshc,ashutoshc,ashutoshc,07/Apr/10 17:55,14/May/10 06:47,14/Mar/19 03:07,08/Apr/10 00:55,0.7.0,,,,,,,0.7.0,,,,,,,0,,,,,,,,,,,,,"As a part of PIG-1292 a check was introduced to make sure loader used in ""collected"" group-by implements CollectableLoader (new interface in that patch). In its method, loader may use udf context to store some info. We need to make sure that udf context signature is setup correctly in such cases. This is already the case in trunk, need to backport it to 0.7 branch. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Apr/10 17:56;ashutoshc;backport.patch;https://issues.apache.org/jira/secure/attachment/12441064/backport.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-04-07 18:12:59.503,,,no_permission,,,,,,,,,,,,164855,,,,Thu Apr 08 00:55:49 UTC 2010,,,,,,,0|i0gv0v:,96456,,,,,,,,,,07/Apr/10 17:56;ashutoshc;Simple one line fix. Test cases included.,07/Apr/10 18:12;pkamath;+1,"08/Apr/10 00:55;ashutoshc;Since hudson is flaky once again. Ran the full test - suite. All of it passed. Ran test-patch:

{noformat}
     [exec] 	+1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{noformat}

Patch checked-in for 0.7 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bin/pig script does not pick up correct jar libraries,PIG-1359,12461418,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,zjffdu,azaroth,azaroth,07/Apr/10 15:38,17/Dec/10 22:44,14/Mar/19 03:07,22/May/10 01:51,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"The bin/pig script tries to load pig jar libraries from the pig-*-core.jar using this bash fragment

{code}

# for releases, add core pig to CLASSPATH
for f in $PIG_HOME/pig-*core.jar; do
    CLASSPATH=${CLASSPATH}:$f;
done

# during development pig jar might be in build
for f in $PIG_HOME/build/pig-*-core.jar; do
    CLASSPATH=${CLASSPATH}:$f;
done

{code} 

The pig-\*-core.jar does not contain the dependencies for pig that are found in build/ivy/lib/Pig/\*.jar (jline).
The script does not even pick the pig.jar in PIG_HOME that is produced as a result of the ant build process.

This results in the following error after successfully building pig:

{code} 

Exception in thread ""main"" java.lang.NoClassDefFoundError: jline/ConsoleReaderInputStream
Caused by: java.lang.ClassNotFoundException: jline.ConsoleReaderInputStream

{code} 
","Linux Ubuntu 8.10, java-6-sun",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Apr/10 07:57;azaroth;ASF.LICENSE.NOT.GRANTED--pig-1359.patch;https://issues.apache.org/jira/secure/attachment/12441330/ASF.LICENSE.NOT.GRANTED--pig-1359.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-04-12 20:56:25.72,,,no_permission,,,,,,,,,,,,164852,,,,Sat May 22 01:50:42 UTC 2010,,,,,,,0|i0guzj:,96450,Added jars in $PIG_HOME/lib/ and $PIG_HOME/build/ivy/lib/Pig/ to Pig startup classpath.,,,,,,,,,"10/Apr/10 08:04;azaroth;Added a patch that picks up jars in ivy's build folder ""build/ivy/lib/Pig/"" and the main lib folder ""lib/""","12/Apr/10 20:56;daijy;Hi, Gianmarco,
Thanks for your concern. Actually we need one additional step to make bin/pig work. We shall copy $PIG_HOME/build/pig-0.8.0-dev.jar to $PIG_HOME/pig-0.8.0-core.jar. This will be handled in ant's ""package"" target when releasing. But if you check out from svn, we will do this additional step to work with bin/pig.","12/Apr/10 21:18;daijy;The comment change ""# Set the version for Hadoop, default to 17"" -> ""# Set the version for Hadoop, default to 20"" is totally valid, we will change it.","20/May/10 03:03;zjffdu;Hi Daniel,

Does it mean in development stage, I have to invoke ant package or copy copy $PIG_HOME/build/pig-0.8.0-dev.jar to $PIG_HOME/pig-0.8.0-core.jar manually ? 

Why not change 
{code}
# during development pig jar might be in build
for f in $PIG_HOME/build/pig-*-core.jar; do
    CLASSPATH=${CLASSPATH}:$f;
done
{code}
to 
{code}
# during development pig jar might be in build
for f in $PIG_HOME/build/pig-*-dev.jar; do
    CLASSPATH=${CLASSPATH}:$f;
done
{code}

The *-dev.jar has the dependency, and this is more convenient for development

","20/May/10 18:03;daijy;Yes, I rethink about it and seems we create unnecessary trouble for developing. Both Jeff's change and Gianmarco looks fine. I tend to adopt Jeff's change, since it requires less change. Thoughts?","21/May/10 18:12;daijy;Since there is no objection, Jeff, can you commit your change?","22/May/10 01:50;zjffdu;Daniel, I've committed the patch.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[zebra] TableLoader makes unnecessary calls to build a Job instance that create a new JobClient in the hadoop 0.20.9,PIG-1356,12461304,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,yanz,yanz,06/Apr/10 21:33,14/May/10 06:47,14/Mar/19 03:07,09/Apr/10 15:53,0.7.0,,,,,,,0.7.0,0.8.0,,,,,,0,,,,,,,,,,,,,"This extra JobClient is actually a bug in Hadoop 0.20.9, but Zebra could have avoided the problem by not creating the unnecessary instance of Job.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Apr/10 22:54;yanz;PIG-1356.patch;https://issues.apache.org/jira/secure/attachment/12441228/PIG-1356.patch,06/Apr/10 22:21;yanz;PIG-1356.patch;https://issues.apache.org/jira/secure/attachment/12440958/PIG-1356.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-04-06 22:29:35.95,,,no_permission,,,,,,,,,,,,164849,,,,Fri Apr 09 15:53:29 UTC 2010,,,,,,,0|i0guyf:,96445,,,,,,,,,,06/Apr/10 22:29;xuefuz;+1,"07/Apr/10 04:21;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12440958/PIG-1356.patch
  against trunk revision 930168.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/282/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/282/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/282/console

This message is automatically generated.",08/Apr/10 22:55;yanz;Resubmit the patch hat is based upon latest trunk.,08/Apr/10 22:58;yanz;Test was performed on a user's env. No new test case is needed here.,"09/Apr/10 08:25;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12441228/PIG-1356.patch
  against trunk revision 932161.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/291/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/291/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/291/console

This message is automatically generated.",09/Apr/10 15:53;yanz;Patch committed to the trunk and the 0.7 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
piggybank UPPER udf throws exception if argument is null,PIG-1352,12461048,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,02/Apr/10 18:20,14/May/10 06:47,14/Mar/19 03:07,12/Apr/10 20:44,0.7.0,,,,,,,0.7.0,,,,,,,0,,,,,,,,,,,,,"java.lang.NullPointerException
        at org.apache.pig.piggybank.evaluation.string.UPPER.exec(UPPER.java:62)
        at org.apache.pig.piggybank.evaluation.string.UPPER.exec(UPPER.java:42)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:212)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Apr/10 19:17;thejas;UPPER.patch;https://issues.apache.org/jira/secure/attachment/12440643/UPPER.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-04-06 00:19:26.762,,,no_permission,,,,,,,,,,,,164846,Reviewed,,,Mon Apr 12 20:44:22 UTC 2010,,,Patch Available,,,,0|i0guwv:,96438,,,,,,,,,,"06/Apr/10 00:19;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12440643/UPPER.patch
  against trunk revision 930168.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/279/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/279/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/279/console

This message is automatically generated.",06/Apr/10 22:34;rding;+1. But notice string functions in piggybank have different exception handling code. Opened PIG-1358 to track this issue.,"12/Apr/10 20:44;thejas;Patch committed to trunk, branch-0.7",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigStorage making unnecessary byte array copy when storing data,PIG-1348,12460965,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,ashutoshc,ashutoshc,01/Apr/10 19:10,14/May/10 06:47,14/Mar/19 03:07,16/Apr/10 17:33,0.7.0,,,,,,,0.7.0,,,,impl,,,0,,,,,,,,,,,,,"InternalCachedBag makes estimate of memory available to the VM by using Runtime.getRuntime().maxMemory(). It then uses 10%(by default, though configurable) of this memory and divides this memory into number of bags. It keeps track of the memory used by bags and then proactively spills if bags memory usage reach close to these limits. Given all this in theory when presented with data more then it can handle InternalCachedBag should not run out of memory. But in practice we find OOM happening. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Apr/10 22:56;rding;PIG-1348.patch;https://issues.apache.org/jira/secure/attachment/12440806/PIG-1348.patch,07/Apr/10 17:26;rding;PIG-1348_2.patch;https://issues.apache.org/jira/secure/attachment/12441060/PIG-1348_2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-04-03 00:13:31.376,,,no_permission,,,,,,,,,,,,164842,Reviewed,,,Fri Apr 16 17:33:22 UTC 2010,,,,,,,0|i0guvb:,96431,,,,,,,,,,"01/Apr/10 19:11;ashutoshc;To reproduce, cogroup page_views(from PigMix's dataset) with page_views on user and this exception should occur. Apart from making InternalCachedBag more robust, important thing to figure out here is to see where 90% of available memory is getting used. Also, a related fix went in for this recently: PIG-1307 Might be related to that issue. ",03/Apr/10 00:13;rding;The problem seems not with the InternalCachedBag.  The same script runs fine when replacing the store command with the dump command.  ,05/Apr/10 22:56;rding;This patch removes the extra copying of byte arrays in PigStorage. ,"06/Apr/10 06:39;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12440806/PIG-1348.patch
  against trunk revision 930168.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/271/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/271/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/271/console

This message is automatically generated.","06/Apr/10 23:02;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12440806/PIG-1348.patch
  against trunk revision 930168.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/273/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/273/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/273/console

This message is automatically generated.","07/Apr/10 01:27;ashutoshc;Since this is mostly performance related, there are few more things which we can get in depending on complexity - speedup tradeoff:
1) PigLineRecordWriter#write() is synchronized. Is that needed? I don't see a scenario where multiple threads are writing using same object and thus potentially stomping on each other. Am I missing something here?
2) Within write() I think it can be safely assumed that value is of type Tuple, because argument in putNext() is of type Tuple. Then we can get rid of instanceof.
3) In StorageUtil.putField(), is it possible to get rid of DataType.findType(), possibly by getting hold of schema and getting type information from there. If not, then may be we cache the type info first time, instead of finding it on every call. At the very least, we shall get rid of casts for simple types as thats unnecessary. DataType.isComplex() can be used to determine that. ","07/Apr/10 17:25;rding;Thanks Ashutosh. I changed signature of write() to take values of type Tuple instead of type Object. 

On 1) and 3), Hadoop LineRecordWriter#write() is a synchronized method, and I think that JVM is optimized for 'instanceof'' construct and also for uncontended synchronization. I prefer that we have some performance numbers before adding optimizations.","07/Apr/10 17:33;alangates;bq. In StorageUtil.putField(), is it possible to get rid of DataType.findType(), possibly by getting hold of schema and getting type information from there. If not, then may be we cache the type info first time, instead of finding it on every call. At the very least, we shall get rid of casts for simple types as thats unnecessary. DataType.isComplex() can be used to determine that. 

We have to be careful here.  In the case where a schema is given, it's ok to use that to cast types.  In cases without schema we cannot assume that all records match the first, because Pig does not impose that as a requirement on the data.  So looking at the first record and caching results is not ok.","07/Apr/10 18:54;ashutoshc;1) As far as I can see TextOutputFormat has synchronized write() because it is meant to work even with mappers implementing MultithreadedMapRunner. But since thats not the case for Pig, we can get rid of it especially now that we are putting in our own PigTextOutputFormat instead of using TextOutputformat. 

3) Thats what I meant, if Schema is available, we should use that to find types, instead of reflecting on every call. I suggested the work around of caching for the case if we know user did provide Schema, but we dont have a handle on it. Clearly, if there is no schema, we need to find type every time. I can see that dealing with Complex types even when there is a schema is not straight forward. In any case, casts that are currently there for simple types are unnecessary.

For performance numbers, both of these will save CPU time, if we are convinced that we are always I/O bound we can leave these things as it is. ","07/Apr/10 19:18;dvryaboy;In the spirit of better java and micro-optimizations:

StorageUtil does things like this to convert to bytes:

{code}
out.write(((Integer)field).toString().getBytes());
{code}

Integer's toString() method creates a new string every time, even if the same integer (value-wise) is being converted to a String.  This is better:

{code}
out.wirte(String.valueOf(field).getBytes());
{code}

(This reuses the values, and also collapses the case statement a fair bit, cleaning up the code -- we can batch Integer, Double, etc, together and fall through to just one line of code.)

This discussion should probably go into a separate ticket.","07/Apr/10 20:56;dvryaboy;Umm I am totally wrong about the new object thing.

{code}
public static String valueOf(int i) {
           return Integer.toString(i, 10);
}
{code}

mea culpa.

Though it's still true that this approach can clean up / shorten the code, it's totally untrue that there's some magical object reuse happening if you do it.

-D

","08/Apr/10 22:18;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12441060/PIG-1348_2.patch
  against trunk revision 931986.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/289/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/289/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/289/console

This message is automatically generated.",16/Apr/10 17:33;rding;The patch is committed to both trunk and 0.7 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clear up output directory for a failed job,PIG-1347,12460888,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ashitosh,daijy,daijy,01/Apr/10 04:03,17/Dec/10 22:44,14/Mar/19 03:07,26/May/10 18:33,0.7.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"FileLocalizer.deleteOnFail suppose to track the output files need to be deleted in case the job fails. However, in the current code base, deleteOnFail is dangling. registerDeleteOnFail and triggerDeleteOnFail is called by nobody. We need to bring it back.",,,,,,,,,,,,,,,,,,,PIG-258,,,,,,,,,,,,,26/May/10 17:39;daijy;PIG-1347-1.patch;https://issues.apache.org/jira/secure/attachment/12445570/PIG-1347-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-05-26 18:06:58.477,,,no_permission,,,,,,,,,,,,164841,Reviewed,,,Wed May 26 18:33:38 UTC 2010,,,,,,,0|i0guuv:,96429,,,,,,,,,,"26/May/10 05:50;daijy;In current code, we use StoreFunc.cleanupOnFailure for this purpose. FileLocalizer.deleteOnFail should be removed. So this issue is fixed in trunk, we should remove redundant code.",26/May/10 17:31;daijy;Remove redundant code.,"26/May/10 18:06;ashutoshc;Patch is pretty straightforward and harmless as it only removes code and does not add any thing new. Only concern I have is FileLocalizer.registerDeleteOnFail() is a public method so its possible that some one using Pig's java api is using this method to do the cleanup himself previously.  So, this can be considered as backward incompatible change. But, Daniel explained to me that this method was meant for Pig's internal usage and clean up in any case was taken care by Pig before the recent store func changes, so user need not to worry about it. So, its extremely unlikely that someone is using it. 
So, +1 on committing.",26/May/10 18:33;daijy;Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
In unit tests Util.executeShellCommand relies on java commands being in the path and does not consider JAVA_HOME,PIG-1346,12460861,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,31/Mar/10 21:42,14/May/10 06:47,14/Mar/19 03:07,02/Apr/10 06:28,0.6.0,0.7.0,,,,,,0.6.0,0.7.0,,,,,,0,,,,,,,,,,,,,"Util.executeShellCommand is currently used in unit tests to execute java related binaries like ""java"", ""javac"", ""jar"" - this method should check if JAVA_HOME is set and use $JAVA_HOME/bin/java etc. If JAVA_HOME is not set, the method can try and execute the command as-is.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Apr/10 23:21;pkamath;PIG-1346-2.patch;https://issues.apache.org/jira/secure/attachment/12440552/PIG-1346-2.patch,31/Mar/10 21:47;pkamath;PIG-1346.patch;https://issues.apache.org/jira/secure/attachment/12440408/PIG-1346.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-04-01 04:48:13.685,,,no_permission,,,,,,,,,,,,164840,Reviewed,,,Fri Apr 02 06:28:05 UTC 2010,,,,,,,0|i0guuf:,96427,,,,,,,,,,"01/Apr/10 04:48;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12440408/PIG-1346.patch
  against trunk revision 929737.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 12 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/265/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/265/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/265/console

This message is automatically generated.","01/Apr/10 18:28;daijy;+1, patch looks good.","01/Apr/10 23:21;pkamath;The earlier patch was using System.getProperty(""java.home"") - apparently ant sometimes appends ""jre"" to $JAVA_HOME as the value of the java.home property - this causes failures since $JAVA_HOME/jre/bin/ does not contain javac. I have changed this code to use System.getEnv(""JAVA_HOME"") instead.",01/Apr/10 23:24;daijy;+1,"02/Apr/10 04:30;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12440552/PIG-1346-2.patch
  against trunk revision 930123.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 12 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/278/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/278/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/278/console

This message is automatically generated.","02/Apr/10 06:28;pkamath;Patch committed to trunk, branch-0.7 and branch-0.6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig_log file missing even though Main tells it is creating one and an M/R job fails ,PIG-1343,12460762,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nrai,viraj,viraj,31/Mar/10 00:26,17/Dec/10 22:44,14/Mar/19 03:07,30/Aug/10 17:44,0.6.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"There is a particular case where I was running with the latest trunk of Pig.

{code}
$java -cp pig.jar:/home/path/hadoop20cluster org.apache.pig.Main testcase.pig

[main] INFO  org.apache.pig.Main - Logging error messages to: /homes/viraj/pig_1263420012601.log

$ls -l pig_1263420012601.log
ls: pig_1263420012601.log: No such file or directory
{code}

The job failed and the log file did not contain anything, the only way to debug was to look into the Jobtracker logs.

Here are some reasons which would have caused this behavior:
1) The underlying filer/NFS had some issues. In that case do we not error on stdout?
2) There are some errors from the backend which are not being captured

Viraj
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Aug/10 15:28;nrai;1343.patch;https://issues.apache.org/jira/secure/attachment/12452940/1343.patch,01/Jun/10 17:12;ashitosh;PIG-1343-1.patch;https://issues.apache.org/jira/secure/attachment/12446033/PIG-1343-1.patch,30/Aug/10 17:43;rding;PIG-1343_6.patch;https://issues.apache.org/jira/secure/attachment/12453444/PIG-1343_6.patch,27/Aug/10 01:20;nrai;PIG_1343_5.patch;https://issues.apache.org/jira/secure/attachment/12453197/PIG_1343_5.patch,26/Aug/10 19:44;nrai;pig_1343_2.patch;https://issues.apache.org/jira/secure/attachment/12453159/pig_1343_2.patch,26/Aug/10 22:51;nrai;pig_1343_4.patch;https://issues.apache.org/jira/secure/attachment/12453182/pig_1343_4.patch,,,,,,6.0,,,,,,,,,,,,,,,,,,,2010-05-26 05:46:50.128,,,no_permission,,,,,,,,,,,,164838,Reviewed,,,Mon Aug 30 17:42:00 UTC 2010,,,,,,,0|i0gut3:,96421,,,,,,,,,,"26/May/10 05:46;daijy;This script will reproduce the issue:
{code}
a = load '1.txt' as (a0:int);
b = foreach a generate StringSize(a0);
store b into '111';
{code}

However, if we change store with dump, we get log file.","01/Jun/10 17:24;ashitosh;When we use store , the exception is not  thrown when a job is failed .
I have checked for the job's status in registerQuery method of PigServer.java and threw the exceptions accordingly.
looking for suggestions ","01/Jun/10 22:01;daijy;In this case hadoop does not give a meaningful message in MapReduceLauncher:282, backendException is null. But take a look why in ""dump"", this ""null backendException"" actually trigger a message into pig log, but in ""store"", Pig silently stop without a trace.","26/Jul/10 21:12;olgan;Ashitosh, are you planning to finish working on this JIRA or should we re-assign in to somebody else? Thanks!",27/Jul/10 14:42;ashitosh;Hi .. I'm really sorry for such a long delay .. I'll resume my work on this issue.. thanks ,"30/Jul/10 16:57;daijy;Hi, Ashitosh,
As you know, Pig 0.8 will code freeze by the end of August. Are you able to finish the patch by the mid of Aug? Thanks.","03/Aug/10 03:47;ashitosh;Hi sir, Could you please assign this task to someone else for now .. though i would try to submit the patch. thanks","24/Aug/10 15:28;nrai;This patch will generate an error, where a job has failed but MR does not return any exception.","24/Aug/10 19:27;rding;The log file is created when running in batch mode, but not in interactive mode.",26/Aug/10 19:44;nrai;Implemented the interactive mode logging as well.,"26/Aug/10 21:18;rding;
The new patch logs NPE instead of the intended message:

{code}
[main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2999: Unexpected internal error. null
{code}","27/Aug/10 18:42;rding;
I run above script in local mode, both batch mode and interactive mode now generate the expected result:

{code}
ERROR 2244: Job failed, hadoop does not return any error message
{code}",30/Aug/10 17:42;rding;Patch is committed to the trunk. Thanks Niraj.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[zebra] The zebra version number should be changed from 0.7 to 0.8,PIG-1340,12460755,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,yanz,yanz,yanz,30/Mar/10 23:30,17/Dec/10 22:44,14/Mar/19 03:07,31/Mar/10 22:38,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,31/Mar/10 18:50;yanz;PIG-1340.patch;https://issues.apache.org/jira/secure/attachment/12440377/PIG-1340.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-03-31 21:42:14.709,,,no_permission,,,,,,,,,,,,164836,,,,Wed Mar 31 22:38:40 UTC 2010,,,,,,,0|i0gurr:,96415,,,,,,,,,,31/Mar/10 21:42;chaow;+1,31/Mar/10 22:38;yanz;Committed to the trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDFFinder should find LoadFunc used by POCast,PIG-1335,12460483,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,28/Mar/10 22:51,14/May/10 06:47,14/Mar/19 03:07,01/Apr/10 22:50,0.7.0,,,,,,,0.7.0,,,,impl,,,0,,,,,,,,,,,,,"UDFFinder doesn't look into POCast so it will miss LoadFunc used by POCast for lineage. We could see ""class not found"" exception in some cases. Here is a sample script:

{code}
a = load '1.txt' using CustomLoader() as (a0, a1, a2);
b = group a by a0;
c = foreach b generate flatten(a);
d = order c by a0;
e = foreach d generate(a1+a2);  -- use lineage
dump e;
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Mar/10 22:52;daijy;PIG-1335-1.patch;https://issues.apache.org/jira/secure/attachment/12440020/PIG-1335-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-03-29 08:02:57.764,,,no_permission,,,,,,,,,,,,164832,Reviewed,,,Thu Apr 01 22:50:32 UTC 2010,,,,,,,0|i0gupr:,96406,,,,,,,,,,"29/Mar/10 08:02;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12440020/PIG-1335-1.patch
  against trunk revision 928384.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 530 release audit warnings (more than the trunk's current 529 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/254/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/254/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/254/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/254/console

This message is automatically generated.",30/Mar/10 19:15;rding;+1,01/Apr/10 22:50;daijy;Patch committed to both trunk and 0.7 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig version incorrect post-0.7 split,PIG-1329,12460255,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,dvryaboy,dvryaboy,dvryaboy,25/Mar/10 16:12,25/Mar/10 17:05,14/Mar/19 03:07,25/Mar/10 17:05,0.8.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,There's a typo in build.xml that makes the current pig version 0.88888888.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Mar/10 16:13;dvryaboy;PIG-1329.patch;https://issues.apache.org/jira/secure/attachment/12439793/PIG-1329.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-03-25 16:59:33.758,,,no_permission,,,,,,,,,,,,41714,,,,Thu Mar 25 17:05:16 UTC 2010,,,,,,,0|i0gun3:,96394,,,,,,,,,,25/Mar/10 16:15;dvryaboy;This is trivial and does not require tests. ,"25/Mar/10 16:59;ashutoshc;+1, I also hit this issue yesterday. Seems to be typo while branching for 0.7.",25/Mar/10 17:05;dvryaboy;Committed. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect column pruning after multiple JOIN operations,PIG-1327,12460232,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,ankur,ankur,25/Mar/10 13:40,14/May/10 06:47,14/Mar/19 03:07,06/Apr/10 20:46,0.6.0,,,,,,,0.7.0,,,,,,,0,,,,,,,,,,,,,"In a script with multiple JOIN and GROUP operations, the column pruner incorrectly removes some of the fields that it shouldn't. Here is a script that demonstrates the issue

 = LOAD 'data1' USING PigStorage() AS (a:chararray, b:chararray, c:long);
B = LOAD 'data2' USING PigStorage() AS (x:chararray, y:chararray, z:long);
C = LOAD 'data3' using PigStorage() AS (d:chararray, e:chararray, f:chararray);

join1 = JOIN B by x, A by a;
filtered1 = FILTER join1  BY y == b;
InterimData = FOREACH filtered1 GENERATE a, b, c, y, z;
join2 = JOIN InterimData BY b LEFT OUTER, C BY d  PARALLEL 2;
proj = FOREACH join2 GENERATE a,b,y,z,e,f;
TopNPrj = FOREACH proj GENERATE a, (( e is not null and e != '') ? e : 'None') , z;
TopNDataGrp = GROUP TopNPrj BY (a, e) PARALLEL 2;
TopNDataSum = FOREACH TopNDataGrp GENERATE flatten(group) as (a, e), SUM(TopNPrj.z) as views;
TopNDataRegrp = GROUP TopNDataSum BY (a) PARALLEL 2;
TopNDataCount = FOREACH TopNDataRegrp { OrderedData = ORDER TopNDataSum BY views desc; LimitedData = LIMIT OrderedData 50; GENERATE LimitedData; }
TopNData = FOREACH TopNDataCount GENERATE flatten($0) as (a, e, views);
store TopNData into 'tmpTopN';
TopNData_stored = load 'tmpTopN' as (a:chararray, b:chararray, c:long);
joinTopNData = JOIN TopNData_stored BY (a,b) RIGHT OUTER, proj BY (a,b) PARALLEL 2;
describe joinTopNData;
STORE  joinTopNData  INTO 'output';

The column 'f' from relation 'C' participating in the 2nd JOIN is missing from the final join ouput",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-03-25 19:15:09.761,,,no_permission,,,,,,,,,,,,164825,,,,Tue Apr 06 20:46:41 UTC 2010,,,,,,,0|i0gumf:,96391,,,,,,,,,,25/Mar/10 19:15;olgan;is this reproducible with 0.7.0 code?,"26/Mar/10 03:46;ankur;Yes, I verified that","31/Mar/10 18:15;daijy;Hi, Ankur,
I tested on current trunk. I see:

joinTopNData: {TopNData_stored::a: chararray,TopNData_stored::b: chararray,TopNData_stored::c: long,proj::InterimData::A::a: chararray,proj::InterimData::A::b: chararray,proj::InterimData::B::y: chararray,proj::InterimData::B::z: long,proj::C::e: chararray,proj::C::f: chararray}
2010-03-31 11:10:08,889 [main] INFO  org.apache.pig.impl.logicalLayer.optimizer.PruneColumns - No column pruned for C
2010-03-31 11:10:08,889 [main] INFO  org.apache.pig.impl.logicalLayer.optimizer.PruneColumns - No map keys pruned for C
2010-03-31 11:10:08,889 [main] INFO  org.apache.pig.impl.logicalLayer.optimizer.PruneColumns - No column pruned for B
2010-03-31 11:10:08,889 [main] INFO  org.apache.pig.impl.logicalLayer.optimizer.PruneColumns - No map keys pruned for B
2010-03-31 11:10:08,890 [main] INFO  org.apache.pig.impl.logicalLayer.optimizer.PruneColumns - Columns pruned for A: $2
2010-03-31 11:10:08,890 [main] INFO  org.apache.pig.impl.logicalLayer.optimizer.PruneColumns - No map keys pruned for A
2010-03-31 11:10:08,890 [main] INFO  org.apache.pig.impl.logicalLayer.optimizer.PruneColumns - No column pruned for TopNData_stored
2010-03-31 11:10:08,890 [main] INFO  org.apache.pig.impl.logicalLayer.optimizer.PruneColumns - No map keys pruned for TopNData_stored

And I give dummy data files which data1, data2, data3 all contains one line ""1\t1\t1"", and I see the output has 9 columns which matches the schema. Can you check it again?",06/Apr/10 20:46;olgan;closing since there is no reproducible case,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Zebra] Invalid type for source_table field when using order-preserving Sorted Table Union,PIG-1318,12459908,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,gauravj,gauravj,22/Mar/10 20:28,14/May/10 06:47,14/Mar/19 03:07,23/Mar/10 23:44,0.7.0,,,,,,,0.7.0,,,,impl,,,0,,,,,,,,,,,,,"When we are trying to use order-preserving sorted union:
....

We got the following schema, where the type of 'source_table' is (null) with no column name:

{id: chararray,name: chararray,context: chararray,writer: chararray,rev: chararray,schema: chararray,(null)}

I tried to project the 'source_table' field but failed:

B = FOREACH A GENERATE id, $6; 
DUMP B;

But then we got exception org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias B.

Can you guys please let us know how to access this column? Or is the symptom described above is a bug?

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Mar/10 18:45;gauravj;PIG-1318.patch;https://issues.apache.org/jira/secure/attachment/12439599/PIG-1318.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-03-23 18:51:46.584,,,no_permission,,,,,,,,,,,,164817,,,,Tue Mar 23 23:45:18 UTC 2010,,,,,,,0|i0guiv:,96375,,,,,,,,,,"23/Mar/10 18:49;gauravj;
fix for the jira",23/Mar/10 18:51;yanz;+1,"23/Mar/10 23:44;yanz;My internal Hudson results are as follows:

     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.


Committed to the trunk.","23/Mar/10 23:45;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12439599/PIG-1318.patch
  against trunk revision 926404.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/265/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/265/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/265/console

This message is automatically generated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add DateTime Support to Pig,PIG-1314,12459893,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,zjshen,russell.jurney,rjurney,22/Mar/10 18:27,22/Feb/13 04:53,14/Mar/19 03:07,15/Oct/12 23:20,0.7.0,,,,,,,0.11,,,,data,,,6,gsoc2012,,,,,,,,,,,,"Hadoop/Pig are primarily used to parse log data, and most logs have a timestamp component.  Therefore Pig should support dates as a primitive.

Can someone familiar with adding types to pig comment on how hard this is?  We're looking at doing this, rather than use UDFs.  Is this a patch that would be accepted?

This is a candidate project for Google summer of code 2012. More information about the program can be found at https://cwiki.apache.org/confluence/display/PIG/GSoc2012",,,2419200,2419200,,0%,2419200,2419200,,,PIG-1429,PIG-1310,,,,,,,PIG-2982,PIG-2981,PIG-2980,AVRO-739,HIVE-1269,,,,,,,,,08/Jun/12 19:26;zjshen;PIG-1314-1.patch;https://issues.apache.org/jira/secure/attachment/12531432/PIG-1314-1.patch,19/Jun/12 15:04;zjshen;PIG-1314-2.patch;https://issues.apache.org/jira/secure/attachment/12532561/PIG-1314-2.patch,09/Jul/12 16:58;zjshen;PIG-1314-3.patch;https://issues.apache.org/jira/secure/attachment/12535695/PIG-1314-3.patch,30/Jul/12 08:10;zjshen;PIG-1314-4.patch;https://issues.apache.org/jira/secure/attachment/12538316/PIG-1314-4.patch,13/Aug/12 05:48;zjshen;PIG-1314-5.patch;https://issues.apache.org/jira/secure/attachment/12540610/PIG-1314-5.patch,16/Aug/12 08:48;zjshen;PIG-1314-6.patch;https://issues.apache.org/jira/secure/attachment/12541201/PIG-1314-6.patch,17/Aug/12 07:01;zjshen;PIG-1314-7.patch;https://issues.apache.org/jira/secure/attachment/12541333/PIG-1314-7.patch,26/May/12 19:20;zjshen;joda_vs_builtin.zip;https://issues.apache.org/jira/secure/attachment/12529881/joda_vs_builtin.zip,,,,8.0,,,,,,,,,,,,,,,,,,,2010-03-22 19:02:09.193,,,no_permission,,,,,,,,,,,,35766,,,,Mon Oct 15 23:20:29 UTC 2012,,,,,,,0|i0a0kv:,56367,,,,,,,pig date datetime time support primitive type,,,"22/Mar/10 19:02;alangates;Major +1.  Adding DateTime as a Pig primitive is definitely a good idea.  It's on our list of things to do (http://wiki.apache.org/pig/PigJournal).  A brief overview of the work to be done:

# Add support in parser, both for declaring an input to be of type datetime and datetime constants
# Add support in TypeChecker for datetime types, including any allowed type promotions (ie implicit casts)
# Change LoadCaster interface to include bytesToDateTime method, add method to default implementation
# Determerine which builtin UDFs that we want for datetime and get agreement from community.  Implement these UDFs.
# Implement any allowed cast operators for datetime (probably just string <-> datetime).
# Implement datetime class represents datetime in memory.  This needs to implement WritableComparable so that it can be serialized and compared in Hadoop
# Implement raw comparator for the type so it can be used as a key in groups bys and joins.
# Change physical operators and builtin UDFs to handle processing of datetime types.
# Change data conversion and type discovery routines in DataType
# And, of course, add prolific tests

The other question is backward compatibility.  I can think of only two backward incompatible changes
# Addition of bytesToDateTime in the LoadCaster interface.  Given that this will only require a change if people recompile their implementation, and AFAIK there are no implementations of LoadCaster before our default implementation, I think this is ok.
# Changes to Pig Latin to specify a field as of type date, plus however we denote datetime strings.  We need to make these as unobtrusive as possible, but again I think it will be ok, though we'll need to get community buy in on it.

Would such a patch be accepted?  If it's of good quality deals with backward compatibility concerns, certainly.  In time for 0.8, I don't know.  We try to do a release every three months, with a feature cut off about a month before release (give or take).  Branching and feature cutoff for 0.7 is today, so branching and features cut off for 0.8 will probably be in June.  

If you want to pursue this, the first step should be a brief design that says how you'll go about doing it.  It should cover things like which date format will you use (SQL, something else)?  Which date function do you think should be built in?  How to you plan to store this type in memory?  Are there existing datetime libraries you can leverage or incorporate to avoid rebuilding the wheel?  It's easiest to write up the design on Pig's wiki and then link to it on this bug.  This will give users and developers a chance to review your thoughts and give feedback.
","22/Mar/10 21:31;rjurney;Thanks, Alan.  That is quite helpful.  Let me look into it and see about feasibility.  

What about durations as well?  http://en.wikipedia.org/wiki/ISO_8601#Durations ISO8601 durations would be very handy in enabling use of pig operators on datetimes via +/-, etc.  This might be something to do later, though.","22/Mar/10 22:59;alangates;I think durations would be useful, and others have mentioned to me that they'd like to have them.  As you note, this might be a good phase 2 addition, as getting datetime in alone will be a fair chunk of work.",23/Mar/10 08:34;rjurney;The UDFs in PIG-1310 are a segway to full datetime support.  They can be used until datetimes are supported in Pig.,"23/Mar/10 08:36;rjurney;I would not say this blocks PIG-1310 at all - the UDFs there simply treat ISO dates as strings, which works reasonably well.  They should also handle Long unix times, and will in a next patch.  In any case, this isn't a blocker to that ticket, for which a patch was just submitted.",23/Mar/10 20:45;rjurney;Changing from blocks to related.,"29/May/10 04:20;russell.jurney;As a first pass, I am going to add Boolean, which should be easier than DateTime, but will inform this implementation.  See PIG-1429","30/May/10 05:57;russell.jurney;Ok, thinking about really doing this soon, after Boolean.  I'd like to add two new primitives to Pig - DateTime and Duration.  

I'd do this on the wiki, but I don't have edit access.  Can someone please grant the ability to make a new page to user RussellJurney on the Pig wiki?

Design Notes:

1) I'd like to use Jodatime for this, as I did in the DateTime UDFs.  It is possible to use the Java date libs, but it would be painful to do so.  Jodatime also performs better than Java's native date classes.  It is Apache 2.0 licensed and is already pulled in via ivy in the DateTime UDFs - see PIG-1310

2) Date Format for text/dumps: ISO8601.  Looks like: [YYYY][MM][DD]T[hh][mm]Z  It is a human readable, sortable/comparable, international standard.  See http://en.wikipedia.org/wiki/ISO_8601#Dates

2.5) In memory type: org.joda.time.DateTime.  See http://joda-time.sourceforge.net/apidocs/org/joda/time/DateTime.html

The internal format of jodatime is a Long epoch/Unix/POSIX time.  See http://joda-time.sourceforge.net/faq.html#internalstorage

3) Duration Format for text/dumps: ISO8601.  Looks like: P[n]Y[n]M[n]DT[n]H[n]M[n]S  It is a human readable, sortable/comparable, international standard.  See http://en.wikipedia.org/wiki/ISO_8601#Durations

3.5) In-memory format: org.joda.time.Duration.  See http://joda-time.sourceforge.net/apidocs/org/joda/time/Duration.html

4) All date functions in PIG-1310 should be included, except those replaced by the use of operators on datetimes and durations.  Adding/subtracting datetimes should result in a duration.  Durations can be added/subtracted/divided/multiplied/negated.  

Date/Duration truncation, date differences, date parsing/conversion should be included.  Conversion from int/long POSIX, SQL and datemonth should be included.  Conversion from any string with a DateFormat string should be included.

5) Casting to and from Integer and Long should be supported, as a Unix/POSIX time.  Casting to/from chararray in ISO8601 format should be supported.

Comments?  Suggestions?","31/May/10 14:04;russell.jurney;Hmmm not sure if I should use durations or periods, or both.  See http://joda-time.sourceforge.net/apidocs/org/joda/time/Period.html
",02/Jul/10 06:18;russell.jurney;Been thinking about this... I don't think we should add a full datetime type at this time.  See comments in PIG-1314 on alternative approach using builtins.,"02/Jul/10 06:21;russell.jurney;I suck at JIRA. See proposal in PIG-1430.

","26/Jul/10 21:06;olgan;Russell, are you still planning to finish this for Pig 0.8.0 release?",31/Aug/10 17:44;olgan;Unlinking from 0.8 since we are branching today,30/Apr/11 00:24;jeromatron;I think this would be nice also when outputting from pig scripts using DBStorage to an RDBMS - to be able to serialize properly to the db's timestamp or date type (without extra UDF work).,"08/Sep/11 15:12;zjshen;I've solved the related issue PIG-1429. If nobody is currently working on this issue, I volunteer to investigate into it.",08/Sep/11 22:39;daijy;That will be great. Here is a specification I wrote: https://cwiki.apache.org/confluence/display/PIG/DateTime+type+specification. Take a look and we can discuss.,17/Mar/12 16:35;zjshen;GSoC is back! I'd like to apply it with this issue. The proposal draft will come in following days:-),17/Mar/12 17:40;daijy;Looking forward to your proposal!,"25/Mar/12 14:43;zjshen;Hi folks,

Below is my proposal draft. Any comments are welcome:-)

==

Proposal Title: Adding the Datetime Type as a Primitive for Pig


Student Name: Zhijie Shen 
Student E-mail: zjshen14@gmail.com 

Organization/Project: Apache Software Foundation - Pig 
Assigned Mentor: Daniel Dai /Russell Jurney


Proposal Abstract: 

Apache Pig is a platform for analyzing large data sets based on Hadoop. Currently Pig does not support the primitive datetime type [1], which is a desired feature to be implemented. In this proposal, I explain my plan to implement the primitive datetime type, including the details of my solution and schedule. Additionally, I briefly introduce my background and the motivation of applying GSoC'12. 

Detailed Description: 

1. Understanding of the Project

1.1 What is Apache Pig?

Apache Pig is a platform for analyzing large data sets. Notably, at Yahoo! 40% of all Hadoop jobs are run with Pig [5]. Pig has is own dataflow language, named Pig Latin, which encapsulates map/reduce jobs step-by-step, and offers the relational primitives such as LOAD, FOREACH, GROUP, FILTER and JOIN. Pig provides many built-in functions, but also  allow users to define their user-defined functions (UDFs) to achieve particular purposes. There are more benefits: Pig can operates on the plain files directly without any schema information; it has a flexible, nested data model, which is more compatible with that of major programming languages; it provides a debugging environment.

1.2 Why primitive datetime type is required?

Datetime is a conventional data type in many of database management systems as well as programming languages. Within the Hadoop ecosystem, Hive, which is an analog of Pig, also supports the primitive datetime type (timestamp actually). In contrast, Pig does not fully support this type. Currently, users can only use the string type for the datetime data, and rely on the UDF which takes datetime strings. However, Pig is supposed to primarily parse log data, and most log data has attributes in the datetime type. 

Consequently, it is desired for Pig to support the datetime type as a primitive. By doing so, we can expect the following benefits: a more compact serialized format, working with conventional operators (+/-/==/!=/</>), a dedicated faster comparator, being sortable, fewer times of runtime conversion from string, and relieving users
 from deciding the input datetime string format.


2. Roadmap of Implementing the New Feature

2.1 To Do List

2.1.1  Adding Support in Antlr Parser

Pig Latin supports the assign data type explicitly, such that the “datetime” keyword and some constants, such as “now()” and “today()” can be recognized. The related syntax needs to be added into 5 antlr scripts: AliasMasker.g, AstPrinter.g, AstValidator.g, LogicalPlanGenerator.g, QueryParser.g.

2.1.2 Adding Datetime as a Primitive

The dateime type should be added into the DataType class, and the basic conversion between it and other data types need to be defined. Previously, the internal data structure relies on Joda datetime data type, which is more powerful than java.util.DateTime, but much easier than java.util.Calendar. Hence it is wise to keep this convention.
 Moreover, be careful that implicit type cast from/to the datetime type is not allowed.

I also need to change the LoadCaster and StoreCaster interfaces to include bytesToDateTime/toBytes(DateTime) method, and add details to the classes that implemented these two interfaces. In addition, I need override +/-/==/!=/</> operators for the datetime type, mapping the to some bulitin EvalFuncs. The TypeCheckingExpVisitor class needs to be modified as well to support the datetime type vailidation. One important issue is that according to my previous experience, the data type related code in Pig is widely spread, such that I need to be careful all the related parts are touched.

2.1.3 Refactoring of the Datetime Related UDFs

Thanks Russell Jurney for having implemented a number of useful datetime related UDFs, which can be utilized for the primitive datetime type as well. Part of the UDF Classes located in the “org.apache.pig.piggybank.evaluation.datetime” package  under the “contrib” folder need to be move to the “org.apache.pig.builtin” package under the “src” folder. Below are the related UDFs:

int DiffDate(DateTime d1, DateTime d2)
int YearsBetween(DateTime d1, DateTime d2)
int MonthsBetween(DateTime d1, DateTime d2)
int DaysBetween(DateTime d1, DateTime d2)
int HoursBetween(DateTime d1, DateTime d2)
int MinutesBetween(DateTime d1, DateTime d2)
int SecondsBetween(DateTime d1, DateTime d2)
int GetYear(DateTime d1)
int GetMonth(DateTime d1)
int GetDate(DateTime d1)
int GetHour(DateTime d1)
int GetMinute(DateTime d1)
int GetSecond(DateTime d1)
DateTime DateAdd(DateTime d1)
String ToString(DateTime d, String format)
 (Probably rename it DateTimeFormat)

The remaining UDFs can be eliminated, while their logics can be used in the primitive type conversion part, which has been introduced in the previous section. Below are the UDFs of this kind:

DateTime ToDate(String s)
DateTime ToDate(String s, String format)
DateTime ToDate(String s, String format, String timezone)
DateTime toDate(long t)
String ToString(DateTime d)
long ToUnixTime(DateTime d)

Probably the following additional UDFs are also required, I need to discuss these with the community:

DateTime Now()
DateTime Today()
bool IsDateTime(String s)

2.1.4 Test Cases

A large number of test cases are required to test the parser, the datatime operations and conversion, and loading from / storing into the secondary storage.

2.1.5 Documentation

A user manual is required to describe how to use datetime primitive, such as the input format, the supported built-in functions.

2.2 Project Schedule 

During the summer, I will have not much workload except writing my Ph.D. thesis. Hence it is possible for me to spend around 40 hours per week on this project. The concrete schedule are summarized as follows: 

Present - May 20 (before official start of summer of code): Reading the related code in detail, and keeping touch with the community to clarify some issues, such as the necessary built-in UDFs and the rules of data conversion.

May 21 - Jun 3 (two weeks):  Adding the datetime into the primitive type list, and completing the functionality of  parsing the datetime keyword and constraints, such that the string representing a datetime can be recognized from Pig Lating scripts.

Jun 4 - Jun 24 (thee weeks): Implementing type conversion (from/to string) and loading/storing cast functionality. After this step, data of the datetime type can be correctly reading from/storing into the secondary storage.

Jun 25 - Jul 8 (two weeks until mid-term evaluation): Completing the remaining part of the type conversion (e.g., between the datatime type and the long type),  dealing with some issues that have not been foreseen yet, and preparing for the mid-term evaluation.

Jul 9 - Jul 29 (three weeks): Refactoring the datetime related UDFs, adding new required UDFs, and overloading the primitive operators, such that all the defined operations on datetime values are supported after this step.

Jul 30 - Aug 5 (one week):  Writing the test cases to systematically verify the code, debugging the possible bugs. After this step, the coding part is nearly done.

Aug 6 - Aug 12 (one week until final evaluation ): Documenting the user manual to show how to work with the datetime type, and preparing for the final evaluation.

Additional Information: 

I am a Ph.D. student from National University of Singapore. My research topics are large scale multimedia systems, geo-referenced video systems and P2P video streaming. In addition to research, I love programming and have long-term experience in several languages, including Java.  Moreover, I am quite interested in distributed systems and big data, and have acquired solid background knowledge.  I used to take the course - ""Parallel and Distributed Databases"", drafted a survey of the cloud storage systems (including Pig) [4] and obtained the A+ score. 

Notably, I am a open source advocate, and have contributed to it to some extent. Last year, I  have participated into GSoC with a Pig project. I successfully implemented the nested cross feature [2]. And I overfulfiled my proposed task by contributing one more patch of adding the primitive boolean type [3], which is somewhat similar to the task proposed for this year's GsoC. Therefore, I am quite familiar with this task and confident of completing it on time. Last but not least, I enjoy the long term participation into the Pig community, and am willing to keep contributing to it.


Reference:

[1] https://issues.apache.org/jira/browse/PIG-1314W
[2] https://issues.apache.org/jira/browse/PIG-1916
[3] https://issues.apache.org/jira/browse/PIG-1429
[4] http://www.comp.nus.edu.sg/~z-shen/survey.pdf
[5] http://wiki.apache.org/pig/OldFrontPage","25/Mar/12 14:44;zjshen;By the way, who would like to mentor this issue?","26/Mar/12 01:28;russell.jurney;I am happy to help regarding questions about the DateTime UDFs, but do not remember the internals of my attempt to add Boolean in preparation for DateTime.  I suggest the comitter who got Boolean working would be a good candidate?","26/Mar/12 01:41;zjshen;Coincidentally, I'm that person making Boolean working:-)

Daniel helped me a lot to work out that issue, if he'd like to mentor this one, it will also be awesome.",26/Mar/12 08:04;daijy;I would like to mentor this.,"03/Apr/12 05:09;zjshen;I've pasted the proposal to the official website: http://www.google-melange.com/gsoc/proposal/review/google/gsoc2012/zjshen/21002

Any comments are welcome, such that I can improve the proposal in the remaining days.",03/Apr/12 05:17;prkommireddi;Thanks Zhijie. Can you please make it public?,"03/Apr/12 05:26;zjshen;Ah, I forgot doing it. Public now:-)","26/May/12 19:20;zjshen;As suggested by Thejas, I've done performance comparison between JODA and builtin datetime-related objects. For each function, I repeated 100,000 times of computation, and calculated the time respectively. Please refer to the attachment for the code details. Bellow is the summary of the results (unit is millisecond):

ISOToSecond: JODA-958 Builtin-1326
ISOToMinute: JODA-532 Builtin-850
ISOToHour: JODA-414 Builtin-680
ISOToDay: JODA-475 Builtin-685
ISOToMonth: JODA-463 Builtin-692
ISOToYear: JODA-462 Builtin-715
ISOSecondsBetween: JODA-961 Builtin-968
ISOMinutesBetween: JODA-734 Builtin-565
ISOHoursBetween: JODA-596 Builtin-656
ISODaysBetween: JODA-592 Builtin-555
ISOMonthsBetween: JODA-586 Builtin-968
ISOYearsBetween: JODA-654 Builtin-952
ISOToUnix: JODA-678 Builtin-6965
UnixToISO: JODA-225 Builtin-206
Custom Format 1 [yyyy.MM.dd G 'at' HH:mm:ss.SSS Z]: JODA-596 Builtin-6914
Custom Format 2 [yyyyy.MMMMM.dd GGG hh:mm aaa]: JODA-534 Builtin-425

Two major conclusions are as follows:
1. The datetime operations with the help of JODA generally performs as good as those with the builtin data structure (according to my implementation), except the operation of parsing a time string.
2. It is found that based on my implementation, the builtin data structure needs one more order of magnitude of time to parse a time string when the format has a timezone component (i.e., ""Z"").

To sum up, my suggestion is that since JODA provides no worse performance and more trustworthy correctness, I vote for going on with JODA when implementing the datetime primitive type.","26/May/12 20:44;russell.jurney;I concur about JODA. So far as I know you can't even parse ISO times with java builtins without using javax.xml.bind.DatatypeConverter, and it is ugly and slow.
","28/May/12 12:58;zjshen;One quick issue: we need to give a name to the new type. We are supposed to use ""DATETIME"", correct? Or ""DATE"", ""TIMESTAMP""?","28/May/12 23:31;thejas;bq. One quick issue: we need to give a name to the new type. We are supposed to use ""DATETIME"", correct? Or ""DATE"", ""TIMESTAMP""?
""datetime"" makes sense when it has both date and time (hrs,mins,secs) parts to it. The problem with using (unix) timestamp, is that the date range is limited to 78 years. Using jodatime, we will be able to support much larger date range than timestamp.

","29/May/12 00:34;russell.jurney;""DATETIME"" makes sense, but ""TIMESTAMP"" is a good (simple) alias for DATETIME(NOW).  ""DATE"" is a good alias for a date-truncated DATETIME.

I'm not sure if you would want to implement these in Pig... as there is clearly less utility than in a database, where for instance a TIMESTAMP can be updated whenever a field is written or updated. Maybe ""DATE"" and not ""TIMESTAMP,"" but only as an afterthought?","29/May/12 02:22;thejas;CURRENT_TIME() might be a more intuitive alias for DATETIME(NOW). I think we can consider adding support for DATE and CURRENT_TIMESTAMP() as a next step after adding  DATETIME. We can focus on DATETIME in this jira.

I also had a look at timestamp datatype that was added to hive, to see if it will be interoperable (through hcatalog). The only difference is that hive timestamp type supports storing up to nano second precision, while jodatime supports only up to millisecond. Nanoseconds are not likely to be used in most cases, so loosing that precision when converting hive timestamp to pig datetime should be OK in most cases. The range of years supported in both cases is also approximately the same.



","31/May/12 14:39;zjshen;When adding the DateTime type for Pig, we need to take care of the I/O with AVRO, which still doesn't support the Date/Time type.","03/Jun/12 16:56;zjshen;One more issue needs to be clarified:

In the AugmentBaseDataVisitor class, there're two functions: Object GetSmallerValue(Object v) and Object GetLargerValue(Object v). where if v is a numeric value, v is added or reduced by one while if v is a byte array, it is added or reduced by one byte. Then, how do we do if v is a datetime? I vote for returning null, and am looking forward to the community's opinions.

By the way, how about if v is a boolean, which seems not to be handled?","04/Jun/12 17:02;thejas;bq. When adding the DateTime type for Pig, we need to take care of the I/O with AVRO, which still doesn't support the Date/Time type.
StoreFuncs that write in avro format will need to throw an exception if the schema being stored contains a datetime type. That will force the users to serialize datetime as some other type. As long as we are not breaking existing pig queries don't use datetime type, we should be fine. Avro is just one of the many formats.

Regarding AugmentBaseDataVisitor, that is used for example generation. (see [sigmod paper on illustrate feature | http://infolab.stanford.edu/~olston/publications/sigmod09.pdf] for details) . For example, if there is no value in col1 in sample that satisfies ""col1 > 0"", a value of col1 > 0 is generated. This will be useful for datetime type as well. 
To have a more realistic value generated (similar to values in input), I think we should increment/decrement the smallest field that is non zero. For example if the millisecond and second fields are 0, but hour field is non zero, increment that. If all time parts are 0, but day of month is not, increment that.
In case of boolean, as we don't support > or < operations, these functions do not make sense. 

Thanks for bringing this up. I had forgot about this use case. We should add a few unit tests for example generation that involve datetime.

 ","08/Jun/12 19:26;zjshen;I've modified the codes in the src package related to the primitive DateTime (see the attached file). As the code related to data type is widely spread in the project, I still need to go through it more times to figure the potential missing parts.

Up till now, there's some more issues that need to be discussed:

1. Pig can also import into and export from HBase storage, which also doesn't have the primitive DataTime. Throw exception in this case as well, correct?

2.For the type casting between DateTime and other types of data, how about following the rules below:
a. Allow: DateTime <-- Numeric value (being converted to Long first)
b. Allow: DateTime <-- String
c. Not allow: DateTime <-- Boolean
d. Only explicit casting allowed

3. DateTime is serialized as a Long value (Unix timestamp) when it is necessary.",08/Jun/12 20:21;russell.jurney;Avro might store DateTimes as an ISO string?,"11/Jun/12 15:25;zjshen;{quote}
Avro might store DateTimes as an ISO string?
{quote}

It's possible, but there seems to be one problem. If we store a datetime as an iso string, how do we determine whether a string is just a string or a datetime when it is loaded?

One more issue is that it' good to keep all the IO targets that does not support datetime handle the IO process uniformly. Hence if we conclude the design for Avro, we should keep to it for the others.","19/Jun/12 15:04;zjshen;I've updated the patch with the following changes:

1. Editing some codes related to IO.
2. Implemented most of the UDF in lised https://cwiki.apache.org/confluence/display/PIG/DateTime+type+specification, excluding DateAdd, whose functionality is not the clear to me.
3. Correcting some error when merging the my modifications with the latest version in the repository.

There's following issues to be discussed:
1. the output datatype of DiffDate(DateTime d1, DateTime d2) should use long instead of int, because the diff may be too large for int range to conver.
2. what does DateTime DateAdd(DateTime d1) mean? Adding datetime based on the current time?
3. we allow explicit cast between datetime and string, correct? Similarly, do we allow  explicit cast between datetime and long/int (representing unix timestamp)?","25/Jun/12 06:19;thejas;bq. 1. Pig can also import into and export from HBase storage, which also doesn't have the primitive DataTime. Throw exception in this case as well, correct?
Yes. The exception should be thrown from HBaseStorage.

bq. if we conclude the design for Avro, we should keep to it for the others.
Please note that pig does not have a way of know if the format will support datetime. The behavior will be controlled by the storage func implementation. But for the ones that are part of pig codebase, I think we should throw an exception. 

bq. 3. DateTime is serialized as a Long value (Unix timestamp) when it is necessary.
JodaTime supports milliseconds as well. Will we be able to convert all values within limits of JodaTime date into a long ?

bq. the output datatype of DiffDate(DateTime d1, DateTime d2) should use long instead of int, because the diff may be too large for int range to conver.
Makes sense, we should use a type that is appropriate for range.

bq. what does DateTime DateAdd(DateTime d1) mean? Adding datetime based on the current time?
Not sure. Daniel, do you know ?

bq. we allow explicit cast between datetime and string, correct? Similarly, do we allow explicit cast between datetime and long/int (representing unix timestamp)?
Yes, we should support explicit cast between these types. Though conversion to int might not be successful for all datetime values. 

","25/Jun/12 17:37;thejas;bq. what does DateTime DateAdd(DateTime d1) mean? Adding datetime based on the current time?
Discussed this with Daniel. I think it makes sense to replace this with different functions -
// add number of days specified in days param to the DateTime date. 
// The days param can be positive or negative
AddYears(DateTime date, int days);

Similarly we should have AddMonths, AddDays, AddHours ..

","26/Jun/12 00:58;russell.jurney;A couple comments:

1) Don't persist DateTimes as ints/longs unless you also persist a timezone offset with it somehow (is this possible?). Persisting timezones is one of the key benefits of a DateTime type in my opinion. At Hadoop scale you are often dealing with events from different sites/locations. DateTime needs timezone, or we can just use long/unix time.
2) Consider using jodatime/ISO8601 durations for date math, as a separate type. i.e.  If this extends scope too far, save it for later.  http://en.wikipedia.org/wiki/ISO_8601#Durations

Although it may be inefficient, I would encourage an ISO8601 string representation during serialization.","26/Jun/12 01:59;thejas;bq. 1) Don't persist DateTimes as ints/longs unless you also persist a timezone offset with it somehow (is this possible?).
I forgot about timezone. We need to serialize the timezone information as well, while supporting the same range of dates as JodaTime . With int/long this will not be possible. (Zhijie can you confirm ?)

bq. 2) Consider using jodatime/ISO8601 durations for date math, as a separate type. i.e. If this extends scope too far, save it for later. http://en.wikipedia.org/wiki/ISO_8601#Durations
+1 . This is much cleaner. Lets use replace the Add* functions with just AddDuration . For example AddDuration(d1, ""P3Y""), would return d1 + 3 years. 
","26/Jun/12 04:11;zjshen;Dear Thejas and Russell,

{quote}
1) Don't persist DateTimes as ints/longs unless you also persist a timezone offset with it somehow (is this possible?).
I forgot about timezone. We need to serialize the timezone information as well, while supporting the same range of dates as JodaTime . With int/long this will not be possible. (Zhijie can you confirm ?)
{quote}

As far as I know, either Java builtin Date or Joda DateTime uses millisecond-shift (stored in a long integer variable) from the midnight UTC, which is not exactly the Unix time. Importantly, the millisecond-shift has nothing to do with the time zone. For example, both

new DateTime(9223372017043199999L, DateTimeZone.UTC).getMillis();

and

new DateTime(9223372017043199999L, DateTimeZone.forID(""Asia/Singapore"").getMillis();

will return the same value, that is, 9223372017043199999L. The time zone only determines only determines the ISO time string, such that the two DateTime objects will output different ISO time strings when toString() is called. Hence I think the long variable which represents the millisecond-shift is good for internal serialization. When we need to convert the DateTime object to Unix time string, we may use the default time zone of the Pig environment (I'm still working on this. Please let me know how you think the Pig-wide time zone should be set.) or the user-defined time zone (We probably need one more UDF String ToString(DateTime d, String format, String timezone)).

AS to Pig DateTime, internal Joda DateTime objects is either created with the long variable of millisecond-shift or with ISO time string. Initialization with a long variable (from Long.MIN_VALUE to Long.MAX_VALUE) has no range problem when getMillis() is called, obtaining the result ranged from Long.MIN_VALUE to Long.MAX_VALUE as well. Initialization with a ISO time string, the JODA DateTime object only accepts the year in the range [-292275054,292278993], such that the corresponding millisecond-shift is also within [Long.MIN_VALUE, Long.MAX_VALUE]. In summary, the range will be fine when Long is used for serialization.

Please correct me if I'm wrong. Thanks a lot!

{quote}
2) Consider using jodatime/ISO8601 durations for date math, as a separate type. i.e. If this extends scope too far, save it for later. http://en.wikipedia.org/wiki/ISO_8601#Durations
+1 . This is much cleaner. Lets use replace the Add* functions with just AddDuration . For example AddDuration(d1, ""P3Y""), would return d1 + 3 years.
{quote}

+1. In this way, it is more flexible for users to define the amount of time to add/subtract. Since the ISO duration is non-negative (Please correct me if I'm wrong), we need to SubstractDuration as well.","26/Jun/12 04:31;russell.jurney;Whatever the format is, I think we should serialize/persist DateTimes in a way that the timezone stays with the datetime. ","26/Jun/12 19:03;thejas;bq. As far as I know, either Java builtin Date or Joda DateTime uses millisecond-shift (stored in a long integer variable) from the midnight UTC, which is not exactly the Unix time. 
Yes, as you noted, the difference is unix timestamp can store upto +/- 292 Billion years, while Joda DateTime supports only +/- 292 Milllion years. Which should be sufficient for most practical purposes! :)

bq. The time zone determines only determines the ISO time string,
It also affects the field values, (getDayOfWeek(), getHourOfDay() etc. In your data, you can have dates belonging to different timezones, and users might want to retain that information. 
An example of use case where timezone also needs to be stored - if you want to do analysis of how many people come to a global website during their morning hours, you want to .getHourOfDay() to return the hour as per local timezone. 

We need an efficient way to serialize timezone along with the long. Can you propose something ? (Maybe, just make it efficient for 256 most 'popular' timezones and store it a byte. And not have the byte for UTC. For other timezones,  add a timezone string ?) 

bq. When we need to convert the DateTime object to Unix time string, we may use the default time zone of the Pig environment 
If the date field has the timezone value in it, we don't have to rely on default time zone to convert to unix time stamp. (assuming that is what you meant by 'unix time *string*' )
But udfs like DateTime ToDate(String s) where timezone might not be specified, we need a default timezone. I think we should use the default timezone on the pig client machine. Using the default time zone on each task tracker node can lead to a nightmare in debugging if one of the nodes happens to have a different timezone. We should allow the user to set a default timezone using a pig property. 

bq. We probably need one more UDF String ToString(DateTime d, String format, String timezone)
Having timezone argument in this call is necessary only if user wants to print the time for a different timezone. This is useful, but not mandatory. 


bq.Since the ISO duration is non-negative (Please correct me if I'm wrong), we need to SubstractDuration as well.
Yes, you are right. I could not find any references to negative values in ISO duration. Lets add SubstractDuration

Trivia from wikipedia: 64 bit unix timestamp, in the negative direction, goes back more than twenty times the age of the universe 
","26/Jun/12 20:48;russell.jurney;Jodatime seems to solve these problems. Serializing from a string without a timezone, it does things in a reasonable manner.  Serializing things from a string with a timezone, it does things in a reasonable manner.

Are we discussing a user-facing API, or an internal storage mechanism?  I'm not clear on which.  Regarding the interface, presenting integers to a user as an interface seems wrong to me.  Excluding certain timezones in the name of efficiency also seems wrong to me.  The point of a datetime type is to add timezones, otherwise we can simply use longs.

As an internal storage mechanism, I'm un-opinionated, so long as all timezones are retained at all times.","26/Jun/12 23:18;thejas;bq. Are we discussing a user-facing API, or an internal storage mechanism? 
Some questions were about interface, some about internal storage.

bq. Regarding the interface, presenting integers to a user as an interface seems wrong to me. 
Converting dates to integer is something user can optionally do, this is not expected to be a common use case. String representation of date literals will also be supported. Most operations will be on date type itself, without converting it to int/string.

bq. Excluding certain timezones in the name of efficiency also seems wrong to me.
All timezones supported by JodaTime will be supported. I was only proposing that we encode the timezone info efficiently, at least for most likely used ones. I think converting the string timezone (location name) to UTC offset in minutes, is one possibility. 

","27/Jun/12 02:11;zjshen;Hi Thejas and Russell,

I'll do serialization for timezone as well.

{quote}
I think converting the string timezone (location name) to UTC offset in minutes, is one possibility.
{quote}

In my opinion, this kind of compression is lossy. Several time zones may share the same UTC offset, such that when the reverse operation is to do, it will be unknown which timezone the UTC offset should be converted to.

{quote}
We need an efficient way to serialize timezone along with the long. Can you propose something ? (Maybe, just make it efficient for 256 most 'popular' timezones and store it a byte. And not have the byte for UTC. For other timezones, add a timezone string ?)
{quote}

The time zone class in either builtin and joda has the function ""getAvailableIDs"", which returns all the available time zone strings. On my machine, I got 616 from the builtin time zone while 558 from the joda one. Probably we can have a one-to-one mapping between the time zone strings and the integer ids in short variables. However the ""available"" in the function ""getAvailableIDs"" sounds tricky. I'm not sure whether ""getAvailableIDs"" returns the same time zone list on all machines or is machine-dependent.
","27/Jun/12 03:32;thejas;bq. Several time zones may share the same UTC offset, such that when the reverse operation is to do, it will be unknown which timezone the UTC offset should be converted to.
Yes, it will be lossy, but the part that is important for date calculations is preserved. The ISO spec only has offset for timezone. I don't think we have to allow datetime field to be used for storing location information. Does JodaTime preserve the location string ?

bq. I'm not sure whether ""getAvailableIDs"" returns the same time zone list on all machines or is machine-dependent.
It depends on the release/jar (http://joda-time.sourceforge.net/tz_update.html). As pig will be shipping this jar to the nodes, it is ok to assume that it will be the same across all nodes for a query. So it is safe to rely on the id for intermediate serialization. 
But won't jodatime support a timezone outside this list, If the user specifies a date using the UTC offset format ?



","27/Jun/12 08:45;zjshen;{quote}
Yes, it will be lossy, but the part that is important for date calculations is preserved. The ISO spec only has offset for timezone. I don't think we have to allow datetime field to be used for storing location information. Does JodaTime preserve the location string ?
{quote}

Yes, I think so. If I get an DateTimeZone object by DateTimeZone.forID(""asia/singapore""), the returned DateTimeZone object doesn't change to ""+08:00"", but keeps ""asia/singapore"". We'd better preserve it because when users want to output the time in their customized format that has ""z"" in the pattern string, the exact timezone can be outputed.

{quote}
But won't jodatime support a timezone outside this list, If the user specifies a date using the UTC offset format ?
{quote}

Yes, DateTimeZone.forID() also allows UTC offset string as input, such as ""+08:00"", though it is not in the list. However, the offset can be value in the range [-23:59:59.999, +23:59:59.999], and the minimal granularity can be the millisecond

Then, we are expected to have a combined lookup table that maps canonical timezone ids and UTC offset to their concise representation. Do you have any suggestion here? Or we temporally set aside the performance issue right now, and move forward to make timezone serialization work by simply serializing the timezone id string.","27/Jun/12 18:54;thejas;bq. Or we temporally set aside the performance issue right now, and move forward to make timezone serialization work by simply serializing the timezone id string.
We can add features later, but dropping features later won't be good. In my opinion, the support for long timezone name is not going to be needed by most people. I think we can support it only for creating a DateTime field, but say that pig will not preserve the long name. Pig will only retain hours+minute offset (no seconds and milliseconds!). The hour+min offset form is portable and more likely to be supported by other serialization formats. 
",27/Jun/12 20:02;russell.jurney;This sounds good to me.,"28/Jun/12 14:08;zjshen;Hi Thejas, I'll take your suggestions. Thanks!","03/Jul/12 09:42;zjshen;There's some issues with loading/storing pig data. When store a DateTime object with ""Utf8StorageConverter"" without using UDFs to convert it to some string, should we serialize it as a millis+timezone composite, or output an UTC-style datetime string (e.g., 2012-07-03T08:14:19.962+01:00))? The latter operation behaves the same as uses ""String ToString(DateTime d)"" before storing the string? Personally, I like the latter choice, because the data is directly readable from the stored files.

On the other hand, if a datetime object is stored in the file as a datetime string, when we load it again as a datetime object, should we use the default timezone or use the one specified in the timezone string (e.g., +01:00 in the last example)? I again prefer the second choice. When we use Pig, it is possible to do a bunch of store/load to achieve some goal. The timezone information need to be preserved. For example, let's assume +08:00 is the default timezone. A datatime object whose individual timezone is -04:00 is stored as a string, which will have -04:00 as suffix. When the string is loaded as a datetime object for further process, we'd better keep to the previously used timezone, -04:00, instead of the default one.

How do you think about this? Thanks!
","03/Jul/12 18:29;thejas;PigStorage is meant to be a human readable format. So that is another reason to store the timestamp in the ISO string as you suggested. 
Yes, If the timezone is specified in the string, pig should use that value. But the timezone part and time part of the datetime string should be optional. Does jodatime support that ?
","04/Jul/12 01:57;zjshen;{quote}
But the timezone part and time part of the datetime string should be optional. Does jodatime support that?
{quote}

Yes, these two parts are not mandatory. The default time value is ""00:00:00.000"" while the default timezone offset is ""+00:00"". When the datetime object is outputed an ISO-format string, the default parts will be filled up (e.g., 2012-07-03T00:00:00.000Z).","09/Jul/12 16:58;zjshen;Here's the newest patch of this issue, which contains the following changes since the last one:

1. Including the timezone when serializing datetime objects.

2. Implementing the additional UDFs that we have discussed.

3. Updating my previous modifications to solve some conflicts with the PIG-2632 patch.

4. Adding ""timezone"" configuration for Pig.

Util now, the patch can basically make the primitive datetime type work.

However, I've not do the thorough test yet. Therefore, my next step (in the second half of GSOC) will focus on coding the test cases, fixing bugs, etc.
","12/Jul/12 02:02;thejas;Zhijie,
I have added comments on your latest patch in  https://reviews.apache.org/r/5414/.
Yes, lets focus on test cases now, so that we can get an initial version committed. ","13/Jul/12 04:03;zjshen;Hi Thejas,

Thanks for your review. I'll check out your comments.","30/Jul/12 08:10;zjshen;Hi Thejas,

Here's my latest patch. Compared to the last one, there are the following modifications:

1. I've modified the code according to many of review comments.

2. I've added many test cases, but some are still missing. I'll add more in the following days.

3. I've fixed some bugs while running the newly added test cases.

There's still some issues related to timezone I need to discuss with you:

1. You've mentioned that we need to propagate the timezone from the client to backend, where the udfs get executed. How the timezone should be propagated to the backend, which I assume the machine that runs the code? Previously I made the timezone setting in pig.properties, which will be loaded when PigServer runs, such that the default timezone will be set. Consequently, if a datetime object is created without specifying the timezone, the default one will be used. However, do you mean some other way?

2. According to our previous discussion, ToDate() can take different type of timezone input, either location or UTC offset. However, two timezones of the two different types may be treated not equal even when the offset is same. For example, new DateTime(0L, DateTimeZone.forID(""asia/singapore"")) and new DateTime(0L, DateTimeZone.forID(""+08:00"")) are not equal. As we previously chose the UTC offset to be the basic timezone representation, I convert the location-based timezone to the utc-offset one and only use utc-offset style internally. Therefore, the aforementioned two equal datetime objects will not be mis-treated.

Regards,
Zhijie","03/Aug/12 23:21;thejas;bq. 1. You've mentioned that we need to propagate the timezone from the client to backend, where the udfs get executed. How the timezone should be propagated to the backend, which I assume the machine that runs the code? 
Yes
bq. Previously I made the timezone setting in pig.properties, which will be loaded when PigServer runs, such that the default timezone will be set. Consequently, if a datetime object is created without specifying the timezone, the default one will be used. However, do you mean some other way?
It is possible that some of the task nodes  might be misconfigured and have different default time zone. In such cases, the results won't be what you want and it will be very difficult to debug. So the default timezone on the client should be used in the nodes as well. 

bq. I convert the location-based timezone to the utc-offset one and only use utc-offset style internally. Therefore, the aforementioned two equal datetime objects will not be mis-treated.
Sounds good.
","13/Aug/12 05:48;zjshen;Hi Thejas,

I attached my newest patch (the same as that in my previous email sent to you). Compared the last version, there's following improvement:

1. More test cases have been added, such that the test cases are nearly completed.

2. Fix some bugs according to the test cases, including the builtin functions (e.g., argToFuncMapping).

3. I add some more builtin functions: MilliSecondsBetween, GetMilliSecond, ToMillSeconds, since the granularity of pig DateTime is set to millsecond.

I've also some comments:

1. DiffDate behaves similar to DaysBetween, except that the former function return opposite values if two arguments change their order.

2. According to your last response, I'm not clear how the default timezone of client can be sent to the server with the code. In my opinion, the default timezone should be specified on the server side by configuration, which should be taken care of by administrators. How do you think about this.

I think this patch is close to commit. Please check it out. Thanks!","15/Aug/12 01:42;thejas;bq. 2. According to your last response, I'm not clear how the default timezone of client can be sent to the server with the code. In my opinion, the default timezone should be specified on the server side by configuration, which should be taken care of by administrators. How do you think about this.

I believe you should be able to set the default timezone property in PigContext constructor, and also let user override the default. In backend, you can access the value using something like - PigMapReduce.sJobConfInternal.get().get(""pig.datetime.default.tz"").
",15/Aug/12 02:24;russell.jurney;I agree with Thejas. The user will want to control the timezone of NOW() without having to reconfigure the hadoop cluster/contact the hadoop administrator. Setting this on the client is consistent with Pig as a client-side technology.,"15/Aug/12 03:39;zjshen;{quote}
I believe you should be able to set the default timezone property in PigContext constructor, and also let user override the default. In backend, you can access the value using something like - PigMapReduce.sJobConfInternal.get().get(""pig.datetime.default.tz"").
{quote}

Thank you, Thejas! Let me investigate this issue.","16/Aug/12 08:48;zjshen;Hi Thejas,

I attached my latest patch. In this version, I fixed the default timezone issue. Pig can obtain the timezone string from PigContext, which can be loaded from either the default property files or some user supplied sources. Instead of calling PigMapReduce.sJobConfInternal.get().get(""pig.datetime.default.tz"") every time when no user-supplied timezone is specified for DateTime construction, I configure the default timezone of joda when PigGenericMapBase and PigGenericMapReduce are at the setup() stage. Therefore, when no timezone is specified for DateTime construction, the created DateTime object will automatically use the default timezone. I think by doing this,  users do not need to touch too much detail (calling  PigMapReduce.sJobConfInternal) when writing their UDFs that are related to DateTime, and avoid the ambiguity that PigMapReduce.sJobConfInternal.get().get(""pig.datetime.default.tz"") and DateTimeZone.getDefault().getID() may sometimes be different.","17/Aug/12 00:34;russell.jurney;I have one suggestion - add getWeeks and weeksBetween, if it isn't inconvenient. I think Jodatime can do this. It is useful when dealing in weeks.","17/Aug/12 02:48;zjshen;{quote}
I have one suggestion - add getWeeks and weeksBetween, if it isn't inconvenient. I think Jodatime can do this. It is useful when dealing in weeks.
{quote}

Yes, week field should be useful. In addition to it, I think it's better to add getWeekYear as well, because using weeks of year alone may cause ambiguity sometimes. For example, both ""2008-12-31"" and ""2009-01-01"" are week 1 of weekyear 2009, though the two dates are in two different years.

In addition, do you think it is better to rename some time UDFs as follows?

getMonth -> getMonthOfYear
getDay -> getDayOfMonth (do we need getDayOfWeek and getDayOfYear as well?)
getHour -> getHourOfDay
getMinute -> getMinuteOfHour
getSecond -> getSecondOfMinute
getMilliSecond -> getMilliOfSecond

The changes will make UDFs' names longer but clearer.","17/Aug/12 07:01;zjshen;Hi Thejas,

I've updated the patch again. I'll be sorry if it disturbs your review of the code. In the latest version, I added three more datetime related UDFs and related test cases according to Russell's suggestion:

1. WeeksBetween
2. GetWeek
3. GetWeekYear

In addition, I modify the code of XXXXBetween UDFs. Previously, all the UDFs in this category leverages joda to compute the interval. While joda can only return the integer interval, the actual interval is likely to be so big that it has to be stored in a long variable. Therefore, for the datetime fields of fixed length:

1. MilliSecondsBetween
2. SecondsBetween
3. MinutesBetween
4. HoursBetween
5. DaysBetween
6. WeeksBetween

I adopted my our computation methods. On the other side, fot the datetime fields of flexible length:

1. MonthsBetween
2. YearsBetween

I keep to the joda methods. We may improve this later.

At last, I remove the DiffDate UDF, because it is the same as DaysBetween.","24/Aug/12 01:21;thejas;PIG-1314-7.patch committed to trunk! Thanks Zhijie.

We need to update the documentation regarding this change. Can you please upload a new patch for that ? To see generated docs, run - ant -Dforrest.home=<Forrest installation dir> docs. The files to be edited are under - trunk/src/docs/src/documentation/ .

We should also add a few end to end test cases for datetime. See https://cwiki.apache.org/confluence/display/PIG/HowToTest#HowToTest-EndtoendTesting . We should have a few queries that do some of the basic operations on date time, and queries that have order-by , group and join on date fields. 
These can be submitted as multiple patches.  ","24/Aug/12 01:27;thejas;We also need to have some test cases that set the timezone property. This might not be easy to do in the e2e framework, so unit test cases are better candidate for this. Please let me know if you need any help.
","24/Aug/12 01:29;zjshen;Hi Thejas, let me do that.","28/Aug/12 22:51;julienledem;Hi Thejas,
this commit added JobControlCompiler.java.orig which I suspect is not what you intended.
http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java.orig?view=log&pathrev=1376800
Could you double check?
Thanks, Julien","28/Aug/12 23:06;thejas;Yes, that was not intentional. Deleted JobControlCompiler.java.orig in svn.
","15/Oct/12 22:18;dvryaboy;A chunk of this is committed, and it's not clear what's left to do. Can we close this and create a new ticket for the remaining work?","15/Oct/12 23:20;thejas;As Dmitriy suggested, closing this jira and opened new ones for remaining work - PIG-2980, PIG-2981, PIG-2982 .
"
PigServer leaks memory over time,PIG-1313,12459881,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,billgraham,billgraham,billgraham,22/Mar/10 16:57,17/Dec/10 22:43,14/Mar/19 03:07,01/Apr/10 22:49,0.7.0,,,,,,,0.8.0,,,,impl,,,0,,,,,,,,,,,,,"When {{PigServer}} runs it creates temporary files using the {{FileLocalizer.getTemporaryPath(..)}}. This static method creates and returns a handle to a temporary file (as an instance of {{ElementDescriptor}}). The {{ElementDescriptors}} returned by this method are kept on a static {{Stack}} named {{toDelete}}. The items on {{toDelete}} get removed by the {{FileLocalizer.deleteTempFile()}} method.

The only place in the code where I see {{FileLocalizer.deleteTempFile()}} called is in the Main class. {{PigServer}} does not call that method though, so a long-running VM that repeatedly uses instances of {{PigServer}} to run jobs will leak memory via {{toDelete}}.

One suggested fix is to have {{PigServer.shutdown()}} call {{FileLocalizer.deleteTempFile()}}, but this would cause problems in a multi-threaded environment, since it seems {{ElementDescriptors}} are pushed onto the {{toDelete}} stack before they're used, not once they're done with. With this approach, running multiple instances of {{PigServer}} in separate threads could cause one completed job to clobber the other's still-in-use temp files.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Mar/10 20:39;billgraham;PIG-1313-0.4.0-1.patch;https://issues.apache.org/jira/secure/attachment/12440102/PIG-1313-0.4.0-1.patch,01/Apr/10 18:27;billgraham;PIG-1313-0.4.0-4.patch;https://issues.apache.org/jira/secure/attachment/12440532/PIG-1313-0.4.0-4.patch,29/Mar/10 20:59;billgraham;PIG-1313-1.patch;https://issues.apache.org/jira/secure/attachment/12440107/PIG-1313-1.patch,29/Mar/10 20:39;billgraham;PIG-1313-1.patch;https://issues.apache.org/jira/secure/attachment/12440101/PIG-1313-1.patch,29/Mar/10 22:02;billgraham;PIG-1313-2.patch;https://issues.apache.org/jira/secure/attachment/12440121/PIG-1313-2.patch,30/Mar/10 21:08;billgraham;PIG-1313-3.patch;https://issues.apache.org/jira/secure/attachment/12440276/PIG-1313-3.patch,01/Apr/10 16:45;billgraham;PIG-1313-4.patch;https://issues.apache.org/jira/secure/attachment/12440524/PIG-1313-4.patch,22/Mar/10 23:45;billgraham;Pig1313Reproducer.java;https://issues.apache.org/jira/secure/attachment/12439527/Pig1313Reproducer.java,,,,8.0,,,,,,,,,,,,,,,,,,,2010-03-22 22:17:18.975,,,no_permission,,,,,,,,,,,,164813,Reviewed,,,Thu Apr 01 22:49:39 UTC 2010,,,,,,,0|i0guhb:,96368,PigServer shutdown() method cleans up after itself better.,,,,,,,,,"22/Mar/10 22:17;daijy;Yes, your observation is valid. When we design PigServer, we do not put long running and multi-thread in mind. We already have [PIG-240|https://issues.apache.org/jira/browse/PIG-240] in an attempt to make PigServer multi-thread. In your case, it seems all you need is to intermittently instantiate PigServer and run script within one java VM. So we need to ensure when the first PigServer is done, we reclaim all the resources local to that particular PigServer instance. And PigServer.shutdown sounds like a reasonable place to do that. 

FileLocalizer.toDelete and FileLocalizer.deleteOnFail are such resources local to PigServer instance. Seems that we should move them into PigServer so that we can clear them safely during shutdown. ","22/Mar/10 23:39;billgraham;Thanks for the pointer to PIG-240. The patch included make it look like that bug is focusing specifically on the LogicalPlan refactor. 

In my case I do need a multi-threaded PigServer instance. My VM is running a scheduler which wakes up and invokes scripts periodically and it is possible that one script will be kicked off while another is already running.  As a result, calling FileLocalizer.deleteTempFiles() can cause problem. This is because the toDelete Stack and others are static.

One possible fix would be to bind a toDelete Stack to each thread local instance, or to make FileLocalizer non-static. The later seems cleaner but would require modifying a lot more code.","22/Mar/10 23:45;billgraham;Attaching {{Pig1313Reproducer.java}}, which is a class that can be used to reproduce the issue. It re-runs a PigServer job in an infinite loop.

It's difficult to expose run time info about the toDelete Stack because it's private but you can run a script like this to repeatedly dump the size/count of the elements on the Stack:

{{while true; do jmap -histo <pid> | grep HFile; sleep 5; done}}

Over time the overall count of HFile objects will increase.","24/Mar/10 00:01;billgraham;Looking more closely at FileLocalizer it does seem that by just making toDelete, deleteOnFail and relativeRoot into ThreadLocals, this issue around multiple threads and temp files could be solved. If there's agreement on this approach, I can take on this issue and contribute a patch. Thoughts?
","24/Mar/10 21:28;alangates;I'm not sure I understand all the pros and cons of Daniel's suggestion of moving the variables to PigServer versus Bill's suggestion of making them ThreadLocal.  The advantages I can see of moving the values to PigServer are:

# It's clearer to other developers what's going on, since they can see that these values are associated with an instance of PigServer.  Otherwise we're constructing a hidden dependency between the lifetime of PigServer and the thread it's running in.
# If at some future point Pig's frontend is multi-threaded this will still work (granted this is unlikely or at least far in the future)

The advantage I see with Bill's proposal is it's less change.

Are there other things I'm missing here?","24/Mar/10 22:29;billgraham;You summed it up well Alan. My ThreadLocal suggestion was really just because we could modify one class internally instead of doing a much larger refactor. 

I'm unclear though on how we'd go about moving FileLocalizer.toDelete and FileLocalizer.deleteOnFail into PigServer? Currently, calls to the FileLocalizer methods that create these temp file objects happen all over the codebase in places where the calling code wouldn't have a handle to their PigServer instance AFAIK. Unless they could get the PigServer from the PigContext or something of the sort. Otherwise, it would need to be a static call to PigServer methods, and we've just moved the same problem to another class.

","25/Mar/10 18:15;daijy;I think ThreadLocal could be a simpler alternative solution for now. In the long term, we should move static FileLocalizer variable into PigServer, since user may not launch different thread for different PigServer, ThreadLocal can not solve all situation. But moving static variables require more code changes. So I am fine to use ThreadLocal for now if it solves Bill's problem. Hi, Bill, can you give a patch on this?","25/Mar/10 18:47;billgraham;Sure, I can work on a patch. Not sure how I'd create a unit test for this though, since it's a change to how private internals are managed. We should be able to assert the fix though with the attached test harness that reproduces the issue.

Can you please assign this JIRA to me, or give me permissions to assign?","25/Mar/10 19:05;daijy;In this case, we can skip the unit test and test it manually, just put a notes on the Jira. Thanks!

I don't have the permission to assign it to you, but you cannot see ""assign to me"" link on your Jira?","25/Mar/10 19:06;alangates;I've added you to the contributors list in JIRA, so you should now be able to assign issues to yourself.","29/Mar/10 20:39;billgraham;Attaching two patches, one built from the trunk and one built from the 0.4.0 branch (since 0.4.0 is what we're using). Not sure if patches to older branches are still being applied or not though. Let me know if I should provide other patches for other branches.

The patch includes the following changes to FileLocalizer:

- toDelete, deleteOnFail and relativeRoot are ThreadLocals.
- The initialized attribute and the init method has been removed, since they're no longer needed.

In PigServer I've done the following:

- FileLocalizer.deleteTempFiles() is called from the shutdown method.

I've run my server with this patch applied for a few days now and the memory consumption has stabilized. I also no longer see exceptions in the logs from when a PigServers temp files would go missing.","29/Mar/10 20:55;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12440102/PIG-1313-0.4.0-1.patch
  against trunk revision 928384.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/255/console

This message is automatically generated.","29/Mar/10 21:20;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12440107/PIG-1313-1.patch
  against trunk revision 928384.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/256/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/256/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/256/console

This message is automatically generated.","29/Mar/10 22:02;billgraham;Previous patch caused issues with dependencies in test code, which I've resolved in PIG-1313-2.patch. Also, the 0.4.0 patch was getting picked up and was causing failures. Holding off on reattaching that patch until tests pass with the trunk patch. ","30/Mar/10 03:07;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12440121/PIG-1313-2.patch
  against trunk revision 928384.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/257/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/257/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/257/console

This message is automatically generated.","31/Mar/10 04:05;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12440276/PIG-1313-3.patch
  against trunk revision 929236.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/272/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/272/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/272/console

This message is automatically generated.",31/Mar/10 19:19;daijy;I will review it.,01/Apr/10 00:48;daijy;+1. This patch faithfully translate static variable into ThreadLocal variable. Will commit it shortly.,"01/Apr/10 04:00;daijy;I read the patch again. Two things I notice:
1. PigServer.shutdown() should be explicitly called user in order to take advantage of it. We need to put some document on it.
2. deleteOnFail is now dangling, no one is calling registerDeleteOnFail/triggerDeleteOnFail. We will address it in a seperate Jira.","01/Apr/10 16:45;billgraham;# Attaching PIG-1313-4.patch with additional Javadocs on PigServer and PigServer.shutdown(). 
# triggerDeleteOnFail is still called by TestMultQueryLocal.executePlan, but you are correct in that no one is (or was) calling registerDeleteOnFail, which is the only entry point to push something onto the deleteOnFail stack.

I will gladly remove deleteOnFail and all calls to it as part of this JIRA, or we can handle it in another one if that's cleaner w.r.t. issue tracking. Let me know. ","01/Apr/10 17:13;daijy;Thanks, Bill,
Let's leave triggerDeleteOnFail. This is the thing we want to fix. I've opened another Jira [PIG-1347|https://issues.apache.org/jira/browse/PIG-1347] for that. This patch is good to go and I will commit it shortly.",01/Apr/10 18:27;billgraham;Here's the same patch for 0.4.0 if anyone wants it.,"01/Apr/10 22:49;daijy;PIG-1313-4.patch committed to trunk. Will come with Pig 0.8 release. This issue is about memory leak and it is hard to write a unit test for it. Bill tested it manually and it works. 

Thanks Bill for contributing!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sort Merge Cogroup,PIG-1309,12459695,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,19/Mar/10 18:46,17/Dec/10 22:43,14/Mar/19 03:07,09/Jul/10 17:49,,,,,,,,0.7.0,0.8.0,,,impl,,,0,,,,,,,,,,,,,"In never ending quest to make Pig go faster, we want to parallelize as many relational operations as possible. Its already possible to do Group-by( PIG-984 ) and Joins( PIG-845 , PIG-554 ) purely in map-side in Pig. This jira is to add map-side implementation of Cogroup in Pig. Details to follow.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Jul/10 19:23;ashutoshc;PIG_1309_7.patch;https://issues.apache.org/jira/secure/attachment/12448592/PIG_1309_7.patch,19/Mar/10 18:48;ashutoshc;mapsideCogrp.patch;https://issues.apache.org/jira/secure/attachment/12439314/mapsideCogrp.patch,30/Mar/10 00:54;ashutoshc;pig-1309_1.patch;https://issues.apache.org/jira/secure/attachment/12440159/pig-1309_1.patch,01/Apr/10 21:45;ashutoshc;pig-1309_2.patch;https://issues.apache.org/jira/secure/attachment/12440542/pig-1309_2.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2010-03-19 21:31:38.766,,,no_permission,,,,,,,,,,,,164809,,,,Fri Sep 03 15:24:33 UTC 2010,,,,,,,0|i0guf3:,96358,"With this patch, it is now possible to perform map-side cogroup if data is sorted and loader implements certain interfaces. Primary algorithm is based on sort-merge join with additional restrictions. 

Following preconditions must be met to use this feature: 
1) No other operations can be done between load and cogroup statements. 
2) Data must be sorted on join keys for all tables in ASC order. 
3) Nulls are considered smaller then everything. So, if data contains null keys, they should occur before anything else. 
4) Left-most loader must implement {CollectableLoader} interface as well as {OrderedLoadFunc}. 
5) All other loaders must implement IndexableLoadFunc. 
6) Type information must be provided in schema for all the loaders.

Note that Zebra loader satisfies all of these conditions, so can be used out of box. 

Similar conditions apply to map-side outer joins (using merge) (PIG-1353) as well. 

Example: 
A = load 'data1' using org.apache.hadoop.zebra.pig.TableLoader('id:int', 'sorted'); 
B = load 'data2' using org.apache.hadoop.zebra.pig.TableLoader('id:int', 'sorted'); 
C = COGROUP A by id, B by id using 'merge'; 
",,,,,,,,,19/Mar/10 18:48;ashutoshc;Preliminary patch to discuss the approach. Not ready for inclusion yet.,19/Mar/10 21:31;alangates;Here's a write up on the design behind this:  http://wiki.apache.org/pig/MapSideCogroup,"19/Mar/10 21:56;alangates;Comments:

A liberal dose of comments would help greatly in understanding what the various helper methods are doing.

You use LocalRearrange to split the keys and values.  What's the overhead of that?  Would it be more efficient to factor the key splitting code out of LR and share it between LR and here?

I don't understand the need for pullTuplesFromSideLoaders().  In setup() you put one tuple from each input into the heap.  Then you pull from the heap until you see a key change.  But I don't understand the next step.  At key change you call pullTuplesFromSideLoaders().  But if you've been adding into the heap as you pull tuples there's no need to pull anything from the side loaders at this point.  All you should need to do is package up the bags you've build and return them as your tuple.

Also, it appears your using pullTuplesFromSideLoaders() to fill the heap.  You shouldn't be pulling all tuples for a current key from side loaders, as you're likely to miss tuples with keys that are in the side loaders but not in the main loader.  The algorithm should be that as you pull a tuple from the heap, you place the next tuple from that same stream into the heap.  The heap will guarantee that your tuples come out in order.
",24/Mar/10 21:40;ashutoshc;Did offline review with Alan. Found a subtle bug in POMergeCogroup#getNext(). Fixed that and added more tests. Still need to tidy up things at few places. Looking for suggestion for better test cases that cover all the edge cases. ,30/Mar/10 00:54;ashutoshc;Getting closer. Running through hudson to find out if it breaks anything.,"30/Mar/10 09:09;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12440159/pig-1309_1.patch
  against trunk revision 928950.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 88 javac compiler warnings (more than the trunk's current 87 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/258/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/258/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/258/console

This message is automatically generated.",30/Mar/10 23:24;alangates;I'm not clear on the need for the typedComparator logic in MapReduceOper.  Can you explain why that's necessary?,"30/Mar/10 23:54;ashutoshc;To build index, we sample every split and get an index entry corresponding to the split. After sampling all the index entries are sorted and then index is written to disk. When I first wrote MergeJoin I wasn't able to figure out how to use hadoop sorting to sort the index. So, there is a comment in MRCompiler for that:
{noformat}
// Sorting of index can possibly be achieved by using Hadoop sorting 
// between map and reduce instead of Pig doing sort. If that is so, 
// it will simplify lot of the code below.
{noformat}
Now I figured it out :) By default, if LocalRearranges produce key of type tuple Pig supplies raw binary comparators (PigTupleWritableComparator) to hadoop to compare tuples, which ignores the semantics of tuple. We need to override that behavior to make Pig supply correct version of tuple comparator (PigTupleRawComparator).  We need to communicate this info to JobControlCompiler from MRCompiler. So, I am doing the same through MapReduceOper object. 

As a nice side-effects of this 
a) code in MRCompiler is indeed simplified now
b) We got rid of extra index sorting inside reducer. ","31/Mar/10 17:27;alangates;Cool.  Thanks for the explanation.  Code looks good, +1.","01/Apr/10 21:45;ashutoshc;Updated the patch to fix test failures, javac warnings and more comments.

Result of test-patch on latest patch:
{noformat}
 [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 9 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec] 
{noformat}

Result of test-commit:
{noformat}
test-commit:
    [mkdir] Created dir: /homes/chauhana/scratch/latest/build/test/logs
    [junit] Running org.apache.pig.test.TestAdd
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.036 sec
	:
	:
    [junit] Running org.apache.pig.test.TestTypeCheckingValidatorNoSchema
    [junit] Tests run: 13, Failures: 0, Errors: 0, Time elapsed: 0.165 sec
BUILD SUCCESSFUL
{noformat}

Patch checked in trunk.
","08/Apr/10 16:46;yanz;Zebra's test case for this feature needs to be added to the 0.7 branch if and when this feature is to be supported therein. I have created a JIRA, PIG-1367,  for tracking this addition should it become necessary. The test case is actually part of the patch for PIG-1315 that is committed as whole to the trunk but committed to the 0.7 branch without that test case.","02/Jul/10 19:23;ashutoshc;Backport of merge cogroup for 0.7 branch. Since, hudson can test only for trunk. Manually ran all the tests, all passed.",07/Jul/10 19:30;rding;+1. Please commit to 0.7 branch.,09/Jul/10 17:49;ashutoshc;Patch checked-in to 0.7 branch as well.,"03/Sep/10 11:38;mridulm@yahoo-inc.com;
Condition (1) refers to only explicit (user specified) statements right ?
Not implicit project introduced by pig to conform to schema ?


Regards,
Mridul



",03/Sep/10 15:24;ashutoshc;Correct. Condition(1) is implied only for user specified statements.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inifinite loop in JobClient when reading from BinStorage Message: [org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 2],PIG-1308,12459595,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pradeepkth,viraj,viraj,18/Mar/10 23:50,14/May/10 06:47,14/Mar/19 03:07,22/Mar/10 18:03,,,,,,,,0.7.0,,,,,,,0,,,,,,,,,,,,,"Simple script fails to read files from BinStorage() and fails to submit jobs to JobTracker. This occurs with trunk and not with Pig 0.6 branch.

{code}
data = load 'binstoragesample' using BinStorage() as (s, m, l);
A = foreach ULT generate   s#'key'         as value;
X = limit A 20;
dump X;
{code}

When this script is submitted to the Jobtracker, we found the following error:
2010-03-18 22:31:22,296 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 2
2010-03-18 22:32:01,574 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 2
2010-03-18 22:32:43,276 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 2
2010-03-18 22:33:21,743 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 2
2010-03-18 22:34:02,004 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 2
2010-03-18 22:34:43,442 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 2
2010-03-18 22:35:25,907 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 2
2010-03-18 22:36:07,402 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 2
2010-03-18 22:36:48,596 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 2
2010-03-18 22:37:28,014 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 2
2010-03-18 22:38:04,823 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 2
2010-03-18 22:38:38,981 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 2
2010-03-18 22:39:12,220 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 2

Stack Trace revelead 

at org.apache.pig.impl.io.ReadToEndLoader.init(ReadToEndLoader.java:144)
        at org.apache.pig.impl.io.ReadToEndLoader.<init>(ReadToEndLoader.java:115)
        at org.apache.pig.builtin.BinStorage.getSchema(BinStorage.java:404)
        at org.apache.pig.impl.logicalLayer.LOLoad.determineSchema(LOLoad.java:167)
        at org.apache.pig.impl.logicalLayer.LOLoad.getProjectionMap(LOLoad.java:263)
        at org.apache.pig.impl.logicalLayer.ProjectionMapCalculator.visit(ProjectionMapCalculator.java:112)
        at org.apache.pig.impl.logicalLayer.LOLoad.visit(LOLoad.java:210)
        at org.apache.pig.impl.logicalLayer.LOLoad.visit(LOLoad.java:52)
        at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:69)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.impl.logicalLayer.optimizer.LogicalTransformer.rebuildProjectionMaps(LogicalTransformer.java:76)
        at org.apache.pig.impl.logicalLayer.optimizer.LogicalOptimizer.optimize(LogicalOptimizer.java:216)
        at org.apache.pig.PigServer.compileLp(PigServer.java:883)
        at org.apache.pig.PigServer.store(PigServer.java:564)

The binstorage data was generated from 2 datasets using limit and union:
{code}
Large1 = load 'input1'  using PigStorage();
Large2 = load 'input2' using PigStorage();
V = limit Large1 10000;
C = limit Large2 10000;
U = union V, C;
store U into 'binstoragesample' using BinStorage();
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Mar/10 06:23;pkamath;PIG-1308.patch;https://issues.apache.org/jira/secure/attachment/12439354/PIG-1308.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-03-20 06:23:37.69,,,no_permission,,,,,,,,,,,,164808,Reviewed,,,Mon Mar 22 18:03:02 UTC 2010,,,,,,,0|i0guen:,96356,,,,,,,,,,"20/Mar/10 06:23;pkamath;The root cause of the issue is that the OpLimitOptimizer has a relaxed check() implementation which only checks if the node matched by RuleMatcher is a LOLimit which would be true any time there is a LOLimit in the plan. This results in the optimizer running 500 (the current max) iterations of all rules since the OpLimitOptimizer always matches.

The attached patch fixes the issue by tightening the implementation of OpLimitOptimizer.check() to return false in cases where LOLimit cannot be pushed up.","21/Mar/10 12:28;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12439354/PIG-1308.patch
  against trunk revision 925513.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 4 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/259/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/259/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/259/console

This message is automatically generated.",22/Mar/10 17:30;daijy;+1,22/Mar/10 18:03;pkamath;Patch committed to trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
when we spill the DefaultDataBag we are not setting the sized changed flag to be true.,PIG-1307,12459566,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,breed,breed,breed,18/Mar/10 20:19,14/May/10 06:47,14/Mar/19 03:07,20/Mar/10 01:13,,,,,,,,0.7.0,,,,,,,0,,,,,,,,,,,,,"pig uses a size changed flag to indicate when we should recalculate the memory footprint of the bag. the setting of this flag is sprinkled throughout the code. unfortunately, it is missing in DefaultDataBag.spill(). there may be other cases as well. the problem with this case is that when the low memory threshold kicks in, bags are spilled until the desired amount of memory is ""freed"". since the flag is not being reset subsequent calls to the threshold events will retrigger the spill() and think more memory was freed even though nothing was actually spilled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Mar/10 20:36;breed;PIG-1307.patch;https://issues.apache.org/jira/secure/attachment/12439206/PIG-1307.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-03-19 02:34:26.736,,,no_permission,,,,,,,,,,,,164807,Reviewed,,,Sat Mar 20 01:13:14 UTC 2010,,,,,,,0|i0gue7:,96354,,,,,,,,,,18/Mar/10 20:36;breed;this patch fixes the problem by using changes in the size of the array to trigger recalculation of memory size rather than the flag. (this simplifies things a bit by removing all the places where we set flags.),"19/Mar/10 02:34;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12439206/PIG-1307.patch
  against trunk revision 924558.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/243/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/243/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/243/console

This message is automatically generated.","19/Mar/10 21:25;daijy;Hi, Ben,
Is the patch ready? Do you need help to add some test cases?",19/Mar/10 22:49;breed;i don't have test cases since the test cases would be orders of magnitude higher more difficult to write than the patch and may not reproduce the problem across different machine configurations.,19/Mar/10 22:56;daijy;+1. Will commit it shortly.,"20/Mar/10 01:13;daijy;Performed a manual test, and it works. Patch committed. Thanks Ben!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document  in Load statement syntax that Pig and underlying M/R does not handle concatenated bz2 and gz files correctly,PIG-1305,12459416,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,viraj,viraj,17/Mar/10 19:01,05/Nov/10 22:25,14/Mar/19 03:07,22/Mar/10 22:39,,,,,,,,0.9.0,,,,documentation,,,0,,,,,,,,,,,,,"The Pig Reference Manual needs to be updated:

Relational Operators

Syntax:

LOAD 'data' [USING function] [AS schema];

'data' 

Please note:
Pig reads in both bz2 and gz formats correctly as long as they are not concatenated gzip or bz2 generated in this manner. cat *.bz2  > text/concat.bz2. Your M/R jobs may succeed but the results will not be accurate.

Viraj",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,164805,,,,2010-03-17 19:01:54.0,,,,,,,0|i0gudb:,96350,"Documentation updated (Pig Latin Ref Manual 2, Load/Store Statements).
Fix will be committed as part of PIG-1320.

Thanks/C",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unable to set outgoing format for org.apache.pig.piggybank.evaluation.util.apachelogparser.DateExtractor,PIG-1303,12459401,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,jrussek,jrussek,17/Mar/10 16:59,14/Feb/11 23:54,14/Mar/19 03:07,03/May/10 19:03,0.6.0,,,,,,,0.7.0,0.8.0,,,,,,0,,,,,,,,,,,,,"I'm unable to set the format of the outgoing date string in the constructor as it's supposed to work. 
The only way i could change the format was to change the default in the java class and rebuild piggybank.
Apparently this has something to do with the way pig instantiates DateExtractor, quoting a replier on the mailing list:

David Vrensk said:

I ran into the same problem a couple of weeks ago, and
played around with the code inserting some print/log statements.  It turns
out that the arguments are only used in the initial constructor calls, when
the pig process is starting, but once pig reaches the point where it would
use the udf, it creates new DateExtractors without passing the arguments.

","pig 0.6.0 on a fedora linux machine, jdk 1.6 u11",,,,,,,,,,PIG-1386,,,,,,,,,,,,,,,,,,,,,15/Apr/10 16:35;jrussek;ASF.LICENSE.NOT.GRANTED--TypeCheckingVisitor.java.diff;https://issues.apache.org/jira/secure/attachment/12441845/ASF.LICENSE.NOT.GRANTED--TypeCheckingVisitor.java.diff,26/Apr/10 08:30;dvryaboy;PIG-1303.patch;https://issues.apache.org/jira/secure/attachment/12442837/PIG-1303.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-04-23 02:35:32.561,,,no_permission,,,,,,,,,,,,41715,,,,Mon May 03 19:03:45 UTC 2010,,,,,,,0|i0gucf:,96346,,,,,,,,,,"15/Apr/10 16:35;jrussek;Hello everybody,
this fixes it for us.
Apparently due to the Typemagic the FuncSpec that is returned by our EvalFunc.getArgToFuncMapping() which doesn't contain the constructor arguments is used instead of the perfectly fine one in ""func"", which contains the constructor arguments.
We've just copied the arguments from func to matchingSpec to make it work.
Johannes","23/Apr/10 02:35;hcbusy;Okay, so, here's a thought:

I'm kind of stuck writing the initial/intermed/Final methods for an algebraic EvalFunc that has constructor parameters because I couldn't pass the parameters in.


A suggestion is to do this (without being incompatible with previous versions)

Alter EvalFunc's profile so that

{code}
public abstract class EvalFunc<T>  {

   protected handleChildConstructorParameters(Object... childConstructor){
      // by default do nothing.
   }

    public EvalFunc(Object... constructorParameters){
        handleChildConstructorParameters(constructorParameters);
        ... then do everything else it used to do.
    }
}
{code}


The reason why this is necessary is because I'll need to overrite handleChildConstructorParameters in my Algebraic EvalFunc to do some things before the rest of EvalFunc()'s constructor continues. This will help fix this date format problem for Algebraic evalfunc's.


","23/Apr/10 06:14;dvryaboy;hc,
I haven't gone through the code, but I think we should just try to do the same thing for initial, intermediate, and final functions, in terms of instantiation, as is already being done for the regular EvalFunc, and construct a better FuncSpec. I doubt we need the handleChildConstructorParams function.","23/Apr/10 16:04;hcbusy;But the problem is that inside the EvalFunc constructor, in case of Algebraic classes, it constructs each of Initial, Intermediate and final which are EvalFunc's that, in my case, require a parameter to operate correctly.

If I declare the helper class that represent the initial/intermediate/final 


{code}
	public class HelperClass extends EvalFunc<Tuple> {
		public HelperClass() {
			super();
		}

		public Tuple exec(Tuple input) throws IOException {
			return extreme(fieldIndex, sign, input, reporter);
		}

	}
{code}

where the fieldIndex and sign come from the surrounding class (note the class is not static) then the code crashes. It's not able to construct the HelperClass with this error

{quote}
could not instantiate 'org.apache.pig.piggybank.evaluation.ExtremalTupleByNthField$HelperClass' with arguments 'null'
java.lang.RuntimeException: could not instantiate 'org.apache.pig.piggybank.evaluation.ExtremalTupleByNthField$HelperClass' with arguments 'null'
        at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:498)
        at org.apache.pig.EvalFunc.getReturnTypeFromSpec(EvalFunc.java:136)
        at org.apache.pig.EvalFunc.<init>(EvalFunc.java:123)
        at org.apache.pig.piggybank.evaluation.ExtremalTupleByNthField.<init>(ExtremalTupleByNthField.java:77)
        at org.apache.pig.piggybank.evaluation.TestExtremalTupleByNthField.testMin(Unknown Source)
Caused by: java.lang.InstantiationException: org.apache.pig.piggybank.evaluation.ExtremalTupleByNthField$HelperClass
        at java.lang.Class.newInstance0(Class.java:340)
        at java.lang.Class.newInstance(Class.java:308)
        at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:468)
{quote}

Basically, I think it's not able to construct because the class can only be constructed from an instance of ExtremalTupleByNthField.
{code}
        ExtremalTupleByNthField etbnf = new ExtremalTupleByNthField(""1"",""max"");
        etbnf.new ExtremalTupleByNthField.HelperClass();
{code}

So my solution to this problem was to make this class static. But make it so that EvalFunc can take a vararg that will eventually contain the actual parameters.

the handleChildConstructorParameters method in the EvalFunc will construct a string that represents the call into the initial/intermediate/final methods but it contains parameters that came from the ExtremalTupleByNthField.","23/Apr/10 16:53;hcbusy;Hmm, okay, so let me shorten my problem. Basically the functions 

getInitial, getIntermed, and getFinal in my Algebraic class doesn't have access to the constructor parameters. The reason is this. in Java, the super() constructor can only be called as the very first thing that the deriving class's constructor does, so my udfs has constructors that look like this:


{code}
	 public ExtremalTupleByNthField(String fieldIndexString, String order) {
		super();
                parameters = ""('""+fieldIndexString+""','""+order+""'"";
         }
       @Override
	public String getInitial() {
		return HelperClass.class.getName()+parameters;
	}
{code}

But the problem is EvalFunc() constructor calls the child class's getInitial() to type check. When it does this, it finds that my getInitial() returns something in complete because the ""parameters"" member variable hasn't been initialized yet. This is a pretty mundane problem with java programs and the way to fix it is what I've submitted in the patch calling an overridden method in the super()'s constructor.

I mean, I don't see any other way to do this, but I'd be willing to work on another implementation if you can suggest one?

","23/Apr/10 17:06;dvryaboy;Right, I'm with you on what the problem is.

My suggestion is to look at the FuncSpec of the parent EvalFunc, and if it includes parameters, assume an equivalent FuncSpec should be created for initial, intermediate, and final classes. Essentially do the same thing we do for regular EvalFuncs when they are DEFINEd to use parameterized constructors.

You would then have to make sure that initial, intermediate, and final classes have the required constructors, but that's what you'd expect to have to do anyway.","23/Apr/10 17:28;jrussek;I'm sorry, but is this really the problem with the DateExtractor UDF?
what i've seen during debugging is that it implements getArgToFuncMapping which returns a funcspec without the constructor arguments. I've tried to look into it but i didn't find an easy way to figure the initial constructor arguments out when getArgToFuncMapping is being called.
also, DateExtractor isn't even algebraic. i'm not sure, but what's wrong with keeping the constructor arguments from the first instantiation and apply them to any further ones, may it be algebraic child classes or pig type-specific implementations?
","23/Apr/10 17:38;dvryaboy;Johannes, it's not, it's just a related root cause (inconsistent use of UDF constructors)
Keeping constructor arguments and applying them to all instantiations is exactly what I am advocating.
","23/Apr/10 17:43;jrussek;alright, i got confused :)
+1 then!",26/Apr/10 08:31;dvryaboy;I think this qualifies as a bug fix and should be applied to 0.7 as well as going forward into 0.8,"26/Apr/10 08:36;dvryaboy;Attached a patch for both regular UDFs and Algebraics. It's essentially what Johannes posted, I just added a test (but see the rest of this comment), and applied the same logic to Algebraic EvalFunc construction.

The test only really tests algebraic instantiation; I haven't been successful in reproducing the normal instantiation problem in test mode (which is probably why it's gone undetected so far). There's a test for it in my patch, but it's a bad one -- the test actually passes even without the patch to the TypeCheckingVisitor.  

Johannes, could you try applying this patch and let us know if it fixes your DateExtractor problem?

hc, this patch should unblock you for PIG-1386","26/Apr/10 13:46;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12442837/PIG-1303.patch
  against trunk revision 937570.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/302/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/302/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/302/console

This message is automatically generated.","27/Apr/10 05:35;hcbusy;+(google^2)

that worked!","27/Apr/10 16:12;alangates;Dmitry, I'll try to get to reviewing this patch today.","28/Apr/10 01:04;alangates;Sorry, I didn't make it to reviewing this today.  I'll put it at the top of tomorrow's list.  

On the 0.7 question, I'm open to that as long as we test it really well.","28/Apr/10 12:38;jrussek;Dmitriy, i should be able to test your patch on my issues by tomorrow.
",28/Apr/10 22:58;alangates;In general code looks good.  I think we should add a test for a function that uses the argsToFuncMapping mechanism since that's a totally separate code path from algebraic.,"28/Apr/10 23:15;dvryaboy;Agreed - but as you can see, I added a test like that (testRegularInstantiation), except it always passes, even without the patch. Even though theoretically it shouldn't. Can you suggest a legitimate way of testing this, barring an actual cluster test? Johannes, maybe you have a testcase that's reproducible when using the MiniCluster?","01/May/10 00:18;alangates;I think the testing you've done is probably adequate.  We're looking at trying to start the release process for 0.7 next week, so let's get this checked in so it can make it.","03/May/10 19:03;dvryaboy;Committed to 0.7 and trunk. Thanks for the help, Johannes!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problem pruning columns with UDF,PIG-1301,12459271,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,agroh,agroh,16/Mar/10 15:06,14/May/10 06:47,14/Mar/19 03:07,16/Mar/10 19:35,0.6.0,,,,,,,0.7.0,,,,impl,,,0,,,,,,,,,,,,,"I just upgraded to pig 0.6.0.

I have a pig file like
raw = load 'foo.csv' using PigStorage() as (field1:chararray, field2:chararray);

define contains com.mycompany.pig.Contains();

rawactions = foreach raw generate contains(field1, field2) as junk,  field1;

reqcnt = foreach rawactions generate field1;

dump reqcnt

When I try to run this code, I get an error:
Problem with input: (Name: Project 1-40 Projections: [1] Overloaded: false Operator Key: 1-40) of User-defined function: (Name: UserFunc 1-39 function: com.mycompany.pig.Contains Operator Key: 1-39)
Thrown from line 98 of LOUserFunction.java

This was caused by another FrontEndException 
Attempt to access field: 1 from schema: {field1: chararray}
from Schema.java

I also investigated changing the pig code
if you change
rawactions = foreach raw generate contains(field1, field2) as junk,  field1;

to either
rawactions = foreach raw generate contains(field2, field2) as junk,  field1;
or
rawactions = foreach raw generate contains(field2, field2) as junk,  field1;

or if you change
reqcnt = foreach rawactions generate field1;
to
reqcnt = foreach rawactions generate field1, junk;

It all works correctly.

The problem appears to be that it prunes out field2, but then gets confused and does not prune out the plan associated with the UDF contains, since field1 is not pruned.  So if the UDF only references field2 it will get removed, if it only references field1 the field will have not been pruned and it can run.

I eventually tracked this down to the code around 947 of LOForEach.java
            for (LOProject loProject : projectFinder.getProjectSet()) {
                Pair<Integer, Integer> pair = new Pair<Integer, Integer>(0,
                        loProject.getCol());
                if (!columns.contains(pair)) {
                    allPruned = false;
                    break;
                }
            }
            if (allPruned) {
                planToRemove.add(i);
            }

In the example pig, allPruned is false for the plan associated the UDF.  This is because field1 is both a column for the UDF and for the ForEach in general.  Since field1 is not pruned, the plan is not removed and bad things happen later.

I don't really understand the pruning code all that well, so I don't have a fix for it.  I hope that it will be clear to someone who understands this code better.  I can provide a better test case for this if necessary.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-03-16 19:34:43.775,,,no_permission,,,,,,,,,,,,164803,,,,Tue Mar 16 19:34:43 UTC 2010,,,,,,,0|i0gubb:,96341,,,,,,,,,,"16/Mar/10 19:34;daijy;Thanks for reporting. I tried the script on trunk. Seems on trunk we have fixed that. The code you mentioned do have the problem you mentioned. But in the trunk, we already changed the code to:

{code}
boolean anyPruned = false;
for (LOProject loProject : projectFinder.getProjectSet()) {
    Pair<Integer, Integer> pair = new Pair<Integer, Integer>(0, loProject.getCol());
    if (columns.contains(pair)) {
        anyPruned = true;
        break;
    }
}
{code}

The fix will come with next Pig release. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigStorage does not load tuples with large #s.,PIG-1300,12459222,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,briantd,briantd,16/Mar/10 00:41,14/May/10 06:47,14/Mar/19 03:07,16/Mar/10 01:15,,,,,,,,0.7.0,,,,data,,,0,,,,,,,,,,,,,"Say I have a file 'a' with the following entry:
(30010401402)

grunt> A = LOAD 'a' AS (t:tuple(a:chararray));
grunt> DUMP A;
2010-03-15 17:37:23,333 [main] WARN  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigHadoopLogger - org.apache.pig.builtin.PigStorage: Unable to interpret value [B@353c375 in field being converted to type tuple, caught Exception <For input string: ""30010401402""> field discarded
2010-03-15 17:37:23,335 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Successfully stored result in: ""file:/tmp/temp-1345435162/tmp-308780808""
2010-03-15 17:37:23,335 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Records written : 1
2010-03-15 17:37:23,335 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Bytes written : 0
2010-03-15 17:37:23,335 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - 100% complete!
2010-03-15 17:37:23,336 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Success!!
()

If I have another file 'b' with the following entry:
(30010401402L)

grunt> B = LOAD 'b' AS (t:tuple(a:chararray));
grunt> DUMP B;
2010-03-15 17:39:10,051 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Successfully stored result in: ""file:/tmp/temp-1630850555/tmp1316256240""
2010-03-15 17:39:10,051 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Records written : 1
2010-03-15 17:39:10,051 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Bytes written : 0
2010-03-15 17:39:10,051 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - 100% complete!
2010-03-15 17:39:10,052 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Success!!
((30010401402L))

Is there a way to get the load in the first example to work?  Or do I need to start affixing an L to all my #s? 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-03-16 00:46:34.418,,,no_permission,,,,,,,,,,,,164802,,,,Tue Mar 16 01:14:53 UTC 2010,,,,,,,0|i0guav:,96339,,,,,,,,,,16/Mar/10 00:46;daijy;Which version of Pig are you using? Can you try it on trunk? Looks like it should be fixed in PIG-613.,"16/Mar/10 01:05;briantd;This is with version 0.5+11.1 (cloudera), and with the recently released 0.6.","16/Mar/10 01:14;daijy;I just tried, it works in trunk. The fix will come with next release (0.7).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement Pig counter  to track number of output rows for each output files ,PIG-1299,12459198,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,15/Mar/10 22:12,17/Dec/10 22:43,14/Mar/19 03:07,08/Apr/10 22:54,0.6.0,,,,,,,0.8.0,,,,,,,0,,,,,,,,,,,,,"When running a multi-store query, the Hadoop job tracker often displays only 0 for ""Reduce output records"" or ""Map output records"" counters, This is incorrect and misleading. Pig should implement an ""output records"" counter for each output files in the query. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Apr/10 19:09;rding;PIG-1299.patch;https://issues.apache.org/jira/secure/attachment/12441073/PIG-1299.patch,25/Mar/10 17:31;rding;PIG-1299.patch;https://issues.apache.org/jira/secure/attachment/12439800/PIG-1299.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-03-26 21:36:11.716,,,no_permission,,,,,,,,,,,,164801,Reviewed,,,Thu Apr 08 22:52:11 UTC 2010,,,,,,,0|i0guaf:,96337,,,,,,,,,,"25/Mar/10 17:31;rding;This patch adds a Hadoop counter group: MultiStoreCounters. In the case of a multi-store job, this counter group displays (in the job tracker admin UI) the number of output records to each store in the job. It also log the information in the pig log.


","26/Mar/10 21:36;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12439800/PIG-1299.patch
  against trunk revision 927640.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 89 javac compiler warnings (more than the trunk's current 87 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/250/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/250/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/250/console

This message is automatically generated.","06/Apr/10 21:23;pkamath;Changes are mostly good - a few comments:
1) Instead of creating a wrapper RecordWriter in MapReducePOStoreImpl, the incrementing of the counter should be done in POStore.getNext() - POStore holds a reference to MapReducePOStoreImpl, so the counter is available for incrementing. This way, we will still keep our contract to StoreFunc that the RecordWriter instance provided in prepareToWrite() is the same as the one given by StoreFunc.getOutputFormat().getRecordWriter(). With this change, the change to BinStorage should be reverted.
2) Is the check for store.isMultiStore() required in MapReducePOStoreImpl - I think MapReducePOStoreImpl is used only with multi-store POStore(s) - so the check seems redundant
3) If javac warnings can be addressed, please address them - also unit tests along the lines of those in TestCounters would be good.","07/Apr/10 19:09;rding;Thanks Pradeep.  The new patch addresses the comments.

The patch adds a new Hadoop counter group--MultiStoreCounters--that counts the numbers of output records in each store of a MultiQuery script. ",08/Apr/10 21:15;pkamath;+1,"08/Apr/10 21:42;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12441073/PIG-1299.patch
  against trunk revision 931986.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 88 javac compiler warnings (more than the trunk's current 87 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/278/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/278/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/278/console

This message is automatically generated.",08/Apr/10 22:52;rding;The test failure was caused by hudson environment. I run failed tests manually and they all passed. This patch does add one javac warning because it imports a deprecated Hadoop class (Counters). ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Restore file traversal behavior to Pig loaders,PIG-1298,12459194,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,15/Mar/10 21:57,14/May/10 06:47,14/Mar/19 03:07,20/Mar/10 00:44,0.7.0,,,,,,,0.7.0,,,,,,,0,,,,,,,,,,,,,"Given a location to a Pig loader, it is expected to recursively load all the files under the location (i.e., all the files returned with  ""ls -R"" command). However, after the transition to using Hadoop 20 API,  only files returned with ""ls"" command are loaded.

 

  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Mar/10 21:28;rding;PIG-1298.patch;https://issues.apache.org/jira/secure/attachment/12439212/PIG-1298.patch,19/Mar/10 17:13;rding;PIG-1298_1.patch;https://issues.apache.org/jira/secure/attachment/12439294/PIG-1298_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-03-19 02:41:38.126,,,no_permission,,,,,,,,,,,,164800,Reviewed,,,Fri Mar 19 22:47:29 UTC 2010,,,,,,,0|i0gu9z:,96335,,,,,,,,,,"18/Mar/10 21:28;rding;This patch added PigFileInputFormat, PigTextInputFormat and PigSequenceFileInputFormat to support multi-level/recursive file listing for the loaders. This is a stopgap until MAPREDUCE-1577 is fixed.

As a consequence, custom loaders (existing or new) that use Hadoop text-based InputFormat and would like to support this feature should use the corresponding Pig InputFormat.","19/Mar/10 02:41;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12439212/PIG-1298.patch
  against trunk revision 924558.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 9 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 523 release audit warnings (more than the trunk's current 522 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/255/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/255/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/255/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/255/console

This message is automatically generated.",19/Mar/10 17:13;rding;Fix release audit issue.,19/Mar/10 22:47;ashutoshc;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skewed join fail due to negative partition index,PIG-1296,12459172,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,15/Mar/10 17:24,14/May/10 06:47,14/Mar/19 03:07,18/Mar/10 21:51,0.7.0,,,,,,,0.7.0,,,,impl,,,0,,,,,,,,,,,,,"Skewed join throw stack:

java.io.IOException: Illegal partition for Partition: -1 Null: false index: 0 (fc52di95l6m3j,20100210) (-3648)
            at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:904)
            at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:541)
            at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
            at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$MapWithPartitionIndex.collect(PigMapReduce.java:187)
            at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$MapWithPartitionIndex.runPipeline(PigMapReduce.java:206)
            at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:227)
            at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:52)
            at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
            at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
            at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
            at org.apache.hadoop.mapred.Child.main(Child.java:159)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Mar/10 17:31;daijy;PIG-1296-1.patch;https://issues.apache.org/jira/secure/attachment/12438842/PIG-1296-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-03-15 23:01:40.145,,,no_permission,,,,,,,,,,,,164799,Reviewed,,,Thu Mar 18 21:51:12 UTC 2010,,,,,,,0|i0gu9b:,96332,,,,,,,,,,"15/Mar/10 17:30;daijy;In SkewedPartitioner.java, we have:

return (Math.abs(keyTuple.hashCode()) % totalReducers);

In javadoc, we have the note:
Math.abs(int): if the argument is equal to the value of Integer.MIN_VALUE, the most negative representable int value, the result is that same value, which is negative.

We hit exactly the same situation in this case.","15/Mar/10 23:01;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12438842/PIG-1296-1.patch
  against trunk revision 923043.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/238/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/238/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/238/console

This message is automatically generated.",18/Mar/10 21:08;daijy;Unit test is hard to construct. We manually tested the failing script and it works.,"18/Mar/10 21:47;alangates;+1

As a random side note, I can't believe Math.abs is set to return a negative value in some cases.  That's insane.",18/Mar/10 21:51;daijy;Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
