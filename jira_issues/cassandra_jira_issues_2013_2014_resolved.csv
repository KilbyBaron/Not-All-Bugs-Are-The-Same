Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Log Work,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Outward issue link (Duplicate),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Regression),Outward issue link (Required),Outward issue link (Required),Outward issue link (Supercedes),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reviewer),Custom field (Reviewers),Custom field (Severity),Custom field (Severity),Custom field (Since Version),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
SSL problems with inter-DC communication,CASSANDRA-5391,12639310,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,yukim,ondrej.cernos,ondrej.cernos,27/Mar/13 13:43,12/Mar/19 14:20,13/Mar/19 22:28,04/Apr/13 00:33,1.2.4,,,,,,0,,,,,"I get SSL and snappy compression errors in multiple datacenter setup.

The setup is simple: 3 nodes in AWS east, 3 nodes in Rackspace. I use slightly modified Ec2MultiRegionSnitch in Rackspace (I just added a regex able to parse the Rackspace/Openstack availability zone which happens to be in unusual format).

During {{nodetool rebuild}} tests I managed to (consistently) trigger the following error:

{noformat}
2013-03-19 12:42:16.059+0100 [Thread-13] [DEBUG] IncomingTcpConnection.java(79) org.apache.cassandra.net.IncomingTcpConnection: IOException reading from socket; closing
java.io.IOException: FAILED_TO_UNCOMPRESS(5)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:78)
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
	at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:391)
	at org.apache.cassandra.io.compress.SnappyCompressor.uncompress(SnappyCompressor.java:93)
	at org.apache.cassandra.streaming.compress.CompressedInputStream.decompress(CompressedInputStream.java:101)
	at org.apache.cassandra.streaming.compress.CompressedInputStream.read(CompressedInputStream.java:79)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at org.apache.cassandra.utils.BytesReadTracker.readUnsignedShort(BytesReadTracker.java:140)
	at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:361)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:371)
	at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:160)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:226)
	at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:166)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:66)
{noformat}

The exception is raised during DB file download. What is strange is the following:

* the exception is raised only when rebuildig from AWS into Rackspace
* the exception is raised only when all nodes are up and running in AWS (all 3). In other words, if I bootstrap from one or two nodes in AWS, the command succeeds.

Packet-level inspection revealed malformed packets _on both ends of communication_ (the packet is considered malformed on the machine it originates on).

Further investigation raised two more concerns:

* We managed to get another stacktrace when testing the scenario. The exception was raised only once during the tests and was raised when I throttled the inter-datacenter bandwidth to 1Mbps.

{noformat}
java.lang.RuntimeException: javax.net.ssl.SSLException: bad record MAC
	at com.google.common.base.Throwables.propagate(Throwables.java:160)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
	at java.lang.Thread.run(Thread.java:662)
Caused by: javax.net.ssl.SSLException: bad record MAC
	at com.sun.net.ssl.internal.ssl.Alerts.getSSLException(Alerts.java:190)
	at com.sun.net.ssl.internal.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1649)
	at com.sun.net.ssl.internal.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1607)
	at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:859)
	at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:755)
	at com.sun.net.ssl.internal.ssl.AppInputStream.read(AppInputStream.java:75)
	at org.apache.cassandra.streaming.compress.CompressedInputStream$Reader.runMayThrow(CompressedInputStream.java:151)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	... 1 more
{noformat}

This is pure SSL error with no snappy interference.

* I managed to trigger the exception during {{nodetool repair}} tests when replacing dead node with a new one _on the aws side_, which means the problem is not restricted to the one-way scenario only.

{noformat}
2013-03-27 14:06:03.033+0100 [Thread-137] [INFO] StreamInSession.java(136) org.apache.cassandra.streaming.StreamInSession: Streaming of file /path/to/cassandra/data/ks/cf/ks-cf-ib-2-Data.db sections=3 progress=0/20513 - 0% for org.apache.cassandra.streaming.StreamInSession@14450ae7 failed: requesting a retry.
2013-03-27 14:06:03.033+0100 [Thread-138] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-98-Data.db
2013-03-27 14:06:03.033+0100 [Thread-138] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-98-Filter.db
2013-03-27 14:06:03.034+0100 [Thread-138] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-98-TOC.txt
2013-03-27 14:06:03.034+0100 [Thread-137] [DEBUG] IncomingTcpConnection.java(91) org.apache.cassandra.net.IncomingTcpConnection: IOException reading from socket; closing
java.io.IOException: FAILED_TO_UNCOMPRESS(5)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:78)
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
	at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:391)
	at org.apache.cassandra.io.compress.SnappyCompressor.uncompress(SnappyCompressor.java:93)
	at org.apache.cassandra.streaming.compress.CompressedInputStream.decompress(CompressedInputStream.java:101)
	at org.apache.cassandra.streaming.compress.CompressedInputStream.read(CompressedInputStream.java:79)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:320)
	at org.apache.cassandra.utils.BytesReadTracker.readUnsignedShort(BytesReadTracker.java:140)
	at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:361)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:371)
	at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:160)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:238)
	at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:178)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:78)
{noformat}","$ /etc/alternatives/jre_1.6.0/bin/java -version
java version ""1.6.0_23""
Java(TM) SE Runtime Environment (build 1.6.0_23-b05)
Java HotSpot(TM) 64-Bit Server VM (build 19.0-b09, mixed mode)
$ uname -a
Linux hostname 2.6.32-358.2.1.el6.x86_64 #1 SMP Tue Mar 12 14:18:09 CDT 2013 x86_64 x86_64 x86_64 GNU/Linux
$ cat /etc/redhat-release 
Scientific Linux release 6.3 (Carbon)
$ facter | grep ec2
...
ec2_placement => availability_zone=us-east-1d
...
$ rpm -qi cassandra
cassandra-1.2.3-1.el6.cmp1.noarch
(custom built rpm from cassandra tarball distribution)",,,,,,,,,,,,,CASSANDRA-5390,,,,,,,,,,03/Apr/13 13:15;yukim;5391-1.2.3.txt;https://issues.apache.org/jira/secure/attachment/12576777/5391-1.2.3.txt,29/Mar/13 03:34;yukim;5391-1.2.txt;https://issues.apache.org/jira/secure/attachment/12576020/5391-1.2.txt,03/Apr/13 13:15;yukim;5391-v2-1.2.txt;https://issues.apache.org/jira/secure/attachment/12576778/5391-v2-1.2.txt,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-03-27 16:29:13.577,,,no_permission,,,,,,,,,,,,319780,,,Mon Jul 01 23:14:59 UTC 2013,,,,,,0|i1j6cf:,320121,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,27/Mar/13 14:23;ondrej.cernos;With SSL switched off all the scenarios work well.,"27/Mar/13 15:24;ondrej.cernos;After clarifying CASSANDRA-5390 I tried to switch to {{DeflateCompressor}} SSTable compression algorithm. The problem is compression-algorithm independent:

{noformat}
2013-03-27 16:19:57.633+0100 [Thread-31] [INFO] StreamInSession.java(136) org.apache.cassandra.streaming.StreamInSession: component=c4 Streaming of file /mnt/ebs/cassandra/data/c4/user_profile_settings/c4-user_profile_settings-ib-2-Data.db sections=130 progress=0/1628502 - 0% for org.apache.cassandra.streaming.StreamInSession@20f92649 failed: requesting a retry.
2013-03-27 16:19:57.633+0100 [Thread-31] [DEBUG] IncomingTcpConnection.java(91) org.apache.cassandra.net.IncomingTcpConnection: component=c4 IOException reading from socket; closing
java.io.IOException: CRC unmatched
	at org.apache.cassandra.streaming.compress.CompressedInputStream.decompress(CompressedInputStream.java:111)
	at org.apache.cassandra.streaming.compress.CompressedInputStream.read(CompressedInputStream.java:79)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:320)
	at org.apache.cassandra.utils.BytesReadTracker.readUnsignedShort(BytesReadTracker.java:140)
	at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:361)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:371)
	at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:160)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:238)
	at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:178)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:78)
{noformat}","27/Mar/13 16:19;ondrej.cernos;Update:

With SSTable compression switched off the bug disappears. When I run nodetool rebuild us-east on a Rackspace node, it fetches the data correctly and when I compare the md5 of the DB file on an AWS node (after flush and compaction), it is exactly the same as on the Rackspace node.
It means the problem is only with compressed SSTables, but the problem is independent on chosen compression algorithm. And only with SSL switched on for inter-DC communication.","27/Mar/13 16:29;jbellis;Can you shed any light, Jake?","27/Mar/13 16:49;tjake;Nope, we don't use SSL.  Does it work when you disable internode compression?","27/Mar/13 17:03;ondrej.cernos;Internode compression settings didn't have any influence on the problem. The case is very strange:

* it happens only when SSTables are compressed (see the update above)
* it is independent on SSTable compression implementation however (also see above)
* it happens only when ""enough"" (all 3) nodes are switched on in AWS. With 2 or 1 only the problem disappears

Do you have a recommendation on what to investigate further? I already asked the network team to check networking - they say all is ok - and our operations, who also cannot identify anything, except for the fact MTU is different in Rackspace and AWS, so packet from AWS to Rackspace get fragmented.","28/Mar/13 14:44;ondrej.cernos;I am becoming quite sure the problem is a race condition in Cassandra code handling decompression of sstables when these are streamed from the remote datacenter.

Both traces - when snappy is used and when the java zip is used - share the same calls, see above.

I switched trace level in log4j and this is what I found:

* when 2 and more nodes live in the remote DC, cassandra fires two threads downloading the same file
* when only 1 node lives in the remote DC, only one thread downloads the file

This is how it looks in log:

{noformat}
2013-03-28 13:44:57.301+0100 [Thread-22] [DEBUG] StreamInSession.java(104) org.apache.cassandra.streaming.StreamInSession: Adding file /path/to/cassandra/data/ks/cf/ks-cf-ib-2-Data.db to Stream Request queue
2013-03-28 13:44:57.301+0100 [Thread-22] [DEBUG] StreamInSession.java(104) org.apache.cassandra.streaming.StreamInSession: Adding file /path/to/cassandra/data/ks/cf/ks-cf-ib-1-Data.db to Stream Request queue
2013-03-28 13:44:57.338+0100 [Thread-23] [DEBUG] StreamInSession.java(104) org.apache.cassandra.streaming.StreamInSession: Adding file /path/to/cassandra/data/ks/cf/ks-cf-ib-2-Data.db to Stream Request queue
2013-03-28 13:44:57.340+0100 [Thread-23] [DEBUG] StreamInSession.java(104) org.apache.cassandra.streaming.StreamInSession: Adding file /path/to/cassandra/data/ks/cf/ks-cf-ib-1-Data.db to Stream Request queue
{noformat}

And here is the result grepped on the two threads:

{noformat}
2013-03-28 13:44:57.477+0100 [Thread-22] [TRACE] SSTableWriter.java(145) org.apache.cassandra.io.sstable.SSTableWriter: wrote DecoratedKey(-8516046549581000893, 6663363133663230623932663663303732623735653332643964616261623165) at 183591
2013-03-28 13:44:57.477+0100 [Thread-22] [TRACE] SSTableWriter.java(463) org.apache.cassandra.io.sstable.SSTableWriter: wrote index entry: org.apache.cassandra.db.RowIndexEntry@7b553d18 at 16192
2013-03-28 13:44:57.477+0100 [Thread-22] [TRACE] SSTableWriter.java(145) org.apache.cassandra.io.sstable.SSTableWriter: wrote DecoratedKey(-8513551951874950453, 3934363831326161323235653165613662613039346233356264386461653735) at 183995
2013-03-28 13:44:57.478+0100 [Thread-22] [TRACE] SSTableWriter.java(463) org.apache.cassandra.io.sstable.SSTableWriter: wrote index entry: org.apache.cassandra.db.RowIndexEntry@d5f0688 at 16238
2013-03-28 13:44:57.501+0100 [Thread-22] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-1-Data.db
2013-03-28 13:44:57.501+0100 [Thread-22] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-1-Filter.db
2013-03-28 13:44:57.501+0100 [Thread-22] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-1-TOC.txt
2013-03-28 13:44:57.501+0100 [Thread-22] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-1-CompressionInfo.db
2013-03-28 13:44:57.502+0100 [Thread-22] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-1-Index.db
2013-03-28 13:44:57.502+0100 [Thread-22] [DEBUG] SSTable.java(154) org.apache.cassandra.io.sstable.SSTable: Deleted /path/to/cassandra/data/ks/cf/ks-cf-tmp-ib-1
2013-03-28 13:44:57.503+0100 [Thread-22] [INFO] StreamInSession.java(136) org.apache.cassandra.streaming.StreamInSession: Streaming of file /path/to/cassandra/data/ks/cf/ks-cf-ib-2-Data.db sections=130 progress=67628/1583497 - 4% for org.apache.cassandra.streaming.StreamInSession@21400eb0 failed: requesting a retry.
2013-03-28 13:44:57.504+0100 [Thread-22] [DEBUG] IncomingTcpConnection.java(91) org.apache.cassandra.net.IncomingTcpConnection: IOException reading from socket; closing
java.io.IOException: CRC unmatched
        at org.apache.cassandra.streaming.compress.CompressedInputStream.decompress(CompressedInputStream.java:111)
        at org.apache.cassandra.streaming.compress.CompressedInputStream.read(CompressedInputStream.java:79)
        at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:320)
        at org.apache.cassandra.utils.BytesReadTracker.readUnsignedShort(BytesReadTracker.java:140)
        at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:361)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:371)
        at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:160)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:238)
        at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:178)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:78)

2013-03-28 13:44:58.070+0100 [Thread-23] [TRACE] SSTableWriter.java(463) org.apache.cassandra.io.sstable.SSTableWriter: wrote index entry: org.apache.cassandra.db.RowIndexEntry@db766c1 at 106582
2013-03-28 13:44:58.071+0100 [Thread-23] [TRACE] SSTableWriter.java(145) org.apache.cassandra.io.sstable.SSTableWriter: wrote DecoratedKey(-4829320003365722996, 6333383266393230353964313666633136356335333437353637373735653065) at 1217942
2013-03-28 13:44:58.071+0100 [Thread-23] [TRACE] SSTableWriter.java(463) org.apache.cassandra.io.sstable.SSTableWriter: wrote index entry: org.apache.cassandra.db.RowIndexEntry@3bb0ff0 at 106628
2013-03-28 13:44:58.071+0100 [Thread-23] [TRACE] SSTableWriter.java(145) org.apache.cassandra.io.sstable.SSTableWriter: wrote DecoratedKey(-4827623571007838156, 6162376162333238393739643930336266616566393039376131366238386166) at 1218191
2013-03-28 13:44:58.071+0100 [Thread-23] [TRACE] SSTableWriter.java(463) org.apache.cassandra.io.sstable.SSTableWriter: wrote index entry: org.apache.cassandra.db.RowIndexEntry@6e135779 at 106674
2013-03-28 13:44:58.091+0100 [Thread-23] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-2-Data.db
2013-03-28 13:44:58.091+0100 [Thread-23] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-2-Filter.db
2013-03-28 13:44:58.091+0100 [Thread-23] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-2-TOC.txt
2013-03-28 13:44:58.091+0100 [Thread-23] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-2-CompressionInfo.db
2013-03-28 13:44:58.091+0100 [Thread-23] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-2-Index.db
2013-03-28 13:44:58.091+0100 [Thread-23] [DEBUG] SSTable.java(154) org.apache.cassandra.io.sstable.SSTable: Deleted /path/to/cassandra/data/ks/cf/ks-cf-tmp-ib-2
2013-03-28 13:44:58.091+0100 [Thread-23] [INFO] StreamInSession.java(136) org.apache.cassandra.streaming.StreamInSession: Streaming of file /path/to/cassandra/data/ks/cf/ks-cf-ib-2-Data.db sections=131 progress=406399/1638227 - 24% for org.apache.cassandra.streaming.StreamInSession@37d40164 failed: requesting a retry.
2013-03-28 13:44:58.092+0100 [Thread-23] [DEBUG] IncomingTcpConnection.java(91) org.apache.cassandra.net.IncomingTcpConnection: IOException reading from socket; closing
java.io.IOException: CRC unmatched
        at org.apache.cassandra.streaming.compress.CompressedInputStream.decompress(CompressedInputStream.java:111)
        at org.apache.cassandra.streaming.compress.CompressedInputStream.read(CompressedInputStream.java:79)
        at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:320)
        at org.apache.cassandra.utils.BytesReadTracker.readUnsignedShort(BytesReadTracker.java:140)
        at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:361)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:371)
        at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:160)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:238)
        at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:178)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:78)
{noformat}

This is common for snappy and java zip:

{noformat}
	at org.apache.cassandra.streaming.compress.CompressedInputStream.read(CompressedInputStream.java:79)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:320)
	at org.apache.cassandra.utils.BytesReadTracker.readUnsignedShort(BytesReadTracker.java:140)
	at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:361)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:371)
	at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:160)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:238)
	at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:178)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:78)
{noformat}

When the download runs in single thread, the problem disappears.",28/Mar/13 15:32;ondrej.cernos;How does cassandra compute the number of threads involved in streaming?,"28/Mar/13 16:06;yukim;When sending file, it is single-threaded per destination.
",28/Mar/13 17:55;ondrej.cernos;How does this information match the observed behaviour? I can clearly see two threads downloading the file.,"29/Mar/13 03:34;yukim;CompressedFileStreamTask is not sending the right part of the file when using inter-node encryption, and that causes various IOException described here.

Patch attached for fix.",29/Mar/13 08:38;iamaleksey;+1,29/Mar/13 14:59;yukim;Committed. Thanks!,"03/Apr/13 09:18;jan.chochol;Hi everyone, I am working with Ondrej on same problem.
I just looked to patch (git commit cc6429e2722e764dce8cad77660732146ed596ab) and I am not sure that it is exactly correct.
Problematic situation is, when {{length}} > {{CHUNK_SIZE}} (in {{CompressedFileStreamTask.stream()}}). In this case data will be sent in more chunks, but before every chunk this code will be executed:
{noformat}
file.seek(section.left);
{noformat}
Sending only first chunk every time will probably lead to described error.
I would suggest to move {{file.seek}} before beginning of {{while}} cycle (than file pointer will be moved by {{readFully}}) or change mentioned code to
{noformat}
file.seek(section.left + bytesTransferred);
{noformat}","03/Apr/13 09:40;krummas;that sounds and looks reasonable, reopening to let @yukim have a look",03/Apr/13 09:49;ondrej.cernos;I tested the patch and verified it doesn't fix the issue.,"03/Apr/13 11:29;ondrej.cernos;I tried Jan's proposal to move the seek out of the while loop and let the pointer be moved by the readFully method call, but with no luck. I'll let someone with more Cassandra internals knowledge to dive into this.","03/Apr/13 13:15;yukim;Jan, Ondřej,

Thanks for reporting.
What I wanted to do was to position the file pointer to the beginning of section for each loop, as uncompressed version do.
I attached two patches (-1.2.3 to apply for 1.2.3 release and -1.2 for current 1.2 branch). Can you try these?",03/Apr/13 15:58;ondrej.cernos;[This|https://issues.apache.org/jira/secure/attachment/12576777/5391-1.2.3.txt] patch seems to work. So the issue may be resolved now. Thanks!,"04/Apr/13 00:33;yukim;Committed, thanks!","01/Jul/13 23:14;mikelococo;I don't think I see any code changes in the 1.1.x branch as a result of this bug. Does the bug not apply to 1.1.x (aka, it was introduced in the 1.2.0 streaming refactor?), or does 1.1.12 (and 1.1.9, on which Datastax Enterprise is based) still suffer from this?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTableScanner can Skip Rows with vnodes,CASSANDRA-6638,12692196,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,thobbs,thobbs,30/Jan/14 00:31,12/Mar/19 14:20,13/Mar/19 22:29,30/Jan/14 17:22,2.0.5,,,,,,0,,,,,"CASSANDRA-2524 added multiple range support to SSTableScanner, but it looks like there is at least one case where keys can be skipped.  This can result in cleanup removing legitimate keys.

See the attached patch that adds a unit test to reproduce the case.",,,,,,,,,,,,,,,,,,,,,,,,30/Jan/14 00:32;thobbs;6638-repro-test.txt;https://issues.apache.org/jira/secure/attachment/12626026/6638-repro-test.txt,30/Jan/14 11:57;slebresne;6638.txt;https://issues.apache.org/jira/secure/attachment/12626095/6638.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-01-30 01:30:35.558,,,no_permission,,,,,,,,,,,,370787,,,Thu Jan 30 17:22:38 UTC 2014,,,,,,0|i1rwkf:,371095,,,,,,,,thobbs,thobbs,,,2.0.1,,,,,,,30/Jan/14 01:30;rcoli;Are there non-cleanup related consequences of this issue?,"30/Jan/14 11:57;slebresne;Attaching simple fix (the patch includes Tyler's unit test).

For the records, the original reported of the issue, Ignace Desimpel, provided  the following analysis on the mailing list:
{quote}
To see what is wrong, think of having 3 ranges in the list, and both the first and second range will not produce a valid currentKey. The first time in the loop we get the first range, and then call seekToCurrentRangeStart(). That routine doesn’t do anything in that case, so then the first key is read from the sstable. But this first key does not match the first range, so we loop again. We get the second range and call seekToCurrentRangeStart() again. Again this does not do anything, leaving all file pointers. So then a new currentKey is read from the sstable BUT that should not be the case. We should, in that case, continue to test with the ‘old’ currentKey.
{quote}
(which is a fair description of what triggers the problem) and proposed a fix. That fix was modifying the logic of KeyScanningIterator.computeNext(), and while the fix itself is probably fine, it complicate the logic a bit and I think it's simpler and cleaner to just fix seekToCurrentRangeStart to really always seek to the first key greater than the current range start (that was clearly the intent given the comment when indexPosition = -1 in that method, but the method wrongly assumed we were always at the beginning of the sstable). So that's what the attached patch does. I'll note that this does mean we might re-read the same key if the sstable have no keys for at least 2 consecutive range, but this really doesn't matter in practice so we should stick to cleaner code.

bq. Are there non-cleanup related consequences of this issue?

No. This can only ever happen if SSTableScanner is uses with at least 2 ranges and cleanup is the only place we do that (I double-checked). I'll also note that this could affect non-vnode cases though it's a lot less likely, at least provided you use a random partitionner, as you need for a sstable to have no keys for a given local range, which is possible but much less likely without vnodes. Lastly, this might skip at most 1 row per-local-range.","30/Jan/14 13:31;ignaced;Thanks for the simple patch! 
Related to efficiency : 
Suppose we have sstable data in every other range.
Then the first range gets data matching the range. The second does not, and a 'file' seek is 
done to the first key greater than the left of the this second range, thus to the first sample key in the third range.
And then a loop is started over all the ifile entries until end of ifile file (there is no upper boundary check)! 
That would repeat itself over and over again if we happen to have such a data and range arrangement and depending on the number of vnodes.
Correct? That means a lot of work for nothing?
","30/Jan/14 14:14;slebresne;bq. And then a loop is started over all the ifile entries until end of ifile file (there is no upper boundary check)!

There is an upper check, we compare each deserialized index key to the range we're ""seeking to the start"" of. If we've seek to a key in the third range, that index key will sort after the 2nd range start and we'll exist the loop right away.","30/Jan/14 14:48;ignaced;Sorry, my mistake.","30/Jan/14 16:19;thobbs;+1 on the patch with a minor nit: can you remove the ""this will currently fail"" comment and mention 6638 in the new test?","30/Jan/14 17:22;slebresne;Committed with the nit, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit test failures on 1.2 branch,CASSANDRA-6375,12679926,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,,slebresne,slebresne,19/Nov/13 11:21,12/Mar/19 14:20,13/Mar/19 22:29,19/Nov/13 17:36,1.2.12,,,,,,0,,,,,"On my box, I get a number of reproducible test failures:
# LeaveAndBootstrapTest
{noformat}
[junit] Testsuite: org.apache.cassandra.service.LeaveAndBootstrapTest
[junit] Tests run: 6, Failures: 2, Errors: 0, Time elapsed: 13.145 sec
[junit] 
[junit] ------------- Standard Error -----------------
[junit]  WARN 12:11:43,275 Node /127.0.0.3 'leaving' token mismatch. Long network partition?
[junit] ------------- ---------------- ---------------
[junit] Testcase: newTestWriteEndpointsDuringLeave(org.apache.cassandra.service.LeaveAndBootstrapTest):	FAILED
[junit] mismatched endpoint sets expected:<[/127.0.0.4, /127.0.0.5]> but was:<[/127.0.0.4]>
[junit] junit.framework.AssertionFailedError: mismatched endpoint sets expected:<[/127.0.0.4, /127.0.0.5]> but was:<[/127.0.0.4]>
[junit] 	at org.apache.cassandra.service.LeaveAndBootstrapTest.newTestWriteEndpointsDuringLeave(LeaveAndBootstrapTest.java:131)
{noformat}
# TokenMetadataTest
{noformat}
[junit] Testsuite: org.apache.cassandra.locator.TokenMetadataTest
[junit] Tests run: 3, Failures: 1, Errors: 0, Time elapsed: 0.76 sec
[junit] 
[junit] Testcase: testRingIterator(org.apache.cassandra.locator.TokenMetadataTest):	FAILED
[junit] [] expected:<2> but was:<0>
[junit] junit.framework.AssertionFailedError: [] expected:<2> but was:<0>
[junit] 	at org.apache.cassandra.locator.TokenMetadataTest.testRingIterator(TokenMetadataTest.java:55)
[junit] 	at org.apache.cassandra.locator.TokenMetadataTest.testRingIterator(TokenMetadataTest.java:63)
{noformat}
# ScrubTest
{noformat}
[junit] Testsuite: org.apache.cassandra.db.ScrubTest
[junit] Tests run: 4, Failures: 1, Errors: 0, Time elapsed: 12.499 sec
[junit] 
[junit] ------------- Standard Error -----------------
[junit]  WARN 12:16:27,799 Out of order row detected (DecoratedKey(63, 63) found after DecoratedKey(7a, 7a))
[junit]  WARN 12:16:27,801 Out of order row detected (DecoratedKey(79, 79) found after DecoratedKey(7a, 7a))
[junit]  WARN 12:16:27,802 Out of order row detected (DecoratedKey(64, 64) found after DecoratedKey(7a, 7a))
[junit]  WARN 12:16:28,289 3 out of order rows found while scrubbing SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard3/Keyspace1-Standard3-ia-1-Data.db'); Those have been written (in order) to a new sstable (SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard3/Keyspace1-Standard3-ic-3-Data.db'))
[junit] ------------- ---------------- ---------------
[junit] Testcase: testScrubOneRow(org.apache.cassandra.db.ScrubTest):	FAILED
[junit] expected:<1> but was:<10>
[junit] junit.framework.AssertionFailedError: expected:<1> but was:<10>
[junit] 	at org.apache.cassandra.db.ScrubTest.testScrubOneRow(ScrubTest.java:94)
{noformat}

While running the whole test suites I also ran into the following stack:
{noformat}
[junit] Testsuite: org.apache.cassandra.dht.BootStrapperTest
[junit] Tests run: 4, Failures: 0, Errors: 0, Time elapsed: 14.345 sec
[junit] 
[junit] ------------- Standard Error -----------------
[junit]  WARN 11:16:50,833 No host ID found, created cb1c4ca1-c451-42ae-b205-36258dfe4f96 (Note: This should happen exactly once per node).
[junit]  WARN 11:16:51,193 Generated random token [f368755beab4290b7e70895776c6e14e]. Random tokens will result in an unbalanced ring; see http://wiki.apache.org/cassandra/Operations
[junit] ERROR 11:16:51,724 Fatal exception in thread Thread[PendingRangeCalculator:1,5,main]
[junit] java.util.ConcurrentModificationException
[junit] 	at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1115)
[junit] 	at java.util.TreeMap$EntryIterator.next(TreeMap.java:1151)
[junit] 	at java.util.TreeMap$EntryIterator.next(TreeMap.java:1146)
[junit] 	at com.google.common.collect.AbstractMultimap$EntryIterator.findValueIteratorAndKey(AbstractMultimap.java:1152)
[junit] 	at com.google.common.collect.AbstractMultimap$EntryIterator.next(AbstractMultimap.java:1166)
[junit] 	at com.google.common.collect.AbstractMultimap$EntryIterator.next(AbstractMultimap.java:1136)
[junit] 	at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1067)
[junit] 	at com.google.common.collect.ForwardingIterator.next(ForwardingIterator.java:48)
[junit] 	at com.google.common.collect.Maps$UnmodifiableEntries$1.next(Maps.java:953)
[junit] 	at com.google.common.collect.Maps$UnmodifiableEntries$1.next(Maps.java:951)
[junit] 	at com.google.common.collect.AbstractMultimap.putAll(AbstractMultimap.java:272)
[junit] 	at com.google.common.collect.TreeMultimap.putAll(TreeMultimap.java:74)
[junit] 	at org.apache.cassandra.utils.SortedBiMultiValMap.create(SortedBiMultiValMap.java:60)
[junit] 	at org.apache.cassandra.locator.TokenMetadata.cloneOnlyTokenMap(TokenMetadata.java:598)
[junit] 	at org.apache.cassandra.locator.TokenMetadata.cloneAfterAllLeft(TokenMetadata.java:619)
[junit] 	at org.apache.cassandra.service.PendingRangeCalculatorService.calculatePendingRanges(PendingRangeCalculatorService.java:139)
[junit] 	at org.apache.cassandra.service.PendingRangeCalculatorService$PendingRangeTask.run(PendingRangeCalculatorService.java:67)
[junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[junit] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[junit] 	at java.lang.Thread.run(Thread.java:744)
{noformat}
This doesn't end up failing the test and I was actually not able to reproduce when running BootStrapperTest individually, but I don't know if we understand why that can happen during the test (and if it's just an artifact of testing or a real thing).
",,,,,,,,,,,,,,,,,,,,,,,,19/Nov/13 17:08;slebresne;0001-LeaveAndBootstrapTest.txt;https://issues.apache.org/jira/secure/attachment/12614650/0001-LeaveAndBootstrapTest.txt,19/Nov/13 17:16;slebresne;0002-TMD-ConcurrentModificationException.txt;https://issues.apache.org/jira/secure/attachment/12614654/0002-TMD-ConcurrentModificationException.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-11-19 16:13:54.207,,,no_permission,,,,,,,,,,,,359283,,,Tue Nov 19 17:36:03 UTC 2013,,,,,,0|i1pxqv:,359582,,,,,,,,,,,,,,,,,,,"19/Nov/13 15:59;slebresne;Btw, also got all pig tests to fail with the following exceptions:
{noformat}
    [junit] Testcase: org.apache.cassandra.pig.CqlTableDataTypeTest:	Caused an ERROR
    [junit] null
    [junit] java.lang.ExceptionInInitializerError
    [junit] 	at org.apache.cassandra.pig.PigTestBase.startHadoopCluster(PigTestBase.java:104)
    [junit] 	at org.apache.cassandra.pig.CqlTableDataTypeTest.setup(CqlTableDataTypeTest.java:198)
    [junit] Caused by: java.lang.NullPointerException
    [junit] 	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:422)
    [junit] 	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:280)
    [junit] 	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:124)
    [junit] 	at org.apache.pig.test.MiniCluster.setupMiniDfsAndMrClusters(MiniCluster.java:50)
    [junit] 	at org.apache.pig.test.MiniGenericCluster.<init>(MiniGenericCluster.java:49)
    [junit] 	at org.apache.pig.test.MiniCluster.<init>(MiniCluster.java:31)
    [junit] 	at org.apache.pig.test.MiniGenericCluster.<clinit>(MiniGenericCluster.java:45)
{noformat}
I wouldn't block a release because of pig tests, but if it does not just fail for me, it would be nice to fix it too.","19/Nov/13 16:13;brandon.williams;Those fail for me too, but that should be an easy bisect.","19/Nov/13 16:23;slebresne;Looking at TokenMedataTest, it was just assuming the last test in the file was actually running last and apparently that wasn't happening on my box. Ninja-fixed that one in commit 0a5a766 to not depend on the tests execution order.","19/Nov/13 17:08;slebresne;For LeaveAndBootstrapTest, this bisects to CASSANDRA-6244. So I think this is just a case of ""we've made things asynchronous so we now check the expected result before the computation is done"". Tried adding a few calls to PRCS.blockUntilFinished in the few places that were failing for me and that seems to fix the test. Attaching the resulting patch. [~brandon.williams] can you check it's not entirely stupid?","19/Nov/13 17:13;brandon.williams;Looks pretty similar to what I did in 7de6f9666 to fix them in 1.1, except I used the shotgun method :) So if that fixes it, then +1","19/Nov/13 17:16;slebresne;Regarding the ConcurrentModificationException, it seems that the only reason this could get triggered is due to TMD.clearUnsafe(). As this is called by tests, this is not a real problem, but what about making it grab the writeLock like any good citizen to avoid getting scarry stack traces (and don't discard a real bug later on because we've grown used to discarding such stack)? Attaching patch to do that.","19/Nov/13 17:36;slebresne;Alright, ScrubTest was another instance of tests expecting to run in a particular order (don't know why my box don't run them in the order they are declared but well, expecting a particular order is a bad idea in any case) so ninja-fixed that. I've also committed the 2 patches attached above.

This fixes the failure I'm saying, except for the pig tests, but as those are clearly a setup thing I don't want to block 1.2.12 for that and I've open CASSANDRA-6376 to deal with them. Closing this issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inserts are blocked in 2.1,CASSANDRA-6154,12672654,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,enigmacurry,enigmacurry,07/Oct/13 17:41,12/Mar/19 14:07,13/Mar/19 22:29,08/Oct/13 22:30,,,,,,,0,,,,,"With cluster sizes >1 inserts are blocked indefinitely:

{code}
$ ccm create -v git:trunk test
Fetching Cassandra updates...
Current cluster is now: test
$ ccm populate -n 2
$ ccm start
$ ccm node1 cqlsh
Connected to test at 127.0.0.1:9160.
[cqlsh 4.0.1 | Cassandra 2.1-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.37.0]
Use HELP for help.
cqlsh> CREATE KEYSPACE timeline WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
cqlsh> USE timeline;
cqlsh:timeline> CREATE TABLE user_events (userid text, event timestamp, value text, PRIMARY KEY (userid, event));
cqlsh:timeline> INSERT INTO user_events (userid, event , value ) VALUES ( 'ryan', '2013-10-07', 'attempt');
{code}

The last INSERT statement never returns..",,,,,,,,,,,,,,,,,,,,,,,,07/Oct/13 21:16;jbellis;6154-v2.txt;https://issues.apache.org/jira/secure/attachment/12607242/6154-v2.txt,07/Oct/13 19:42;jbellis;6154.txt;https://issues.apache.org/jira/secure/attachment/12607223/6154.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-10-07 19:11:25.941,,,no_permission,,,,,,,,,,,,352277,,,Tue Oct 08 22:15:29 UTC 2013,,,,,,0|i1oqjb:,352565,2.1 rc3,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"07/Oct/13 19:11;brandon.williams;Bisect points at CASSANDRA-6132, specfically the ninja commit in 5440a0a6767544d6ea1ba34f5d2a3e223f260fb5",07/Oct/13 19:42;jbellis;Looks like merge to trunk from 2.0 was syntactically correct but semantically broken.  Attached.,"07/Oct/13 20:14;brandon.williams;Not quite.

{noformat}
ERROR [GossipStage:1] 2013-10-07 20:09:15,849 Caller+0   at org.apache.cassandra.service.CassandraDaemon$2.uncaughtException(CassandraDaemon.java:134)
 - Exception in thread Thread[GossipStage:1,5,main]
java.lang.AssertionError: null
        at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:552) ~[main/:na]
        at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:576) ~[main/:na]
        at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:571) ~[main/:na]
        at org.apache.cassandra.gms.Gossiper.markAlive(Gossiper.java:808) ~[main/:na]
        at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:849) ~[main/:na]
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:934) ~[main/:na]
        at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:49) ~[main/:na]
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56) ~[main/:na]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_17]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_17]
{noformat}",07/Oct/13 21:16;jbellis;v2,07/Oct/13 21:37;brandon.williams;+1,07/Oct/13 21:47;jbellis;committed,"08/Oct/13 18:42;enigmacurry;Thanks, working now.","08/Oct/13 20:03;brandon.williams;Actually, there's something wrong in the 2.0 branch that points at this commit too, but it's harder to trigger.  Using a batch such as:

{noformat}
            BEGIN BATCH
            INSERT INTO users (id, firstname, lastname) VALUES (0, 'Jack', 'Sparrow')
            INSERT INTO users (id, firstname, lastname) VALUES (1, 'Will', 'Turner')
            APPLY BATCH
{noformat}

at ONE with 2 of 3 nodes down should timeout, but never returns.",08/Oct/13 20:59;jbellis;Can you set trace probability to 100% and get a trace?,08/Oct/13 21:27;jbellis;Suspect patch for 6165 will fix this too.,"08/Oct/13 21:46;lizou;[~jbellis] I just pulled the Cassandra-2.0 source code and did a quick testing. Though slightly better than yesterday's trunk load. But there are still lots of assertion errors for  {{MessagingService.addCallback()}} without the argument of consistency level for MessageOut<RowMutation>. E.g.

{noformat}
   private void doDeliverHintsToEndpoint(InetAddress endpoint)
    {
     ...
                MessageOut<RowMutation> message = rm.createMessage();
     ...
                MessagingService.instance().sendRR(message, endpoint, responseHandler);
                responseHandlers.add(responseHandler);
{noformat}

and
{noformat}
    public static void sendToHintedEndpoints(final RowMutation rm,
                                             Iterable<InetAddress> targets,
                                             AbstractWriteResponseHandler responseHandler,
                                             String localDataCenter,
                                             ConsistencyLevel consistency_level)
    throws OverloadedException
    {
    ...
                    // belongs on a different server
                    if (message == null)
                        message = rm.createMessage();
                    String dc = DatabaseDescriptor.getEndpointSnitch().getDatacenter(destination);
                    // direct writes to local DC or old Cassandra versions
                    // (1.1 knows how to forward old-style String message IDs; updated to int in 2.0)
                    if (localDataCenter.equals(dc) || MessagingService.instance().getVersion(destination) < MessagingService.VERSION_20)
                    {
                        MessagingService.instance().sendRR(message, destination, responseHandler);
                    }
...
{noformat}

In the above examples, the {{sendRR()}} will call {{addCallback()}} which asserts as the message is of  type RowMutation.
",08/Oct/13 21:53;jbellis;CASSANDRA-6165,"08/Oct/13 22:03;lizou;Yes, this has been observed in my test this afternoon after one node was killed.

I think that we should systematically check for this issue, as some other places can also have this {{MessagingService.addCallback()}} assertion issue.",08/Oct/13 22:15;brandon.williams;CASSANDRA-6165 did indeed fix the batchlog problem here.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AIOOBE when doing select count(*) from on a mixed cluster.,CASSANDRA-6707,12695167,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,thobbs,pkolaczk,pkolaczk,14/Feb/14 08:56,12/Mar/19 14:07,13/Mar/19 22:29,17/Feb/14 13:54,2.0.6,,,,,,0,,,,,"After upgrading one node from 1.2 to 2.0, the following query fails with timeout:

{noformat}
Connected to test at localhost:9160.
[cqlsh 4.1.0 | Cassandra 2.0.5.1-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.39.0]
Use HELP for help.
cqlsh> select count(*) from cfs.sblocks;
Request did not complete within rpc_timeout.
{noformat}

Table definition:
{noformat}
cqlsh> describe columnfamily cfs.sblocks;

CREATE TABLE sblocks (
  key blob,
  column1 blob,
  value blob,
  PRIMARY KEY (key, column1)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.000068 AND
  caching='KEYS_ONLY' AND
  comment='Stores blocks of information associated with a inode' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='true' AND
  default_time_to_live=0 AND
  speculative_retry='99.0PERCENTILE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'com.datastax.bdp.hadoop.cfs.compaction.CFSCompactionStrategy'} AND
  compression={};
{noformat}

The 1.2 node reports the following error:
{noformat}
ERROR 08:38:02,006 Exception in thread Thread[Thread-32,5,main]
java.lang.ArrayIndexOutOfBoundsException: 36
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:59)
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:208)
	at org.apache.cassandra.net.IncomingTcpConnection.handleModernVersion(IncomingTcpConnection.java:140)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:83)
{noformat}

There were no errors during the upgrade.
","old nodes: Cassandra 1.2.16 from DSE 3.2.5  (unreleased)
new node: Cassandra  2.0.5 from DSE 4.0.0 (unreleased)",,,,,,,,,,,,,,,,,,,,,,,14/Feb/14 22:20;thobbs;6707.patch;https://issues.apache.org/jira/secure/attachment/12629131/6707.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-14 13:32:57.443,,,no_permission,,,,,,,,,,,,373675,,,Mon Feb 17 13:54:55 UTC 2014,,,,,,0|i1se8n:,373975,,,,,,,,pkolaczk,pkolaczk,,,,,,,,,,"14/Feb/14 09:00;pkolaczk;The following works fine:
{noformat}
select * from cfs.sblocks limit 1;  
{noformat}","14/Feb/14 13:32;jbellis;Via Sylvain,

bq.  I think I know what the problem is: count(*) on 2.0 blindly use the new pagers and when it's a range query, this uses PagedRangeCommand which 1.2 nodes don't have. Not sure what's the right fix is. Probably just stick to non-paged calls if there is 1.2 nodes in the cluster, but that's slightly painful.","14/Feb/14 22:20;thobbs;6707.patch (and [branch|https://github.com/thobbs/cassandra/tree/6707]) takes the approach suggested by Sylvain.  I tested this against a two-node cluster like so:
# Start two 1.2 nodes
# Insert 1m rows, verify {{select count\(*\)}} returns 1m
# Upgrade one node to cassandra-2.0 plus the patch, verify that {{select count\(*\)}} with the 2.0 node as the coordinator doesn't send a paging query to the 1.2 node
# Upgrade the second node to cassandra-2.0, verify that the paging query is now used

However, I will note that after upgrading the first node to 2.0, the results of {{select count\(*\)}} were consistent but incorrect (I got 957815 as the result).  After upgrading the second node to 2.0 I got another consistent but incorrect result (912873).  I couldn't reproduce this when I started a new two node cluster with just 2.0 plus the patch, so I might have made a mistake when upgrading and testing the patch, or there might be a separate issue with counting.  Unfortunately I don't have time to investigate this at the moment.","15/Feb/14 14:36;pkolaczk;I tested it and something went wrong.
I'm unable to run the query from hive:

{noformat}
java.io.IOException: java.io.IOException: java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:244)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:538)
	at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.<init>(MapTask.java:197)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:418)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:266)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
	at org.apache.hadoop.mapred.Child.main(Child.java:260)
Caused by: java.io.IOException: java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
	at org.apache.hadoop.hive.cassandra.cql3.input.HiveCqlInputFormat.getRecordReader(HiveCqlInputFormat.java:102)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:241)
	... 9 more
Caused by: java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
	at org.apache.cassandra.hadoop.cql3.CqlPagingRecordReader.initialize(CqlPagingRecordReader.java:161)
	at org.apache.hadoop.hive.cassandra.cql3.input.CqlHiveRecordReader.initialize(CqlHiveRecordReader.java:91)
	at org.apache.hadoop.hive.cassandra.cql3.input.HiveCqlInputFormat.getRecordReader(HiveCqlInputFormat.java:96)
	... 10 more
Caused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
	at java.util.ArrayList.rangeCheck(ArrayList.java:604)
	at java.util.ArrayList.get(ArrayList.java:382)
	at org.apache.cassandra.hadoop.cql3.CqlPagingRecordReader.retrieveKeys(CqlPagingRecordReader.java:710)
	at org.apache.cassandra.hadoop.cql3.CqlPagingRecordReader.initialize(CqlPagingRecordReader.java:155)
	... 12 more
{noformat}

Additionally, after I upgraded the whole cluster to 2.0, it lost all data and select is returning 0 rows. Not sure what happened. Unfortunately I forgot to record the count(*) number returned before I started the upgrade. I have to retry and see if it is reproducible.
","15/Feb/14 21:15;pkolaczk;I repeated the experiment.

Before upgrade:
{noformat}

cqlsh> select count(*) from ""PortfolioDemo"".""Stocks"";

 count
-------
  2776

cqlsh> exit;

{noformat}

After upgrading one of three nodes:
{noformat}
automaton@ubuntu:~$ cqlsh
Connected to test at localhost:9160.
[cqlsh 4.1.0 | Cassandra 2.0.5-CASSANDRA-6707-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.39.0]
Use HELP for help.
cqlsh> select count(*) from ""PortfolioDemo"".""Stocks"";

 count
-------
  1829

(1 rows)
{noformat}

After upgrading another one (2/3):
{noformat}
automaton@ubuntu:~$ cqlsh
Connected to test at localhost:9160.
[cqlsh 4.1.0 | Cassandra 2.0.5-CASSANDRA-6707-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.39.0]
Use HELP for help.
cqlsh> select count(*) from ""PortfolioDemo"".""Stocks"";

 count
-------
   910

(1 rows)
{noformat}

After upgrading all:
{noformat}
cqlsh> select count(*) from ""PortfolioDemo"".""Stocks"";

 count
-------
     0

(1 rows)
{noformat}


",15/Feb/14 21:31;pkolaczk;Restarting the cluster does not bring the data back. I'm afraid it lost them for good.,15/Feb/14 21:37;iamaleksey;Looks like an issue with counting to me. You can't really lose any data by running a SELECT query.,"15/Feb/14 21:49;pkolaczk;Are you sure?
{noformat}
[cqlsh 4.1.0 | Cassandra 2.0.5-CASSANDRA-6707-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.39.0]
Use HELP for help.
cqlsh> select * from ""PortfolioDemo"".""Stocks"";

(0 rows)
{noformat}

Previously it *did* return results.",15/Feb/14 21:52;iamaleksey;Try looking it up by key.,"15/Feb/14 22:38;pkolaczk;No luck. :(

{noformat}

cqlsh> select * from ""PortfolioDemo"".""StockHist"" where key='JZL';

(0 rows)
{noformat}

Previously I did:
{noformat}
cqlsh> select * from ""PortfolioDemo"".""StockHist"";

 key  | column1    | value
------+------------+----------
  JZL | 2013-11-08 |   5.4057
  JZL | 2013-11-09 |   164.12
  JZL | 2013-11-10 |   479.89
  JZL | 2013-11-11 |   822.42
  JZL | 2013-11-12 |   225.37
  JZL | 2013-11-13 |   613.18
  JZL | 2013-11-14 |   387.83
...
{noformat}","16/Feb/14 11:01;pkolaczk;I repeated the experiment with one minor change. This time I did nodetool flush before upgrading the first node and I didn't do it before upgrading the second node. Here are the results:

Before upgrade:
{noformat}
Connected to test at localhost:9160.
[cqlsh 3.1.8 | Cassandra 1.2.15.1-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.36.2]
Use HELP for help.
cqlsh> select * from ""PortfolioDemo"".""StockHist"" where key='JZL';

 key | column1    | value
-----+------------+--------
 JZL | 2013-11-09 | 517.41
 JZL | 2013-11-10 | 621.67
 JZL | 2013-11-11 | 647.35
 JZL | 2013-11-12 | 189.38
 JZL | 2013-11-13 | 29.725
 JZL | 2013-11-14 | 385.86
 JZL | 2013-11-15 | 900.27
 JZL | 2013-11-16 |  210.4
 JZL | 2013-11-17 | 844.64
 JZL | 2013-11-18 | 619.82
 JZL | 2013-11-19 | 956.49
 JZL | 2013-11-20 | 928.05
 JZL | 2013-11-21 | 542.06
 JZL | 2013-11-22 | 437.06
 JZL | 2013-11-23 | 806.88
 JZL | 2013-11-24 | 179.27
 JZL | 2013-11-25 | 207.45
 JZL | 2013-11-26 |  848.3
 JZL | 2013-11-27 | 715.32
 JZL | 2013-11-28 | 445.85
 JZL | 2013-11-29 | 821.07
 JZL | 2013-11-30 |  873.4
 JZL | 2013-12-01 | 625.07
 JZL | 2013-12-02 | 21.017
 JZL | 2013-12-03 | 881.37
 JZL | 2013-12-04 | 443.81
 JZL | 2013-12-05 |  432.7
 JZL | 2013-12-06 | 850.86
 JZL | 2013-12-07 | 38.699
 JZL | 2013-12-08 | 612.97
 JZL | 2013-12-09 | 158.81
 JZL | 2013-12-10 | 378.39
 JZL | 2013-12-11 | 245.21
 JZL | 2013-12-12 | 428.54
 JZL | 2013-12-13 | 664.41
 JZL | 2013-12-14 | 784.94
 JZL | 2013-12-15 | 820.02
 JZL | 2013-12-16 | 859.82
 JZL | 2013-12-17 |  258.5
 JZL | 2013-12-18 | 731.21
 JZL | 2013-12-19 | 384.75
 JZL | 2013-12-20 | 696.25
 JZL | 2013-12-21 |  936.1
 JZL | 2013-12-22 | 781.04
 JZL | 2013-12-23 | 113.63
 JZL | 2013-12-24 | 254.12
 JZL | 2013-12-25 | 120.91
 JZL | 2013-12-26 | 65.565
 JZL | 2013-12-27 |  378.6
 JZL | 2013-12-28 | 712.02
 JZL | 2013-12-29 | 953.41
 JZL | 2013-12-30 | 788.21
 JZL | 2013-12-31 | 236.73
 JZL | 2014-01-01 | 727.81
 JZL | 2014-01-02 | 128.59
 JZL | 2014-01-03 | 290.18
 JZL | 2014-01-04 | 930.29
 JZL | 2014-01-05 | 160.98
 JZL | 2014-01-06 | 992.55
 JZL | 2014-01-07 | 92.251
 JZL | 2014-01-08 | 456.14
 JZL | 2014-01-09 | 969.27
 JZL | 2014-01-10 | 769.52
 JZL | 2014-01-11 | 864.01
 JZL | 2014-01-12 | 516.35
 JZL | 2014-01-13 | 547.88
 JZL | 2014-01-14 | 128.87
 JZL | 2014-01-15 | 847.73
 JZL | 2014-01-16 | 232.34
 JZL | 2014-01-17 | 491.26
 JZL | 2014-01-18 | 196.56
 JZL | 2014-01-19 |  57.35
 JZL | 2014-01-20 | 978.43
 JZL | 2014-01-21 | 588.59
 JZL | 2014-01-22 | 377.69
 JZL | 2014-01-23 | 772.32
 JZL | 2014-01-24 | 377.71
 JZL | 2014-01-25 | 121.46
 JZL | 2014-01-26 | 202.91
 JZL | 2014-01-27 | 679.37
 JZL | 2014-01-28 | 558.55
 JZL | 2014-01-29 | 493.89
 JZL | 2014-01-30 | 759.51
 JZL | 2014-01-31 | 331.46
 JZL | 2014-02-01 | 291.12
 JZL | 2014-02-02 | 533.44
 JZL | 2014-02-03 | 950.21
 JZL | 2014-02-04 | 920.72
 JZL | 2014-02-05 | 843.61
 JZL | 2014-02-06 | 447.53
 JZL | 2014-02-07 | 797.89
 JZL | 2014-02-08 | 419.86
 JZL | 2014-02-09 | 640.36
 JZL | 2014-02-10 | 123.98
 JZL | 2014-02-11 | 339.73
 JZL | 2014-02-12 | 833.88
 JZL | 2014-02-13 | 699.87
 JZL | 2014-02-14 |  705.4
 JZL | 2014-02-15 | 655.25
 JZL | 2014-02-16 | 950.93

cqlsh> select * from ""PortfolioDemo"".""StockHist"" where key='JZL' and column1='2013-11-09';

 key | column1    | value
-----+------------+--------
 JZL | 2013-11-09 | 517.41

cqlsh> exit;
automaton@ubuntu:~$ cqlsh
Connected to test at localhost:9160.
[cqlsh 3.1.8 | Cassandra 1.2.15.1-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.36.2]
Use HELP for help.
cqlsh> select count(*) from ""PortfolioDemo"".""Stocks"";

 count
-------
  2767

cqlsh> exit;
{noformat}

After nodetool flush on the first node and upgrade of the first node:
{noformat}
Connected to test at localhost:9160.
[cqlsh 4.1.0 | Cassandra 2.0.5-CASSANDRA-6707-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.39.0]
Use HELP for help.
cqlsh> select count(*) from ""PortfolioDemo"".""Stocks"";

 count
-------
  2767

(1 rows)

cqlsh> select * from ""PortfolioDemo"".""StockHist"" where key='JZL' and column1='2013-11-09';

 key | column1    | value
-----+------------+--------
 JZL | 2013-11-09 | 517.41

(1 rows)

cqlsh> exit;
{noformat}

After upgrading the second node (*no nodetool flush* - deliberately):
{noformat}
automaton@ubuntu:~$ cqlsh
Connected to test at localhost:9160.
[cqlsh 4.1.0 | Cassandra 2.0.5-CASSANDRA-6707-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.39.0]
Use HELP for help.
cqlsh> select count(*) from ""PortfolioDemo"".""Stocks"";

 count
-------
  1853

(1 rows)

cqlsh> select * from ""PortfolioDemo"".""StockHist"" where key='JZL' and column1='2013-11-09';

(0 rows)
{noformat}

There were *no errors nor warnings* reported in the system.log when the new nodes were starting up.
It looks like we've got a serious problem with commit-log. [~thobbs] loaded more data than me, so probably it managed to flush most data. That explains pretty well his results (count being slightly below the correct value).",16/Feb/14 11:04;pkolaczk;BTW: I double checked - commit log location did not change.,16/Feb/14 11:06;pkolaczk;Flushing the second node *after the upgrade* does not bring the data back. So I guess those data were not loaded into memtable from CL.,"16/Feb/14 18:43;iamaleksey;You are supposed to 'nodetool drain' before upgrading (or, at least I always assumed so).","16/Feb/14 19:45;pkolaczk;Ok, I retry with nodetool drain.","16/Feb/14 20:04;benedict;bq. You are supposed to 'nodetool drain' before upgrading (or, at least I always assumed so).

Even so, I don't think there's a good reason for this to fail - nothing has changed in log replay. Seems like we should be resilient to this if possible.

I would guess it's because any schema upgrading hasn't happened before we replay the CL, so we don't find the CFs the records are associated with, as I can't see any other reason for this to go wrong.","16/Feb/14 20:09;iamaleksey;bq. I would guess it's because any schema upgrading hasn't happened before we replay the CL

Nope, most definitely not it (no breaking schema changes that could affect this between 1.2 and 2.0).

The RowMutation serialization, however, has changed between VERSION_12 and VERSION_20 (keyspace name present/absent). The commitlog is, and has always been, implicitly versioned, even if the replay code itself hasn't changed.","16/Feb/14 20:15;benedict;bq. The RowMutation serialization, however, has changed between VERSION_12 and VERSION_20 (keyspace name present/absent). The commitlog is, and has always been, implicitly versioned, even if the replay code itself hasn't changed.

Yes, but it is supposed to be able to replay these older versions. Both the replay and the RM are aware of the version change, so if this is the problem it is a bug.","16/Feb/14 20:15;iamaleksey;See the comment (and code) at/after https://github.com/apache/cassandra/blob/cassandra-2.0/src/java/org/apache/cassandra/db/commitlog/CommitLogReplayer.java#L269

Not saying that this is the right/proper way to do it, just pointing to how it currently works.","16/Feb/14 20:19;benedict;https://github.com/apache/cassandra/blob/cassandra-2.0/src/java/org/apache/cassandra/db/commitlog/CommitLogReplayer.java#L237

https://github.com/apache/cassandra/blob/cassandra-2.0/src/java/org/apache/cassandra/db/RowMutation.java#L274

Not sure why we go to such lengths to make sure we can do it elsewhere then? It seems like all the related code thinks it can replay these version changes...","16/Feb/14 20:27;benedict;Note that comment is from 1.1; it's not clear it's up-to-date with the latest intent. Especially since it doesn't assume the current version - it passes in the actual version of the CLS, whereas in 1.1 it really did assume.

https://github.com/apache/cassandra/blob/cassandra-1.1/src/java/org/apache/cassandra/db/commitlog/CommitLogReplayer.java#L202

Also, if there were an error deserializing, it should throw an exception that would be logged as an error.","17/Feb/14 11:49;pkolaczk;If it can deserialize, it should do it. If it cannot (unsupported version), it should bail out, shout at the user ""don't do this! do nodetool drain first!"" and terminate without touching any data. This way it is now, I'm pretty sure some of the customers will run into this, and telling them it was their fault wouldn't make them less unhappy. We're not mongo to penalize them with silent data loss.","17/Feb/14 11:59;pkolaczk;I tested again doing nodetool drain before upgrading every single node and counts were correct. [~thobbs] patch works fine and lgtm.
+1

Shouldn't we move the commitlog replay problem to another ticket?","17/Feb/14 13:41;iamaleksey;Committed to 2.0, waiting for 2.0->trunk merge of another issue before committing to trunk.","17/Feb/14 13:54;iamaleksey;Committed, thanks everybody.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition during node bootstrapping,CASSANDRA-6648,12693094,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,sbtourist,sbtourist,sbtourist,04/Feb/14 12:37,12/Mar/19 14:07,13/Mar/19 22:29,05/Feb/14 00:37,1.2.15,2.0.5,,,,,0,qa-resolved,,,,"When bootstrapping a new node, data is ""missing"" as if the new node didn't actually bootstrap, which I tracked down to the following scenario:

1) New node joins token ring and waits for schema to be settled before actually bootstrapping.
2) The schema scheck somewhat passes and it starts bootstrapping.
3) Bootstrapping doesn't find the ks/cf that should have received from the other node.
4) Queries at this point cause NPEs, until when later they ""recover"" but data is missed.

The problem seems to be caused by a race condition between the migration manager and the bootstrapper, with the former running after the latter.
I think this is supposed to protect against such scenarios:
{noformat}
            while (!MigrationManager.isReadyForBootstrap())
            {
                setMode(Mode.JOINING, ""waiting for schema information to complete"", true);
                Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);
            }
{noformat}

But MigrationManager.isReadyForBootstrap() implementation is quite fragile and doesn't take into account ""slow"" schema propagation.",,,,,,,,,,,,,,,,,,,,,,,,04/Feb/14 16:41;brandon.williams;6648-v2.txt;https://issues.apache.org/jira/secure/attachment/12626896/6648-v2.txt,04/Feb/14 23:14;brandon.williams;6648-v3-1.2.txt;https://issues.apache.org/jira/secure/attachment/12627020/6648-v3-1.2.txt,04/Feb/14 22:54;brandon.williams;6648-v3.txt;https://issues.apache.org/jira/secure/attachment/12627010/6648-v3.txt,04/Feb/14 16:02;sbtourist;CASSANDRA-6648.patch;https://issues.apache.org/jira/secure/attachment/12626888/CASSANDRA-6648.patch,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2014-02-04 12:40:25.621,,,no_permission,,,,,,,,,,,,371680,,,Wed Feb 05 15:15:52 UTC 2014,,,,,,0|i1s20n:,371980,1.2.14,,,,,,,thobbs,thobbs,,,1.2.14,,,,,,enigmacurry,04/Feb/14 12:40;brandon.williams;How many keyspaces/CFs are defined when this occurs?,"04/Feb/14 12:45;sbtourist;I reproduced running cassandra-stress: two times in a row by running a local node, inserting 1000 keys, and then bootstrapping a second node.","04/Feb/14 16:02;sbtourist;The race condition is caused by a change applied in CASSANDRA-6615, which doesn't check for the endpoint to be ""not alive"" in order to be a fat client.

As a consequence, during bootstrap, the seed node is considered a fat client (because it *is* alive but has no tokens yet), and no schema pull is executed by the MigrationManager.

This patch fixes the Gossiper by adding back the ""isAlive"" check (it shouldn't cause any actual problems, but given my limited understanding of CASSANDRA-6615, I could be totally wrong), and makes the MigrationManager more robust against such race conditions by having the isReadyForBootstrap method check the schema version against the pre-bootstrap empty one, so that the bootstrap process will not make any progress if the schema is not correctly pulled (which is imho a big improvement over silently failing).","04/Feb/14 16:41;brandon.williams;v2 builds on Sergio's patch, but changes the gossiper's own fat client check (not isFatClient) to ignore epstate.isAlive and just rely on the timestamp of the last update and membership.",04/Feb/14 16:51;sbtourist;Works for me.,"04/Feb/14 21:15;brandon.williams;Thinking about this a bit more, I'm inclined to think that a) isFatClient should never have checked epState.isAlive, since a fat client can be either alive or dead, and neither make it more or less of a fat client, and thus b) onAlive is the wrong event for MM to be looking at to decide on pulling schema, since potentially *every* node actually IS a fat client when first seen.  The true source of 'fatclientness' or not is TMD.isMember, but SS hasn't processed the onJoin event yet when onAlive is called.  We could possibly fix this by having isFatClient check for the presence of TOKENS, which a fat client shouldn't have, or we could make SS.onJoin trigger MM.maybeScheduleSchemaPull after it has processed the join event.","04/Feb/14 22:54;brandon.williams;v3, influenced by my last comment, removes the MM subscription to gossip and instead is notified by SS after it has completed any onAlive/onJoin events.  isFatClient doesn't check aliveness, and MM.passiveAnnounce remains since that's the only way the schema state gets populated in gossip.","04/Feb/14 23:23;enigmacurry;Both 6648-v3.txt and 6648-v3-1.2.txt are +1 from me.

Tested with the following:

{code}
ccm create bootstrap_bug
ccm populate -n 3
ccm start
ccm node1 stress -n 10000

# Bootstrap a new node:
ccm add -b node4 -t 127.0.0.4:9160 -l 127.0.0.4:7000 -j 7400 --binary-itf 127.0.0.4:9042
ccm node4 start

# Query data from the new node:
ccm node4 cqlsh

cqlsh>  select * from ""Keyspace1"".""Standard1"" limit 10;
{code}

Pre-patch, they both errored out saying ""Bad Request: Keyspace Keyspace1 does not exist"". Now they come back with the data.","04/Feb/14 23:45;sbtourist;I'll have a better look tomorrow, but I think Schema.instance.updateVersionAndAnnounce() should be removed from StorageService#joinTokenRing, as it isn't really needed and could race by being concurrently called with schema merges.",05/Feb/14 00:22;thobbs;+1 on 6648-v3 and 6648-v3-1.2,"05/Feb/14 00:37;brandon.williams;Committed, since v3 didn't change any logic with updateVersionAndAnnounce, just rearranged the event order.  If there's a race there let's open a new ticket for it.","05/Feb/14 07:52;slebresne;If this is a regression of CASSANDRA-6615, it doesn't affect 2.0.4, does it? (asking because of the current 'reproduced in').

[~enigmacurry] would be great if you could push that test of yours above as a dtest.","05/Feb/14 09:51;sbtourist;Works for me.

Regarding the Schema.instance.updateVersionAndAnnounce() call inside StorageService#joinTokenRing, it's definitely race-prone with regard to Gossiper notifications causing a major state change and a schema update (which I reproduced by using breakpoints on the two concurrent threads): but, even if a lost update can occur and re-establish an empty version, I verified the correct schema version is later recomputed by another schema pull, hence this shouldn't cause any actual problems.","05/Feb/14 15:15;enigmacurry;{quote}
Ryan McGuire would be great if you could push that test of yours above as a dtest.
{quote}

[Done|https://github.com/riptano/cassandra-dtest/commit/17e8f3f5680f6d78755577d8817b1eeb64b642d8]. We already have a few different tests to catch this, but all were being masked for one reason or another :(",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IllegalArgumentException when Preparing Statements,CASSANDRA-6592,12689086,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,thobbs,thobbs,15/Jan/14 20:59,12/Mar/19 14:07,13/Mar/19 22:29,17/Oct/14 13:44,1.2.14,2.0.5,,,,,1,,,,,"When preparing a lot of statements with the python native driver, I occasionally get an error response with an error that corresponds to the following stacktrace in the cassandra logs:

{noformat}
ERROR [Native-Transport-Requests:126] 2014-01-11 13:58:05,503 ErrorMessage.java (line 210) Unexpected exception during request
java.lang.IllegalArgumentException
        at com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap.checkArgument(ConcurrentLinkedHashMap.java:259)
        at com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$BoundedEntryWeigher.weightOf(ConcurrentLinkedHashMap.java:1448)
        at com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap.put(ConcurrentLinkedHashMap.java:764)
        at com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap.put(ConcurrentLinkedHashMap.java:743)
        at org.apache.cassandra.cql3.QueryProcessor.storePreparedStatement(QueryProcessor.java:255)
        at org.apache.cassandra.cql3.QueryProcessor.prepare(QueryProcessor.java:221)
        at org.apache.cassandra.transport.messages.PrepareMessage.execute(PrepareMessage.java:77)
        at org.apache.cassandra.transport.Message$Dispatcher.messageReceived(Message.java:287)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:43)
        at org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:67)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

Looking at the CLHM source, this means we're giving the statement a weight that's less than 1.  I'll also note that these errors frequently happen in clumps of 2 or 3 at a time.",,,,,,,,,,,,,,,,,,,,,,,,30/Jan/14 18:01;slebresne;6592-2.0.txt;https://issues.apache.org/jira/secure/attachment/12626134/6592-2.0.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-23 00:51:26.071,,,no_permission,,,,,,,,,,,,368053,,,Fri Oct 17 13:44:13 UTC 2014,,,,,,0|i1rftr:,368359,2.1.0,,,,,,,thobbs,thobbs,,,,,,,,,,23/Jan/14 00:51;dctrwatson;We get this error at least twice a day which requires restarting the node(s),23/Jan/14 19:37;dctrwatson;Downgraded back to 1.2.10 and no longer getting this error,"28/Jan/14 21:36;thobbs;I also get a lot of responses like this mixed in:

{noformat}
InvalidRequest: code=2200 [Invalid query] message=""Prepared statement of size 97972368 bytes is larger than allowed maximum of 15179776 bytes.""
{noformat}

So it seems like our size calculation code can err in either direction.","29/Jan/14 09:34;slebresne;Not sure what happens here. In particular the ""is larger than allowed"" exception suggests MemoryMeter is sometimes returning something whack.  That being said, this sound like a race (comments above suggests this doesn't reproduce reliably on a specific statement) and we do use the same MemoryMeter instance concurrently. So while it's not clear why doing so would be wrong, it's probably worth removing that variable from the equation and see if that fixes it. So attaching a patch to switch to a ThreadLocal for the MemoryMeter. [~thobbs], since you seem to be able to reproduce relatively easily, can you try said patch and see if it helps or not?

If it doesn't, we might want to revert CASSANDRA-6107 until we understand what's going on.
","29/Jan/14 10:02;benedict;MemoryMeter itself is definitely threadsafe, and they all share the Instrumentation, so making it ThreadLocal won't make those calls threadsafe. The VM calls look threadsafe to me, anyway:

{code}
JvmtiEnv::GetObjectSize(jobject object, jlong* size_ptr) {
  oop mirror = JNIHandles::resolve_external_guard(object);
  NULL_CHECK(mirror, JVMTI_ERROR_INVALID_OBJECT);

  if (mirror->klass() == SystemDictionary::Class_klass()) {
    if (!java_lang_Class::is_primitive(mirror)) {
        mirror = java_lang_Class::as_klassOop(mirror);
        assert(mirror != NULL, ""class for non-primitive mirror must exist"");
    }
  }

  *size_ptr = mirror->size() * wordSize;
  return JVMTI_ERROR_NONE;
} /* end GetObjectSize */

klassOop java_lang_Class::as_klassOop(oop java_class) {
  //%note memory_2
  assert(java_lang_Class::is_instance(java_class), ""must be a Class object"");
  klassOop k = klassOop(java_class->obj_field(_klass_offset));
  assert(k == NULL || k->is_klass(), ""type check"");
  return k;
}
{code}

It's possible the object being metered is being concurrently modified? Certainly the CFMetaData permits this. And I note we measure it twice (one minus the other), to really permit exploding the problem. In a BatchStatement we might measure it many times. We should never measure something that is not immutable if we want to get sensible results.

As an example, I would suggest something like the following in ModificationStatement:

{code}
    public long measureForPreparedCache(MemoryMeter meter)
    {
        return meter.measure(this) + meter.measureDeep(attrs)- meter.measureDeep(boundTerms) + meter.measureDeep(processedKeys) + meter.measureDeep(columnOperations);
    }
{code}

","29/Jan/14 11:09;slebresne;bq. It's possible the object being metered is being concurrently modified? Certainly the CFMetaData permits this.

Yes it's possible and yes it's definitively dodgy. But one would hope that MemoryMeter don't return completely random results just because the measured object is mutated (but maybe that assumption is wrong). So that the error due to a concurrent modification of CFMetaData shouldn't exceed a couple of KB.  Which might explain occasional negative size computations, but doesn't entirely explain to me why Tyler is seeing ""a lot of"" statement being measured at more than 15MB. Unless Tyler is building ""a lot of"" super huge prepared batch statements that is, which I've assumed is not the case but maybe I'm wrong on that part.

In any case, I can agree that the current subtraction is misguided (it just doesn't seem to explain it all). Adding each fields individually in the measure method is slightly annoying to maintain so this is what this was trying to avoid but well, that's a bad idea so attaching a patch that changes that. [~thobbs] can you check if that solves both of the errors you see?
","29/Jan/14 11:20;benedict;Yeah, whilst I think measuring CFM is dangerous (certainly it could lead to the negative numbers) I'm suspicious of that being the cause for such dramatic spikes.

If you try using the jamm-0.2.6.jar I included in CASSANDRA-5549 you could try running with different estimation strategies - if either of the guess* strategies give nonsense, it's either JAMM doing something weird internally (unlikely) or some weird interaction of mutating object being passed into it (still most plausible). If they don't, it might be Instrumentation after all, but looking at the hotspot code that seems the least likely of all.

*Note the ""unsafe"" guess is perfectly accurate, as far as I could test with extensive random class generation, so it's not actually particularly guess-y.","29/Jan/14 14:10;cburroughs;""Memory estimation seems funky/wrong"" issues were also discussed in CASSANDRA-5939",30/Jan/14 16:17;jbellis;Solved for 1.2.14 by reverting CASSANDRA-6107 (and reducing the statement count from 100k to 50k to try to avoid the original OOM).,"30/Jan/14 18:01;slebresne;Summing up where we are:
* Tyler tested the 2nd patch which seems to fix the issue.
* It seems MemoryMeter can return pretty funky results (~100MB for a statement that Tyler confirmed as really small) when measuring CFMetaData. While there is some theory, it's no yet entirely clear why it can produce such big error.
* While the 2nd patch is likely fine, given the previous point and to take 0 risk with 1.2, we're reverted CASSANDRA-6107 there.

Now, for 2.0, attaching a rebase of the patch with correct handling of null values (which was missing from the first patch -- could maybe be slightly cleaner to make MemoryMeter.measureDeep handle null on the long run).

I'll note that while the patch seems to properly fix this issue, it might be worth digging a bit more separatly to understand what was confusing MemoryMeter exactly, since we use it in other places.
","31/Jan/14 01:01;thobbs;+1 on the rebase of the 2.0 patch.  I've run my repro script for 5x the max time it took me to reproduce before with no errors.

Just to add more details for posterity, this was the schema:

{noformat}
CREATE TABLE duration_test.ints (key int, copy int, value int, RIMARY KEY (key, copy))
{noformat}

The statement that triggered the IllegalArgumentException was this:

{noformat}
DELETE FROM duration_test.ints USING TIMESTAMP 123456 WHERE key=? AND copy=?;
{noformat}

with the internal representation:
{noformat}
DeleteStatement(name=duration_test.ints, columns=[], keys=[key EQ ?, copy EQ ?]
{noformat}

Here are some of the negative sizes that statement got:  -20936, -25872, -19856, -4784, -16152.  (When reproducing to test this ticket, I never saw the large positive sizes, so that might have been a driver error, I suppose.)  Typically the IAE's would come in bursts of ~5 within a second, and then it would be several hours before the next round of errors.

The only other activity while this happened was preparation of some insert statements and execution of various selects, inserts, and deletes (both prepared and non-prepared).  There were no schema changes or anything like that.","31/Jan/14 09:18;slebresne;Alright, committed, thanks.

bq. When reproducing to test this ticket, I never saw the large positive sizes, so that might have been a driver error, I suppose

Hum, I guess that's kind of good news since the negative size themselves are much more easy to explain, it's the large positive ones that were a tad puzzling. Anyway, keep us posted if you ever reproduce these ones and it doesn't appear to be an obvious driver error.
","31/Jan/14 14:19;benedict;I think I have just sussed this bug out at last.

And it would have also been fixed by CASSANDRA-5549 if I'm correct (well, except for a brief risk period at startup). This fix is still the best solution.

The Metering of Memtable memory occupancy iterated over the entire Memtable, which included its fields, which included its ColumnFamilyStore, which included its CompactionStrategy. Since the metering uses reflection to iterate over the fields of the CompactionStrategy, the CompactionStrategy's class would have its SoftReference<Field> set whenever this enumeration occured. A GC would clear these references. The CFMetaData has a reference to this _Class_ object, and so if the two measurements happened across a GC, we would get different values.

Note this cannot explain the absurdly large values, but since we currently think they might be something else, I feel comfortable that this is a satisfactory explanation for the reason behind this, and why it would reoccur but also why it was infrequent.","31/Jan/14 14:38;benedict;Actually, this would have to occur across a metering *following* a GC, not across a GC, obviously. The other way would cause a bump in size of similar magnitude, not a drop.",31/Jan/14 14:44;jbellis;That's pretty subtle.,"14/Oct/14 23:38;kishkaru;I got the same error message as well, while (ironically) working on the ruby duration test. It happens occasionally, around 20% of the time. Here are some sample messages:

{noformat}
Prepared statement of size 4451848 bytes is larger than allowed maximum of 2027520 bytes.
Prepared statement of size 4434568 bytes is larger than allowed maximum of 2027520 bytes.
{noformat}

This is my schema, and the offending prepare statement:

{noformat}
@session.execute(""CREATE TABLE duration_test.ints (
                        key INT,
                        copy INT,
                        value INT,
                        PRIMARY KEY (key, copy))""
)
{noformat}

{noformat}
select = @session.prepare(""SELECT * FROM ints WHERE key=?"")
{noformat}

Now, I notice that if I explicitly specify the keyspace in the prepare, I don't get the error.","16/Oct/14 17:00;jbellis;Is this exactly the same stacktrace?  If not, should be a separate ticket.  (Even if it is, it should be a new ticket since this one was committed several version ago.)","17/Oct/14 13:44;slebresne;As Jonathan said, please open a separate ticket with details on your exact problem since some patch has already be committed for this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Random tombstones after adding a CF with sstableloader,CASSANDRA-6527,12686418,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,yukim,br1985,br1985,26/Dec/13 13:58,12/Mar/19 14:07,13/Mar/19 22:29,26/Dec/13 22:01,2.0.4,,,,,,0,,,,,"I've marked this bug as critical since it results in a data loss without any warnings.

Here's the scenario:

- create a fresh one-node cluster with cassandra 1.2.11
- add a sample row:

{code}
CREATE KEYSPACE keyspace1 WITH replication = {'class':'SimpleStrategy', 'replication_factor':1};
use keyspace1;
create table table1 (key text primary key, value1 text);
update table1 set value1 = 'some-value' where key = 'some-key';
{code}

- flush, drain, shutdown the cluster - you should have a single sstable:

{code}
root@l1:~# ls /var/lib/cassandra/data/keyspace1/table1/
keyspace1-table1-ic-1-CompressionInfo.db  
keyspace1-table1-ic-1-Filter.db  
keyspace1-table1-ic-1-Statistics.db  
keyspace1-table1-ic-1-TOC.txt
keyspace1-table1-ic-1-Data.db             
keyspace1-table1-ic-1-Index.db   
keyspace1-table1-ic-1-Summary.db
{code}

with a perfectly correct content:

{code}
root@l1:~# sstable2json /var/lib/cassandra/data/keyspace1/table1/keyspace1-table1-ic-1-Data.db
[
{""key"": ""736f6d652d6b6579"",""columns"": [["""","""",1387822268786000], [""value1"",""some-value"",1387822268786000]]}
]
{code}

- create a new cluster with 2.0.3 (we've used 3 nodes with replication=2, but I guess it doesn't matter)

- copy sstable from the machine in the old cluster to one of the machines in the new cluster (we do not want to use old sstableloader)

- load sstables with sstableloader:

{code}
sstableloader -d 172.16.9.12 keyspace1/table1
{code}

- analyze the content of newly loaded sstable:

{code}
root@l13:~# sstable2json /var/lib/cassandra/data/keyspace1/table1/keyspace1-table1-jb-1-Data.db
[
{""key"": ""736f6d652d6b6579"",""metadata"": {""deletionInfo"": {""markedForDeleteAt"":294205259775,""localDeletionTime"":0}},""columns"": [["""","""",1387824835597000], [""value1"",""some-value"",1387824835597000]]}
]
{code}

There's a random tombstone inserted!

We've hit this bug in production. We never use delete for this column family, but the tombstones appeared for each row. The timestamp looks random. In our case it was mostly in past, but sometimes (about 3% rows) it was in the future. That's even worse than missing a row. In that case you cannot simply add it again - tombstone from the future will hide it.

Fortunately, we have noticed that quickly and canceled the migration. However, we were quite lucky. There are no warnings or errors during the whole process. Losing less than 3% of data may be hard to noticed at first sight for many kind of apps.
",,,,,,,,,,,,,,,,,,,,,,,,26/Dec/13 21:16;yukim;6527-2.0.txt;https://issues.apache.org/jira/secure/attachment/12620552/6527-2.0.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-26 19:17:58.772,,,no_permission,,,,,,,,,,,,365407,,,Thu Dec 26 22:01:34 UTC 2013,,,,,,0|i1qzef:,365709,2.0.3,,,,,,,jbellis,jbellis,,,2.0 beta 1,,,,,,,"26/Dec/13 19:17;yukim;Confirmed.

SSTable version 'ic' has the following in partition header.

Partition size: 8 bytes
Partition deletion time: 4 bytes + 8 bytes
Cell count: 4 bytes

When streaming 'ic' to 2.0, it is reading 4 bytes + 8 bytes for deletion time. Then, partition size and cell count are not needed above 2.0, so it skips those 8 bytes and 4 bytes.
It should be 1) skip partition size (8 bytes), 2) read deletion time, then 3) skip cell count (4 bytes).
","26/Dec/13 21:16;yukim;(also: https://github.com/yukim/cassandra/commits/6527)

Patch as well as test case attached.
","26/Dec/13 21:18;yukim;Note that this only happens when streaming ""ic"" version of SSTable.
",26/Dec/13 21:25;jbellis;+1,26/Dec/13 22:01;yukim;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replaying a commit led to java.lang.StackOverflowError and node crash,CASSANDRA-6181,12673448,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,jdamick,jdamick,11/Oct/13 15:01,12/Mar/19 14:07,13/Mar/19 22:29,23/Oct/13 14:18,1.2.12,2.0.2,,,,,1,,,,,"2 of our nodes died after attempting to replay a commit.  I can attach the commit log file if that helps.
It was occurring on 1.2.8, after several failed attempts to start, we attempted startup with 1.2.10.  This also yielded the same issue (below).  The only resolution was to physically move the commit log file out of the way and then the nodes were able to start...  

The replication factor was 3 so I'm hoping there was no data loss...

{code}
 INFO [main] 2013-10-11 14:50:35,891 CommitLogReplayer.java (line 119) Replaying /ebs/cassandra/commitlog/CommitLog-2-1377542389560.log
ERROR [MutationStage:18] 2013-10-11 14:50:37,387 CassandraDaemon.java (line 191) Exception in thread Thread[MutationStage:18,5,main]
java.lang.StackOverflowError
        at org.apache.cassandra.db.marshal.TimeUUIDType.compareTimestampBytes(TimeUUIDType.java:68)
        at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.java:57)
        at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.java:29)
        at org.apache.cassandra.db.marshal.AbstractType.compareCollectionMembers(AbstractType.java:229)
        at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:81)
        at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:31)
        at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:439)
        at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
        at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
        at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
        at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
        at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
        at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
        at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
        at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)

.... etc.... over and over until ....

        at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
        at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
        at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
        at org.apache.cassandra.db.RangeTombstoneList.add(RangeTombstoneList.java:144)
        at org.apache.cassandra.db.RangeTombstoneList.addAll(RangeTombstoneList.java:186)
        at org.apache.cassandra.db.DeletionInfo.add(DeletionInfo.java:180)
        at org.apache.cassandra.db.AtomicSortedColumns.addAllWithSizeDelta(AtomicSortedColumns.java:197)
        at org.apache.cassandra.db.AbstractColumnContainer.addAllWithSizeDelta(AbstractColumnContainer.java:99)
        at org.apache.cassandra.db.Memtable.resolve(Memtable.java:207)
        at org.apache.cassandra.db.Memtable.put(Memtable.java:170)
        at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:745)
        at org.apache.cassandra.db.Table.apply(Table.java:388)
        at org.apache.cassandra.db.Table.apply(Table.java:353)
        at org.apache.cassandra.db.commitlog.CommitLogReplayer$1.runMayThrow(CommitLogReplayer.java:258)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
{code}

",1.2.8 & 1.2.10 - ubuntu 12.04,,,,,,,,,,,,,,,,,,,,,,,22/Oct/13 09:44;slebresne;6181.txt;https://issues.apache.org/jira/secure/attachment/12609619/6181.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-11 15:46:08.571,,,no_permission,,,,,,,,,,,,353071,,,Tue Nov 19 11:31:55 UTC 2013,,,,,,0|i1ovev:,353358,,,,,,,,frousseau,frousseau,,,1.2.8,,,,,,,11/Oct/13 15:46;jbellis;Pattern matching {{RangeTombstoneList}} to Sylvain.,"11/Oct/13 17:03;slebresne;[~jdamick] Actually, the commit log would be useful if you have it, though ideally I'd need the schema too. Feel free to send that to me in private if you prefer.",11/Oct/13 18:25;jdamick;i sent you a link in email @ datastax.,"22/Oct/13 09:44;slebresne;I unfortunately haven't been to reproduce with the commit log from Jeffrey.

That being said, looking at the stacktrace more closely, I don't think that this is an infinite loop. Rather, in some insertion cases, we have to iterate over all (or a large part) of the range tombstones and that is currently done recursively so this can blow up the stack. The blow-up does reproduce rather easily in a unit test (with 3K range tombstone, which is not small, but not all that much). I though we would be unlikely to run into that case with the way range tombstones are used in practice, but I suppose that's still possible if you have multiple clustering columns so maybe that's just that.

Anyway, I don't really another fix than to rewrite the logic non-recursively.  Attaching a patch for this. This is probably a little bit more involved that what I'd like to push in 1.2 at this point, but at same I don't think there is any simpler way to fix this. On the bright side, RangeTombstoneList is relatively well covered by unit tests.

[~exabytes18], [~jdamick]: If you guys could check that the attached patch does fix this for you, that would be awesome.
","22/Oct/13 12:41;frousseau;Don't know if this can help but, unit tests do not set ""-Xss"" parameter and uses the default value (1M in general, see http://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.html ) while default Xss is 180k for  Cassandra1.2

Here are some raw numbers using unit tests and setting Xss in the build.xml :
N = 193; // 180k
N = 313; // 228k
N = 394; // 256k
N = 1036; // 512k
N = 2321; // 1024k

This number represents the number of inserted tombstones before the tests starts failing (ie : incrementing this number by one : tests fails with SOException)
This should probably explain why it was not reproducible.
By the way, maybe the unit tests should set the Xss parameter in order to be as close as possible of a running cassandra instance ?

","23/Oct/13 08:23;slebresne;Right, it's really not that hard to get into with the default stack trace, which probably explain why we already have 2 people reporting this.

bq. maybe the unit tests should set the Xss parameter in order to be as close as possible of a running cassandra instance ?

Not a bad idea. I've ninja-committed that, though I fixed it to 256k because that's the default in 2.0 and for 1.2, 180k is known to cause problem on some platforms and didn't wanted with that.","23/Oct/13 13:51;frousseau;After reviewing patch, here are some changes in RangeTombstone.java:

L143: '(pos >= 0 ? pos : -pos-1)' should probably be '(pos >= 0 ? pos+1 : -pos-1)'
if pos is positive, it means that start == ends[pos], and the interval should be inserted at (pos+1)


Below is a simple test to reproduce this behaviour:

{noformat}
    @Test
    public void overlappingPreviousEndEqualsStartTest1()
    {
        RangeTombstoneList l = new RangeTombstoneList(cmp, 0);
        // add a RangeTombstone, so, last insert is not in insertion order
        l.add(rt(11, 12, 2));
        l.add(rt(1, 4, 2));
        l.add(rt(4, 10, 5));

        assertEquals(2, l.search(b(3)).markedForDeleteAt);
        assertEquals(5, l.search(b(4)).markedForDeleteAt);
        assertEquals(5, l.search(b(8)).markedForDeleteAt);
        assertEquals(3, l.size());
    }
{noformat}

Some very minor changes in comments:
L200: should replace 'insertFrom' to 'addInternal'
L394: should the commented line be : setInternal(i, start, ends[i], markedAt, delTime) ? (it seems more in the spirit)
L422: TODO can be removed, because implemented a few lines above

Otherwise, LGTM
","23/Oct/13 14:17;slebresne;Correct about L143, thanks. Updated and committed, thanks.","16/Nov/13 23:41;exabytes18;[~slebresne] Can you give a concrete cql schema + access pattern which causes this to occur? What would the effective usage limits be before and after this patch (i.e. how much improvement does this patch provide)?

Our application does a certain number of deletions which are unavoidable. It seems under our current design, cassandra cannot accommodate our workload and we're trying to understand what to do differently.","18/Nov/13 13:45;slebresne;bq. What would the effective usage limits be before and after this patch

Before, it would crash with the exception in the description above, after it won't. This is really ""just"" a bug fix, if you don't run into it, there is nothing this patch will do for you. If you do, then it will fix the problem.","18/Nov/13 23:00;exabytes18;Sorry, I mean what sort of DELETE statements cause this to happen? From reading these comments, it seems that there's some threshold at which too many deletions cause this to occur? Does the patch raise or eliminate this limit?","19/Nov/13 09:32;slebresne;This is related to range tombstones. We mainly use range tombstone in two cases:
# you do a 'DELETE FROM X WHERE Y' (so the whole CQL3 row) *and* Y contains at least a clustering column (if it contains only the partition key for instance, we don't use a range tombstone).
# you set a collection (but not when you append/add/remove to it).

And this may be triggered if you reach some amount of such range tombstones within one partition. But 1) this is not guaranteed at all, as this depends on the ranges covered by those tombstones and the insertion order of those tombstones (and the exact conditions can't be easily characterize, sorry) and 2) the exact number of tombstones after which this may be triggered depends on the stack size (see Fabien's comment above).

In any case, the patch eliminate this entirely.",19/Nov/13 11:31;exabytes18;Fantastic. Thanks for the clarification!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2.0 HSHA server introduces corrupt data,CASSANDRA-6285,12677127,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,xedin,davedamoon,davedamoon,01/Nov/13 18:45,12/Mar/19 14:07,13/Mar/19 22:29,19/May/14 21:39,2.0.8,,,,,,4,,,,,"After altering everything to LCS the table OpsCenter.rollups60 amd one other none OpsCenter-Table got stuck with everything hanging around in L0.
The compaction started and ran until the logs showed this:
ERROR [CompactionExecutor:111] 2013-11-01 19:14:53,865 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:111,1,RMI Runtime]
java.lang.RuntimeException: Last written key DecoratedKey(1326283851463420237, 37382e34362e3132382e3139382d6a7576616c69735f6e6f72785f696e6465785f323031335f31305f30382d63616368655f646f63756d656e74736c6f6f6b75702d676574426c6f6f6d46696c746572537061636555736564) >= current key DecoratedKey(954210699457429663, 37382e34362e3132382e3139382d6a7576616c69735f6e6f72785f696e6465785f323031335f31305f30382d63616368655f646f63756d656e74736c6f6f6b75702d676574546f74616c4469736b5370616365557365640b0f) writing into /var/lib/cassandra/data/OpsCenter/rollups60/OpsCenter-rollups60-tmp-jb-58656-Data.db
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:141)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:164)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:296)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)

Moving back to STC worked to keep the compactions running.
Especialy my own Table i would like to move to LCS.
After a major compaction with STC the move to LCS fails with the same Exception.","4 nodes, shortly updated from 1.2.11 to 2.0.2",,,,,,,,,,,,,,,,,,,,,,,06/Mar/14 20:29;mshuler;6285_testnotes1.txt;https://issues.apache.org/jira/secure/attachment/12633219/6285_testnotes1.txt,04/Mar/14 19:40;xedin;CASSANDRA-6285-disruptor-heap.patch;https://issues.apache.org/jira/secure/attachment/12632583/CASSANDRA-6285-disruptor-heap.patch,10/Mar/14 09:42;kvaster;cassandra-attack-src.zip;https://issues.apache.org/jira/secure/attachment/12633659/cassandra-attack-src.zip,28/Jan/14 18:24;rhatch;compaction_test.py;https://issues.apache.org/jira/secure/attachment/12625617/compaction_test.py,10/Mar/14 09:33;kvaster;disruptor-high-cpu.patch;https://issues.apache.org/jira/secure/attachment/12633656/disruptor-high-cpu.patch,10/Mar/14 09:33;kvaster;disruptor-memory-corruption.patch;https://issues.apache.org/jira/secure/attachment/12633657/disruptor-memory-corruption.patch,09/May/14 19:50;brandon.williams;enable_reallocate_buffers.txt;https://issues.apache.org/jira/secure/attachment/12644174/enable_reallocate_buffers.txt,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,2013-11-29 14:47:43.373,,,no_permission,,,,,,,,,,,,356503,,,Mon Jan 26 21:39:11 UTC 2015,,,,,,0|i1pgiv:,356791,2.0.6,,,,,,,brandon.williams,brandon.williams,,,2.0.0,,,,,,rbranson,"02/Nov/13 09:15;davedamoon;After removing all the Data from the OpsCenters Keyspace (and using LCS) and collectiong new Data for a night, the command nodetool compact OpsCenter rollups60 failed with this Exception:

Error occurred during compaction
java.util.concurrent.ExecutionException: java.lang.RuntimeException: Last written key DecoratedKey(-6663228376520744598, 37382e34362e3132382e3139382d6a7576616c69735f6e6f72785f6c6f6767696e672d706572666f726d616e63655f67726f757065642d67657452656164436f756e740b0f0000000100000009726f6c6c75707336) >= current key DecoratedKey(-6896470603826733036, 37382e34362e3132382e3139382d6d65646970726569735f7365617263685f696e6465785f323031335f31305f30382d6d756c7469776f7264735f70686f6e656d732d6765744c69766553535461626c65436f756e74) writing into /var/lib/cassandra/data/OpsCenter/rollups60/OpsCenter-rollups60-tmp-jb-14-Data.db
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:188)
	at org.apache.cassandra.db.compaction.CompactionManager.performMaximal(CompactionManager.java:281)
	at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1845)
	at org.apache.cassandra.service.StorageService.forceKeyspaceCompaction(StorageService.java:2167)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
	at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1487)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:848)
	at sun.reflect.GeneratedMethodAccessor35.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at sun.rmi.transport.Transport$1.run(Transport.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:556)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:811)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:670)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.RuntimeException: Last written key DecoratedKey(-6663228376520744598, 37382e34362e3132382e3139382d6a7576616c69735f6e6f72785f6c6f6767696e672d706572666f726d616e63655f67726f757065642d67657452656164436f756e740b0f0000000100000009726f6c6c75707336) >= current key DecoratedKey(-6896470603826733036, 37382e34362e3132382e3139382d6d65646970726569735f7365617263685f696e6465785f323031335f31305f30382d6d756c7469776f7264735f70686f6e656d732d6765744c69766553535461626c65436f756e74) writing into /var/lib/cassandra/data/OpsCenter/rollups60/OpsCenter-rollups60-tmp-jb-14-Data.db
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:141)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:164)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:296)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	... 3 more","29/Nov/13 14:47;brevilo;Just in case it helps: I'm getting almost identical exceptions while running a single node stress test using {{cassandra-stress}} with Cassandra 2.0.2 (DSC) on Debian Wheezy with 2 GB RAM. I'm running 10e7 write ops on a single HDD, using (more or less) Cassandra's default configuration, specifically STC.

{noformat}
ERROR [CompactionExecutor:14] 2013-11-29 15:33:39,978 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:14,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(-3658992336117051287, 3033353732383438) >= current key DecoratedKey(-4078405136366838408, 3033353634323236) writing into /srv3/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-tmp-jb-106-Data.db
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:141)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:164)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
{noformat}","15/Jan/14 18:06;brandon.kearby;I'm getting it as well on 2.0.4. I'm testing a new cluster. I don't get the error with one node, but when I add two or more I get the same error.
{code}
ERROR [CompactionExecutor:6] 2014-01-15 17:13:13,395 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:6,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(-1983406872803353678, 545749545445523a333535383030333439353835353830303334) >= current key DecoratedKey(-7683510718755081698, 545749545445523a333639333235363931383339333238323537) writing into /BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-tmp-jb-121-Data.db
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:142)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:165)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{code}

Here's the schema I'm testing with:
{code}
create column family signal
  with column_type = 'Standard'
  and comparator = 'UTF8Type'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'UTF8Type'
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and gc_grace = 432000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'
  and caching = 'ALL'
  and compaction_strategy_options = {'sstable_size_in_mb' : '160'}
  and comment = 'A store of information about each individual signal.'
  and column_metadata = [
    {column_name : 'type',
    validation_class : UTF8Type},
    {column_name : 'foo_id',
    validation_class : LongType},
    validation_class : UTF8Type}]
  and compression_options = {'sstable_compression' : 'org.apache.cassandra.io.compress.LZ4Compressor'};
{code}",15/Jan/14 18:16;jbellis;Can you enable snapshot_before_compaction and post the sstables that it's trying to compact?  I can get you a private upload place if necessary.,"15/Jan/14 18:24;brandon.kearby;Sure,

BTW, I tried changing to SizeTieredCompactionStrategy and got the same error. I'll enable snapshot_before_compaction.","15/Jan/14 18:47;brandon.kearby;Snapshot of compaction before failing
Added attachment: system-compactions_in_progress-jb-25-Data.db

Logs before failing
{code}
INFO [CompactionExecutor:6] 2014-01-15 18:38:47,690 ColumnFamilyStore.java (line 740) Enqueuing flush of Memtable-compactions_in_progress@856586691(847/8470 serialized/live bytes, 35 ops)
 INFO [FlushWriter:3] 2014-01-15 18:38:47,691 Memtable.java (line 333) Writing Memtable-compactions_in_progress@856586691(847/8470 serialized/live bytes, 35 ops)
 INFO [FlushWriter:3] 2014-01-15 18:38:47,700 Memtable.java (line 373) Completed flushing /BigData/lib/cassandra/data/system/compactions_in_progress/system-compactions_in_progress-jb-24-Data.db (304 bytes) for commitlog position ReplayPosition(segmentId=1389810508756, position=33429429)
 INFO [CompactionExecutor:6] 2014-01-15 18:38:47,703 CompactionTask.java (line 115) Compacting [SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-45-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-46-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-67-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-72-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-78-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-75-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-56-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-62-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-66-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-49-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-47-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-57-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-61-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-79-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-65-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-50-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-58-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-77-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-54-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-53-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-48-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-64-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-68-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-76-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-55-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-74-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-60-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-52-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-69-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-71-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-73-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-63-Data.db')]
 INFO [CompactionExecutor:6] 2014-01-15 18:39:05,208 ColumnFamilyStore.java (line 740) Enqueuing flush of Memtable-compactions_in_progress@2046507929(0/0 serialized/live bytes, 1 ops)
 INFO [FlushWriter:3] 2014-01-15 18:39:05,208 Memtable.java (line 333) Writing Memtable-compactions_in_progress@2046507929(0/0 serialized/live bytes, 1 ops)
 INFO [FlushWriter:3] 2014-01-15 18:39:05,218 Memtable.java (line 373) Completed flushing /BigData/lib/cassandra/data/system/compactions_in_progress/system-compactions_in_progress-jb-25-Data.db (42 bytes) for commitlog position ReplayPosition(segmentId=1389810508756, position=33430117)
ERROR [CompactionExecutor:6] 2014-01-15 18:39:05,220 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:6,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(-5705444534806265577, 0000000000000000000000000000000000000000000000000000) >= current key DecoratedKey(-7490754936938484492, 00ab1b0000000000000000000000000000000000000000000000) writing into /BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-tmp-jb-80-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:142)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:165)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
 INFO [MemoryMeter:1] 2014-01-15 18:39:08,865 Memtable.java (line 451) CFS(Keyspace='system', ColumnFamily='sstable_activity') liveRatio is 14.596825396825396 (just-counted was 14.596825396825396).  calculation took 1ms for 84 cells
 INFO [MemoryMeter:1] 2014-01-15 18:43:44,575 Memtable.java (line 451) CFS(Keyspace='system', ColumnFamily='sstable_activity') liveRatio is 14.591111111111111 (just-counted was 14.585396825396826).  calculation took 4ms for 210 cells
(END) 
{code}","15/Jan/14 18:50;jbellis;can you tar up all the components, not just .db?","15/Jan/14 18:54;jbellis;... for all the sstables in the ""Compacting"" list","15/Jan/14 19:50;brandon.kearby;Hi Jonathan,

Here's a link to what you need: http://bhorne.test.s3.amazonaws.com/cassandra.tar.gz
","15/Jan/14 21:11;brandon.kearby;So it seems like it might be related to the hsa server. BTW, I was getting https://issues.apache.org/jira/browse/CASSANDRA-6373 where it would hang describing the ring. So I upgraded to thrift-server-0.3.3.jar. When running with the sync server, I don't get the error above.

A little more context, I'm using pig and the CassandraStorage class to drive the writes. Running as a map task with 12 concurrent mappers creates 2773 connections!

 lsof -i tcp:9160 | wc -l
2773
",15/Jan/14 22:47;jbellis;The  .tar.gz does not contain the sstables mentioned in the error message,15/Jan/14 22:53;brandon.kearby;Correct. The tar contains a full log file with another example of the error.,"15/Jan/14 22:57;jbellis;[~enigmacurry] Can your team reproduce w/ the schema above and the sstables from the tarball?

{noformat}
Compacting [SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-2-Data.db'), STableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-1-Data.db')]
{noformat}","15/Jan/14 23:34;brandon.kearby;BTW, It happens when we use hsha. The schema above is abbreviated as I've left out a lot of the payload from the signal table. Ping me if you need the full signal table definition.","15/Jan/14 23:40;brandon.kearby;When I cranked up the  number of rpc_max_threads and changed to sync, it stopped happening.","27/Jan/14 18:55;brandon.kearby;After doing some more digging, looks like my issue is the same as https://issues.apache.org/jira/browse/CASSANDRA-4687
",27/Jan/14 19:10;jbellis;[~rhatch] would still be useful to try to repro w/ Brandon's instructions since we don't have a way to repro 4687 yet.,"27/Jan/14 20:39;rhatch;[~jbellis] -- I was able to get the exception to occur by doing the following:

create a new cluster with ccm, and populate with 3 nodes
{noformat}
create keyspace SocialData with placement_strategy='org.apache.cassandra.locator.SimpleStrategy' and strategy_options = {replication_factor:3};
{noformat}
create signal Column Family (I had to modify schema above a little bit to make it work):
{noformat}
create column family signal
  with column_type = 'Standard'
  and comparator = 'UTF8Type'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'UTF8Type'
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and gc_grace = 432000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'
  and caching = 'ALL'
  and compaction_strategy_options = {'sstable_size_in_mb' : '160'}
  and comment = 'A store of information about each individual signal.'
  and column_metadata = [
    {column_name : 'type', validation_class : UTF8Type},
    {column_name : 'foo_id', validation_class : LongType}]
  and compression_options = {'sstable_compression' : 'org.apache.cassandra.io.compress.LZ4Compressor'}; 
{noformat}
stopped the nodes
copied all the files from the provided tar's /data/SocialData/ directory to one of my nodes
started the nodes up again
At this point I didn't find any data in the signal column family (using 'list signal;')
The exception appeared in the node's log
{noformat}
ERROR [CompactionExecutor:10] 2014-01-27 12:45:26,734 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:10,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(4322717900587903123, 706f737431353834373031323038270903ae0022076d9f) >= current key DecoratedKey(-7009815163526224622, 545749545445523a333533343836323333393032363439333437) writing into /home/rhatch/.ccm/test_cluster_1390845354/node1/data/SocialData/signal/SocialData-signal-tmp-jb-7-Data.db
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:141)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:164)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{noformat}
I was curious if repair would have any bearing, so I ran repair on one node (after which I can see data in the signal table), then I stopped and started the nodes again -- a similar exception appears in the log for all 3 nodes ('Last written key DecoratedKey ...').

I'm not 100% certain if my procedure for using the provided tar's test data was correct, so let me know if there's anything obvious I missed and I'll run through it again.",27/Jan/14 20:45;jbellis;Was that 2.0 HEAD or 2.0.4?,"27/Jan/14 20:50;rhatch;oh sorry, forgot that detail. I reproduced from the cassandra-2.0.2 tag.",27/Jan/14 20:52;jbellis;Can you try 2.0 HEAD as well just to be sure?,"27/Jan/14 21:11;rhatch;OK, appears we have the same issue on 2.0 HEAD as well (8bbb6e...) -- exception appears on startup using the procedure I included earlier.",27/Jan/14 21:17;rhatch;I'm going to attempt to condense this down to a simple dtest as well.,"27/Jan/14 21:45;brandon.kearby;Hi [~rhatch], 

Here's the full schema I'm using to test with:

create keyspace SocialData
  with placement_strategy = 'NetworkTopologyStrategy'
  and strategy_options = {DC-Analytics : 3}
  and durable_writes = true;

use SocialData;

create column family signal
  with column_type = 'Standard'
  and comparator = 'UTF8Type'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'UTF8Type'
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and gc_grace = 432000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'
  and caching = 'NONE'
  and compaction_strategy_options = {'sstable_size_in_mb' : '160'}
  and comment = 'A store of information about each individual signal.'
  and column_metadata = [
    {column_name : 'type',
    validation_class : UTF8Type},
    {column_name : 'department_id',
    validation_class : LongType},
    {column_name : 'ecosystem_account_id',
    validation_class : UTF8Type},
    {column_name : 'content_type',
    validation_class : UTF8Type},
    {column_name : 'rating_count',
    validation_class : LongType},
    {column_name : 'service_account_id',
    validation_class : UTF8Type},
    {column_name : 'time',
    validation_class : LongType},
    {column_name : 'organization_id',
    validation_class : LongType},
    {column_name : 'conversation_id',
    validation_class : UTF8Type},
    {column_name : 'favorites_count',
    validation_class : LongType},
    {column_name : 'dislike_count',
    validation_class : LongType},
    {column_name : 'url',
    validation_class : UTF8Type},
    {column_name : 'impressions',
    validation_class : LongType},
    {column_name : 'network_strength',
    validation_class : LongType},
    {column_name : 'parent_signal_id',
    validation_class : UTF8Type},
    {column_name : 'account_snapshot_id',
    validation_class : UTF8Type},
    {column_name : 'region_id',
    validation_class : LongType},
    {column_name : 'time_bucket',
    validation_class : LongType},
    {column_name : 'enriched_on',
    validation_class : LongType},
    {column_name : 'dachis_account_id',
    validation_class : UTF8Type},
    {column_name : 'text',
    validation_class : UTF8Type},
    {column_name : 'sentiment',
    validation_class : LongType},
    {column_name : 'like_count',
    validation_class : LongType},
    {column_name : 'industry_id',
    validation_class : LongType},
    {column_name : 'service',
    validation_class : UTF8Type},
    {column_name : 'cloned_from',
    validation_class : UTF8Type},
    {column_name : 'constituent_type',
    validation_class : UTF8Type},
    {column_name : 'listings_count',
    validation_class : LongType},
    {column_name : 'network_size',
    validation_class : LongType},
    {column_name : 'analyzed',
    validation_class : Int32Type},
    {column_name : 'username',
    validation_class : UTF8Type},
    {column_name : 'service_signal_id',
    validation_class : UTF8Type},
    {column_name : 'language',
    validation_class : UTF8Type},
    {column_name : 'brand_id',
    validation_class : LongType},
    {column_name : 'rating',
    validation_class : LongType},
    {column_name : 'relationship_id',
    validation_class : UTF8Type}]
  and compression_options = {'sstable_compression' : 'org.apache.cassandra.io.compress.LZ4Compressor'};

","28/Jan/14 18:22;rhatch;I was able to repro with the data provided in the tar (as noted above). Unfortunately my attempts to use dtest to reproduce the issue ""from scratch"" haven't been successful. First I tried testing with dtest w/CQL, but had no luck, though I may have missed something when trying to translate everything into CQL.

Next I tried dtest w/thrift but similarly wasn't able to trigger the issue in this way. I tried using the hsha rpc_server and was getting what appeared to be an unrelated error:
{noformat}
ERROR [Thrift-Selector_0] 2014-01-28 11:09:16,762 Message.java (line 153) Read an invalid frame size of 0. Are you using TFramedTransport on the client side?
{noformat}

I'll attach my basic dtest here in case it's useful later, but as for now I can't repro the issue without the provided db.","31/Jan/14 22:06;thobbs;[~brandon.kearby] It looks like there may be a few things going on here.

The first is that some of your column names are not valid UTF-8.  I'm not terribly familiar with the UTF-8 specs, but they seem to fail validation in different ways, and Python seems to agree that they are not valid UTF-8, so I don't think it's a problem with our validation code.  Did you change the comparator from BytesType to UTF8Type at some point?  It might not be relevant to this ticket, but you may want to check on that on your end.

The second problem is that SocialData-signal-jb-2-Data.db has some out-of-order rows.  It looks like about 9 rows are randomly out of place in the sstable.  Running scrub would fix this, but I think it's erroring on UTF8 validation.  If I change the comparator to BytesType, the scrub completes and the rows are written in order.  So the problem is not necessarily with compaction itself but with out-of-order rows being written to sstables.

Given that switching from hsha to sync seemed to fix the problem, I wonder if that's part of the original cause.","06/Feb/14 17:32;brandon.kearby;[~thobbs], The odd thing is that with hsha, it works with one node. When we have two or more nodes in the cluster, it starts getting these errors.","06/Feb/14 18:59;ravilr;cc [~xedin]
we were also seeing such random out of place partitions/rows in sstables (rows not hashing to the node) while using disruptor based hsha thrift server, causing compaction to fail with out of order keys. this used to happen on freshly flushed sstables in L0.  We also used to see thrift validation failing on some columns while reading back.  We don't see these after switching back to sync server.


 ","06/Feb/14 20:29;xedin;[~ravilr] Can you try with the most recent release of hsha, version 0.3.3? Just remove the old jar and drop in new one, that should be sufficient.","06/Feb/14 20:58;brandon.kearby;[~xedin], I was running with 0.3.3. The previous version would hang on describe ring for me.","06/Feb/14 22:19;xedin;[~rhatch] The 0 frame size you are seeing is a known Thrift problem which happens even with stock server implementations, they are working on it but it shouldn't cause any problems as such frames are ignored (it could also happen if something does e.g. telnet to the thrift port). I'm not sure that this is a problem with HsHa directly but might be unveiled by the increased throughput you can get with HsHa comparing to sync, it looks exactly like https://issues.apache.org/jira/browse/CASSANDRA-4687 (as [~brandon.kearby] mentioned) so can you try disabling key_cache and try uploading again with hsha?","06/Feb/14 23:12;brandon.kearby;[~xedin], I've tried disabling the key_cache and it didn't help. That was the last thing I tried.","06/Feb/14 23:26;xedin;I see that you have it set to NONE in CF schema but have you also tried disabling it all together in yaml? I'm not saying that it would help but trying to eliminate all possibilities. It's just not obvious to me if it's a hsha problem how Thrift could actually be correctly interpreting erroneous data from the socket, dispatching it the right Thrift handler and deserializing whole mutation (and meta information) to insert it into storage...","07/Feb/14 02:56;ravilr;Also, one more factor with disruptor based hsha is direct memory/Unsafe versus heap-based message buffers. When we encountered this issue, we were running with jna,  hence was using direct memory buffers. I didn't test with heap-based message buffers. ","17/Feb/14 19:00;ngrigoriev;I have started seeing these too. Surprisingly...after adding OpsCenter CE to my cluster. I do not see these associated with my own data.

{code}
java.lang.RuntimeException: Last written key DecoratedKey(3542937286762954312, 31302e332e34352e3135382d676574466c757368657350656e64696e67) >= current
key DecoratedKey(-2152912038130700738, 31302e332e34352e3135362d77696e7465726d7574655f6a6d657465722d776d5f6170706c69636174696f6e732d676574526563656e744
26c6f6f6d46) writing into /hadoop/disk1/cassandra/data/OpsCenter/rollups300/OpsCenter-rollups300-tmp-jb-5055-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:142)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:165)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
{code}",18/Feb/14 00:48;kohlisankalp;This issue is also in logs attached in CASSANDRA-6716. ,"20/Feb/14 20:06;ngrigoriev;Can confirm on my side. I have switched to ""sync"" RPC server and after few scrubs/restarts I am running my load tests on a 6-node 2.0.5 cluster without a single exception in last ~8 hours.

I tried to correlate the moment I started getting large number of FileNotFoundException's with other events in my cluster....realized that it was not exactly 2.0.5 upgrade. It seems to correlate mostly with a moment when my jmeter server went out of free space and a bunch of tests crashed. Obviously, these crashes have terminated a few hundreds of client connections to Cassandra.

Not sure if it is related but it seems that from that moment it was some sort of snowball effect.","03/Mar/14 09:37;krummas;I think we can conclude that this only happens with HSHA

I brought back TThreadedSelectorServer instead of the Disruptor based server and have been running it for a few hours without the bug happening.

Could someone ([~kvaster] ?) try out https://github.com/krummas/cassandra/commits/marcuse/hsha and see if you can break it?

Note that i had to make a change in thrift 0.9.1 to get it to build and work, I'll follow up on that if this seems to solve the issue.","03/Mar/14 11:06;kvaster;https://www.dropbox.com/s/3ldg10zh7qvva27/cassandra-attack.jar

Schema is located inside jar file - cassandra.txt

1. Start cassandra
2. java -jar cassandra-attack.jar
3. Stop cassandra
4. Start cassandra - commit logs will be corrupted.","03/Mar/14 13:38;ngrigoriev;[~krummas]

I think using HSHA makes it easier to reproduce but...I am running SYNC for over a week now and recently I have experienced the same issue again.

We had another unclean shutdown (hrrr...some people are smarter than the UPSes ;) ) and after bringing the nodes back I have found that  on one node my compactions constantly fail with FileNotFoundException. Even worse, I can't scrub the keyspace/CF in question because ""scrub"" fails instantly with ""RuntimeException: Tried to hard link to file that does not exist..."". I have reported that one too. It is impossible to scrub. The only way to fix that issue I have found so far is to restart Cassandra on that node, stop compactions as soon as it starts (well, I could disable them differently, I assume) and then scrub. Sometimes I have to do it in several iterations to complete the process. Once I scrub all problematic KS/CFs I see no more exceptions.","03/Mar/14 13:42;kvaster;Bug with FileNotFoundException is not related to HsHa problem.
And about several iterations for scrub:
https://issues.apache.org/jira/browse/CASSANDRA-6791
","03/Mar/14 15:31;jbellis;According to http://mail-archives.apache.org/mod_mbox/cassandra-user/201402.mbox/%3C038601cf28ea$a2e504d0$e8af0e70$@struq.com%3E the 0.9 TThreadSelectorServer works well, although I'm not sure if he means that it performs better than 0.8 TTSS or just that it doesn't cause corruption. :)","03/Mar/14 16:19;kvaster;I've tried to investigate problem with HsHaDistruptorServer, but with no luck. Telling the truth, I see no reason for that server to corrupt data.
Also HsHaDistruptor do not corrupt data in case useHeapBasedAllocation is turned on.
More over if you look at disruptor-thrift-server code - Message.reallocateDataBuffer and turn on heap based allocation only for dataBuffer then you will not see corruption.","03/Mar/14 18:14;xedin;[~ngrigoriev] and [~brandon.kearby] can you try setting useHeapBasedAllocation to ""true"" ? I'm fine with switch back to TThreadedSelectorServer if that helps.","03/Mar/14 19:22;ngrigoriev;[~xedin]

That seems to be a parameter of the Thrift server...How do I control this parameter? Or I should just disable JNA?","03/Mar/14 19:45;xedin;You can do it via JMX or disable JNA, I can also make a patch with would set it explicitly in Cassandra code.","04/Mar/14 17:18;chris.wirt;[~Jonathan Ellis] [~Marcus Eriksson] That was my post from the user mail list.
After our 1.2.14 -> 2.0.5 upgrade and failure to get the new HsHa stable in our system, we moved to using the thrift 0.9.1 TTSS with reasonable success. We've now been running for two weeks under a relatively high read load.
We haven't seen any ""DecoratedKey != ..."" errors.
We have seen some warnings on start up about SSTable rows being out of order.
We have seen commit logs starting to build up with ""All time blocked"" on the FlushWriter incrementing.

Performance comparisons might be a little unfair, but certainly our p95, p99 have overall improved.

Obviously very keen to not be running a custom build of C*.",04/Mar/14 17:26;krummas;[~chris.wirt] could you paste those startup log lines?,"04/Mar/14 17:45;chris.wirt;These have since disappeared. I just restarted this node just now to check. We haven't run a scrub.

 WARN [main] 2014-02-16 23:23:02,032 LeveledManifest.java (line 171) At level 1, SSTableReader(path='/disk2/cassandra/data/struqrealtime/impressionstorev2/struqrealtime-impressionstorev2-jb-118905-Data.db') [DecoratedKey(-1513272878957942943, c41c955b40274acfa466ccb6079a21e5), DecoratedKey(6301362410765453237, 43ae61aacbc446be92c8bdea1d43e342)] overlaps SSTableReader(path='/disk2/cassandra/data/struqrealtime/impressionstorev2/struqrealtime-impressionstorev2-jb-116400-Data.db') [DecoratedKey(3953001739649874864, 5811ce41b7014917ab82eb32e8861ca5), DecoratedKey(9190609424240623933, 4e5b00a5a7594289924674974f44a995)].  This could be caused by a bug in Cassandra 1.1.0 .. 1.1.3 or due to the fact that you have dropped sstables from another node into the data directory. Sending back to L0.  If you didn't drop in sstables, and have not yet run scrub, you should do so since you may also have rows out-of-order within an sstable
",04/Mar/14 18:41;krummas;[~chris.wirt] those entries are unrelated (and fixed in CASSANDRA-6688),"04/Mar/14 19:40;xedin;The patch sets heap based allocation by default in disruptor server, should make it easier for people to test that scenario...","06/Mar/14 19:05;jbellis;[~mshuler] Can you test hsha with Viktor's jar above?  (https://issues.apache.org/jira/browse/CASSANDRA-6285?focusedCommentId=13917950&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13917950)

I want to know if
# you can reproduce with a single node
# if not, if you can reproduce with multiple nodes
# assuming either 1 or 2, if you can still reproduce after applying Pavel's heap allocation path",06/Mar/14 19:08;mshuler;Sure - let me see what I can find out.,"06/Mar/14 20:29;mshuler;6285_testnotes1.txt attached.

Neither a single node with hsha, nor a 3 node ccm cluster with hsha gave me any interesting errors with the attack jar.  Should I go back and try some of the previous repro steps and check yay/nay on the patch fixing this for those?","06/Mar/14 20:37;xedin;[~mshuler] Can you try the same on the machine running Linux (if you haven't done that yet)? 

Edit: from the log it looks like Disruptor wasn't using the off-heap memory because JNA is disabled, ""Off-heap allocation couldn't be used as JNA is not present in classpath or broken, using on-heap instead."" So it would be great if you could test this on Linux with jna enabled.

Thanks!",06/Mar/14 20:38;mshuler;I'm using a linux machine  :)  - and will link in JNA - good suggestion.,"06/Mar/14 20:51;mshuler;With jna enabled, yes, on a single node, after running the attack jar and restarting c*, I get:
{noformat}
 INFO [main] 2014-03-06 14:46:51,272 ColumnFamilyStore.java (line 254) Initializing tmp.CF
 INFO [main] 2014-03-06 14:46:51,277 ColumnFamilyStore.java (line 254) Initializing system_traces.sessions
 INFO [main] 2014-03-06 14:46:51,280 ColumnFamilyStore.java (line 254) Initializing system_traces.events
 INFO [main] 2014-03-06 14:46:51,281 CassandraDaemon.java (line 291) completed pre-loading (5 keys) key cache.
 INFO [main] 2014-03-06 14:46:51,288 CommitLog.java (line 130) Replaying /var/lib/cassandra/commitlog/CommitLog-3-1394138577628.log, /var/lib/
cassandra/commitlog/CommitLog-3-1394138577629.log
 INFO [main] 2014-03-06 14:46:51,311 CommitLogReplayer.java (line 184) Replaying /var/lib/cassandra/commitlog/CommitLog-3-1394138577628.log (C
L version 3, messaging version 7)
ERROR [main] 2014-03-06 14:46:51,432 CommitLogReplayer.java (line 306) Unexpected error deserializing mutation; saved to /tmp/mutation77387084
28696995512dat and ignored.  This may be caused by replaying a mutation against a table with the same name but incompatible schema.  Exception
 follows: 
org.apache.cassandra.serializers.MarshalException: Invalid version for TimeUUID type.
        at org.apache.cassandra.serializers.TimeUUIDSerializer.validate(TimeUUIDSerializer.java:39)
        at org.apache.cassandra.db.marshal.AbstractType.validate(AbstractType.java:172)
        at org.apache.cassandra.db.commitlog.CommitLogReplayer.recover(CommitLogReplayer.java:276)
        at org.apache.cassandra.db.commitlog.CommitLogReplayer.recover(CommitLogReplayer.java:97)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:151)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:131)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:312)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:471)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:560)
{noformat}

I'll double-check a 3 node cluster, then patch and see where I get.

(edit) this looks quite different than the previously posted errors - not sure if I'm on the right track, here..","06/Mar/14 21:35;mshuler;Both a single local hsha node and 3x node ccm cluster with hsha (jna on both) throw the above errors after attack.jar run and restart.  The patch does appear to fix both single and ccm cluster.  My pre-patch ccm cluster never fully restarted, but do we need logs or anything from before/after?","06/Mar/14 21:39;xedin;I don't think we need logs, [~jbellis] I'm thinking of commiting attached patch which should help meanwhile I working on resolving off-heap problem, WDYT?","06/Mar/14 21:56;benedict;So, I think there may potentially be at least two races in the off heap deallocation. I suspect that may not be everything, though, as these two races probably won't cause the problem often. These are predicated on the assumption that thrift doesn't copy data from the DirectByteBuffer that the hsha server provides to it, so could be wrong, but anyway:

1) CL appends can be lagged behind the memtable update and, as a result, the acknowledgment to the client of success writing. If the CL record contains the ByteBuffer when it is freed, and that address is then reused in another allocation, it will write incorrect data to the commit log.
2) I believe thrift calls are two stage. If this is the case, and the client disconnects in between sending the first stage and receiving the result in the second stage, the buffer could be freed whilst still in flight to the memtable/CL

These are just quick ideas for where it might be, I haven't familiarised myself fully with thrift, the disruptor etc. to be certain if these are plausible, but it may turn out to be useful so thought I'd share.","06/Mar/14 23:21;rbranson;Unfortunately this hit us during our 2.0.5 upgrade and 'sync' is not an option for the # of connections we have per node (tried this). We've been running Marcus' patch in prod and limping along on it, but it looks like the requestInvoke() override is causing the requests to get executed on the selector pool (which is limited by CPU #) instead of the executor service so our response times are pretty bad. The lack of anything showing up in the JMX for the executor service definitely points towards this. ","06/Mar/14 23:57;xedin;[~rbranson] Which Marcus' patch are you talking about? Also I want to clarify one thing - disruptor server doesn't use requestInvoke(FrameBuffer) but dispatchInvoke(Message) which schedules message to executor pool based on ring buffer (WorkerPool) so actual execution is done in the separate thread. I can attach a patch which would switch back to TThreadedSelectorServer which is packed with Thrift (the only different between it and disruptor is that it schedules to classic thread pool), maybe disruptor server wasn't as good an idea for all of the real world use cases...","07/Mar/14 00:34;benedict;It looks like thrift doesn't retain the DirectByteBuffer, just reads straight from them. The only possible window for corruption is during the construction of the thrift args object, which is a fairly narrow window.","07/Mar/14 02:27;rbranson;We put in the TThreadedSelectorServer patch from Marcus. On top of that, to get our response times down from the 10x what they should be, I rolled out a larger hard-coded selector thread pool size of 256 (instead of the # of processors -- a measly 16). This is shaping up nicely.",07/Mar/14 02:38;xedin;[~rbranson] So what you are saying is that after problem with disruptor you never tried it again with on-heap buffers but switched to TThreadedSelectorServer and increased selector pool size and the requestInvoke() is the problem with TThreadedSelectorServer?,07/Mar/14 02:53;rbranson;[~xedin]: We had perf issues with the disruptor as well (sudden spikes of CPU to 100%) + this so I just wanted to get production away from it ASAP.,07/Mar/14 03:22;xedin;[~rbranson] But the most important question for this ticket at least is - did you run with on or off heap buffers? I can bring TThreadedSelectorServer back in this ticket or just go with on-heap buffers and disruptor. [~mshuler] do you have any performance tests related to Thrift server? Maybe there is a low hanging fruit in there to fix up the spikes that Rick mentioned if we can reproduce.,"07/Mar/14 03:36;jbellis;bq. I'm thinking of commiting attached patch which should help meanwhile I working on resolving off-heap problem

Yeah, let's do this for now and roll 2.0.6 so we can stop the bleeding, then figure out whether doing more work on disruptor or TTSS is better for 2.0.7.","07/Mar/14 03:59;rbranson;[~xedin]: off-heap for disruptor.

I think that we should really consider bringing back the old HSHA implementation from 1.2 as the ""hsha"" and allow switching to the disruptor implementation as another rpc_server_type for those that want to try it out.","07/Mar/14 04:10;brandon.williams;bq. I think that we should really consider bringing back the old HSHA implementation from 1.2 as the ""hsha"" and allow switching to the disruptor implementation as another rpc_server_type for those that want to try it out.

I think I'm inclined to agree with this, aside from creating yaml creation problems in a minor.  If we just give up and effectively revert, what have we lost?  We need 2.0 stabilization sooner rather than later now, in all aspects.  If we can't trust disruptor except with a small change, let's just not trust it yet and worry about that in a future release.  With 2.1 beta already out, we can't tolerate much instability in the 2.0 branch.","07/Mar/14 04:37;xedin;I'm not sure if there is a point of going all the way back to original HsHa when there is TThrededSelectorServer, but I'm fine with going with disruptor as a separate option, something like ""disruptor_hsha"" and making ""hsha"" - TThrededSelectorServer from Thrift, that's how I wanted it originally. Also I just want to mention that people have reported that disruptor works for them with on-heap buffers, so I am not sure if we need to go all paranoid about this...","07/Mar/14 07:02;krummas;My branch from above needed a tiny hack to thrift (https://github.com/krummas/thrift/commit/01ba2a3f3d386d0981371aab2494470e2a78e596), so if we want to roll with TTSS we should refactor our thrift usage a bit to avoid that hack","07/Mar/14 09:35;xedin;Ah, so they have finally made transport a protected field in FrameBuffer... Well, that considerably complicates things with switching back TThreadedSelectorServer.","07/Mar/14 10:21;slebresne;Alright, I've committed Pavel's patch above as a stopgap solution as discussed above because I want to start a vote on 2.0.6 asap (the changelog is getting pretty big). I've created CASSANDRA-6815 to decide what we want the followup for that to be for 2.0.7.","07/Mar/14 20:53;mshang;To add to [~rbranson]'s input, we're also seeing the same stacktrace as [~mshuler] (TimeUUID MarshalException). I inspected the row mutations that caused it. Three ranges were nonsensical: the key, the column name, and the value. By nonsensical, I mean that they don't match my expectation of what we are inserting in production data. All other ranges seemed fine (timestamps, masks, sizes, cfid). The key, column name, and value were read successfully, so their length metadata was good. For our data, the column comparator is TimeUUID. Our client library is pycassa. Whereas pycassa generates tuuids like this: 913d7fea-a631-11e3-8080-808080808080, the nonsensical column names look like this: 22050aa4-de11-e380-8080-80808080800b and this: 10c326eb-86a4-e211-e380-808080808080. Most are of the first form. By shifting these nonsensical tuuids to the left or right by an octet, you get a reasonable tuuid. I don't have a similar insight into the nonsensical keys and values, but they could also be left or right shifted.","10/Mar/14 03:55;enigmacurry;I'd like to be able to reproduce this in dtests to track this bug. Seeing as [~rhatch]'s python test wasn't able to repro this issue, and a [quick test I wrote|https://github.com/riptano/cassandra-dtest/blob/cassandra-6285/test_6285.py] doesn't either, does anyone have a simple way to reproduce this issue? 

[~kvaster] would you mind sharing the source code for your attack jar?","10/Mar/14 09:33;kvaster;Attached patches for on-heap disruptor.

First pacth (disruptor-high-cpu.patch) turns off any key interests in case we're waiting for message to be processed. We need that cause processing may be delayed in case of high load and there may be something available to read from stream. In that case we'll have 100% cpu core usage.

Second patch (disruptor-memory-corruption.patch) makes copy from off-heap ByteBuffer when reading binary data. This binary data may be stored inside cassandra as is even after message processing. And binary data can be corrupted - cause it's memory may be already deallocated.","10/Mar/14 09:42;kvaster;Attached cassandra-attack-src.zip - eclipse project for making high load test on cassandra.
This attack uses 100 threads to make writes, reads and deletes.","10/Mar/14 09:51;benedict;Hmm. Just taking a look at Viktor's patch, I realised that my initial conclusions were actually quite plausible and probably (one of) the causes of the problem. When I dismissed them, I didn't realise we were using a custom TBinaryProtocol implementation. In particular (1) is definitely possible, and probably the cause of the issue, although the attack jar source would be helpful to figure out of there are any other potential causes. We should be able to force the problem to occur by artificially delaying the commit log write to prove this.

Either way, I don't think Viktor's patch is the best way to deal with this problem, as it leaves cleaning up the direct buffers to GC. Since we could be creating a lot of these, we could create an awful lot of artificial memory pressure. Honestly, I think the best solution is to simply avoid using direct buffers with thrift, at least until 2.1, which should fix this problem by ensuring the CL _write_ (if not commit) has happened before performing the memtable insertion.",10/Mar/14 09:55;kvaster;My patch is defenetly NOT GOOD. Also for me that patch means anothe thing: it seems that we have 'success' answer  before data is passed to commitlog... I don't think that this is good.,"10/Mar/14 09:59;benedict;bq. we have 'success' answer before data is passed to commitlog

Yes, see my comment from a few days ago:

bq. 1) CL appends can be lagged behind the memtable update and, as a result, the acknowledgment to the client of success writing. If the CL record contains the ByteBuffer when it is freed, and that address is then reused in another allocation, it will write incorrect data to the commit log.

This is an absolutely plausible scenario since we do actually slice directly from the DirectByteBuffer, which I previously thought we did not.","10/Mar/14 10:05;benedict;Has anybody tested this problem against 2.1? As if this is the only issue, it should be fixed there.","10/Mar/14 19:17;enigmacurry;[~benedict] I haven't yet been able to reproduce this with anything other than Viktor's attack jar. I'm thinking Java's threading is beating Python's threading here, so I [created a dtest|https://github.com/riptano/cassandra-dtest/blob/master/thrift_hsha_test.py] that just run's his jar directly. This test is currently passing on cassandra-2.0 and cassandra-2.1 HEAD. ",10/Mar/14 19:23;jbellis;You'd want to revert Pavel's patch from 2.1 to test Benedict's theory.,10/Mar/14 21:17;kvaster;You may set threads count to only one in cass-atack jar and you will be still able to reproduce error.,"11/Mar/14 00:05;benedict;bq. You'd want to revert Pavel's patch from 2.1

beta1 should be fine to test against for this","12/Mar/14 10:46;kvaster;I've tried my test with beta1 and I can confirm that I was not able to reproduce bug.
I think that it will be better to not use disruptor on 2.0.x even with on-heap allocation (we can still reuse buffer in case message will be of equal size when previous).
And it should be safe to use disruptor on 2.1 branch.

We'll be waiting for 2.1 release, cause it really impressed me over 2.0","14/Mar/14 12:45;kvaster;Telling the truth, I don't think that this is really fixed in 2.0.6.
It's not easy to reproduce bug right now, but I think it can be. thrift-disruptor server does not allocate new Buffer for new message in case new message is of equal size with previous. In that case bug can be reproduced even with on-heap allocation.",14/Mar/14 12:54;benedict;+1. That needs to be fixed as well.,"18/Mar/14 15:17;rcoli;{quote}
... Telling the truth, I don't think that this is really fixed in 2.0.6.
{quote}
If hsha is irrevocably broken with data corruption risk in 2.0 line, could we either get it wired off in the next point release, or some messaging in NEWS.txt that instructs people not to use it? My preference is the former to cover upgraders who are foolish enough to not read NEWS.txt; I am unable to see the benefit of leaving it usable if it is known broken.
",20/Mar/14 19:49;appodictic;I read thought this. Does it make sense to call this HSHA2 and restore the old code and call it HSHA? I,"09/May/14 18:01;rbranson;This is not fixed. Still seeing the same exception running 2.0.6.

ERROR [CompactionExecutor:7] 2014-05-09 17:59:58,640 CassandraDaemon.java (line 196) Exception in thread Thread[CompactionExecutor:7,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(132126721345628486111245439753727165857, 0f3b67f2) >= current key DecoratedKey(37424530135488872684523334498941679307, 196b70ab) writing into /data/cassandra/data/redacted/Redacted/redacted-Redacted-tmp-jb-156533-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:142)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:165)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)",09/May/14 19:31;brandon.williams;The line in question: https://github.com/xedin/disruptor_thrift_server/commit/77d6715af0eeba4c52f42fa6ba6549c8ae52ffa7#diff-18c889f19dc9fbeb73af99dcff152b6eR421,09/May/14 19:50;brandon.williams;Patch to enable buffer reallocation.,"09/May/14 23:05;jbellis;I thought we were reallocating by default but I must have gotten that confused with on-heap buffers above.  If Viktor is right, reusing buffers is always potentially dangerous and should just be removed.  Can you comment, [~xedin]?","09/May/14 23:45;xedin;No, by default it's turned off, because Thrift side expectation is that once the invocation is complete nobody else holds the buffers, but it seems like the problem is that on Cassandra side we actually never copy the buffer for the commit log (or was it something else?). So we need to set thrift server to alwayReallocate explicitly.

[~rbranson] I can give you updated jar so you don't have to wait for the release of Cassandra which would have alwaysReallocate set to true by default.","09/May/14 23:55;xedin;So I can do two things, a). set alwaysReuse to true by default and release 0.3.5 today b). you can just switch to alwaysReallocate(true) in the configuration for 2.0.8, either works for me.","10/May/14 04:23;jbellis;Yeah, we do treat BB as immutable so CL would understandably not expect Thrift to pull the rug out from under it.

I'm fine with calling alwaysReallocate on the Cassandra side in the interest of not changing things out from under any other users.","10/May/14 16:10;brandon.williams;I have no issue with doing a) _AND_ b), just to be extra safe, if we know this puts the nail in this ticket's coffin.","11/May/14 16:12;appodictic;I was poking around the dependency a bit

{quote}
Running com.thinkaurelius.thrift.OffHeapMultiRequestTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.515 sec
Running com.thinkaurelius.thrift.OnHeapMultiConnectionWithReallocateTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.481 sec
Running com.thinkaurelius.thrift.OffheapMultiConnectionWithRellocateTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.478 sec
Running com.thinkaurelius.thrift.OnHeapMultiConnectionTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.417 sec
Running com.thinkaurelius.thrift.OnHeapMultiRequestWithReallocateTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.735 sec
Running com.thinkaurelius.thrift.OffHeapMultiRequestWithReallocateTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.437 sec
Running com.thinkaurelius.thrift.OffHeapMultiConnectionTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.282 sec
Running com.thinkaurelius.thrift.OnHeapMultiRequestTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.491 sec
{quote}

Q. Are a few tests that run in roughly 20 seconds enough to prove that this component fit for production? These highly concurrent off heap systems can have very subtle bugs as this ticket shows.

If I understand correct HSHA is not the default, the artifact has good test coverage, and only a handful of findbugs issues. Is there any piece that is going to run end-to-end or attempt to load/concurrently test these classes and be more rigorous then the previous system? Can that be made?","11/May/14 20:57;xedin;[~brandon.williams] I have released 0.3.5 just now, with reallocation and on-heap buffers turned on by default. +1 on the change so it's either we commit 0.3.5 or your patch.

[~appodictic] Here is a definition of [unit testing|http://en.wikipedia.org/wiki/Unit_testing], in my tests I cover single/multi connection on-heap/off-heap +/- reallocation scenarios, basically everything to prove that server functions properly in all of the modes and returns correct results based on the operation being used. end-to-end tests are what systems which integrate project are supposed to do and that is done by stress and bpdlab testing. If you have been following discussion in this ticket you must have already realized that the problem is not caused by HsHa server directly but rather by the fact that Cassandra holds Thrift buffers even after blocking evaluation is finished.",12/May/14 04:36;xedin;[~brandon.williams] 0.3.5 is already available on [maven central|http://search.maven.org/remotecontent?filepath=com/thinkaurelius/thrift/thrift-server/0.3.5/thrift-server-0.3.5.jar] so act as you think appropriate.,"12/May/14 17:52;appodictic;@Pavel I understand what you are saying. I understand what unit test is. Whenever I submit patches to the ASF they come with tests. ::cough:: ::cough::. In any case, what I was saying is the external dependency does not do load testing. Cassandra does not default to hsha. I DO NOT see any reference in this conversation into how exactly the HSHA server is now load/correctness tested. If such a test exists great, if not potentially should be added.","12/May/14 18:17;appodictic;Also while we are on the topic.

{quote}
# The default is sync because on Windows hsha is about 30% slower.  On Linux,
# sync/hsha performance is about the same, with hsha of course using less memory.
#
# Alternatively,  can provide your own RPC server by providing the fully-qualified class name
# of an o.a.c.t.TServerFactory that can create an instance of it.
rpc_server_type: sync
{quote}

The logic behind this default confuses me. The the vast majority of the cassandra user base is linux. We chose 'sync' so the uncommon case is not slowed down. Clearly anyone using linux should switch to hsha because it uses less memory and is wiched fast according to github tests. But not being the default does it really get performance/correctness evaluated in any meaningful way?

","12/May/14 20:07;benedict;For many workloads sync is faster than async on linux also (by a significant margin), so perhaps the docs should be updated.","13/May/14 18:06;rbranson;I think what might help this specific quality issue out is just moving to the new HSHA implementation entirely in a later version and removing the choice. The new HSHA supposedly eliminates the performance issues that made it not a good default choice, so it appears as if there's no advantage to having the other choices.","13/May/14 18:41;brandon.williams;For 2.1, I can get behind that I think, especially calling it 'disruptor' or pretty much anything besides 'HSHA.'  For 2.0 though it's hard to swallow in a minor.","13/May/14 21:11;rbranson;I did some more digging around on our cluster that was running 2.0.6 when it saw the corruption: it took anywhere from a few hours to 48 hours for the first compaction with the out of order key exception to throw. These nodes are receiving thousands of writes per second, so it's not going to be trivially reproducible. We've been running one of the nodes with 2.0.8-tenative + enable_reallocate_buffers.txt and will report back once we've reached 72 hours and are comfortable rolling this out wide to our own clusters.",19/May/14 18:36;rbranson;Haven't been able to repro in over 5 days. We're considering the enable_reallocate_buffers.txt patch fixed and production-ready.,"19/May/14 19:04;brandon.williams;I committed this patch to 2.0, but did not update the disruptor jar for fear of any further regressions, so the patch Rick tested is in there.  For 2.1, I committed both this patch and disruptor 0.3.5.","19/May/14 19:13;brandon.williams;Oops, wait, I only changed the maven dependency.  [~mishail] could you clean up the 2.1+ side of things?",19/May/14 21:35;mishail;[~brandon.williams] done.,19/May/14 21:39;brandon.williams;Thanks.,"21/Oct/14 18:41;sterligovak;It looks like this is not fixed in 2.1.0. We have cassandra under heavy load through binary interface and only OpsCenter by thrift. OpsCenter rollups are corrupted in about an hour after scrub.

{quote}
ERROR [CompactionExecutor:71] 2014-10-21 22:16:39,950 CassandraDaemon.java:166 - Exception in thread Thread[CompactionExecutor:71,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(-7581200918995348250, 39352e3130382e3234322e32302d6973732d73686172645f696e666f2d676574426c6f6f6d46696c74657246616c7365506f73697469766573) >= current key DecoratedKey(-8301289422298317140, 800100010000000c62617463685f6d75746174650006d04a0d00010b0d0000000100000025) writing into /ssd/cassandra/data/OpsCenter/rollups60/OpsCenter-rollups60-tmp-ka-9128-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:172) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:196) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:110) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:177) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:74) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:235) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_51]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]
        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]
{quote}

We'll try to switch to sync and see what will happen.

Is it possible that streaming hangs because of that exception? Is it possible that this exception affect minor compactions of other keyspaces?","21/Oct/14 18:51;ngrigoriev@gmail.com;[~sterligovak] I was always wondering why did I always see these problems appearing for OpsCenter keyspace. My keyspace had much more traffic but when I had this problem - it always manifested itself with OpsCenter keyspace. Even when I was also using Thrift (we use native protocol now).

I even remember disabling OpsCenter to prove the point :) 

","21/Oct/14 23:37;sterligovak;Have you proven that it's really related to OpsCenter?

We've switched to ""sync"", but still get corrupted sstables. Now we get exception not during compaction, but at start:
{quote}
ERROR [SSTableBatchOpen:10] 2014-10-22 02:47:48,762 CassandraDaemon.java:166 - Exception in thread Thread[SSTableBatchOpen:10,5,main]
java.lang.IllegalStateException: SSTable first key DecoratedKey(4206305143314087741, 800100010000000c62617463685f6d7574617465000010250d00010b0d000000010000004e33372e3134302e3134312e3231322d6973732d736c6f745f636f6e66696775726174696f6e5f746172) > last key DecoratedKey(-4632241097675266745, 800100010000000c62617463685f6d7574617465000010260d00010b0d000000010000005133372e3134302e3134312e3231322d6973732d736c6f745f636f6e66696775726174696f6e5f746172676574)
        at org.apache.cassandra.io.sstable.SSTableReader.validate(SSTableReader.java:1083) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:398) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:294) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableReader$4.run(SSTableReader.java:430) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_51]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]
        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]
{quote}

And nodetools scrub doesn't help. It finds no errors and after restart we get same exceptions.","22/Oct/14 00:56;ngrigoriev@gmail.com;I think this is the error that you cannot fix by scrubbing. Corrupted sstable. I was fixing those by deleting the sstables and doing repairs. Unfortunately, if that happens on many nodes there is a risk of data loss.

As for the OpsCenter - do not get me wrong ;) I did not want to say that OpsCenter was directly responsible for these troubles. But I do believe that OpsCenter does something particular that reveals the bug in hsha server. At least this was my impression. After disabling OpsCenter and fixing the outstanding problems I do not recall seeing those errors anymore. And I was also using Thrift and I was writing and reading 100x more data than OpsCenter.

","22/Oct/14 06:57;xedin;[~sterligovak] Did you get any WARN messages like this ""N out of order rows found while scrubbing <file>; Those have been written (in order) to a new sstable <new-file>"" while running scrub? Anyhow, you will have to delete affected files and repair from the neighbors, I'm also not sure how much of an involvement Thrift has in this because the only thing that could go wrong (shared buffers) was already fixed to be copied for every request and everything is allocated on-heap....

[~rbranson] Are you running HsHa with 2.1 or still on 2.0 ? ","22/Oct/14 07:37;sterligovak;[~xedin] No, I've not seen such messages. sstablescrub failed with NPE. sstables were corrupted on all 17 nodes. I removed them manually and there was no errors overnight. It seems sync really impacted the problem. Maybe there are some another problem which hides with sync server. I still have problems - validation hangs on one table on all nodes :(.","23/Oct/14 18:38;ngrigoriev@gmail.com;By the way, I am getting 

{code}
ERROR [CompactionExecutor:2333] 2014-10-23 18:29:53,590 CassandraDaemon.java (line 199) Exception in thread Thread[Compactio
nExecutor:2333,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(1156541975678546868, 001000000000111100000000000003bc510f000010000
0000003bc510f00000000111100000000100000000000004000000000000000000100) >= current key DecoratedKey(36735936098318717, 001000
0000001111000000000000015feb8a00001000000000015feb8a00000000111100000000100000000000004000000000000000000100) writing into /
cassandra-data/disk2/myks/mytable/myks-mytable-tmp-jb-94445-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:142)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:165)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:198)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{code}

with 2.0.10 release. I am using native protocol. I believe native protocol handler is based on HSHA, am I right? Anyway, I am getting those too.","23/Oct/14 19:12;xedin;[~ngrigoriev] No, native protocol is not using Thrift, which further confirms that it's cross transport problem, I think we should create a separate ticket to handle it.
[~sterligovak] Do you still have stacktrace for the NPE you've got while scrubbing?","24/Oct/14 09:25;sterligovak;[~xedin] That NPE happend once and unfortunatelly I have not saved it. If I'll get it once more I'll save this sstable.
I totally removed OpsCenter keyspace (with sstables) and recreated them. I don't get ""Last written key DecoratedKey"" any more. By the way, this error definetely causees streams to hang on 100%.

I have several strange things happening now:
  - I've noticed that it takes about 30 minutes between ""nodetool repair"" and first pending AntiEntropySession. Is that ok?
  - Repair is already running for 24 hours (~13GB per node, 17 nodes). What's the number of AntiEntropySessions to finish single repair? Number of key ranges?
{quote}
Pool Name                    Active   Pending      Completed   Blocked  All time blocked
CounterMutationStage              0         0              0         0                 0
ReadStage                         0         0         392196         0                 0
RequestResponseStage              0         0        5271906         0                 0
MutationStage                     0         0       19832506         0                 0
ReadRepairStage                   0         0           2280         0                 0
GossipStage                       0         0         453830         0                 0
CacheCleanupExecutor              0         0              0         0                 0
MigrationStage                    0         0              0         0                 0
ValidationExecutor                0         0          39446         0                 0
MemtableReclaimMemory             0         0          29927         0                 0
InternalResponseStage             0         0         588279         0                 0
AntiEntropyStage                  0         0        5325285         0                 0
MiscStage                         0         0              0         0                 0
CommitLogArchiver                 0         0              0         0                 0
MemtableFlushWriter               0         0          29927         0                 0
PendingRangeCalculator            0         0             30         0                 0
MemtablePostFlush                 0         0         135734         0                 0
CompactionExecutor               31        31         502175         0                 0
AntiEntropySessions               3         3           3446         0                 0
HintedHandoff                     0         0             44         0                 0

Message type           Dropped
RANGE_SLICE                  0
READ_REPAIR                  0
PAGED_RANGE                  0
BINARY                       0
READ                         0
MUTATION                     2
_TRACE                       0
REQUEST_RESPONSE             0
COUNTER_MUTATION             0
{quote}
  - Some validation compactions run for more than 100% (1923%). I thinks that it's CASSANDRA-7239, right?
  - the amount of sstables for some CFs is about 15 000 and continues to grow during repair.
  - There are several following exceptions during repair
{quote}
ERROR [RepairJobTask:80] 2014-10-24 13:27:31,717 RepairJob.java:127 - Error occurred during snapshot phase
java.lang.RuntimeException: Could not create snapshot at /37.140.189.163
        at org.apache.cassandra.repair.SnapshotTask$SnapshotCallback.onFailure(SnapshotTask.java:77) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.net.MessagingService$5$1.run(MessagingService.java:347) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_51]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]
        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]
ERROR [AntiEntropySessions:141] 2014-10-24 13:27:31,724 RepairSession.java:303 - [repair #da2cb020-5b5f-11e4-a45e-d9cec1206f33] session completed with the following error
java.io.IOException: Failed during snapshot creation.
        at org.apache.cassandra.repair.RepairSession.failedSnapshot(RepairSession.java:344) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.repair.RepairJob$2.onFailure(RepairJob.java:128) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at com.google.common.util.concurrent.Futures$4.run(Futures.java:1172) ~[guava-16.0.jar:na]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]
        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]
ERROR [AntiEntropySessions:141] 2014-10-24 13:27:31,724 CassandraDaemon.java:166 - Exception in thread Thread[AntiEntropySessions:141,5,RMI Runtime]
java.lang.RuntimeException: java.io.IOException: Failed during snapshot creation.
        at com.google.common.base.Throwables.propagate(Throwables.java:160) ~[guava-16.0.jar:na]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_51]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]
        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]
Caused by: java.io.IOException: Failed during snapshot creation.
        at org.apache.cassandra.repair.RepairSession.failedSnapshot(RepairSession.java:344) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.repair.RepairJob$2.onFailure(RepairJob.java:128) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at com.google.common.util.concurrent.Futures$4.run(Futures.java:1172) ~[guava-16.0.jar:na]
        ... 3 common frames omitted
{quote}",29/Oct/14 15:43;krummas;I think the cause of the latest exceptions in this ticket is CASSANDRA-8211,"26/Jan/15 21:39;rbfblk;I am getting this exception using Thrift HSHA in 2.1.0:

{quote}
 INFO [CompactionExecutor:8] 2015-01-26 13:32:51,818 CompactionTask.java (line 138) Compacting [SSTableReader(path='/tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-ka-2-Data.db'), SSTableReader(path='/tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-ka-1-Data.db')]
 INFO [CompactionExecutor:8] 2015-01-26 13:32:51,890 ColumnFamilyStore.java (line 856) Enqueuing flush of compactions_in_progress: 212 (0%) on-heap, 20 (0%) off-heap
 INFO [MemtableFlushWriter:8] 2015-01-26 13:32:51,892 Memtable.java (line 326) Writing Memtable-compactions_in_progress@1155018639(0 serialized bytes, 1 ops, 0%/0% of on/off-heap limit)
 INFO [MemtableFlushWriter:8] 2015-01-26 13:32:51,896 Memtable.java (line 360) Completed flushing /tmp/cass_test/cassandra/TestCassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-2-Data.db (42 bytes) for commitlog position ReplayPosition(segmentId=1422296630707, position=430226)
ERROR [CompactionExecutor:8] 2015-01-26 13:32:51,906 CassandraDaemon.java (line 166) Exception in thread Thread[CompactionExecutor:8,1,RMI Runtime]
java.lang.RuntimeException: Last written key DecoratedKey(131206587314004820534098544948237170809, 800100010000000c62617463685f6d7574617465000000) >= current key DecoratedKey(14775611966645399672119169777260659240, 726f776b65793030385f31343232323937313537353835) writing into /tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-tmp-ka-3-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:172) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:196) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:110) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:177) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:74) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:235) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_40]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_40]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_40]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_40]
        at java.lang.Thread.run(Thread.java:724) [na:1.7.0_40]
{quote}

I don't think it's caused by CASSANDRA-8211, because it happens during the first compaction that takes place between the first 2 SSTables to get flushed from an initially empty column family.

Also, I've only been able to reproduce it when using both *hsha* for the rpc server and *offheap_objects* for memtable allocation. If I switch either to sync or to offheap_buffers or heap_buffers then I cannot reproduce the problem. Also under the same circumstances I'm pretty sure I've seen incorrect data being returned to a client multiget_slice request before any SSTables had been flushed yet, so I presume this is corruption that happens before any flush/compaction takes place.

nodetool scrub yielded these errors:

{quote}
 INFO [CompactionExecutor:9] 2015-01-26 13:48:01,512 OutputHandler.java (line 42) Scrubbing SSTableReader(path='/tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-ka-2-Data.db') (168780 bytes)
 INFO [CompactionExecutor:10] 2015-01-26 13:48:01,512 OutputHandler.java (line 42) Scrubbing SSTableReader(path='/tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-ka-1-Data.db') (135024 bytes)
 WARN [CompactionExecutor:9] 2015-01-26 13:48:01,531 OutputHandler.java (line 52) Out of order row detected (DecoratedKey(14775611966645399672119169777260659240, 726f776b65793030385f31343232323937313537353835) found after DecoratedKey(131206587314004820534098544948237170809, 800100010000000c62617463685f6d7574617465000000))
 WARN [CompactionExecutor:9] 2015-01-26 13:48:01,534 OutputHandler.java (line 57) Error reading row (stacktrace follows):
java.lang.RuntimeException: Last written key DecoratedKey(131206587314004820534098544948237170809, 800100010000000c62617463685f6d7574617465000000) >= current key DecoratedKey(131206587314004820534098544948237170809, 800100010000000c62617463685f6d7574617465000000) writing into /tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-tmp-ka-4-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:172) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:196) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:110) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableRewriter.tryAppend(SSTableRewriter.java:141) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.Scrubber.scrub(Scrubber.java:186) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionManager.scrubOne(CompactionManager.java:592) [apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionManager.access$300(CompactionManager.java:100) [apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionManager$3.execute(CompactionManager.java:315) [apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:270) [apache-cassandra-2.1.0.jar:2.1.0]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) [na:1.7.0_40]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_40]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_40]
        at java.lang.Thread.run(Thread.java:724) [na:1.7.0_40]
 WARN [CompactionExecutor:9] 2015-01-26 13:48:01,534 OutputHandler.java (line 52) Row starting at position 25342 is unreadable; skipping to next
 WARN [CompactionExecutor:10] 2015-01-26 13:48:01,534 OutputHandler.java (line 52) Out of order row detected (DecoratedKey(29459452031265566667651334397450214244, 726f776b65793030355f31343232323936393033323837) found after DecoratedKey(131206587314004820534098544948237170809, 800100010000000c62617463685f6d7574617465000000))

etc...
{quote}

EDIT: I copied my comment to a new issue (CASSANDRA-8719) since this issue one is long closed"
Updating cql created table through cassandra-cli transform it into a compact storage table,CASSANDRA-6370,12679734,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,arodrime,arodrime,18/Nov/13 14:23,12/Mar/19 14:07,13/Mar/19 22:29,18/Nov/13 16:46,1.2.12,,,,,,0,,,,,"To reproduce :

echo ""CREATE TABLE test (aid int, period text, event text, viewer text, PRIMARY KEY (aid, period, event, viewer) );"" | cqlsh -kmykeyspace;

echo ""describe table test;"" | cqlsh -kmykeyspace;

Output >
CREATE TABLE test (
  aid int,
  period text,
  event text,
  viewer text,
  PRIMARY KEY (aid, period, event, viewer)
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};

Then do :

echo ""update column family test with dclocal_read_repair_chance = 0.1;"" | cassandra-cli -kmykeyspace

And finally again : echo ""describe table test;"" | cqlsh -kmykeyspace;

Output >

CREATE TABLE test (
  aid int,
  column1 text,
  column2 text,
  column3 text,
  column4 text,
  value blob,
  PRIMARY KEY (aid, column1, column2, column3, column4)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};

This is quite annoying in production. If it is happening to you: 
UPDATE system.schema_columnfamilies SET column_aliases = '[""period"",""event"",""viewer""]' WHERE keyspace_name='mykeyspace' AND columnfamily_name='test'; should help restoring the table. (Thanks Sylvain for this information.)",,,,,,,,,,,,,,,,,,,,,,,,18/Nov/13 15:42;slebresne;6370.txt;https://issues.apache.org/jira/secure/attachment/12614406/6370.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-18 14:41:39.718,,,no_permission,,,,,,,,,,,,359092,,,Mon Nov 18 16:46:33 UTC 2013,,,,,,0|i1pwhj:,359382,1.2.11,,,,,,,jbellis,jbellis,,,1.2.2,,,,,,,18/Nov/13 14:41;jbellis;Why do you think cli does not list cql-created tables?,"18/Nov/13 15:13;slebresne;I'm going to reopen because while I agree that you should absolutely stick to cqlsh when dealing with CQL3 tables, I think it doesn't cost us much to either make sure it's not too easy to shoot yourself in the foot or at least disallow modifications of CQL3 table from thrift if that screw them up (especially since it's pretty damn hard to get back on your feet afterwards unless you're very familiar with the schema code).","18/Nov/13 15:15;devdazed;I tend to agree. If there is any unexpected behavior that could arise then it should be prevented from happening, a big warning like ""THIS WILL ALTER YOUR TABLE WITH COMPACT STORAGE ... Continue Y/N?"", so the user is aware of what is happening.  Simply saying ""it's hidden when you list it"" is not a solution IMO.","18/Nov/13 15:37;arodrime;I didn't try to list any table here.

I heard was thrift / cql were abstractions of the same data and that we could continuing to use both. I thought cassandra-cli was also compatible with cql tables. Even more, I had no error or warning while running my cassandra-cli command.

In my point of view (which was wrong at this time, I agree now) this was a ""normal"" usage of cassandra that resulted into a bug in my productions servers. My error is now fixed as you can read at the end of the description. My point here is to help the Cassandra team to make Cassandra more robust to avoid more issues of this kind.

Do whatever you want with this report, but there is no need of bashing me, this wasn't trivial, and the fact you hide the cql-created tables doesn't help in any way since I didn't list them. 

This happen to me for being an early Cassandra adopter who use to change the schema using cassandra-cli.","18/Nov/13 15:42;slebresne;Let's keep it simple, attaching a patch that just refuse modifications to CQL3 tables from thrift. We don't allow to create them or list them so there's no good reason to allow modifying them and that way we make sure to avoid subtle screw-ups. And if you really want to shoot yourself in the foot by messing up with the underlying schema layout, that's what the System tables are for.",18/Nov/13 15:53;jbellis;+1,18/Nov/13 16:32;iamaleksey;Edit: d-oh.,"18/Nov/13 16:46;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair Freeze/Gossip Invisibility Issues 1.2.4,CASSANDRA-5432,12641162,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,vijay2win@yahoo.com,arya,arya,06/Apr/13 01:02,12/Mar/19 14:07,13/Mar/19 22:29,06/May/13 07:16,1.2.5,,,,,,1,,,,,"Read comment 6. This description summarizes the repair issue only, but I believe there is a bigger problem going on with networking as described on that comment. 


Since I have upgraded our sandbox cluster, I am unable to run repair on any node and I am reaching our gc_grace seconds this weekend. Please help. So far, I have tried the following suggestions:

- nodetool scrub
- offline scrub
- running repair on each CF separately. Didn't matter. All got stuck the same way.

The repair command just gets stuck and the machine is idling. Only the following logs are printed for repair job:

 INFO [Thread-42214] 2013-04-05 23:30:27,785 StorageService.java (line 2379) Starting repair command #4, repairing 1 ranges for keyspace cardspring_production
 INFO [AntiEntropySessions:7] 2013-04-05 23:30:27,789 AntiEntropyService.java (line 652) [repair #cc5a9aa0-9e48-11e2-98ba-11bde7670242] new session: will sync /X.X.X.190, /X.X.X.43, /X.X.X.56 on range (1808575600,42535295865117307932921825930779602032] for keyspace_production.[comma separated list of CFs]
 INFO [AntiEntropySessions:7] 2013-04-05 23:30:27,790 AntiEntropyService.java (line 858) [repair #cc5a9aa0-9e48-11e2-98ba-11bde7670242] requesting merkle trees for BusinessConnectionIndicesEntries (to [/X.X.X.43, /X.X.X.56, /X.X.X.190])
 INFO [AntiEntropyStage:1] 2013-04-05 23:30:28,086 AntiEntropyService.java (line 214) [repair #cc5a9aa0-9e48-11e2-98ba-11bde7670242] Received merkle tree for ColumnFamilyName from /X.X.X.43
 INFO [AntiEntropyStage:1] 2013-04-05 23:30:28,147 AntiEntropyService.java (line 214) [repair #cc5a9aa0-9e48-11e2-98ba-11bde7670242] Received merkle tree for ColumnFamilyName from /X.X.X.56

Please advise. ","Ubuntu 10.04.1 LTS
C* 1.2.3
Sun Java 6 u43
JNA Enabled
Not using VNodes",,,,,,,,,,,,,,,,,,,,,,,25/Apr/13 21:10;vijay2win@yahoo.com;0001-CASSANDRA-5432.patch;https://issues.apache.org/jira/secure/attachment/12580591/0001-CASSANDRA-5432.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-06 03:50:52.896,,,no_permission,,,,,,,,,,,,321578,,,Mon May 06 07:16:38 UTC 2013,,,,,,0|i1jhgf:,321923,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"06/Apr/13 03:50;yukim;If that's the only log you get so far, then the node is waiting merkle tree response from /x.x.x.190.
Check if you have any error on that node.
",06/Apr/13 06:07;arya;Thanks Yuki. node x.x.x.190 is the node which I triggered repair on (self). The logs above belong to that node and stop right there. There is no error or exception.,"06/Apr/13 06:16;arya;I really need to get this going before sunday. I also looked at all other nodes logs, and nothing interesting. ","06/Apr/13 09:14;arya;OK, I found the problem, but something is changed in this release regarding the networking that is not clear to me. I use EC2. I had to open all TCP ports to the world for the repairs to work. They didn't even work when I allowed all TCP within our C*'s security group. This is not acceptable as it is a security risk. What was changed in 1.2.3 in terms of repair routing? Shouldn't it just use the storage port?

We use Ec2MultiRegionSnitch, so it returns DNS that resolved to local ips for in-region communication and public ips for cross-region communication. I have a C* 1.1.10 cluster in production and it is working fine without having to open the security group wide open. 

Please advice.","06/Apr/13 15:40;brandon.williams;Nothing changed with port usage.  There are standard ways to see what ports a process is using to check this, though.","16/Apr/13 21:44;arya;I narrowed this down to non-ssl storage port, and they must be opened on the public IPs. Here are the steps to reproduce:

This is a working configuration:
Cassandra 1.1.10 Cluster with 12 nodes in us-east-1 and 12 nodes in us-west-2
Using Ec2MultiRegionSnitch and SSL enabled for DC_ONLY and NetworkTopologyStrategy with strategy_options: us-east-1:3;us-west-2:3;
C* instances have a security group called 'cluster1'
security group 'cluster1' in each region is configured as such
Allow TCP:
7199 from cluster1 (JMX)
1024 - 65535 from cluster1 (JMX Random Ports)
7100 from cluster1 (Configured Normal Storage)
7103 from cluster1 (Configured SSL Storage)
9160 from cluster1 (Configured Thrift RPC Port)
9160 from <client_group>
foreach node's public IP we also have this rule set to enable cross region comminication:
7103 from public_ip

The above is a functioning and happy setup. You run repair, and it finishes successfully.

Broken Setup:

Upgrade to 1.2.4 without changing any of the above security group settings:

Run repair. The repair will not receive the Merkle Tree for itself. Thus hanging. See description. The test in description was done with one region with strategy of us-east-1:3, but other settings were exactly the same.

Now for each public_ip add a security group rule as such to cluster1 security group:

Allow TCP: 7100 from public_ip

Run repair. Things will magically work now. 

If nothing in terms of port and networking has changed in 1.2, then why the above is happening? I can constantly reproduce it. 

This also affects gossip. If you don't have the JMX Ports open on public ips, then gossip would not see any node except itself after a snap restart of all nodes all at once. 

","16/Apr/13 22:21;jbellis;Gossip does not touch JMX.  JMX is not used internally at all; it's only there to let nodetool invoke methods.

Please see the user mailing list for troubleshooting help, Jira is not a good place for that.",16/Apr/13 22:24;arya;I have used the IRC channel already. It was suggested to me to open a JIRA ticket as no one could help.,"16/Apr/13 22:39;arya;I added a correction. It is not JMX Jonathan, you are right. It is opening the non-ssl storage port on public IPs that fixes it. We didn't have to do this on 1.1.10.","17/Apr/13 08:35;jbellis;You said above that you had it configured this way in 1.1 as well:

{quote}
7100 from cluster1 (Configured Normal Storage)
7103 from cluster1 (Configured SSL Storage)
{quote}

In any case, it is not a bug for you to need both open; Cassandra will use SSL between datacenters (regions), and non-ssl on the private IP within the same one.","19/Apr/13 05:29;arya;> non-ssl on the private IP within the same one [region]

OK, a little more digging, and I found the root cause which I believe is a bug, so I am re-opening this.

See this log snippet for a repair sessions I triggered on a node in a single region in AWS:

 INFO [AntiEntropySessions:1] 2013-04-19 04:28:16,587 AntiEntropyService.java (line 651) [repair #8e59b7c0-a8a9-11e2-ba85-d39d57f66b97] new session: will sync /54.242.X.YYY, /54.224.XX.YYY, /50.17.XXX.YYY on range (99249023685273718510150927169407637270,127605887595351923798765477788721654890] for cardspring_production.[App]
 INFO [AntiEntropySessions:1] 2013-04-19 04:28:16,591 AntiEntropyService.java (line 857) [repair #8e59b7c0-a8a9-11e2-ba85-d39d57f66b97] requesting merkle trees for App (to [/54.224.XX.YYY, /50.17.XXX.YYY, /54.242.X.YYY])
DEBUG [WRITE-/50.17.159.210] 2013-04-19 04:28:16,592 OutboundTcpConnection.java (line 260) attempting to connect to /10.170.XX.YYY
DEBUG [WRITE-/54.224.36.214] 2013-04-19 04:28:16,593 OutboundTcpConnection.java (line 260) attempting to connect to /10.121.XX.YYY
DEBUG [WRITE-/54.242.1.111] 2013-04-19 04:28:16,593 OutboundTcpConnection.java (line 260) attempting to connect to /54.242.X.YYY

Notice the last line. This is the public IP of the node running repair. Why is this picking up the public ip address for itself to send the tree request? This is the source of problem. In AWS you cannot communicated through public ip address with security group rules that are defined based on group names in the same region, which is a common use case. Hence the tree request gets stuck at sending point to itself. 



","19/Apr/13 17:44;vijay2win@yahoo.com;Arya, 
The first time we start the communication to a node we try to Initiate communications we use the public IP and eventually once we have the private IP we will switch back to local ip's.

I am confused with the analysis, because the nodes should have been connected and communicating and Tree request is another message in the same channel as any other message. 
Are the nodes up in the first place?

{code}
                this.treeRequests = new RequestCoordinator<TreeRequest>(isSequential)
                {
                    public void send(TreeRequest r)
                    {
                        MessagingService.instance().sendOneWay(r.createMessage(), r.endpoint);
                    }
                };
{code}","20/Apr/13 00:07;arya;Hey Vijay,

Good to see you here. Sorry if my analysis is unclear. Here is my take:

> The first time we start the communication to a node we try to Initiate communications we use the public IP and eventually once we have the private IP we will switch back to local ip's.

Has this always been the case? Because if you are using public ips (not public dns name), there has to be explicit security rules on public ips to allow this. Otherwise, if in security groups you are opening the ports to the machines in the same group using their security group name, it allows traffic only within their private ips, so this won't work. 

We use Priam (your awesome tooling), and as you know, it opens up only the SSL port on the public IPs for cross region communication. And from the operator's perspective, that is the correct thing to do. I only have the SSL port open on public IPs and don't want to open the non SSL port for security reasons. Now, all other ports like non SSL, JMX, etc are opened the way I described using security group names and it allows traffic on private IPs. It is just the way AWS has been. So, if within the same region, you are trying to connect to any machine using public ip, it won't work. 

Here is how I achieved the scenario above and I believe they are all co-related to the statement you said that all machine connect to public IPs first.

Setup a cluster as I described in my previous comment. It can be a single region. Restart all machines at the same time. Each machine would only see itself as UP. Everyone else is reported to be DOWN in nodetool ring. I am guessing that it is because they are trying to send gossips to public IPs but only SSL port is open on public IPs. The cluster is configured to only do SSL cross datacenter/region not within the same region. So, now I am left with bunch of nodes that only see themselves in the ring. I go to my AWS console, open up the non SSL port on every single public IP in that security group. Now all the nodes see each other. 

By now, I had a theory about nodes wanting to communicate through the public ip which is not possible, so I stepped into troubleshooting repairs. I know that with current settings repair would succeed. Since the nodes see each other now, I go to security groups and remove the non SSL on public IP rules that I added in previous step. Start the repair, and I ended up with the log message as above. The public ip mentioned in the log, belongs to the node that owns the log and is running repair, so it tried to communicated to itself using its own public IP. 

Did I make sense? I can call you to describe it over the phone, but basically this setup used to work on 1.1.10 but does not work on 1.2.4. I have attached the debugger to a node and am trying to trace  the code. I'll let you know if I find something new.","20/Apr/13 05:29;vijay2win@yahoo.com;Hi Arya, Thanks and you can call me anytime but it will help others if we keep the discussion here.

{quote}
Has this always been the case? 
{quote}
As far as i know, yes.

{quote}
 I go to security groups and remove the non SSL on public IP rules that I added in previous step.
{quote}
I think you should not remove the IP's. Priam opens up ports for the local nodes and also the remote nodes within the security group (http://goo.gl/l9Q1T). Looks like you shouldn't do the above because you are now disabling cassandra from restarting the connections.

Also the reason you are seeing all the nodes to be UP in a multi region case event though they cannot communicate within the DC is because of the issue mentioned in CASSANDRA-3533, I can almost bet that the read/write requests will be failing in the local DC, If not try after restarting nodes. :)

Let me know if you still have issues or disagree.
","21/Apr/13 02:53;arya;Priam only opens one port, and that is the SSL port on public IPs (see line 74): http://goo.gl/vY8WX 

I did not remove the IPs from security group. I left the IP rules for the SSL port as were set by Priam. I only remove the NON SSL port rules on public IPs which I had added manually to work around this issue.","21/Apr/13 23:54;vijay2win@yahoo.com;Priam opens port for other DC's to talk to each other but nothing to do within, i still doubt the SG setup coz all IP's within a security group should be opened for both ports. 
May be CASSANDRA-5171 created a side effect, which i am not sure.

[~jbrown] do you mind verifying it with 1.2.4? Verifying it with Priam is a bigger undertaking for me now :)",23/Apr/13 06:06;arya;I was actually suspicious about that. I can roll back that patch and try it. Give me till end of the week. My hands are tied up right now.,"24/Apr/13 23:38;arya;So, I rolled back CASSANDRA-5171. Pushed it to my test cluster. The gossip issue where nodes after restart didn't see each other got fixed. The repair still tried to connect to the machine running repair (self) with its public IP for requesting MerkleTree where it gets stuck, so it has the same issue. Some behavior changed though, and the OutBoundTCPConnection didn't report connecting to other 2 replicas for requesting MerkleTree, so I only saw the message when trying to connect. Here is the snippet: 

 INFO [Thread-458] 2013-04-24 23:21:16,543 StorageService.java (line 2407) Starting repair command #1, repairing 1 ranges for keyspace app_production
DEBUG [Thread-458] 2013-04-24 23:21:16,580 StorageService.java (line 2547) computing ranges for 1808575600, 7089215977519551322153637656637080005, 14178431955039102644307275311465584410, 4253529586511
7307932921825930779602030, 49624511842636859255075463585608106435, 56713727820156410577229101240436610840, 85070591730234615865843651859750628460, 92159807707754167187997289514579132865, 9924902368527
3718510150927169407637270, 127605887595351923798765477788721654890, 134695103572871475120919115443550159295, 141784319550391026443072753098378663700
 INFO [AntiEntropySessions:1] 2013-04-24 23:21:16,587 AntiEntropyService.java (line 651) [repair #a9a87e40-ad35-11e2-945a-050d956ff11b] new session: will sync /YYY.XX.98.11, /YY.XXX.107.137, /YY.XXX.133.163 on range (99249023685273718510150927169407637270,127605887595351923798765477788721654890] for cardspring_production.[App]
 INFO [AntiEntropySessions:1] 2013-04-24 23:21:16,598 AntiEntropyService.java (line 857) [repair #a9a87e40-ad35-11e2-945a-050d956ff11b] requesting merkle trees for App (to [/XX.YYY.107.137, /XX.YYY.133.163, /XXX.YY.98.11])
DEBUG [WRITE-/107.20.98.11] 2013-04-24 23:21:16,601 OutboundTcpConnection.java (line 260) attempting to connect to /XXX.YY.98.11
 INFO [AntiEntropyStage:1] 2013-04-24 23:21:19,111 AntiEntropyService.java (line 213) [repair #a9a87e40-ad35-11e2-945a-050d956ff11b] Received merkle tree for App from /XX.YYY.133.163
DEBUG [ScheduledTasks:1] 2013-04-24 23:21:19,409 GCInspector.java (line 121) GC for ParNew: 54 ms for 1 collections, 669806384 used; max is 4211081216
 INFO [AntiEntropyStage:1] 2013-04-24 23:21:20,408 AntiEntropyService.java (line 213) [repair #a9a87e40-ad35-11e2-945a-050d956ff11b] Received merkle tree for App from /XX.YYY.107.137

See the debug line with OutboundTcpConnection. It is trying to connect to public IP of self (XXX.YY.98.11), which is still an issue. What I was expecting to see before this line was two other consecutive lines like before where it showed OutboundTcpConnection trying to connect to other nodes as well. Despite them returning the MerkleTrees, those log lines did not show. So, connection was made successfully to the other nodes somehow. ","25/Apr/13 09:16;ondrej.cernos;I have exactly the same issue as Arya.

I also had to open non-SSL ports from within the datacenter in order to create the cluster.

I was wondering if it could be a networking issue (we use mixed aws-private cloud setup), so it is good to see we are not alone with this.","25/Apr/13 09:25;ondrej.cernos;Please see also CASSANDRA-5493 - the MessagingService also reports dropped messages on _itself_ using it's public IP. The output displays 3 public IPs and 2 private (the private IP of the node itself is not included), while the remote DC is reported correctly. This seems related.","25/Apr/13 21:10;vijay2win@yahoo.com;attached reverts CASSANDRA-5171 and adds handling of local endpoint.

Arya, mind testing this?","26/Apr/13 00:40;arya;Sure, I should be able to get back to you either tonight or tomorrow.",26/Apr/13 05:06;arya;+1 works for me. Thank you.,"29/Apr/13 15:02;jbellis;Why does ""let's use the last-known location of this node"" cause problems?","29/Apr/13 17:05;vijay2win@yahoo.com;The problem is that we need Private_ip to communicate within DC/region is not available until the gossiping with nodes. 
Since we dont have the private information but we do have the rest (DC/RACK), we are trying to connect via public IP.

Removing that optimization forces us to assume it is in other DC and hence using public IP and SSL port, eventually when we receive the private IP we reset the status to use the right (private_ip) connection.
You may ask why not store the private IP? well we could but currently the reset connection (to private IP) logic is in the snitch.","01/May/13 12:04;brandon.williams;I never thought CASSANDRA-5171 was a really big gain anyway, but it looked innocuous enough at the time. +1 on reverting it.",06/May/13 07:16;slebresne;Took the liberty to commit as I want to re-roll 1.2.5.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lost row marker after TTL expires,CASSANDRA-5762,12657720,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,tnr,tnr,15/Jul/13 13:18,12/Mar/19 14:07,13/Mar/19 22:29,22/Jul/13 12:19,1.2.7,,,,,,0,,,,,"I have the following table

cqlsh:loginproject> DESCRIBE TABLE gameservers;
 
CREATE TABLE gameservers (
  address inet PRIMARY KEY,
  last_update timestamp,
  regions blob,
  server_status boolean
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};



after inserting a row and executing the following command:
UPDATE gameservers USING TTL 10 SET server_status = true WHERE address = '192.168.0.100'

after waiting for the ttl to expire, the row will lose its rowmarker making ""select address from gameservers"" returning 0 results although there are some.

in cassandra-cli the table looks like this:
[default@loginproject] list gameservers;
Using default limit of 100
Using default cell limit of 100
-------------------
RowKey: 192.168.0.100
=> (name=last_update, value=0000000000000017, timestamp=1373884433543000)
=> (name=regions, value=<truncated>, timestamp=1373883701652000)

1 Row Returned.
Elapsed time: 345 msec(s).
[default@loginproject]


",Ubuntu 12.04,,,,,,,,,,,,,,,,,,,,,,,16/Jul/13 12:24;slebresne;0001-Always-do-slice-queries-for-CQL3-tables.txt;https://issues.apache.org/jira/secure/attachment/12592529/0001-Always-do-slice-queries-for-CQL3-tables.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-15 14:16:48.937,,,no_permission,,,,,,,,,,,,337940,,,Wed Jan 15 12:11:16 UTC 2014,,,,,,0|i1mafb:,338262,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"15/Jul/13 14:16;slebresne;This is a rather annoying problem. To sum it up, if you TTL one column, the row marker is ttled too, because we have no way to know that this is not the last live column in that CQL3 row, but then if there was other non-ttled columns, they will survive the death of the row marker which breaks the invariant that a live row always has a row marker.

Tbh, I have a hard time coming with a way to fix this. And so the only workaround that comes to mind would be disallow setting a TTL on an individual columns. That is, you'd have to update all columns of the row to be able to use a TTL, which I hate because 1) it'll look random syntax wise and 2) I'm convinced that being able to TTL individual columns in a row is useful.","16/Jul/13 12:24;slebresne;As much as this pains me, I don't see any easy way to make this work outside of doing a read-before-write (which is not acceptable).

It ""might"" be possible to make it work (without read-before-write) by specializing the row marker in the storage so that it tracks TTL to provide the desired behavior but at best that wouldn't be trivial and would probably make the row marker prohibitive in term of storage (though something like CASSANDRA-4175 might help make it more reasonable). In any case, it's _at best_ a solution for 2.1 but not before that, and that's leaving aside the debate of whether the feature is worth the complexity.

In the meantime, the best workaround I can come with would be to force SELECT queries to slice the whole CQL3 row even when only some columns are selected.  That is, we would revert to what we did for selects before CASSANDRA-4361. Tbh, this probably wouldn't have much impact on performance since 1) CQL3 rows are bound to be relatively small and 2) we now optimize slice queries relatively well for that kind of case (partly in 1.2 with promoted index and even more in 2.0 with CASSANDRA-5514) so that queries by names probably don't have that much benefits anymore.

Doing that would fix the problem is most cases, including the one of the description since it'll basically relegate the row marker to only mark rows where only the PK is set. This does not fix it fully though, since if you do
{noformat}
CREATE TABLE test (k int PRIMARY KEY, a int, b int);
INSERT INTO test (k, a, b) VALUES (0, 1, 2);
UPDATE test USING TTL=1 SET b=3 WHERE k=0;
// wait 2 seconds
DELETE a FROM test WHERE k=0;
SELECT * FROM test WHERE k=0;
{noformat}
then the last select will return no results, even though it kind of should return one result (with {{a == null}} and {{b == null}}) since we haven't done a full row deletion. But then we could accept that as a whacky known special situation (don't get me wrong, I don't like it, it's just that ""we have a problem and I don't have a better solution""). And to be fair, you would really have to try fairly hard to get bitten by this.

Attaching the patch that do what's above for info (IN queries on the last clustering column, which we support. make that slightly more annoying that one would hope, but it's not too much of a big deal either).

As mentionned above, another workaround could be to not let user get into that state by forcing all the (CQL3) columns of the (CQL3) row to be set int the statement if a TTL is used.

The (imho big) problem is that this is a breaking change. If someone is using different TTL in the same CQL3 row (and his application do depends on it), it basically cannot upgrade (short of migrating data that have differents TTL into their own separate table, which is extremely painful). Part of me is also pretty convinced that the convenience of being able to set TTL to individual columns outweight the ""not exactly right"" behavior of the special case above (especially since only people that *needs* per-columns TTL will ever run into that special case).
",18/Jul/13 19:17;iamaleksey;+1,"22/Jul/13 12:19;slebresne;Alright, committed, thanks","10/Jan/14 22:39;jjordan;Could we keep a ""something in here has a TTL"" flag and do the optimized way unless it is set?  I think we even already keep that kind of data to do the sstable tombstone compaction?  That way if you don't use TTL's, and you have big columns, you don't get the heap penalty (CASSANDRA-6569).","11/Jan/14 11:18;slebresne;bq. Could we keep a ""something in here has a TTL"" flag and do the optimized way unless it is set?

I don't see how we could maintain such flag. We'd need to know no TTL is involved whatsoever at the coordinator level (so before we're reached the replicas), which I don't think is possible to do reliably. What we could probably do is add a per-table ""disable ttl"" option that would make inserts with TTL rejected. And we could optimize when this option is set. That being said, if you want to store both small and large values in the same row and strongly rely on only being able to query only the small ones often, you might be better off using 2 tables, one for the small values, one for the large ones.","15/Jan/14 12:11;slebresne;Actually, CASSANDRA-6588 is probably a better idea than my ""disable ttl option"" above.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AE in ArrayBackedSortedColumns,CASSANDRA-5856,12662410,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,iamaleksey,brandon.williams,brandon.williams,07/Aug/13 15:47,12/Mar/19 14:07,13/Mar/19 22:29,17/Aug/13 15:53,1.2.9,,,,,,1,,,,,"{noformat}
ERROR [ReadStage:3] 2013-08-07 06:58:21,485 CassandraDaemon.java (line 192) Exception in thread Thread[ReadStage:3,5,main]
java.lang.AssertionError: Added column does not sort as the last column
    at org.apache.cassandra.db.ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:131)
    at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:119)
    at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:114)
    at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:171)
    at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:136)
    at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:84)
    at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:291)
    at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
    at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1390)
    at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1213)
    at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1125)
    at org.apache.cassandra.db.Table.getRow(Table.java:347)
    at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:70)
    at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1047)
    at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1593)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
{noformat}

test_column_index_stress in wide_rows_test will reproduce this within ~20 runs and bisect strongly points to a regression in CASSANDRA-5762",,,,,,,,,,,,,,,,,,,,,,,,12/Aug/13 05:55;jbellis;5856-assert.txt;https://issues.apache.org/jira/secure/attachment/12597413/5856-assert.txt,16/Aug/13 00:47;iamaleksey;5856.txt;https://issues.apache.org/jira/secure/attachment/12598335/5856.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-08-12 04:46:47.221,,,no_permission,,,,,,,,,,,,342414,,,Sat Aug 17 15:53:35 UTC 2013,,,,,,0|i1n1wf:,342719,,,,,,,,jbellis,jbellis,,,1.2.7,,,,,,,12/Aug/13 04:46;jbellis;First guess: something's getting confused about reversed-ness.  Is that part of the test in question?,"12/Aug/13 05:07;jbellis;CASSANDRA-5762 is probably causing this indirectly by forcing more code to go through the slice path, rather than introducing a bug in collation directly.","12/Aug/13 05:34;jbellis;bq. First guess: something's getting confused about reversed-ness

Second guess: there's a bug in ISR's code for reversed fetches (CASSANDRA-5712).

Might need to make it print out the cells in collectReducedColumns to see...",12/Aug/13 05:49;jbellis;If there's a bug in ISR it's probably older than 5712.  {{prefetched}} makes my head hurt.,"12/Aug/13 05:55;jbellis;Patch to get more information from the assert.

(NB: the existing error message indicates that this is NOT a reversed slice.  So, beats the hell out of me how this could be erroring out.  Hence, the need for more information.)",12/Aug/13 10:32;iamaleksey;I'll get to it today/tomorrow. Yeah. 5762 is very unlikely to be the cause of it.,"13/Aug/13 16:00;jbellis;Two of the patched assertion failures:

{noformat}
java.lang.AssertionError: Added cell val31254: does not sort as the last; contents are val2960::false:0@1376407099059000,val2960:value:false:4@1376407099059000,val31254::false:0@1376407110826001,val31254:value:false:4@1376407110826001, with comparator org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)
{noformat}

{noformat}
java.lang.AssertionError: Added cell val54806: does not sort as the last; contents are val22917::false:0@1376408295872003,val22917:value:false:4@1376408295872003,val54806::false:0@1376408305568001,val54806:value:false:4@1376408305568001, with comparator org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type
{noformat}
","13/Aug/13 16:05;brandon.williams;Here's the most concise one I've seen:

{noformat}
java.lang.AssertionError: Added cell val11599: does not sort as the last; contents are val11599::false:0@1376409359730001,val11599:value:false:4@1376409359730001, with comparator org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)
{noformat}","15/Aug/13 18:10;brandon.williams;So, this is a lot simpler than the test makes it look.  It's caused simple by asking for the same column by name twice:

{noformat}
cqlsh> create keyspace foo WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
cqlsh> use foo;
cqlsh:foo> create table bar (row varchar, name varchar, value int, PRIMARY KEY (row, name));
cqlsh:foo> update bar set value = 1 WHERE row = 'baz' AND name = 'qux';
cqlsh:foo> select value from bar where row='baz' AND name in ('qux', 'qux');
Request did not complete within rpc_timeout.
{noformat}

Results in:
{noformat}
java.lang.AssertionError: Added cell qux: does not sort as the last; contents are qux::false:0@1376590034567000,qux:value:false:4@1376590034567000, with comparator org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)
{noformat}","17/Aug/13 15:23;jbellis;+1

Nit: can we make buildBound return Collection to avoid the extra copy?","17/Aug/13 15:53;iamaleksey;Committed, thanks.

bq. Nit: can we make buildBound return Collection to avoid the extra copy?

No, unfortunately we can't. Well, we kinda can, but we'd then have to create an extra iterator for getKeyBound (won't be able to just .get(0)) and two extra iterators in makeFilter (again, won't be able to just .get(i) - and we need to iterate over startBounds and endBounds simultaneously to build the ColumnSlices).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL 'set' returns incorrect value,CASSANDRA-5805,12659851,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,yukim,yukim,25/Jul/13 17:40,12/Mar/19 14:07,13/Mar/19 22:29,25/Jul/13 20:35,2.0 rc1,,,,,,0,,,,,"CQL 'set' returns incorrect value after flush.
Create the following table:

{code}
CREATE KEYSPACE ks WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
USE ks;
CREATE TABLE cf ( k int PRIMARY KEY , s set<int> );
{code}

Insert data:

{code}
INSERT INTO cf (k, s) VALUES (1, {1});
INSERT INTO cf (k, s) VALUES (1, {2});
{code}

This should return:

{code}
cqlsh:ks> SELECT * FROM cf;

 k | s
---+--------
 1 | {2}
{code}

and it does when no flush has happened.

But when I do flush after each insert, it starts returning:

{code}
cqlsh:ks> SELECT * FROM cf;

 k | s
---+--------
 1 | {1, 2}
{code}

'system.local' table flushes every time it inserts(updates) tokens, and this behavior is causing 'nodetool move' to act weirdly.",,,,,,,,,,,,,,,,,,,,,,,,25/Jul/13 19:00;slebresne;5805.txt;https://issues.apache.org/jira/secure/attachment/12594235/5805.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-25 18:17:14.03,,,no_permission,,,,,,,,,,,,340043,,,Thu Jul 25 20:35:30 UTC 2013,,,,,,0|i1mndb:,340361,,,,,,,,jbellis,jbellis,,,,,,,,,,"25/Jul/13 17:57;yukim;I tested with 1.2 branch with the same direction above, and I got the correct value.","25/Jul/13 18:17;iamaleksey;Maps and Lists are also affected, but I'm not certain that the issue is necessarily collection-related.","25/Jul/13 19:00;slebresne;That's a bug in the part specific to trunk from CASSANDRA-5677, my bad. Attaching patch to fix. I've also pushed a dtest that capture this bug.",25/Jul/13 19:13;jbellis;+1,"25/Jul/13 20:35;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make it safe to concurrently access ABSC after its construction,CASSANDRA-6742,12696088,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,iamaleksey,enigmacurry,enigmacurry,19/Feb/14 22:13,12/Mar/19 14:07,13/Mar/19 22:29,20/Feb/14 18:37,2.1 beta2,,,,,,0,,,,,"This is a physical four node cluster. Configuration is attached.

Create a keyspace and table from the first node: 

{code}
CREATE KEYSPACE ""Keyspace1"" WITH replication = {
  'class': 'SimpleStrategy',
  'replication_factor': '1'
};

USE ""Keyspace1"";

CREATE TABLE ""Counter1"" (
  key blob,
  column1 ascii,
  value counter,
  PRIMARY KEY (key, column1)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='NONE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={};
{code}

And the find the following in the logs:

{code}
INFO  [Thrift:1] 2014-02-19 14:04:35,828 MigrationManager.java:210 - Create new ColumnFamily: org.apache.cassandra.config.CFMetaData@d824292[cfId=d1bc0c30-99b1-11e3-a5f9-c187ff8103e2,ksName=Keyspace1,cfName=Counter1,cfType=Standard,comparator=org.apache.cassandra.db.marshal.AsciiType,comment=,readRepairChance=0.1,dclocalReadRepairChance=0.0,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.CounterColumnType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=3 cap=3]=ColumnDefinition{name=key, type=org.apache.cassandra.db.marshal.BytesType, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=5 cap=5]=ColumnDefinition{name=value, type=org.apache.cassandra.db.marshal.CounterColumnType, kind=COMPACT_VALUE, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=7 cap=7]=ColumnDefinition{name=column1, type=org.apache.cassandra.db.marshal.AsciiType, kind=CLUSTERING_COLUMN, componentIndex=null, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionParameters={},bloomFilterFpChance=0.01,memtableFlushPeriod=0,caching=KEYS_ONLY,defaultTimeToLive=0,minIndexInterval=128,maxIndexInterval=2048,speculativeRetry=NONE,populateIoCacheOnFlush=false,droppedColumns={},triggers={},rowsPerPartitionToCache=100]
ERROR [WRITE-/172.16.1.211] 2014-02-19 14:04:35,838 OutboundTcpConnection.java:256 - error writing to /172.16.1.211
java.lang.ArrayIndexOutOfBoundsException: -1
        at org.apache.cassandra.db.ArrayBackedSortedColumns.internalAppendOrReconcile(ArrayBackedSortedColumns.java:231) ~[main/:na]
        at org.apache.cassandra.db.ArrayBackedSortedColumns.sortCells(ArrayBackedSortedColumns.java:143) ~[main/:na]
        at org.apache.cassandra.db.ArrayBackedSortedColumns.maybeSortCells(ArrayBackedSortedColumns.java:103) ~[main/:na]
        at org.apache.cassandra.db.ArrayBackedSortedColumns.getColumnCount(ArrayBackedSortedColumns.java:313) ~[main/:na]
        at org.apache.cassandra.db.ColumnFamilySerializer.contentSerializedSize(ColumnFamilySerializer.java:117) ~[main/:na]
        at org.apache.cassandra.db.ColumnFamilySerializer.serializedSize(ColumnFamilySerializer.java:132) ~[main/:na]
        at org.apache.cassandra.db.Mutation$MutationSerializer.serializedSize(Mutation.java:337) ~[main/:na]
        at org.apache.cassandra.service.MigrationManager$MigrationsSerializer.serializedSize(MigrationManager.java:397) ~[main/:na]
        at org.apache.cassandra.service.MigrationManager$MigrationsSerializer.serializedSize(MigrationManager.java:371) ~[main/:na]
        at org.apache.cassandra.net.MessageOut.serialize(MessageOut.java:116) ~[main/:na]
        at org.apache.cassandra.net.OutboundTcpConnection.writeInternal(OutboundTcpConnection.java:273) [main/:na]
        at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:225) [main/:na]
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:163) [main/:na]
{code}

Among a few other similar errors. See the attached log. There were no errors in the log of the node it was trying to contact.",,,,,,,,,,,,,,,,,,,,,,,,20/Feb/14 16:32;iamaleksey;6742-v2.txt;https://issues.apache.org/jira/secure/attachment/12630089/6742-v2.txt,20/Feb/14 16:29;iamaleksey;6742-v2.txt;https://issues.apache.org/jira/secure/attachment/12630087/6742-v2.txt,20/Feb/14 17:11;iamaleksey;6742-v3.txt;https://issues.apache.org/jira/secure/attachment/12630092/6742-v3.txt,19/Feb/14 22:31;enigmacurry;bdplab0.alternate.log;https://issues.apache.org/jira/secure/attachment/12629911/bdplab0.alternate.log,19/Feb/14 22:15;enigmacurry;bdplab0.log;https://issues.apache.org/jira/secure/attachment/12629906/bdplab0.log,19/Feb/14 22:13;enigmacurry;bdplab0_cassandra.yaml;https://issues.apache.org/jira/secure/attachment/12629905/bdplab0_cassandra.yaml,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2014-02-19 23:28:17.325,,,no_permission,,,,,,,,,,,,374565,,,Thu Feb 20 18:37:00 UTC 2014,,,,,,0|i1sjpz:,374865,2.1 beta1,,,,,,,benedict,benedict,,,,,,,,,,19/Feb/14 22:31;enigmacurry;I don't always get the same error. Attached is bdplab0.alternate.log which shows a NPE instead.,19/Feb/14 23:28;iamaleksey;Fixed in d5c3d2fa56e63c9ae5cd6d6a0cab06b5eb2b6b9f.,"20/Feb/14 16:28;iamaleksey;[~benedict] Moving CASSANDRA-6662 discussion here. Attaching a cleaned up/rewritten version of the suggested patch, with

- changed Factory interface that accepts initialCapacity now, making it more generic than 'unitFactory'. Should use it from now on when we know the final size.
- altered sort() to not shadow *size*, and appendOrReconcile() to not mutate the passed variable as well (and got rid of rightStart/rightEnd vars)
- moved fast add all path into a separate method, and got rid of all the redundant *this*

Actually, I think we can do better than this, avoiding volatile altogether. Will attach a v3 soon.","20/Feb/14 16:29;iamaleksey;Reopening, b/c while the issue is fixed, we can do better.","20/Feb/14 16:35;benedict;LGTM so far, though I might have appendOrReconcile encapsulate applying -1 when necessary, to keep the merge consistent in sortCells().","20/Feb/14 17:11;iamaleksey;We add in sorted order a lot, especially on reads, thus if making size volatile is an issue, so is making unsortedSize volatile.

So v3 introduces a separate boolean volatile isSorted flag that gets updated only when we break the sorting order or sort the cells (in other words, almost never), and reverts all the complexity added by the recent revisions.","20/Feb/14 17:27;benedict;Like it.

Only question is fastAddAll: you've limited it to only support array backed, but we sometimes build directly from BTree, and the second path works for any source CF type (skipping N comparisons, since we know there will be no reconcile necessary if building from a CF)","20/Feb/14 18:37;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
streaming fails,CASSANDRA-5418,12640335,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,radev,radev,02/Apr/13 17:11,12/Mar/19 14:07,13/Mar/19 22:29,11/Apr/13 16:13,1.2.5,,,,,,1,,,,,"When I run *nodetool repair* on cas01 node it get's stuck at some point.

I see following exceptions in cas01 system.log:
{quote}
ERROR [Streaming to /10.10.45.60:28] 2013-04-02 09:03:55,353 CassandraDaemon.java (line 132) Exception in thread Thread[Streaming to /10.10.45.60:28,5,main]
java.lang.RuntimeException: java.io.EOFException
	at com.google.common.base.Throwables.propagate(Throwables.java:160)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.cassandra.streaming.FileStreamTask.receiveReply(FileStreamTask.java:193)
	at org.apache.cassandra.streaming.compress.CompressedFileStreamTask.stream(CompressedFileStreamTask.java:114)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	... 3 more


ERROR [Thread-2076] 2013-04-02 09:07:12,261 CassandraDaemon.java (line 132) Exception in thread Thread[Thread-2076,5,main]
java.lang.AssertionError: incorrect row data size 130921 written to /var/lib/cassandra/data/EDITED/content_list/footballsite-content_list-tmp-ib-3660-Data.db; correct is 131074
	at org.apache.cassandra.io.sstable.SSTableWriter.appendFromStream(SSTableWriter.java:285)
	at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:179)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:238)
	at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:178)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:78)

{quote}

On other machines there are some exceptions too:
{quote}
ERROR [Thread-1424] 2013-04-02 09:07:12,248 CassandraDaemon.java (line 132) Exception in thread Thread[Thread-1424,5,main]
java.lang.AssertionError: incorrect row data size 130921 written to /var/lib/cassandra/data/EDITED/content_list/footballsite-content_list-tmp-ib-2268-Data.db; correct is 131074
	at org.apache.cassandra.io.sstable.SSTableWriter.appendFromStream(SSTableWriter.java:285)
	at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:179)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:238)
	at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:178)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:78)
ERROR [Streaming to /10.10.45.58:55] 2013-04-02 09:07:12,263 CassandraDaemon.java (line 132) Exception in thread Thread[Streaming to /10.10.45.58:55,5,main]
java.lang.RuntimeException: java.io.EOFException
	at com.google.common.base.Throwables.propagate(Throwables.java:160)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.cassandra.streaming.FileStreamTask.receiveReply(FileStreamTask.java:193)
	at org.apache.cassandra.streaming.compress.CompressedFileStreamTask.stream(CompressedFileStreamTask.java:114)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	... 3 more

{quote}

Then I see frozen status in *nodetool netstats* and repair never completes.","5 nodes, vnodes enabled, encryption disabled, compression enabled, RackInferring snitch, Centos 6, Oracle JVM with JNA enabled.",,,,,,,,,,,,,,,,,,,,,,,11/Apr/13 15:31;yukim;0001-add-RangeTombstone-transfer-test.patch;https://issues.apache.org/jira/secure/attachment/12578229/0001-add-RangeTombstone-transfer-test.patch,11/Apr/13 10:32;radev;5418-1.2-v2.txt;https://issues.apache.org/jira/secure/attachment/12578187/5418-1.2-v2.txt,11/Apr/13 10:37;radev;5418-1.2-v3.txt;https://issues.apache.org/jira/secure/attachment/12578191/5418-1.2-v3.txt,10/Apr/13 21:56;radev;5418-1.2.txt;https://issues.apache.org/jira/secure/attachment/12578090/5418-1.2.txt,11/Apr/13 12:35;slebresne;5418-v4.txt;https://issues.apache.org/jira/secure/attachment/12578203/5418-v4.txt,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2013-04-02 18:43:13.652,,,no_permission,,,,,,,,,,,,320798,,,Thu Apr 11 16:13:43 UTC 2013,,,,,,0|i1jcmn:,321139,,,,,,,,yukim,yukim,,,,,,,,,,"02/Apr/13 18:43;brandon.williams;You have corruption and need to run scrub first.  That said, we could probably at least abort the repair session in this case.  What do you think [~yukim]?","03/Apr/13 02:44;radev;I've run *nodetool scrub* on each node, it went over all column families.
And yet on next run of *nodetool repair* I still see exceptions in logs:
{code}
 INFO [AntiEntropyStage:1] 2013-04-02 19:37:11,095 StreamOutSession.java (line 162) Streaming to /10.10.45.59
ERROR [Thread-2171] 2013-04-02 19:37:11,184 CassandraDaemon.java (line 132) Exception in thread Thread[Thread-2171,5,main]
java.lang.AssertionError: incorrect row data size 729492 written to /var/lib/cassandra/data/footballsite/content_list/footballsite-content_list-tmp-ib-2235-Data.db; correct is 731241
	at org.apache.cassandra.io.sstable.SSTableWriter.appendFromStream(SSTableWriter.java:285)
	at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:179)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:238)
	at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:178)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:78)
ERROR [Thread-2173] 2013-04-02 19:37:11,187 CassandraDaemon.java (line 132) Exception in thread Thread[Thread-2173,5,main]
java.lang.AssertionError: incorrect row data size 241378 written to /var/lib/cassandra/data/footballsite/content_list/footballsite-content_list-tmp-ib-2236-Data.db; correct is 241696
	at org.apache.cassandra.io.sstable.SSTableWriter.appendFromStream(SSTableWriter.java:285)
	at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:179)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:238)
	at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:178)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:78)
 INFO [Streaming to /10.10.45.60:6] 2013-04-02 19:37:11,190 StreamReplyVerbHandler.java (line 44) Successfully sent /var/lib/cassandra/data/footballsite/content_list/footballsite-content_list-ib-2176-Data.db to /10.10.45.60
 INFO [Streaming to /10.10.45.59:4] 2013-04-02 19:37:11,216 StreamReplyVerbHandler.java (line 44) Successfully sent /var/lib/cassandra/data/footballsite/content_list/footballsite-content_list-ib-2176-Data.db to /10.10.45.59
ERROR [Streaming to /10.10.45.60:6] 2013-04-02 19:37:11,243 CassandraDaemon.java (line 132) Exception in thread Thread[Streaming to /10.10.45.60:6,5,main]
java.lang.RuntimeException: java.io.EOFException
	at com.google.common.base.Throwables.propagate(Throwables.java:160)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.cassandra.streaming.FileStreamTask.receiveReply(FileStreamTask.java:193)
	at org.apache.cassandra.streaming.compress.CompressedFileStreamTask.stream(CompressedFileStreamTask.java:114)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	... 3 more
 INFO [AntiEntropyStage:1] 2013-04-02 19:37:11,251 StreamingRepairTask.java (line 223) [streaming task #62cdba10-9c07-11e2-a79f-1fa905df867b] task succeeded
ERROR [Streaming to /10.10.45.59:4] 2013-04-02 19:37:11,265 CassandraDaemon.java (line 132) Exception in thread Thread[Streaming to /10.10.45.59:4,5,main]
java.lang.RuntimeException: java.io.EOFException
	at com.google.common.base.Throwables.propagate(Throwables.java:160)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.cassandra.streaming.FileStreamTask.receiveReply(FileStreamTask.java:193)
	at org.apache.cassandra.streaming.compress.CompressedFileStreamTask.stream(CompressedFileStreamTask.java:114)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	... 3 more

{code}",03/Apr/13 05:47;krummas;are you running internode encryption?,"03/Apr/13 05:55;radev;No, but I'm using vnodes, if that's making any difference.","05/Apr/13 15:04;radev;Since that I've tried to run offline sstablescrub, it didn't help. As was suggested on IRC, I've tried sstable2json on data file and it worked. But when joining node tries to stream file - it fails repeatedly on the same file.","05/Apr/13 15:06;radev;Also, I've tried shutting down one node, moving it's files to another box and starting it up with different IP/hostname - node was up and serving properly, but problem persisted.",05/Apr/13 18:04;arya;I upgraded our 4 node sandbox cluster from 1.1.10 to 1.2.3. It is impossible to run repair on any node. They all get stuck without any exception in the log. Could my issue be related? Is there a workaround? I have 2 more days till my gc_grace.,"07/Apr/13 20:48;radev;I've tried to build latest source from 1.2 branch. And it worked properly, our cluster is repairing again and performing normally.
","10/Apr/13 12:29;radev;Actually, it helped only temporarily, and appendFromStream now asserts when I try to bootstrap new node.","10/Apr/13 12:31;radev;We are not using internode encryption, but compression is used.","10/Apr/13 15:25;yukim;Igor, can you provide more info about this?
Do you see the same AssertionError for every CFs or the specific one? If the latter, can you post the definition of that CF?
","10/Apr/13 20:57;radev;It's the same column family. We're doing lot's of deletes for it.
Seems that assertion is caused by element written twice on ColumnIndexer block boundary.
But column_index_size_in_kb is same on every node and set to default 64k.",10/Apr/13 21:55;radev;Avoid duplication of columns on index block boundary when appending from stream (source stream already duplicated them).,10/Apr/13 21:56;radev;Patch against branch 1.2,"11/Apr/13 03:32;yukim;Igor, thanks for the patch.
I think that would probably work, since the only code path that could write extra bytes is there, but I want to confirm by writing unit test for this. I'm working on it right now.","11/Apr/13 10:32;radev;I've looked over the ColumnIndex.Builder code again and saw that it can build incorrect index (endPosition updated twice). So, added fromStream flag and skip logic to ColumnIndex.Builder.","11/Apr/13 10:37;radev;v3 includes assertion, maybe will catch if column_index_size_in_kb is changed.","11/Apr/13 12:35;slebresne;I agree on the source of the problem. On the patch however, since the goal should be to write only what we get from the stream (since we've used the dataSize from the stream), it would feel more natural to me to just skip tombstoneTracker.writeOpenedMarker (in which case we really can skip the tombstone tracker completely and save a few CPU cycles). I'm attaching a v5 patch that implement this (imo simpler) alternative.

Now as was noted above, this fix (whatever version of the patch we use) has the small downside that if the source and destination don't have the same column_index_size_in_kb, we'll be screwed. This is definitively a much less problem that this issue and so we should still fix this, but for 2.0, once CASSANRA-4180 gets in, then we should more or less revert this fix because it won't be necessary anymore. I've create CASSANRA-5454 so we don't forget about it.
","11/Apr/13 15:31;yukim;So I created unit test to stream RangeTombstones between column index boundaries. (Patch attached)
It fails with the same stack trace here on current 1.2 branch, but it passes with 5418-v4.
So I will commit v4 and test.","11/Apr/13 16:13;yukim;Committed.
Thanks Igor and Sylvain!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool repair -pr on all nodes won't repair the full range when a Keyspace isn't in all DC's,CASSANDRA-5424,12640546,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,yukim,jjordan,jjordan,03/Apr/13 16:58,12/Mar/19 14:06,13/Mar/19 22:29,19/Apr/13 21:34,1.2.5,,,,,,0,,,,,"nodetool repair -pr on all nodes won't repair the full range when a Keyspace isn't in all DC's

Commands follow, but the TL;DR of it, range (127605887595351923798765477786913079296,0] doesn't get repaired between .38 node and .236 node until I run a repair, no -pr, on .38

It seems like primary arnge calculation doesn't take schema into account, but deciding who to ask for merkle tree's from does.

{noformat}
Address         DC          Rack        Status State   Load            Owns                Token                                       
                                                                                           127605887595351923798765477786913079296     
10.72.111.225   Cassandra   rack1       Up     Normal  455.87 KB       25.00%              0                                           
10.2.29.38      Analytics   rack1       Up     Normal  40.74 MB        25.00%              42535295865117307932921825928971026432      
10.46.113.236   Analytics   rack1       Up     Normal  20.65 MB        50.00%              127605887595351923798765477786913079296     

create keyspace Keyspace1
  with placement_strategy = 'NetworkTopologyStrategy'
  and strategy_options = {Analytics : 2}
  and durable_writes = true;

-------
# nodetool -h 10.2.29.38 repair -pr Keyspace1 Standard1
[2013-04-03 15:46:58,000] Starting repair command #1, repairing 1 ranges for keyspace Keyspace1
[2013-04-03 15:47:00,881] Repair session b79b4850-9c75-11e2-0000-8b5bf6ebea9e for range (0,42535295865117307932921825928971026432] finished
[2013-04-03 15:47:00,881] Repair command #1 finished

root@ip-10-2-29-38:/home/ubuntu# grep b79b4850-9c75-11e2-0000-8b5bf6ebea9e /var/log/cassandra/system.log
 INFO [AntiEntropySessions:1] 2013-04-03 15:46:58,009 AntiEntropyService.java (line 676) [repair #b79b4850-9c75-11e2-0000-8b5bf6ebea9e] new session: will sync a1/10.2.29.38, /10.46.113.236 on range (0,42535295865117307932921825928971026432] for Keyspace1.[Standard1]
 INFO [AntiEntropySessions:1] 2013-04-03 15:46:58,015 AntiEntropyService.java (line 881) [repair #b79b4850-9c75-11e2-0000-8b5bf6ebea9e] requesting merkle trees for Standard1 (to [/10.46.113.236, a1/10.2.29.38])
 INFO [AntiEntropyStage:1] 2013-04-03 15:47:00,202 AntiEntropyService.java (line 211) [repair #b79b4850-9c75-11e2-0000-8b5bf6ebea9e] Received merkle tree for Standard1 from /10.46.113.236
 INFO [AntiEntropyStage:1] 2013-04-03 15:47:00,697 AntiEntropyService.java (line 211) [repair #b79b4850-9c75-11e2-0000-8b5bf6ebea9e] Received merkle tree for Standard1 from a1/10.2.29.38
 INFO [AntiEntropyStage:1] 2013-04-03 15:47:00,879 AntiEntropyService.java (line 1015) [repair #b79b4850-9c75-11e2-0000-8b5bf6ebea9e] Endpoints /10.46.113.236 and a1/10.2.29.38 are consistent for Standard1
 INFO [AntiEntropyStage:1] 2013-04-03 15:47:00,880 AntiEntropyService.java (line 788) [repair #b79b4850-9c75-11e2-0000-8b5bf6ebea9e] Standard1 is fully synced
 INFO [AntiEntropySessions:1] 2013-04-03 15:47:00,880 AntiEntropyService.java (line 722) [repair #b79b4850-9c75-11e2-0000-8b5bf6ebea9e] session completed successfully

root@ip-10-46-113-236:/home/ubuntu# grep b79b4850-9c75-11e2-0000-8b5bf6ebea9e /var/log/cassandra/system.log
 INFO [AntiEntropyStage:1] 2013-04-03 15:46:59,944 AntiEntropyService.java (line 244) [repair #b79b4850-9c75-11e2-0000-8b5bf6ebea9e] Sending completed merkle tree to /10.2.29.38 for (Keyspace1,Standard1)

root@ip-10-72-111-225:/home/ubuntu# grep b79b4850-9c75-11e2-0000-8b5bf6ebea9e /var/log/cassandra/system.log
root@ip-10-72-111-225:/home/ubuntu# 

-------
# nodetool -h 10.46.113.236  repair -pr Keyspace1 Standard1
[2013-04-03 15:48:00,274] Starting repair command #1, repairing 1 ranges for keyspace Keyspace1
[2013-04-03 15:48:02,032] Repair session dcb91540-9c75-11e2-0000-a839ee2ccbef for range (42535295865117307932921825928971026432,127605887595351923798765477786913079296] finished
[2013-04-03 15:48:02,033] Repair command #1 finished

root@ip-10-46-113-236:/home/ubuntu# grep dcb91540-9c75-11e2-0000-a839ee2ccbef /var/log/cassandra/system.log
 INFO [AntiEntropySessions:5] 2013-04-03 15:48:00,280 AntiEntropyService.java (line 676) [repair #dcb91540-9c75-11e2-0000-a839ee2ccbef] new session: will sync a0/10.46.113.236, /10.2.29.38 on range (42535295865117307932921825928971026432,127605887595351923798765477786913079296] for Keyspace1.[Standard1]
 INFO [AntiEntropySessions:5] 2013-04-03 15:48:00,285 AntiEntropyService.java (line 881) [repair #dcb91540-9c75-11e2-0000-a839ee2ccbef] requesting merkle trees for Standard1 (to [/10.2.29.38, a0/10.46.113.236])
 INFO [AntiEntropyStage:1] 2013-04-03 15:48:01,710 AntiEntropyService.java (line 211) [repair #dcb91540-9c75-11e2-0000-a839ee2ccbef] Received merkle tree for Standard1 from a0/10.46.113.236
 INFO [AntiEntropyStage:1] 2013-04-03 15:48:01,943 AntiEntropyService.java (line 211) [repair #dcb91540-9c75-11e2-0000-a839ee2ccbef] Received merkle tree for Standard1 from /10.2.29.38
 INFO [AntiEntropyStage:1] 2013-04-03 15:48:02,031 AntiEntropyService.java (line 1015) [repair #dcb91540-9c75-11e2-0000-a839ee2ccbef] Endpoints a0/10.46.113.236 and /10.2.29.38 are consistent for Standard1
 INFO [AntiEntropyStage:1] 2013-04-03 15:48:02,032 AntiEntropyService.java (line 788) [repair #dcb91540-9c75-11e2-0000-a839ee2ccbef] Standard1 is fully synced
 INFO [AntiEntropySessions:5] 2013-04-03 15:48:02,032 AntiEntropyService.java (line 722) [repair #dcb91540-9c75-11e2-0000-a839ee2ccbef] session completed successfully

root@ip-10-2-29-38:/home/ubuntu# grep dcb91540-9c75-11e2-0000-a839ee2ccbef /var/log/cassandra/system.log
 INFO [AntiEntropyStage:1] 2013-04-03 15:48:01,898 AntiEntropyService.java (line 244) [repair #dcb91540-9c75-11e2-0000-a839ee2ccbef] Sending completed merkle tree to /10.46.113.236 for (Keyspace1,Standard1)

root@ip-10-72-111-225:/home/ubuntu# grep dcb91540-9c75-11e2-0000-a839ee2ccbef /var/log/cassandra/system.log
root@ip-10-72-111-225:/home/ubuntu# 

-------
# nodetool -h 10.72.111.225  repair -pr Keyspace1 Standard1
[2013-04-03 15:48:30,417] Starting repair command #1, repairing 1 ranges for keyspace Keyspace1
[2013-04-03 15:48:30,428] Repair session eeb12670-9c75-11e2-0000-316d6fba2dbf for range (127605887595351923798765477786913079296,0] finished
[2013-04-03 15:48:30,428] Repair command #1 finished

root@ip-10-72-111-225:/home/ubuntu# grep eeb12670-9c75-11e2-0000-316d6fba2dbf /var/log/cassandra/system.log
 INFO [AntiEntropySessions:1] 2013-04-03 15:48:30,427 AntiEntropyService.java (line 676) [repair #eeb12670-9c75-11e2-0000-316d6fba2dbf] new session: will sync /10.72.111.225 on range (127605887595351923798765477786913079296,0] for Keyspace1.[Standard1]
 INFO [AntiEntropySessions:1] 2013-04-03 15:48:30,428 AntiEntropyService.java (line 681) [repair #eeb12670-9c75-11e2-0000-316d6fba2dbf] No neighbors to repair with on range (127605887595351923798765477786913079296,0]: session completed

root@ip-10-46-113-236:/home/ubuntu# grep eeb12670-9c75-11e2-0000-316d6fba2dbf /var/log/cassandra/system.log
root@ip-10-46-113-236:/home/ubuntu# 

root@ip-10-2-29-38:/home/ubuntu# grep eeb12670-9c75-11e2-0000-316d6fba2dbf /var/log/cassandra/system.log
root@ip-10-2-29-38:/home/ubuntu# 

---
root@ip-10-2-29-38:/home/ubuntu# nodetool -h 10.2.29.38 repair Keyspace1 Standard1
[2013-04-03 16:13:28,674] Starting repair command #2, repairing 3 ranges for keyspace Keyspace1
[2013-04-03 16:13:31,786] Repair session 6bb81c20-9c79-11e2-0000-8b5bf6ebea9e for range (42535295865117307932921825928971026432,127605887595351923798765477786913079296] finished
[2013-04-03 16:13:31,786] Repair session 6cb05ed0-9c79-11e2-0000-8b5bf6ebea9e for range (0,42535295865117307932921825928971026432] finished
[2013-04-03 16:13:31,806] Repair session 6d24a470-9c79-11e2-0000-8b5bf6ebea9e for range (127605887595351923798765477786913079296,0] finished
[2013-04-03 16:13:31,807] Repair command #2 finished

root@ip-10-2-29-38:/home/ubuntu# grep 6d24a470-9c79-11e2-0000-8b5bf6ebea9e /var/log/cassandra/system.log
 INFO [AntiEntropySessions:7] 2013-04-03 16:13:31,065 AntiEntropyService.java (line 676) [repair #6d24a470-9c79-11e2-0000-8b5bf6ebea9e] new session: will sync a1/10.2.29.38, /10.46.113.236 on range (127605887595351923798765477786913079296,0] for Keyspace1.[Standard1]
 INFO [AntiEntropySessions:7] 2013-04-03 16:13:31,065 AntiEntropyService.java (line 881) [repair #6d24a470-9c79-11e2-0000-8b5bf6ebea9e] requesting merkle trees for Standard1 (to [/10.46.113.236, a1/10.2.29.38])
 INFO [AntiEntropyStage:1] 2013-04-03 16:13:31,751 AntiEntropyService.java (line 211) [repair #6d24a470-9c79-11e2-0000-8b5bf6ebea9e] Received merkle tree for Standard1 from /10.46.113.236
 INFO [AntiEntropyStage:1] 2013-04-03 16:13:31,785 AntiEntropyService.java (line 211) [repair #6d24a470-9c79-11e2-0000-8b5bf6ebea9e] Received merkle tree for Standard1 from a1/10.2.29.38
 INFO [AntiEntropyStage:1] 2013-04-03 16:13:31,805 AntiEntropyService.java (line 1015) [repair #6d24a470-9c79-11e2-0000-8b5bf6ebea9e] Endpoints /10.46.113.236 and a1/10.2.29.38 are consistent for Standard1
 INFO [AntiEntropyStage:1] 2013-04-03 16:13:31,806 AntiEntropyService.java (line 788) [repair #6d24a470-9c79-11e2-0000-8b5bf6ebea9e] Standard1 is fully synced
 INFO [AntiEntropySessions:7] 2013-04-03 16:13:31,806 AntiEntropyService.java (line 722) [repair #6d24a470-9c79-11e2-0000-8b5bf6ebea9e] session completed successfully

root@ip-10-46-113-236:/home/ubuntu# grep 6d24a470-9c79-11e2-0000-8b5bf6ebea9e /var/log/cassandra/system.log 
 INFO [AntiEntropyStage:1] 2013-04-03 16:13:31,665 AntiEntropyService.java (line 244) [repair #6d24a470-9c79-11e2-0000-8b5bf6ebea9e] Sending completed merkle tree to /10.2.29.38 for (Keyspace1,Standard1)
{noformat}
",,,,,,,,,,,,,,CASSANDRA-7317,,,,,,,,,,10/Apr/13 17:56;yukim;5424-1.1.txt;https://issues.apache.org/jira/secure/attachment/12578041/5424-1.1.txt,16/Apr/13 04:19;yukim;5424-v2-1.2.txt;https://issues.apache.org/jira/secure/attachment/12578865/5424-v2-1.2.txt,16/Apr/13 20:00;yukim;5424-v3-1.2.txt;https://issues.apache.org/jira/secure/attachment/12579001/5424-v3-1.2.txt,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-04-09 16:36:02.759,,,no_permission,,,,,,,,,,,,321006,,,Mon Jun 03 17:15:21 UTC 2013,,,,,,0|i1jdwv:,321347,,,,,,,,jbellis,jbellis,,,,,,,,,,04/Apr/13 19:15;jjordan;Tested back on 1.1.7 (before some recent repair changes) and it has the same issue.,"09/Apr/13 16:36;yukim;CASSANDRA-3912 changed the behavior of repair to not perform if given range is not part of the local node for Keyspace that does not have replica.
For above case, StorageService#getLocalRanges [here|https://github.com/apache/cassandra/blob/cassandra-1.1.9/src/java/org/apache/cassandra/service/AntiEntropyService.java#L160] would return null for /10.72.111.225 and range (127605887595351923798765477786913079296,0].

Repair always sends merkle tree request to local node and synchronizes with others, so the desired behavior would be to just send merkle tree requests to those who have replica and let them synchronize.","10/Apr/13 17:16;yukim;Patch attached against 1.1.

It is basically rewrite of AntiEntropyService.getNeighbors, but I moved that static method to StorageService and renamed as getReplicaNodes because I felt that is more suitable place. And the method returns addresses of the replica nodes for given KS and range. Previously the method does not return the address of the local node, but the new version does only when the local node holds the replica.

So for the above case, /10.72.111.225 sends tree request only to other nodes in Analytics DC for the range it holds, and if there is difference, the node let others to repair the data each other.","15/Apr/13 16:17;jbellis;As you know, I'm pretty leery of making anything but the most superficial changes to 1.1.x at this point.

Am I correct that a workaround would be, ""only run repair against a node that is an owner of the given range?""",15/Apr/13 16:29;jjordan;The work around is always use repair no -pr,"15/Apr/13 16:43;jbellis;Thinking about it, -pr really should NOT affect ranges that aren't replicated to the node in question.  That's the whole point of that option!

It looks to me like the real bug here is that repair is not NTS-aware: the ""primary range"" for .38 for Keyspace1 should be (127605887595351923798765477786913079296, 42535295865117307932921825928971026432], not (0, 42535295865117307932921825928971026432].","16/Apr/13 04:19;yukim;Ok, this time I created patch against 1.2.

We've been calculating the primary range just from the tokens of the node. The patch changes this to use replication strategy's calculateNaturalEndpoint, and use the first one returned by the method as ""the primary range"". In order to do this in NTS though, I have to tweak a little bit(Set instead of List to use internally).

By this way, we get the primary ranges for .38 for Keyspace1 above are (127...296, 0] and (0, 425...32]. For .225, it returns empty range(btw I had to fix repair for empty range also).
When using vnodes, it is not guaranteed to have consecutive ranges, so I decided to return in two separate ranges.","16/Apr/13 15:42;jbellis;Some questions:
- Were we relying on the Set behavior to de-duplicate entries in {{replicas}} before copying it into an ArrayList at the end, or was that just a case of being over-cautious?
- Why don't we need to check {{ranges.size > 0}} any more in {{forceRepairAsync}}?
- Do we need to fix other uses of {{tokenMetadata.getPrimaryRangesFor}} such as {{SS.sampleKeyRange}}?
- Can we use {{getCachedEndpoints}} instead of {{calculateNaturalEndpoints}}?

Also:
- It's probably worth adding some comments to {{getPrimaryRangesForEndpoint}} -- superficially, it looks like it is incorrect since it is still using the non-Strategy-aware {{metadata.getPredecessor}}, but after working some examples I am satisfied that it does the right thing, as it does here.
","16/Apr/13 16:06;yukim;bq. Were we relying on the Set behavior to de-duplicate entries in replicas before copying it into an ArrayList at the end, or was that just a case of being over-cautious?

hmm, I think we need to check if we have duplicates.

bq. Why don't we need to check ranges.size > 0 any more in forceRepairAsync?

I added 'isEmpty' check at the beginning instead. Without that, repair command hangs on client side.

bq. Do we need to fix other uses of tokenMetadata.getPrimaryRangesFor such as SS.sampleKeyRange?

I was not sure if we need to fix. It looks like sampleKeyRange is only used by nodetool.

bq. Can we use getCachedEndpoints instead of calculateNaturalEndpoints?

Probably we can use getNaturalEndpoints, which uses cached endpoints.

I'll brush up my patch with comments and unit tests.","16/Apr/13 16:19;jbellis;bq. It looks like sampleKeyRange is only used by nodetool

It's a minor problem (looks like it's mostly there to support OPP: CASSANDRA-2917) but we should probably fix it.

Also, it looks like Bootstrap is using it to determine where to bisect ranges.  We should fix that one way or another (where ""another"" might be ""get rid of token selection on bootstrap and force people to either use vnodes or specify token manually"").  Separate ticket as followup is fine here IMO.","16/Apr/13 16:26;jbellis;bq. get rid of token selection on bootstrap and force people to either use vnodes or specify token manually

To clarify: this would be best done in 2.0.","16/Apr/13 20:00;yukim;v3 attached.

- NTS now uses LinkedHashSet in calculateNaturalEndpoint to preserve insertion order while eliminating duplicates.

- I think it is unsafe to use cached endpoints through getNaturalEndpoints since tokenMetadata cannot be consistent inside getPrimaryRangesForEndpoint, so I stick with impl from v2.

- fix sampleKeyRange. I think the problem is that the name tokenMetadata.getPrimaryRangeFor is confusing. Probably we should rename that to just getRangeFor.

- Added test for getPrimaryRangesForEndpoint to StorageServiceServerTest.
","19/Apr/13 20:51;jbellis;I think this is fine the way it was:

{code}
-        if (ranges.size() > 0)
-        {
-            new Thread(createRepairTask(cmd, keyspace, ranges, isSequential, isLocal, columnFamilies)).start();
-        }
+        new Thread(createRepairTask(cmd, keyspace, ranges, isSequential, isLocal, columnFamilies)).start();
{code}

Otherwise LGTM.  Created CASSANDRA-5499 for followup.",19/Apr/13 21:34;yukim;Committed with above fix. Thanks!,"21/May/13 19:37;rcoli;{quote}
get rid of token selection on bootstrap and force people to either use vnodes or specify token manually
{quote}

This has seemed operationally sane to me since approximately 0.6 series. We gain almost nothing (noobs will really be discouraged by having to set a token manually?) and expose ourselves to unnecessary complexity and edge cases like this. +1
",21/May/13 19:39;jbellis;Done in CASSANDRA-5518.,"03/Jun/13 12:19;alprema;We just applied 1.2.5 on our cluster and the repair hanging is fixed, but the -pr is still not working as expected.
Our cluster has two datacenters, let's call them dc1 and dc2, we created a Keyspace Test_Replication with replication factor _\{ dc1: 3 \}_ (no info for dc2) and ran a nodetool repair Test_Replication (that used to hang) on dc2 and it exited saying there was nothing to do (which is OK).
Then we changed the replication factor to _\{ dc1: 3, dc2: 3 \}_ and started a nodetool repair -pr Test_Replication on cassandra11@dc2 which output this:
{code}
user@cassandra11:~$ nodetool repair -pr Test_Replication
[2013-06-03 13:54:53,948] Starting repair command #1, repairing 1 ranges for keyspace Test_Replication
[2013-06-03 13:54:53,985] Repair session 676c00f0-cc44-11e2-bfd5-3d9212e452cc for range (0,1] finished
[2013-06-03 13:54:53,985] Repair command #1 finished
{code}
But even after flushing the Keyspace, there was no data on the server.
We then ran a full repair:
{code}
user@cassandra11:~$ nodetool repair  Test_Replication
[2013-06-03 14:01:56,679] Starting repair command #2, repairing 6 ranges for keyspace Test_Replication
[2013-06-03 14:01:57,260] Repair session 63632d70-cc45-11e2-bfd5-3d9212e452cc for range (0,1] finished
[2013-06-03 14:01:57,260] Repair session 63650230-cc45-11e2-bfd5-3d9212e452cc for range (56713727820156410577229101238628035243,113427455640312821154458202477256070484] finished
[2013-06-03 14:01:57,260] Repair session 6385d0a0-cc45-11e2-bfd5-3d9212e452cc for range (1,56713727820156410577229101238628035242] finished
[2013-06-03 14:01:57,260] Repair session 639f7320-cc45-11e2-bfd5-3d9212e452cc for range (56713727820156410577229101238628035242,56713727820156410577229101238628035243] finished
[2013-06-03 14:01:57,260] Repair session 63af51a0-cc45-11e2-bfd5-3d9212e452cc for range (113427455640312821154458202477256070484,113427455640312821154458202477256070485] finished
[2013-06-03 14:01:57,295] Repair session 63b12660-cc45-11e2-bfd5-3d9212e452cc for range (113427455640312821154458202477256070485,0] finished
[2013-06-03 14:01:57,295] Repair command #2 finished
{code}
After which we could find the data on dc2 as expected.

So it seems that -pr is still not working as expected, or maybe we're doing/understanding something wrong.
(I was not sure if I should open a new ticket or comment this one so please let me know if I should move it)","03/Jun/13 14:01;jbellis;What *should* happen is that if you repair -pr on each node in dc2, then you will repair the full token space.  But for a single node, YMMV.  In particular, it's quite possible that this is correct:

bq. Repair session 676c00f0-cc44-11e2-bfd5-3d9212e452cc for range (0,1] finished

Note the tiny range involved.  (This indicates that your dc2 tokens are not balanced, btw.)","03/Jun/13 14:43;jbellis;bq. This indicates that your dc2 tokens are not balanced, btw

Hmm.  Actually I don't see how repair could generate only a single range in a 2-DC setup and NTS.  Can you post your ring?","03/Jun/13 14:52;jjordan;With the following replication:
{noformat}
{ dc1: 3, dc2: 3 }
{noformat}

And the following ring:
{noformat}
node dc  token
n0   dc1 0
n1   dc2 1
{noformat}

That is the expected output from ""nodetool -h n1 repair -pr"".  Do a ""nodetool -h n0 repair -pr"" and n1 will get a bunch of data.  -pr only repairs from current token to previous token, if you don't have any data with a token of ""1"", then repair -pr won't do much for repairing n1.","03/Jun/13 15:28;jbellis;I should have said, 2-DC setup, NTS, and replicas in both DC.  And more than one node in each DC.

In any case, I do see the problem now.  Working on a fix.","03/Jun/13 15:54;jjordan;If there is a problem, glad you found it, but I don't see how multiple nodes changes the fact that the primary range of n1 is only (0,1] if both DC's have replicas.","03/Jun/13 16:03;alprema;*[EDIT] I didn't see your latests posts before posting, but I hope the extra data can help anyway*

You were right to say that I need to run the repair -pr on the three nodes, because I only have one row (it's a test) in the CF so I guess I had to run the repair -pr on the node in charge of this key.
But I restarted my test and did the repair on all three nodes, and it didn't work either; here's the output:
{code}
user@cassandra11:~$ nodetool repair -pr Test_Replication
[2013-06-03 13:54:53,948] Starting repair command #1, repairing 1 ranges for keyspace Test_Replication
[2013-06-03 13:54:53,985] Repair session 676c00f0-cc44-11e2-bfd5-3d9212e452cc for range (0,1] finished
[2013-06-03 13:54:53,985] Repair command #1 finished
{code}

{code}
user@cassandra12:~$ nodetool repair -pr Test_Replication
[2013-06-03 17:33:17,844] Starting repair command #1, repairing 1 ranges for keyspace Test_Replication
[2013-06-03 17:33:17,866] Repair session e9f38c50-cc62-11e2-af47-db8ca926a9c5 for range (56713727820156410577229101238628035242,56713727820156410577229101238628035243] finished
[2013-06-03 17:33:17,866] Repair command #1 finished
{code}

{code}
user@cassandra13:~$ nodetool repair -pr Test_Replication
[2013-06-03 17:33:29,689] Starting repair command #1, repairing 1 ranges for keyspace Test_Replication
[2013-06-03 17:33:29,712] Repair session f102f3a0-cc62-11e2-ae98-39da3e693be3 for range (113427455640312821154458202477256070484,113427455640312821154458202477256070485] finished
[2013-06-03 17:33:29,712] Repair command #1 finished
{code}

The data is still not copied to the new datacenter, and I don't understand why the repair is made for those ranges (a range of 1??), it could be a problem of unbalanced cluster as you suggested, but we distributed the tokens as advised (+1 on the nodes of the new datacenter) as you can see in the following nodetool status:

{code}
user@cassandra13:~$ nodetool status
Datacenter: dc1
=====================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address         Load       Owns   Host ID                               Token                                    Rac
UN  cassandra01     102 GB     33.3%  fa7672f5-77f0-4b41-b9d1-13bf63c39122  0                                        RC1
UN  cassandra02     88.73 GB   33.3%  c799df22-0873-4a99-a901-5ef5b00b7b1e  56713727820156410577229101238628035242   RC1
UN  cassandra03     50.86 GB   33.3%  5b9c6bc4-7ec7-417d-b92d-c5daa787201b  113427455640312821154458202477256070484  RC1
Datacenter: dc2
======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address         Load       Owns   Host ID                               Token                                    Rac
UN  cassandra11     51.21 GB   0.0%   7b610455-3fd2-48a3-9315-895a4609be42  1                                        RC2
UN  cassandra12     45.02 GB   0.0%   8553f2c0-851c-4af2-93ee-2854c96de45a  56713727820156410577229101238628035243   RC2
UN  cassandra13     36.8 GB    0.0%   7f537660-9128-4c13-872a-6e026104f30e  113427455640312821154458202477256070485  RC2
{code}

Furthermore the full repair works, as you can see in this log:

{code}
user@cassandra11:~$ nodetool repair  Test_Replication
[2013-06-03 17:44:07,570] Starting repair command #5, repairing 6 ranges for keyspace Test_Replication
[2013-06-03 17:44:07,903] Repair session 6d37b720-cc64-11e2-bfd5-3d9212e452cc for range (0,1] finished
[2013-06-03 17:44:07,903] Repair session 6d3a0110-cc64-11e2-bfd5-3d9212e452cc for range (56713727820156410577229101238628035243,113427455640312821154458202477256070484] finished
[2013-06-03 17:44:07,903] Repair session 6d4d6200-cc64-11e2-bfd5-3d9212e452cc for range (1,56713727820156410577229101238628035242] finished
[2013-06-03 17:44:07,903] Repair session 6d581060-cc64-11e2-bfd5-3d9212e452cc for range (56713727820156410577229101238628035242,56713727820156410577229101238628035243] finished
[2013-06-03 17:44:07,903] Repair session 6d5ea010-cc64-11e2-bfd5-3d9212e452cc for range (113427455640312821154458202477256070484,113427455640312821154458202477256070485] finished
[2013-06-03 17:44:07,934] Repair session 6d604dc0-cc64-11e2-bfd5-3d9212e452cc for range (113427455640312821154458202477256070485,0] finished
[2013-06-03 17:44:07,934] Repair command #5 finished
{code}

I hope this information can help, please let me know if you think it's a configuration issue, in which case I would talk to the mailing list.","03/Jun/13 16:12;jjordan;[~alprema] you need to run it on all 6 nodes.  repair -pr only repairs the primary range, when ever you use repair -pr you must run repair on every node which owns the data for the KS you are repairing.  If the KS is only in DC1, that is 3 nodes, if it is in DC1 and DC2 that is 6 nodes.","03/Jun/13 16:13;jbellis;I was right the first time; this is correct behavior.  Quoting from CASSANDRA-5608:

bq. The right way to use -pr is still to repair everywhere the data exists; if we made -pr affect everything in the DC regardless of other replicas, then repairing the full cluster would repair each range 1x for each DC, which is not what we want","03/Jun/13 17:15;alprema;I redid the same test (creating the keyspace with data, then changing its replication factor so it's replicated in DC2, then repairing) and it turns out that if you don't run a repair on DC2 before changing the replication factor, the repair -pr works fine \-_\-.

Anyway, your solution worked, thank you for your help and sorry I polluted JIRA with my questions.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CAS should distinguish promised and accepted ballots,CASSANDRA-6023,12668469,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,13/Sep/13 11:48,12/Mar/19 14:17,13/Mar/19 22:29,16/Sep/13 06:48,2.0.1,,,,,,0,LWT,,,,"Currently, we only keep 1) the most recent promise we've made and 2) the last update we've accepted. But we don't keep the ballot at which that last update was accepted. And because a node always promise to newer ballot, this means an already committed update can be replayed even after another update has been committed. Re-committing a value is fine, but only as long as we've not start a new round yet.

Concretely, we can have the following case (with 3 nodes A, B and C) with the current implementation:
* A proposer P1 prepare and propose a value X at ballot t1. It is accepted by all nodes.
* A proposer P2 propose at t2 (wanting to commit a new value Y). If say A and B receive the commit of P1 before the propose of P2 but C receives those in the reverse order, we'll current have the following states:
{noformat}
A: in-progress = (t2, _), mrc = (t1, X)
B: in-progress = (t2, _), mrc = (t1, X)
C: in-progress = (t2, X), mrc = (t1, X)
{noformat}
Because C has received the t1 commit after promising t2, it won't have removed X during t1 commit (but note that the problem is not during commit, that example still stand if C never receive any commit message).
* Now, based on the promise of A and B, P2 will propose Y at t2 (C don't see this propose in particular, not before he promise on t3 below at least). A and B accepts, P2 will send a commit for Y.
* In the meantime a proposer P3 submit a prepare at t3 (for some other irrelevant value) which reaches C before it receives P2 propose&commit. That prepare reaches A and B too, but after the P2 commit. At that point the state will be:
{noformat}
A: in-progress = (t3, _), mrc = (t2, Y)
B: in-progress = (t3, _), mrc = (t2, Y)
C: in-progress = (t3, X), mrc = (t2, Y)
{noformat}
In particular, C still has X as update because each time it got a commit, it has promised to a more recent ballot and thus skipped the delete. The value is still X because it has received the P2 propose after having promised t3 and has thus refused it.
* P3 gets back the promise of say C and A. Both response has t3 as in-progress ballot (and it is more recent than any mrc) but C comes with value X. So P3 will replay X. Assuming no more contention this replay will succeed and X will be committed at t3.

At the end of that example, we've comitted X, Y and then X again, even though only P1 has ever proposed X.

I believe the correct fix is to keep the ballot of when an update is accepted (instead of using the most recent promised ballot). That way, in the example above, P3 would receive from C a promise on t3, but would know that X was accepted at t1. And so P3 would be able to ignore X since the mrc of A will tell him it's an obsolete value.
",,,,,,,,,,,,,,,,,,,,,,,,13/Sep/13 12:03;slebresne;0001-Distinguish-between-promised-and-accepted-ballots.txt;https://issues.apache.org/jira/secure/attachment/12603003/0001-Distinguish-between-promised-and-accepted-ballots.txt,13/Sep/13 12:03;slebresne;0002-Populate-commitsByReplica-in-PrepareCallback.txt;https://issues.apache.org/jira/secure/attachment/12603004/0002-Populate-commitsByReplica-in-PrepareCallback.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-09-14 21:02:42.196,,,no_permission,,,,,,,,,,,,348403,,,Mon Sep 16 06:48:16 UTC 2013,,,,,,0|i1o2q7:,348700,,,,,,,,jbellis,jbellis,,,2.0.0,,,,,,,"13/Sep/13 12:03;slebresne;Attaching patch for the suggestion above. The patch also simplify slightly SK.savePaxosCommit: we used to not erase the update if the commit was older than in-progress. I believe that was a bit buggy and in any case unecessary since we write with the commit timestamp (so that there was no risk to erase a more recent update in fact). The other thing we were doing is to update the in-progress ballot if the commit was newer: I'm not sure that has any benefit and it makes me nervous to update in-progress outside of the prepare phase. Besides, if we remove that, we don't need to read the state to commit, which save a read and the lock acquisition on every commit.

I'm including a 2nd trivial patch that adds the population of commitsByReplica in PrepareCallback. It's partly unrelated to the problem of this ticket but it's clearly wrong and I'm not sure that warrant a separate ticket.
","14/Sep/13 21:02;jbellis;I'm pretty sure we don't need to make everything volatile/concurrent in PrepareCallback, since the latch and synchronized establish happens-before for the StorageProxy thread.

I'm not crazy about PrepareResponse including two different values for inProgressCommit.  Can we clean that up somehow?

Might also be worth renaming in_progress_ballot to promised_ballot to be a little more clear on the distinction vs proposal_ballot.

Rest LGTM.","15/Sep/13 13:11;jbellis;... Thinking about it more, I'm fine with the PrepareResponse, and it's not worth the backwards compatibility code for a column rename.  +1","16/Sep/13 06:48;slebresne;Ok. Committed then, thanks.

(I did remove the volatile though. I got a ConcurrentModificationException during one run on commitsByReplica and instead of just fixing that I went overboard with the volatile. But you're right, the latch makes it unnecessary).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deleted row resurrects if was not compacted in GC Grace Timeout due to thombstone read optimization in CollactionController,CASSANDRA-6025,12668481,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,m0nstermind,m0nstermind,13/Sep/13 13:40,12/Mar/19 14:17,13/Mar/19 22:29,19/Sep/13 22:44,2.0.1,,,,,,0,,,,,"How to reproduce:
1. Insert column
2. Flush, so you'll have sstable-1
3. Delete just inserted column
4. Flush, now you have sstable-2 as well
5. Left it uncompacted for more then gc grace time or just use 0, so you dont have to wait
6. Read data form column. You'll read just deleted column


{code}
            /* add the SSTables on disk */
// This sorts sstables in the order sstable-2, sstable-1
            Collections.sort(view.sstables, SSTable.maxTimestampComparator);
//...
            for (SSTableReader sstable : view.sstables)
            {
//...
                if (iter.getColumnFamily() != null)
//...
                    while (iter.hasNext())
                    {
                        OnDiskAtom atom = iter.next();
// the problem is here. reading atom after gc grace time
// makes this condition false. so tombstone from sstable-2
// is not placed to temp container and is just thrown away.
// On next iteration of outer for statement an original
// data inserted in step 1 from sstable-1 will be read and
// placed to temp.
                        if (atom.getLocalDeletionTime() >= gcBefore)
                            temp.addAtom(atom);
//
                    }

// .. so at the end of the for statemet we resolve data from temp. which
// do not have tombstone at all -> data are resurrected.
               container.addAll(temp, HeapAllocator.instance);
 
}
{code}
",,,,,,,,,,,,,,,,,,,,,,,,13/Sep/13 13:43;m0nstermind;6025.diff;https://issues.apache.org/jira/secure/attachment/12603011/6025.diff,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-19 22:44:14.404,,,no_permission,,,,,,,,,,,,348415,,,Thu Sep 19 22:44:14 UTC 2013,,,,,,0|i1o2sv:,348712,,,,,,,,jbellis,jbellis,,,2.0.0,,,,,,,"13/Sep/13 13:43;m0nstermind;Fixed it by just deleting this condition. It seems that it is there by intention of minor optimization.
",19/Sep/13 22:44;jbellis;This is caused by CASSANDRA-5577.  I've reverted that and added a test case to catch the problem.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in snapshot repair,CASSANDRA-6011,12668274,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,nickmbailey,nickmbailey,12/Sep/13 14:57,12/Mar/19 14:17,13/Mar/19 22:29,13/Sep/13 16:41,1.2.10,2.0.1,,,,,0,,,,,"When we do a snapshot/sequential repair, we use the repair session id as the snapshot name. Unfortunately in Directories.java when we delete a snapshot, we delete it for all column families, even when called on a specific cf store.

So what can happen is this:

Node B finishes validation compaction for CF1 and Notifies Node A
Node B *starts* to delete snapshot for CF1
Node A finishes repair of CF1 and starts repair of CF2
Node B takes snapshot of CF2 and starts validation compaction, but the previous validation compaction is still deleting snapshots, so the snapshot it wants to run a validation on gets deleted out from under it.

I've only reproduced on 1.2.6, but looking at the code this definitely looks like it exists in 1.2 HEAD. Not positive about 2.0.

I think the fix is just to update Directories.java to not delete the snapshot from all column families.",,,,,,,,,,,,,,,,,,,,,,,,12/Sep/13 17:11;yukim;6011-1.2.txt;https://issues.apache.org/jira/secure/attachment/12602828/6011-1.2.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-12 17:11:35.21,,,no_permission,,,,,,,,,,,,348208,,,Fri Sep 13 16:41:59 UTC 2013,,,,,,0|i1o1jb:,348504,1.2.6,,,,,,,jbellis,jbellis,,,,,,,,,,"12/Sep/13 17:11;yukim;Patch attached to clear snapshot only for validated CF, not entire keyspace.",12/Sep/13 18:31;jbellis;+1,"13/Sep/13 15:34;nickmbailey;if we end up having another 1.1 release, this is probably worth getting in there as well","13/Sep/13 16:41;yukim;Committed, including 1.1 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Performing a ""Select count(*)"" when replication factor < node count causes assertion error and timeout",CASSANDRA-6004,12667965,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,james0915,james0915,11/Sep/13 02:04,12/Mar/19 14:17,13/Mar/19 22:29,16/Sep/13 15:24,2.0.1,,,Legacy/CQL,,,0,,,,,"When performing a ""Select Count()"" query on a table belonging to a keyspace with a replication factor less than the total node count, the following error is encountered which ultimately results in an rpc_timeout for the request:

ERROR 18:47:54,660 Exception in thread Thread[Thread-5,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.filter.IDiskAtomFilter$Serializer.deserialize(IDiskAtomFilter.java:116)
	at org.apache.cassandra.db.RangeSliceCommandSerializer.deserialize(RangeSliceCommand.java:247)
	at org.apache.cassandra.db.RangeSliceCommandSerializer.deserialize(RangeSliceCommand.java:156)
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:99)
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:148)
	at org.apache.cassandra.net.IncomingTcpConnection.handleModernVersion(IncomingTcpConnection.java:125)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:73)

The issue is not encountered when the replication factor is >= node count

To replicate the issue:
1) Create the keyspace: CREATE KEYSPACE demodb WITH REPLICATION = {'class' : 'SimpleStrategy', 'replication_factor': 1};

2) Create the table CREATE TABLE users (
  user_name varchar,
  password varchar,
  gender varchar,
  session_token varchar,
  state varchar,
  birth_year bigint,
  PRIMARY KEY (user_name));

3) Do a CQL query: ""SELECT count( * ) FROM demodb.users"" ;

The issue is reproducible even if the table is empty. Both CQLSH and client (astyanax) api calls are affected. Tested on two different clusters (2-node and 8-node)","Two node setup
Ubuntu Server 12.04
Tested on JDK 1.6 and 1.7",,,,,,,,,,,,,,,,,,,,,,,16/Sep/13 07:38;slebresne;6004.txt;https://issues.apache.org/jira/secure/attachment/12603299/6004.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-11 17:47:12.746,,,no_permission,,,,,,,,,,,,347901,,,Wed Sep 25 19:36:35 UTC 2013,,,,,,0|i1nznb:,348198,2.0.0,,,,,,,jbellis,jbellis,,,2.0.0,,,,,,,"11/Sep/13 17:47;brandon.williams;Reproduces well.  Looks like something related to CASSANDRA-4415, and IDAF is receiving a type of -1 it wasn't expecting.",16/Sep/13 07:38;slebresne;Seems CASSANDRA-4415 had forgotten to register the verb handlers correctly. Attaching simple patch to fix.,16/Sep/13 11:56;jbellis;+1,"16/Sep/13 15:24;slebresne;Committed, thanks","17/Sep/13 10:26;vongocminh;Hello,

We got the same issue on our 4-node cluster on normal ""SELECT *"" query but changing replication_factor to 4 did not help.

I rolled back our dev cluster to 1.2.9 while waiting for the patch. Everything is working now but we don't have Batch for PreparedStatements (CASSANDRA-4693).

Could you please confirm that the bugfix is not related to the condition (replication_factor < node_count)? and not specific to ""SELECT COUNT(1)"" either?

Thanks for your help.
Best regards,
Minh","17/Sep/13 10:38;slebresne;bq. could you please confirm that the bugfix is not related to the condition

I can. It's not related to the replication factor. And it will affect any select that requests a range of partition keys (select * without conditions is one) and that use the native protocol v2.","17/Sep/13 23:00;james0915;Hi Minh,

For existing keyspaces, I was able to workaround the issue in our cluster by changing the replication factor to be the same as the number of nodes and then doing a ""nodetool repair"" on each node. I tested ""Select *"" and ""Select count"", both works.

Not sure, (Sylvain can confirm or not) but having (replication = nodecount) probably allows queries to be performed without having to request a range of partition keys which triggers the bug as Sylvain mentioned.
","25/Sep/13 15:39;vongocminh;Hello,

I've deployed the new version v2.0.1 but it seems that the bug is not fixed. Here is how to reproduce the bug:
{code}

CREATE KEYSPACE mykeyspace WITH replication = {
    'class': 'SimpleStrategy',
    'replication_factor': '3'
};

USE mykeyspace;

CREATE TABLE mytable (
    id text,
    num int,
    str text,

    PRIMARY KEY (id, num)
);
CREATE INDEX ON mytable(str);
{code}

The following request always fails with rpc_timeout:
{code}
SELECT * FROM mytable WHERE str='test' AND num=1; -- NOT OK
{code}

But when we execute a ""should-be-the-same"" query, it works:
{code}
SELECT * FROM mytable WHERE num=1 AND str='test'; -- OK
{code}

And by miracle, the first query becomes functionals
{code}
SELECT * FROM mytable WHERE str='test' AND num=1; -- now is OK
{code}

Could you please have a look at the issue? It might be related to C* native secondary index?

Thanks for your help.
Best regards,
Minh","25/Sep/13 19:36;brandon.williams;[~vongocminh] I can't reproduce on 2.0 HEAD, but if it is an issue it's not related to this one so please open a new ticket.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CAS may return false but still commit the insert,CASSANDRA-6013,12668285,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,slebresne,slebresne,12/Sep/13 15:31,12/Mar/19 14:17,13/Mar/19 22:29,16/Sep/13 06:40,2.0.1,,,,,,0,LWT,,,,"If a Paxos proposer proposes some value/update and that propose fail, there is no guarantee on whether this value will be accepted or not ultimately. Paxos guarantees that we'll agree on ""a"" value (for a given round in our case), but does not guarantee that the proposer of the agreed upon value will know it.  In particular, if for a given proposal at least one accepter has accepted it but not a quorum does, then that value might (but that's not guaranteed either) be replayed (and committed) by another proposer.

Currently, if a proposer A proposes some update U but it is rejected, A will sleep a bit and retry U. But if U was accepted by at least one acceptor, some other proposer B might replay U, succeed and commit it. If A does its retry after that happens, he will prepare, check the condition, and probably find that the conditions don't apply anymore since U has been committed already. It will thus return false, even though U has been in fact committed.

Unfortunately I'm not sure there is an easy way for a proposer whose propose fails to know if the update will prevail or not eventually. Which mean the only acceptable solution I can see would be to return to the user ""I don't know"" (through some exception for instance). Which is annoying because having a proposal rejected won't be an extremely rare occurrence, even with relatively light contention, and returning ""I don't know"" often is a bit unfriendly.",,,,,,,,,,,,,,,,,,,,,,,,13/Sep/13 15:53;slebresne;6013-v2.txt;https://issues.apache.org/jira/secure/attachment/12603033/6013-v2.txt,14/Sep/13 12:24;jbellis;6013-v3.txt;https://issues.apache.org/jira/secure/attachment/12603192/6013-v3.txt,14/Sep/13 13:05;slebresne;6013-v4.patch;https://issues.apache.org/jira/secure/attachment/12603193/6013-v4.patch,13/Sep/13 07:25;jbellis;6013.txt;https://issues.apache.org/jira/secure/attachment/12602973/6013.txt,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-09-12 17:48:15.498,,,no_permission,,,,,,,,,,,,348219,,,Mon Sep 16 06:40:48 UTC 2013,,,,,,0|i1o1lr:,348515,,,,,,,,slebresne,slebresne,,,,,,,,,,"12/Sep/13 17:48;jbellis;bq. if for a given proposal at least one accepter has accepted it but not a quorum does, then that value might (but that's not guaranteed either) be replayed (and committed) by another proposer

Why not have the new leader require a quorum of replicas to say ""I have this unfinished business"" before replaying it?

(I'm pretty sure I had this logic in originally but you talked me out of it in the name of code simplification.)","13/Sep/13 07:25;jbellis;Okay, so the problem is not the retry per se, but when we have a ""split decision"" on the nodes that reply.  We can reduce the likelihood of that happening by waiting for all known live endpoints if that's required to hear from a majority.

If we still don't hear from a majority, we can return a timeout; it's valid for a transaction to be committed after a timeout.

Patch for the above attached.","13/Sep/13 15:53;slebresne;Unfortunately, I think this is a little grimmer than that. The problem is that a proposer shouldn't move on unless the propose was successful (in which case it returns to the client) or it is sure that the propose will *not* be replayed (if it is sure of that, then retrying the proposed value with a newer ballot is safe; the current problem is that we retry with a newer ballot when we're not sure of that). In other words, we should timeout unless we are either successful or all nodes have answered and none have accepted. I'm attaching a v2 doing that (but still tries to timeout as little as possible without compromising correctness).

Unfortunately, this mean we'll timeout as soon as a proposer gets a propose reject but at least one acceptor had accepted it, which is not an extremely rare condition even with moderate contention. That being said, the current behavior is plain wrong, so unless someone has a much better idea that is easy to implement, we should probably go ahead with this for now.
",14/Sep/13 12:23;jbellis;It looks to me like both uses of requiredTargets should actually be totalTargets.  v3 attached.,"14/Sep/13 13:05;slebresne;I don't follow. getSuccessful/getAcceptCount is supposed to returned how many successful accepts we got. So that's how much time we decremented remainingRequired, i.e. its initial value (requiredTargets) minus it's current value. Similarly, in isFullyRefused, we want to validate that remainingRequired was never decremented (no-one accepted), so we want to compare it's current value with its initial value, requiredTargets (comparing to totalTargets will in fact always fail).

I guess the code is more straightforward if we keep the number of accepts instead of the number of remaining accept: attaching v4 with that version (which is equivalent to v2, but with the updated comment of v3).
",14/Sep/13 15:59;jbellis;+1,"16/Sep/13 06:40;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CAS does not always correctly replay inProgress rounds,CASSANDRA-6012,12668284,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,12/Sep/13 15:29,12/Mar/19 14:17,13/Mar/19 22:29,13/Sep/13 12:32,2.0.1,,,,,,0,LWT,,,,"Paxos says that on receiving the result of a prepare from a quorum of acceptors, the proposer should propose the value of the higher-number proposal accepted amongst the ones returned by the acceptors, and only propose his own value if no acceptor has send us back a previously accepted value.

But in PrepareCallback we only keep the more recent inProgress commit regardless of whether is has an update. Which means we could ignore a value already accepted by some acceptors if any of the acceptor send us a more recent ballot than the other acceptor but with no values. The net effect is that we can mistakenly accept two different values for the same round.
",,,,,,,,,,,,,,,,,,,,,,,,12/Sep/13 15:46;slebresne;0001-Don-t-skip-paxos-old-round-replay-if-there-is-a-value-.txt;https://issues.apache.org/jira/secure/attachment/12602815/0001-Don-t-skip-paxos-old-round-replay-if-there-is-a-value-.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-12 21:44:05.801,,,no_permission,,,,,,,,,,,,348218,,,Fri Sep 13 12:32:39 UTC 2013,,,,,,0|i1o1lj:,348514,,,,,,,,jbellis,jbellis,,,2.0.0,,,,,,,"12/Sep/13 15:30;slebresne;Attaching fix: as far as checking if we should finish an inProgress round, we only need to keep the most recent inProgress commit that has a value. But so as to not break the optimization of CASSANDRA-5667, the patch also keep the most recent inProgress, regardless of whether it has a value or not.
",12/Sep/13 21:44;jbellis;+1,"13/Sep/13 12:32;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraStorage broken for bigints and ints,CASSANDRA-6102,12670654,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,alexliu68,jalkanen,jalkanen,26/Sep/13 08:07,12/Mar/19 14:17,13/Mar/19 22:29,12/Oct/13 16:51,1.2.11,,,,,,0,,,,,"I am seeing something rather strange in the way Cass 1.2 + Pig seem to handle integer values.

Setup: Cassandra 1.2.10, OSX 10.8, JDK 1.7u40, Pig 0.11.1.  Single node for testing this. 

First a table:

{noformat}
> CREATE TABLE testc (
 key text PRIMARY KEY,
 ivalue int,
 svalue text,
 value bigint
) WITH COMPACT STORAGE;

> insert into testc (key,ivalue,svalue,value) values ('foo',10,'bar',65);
> select * from testc;

key | ivalue | svalue | value
-----+--------+--------+-------
foo |     10 |    bar |     65
{noformat}

For my Pig setup, I then use libraries from different C* versions to actually talk to my database (which stays on 1.2.10 all the time).

Cassandra 1.0.12 (using cassandra_storage.jar):

{noformat}
testc = LOAD 'cassandra://keyspace/testc' USING CassandraStorage();
dump testc
(foo,(svalue,bar),(ivalue,10),(value,65),{})
{noformat}

Cassandra 1.1.10:

{noformat}
testc = LOAD 'cassandra://keyspace/testc' USING CassandraStorage();
dump testc
(foo,(svalue,bar),(ivalue,10),(value,65),{})
{noformat}

Cassandra 1.2.10:

{noformat}
(testc = LOAD 'cassandra://keyspace/testc' USING CassandraStorage();
dump testc
foo,{(ivalue,
),(svalue,bar),(value,A)})
{noformat}


To me it appears that ints and bigints are interpreted as ascii values in cass 1.2.10.  Did something change for CassandraStorage, is there a regression, or am I doing something wrong?  Quick perusal of the JIRA didn't reveal anything that I could directly pin on this.

Note that using compact storage does not seem to affect the issue, though it obviously changes the resulting pig format.

In addition, trying to use Pygmalion 

{noformat}
tf = foreach testc generate key, flatten(FromCassandraBag('ivalue,svalue,value',columns)) as (ivalue:int,svalue:chararray,lvalue:long);
dump tf

(foo,
,bar,A)
{noformat}

So no help there. Explicitly casting the values to (long) or (int) just results in a ClassCastException.
","Cassandra 1.2.9 & 1.2.10, Pig 0.11.1, OSX 10.8.x",,,,,,,,,,,,,,,,,,,,,,,28/Sep/13 05:52;alexliu68;6102-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12605644/6102-1.2-branch.txt,12/Oct/13 01:10;alexliu68;6102-2.0-branch.txt;https://issues.apache.org/jira/secure/attachment/12608123/6102-2.0-branch.txt,08/Oct/13 11:00;brandon.williams;6102-v2.txt;https://issues.apache.org/jira/secure/attachment/12607345/6102-v2.txt,09/Oct/13 02:56;alexliu68;6102-v3.txt;https://issues.apache.org/jira/secure/attachment/12607497/6102-v3.txt,11/Oct/13 19:45;alexliu68;6102-v4.txt;https://issues.apache.org/jira/secure/attachment/12608062/6102-v4.txt,11/Oct/13 19:53;alexliu68;6102-v5.txt;https://issues.apache.org/jira/secure/attachment/12608063/6102-v5.txt,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2013-09-26 15:38:01.396,,,no_permission,,,,,,,,,,,,350483,,,Sat Oct 12 16:51:17 UTC 2013,,,,,,0|i1ofj3:,350776,1.2.10,,,,,,,brandon.williams,brandon.williams,,,1.2.9,,,,,,,"26/Sep/13 15:38;brandon.williams;Hmm, ints, longs, and floats all work in the test_storage.pig script, but those are all populated by thrift.  We might have fixed this recently, but if not there's probably another bug with cql detection in ACS.","26/Sep/13 18:09;alexliu68;[~jalkanen] Can you try it with CqlStorage which should work? We recommend to use CqlStorage unless you have to use CassandraStorage. There is some issue with CassandraStorage which can't get the right validator type for the columns based on system tables. 

We may needs fall back to thrift api to get the metadata for COMPACT STORAGE cql table.","26/Sep/13 18:46;alexliu68;I propose that we implement the following
{code}
CqlStorage supports all kind of tables/column families including old thrift column families, 
new Cql tables with/without Compact storage. (this is already done)

CassandraStorage supports only old thrift column families PLUS Cql tables with Compact storage. 
It DOES NOT support other Cql tables. (I am changing code for this)
{code}

Any objection/thought?","26/Sep/13 19:06;brandon.williams;I'm mostly ok with it, except I really want to get away from the cli for CASSANDRA-5709, and long-term I think we need to.  On the other hand, if we had CASSANDRA-5695 we could just get rid of those tests, so I'm still undecided.  Can you explain what the problem is here?","26/Sep/13 21:57;alexliu68;Let me double check it, I may fix the issue.","28/Sep/13 05:22;alexliu68;For regular Cql table e.g.
{code}
CREATE TABLE testm (
  m text,
  n text,
  o text,
  p text,
  q text,
  r text,
  PRIMARY KEY (m, n, o)
)

cqlsh:test> select * from testm;

 m  | n  | o  | p  | q  | r
----+----+----+----+----+----
  m |  n |  o |  p |  q |  r
 m1 | n1 | o1 | p1 | q1 | r1
{code}

the schema and result is 

{code}
(m,{((n,o,),),((n,o,p),p),((n,o,q),q),((n,o,r),r)})
(m1,{((n1,o1,),),((n1,o1,p),p1),((n1,o1,q),q1),((n1,o1,r),r1)})
grunt> describe test3;
test3: {key: chararray,columns: {(name: (),value: bytearray)}}
{code}

System.schema_columns

{code}
qlsh:test> select * from system.schema_columns where keyspace_name='test' and columnfamily_name='testm';  

 keyspace_name | columnfamily_name | column_name | component_index | index_name | index_options | index_type | validator
---------------+-------------------+-------------+-----------------+------------+---------------+------------+------------------------------------------
          test |             testm |           p |               2 |       null |          null |       null | org.apache.cassandra.db.marshal.UTF8Type
          test |             testm |           q |               2 |       null |          null |       null | org.apache.cassandra.db.marshal.UTF8Type
          test |             testm |           r |               2 |       null |          null |       null | org.apache.cassandra.db.marshal.UTF8Type
{code}

Because the column is composite, so we need use the last component of the composite columns to find the validator
","28/Sep/13 05:23;alexliu68;For compact storage cql table, we just need treat it as thrift table.","28/Sep/13 05:42;jalkanen;What is the patch against? Trying it on both trunk and cassandra-1.2 branches; on cassandra-1.2 it fails because of

    [javac] /Users/jalkanen/Eclipse/cassandra/src/java/org/apache/cassandra/hadoop/pig/CqlStorage.java:57: error: CqlStorage is not abstract and does not override abstract method getColumnMetadata(Client) in AbstractCassandraStorage

","28/Sep/13 06:01;jalkanen;Confirmed working; result for the test case above is now (Pig 0.11.1 and patched 1.2 latest)

grunt> testc = LOAD 'cassandra://keyspace/testc' USING CassandraStorage();
grunt> dump testc;
(foo,(ivalue,10),(svalue,bar),(value,65),{})

Thank you!","07/Oct/13 20:32;brandon.williams;This part:

{noformat}
+        // Don't want to create another TBase class, so use CfDef.populate_io_cache_on_flush 
+        // to store flag of compact storage cql table.
+        if (cql3Table && !(parseType(cfDef.comparator_type) instanceof AbstractCompositeType))
+            cfDef.setPopulate_io_cache_on_flush(true);
+        
+        // Don't want to create another TBase class, so use CfDef.replicate_on_write 
+        // to store flag of cql table.
+        if (cql3Table)
+            cfDef.setReplicate_on_write(true); 
{noformat}

Feels like a hack that is going to bite us down the road when those options really do get removed.","07/Oct/13 21:25;alexliu68;Since thrift structs do not support inheritance, that is, a struct may not extend other structs, so the choice is to create a new struct CfInfo which has the following properties

{code}
struct CfInfo {
    1: required string keyspace,
    2: required string name,
    3: optional string column_type=""Standard"",
    4: optional string comparator_type=""BytesType"",
    5: optional string subcomparator_type,
    6: optional list<ColumnDef> column_metadata,
    7: optional string default_validation_class,
    8: optional string key_validation_class,
    9: optional boolean compact_cql_table,
    10: optional boolean cql3_table
}
{code}

CfInfo is a new struct, so it doesn't affect the other existing code.

or add two additional properties to CfDef
{code}
     39: optional boolean compact_cql_table,
    40: optional boolean cql3_table
{code}
It adds additional properties only used by Pig, so a little overhead.


",07/Oct/13 21:27;alexliu68;[~brandon.williams] which one is better?,"08/Oct/13 10:58;brandon.williams;What if we just tracked them locally outside of the thrift structs, like this?","08/Oct/13 16:58;alexliu68;It needs be serialized so that it can be transferred to the task nodes. We use Thrift serialization so the class need be generated by thrift interface(thrift structs). If we change to other serialization mechanism, we don't need TBase class.","08/Oct/13 17:10;brandon.williams;Ah, right.  Well, I'm open to other options so we don't chain ourselves to thrift more than we have to, but between modifying CfDef just for this or creating CfInfo, I'd prefer CfInfo.","09/Oct/13 02:25;alexliu68;A custom serialization format is implemented as followings

{code}
<compact><cql3table><CfDef>
{code}
e.g. 11<CfDef> as compact cql3 table
01<CfDef> as none compact cql3 table
where 1 as true, 0 as false.",09/Oct/13 02:58;alexliu68;6102-v3.txt patch is attached which implemnts the new custom serialization format.,11/Oct/13 19:46;alexliu68;6102-v4.txt patch is attached which reverts back the UUIDType mappings,11/Oct/13 20:36;brandon.williams;Committed.,11/Oct/13 22:44;brandon.williams;Can you post a version against 2.0 or trunk?  git is giving me all kinds of problems trying to merge this.,"11/Oct/13 23:41;alexliu68;6102-2.0-branch.txt patch is on top of cassandra-2.0 branch

","12/Oct/13 05:08;mishail;It looks like https://github.com/apache/cassandra/commit/bc8e2475fa71f4bbbf95d4294d78b96a1aa1211c broke the trunk
{noformat}
    [javac] C:\Users\mishail\workspace\cassandra\src\java\org\apache\cassandra\hadoop\pig\AbstractCassandraStorage.java:135: error: variable validators is already defined in method columnToTuple(IColumn,AbstractCassandraStorage.CfInfo,AbstractType)
    [javac]             Map<ByteBuffer,AbstractType> validators = getValidatorMap(cfDef);
    [javac]                                          ^
    [javac] C:\Users\mishail\workspace\cassandra\src\java\org\apache\cassandra\hadoop\pig\AbstractCassandraStorage.java:157: error: cannot find symbol
    [javac]             for (IColumn subcol : col.getSubColumns())
    [javac]                  ^
    [javac]   symbol:   class IColumn
    [javac]   location: class AbstractCassandraStorage
{noformat}",12/Oct/13 16:51;brandon.williams;Committed the 2.0 patch and merged it up.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Node OOMs on commit log replay when starting up,CASSANDRA-6087,12670273,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,m0nstermind,m0nstermind,m0nstermind,24/Sep/13 13:37,12/Mar/19 14:17,13/Mar/19 22:29,24/Sep/13 15:48,1.2.11,2.0.2,,,,,0,,,,,"After some activity on batchlogs and hints and CFs restarted nodes without finished draining. On startup it OOMs. 

Investigating it found, that memtables occupied all available RAM, because MeteredFlusher is started later in setup code, so memtables cannot flush. 

Fixed by starting metered flusher before commit log replay starts",,,,,,,,,,,,,,,,,,,,,,,,24/Sep/13 13:37;m0nstermind;CassandraDaemon.txt;https://issues.apache.org/jira/secure/attachment/12604795/CassandraDaemon.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-24 15:48:33.541,,,no_permission,,,,,,,,,,,,350102,,,Tue Sep 24 15:48:33 UTC 2013,,,,,,0|i1od6v:,350396,,,,,,,,jbellis,jbellis,,,,,,,,,,24/Sep/13 15:48;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException causing query timeout,CASSANDRA-6098,12670548,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,lexlythius,lexlythius,25/Sep/13 18:51,12/Mar/19 14:17,13/Mar/19 22:29,01/Oct/13 07:02,2.0.2,,,,,,0,,,,,"A common SELECT query could not be completed failing with.

{noformat}
Request did not complete within rpc_timeout.
{noformat}

output.log showed this:
{noformat}
ERROR 15:38:04,036 Exception in thread Thread[ReadStage:170,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1867)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.index.composites.CompositesIndexOnRegular.isStale(CompositesIndexOnRegular.java:97)
        at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:247)
        at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:102)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
        at org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1651)
        at org.apache.cassandra.db.index.composites.CompositesSearcher.search(CompositesSearcher.java:50)
        at org.apache.cassandra.db.index.SecondaryIndexManager.search(SecondaryIndexManager.java:525)
        at org.apache.cassandra.db.ColumnFamilyStore.search(ColumnFamilyStore.java:1639)
        at org.apache.cassandra.db.RangeSliceCommand.executeLocally(RangeSliceCommand.java:135)
        at org.apache.cassandra.service.StorageProxy$LocalRangeSliceRunnable.runMayThrow(StorageProxy.java:1358)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1863)
{noformat}","CQLSH 4.0.0
Cassandra 2.0.0
Oracle Java 1.7.0_40
Ubuntu 12.04.3 x64",,,,,,,,,,,,,,,,,,,,,,,30/Sep/13 18:53;slebresne;6098.txt;https://issues.apache.org/jira/secure/attachment/12605949/6098.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-26 12:11:52.602,,,no_permission,,,,,,,,,,,,350377,,,Tue Oct 01 07:02:23 UTC 2013,,,,,,0|i1oevj:,350670,,,,,,,,jbellis,jbellis,,,,,,,,,,"25/Sep/13 19:11;lexlythius;This happens only when querying for some particular values on a secondary index. Cannot reproduce it querying by primary key.

Neither of the following did any good:
{noformat}
Restarting Cassandra
nodetool rebuild_index DB TABLE INDEX
nodetool invalidaterowcache
nodetool invalidatekeycache
nodetool cleanup
nodetool repair
{noformat}","25/Sep/13 19:25;lexlythius;Dropping the secondary index and creating it again solved the problem at hand.
I still think the issue is worth looking into, though.
","26/Sep/13 12:11;slebresne;Right, that's a legit issue. Attaching simple patch to fix.","26/Sep/13 13:27;jbellis;Patch looks good, can you add a unit test?",30/Sep/13 18:53;slebresne;Update patch with some unit testing inside.,30/Sep/13 19:22;jbellis;+1,"01/Oct/13 07:02;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: allow names for bind variables,CASSANDRA-6033,12668796,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,16/Sep/13 15:58,12/Mar/19 14:17,13/Mar/19 22:29,19/Sep/13 10:59,2.0.1,,,,,,0,,,,,"Currently bind variables are ""anonymous"", they're just a question mark. What this means is that the only reliable way to reference those variables after preparation is through position their position in the query string, which is not excessively user friendly.

Of course some driver may be tempted to add their own version of named variables, but that forces said driver to parse the query string in the first place, which is something we've tried to avoid so far. Besides, this is useful enough that making it part of CQL would make this more consistent amongst driver rather than having everyone coming up with its own syntax.

I'll add that because we are already sending column names in the metadata, I believe we can support this without any change to the protocol. The idea would be to support queries like this (happy to discuss the exact syntax):
{noformat}
SELECT * FROM test WHERE key = ?my_key AND time > ?t_low AND time <= ?t_high
{noformat}

From the Cassandra side, the only thing that this would change is that in the result set returned to the client, the column names would be 'my_key', 't_low' and 't_high' respectively rather than 'key', 'time' and 'time' as they are now.  And so in particular using an anymous variable would be equivalent to using a name one with the name of the CQL column the variable is bound to.

Driver side, the driver would just have to keep a map of each name to their position in the metadata to provide reliable setter by names.
",,,,,,,,,,,,,,,,,,,,,,,,19/Sep/13 08:58;slebresne;6033.txt;https://issues.apache.org/jira/secure/attachment/12604011/6033.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-17 22:32:10.559,,,no_permission,,,,,,,,,,,,348730,,,Thu Sep 19 10:59:45 UTC 2013,,,,,,0|i1o4r3:,349028,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"17/Sep/13 22:32;jbellis;I've already wished for this a few times, but to play devil's advocate, isn't it a bit late to change this now that we have multiple drivers in production?  I submit that users have already made their peace with this.","18/Sep/13 07:14;slebresne;I've seen at least a few users asked for something like this on the driver side, so not totally sure everyone is fully happy with the status quo. I mean, if I was convinced that everyone would say ""well, that's not supported, fair enough, I'll do without it, no biggy"", then I'd agree with you. But my fear is that people will be annoyed and start messing with query strings to try doing the equivalent of this. I'm afraid of drivers doing it all in their own way and being made more fragile by having to parse/search-and-replace the query string.

Do I wish we'd have done that sooner: sure. But typically my suggestion would be to do that for the protocol v2 only, i.e. change the spec of the protocol to say that the name in the result set is not the name of the CQL3 column, but rather the name of the bind variables. Which by default would be the CQL3 column name. We can do that spec change now, in 2.0.1, so client drivers can know what to expect. I strongly doubt anyone is in production with C* 2.0.0 and the native protocol v2 (if only because it's buggy in 2.0.0: CASSANDRA-6040). Then we can ship the syntax change in say 2.0.2: it's not a very complicated change to make.","18/Sep/13 08:06;slebresne;Btw, let me add that the name we currently send in the metadata as result of a PREPARE is already not always a proper CQL3 column name. For instance, if you prepare:
{noformat}
UPDATE foo USING TTL ? SET m[?] = ? WHERE k = 0
{noformat}
then the bind variables will be respectively named '[ttl]', 'key(m)' and 'value(m)'.

So existing drivers already cannot really assume that the name returned is a true CQL3 column name. Just to say that I doubt we'll break any existing driver by making this change, if that is the fear.
","18/Sep/13 15:45;slebresne;Attaching patch for this, with the small variation that the final syntax uses :name rather than ?name as that's a lot more common.

The CQL doc and protocol spec would probably need some updating but I'll do that upon commit if we're good on the patch.","18/Sep/13 20:26;iamaleksey;Can you update for CASSANDRA-4210? I've ghetto-rebased with apply --reject, but that's not enough.",19/Sep/13 08:58;slebresne;Rebased patch attached,19/Sep/13 09:27;iamaleksey;+1,"19/Sep/13 10:59;slebresne;Committed (with doc updates), thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when running sstablesplit on valid sstable,CASSANDRA-6026,12668515,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,dmeyer,dmeyer,13/Sep/13 17:45,12/Mar/19 14:17,13/Mar/19 22:29,17/Sep/13 07:12,1.2.10,,,,,,0,,,,,"#create cluster
ccm create --cassandra-version git:cassandra-1.2 test
ccm populate -n 1
ccm start

#run stress
ccm node1 stress -n 10000000 -o insert
ccm node1 compact

cd ~/.ccm/test/node1/data
../bin/sstablesplit -n 100 ./Keyspace1/Standard1/Keyspace1-Standard1-ic-16-Data.db

#Expected
single large sstable should be split into multiple sstables with max size 100 MB

#Actual
ERROR 10:14:06,992 Error in ThreadPoolExecutor
java.lang.NullPointerException
        at org.apache.cassandra.io.sstable.SSTableDeletingTask.run(SSTableDeletingTask.java:70)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)


Notes: It seems like the split occurs and can be recompacted.
Last known commit where split was working on 1.2 branch: 47b2cd6620894bf0c4c4584036eab49a2e14a50e
Have not bisected further.

sstablesplit is also broken on 2.0 branch; however, it fails differently.  Filing separate bug on that.
","Environment: java version ""1.7.0_40""
Java(TM) SE Runtime Environment (build 1.7.0_40-b43)
Java HotSpot(TM) 64-Bit Server VM (build 24.0-b56, mixed mode)

(Sun jvm on mac)",,,,,,,,,,,,,,,,,,,,,,,16/Sep/13 08:31;slebresne;6026.txt;https://issues.apache.org/jira/secure/attachment/12603303/6026.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-16 08:31:20.843,,,no_permission,,,,,,,,,,,,348449,,,Tue Sep 17 07:12:51 UTC 2013,,,,,,0|i1o30f:,348746,1.2.10,,,,,,,brandon.williams,brandon.williams,,,,,,,,,dmeyer,"13/Sep/13 17:59;dmeyer;This issue is probably related to CASSANDRA-6027; however, the actual behavior is different so two bugs filed.",13/Sep/13 18:57;dmeyer;Will try to find the exact commit that introduced this error.  should have it this afternoon...,"13/Sep/13 22:07;dmeyer;f663a996c799bd93963a50b418ed5fcde2e48a40 is the first bad commit
commit f663a996c799bd93963a50b418ed5fcde2e48a40
Author: Aleksey Yeschenko <aleksey@apache.org>
Date:   Thu Sep 12 20:35:31 2013 +0300

    Add SSTableDeletingNotification to DataTracker

    patch by Piotr Kołaczkowski; reviewed by Aleksey Yeschenko for
    CASSANDRA-6010

:100644 100644 1c0958922018c8f05258ea6049617d5609466800 51be09daa65bd271ee9942dc97ee1547205049d4 M	CHANGES.txt
:040000 040000 a2dec4c205cfe751271801de219082d9551f2e9b 167f3f08984fa34dd35a1cbaf6ef13bde2f9d228 M	src","16/Sep/13 08:31;slebresne;So, this is because CASSANDRA-6010 forgot to protect against the fact where the tracker was not set in SSTableDeletingTask (which is the case here). Trivial patch attached.

However CASSANDRA-6010 is marked as fixed in 1.2.10, so either the fix version is wrong there or this ticket lies about having reproduced in 1.2.9 :)",16/Sep/13 18:21;brandon.williams;+1,16/Sep/13 19:07;dmeyer;+1 patch verified.,"17/Sep/13 07:12;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"'Internal application error' on SELECT .. WHERE col1=val AND col2 IN (1,2)",CASSANDRA-6050,12669202,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,nsv,nsv,18/Sep/13 08:02,12/Mar/19 14:17,13/Mar/19 22:29,19/Sep/13 10:36,2.0.1,,,,,,0,cql3,,,,"Query with error: SELECT * FROM user WHERE login='nsv' AND st IN ('1','2') ALLOW FILTERING;

Query works:
SELECT * FROM user WHERE login='nsv' AND st IN ('1') ALLOW FILTERING;
-- Single item inside IN

Table definition: 
CREATE COLUMNFAMILY user (
     KEY uuid PRIMARY KEY,
     name text,
     avatar text,
     email text,
     phone text,
     login text,
     pw text,
     st text
);

From /var/log/cassandra/output.log:
ERROR 11:58:52,454 Internal error processing execute_cql3_query
java.lang.AssertionError
	at org.apache.cassandra.cql3.statements.SelectStatement.getIndexExpressions(SelectStatement.java:749)
	at org.apache.cassandra.cql3.statements.SelectStatement.getRangeCommand(SelectStatement.java:303)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:155)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:56)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:101)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:117)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:108)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1920)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4372)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4356)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)

","cqlsh, pdo_cassandra",,,,,,,,,,,,,,,,,,,,,,,18/Sep/13 13:18;slebresne;6050.txt;https://issues.apache.org/jira/secure/attachment/12603819/6050.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-18 13:18:36.522,,,no_permission,,,,,,,,,,,,349134,,,Thu Sep 19 10:36:31 UTC 2013,,,,,,0|i1o78n:,349432,2.0.0,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"18/Sep/13 13:18;slebresne;We've apparently broken validation in 2.0 (1.2 is not affected): we don't support IN on non-primary key columns, even when there is an index (the only reason this work with only 1 element in the IN is that the code don't distinguish that from a EQ; that's probably a mistake in the first place, but for compatibility sake I think we should leave it that way). Attaching patch that fix the validation (I pushed a dtests too).",18/Sep/13 19:14;iamaleksey;+1,"19/Sep/13 10:36;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stress stopped calculating latency stats,CASSANDRA-6153,12672644,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mishail,enigmacurry,enigmacurry,07/Oct/13 16:31,12/Mar/19 14:17,13/Mar/19 22:29,08/Oct/13 18:39,2.1 beta1,,,Legacy/Tools,,,0,,,,,"In trunk, cassandra-stress has stopped calculating all latency information:

From trunk:
{code}
$ ccm node1 stress
Created keyspaces. Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,latency,95th,99.9th,elapsed_time
89995,8999,8999,0.0,0.0,0.0,10
304267,21427,21427,0.0,0.0,0.0,20
514791,21052,21052,0.0,0.0,0.0,30
727471,21268,21268,0.0,0.0,0.0,40
926467,19899,19899,0.0,0.0,0.0,50
1000000,7353,7353,0.0,0.0,0.0,54


Averages from the middle 80% of values:
interval_op_rate          : 21249
interval_key_rate         : 21249
latency median            : 0.0
latency 95th percentile   : 0.0
latency 99.9th percentile : 0.0
Total operation time      : 00:00:54
END
{code}

From 2.0:
{code}
$ ccm node1 stress
Created keyspaces. Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,latency,95th,99.9th,elapsed_time
66720,6672,6672,0.2,25.6,201.6,10
289577,22285,22285,0.2,3.4,201.1,20
489105,19952,19952,0.2,1.8,201.2,30
660916,17181,17181,0.2,1.6,87.9,40
847452,18653,18653,0.2,1.6,108.8,50
1000000,15254,15254,0.2,1.6,108.9,59


Averages from the middle 80% of values:
interval_op_rate          : 19517
interval_key_rate         : 19517
latency median            : 0.2
latency 95th percentile   : 2.1
latency 99.9th percentile : 149.8
Total operation time      : 00:00:59
END
{code}",,,,,,,,,,,,,,,,,,,,,,,,08/Oct/13 18:01;mishail;trunk-6153.patch;https://issues.apache.org/jira/secure/attachment/12607397/trunk-6153.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-08 06:48:21.124,,,no_permission,,,,,,,,,,,,352267,,,Tue Oct 08 18:41:12 UTC 2013,,,,,,0|i1oqh3:,352555,2.1 rc3,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"08/Oct/13 06:48;mishail;I believe it's related to https://github.com/apache/cassandra/commit/5dabd1cc0c65b329ff518d7ad3f09e4c11494f18
Not sure why we need to convert from nanos there",08/Oct/13 10:30;brandon.williams;I think you're right about the cause.  Ping [~dbrosius],"08/Oct/13 17:41;mishail;There was an attempt to migrate to Metrics 3.0.1 (https://issues.apache.org/jira/browse/CASSANDRA-5838) which was later reverted.

* Initial commit: https://github.com/apache/cassandra/commit/c27a161920a2227cd04f8338a75732920694b1db
* Then [~dbrosius] updated Stress to work with new metrics: https://github.com/apache/cassandra/commit/5dabd1cc0c65b329ff518d7ad3f09e4c11494f18
* And then Metrics were reverted back to 2.2.0 https://github.com/apache/cassandra/commit/3205e5dbbc8fb8f365b72137cf1c1ea50f15cab6 but changes for Stress weren't rolled back

",08/Oct/13 18:02;mishail;Reverted 5dabd1cc0c65b329ff518d7ad3f09e4c11494f18 ,"08/Oct/13 18:39;brandon.williams;Committed, thanks!",08/Oct/13 18:41;enigmacurry;+1. Thanks.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OOM in Cassandra 2.0.1,CASSANDRA-6149,12672509,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,depend,depend,06/Oct/13 04:30,12/Mar/19 14:17,13/Mar/19 22:29,07/Oct/13 19:21,2.0.2,,,,,,0,,,,,"I have a program to stress test Cassandra. What it does is remove/insert rows with a small set of row keys as fast as possible. Two CFs are involved. When I test against C* 1.2.3 with default configurations, it ran for 24 hours and C* doesn't having any issue. However after I upgraded to C* 2.0.1, C* crashes on OOM within 1-2 minutes. I can consistently reproduce this.

I built C* from the source and found out the last good changeset is cfa097cdd5e28d7fe8204248e246a1fae226d2c0. As soon as I include the next changeset 1e0d9513b748fae4ec0737283da71c65e9272102, C* starts to crash. What's interesting is although it seems the change was reverted by fc1a7206fe15882fd64e7ba8eb68ba9dc320275f. C* built from fc1a7206fe15882fd64e7ba8eb68ba9dc320275f has the same problem - OOM within minutes.

I didn't test against the official 2.0.0. But the C* built from 03045ca22b11b0e5fc85c4fabd83ce6121b5709b seems OK. I assume that's what 2.0.0 is.

I use default configurations in all cases. I didn't tune anything.",Windows 7 64 bit. Java 64-bit 1.7.0_25. Cassandra 2.0.1,,,,,,,,,,,,,,,,,,,,,,,06/Oct/13 15:52;jbellis;6149-debug.txt;https://issues.apache.org/jira/secure/attachment/12607082/6149-debug.txt,06/Oct/13 18:47;jbellis;6149.txt;https://issues.apache.org/jira/secure/attachment/12607096/6149.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-10-06 06:24:13.578,,,no_permission,,,,,,,,,,,,352136,,,Mon Oct 07 19:21:03 UTC 2013,,,,,,0|i1opnz:,352424,2.0.1,,,,,,,xedin,xedin,,,2.0.1,,,,,,,"06/Oct/13 06:24;jbellis;It doesn't entirely shock me that 1e0d9513b748fae4ec0737283da71c65e9272102 could cause problems (CASSANDRA-5661).

What does the heap dump show is using all the memory?  http://www.eclipse.org/mat/","06/Oct/13 12:48;depend;MAT shows ""One instance of ""com.google.common.cache.LocalCache"" loaded by ""sun.misc.Launcher$AppClassLoader @ 0xd03530b0"" occupies 942,959,256 (91.01%) bytes.""

Another thing I notice is this OOM accompanoes with a LOT of ""Unable to delete ... (it will be removed on server restart; we'll also retry after GC)"" errors. I understand it might be OK on Windows. But the amount of such errors I see with 1e0d9513b748fae4ec0737283da71c65e9272102 seem unusual. In fact I don't see a single such error   prior to 1e0d9513b748fae4ec0737283da71c65e9272102.","06/Oct/13 15:52;jbellis;I think we have a bug in cache size estimation, but I'm not sure how that turns into more-memory-used-than-1-2 which simply caches all readers.

Attached is a patch to add some debug logging.  Can you test after enabling debug on org.apache.cassandra.service.FileCacheService?","06/Oct/13 17:43;depend;I got a bunch of ""less than"" estimation. Here is the part right before OOM:

...
DEBUG [ReadStage:32] 2013-10-06 13:33:36,610 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\MetaData\Keyspace1-MetaData-jb-1-Data.db
DEBUG [ReadStage:22] 2013-10-06 13:33:36,614 FileCacheService.java (line 127) Estimated memory usage is 327680 compared to actual usage 464191488
DEBUG [ReadStage:22] 2013-10-06 13:33:36,614 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\MetaData\Keyspace1-MetaData-jb-1-Data.db
 INFO [ScheduledTasks:1] 2013-10-06 13:33:36,913 GCInspector.java (line 116) GC for ConcurrentMarkSweep: 962 ms for 4 collections, 1036218016 used; max is 1037959168
DEBUG [ReadStage:24] 2013-10-06 13:33:36,916 FileCacheService.java (line 127) Estimated memory usage is 327680 compared to actual usage 464257024
DEBUG [ReadStage:24] 2013-10-06 13:33:36,916 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\Keyword\Keyspace1-Keyword-jb-1-Data.db
DEBUG [ReadStage:25] 2013-10-06 13:33:36,918 FileCacheService.java (line 127) Estimated memory usage is 327680 compared to actual usage 464322560
DEBUG [ReadStage:25] 2013-10-06 13:33:36,919 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\MetaData\Keyspace1-MetaData-jb-1-Data.db
DEBUG [ReadStage:19] 2013-10-06 13:33:36,920 FileCacheService.java (line 127) Estimated memory usage is 327680 compared to actual usage 464388096
DEBUG [ReadStage:19] 2013-10-06 13:33:36,920 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\MetaData\Keyspace1-MetaData-jb-1-Data.db
DEBUG [ReadStage:10] 2013-10-06 13:33:36,939 FileCacheService.java (line 127) Estimated memory usage is 327680 compared to actual usage 464453632
DEBUG [ReadStage:10] 2013-10-06 13:33:36,940 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\MetaData\Keyspace1-MetaData-jb-1-Data.db
DEBUG [ReadStage:10] 2013-10-06 13:33:36,941 FileCacheService.java (line 127) Estimated memory usage is 327680 compared to actual usage 464519168
DEBUG [ReadStage:10] 2013-10-06 13:33:36,941 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\MetaData\Keyspace1-MetaData-jb-1-Data.db
DEBUG [ReadStage:30] 2013-10-06 13:33:37,171 FileCacheService.java (line 127) Estimated memory usage is 327680 compared to actual usage 464584704
DEBUG [ReadStage:30] 2013-10-06 13:33:37,171 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\MetaData\Keyspace1-MetaData-jb-1-Data.db
DEBUG [ReadStage:7] 2013-10-06 13:33:37,171 FileCacheService.java (line 127) Estimated memory usage is 327680 compared to actual usage 464584704
DEBUG [ReadStage:8] 2013-10-06 13:33:37,171 FileCacheService.java (line 127) Estimated memory usage is 327680 compared to actual usage 464584704
DEBUG [ReadStage:8] 2013-10-06 13:33:37,171 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\MetaData\Keyspace1-MetaData-jb-1-Data.db
DEBUG [ReadStage:11] 2013-10-06 13:33:37,171 FileCacheService.java (line 127) Estimated memory usage is 327680 compared to actual usage 464584704
DEBUG [ReadStage:11] 2013-10-06 13:33:37,171 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\MetaData\Keyspace1-MetaData-jb-1-Data.db
DEBUG [ReadStage:7] 2013-10-06 13:33:37,171 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\MetaData\Keyspace1-MetaData-jb-1-Data.db
DEBUG [ReadStage:17] 2013-10-06 13:33:39,618 FileCacheService.java (line 127) Estimated memory usage is 327680 compared to actual usage 464846848
DEBUG [ReadStage:17] 2013-10-06 13:33:39,848 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\MetaData\Keyspace1-MetaData-jb-1-Data.db
ERROR [ReadStage:23] 2013-10-06 13:33:39,848 CassandraDaemon.java (line 185) Exception in thread Thread[ReadStage:23,5,main]
java.lang.OutOfMemoryError: Java heap space
...

It does look like an estimation error.",06/Oct/13 17:58;jbellis;How many requests/hits are in FileCacheMetrics?  (Use JConsole to look at o.a.c.metrics.FCM),"06/Oct/13 18:47;jbellis;While the size bug is the proximate cause of the OOM, it looks like it's never actually re-using the cached entries because it's comparing non-absolute with absolute paths.  Attached patch fixes both.",06/Oct/13 23:41;depend;fix is confirmed. Thank you.,07/Oct/13 18:39;xedin;+1,07/Oct/13 19:21;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexedSliceReader can skip columns when fetching multiple contiguous slices,CASSANDRA-6119,12671547,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,frousseau,frousseau,frousseau,01/Oct/13 10:32,12/Mar/19 14:17,13/Mar/19 22:29,03/Oct/13 09:11,1.2.11,,,,,,0,,,,,"This was observed using SliceQueryFilter with multiple slices.

Let's have a row ""a"" having the following column list : ""colA"", ""colB"", ""colC"", ""colD""
Then select 2 ranges : [""colA"", ""colB""], [""colC"", ""colD""]
Expected result is the four columns
But only 3 are returned (""colA"", ""colB"", ""colD"")

To reproduce the above scenario in the unit tests, you can modify the test ""ColumnFamilyStoreTest.testMultiRangeIndexed"" by replacing the original line :
        String[] letters = new String[] { ""a"", ""b"", ""c"", ""d"", ""e"", ""f"", ""g"", ""h"", ""i"" };
by this one (""f"" letter has been removed) :
        String[] letters = new String[] { ""a"", ""b"", ""c"", ""d"", ""e"", ""g"", ""h"", ""i"" };

Anyway, a patch is attached which adds more unit tests, and modifies IndexedSliceReader.IndexedBlockFetcher & IndexedSliceReader.SimpleBlockFetcher 
",,,,,,,,,,,,,,,,,,,,,,,,01/Oct/13 16:40;slebresne;6119-v2.txt;https://issues.apache.org/jira/secure/attachment/12606130/6119-v2.txt,01/Oct/13 10:36;frousseau;6119.patch;https://issues.apache.org/jira/secure/attachment/12606075/6119.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-10-01 16:40:28.68,,,no_permission,,,,,,,,,,,,351257,,,Thu Oct 03 09:11:21 UTC 2013,,,,,,0|i1okaf:,351549,1.2.10,,,,,,,slebresne,slebresne,,,,,,,,,,"01/Oct/13 16:40;slebresne;Thanks Fabien, you're right, ISR can skip columns.

On the patch itself, it seems to me the actual bug if about dealing with the last column when exiting the main loop of both fetcher, the setNextSlice seem unrelated to the bug itself. And I believe those setNextSlice changes mainly inline in setNextSlice the tests that would be done by the next iterator of the main loop otherwise, so I'm not sure they are really optimisations. If I'm right about that, I'd really rather leave those changes away because the logic of ISR is already not the easier to follow, so I'd rather limit the number of cases as much as possible (unless there is a clear benefit performance wise).

As for the stopping conditions of the ""main"" BlockFetcher loops, I believe that for IndexedBlockFetcher it's enough to just not exit the loop until {{column != null}}, which avoids having to add a new case at the end. As for SimpleBlockFetcher, figured we could slightly refactor the loop to mimic the method of IndexedBlockFetcher too. Attaching a v2 with what I have in mind.
","02/Oct/13 10:33;frousseau;After having reviewed your patch, it has definitely a better approach minimising code change and better readability

So I'm +1 for your patch","03/Oct/13 09:11;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnsortedColumns.addAll(ColumnFamily) doesn't add the deletion info of the CF in argument,CASSANDRA-6115,12671458,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,sargun,sargun,30/Sep/13 22:17,12/Mar/19 14:17,13/Mar/19 22:29,01/Oct/13 15:39,2.0.2,,,,,,0,,,,,"Steps to reproduce:
https://gist.github.com/sargun/6771123

Doing an action outside of a BATCH results in different results than within the batch, even though the query is successful.",3 nodes on a same local host,,,,,,,,,,,,,,,,,,,,,,,01/Oct/13 08:08;slebresne;6115.txt;https://issues.apache.org/jira/secure/attachment/12606061/6115.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-30 22:39:13.196,,,no_permission,,,,,,,,,,,,351164,,,Tue Oct 01 15:39:36 UTC 2013,,,,,,0|i1ojpr:,351456,2.0.0,2.0.1,,,,,,iamaleksey,iamaleksey,,,2.0.0,,,,,,,"30/Sep/13 22:39;jbellis;Can you reproduce, Daniel?","30/Sep/13 23:49;iamaleksey;1.2 is unaffected, this is a 2.0 regression. The issue is not batch-related, something's likely wrong with resolve().",01/Oct/13 08:08;slebresne;The problem is that UnsortedColumns.addAll(ColumnFamily) doesn't add the deletion info of the CF in argument while it should. Attaching oneliner fix (I've pushed a dtest too). ,01/Oct/13 10:40;iamaleksey;+1,"01/Oct/13 15:39;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in Gossip propagation,CASSANDRA-6125,12671644,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,sbtourist,sbtourist,01/Oct/13 19:01,12/Mar/19 14:17,13/Mar/19 22:29,18/Sep/14 21:38,2.0.11,2.1.1,,,,,0,,,,,"Gossip propagation has a race when concurrent VersionedValues are created and submitted/propagated, causing some updates to be lost, even if happening on different ApplicationStatuses.
That's what happens basically:
1) A new VersionedValue V1 is created with version X.
2) A new VersionedValue V2 is created with version Y = X + 1.
3) V2 is added to the endpoint state map and propagated.
4) Nodes register Y as max version seen.
5) At this point, V1 is added to the endpoint state map and propagated too.
6) V1 version is X < Y, so nodes do not ask for his value after digests.

A possible solution would be to propagate/track per-ApplicationStatus versions, possibly encoding them to avoid network overhead.",,,,,,,,,,,,,CASSANDRA-5913,,,,,,,,,,,18/Sep/14 20:34;brandon.williams;6125.txt;https://issues.apache.org/jira/secure/attachment/12669797/6125.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-09 15:28:30.019,,,no_permission,,,,,,,,,,,,351354,,,Wed Jul 22 02:48:04 UTC 2015,,,,,,0|i1okvz:,351646,,,,,,,,jasobrown,jasobrown,,,,,,,,,,09/Oct/13 15:28;brandon.williams;Can you point me to a spot in code where step 5 happens?,"09/Oct/13 16:22;sbtourist;Step 5 happens in Gossiper#addLocalApplicationState: it's all about concurrent state changes, and even if the race condition window may seem very small, it's actually there, and if it bites, it bites hard (as effects are very difficult to trace back).",29/Jul/14 18:57;mshuler;Setting fixVer to next 2.0/2.1 versions and assigning.,"18/Sep/14 20:34;brandon.williams;bq. A possible solution would be to propagate/track per-ApplicationStatus versions, possibly encoding them to avoid network overhead

That's a fairly invasive measure.  In practice there are very few instances where we update two application states together, and really only one case: STATUS and TOKENS.  Instead, we can just create a utility method that locks the gossip task, adds the states, and then releases it, so there can be no propagation in between.  Patch to do this.","18/Sep/14 20:40;brandon.williams;There's still a small window where a remote node could gossip with us and receive a partial state, but that looks confined to node replacement where we'll be in a dead state and nobody will initiate a round with us, so we just need to isolate our own propagation.","18/Sep/14 21:30;jasobrown;+1 on the patch, and I agree with [~brandon.williams]'s thoughts on keeping the protocol simple. As long as versioned values are added to the endpointState in the order in which you assign versions (which this patch provides), there should be no problem. This should be a low bar to clear for anyone who really wants to muck around in gossip-land :).
",18/Sep/14 21:38;brandon.williams;Committed.,"22/Jul/15 02:48;peter-librato;We've seen this bug or something like it on 2.0.11 with 45 nodes in a fairly noisy AWS environment but other than CASSANDRA-8336 I don't see any fixes to gossip post 2.0.11.

The nodetool status command doesn't list the node that does't have status info. It's not up or down, it's simply not there and this impacts % ownership.
In a recent instance of this 4 nodes had the same ""status hole"" but only 2 of the 4 had different nodetool ring output compared to the other 41 ""no status hole"" members of the ring.

Restarting cassandra on the node that has a missing STATUS entry in gossip ""fixes"" the problem in that the hole goes away. This is something we used to see more commonly before 2.0.11 so it does appear this fix works but are there other places where a race might be happening?

{code}
/10.xx.yyy.169
  generation:1436544814
  heartbeat:2986679
  SEVERITY:0.0
  HOST_ID:7d22299f-b35b-4035-82bc-e2b603a655d7
  LOAD:2.555557836E11
  RACK:1e
  NET_VERSION:7
  DC:us-east
  RPC_ADDRESS:10.xx.yyy.169
  RELEASE_VERSION:2.0.11
  SCHEMA:0f72be52-2751-33a6-a172-8511e943b2ec
/10.xx.yyy.175
  generation:1419877470
  heartbeat:53496976
  SEVERITY:1.2787723541259766
  HOST_ID:c87ed8db-76b6-485a-ac2f-32c2822b1ef5
  LOAD:3.08812188602E11
  RACK:1e
  NET_VERSION:7
  STATUS:NORMAL,-1010822684895662807
  DC:us-east
  RPC_ADDRESS:10.xx.yyy.175
  RELEASE_VERSION:2.0.11
  SCHEMA:0f72be52-2751-33a6-a172-8511e943b2ec
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra-cli backward compatibility issue with Cassandra 2.0.1,CASSANDRA-6140,12672150,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,doanduyhai,doanduyhai,03/Oct/13 14:59,12/Mar/19 14:17,13/Mar/19 22:29,05/Nov/13 18:18,2.0.3,,,,,,0,,,,,"Currently we are using Cassandra 1.2.6 and we want to migrate to 2.0.1.

 We still use Thrift for some column families (migration to CQL3 is not done yet for them). We have cassandra-cli script to drop/create fresh keyspace, re-create column families and populate referential data:

*Schema creation script*
{code}
drop keyspace xxx;
create keyspace xxx with placement_strategy ...

create column family offers with 
key_validation_class = UTF8Type and
comparator = 'CompositeType(UTF8Type)'  and 
default_validation_class = UTF8Type;
{code}

*Data insertion script*:
{code}
set offers['OFFER1'][PRODUCT1']='test_product';
...
{code}

 When executing the data insertion script with Cassandra 2.0.1, we have the following stack trace:
{code}
Invalid cell for CQL3 table offers. The CQL3 column component (COL1) does not correspond to a defined CQL3 column
InvalidRequestException(why:Invalid cell for CQL3 table offers. The CQL3 column component (COL1) does not correspond to a defined CQL3 column)
	at org.apache.cassandra.thrift.Cassandra$insert_result$insert_resultStandardScheme.read(Cassandra.java:21447)
	at org.apache.cassandra.thrift.Cassandra$insert_result$insert_resultStandardScheme.read(Cassandra.java:21433)
	at org.apache.cassandra.thrift.Cassandra$insert_result.read(Cassandra.java:21367)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_insert(Cassandra.java:898)
	at org.apache.cassandra.thrift.Cassandra$Client.insert(Cassandra.java:882)
	at org.apache.cassandra.cli.CliClient.executeSet(CliClient.java:987)
	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:231)
	at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:201)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:327)
{code}

 This data insertion script works pecfectly with Cassandra 1.2.6.

 We face the same issue with Cassandra 2.0.0. It looks like the cassandra-cli commands no longer works with Cassandra 2.0.0...

  

","Linux Ubuntu, Cassandra 2.0.0",,,,,,,,,,,,,,,,,,,,,,,05/Nov/13 16:33;slebresne;6140.txt;https://issues.apache.org/jira/secure/attachment/12612202/6140.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-21 15:39:32.479,,,no_permission,,,,,,,,,,,,351776,,,Tue Nov 05 18:18:47 UTC 2013,,,,,,0|i1ongn:,352064,2.0.1,,,,,,,iamaleksey,iamaleksey,,,2.0.0,,,,,,,21/Oct/13 15:39;jbellis;Can you try 2.0 HEAD?,"02/Nov/13 09:34;doanduyhai;I just pull Cassandra from github and tried with trunk (cassandra-2.1-SNAPSHOT) and also with branch cassandra-2.0, same result.

 The stack trace is the same","03/Nov/13 05:42;dbrosius;Validation code thinks the table is a cql table, given this test
{code}
boolean isCQL3Table = metadata.hasCompositeComparator() && !metadata.isDense() && !metadata.isSuper();
{code}
and then fails, because there are no column names in the meta data
{code}
ColumnIdentifier columnId = new ColumnIdentifier(CQL3ColumnName, composite.types.get(columnIndex));
if (metadata.getColumnDefinition(columnId) == null)
     throw new org.apache.cassandra.exceptions.InvalidRequestException(String.format(""Invalid cell for CQL3 table %s. The CQL3 column component (%s) does not correspond to a defined CQL3 column"", metadata.cfName, columnId));
{code}","03/Nov/13 05:58;dbrosius;As an aside, is there a reason to allow a single component composite, as above:

CompositeType(UTF8Type)","03/Nov/13 09:24;doanduyhai;""As an aside, is there a reason to allow a single component composite""

 Yes, we want to be able to query with strict inequality on 'PRODUCT', they are ordered by name","05/Nov/13 16:33;slebresne;The problem is that there is no definitive marker for a ""CQL3 table"" (in hindsight, adding a new CQL3 ColumnFamilyType for CQL3 table would have made things a lot easier, but it's possibly a bit late now). So we ""guess"" more than anything else if a table is really a CQL3 one and in that case we're wrong.  More precisely, the reason the code thinks it's a CQL3 table is because it looks exactly like the table that would have been defined by: {noformat}
CREATE TABLE (
  key text,
  column1 text,
  PRIMARY KEY (key, column1)
)
{noformat}

Anyway, I guess the simpler fix here is to only do validation on the thrift side when we're sure that the table can't have been created from thrift. That will be good enough in almost all cases, the case of tables having only a PRIMARY KEY and no other columns being the exception. Simple patch attached.
",05/Nov/13 16:58;iamaleksey;+1,"05/Nov/13 18:18;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Node refuses to start with exception in ColumnFamilyStore.removeUnfinishedCompactionLeftovers when find that some to be removed files are already removed,CASSANDRA-6086,12670272,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thobbs,m0nstermind,m0nstermind,24/Sep/13 13:30,12/Mar/19 14:17,13/Mar/19 22:29,30/Dec/13 20:05,2.0.5,,,,,,2,,,,,"Node refuses to start with
{code}
Caused by: java.lang.IllegalStateException: Unfinished compactions reference missing sstables. This should never happen since compactions are marked finished before we start removing the old sstables.
      at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:544)
      at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:262)
{code}

IMO, there is no reason to refuse to start discivering files that must be removed are already removed. It looks like pure bug diagnostic code and mean nothing to operator (nor he can do anything about this).

Replaced throw of excepion with dump of diagnostic warning and continue startup.
",,,,,,,,,,,,,,,,,,,,,,,,19/Dec/13 23:58;thobbs;6086-2.0-v3.txt;https://issues.apache.org/jira/secure/attachment/12619699/6086-2.0-v3.txt,19/Nov/13 17:14;yukim;6086-v2.txt;https://issues.apache.org/jira/secure/attachment/12614653/6086-v2.txt,24/Sep/13 13:31;m0nstermind;removeUnfinishedCompactionLeftovers.txt;https://issues.apache.org/jira/secure/attachment/12604794/removeUnfinishedCompactionLeftovers.txt,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-09-24 15:31:46.891,,,no_permission,,,,,,,,,,,,350101,,,Mon Dec 30 20:05:28 UTC 2013,,,,,,0|i1od6n:,350395,,,,,,,,yukim,yukim,,,2.0.0,,,,,,,"24/Sep/13 15:31;yukim;The primary reason we have removeUnfinishedCompacitonLeftovers check is to not overcount counters stored.
If we do not stop at the original exception, then we will have both compacted SSTables and leftovers which may produce unexpected counter value.

Though we have the report that ""this happens""(CASSANDRA-6008), we should fix somehow.
Maybe we can consider compaction is finished even if we have entry in compactions_in_progress but some or all of the input SSTables are missing.","24/Sep/13 16:06;m0nstermind;Well, the common with CASSANDRA-6008 is that in my case node was dead before restart due to OOM. However, unlike 6008, CFs on which it reported this error was never truncated.","24/Sep/13 16:09;m0nstermind;From the other hand, the leftovers about to remove are already missing (i.e. already removed) when exception is thrown. I am not sure how can it  influence counter value.","08/Oct/13 17:21;yukim;bq. From the other hand, the leftovers about to remove are already missing (i.e. already removed) when exception is thrown.

Is this the case that, for example, you have SSTable 'C' that is produced from compacting SSTables 'A' and 'B'('C' has ancestors 'A' and 'B'), and 'A' and 'B' are already deleted, but removeUnfinishedCompactionLeftovers reports you have unfinished compaction of 'A' and 'B'?

In above case, you definitely don't want to proceed, since the code tries remove SSTable 'C' afterwards.
Patch v2 is attached to prevent deleting SSTable if its ancestors are already missing after printing warning.

(I'm still looking for the case why the above happened though. We supposed not to have entries in compaction_in_progress when those are removed.)
","17/Oct/13 15:04;cowardlydragon;Well, how would one repair this for now, beyond reconstructing a virgin node and then replication? If there are counters, certainly you could provide an option for an admin to reset them or accept invalid values and get access to the rest of the data, or mark it in some (tombstone-ish?) way that other nodes with more accurate values for the counters can then replicate the correct state?

And yes, we just got this.","17/Oct/13 15:08;ngrigoriev;@Constance,

I was able to repair my node - see CASSANDRA-6008. Used the suggestion I received + had to add some flavor to it :)","08/Nov/13 22:26;jre;Hi, we are able to consistently reproduce this issue:
{noformat}
ERROR 23:14:06,001 Exception encountered during startup
java.lang.IllegalStateException: Unfinished compactions reference missing sstables. This should never happen since compactions are marked finished before we start removing the old sstables.
       at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:489)
       at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:264)
       at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:461)
       at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:504)
java.lang.IllegalStateException: Unfinished compactions reference missing sstables. This should never happen since compactions are marked finished before we start removing the old sstables.
       at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:489)
       at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:264)
       at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:461)
       at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:504)
Exception encountered during startup: Unfinished compactions reference missing sstables. This should never happen since compactions are marked finished before we start removing the old sstables.
{noformat}

Here are the two ways in which we have found to reproduce this issue:
# Buffer a large amount of CL.LOCAL_QUORUM writes into a Cassandra CF, then drop the CF while the writes are still buffered.
# Buffer a large amount of CL.LOCAL_QUORUM writes into a Cassandra CF, then restart some nodes while before the buffer has finished draining.","19/Nov/13 17:14;yukim;Rebased patch against cassandra-2.0 attached.

Again, the patch changes log ERROR and stop starting C* to print WARN but prevent deleting SSTable if its ancestors are already missing.","11/Dec/13 20:21;thobbs;[~yukim] the patch looks pretty good.  Can you add a test like {{ColumnFamilyStoreTest.testRemoveUnifinishedCompactionLeftovers()}} to cover this?  Also, warning logs should either suggest an action or let the user know that no action is required; in this case, we should tell the user to report the issue but that no corrective action is needed.","17/Dec/13 20:23;yukim;[~thobbs] Added test and slightly changed message: https://github.com/yukim/cassandra/commits/6086

I'm not good at wording, so suggestion is welcome.","19/Dec/13 23:58;thobbs;The v3 patch (and [branch|https://github.com/thobbs/cassandra/tree/6086]) builds on [~yukim]'s v2 patch and adds incremental deletions of entries in {{compactions_in_progress}} as discussed in CASSANDRA-6008.  Additionally, this move the warning log to debug level, since there's not anything the user can do or should do.","30/Dec/13 20:05;yukim;Thanks Tyler, committed.
(I removed LegacyLeveledManifest related change in trunk since it no longer exists.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AE in hinted handoff delivery,CASSANDRA-6165,12672895,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,08/Oct/13 20:28,12/Mar/19 14:17,13/Mar/19 22:29,08/Oct/13 22:29,2.0.2,,,,,,0,,,,,"I suspect our old friend CASSANDRA-6132 is related to this as well:

{noformat}
ERROR [HintedHandoff:2] 2013-10-08 20:14:39,976 Caller+0     at org.apache.cassandra.service.CassandraDaemon$2.uncaughtException(CassandraDaemon.java:134)
 - Exception in thread Thread[HintedHandoff:2,1,main]
java.lang.AssertionError: null
    at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:543) ~[main/:na]
    at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:594) ~[main/:na]
    at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:573) ~[main/:na]
    at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:568) ~[main/:na]
    at org.apache.cassandra.db.HintedHandOffManager.doDeliverHintsToEndpoint(HintedHandOffManager.java:433) ~[main/:na]
    at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:304) ~[main/:na]
    at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:92) ~[main/:na]
    at org.apache.cassandra.db.HintedHandOffManager$4.run(HintedHandOffManager.java:525) ~[main/:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_17]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_17]
    at java.lang.Thread.run(Thread.java:722) ~[na:1.7.0_17]
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,08/Oct/13 21:27;jbellis;6165.txt;https://issues.apache.org/jira/secure/attachment/12607440/6165.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-08 21:27:02.293,,,no_permission,,,,,,,,,,,,352518,,,Tue Oct 08 22:30:23 UTC 2013,,,,,,0|i1os0n:,352805,,,,,,,,brandon.williams,brandon.williams,,,2.1 rc3,,,,,,,08/Oct/13 21:27;jbellis;Patch (against 2.0),08/Oct/13 22:14;brandon.williams;+1,08/Oct/13 22:30;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra does not start on Ubuntu 13.04 RUssian,CASSANDRA-6162,12672840,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,dbrosius,nsv,nsv,08/Oct/13 16:12,12/Mar/19 14:17,13/Mar/19 22:29,08/May/14 04:41,2.0.9,2.1 rc1,,Packaging,,,0,,,,,"Output just after install:

vm.max_map_count = 1048575
expr: синтаксическая ошибка
^^^^ RU: syntax error
expr: синтаксическая ошибка
^^^^ RU: syntax error
/etc/init.d/cassandra: 59: [: Illegal number: 
/etc/init.d/cassandra: 63: [: Illegal number: 
/etc/init.d/cassandra: 67: [: Illegal number: 
expr: синтаксическая ошибка
^^^^ RU: syntax error
/etc/init.d/cassandra: 81: [: Illegal number: 
xss =  -ea -javaagent:/usr/share/cassandra/lib/jamm-0.2.5.jar -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -XmsM -XmxM -XmnM -XX:+HeapDumpOnOutOfMemoryError -Xss256k","Ubuntu 13.04, Russian (!) ,java version ""1.7.0_40""",,,,,,,,,,,,,,,,,,,,,,,08/May/14 03:11;dbrosius;6162.txt;https://issues.apache.org/jira/secure/attachment/12643894/6162.txt,08/Oct/13 16:17;nsv;CASSANDRA-6162.patch;https://issues.apache.org/jira/secure/attachment/12607375/CASSANDRA-6162.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-01-23 15:42:13.639,,,no_permission,,,,,,,,,,,,352463,,,Thu May 08 04:41:20 UTC 2014,,,,,,0|i1orof:,352750,2.0.1,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"08/Oct/13 16:17;nsv;Uncommenting lines

#MAX_HEAP_SIZE=""4G""
#HEAP_NEWSIZE=""800M""

in /etc/cassandra/cassandra-env.sh makes it run.

This solution seems to be not right, the actual problem looks like parsing of localized output of system utilities during environment discovery.","23/Jan/14 15:42;andreas.petter;confirmed, this is the case for German version, too. Installed 2.0.4 directly from the deb-package from the Apache site.

The problem lies in line 22 of cassandra-env.sh, in the awk expression which expects the result of  free -m to contain ""Mem:"" which is not the case for localized ubuntu versions.
Maybe this can be fixed (at least it works for german localization, but i suspect it is generic enough to work for others, too) by searching for "":"" only and returning only the first line with grep, i.e.: 
system_memory_in_mb=`free -m | awk '/:/ {print $2}' | grep -i """" --max-count=1`

However, i'm not a shell scripter and so this must be tested elsewhere, too. Sergey, can you test it for the russian version, please?","08/May/14 03:11;dbrosius;fix for linux... similar fix can be applied to other os's if folks can provide the output of the other commands for each os.

6162.txt against 2.0",08/May/14 03:20;brandon.williams;Why the 'exit' in awk?,08/May/14 03:36;dbrosius;so you only print the first line,08/May/14 04:20;brandon.williams;Ah. +1,08/May/14 04:41;dbrosius;committed to cassandra-2.0 as commit 16fd1a4a89958595ca2ae44fdac2eb7aa1ad6be2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in CqlRecordWriter: Related to AbstractCassandraStorage handling null values,CASSANDRA-6180,12673418,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,alexliu68,hkropp,hkropp,11/Oct/13 11:56,12/Mar/19 14:17,13/Mar/19 22:29,25/Oct/13 16:13,1.2.12,2.0.3,,,,,0,,,,,"I encountered an issue with the {{CqlStorage}} and it's handling of null values. The {{CqlRecordWriter}} throws an NPE when a value is null. I found a related ticket CASSANDRA-5885 and applied the there stated fix to the {{AbstractCassandraStorage}}.
Instead of converting {{null}} values to {{ByteBuffer.wrap(new byte[0])}} {{AbstractCassandraStorage}} returns {{(ByteBuffer)null}}

This issue can be reproduced with the attached files: {{test_null.cql}}, {{test_null_data}}, {{null_test.pig}}

A fix can be found in the attached patch.

{code}
java.io.IOException: java.lang.NullPointerException
	at org.apache.cassandra.hadoop.cql3.CqlRecordWriter$RangeClient.run(CqlRecordWriter.java:248)
Caused by: java.lang.NullPointerException
	at org.apache.thrift.protocol.TBinaryProtocol.writeBinary(TBinaryProtocol.java:194)
	at org.apache.cassandra.thrift.Cassandra$execute_prepared_cql3_query_args.write(Cassandra.java:41253)
	at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:63)
	at org.apache.cassandra.thrift.Cassandra$Client.send_execute_prepared_cql3_query(Cassandra.java:1683)
	at org.apache.cassandra.thrift.Cassandra$Client.execute_prepared_cql3_query(Cassandra.java:1673)
	at org.apache.cassandra.hadoop.cql3.CqlRecordWriter$RangeClient.run(CqlRecordWriter.java:232)
{code}","Pig, CqlStorage",,,,,,,,,,,,,,,,,,,,,,,23/Oct/13 08:24;alexliu68;6180-v2-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12609824/6180-v2-1.2-branch.txt,11/Oct/13 11:57;hkropp;null_test.pig;https://issues.apache.org/jira/secure/attachment/12607982/null_test.pig,11/Oct/13 11:57;hkropp;patch.txt;https://issues.apache.org/jira/secure/attachment/12607985/patch.txt,11/Oct/13 11:57;hkropp;test_null.cql;https://issues.apache.org/jira/secure/attachment/12607983/test_null.cql,11/Oct/13 11:57;hkropp;test_null_data;https://issues.apache.org/jira/secure/attachment/12607984/test_null_data,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2013-10-13 16:57:46.327,,,no_permission,,,,,,,,,,,,353041,,,Fri Oct 25 16:13:34 UTC 2013,,,,,,0|i1ov87:,353328,1.2.10,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,11/Oct/13 11:57;hkropp;Files to reproduce this issue and a patch.,"13/Oct/13 16:57;jbellis;I don't think this is semantically correct.  ""UPDATE foo SET x = null"" means ""set x to a tombstone,"" which is not the same as setting it to an empty byte[].

/cc [~alexliu68]","14/Oct/13 05:36;alexliu68;Thrift API TBinaryProtocol.writeBinary method 

{code}
    public void writeBinary(ByteBuffer buffer) throws TException
    {
        writeI32(buffer.remaining());

        if (buffer.hasArray())
        {
            trans_.write(buffer.array(), buffer.position() + buffer.arrayOffset(), buffer.remaining());
        }
        else
        {
            byte[] bytes = new byte[buffer.remaining()];

            int j = 0;
            for (int i = buffer.position(); i < buffer.limit(); i++)
            {
                bytes[j++] = buffer.get(i);
            }

            trans_.write(bytes);
        }
    }
{code}

throws NPE if the variable is null instead of a BufferBuffer with empty byte. It looks like a bug for thrift execute_prepared_cql3_query which can't handle a null variable.

CASSANDRA-5081 may only work for binary protocol.","14/Oct/13 05:55;alexliu68;If we do need use  ByteBuffer.wrap(new byte[0]) to fix the issue, this should only apply to CqlStorage, CassandraStorage should stay the old way.","14/Oct/13 09:04;hkropp;I agree that {{byte[]}} is not semantically equivalent to {{null}}. The representation of {{null}} in Cassandra seems to me like an issue on it's own.

How about limiting it to {{CqlStorage}} and making it configurable? So the user can deliberately change the representation of {{null}}/empty values.

At least I can query easily for {{byte[]}} but not for {{null}}.
{code}
> CREATE INDEX null_idx ON null_table(null_value);
> DELETE null_value FROM null_table WHERE null_id = 'a';
> SELECT COUNT(*) FROM null_table WHERE null_value = '';
 count
-------
     6 
> SELECT * FROM null_table WHERE null_value = null;
Bad Request: Unsupported null value for indexed column null_value
{code}","23/Oct/13 08:25;alexliu68;6180-v2-1.2-branch.txt is attached to set it to empty byte array for CqlStorage, keep it null for CassandraStorage",25/Oct/13 16:13;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to modify column_metadata via thrift,CASSANDRA-6182,12673463,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,nickmbailey,nickmbailey,11/Oct/13 17:08,12/Mar/19 14:17,13/Mar/19 22:29,25/Oct/13 17:11,2.0.3,,,,,,0,,,,,"Reproduced on 2.0 HEAD

{noformat}
[default@unknown] use opscenter;
Authenticated to keyspace: OpsCenter
[default@OpsCenter] create column family test with column_metadata = [{column_name: '1111', validation_class: LongType}];
637fffa1-a10f-3d89-8be6-8a316af05dd2
[default@OpsCenter] update column family test with column_metadata=[];
e49e435b-ba2a-3a08-8af0-32b897b872b8
[default@OpsCenter] show schema;

<other entries removed>

create column family test
  with column_type = 'Standard'
  and comparator = 'BytesType'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'BytesType'
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and populate_io_cache_on_flush = false
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'
  and caching = 'KEYS_ONLY'
  and default_time_to_live = 0
  and speculative_retry = 'NONE'
  and column_metadata = [
    {column_name : '1111',
    validation_class : LongType}]
  and compression_options = {'sstable_compression' : 'org.apache.cassandra.io.compress.LZ4Compressor'}
  and index_interval = 128;
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,21/Oct/13 14:23;slebresne;6182.txt;https://issues.apache.org/jira/secure/attachment/12609434/6182.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-21 14:23:10.341,,,no_permission,,,,,,,,,,,,353086,,,Fri Oct 25 17:11:54 UTC 2013,,,,,,0|i1ovi7:,353373,2.0.1,,,,,,,jbellis,jbellis,,,,,,,,,,21/Oct/13 14:23;slebresne;Attaching patch. This is a regression from CASSANDRA-5579 for column definitions where the comparator is not UTF8Type (as is the case in this example).,25/Oct/13 00:42;jbellis;+1,"25/Oct/13 17:11;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get java.util.ConcurrentModificationException while bulkloading from sstable for widerow table,CASSANDRA-6129,12671715,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,ksaritek,ksaritek,02/Oct/13 06:12,12/Mar/19 14:17,13/Mar/19 22:29,05/Oct/13 00:43,2.0.2,,,Legacy/Tools,,,0,,,,,"I haven't faced that problem with cassandra 1.2.6

I have created widerow sstables with SSTableSimpleUnsortedWriter. When i tried to load sstables by sstableloader, I got java.util.ConcurrentModificationException after a while (not at the beggining of the streaming).

Exception is :
progress: [/192.168.103.5 0/39 (0%)] [/192.168.103.3 0/39 (0%)] [/192.168.103.1 0/39 (0%)] [total: 0% - 15MB/s (avg: 0MB/s)] INFO 00:45:23,542 [Stream #c0f53e00-2ae2-11e3-ab6b-99a3e9e32246] Session with /192.168.103.3 is complete
progress: [/192.168.103.5 0/39 (0%)] [/192.168.103.3 0/39 (0%)] [/192.168.103.1 0/39 (0%)] [total: 0% - 3MB/s (avg: 1MB/s)]Exception in thread ""STREAM-OUT-/192.168.103.3"" java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:894)
	at java.util.HashMap$EntryIterator.next(HashMap.java:934)
	at java.util.HashMap$EntryIterator.next(HashMap.java:932)
	at org.apache.cassandra.tools.BulkLoader$ProgressIndicator.handleStreamEvent(BulkLoader.java:129)
	at org.apache.cassandra.streaming.StreamResultFuture.fireStreamEvent(StreamResultFuture.java:198)
	at org.apache.cassandra.streaming.StreamResultFuture.handleProgress(StreamResultFuture.java:191)
	at org.apache.cassandra.streaming.StreamSession.progress(StreamSession.java:474)
	at org.apache.cassandra.streaming.StreamWriter.write(StreamWriter.java:105)
	at org.apache.cassandra.streaming.messages.FileMessage$1.serialize(FileMessage.java:73)
	at org.apache.cassandra.streaming.messages.FileMessage$1.serialize(FileMessage.java:45)
	at org.apache.cassandra.streaming.messages.StreamMessage.serialize(StreamMessage.java:44)
	at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.sendMessage(ConnectionHandler.java:384)
	at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.run(ConnectionHandler.java:357)
	at java.lang.Thread.run(Thread.java:781)
progress: [/192.168.103.5 0/39 (3%)] [/192.168.103.3 0/39 (0%)] [/192.168.103.1 0/39 (2%)] [total: 1% - 2147483647MB/s (avg: 12MB/s)]Exception in thread ""STREAM-OUT-/192.168.103.1"" java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:894)
	at java.util.HashMap$KeyIterator.next(HashMap.java:928)
progress: [/192.168.103.5 0/39 (3%)] [/192.168.103.3 0/39 (0%)] [/192.168.103.1 0/39 (2%)] [total: 1% - 2147483647MB/s (avg: 12MB/s)]
	at org.apache.cassandra.streaming.StreamResultFuture.fireStreamEvent(StreamResultFuture.java:198)
	at org.apache.cassandra.streaming.StreamResultFuture.handleProgress(StreamResultFuture.java:191)
	at org.apache.cassandra.streaming.StreamSession.progress(StreamSession.java:474)
	at org.apache.cassandra.streaming.StreamWriter.write(StreamWriter.java:105)
	at org.apache.cassandra.streaming.messages.FileMessage$1.serialize(FileMessage.java:73)
	at org.apache.cassandra.streaming.messages.FileMessage$1.serialize(FileMessage.java:45)
	at org.apache.cassandra.streaming.messages.StreamMessage.serialize(StreamMessage.java:44)
	at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.sendMessage(ConnectionHandler.java:384)
	at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.run(ConnectionHandler.java:357)
	at java.lang.Thread.run(Thread.java:781)
","three cassandra 2.0.1 node
jdk 7
linux - ubuntu",,,,,,,,,,,,,,,,,,,,,,,04/Oct/13 21:03;jbellis;6129-CLQ.txt;https://issues.apache.org/jira/secure/attachment/12606890/6129-CLQ.txt,03/Oct/13 12:37;ksaritek;BulkLoader.diff;https://issues.apache.org/jira/secure/attachment/12606577/BulkLoader.diff,04/Oct/13 17:17;mishail;trunk-6129-synch-iter.patch;https://issues.apache.org/jira/secure/attachment/12606829/trunk-6129-synch-iter.patch,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-10-02 17:43:13.325,,,no_permission,,,,,,,,,,,,351425,,,Sat Oct 05 00:43:05 UTC 2013,,,,,,0|i1olb3:,351714,2.0.0,2.0.1,,,,,,mishail,mishail,,,2.0.0,,,,,,ksaritek,02/Oct/13 17:43;jbellis;Converted the map in question to CHM.  Thanks for the report!,03/Oct/13 05:23;ksaritek;Is there a fix for that? What is CHM stand for? ,"03/Oct/13 05:43;ksaritek;Got it, have a looked commits at git. Convert from hashmap to concurrenthashmap.

Thanks :)","03/Oct/13 10:26;ksaritek;I have tested against 2.0.1 and got the problem. Seems that concurrent set is needed for progressByHost. 

I made some more changes and attached git diff file (BulkLoader.diff for BulkLoader.java)

wait for your comment.

Thanks
Koray ",03/Oct/13 14:31;jbellis;done in 6ca9b4842942db6ff7a978f1054bb619f07a60ad,"04/Oct/13 09:05;ksaritek;At BulkLoad process, get exception from StreamFuture eventListeners list as:
INFO 17:27:46,987 [Stream #f92ac500-2c37-11e3-ad9c-99a3e9e32246] Prepare completed. Receiving 0 files(0 bytes), sending 1 files(115 bytes)
ERROR 17:27:47,005 Error in ThreadPoolExecutor
java.util.ConcurrentModificationException
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:819)
	at java.util.ArrayList$Itr.next(ArrayList.java:791)
	at org.apache.cassandra.streaming.StreamResultFuture.fireStreamEvent(StreamResultFuture.java:197)
	at org.apache.cassandra.streaming.StreamResultFuture.handleSessionPrepared(StreamResultFuture.java:175)
	at org.apache.cassandra.streaming.StreamSession.startStreamingFiles(StreamSession.java:620)
	at org.apache.cassandra.streaming.StreamSession.onInitializationComplete(StreamSession.java:400)
	at org.apache.cassandra.streaming.StreamSession$1.run(StreamSession.java:200)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:781)
ERROR 17:27:47,005 Error in ThreadPoolExecutor
java.util.ConcurrentModificationException
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:819)
	at java.util.ArrayList$Itr.next(ArrayList.java:791)
	at org.apache.cassandra.streaming.StreamResultFuture.fireStreamEvent(StreamResultFuture.java:197)
	at org.apache.cassandra.streaming.StreamResultFuture.handleSessionPrepared(StreamResultFuture.java:175)
	at org.apache.cassandra.streaming.StreamSession.startStreamingFiles(StreamSession.java:620)
	at org.apache.cassandra.streaming.StreamSession.onInitializationComplete(StreamSession.java:400)
	at org.apache.cassandra.streaming.StreamSession$1.run(StreamSession.java:200)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:781)
Exception in thread ""StreamConnectionEstablisher:1"" Exception in thread ""StreamConnectionEstablisher:2"" java.util.ConcurrentModificationExceptionjava.util.ConcurrentModificationException

	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:819)	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:819)

	at java.util.ArrayList$Itr.next(ArrayList.java:791)	at java.util.ArrayList$Itr.next(ArrayList.java:791)

	at org.apache.cassandra.streaming.StreamResultFuture.fireStreamEvent(StreamResultFuture.java:197)	at org.apache.cassandra.streaming.StreamResultFuture.fireStreamEvent(StreamResultFuture.java:197)

	at org.apache.cassandra.streaming.StreamResultFuture.handleSessionPrepared(StreamResultFuture.java:175)	at org.apache.cassandra.streaming.StreamResultFuture.handleSessionPrepared(StreamResultFuture.java:175)

	at org.apache.cassandra.streaming.StreamSession.startStreamingFiles(StreamSession.java:620)	at org.apache.cassandra.streaming.StreamSession.startStreamingFiles(StreamSession.java:620)

	at org.apache.cassandra.streaming.StreamSession.onInitializationComplete(StreamSession.java:400)	at org.apache.cassandra.streaming.StreamSession.onInitializationComplete(StreamSession.java:400)

	at org.apache.cassandra.streaming.StreamSession$1.run(StreamSession.java:200)	at org.apache.cassandra.streaming.StreamSession$1.run(StreamSession.java:200)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:781)	at java.lang.Thread.run(Thread.java:781)

Terminated",04/Oct/13 17:17;mishail;Attached a patch. As per JavaDoc: {{It is imperative that the user manually synchronize on the returned list when iterating over it}},"04/Oct/13 21:03;jbellis;IMO, simpler to switch to CLQ since we don't actually need List API; attached.  WDYT?",04/Oct/13 22:55;mishail;+1,05/Oct/13 00:43;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sets not stored by INSERT with IF NOT EXISTS,CASSANDRA-6069,12669574,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,urandom,urandom,19/Sep/13 22:39,12/Mar/19 14:17,13/Mar/19 22:29,23/Sep/13 14:42,2.0.2,,,,,,0,LWT,,,,An {{INSERT}} of a {{set}} column type is not stored when using {{IF NOT EXISTS}},,,,,,,,,,,,,,,,,,,,,,,,23/Sep/13 13:25;slebresne;6069.txt;https://issues.apache.org/jira/secure/attachment/12604589/6069.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-23 13:25:48.361,,,no_permission,,,,,,,,,,,,349506,,,Mon Sep 23 14:42:51 UTC 2013,,,,,,0|i1o9jb:,349804,2.0.0,,,,,,,iamaleksey,iamaleksey,,,2.0.0,,,,,,,"23/Sep/13 13:25;slebresne;This is a bit annoying. When we insert a new collection value, we also insert a range tombstone to remove the potential previous value. That range tombstone has a tombstone of t-1 (where t is the timestamp of the actual insert) so it doesn't conflict with the new inserts. But a CAS write rewrites all the timestamps to make sure they match the order decided by Paxos, which in this case mean the range tombstone ends up removing the data we're inserting.

Attaching a patch that implements a simple solution by making paxos use t-1 for row and range tombstones when rewriting the operation timestamp. In general this should be ok since tombstones wins over normal inserts so the tombstone will still always delete anything that has a timestamp < t. It does mean however that if someone does a row/range deletion and inserts in the same CAS operation, the deletions won't win contrarly to what happens with other operations. That being said:
# deleting something you are just inserting is a bit of an anti-social thing to do.
# I don't think users can actually do it today with CAS because we don't allow batches.
So feels like a reasonable fix.

The other solution would be to special case the paxos code for that specific CQL3 collections case, but that's going to be a tad more painful.
",23/Sep/13 13:55;iamaleksey;+1,"23/Sep/13 14:42;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AE while saving cache,CASSANDRA-6208,12674161,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,beobal,brandon.williams,brandon.williams,16/Oct/13 17:38,12/Mar/19 14:17,13/Mar/19 22:29,16/Oct/13 18:47,1.2.11,,,,,,0,,,,,"After running stress with a 2i and leaving the node idle for a while, I receive this trace when the cache tries to autosave:

{noformat}
ERROR 16:07:05,499 Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.AssertionError
        at org.apache.cassandra.config.CFMetaData.<init>(CFMetaData.java:314)
        at org.apache.cassandra.config.CFMetaData.<init>(CFMetaData.java:307)
        at org.apache.cassandra.cache.AutoSavingCache$Writer.<init>(AutoSavingCache.java:213)
        at org.apache.cassandra.cache.AutoSavingCache.getWriter(AutoSavingCache.java:74)
        at org.apache.cassandra.cache.AutoSavingCache.submitWrite(AutoSavingCache.java:176)
        at org.apache.cassandra.cache.AutoSavingCache$1.run(AutoSavingCache.java:90)
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:75)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
{noformat}

CASSANDRA-5732 appears responsible.",,,,,,,,,,,,,,,,,,,,,,,,16/Oct/13 18:43;beobal;0001-CASSANDRA-6208-Removed-new-asserts-from-5732-in-CFMD.patch;https://issues.apache.org/jira/secure/attachment/12608774/0001-CASSANDRA-6208-Removed-new-asserts-from-5732-in-CFMD.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-16 18:43:19.9,,,no_permission,,,,,,,,,,,,353783,,,Wed Oct 16 18:47:45 UTC 2013,,,,,,0|i1oztr:,354075,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,16/Oct/13 17:40;brandon.williams;This affects the current tentative release of 1.2.11,"16/Oct/13 18:43;beobal;Looks like a couple of the new asserts in CFMD constructor will cause this. They weren't essential to CASSANDRA-5732 anyway, so I've removed all of them just in case there are other cases we've not seen yet.
","16/Oct/13 18:47;brandon.williams;Works for me, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstableloader broken in 2.0 HEAD,CASSANDRA-6205,12673988,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thobbs,nickmbailey,nickmbailey,15/Oct/13 21:12,12/Mar/19 14:17,13/Mar/19 22:29,22/Oct/13 15:44,2.0.2,,,,,,0,,,,,"The code for tracking sstable coldness is also executing when running sstableloader which causes problems.

{noformat}
Exception in thread ""main"" java.lang.RuntimeException: Error validating SELECT * FROM sstable_activity WHERE keyspace_name='test_backup_restore' and columnfamily_name='cf0' and generation=1
at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:190)
at org.apache.cassandra.db.SystemKeyspace.getSSTableReadMeter(SystemKeyspace.java:907)
at org.apache.cassandra.io.sstable.SSTableReader.<init>(SSTableReader.java:337)
at org.apache.cassandra.io.sstable.SSTableReader.openForBatch(SSTableReader.java:160)
at org.apache.cassandra.io.sstable.SSTableLoader$1.accept(SSTableLoader.java:112)
at java.io.File.list(File.java:1087)
at org.apache.cassandra.io.sstable.SSTableLoader.openSSTables(SSTableLoader.java:73)
at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:155)
at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:68)
Caused by: org.apache.cassandra.db.KeyspaceNotDefinedException: Keyspace system does not exist
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,17/Oct/13 22:08;thobbs;6205.patch;https://issues.apache.org/jira/secure/attachment/12609035/6205.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-16 06:50:31.258,,,no_permission,,,,,,,,,,,,353611,,,Tue Oct 22 15:44:03 UTC 2013,,,,,,0|i1oyrz:,353903,,,,,,,,slebresne,slebresne,,,,,,,,,,"16/Oct/13 06:50;slebresne;For what that's worth, the patch at CASSANDRA-5894 fixes that too.","17/Oct/13 19:22;thobbs;I haven't verified it, but I think the patch for CASSANDRA-5894 won't fix this because it only avoids opening an SSTR after closing the writer.","17/Oct/13 22:08;thobbs;I verified that CASSANDRA-5894 doesn't fix this.

Attached patch 6205.patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-6205]) skips the read meter setup when in client mode.

I'm also looking into why the dtest that uses sstableloader didn't catch this.","18/Oct/13 16:23;thobbs;sstableloader was indeed failing on the dtest, but the failure was being ignored by the test.  I've opened a [pull request|https://github.com/riptano/cassandra-dtest/pull/23] to fix the dtest once this is committed.",21/Oct/13 09:27;slebresne;+1,"22/Oct/13 15:44;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair hangs when a new datacenter is added to a cluster,CASSANDRA-6210,12674215,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,rspitzer,rspitzer,16/Oct/13 23:01,12/Mar/19 14:17,13/Mar/19 22:29,28/Jan/14 15:21,2.0.5,,,,,,0,,,,,"Attempting to add a new datacenter to a cluster seems to cause repair operations to break. I've been reproducing this with 20~ node clusters but can get it to reliably occur on 2 node setups.

{code}
##Basic Steps to reproduce
#Node 1 is started using GossipingPropertyFileSnitch as dc1
#Cassandra-stress is used to insert a minimal amount of data
$CASSANDRA_STRESS -t 100 -R org.apache.cassandra.locator.NetworkTopologyStrategy  --num-keys=1000 --columns=10 --consistency-level=LOCAL_QUORUM --average-size-values -
-compaction-strategy='LeveledCompactionStrategy' -O dc1:1 --operation=COUNTER_ADD
#Alter ""Keyspace1""
ALTER KEYSPACE ""Keyspace1"" WITH replication = {'class': 'NetworkTopologyStrategy', 'dc1': 1 , 'dc2': 1 };
#Add node 2 using GossipingPropertyFileSnitch as dc2
run repair on node 1
run repair on node 2
{code}

The repair task on node 1 never completes and while there are no exceptions in the logs of node1, netstat reports the following repair tasks
{code}
Mode: NORMAL
Repair 4e71a250-36b4-11e3-bedc-1d1bb5c9abab
Repair 6c64ded0-36b4-11e3-bedc-1d1bb5c9abab
Read Repair Statistics:
Attempted: 0
Mismatch (Blocking): 0
Mismatch (Background): 0
Pool Name                    Active   Pending      Completed
Commands                        n/a         0          10239
Responses                       n/a         0           3839
{code}

Checking on node 2 we see the following exceptions
{code}
ERROR [STREAM-IN-/10.171.122.130] 2013-10-16 22:42:58,961 StreamSession.java (line 410) [Stream #4e71a250-36b4-11e3-bedc-1d1bb5c9abab] Streaming error occurred
java.lang.NullPointerException
        at org.apache.cassandra.streaming.ConnectionHandler.sendMessage(ConnectionHandler.java:174)
        at org.apache.cassandra.streaming.StreamSession.prepare(StreamSession.java:436)
        at org.apache.cassandra.streaming.StreamSession.messageReceived(StreamSession.java:358)
        at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:293)
        at java.lang.Thread.run(Thread.java:724)
...
ERROR [STREAM-IN-/10.171.122.130] 2013-10-16 22:43:49,214 StreamSession.java (line 410) [Stream #6c64ded0-36b4-11e3-bedc-1d1bb5c9abab] Streaming error occurred
java.lang.NullPointerException
        at org.apache.cassandra.streaming.ConnectionHandler.sendMessage(ConnectionHandler.java:174)
        at org.apache.cassandra.streaming.StreamSession.prepare(StreamSession.java:436)
        at org.apache.cassandra.streaming.StreamSession.messageReceived(StreamSession.java:358)
        at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:293)
        at java.lang.Thread.run(Thread.java:724)
{code}

Netstats on node 2 reports
{code}
automaton@ip-10-171-15-234:~$ nodetool netstats
Mode: NORMAL
Repair 4e71a250-36b4-11e3-bedc-1d1bb5c9abab
Read Repair Statistics:
Attempted: 0
Mismatch (Blocking): 0
Mismatch (Background): 0
Pool Name                    Active   Pending      Completed
Commands                        n/a         0           2562
Responses                       n/a         0           4284

{code}
","Amazon Ec2
2 M1.large nodes",,,,,,,,,,,,,,,,,,,,,,,17/Jan/14 22:28;yukim;6210-2.0.txt;https://issues.apache.org/jira/secure/attachment/12623737/6210-2.0.txt,19/Dec/13 16:58;rspitzer;RepairLogs.tar.gz;https://issues.apache.org/jira/secure/attachment/12619601/RepairLogs.tar.gz,17/Jan/14 18:00;rspitzer;patch_1_logs.tar.gz;https://issues.apache.org/jira/secure/attachment/12623694/patch_1_logs.tar.gz,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-10-17 08:53:53.046,,,no_permission,,,,,,,,,,,,353837,,,Tue Jan 28 15:21:29 UTC 2014,,,,,,0|i1p053:,354129,2.0.1,,,,,,,rspitzer,rspitzer,,,2.0.0,,,,,,,"17/Oct/13 08:53;jbellis;Is this just ""I can't repair until bootstrapping is finished?""","17/Oct/13 17:13;rspitzer;I see this effect with both auto_bootstrap: false (in cassandra.yaml). Although if it is just a bootstrapping issue, I think we should have a better failure mode.  I'll try doing the keyspace alteration after adding the new node and see if that fixes things.","17/Oct/13 17:16;rspitzer;Double checked that auto_bootstrap was false on both nodes. Re ran test

On node 1 (netstats)
{code}
Mode: NORMAL
Repair e50825a0-374e-11e3-854d-f97c22942f02
Read Repair Statistics:
Attempted: 0
Mismatch (Blocking): 0
Mismatch (Background): 0
Pool Name                    Active   Pending      Completed
Commands                        n/a         0           2562
Responses                       n/a         0           1284
{code}

On node 2 (System.log)
{code}
ERROR [STREAM-IN-/10.171.10.24] 2013-10-17 17:09:34,102 StreamSession.java (line 410) [Stream #e50825a0-374e-11e3-854d-f97c22942f02] Streaming error occurred
java.lang.NullPointerException
        at org.apache.cassandra.streaming.ConnectionHandler.sendMessage(ConnectionHandler.java:174)
        at org.apache.cassandra.streaming.StreamSession.prepare(StreamSession.java:436)
        at org.apache.cassandra.streaming.StreamSession.messageReceived(StreamSession.java:358)
        at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:293)
        at java.lang.Thread.run(Thread.java:724){code}","17/Oct/13 17:28;rspitzer;-Moving the alter of the keyspace to after bringing up the second Node fixes this.-

I spoke to soon it failed on the second trial.","17/Oct/13 19:03;brandon.williams;Can't repro on the 2.0 branch with these steps.

bq. Moving the alter of the keyspace to after bringing up the second Node fixes this.

That's because there's nothing to repair without the second replica defined.",17/Oct/13 19:39;rspitzer;If I execute nodetool rebuild before doing the repair then repair completes successfully (because it doesn't have to stream anything?) ,17/Oct/13 19:54;rspitzer;The same test works on 1.2.10 (DSE w/o vnodes),"17/Oct/13 20:17;rspitzer;[~brandon.williams], I moved the alter statement too after bringing up the second node, but not after running repair. So it still has data to repair.

So this sequence still has a bug
{code}
Node 1 up dc1
Stress
Node 2 up dc2
Alter keyspace
repair on node1
{code}

But this sequence seems to work fine
{code}
node 1 up
stress
node 2 up
Alter Keyspace
Nodetool rebuild on node 2
Repair
{code}",17/Oct/13 20:36;brandon.williams;Can you try 2.0 HEAD?,"17/Oct/13 21:12;rspitzer;Same error on 2.0 Head, when I try to repair without issuing the rebuild.

Last commit
{code}
commit 886f16ca1c14f1209151579408b59cd699832cbf
Author: Brandon Williams <brandonwilliams@apache.org>
Date:   Thu Oct 17 13:37:04 2013 -0500
{code}




",17/Oct/13 22:16;rspitzer;The problem doesn't manifest when i'm not using vnodes on 2.0-HEAD,"17/Dec/13 21:10;yukim;I followed the steps bellow:

{code}
Node 1 up dc1
Stress
Node 2 up dc2
Alter keyspace
repair on node1
{code}

And with auto_bootstrap: false, I got the following and repair hung:

{code}
ERROR [AntiEntropyStage:1] 2013-12-17 15:03:08,945 CassandraDaemon.java (line 187) Exception in thread Thread[AntiEntropyStage:1,5,main]
java.lang.AssertionError: Unknown keyspace Keyspace1
        at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:262)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:110)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:88)
        at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:46)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:60)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
{code}

We should 'catch-all' in RepairVerbHandler to prevent hang at least.
It was not the same exception.

[~rspitzer], can you reproduce with 'log4j.logger.org.apache.cassandra.streaming=DEBUG' in your log4j-server.properties and attach the log here?","18/Dec/13 05:26;rspitzer;Sure, It was a while back so things have most likely changed in the interim. I'll get the test running tomorrow with the DEBUG statements turned on. ","18/Dec/13 23:36;rspitzer;Repair running on this node
{code}
 INFO [AntiEntropyStage:1] 2013-12-18 22:39:28,209 StreamResultFuture.java (line 82) [Stream #40d875d0-6835-11e3-a172-3729e500a0e7] Executing streaming plan for Repair
 INFO [AntiEntropyStage:1] 2013-12-18 22:39:28,209 StreamResultFuture.java (line 86) [Stream #40d875d0-6835-11e3-a172-3729e500a0e7] Beginning stream session with /10.171.81.22
DEBUG [StreamConnectionEstablisher:2] 2013-12-18 22:39:28,210 ConnectionHandler.java (line 78) [Stream #40d875d0-6835-11e3-a172-3729e500a0e7] Sending stream init for incoming stream
DEBUG [StreamConnectionEstablisher:2] 2013-12-18 22:39:28,211 ConnectionHandler.java (line 84) [Stream #40d875d0-6835-11e3-a172-3729e500a0e7] Sending stream init for outgoing stream
DEBUG [STREAM-OUT-/10.171.81.22] 2013-12-18 22:39:28,212 ConnectionHandler.java (line 356) [Stream #40d875d0-6835-11e3-a172-3729e500a0e7] Sending Prepare (1 requests,  2 files}
{code}

On requested node
{code}
DEBUG [STREAM-IN-/10.172.27.174] 2013-12-18 22:39:28,296 ConnectionHandler.java (line 292) [Stream #40d875d0-6835-11e3-a172-3729e500a0e7] Received Prepare (1 requests,  2 files}
ERROR [STREAM-IN-/10.172.27.174] 2013-12-18 22:39:28,314 StreamSession.java (line 410) [Stream #40d875d0-6835-11e3-a172-3729e500a0e7] Streaming error occurred
java.lang.NullPointerException
        at org.apache.cassandra.streaming.ConnectionHandler.sendMessage(ConnectionHandler.java:174)
        at org.apache.cassandra.streaming.StreamSession.prepare(StreamSession.java:436)
        at org.apache.cassandra.streaming.StreamSession.messageReceived(StreamSession.java:358)
        at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:293)
        at java.lang.Thread.run(Thread.java:724)
DEBUG [STREAM-IN-/10.172.27.174] 2013-12-18 22:39:28,316 ConnectionHandler.java (line 153) [Stream #40d875d0-6835-11e3-a172-3729e500a0e7] Closing stream connection handler on /10.172.27.174
 INFO [STREAM-IN-/10.172.27.174] 2013-12-18 22:39:28,317 StreamResultFuture.java (line 181) [Stream #40d875d0-6835-11e3-a172-3729e500a0e7] Session with /10.172.27.174 is complete
 WARN [STREAM-IN-/10.172.27.174] 2013-12-18 22:39:28,317 StreamResultFuture.java (line 210) [Stream #40d875d0-6835-11e3-a172-3729e500a0e7] Stream failed
{code}","19/Dec/13 16:58;rspitzer;Attached RepairLogs.tar.gz Which has logs from all the nodes involved

They contain data from two trial runs, one where counter data is attempted to be repaired, and one where standard inserts are repaired. The Shutdown indicates the separation between these tests. 

Null pointers are seen on 10.171.49.137 and 10.171.81.22","23/Dec/13 15:56;yukim;Thanks Russell.

From the log, I can see streaming session on receiver side did not set up correctly, though sender thought we were ready. Sender wrote stream init message through SocketChannel API but the message was not sent/received.
I think there are two ways to fix this. 1) Sender waits for ACK from receiver or 2) Receiver holds response to sender until ready.
I'm tend to fix this using 2) rather than add another message for ACK, since the next message sender would send is only one stream PREPARE message.

Let me work on that patch as well as to fix AE I encountered above.","16/Jan/14 23:32;yukim;I think the actual cause of this is StreamInitMessage is not completely written. We should have checked if SocketChannel#write does write out content of message buffer. Patch attached to fix this.

[~rspitzer] Can you test with the patch applied to 2.0 branch?","17/Jan/14 17:51;rspitzer;Ran the test again last night and repair reported the following exceptions, I'll have the logs up in a moment.

Setup:
4 Nodes, 2 per DC

{code}
ERROR [AntiEntropySessions:2] 2014-01-17 06:59:13,320 RepairSession.java (line 278) [repair #def293b0-7f44-11e3-b180-d1c68624042f] session completed with the following error
org.apache.cassandra.exceptions.RepairException: [repair #def293b0-7f44-11e3-b180-d1c68624042f on Keyspace1/Standard1, (-4559856749309798061,-4559456353371206248]] Sync failed between /10.171.121.18 and /10.196.16.123
        at org.apache.cassandra.repair.RepairSession.syncComplete(RepairSession.java:200)
        at org.apache.cassandra.service.ActiveRepairService.handleMessage(ActiveRepairService.java:204)
        at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:59)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:60)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
ERROR [AntiEntropySessions:2] 2014-01-17 06:59:13,325 CassandraDaemon.java (line 192) Exception in thread Thread[AntiEntropySessions:2,5,RMI Runtime]
java.lang.RuntimeException: org.apache.cassandra.exceptions.RepairException: [repair #def293b0-7f44-11e3-b180-d1c68624042f on Keyspace1/Standard1, (-4559856749309798061,-4559456353371206248]] Sync failed between /10.171.121.18 and /10.196.16.123
        at com.google.common.base.Throwables.propagate(Throwables.java:160)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
Caused by: org.apache.cassandra.exceptions.RepairException: [repair #def293b0-7f44-11e3-b180-d1c68624042f on Keyspace1/Standard1, (-4559856749309798061,-4559456353371206248]] Sync failed between /10.171.121.18 and /10.196.16.123
        at org.apache.cassandra.repair.RepairSession.syncComplete(RepairSession.java:200)
        at org.apache.cassandra.service.ActiveRepairService.handleMessage(ActiveRepairService.java:204)
        at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:59)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:60)
        ... 3 more
 INFO [AntiEntropySessions:4] 2014-01-17 06:59:13,328 RepairSession.java (line 236) [repair #df5d6370-7f44-11e3-b180-d1c68624042f] new session: will sync /10.171.121.18, /10.198.2.16 on range (-5516517151222322415,-5504449186624942606] for Keyspace1.[SuperCounter1, Super1, Counter3, Standard1, Counter1]
{code}",17/Jan/14 18:00;rspitzer;Logs from patched cluster running repair test,"17/Jan/14 22:28;yukim;[~rspitzer] thanks for the log, I'm still seeing the same NPE.

Updated patch attached. This version also enqueues message until connection for output is established. Can you test with it?","17/Jan/14 22:36;rspitzer;Of course, Setting up now.",28/Jan/14 02:21;rspitzer;Newest patch looks good to me. Passes extended repair tests.,"28/Jan/14 15:21;yukim;Thanks for testing.
Committed to 2.0 and trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cassandra 2.0 won't start up with Java 7u40 with Client JVM.  (works on Server JVM, and both JVMs 7u25)",CASSANDRA-6190,12673612,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,slowenthal,slowenthal,13/Oct/13 17:30,12/Mar/19 14:17,13/Mar/19 22:29,25/Oct/13 20:30,1.2.14,2.0.3,,Local/Config,,,0,,,,,"Java 7u40 on some platforms do not recognize the the -XX:+UseCondCardMark JVM option.  7u40 on Macintosh works correctly,  If I use the tarball 7u40 version of 7, we encounter the error below. I tried 7u25 (the previous release) and it functioned correctly.

ubuntu@ubuntu:~$ Unrecognized VM option 'UseCondCardMark'
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.

",Ubuntu 13.04 32- and 64-bit  JDK 7u40  (tried JRE 7u25),,,,,,,,,,,,,,,,,,,,,,,15/Oct/13 13:35;brandon.williams;6190.txt;https://issues.apache.org/jira/secure/attachment/12608487/6190.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-13 17:39:16.825,,,no_permission,,,,,,,,,,,,353235,,,Wed Jan 29 15:07:38 UTC 2014,,,,,,0|i1owgn:,353528,,,,,,,,urandom,urandom,,,,,,,,,,"13/Oct/13 17:39;jbellis;So 7u40 doesn't work on Ubuntu, but does work on OS X?

Is this Oracle JDK or OpenJDK?","13/Oct/13 17:57;brandon.williams;Oracle 7u40 runs just fine on Debian, I can't imagine it wouldn't work on Ubuntu.","13/Oct/13 17:59;slowenthal;This is Oracle JDK.   It works on OSX.  I checked system.log and jconsole on mac to ensure that it's picked up, and we are running 7u40, and both are true.",13/Oct/13 18:30;pmcfadin;Just tried c* 2.0.1 using Oracle JDK 7u40 on CentOS 6.4. Works without modification. ,13/Oct/13 18:38;jbellis;I wonder if Steve has a 1.6 jvm installed that is being picked up instead.  (Especially since he mentions tarballs.),"13/Oct/13 18:45;brandon.williams;That would be my guess.  Steve, can you paste the output of 'dpkg -l | grep jre' and if you have results, trying removing all those packages so there can only be one java install on the system? (the oracle one, which I assume is from a tarball)",13/Oct/13 18:46;enigmacurry;[~slowenthal] Put an 'echo $JAVA' as the first line of the launch_service() function in bin/cassandra and you can verify that you're using the right java.,"13/Oct/13 22:06;slowenthal;I've even tried it by explicitly setting JAVA_HOME to point at my various javas.  Remember - if we aren't using Java 7, we don't fall into the code that adds that parameter.   Java 7 gets put in the system.log.","14/Oct/13 01:47;slowenthal;Here is a simple test.  Not the ""FileNotFoundException"" is good - I ran C* with the wrong permissions, so that is the expected result.

Unix Info:
Linux ubuntu 3.8.0-19-generic #29-Ubuntu SMP Wed Apr 17 18:19:42 UTC 2013 i686 i686 i686 GNU/Linux
u


ubuntu@ubuntu:~$ export JAVA_HOME=~/Downloads/jre1.7.0_40/
ubuntu@ubuntu:~$ cassandra
xss =  -ea -javaagent:/usr/share/cassandra/lib/jamm-0.2.5.jar -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms1024M -Xmx1024M -Xmn100M -XX:+HeapDumpOnOutOfMemoryError -Xss256k
ubuntu@ubuntu:~$ Unrecognized VM option 'UseCondCardMark'
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.

ubuntu@ubuntu:~$ export JAVA_HOME=~/Downloads/jre1.7.0_25/
ubuntu@ubuntu:~$ cassandra
xss =  -ea -javaagent:/usr/share/cassandra/lib/jamm-0.2.5.jar -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms1024M -Xmx1024M -Xmn100M -XX:+HeapDumpOnOutOfMemoryError -Xss256k
ubuntu@ubuntu:~$ log4j:ERROR setFile(null,true) call failed.
java.io.FileNotFoundException: /var/log/cassandra/system.log (Permission denied)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(Unknown Source)
","15/Oct/13 03:16;slowenthal;It does work on windows - set JAVA_HOME manually to a 7u40 home, and add the JVM option to cassandra.bat. Verified with jconsole.","15/Oct/13 04:14;slowenthal;Mystery Solved:  In 7u40, the option is missing in the Client JVM.  In 7u25, it's available in both the client and server JVM.

ubuntu@ubuntu:~/Downloads$ cd jre1.7.0_40/
ubuntu@ubuntu:~/Downloads/jre1.7.0_40$ find . -type f -exec grep UseCondCard {} +
Binary file ./lib/i386/server/libjvm.so matches

ubuntu@ubuntu:~/Downloads/jre1.7.0_25$ find . -type f -exec grep UseCondCard {} +
Binary file ./lib/i386/server/libjvm.so matches
Binary file ./lib/i386/client/libjvm.so matches
ubuntu@ubuntu:~/Downloads/jre1.7.0_25$ 


","15/Oct/13 05:46;slowenthal;Oracle Bug report filed 9007478 

Dear Java Developer,

Thank you for reporting this issue.

We have determined that this report is a new bug and have entered the bug into our bug tracking system under Bug Id: 9007478 . You can look for related issues on the Java Bug Database at http://bugs.sun.com.

We will try to process all newly posted bugs in a timely manner, but we make no promises about the amount of time in which a bug will be fixed. If you just reported a bug that could have a major impact on your project, consider using one of the technical support offerings available at Oracle Support.

Thanks again for your submission!

Regards,
Java Developer Support",15/Oct/13 05:57;slowenthal;It looks like java defaults to client if the machine has only 1 core.  Our training VM was created with a single core.,15/Oct/13 10:47;jbellis;It has to be 32bit as well. http://docs.oracle.com/javase/7/docs/technotes/guides/vm/server-class.html,"15/Oct/13 13:35;brandon.williams;Patch to also check for 64bit java when adding the UseConfCardMark flag.  We complain about 32bit JVMs later, so this at least gets us past the confusing error message.","16/Oct/13 05:20;slowenthal;Here is a response from Oracle:

Hi Steve,
  I just wanted to close the loop on the bug you filled about UseCondCardMark not being available with the client JVM in 7u40.  The issue is that UseCondCardMark was never really available in client.  It was a NOP, and in 7u40 we moved the flag to work only with server as it should be.  Sorry for the confusion.


--
Azeem Jiva
@javawithjiva
","16/Oct/13 05:23;slowenthal;Also 7u45 is out today, and it exhibits the same problem.",16/Oct/13 12:27;brandon.williams;Can you test with my patch?,"25/Oct/13 20:22;urandom;+1, lgtm",25/Oct/13 20:30;brandon.williams;Committed,29/Jan/14 14:50;jeromatron;would this make sense to backport to the 1.2 line?  It's been reproduced with a 32 bit JDK 7 with 1.2.13.,29/Jan/14 14:58;brandon.williams;Done.,29/Jan/14 15:07;jeromatron;Thanks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
speculative retry can sometimes violate consistency,CASSANDRA-6194,12673619,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,13/Oct/13 18:56,12/Mar/19 14:17,13/Mar/19 22:29,23/Oct/13 15:52,2.0.2,,,,,,0,,,,,"This is most evident with intermittent failures of the short_read dtests.  I'll focus on short_read_reversed_test for explanation, since that's what I used to bisect.  This test inserts some columns into a row, then deletes a subset, but it performs each delete on a different node, with another node down (hints are disabled.)  Finally it reads the row back at QUORUM and checks that it doesn't see any deleted columns, however with speculative retry on this often fails.  I bisected this to the change that made 99th percentile SR the default reliably by looping the test enough times at each iteration to be sure it was passing or failing.",,,,,,,,,,,,,,,,,,,,,,,,22/Oct/13 13:53;jbellis;6194.txt;https://issues.apache.org/jira/secure/attachment/12609652/6194.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-22 13:53:40.289,,,no_permission,,,,,,,,,,,,353242,,,Wed Oct 23 15:52:09 UTC 2013,,,,,,0|i1owi7:,353535,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"13/Oct/13 18:58;brandon.williams;Unfortunately I haven't been able to repro manually and ccm has a problem with logging at debug on 2.0+ currently.  10 iterations of the test should be enough to trigger, though I bisected again to the same point with 30.",22/Oct/13 13:53;jbellis;Patch attached to wait for all the contacted replicas on DME.  Not sure how this could cause the test failure though so it's kind of a shot in the dark.,"22/Oct/13 17:10;slebresne;For the record, the error returned by the dtest is:
{noformat}
======================================================================
FAIL: short_read_reversed_test (consistency_test.TestConsistency)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mcmanus/Git/dtest-cassandra/consistency_test.py"", line 309, in short_read_reversed_test
    assert res[i][1] == 'value%d' % (5-i), 'Expecting value%d, got %s (%s)' % (5-i, res[i][1], str(res))
AssertionError: Expecting value5, got value8 ([[u'c000008', u'value8'], [u'c000005', u'value5'], [u'c000004', u'value4']])

----------------------------------------------------------------------
Ran 1 test in 130.035s
{noformat}","22/Oct/13 19:44;brandon.williams;Worth noting that there's no real pattern to the columns, other than they're wrong

{noformat}
AssertionError: Expecting value5, got value7 ([[u'c000007', u'value7'], [u'c000005', u'value5'], [u'c000004', u'value4']])
{noformat}","22/Oct/13 20:08;jbellis;The problem is that when we speculate, we do multiple data reads, and RowDigestResolver assumes there is only one data read.  (If there is more than one, it does not error out but silently drops all but one.)

So if the speculative read results in triggering the callback's ""we have enough replies to satisfy CL"" logic, and the speculative data read finished before the digest, we effectively do CL.ONE logic instead of CL.QUORUM.

https://github.com/jbellis/cassandra/commits/6194 includes a fix for this and also a fix for DigestMismatch logic with SR.",22/Oct/13 20:36;brandon.williams;This fixes the test for me.,23/Oct/13 15:52;iamaleksey;LGTM and committed (also made 99PERCENTILE the default again).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
counter increment hangs,CASSANDRA-6175,12673100,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,09/Oct/13 19:57,12/Mar/19 14:17,13/Mar/19 22:29,09/Oct/13 21:55,2.0.2,,,,,,0,,,,,"In a three node cluster, a simple counter increment at quorum hangs the query indefinitely.  git blames our familiar friend CASSANDRA-6132 once again.",,,,,,,,,,,,,,,,,,,,,,,,09/Oct/13 21:10;jbellis;6175.txt;https://issues.apache.org/jira/secure/attachment/12607654/6175.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-09 20:25:24.189,,,no_permission,,,,,,,,,,,,352723,,,Wed Oct 09 21:55:59 UTC 2013,,,,,,0|i1ot9z:,353010,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"09/Oct/13 20:01;brandon.williams;Possible trace:

{noformat}
ERROR [ReplicateOnWriteStage:1] 2013-10-09 19:50:34,516 CassandraDaemon.java (line 191) Exception in thread Thread[ReplicateOnWriteStage:1,5,main]
java.lang.AssertionError
    at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:524)
    at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:572)
    at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:553)
    at org.apache.cassandra.service.StorageProxy.sendMessagesToOneDCInternal(StorageProxy.java:642)
    at org.apache.cassandra.service.StorageProxy.sendMessagesToOneDC(StorageProxy.java:624)
    at org.apache.cassandra.service.StorageProxy.sendToHintedEndpoints(StorageProxy.java:551)
    at org.apache.cassandra.service.StorageProxy$7$1.runMayThrow(StorageProxy.java:804)
    at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1631)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
{noformat}

I say possible because it didn't show up until I killed the process with prejudice.",09/Oct/13 20:25;jbellis;Something's broke; sendMessagesToOneDC is only present in 1.2,"09/Oct/13 20:28;brandon.williams;That was somewhere in the bisect, so I don't know where it was.  It will totally hang on 2.0 though :)","09/Oct/13 21:05;jbellis;The actual problem:

{noformat}
java.lang.AssertionError
	at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:551)
	at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:606)
	at org.apache.cassandra.service.StorageProxy.mutateCounter(StorageProxy.java:1039)
	at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:502)
	at org.apache.cassandra.service.StorageProxy.mutateWithTriggers(StorageProxy.java:577)
	at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithoutCondition(ModificationStatement.java:379)
	at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:363)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:126)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:142)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:133)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1936)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4394)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4378)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
{noformat}",09/Oct/13 21:10;jbellis;Assertion fix attached.,09/Oct/13 21:15;brandon.williams;+1,09/Oct/13 21:55;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LOCAL_ONE doesn't work for SimpleStrategy,CASSANDRA-6238,12675555,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,alexliu68,alexliu68,alexliu68,24/Oct/13 20:14,12/Mar/19 14:17,13/Mar/19 22:29,07/Nov/13 06:10,1.2.12,2.0.3,,,,,0,,,,,"LOCAL_ONE only works for NetworkTopologyStrategy which has DC specification. Any other strategy fails.

If there is no  DC specified in the strategy, we should treat LOCAL_ONE as ONE",,,,,,,,,,,,,,CASSANDRA-6151,,,,,,,,,,24/Oct/13 21:54;alexliu68;6238-v2.txt;https://issues.apache.org/jira/secure/attachment/12610170/6238-v2.txt,24/Oct/13 20:36;alexliu68;6238.txt;https://issues.apache.org/jira/secure/attachment/12610147/6238.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-10-25 05:41:03.583,,,no_permission,,,,,,,,,,,,355132,,,Thu Nov 07 12:51:09 UTC 2013,,,,,,0|i1p82v:,355420,,,,,,,,jasobrown,jasobrown,,,,,,,,,,"24/Oct/13 21:57;alexliu68;6238.txt change only for LOCAL_ONE.
6238-v2.txt change is for both LOCAL_ONE and LOCAL_QUORUM

I am not sure that we want LOCAL_QUORUM to support SimpleStrategy as well, so I attach v2 for reference.","25/Oct/13 05:41;jasobrown;I used LOCAL_QUORUM's failing on non-NTS as my template I went by when making LOCAL_ONE only usable with NTS. I don't think it makes sense for the two CLs to behave differently: either they both should support non-NTS or not.  I'm not sure if I feel too strongly either way, but I'm more inclined to retain the current CL.LOCAL_* requiring NTS.","25/Oct/13 07:46;alexliu68;I understand it comes as the same way as original LOCAL_QUORUM.

SimpleStrategy is for a cluster without multiple DCs. NTS is for multiple DC cluster. SimpleStrategy could be treated as a special NTS that is only for a one DC cluster.

The default CL of Cassandra Hadoop integration is LOCAL_ONE now, it should work for all types of strategy.  LOCAL_ONE looks like a better candidate as a default CL than ONE. When people first start using C*, it is common to be a one DC cluster, then later it may grow up to  a multiple DC cluster. If LOCAL_ONE work for all types of strategy,  manual change the default CL is not needed.  If we keep LOCAL_* requiring NTS, we has to update all the hadoop/pig examples and tests to use ONE. 

The main reason to make LOCAL_* work for all types of strategy is that LOCAL_* are better candidates as default CLs. The default CL should work for all types of strategy.

","25/Oct/13 15:53;brandon.williams;I agree that having LOCAL_ONE be the hadoop default is advantageous, and having hadoop not work out of the box for those that just want to play with it (using SS) isn't very nice.","07/Nov/13 05:52;jasobrown;I'm fine with making LOCAL_ONE work with SS, but we should also make LOCAL_QUORUM do so, as well. If there's no objections to making the LOCAL_*'s consistent, let's move forward. I'll review the patch by the morning.","07/Nov/13 06:10;jasobrown;committed to 1.2, 2.0, and trunk.","07/Nov/13 12:51;iamaleksey;If we made both LO and LQ work with SS, why not lift the restriction for EACH_QUORUM as well, for consistency-sake?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Authentication is broken for the protocol v1 on C* 2.0,CASSANDRA-6233,12675250,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,23/Oct/13 13:56,12/Mar/19 14:17,13/Mar/19 22:29,28/Oct/13 09:31,2.0.2,,,,,,3,,,,,"CASSANDRA-5664 simplified the decoding method of CredentialsMessage by using CBUtil.readStringMap (instead of duplicating the code). Unfortunately, that latter method turns his keys to uppercase (to provide some form of case insensitivity for keys), and in the case of CredentialsMessage this breaks PasswordAuthenticator that expect lowercased keys (besides, it's a bad idea to mess up with the case of the credentials map in general).

Making CBUtil.readStringMap uppercase keys was probably a bad idea in the first place (as nothing in the method name imply this), so attaching patch that remove this (and uppercase keys specifically in StartupMessage where that was done on purpose).",,,,,,,,,,,,,,CASSANDRA-6243,,,,,,,,,,23/Oct/13 13:57;slebresne;6233.txt;https://issues.apache.org/jira/secure/attachment/12609856/6233.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-24 20:13:40.246,,,no_permission,,,,,,,,,,,,354870,,,Sun Oct 27 19:01:24 UTC 2013,,,,,,0|i1p6gv:,355159,,,,,,,,iamaleksey,iamaleksey,,,2.0.1,,,,,,,"24/Oct/13 20:13;vchekan;It seems authenticated login is not covered by any unit tests...
Would it be better to use apache's CaseInsensitiveMap?
http://commons.apache.org/proper/commons-collections/javadocs/api-release/org/apache/commons/collections4/map/CaseInsensitiveMap.html","25/Oct/13 18:11;enigmacurry;[~vchekan] there are [multiple authentication tests|https://github.com/riptano/cassandra-dtest/blob/master/auth_test.py] in cassandra-dtest.

[~slebresne] - What is protocol v1? Can you give me an example of how this breaks?

I tried [creating this test|https://github.com/EnigmaCurry/cassandra-dtest/commit/11042a79a3190211e40981271cfcca9c06aee456] and it's passing on cassandra-2.0, but also on 2.0.1, so I must not be exercising the same thing you're describing here. 
I also notice that the patch looks like it's already been committed to cassandra-2.0 branch (not sure if that was intentional or not.)","26/Oct/13 12:25;slebresne;I'm talking of the native protocol. cassandra-dtest uses CQL-over-thrift so there is no way to reproduce this bug with it. To produce, you'd need to for example use the Datastax java driver 1.0.4 against C* 2.0.1. The steps to reproduce are there: https://datastax-oss.atlassian.net/browse/JAVA-190","26/Oct/13 13:02;iamaleksey;Committed by Sylvain in 86b26b67fe9dd804b84a56c2535726b966d28d13, so is part of 2.0.2. 'formal' +1 here. (needs updating CHANGES.txt).","27/Oct/13 19:01;slebresne;Hum, I think I screwed up, didn't meant to commit it before the review. But well, I guess it's done now :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift's prepare_cql*_query() should validate login,CASSANDRA-6254,12676108,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,28/Oct/13 12:39,12/Mar/19 14:17,13/Mar/19 22:29,28/Oct/13 13:28,1.2.12,2.0.3,,,,,0,,,,,"Non-logged in users shouldn't be able to prepare statements when authentication is enabled.

Native protocol is not affected by this, since it doesn't let you do anything unless you authenticate when auth is enabled.",,,,,,,,,,,,,,,,,,,,,,,,28/Oct/13 12:40;iamaleksey;6254.txt;https://issues.apache.org/jira/secure/attachment/12610547/6254.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-28 13:13:47.225,,,no_permission,,,,,,,,,,,,355605,,,Mon Oct 28 13:28:57 UTC 2013,,,,,,0|i1pazr:,355893,,,,,,,,slebresne,slebresne,,,,,,,,,,28/Oct/13 12:40;iamaleksey;A trivial patch attached.,28/Oct/13 13:13;slebresne;+1,"28/Oct/13 13:28;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2.0.x leaks file handles,CASSANDRA-6275,12676842,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,graham.sanderson,ash2k,ash2k,31/Oct/13 11:14,12/Mar/19 14:17,13/Mar/19 22:29,20/Nov/13 16:53,2.0.3,,,,,,6,,,,,"Looks like C* is leaking file descriptors when doing lots of CAS operations.

{noformat}
$ sudo cat /proc/15455/limits
Limit                     Soft Limit           Hard Limit           Units    
Max cpu time              unlimited            unlimited            seconds  
Max file size             unlimited            unlimited            bytes    
Max data size             unlimited            unlimited            bytes    
Max stack size            10485760             unlimited            bytes    
Max core file size        0                    0                    bytes    
Max resident set          unlimited            unlimited            bytes    
Max processes             1024                 unlimited            processes
Max open files            4096                 4096                 files    
Max locked memory         unlimited            unlimited            bytes    
Max address space         unlimited            unlimited            bytes    
Max file locks            unlimited            unlimited            locks    
Max pending signals       14633                14633                signals  
Max msgqueue size         819200               819200               bytes    
Max nice priority         0                    0                   
Max realtime priority     0                    0                   
Max realtime timeout      unlimited            unlimited            us 
{noformat}

Looks like the problem is not in limits.

Before load test:
{noformat}
cassandra-test0 ~]$ lsof -n | grep java | wc -l
166

cassandra-test1 ~]$ lsof -n | grep java | wc -l
164

cassandra-test2 ~]$ lsof -n | grep java | wc -l
180
{noformat}

After load test:
{noformat}
cassandra-test0 ~]$ lsof -n | grep java | wc -l
967

cassandra-test1 ~]$ lsof -n | grep java | wc -l
1766

cassandra-test2 ~]$ lsof -n | grep java | wc -l
2578
{noformat}

Most opened files have names like:
{noformat}
java      16890 cassandra 1636r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1637r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1638r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1639r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1640r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1641r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1642r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1643r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1644r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1645r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1646r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1647r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1648r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1649r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1650r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1651r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1652r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1653r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1654r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1655r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1656r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
{noformat}

Also, when that happens it's not always possible to shutdown server process via SIGTERM. Have to use SIGKILL.

p.s. See mailing thread for more context information https://www.mail-archive.com/user@cassandra.apache.org/msg33035.html","java version ""1.7.0_25""
Java(TM) SE Runtime Environment (build 1.7.0_25-b15)
Java HotSpot(TM) 64-Bit Server VM (build 23.25-b01, mixed mode)
Linux cassandra-test1 2.6.32-279.el6.x86_64 #1 SMP Thu Jun 21 15:00:18 EDT 2012 x86_64 x86_64 x86_64 GNU/Linux",,,,,,,,,,,,,,,,,,,,,,,20/Nov/13 05:07;jbellis;6275.txt;https://issues.apache.org/jira/secure/attachment/12614790/6275.txt,15/Nov/13 01:39;mshuler;c_file-descriptors_strace.tbz;https://issues.apache.org/jira/secure/attachment/12613994/c_file-descriptors_strace.tbz,31/Oct/13 11:16;ash2k;cassandra_jstack.txt;https://issues.apache.org/jira/secure/attachment/12611371/cassandra_jstack.txt,11/Nov/13 17:34;gianlucaborello;leak.log;https://issues.apache.org/jira/secure/attachment/12613184/leak.log,13/Nov/13 13:32;baldrick;position_hints.tgz;https://issues.apache.org/jira/secure/attachment/12613598/position_hints.tgz,31/Oct/13 15:49;baldrick;slog.gz;https://issues.apache.org/jira/secure/attachment/12611411/slog.gz,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2013-10-31 12:03:49.18,,,no_permission,,,,,,,,,,,,356218,,,Wed Nov 20 16:53:46 UTC 2013,,,,,,0|i1perj:,356506,2.0.1,2.0.2,,,,,,jbellis,jbellis,,,,,,,,,,31/Oct/13 11:16;ash2k;Attaching thread dump of process when it is not possible to shutdown it cleanly.,"31/Oct/13 12:03;brandon.williams;This does look rather damning, and I know _something_ is up here because we can't run 2.0 with ccm on debug, so maybe this is it.",31/Oct/13 15:49;baldrick;Part of system.log showing lots of memtable flush messages.,"31/Oct/13 15:50;baldrick;We had to downgrade from 2.0.2 to 1.2.11 due to C* leaking vast numbers of file descriptors (same issue in 2.0.1), approximately 50 a second.  We didn't have any trouble shutting down servers, we also were *not* using Paxos, so might be a different issue.  Like in this report, the same sstable files had been opened again and again (and again) only they were from one of our own column families, not from a system column family.  This table was being polled repeatedly (once per second per program) by about 20 programs using the native protocol; the polling was doing a range slice.  I haven't been able to reproduce the file descriptor leak in our test environment.

One thing I noticed is that the system logs were chock-a-block with memtable flushing messages, many many more than we would get with 1.2.11 (I've attached a snippet).","31/Oct/13 19:29;mishail;bq. Also, when that happens it's not always possible to shutdown server process via SIGTERM. Have to use SIGKILL.

As far as I understand here *what* is happening

* {{SIGTERM handler}} waits for {{StorageServiceShutdownHook}} 
* {{StorageServiceShutdownHook}} waits (up to *3600 sec == 1hr*) for {{mutationStage}} threads to complete. 
* {{""MutationStage:2718""}} thread performs {{ColumnFamilyStore.forceBlockingFlush}} initiated by {{TruncateVerbHandler.doVerb}} and waits for {{""MemtablePostFlusher:1""}} 
* {{""MemtablePostFlusher:1""}} is waiting on {{CountDownLatch.await}} (in {{WrappedRunnable}} returned from {{ColumnFamilyStore.switchMemtable)}}.  It will wait until the latch is counted down to zero.

There is also another call to {{ColumnFamilyStore.forceBlockingFlush}} from {{""OptionalTasks:1"":BatchlogManager.cleanup()}} . 
","31/Oct/13 20:10;jbellis;Hmm.  It's possible that we shouldn't be running Truncate on the Mutation stage.  But, I don't think Duncan or Mikhail mentioned running truncate so there is probably something else wrong as well.",31/Oct/13 20:34;mishail;[~jbellis] still there is {{TruncateVerbHandler.doVerb}} in Mikhail's [^cassandra_jstack.txt],31/Oct/13 20:42;brandon.williams;[~baldrick] are you using the native protocol?,"31/Oct/13 21:13;baldrick;Yes, I'm using the native protocol.  No use of truncate on my part.",31/Oct/13 21:34;jbellis;Let's create a new ticket for the truncate hang then.,01/Nov/13 02:51;ash2k;My load test uses TRUNCATE before it starts. I will check if that leak happens without it.,"01/Nov/13 03:14;ash2k;I tried without TRUNCATE - no leaking (test TRUNCATEs 3 tables before it starts).

Before test:
{noformat}
[root@cassandra-test0 ~]$ lsof -n | grep java | wc -l
169

[root@cassandra-test1 ~]$ lsof -n | grep java | wc -l
167

[root@cassandra-test2 ~]# lsof -n | grep java | wc -l
173
{noformat}

After test:
{noformat}
[root@cassandra-test0 ~]$ lsof -n | grep java | wc -l
172

[root@cassandra-test1 ~]$ lsof -n | grep java | wc -l
172

[root@cassandra-test2 ~]# lsof -n | grep java | wc -l
183
{noformat}","01/Nov/13 08:30;baldrick;In my case I didn't use truncate, however I did do one exotic operation: I used alter table to drop a no longer needed column a few days before I noticed this issue.

Before downgrading I made a copy of the entire contents of /var/lib/cassandra/, so I could try recreating the 2.0.2 cluster and the problem in some virtual machines using these.",01/Nov/13 08:34;baldrick;I also changed sstable_compression from SnappyCompressor to LZ4Compressor.,"11/Nov/13 17:34;gianlucaborello;We are experiencing a similar issue in 2.0.2.

It started happening after we set a TTL for all our columns in a very limited datastore (just a few GBs).

We can easily see the fd count rapidly increase to 100000+, and the majority of fds are (from lsof):

{noformat}
java      13168       cassandra  267r      REG                9,0   273129  671089723 /raid0/cassandra/data/draios/process_counters_by_exe/draios-process_counters_by_exe-jb-231-Data.db (deleted)
java      13168       cassandra  268r      REG                9,0   273129  671089723 /raid0/cassandra/data/draios/process_counters_by_exe/draios-process_counters_by_exe-jb-231-Data.db (deleted)
java      13168       cassandra  269r      REG                9,0   273129  671089723 /raid0/cassandra/data/draios/process_counters_by_exe/draios-process_counters_by_exe-jb-231-Data.db (deleted)
java      13168       cassandra  270r      REG                9,0   273129  671089723 /raid0/cassandra/data/draios/process_counters_by_exe/draios-process_counters_by_exe-jb-231-Data.db (deleted)
java      13168       cassandra  271r      REG                9,0   273129  671089723 /raid0/cassandra/data/draios/process_counters_by_exe/draios-process_counters_by_exe-jb-231-Data.db (deleted)
java      13168       cassandra  272r      REG                9,0   273129  671089723 /raid0/cassandra/data/draios/process_counters_by_exe/draios-process_counters_by_exe-jb-231-Data.db (deleted)
java      13168       cassandra  273r      REG                9,0   273129  671089723 /raid0/cassandra/data/draios/process_counters_by_exe/draios-process_counters_by_exe-jb-231-Data.db (deleted)
{noformat}

I'm attaching the log of the exception (leak.log). You can see the exceptions, and then Cassandra eventually shuts down. We had to temporarily downgrade to 1.2.11
","11/Nov/13 20:45;jbellis;[~ash2k] or others, can you verify if this is also a problem in 1.2.11?","11/Nov/13 20:49;gianlucaborello;[~jbellis], FWIW 1.2.11 is working fine for us (I have one week of uptime so far since the downgrade).",11/Nov/13 20:55;jbellis;Can you have a look [~krummas] ?,"12/Nov/13 06:51;baldrick;All our leaked fd's were for a large table that uses TTL.  We also don't have any problems with 1.2.11 (which we had to downgrade to, just like Gianluca).","12/Nov/13 07:14;ash2k;[~jbellis] my test workload uses LWT so it cannot be run on 1.2.x. The written columns themself do not use TTL but AFAIK Paxos table uses TTL. And in my case I see a lot of open Paxos-related files. So, taking into account what others said above, looks like the problem is somehow connected to TTL.","12/Nov/13 19:30;krummas;been looking at this a bit and can't really reproduce, suspected CASSANDRA-5228 - but that seems to work (or, found a bug, but unrelated to this, CASSANDRA-6337)

slog.gz looks a bit like what [~mkjellman] reported in CASSANDRA-5241 (looping flushing of system tables) but that was resolved a long time ago.

does anyone have a way to reproduce? [~ash2k] would it be possible to post your load test?
","13/Nov/13 02:11;rcoli;A brief note to mention that when durable_writes are disabled, handling of clean shutdown (via SIGTERM/StorageServiceShutdownHook) has additional blocking while waiting for drain. If one is investigating and/or modifying the behavior of the shutdown hook, they should be aware that there are two different cases to test. See CASSANDRA-2958.","13/Nov/13 03:01;ash2k;[~krummas] sorry, I cannot post that code. The scenario is something like this:
{code}
TRUNCATE table1;
TRUNCATE table2;
TRUNCATE table3;
loop {
    SELECT FROM table1 WHERE key='xxx' (quorum);
    INSERT INTO table2 (quorum);
    INSERT INTO table3 IF NOT EXISTS; (should sometimes fail)
    UPDATE table1 WHERE  key = 'someid' IF column='zzz'; (should sometimes fail)
}
{code}
As I said, if I remove TRUNCATEs it do not leak.","13/Nov/13 13:30;baldrick;OK, here is how you can reproduce.

1) Create this keyspace:

CREATE KEYSPACE all_production WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};

2) Create a table as follows:

use all_production;
CREATE TABLE position_hints (shard int, date text, when timeuuid, sequence bigint, syd int, broker uuid, engine uuid, confirmed bigint, open_buy bigint, open_sell bigint, PRIMARY KEY ((shard, date), when)) with clustering order by (when desc);

3) Stop Cassandra.  Untar the attached file in /var/lib/cassandra/data/all_production/ to populate the position_hints table.

4) Start Cassandra.

5) Prepare a large number of queries as follows:

for (( i = 0 ; i < 1000000 ; i = i + 1 )) ; do echo ""select * from position_hints where shard=1 and date='2013-10-30' and when>ba719c52-4182-11e3-a471-003048feded4 limit 1;"" ; done > /tmp/queries

6) In cqlsh:

use all_production;
source '/tmp/queries';

7) Enjoy watching the number of fd's used by Cassandra go up and up.",13/Nov/13 13:32;baldrick;Untar this to populate the position_hints table.,"13/Nov/13 15:28;jbellis;Can you reproduce the above on 2.0.2 or 2.0.2 HEAD, [~mshuler]?","14/Nov/13 23:29;mshuler;Reproduced in 2.0.2.
On m1.medium, running C*, the open files were about 910.  My query is still running, and I'm at >3500 open files.
(I'll work on cassandra-2.0 branch HEAD, next)",15/Nov/13 00:54;mshuler;Same results on cassandra-2.0 HEAD.,15/Nov/13 01:39;mshuler;c_file-descriptors_strace.tbz is a strace of the C* java process and children while running the query to about 27k open files.  This was on the cassandra-2.0 branch in git.,"15/Nov/13 16:49;krummas;ok, this is what i have so far, i can also reproduce on an m1.medium in EC2, ubuntu 13.10 which has 3.11.x kernel.

i cannot reproduce on my laptop (debian squeeze) or my server (rhel 6), both run kernel 2.6.x. (jdk7u45 on all)

it happens with trivial tables/data as well, so seems unrelated to TTL or truncate etc

just starting cassandra up shows ~50 open FDs for the same Data.db-file",15/Nov/13 16:55;baldrick;I originally saw the issue on Ubuntu 10.04 (kernel 2.6.32) and reproduced it on Ubuntu 13.10 (kernel 3.11.0).,"15/Nov/13 16:56;pieterc;I also have the problem on Ubuntu 12.04 (Linux de-cass00 3.8.0-30-generic #44~precise1-Ubuntu SMP Fri Aug 23 18:32:41 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux)

Posted something om mailing list because I was not sure if it was a bug... (Here you can find more info, In some cases I hade a deleted file more than 50k times open)
http://www.mail-archive.com/user@cassandra.apache.org/msg32999.html

Temporary fix was to raise the nofile limit to 1kk...","15/Nov/13 17:03;mshuler;My tests were on Ubuntu precise, same kernel as above, with JVM version 1.7_25.","18/Nov/13 22:24;jre;We recently ran into this issue after upgrading to OpsCenter-4.0.0, it is quite easy to reproduce:
# Install Cassandra-2.0.2
# Install OpsCenter-4.0.0 on above cluster.

I upgraded OpsCenter on Friday, and by Sunday I had reached 1 Million open file handles.  I had to kill -9 the Cassandra processes as it wouldn't respond to sockets, DSC20 restart scripts reported successfully killing the processes but in fact did not.

{noformat}
[root@cassandra2 ~]# lsof -u cassandra|wc -l
175416
[root@cassandra2 ~]# lsof -u cassandra|grep -c OpsCenter
174474
{noformat}

Most of the handles show as ""deleted""
{noformat}
[root@cassandra2 ~]# lsof -u cassandra|grep -c deleted
174449
{noformat}","18/Nov/13 22:52;graham sanderson;Note also, that most if not all of the deleted files are of the form

{code}
java    14018 cassandra  586r   REG               8,33   8792499       1251 /data/1/cassandra/OpsCenter/rollups60/OpsCenter-rollups60-jb-4656-Data.db (deleted)
java    14018 cassandra  587r   REG               8,33  27303760       1254 /data/1/cassandra/OpsCenter/rollups60/OpsCenter-rollups60-jb-4655-Data.db (deleted)
java    14018 cassandra  588r   REG               8,33   8792499       1251 /data/1/cassandra/OpsCenter/rollups60/OpsCenter-rollups60-jb-4656-Data.db (deleted)
java    14018 cassandra  589r   REG               8,33  27303760       1254 /data/1/cassandra/OpsCenter/rollups60/OpsCenter-rollups60-jb-4655-Data.db (deleted)
java    14018 cassandra  590r   REG               8,33  10507214        936 /data/1/cassandra/OpsCenter/rollups60/OpsCenter-rollups60-jb-4657-Data.db (deleted)
{code}
We have 7 data disks per node (don't know if this contributes to the problem), and the number of such (open but) deleted files is very ill balanced with 93% on two of the 7 disks (on this particular node)... the distribution of live data file size for OpsCenter/rollups60 is a little uneven with the same data mounts that have more deleted files having more actual live data, but the deleted file counts per mount point vary by several order of magnitudes whereas the data size itself does not.","19/Nov/13 14:00;jbellis;[~mshuler] Can you reproduce in 1.2?  How about 2.0.0?

If in the latter but not the former it's going to be a bitch to bisect but I don't have any better ideas.","19/Nov/13 16:04;capncrunch4me;Environment Centos 6.4 ~ Kernel 3.10 (elrepo) ~ Cassandra 2.0.2 ~ Opscenter 4.0.

Noted that Opscenter extremely exacerbates the issue, as the roll-up CFs cause open files to grow at an incredible pace. Turning Opscenter off causes rate to slow and/or stop. After stopping opscenter, the opscenter open files on all C* nodes never release although the open files dont continue to grow.

There are 10's of thousands of these open:

/data/cassandra/OpsCenter/rollups60/OpsCenter-rollups60-jb-15491-Data.db (deleted)
/data/cassandra/OpsCenter/rollups60/OpsCenter-rollups60-jb-15491-Data.db (deleted)
/data/cassandra/OpsCenter/rollups60/OpsCenter-rollups60-jb-15491-Data.db (deleted)
/data/cassandra/OpsCenter/rollups60/OpsCenter-rollups60-jb-15491-Data.db (deleted)

A restart of cassandra and keeping opscenter off will keep file descriptors within a comfortable range. 

","19/Nov/13 20:37;graham sanderson;Yes I believe we can mitigate the problem in the OpCenter case, however it is a good test bed since it makes the problem easy to spot - note it seems to be worse under high read/write activity on tracked keyspaces/CFs, however that makes sense.

Note I was poking (somewhat blindly) thru the (2.0.2) code (partly out of interest) looking for what might be leaking these file handles, and I also took a heap dump. I discovered what turned out to be CASSANDRA-6358 which leaks FileDescriptors though their refCounts all seemed to be 0. In any case there weren't enough (total FileDescriptors - in the heap dump) to account for the problem. They were also for mem-mapped files (the ifile in SSTableReader) and none of the leaked deleted file handles were mem-mapped (since they were compressed data files)

That said CASSANDRA-6358 was pinning the SSTableReaders in memory (since the Runnable was an anonymous inner class), so someone with more knowledge of the code might have a better idea if this might be a problem (other than the memory leak)

I don't have an environment yet where I can easily build and install code changes, though we could downgrade our system test environment to 2.0.0 to see if we can reproduce the problem there - unsure if we can downgrade to 1.2.X easily given our current testing.

Note while I was looking at the code I came across CASSANDRA-5555... What caught my eye was the interaction between FileCacheService and RAR.deallocate, but more specifically related to the fact that this change, added a concurrent structure inside another separate concurrent structure, and it seemed like there might be a case where a RAR was recycled into a concurrent queue that was already removed and drained, in which case it would get GCed without close, presumably causing a file handle leak on the native side. Though I couldn't come up with any significantly convincing interactions that would cause this to happen without some very very unlucky things happening (and my knowledge of the google cache implementation was even more limited!), so this is unlikely the cause of this issue (especially if the issue doesn't happen in the 1.2.7+ branch), because I think nearly all deleted data files are being leakd, and finally because there is no particular correlation with TTL.","19/Nov/13 21:08;mishail;bq. What caught my eye was the interaction between FileCacheService and RAR.deallocate, but more specifically related to the fact that this change, added a concurrent structure inside another separate concurrent structure, and it seemed like there might be a case where a RAR was recycled into a concurrent queue that was already removed and drained, in which case it would get GCed without close, presumably causing a file handle leak on the native side

I might be wrong, but what are you saying correlates with observations from CASSANDRA-6283. [~Andie78] experiminted and 
bq. found out, that a finalizer fixes the problem. So after GC the files will be deleted (not optimal, but working fine). It runs now 2 days continously without problem. Possible fix/test:I wrote the following finalizer at the end of class org.apache.cassandra.io.util.RandomAccessReader: { deallocate(); super.finalize(); } }",19/Nov/13 21:23;mishail;I wonder what would happen with {{file_cache_size_in_mb: 0 }}. RAR should be explicitly deallocated then.,19/Nov/13 23:50;graham sanderson;Trying that now (one node with that setting),"20/Nov/13 00:16;jre;[~mishail] We (Graham Sanderson and I work together) added 'file_cache_size_in_mb: 0' to cassandra.yaml on one of the nodes, and restarted that node plus another with the default (unspecified) file_cache_size_in_mb setting to run an A/B test.  Both nodes still leak file handles, however, the node with the default setting leaks much faster (about 3-4x the leak rate).

CASSANDRA-6283 appears to be an exact duplicate of this problem, Windows and Linux JVMs appear to exhibit the exact same file handle leak behavior.","20/Nov/13 00:25;graham sanderson;Note that this would tend to imply that I was wrong (at least about the particular code path), and the change in leak rate may be attributable to less throughput without the file cache. Note the leak rate does seem quite related to how hard we are hitting the server as mentioned before, so a threading bug elsewhere might be the cause.

Note nominally buffer in RAR should be volatile, but then any code path thru close where buffer's latest value is stale would end up calling deallocate anyway (at least in the case that file_cache_size_in_mb is off; I didn't think through the other case.

So given the finalizer fix - which we can try and build here to test out (unless someone has it pre-built) - seems to imply that it is just someone failing to call close() under load conditions.",20/Nov/13 04:00;graham sanderson;We were able to confirm the finalizer fix stopped the leak,"20/Nov/13 04:09;graham sanderson;Note I believe the problem is caused by CASSANDRA-5514 (2.0 beta 1)

I don't have a patch because I don't know the exact patch for reasons below

Here is CollationController.java starting at line 264: (as of current 2.0 branch and 2.0.2)

{code}
            // Check for row tombstone in the skipped sstables
            if (skippedSSTables != null)
            {
                for (SSTableReader sstable : skippedSSTables)
                {
                    if (sstable.getMaxTimestamp() <= minTimestamp)
                        continue;

                    sstable.incrementReadCount();
                    OnDiskAtomIterator iter = filter.getSSTableColumnIterator(sstable);
                    if (iter.getColumnFamily() == null)
                        continue;

                    ColumnFamily cf = iter.getColumnFamily();
                    // we are only interested in row-level tombstones here, and only if markedForDeleteAt is larger than minTimestamp
                    if (cf.deletionInfo().getTopLevelDeletion().markedForDeleteAt > minTimestamp)
                    {
                        includedDueToTombstones++;
                        iterators.add(iter);
                        returnCF.delete(cf.deletionInfo().getTopLevelDeletion());
                        sstablesIterated++;
                    }
                }
            }
{code}

Note if the last ""if"" test does not succeed, then ""iter"" is neither closed, nor is it added to the ""iterators"" list to be closed in the finally section at the end - it would have been easy for me to add it always to ""iterators"" list except that ""iterators"" is referenced lower in the function:

{code}
            if (iterators.isEmpty())
                return null;

            Tracing.trace(""Merging data from memtables and {} sstables"", sstablesIterated);
            filter.collateOnDiskAtom(returnCF, iterators, gcBefore);
{code}

Being new to the code, I cannot say whether it should be in ""iterators"" at that point, or just have been closed (quietly) above
","20/Nov/13 04:32;graham sanderson;Also note stack trace for all leaked files we saw - someone can perhaps use this to help figure out what this actually affects (i.e. some of the iter's RARs may have been owned by someone else in which case AOK)

{code}
ERROR [Finalizer] 2013-11-20 03:43:42,129 RandomAccessReader.java (line 399) LEAK finalizer had to clean up
java.lang.Exception: RAR for /data/5/cassandra/OpsCenter/rollups60/OpsCenter-rollups60-jb-6882-Data.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:66)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:43)
	at org.apache.cassandra.io.util.CompressedPoolingSegmentedFile.createReader(CompressedPoolingSegmentedFile.java:48)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:39)
	at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1182)
	at org.apache.cassandra.db.columniterator.IndexedSliceReader.setToRowStart(IndexedSliceReader.java:108)
	at org.apache.cassandra.db.columniterator.IndexedSliceReader.<init>(IndexedSliceReader.java:84)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:65)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:42)
	at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:167)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:273)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1467)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1286)
	at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:332)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:65)
	at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:47)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{code}",20/Nov/13 05:07;jbellis;Nailed it.  (It should be closed.)  Patch attached.,"20/Nov/13 05:47;jre;I accidentally hit the ""Testing"" button, and don't see a way to revert.  I've build cassandra-2.0.2 with just this patch applied, and we are testing it now, but I didn't mean to change the status.",20/Nov/13 12:42;jbellis;(Reject takes it back to Open.),20/Nov/13 14:19;mshuler;I will test this out this morning!,"20/Nov/13 14:35;jre;So we've been running this all night, have written a few hundred GB of data with some products we're developing, all the while OpsCenter 4.0.0 was doing TTL'd rollups and what not.  Deleted file count remained at 1 the entire time, never increasing, and total file count remained below 1000.

FYI, the single undeleted file looks like some temporary file randomly generated on Cassandra startup that gets deleted but not closed for the processes' duration, example:
{noformat}
java    1925 cassandra   44u   REG              253,4      4096         13 /tmp/ffi441Hpl (deleted)
{noformat}","20/Nov/13 14:54;brandon.williams;That's probably JNA, or snappy.","20/Nov/13 16:04;slebresne;That patch lgtm, +1.","20/Nov/13 16:11;mshuler;Patch works for me, too :)",20/Nov/13 16:53;jbellis;committed.  thanks everyone!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Windows 7 data files kept open / can't be deleted after compaction.,CASSANDRA-6283,12677050,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,JoshuaMcKenzie,Andie78,Andie78,01/Nov/13 12:52,12/Mar/19 14:17,13/Mar/19 22:29,01/May/14 00:11,2.0.7,2.1.0,2.2.0 beta 1,Legacy/Local Write-Read Paths,Local/Compaction,,3,compaction,Windows,,,"Files cannot be deleted, patch CASSANDRA-5383 (Win7 deleting problem) doesn't help on Win-7 on Cassandra 2.0.2. Even 2.1 Snapshot is not running. The cause is: Opened file handles seem to be lost and not closed properly. Win 7 blames, that another process is still using the file (but its obviously cassandra). Only restart of the server makes the files deleted. But after heavy using (changes) of tables, there are about 24K files in the data folder (instead of 35 after every restart) and Cassandra crashes. I experiminted and I found out, that a finalizer fixes the problem. So after GC the files will be deleted (not optimal, but working fine). It runs now 2 days continously without problem. Possible fix/test:
I wrote the following finalizer at the end of class org.apache.cassandra.io.util.RandomAccessReader:

{code:title=RandomAccessReader.java|borderStyle=solid}
@Override
protected void finalize() throws Throwable {
	deallocate();
	super.finalize();
}
{code}

Can somebody test / develop / patch it? Thx.",Windows 7 (32) / Java 1.7.0.45,,,,,,,,,,,,,CASSANDRA-3613,,,,,,,,,,25/Feb/14 20:55;JoshuaMcKenzie;6283_StreamWriter_patch.txt;https://issues.apache.org/jira/secure/attachment/12631037/6283_StreamWriter_patch.txt,23/Nov/13 03:08;graham sanderson;leakdetect.patch;https://issues.apache.org/jira/secure/attachment/12615443/leakdetect.patch,05/Mar/14 18:38;Andie78;neighbor-log.zip;https://issues.apache.org/jira/secure/attachment/12632870/neighbor-log.zip,05/Mar/14 18:38;Andie78;root-log.zip;https://issues.apache.org/jira/secure/attachment/12632869/root-log.zip,07/Nov/13 15:26;Andie78;screenshot-1.jpg;https://issues.apache.org/jira/secure/attachment/12612628/screenshot-1.jpg,07/Nov/13 15:37;Andie78;system.log;https://issues.apache.org/jira/secure/attachment/12612629/system.log,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2013-11-19 03:35:41.681,,,no_permission,,,,,,,,,,,,356426,,,Wed Oct 22 16:02:58 UTC 2014,,,,,,0|i1pg1r:,356714,2.0.2,2.0.3,2.1 rc3,,,,,jbellis,jbellis,,,2.0.1,,,,,,,07/Nov/13 15:26;Andie78;nodetool repair <ks> -pr causes neighbour-node to crash. Win7 file problem on neighbour node as well (screenshot-1.jpg). See following comment.,"07/Nov/13 15:37;Andie78;system.log after nodetool repair at neighbour (snipet):
{panel:title=system.log}
""FSWriteError ... Caused by: java.nio.file.FileSystemException: D:\Programme\cassandra\data\nieste\timezones\snapshots\dac98330-47bc-11e3-b167-eb1c24a59bb8\nieste-timezones-jb-8-Index.db: Der Prozess kann nicht auf die Datei zugreifen, da sie von einem anderen Prozess verwendet wird.""
{panel}
Means, cassandra didn't close the file after last access. This Win7-file-access issue seems to affect important areas in cassandra. Is there a plan to fix it? Thx.","12/Nov/13 15:09;Andie78;Another relying critical issue: Cassandra can't delete cache files.
{panel:title=system.log}
""WARN [CompactionExecutor:53] 2013-11-10 21:41:59,862 AutoSavingCache.java (line 277) Failed to delete D:\Programme\cassandra\saved_caches\system-schema_columns-KeyCache-b.db""

That leads to a crash on next start, system.log:
""ERROR [main] 2013-11-12 11:02:43,654 CassandraDaemon.java (line 478) Exception encountered during startup
java.lang.OutOfMemoryError: Java heap space
	at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:394)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:355)
	at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:352)
	at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:119)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:264)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:409)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:386)
	at org.apache.cassandra.db.index.AbstractSimplePerColumnSecondaryIndex.init(AbstractSimplePerColumnSecondaryIndex.java:52)
	at org.apache.cassandra.db.index.SecondaryIndexManager.addIndexedColumn(SecondaryIndexManager.java:274)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:409)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:381)
	at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:314)
	at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:268)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:110)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:88)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:274)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:461)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:504)""
{panel}",14/Nov/13 16:13;Andie78;I have no Jira response yet. Do I need to put an assignee or somebody will join up? Is there a plan to fix this in 2.0.3 ? Thx for any information!,"19/Nov/13 03:35;mishail;It could be related to http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4831749, which means the only way to ""fix"" it is to set {{disk_access_mode: standard}}","19/Nov/13 04:13;jbellis;That could be, but we try to munmap before deleting.  Is there a bug there?",19/Nov/13 11:29;Andie78;Thx for reply. I  tried already disk_access_mode standard and mmap. Same result. Maybe WinXP and Linux don't care about closing files after reading but Win7?,"19/Nov/13 20:36;mishail;[~Andie78] Do you you see ""can't delete"" exceptions only of files like {{\snapshots\dac98330-47bc-11e3-b167-eb1c24a59bb8\}} (i.e. from a snapshots subfolder) or with other files as well?

I suspect that CASSANDRA-4050 causes {{doValidationCompaction}} to fail (and entire RepairSession as well). - as far as I understand REPAIR without ""parallel"" option (which is default  behavior) creates a snapshot, validates and then deletes the snapshot. The last step fails on WIndows",19/Nov/13 21:27;mishail;It can be related to CASSANDRA-6275 as well,"20/Nov/13 10:42;Andie78;I see ""can't delete"" in regular compaction as well. It happens, when I make a lot of changes to a CF and Cassandra crashes. See my first comment. Without programming that finalizer, which calls ""deallocate()"", I couldn't use cassandra. I think in general, the file handling has to be programmed more accurately like my finalizer shows. Means closing all files direct after access. What about resource objects in java, which are responsible for closing files latest on GC?",20/Nov/13 16:31;jre;There's a patch in CASSANDRA-6275 that should resolve the issue you're seeing.  Can you test it?,"20/Nov/13 20:22;mishail;[~Andie78] regarding your 1st comment
bq. nodetool repair <ks> -pr causes neighbour-node to crash

here is how I see it:

* Your nodes are running with {{disk_failure_policy=stop}}
* You started a repair on a node
* Repair tasks do a ""validation"" and creates a snapshot of a column familly
* After ""validation"" the repair tasks tries to clean up the snapshot and fails.
* The fail is a ""disk failure"" and causes the node to stop. (because {{disk_failure_policy=stop}} )

Do you still observe the problem with you GC-patch or with the patch for CASSANDRA-6275?","21/Nov/13 11:52;Andie78;I will test ""2.0.3-tentative"".","21/Nov/13 13:23;Andie78;I'm actuall importing logfiles (Heavy writing into one KS, Heavy changing into other KS (states of imported files)). The result with the pach for CASSANDRA-6275:

- After 100 imported logfiles no Delete-Error in System.log :-)
- After some delay old files are deleted. :-)

nodetool repair I didn't test yet.

Would like to test my (heavy) import over weekend. I will update my whole cluster to 2.0.3-tentative and set up nodetool repair jobs. I will inspect all system-log files and tell the result here.","22/Nov/13 12:06;Andie78;During my importing I got the warnings:
{panel:title=system.log}
 WARN [CompactionExecutor:13] 2013-11-22 03:10:49,864 AutoSavingCache.java (line 277) Failed to delete D:\Programme\cassandra\saved_caches\nieste-nfiles-KeyCache-b.db
{panel}
and other cache files too. Later the same:
{panel:title=system.log}
 WARN [CompactionExecutor:22] 2013-11-22 07:10:49,896 AutoSavingCache.java (line 277) Failed to delete D:\Programme\cassandra\saved_caches\nieste-nfiles-KeyCache-b.db
{panel}","22/Nov/13 17:30;Andie78;I deployed now 2.0.3-tentative on the whole cluster. Result: With nodetool repair patch for CASSANDRA-6275 doen't work. Neighbour nodes crash again with disk_failure_policy=stop.
{panel:title=system.log}
ERROR [ValidationExecutor:3] 2013-11-22 18:21:49,591 FileUtils.java (line 417) Stopping gossiper
 WARN [ValidationExecutor:3] 2013-11-22 18:21:49,591 StorageService.java (line 279) Stopping gossip by operator request
ERROR [ValidationExecutor:4] 2013-11-22 18:21:50,361 Validator.java (line 242) Failed creating a merkle tree for [repair #923a7360-539a-11e3-8fde-eb1c24a59bb8 on nieste/evrangesdevice, (-787066926799647148,-773294852829911898]], /10.9.9.240 (see log for details)
ERROR [ValidationExecutor:4] 2013-11-22 18:21:50,371 CassandraDaemon.java (line 187) Exception in thread Thread[ValidationExecutor:4,1,main]
FSWriteError in D:\Programme\cassandra\data\nieste\evrangesdevice\snapshots\923a7360-539a-11e3-8fde-eb1c24a59bb8\nieste-evrangesdevice-jb-9-Index.db
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:120)
	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:382)
	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:378)
	at org.apache.cassandra.db.Directories.clearSnapshot(Directories.java:416)
	at org.apache.cassandra.db.ColumnFamilyStore.clearSnapshot(ColumnFamilyStore.java:1801)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:810)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
	at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.nio.file.FileSystemException: D:\Programme\cassandra\data\nieste\evrangesdevice\snapshots\923a7360-539a-11e3-8fde-eb1c24a59bb8\nieste-evrangesdevice-jb-9-Index.db: Der Prozess kann nicht auf die Datei zugreifen, da sie von einem anderen Prozess verwendet wird.

	at sun.nio.fs.WindowsException.translateToIOException(Unknown Source)
	at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
	at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(Unknown Source)
	at sun.nio.fs.AbstractFileSystemProvider.delete(Unknown Source)
	at java.nio.file.Files.delete(Unknown Source)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:116)
	... 11 more
ERROR [ValidationExecutor:4] 2013-11-22 18:21:50,371 FileUtils.java (line 417) Stopping gossiper
 WARN [ValidationExecutor:4] 2013-11-22 18:21:50,371 StorageService.java (line 279) Stopping gossip by operator request
ERROR [ValidationExecutor:2] 2013-11-22 18:21:51,221 FileUtils.java (line 423) Stopping RPC server
ERROR [ValidationExecutor:2] 2013-11-22 18:21:51,221 FileUtils.java (line 429) Stopping native transport
{panel}","22/Nov/13 17:56;Andie78;Actual I have only 4 nodes. But I want to enlarge my cluster with different racks (rooms with regular Desktop-PCs in different buildings in the same area). For that, I need to switch to NetworkTopologySnitch as well. Is repair with disk_failure_policy=ignore effective in that case? What can I do in my production cluster? Avoid repairing or repair and just ignore the delete-errors?","22/Nov/13 19:18;mishail;bq. Avoid repairing or repair and just ignore the delete-errors?
That doesn't look like a viable approach. I would try to run repairs with {{-par}} option. If I read the code correctly then a snapshots will not be used in this case.

{code}
- name: repair [keyspace] [cfnames]
    help: |
      Repair one or more column families
         Use -pr to repair only the first range returned by the partitioner.
         Use -par to carry out a parallel repair.
{code}

But to be honest - I really don't know what are other differences between a sequential and parallel repair ","22/Nov/13 19:37;mishail;[~Andie78] BTW, did you try to run a repair with your finalizer-patch? CASSANDRA-6275 fixed one leak and there might be others",22/Nov/13 21:57;Andie78;Not yet. Just read some of cassandra source and Im not familiar with cassandra at all. That finalizer patch is only emergency and doesnt fix the source of the problem (lost file handels).,"22/Nov/13 22:10;Andie78;btw I read somewhere, in java it is not a nice program style to write finalizers (but in C++ destrutors are common, for example used in resource objects, which automatic always close resources). whats your opinion about finalizers in java?","23/Nov/13 03:08;graham sanderson;For what it is worth, if you think there are other file leaks, this is the change I made locally to determine the allocation stack trace for CASSANDRA-6275 ... obviously from there you have some work to do.","23/Nov/13 03:11;graham sanderson; But yes finalizers in java are a very bad idea, and not like destructors in any realistic way - so better to determine the source of your leak (maybe my attached patch will help, but don't keep that around when you are done - exception stack traces are very expensive) and fix that. Of course if the finalizer enables you to have a functioning cluster, then that is a good step, but it wouldn't be the correct fix in the codebase.","28/Nov/13 12:29;graham sanderson;I am confused, did CASSANDRA-6275 make things worse? If not, or if in any way you still need the finalizer change to fix the problem, then apply my leakdetect.patch, and search logs for LEAK and post here. It has the same finalizer behavior it just logs where the RARs that have to be be cleaned up by finalizer were allocated.","29/Nov/13 10:01;Andie78;{panel:title=Short brief}
- No compaction problems anymore (CASSANDRA-6275 fixed it),
- Still cache-deleting problem (See my comment at 22/Nov/13 13:06),
- Still repair problem when using snapshot-files (without -par option) (see comment of Mikhail Stepura at 20/Nov/13 21:22).
{panel}
I applied the leakdetect.patch. I will comment the results.","29/Nov/13 11:31;Andie78;I think I can cancel repair without -par. After repairing one keyspace I got al lot of errors but not the leak-detect-messages. I think, not RAR is responsible during repair. The error on the Validation Node:
{panel:title=system.log}
ERROR [ValidationExecutor:1] 2013-11-29 12:15:38,370 Validator.java (line 242) Failed creating a merkle tree for [repair #92b6ccb0-58e7-11e3-aac3-b13a5fe180aa on nieste/niesteplants, (-5215786285174483271,-5206407297765302700]], /10.6.8.78 (see log for details)
ERROR [ValidationExecutor:1] 2013-11-29 12:15:38,370 CassandraDaemon.java (line 187) Exception in thread Thread[ValidationExecutor:1,1,main]
FSWriteError in D:\Programme\cassandra\data\nieste\niesteplants\snapshots\92b6ccb0-58e7-11e3-aac3-b13a5fe180aa\nieste-niesteplants-jb-19-Data.db
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:120)
	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:382)
	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:378)
	at org.apache.cassandra.db.Directories.clearSnapshot(Directories.java:416)
	at org.apache.cassandra.db.ColumnFamilyStore.clearSnapshot(ColumnFamilyStore.java:1801)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:810)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
	at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.nio.file.FileSystemException: D:\Programme\cassandra\data\nieste\niesteplants\snapshots\92b6ccb0-58e7-11e3-aac3-b13a5fe180aa\nieste-niesteplants-jb-19-Data.db: Der Prozess kann nicht auf die Datei zugreifen, da sie von einem anderen Prozess verwendet wird.

	at sun.nio.fs.WindowsException.translateToIOException(Unknown Source)
	at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
	at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(Unknown Source)
	at sun.nio.fs.AbstractFileSystemProvider.delete(Unknown Source)
	at java.nio.file.Files.delete(Unknown Source)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:116)
	... 11 more
{panel}
The error on the repair-node:
{panel:title=system.log}
ERROR [AntiEntropySessions:1] 2013-11-29 12:15:38,419 RepairSession.java (line 278) [repair #92b6ccb0-58e7-11e3-aac3-b13a5fe180aa] session completed with the following error
org.apache.cassandra.exceptions.RepairException: [repair #92b6ccb0-58e7-11e3-aac3-b13a5fe180aa on nieste/niesteplants, (-5215786285174483271,-5206407297765302700]] Validation failed in /10.9.9.69
	at org.apache.cassandra.repair.RepairSession.validationComplete(RepairSession.java:152)
	at org.apache.cassandra.service.ActiveRepairService.handleMessage(ActiveRepairService.java:188)
	at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:59)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:60)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
ERROR [AntiEntropySessions:1] 2013-11-29 12:15:38,429 CassandraDaemon.java (line 187) Exception in thread Thread[AntiEntropySessions:1,5,RMI Runtime]
java.lang.RuntimeException: org.apache.cassandra.exceptions.RepairException: [repair #92b6ccb0-58e7-11e3-aac3-b13a5fe180aa on nieste/niesteplants, (-5215786285174483271,-5206407297765302700]] Validation failed in /10.9.9.69
	at com.google.common.base.Throwables.propagate(Throwables.java:160)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: org.apache.cassandra.exceptions.RepairException: [repair #92b6ccb0-58e7-11e3-aac3-b13a5fe180aa on nieste/niesteplants, (-5215786285174483271,-5206407297765302700]] Validation failed in /10.9.9.69
	at org.apache.cassandra.repair.RepairSession.validationComplete(RepairSession.java:152)
	at org.apache.cassandra.service.ActiveRepairService.handleMessage(ActiveRepairService.java:188)
	at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:59)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:60)
	... 3 more
{panel}
{panel:title=NodeTool}
Starting NodeTool
[2013-11-29 12:15:37,329] Starting repair command #1, repairing 256 ranges for keyspace nieste
[2013-11-29 12:19:46,219] Repair session 92b6ccb0-58e7-11e3-aac3-b13a5fe180aa for range (-5215786285174483271,-5206407297765302700] failed with error org.apache.cassandra.exceptions.RepairException: [repair #92b6ccb0-58e7-11e3-aac3-b13a5fe180aa on nieste/niesteplants, (-5215786285174483271,-5206407297765302700]] Validation failed in /10.9.9.69
[2013-11-29 12:19:46,219] Repair session 935b9830-58e7-11e3-aac3-b13a5fe180aa for range (8782854129978293476,8784492934430401685] failed with error org.apache.cassandra.exceptions.RepairException: [repair #935b9830-58e7-11e3-aac3-b13a5fe180aa on nieste/niesteplants, (8782854129978293476,8784492934430401685]] Validation failed in /10.9.9.69
[2013-11-29 12:19:46,219] Repair session 93b725b0-58e7-11e3-aac3-b13a5fe180aa for range (-5799639987122737930,-5786898749415113092] failed with error org.apache.cassandra.exceptions.RepairException: [repair #93b725b0-58e7-11e3-aac3-b13a5fe180aa on nieste/nfiles, (-5799639987122737930,-5786898749415113092]] Validation failed in /10.9.9.240
[2013-11-29 12:19:46,229] Repair session 94b16430-58e7-11e3-aac3-b13a5fe180aa for range (2664807773952357126,2669403005419855407] failed with error org.apache.cassandra.exceptions.RepairException: [repair #94b16430-58e7-11e3-aac3-b13a5fe180aa on nieste/nfiles, (2664807773952357126,2669403005419855407]] Validation failed in /10.9.9.240
[2013-11-29 12:19:46,229] Repair session 9503c9f0-58e7-11e3-aac3-b13a5fe180aa for range (-6420115574437655437,-6410524043851626540] finished
{panel}
Im curious, if cache-file-deleting will write leak-messages...
I'm using C* 2.0.3-release with leak-detection.patch.","02/Dec/13 12:01;Andie78;Over weekend I got the following:
{panel:title=system.log}
ERROR [STREAM-IN-/10.6.8.78] 2013-11-29 17:57:20,266 StreamSession.java (line 410) [Stream #ea9bd2c0-5916-11e3-a5b5-b13a5fe180aa] Streaming error occurred
java.lang.RuntimeException: Outgoing stream handler has been closed
	at org.apache.cassandra.streaming.ConnectionHandler.sendMessage(ConnectionHandler.java:175)
	at org.apache.cassandra.streaming.StreamSession.prepare(StreamSession.java:436)
	at org.apache.cassandra.streaming.StreamSession.messageReceived(StreamSession.java:358)
	at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:293)
	at java.lang.Thread.run(Unknown Source)
 WARN [STREAM-IN-/10.6.8.78] 2013-11-29 17:57:20,266 StreamResultFuture.java (line 210) [Stream #ea9bd2c0-5916-11e3-a5b5-b13a5fe180aa] Stream failed
ERROR [NonPeriodicTasks:1] 2013-11-29 17:58:54,266 SSTableDeletingTask.java (line 81) Unable to delete D:\Programme\cassandra\data\system\schema_columns\system-schema_columns-jb-468-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR [NonPeriodicTasks:1] 2013-11-29 17:58:54,266 SSTableDeletingTask.java (line 81) Unable to delete D:\Programme\cassandra\data\system\schema_columns\system-schema_columns-jb-469-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR [NonPeriodicTasks:1] 2013-11-29 17:58:58,446 SSTableDeletingTask.java (line 81) Unable to delete D:\Programme\cassandra\data\system\schema_columnfamilies\system-schema_columnfamilies-jb-525-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR [NonPeriodicTasks:1] 2013-11-29 17:58:58,446 SSTableDeletingTask.java (line 81) Unable to delete D:\Programme\cassandra\data\system\schema_columnfamilies\system-schema_columnfamilies-jb-527-Data.db (it will be removed on server restart; we'll also retry after GC)
{panel}
The deleting problem occurs only once. So my GC patch seems to work. But there's no log of the leakdetect-patch.
{panel:title=system.log 10.6.8.78}
ERROR [CompactionExecutor:40] 2013-11-29 17:56:16,368 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:40,1,main]
java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:61)
	at java.util.concurrent.ThreadPoolExecutor.reject(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.execute(Unknown Source)
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.execute(DebuggableThreadPoolExecutor.java:145)
	at java.util.concurrent.AbstractExecutorService.submit(Unknown Source)
	at org.apache.cassandra.db.ColumnFamilyStore.switchMemtable(ColumnFamilyStore.java:752)
	at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:817)
	at org.apache.cassandra.db.SystemKeyspace.forceBlockingFlush(SystemKeyspace.java:420)
	at org.apache.cassandra.db.SystemKeyspace.finishCompaction(SystemKeyspace.java:197)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:225)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
{panel}","13/Jan/14 12:47;Andie78;Hello,
with 2.0.4 and leak detect patch I got expected errors
{panel:title=system.log}
ERROR [Finalizer] 2014-01-13 13:13:15,033 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-3418-Data.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:66)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
	at org.apache.cassandra.io.compress.CompressedThrottledReader.<init>(CompressedThrottledReader.java:34)
	at org.apache.cassandra.io.compress.CompressedThrottledReader.open(CompressedThrottledReader.java:48)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1355)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:67)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1161)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1173)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:244)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:250)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:126)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
ERROR [Finalizer] 2014-01-13 13:13:15,053 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-4984-Data.db allocated
...
ERROR [Finalizer] 2014-01-13 13:13:15,073 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-4984-Index.db allocated
...
ERROR [CompactionExecutor:2] 2014-01-13 13:13:15,073 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.IllegalArgumentException: bufferSize must be positive
...
ERROR [Finalizer] 2014-01-13 13:13:15,083 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-5067-Data.db allocated
...
ERROR [Finalizer] 2014-01-13 13:13:15,123 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-5067-Index.db allocated
...
ERROR [Finalizer] 2014-01-13 13:13:15,133 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-4302-Data.db allocated
...
ERROR [Finalizer] 2014-01-13 13:13:15,153 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-4302-Index.db allocated
...
ERROR [Finalizer] 2014-01-13 13:13:15,163 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-4981-Data.db allocated
...
ERROR [Finalizer] 2014-01-13 13:13:15,173 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-4981-Index.db allocated
...
ERROR [Finalizer] 2014-01-13 13:13:15,183 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-5068-Data.db allocated
...
ERROR [Finalizer] 2014-01-13 13:13:15,193 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-5068-Index.db allocated
...
ERROR [Finalizer] 2014-01-13 13:13:15,193 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-4959-Data.db allocated
...
ERROR [Finalizer] 2014-01-13 13:13:15,203 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-4959-Index.db allocated
...
ERROR [CompactionExecutor:1] 2014-01-13 13:13:15,223 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.IllegalArgumentException: bufferSize must be positive
...
ERROR [Finalizer] 2014-01-13 13:13:15,243 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-4991-Index.db allocated
	...
ERROR [Finalizer] 2014-01-13 13:13:15,243 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-5064-Data.db allocated
	...
ERROR [Finalizer] 2014-01-13 13:13:15,253 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-5064-Index.db allocated
	...
ERROR [Finalizer] 2014-01-13 13:13:15,263 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-5062-Data.db allocated
	...
ERROR [Finalizer] 2014-01-13 13:13:15,263 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-5062-Index.db allocated
	...
ERROR [Finalizer] 2014-01-13 13:13:15,273 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-5066-Data.db allocated
...
ERROR [Finalizer] 2014-01-13 13:14:24,334 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\KSlogdata\CFlogdata\KSlogdata-CFlogdata-jb-26894-Data.db allocated
	...
ERROR [Finalizer] 2014-01-13 13:14:54,544 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\evrangesncom\events-evrangesncom-jb-349-Data.db allocated
	...
ERROR [Finalizer] 2014-01-13 13:14:54,554 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\evrangesdevice\events-evrangesdevice-jb-939-Data.db allocated
	...
ERROR [Finalizer] 2014-01-13 13:14:54,574 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbyplant\events-eventsbyplant-jb-795-Data.db allocated

and more...
{panel}","13/Jan/14 18:12;mishail;[~Andie78] Could you please post the full stack stack trace for this exception.
{quote}
ERROR [CompactionExecutor:2] 2014-01-13 13:13:15,073 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.IllegalArgumentException: bufferSize must be positive
...
{quote}","14/Jan/14 10:32;Andie78;Hello,

this problem is occurred, when node was not shutdown probably. As I know, that issue is known as CASSANDRA-6531. Here is the stack trace:
{panel:title=system.log}
ERROR [ReadStage:2385] 2014-01-14 10:57:11,875 CassandraDaemon.java (line 187) Exception in thread Thread[ReadStage:2385,5,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException: bufferSize must be positive
	at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:49)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:60)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: bufferSize must be positive
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:75)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:43)
	at org.apache.cassandra.io.util.CompressedPoolingSegmentedFile.createReader(CompressedPoolingSegmentedFile.java:48)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:39)
	at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1195)
	at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:57)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:65)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:42)
	at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:167)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:250)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1516)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1335)
	at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:245)
	at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:105)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1710)
	at org.apache.cassandra.db.index.composites.CompositesSearcher.search(CompositesSearcher.java:53)
	at org.apache.cassandra.db.index.SecondaryIndexManager.search(SecondaryIndexManager.java:537)
	at org.apache.cassandra.db.ColumnFamilyStore.search(ColumnFamilyStore.java:1698)
	at org.apache.cassandra.db.RangeSliceCommand.executeLocally(RangeSliceCommand.java:135)
	at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:39)
	... 4 more
ERROR [Finalizer] 2014-01-14 10:57:12,005 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\nieste\niesteinverters\nieste-niesteinverters-jb-2669-Data.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:66)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:43)
	at org.apache.cassandra.io.util.CompressedPoolingSegmentedFile.createReader(CompressedPoolingSegmentedFile.java:48)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:39)
	at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1195)
	at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:57)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:65)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:42)
	at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:167)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:250)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1516)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1335)
	at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:245)
	at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:105)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1710)
	at org.apache.cassandra.db.index.composites.CompositesSearcher.search(CompositesSearcher.java:53)
	at org.apache.cassandra.db.index.SecondaryIndexManager.search(SecondaryIndexManager.java:537)
	at org.apache.cassandra.db.ColumnFamilyStore.search(ColumnFamilyStore.java:1698)
	at org.apache.cassandra.db.RangeSliceCommand.executeLocally(RangeSliceCommand.java:135)
	at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:39)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:60)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
ERROR [CompactionExecutor:446] 2014-01-14 11:02:57,342 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:446,1,main]
java.lang.IllegalArgumentException: bufferSize must be positive
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:75)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
	at org.apache.cassandra.io.compress.CompressedThrottledReader.<init>(CompressedThrottledReader.java:34)
	at org.apache.cassandra.io.compress.CompressedThrottledReader.open(CompressedThrottledReader.java:48)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1355)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:67)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1161)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1173)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:244)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:250)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:126)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
ERROR [Finalizer] 2014-01-14 11:02:57,552 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\KSlogdata\CFlogdata\KSlogdata-CFlogdata-jb-32763-Data.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:66)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
	at org.apache.cassandra.io.compress.CompressedThrottledReader.<init>(CompressedThrottledReader.java:34)
	at org.apache.cassandra.io.compress.CompressedThrottledReader.open(CompressedThrottledReader.java:48)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1355)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:67)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1161)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1173)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:244)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:250)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:126)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
{panel}","14/Jan/14 17:42;Andie78;Yesterday I updated one node with 2.0.4-rel incl. finalizer-patch (see results above). Nodetool repair -par caused node to repair ""endless"" and collecting about 65K Files in datafolder. I updated now to pre-2.0.5 from today (commit f6f50ddffe0821617fe29482f9ec918608560381). After starting, a lot of LEAK messages and File-Not-Found messages appeared in system.log. But files reduce.
{panel:title=system.log (pre-2.0.5)}
ERROR [SSTableBatchOpen:1] 2014-01-14 18:18:42,753 CassandraDaemon.java:139 - Exception in thread Thread[SSTableBatchOpen:1,5,main]
java.lang.RuntimeException: java.io.FileNotFoundException: D:\Programme\cassandra\data\KSlogdata\CFlogdata\KSlogdata-CFlogdata-jb-27051-Index.db (Das System kann die angegebene Datei nicht finden)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:109) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:97) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.sstable.SSTableReader.buildSummary(SSTableReader.java:595) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:575) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:527) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:328) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:230) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.sstable.SSTableReader$4.run(SSTableReader.java:364) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[na:1.7.0_25]
	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source) ~[na:1.7.0_25]
	at java.util.concurrent.FutureTask.run(Unknown Source) ~[na:1.7.0_25]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[na:1.7.0_25]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[na:1.7.0_25]
	at java.lang.Thread.run(Unknown Source) ~[na:1.7.0_25]
Caused by: java.io.FileNotFoundException: D:\Programme\cassandra\data\KSlogdata\CFlogdata\KSlogdata-CFlogdata-jb-27051-Index.db (Das System kann die angegebene Datei nicht finden)
	at java.io.RandomAccessFile.open(Native Method) ~[na:1.7.0_25]
	at java.io.RandomAccessFile.<init>(Unknown Source) ~[na:1.7.0_25]
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:63) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:105) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	... 13 common frames omitted
...
ERROR [Finalizer] 2014-01-14 18:27:45,076 RandomAccessReader.java:401 - LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\system\compactions_in_progress\system-compactions_in_progress-ka-5012-Statistics.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:65) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:105) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:97) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.sstable.metadata.MetadataSerializer.deserialize(MetadataSerializer.java:88) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.sstable.metadata.MetadataSerializer.deserialize(MetadataSerializer.java:98) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.sstable.SSTableReader.getApproximateKeyCount(SSTableReader.java:167) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:125) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:66) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:198) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[na:1.7.0_25]
	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source) ~[na:1.7.0_25]
	at java.util.concurrent.FutureTask.run(Unknown Source) ~[na:1.7.0_25]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[na:1.7.0_25]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[na:1.7.0_25]
	at java.lang.Thread.run(Unknown Source) ~[na:1.7.0_25]
...
ERROR [main] 2014-01-14 18:27:45,446 CassandraDaemon.java:435 - Exception encountered during startup
java.lang.NullPointerException: null
	at org.apache.cassandra.db.Directories.<init>(Directories.java:192) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:487) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:211) [apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:418) [apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:505) [apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
>>Cassandra shutted down<<
{panel}
I deleted files in folder ""\data\system\compactions_in_progress"". Now its not shutting down anymore. Still leak failures, no ""FileNotFound"" yet. ""bufferSize must be positive"" is still appearing (CASSANDRA-6531 seems not to work).","17/Feb/14 16:08;JoshuaMcKenzie;I've reproduced leaked file handles on repair in a lab on 2.0.5 w/leakfinalizer.patch.  Nodes start up without issue - I'm not seeing any LEAK or File-Not-Found on regular init, and the LEAK aren't showing up until repair kicks off.  W/the finalizer patch repair runs through to completion.  Andreas - have you had a chance to try out 2.0.5 w/the patch yet?

lastly - the leaks I'm seeing look like they're all isolated to a single case - streaming data outbound during the repair process:
{code:title=error|borderStyle=solid}
ERROR [Finalizer] 2014-02-17 09:21:52,922 RandomAccessReader.java (line 399) LEAK finalizer had to clean up
java.lang.Exception: RAR for C:\var\lib\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-jb-41-CRC.db allocated
        at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:66)
        at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:106)
        at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:98)
        at org.apache.cassandra.io.util.DataIntegrityMetadata$ChecksumValidator.<init>(DataIntegrityMetadata.java:53)
        at org.apache.cassandra.io.util.DataIntegrityMetadata.checksumValidator(DataIntegrityMetadata.java:40)
        at org.apache.cassandra.streaming.StreamWriter.write(StreamWriter.java:76)
        at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:59)
        at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:42)
        at org.apache.cassandra.streaming.messages.StreamMessage.serialize(StreamMessage.java:45)
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.sendMessage(ConnectionHandler.java:383)
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.run(ConnectionHandler.java:355)
        at java.lang.Thread.run(Thread.java:744)
{code}

Andreas - could you confirm whether or not this matches what you're seeing in your environment?  I'm curious if this is a dangling file handle like we've seen in other related tickets or if this is perhaps a race condition on access Windows is intolerant of.","17/Feb/14 19:45;Andie78;Actually Im still running 2.0.5 Snapshot with my finalizer patch. During normal operation no fault, but I didn't check the logs in the last 2 weeks yet. The repair jobs I start with -par option. I will update to 2.0.5-rel with my patch and test repair w/o -par option and compare the logs to yours. Correct my plan, if necessary.","17/Feb/14 23:58;JoshuaMcKenzie;With -par on the test.  Without -par I'm seeing the same errors you were seeing above indicating that snapshots have open handles:

{code:title=Non-par|borderStyle=solid}
ERROR [ValidationExecutor:3] 2014-02-17 17:52:57,092 Validator.java (line 242) Failed creating a merkle tree for [repair #99973cf0-982e-11e3-9370-639bcb1c8d6c on Keyspace1/Standard1, (-390084131511610885,-345083722760460
251]], /10.193.84.101 (see log for details)
ERROR [ValidationExecutor:3] 2014-02-17 17:52:57,092 CassandraDaemon.java (line 192) Exception in thread Thread[ValidationExecutor:3,1,main]
FSWriteError in \var\lib\cassandra\data\Keyspace1\Standard1\snapshots\99973cf0-982e-11e3-9370-639bcb1c8d6c\Keyspace1-Standard1-jb-37-Data.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:120)
        at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:382)
        at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:378)
        at org.apache.cassandra.db.Directories.clearSnapshot(Directories.java:416)
        at org.apache.cassandra.db.ColumnFamilyStore.clearSnapshot(ColumnFamilyStore.java:1881)
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:810)
        at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
        at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.nio.file.FileSystemException: \var\lib\cassandra\data\Keyspace1\Standard1\snapshots\99973cf0-982e-11e3-9370-639bcb1c8d6c\Keyspace1-Standard1-jb-37-Data.db: The process cannot access the file because it is
 being used by another process.
{code}

I want to confirm that repair w/-par on 2.0.5 doesn't give you much trouble, if any, and then tackle -par vs. non separately.","18/Feb/14 02:45;graham sanderson;Unless I'm missing something, the code at StreamWriter.java:76

{code}
            validator = DataIntegrityMetadata.checksumValidator(sstable.descriptor);
{code}

creates a file reference that is never closed, which seems like it is a bug on all platforms - of course it might not manifest in the same way (especially if multiple threads/timing are involved).","18/Feb/14 12:21;Andie78;I updated 2 of 8 nodes to 2.0.5-rel, with finalizer patch and LEAK-logging. Result: After Start a lot of LEAK-finalizer Errors like:
{panel:title=start-up node}
ERROR [Finalizer] 2014-02-18 12:53:42,388 RandomAccessReader.java (line 394) LEAK finalizer had to clean up 
ERROR [Finalizer] 2014-02-18 12:53:42,388 RandomAccessReader.java (line 394) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\system\schema_keyspaces\system-schema_keyspaces-jb-433-Data.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:63)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:55)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1362)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:67)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1147)
	at org.apache.cassandra.db.RowIteratorFactory.getIterator(RowIteratorFactory.java:69)
	at org.apache.cassandra.db.ColumnFamilyStore.getSequentialIterator(ColumnFamilyStore.java:1599)
	at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1718)
	at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1656)
	at org.apache.cassandra.db.SystemKeyspace.serializedSchema(SystemKeyspace.java:767)
	at org.apache.cassandra.db.DefsTables.loadFromKeyspace(DefsTables.java:121)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:525)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:242)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:462)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:552)
ERROR [Finalizer] 2014-02-18 12:53:42,388 RandomAccessReader.java (line 394) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\system\local\system-local-jb-168-Data.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:63)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:43)
	at org.apache.cassandra.io.util.CompressedPoolingSegmentedFile.createReader(CompressedPoolingSegmentedFile.java:48)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:39)
	at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1195)
	at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:57)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:65)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:42)
	at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:167)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:250)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1560)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1379)
	at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:327)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:65)
	at org.apache.cassandra.cql3.statements.SelectStatement.readLocally(SelectStatement.java:232)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:250)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:58)
	at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:255)
	at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:526)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:233)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:462)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:552)
{panel}
After Starting >>nodetool repair -par nieste<< I got again a lot of such messages (""nieste"" is my cql3 KS):
{panel:title=with -par}
ERROR [Finalizer] 2014-02-18 13:00:43,961 RandomAccessReader.java (line 394) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\nieste\nfiles\nieste-nfiles-jb-1150-Index.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:63)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:103)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:90)
	at org.apache.cassandra.io.util.BufferedPoolingSegmentedFile.createReader(BufferedPoolingSegmentedFile.java:45)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:39)
	at org.apache.cassandra.io.util.SegmentedFile$SegmentIterator.next(SegmentedFile.java:162)
	at org.apache.cassandra.io.util.SegmentedFile$SegmentIterator.next(SegmentedFile.java:143)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:936)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:871)
	at org.apache.cassandra.io.sstable.SSTableReader.getPositionsForRanges(SSTableReader.java:783)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1186)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1174)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:252)
	at org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionIterable.<init>(CompactionManager.java:888)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:787)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
	at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
ERROR [Finalizer] 2014-02-18 13:00:44,371 RandomAccessReader.java (line 394) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\nieste\niesteinverters\nieste-niesteinverters-jb-1492-Index.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:63)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:103)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:95)
	at org.apache.cassandra.io.sstable.SSTableReader.openIndexReader(SSTableReader.java:1369)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:97)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1190)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1174)
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getScanners(LeveledCompactionStrategy.java:194)
	at org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionIterable.<init>(CompactionManager.java:888)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:787)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
	at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
{panel}
On the neighbour-node I got the same errors:
{panel:title=neighbour}
ERROR [Finalizer] 2014-02-18 13:03:18,203 RandomAccessReader.java (line 394) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\nieste\niesteinverters\nieste-niesteinverters-jb-3271-Data.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:63)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
	at org.apache.cassandra.io.compress.CompressedThrottledReader.<init>(CompressedThrottledReader.java:34)
	at org.apache.cassandra.io.compress.CompressedThrottledReader.open(CompressedThrottledReader.java:48)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1355)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:96)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1190)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1174)
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getScanners(LeveledCompactionStrategy.java:194)
	at org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionIterable.<init>(CompactionManager.java:888)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:787)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
	at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
ERROR [Finalizer] 2014-02-18 13:03:27,543 RandomAccessReader.java (line 394) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\nieste\niesteplants\nieste-niesteplants-jb-7-Index.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:63)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:103)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:90)
	at org.apache.cassandra.io.util.BufferedPoolingSegmentedFile.createReader(BufferedPoolingSegmentedFile.java:45)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:39)
	at org.apache.cassandra.io.util.SegmentedFile$SegmentIterator.next(SegmentedFile.java:162)
	at org.apache.cassandra.io.util.SegmentedFile$SegmentIterator.next(SegmentedFile.java:143)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:936)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:871)
	at org.apache.cassandra.io.sstable.SSTableReader.getPositionsForRanges(SSTableReader.java:783)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1186)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1174)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:252)
	at org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionIterable.<init>(CompactionManager.java:888)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:787)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
	at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
{panel}
Repair succeed. So only lost filehandles. The strings ""delete"" or ""FSWriteError"" don't appear in any of these logs (w/-par).",18/Feb/14 15:16;JoshuaMcKenzie;Thanks for checking that Andreas - I'll move forward with the assumption that what I've reproduced in the lab is just a small subset of the problems you're seeing in prod and go off your logs as reference.,18/Feb/14 17:05;Andie78;You're welcome! :-),"18/Feb/14 21:30;JoshuaMcKenzie;A couple of thoughts - linux allows you to delete a file even though another process has a handle to it, keeping a reference in the /proc filesystem (http://archive09.linux.com/articles/58142) until application shutdown.  I'm thinking it's possible that on linux FileUtils.deleteWithConfirm is deleting a file that another thread has a handle to and we're none the wiser.

Commenting out the thrown exception in that function removes all LEAK messages and other errors for me when running nodetool.bat repair without -par, and after a run I have snapshot files laying around that can't be deleted but don't show up in the handle search in process explorer.  Stopping java makes them deletable so it's clearly a JVM handle thing...","19/Feb/14 11:42;Andie78;I think, not stopping java in general allows deleting, but stopping the cassandra process which keeps file handles open. When I first studied the C* code, I found it difficult to see, how filehandles are spread around in different classes (?). Not easy, to keep the overview for me, but maybe its because of not enough java experience compared to C++ where I had to care for handles as well (file, memory). Later in C++, there were autopointers (boost/TR1) with a finalizer-like approach, where memory was deleted automatically (analog to closing files), when handle went out of scope or the programmer ""forgot"" to free/close. I don't know about the linux' process explorer, where filehandles are expected. But if I probably close every filehandle after use (even only reading), does it matter, what's in this process explorer, as long C* is able to delete? BTW if only a read process don't close the file handle, it makes already sense for me to forbid to delete, as long the os knows, the file is still used any way, except the user has to kill or restart the process, where the handles automatically disappear, mostly caused by problems. For me, it's a failure of the os as manager, to allow delete a file, as long there are any filehandles on it. This strict permission allows me as well, to detect failures in my program logic.","19/Feb/14 14:55;JoshuaMcKenzie;There's pros and cons to either side of the debate - there's some benefits to being able to change file handles while in use such as being able to run updates, file recovery, etc.  Apparently this is a throwback in OS design to MS-DOS 3.3 as far as file-level locking is concerned.

Coming from a C++ background myself I know what you mean w/smart pointers and RAII-type resource management, but I believe the problem we're running into here is language independent.  We have atomic reference-counting implementations in the SSTableReaders to allow multiple concurrent read-only access to the data structures which would be a complicated implementation regardless of language choice.

I mentioned process explorer not seeing the file handle lock because I found it an oddity - my expectation is that anything from Sysinternals and Russinovich is bullet-proof, so I'm wondering how the OS got into a state where a file is locked yet I can't query the process that has said lock, even though stopping the JVM clearly released it.","19/Feb/14 21:54;JoshuaMcKenzie;Referencing Mikhail's earlier comment - this looks like the same problem from CASSANDRA-4050 where hard links can't be deleted on NTFS if a process has the file or another hard link locked.  Code inspection for snapshot process looks clean and that would explain the intermittent locking issue where some snapshots remove without issue and others fail.

Referencing http://superuser.com/questions/301303/one-hardlink-is-locked-how-do-i-remove-the-other and the comments in the other ticket - on Windows we'll need to account for gracefully failing file deletion on snapshot cleanup, move those hard links to a $CWD/tmp folder for instance, and then clean up that folder on process shutdown.","19/Feb/14 22:12;jbellis;So if we have a handle open to the original file still, we can't delete the snapshot hardlink either?","19/Feb/14 23:29;JoshuaMcKenzie;Looks like that's the case, though the documentation for DeleteFile (http://msdn.microsoft.com/en-us/library/windows/desktop/aa363915(v=vs.85).aspx) doesn't indicate anything about hard links to open files being non-deletable.  Every use case I'm digging up shows that that's the case however:

http://superuser.com/questions/678357/how-to-delete-windows-ntfs-hard-link-mklink-h-while-original-is-in-use

There's an ecosystem of tools surrounding working around this type of thing on Windows - PendMoves and MoveFile from sysinternals, for instance

We could go a route similar to FileUtils.closeQuietly with a FileUtils.deleteQuietly or removeHardLink to more clearly communicate intent, try to delete it, move it to a tmp location if failure, and then either periodically retry delete on the links or remove them at shutdown.","20/Feb/14 09:49;Andie78;So the repair issue (with snapshots) is another problem compared to the compaction issue, where my finalizer patch works perfectly in normal operation and shows, that handles are not closed from C* ? Relying to compaction issue, if C* generates a tmp folder, there will be thousands of files (I had for instance more than 64K files in one KS w/o finalizer patch). Is that a good idea for the operating system, when C* is running for weeks?","20/Feb/14 16:17;JoshuaMcKenzie;It's possible the two are the same underlying problem.  We use hardlinks both during the snapshot process and during flushing of memtables - I'll have to think for a bit on the best way to test that theory out since your stack shows us the LEAK being cleaned up but doesn't (and can't) give us any indication of who it is that might still hold the reference to the handle.

As for the tmp folder and file growth - the hard links won't be kept open in the FD table for java so we shouldn't run the risk of an FD limit, however the drive space would still be used until the process was shut down.  I was thinking the addition of a periodic ""delete tmp/moved_hard_links/*"" type pass should help mitigate that, though it smells hacky to me.

Another option would be copying files instead of hard linking them during these operations on Windows only, but I think the performance ramifications of that make that option unworkable.","20/Feb/14 23:16;JoshuaMcKenzie;As a final confirmation of this behavior I tested this locally.  I created and locked a file named test.txt (opened it), and then hard linked and attempted to delete it.

C:\Users\jmckenzie\Desktop\FileLocker>mklink /H link.txt test.txt
Hardlink created for link.txt <<===>> test.txt

C:\Users\jmckenzie\Desktop\FileLocker>del link.txt
C:\Users\jmckenzie\Desktop\FileLocker\link.txt
The process cannot access the file because it is being used by another process.

On Windows, if the original file is locked the hard link cannot be deleted even if it's not in use.

I'll start prototyping a sweep and delete model.  We have some unit tests that are failing on Windows with similar stacks to what Andreas is getting on startup so I'll look into those along with this.","21/Feb/14 10:18;Andie78;Sounds interesting, I didn't work with such links yet. Can somebody check that behavior on Windows XP (32) as well? Before migrating to Win7, we didn't have that problems. Our nodes run on Win7 (32 Bit).",21/Feb/14 12:30;jbellis;(Supporting XP is not a goal so it would be of academic interest only.),"21/Feb/14 12:40;Andie78;(Yep, but I thought, the difference can be the key to the solution :-) )","21/Feb/14 22:49;JoshuaMcKenzie;Regarding the LEAK on repair -par:  graham sanderson on the 17th appeared to be on the mark.  Adding FileUtils.closeQuietly(validator) in the finally block on StreamWriter.java plugged all LEAK messages from nodetool.bat repair -par.  I've started looking at the other leaks Andreas linked on here and the 1st I tracked down doesn't look like it should be leaking:
{code:title=LEAK message}
ERROR [Finalizer] 2014-02-18 12:53:42,388 RandomAccessReader.java (line 394) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\system\schema_keyspaces\system-schema_keyspaces-jb-433-Data.db allocated
at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:63)
at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:55)
at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1362)
at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:67)
at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1147)
at org.apache.cassandra.db.RowIteratorFactory.getIterator(RowIteratorFactory.java:69)
at org.apache.cassandra.db.ColumnFamilyStore.getSequentialIterator(ColumnFamilyStore.java:1599)
at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1718)
at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1656)
at org.apache.cassandra.db.SystemKeyspace.serializedSchema(SystemKeyspace.java:767)
at org.apache.cassandra.db.DefsTables.loadFromKeyspace(DefsTables.java:121)
at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:525)
at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:242)
at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:462)
at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:552)
{code}
Which should be closed at:
{code:title=ColumnFamilyStore.filter()}
...
        finally
        {
            try
            {
                rowIterator.close();
                Tracing.trace(""Scanned {} rows and matched {}"", total, matched);
            }
...
{code}

I'll look into the LEAK messages on some of the others you got Andreas and see if the code where the stack was captured looks like it leaks the handle.

Regarding the locked file handles on snapshots: It's not looking great for our options on hard links for the snashots on non-par repair.  On win7 I basically can't touch hard links to locked files which is different than the note mentioned on StackExchange:

{code:title=win7 move / delete}
C:\Users\jmckenzie\Desktop\FileLocker>mklink/H link.txt test.txt
Hardlink created for link.txt <<===>> test.txt
C:\Users\jmckenzie\Desktop\FileLocker>rename link.txt newlink.txt
The process cannot access the file because it is being used by another process.
C:\Users\jmckenzie\Desktop\FileLocker>move link.txt newlink.txt
The process cannot access the file because it is being used by another process.
        0 file(s) moved.
{code}

And as for behavior on winxp - it looks like this behavior was the same there as far as not being able to delete hard links to locked files:
{code:title=winxp delete hard link}
C:\Documents and Settings\jmckenzie\Desktop>fsutil hardlink create link.txt test.txt
Hardlink created for C:\Documents and Settings\jmckenzie\Desktop\link.txt <<===>
> C:\Documents and Settings\jmckenzie\Desktop\test.txt
C:\Documents and Settings\jmckenzie\Desktop>del link.txt
C:\Documents and Settings\jmckenzie\Desktop\link.txt
The process cannot access the file because it is being used by another process.
{code}

I believe this is why we have SSTableDeletingTask.java and the recurrent runs on CMS GC and rescheduling self.  We could follow the implementation pattern and have a SnapshotFileDeletingTask and register w/StorageService.tasks the same way we do on SSTableDeleting, but that doesn't solve the locked file handle move problems we have unfortunately.  It doesn't look like we use FileUtils.renameWithConfirm() in too many places in our code-base so it might be ok to document that as unsafe on Windows and deal with the other case.","25/Feb/14 20:53;JoshuaMcKenzie;With our current io implementation, snapshots won't be deletable as long as the original sstable is locked.  It's going to take a writing of a new FileDataInput based on FileChannel w/jdk7 using the FILE_SHARE_DELETE flag to allow deletion of hard links while the original file is open (thanks for the heads up on that Jonathan).

Bug with behavior: http://bugs.java.com/view_bug.do?bug_id=6607535
jdk7 support: http://www.docjar.com/html/api/sun/nio/fs/WindowsChannelFactory.java.html

I'm attaching a patch to fix the repair -par leak and will work on tracking down some of those startup leaks as I'm seeing those as well.",26/Feb/14 23:47;JoshuaMcKenzie;Andreas - do you have steps to reproduce the LEAK's traced to doValidationCompaction on 2.0.5 w/nodetool.bat repair -par <KS>?  The code surrounding that resource looks like it shouldn't have leaked the way your log indicates.,"27/Feb/14 15:56;JoshuaMcKenzie;Re: your other 2.0.5 leaks - A trace on the code surrounding loadFromKeyspace also looks clean, as does the one sourcing from cql3...readLocally.  Short of the JVM dying during a finally block, google's Cache not firing its removal event listener, or our MergeIterator close implementation leaking things - tag 2.0.5 looks like it shouldn't be producing these LEAKS.

If we had file handle leaks in our core select statements in CQL3 and Keyspace initialization I'd expect to be seeing them in local testing.","05/Mar/14 12:57;Andie78;Hello,
since I don't know all code areas of C*, I describe, what I tested to reproduce: I cleaned system.log and used again C* 2.0.5-rel with LEAK detection and finalizer-patch in RAR.java. After starting again C* w/o doing anything I got a lot LEAK messages. I waited until C* finished his own work (mainly compacting I think). Now I started repair -par. Result are a lot of LEAK messages. Here the first one:

{panel:title=nodetool repair -par events}
ERROR [Finalizer] 2014-03-05 13:45:25,932 RandomAccessReader.java (line 394) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbyproject\events-eventsbyproject-jb-2002-Index.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:63)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:103)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:90)
	at org.apache.cassandra.io.util.BufferedPoolingSegmentedFile.createReader(BufferedPoolingSegmentedFile.java:45)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:39)
	at org.apache.cassandra.io.util.SegmentedFile$SegmentIterator.next(SegmentedFile.java:162)
	at org.apache.cassandra.io.util.SegmentedFile$SegmentIterator.next(SegmentedFile.java:143)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:936)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:871)
	at org.apache.cassandra.io.sstable.SSTableReader.getPositionsForRanges(SSTableReader.java:783)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1186)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1174)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:252)
	at org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionIterable.<init>(CompactionManager.java:888)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:787)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
	at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
{panel}
{panel:title=neighbor node}
ERROR [Finalizer] 2014-03-05 13:50:54,061 RandomAccessReader.java (line 394) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\evrangesdevice\events-evrangesdevice-jb-905-Index.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:63)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:103)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:90)
	at org.apache.cassandra.io.util.BufferedPoolingSegmentedFile.createReader(BufferedPoolingSegmentedFile.java:45)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:39)
	at org.apache.cassandra.io.util.SegmentedFile$SegmentIterator.next(SegmentedFile.java:162)
	at org.apache.cassandra.io.util.SegmentedFile$SegmentIterator.next(SegmentedFile.java:143)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:936)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:871)
	at org.apache.cassandra.io.sstable.SSTableReader.getPositionsForRanges(SSTableReader.java:788)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1186)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1174)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:252)
	at org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionIterable.<init>(CompactionManager.java:888)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:787)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
	at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
{panel}
Repair successfully finished. If I can make more tests, let me know. After Thursday I will be on holiday for 3 weeks and in office again at Mon, 03/31/2014.",05/Mar/14 15:01;JoshuaMcKenzie;Could you attach the most recent system.log from the root and neighbor nodes to this ticket?  Might help see if there's anything else going on there in the environment involved in this.,05/Mar/14 18:38;Andie78;I attached logs (root-log.zip and neighbor-log.zip) during >nodetool repair -par events<. C* 2.0.5-rel with LEAK-log and finalizer-patch under Win-7.,"10/Mar/14 18:54;JoshuaMcKenzie;I've created CASSANDRA-6832 regarding the leak on repair -par to get a patch into the code-base for it without being blocked on this.

Andreas - those logs have 100+megs of the same stack traces repeatedly which unfortunately doesn't help me much.  I was hoping to see some information about process start-up in there.  Do you have a small dataset I could use to attempt to reproduce the errors you're seeing?  Alternatively - if you start up a clean test cluster on 2.0.5 in your environment do you see those errors?","04/Apr/14 22:02;JoshuaMcKenzie;On CASSANDRA-4050 we're converting our RandomAccessReader to use nio which should fix the ""can't delete a hard-link while original file is open"" for most use-cases.  Unfortunately you cannot delete hard-linked files on Windows if you have a memory-mapped segment in the original file - I've done some benchmarking on CASSANDRA-6890 regarding removing memory mapped I/O and the performance cost / feature loss is high enough that we're going to keep it for now.

I'll put together a patch for this ticket to create something similar to an SSTableDeletingTask for a snapshot folder - walk the files and try to delete them, re-scheduling a job to try and clear this folder again after a GC if there's any failures due to access violations.  That combined with CASSANDRA-4050 should give us immediate and full clear on compressed cf's and partial / incrementally improving snapshot clearing on snapshots where there's memory mapped readers to the original sstables.

I don't like having partially cleared out snapshots floating around on the file-system though.  I'd guess this will cause some confusion for people in the future.","07/Apr/14 13:12;Andie78;That means in short words, deletion during normal operation should work? And nodetool compact w/o -par should work as well? Tell me, if it's ready to test in my environment.","07/Apr/14 15:18;JoshuaMcKenzie;Deletion during normal operation will fail gracefully and schedule retry if the original file is open.  This combined with CASSANDRA-4050 on compressed CF's should immediately delete the snapshot files and work on repair, otherwise the snapshots will hang around on disk until the original readers are closed / unmapped and sstables compacted.

Not ready to test yet - I'll post a patch when it's ready.","07/Apr/14 21:07;jbellis;bq. That combined with CASSANDRA-4050 should give us immediate and full clear on compressed cf's and partial / incrementally improving snapshot clearing on snapshots where there's memory mapped readers to the original sstables.

I'd rather disallow mmaped i/o on Windows entirely pending making buffered i/o speed-competitive with mmap (at which point we can drop mmap entirely).

-1 on ""snapshot deletion scheduler"" band aids.","07/Apr/14 21:43;JoshuaMcKenzie;I originally got the idea from the SSTableDeletingTask implementation (as mentioned prior) but that works too.

As of CASSANDRA-6907 we currently have snapshot-based repair disabled for Windows in 2.0.7+, so do we even need to disable the mmap'ed I/O path on Windows prior to optimizing buffered I/O and removing mmap as an option?","07/Apr/14 21:55;jbellis;I'd love to get rid of SSTDT so at least I'm consistent. :)

Pretty sure we always use mmap on index files, so we'd need to fix that, and we'd also want to make sure we use buffered i/o even on uncompressed tables.","07/Apr/14 22:13;JoshuaMcKenzie;Erring on the side of less complexity is never a bad thing in my book

To be safe leaving mmap in we'd have to disable snapshot deletion via nodetool to go along with removing it from repair which smells super-hacky - I'm on board (edit: with disabling mmap on Windows).  I'll create a ticket for removing mmap on index files and disabling mmap'ed I/O on Windows.  We reference 4050 and that from here and I think this ticket's done.  ","01/May/14 00:11;JoshuaMcKenzie;Reference CASSANDRA-6993 and CASSANDRA-4050 for patches, CASSANDRA-6890 for performance #'s.","14/Jul/14 09:34;bamboo82;Similar issue reappeared in Windows 2012, but fine in Windows 2008 R2. Could someone confirm it?   It seemed to be related to clearSnapshot during Repair phrase
Running 2.0.9

ERROR [ValidationExecutor:3] 2014-07-13 17:24:28,631 CassandraDaemon.java (line 199) Exception in thread Thread[ValidationExecutor:3,1,main]
FSWriteError in \cassandra\1\data\Commenter3\PostSource_Index\snapshots\3796e3c0-0aed-11e4-8d77-1f5ea6ee0a78\Commenter3-PostSource_Index-ic-21-Index.db
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:122)
	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:384)
	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:380)
	at org.apache.cassandra.db.Directories.clearSnapshot(Directories.java:488)
	at org.apache.cassandra.db.ColumnFamilyStore.clearSnapshot(ColumnFamilyStore.java:1877)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:811)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:63)
	at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:398)
	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.nio.file.FileSystemException: \cassandra\1\data\Commenter3\PostSource_Index\snapshots\3796e3c0-0aed-11e4-8d77-1f5ea6ee0a78\Commenter3-PostSource_Index-ic-21-Index.db: The process cannot access the file because it is being used by another process.

	at sun.nio.fs.WindowsException.translateToIOException(Unknown Source)
	at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
	at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(Unknown Source)
	at sun.nio.fs.AbstractFileSystemProvider.delete(Unknown Source)
	at java.nio.file.Files.delete(Unknown Source)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:118)
	... 12 more",14/Jul/14 15:30;JoshuaMcKenzie;Was that repair on a range of keys rather than a keyspace?,14/Jul/14 15:56;JoshuaMcKenzie;Created CASSANDRA-7541 to track this.,21/Oct/14 20:43;daniel.nuriyev;Any hope to have this fixed in the current version? Inability to delete unused files on windows is critical and leads to accumulation of gigabytes on the disk. Waiting for version 3.0 is too long.,"21/Oct/14 23:49;JoshuaMcKenzie;[~daniel.nuriyev] What unused files are you unable to delete on 2.1 / how are they created?  Other than manually created snapshots we should have snapshot-based operations (repair specifically) bypassed which is where these problems originate.

The changes in CASSANDRA-4050 that truly fix this issue are very low-level and too high risk to put into the 2.X branch unfortunately.","22/Oct/14 14:35;daniel.nuriyev;I had a bunch of these in the logs:
ERROR [NonPeriodicTasks:1] 2014-10-20 12:46:02,107 SSTableDeletingTask.java (line 81) Unable to delete var\lib\cassandra\data\system\schema_columnfamilies\system-schema_columnfamilies-jb-1-Data.db (it will be removed on server restart; we'll also retry after GC)

Which means that older files are not deleted after compacting into a bigger file.

Does the fix of this JIRA ticket cover this issue?
",22/Oct/14 16:02;JoshuaMcKenzie;A combination of CASSANDRA-4050 and CASSANDRA-6993 should resolve this issue.  The inability to delete sstables without node restart is a known issue in the 2.X line and will be resolved in 3.0.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong tracking of minLevel in Leveled Compaction Strategy causing serious performance problems,CASSANDRA-6284,12677083,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,JiriHorky,JiriHorky,JiriHorky,01/Nov/13 15:15,12/Mar/19 14:17,13/Mar/19 22:29,22/Nov/13 23:07,2.0.4,,,,,,0,lcs,,,,"Hi,

since version 2.0.0 (incl. beta), Leveled Compaction Strategy contains a hard-to-spot bug in choosing of sstable candidates to be compacted with tables in higher level. It always chooses first sstable in L1 and only first 1/10 of sstables in other higher levels.

This is caused by an error when determining ""minLevel"" of compacted tables in replace() function in LeveledManifest.java which is then used as an index to lastCompactedKeys array to ensure sort of ""round robin"" selection of SStables for compaction in each level. In the newer versions the minLevel is computed as the minimum of levels of newly created sstables instead of the old sstables. 
Typically compaction takes one table from L(X), compacts it with N tables in L(X+1) and produces M tables in L(X+1). Thus, the lastCompactedKey is improperly accounted to one level higher then it should be.

This causes serious performance problems as the uniform token range distribution across sstables in one level is broken.
In L1, the first SStable is always chosen to be compacted with overlapping tables in L2. Since a newly created tables in L0 contains practically whole range of keys of a given node, and the rest of ~9 tables in L1 are never pushed to the higher levels, they tend to contain higher and higher keys over time in very narrow token range. As a direct consequence, the first (the chosen) SStable in L1 (after a compaction of L1 tables with the L0 table) thus contains much wider range than anticipated ~1/10 , which forces compaction with many more tables in L2 than normally expected due to bigger overlap.
The similar problem appears in higher levels as well.

We noticed gradual performance degradation since we upgraded C* from 1.2.9 to 2.0.0 aprox. 1 month ago which we tracked down to increased compaction activity. We noticed that the number of sstables processed in one compaction is much higher than expected. The compaction IO activity in our case is more than 5 higher than in 1.2.9 version and only becomes worse.",,,,,,,,,,,,,,,,,,,,,,,,01/Nov/13 15:23;JiriHorky;LeveledManifest.bug.6284.patch;https://issues.apache.org/jira/secure/attachment/12611617/LeveledManifest.bug.6284.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-01 16:52:42.978,,,no_permission,,,,,,,,,,,,356459,,,Fri Nov 22 23:07:58 UTC 2013,,,,,,0|i1pg93:,356747,2.0.0,2.0.1,2.0.2,2.0 beta 1,2.0 beta 2,2.0 rc1,2.0 rc2,jbellis,jbellis,,,2.0 beta 1,,,,,,,01/Nov/13 15:21;JiriHorky;The attached patch fixes this bug. The patch applies to 2.0.0. as well as to 2.0.2.,01/Nov/13 16:52;jbellis;Patch LGTM.  Can you add a test case to LeveledCompactionStrategyTest?,"01/Nov/13 20:05;rcoli;Is 2.0.0 beta is the correct ""since"" for this ticket?",01/Nov/13 20:30;jbellis;Probably.  Git annotate if you're not sure.,"01/Nov/13 21:26;rcoli;For the record :

git annotate says CASSANDRA-4872 introduces the MAX_LEVEL line fixed in the second part of the patch.

So ""since"" 2.0.1 beta 1 is correct. :)","04/Nov/13 09:56;JiriHorky;Regarding adding the test case - I think that the right test should  insert enough data to have something in L3, and then try to go through all tables in L1 and L2 and see 1) whether all sstables contain more or less the right token range (1/10 for L1 and 1/100 for L2) with some (possibly relatively high) tolerance and 2) that both levels effectively contain the whole token range the node is responsible for. Unfortunately, I won't be able to contribute that sooner than in a week and can't really promise that it will be good enough afterwards (I am not a Java programmer :-)",22/Nov/13 23:07;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossiper blocks when updating tokens and turns node down,CASSANDRA-6297,12677445,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,sbtourist,sbtourist,04/Nov/13 20:30,12/Mar/19 14:17,13/Mar/19 22:29,04/Nov/13 21:26,1.2.12,2.0.3,,,,,0,,,,,"The GossipStage call to SystemTable.updateTokens causes a blocking memtable flush that may get stuck in the postFlushExecutor queue while waiting for other memtables to flush; as a consequence, the Gossiper itself ""blocks"" and the node is turned down.",,,,,,,,,,,,,,,,,,,,,,,,04/Nov/13 20:51;jbellis;6297-v2.txt;https://issues.apache.org/jira/secure/attachment/12612025/6297-v2.txt,04/Nov/13 20:48;jbellis;6297.txt;https://issues.apache.org/jira/secure/attachment/12612024/6297.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-11-04 20:48:09.237,,,no_permission,,,,,,,,,,,,356820,,,Mon Nov 04 21:26:53 UTC 2013,,,,,,0|i1pihj:,357110,1.2.11,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"04/Nov/13 20:48;jbellis;Thinking it through, the simplest solution is to just remove the blocking flush.  This does mean that we have a longer potential window of non-durability for the peer information under Periodic CommitLog, but this does not make things qualitatively worse -- e.g., if we were down entirely during the node addition we would also have to deal with not having the peer information on restart.

I see alternatives to removing the blocking flush as falling into two categories:
# semantically equivalent solutions with more complex implementations (e.g. moving updateTokens into another thread or executor)
# dramatically complex gymnastics that aren't worth the small extra benefit, such as adding a special commitlog sync instead of the blocking flush",04/Nov/13 20:51;jbellis;v2 removes some other blocking flushes that are not critical,04/Nov/13 21:05;brandon.williams;+1,04/Nov/13 21:25;sbtourist;Works for me.,04/Nov/13 21:26;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Serialization bug in PagedRangeCommand,CASSANDRA-6299,12677534,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,05/Nov/13 09:34,12/Mar/19 14:17,13/Mar/19 22:29,05/Nov/13 15:35,2.0.3,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Nov/13 09:40;slebresne;6299.txt;https://issues.apache.org/jira/secure/attachment/12612141/6299.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-05 13:13:43.492,,,no_permission,,,,,,,,,,,,356909,,,Tue Nov 05 15:35:38 UTC 2013,,,,,,0|i1pj1b:,357199,,,,,,,,iamaleksey,iamaleksey,,,2.0.0,,,,,,,"05/Nov/13 09:40;slebresne;There is a typo in the serialization of PagedRangeCommand so that if it has index expressions, it serializes their value with a 4-bytes length but read them with short 2-bytes length. In practice, this ends up breaking the underlying connection and results in query timeouts as described in [this thread|https://groups.google.com/a/lists.datastax.com/d/msg/java-driver-user/ao1ohSLpjRM/4rfCNk1OUM0J].

Patch attached. The patch also fixes a few problem in the serializedSize() method (though I don't think it's used in practice).
",05/Nov/13 13:13;iamaleksey;+1,"05/Nov/13 15:35;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ArrayIndexOutOfBound when using count(*) with over 10,000 rows",CASSANDRA-6333,12678837,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,thattolleyguy,thattolleyguy,12/Nov/13 16:59,12/Mar/19 14:17,13/Mar/19 22:29,22/Nov/13 07:46,2.0.3,,,,,,0,,,,,"We've been getting a TSocket read 0 bytes error when we try and run SELECT count(*) FROM <table> if the table has over 10,000 rows.

I've been able to reproduce the problem by using cassandra-stress to insert different number of rows. When I insert under 10,000, the count is returned. When I insert exactly 10,000, I get a message that my results were limited to 10,000 by default. If insert 10,001, I get the exception below.

{code}
ERROR [Thrift:4] 2013-11-12 09:54:04,850 CustomTThreadPoolServer.java (line 212) Error occurred during processing of message.
java.lang.ArrayIndexOutOfBoundsException: -1
	at java.util.ArrayList.elementData(ArrayList.java:371)
	at java.util.ArrayList.remove(ArrayList.java:448)
	at org.apache.cassandra.cql3.ResultSet.trim(ResultSet.java:92)
	at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:848)
	at org.apache.cassandra.cql3.statements.SelectStatement.pageCountQuery(SelectStatement.java:196)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:163)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:57)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:129)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:145)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:136)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1936)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4394)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4378)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
{code}
","Cassandra 2.0.2, Ubuntu 12.04.3 LTS, Oracle Java 1.7.0_21",,,,,,,,,,,,,,,,,,,,,,,15/Nov/13 14:34;slebresne;6333.txt;https://issues.apache.org/jira/secure/attachment/12614064/6333.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-13 16:14:05.851,,,no_permission,,,,,,,,,,,,358204,,,Fri Nov 22 07:47:26 UTC 2013,,,,,,0|i1pr07:,358494,2.0.2,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"13/Nov/13 16:14;slebresne;Either that has been fixed in the current cassandra-2.0 tip or I'll need complete information to reproduce. I've added the following dtest: https://github.com/riptano/cassandra-dtest/commit/971c4d629a8524205d7bedc1f49315be043a6ceb and tried doing:
{noformat}
> ccm node1 stress -n 10001
...
> ccm node1 cqlsh
Connected to test at 127.0.0.1:9160.
[cqlsh 4.1.0 | Cassandra 2.0.2-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.38.0]
Use HELP for help.
cqlsh> use ""Keyspace1"";
cqlsh:Keyspace1> SELECT COUNT(*) FROM ""Standard1"" LIMIT 10002;

 count
-------
 10001

(1 rows)
{noformat}
All of that seems to be working fine as far as I can tell.","13/Nov/13 17:21;thattolleyguy;If I don't use a limit on the select statement throws the exception.

{noformat}
cqlsh:Keyspace1> select count(*) from ""Standard1"" limit 10002;

 count
-------
 10002

(1 rows)

cqlsh:Keyspace1> select count(*) from ""Standard1"";
TSocket read 0 bytes
cqlsh:Keyspace1> 
{noformat}","15/Nov/13 14:34;slebresne;Ok, this is due to the fact that SP.getRangeSlice might return more results than asked (due to reconciliation, you need > 1 node) which was confusing the pager logic. Attaching patch so that the pager trim the result in that case.",22/Nov/13 01:07;iamaleksey;+1,"22/Nov/13 07:47;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LIMIT fetches one less than requested value,CASSANDRA-6330,12678626,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,BrandenVisser,BrandenVisser,11/Nov/13 18:28,12/Mar/19 14:17,13/Mar/19 22:29,12/Nov/13 18:13,1.2.12,,,,,,0,,,,,"Using Cassandra 1.2.11, the following sequence demonstrates the issue:

{code:sql}
CREATE TABLE blah (key text, column text, value text, PRIMARY KEY (key, column)) WITH COMPACT STORAGE;
INSERT INTO blah (key, column, value) VALUES ('a', 'a', 'a');
INSERT INTO blah (key, column, value) VALUES ('a', 'b', 'e');
INSERT INTO blah (key, column, value) VALUES ('a', 'c', 'e');
INSERT INTO blah (key, column, value) VALUES ('a', 'd', 'e');
INSERT INTO blah (key, column, value) VALUES ('a', 'e', 'e');
SELECT column FROM blah WHERE key = 'a' AND column < 'c' ORDER BY column DESC LIMIT 2;

 column
--------
      b
{code}

However I would expect columns b and a to both be returned. Only seems to be an issue if the range bound is an exact match, and only if ORDER BY column DESC is used.",,,,,,,,,,,,,,,,,,,,,,,,12/Nov/13 08:31;slebresne;6330.txt;https://issues.apache.org/jira/secure/attachment/12613343/6330.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-12 08:31:15.626,,,no_permission,,,,,,,,,,,,357993,,,Tue Nov 12 18:13:18 UTC 2013,,,,,,0|i1pppj:,358283,,,,,,,,iamaleksey,iamaleksey,,,1.2.0,,,,,,,"11/Nov/13 18:35;BrandenVisser;Should clarify, version is 1.2.11-SNAPSHOT as of this weekend.",12/Nov/13 08:31;slebresne;This is indeed an oversight (that has been there basically forever). Attaching simple fix; I've pushed a dtests for it already.,12/Nov/13 14:36;iamaleksey;+1,"12/Nov/13 18:13;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LOCAL_ONE code in the native protocol is not the same in C* 1.2 and C* 2.0,CASSANDRA-6347,12679251,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,14/Nov/13 17:26,12/Mar/19 14:17,13/Mar/19 22:29,19/Nov/13 10:58,1.2.12,,,,,,0,,,,,"When LOCAL_ONE was added (CASSANDRA-6202), it was unfortunately not given the same code (the one used by the native protocol) in C* 1.2 and C* 2.0.  In 1.2 it's 8 (even though the specification document pretends it's 10) while it's 10 in 2.0.

This basically breaks backward compatibility for the v1 protocol between C* 1.2 and C* 2.0. Now, we could ""fix"" 2.0 adding special cases for the v1 protocol but that's going to be a bit of a pain, so instead I suggest to just switch to 10 in 1.2. Since the spec was wrong anyway and nobody complained so far this suggest no-one has really added support for LOCAL_ONE in the native protocol against 1.2.11, so if we change it now we can just say to people to upgrade to 1.2.12 directly if they want to use LOCAL_ONE with the native protocol. Attaching simple patch for that.
",,,,,,,,,,,,,,,,,,,,,,,,14/Nov/13 17:27;slebresne;6347.txt;https://issues.apache.org/jira/secure/attachment/12613880/6347.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-15 06:15:44.097,,,no_permission,,,,,,,,,,,,358616,,,Tue Nov 19 10:58:26 UTC 2013,,,,,,0|i1ptjr:,358906,,,,,,,,jasobrown,jasobrown,,,,,,,,,,"15/Nov/13 06:15;jasobrown;Crap, I thought I fixed all the id madness across 1.2 vs 2.0, thrift  vs native protocol, native protocol 1 vs 2. <sigh> . Thanks for discovering and fixing. 

+1
","19/Nov/13 10:58;slebresne;Committed (a few days ago but forgot to close), thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig CqlStorage generates  ERROR 1108: Duplicate schema alias,CASSANDRA-6309,12677888,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,alexliu68,thunderstumpges,thunderstumpges,06/Nov/13 23:15,12/Mar/19 14:17,13/Mar/19 22:29,13/Dec/13 17:42,1.2.13,2.0.4,,,,,1,,,,,"In Pig after loading a simple CQL3 table from Cassandra 2.0.1, and dumping contents, I receive:
Caused by: org.apache.pig.impl.plan.PlanValidationException: ERROR 1108: Duplicate schema alias: author in ""cm""

 cm = load 'cql://thunder_test/cassandra_messages' USING CqlStorage;
 dump cm
ERROR org.apache.pig.tools.grunt.Grunt - org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias cm
...
Caused by: org.apache.pig.impl.plan.PlanValidationException: ERROR 1108: Duplicate schema alias: author in ""cm""
        at org.apache.pig.newplan.logical.visitor.SchemaAliasVisitor.validate(SchemaAliasVisitor.java:75)


running 'describe cm' gives:
cm: {message_id: chararray,author: chararray,author: chararray,body: chararray,message_id: chararray}

The original table schema in Cassandra is:
CREATE TABLE cassandra_messages (
  message_id text,
  author text,
  body text,
  PRIMARY KEY (message_id, author)
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='null' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='NONE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};

it appears that the code in CqlStorage.getColumnMetadata at ~line 478 takes the ""keys"" columns (in my case, message_id and author) and appends the columns from getColumnMeta (which has all three columns). Thus the keys columns are duplicated.

",,,,,,,,,,,,,,,,,,,,,,,,07/Nov/13 22:07;alexliu68;6309-2.0.txt;https://issues.apache.org/jira/secure/attachment/12612718/6309-2.0.txt,05/Dec/13 21:58;alexliu68;6309-fix-pig-test-compiling.txt;https://issues.apache.org/jira/secure/attachment/12617246/6309-fix-pig-test-compiling.txt,11/Dec/13 05:27;alexliu68;6309-trunk-branch.txt;https://issues.apache.org/jira/secure/attachment/12618187/6309-trunk-branch.txt,22/Nov/13 22:20;alexliu68;6309-v2-2.0-branch.txt;https://issues.apache.org/jira/secure/attachment/12615395/6309-v2-2.0-branch.txt,05/Dec/13 21:20;alexliu68;6309-v3.txt;https://issues.apache.org/jira/secure/attachment/12617243/6309-v3.txt,22/Nov/13 22:20;alexliu68;LOCAL_ONE-write-for-all-strategies-v2.txt;https://issues.apache.org/jira/secure/attachment/12615396/LOCAL_ONE-write-for-all-strategies-v2.txt,07/Nov/13 22:07;alexliu68;LOCAL_ONE-write-for-all-strategies.txt;https://issues.apache.org/jira/secure/attachment/12612719/LOCAL_ONE-write-for-all-strategies.txt,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,2013-11-07 00:04:25.233,,,no_permission,,,,,,,,,,,,357263,,,Fri Dec 13 17:42:47 UTC 2013,,,,,,0|i1pl7j:,357553,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"06/Nov/13 23:52;thunderstumpges;just as a note, I'm using code from the tip of the cassandra-2.0 branch. I'm a little confused because the code which combines the list from ""getKeysMeta"" with that of ""getColumnMeta"" has been around a while. There was some refactoring of getKeysMeta recently, but not sure how that could have changed things. 

I have a naive patch which simply loops the columns in getColumnMeta and only adds them to ""keys"" collection if they don't already exist. Can add that as a patch if y'all think that's an appropriate way to solve this thing.
 ",07/Nov/13 00:04;alexliu68;It works in 1.2 branch. It looks like some changes to table metadata implementation causes the issue.,"07/Nov/13 19:40;alexliu68;The partition and cluster keys are added to system.schema_columns table, so to do a quick fix, we just ignore those columns as for now. When all other system_schema_* changes are done in the later release, we will make the final changes accordingly.","07/Nov/13 20:26;alexliu68;Some thrift column families tests are broken too

{code}
    [junit] Testcase: testCassandraStorageFullCopy(org.apache.cassandra.pig.ThriftColumnFamilyTest):	Caused an ERROR
    [junit] null
    [junit] NotFoundException()
    [junit] 	at org.apache.cassandra.thrift.Cassandra$get_result$get_resultStandardScheme.read(Cassandra.java:10059)
    [junit] 	at org.apache.cassandra.thrift.Cassandra$get_result$get_resultStandardScheme.read(Cassandra.java:1)
    [junit] 	at org.apache.cassandra.thrift.Cassandra$get_result.read(Cassandra.java:9942)
    [junit] 	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
    [junit] 	at org.apache.cassandra.thrift.Cassandra$Client.recv_get(Cassandra.java:615)
    [junit] 	at org.apache.cassandra.thrift.Cassandra$Client.get(Cassandra.java:600)
    [junit] 	at org.apache.cassandra.pig.ThriftColumnFamilyTest.getColumnValue(ThriftColumnFamilyTest.java:808)
    [junit] 	at org.apache.cassandra.pig.ThriftColumnFamilyTest.testCassandraStorageFullCopy(ThriftColumnFamilyTest.java:392)
    [junit] 
{code}","07/Nov/13 21:50;alexliu68;This turns out to be written path needs ""LOCAL_ONE"" be supported by Simple Strategy","07/Nov/13 22:08;alexliu68;If we can't get the patch for LOCAL_ONE write path in this ticket, I can post it to CASSANDRA-6238","07/Nov/13 23:48;thunderstumpges;Thanks for the quick turn-around Alex! I appreciate it. The patch looks good to me. better than what I hacked together locally. And the patch for LOCAL_ONE I haven't run into because we're running 2.0.1 on our servers still, so I had to override my consistency for reads and writes to ONE anyway.

thanks again!
Thunder","22/Nov/13 20:35;brandon.williams;Patch fixes the duplicate schema problem, but the thrift test fails.","22/Nov/13 22:24;alexliu68;V2 patch is attached, which fix the unit tests.","22/Nov/13 22:37;brandon.williams;v2 removes all the license headers from the new tests, can you rebase it?","22/Nov/13 23:31;alexliu68;https://github.com/apache/cassandra/blob/cassandra-2.0/test/unit/org/apache/cassandra/pig/ThriftColumnFamilyDataTypeTest.java#L20

https://github.com/apache/cassandra/blob/cassandra-2.0/test/unit/org/apache/cassandra/pig/CqlTableDataTypeTest.java#L20

The patch removes the duplicate license header.",05/Dec/13 21:20;alexliu68;V3 patch is based on the latest 2.0 branch,"05/Dec/13 21:59;alexliu68;6309-fix-pig-test-compiling.txt fixes the build issue for pig test

","06/Dec/13 00:29;alexliu68;Trunk bumped Pig version to 0.11.0, the pig unit tests fail

{code}
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testCqlStorageListType(org.apache.cassandra.pig.CqlTableDataTypeTest):	Caused an ERROR
    [junit] Unable to open iterator for alias list_rows
    [junit] org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias list_rows
    [junit] 	at org.apache.pig.PigServer.openIterator(PigServer.java:836)
    [junit] 	at org.apache.cassandra.pig.CqlTableDataTypeTest.testCqlStorageListType(CqlTableDataTypeTest.java:332)
    [junit] Caused by: org.apache.pig.PigException: ERROR 1002: Unable to store alias list_rows
    [junit] 	at org.apache.pig.PigServer.storeEx(PigServer.java:935)
    [junit] 	at org.apache.pig.PigServer.store(PigServer.java:898)
    [junit] 	at org.apache.pig.PigServer.openIterator(PigServer.java:811)
    [junit] Caused by: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobCreationException: ERROR 2017: Internal error creating job configuration.
    [junit] 	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJob(JobControlCompiler.java:848)
    [junit] 	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.compile(JobControlCompiler.java:294)
    [junit] 	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:177)
    [junit] 	at org.apache.pig.PigServer.launchPlan(PigServer.java:1264)
    [junit] 	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1249)
    [junit] 	at org.apache.pig.PigServer.storeEx(PigServer.java:931)
    [junit] Caused by: java.io.IOException: Serialization error: org.apache.log4j.Level
    [junit] 	at org.apache.pig.impl.util.ObjectSerializer.serialize(ObjectSerializer.java:47)
    [junit] 	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJob(JobControlCompiler.java:526)
    [junit] Caused by: java.io.NotSerializableException: org.apache.log4j.Level
    [junit] 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
    [junit] 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
    [junit] 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
    [junit] 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
    [junit] 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
    [junit] 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
    [junit] 	at org.apache.pig.impl.util.ObjectSerializer.serialize(ObjectSerializer.java:43)
{code}",10/Dec/13 23:38;alexliu68;It looks like logback replacing log4j causes the issue. I updated Pig to 0.11.1 on cassandra-2.0 branch and the pig unit tests passed.,"11/Dec/13 03:55;alexliu68;6309-trunk-branch.txt is on tunk, which changes the build.xml to only add runtime log4j lib to pig-test, other tests stay with logback.",13/Dec/13 17:42;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"select with ""in"" clause wrongly returns empty result",CASSANDRA-6327,12678465,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,baldrick,baldrick,10/Nov/13 17:42,12/Mar/19 14:17,13/Mar/19 22:29,14/Nov/13 08:33,2.0.3,,,,,,0,,,,,"This query returns no result:

cqlsh:tick_data> select syd from current_prices where shard = 1 and syd in (1, 556129);

(0 rows)

However this query does return a result, showing that the previous query was wrong to return no result:

cqlsh:tick_data> select syd from current_prices where shard = 1 and syd in (556129);

 syd
--------
 556129

(1 rows)

This can be reproduced as follows:

(a) Create a keyspace tick_data:

create keyspace tick_data WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};

(b) Create a table current_prices:

CREATE TABLE current_prices (
  shard int,
  syd int,
  ask decimal,
  bid decimal,
  currency ascii,
  when timeuuid,
  PRIMARY KEY (shard, syd)
);

(c) Stop Cassandra and untar the attached tar file in /var/lib/cassandra/data/tick_data/.  It populates the current_prices table.

(d) Restart Cassandra and perform the above selects.","Cassandra 2.0.2, x86-64 Ubuntu 13.10",,,,,,,,,,,,,,,,,,,,,,,13/Nov/13 20:40;krummas;0001-check-if-any-of-the-slices-intersects.patch;https://issues.apache.org/jira/secure/attachment/12613679/0001-check-if-any-of-the-slices-intersects.patch,12/Nov/13 09:38;slebresne;6327.txt;https://issues.apache.org/jira/secure/attachment/12613348/6327.txt,10/Nov/13 17:44;baldrick;current_prices.tar;https://issues.apache.org/jira/secure/attachment/12613056/current_prices.tar,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-11-12 09:38:15.024,,,no_permission,,,,,,,,,,,,357840,,,Mon Jun 09 13:06:21 UTC 2014,,,,,,0|i1porj:,358130,,,,,,,,iamaleksey,iamaleksey,,,2.0.0,,,,,,,10/Nov/13 17:44;baldrick;Untar this file to populate the current_prices table.,"12/Nov/13 09:38;slebresne;This is a bug in the column slice intersection logic from CASSANDRA-5514. More precisely, when there was multiple column slices, the logic was returning no intersection as soon as one of the slice was not intersecting, but it should do the exact reverse, it should consider it an intersection as one as any of the slice intersect but wait to have test all slices before saying it doesn't intersect.

Attaching simple patch to fix.",12/Nov/13 09:38;slebresne;PS: I've pushed a dtests for this.,12/Nov/13 14:49;iamaleksey;+1,"12/Nov/13 18:23;slebresne;Committed, thanks","13/Nov/13 20:40;krummas;the fix broke KeyspaceTest#testLimitSSTablesComposites

this patch tests all slices, but keeps old intersection logic - all components of the composite need to intersect","13/Nov/13 21:08;mshuler;0001-check-if-any-of-the-slices-intersects.patch fixes KeyspaceTest for me on current cassandra-2.0 branch (1d3a4dd).  This is the only test I have run, at the moment :)

    [junit] Testsuite: org.apache.cassandra.db.KeyspaceTest
    [junit] Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 23.567 sec",13/Nov/13 21:23;iamaleksey;[~krummas] Oh. Right. Makes sense.Go ahead.,"14/Nov/13 08:33;slebresne;Correct (more precisely the new version is correct in term of behavior but not as efficient as can be in terms of avoiding sstables).
Committed a slight variation of Markus fixup: there is no need to continue testing a given slice when we know it doesn't intersect.","25/Nov/13 20:38;cowardlydragon;Does this fix CASSANDRA-6137?

Nevermind, you put in a comment saying it probably was... 

I have an audit job looking for this, so if we see it come up, we'll let you know once we move prod to 2.0.3

Any idea why compaction run right after schema creation seemed to fix this?","25/Nov/13 20:45;krummas;there is logic in 2.0 to be able to skip sstables on the read path, a (major) compaction means there is only one sstable, and it wont be skipped since atleast one of slices will intersect it","09/Jun/14 13:06;recastrodiaz;I noticed this bug on Cassandra version 1.X. Then we upgraded to 2.0.3 which solved this issue. It seems, however, to have come back in version 2.0.7.
Should I open a new issue?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Endpoint cache invalidation causes CPU spike (on vnode rings?),CASSANDRA-6345,12679044,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,rbranson,rbranson,13/Nov/13 16:06,12/Mar/19 14:17,13/Mar/19 22:29,26/Nov/13 20:11,1.2.13,2.0.4,,,,,2,,,,,"We've observed that events which cause invalidation of the endpoint cache (update keyspace, add/remove nodes, etc) in AbstractReplicationStrategy result in several seconds of thundering herd behavior on the entire cluster. 

A thread dump shows over a hundred threads (I stopped counting at that point) with a backtrace like this:

        at java.net.Inet4Address.getAddress(Inet4Address.java:288)
        at org.apache.cassandra.locator.TokenMetadata$1.compare(TokenMetadata.java:106)
        at org.apache.cassandra.locator.TokenMetadata$1.compare(TokenMetadata.java:103)
        at java.util.TreeMap.getEntryUsingComparator(TreeMap.java:351)
        at java.util.TreeMap.getEntry(TreeMap.java:322)
        at java.util.TreeMap.get(TreeMap.java:255)
        at com.google.common.collect.AbstractMultimap.put(AbstractMultimap.java:200)
        at com.google.common.collect.AbstractSetMultimap.put(AbstractSetMultimap.java:117)
        at com.google.common.collect.TreeMultimap.put(TreeMultimap.java:74)
        at com.google.common.collect.AbstractMultimap.putAll(AbstractMultimap.java:273)
        at com.google.common.collect.TreeMultimap.putAll(TreeMultimap.java:74)
        at org.apache.cassandra.utils.SortedBiMultiValMap.create(SortedBiMultiValMap.java:60)
        at org.apache.cassandra.locator.TokenMetadata.cloneOnlyTokenMap(TokenMetadata.java:598)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:104)
        at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:2671)
        at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:375)

It looks like there's a large amount of cost in the TokenMetadata.cloneOnlyTokenMap that AbstractReplicationStrategy.getNaturalEndpoints is calling each time there is a cache miss for an endpoint. It seems as if this would only impact clusters with large numbers of tokens, so it's probably a vnodes-only issue.

Proposal: In AbstractReplicationStrategy.getNaturalEndpoints(), cache the cloned TokenMetadata instance returned by TokenMetadata.cloneOnlyTokenMap(), wrapping it with a lock to prevent stampedes, and clearing it in clearEndpointCache(). Thoughts?","30 nodes total, 2 DCs
Cassandra 1.2.11
vnodes enabled (256 per node)",,,,,,,,,,,,,,,,,,,,,,,15/Nov/13 07:28;rbranson;6345-rbranson-v2.txt;https://issues.apache.org/jira/secure/attachment/12614030/6345-rbranson-v2.txt,13/Nov/13 21:52;rbranson;6345-rbranson.txt;https://issues.apache.org/jira/secure/attachment/12613706/6345-rbranson.txt,13/Nov/13 21:26;jbellis;6345-v2.txt;https://issues.apache.org/jira/secure/attachment/12613697/6345-v2.txt,18/Nov/13 03:52;jbellis;6345-v3.txt;https://issues.apache.org/jira/secure/attachment/12614329/6345-v3.txt,20/Nov/13 04:40;jbellis;6345-v4.txt;https://issues.apache.org/jira/secure/attachment/12614786/6345-v4.txt,22/Nov/13 14:29;jbellis;6345-v5.txt;https://issues.apache.org/jira/secure/attachment/12615340/6345-v5.txt,13/Nov/13 21:06;jbellis;6345.txt;https://issues.apache.org/jira/secure/attachment/12613688/6345.txt,13/Nov/13 21:48;rbranson;half-way-thru-6345-rbranson-patch-applied.png;https://issues.apache.org/jira/secure/attachment/12613704/half-way-thru-6345-rbranson-patch-applied.png,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,2013-11-13 17:05:30.071,,,no_permission,,,,,,,,,,,,358409,,,Mon Dec 09 23:50:02 UTC 2013,,,,,,0|i1ps9r:,358699,1.2.11,,,,,,,rbranson,rbranson,,,,,,,,,,"13/Nov/13 17:05;jbellis;Interesting.  Could we optimize cOTM instead?  That would definitely be the simplest solution.

E.g. it looks like TreeMultimap.putAll actually loops over each entry and calls put one at a time which is the worst-case scenario for binary tree rebalancing -- quadratic time.","13/Nov/13 17:30;jbellis;bq. it looks like TreeMultimap.putAll actually loops over each entry and calls put one at a time

I don't see a way around this: https://code.google.com/p/guava-libraries/issues/detail?id=1579","13/Nov/13 17:34;benedict;A Treap? Can be cheaply built, cheaply merged and cheaply cloned.

Also, anything cheaply cloneable would work for that operation. A SnapTree that is wrapped to support multi-map functionality would also work.",13/Nov/13 18:01;jbellis;Yes.  Too bad the implementation classes like AbstractSortedKeySortedSetMultimap are package-private.,"13/Nov/13 20:47;jbellis;Actually I think a STM multimap would still get messy quickly since you need to do a ""deep"" clone -- cloning the top-level Map would leave the values (sub-collections) sharing a reference.","13/Nov/13 20:52;benedict;Just have the wrapper make any updates to a collection replace the collection instead of modifying it.

[NB: I haven't looked to see if this would have any negative performance implications on the update side, I'm assuming the reads are more frequent and/or collections small... if not, a Treap is probably the better choice as my old Jjoost implementation (IIRC) supports snapshotting and multiple values are dealt with inside the tree itself, not as a collection]","13/Nov/13 21:06;jbellis;bq. Proposal: In AbstractReplicationStrategy.getNaturalEndpoints(), cache the cloned TokenMetadata instance returned by TokenMetadata.cloneOnlyTokenMap(), wrapping it with a lock to prevent stampedes, and clearing it in clearEndpointCache().

Why not just use a sharded lock to prevent stampedes directly w/o the caching complexity, as in the attached?","13/Nov/13 21:26;jbellis;I see, with vnodes we have enough ranges that we can have a thundering herd even if each range only clones once.

v2 attached with the approach you described originally.",13/Nov/13 21:44;rbranson;Attached a patch we deployed to production that fixed the issue.,"13/Nov/13 21:48;rbranson;CPU user% graph during the rollout of the patch I attached on 1 DC (15 nodes) of the cluster. Around ~21:05 the patch starts to roll out and spikes are seen. The node in question receives the patch at ~21:30, and afterwards the spikes are gone. The rollout finishes at ~21:45.",13/Nov/13 23:26;jbellis;I have to admit I like it better without the custom wrapper class. :),"14/Nov/13 00:25;rbranson;Well, I started writing the patch this morning and I don't write multi-threaded Java code every day, so I'm overly careful ;) The only theoretical advantage to my patch is that it allows concurrent readers.","15/Nov/13 04:43;rbranson;Unfortunately both of the patches suffer from a deadlock, since the invalidation and fill are wrapped up in TokenMetadata's locks.

T1 acquires cache read lock
T2 acquires TokenMetadata write lock
T1 acquires cache write lock on miss
T2 is blocked on cache write lock trying to invalidate
T1 is blocked on TokenMetadata read lock trying to cloneOnlyTokenMap to fill the cache

Trying to work on a fix.","15/Nov/13 07:28;rbranson;Attached a new patch with the deadlock fixed. We're running this on a production cluster.

The primary issue was the callback for invalidation from TokenMetadata to all of the registered AbstractReplicationStrategy instances. This was asking for it anyway, so in the patch I replaced the ""push"" invalidation with simple versioning of the TokenMetadata endpoints. TokenMetadata bumps it's version number each time the cache would need to be invalidated, and AbstractReplicationStrategy checks it's version when it needs to do a read, invalidating if necessary. This gets the invalidation out of the gossip threads and into the RPC threads, which is probably a good thing. The only thing I'm not super crazy about is the extra hot path read lock acquisition on TokenMetadata.getEndpointVersion(), which might be avoidable.","18/Nov/13 03:52;jbellis;I think we can craft a simpler solution (v3) by using an AtomicReference to the TM clone.  This removes the possibility of deadlock since clearEndpointCache now only makes non-blocking calls.

I've also refined it to use a Striped<Lock> per-keyToken, as well as synchronizing the TM clone itself, since concurrent endpoint computation is fine.","18/Nov/13 18:51;rbranson;I like the simpler approach. I still think the callbacks for invalidation are asking for it ;) I also think perhaps the stampede lock should be more explicit than a synchronized lock on ""this"" to prevent unintended blocking from future modifications.

Either way, I think the only material concern I have is the order that TokenMetadata changes get applied to the caches in AbstractReplicationStrategy instances. Shouldn't the invalidation take place on all threads in all instances of AbstractReplicationStrategy before returning from an endpoint-mutating write operation in TokenMetadata? It seems as if just setting the cache to empty would allow a period of time where TokenMetadata write methods had returned but not all threads have seen the mutation yet because they are still holding onto the old clone of TM. This might be alright though, I'm not sure. Thoughts?","20/Nov/13 04:25;jbellis;bq. It seems as if just setting the cache to empty would allow a period of time where TokenMetadata write methods had returned but not all threads have seen the mutation yet

I'm not 100% sure this is what you're talking about, but I see this problem with the existing code (and my v3):

{noformat}
Thread 1                 Thread 2        
getNaturalEndpoints      
cloneOnlyTokenMap        
                         invalidateCachedTokenEndpointValues
endpoints = calculate
cacheEndpoint [based on the now-invalidated token map]
{noformat}

So it doesn't quite work.  We'd need to introduce another AtomicReference on the cache, so that invalidate could create a new Map (so it doesn't matter if someone updates the old one).  But I think you're right that getting rid of the callback approach entirely is better.",20/Nov/13 04:40;jbellis;v4 attached that uses a versioning approach like yours.  I dropped the readLock acquire on version read since it's not necessary to block callers during the update.  (A few extra over-broad replica set operations won't hurt.),"20/Nov/13 20:49;rbranson;+100 at removing those pub/sub callbacks :)

The concurrency issues I bring up are probably because I'm unfamiliar with the ""guarantees"" needed by TokenMetadata updates. It looks like the current release code is subject to the issue I brought up, where method calls on TokenMetadata that change state return successfully before all threads applying mutations have ""seen"" the update. There will be some mutations in progress that are using ""stale"" token data to apply writes even after TokenMetadata write methods returns as successful. So this does not appear to be a regression, but I'm just being overly cautious having been burned by these sort of double-caching scenarios before. You bring up the point that over-broad operations are ok, and I agree, but I'm more concerned about operations that are too narrow. It seems that unless I'm missing something either is possible with the current release code, and thus these patches as well (including mine).

TokenMetadata#updateNormalTokens is (implicitly) relying on the removeFromMoving call to bump the version, but the tokenToEndpointMap is updated afterwards, which means internal data is updated after the version is bumped. IMHO to be defensive, any time the write lock is acquired in TokenMetadata, the version should be bumped in the finally block before the lock is released. I don't think this is exposing a bug in the existing patch though, because cloneOnlyTokenMap will be blocked until the write lock is released in the finally block.

Is the idea with the striped lock on the endpoint cache in AbstractReplicationStrategy to help smooth out the stampede effect when the ""global"" lock on the cached TM gets released after the fill? How much do you think it's worth the extra complexity? FWIW, my v2 patch suffers from this issue and it hasn't reared itself in production. The write load for the machines in the cluster I've been looking at is comparatively low though compared to many others at 6-7k/sec peak on an 8-core box.","22/Nov/13 14:29;jbellis;bq. It seems that unless I'm missing something either is possible with the current release code, and thus these patches as well

Technically correct, but in practice we're in pretty good shape.  The sequence is:

# Add the changing node to pending ranges
# Sleep for RING_DELAY so everyone else starts including the new target in their writes
# Flush data to be transferred
# Send over data for writes that happened before (1)

Step 1 happens on every coordinator.  2-4 only happen on the node that is giving up a token range.

The guarantee we need is that any write that happens before the pending range change, completes before the subsequent flush.

Even if we used TM.lock to protect the entire ARS sequence (guaranteeing that no local write is in progress once the PRC happens) we could still receive writes from other nodes that began their PRC change later.  

So we rely on the RING_DELAY (30s) sleep.  I suppose a GC pause for instance at just the wrong time could theoretically mean a mutation against the old state gets sent out late, but I don't see how we can improve it.

bq. IMHO to be defensive, any time the write lock is acquired in TokenMetadata, the version should be bumped in the finally block before the lock is released

Haven't thought this through as much.  What are you saying we should bump that we weren't calling invalidate on before?

bq. Is the idea with the striped lock on the endpoint cache in AbstractReplicationStrategy to help smooth out the stampede effect when the ""global"" lock on the cached TM gets released after the fill?

I'm trying to avoid a minor stampede on calculateNaturalEndpoints (CASSANDRA-3881) but it's probably premature optimization.  v5 attached w/o that.","26/Nov/13 17:33;rbranson;Thanks for taking the time to explain the consistency story. It makes perfect sense. 

My defensiveness comment suggested bumping the version number each time the TM write lock is released, which would be in addition to the existing invalidations. You're probably a much better gauge on the usefulness of this, so up to you.

Really nice that the v5 patch is so compact. Two minor comments: the endpointsLock declaration is still in there, and not to be all nitpicky but there are two typos in the comments (""wo we keep"" and ""clone got invalidted"").","26/Nov/13 20:11;jbellis;bq. My defensiveness comment suggested bumping the version number each time the TM write lock is released, which would be in addition to the existing invalidations.

Okay.  I'm going to leave this be then, because I don't want to accidentally start invalidating the cache unnecessarily because one of those operations was more common than I thought.  Could address in trunk if you want to open a ticket.

Committed v5 w/ nits fixed.",26/Nov/13 22:26;rbranson;LGTM!,"09/Dec/13 21:24;cburroughs;{noformat}
private volatile long ringVersion = 0;

ringVersion++;
{noformat}

If there is something tricky here that makes an increment on a volatile okay then it deserves a comment.","09/Dec/13 23:50;jbellis;We don't care about keeping an accurate count, only that once it's done with that block it's higher than it was before.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Classcast Exception thrown when under load,CASSANDRA-6322,12678325,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,pookieman,pookieman,08/Nov/13 21:56,12/Mar/19 14:17,13/Mar/19 22:29,11/Nov/13 17:41,2.0.3,,,,,,1,,,,,"Saw this in the logs when running a load test:

ERROR [EXPIRING-MAP-REAPER:1] 2013-11-08 21:52:56,389 CassandraDaemon.java (line 187) Exception in thread Thread[EXPIRING-MAP-REAPER:1,5,main]
java.lang.ClassCastException: org.apache.cassandra.db.CounterMutation cannot be cast to org.apache.cassandra.db.RowMutation
	at org.apache.cassandra.net.MessagingService$5.apply(MessagingService.java:350)
	at org.apache.cassandra.net.MessagingService$5.apply(MessagingService.java:340)
	at org.apache.cassandra.utils.ExpiringMap$1.run(ExpiringMap.java:97)
	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:75)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
","Linux Centos 64 bit, Sun JDK 1.7",,,,,,,,,,,,,,,,,,,,,,,11/Nov/13 16:44;jbellis;6322.txt;https://issues.apache.org/jira/secure/attachment/12613176/6322.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-08 22:44:26.835,,,no_permission,,,,,,,,,,,,357700,,,Tue Nov 12 18:39:27 UTC 2013,,,,,,0|i1pnwn:,357990,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"08/Nov/13 22:44;jbellis;Hmm, CounterMutations should not be marked with shouldHint.",11/Nov/13 16:44;jbellis;Fix attached.,11/Nov/13 16:56;iamaleksey;+1,11/Nov/13 17:41;jbellis;committed,12/Nov/13 18:39;pookieman;Great thanks.. any ideas when you'll be cutting 2.0.3?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError for rows with zero columns,CASSANDRA-6374,12679880,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,sv3k,sv3k,19/Nov/13 06:13,12/Mar/19 14:17,13/Mar/19 22:29,21/Nov/13 21:15,2.0.3,,,,,,3,,,,,"After upgrading from 1.2.5 to 1.2.9 and then to 2.0.2 we've got those exceptions:
{code}
ERROR [FlushWriter:1] 2013-11-18 16:14:36,305 CassandraDaemon.java (line 187) Exception in thread Thread[FlushWriter:1,5,main]
java.lang.AssertionError
        at org.apache.cassandra.io.sstable.SSTableWriter.rawAppend(SSTableWriter.java:198)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:186)
        at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:360)
        at org.apache.cassandra.db.Memtable$FlushRunnable.runWith(Memtable.java:315)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
{code}

Also found similar issue in this thread:
http://www.mail-archive.com/user@cassandra.apache.org/msg32875.html
There Aaron Morton said that its caused by leaving rows with zero columns - that's exactly what we do in some CFs (using Thrift & Astyanax).
",,,,,,,,,,,,,,,,,,,,,,,,20/Nov/13 05:00;jbellis;6374.txt;https://issues.apache.org/jira/secure/attachment/12614789/6374.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-19 13:53:08.922,,,no_permission,,,,,,,,,,,,359238,,,Fri Nov 22 15:37:20 UTC 2013,,,,,,0|i1pxh3:,359537,2.0.2,,,,,,,jasobrown,jasobrown,,,,,,,,,,"19/Nov/13 13:53;jbellis;Under Thrift, a row with zero columns will be removed as soon as it compacts.  So this really is a bad idea.","20/Nov/13 03:25;ash2k;[~jbellis] can you explain please what exactly is a bad idea here? We are ok with ""a row with zero columns will be removed as soon as it compacts"". We just didn't expected this usage to be a problem - is it documented somewhere? It works fine with 1.2.x and do not work with 2.0.x.",20/Nov/13 03:30;jbellis;Rows that sort-of exist for a non-deterministic amount of time are not a feature I intend to support.  That it works in 1.2.x is a bug.,"20/Nov/13 03:45;ash2k;Our scenario of usage is as follows:
We delete specific columns in a specific row. Sometimes those columns are the last ones. How can we delete the whole row if they are the last ones? We cannot read-check-delete_row_or_only_columns because it has a race between check and delete.","20/Nov/13 04:55;jbellis;We still support that.  The assertion only rejects no cells at all.  (A tombstone still counts as a cell.)

That said, it's really the memtable's job to not flush empty rows even if it's rather antisocial to give it a batch containing zero mutations.","20/Nov/13 05:00;jbellis;Patch attached to do that.  I'm a bit nervous though that I'm not 100% sure if 1.2 was actually writing empty rows to sstables, or if it was rejecting them silently somewhere else.",21/Nov/13 20:48;jasobrown;+1. Agreed that flushing a zero column row is not a good idea.,21/Nov/13 21:15;jbellis;Committed,22/Nov/13 03:27;ash2k;Can this be fixed for 2.0.3 please? We need to upgrade our testing cluster to 2.0.x but cannot because of this issue.,"22/Nov/13 15:37;iamaleksey;bq. Can this be fixed for 2.0.3 please? We need to upgrade our testing cluster to 2.0.x but cannot because of this issue.

2.0.3 vote is being restarted, so yes, it will be in 2.0.3.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstableloader does not support client encryption on Cassandra 2.0,CASSANDRA-6378,12680009,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,beobal,dlaube,dlaube,19/Nov/13 18:44,12/Mar/19 14:17,13/Mar/19 22:29,19/Dec/13 10:02,2.0.4,,,,,,0,client,encryption,ssl,sstableloader,"We have been testing backup/restore from one ring to another and we recently stumbled upon an issue with sstableloader. When client_enc_enable: true, the exception below is generated. However, when client_enc_enable is set to false, the sstableloader is able to get to the point where it is discovers endpoints, connects to stream data, etc.

==============BEGIN EXCEPTION==============
sstableloader --debug -d x.x.x.248,x.x.x.108,x.x.x.113 /tmp/import/keyspace_name/columnfamily_name
Exception in thread ""main"" java.lang.RuntimeException: Could not retrieve endpoint ranges:
at org.apache.cassandra.tools.BulkLoader$ExternalClient.init(BulkLoader.java:226)
at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:149)
at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:68)
Caused by: org.apache.thrift.transport.TTransportException: Frame size (352518400) larger than max length (16384000)!
at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:137)
at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:362)
at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:284)
at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:191)
at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
at org.apache.cassandra.thrift.Cassandra$Client.recv_describe_partitioner(Cassandra.java:1292)
at org.apache.cassandra.thrift.Cassandra$Client.describe_partitioner(Cassandra.java:1280)
at org.apache.cassandra.tools.BulkLoader$ExternalClient.init(BulkLoader.java:199)
... 2 more
==============END EXCEPTION==============

",,,,,,,,,,,,,,,,,,,,,,,,18/Dec/13 09:01;beobal;0001-CASSANDRA-6387-Add-SSL-support-to-BulkLoader.patch;https://issues.apache.org/jira/secure/attachment/12619282/0001-CASSANDRA-6387-Add-SSL-support-to-BulkLoader.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-17 14:17:28.622,,,no_permission,,,,,,,,,,,,359366,,,Mon Dec 30 21:00:40 UTC 2013,,,,,,0|i1py9b:,359665,,,,,,,,mishail,mishail,,,,,,,,,,17/Dec/13 14:17;beobal;Updated patch to remove unnecessary exception handling from SSLTransportFactory,"17/Dec/13 22:30;jbellis;Can you review, [~mishail]?","18/Dec/13 07:42;mishail;The only minor comment I have is that {{opts}} parameter for {{org.apache.cassandra.tools.BulkLoader.LoaderOptions.getTransportFactory()}} is never used.


","18/Dec/13 09:01;beobal;Sorry, missed that when refactoring. Attached updated patch with the extraneous parameter removed.",18/Dec/13 19:51;mishail;LGTM ,18/Dec/13 22:17;jbellis;committed,"18/Dec/13 22:50;mshuler;cassandra-2.0 and trunk both fail to build in the same manner:
{code}
build-project:
     [echo] apache-cassandra: /home/mshuler/git/cassandra/build.xml
    [javac] Compiling 43 source files to /home/mshuler/git/cassandra/build/classes/thrift
    [javac] Note: /home/mshuler/git/cassandra/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java uses or overrides a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] Compiling 847 source files to /home/mshuler/git/cassandra/build/classes/main
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/io/util/NativeAllocator.java:22: warning: Unsafe is internal proprietary API and may be removed in a future release
    [javac] import sun.misc.Unsafe;
    [javac]                ^
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/utils/FastByteComparisons.java:25: warning: Unsafe is internal proprietary API and may be removed in a future release
    [javac] import sun.misc.Unsafe;
    [javac]                ^
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/io/sstable/IndexSummary.java:20: warning: Unsafe is internal proprietary API and may be removed in a future release
    [javac] import java.io.Closeable;
    [javac]             ^
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/io/util/Memory.java:29: warning: Unsafe is internal proprietary API and may be removed in a future release
    [javac]     private static final Unsafe unsafe = NativeAllocator.unsafe;
    [javac]                          ^
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/io/util/NativeAllocator.java:26: warning: Unsafe is internal proprietary API and may be removed in a future release
    [javac]     static final Unsafe unsafe;
    [javac]                  ^
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/io/util/NativeAllocator.java:31: warning: Unsafe is internal proprietary API and may be removed in a future release
    [javac]             Field field = sun.misc.Unsafe.class.getDeclaredField(""theUnsafe"");
    [javac]                                   ^
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/io/util/NativeAllocator.java:33: warning: Unsafe is internal proprietary API and may be removed in a future release
    [javac]             unsafe = (sun.misc.Unsafe) field.get(null);
    [javac]                               ^
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:461: error: cannot find symbol
    [javac]             if (transportFactory.supportedOptions().contains(SSLTransportFactory.TRUSTSTORE))
    [javac]                                                              ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:462: error: cannot find symbol
    [javac]                 options.put(SSLTransportFactory.TRUSTSTORE, opts.encOptions.truststore);
    [javac]                             ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:463: error: cannot find symbol
    [javac]             if (transportFactory.supportedOptions().contains(SSLTransportFactory.TRUSTSTORE_PASSWORD))
    [javac]                                                              ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:464: error: cannot find symbol
    [javac]                 options.put(SSLTransportFactory.TRUSTSTORE_PASSWORD, opts.encOptions.truststore_password);
    [javac]                             ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:465: error: cannot find symbol
    [javac]             if (transportFactory.supportedOptions().contains(SSLTransportFactory.PROTOCOL))
    [javac]                                                              ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:466: error: cannot find symbol
    [javac]                 options.put(SSLTransportFactory.PROTOCOL, opts.encOptions.protocol);
    [javac]                             ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:467: error: cannot find symbol
    [javac]             if (transportFactory.supportedOptions().contains(SSLTransportFactory.CIPHER_SUITES))
    [javac]                                                              ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:468: error: cannot find symbol
    [javac]                 options.put(SSLTransportFactory.CIPHER_SUITES, Joiner.on(',').join(opts.encOptions.cipher_suites));
    [javac]                             ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:470: error: cannot find symbol
    [javac]             if (transportFactory.supportedOptions().contains(SSLTransportFactory.KEYSTORE)
    [javac]                                                              ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:472: error: cannot find symbol
    [javac]                 options.put(SSLTransportFactory.KEYSTORE, opts.encOptions.keystore);
    [javac]                             ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:473: error: cannot find symbol
    [javac]             if (transportFactory.supportedOptions().contains(SSLTransportFactory.KEYSTORE_PASSWORD)
    [javac]                                                              ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:475: error: cannot find symbol
    [javac]                 options.put(SSLTransportFactory.KEYSTORE_PASSWORD, opts.encOptions.keystore_password);
    [javac]                             ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/utils/FastByteComparisons.java:114: warning: Unsafe is internal proprietary API and may be removed in a future release
    [javac]       static final Unsafe theUnsafe;
    [javac]                    ^
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/utils/FastByteComparisons.java:120: warning: Unsafe is internal proprietary API and may be removed in a future release
    [javac]         theUnsafe = (Unsafe) AccessController.doPrivileged(
    [javac]                      ^
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/utils/FastByteComparisons.java:125: warning: Unsafe is internal proprietary API and may be removed in a future release
    [javac]                   Field f = Unsafe.class.getDeclaredField(""theUnsafe"");
    [javac]                             ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 12 errors
    [javac] 10 warnings

BUILD FAILED
{code}",18/Dec/13 23:01;mishail;The entire {{SSLTransportFactory.java}} is missed in the commit,19/Dec/13 00:01;jbellis;fixed,30/Dec/13 21:00;dlaube;Thanks for all of the hard work and effort everyone put in to get this fixed!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C* throws AssertionError when using paging and reverse ordering,CASSANDRA-6343,12679011,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,ach,ach,13/Nov/13 13:33,12/Mar/19 14:17,13/Mar/19 22:29,13/Nov/13 17:02,2.0.3,,,,,,0,,,,,"We have a table with CLUSTERING ORDER BY (date DESC). We try to do a query C* with paging and ORDER BY date ASC. 
This leads to the following exception in C* when pager goes to the last page:
{quote}
ERROR [Native-Transport-Requests:1287744] 2013-10-30 01:53:14,720 ErrorMessage.java (line 210) Unexpected exception during request
java.lang.AssertionError: Added column does not sort as the first column
        at org.apache.cassandra.db.ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:115)
        at org.apache.cassandra.db.ColumnFamily.addColumn(ColumnFamily.java:116)
        at org.apache.cassandra.service.pager.AbstractQueryPager.discardLast(AbstractQueryPager.java:238)
        at org.apache.cassandra.service.pager.AbstractQueryPager.discardLast(AbstractQueryPager.java:182)
        at org.apache.cassandra.service.pager.AbstractQueryPager.fetchPage(AbstractQueryPager.java:100)
        at org.apache.cassandra.service.pager.SliceQueryPager.fetchPage(SliceQueryPager.java:33)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:179)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:56)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:101)
        at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:235)
        at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:139)
        at org.apache.cassandra.transport.Message$Dispatcher.messageReceived(Message.java:296)
        at org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:45)
        at org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:69)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
{quote}

","Last time issue has been reproduced with C* version is 2.0.0, DataStax CQL driver version 2.0.0-beta1 on a single node.",,,,,,,,,,,,,,,,,,,,,,,13/Nov/13 15:32;slebresne;6343.txt;https://issues.apache.org/jira/secure/attachment/12613626/6343.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-13 15:32:10.633,,,no_permission,,,,,,,,,,,,358376,,,Wed Nov 13 17:02:22 UTC 2013,,,,,,0|i1ps2f:,358666,2.0.0,,,,,,,iamaleksey,iamaleksey,,,2.0.0,,,,,,,13/Nov/13 15:32;slebresne;Seems reversed completely slipped through the cracks somehow. Attaching patch to fix (includes a unit test).,13/Nov/13 16:11;iamaleksey;+1,"13/Nov/13 17:02;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix prepared statement size computation,CASSANDRA-6369,12679712,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,18/Nov/13 11:33,12/Mar/19 14:17,13/Mar/19 22:29,18/Nov/13 16:46,1.2.12,2.0.3,,,,,0,,,,,"When computed the size of CQLStatement to limit the prepared statement cache (CASSANDRA-6107), we overestimate the actual memory used because the statement include a reference to the table CFMetaData which measureDeep counts. And as it happens, that reference is big: on a simple test preparing a very trivial select statement, I was able to only prepare 87 statements before some started to be evicted because each statement was more than 93K big and more than 92K of that was the CFMetaData object. As it happens there is no reason to account the CFMetaData object at all since it's in memory anyway whether or not there is prepared statements or not.

Attaching a simple (if not extremely elegant) patch to remove what we don't care about of the computation. Another solution would be to use the MemoryMeter.withTrackerProvider option as we do in Memtable, but in the QueryProcessor case we currently use only one MemoryMeter, not one per CF, so it didn't felt necessarilly cleaner. We could create one-shot MemoryMeter object each time we need to measure a CQLStatement but that doesn't feel a lot simpler/cleaner either. But if someone feels religious about some other solution, I don't care.
",,,,,,,,,,,,,,CASSANDRA-6592,,,,,,,,,,18/Nov/13 11:33;slebresne;6369.txt;https://issues.apache.org/jira/secure/attachment/12614371/6369.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-18 15:03:56.833,,,no_permission,,,,,,,,,,,,359070,,,Mon Nov 18 16:46:20 UTC 2013,,,,,,0|i1pwcn:,359360,,,,,,,,lyubent,lyubent,,,1.2.11,,,,,,,18/Nov/13 15:03;lyubent;LGTM.,"18/Nov/13 16:46;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need the root clause in FBUtilities.classForName when there is exception loading class,CASSANDRA-6258,12676196,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,cywjackson,cywjackson,cywjackson,28/Oct/13 19:01,12/Mar/19 14:17,13/Mar/19 22:29,28/Oct/13 19:39,1.2.12,2.0.3,,,,,0,,,,,"We have a custom snitch that works in 1.1, but the same does not work in 1.2 . It throws a ConfigurationException:

{panel}
ERROR 11:39:37,936 Fatal configuration error
org.apache.cassandra.exceptions.ConfigurationException: Unable to find snitch class 'com.apigee.cassandra.OldEC2Snitch'
	at org.apache.cassandra.utils.FBUtilities.classForName(FBUtilities.java:432)
	at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:444)
	at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:530)
	at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:350)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:126)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:216)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
Unable to find snitch class 'com.apigee.cassandra.OldEC2Snitch'
{panel}

However the above exception does not help us understand what's wrong (jar is in the classpath and readable). I've to add the root clause to the ConfigurationException to see the real problem:

{panel}
ERROR 11:42:20,020 Fatal configuration error
org.apache.cassandra.exceptions.ConfigurationException: Unable to find snitch class 'com.apigee.cassandra.OldEC2Snitch'
	at org.apache.cassandra.utils.FBUtilities.classForName(FBUtilities.java:432)
	at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:444)
	at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:530)
	at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:350)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:126)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:216)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
Caused by: java.lang.NoClassDefFoundError: org/apache/cassandra/config/ConfigurationException
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:169)
	at org.apache.cassandra.utils.FBUtilities.classForName(FBUtilities.java:424)
	... 7 more
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.config.ConfigurationException
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
	... 10 more
Unable to find snitch class 'com.apigee.cassandra.OldEC2Snitch'
{panel}",,,,,,,,,,,,,,,,,,,,,,,,28/Oct/13 19:03;cywjackson;cassandra-6258.patch;https://issues.apache.org/jira/secure/attachment/12610624/cassandra-6258.patch,28/Oct/13 19:12;cywjackson;cassandra-6258.patch.v2;https://issues.apache.org/jira/secure/attachment/12610625/cassandra-6258.patch.v2,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-10-28 19:08:00.28,,,no_permission,,,,,,,,,,,,355693,,,Mon Oct 28 19:39:58 UTC 2013,,,,,,0|i1pbjb:,355981,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,28/Oct/13 19:03;cywjackson;attaching diff (based off 1.2) to add the root clause when throwing ConfigurationException,28/Oct/13 19:08;brandon.williams;Your patch has a bunch of unneeded yaml changes.,"28/Oct/13 19:12;cywjackson;ops, diff too much. This one should be clean",28/Oct/13 19:39;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractColumnFamilyInputFormat does not use start and end tokens configured via ConfigHelper.setInputRange(),CASSANDRA-6436,12682396,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,pauloricardomg,pauloricardomg,pauloricardomg,03/Dec/13 14:30,12/Mar/19 14:17,13/Mar/19 22:29,26/Mar/14 15:38,2.0.7,,,Legacy/Tools,,,0,hadoop,patch,,,"ConfigHelper allows to set a token input range via the setInputRange(conf, startToken, endToken) call (ConfigHelper:254).

We used this feature to limit a hadoop job range to a single Cassandra node's range, or even to single row key, mostly for testing purposes. 

This worked before the fix for CASSANDRA-5536 (https://github.com/apache/cassandra/commit/aaf18bd08af50bbaae0954d78d5e6cbb684aded9), but after this ColumnFamilyInputFormat never uses the value of KeyRange.start_token when defining the input splits (AbstractColumnFamilyInputFormat:142-160), but only KeyRange.start_key, which needs an order preserving partitioner to work.

I propose the attached fix in order to allow defining Cassandra token ranges for a given Hadoop job even when using a non-order preserving partitioner.

Example use of ConfigHelper.setInputRange(conf, startToken, endToken) to limit the range to a single Cassandra Key with RandomPartitioner: 

IPartitioner part = ConfigHelper.getInputPartitioner(job.getConfiguration());
Token token = part.getToken(ByteBufferUtil.bytes(""Cassandra Key""));
BigInteger endToken = (BigInteger) new BigIntegerConverter().convert(BigInteger.class, part.getTokenFactory().toString(token));
BigInteger startToken = endToken.subtract(new BigInteger(""1""));
ConfigHelper.setInputRange(job.getConfiguration(), startToken.toString(), endToken.toString());",,,,,,,,,,,,,,,,,,,,,,,,03/Dec/13 14:36;pauloricardomg;cassandra-1.2-6436.txt;https://issues.apache.org/jira/secure/attachment/12616787/cassandra-1.2-6436.txt,03/Dec/13 14:36;pauloricardomg;cassandra-1.2-6436.txt;https://issues.apache.org/jira/secure/attachment/12616786/cassandra-1.2-6436.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-01-10 17:59:34.049,,,no_permission,,,,,,,,,,,,361653,,,Wed Mar 26 15:38:28 UTC 2014,,,,,,0|i1qcbj:,361951,,,,,,,,pkolaczk,pkolaczk,,,1.2.6,,,,,,,03/Dec/13 14:36;pauloricardomg;Fix patch attached.,03/Dec/13 14:36;pauloricardomg;Fix patch attached.,10/Jan/14 17:59;cburroughs;Did this actually get in 1.2.6?  I don't see it in CHANGES.txt,"10/Jan/14 18:21;pauloricardomg;My bad, sorry. It was supposed be ""Since Version"", not ""Fix version"". Updated description.",11/Mar/14 17:57;jbellis;[~pkolaczk] can you review?,"13/Mar/14 10:40;pkolaczk;Yeah, sure.",25/Mar/14 09:34;pkolaczk;LGTM +1,26/Mar/14 15:38;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DELETE with IF <field>=<value> clause doesn't work properly if more then one row are going to be deleted,CASSANDRA-6430,12682177,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thobbs,dyx,dyx,02/Dec/13 17:20,12/Mar/19 14:17,13/Mar/19 22:29,17/Oct/14 09:45,2.0.11,2.1.1,,,,,1,,,,,"CREATE TABLE test(key int, sub_key int, value text, PRIMARY KEY(key, sub_key) );

INSERT INTO test(key, sub_key, value) VALUES(1,1, '1.1');
INSERT INTO test(key, sub_key, value) VALUES(1,2, '1.2');
INSERT INTO test(key, sub_key, value) VALUES(1,3, '1.3');

SELECT * from test;
 key | sub_key | value
-----+---------+-------
   1 |       1 |   1.1
   1 |       2 |   1.2
   1 |       3 |   1.3

DELETE FROM test WHERE key=1 IF value='1.2';
 [applied]
-----------
     False     <=============== I guess second row should be removed

SELECT * from test;
 key | sub_key | value
-----+---------+-------
   1 |       1 |   1.1
   1 |       2 |   1.2
   1 |       3 |   1.3
(3 rows) 

DELETE FROM test WHERE key=1;

SELECT * from test;
(0 rows)          <=========== all rows were removed: OK

",,,,,,,,,,,,,,,,,,,,,,,,16/Oct/14 20:51;thobbs;6430-2.0.txt;https://issues.apache.org/jira/secure/attachment/12675344/6430-2.0.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-05-29 18:35:23.888,,,no_permission,,,,,,,,,,,,361434,,,Fri Oct 17 09:45:39 UTC 2014,,,,,,0|i1qayv:,361733,2.0.2,,,,,,,slebresne,slebresne,,,,,,,,,,"29/May/14 18:35;sungkyu;Looks like IF keyword is for lightweight transaction, and it's assumed that both keys are specified. Maybe it should support the primary key only case, or report proper error message.",03/Jun/14 21:43;jbellis;/cc [~thobbs],"04/Jun/14 08:54;slebresne;Definitively something we need to fix. That said, the first step here is probably to refuse conditions when the delete statement applies to multiple rows, because the code currently doesn't handle it and the fact we let the query fly is mainly a lack of validation.

In the longer run, it's possible to make such query work, but it's unclear to me whether that's a good idea or not (to be read as 'my initial hunch is that it's a bad idea'). Mainly because such query means we'll have to read the full partition during the Paxos rounds, thus making it pretty easy to OOM the server. Also, if we do handle such query, the condition will not only be a condition on the application of the statement anymore, but potentially a selection on what gets deleted. And I'm not sure we have a good way to indicate in the resultSet what got deleted and what didn't,  which is something one would expect.","16/Oct/14 17:24;thobbs;It sounds like the best option for now is to require that the full primary key be specified when IF conditions are used in DELETEs.  If we want to reconsider allowing the statement in the description, we can always do that later.",16/Oct/14 20:51;thobbs;6430-2.0.txt validates that all PK columns are restricted when performing conditional deletes.  I've also pushed a [dtest|https://github.com/thobbs/cassandra-dtest/tree/CASSANDRA-6430] that covers this.,16/Oct/14 20:52;thobbs;[~blerer] can you review this in time for 2.0.11 and 2.1.1?,"17/Oct/14 09:45;slebresne;Did a quick review so it doesn't block the releases. Patch lgtm, committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assertion error in MessagingService.addCallback,CASSANDRA-6476,12684100,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,iconara,iconara,11/Dec/13 20:20,12/Mar/19 14:17,13/Mar/19 22:29,24/Apr/14 15:31,1.2.17,,,,,,0,,,,,"Two of the three Cassandra nodes in one of our clusters just started behaving very strange about an hour ago. Within a minute of each other they started logging AssertionErrors (see stack traces here: https://gist.github.com/iconara/7917438) over and over again. The client lost connection with the nodes at roughly the same time. The nodes were still up, and even if no clients were connected to them they continued logging the same errors over and over.

The errors are in the native transport (specifically MessagingService.addCallback) which makes me suspect that it has something to do with a test that we started running this afternoon. I've just implemented support for frame compression in my CQL driver cql-rb. About two hours before this happened I deployed a version of the application which enabled Snappy compression on all frames larger than 64 bytes. It's not impossible that there is a bug somewhere in the driver or compression library that caused this -- but at the same time, it feels like it shouldn't be possible to make C* a zombie with a bad frame.

Restarting seems to have got them back running again, but I suspect they will go down again sooner or later.","Cassandra 2.0.2 DCE, Cassandra 1.2.15",,,,,,,,,,,,,,,,,,,,,,,22/Apr/14 21:05;brandon.williams;6476.txt;https://issues.apache.org/jira/secure/attachment/12641331/6476.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-12 11:38:49.649,,,no_permission,,,,,,,,,,,,363172,,,Thu Apr 24 15:31:35 UTC 2014,,,,,,0|i1qlof:,363478,,,,,,,,benedict,benedict,,,1.2.11,,,,,,,"12/Dec/13 11:38;slebresne;MessagingService ain't the native transport (fyi, the native transport code doesn't leak outside the org.apache.cassandra.transport package), it's the intra-cluster messaging. In fact the stack trace shows that the write that trigger it don't even come from the native protocol but from thrift (which means you either use thrift for some things or something is whack).

But truth is, given the stack trace, where the writes comes from doesn't matter.  The assertion that fails is the line
{noformat}
assert previous == null;
{noformat}
in MessagingService.addCallback. And that's where things stop to make sense to me. This means that we tried to add a new message to the callback map but there was one with the same messageId already. Except that messageId is very straighforwardly generated by an {{incrementAndGet}} on an static AtomicInteger. And as far as I can tell, no other code inserts in the callback map without grabing a new messageId this way (except setCallbackForTests, but it does is only use in a unit test).

Therefore, it seems the only way such messageId conflict could happen is that we've gone full cycle on the AtomicInteger and hit the same id again. But entries in callbacks expire after the rpc timeout, so that implies > 4 billions requests in about 10 seconds. Sounds pretty unlikely to me.

But I might be missing something obvious: [~jbellis], I believe you might be more familiar with MessagingService, any idea?
","12/Dec/13 12:06;iconara;Sorry, there was another stack trace I meant to attach to the same gist that said something about the native transport. I've added it now: https://gist.github.com/iconara/7917438 (see the second file). Those errors started with ""ERROR [Native-Transport-Requests:7924]"" which made me make the connection between us changing to compressed requests and the errors (since cql-rb only runs over the CQL protocol).

I've looked at the logs but my untrained eyes don't find any more hints as to what happened. I can post the full logs if that helps you.","12/Dec/13 15:41;jbellis;bq. that's where things stop to make sense to me. This means that we tried to add a new message to the callback map but there was one with the same messageId already. Except that messageId is very straighforwardly generated by an incrementAndGet on an static AtomicInteger

Right.  I'm not sure why that assert even exists TBH; I have some idea that in the super distant past SP used to manually inject callbacks in some cases but if so that code is long dead.

Is it possible a bug in native compression code is corrupting random crap elsewhere in the JVM?","12/Dec/13 16:18;slebresne;bq. Is it possible a bug in native compression code is corrupting random crap elsewhere in the JVM?

I have no clue, that would be a pretty serious JVM bug imo if that was the case. It would also be uncanny for ""random corrupted crap"" to trigger the same assertion on different nodes (but well, everything is possible). All I can say in that matter is that the native protocol uses the same compression libs than sstable compression and in basically the same way.",12/Dec/13 17:14;jbellis;[~iconara] did you see the MS asserts on multiple nodes?,"12/Dec/13 19:35;iconara;[~jbellis] Yes, two out of three nodes got the same assertion failures within a minute or two.

I've updated the gist (https://gist.github.com/iconara/7917438) with the full logs (10,000 lines) from the two nodes. The third node has nothing in its logs around the same time (it's all just INFO and nothing that stands out).","12/Dec/13 20:31;jbellis;Huh.  Well, I added some extra detail to the assert in c4d3a313885f14e802247b9354aafa4caaae9804.  Maybe that will show a clue.","18/Apr/14 10:12;kohlisankalp;We saw these asserts on one node for sometime and then it went away. This is 1.2.15. We had some network problem around the same time. Don't know whether that is related. 
java.lang.AssertionError
	at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:541)
	at org.apache.cassandra.service.StorageProxy.sendMessagesToOneDCInternal(StorageProxy.java:638)
	at org.apache.cassandra.service.StorageProxy.sendMessagesToOneDC(StorageProxy.java:603)
	at org.apache.cassandra.service.StorageProxy.sendToHintedEndpoints(StorageProxy.java:530)
	at org.apache.cassandra.service.StorageProxy$2.apply(StorageProxy.java:121)
	at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:384)
	at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:191)
	at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:866)
	at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:849)
	at org.apache.cassandra.thrift.CassandraServer.internal_remove(CassandraServer.java:813)
	at org.apache.cassandra.thrift.CassandraServer.remove(CassandraServer.java:834)
	at org.apache.cassandra.thrift.Cassandra$Processor$remove.getResult(Cassandra.java:3642)
	at org.apache.cassandra.thrift.Cassandra$Processor$remove.getResult(Cassandra.java:3630)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)","22/Apr/14 04:41;rlow;If System.nanoTime() went backwards then it could cause this. Messages won't be expired from the map and once we've sent 2 billion messages we'll reuse IDs. The instance Sankalp pasted the error from had processed approx 1 billion messages so it's conceivable it has added 2bn callbacks.

System.nanoTime() shouldn't go backwards though. It might on really old kernels but not on ours. But it's possible a hardware issue or kernel bug could cause this. It would have to be something correlated between nodes to explain Theo's issue.","22/Apr/14 20:30;benedict;Isn't this most likely a duplicate of CASSANDRA-6948? Hit by never bouncing your node between bootstrap and hitting 4B+ messages, coupled with some dropped messages along the way, caused by shutting down the expiringmap reaper during bootstrap","22/Apr/14 20:43;rlow;Yes, most likely it is. We see it correlated across nodes that were bootstrapped at the same time, which makes sense.",22/Apr/14 20:46;benedict;Want to backport your patch [~brandon.williams]?,"22/Apr/14 20:52;brandon.williams;Unfortunately, I don't think your analysis is correct, because in 1.2 we only spin MS up/down for replace, not bootstrap.  The bootstrap shadow round was added in CASSANDRA-5571 which is 2.0-only.","22/Apr/14 20:56;brandon.williams;[~rlow] are you doing a bootstrap, or are you doing a replace?","22/Apr/14 20:57;rlow;Sorry, the instances we've seen were replaced, not bootstrapped. This is on 1.2.15.

Theo, had you replaced instances too?",22/Apr/14 21:05;brandon.williams;Half a backport from CASSANDRA-6948 to only affect replace.,24/Apr/14 09:04;benedict;LGTM,24/Apr/14 15:31;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayIndexOutOfBoundsException on range query from client,CASSANDRA-6470,12683839,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,enrico.scalavino,enrico.scalavino,10/Dec/13 13:39,12/Mar/19 14:17,13/Mar/19 22:29,29/Jan/14 18:25,2.0.5,,,,,,2,,,,,"schema: 
{noformat}
CREATE TABLE inboxkeyspace.inboxes(user_id bigint, message_id bigint, thread_id bigint, network_id bigint, read boolean, PRIMARY KEY(user_id, message_id)) WITH CLUSTERING ORDER BY (message_id DESC);
CREATE INDEX ON inboxkeyspace.inboxes(read);
{noformat}

query: 
{noformat}
SELECT thread_id, message_id, network_id FROM inboxkeyspace.inboxes WHERE user_id = ? AND message_id < ? AND read = ? LIMIT ? 
{noformat}

The query works if run via cqlsh. However, when run through the datastax client, on the client side we get a timeout exception and on the server side, the Cassandra log shows this exception: 

{noformat}
ERROR [ReadStage:4190] 2013-12-10 13:18:03,579 CassandraDaemon.java (line 187) Exception in thread Thread[ReadStage:4190,5,main]
java.lang.RuntimeException: java.lang.ArrayIndexOutOfBoundsException: 0
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1940)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
        at org.apache.cassandra.db.filter.SliceQueryFilter.start(SliceQueryFilter.java:261)
        at org.apache.cassandra.db.index.composites.CompositesSearcher.makePrefix(CompositesSearcher.java:66)
        at org.apache.cassandra.db.index.composites.CompositesSearcher.getIndexedIterator(CompositesSearcher.java:101)
        at org.apache.cassandra.db.index.composites.CompositesSearcher.search(CompositesSearcher.java:53)
        at org.apache.cassandra.db.index.SecondaryIndexManager.search(SecondaryIndexManager.java:537)
        at org.apache.cassandra.db.ColumnFamilyStore.search(ColumnFamilyStore.java:1669)
        at org.apache.cassandra.db.PagedRangeCommand.executeLocally(PagedRangeCommand.java:109)
        at org.apache.cassandra.service.StorageProxy$LocalRangeSliceRunnable.runMayThrow(StorageProxy.java:1423)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1936)
        ... 3 more
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,28/Jan/14 23:16;enigmacurry;6470-reproduced.tar.gz;https://issues.apache.org/jira/secure/attachment/12625763/6470-reproduced.tar.gz,29/Jan/14 14:50;slebresne;6470.txt;https://issues.apache.org/jira/secure/attachment/12625871/6470.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-12-12 19:32:31.297,,,no_permission,,,,,,,,,,,,362911,,,Wed Jan 29 18:25:44 UTC 2014,,,,,,0|i1qk2v:,363217,,,,,,,,lyubent,lyubent,,,,,,,,,,"12/Dec/13 19:32;jbellis;Can you reproduce, Ryan?","12/Dec/13 20:19;enigmacurry;[~enrico.scalavino] What version of Cassandra and datastax driver are you using? I'll try and recreate this, but if you have a test already written that is easily decoupled from your project can you post that too?",12/Dec/13 22:38;jjordan;[~enrico.scalavino] are you using version 2.0.X of C* and the driver?  The limit being a bind parameter isn't supported unless you are. It may be confusing the driver if you are using the 1.0.X version of the driver (not sure what error gets thrown if you do that).,"13/Dec/13 03:09;marcostrama;I get the same error. I dont know when it has been started. I'm using Cassandra 2.0.2 and Datastax Java Driver 2.0.0-beta2. Query works in cqlsh but fail when running in the client. I tried to re-create (DROP/CREATE) the column family, but the error stills.

=============
Table layout:

cqlsh:pollkan> desc table observed;

CREATE TABLE observed (
  observed timeuuid,
  observer timeuuid,
  blocked boolean,
  PRIMARY KEY (observed, observer)
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='99.0PERCENTILE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};

CREATE INDEX observedBlocked ON observed (blocked);

=============
Query in the cqlsh:

cqlsh:pollkan> SELECT observer FROM observed WHERE observed = fa93c210-4bff-11e3-b48f-5714d8c6f3b2 AND observer > 00000000-0000-1000-0000-000000000000 and blocked = false LIMIT 10000;

 observer
--------------------------------------
 43814f60-5bb1-11e3-97c8-ad396a9e8180

(1 rows)

=============
Query in the client log:

2013-12-13/00:53:03.039/BRST [timeline_1] DEBUG br.com.pollkan.batch.CqlCommands Execute query [SELECT observer FROM observed WHERE observed = ? AND observer > ? and blocked = ? LIMIT 10000;] arguments [[fa93c210-4bff-11e3-b48f-5714d8c6f3b2][00000000-0000-1000-0000-000000000000][false]]

=============
Error in cassandra:

ERROR [ReadStage:52] 2013-12-13 01:04:56,799 CassandraDaemon.java (line 187) Exception in thread Thread[ReadStage:52,5,main]
java.lang.RuntimeException: java.lang.ArrayIndexOutOfBoundsException: 0
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1931)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
        at org.apache.cassandra.db.filter.SliceQueryFilter.start(SliceQueryFilter.java:261)
        at org.apache.cassandra.db.index.composites.CompositesSearcher.makePrefix(CompositesSearcher.java:66)
        at org.apache.cassandra.db.index.composites.CompositesSearcher.getIndexedIterator(CompositesSearcher.java:101)
        at org.apache.cassandra.db.index.composites.CompositesSearcher.search(CompositesSearcher.java:53)
        at org.apache.cassandra.db.index.SecondaryIndexManager.search(SecondaryIndexManager.java:537)
        at org.apache.cassandra.db.ColumnFamilyStore.search(ColumnFamilyStore.java:1649)
        at org.apache.cassandra.db.PagedRangeCommand.executeLocally(PagedRangeCommand.java:109)
        at org.apache.cassandra.service.StorageProxy$LocalRangeSliceRunnable.runMayThrow(StorageProxy.java:1414)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1927)
        ... 3 more

=============
Error from driver log:

2013-12-13/01:05:06.798/BRST [timeline_1] ERROR br.com.pollkan.batch.CqlCommands Exception! [Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)]
com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)
        at com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:69)
        at com.datastax.driver.core.ResultSetFuture.extractCauseFromExecutionException(ResultSetFuture.java:271)
        at com.datastax.driver.core.ResultSetFuture.getUninterruptibly(ResultSetFuture.java:187)
        at com.datastax.driver.core.Session.execute(Session.java:126)
        at br.com.pollkan.batch.CqlCommands.executeQuery(CqlCommands.java:149)
        at br.com.pollkan.batch.BaseBatch.processChild(BaseBatch.java:364)
        at br.com.pollkan.batch.BaseBatch.run(BaseBatch.java:640)
        at java.lang.Thread.run(Thread.java:722)

If need more information, please let me know. Tks","13/Dec/13 11:22;enrico.scalavino;[~jjordan] [~enigmacurry] I am using datastax 2.0.0-rc1 and Cassandra 2.0.3. The query works if I remove the where clause on either the read attribute or on the message_id attribute. 
Unfortunately the code calling the query is buried deep in the project, and the tests only test high level apis. ",13/Dec/13 13:02;dmnorc;I get the same error too. I'm using Cassandra 2.0.2 and Datastax Java Driver 2.0.0-beta2,"18/Dec/13 23:56;marcostrama;I removed the ""blocked"" column from the query (indexed column) and now it works. This helps?","19/Dec/13 10:47;enrico.scalavino;No, also my query works if I remove one of the conditions. That still does not explain why that happens, and why there is an unmanaged exception on the server. ","28/Jan/14 23:16;enigmacurry;I uploaded an Eclipse project (6470-reproduced.tar.gz) that reproduces this issue.

Start up a ccm cluster: 

{code}
ccm create -v git:cassandra-2.0.2
ccm create -v git:cassandra-2.0.2 test
ccm populate -n 3:3
ccm start
{code}

Then run Bug6470Client.

This code actually works with driver version 1.0.3, so I wonder if that's not the cause instead of C*...

I can also confirm that the same queries issued in cqlsh do not repro this issue.","28/Jan/14 23:24;enigmacurry;I also tried from the latest python driver, no problem there.",29/Jan/14 09:07;slebresne;This is likely a problem with the paging over 2ndary indexes (which is why only the 2.0 version of the driver is running into it). I'll have a closer look.,"29/Jan/14 14:50;slebresne;We were not handling empty bounds in DataRange.sliceForKey() (that is indeed used by paging calls) which was returning an empty slice array (which was incorrect but hence the error).

Attached simple fix.",29/Jan/14 17:50;lyubent;+1,"29/Jan/14 18:25;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraStorage should not assume all DataBags are DefaultDataBags,CASSANDRA-6420,12681825,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mps,mps,mps,29/Nov/13 02:41,12/Mar/19 14:17,13/Mar/19 22:29,13/Dec/13 18:15,1.2.14,2.0.4,,,,,0,pig,,,,"CassandraStorage improperly assumes all DataBags are DefaultDataBags. As a result, natural Pig code can't be used with CassandraStorage. For example:
{quote}
{{B = FOREACH A GENERATE $0, TOBAG(TOTUPLE($1, $2));}}
{{STORE B into  'cassandra://MyKeySpace/MyColumnFamily' using CassandraStorage();}}
{quote}
fails with a complaint that a {{NonSpillableDataBag}} can't be converted into a {{DefaultDataBag}}.

Since the {{CassandraStorage}} code only calls methods from {{DataBag}}, there is no need for this artifical restriction. After applying the attached patch, the above code works fine, making CassandraStorage much easier to use.

This is my first submission to Cassandra, so I apologize for any incorrect process. Please let me know what I should do differently. In particular, I am a little unclear where I should put the test. I am thinking I should put it in ThriftColumnFamilyTest.java. Is this correct or should it be somewhere else? I'll create a test as soon as I understand. ",All environments,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,29/Nov/13 02:43;mps;patch.txt;https://issues.apache.org/jira/secure/attachment/12616333/patch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-13 18:15:13.895,,,no_permission,,,,,,,,,,,,361089,,,Mon Dec 23 05:24:03 UTC 2013,,,,,,0|i1q8u7:,361388,1.2.11,2.1 rc3,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"29/Nov/13 02:44;mps;I have attached the diff, but I am afraid  I am a raw newbie and am not following the correct conventions. I apologize for the inconvenience and would greatly appreciate any guidance in this regard. ","30/Nov/13 19:53;mps;When I initially filed this issue, I wasn't sure whether it should be classified as a bug or and improvement. After reading http://www.datastax.com/docs/datastax_enterprise1.0/about_pig, I think it should be classified as a bug, because the data {{B}} in the issue description is in a form that this link says should be suitable for storing into Cassandra.","13/Dec/13 18:15;brandon.williams;Nice catch, committed.","23/Dec/13 05:24;mps;Thanks, Brandon!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Snapshot repair blocks for ever if something happens to the ""I made my snapshot"" response",CASSANDRA-6415,12681617,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,jjordan,jjordan,27/Nov/13 17:47,12/Mar/19 14:17,13/Mar/19 22:29,02/Dec/13 18:52,1.2.13,2.0.4,,,,,1,repair,,,,"The ""snapshotLatch.await();"" can be waiting for ever and block all repair operations indefinitely if something happens that another node doesn't respond.

{noformat}
            public void makeSnapshots(Collection<InetAddress> endpoints)
            {
                try
                {
                    snapshotLatch = new CountDownLatch(endpoints.size());
                    IAsyncCallback callback = new IAsyncCallback()
                    {
                        public boolean isLatencyForSnitch()
                        {
                            return false;
                        }

                        public void response(MessageIn msg)
                        {
                            RepairJob.this.snapshotLatch.countDown();
                        }
                    };
                    for (InetAddress endpoint : endpoints)
                        MessagingService.instance().sendRR(new SnapshotCommand(tablename, cfname, sessionName, false).createMessage(), endpoint, callback);
                    snapshotLatch.await();
                    snapshotLatch = null;
                }
                catch (InterruptedException e)
                {
                    throw new RuntimeException(e);
                }
            }
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,02/Dec/13 16:57;yukim;6415-1.2.txt;https://issues.apache.org/jira/secure/attachment/12616573/6415-1.2.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-27 17:56:30.159,,,no_permission,,,,,,,,,,,,360881,,,Fri Apr 18 20:47:32 UTC 2014,,,,,,0|i1q7kf:,361180,1.2.10,,,,,,,jbellis,jbellis,,,,,,,,,,"27/Nov/13 17:56;yukim;In detail, 'makeSnapshot' happens after receiving all TreeResponses which are handled on single-threaded 'ANTI_ENTROPY' stage. So when it stuck, receiving repair messages or Merkle tree comparisons are all blocked afterward.","02/Dec/13 16:57;yukim;I think it is better to re-think use of executors and threads around repair, for example, right now Differencer runs on ANTI_ENTROPY stage one by one.

But IMO, that would be quite a change for 1.2.x, so I propose just changing Snapshot response message type to INTERNAL_RESPONSE instead of REQUEST_RESPONSE which is droppable by MessagingService. So at least snapshot request messages don't get lost in next 1.2 release.",02/Dec/13 17:28;jbellis;+1,"02/Dec/13 18:52;yukim;Committed, thanks!",05/Dec/13 23:39;jeromatron;See the longer term solution here: CASSANDRA-6455,02/Jan/14 21:58;nickmbailey;This was also fixed in the 2.0 branch in 2.0.4 correct?,02/Jan/14 22:01;jbellis;Yes.,"18/Apr/14 20:47;cywjackson;I ran into the stuck issue on 1.2.10

Upgraded to 1.2.16, I could see repair is not ""stuck"", in a sense I see multiple repair sessions/stages started and finished.

But, in the end (after waiting a long time), I see that there is no more activity from the log, and also compactionstats/netstats, but yet the tpstats still show Active and Pending count in the stages:

AntiEntropyStage                  1         2           5073         0                 0
AntiEntropySessions               1         1             44         0                 0
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in calculateNaturalEndpoints,CASSANDRA-6485,12684529,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,rspitzer,rspitzer,13/Dec/13 19:41,12/Mar/19 14:17,13/Mar/19 22:29,14/Dec/13 04:10,1.2.13,2.0.4,,,,,0,,,,,"I was running a test where I added a new data center to an existing cluster. 

Test outline:
Start 25 Node DC1
Keyspace Setup Replication 3
Begin insert against DC1 Using Stress
While the inserts are occuring
Start up 25 Node DC2
Alter Keyspace to include Replication in 2nd DC
Run rebuild on DC2
Wait for stress to finish
Run repair on Cluster
... Some other operations

Although there are no issues with smaller clusters or clusters without vnodes, Larger setups with vnodes seem to consistently see the following exception in the logs as well as a write operation failing for each exception. Usually this happens between 1-8 times during an experiment. 

The exceptions/failures are Occurring when DC2 is brought online but *before* any alteration of the Keyspace. All of the exceptions are happening on DC1 nodes. One of the exceptions occurred on a seed node though this doesn't seem to be the case most of the time. 

While the test was running, nodetool was run every second to get cluster status. At no time did any nodes report themselves as down. 


{code}
ystem_logs-107.21.186.208/system.log-ERROR [Thrift:1] 2013-12-13 06:19:52,647 CustomTThreadPoolServer.java (line 217) Error occurred during processing of message.
system_logs-107.21.186.208/system.log:java.lang.NullPointerException
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:128)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:2624)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:375)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:190)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:866)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:849)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:749)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.getResult(Cassandra.java:3690)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.getResult(Cassandra.java:3678)
system_logs-107.21.186.208/system.log-	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
system_logs-107.21.186.208/system.log-	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
system_logs-107.21.186.208/system.log-	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
system_logs-107.21.186.208/system.log-	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
system_logs-107.21.186.208/system.log-	at java.lang.Thread.run(Thread.java:724)
{code}",,,,,,,,,,,,,,,,,,,,,,,,13/Dec/13 19:51;jbellis;6485.txt;https://issues.apache.org/jira/secure/attachment/12618670/6485.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-13 19:52:35.265,,,no_permission,,,,,,,,,,,,363601,,,Mon Dec 16 16:19:33 UTC 2013,,,,,,0|i1qoav:,363907,1.2.13,,,,,,,rbranson,rbranson,,,1.2.13,,,,,,,13/Dec/13 19:52;jbellis;It's possible for TM to get nulled out after we check it.  Cache a reference to any non-null TM we observe to fix.,14/Dec/13 00:56;rbranson;LGTM.,14/Dec/13 04:10;jbellis;committed,16/Dec/13 16:19;rspitzer;Patch worked on my test. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Paging queries with IN on the partition key is broken,CASSANDRA-6464,12683632,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,09/Dec/13 17:00,12/Mar/19 14:17,13/Mar/19 22:29,18/Dec/13 10:24,2.0.4,,,,,,2,,,,,"Feels like MultiPartitionPager (which handles paging queries when there is a IN on the partition key) has completely missed CASSANDRA-5714's train. As a result, it completely broken and will typically loop infinitely.

Attaching patch to fix.",,,,,,,,,,,,,,,,,,,,,,,,09/Dec/13 17:01;slebresne;6464.txt;https://issues.apache.org/jira/secure/attachment/12617857/6464.txt,17/Dec/13 20:44;iamaleksey;redundant-stuff.txt;https://issues.apache.org/jira/secure/attachment/12619163/redundant-stuff.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-12-17 20:44:01.83,,,no_permission,,,,,,,,,,,,362704,,,Wed Dec 18 10:24:01 UTC 2013,,,,,,0|i1qirb:,362998,,,,,,,,iamaleksey,iamaleksey,,,2.0 beta 1,,,,,,,"17/Dec/13 20:44;iamaleksey;+1. Attaching a minor nitty patch, removing some redundant constructors and variables.","18/Dec/13 10:24;slebresne;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Concurrency issue in Directories.getOrCreate(),CASSANDRA-6459,12683214,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,rwfowler,rwfowler,rwfowler,06/Dec/13 22:52,12/Mar/19 14:17,13/Mar/19 22:29,07/Dec/13 10:33,1.2.13,2.0.4,,,,,0,,,,,"We're seeing an FSWriteError sometimes during repairs. I think it's because two threads are calling Directories.getOrCreate() on the same directory at about the same time. File.mkdirs() returns false if the directory already exists.

A patch is forthcoming.

Here's the exception we get:

ERROR 2013-12-06 09:31:45,217 [Thread-11051] CassandraDaemon Exception in thread Thread[Thread-11051,5,main]
FSWriteError in /data-1/cassandra/data/RedDeerCollege/Binaries/backups
        at org.apache.cassandra.db.Directories.getOrCreate(Directories.java:483)
        at org.apache.cassandra.db.Directories.getBackupsDirectory(Directories.java:242)
        at org.apache.cassandra.db.DataTracker.maybeIncrementallyBackup(DataTracker.java:165)
        at org.apache.cassandra.db.DataTracker.addSSTables(DataTracker.java:237)
        at org.apache.cassandra.db.ColumnFamilyStore.addSSTables(ColumnFamilyStore.java:911)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:186)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:138)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:238)
        at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:178)

Someone else appears to have seen the same thing a while back:

Here's someone that's had a similar problem:

http://mail-archives.apache.org/mod_mbox/cassandra-user/201206.mbox/%3CA65C3B25-2866-48BE-8584-AB048663611C@thelastpickle.com%3E
",1.2 2.0,,,,,,,,,,,,,,,,,,,,,,,06/Dec/13 22:53;rwfowler;CASSANDRA-6459.txt;https://issues.apache.org/jira/secure/attachment/12617496/CASSANDRA-6459.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-07 10:33:12.3,,,no_permission,,,,,,,,,,,,362466,,,Sat Dec 07 10:33:12 UTC 2013,,,,,,0|i1qhan:,362760,1.2.9,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"06/Dec/13 22:56;rwfowler;I attached a patch and included a test method.

For a concurrency bug, reproducing the problem wasn't actually too hard. It doesn't happen every time, but it happens often enough.","07/Dec/13 10:33;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Batchlog endpoint candidates should be picked randomly, not sorted by proximity",CASSANDRA-6481,12684310,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,12/Dec/13 17:47,12/Mar/19 14:17,13/Mar/19 22:29,12/Dec/13 17:57,1.2.13,2.0.4,,,,,0,,,,,"Batchlog endpoint candidates should be picked randomly, not sorted by proximity. I'll be lazy and just copy-paste some lines from IRC:

[20:23:27] rbranson:	 is there an issue where batch logs tend to get written to a subset of the nodes?
[20:28:04] rbranson:	 I mean all the write batches are going thru 10% of the nodes
[20:28:16] rbranson:	 it means writes won't scale linearly w/the cluster size

Attaching a trivial patch.",,,,,,,,,,,,,,,,,,,,,,,,12/Dec/13 17:48;iamaleksey;6481.txt;https://issues.apache.org/jira/secure/attachment/12618447/6481.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-12 17:51:51.62,,,no_permission,,,,,,,,,,,,363382,,,Thu Dec 12 18:03:06 UTC 2013,,,,,,0|i1qmyv:,363688,,,,,,,,jbellis,jbellis,,,,,,,,,,"12/Dec/13 17:51;jbellis;If dsnitch actually worked then I think this would be fine, but fixing dsnitch is probably out of scope here so +1","12/Dec/13 17:54;jasobrown;+1 on the patch, as well.

[~jbellis] In what way do you think dsnitch doesn't work?","12/Dec/13 17:57;iamaleksey;Committed, thanks.","12/Dec/13 18:03;jbellis;bq. In what way do you think dsnitch doesn't work

CASSANDRA-6465",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batchlog writes consume unnecessarily large amounts of CPU on vnodes clusters,CASSANDRA-6488,12684588,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,rbranson,rbranson,rbranson,14/Dec/13 00:49,12/Mar/19 14:17,13/Mar/19 22:29,17/Dec/13 14:50,1.2.13,2.0.4,,,,,0,,,,,"The cloneTokenOnlyMap call in StorageProxy.getBatchlogEndpoints causes enormous amounts of CPU to be consumed on clusters with many vnodes. I created a patch to cache this data as a workaround and deployed it to a production cluster with 15,000 tokens. CPU consumption drop to 1/5th. This highlights the overall issues with cloneOnlyTokenMap() calls on vnodes clusters. I'm including the maybe-not-the-best-quality workaround patch to use as a reference, but cloneOnlyTokenMap is a systemic issue and every place it's called should probably be investigated.",,,,,,,,,,,,,,,,,,,,,,,,17/Dec/13 14:25;iamaleksey;6488-fix.txt;https://issues.apache.org/jira/secure/attachment/12619106/6488-fix.txt,14/Dec/13 00:49;rbranson;6488-rbranson-patch.txt;https://issues.apache.org/jira/secure/attachment/12618736/6488-rbranson-patch.txt,14/Dec/13 15:38;jbellis;6488-v2.txt;https://issues.apache.org/jira/secure/attachment/12618777/6488-v2.txt,15/Dec/13 10:24;iamaleksey;6488-v3.txt;https://issues.apache.org/jira/secure/attachment/12618811/6488-v3.txt,14/Dec/13 00:51;rbranson;graph (21).png;https://issues.apache.org/jira/secure/attachment/12618737/graph+%2821%29.png,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2013-12-14 15:38:08.88,,,no_permission,,,,,,,,,,,,363660,,,Tue Dec 17 16:18:16 UTC 2013,,,,,,0|i1qonz:,363966,1.2.11,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,14/Dec/13 00:51;rbranson;CPU usage dropping on a production cluster after the attached patch is rolled out.,14/Dec/13 15:38;jbellis;v2 to move the caching logic inside cloneOnlyTokenMap,14/Dec/13 15:39;jbellis;NB: I'm not sure what the changes to candidates/chosenEndpoints do so I've left that out for now.,15/Dec/13 10:24;iamaleksey;v3 merges both and has some minor (stylistic) changes to SP on top.,15/Dec/13 10:31;iamaleksey;Committed in 4be9e6720d9f94a83aa42153c3e71ae1e557d2d9.,"16/Dec/13 16:08;mshuler;This introduced a failure in BootStrapperTest:

{code}
test:
     [echo] running unit tests
    [mkdir] Created dir: /home/mshuler/git/cassandra/build/test/cassandra
    [mkdir] Created dir: /home/mshuler/git/cassandra/build/test/output
    [junit] WARNING: multiple versions of ant detected in path for junit 
    [junit]          jar:file:/usr/share/ant/lib/ant.jar!/org/apache/tools/ant/Project.class
    [junit]      and jar:file:/home/mshuler/git/cassandra/build/lib/jars/ant-1.6.5.jar!/org/apache/tools/ant/Project.class
    [junit] Testsuite: org.apache.cassandra.dht.BootStrapperTest
    [junit] Tests run: 4, Failures: 1, Errors: 0, Time elapsed: 6.177 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit]  WARN 09:47:46,135 No host ID found, created 9019bb70-4d6e-4cf6-b730-140ff5ae4be5 (Note: This should happen exactly once per node).
    [junit]  WARN 09:47:46,262 Generated random token [d9180feb2e806704effa4024e8f4c631]. Random tokens will result in an unbalanced ring; see http://wiki.apache.org/cassandra/Operations
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testSourceTargetComputation(org.apache.cassandra.dht.BootStrapperTest):   FAILED
    [junit] expected:<1> but was:<0>
    [junit] junit.framework.AssertionFailedError: expected:<1> but was:<0>
    [junit]     at org.apache.cassandra.dht.BootStrapperTest.testSourceTargetComputation(BootStrapperTest.java:212)
    [junit]     at org.apache.cassandra.dht.BootStrapperTest.testSourceTargetComputation(BootStrapperTest.java:173)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.dht.BootStrapperTest FAILED

BUILD FAILED
/home/mshuler/git/cassandra/build.xml:1113: The following error occurred while executing this line:
/home/mshuler/git/cassandra/build.xml:1078: Some unit test(s) failed.

Total time: 9 seconds
((4be9e67...)|BISECTING)mshuler@hana:~/git/cassandra$ git bisect bad
4be9e6720d9f94a83aa42153c3e71ae1e557d2d9 is the first bad commit
commit 4be9e6720d9f94a83aa42153c3e71ae1e557d2d9
Author: Aleksey Yeschenko <aleksey@apache.org>
Date:   Sun Dec 15 13:29:56 2013 +0300

    Improve batchlog write performance with vnodes
    
    patch by Jonathan Ellis and Rick Branson; reviewed by Aleksey Yeschenko
    for CASSANDRA-6488

:100644 100644 e5865925f160faabc2506c3a5aac9985c17c1658 b55393b2ed138011bab52f95f2e9b52107709938 M      CHANGES.txt
:040000 040000 dea10aa8044e10eb60002e75f2586a9c8e94b647 7030c09f9713bd3e342e4e012c59b09c86b79a42 M      src
{code}","16/Dec/13 16:18;mshuler;I'm working on the cassandra-2.0 branch, since I didn't mention it above. Around the same time, LeaveAndBootstrapTest, MoveTest, and RelocateTest were new failures - I'm looking at those
- http://cassci.datastax.com/job/cassandra-2.0_test/49/console
","16/Dec/13 16:22;iamaleksey;So, the caching part. [~jbellis] can you have a look? If not, I will, later, but it's potentially 1.2.13 vote-affecting.","16/Dec/13 16:31;mshuler;Commit bb09d3c fully passed all the unit tests in cassandra-2.0 branch.
- http://cassci.datastax.com/job/cassandra-2.0_test/47/console","16/Dec/13 16:47;mshuler;Those same tests look like new failures with this commit in cassandra-1.2 branch also
- http://cassci.datastax.com/job/cassandra-1.2_test/32/console
vs.
- http://cassci.datastax.com/job/cassandra-1.2_test/33/console

(edit for clarity) New unit test failures in c-2.0 and c-1.2 branches with this commit:
- BootStrapperTest
- LeaveAndBootstrapTest
- MoveTest
- RelocateTest","17/Dec/13 14:25;iamaleksey;Separates TM.cloneOnlyTokenMap() and TM.cachedOnlyTokenMap() and only switched SP.getBatchlogEndpoints() and ARS.getNaturalEndpoints() to use the cached version.

They aren't the only methods that *don't* mutate the returned metadata, but going through the rest of the usages and optimizing those can wait.

Also fixes a regression from 6435 in TM.cachedOnlyTokenMap().",17/Dec/13 14:50;jbellis;updated comments and committed,"17/Dec/13 16:18;mshuler;cassandra-1.2 branch, commit 13348c4, is passing these 4 unit tests:
- http://cassci.datastax.com/job/cassandra-1.2_test/35/console

cassandra-2.0 is passing these, also
- http://cassci.datastax.com/job/cassandra-2.0_test/50/console

Thanks all!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LOCAL_QUORUM still doesn't work with SimpleStrategy but don't throw a meaningful error message anymore,CASSANDRA-6545,12687163,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,alexliu68,slebresne,slebresne,03/Jan/14 11:07,12/Mar/19 14:17,13/Mar/19 22:29,08/Jan/14 16:25,1.2.14,,,,,,0,,,,,"It seems it was the intent of CASSANDRA-6238 originally, though I've tracked to the commit of CASSANDRA-6309 (f7efaffadace3e344eeb4a1384fa72c73d8422b0 to be precise) but in any case, ConsistencyLevel.validateForWrite does not reject LOCAL_QUORUM when SimpleStrategy is used anymore, yet ConsistencyLevel.blockFor definitively cast the strategy to NTS for LOCAL_QUORUM (in localQuorumFor() to be precise). Which results in a ClassCastException as reported by https://datastax-oss.atlassian.net/browse/JAVA-241.

Note that while we're at it, I tend to agree with Aleksey comment on CASSANDRA-6238, why not make EACH_QUORUM == QUORUM for SimpleStrategy too?",,,,,,,,,,,,,,,,,,,,,,,,06/Jan/14 19:44;alexliu68;6545-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12621653/6545-1.2-branch.txt,06/Jan/14 20:04;alexliu68;6545-2.0-branch.txt;https://issues.apache.org/jira/secure/attachment/12621660/6545-2.0-branch.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-01-03 22:26:48.527,,,no_permission,,,,,,,,,,,,366158,,,Wed Jan 08 16:25:29 UTC 2014,,,,,,0|i1r473:,366469,,,,,,,,slebresne,slebresne,,,1.2.13,,,,,,,03/Jan/14 22:26;kmueller;We were also bit by this in my duplicate issue 6549,"06/Jan/14 18:38;alexliu68;Lift both EACH_QUORUM and LOCAL_QUORUM. Cast strategy to NTS only if it's an instance of NTS, otherwise don't cast it and use it the same way as QUORUM.",06/Jan/14 22:51;j.casares;I'm also sometimes seeing this in 2.0.4 while testing the java-driver. If I try and rerun the test that fails by itself it will pass.,"08/Jan/14 16:25;slebresne;The changes looks good, though one problem remained in that DatacenterSyncWriteResponseHandler, used by EACH_QUORUM, was still asserting NetworkTopologyStrategy. Anyway, it's simple enough to just fallback on the simple WriteResponseHandler with SimpleStrategy so committed with that added (and a few minor cosmetic change to the original patch). Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift should validate SliceRange start and finish lengths,CASSANDRA-6521,12686065,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,benbromhead,benbromhead,benbromhead,22/Dec/13 16:54,12/Mar/19 14:17,13/Mar/19 22:29,22/Dec/13 17:11,1.2.14,2.0.4,,,,,0,,,,,"To quote [~benbromhead]:

bq. It appears that Cassandra does not check the length of a column name that is part of a range predicate for a *_slice query before it serialises the slice query to pass to the replicas. Names with a length greater than 0xFFFF cause an assertion error to occur in ByteBufferUtil.writeWithShortLength. This further causes subsequent reads on the node to fail until Cassandra is restarted",,,,,,,,,,,,,,,,,,,,,,,,22/Dec/13 17:01;iamaleksey;6521.txt;https://issues.apache.org/jira/secure/attachment/12620094/6521.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-22 17:01:38.057,,,no_permission,,,,,,,,,,,,365017,,,Sun Dec 22 17:11:42 UTC 2013,,,,,,0|i1qx1b:,365326,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,22/Dec/13 17:01;iamaleksey;Attaching Ben's patch with several modifications.,"22/Dec/13 17:11;iamaleksey;Committed, thanks, Ben.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CQLSSTableWriter addRow(Map<String, Object> values) does not work as documented.",CASSANDRA-6526,12686352,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,yarix,yarix,25/Dec/13 13:09,12/Mar/19 14:17,13/Mar/19 22:29,06/Mar/14 09:14,2.0.6,,,,,,0,,,,,"There are 2 bugs in the method
{code}
addRow(Map<String, Object> values)
{code}
First issue is that the map <b>must</b> contain all the column names as keys in the map otherwise the addRow fails (with InvalidRequestException ""Invalid number of arguments, expecting %d values but got %d"").

Second Issue is that the keys in the map must be in lower-case otherwise they may not be found in the map, which will result in a NPE during decompose.
h6. SUGGESTED SOLUTION:
Fix the addRow method with:
{code}
public CQLSSTableWriter addRow(Map<String, Object> values)
    throws InvalidRequestException, IOException
{
    int size = boundNames.size();
    Map<String, ByteBuffer> rawValues = new HashMap<>(size);
    for (int i = 0; i < size; i++) {
        ColumnSpecification spec = boundNames.get(i);
        String colName = spec.name.toString();
        rawValues.put(colName, values.get(colName) == null ? null : ((AbstractType)spec.type).decompose(values.get(colName)));
    }
    return rawAddRow(rawValues);
}
{code}
When creating the new Map for the insert we need to go over all columns and apply null to missing columns.

Fix the method documentation add this line:
{code}
     * <p>
     * Keys in the map <b>must</b> be in lower case, otherwise their value will be null.
     *
{code}",,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,03/Mar/14 11:06;slebresne;6526.txt;https://issues.apache.org/jira/secure/attachment/12632224/6526.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-25 16:45:36.484,,,no_permission,,,,,,,,,,,,365336,,,Thu Mar 06 09:14:19 UTC 2014,,,,,,0|i1qyyn:,365638,,,,,,,,thobbs,thobbs,,,2.0.2,,,,,,,"25/Dec/13 13:10;yarix;below is a test case:
{code:java}
@Test
public void testAddRow() throws Exception
{
	String KS = ""cql_keyspace"";
	String TABLE = ""table1"";
	
	File tempdir = Files.createTempDir();
	File dataDir = new File(tempdir.getAbsolutePath() + File.separator + KS + File.separator + TABLE);
	assertTrue( dataDir.mkdirs());
	
	String schema = ""CREATE TABLE cql_keyspace.table1 (""
			+ ""  k int PRIMARY KEY,""
			+ ""  v1 text,""
			+ ""  v2 int""
			+ "")"";
	String insert = ""INSERT INTO cql_keyspace.table1 (k, v1, v2) VALUES (?, ?, ?)"";
	CQLSSTableWriter writer = CQLSSTableWriter.builder()
			.inDirectory(dataDir)
			.forTable(schema)
			.withPartitioner(StorageService.instance.getPartitioner())
			.using(insert).build();
	
	Map<String, Object> values = new HashMap<>();
	values.put( ""k"", 0 );
	values.put( ""v1"", ""test1"" );
	values.put( ""v2"", 24 );
	writer.addRow( values );
	
	values.clear();  
	values.put( ""k"", 1 );
	values.put( ""v1"", ""test2"" );
//    	values.put( ""v2"", null ); //commented intentionally so that v2 will get null by the writer.
	writer.addRow( values );
	
	values.clear();
	values.put( ""k"", 2 );
	values.put( ""V1"", ""test3 - will not be found, since V1 is not lowercase"" );
	values.put( ""v2"", 42 );
	values.put( ""v3"", ""some ignored key"" );
	writer.addRow( values );
	
	writer.close();
	
	SSTableLoader loader = new SSTableLoader(dataDir, new SSTableLoader.Client()
	{
		public void init(String keyspace)
		{
			for (Range<Token> range : StorageService.instance.getLocalRanges(""cql_keyspace""))
				addRangeForEndpoint(range, FBUtilities.getBroadcastAddress());
			setPartitioner(StorageService.getPartitioner());
		}
		
		public CFMetaData getCFMetaData(String keyspace, String cfName)
		{
			return Schema.instance.getCFMetaData(keyspace, cfName);
		}
	}, new OutputHandler.SystemOutput(false, false));
	
	loader.stream().get();
	
	UntypedResultSet rs = QueryProcessor.processInternal(""SELECT * FROM cql_keyspace.table1;"");
	assertEquals(3, rs.size());
	
	Iterator<UntypedResultSet.Row> iter = rs.iterator();
	UntypedResultSet.Row row;
	
	row = iter.next();
	assertEquals(0, row.getInt(""k""));
	assertEquals(""test1"", row.getString(""v1""));
	assertEquals(24, row.getInt(""v2""));
	
	row = iter.next();
	assertEquals(1, row.getInt(""k""));
	assertEquals(""test2"", row.getString(""v1""));
	assertFalse(row.has(""v2""));
	
	row = iter.next();
	assertEquals(2, row.getInt(""k""));
	assertFalse(row.has(""v1""));
	assertEquals(42, row.getInt(""v2""));
	assertFalse(row.has(""v3""));

}

{code}","25/Dec/13 16:45;jbellis;bq. First issue is that the map <b>must</b> contain all the column names as keys in the map 

This is a bug.

bq. the keys in the map must be in lower-case 

This is not a bug.","29/Dec/13 20:33;yarix;I agree. Yet, I think that adding a documentation (JavaDoc) to the second issue can be very helpful. ",03/Mar/14 11:06;slebresne;Attaching simple patch to fix the bug mentioned and add some doc on when you should lowercase column names.,05/Mar/14 19:13;thobbs;+1,"06/Mar/14 09:14;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cannot select data which using ""WHERE""",CASSANDRA-6525,12686346,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thobbs,silence.chow,silence.chow,25/Dec/13 10:59,12/Mar/19 14:17,13/Mar/19 22:29,21/May/14 09:46,2.0.8,,,,,,0,,,,,"I am developing a system on my single machine using VMware Player with 1GB Ram and 1Gb HHD. When I select all data, I didn't have any problems. But when I using ""WHERE"" and it has just below 10 records. I have got this error in system log:

{noformat}
ERROR [ReadStage:41] 2013-12-25 18:52:11,913 CassandraDaemon.java (line 187) Exception in thread Thread[ReadStage:41,5,main]
java.io.IOError: java.io.EOFException
        at org.apache.cassandra.db.Column$1.computeNext(Column.java:79)
        at org.apache.cassandra.db.Column$1.computeNext(Column.java:64)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:88)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:37)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:82)
        at org.apache.cassandra.db.filter.QueryFilter$2.getNext(QueryFilter.java:157)
        at org.apache.cassandra.db.filter.QueryFilter$2.hasNext(QueryFilter.java:140)
        at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:144)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.<init>(MergeIterator.java:87)
        at org.apache.cassandra.utils.MergeIterator.get(MergeIterator.java:46)
        at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:120)
        at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:80)
        at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:72)
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:297)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1487)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1306)
        at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:332)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:65)
        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1401)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1936)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
Caused by: java.io.EOFException
        at java.io.RandomAccessFile.readFully(Unknown Source)
        at java.io.RandomAccessFile.readFully(Unknown Source)
        at org.apache.cassandra.io.util.RandomAccessReader.readBytes(RandomAccessReader.java:348)
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:392)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:371)
        at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:74)
        at org.apache.cassandra.db.Column$1.computeNext(Column.java:75)
        ... 27 more
{noformat}

E.g.
{{SELECT * FROM table;}}
Its fine.
{{SELECT * FROM table WHERE field = 'N';}}
field is the partition key.
Its said ""Request did not complete within rpc_timeout."" in cqlsh","Linux RHEL5
RAM: 1GB
Cassandra 2.0.3
CQL spec 3.1.1
Thrift protocol 19.38.0",,,,,,,,,,,,,,,,,,,,,,,16/May/14 18:32;thobbs;6525-2.0.txt;https://issues.apache.org/jira/secure/attachment/12645287/6525-2.0.txt,11/Apr/14 21:36;enigmacurry;6981_test.py;https://issues.apache.org/jira/secure/attachment/12639872/6981_test.py,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-12-26 16:38:33.465,,,no_permission,,,,,,,,,,,,365330,,,Fri Jun 06 11:01:08 UTC 2014,,,,,,0|i1qyxb:,365632,2.0.3,2.0.4,2.0.6,,,,,slebresne,slebresne,,,,,,,,,,26/Dec/13 16:38;jbellis;Can you describe how to reproduce starting with a fresh Cassandra install?,"28/Dec/13 11:13;silence.chow;My table have 4 field only
For example
CREATE TABLE test (
  hidden text,
  field2 text,
  field3 text,
  field4 text,
  PRIMARY KEY (hidden, field2 , field3)
);

The query using CQL3: SELECT * FROM test WHERE hidden = 'N';","30/Dec/13 07:21;silence.chow;I think I know how to reproduce my situation now.
I have created a new table same as that table. I can run the query something like SELECT * FROM test WHERE hidden = 'N'; at the beginning, After that I have run a stress test which made all the physical RAM and swap exhaust. Then, the problem happen again.
","21/Jan/14 22:54;vayasin;I got the exact same error message and problem. But for me it was a small table of about 20 rows and i did not stress test.
running a compactions fixed the problem for me. I'm not sure what the cause was but i recently dropped and recreated the table. ",14/Mar/14 14:34;enigmacurry;[~mshuler] can you reproduce?,"14/Mar/14 20:51;mshuler;I tested using cassandra-2.0 git branch on my laptop (16G), which ran fine. I tried on a 4G box and 2G box - both ran fine looping through the script below, while running cassandra-stress in another shell, also. I'm about 10 or so loops through, while looping stress read and write on a 1G virtualbox vm, and it's slow, but I've had no errors, so far. I'll let it keep running a while to see if I can get a timeout or error of some sort

*update* 2.5 hours of looping this while looping stress read/write on my vbox vm and all is well.
*update2* tried the same after dropping my vbox vm to 512M - 45k row load takes about 60 sec. to import and the read takes a little longer to output - vm starts swapping on the 45k read and the load avg nears 40, but it's still working.

{code}
#!/bin/sh

# create some data:
for i in $(seq 1 50); do echo ""N,text blah blah$i,text blah blah$i,text blah blah$i"" >> c6525_1-50.csv ; done
for i in $(seq 51 500); do echo ""N,text blah blah$i,text blah blah$i,text blah blah$i"" >> c6525_51-500.csv ; done
for i in $(seq 501 5000); do echo ""N,text blah blah$i,text blah blah$i,text blah blah$i"" >> c6525_501-5000.csv ; done
for i in $(seq 5001 50000); do echo ""N,text blah blah$i,text blah blah$i,text blah blah$i"" >> c6525_5001-50000.csv ; done

# create our cql to drop/create/import
cat << 'EOF' > c6525_run.cql
DROP KEYSPACE c6525;

CREATE KEYSPACE c6525 WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'};

CREATE TABLE c6525.test (hidden text, field2 text, field3 text, field4 text, PRIMARY KEY (hidden, field2, field3));

COPY c6525.test (hidden, field2, field3, field4) FROM 'c6525_1-50.csv';
SELECT * from c6525.test WHERE hidden = 'N';

COPY c6525.test (hidden, field2, field3, field4) FROM 'c6525_51-500.csv';
SELECT * from c6525.test WHERE hidden = 'N';

COPY c6525.test (hidden, field2, field3, field4) FROM 'c6525_501-5000.csv';
SELECT * from c6525.test WHERE hidden = 'N';

COPY c6525.test (hidden, field2, field3, field4) FROM 'c6525_5001-50000.csv';
SELECT * from c6525.test WHERE hidden = 'N' LIMIT 51000;
EOF

echo; echo ""*** Hit CTL-C to stop looping..***""; echo
sleep 3

# loop it
while true; do echo ""SOURCE 'c6525_run.cql';"" | cqlsh ; sleep 1 ; done
{code}","15/Mar/14 01:40;jbellis;Thanks, Michael.","11/Apr/14 11:36;shyamkg;I am still getting this error in DSE 2.0.5 and 2.0.6.. Tried in various machine mac & ubuntu. 

Steps :
1 -> CREATE TABLE DSQ (
        exchange text,
        sc_code int,
        load_date timeuuid, /* tried timestamp also but same behaviour */
        PRIMARY KEY (exchange, sc_code, load_date)
) 
2 -> Did SSTable load
writer.newRow(compositeColumn.builder().add(bytes(entry.stock_exchange)).add(bytes(entry.sc_code)).add(bytes(new com.eaio.uuid.UUID().toString())).build());
3 -> sstablesload 
Established connection to initial hosts
Opening sstables and calculating sections to stream
Streaming relevant part of stock/DSQ/stock-DSQ-ib-1-Data.db to [/127.0.0.1]
progress: [/127.0.0.1 1/1 (100%)] [total: 100% - 2147483647MB/s (avg: 2MB/s)                            
4 -> No errors in server log
5 -> Log into cqlsh and select * from DSQ; 
6 --> errors in Server log: 
Exception in thread Thread[ReadStage:51,5,main]
java.io.IOError: java.io.EOFException
	at org.apache.cassandra.db.Column$1.computeNext(Column.java:79)
	at org.apache.cassandra.db.Column$1.computeNext(Column.java:64)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:88)
	at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:37)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:82)
	at org.apache.cassandra.db.columniterator.LazyColumnIterator.computeNext(LazyColumnIterator.java:82)
	at org.apache.cassandra.db.columniterator.LazyColumnIterator.computeNext(LazyColumnIterator.java:59)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.filter.QueryFilter$2.getNext(QueryFilter.java:157)
	at org.apache.cassandra.db.filter.QueryFilter$2.hasNext(QueryFilter.java:140)
	at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:144)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.<init>(MergeIterator.java:87)
	at org.apache.cassandra.utils.MergeIterator.get(MergeIterator.java:46)
	at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:120)
	at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:80)
	at org.apache.cassandra.db.RowIteratorFactory$2.getReduced(RowIteratorFactory.java:101)
	at org.apache.cassandra.db.RowIteratorFactory$2.getReduced(RowIteratorFactory.java:75)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:115)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:98)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.ColumnFamilyStore$9.computeNext(ColumnFamilyStore.java:1607)
	at org.apache.cassandra.db.ColumnFamilyStore$9.computeNext(ColumnFamilyStore.java:1603)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1754)
	at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1718)
	at org.apache.cassandra.db.RangeSliceCommand.executeLocally(RangeSliceCommand.java:137)
	at org.apache.cassandra.service.StorageProxy$LocalRangeSliceRunnable.runMayThrow(StorageProxy.java:1418)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1931)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.io.EOFException
	at java.io.RandomAccessFile.readUnsignedShort(RandomAccessFile.java:713)
	at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:361)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:371)
	at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:74)
	at org.apache.cassandra.db.Column$1.computeNext(Column.java:75)
	... 37 more
7 -> client shows Request did not complete within rpc_timeout.","11/Apr/14 15:50;thobbs;https://issues.apache.org/jira/browse/CASSANDRA-6981 is a dupe of this.  I'm re-opening this to investigate further.  Besides this ticket and 6981, I've seen one other case of this: https://github.com/datastax/python-driver/issues/106","11/Apr/14 15:57;thobbs;It's worth noting that in CASSANDRA-6981, setting {{disk_access_mode: standard}} seemed to fix the problem.","11/Apr/14 17:50;mbligh;(copied from 6981)
I thought it was interesting how far apart these two numbers were:

""java.io.IOError: java.io.IOException: mmap segment underflow; remaining is 20402577 but 1879048192 requested""

And that the requested number is vaguely close to 2^^31 - did something do a negative number and wrap a 32 bit signed here?
To be fair, it's not that close to 2^^31, but still way off what was expected?","11/Apr/14 20:58;enigmacurry;fwiw, I've written a multi-threaded test for this using the python-driver. It's attached above as 6981_test.py. I used the criteria stated in CASSANDRA-6981:

bq. created about 16 tables, all the same, each with about 5 text fields and 5 binary fields. Most of those fields had a secondary index. Then insert into all the tables in parallel.

I'm using 16 tables, each with 5 text fields, 5 blob fields, inserting 10,000 rows into each table in parallel, and then selecting that data out based on a single field (blob5) that has 5 diffent options.

I could not reproduce the error in this ticket, however I did get this error several times:

{code}
ERROR [ReadStage:136] 2014-04-11 16:55:36,312 CassandraDaemon.java (line 198) Exception in thread Thread[ReadStage:136,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1920)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.io.util.RandomAccessReader.getTotalBufferSize(RandomAccessReader.java:157)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.getTotalBufferSize(CompressedRandomAccessReader.java:159)
        at org.apache.cassandra.service.FileCacheService.get(FileCacheService.java:96)
        at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:36)
        at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1195)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:57)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:65)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:42)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:167)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:250)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1540)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1369)
        at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:260)
        at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:103)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
        at org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1735)
        at org.apache.cassandra.db.index.composites.CompositesSearcher.search(CompositesSearcher.java:50)
        at org.apache.cassandra.db.index.SecondaryIndexManager.search(SecondaryIndexManager.java:556)
        at org.apache.cassandra.db.ColumnFamilyStore.search(ColumnFamilyStore.java:1723)
        at org.apache.cassandra.db.RangeSliceCommand.executeLocally(RangeSliceCommand.java:135)
        at org.apache.cassandra.service.StorageProxy$LocalRangeSliceRunnable.runMayThrow(StorageProxy.java:1374)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1916)
        ... 3 more
{code}","11/Apr/14 21:40;enigmacurry;Running this a few more times, I was able to get this on 2.0.5:

{code}
ERROR [ReadStage:90] 2014-04-11 17:37:57,768 CassandraDaemon.java (line 192) Exception in thread Thread[ReadStage:90,5,main]
java.lang.RuntimeException: org.apache.cassandra.io.sstable.CorruptSSTableException: java.io.EOFException: EOF after 46084 bytes out of 48857
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1935)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: org.apache.cassandra.io.sstable.CorruptSSTableException: java.io.EOFException: EOF after 46084 bytes out of 48857
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:82)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:65)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:42)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:167)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:250)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1560)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1379)
        at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:166)
        at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:105)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
        at org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1754)
        at org.apache.cassandra.db.index.composites.CompositesSearcher.search(CompositesSearcher.java:53)
        at org.apache.cassandra.db.index.SecondaryIndexManager.search(SecondaryIndexManager.java:537)
        at org.apache.cassandra.db.ColumnFamilyStore.search(ColumnFamilyStore.java:1742)
        at org.apache.cassandra.db.RangeSliceCommand.executeLocally(RangeSliceCommand.java:135)
        at org.apache.cassandra.service.StorageProxy$LocalRangeSliceRunnable.runMayThrow(StorageProxy.java:1418)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1931)
        ... 3 more
Caused by: java.io.EOFException: EOF after 46084 bytes out of 48857
        at org.apache.cassandra.io.util.FileUtils.skipBytesFully(FileUtils.java:392)
        at org.apache.cassandra.utils.ByteBufferUtil.skipShortLength(ByteBufferUtil.java:382)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:70)
        ... 22 more
{code}","11/Apr/14 21:51;enigmacurry;This repros on git:cassandra-2.0 HEAD as well:

{code}
ERROR [ReadStage:82] 2014-04-11 17:49:50,903 CassandraDaemon.java (line 216) Exception in thread Thread[ReadStage:82,5,main]
org.apache.cassandra.io.sstable.CorruptSSTableException: java.io.EOFException: EOF after 35761 bytes out of 48857
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:82)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:65)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:42)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:167)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:250)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1540)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1369)
        at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:164)
        at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:103)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
        at org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1735)
        at org.apache.cassandra.db.index.composites.CompositesSearcher.search(CompositesSearcher.java:50)
        at org.apache.cassandra.db.index.SecondaryIndexManager.search(SecondaryIndexManager.java:556)
        at org.apache.cassandra.db.ColumnFamilyStore.search(ColumnFamilyStore.java:1723)
        at org.apache.cassandra.db.RangeSliceCommand.executeLocally(RangeSliceCommand.java:135)
        at org.apache.cassandra.service.StorageProxy$LocalRangeSliceRunnable.runMayThrow(StorageProxy.java:1374)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1916)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.io.EOFException: EOF after 35761 bytes out of 48857
        at org.apache.cassandra.io.util.FileUtils.skipBytesFully(FileUtils.java:394)
        at org.apache.cassandra.utils.ByteBufferUtil.skipShortLength(ByteBufferUtil.java:382)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:70)
        ... 22 more
{code}",25/Apr/14 00:02;shyamkg;FYI... Same issue also exist in 2.0.7 version as well. ,"25/Apr/14 10:59;shyamkg;I tried couple of things this morning and would like to update

I changed the table definition to have COMPACT STORAGE with LevelCompactionStrategy and loaded the data. 

Server log: 
INFO 06:51:06,415 [Stream #80a0b380-cc67-11e3-a1a2-fb95dccb4714] Received streaming plan for Bulk Load
 INFO 06:51:06,416 [Stream #80a0b380-cc67-11e3-a1a2-fb95dccb4714] Prepare completed. Receiving 1 files(199308 bytes), sending 0 files(0 bytes)
 INFO 06:51:06,466 Enqueuing flush of Memtable-compactions_in_progress@2119789616(131/1310 serialized/live bytes, 7 ops)
 WARN 06:51:06,466 setting live ratio to maximum of 64.0 instead of Infinity
 INFO 06:51:06,466 CFS(Keyspace='system', ColumnFamily='compactions_in_progress') liveRatio is 64.0 (just-counted was 64.0).  calculation took 0ms for 0 cells
 INFO 06:51:06,467 Writing Memtable-compactions_in_progress@2119789616(131/1310 serialized/live bytes, 7 ops)
 INFO 06:51:06,467 [Stream #80a0b380-cc67-11e3-a1a2-fb95dccb4714] Session with /192.168.1.73 is complete
 INFO 06:51:06,468 [Stream #80a0b380-cc67-11e3-a1a2-fb95dccb4714] All sessions completed
 INFO 06:51:06,479 Completed flushing ***/apache-cassandra-2.0.7/data/data/system/compactions_in_progress/system-compactions_in_progress-jb-6-Data.db (158 bytes) for commitlog position ReplayPosition(segmentId=1398421195982, position=197721)
 INFO 06:51:06,483 Compacting [SSTableReader(path='***/apache-cassandra-2.0.7/data/data/stock/dailystockquote/stock-dailystockquote-jb-6-Data.db'), SSTableReader(path='***/apache-cassandra-2.0.7/data/data/stock/dailystockquote/stock-dailystockquote-jb-5-Data.db')]
 INFO 06:51:06,485 Enqueuing flush of Memtable-compactions_in_progress@729498316(0/0 serialized/live bytes, 1 ops)
 INFO 06:51:06,491 Writing Memtable-compactions_in_progress@729498316(0/0 serialized/live bytes, 1 ops)
 INFO 06:51:06,500 Completed flushing ***/apache-cassandra-2.0.7/data/data/system/compactions_in_progress/system-compactions_in_progress-jb-7-Data.db (42 bytes) for commitlog position ReplayPosition(segmentId=1398421195982, position=197800)

Behavioral changes:
Able to query table with no errors at server log but no data was loaded. 
","09/May/14 22:12;thobbs;This seems to require near-OOM conditions to occur.  So far I've only been able to reproduce this in a low-memory environment (~1GB), and it either occurs just before an OOM or when the JVM is on the brink of exhausting its heap space.","09/May/14 23:00;thobbs;Interestingly, this doesn't seem to be reproduceable when the keyspace isn't dropped and recreated.  (Just modify the repro script to remove the ""DROP KEYSPACE"" and use ""IF NOT EXISTS"" on the create statements.)","09/May/14 23:37;thobbs;Considering that drop/recreate seems to be necessary to reproduce the issue and that using a disk_access_mode of ""standard"" with no compression seems to fix the issue, I believe the problem is that old FileCacheService entries are being reused with new SSTables.  The FileCacheService is only used for PoolingSegmentedFiles, which are used if compression or mmap disk access mode are enabled.  Since FileCacheService uses (String) file paths as keys, new SSTables with the same filename can lookup old entries.

The only question is why the old FileCacheService entries are not being invalidated; this basically means that SSTableReader.close() is not being called in some cases.","13/May/14 22:28;thobbs;My initial guess about FileCacheService entries not being invalidated was wrong; they're all being invalidated correctly.  Furthermore, this isn't specific to compressed sstables (it reproduces with and without compression) or to a particular disk_access_mode (both standard and mmap have errors, although the specific errors are different).","14/May/14 20:07;thobbs;The problem is that key cache entries stick around after the keyspace is dropped.  After it's recreated and read, there are key cache hits that return old positions.  I'm not sure why it only seems to be a problem for the secondary index tables; my guess is that the key-cache preheating that happens after compaction is replacing the old entries in the key cache for the data tables.

CASSANDRA-5202 is the correct permanent solution for this, but that's for 2.1.  For 2.0, perhaps we should do something similar to CASSANDRA-6351 and go through the key cache to invalidate all entries for the CF when it's dropped.","15/May/14 07:51;slebresne;bq. For 2.0, perhaps we should do something similar to CASSANDRA-6351 and go through the key cache to invalidate all entries for the CF when it's dropped.

That makes sense to me.",16/May/14 18:32;thobbs;Attached patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-6525-2.0]) invalidates relevant key cache entries when a table is dropped.,"19/May/14 07:51;slebresne;Patch lgtm, but wouldn't make sense to do also invalidate for truncate in CFS.truncateBlocking, just to be on the safe side?","20/May/14 19:35;thobbs;bq. wouldn't make sense to do also invalidate for truncate in CFS.truncateBlocking, just to be on the safe side?

Truncates don't reset the SSTable generation counter ({{CFS.fileIndexGenerator}}), so new tables will have different generation numbers (and hence different key cache keys).","20/May/14 20:14;vkuptcov;We have a cluster with 5 nodes in one DC and a cluster with two nodes in the other without a replication between these datacenters. In all DC we use C* 2.0.5.

Today we've found a bug with similar messages but with the different result. We have dropped and recreated one table in the DC with 5 nodes and just truncated the same table in another DC.
After ~10 hours we have noticed appearing of the following messages in the first DC logs:
{code}
ERROR [ReadStage:231469] 2014-05-20 21:05:20,349 CassandraDaemon.java (line 192) Exception in thread Thread[ReadStage:231469,5,main]
java.io.IOError: java.io.EOFException
        at org.apache.cassandra.db.Column$1.computeNext(Column.java:79)
        at org.apache.cassandra.db.Column$1.computeNext(Column.java:64)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:88)
:
{code}

For the node, on which this messages started, we found several messages like on the other nodes
{code}
 INFO [GossipTasks:1] 2014-05-20 21:20:31,864 Gossiper.java (line 863) InetAddress /10.33.20.91 is now DOWN
 INFO [RequestResponseStage:10] 2014-05-20 21:20:32,186 Gossiper.java (line 849) InetAddress /10.33.20.91 is now UP
 INFO [GossipTasks:1] 2014-05-20 21:26:51,965 Gossiper.java (line 863) InetAddress /10.33.20.91 is now DOWN
{code}
and finally the node has stopped.


We found such effect only in the DC, where we have dropped and recreated table. In the DC with truncate everything is OK.
",20/May/14 20:42;thobbs;[~vkuptcov] that seems consistent with what I found.  I suggest invalidating your key cache in the problematic DC.  You can use {{nodetool invalidatekeycache}} to do this.,"20/May/14 20:50;vkuptcov;Yes, it looks like this. We have deleted the data from /var/lib/cassandra/saved_caches/* and after nodes restarting we don't notice the mentioned exceptions.","21/May/14 07:33;slebresne;bq. Truncates don't reset the SSTable generation counter

Fair enough (though it would still feel cleaner to invalidate the key cache entries, even if it don't result in a bug). But anyway, +1 on the patch.","21/May/14 09:46;slebresne;Patch committed (I want to start a vote for 2.0.8), thanks.",06/Jun/14 11:01;jeromatron;Darn jira hotkeys.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failure to start after unclean shutdown - java.lang.IllegalArgumentException: bufferSize must be positive,CASSANDRA-6531,12686569,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,ngrigoriev,ngrigoriev,27/Dec/13 20:53,12/Mar/19 14:17,13/Mar/19 22:29,28/Dec/13 06:18,1.2.14,2.0.5,,,,,0,compression,,,,"We had a severe power outage in the lab that resulted in unclean shutdown of the Cassandra servers. After the power was back I tried to start the cluster. Two out of 6 nodes cannot start because of this exception:

{code}
 INFO 20:47:11,003 Initializing system.local
 INFO [main] 2013-12-27 20:47:11,003 ColumnFamilyStore.java (line 251) Initializing system.local
 INFO 20:47:11,006 Opening /hadoop/disk1/cassandra/data/system/local/system-local-jb-2478 (5836 bytes)
 INFO [SSTableBatchOpen:1] 2013-12-27 20:47:11,006 SSTableReader.java (line 223) Opening /hadoop/disk1/cassandra/data/system/local/system-local-jb-2478 (5836 bytes)
 INFO 20:47:11,006 Opening /hadoop/disk4/cassandra/data/system/local/system-local-jb-2479 (144 bytes)
 INFO [SSTableBatchOpen:2] 2013-12-27 20:47:11,006 SSTableReader.java (line 223) Opening /hadoop/disk4/cassandra/data/system/local/system-local-jb-2479 (144 bytes)
ERROR 20:47:12,366 Exception encountered during startup
java.lang.IllegalArgumentException: bufferSize must be positive
        at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:55)
        at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1363)
        at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:67)
        at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1147)
        at org.apache.cassandra.db.RowIteratorFactory.getIterator(RowIteratorFactory.java:69)
        at org.apache.cassandra.db.ColumnFamilyStore.getSequentialIterator(ColumnFamilyStore.java:1526)
        at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1645)
        at org.apache.cassandra.db.RangeSliceCommand.executeLocally(RangeSliceCommand.java:137)
        at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:236)
        at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:1)
        at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:255)
        at org.apache.cassandra.db.SystemKeyspace.getUnfinishedCompactions(SystemKeyspace.java:206)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:261)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:461)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:504)
{code}

Collecting the logs now, will attach to the issue in a moment. ","Cassandra 2.0.3 with patches for CASSANDRA-6496 and CASSANDRA-6284, built from source; Linux; XFS on the data disks; 5 data disks; 6 nodes",,,,,,,,,,,,,,,,,,,,,,,27/Dec/13 23:02;jbellis;6531.txt;https://issues.apache.org/jira/secure/attachment/12620674/6531.txt,27/Dec/13 21:04;ngrigoriev;cassandra_jstack.txt;https://issues.apache.org/jira/secure/attachment/12620654/cassandra_jstack.txt,27/Dec/13 21:04;ngrigoriev;system.log.gz;https://issues.apache.org/jira/secure/attachment/12620653/system.log.gz,27/Dec/13 20:55;ngrigoriev;system.log.gz;https://issues.apache.org/jira/secure/attachment/12620651/system.log.gz,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-12-27 23:02:04.873,,,no_permission,,,,,,,,,,,,365560,,,Sun Dec 29 02:49:47 UTC 2013,,,,,,0|i1r0hz:,365868,2.0.3,,,,,,,xedin,xedin,,,1.0.0,,,,,,,27/Dec/13 20:55;ngrigoriev;system log with default logging levels,"27/Dec/13 21:04;ngrigoriev;Startup log with default logging level set to TRACE (log4j.rootLogger=TRACE,stdout,R). Interestingly enough I do not see the original exception in the log when I change it to TRACE and Cassandra process does not terminate. So I include the thread dump just in case.",27/Dec/13 23:02;jbellis;Patch to fsync compression metadata on close.  As near as I can tell this is a problem back to the introduction of compression in 1.0 (CASSANDRA-47). /cc [~xedin] [~tjake],"28/Dec/13 00:03;ngrigoriev;Do I understand correctly that this patch should eliminate/reduce the possibility of this to happen but won't help to start the node in my case? If so, I am going to try to abuse this JIRA a bit and ask if anyone could recommend a method of bringing this node back to life without wiping it? If such a method exists, of course. Thanks!",28/Dec/13 00:42;jbellis;The only straightforward approach (i.e.: doesn't involve writing code to rebuild the corrupt offsets from the data component) is to remove the affected sstables and repair.,28/Dec/13 01:18;xedin;[~jbellis] I think we should fsync twice in this case - first time whe the header is finalized and second time on close.,28/Dec/13 01:51;jbellis;It's still -tmp at this point so a single sync is sufficient.,"28/Dec/13 02:03;ngrigoriev;I believe the log messages were somewhat misleading. After messing with debug logs for a while I have found that the problem was, apparently, not with system KS. I have removed all stuff \*compactions_in_progress\* and that helped one of these two nodes in question to start. Another one seems to have an identity crisis as it believes that another node with the same address has already joined the cluster...but that seems to be another story.","28/Dec/13 02:06;xedin;Ah, I completely forgot about that, sure.",28/Dec/13 06:18;jbellis;committed,"28/Dec/13 06:19;jbellis;(Incidentally, the fix for the compactions in progress bug will be committed soon for CASSANDRA-6086.)","28/Dec/13 16:25;ngrigoriev;Just one more suggestion for the fix. I think it is important to log the name of the problematic file every time an exception is thrown. This helps to correctly associate the exception with the file. In this case RandomAccessReader.java:67 throws an exception without this information. I have replaced that line with:

{code}
            throw new IllegalArgumentException(""bufferSize must be positive while reading "" + file.getAbsolutePath());
{code}","28/Dec/13 17:54;mshuler;committed patch causes:
http://cassci.datastax.com/job/cassandra-2.0_test/76/testReport/junit/org.apache.cassandra.io.compress/CompressedRandomAccessReaderTest/testResetAndTruncateCompressed/",29/Dec/13 02:49;jbellis;Updated close method to be idempotent.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tools error out if they can't make ~/.cassandra,CASSANDRA-6449,12682745,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,kirktrue,jjordan,jjordan,04/Dec/13 18:53,12/Mar/19 14:17,13/Mar/19 22:29,18/Jun/14 19:20,2.0.9,2.1 rc2,,Legacy/Tools,,,2,lhf,,,,"We shouldn't error out if we can't make the .cassandra folder for the new history stuff.

{noformat}
Exception in thread ""main"" FSWriteError in /usr/share/opscenter-agent/.cassandra
	at org.apache.cassandra.io.util.FileUtils.createDirectory(FileUtils.java:261)
	at org.apache.cassandra.utils.FBUtilities.getToolsOutputDirectory(FBUtilities.java:627)
	at org.apache.cassandra.tools.NodeCmd.printHistory(NodeCmd.java:1403)
	at org.apache.cassandra.tools.NodeCmd.main(NodeCmd.java:1122)
Caused by: java.io.IOException: Failed to mkdirs /usr/share/opscenter-agent/.cassandra
	... 4 more
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,23/May/14 17:11;kirktrue;trunk-6449.txt;https://issues.apache.org/jira/secure/attachment/12646548/trunk-6449.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-29 17:53:02.16,,,no_permission,,,,,,,,,,,,362002,,,Wed Jun 18 19:20:02 UTC 2014,,,,,,0|i1qefr:,362297,1.2.11,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"29/Jan/14 17:53;jblangston@datastax.com;From a customer:

The culprit is: / src / java / org / apache / cassandra / utils / FBUtilities.java

File historyDir = new File(System.getProperty(""user.home""), "".cassandra"");

Setting an alternate environment variable HOME doesn't fix. I've tried patching the nodetool wrapper script to provide -Duser.home at runtime, but it seems when defining user.home, I get runtime errors with missing libraries. It would be nice if the tool just honoured $HOME (or let you specify a commandline override without hacking the script).","29/Jan/14 18:15;jblangston@datastax.com;This is the error that occurs when manually defining -Duser.home in the nodetool shell script:

{code}
Exception in thread ""main"" java.lang.NoClassDefFoundError: com/google/common/collect/AbstractMultimap$WrappedSortedSet 
at com.google.common.collect.AbstractMultimap.wrapCollection(AbstractMultimap.java:374) 
at com.google.common.collect.AbstractMultimap.get(AbstractMultimap.java:363) 
at com.google.common.collect.AbstractSetMultimap.get(AbstractSetMultimap.java:59) 
at com.google.common.collect.AbstractSortedSetMultimap.get(AbstractSortedSetMultimap.java:65) 
at com.google.common.collect.TreeMultimap.get(TreeMultimap.java:74) 
at com.google.common.collect.AbstractSortedSetMultimap.get(AbstractSortedSetMultimap.java:35) 
at com.google.common.collect.Multimaps$UnmodifiableMultimap.get(Multimaps.java:563) 
at org.apache.cassandra.locator.TokenMetadata.getTokens(TokenMetadata.java:507) 
at org.apache.cassandra.service.StorageService.getTokens(StorageService.java:2048) 
at org.apache.cassandra.service.StorageService.getTokens(StorageService.java:2042) 
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 
at java.lang.reflect.Method.invoke(Method.java:597) 
at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93) 
at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27) 
at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208) 
at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120) 
at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:264) 
at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836) 
at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:762) 
at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1454) 
at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:74) 
at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1295) 
at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1387) 
at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:818) 
at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 
at java.lang.reflect.Method.invoke(Method.java:597) 
at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:303) 
at sun.rmi.transport.Transport$1.run(Transport.java:159) 
at java.security.AccessController.doPrivileged(Native Method) 
at sun.rmi.transport.Transport.serviceCall(Transport.java:155) 
at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535) 
at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790) 
at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649) 
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918) 
at java.lang.Thread.run(Thread.java:662)
{code}","29/Jan/14 19:25;mishail;NodeCmd honestly tries to ignore IOExceptions, the problem is that {{FBUtilities.getToolsOutputDirectory}} wraps IOExceptions in {{FSWriteError}},

{code:title=org.apache.cassandra.tools.NodeCmd.printHistory}
        FileWriter writer = null;
        try
        {
            final String outputDir = FBUtilities.getToolsOutputDirectory().getCanonicalPath();
.....
        }
        catch (IOException ioe)
        {
            //quietly ignore any errors about not being able to write out history
        }
        finally
        {
            FileUtils.closeQuietly(writer);
        }
{code}",24/Apr/14 16:17;sbassi;I've had another user run into this issue who would like this functionality. I can reproduce the problem just by restricting write access to the users home dir that is running nodetool.,23/May/14 17:12;kirktrue;Simply catching IOError in addition to IOException.,18/Jun/14 19:05;brandon.williams;We probably want to fix this in 1.2 or at least 2.0,18/Jun/14 19:10;jbellis;Patch applies to 2.0 if you point it at src/java/org/apache/cassandra/tools/NodeCmd.java.,"18/Jun/14 19:20;brandon.williams;Good enough for me then, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SELECT someColumns FROM table results in AssertionError in AbstractQueryPager.discardFirst,CASSANDRA-6447,12682664,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,julien.ayme@gmail.com,julien.ayme@gmail.com,04/Dec/13 11:50,12/Mar/19 14:17,13/Mar/19 22:29,20/Dec/13 07:50,2.0.4,,,,,,0,,,,,"I have a query which must read all the rows from the table:
Query: ""SELECT key, col1, col2, col3 FROM mytable""

Here is the corresponding code (this is using datastax driver):
{code}
ResultSet result = session.execute(""SELECT key, col1, col2, col3 FROM mytable"");
for (Row row : result) {
     // do some work with row
}
{code}

Messages sent from the client to Cassandra:
* 1st: {{QUERY SELECT key, col1, col2, col3 FROM mytable([cl=ONE, vals=[], skip=false, psize=5000, state=null, serialCl=ONE])}}

* 2nd: {{QUERY SELECT key, col1, col2, col3 FROM mytable([cl=ONE, vals=[], skip=false, psize=5000, state=java.nio.HeapByteBuffer[pos=24 lim=80 cap=410474], serialCl=ONE])}}

On the first message, everything is fine, and the server returns 5000 rows.
On the second message, paging is in progress, and the server fails in AbstractQueryPager.discardFirst: AssertionError (stack trace attached).

Here is some more info (step by step debugging on reception of 2nd message):
{code}
AbstractQueryPager.fetchPage(int):
* pageSize=5000, currentPageSize=5001, rows size=5002, liveCount=5001
* containsPreviousLast(rows.get(0)) returns true

-> AbstractQueryPager.discardFirst(List<Row>):
* rows size=5002
* first=TreeMapBackedSortedColumns[with TreeMap size=1]

-> AbstractQueryPager.discardHead(ColumnFamily, ...):
* counter = ColumnCounter$GroupByPrefix
* iter.hasNext() returns true (TreeMap$ValueIterator with TreeMap size=1)
* Column c = DeletedColumn
* counter.count() -> c.isLive returns false (c is DeletedColumn)
* counter.live() = 0
* iter.hasNext() returns false
* Math.min(0, toDiscard==1) returns 0

<- AbstractQueryPager.discardFirst(List<Row>):
* discarded = 0;
* count = newCf.getColumnCount() = 0;
{code}
->  assert discarded == 1 *throws AssertionError*

","Cluster: single node server (ubuntu)
Cassandra version: 2.0.3 (server/client)
Client: Datastax cassandra-driver-core 2.0.0-rc1",,,,,,,,,,,,,,,,,,,,,,,12/Dec/13 17:36;slebresne;6447.txt;https://issues.apache.org/jira/secure/attachment/12618443/6447.txt,11/Dec/13 10:50;julien.ayme@gmail.com;cassandra-2.0-6447.patch;https://issues.apache.org/jira/secure/attachment/12618218/cassandra-2.0-6447.patch,04/Dec/13 11:56;julien.ayme@gmail.com;stacktrace.txt;https://issues.apache.org/jira/secure/attachment/12616973/stacktrace.txt,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-12-12 17:36:12.732,,,no_permission,,,,,,,,,,,,361921,,,Fri Dec 20 07:50:27 UTC 2013,,,,,,0|i1qdxr:,362216,,,,,,,,iamaleksey,iamaleksey,,,2.0.3,,,,,,,04/Dec/13 11:56;julien.ayme@gmail.com;the stacktrace,"04/Dec/13 11:59;julien.ayme@gmail.com;I think this issue occurs because the row to discard has only one column, and this column is not live, but I may be wrong here.","04/Dec/13 12:06;julien.ayme@gmail.com;Also, since newCf.getColumnCount() == 0, the rest of the code is still valid (the row will not be included in the returned Rows).
Therefore, I think that the assert statement should be transformed to:
{code}
assert discarded == 1 || discarded == 0;
{code}
Or that the assert statement should be dropped, since it was introduced in the last commit on AbstractQueryPager: 

See: https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commit;h=3c9760bdb986f6c2430adfc13c86ecb75c3246ac
",11/Dec/13 10:50;julien.ayme@gmail.com;Proposed (trivial) patch,"11/Dec/13 10:51;julien.ayme@gmail.com;Updated fix version, since fix should be trivial","12/Dec/13 17:36;slebresne;I believe the fix is a tiny bit less trivial than that. If the first row in discardFirst has no live data, we need to check the following rows until we find one to discard, otherwise paging would end up return twice the same result. Not sure why discardFirst is not handling that correctly since discardLast is, but anyway, attaching patch to fix (the patch also slightly modify discardLast because it was actually not handling the case where there was less live rows than we want to discard).","13/Dec/13 06:14;julien.ayme@gmail.com;Thanks for looking into this issue, and sorry for the assumption that this was trivial (I am still not completely familiar with the architecture of Cassandra, but I am trying to dig into it as best as I can).",19/Dec/13 17:06;iamaleksey;I think we are good this time. +1,20/Dec/13 07:50;slebresne;Committed thanks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra crashes on Solaris sparcv9 using java 64bit,CASSANDRA-6628,12691641,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,shohou,shohou,shohou,28/Jan/14 14:42,12/Mar/19 14:17,13/Mar/19 22:29,04/Feb/14 05:51,2.0.5,,,,,,0,,,,,"When running cassandra 2.0.4 (and other versions) on Solaris and java 64 bit, JVM crashes. Issue is described once in CASSANDRA-4646 but closed as invalid.

The reason for this crash is some memory allignment related problems and incorrect sun.misc.Unsafe usage. If you look into DirectByteBuffer in jdk, you will see that it checks os.arch before using getLong methods.

I have a patch, which check for the os.arch and if it is not one of the known, it reads longs and ints byte by byte.

Although patch fixes the problem in cassandra, it will still crash without similar fixes in the lz4 library. I already provided the patch for Unsafe usage in lz4.",checked 1.2.x line and 2.0.x,,,,,,,,,,,,,,,,,,,,,,,28/Jan/14 14:43;shohou;solaris_unsafe_fix.patch;https://issues.apache.org/jira/secure/attachment/12625582/solaris_unsafe_fix.patch,31/Jan/14 16:29;benedict;tmp.patch;https://issues.apache.org/jira/secure/attachment/12626321/tmp.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-01-31 16:29:21.102,,,no_permission,,,,,,,,,,,,370386,,,Wed Jul 02 18:20:01 UTC 2014,,,,,,0|i1ru7j:,370707,1.2.10,2.0.4,,,,,,benedict,benedict,,,1.1.3,,,,,,,28/Jan/14 14:43;shohou;patch is for version 2.0.4,"31/Jan/14 16:29;benedict;I've attached a patch with some minor changes:

Instead of the proliferation of ""os.arch"" tests checking ""sparcv9"" and ""64"" around the place, I've moved them into DatabaseDescriptor.hasLargeAddressSpace(). This now first checks for the sun.arch.data.model before falling back to os.arch if that isn't present, or is not 32 or 64.

Also, there didn't seem to be a great deal of point to adding the complication to the UnsafeComparator in FastByteComparisons. The native comparer seems like it would do just as well. Unless you have benchmarks demonstrating the performance benefit, of course.

All this said, it looks to me like it might be worth investigating what alignment would be necessary to ensure unsafe.getLong() works safely. If it's a simply that is has to not have the lowest 3 address bits set (likely given that this is the requirement for JVM packing of long positions inside regular objects), we could probably write a special aligned comparator, that does byte comparisons for the first and last <=7 bytes. For Memory we could pad out the first few bytes when we know the offset we'll be putting/reading longs. 

This is probably a better solution, but for now the attached patch should be sufficient.

","31/Jan/14 18:27;benedict;Looking at FastByteComparisons a little closer, I think we should actually address some of these optimisations along with various other changes: currently we do an optimised comparison for big endian architectures, but for x86 we're doing a lot of unnecessary work. We can compare all endianness longs by comparing the least significant 63bits, and then modifying the result based on the most significant bit. This would simplify the algorithm and also make our most common target architecture faster.

At the same time, we're already doing a basic comparison of the last <= 8 bytes, so I suggest we modify this logic to run both ends, and include alignment. [~shohou] would you be able to test this on your hardware?","31/Jan/14 18:33;shohou;Sure, i will check on Monday","31/Jan/14 19:58;benedict;bq.  We can compare all endianness longs by comparing

What I was saying here is nonsense. But irrelevant since we only perform this once per comparison anyway. So let's ignore this.

bq. so I suggest we modify this logic to run both ends, and include alignment

This is actually much harder than I initially realised. It can be done, but I'm not certain it's worth the effort just now. Basically we want to get one of the streams into an aligned position, and then for the other do aligned reads that are potentially offset from the first, plus a couple of shift/ors to create the long we want to actually compare. This will only benefit those architectures where unaligned access are explicitly prohibited. If you want to submit a patch, Dmitry, I'll be happy to review it, but otherwise I'll leave the current patch as is for now.","03/Feb/14 09:45;shohou;I checked your patch and jvm doesn't crash

I also did some performance tests. I don't know the usage pattern for comparator, but my simple tests show that my changes to comparer would make it slower than pure java implementation :) I fully agree that it's better to use java implementation on solaris sparcv9, than change unsafe implementation","03/Feb/14 09:50;benedict;bq. my changes to comparer would make it slower than pure java implementation

This isn't very surprising given what they were doing, but always good to have the confirmation :-)

It should be possible to make a special unsafe comparer tailored for sparcv9 (and any other aligned access only architectures) that is quite a bit faster, in the manner I mention above, but it's not something we're likely to consider a priority in the near future. As always feel free to have a crack at it yourself and submit, I'd be more than happy to review.",03/Feb/14 09:58;benedict;This patch is ready for commit.,04/Feb/14 05:51;jbellis;committed,30/Jun/14 16:50;Mikhail.Skotnikov;Is there version in 1.2.x branch where the issue is fixed?,01/Jul/14 04:00;jbellis;No.,01/Jul/14 07:49;nsmirnov;Could you please fix the issue for 1.2.x branch?,02/Jul/14 18:20;jbellis;No.  2.0.x is quite stable; you should upgrade.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Coordinator's ""java.lang.ArrayIndexOutOfBoundsException: -1"" with CL > 1",CASSANDRA-6629,12691723,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mishail,rskvazh,rskvazh,28/Jan/14 20:28,12/Mar/19 14:17,13/Mar/19 22:29,29/Jan/14 06:21,1.2.14,2.0.5,,Legacy/CQL,,,0,,,,,"I've got this error in system.log on all coordinators
{noformat}
ERROR [Thrift:37555] 2014-01-28 19:53:51,547 CustomTThreadPoolServer.java (line 212) Error occurred during processing of message.
java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.ArrayList.elementData(ArrayList.java:400)
        at java.util.ArrayList.remove(ArrayList.java:477)
        at org.apache.cassandra.db.ArrayBackedSortedColumns$ReverseSortedCollection$1.remove(ArrayBackedSortedColumns.java:373)
        at org.apache.cassandra.db.filter.SliceQueryFilter.trim(SliceQueryFilter.java:249)
        at org.apache.cassandra.db.SliceFromReadCommand.maybeTrim(SliceFromReadCommand.java:101)
        at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:1370)
        at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:1189)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:188)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:163)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:58)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:188)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:222)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:212)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1958)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4486)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4470)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:194)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
{noformat}

It's occurred on coordinator (not always primary or secondary of this uid-PK) when I execute query (PHP or Python client got ""TSocket read 0 bytes"" exception):

{code:sql}SELECT * FROM home_timeline WHERE uid = 0x52dcbc794989a6ea2c8b4569 ORDER BY tuuid DESC LIMIT 32{code}

If limit < 32, then its ok. When ORDER ... ASC its ok. When ConsistencyLevel 1 its ok.
On one node data is inconsistent with two others, and read repair won't work (32-nd element is odd).

Our RF = 3
Cassandra version 2.0.4","15 nodes, 2.0.4, RF=3",,,,,,,,,,,,,,,,,,,,,,,29/Jan/14 05:59;mishail;CASSANDRA-1.2-6629-v2.patch;https://issues.apache.org/jira/secure/attachment/12625796/CASSANDRA-1.2-6629-v2.patch,29/Jan/14 06:05;mishail;CASSANDRA-2.0-6629-v2.patch;https://issues.apache.org/jira/secure/attachment/12625798/CASSANDRA-2.0-6629-v2.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-01-28 21:34:54.766,,,no_permission,,,,,,,,,,,,370469,,,Wed Jan 29 21:43:47 UTC 2014,,,,,,0|i1runj:,370779,2.0.4,,,,,,,iamaleksey,iamaleksey,,,2.0.4,,,,,,,28/Jan/14 20:29;rskvazh;Does this linked with CASSANDRA-6333?,"28/Jan/14 21:34;mishail;Restoring my original comment 

It looks like {{remove}} at {{org.apache.cassandra.db.ArrayBackedSortedColumns.ReverseSortedCollection.iterator}} should use _idx_ (current item) instead of _idx--_ (next item)

{code:title=Current implementation which throws ArrayIndexOutOfBoundsException}
    private class ReverseSortedCollection extends AbstractCollection<Column>
    {
..........
            return new Iterator<Column>()
            {
                int idx = size() - 1;

                public boolean hasNext()
                {
                    return idx >= 0;
                }

                public Column next()
                {
                    return columns.get(idx--);
                }

                public void remove()
                {
                    columns.remove(idx--);
                }
{code}",28/Jan/14 21:56;rskvazh;Does this fix (idx--) solve problem with read repair?,"28/Jan/14 22:14;rskvazh;Wow. It seems ""nodetool scrub"" on node with odd row helped to me. Read repair restore that 32-nd row on other replicas, this CQL query worked properly and without exception now...",28/Jan/14 23:14;mishail;Patch for the problem plus unit-tests to reproduce (without fix)/ validate (with fix),29/Jan/14 04:20;iamaleksey;Wouldn't it be simpler to just alter remove() to return columns.remove(idx + 1) ?,"29/Jan/14 04:30;mishail;bq. Wouldn't it be simpler to just alter remove() to return columns.remove(idx + 1) ?

In this case you'll be able to call {{remove}} arbitrary number of times, without calling {{next}} at all, which breaks the Iterator's contract. 
But yes, that will fix the original problem.
","29/Jan/14 05:19;iamaleksey;All right. I don't like the two indexes though. Can we just get away with a 'nextCalled' boolean? set it to true in next(), set it back to false in remove()?

That, and I'd rather we just threw IllegalStateException with no message, like the stdlib iterators do (w/ no need for Preconditions). The last one is a nit/taste thing, feel free to ignore.","29/Jan/14 05:27;iamaleksey;Oh, and 1.2 looks affected as well.","29/Jan/14 05:39;mishail;bq. Can we just get away with a 'nextCalled' boolean? set it to true in next(), set it back to false in remove()?
Totally makes sense.

bq. That, and I'd rather we just threw IllegalStateException with no message, like the stdlib iterators do (w/ no need for Preconditions)
Adjusted.

v2 of the patch (for 2.0.x) attached.","29/Jan/14 05:44;iamaleksey;LGTM, go ahead.

nit: missing spaces around '+', and I'd rather just see IllegalStateException() there w/ null message - it's not like there can be several reasons for an exception here.",29/Jan/14 05:59;mishail;Attached the patch for 1.2,"29/Jan/14 06:05;mishail;bq. nit: missing spaces around '+', and I'd rather just see IllegalStateException() there w/ null message - it's not like there can be several reasons for an exception here.

Adjusted in both patches","29/Jan/14 06:21;iamaleksey;Committed, thanks.","29/Jan/14 09:53;rskvazh;Guys, what about scrub and read repair problem?","29/Jan/14 21:43;iamaleksey;bq. Guys, what about scrub and read repair problem?

Do you still have a problem?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
upgradesstables does not upgrade indexes causing startup error.,CASSANDRA-6598,12689306,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,enigmacurry,enigmacurry,16/Jan/14 18:16,12/Mar/19 14:17,13/Mar/19 22:29,17/Jan/14 00:35,1.2.14,2.0.5,,,,,0,,,,,"Upgrading a cluster from 1.1.12 -> 1.2.13 -> 2.0 HEAD fails due to upgradesstables not upgrading the index files.

To reproduce:

{code}
# Make sure ccm has all the versions we need:
ccm create -v git:cassandra-2.0 test
ccm remove 
ccm create -v git:cassandra-1.2.13 test
ccm remove

# Create a 1.1.12 cluster:
ccm create -v git:cassandra-1.1.12 test
# Set cluster partitioner:
perl -p -i -e 's/partitioner: null/partitioner: RandomPartitioner/gi' ~/.ccm/test/cluster.conf

ccm populate -n 1
ccm start
ccm node1 stress -- --operation=INSERT --family-type=Standard --num-keys=10000 --create-index=KEYS --compression=SnappyCompressor --compaction-strategy=LeveledCompactionStrategy
ccm flush

ccm node1 drain
ccm status
# Wait until node1 shows DOWN.

# Set cluster version:
perl -p -i -e 's/git_cassandra-1.1.12/git_cassandra-1.2.13/gi' ~/.ccm/test/cluster.conf

# Upgrade node1:
ccm node1 updateconf
ccm node1 start

# Upgrade sstables:
~/.ccm/test/node1/bin/nodetool -p 7100 upgradesstables

ls ~/.ccm/test/node1/data/Keyspace1/Standard1/

# Note the versions on files. Data has been upgraded to version *ic* but indexes are left on version *hf*.


# Upgrade to 2.0:
ccm flush
ccm node1 drain
ccm status
# Wait until node1 shows DOWN.
# Set cluster version:
perl -p -i -e 's/git_cassandra-1.2.13/git_cassandra-2.0/gi' ~/.ccm/test/cluster.conf
ccm node1 updateconf
ccm node1 start
{code}

On this last upgrade attempt, cassandra 2.0 complains that the version for the indexes is incorrect:

{code}
java.lang.RuntimeException: Can't open incompatible SSTable! Current version jb, found file: /home/ryan/.ccm/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1.Idx1-hf-1
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:411)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:393)
        at org.apache.cassandra.db.index.AbstractSimplePerColumnSecondaryIndex.init(AbstractSimplePerColumnSecondaryIndex.java:52)
        at org.apache.cassandra.db.index.SecondaryIndexManager.addIndexedColumn(SecondaryIndexManager.java:274)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:279)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:416)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:388)
        at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:309)
        at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:266)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:110)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:88)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:273)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:462)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:549)
{code}

The same test can be done starting from 1.2 upgrading to 2.0. The index files do not upgrade in this scenario either, however, there is not the same error, possibly because 2.0 is tolerant of version 1.2 indexes?",,,,,,,,,,,,,,,,,,,,,,,,16/Jan/14 20:32;brandon.williams;6598-2.0-npe.txt;https://issues.apache.org/jira/secure/attachment/12623465/6598-2.0-npe.txt,16/Jan/14 19:52;brandon.williams;6598.txt;https://issues.apache.org/jira/secure/attachment/12623452/6598.txt,16/Jan/14 22:43;brandon.williams;news-2.0.txt;https://issues.apache.org/jira/secure/attachment/12623508/news-2.0.txt,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-01-16 18:46:01.317,,,no_permission,,,,,,,,,,,,368273,,,Fri Jan 17 00:35:27 UTC 2014,,,,,,0|i1rh5b:,368578,2.0.5,,,,,,,jbellis,jbellis,,,,,,,,,,"16/Jan/14 18:46;brandon.williams;Even 'upgradesstables -a' doesn't upgrade the index.  Strangely, when following these steps (albeit not in ccm) I get a different error:

{noformat}
 INFO 18:43:59,336 Opening /var/lib/cassandra/data/system/local/system-local-ic-6 (497 bytes)
ERROR 18:43:59,678 Exception encountered during startup
java.lang.NullPointerException
        at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
        at org.apache.cassandra.serializers.AbstractTextSerializer.deserialize(AbstractTextSerializer.java:39)
        at org.apache.cassandra.serializers.AbstractTextSerializer.deserialize(AbstractTextSerializer.java:26)
        at org.apache.cassandra.db.marshal.AbstractType.compose(AbstractType.java:142)
        at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:97)
        at org.apache.cassandra.config.CFMetaData.fromSchemaNoColumnsNoTriggers(CFMetaData.java:1640)
        at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1683)
        at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:307)
        at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:288)
        at org.apache.cassandra.db.DefsTables.loadFromKeyspace(DefsTables.java:130)
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:525)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:242)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:462)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:549)
{noformat}","16/Jan/14 19:52;brandon.williams;The problem is that if you don't specify the ks/cf, seconday indexes aren't added in getValidColumnFamilies.  Patch against 1.2 to solve this.  This being the case, you can workaround this problem by specifying the ks/cf explicitly.

For 2.0, it seems like we should actually be able to open this file, and just warn that upgradesstables needs to be run on it.","16/Jan/14 20:32;brandon.williams;bq. For 2.0, it seems like we should actually be able to open this file, and just warn that upgradesstables needs to be run on it.

Well, unfortunately it's not that simple since 2.0 doesn't know how to open hf files.  Patch to fix the NPE though.","16/Jan/14 20:34;enigmacurry;The patch for 1.2 is tested, lgtm.","16/Jan/14 20:51;enigmacurry;The 2.0 patch works, no errors, upgraded to 2.1.

I confirm that the upgrade of indexes past 1.2 is still failing, but there are no errors, and it appears that 2.0 and 2.1 have no problems loading and dealing with version *ic* (1.2) indexes.","16/Jan/14 20:53;jbellis;+1 on 6598.txt (space after {{for}} though)

Not sure on the npe patch, are those really optional?  /cc [~iamaleksey]","16/Jan/14 20:54;brandon.williams;Once 1.2 is full upgraded you're safe, because 2.0 has code to read ic sstables, but not hf.  So if you start from 1.1 and go to 1.2 (without the patch) and then 2.0, you're kind of stuck, because 2.0 can't read hf and now your system tables are jb, which 1.2 can't read, so going backward and trying again isn't an option.  I think the simplest thing to do here is cover this in NEWS.TXT.","16/Jan/14 20:58;enigmacurry;Would there be any reason to prefer jb or ka indexes over ic, though? If upgradesstables doesn't do it, then is the proposed solution to drop and recreate indexes?","16/Jan/14 21:01;brandon.williams;bq. Would there be any reason to prefer jb or ka indexes over ic, though?

Not really.

bq. If upgradesstables doesn't do it, then is the proposed solution to drop and recreate indexes?

That's probably the simplest, if not cheapest, option.",16/Jan/14 21:12;brandon.williams;Committed first patch.,16/Jan/14 22:43;brandon.williams;Proposed NEWS.TXT changes.,16/Jan/14 22:45;jbellis;+1,"16/Jan/14 23:07;brandon.williams;Committed NEWS changes.  We'll wait and see what Aleksey says about the NPE, but I'm pretty sure it's because that CF never touched CQL at any point in its life.","17/Jan/14 00:03;iamaleksey;+1, it's at least harmless, but (I think) shouldn't be happening post CASSANDRA-5800.",17/Jan/14 00:35;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"During upgrade from 1.2 -> 2.0, upgraded node sees other nodes as Down",CASSANDRA-6554,12687583,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,mshuler,mshuler,07/Jan/14 03:11,12/Mar/19 14:17,13/Mar/19 22:29,09/Jan/14 23:02,2.0.5,,,,,,0,,,,,"During an upgrade from 1.2.13 to 2.0.3/2.0.4, the upgraded node sees the remaining nodes of the cluster as Down.

{code}
automaton@ip-10-139-1-113:~$ nodetool status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address         Load       Owns   Host ID                               Token                                    Rack
UN  10.139.1.113    98.94 MB   33.3%  33b1cd06-e17b-4332-8066-0c6c401e0cf3  -9223372036854775808                     rack1
DN  10.139.11.168   97.51 MB   33.3%  ec97c163-8f2d-4019-a3d1-55df5e4037d4  -3074457345618258603                     rack1
DN  10.238.221.115  97.34 MB   33.3%  73a76d3f-73ef-481d-b603-0833c0ff80c2  3074457345618258602                      rack1
automaton@ip-10-139-1-113:~$ nodetool gossipinfo
/10.238.221.115
  SEVERITY:0.0
  RPC_ADDRESS:0.0.0.0
  DC:datacenter1
  RELEASE_VERSION:1.2.13
  LOAD:1.02066255E8
  STATUS:NORMAL,3074457345618258602
  SCHEMA:8b351435-81ef-3a14-adf7-8555e2f19ecd
  NET_VERSION:6
  RACK:rack1
  HOST_ID:73a76d3f-73ef-481d-b603-0833c0ff80c2
/10.139.1.113
  RPC_ADDRESS:0.0.0.0
  SEVERITY:0.0
  DC:datacenter1
  RELEASE_VERSION:2.0.4
  LOAD:1.03750451E8
  STATUS:NORMAL,-9223372036854775808
  SCHEMA:dfafb212-5b8f-31cb-a80b-2ba58fcef73d
  NET_VERSION:7
  RACK:rack1
  HOST_ID:33b1cd06-e17b-4332-8066-0c6c401e0cf3
/10.139.11.168
  SEVERITY:0.0
  RPC_ADDRESS:0.0.0.0
  DC:datacenter1
  RELEASE_VERSION:1.2.13
  LOAD:1.02245066E8
  STATUS:NORMAL,-3074457345618258603
  SCHEMA:8b351435-81ef-3a14-adf7-8555e2f19ecd
  NET_VERSION:6
  RACK:rack1
  HOST_ID:ec97c163-8f2d-4019-a3d1-55df5e4037d4
{code}","EC2 Ubuntu Precise 12.04
Oracle JRE 1.7_25
C* 1.2.13 upgrade to 2.0.4
",,,,,,,,,,,,,,,,,,,,,,,09/Jan/14 18:52;brandon.williams;6554.txt;https://issues.apache.org/jira/secure/attachment/12622232/6554.txt,07/Jan/14 20:20;mshuler;6554_trace_system.log;https://issues.apache.org/jira/secure/attachment/12621846/6554_trace_system.log,07/Jan/14 21:01;mshuler;6554_trace_system.node1.log;https://issues.apache.org/jira/secure/attachment/12621854/6554_trace_system.node1.log,07/Jan/14 21:01;mshuler;6554_trace_system.node2.log;https://issues.apache.org/jira/secure/attachment/12621855/6554_trace_system.node2.log,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2014-01-07 17:28:26.056,,,no_permission,,,,,,,,,,,,366584,,,Thu Jan 09 23:02:51 UTC 2014,,,,,,0|i1r6tj:,366895,,,,,,,,thobbs,thobbs,,,,,,,,,,"07/Jan/14 17:08;mshuler;So... this morning, while carefully running through my steps to reproduce, the upgraded node0 accepts writes via stress, as well as node1 and node2.
{code}
mshuler@hana:~$ ctool launch -i m1.medium -j 1.7_25 upgradetest 3
mshuler@hana:~$ ctool install -v 1.2.13 -i tar -t http://www.us.apache.org/dist/cassandra/1.2.13/apache-cassandra-1.2.13-bin.tar.gz upgradetest cassandra
Entering the API (tail -f /home/mshuler/.automaton/automaton.log for more info)
mshuler@hana:~$ ctool start upgradetest cassandra

----

mshuler@hana:~$ ctool ssh upgradetest 0
<...>
automaton@ip-10-139-1-113:~$ ./dsc-cassandra/tools/bin/cassandra-stress
Created keyspaces. Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,latency/95th/99.9th,elapsed_time
3869,386,386,104.3,210.7,327.0,10
10832,696,696,84.9,187.9,292.6,20
19889,905,905,72.8,168.2,275.7,30
<...>
966878,2595,2595,24.1,31.1,195.2,435
994244,2736,2736,24.0,31.1,195.2,445
1000000,575,575,23.8,31.1,195.2,447
END
automaton@ip-10-139-1-113:~$ nodetool status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address         Load       Owns (effective)  Host ID                               Token                                    Rack
UN  10.139.11.168   97.51 MB   33.3%             ec97c163-8f2d-4019-a3d1-55df5e4037d4  -3074457345618258603                     rack1
UN  10.139.1.113    97.44 MB   33.3%             33b1cd06-e17b-4332-8066-0c6c401e0cf3  -9223372036854775808                     rack1
UN  10.238.221.115  97.34 MB   33.3%             73a76d3f-73ef-481d-b603-0833c0ff80c2  3074457345618258602                      rack1
automaton@ip-10-139-1-113:~$ nodetool gossipinfo
/10.238.221.115
  RELEASE_VERSION:1.2.13
  DC:datacenter1
  STATUS:NORMAL,3074457345618258602
  RPC_ADDRESS:0.0.0.0
  NET_VERSION:6
  LOAD:1.02066255E8
  SCHEMA:8b351435-81ef-3a14-adf7-8555e2f19ecd
  RACK:rack1
  SEVERITY:0.0
  HOST_ID:73a76d3f-73ef-481d-b603-0833c0ff80c2
/10.139.1.113
  RELEASE_VERSION:1.2.13
  DC:datacenter1
  STATUS:NORMAL,-9223372036854775808
  RPC_ADDRESS:0.0.0.0
  NET_VERSION:6
  LOAD:9.2039687E7
  SCHEMA:8b351435-81ef-3a14-adf7-8555e2f19ecd
  RACK:rack1
  SEVERITY:0.0
  HOST_ID:33b1cd06-e17b-4332-8066-0c6c401e0cf3
/10.139.11.168
  RELEASE_VERSION:1.2.13
  DC:datacenter1
  STATUS:NORMAL,-3074457345618258603
  RPC_ADDRESS:0.0.0.0
  NET_VERSION:6
  LOAD:1.02245066E8
  SCHEMA:8b351435-81ef-3a14-adf7-8555e2f19ecd
  RACK:rack1
  SEVERITY:0.0
  HOST_ID:ec97c163-8f2d-4019-a3d1-55df5e4037d4

----

automaton@ip-10-139-1-113:~$ wget -q http://www.us.apache.org/dist/cassandra/2.0.4/apache-cassandra-2.0.4-bin.tar.gz
automaton@ip-10-139-1-113:~$ tar xzf apache-cassandra-2.0.4-bin.tar.gz
automaton@ip-10-139-1-113:~$ cp -p apache-cassandra-2.0.4/conf/cassandra.yaml apache-cassandra-2.0.4/conf/cassandra.yaml.dist

automaton@ip-10-139-1-113:~$ vi apache-cassandra-2.0.4/conf/cassandra.yaml
[transfer values from 1.2.13 config]

automaton@ip-10-139-1-113:~$ diff apache-cassandra-2.0.4/conf/cassandra.yaml.dist apache-cassandra-2.0.4/conf/cassandra.yaml
10c10
< cluster_name: 'Test Cluster'
---
> cluster_name: upgradetest
24c24
< num_tokens: 256
---
> #num_tokens: 256
30c30
< # initial_token:
---
> initial_token: -9223372036854775808
227c227
<           - seeds: ""127.0.0.1""
---
>           - seeds: 10.139.1.113,10.139.11.168
297c297
< listen_address: localhost
---
> listen_address: 10.139.1.113
335c335
< rpc_address: localhost
---
> rpc_address: 0.0.0.0

----

automaton@ip-10-139-1-113:~$ nodetool drain
automaton@ip-10-139-1-113:~$ ps aux | grep [j]ava
automaton@ip-10-139-1-113:~$ mv dsc-cassandra/ dsc-cassandra_1.2.13
automaton@ip-10-139-1-113:~$ mv apache-cassandra-2.0.4 dsc-cassandra

----

mshuler@hana:~$ ctool start upgradetest -n 0 cassandra

----

automaton@ip-10-139-1-113:~$ nodetool status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address         Load       Owns   Host ID                               Token                                    Rack
UN  10.139.1.113    98.94 MB   33.3%  33b1cd06-e17b-4332-8066-0c6c401e0cf3  -9223372036854775808                     rack1
DN  10.139.11.168   97.51 MB   33.3%  ec97c163-8f2d-4019-a3d1-55df5e4037d4  -3074457345618258603                     rack1
DN  10.238.221.115  97.34 MB   33.3%  73a76d3f-73ef-481d-b603-0833c0ff80c2  3074457345618258602                      rack1
automaton@ip-10-139-1-113:~$ nodetool gossipinfo
/10.238.221.115
  SEVERITY:0.0
  RPC_ADDRESS:0.0.0.0
  DC:datacenter1
  RELEASE_VERSION:1.2.13
  LOAD:1.02066255E8
  STATUS:NORMAL,3074457345618258602
  SCHEMA:8b351435-81ef-3a14-adf7-8555e2f19ecd
  NET_VERSION:6
  RACK:rack1
  HOST_ID:73a76d3f-73ef-481d-b603-0833c0ff80c2
/10.139.1.113
  RPC_ADDRESS:0.0.0.0
  SEVERITY:0.0
  DC:datacenter1
  RELEASE_VERSION:2.0.4
  LOAD:1.03750451E8
  STATUS:NORMAL,-9223372036854775808
  SCHEMA:dfafb212-5b8f-31cb-a80b-2ba58fcef73d
  NET_VERSION:7
  RACK:rack1
  HOST_ID:33b1cd06-e17b-4332-8066-0c6c401e0cf3
/10.139.11.168
  SEVERITY:0.0
  RPC_ADDRESS:0.0.0.0
  DC:datacenter1
  RELEASE_VERSION:1.2.13
  LOAD:1.02245066E8
  STATUS:NORMAL,-3074457345618258603
  SCHEMA:8b351435-81ef-3a14-adf7-8555e2f19ecd
  NET_VERSION:6
  RACK:rack1
  HOST_ID:ec97c163-8f2d-4019-a3d1-55df5e4037d4

automaton@ip-10-139-1-113:~$ ./dsc-cassandra/tools/bin/cassandra-stress
Unable to create stress keyspace: Keyspace names must be case-insensitively unique (""Keyspace1"" conflicts with ""Keyspace1"")
total,interval_op_rate,interval_key_rate,latency,95th,99.9th,elapsed_time
3553,355,355,111.6,195.9,452.3,10
8296,474,474,119.8,199.8,449.7,21
13498,520,520,117.9,197.3,438.7,31
<...>
940610,2883,2883,22.3,29.2,159.4,395
971153,3054,3054,22.3,29.0,159.4,405
1000000,2884,2884,22.0,28.8,160.0,415


Averages from the middle 80% of values:
interval_op_rate          : 2835
interval_key_rate         : 2835
latency median            : 24.2
latency 95th percentile   : 46.5
latency 99.9th percentile : 385.9
Total operation time      : 00:06:55
END
automaton@ip-10-139-1-113:~$
{code}",07/Jan/14 17:28;brandon.williams;So you're saying you can't repro?,"07/Jan/14 17:30;mshuler;So stress seems to write data (today, for whatever reason, which outright failed last night), but the nodes do not appear to be talking nicely to one another:
{code}
node0 (upgraded)

automaton@ip-10-139-1-113:~$ nodetool ring
Note: Ownership information does not include topology; for complete information, specify a keyspace

Datacenter: datacenter1
==========
Address         Rack        Status State   Load            Owns                Token                                       
                                                                               3074457345618258602                         
10.139.1.113    rack1       Up     Normal  22.56 MB        33.33%              -9223372036854775808                        
10.139.11.168   rack1       Down   Normal  241.09 MB       33.33%              -3074457345618258603                        
10.238.221.115  rack1       Down   Normal  243.86 MB       33.33%              3074457345618258602                         

automaton@ip-10-139-1-113:~$ cqlsh
Connected to upgradetest at localhost:9160.
[cqlsh 4.1.0 | Cassandra 2.0.4 | CQL spec 3.1.1 | Thrift protocol 19.39.0]
Use HELP for help.
cqlsh> DROP KEYSPACE ""Keyspace1"";
cqlsh> DESC KEYSPACES 

system  system_traces

cqlsh> 
automaton@ip-10-139-1-113:~$ nodetool ring

Datacenter: datacenter1
==========
Address         Rack        Status State   Load            Owns                Token                                       
                                                                               -9223372036854775808                        
10.139.11.168   rack1       Down   Normal  241.08 MB       66.67%              -3074457345618258603                        
10.238.221.115  rack1       Down   Normal  243.86 MB       66.67%              3074457345618258602                         
10.139.1.113    rack1       Up     Normal  97.75 KB        66.67%              -9223372036854775808                        

automaton@ip-10-139-1-113:~$

node1

automaton@ip-10-139-11-168:~$ nodetool ring

Datacenter: datacenter1
==========
Replicas: 0

Address         Rack        Status State   Load            Owns                Token                                       
                                                                               3074457345618258602                         
10.139.11.168   rack1       Up     Normal  241.08 MB       33.33%              -3074457345618258603                        
10.139.1.113    rack1       Up     Normal  22.56 MB        33.33%              -9223372036854775808                        
10.238.221.115  rack1       Up     Normal  243.86 MB       33.33%              3074457345618258602                         

automaton@ip-10-139-11-168:~$ cqlsh
Connected to upgradetest at localhost:9160.
[cqlsh 3.1.8 | Cassandra 1.2.13 | CQL spec 3.0.0 | Thrift protocol 19.36.2]
Use HELP for help.
cqlsh> DESC KEYSPACES 

system  Keyspace1  system_traces

cqlsh> 
automaton@ip-10-139-11-168:~$ nodetool ring

Datacenter: datacenter1
==========
Replicas: 0

Address         Rack        Status State   Load            Owns                Token                                       
                                                                               3074457345618258602                         
10.139.11.168   rack1       Up     Normal  241.08 MB       33.33%              -3074457345618258603                        
10.139.1.113    rack1       Up     Normal  97.75 KB        33.33%              -9223372036854775808                        
10.238.221.115  rack1       Up     Normal  243.86 MB       33.33%              3074457345618258602                         

automaton@ip-10-139-11-168:~$

node2

automaton@ip-10-238-221-115:~$ nodetool ring

Datacenter: datacenter1
==========
Replicas: 0

Address         Rack        Status State   Load            Owns                Token                                       
                                                                               3074457345618258602                         
10.139.11.168   rack1       Up     Normal  241.08 MB       33.33%              -3074457345618258603                        
10.139.1.113    rack1       Up     Normal  22.56 MB        33.33%              -9223372036854775808                        
10.238.221.115  rack1       Up     Normal  243.86 MB       33.33%              3074457345618258602                         

automaton@ip-10-238-221-115:~$ cqlsh
Connected to upgradetest at localhost:9160.
[cqlsh 3.1.8 | Cassandra 1.2.13 | CQL spec 3.0.0 | Thrift protocol 19.36.2]
Use HELP for help.
cqlsh> DESC KEYSPACES 

system  Keyspace1  system_traces

cqlsh> 
automaton@ip-10-238-221-115:~$ nodetool ring

Datacenter: datacenter1
==========
Replicas: 0

Address         Rack        Status State   Load            Owns                Token                                       
                                                                               3074457345618258602                         
10.139.11.168   rack1       Up     Normal  241.08 MB       33.33%              -3074457345618258603                        
10.139.1.113    rack1       Up     Normal  97.75 KB        33.33%              -9223372036854775808                        
10.238.221.115  rack1       Up     Normal  243.86 MB       33.33%              3074457345618258602                         

automaton@ip-10-238-221-115:~$
{code}","07/Jan/14 17:53;mshuler;I understand the above, due to schema not being able to propogate, is expected, until all the nodes are upgraded.","07/Jan/14 18:15;mshuler;Aha!  Running stress with the whole cluster online, prior to upgrading node0 (above), meant that all nodes had Keyspace1.

The write failure that I believe both of us were seeing last night was running stress on the upgraded node and other nodes without ever having run it prior, so the schemas don't agree and cannot propogate the keyspace:

node0 (upgraded)
{code}
automaton@ip-10-180-239-135:~$ ./dsc-cassandra/tools/bin/cassandra-stress
Created keyspaces. Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,latency,95th,99.9th,elapsed_time
25,2,2,238.9,263.2,263.3,10
25,0,0,238.9,263.2,263.3,20
Operation [26] retried 10 times - error inserting key 0000026 ((TimedOutException))

Operation [19] retried 10 times - error inserting key 0000019 ((TimedOutException))

Operation [18] retried 10 times - error inserting key 0000018 ((TimedOutException))

Operation [3] retried 10 times - error inserting key 0000003 ((TimedOutException))

Operation [46] retried 10 times - error inserting key 0000046 ((TimedOutException))

Operation [47] retried 10 times - error inserting key 0000047 ((TimedOutException))

Operation [15] retried 10 times - error inserting key 0000015 ((TimedOutException))

Operation [6] retried 10 times - error inserting key 0000006 ((TimedOutException))

Operation [2] retried 10 times - error inserting key 0000002 ((TimedOutException))

Operation [37] retried 10 times - error inserting key 0000037 ((TimedOutException))

Operation [14] retried 10 times - error inserting key 0000014 ((TimedOutException))

Operation [41] retried 10 times - error inserting key 0000041 ((TimedOutException))

Operation [39] retried 10 times - error inserting key 0000039 ((TimedOutException))

Operation [22] retried 10 times - error inserting key 0000022 ((TimedOutException))

Operation [0] retried 10 times - error inserting key 0000000 ((TimedOutException))

Operation [16] retried 10 times - error inserting key 0000016 ((TimedOutException))

Operation [45] retried 10 times - error inserting key 0000045 ((TimedOutException))

Operation [12] retried 10 times - error inserting key 0000012 ((TimedOutException))

Operation [32] retried 10 times - error inserting key 0000032 ((TimedOutException))

Operation [9] retried 10 times - error inserting key 0000009 ((TimedOutException))

Operation [10] retried 10 times - error inserting key 0000010 ((TimedOutException))

Operation [44] retried 10 times - error inserting key 0000044 ((TimedOutException))

Operation [34] retried 10 times - error inserting key 0000034 ((TimedOutException))

Operation [5] retried 10 times - error inserting key 0000005 ((TimedOutException))

Operation [49] retried 10 times - error inserting key 0000049 ((TimedOutException))

Operation [11] retried 10 times - error inserting key 0000011 ((TimedOutException))

Operation [54] retried 10 times - error inserting key 0000054 ((TimedOutException))

Operation [33] retried 10 times - error inserting key 0000033 ((TimedOutException))

Operation [51] retried 10 times - error inserting key 0000051 ((TimedOutException))

Operation [57] retried 10 times - error inserting key 0000057 ((TimedOutException))

Operation [58] retried 10 times - error inserting key 0000058 ((TimedOutException))

Operation [53] retried 10 times - error inserting key 0000053 ((TimedOutException))

Operation [55] retried 10 times - error inserting key 0000055 ((TimedOutException))

Operation [8] retried 10 times - error inserting key 0000008 ((TimedOutException))

Operation [42] retried 10 times - error inserting key 0000042 ((TimedOutException))

Operation [52] retried 10 times - error inserting key 0000052 ((TimedOutException))

Operation [20] retried 10 times - error inserting key 0000020 ((TimedOutException))

Operation [31] retried 10 times - error inserting key 0000031 ((TimedOutException))

Operation [63] retried 10 times - error inserting key 0000063 ((TimedOutException))

Operation [59] retried 10 times - error inserting key 0000059 ((TimedOutException))

Operation [64] retried 10 times - error inserting key 0000064 ((TimedOutException))

Operation [66] retried 10 times - error inserting key 0000066 ((TimedOutException))

Operation [61] retried 10 times - error inserting key 0000061 ((TimedOutException))

Operation [67] retried 10 times - error inserting key 0000067 ((TimedOutException))

Operation [60] retried 10 times - error inserting key 0000060 ((TimedOutException))

Operation [70] retried 10 times - error inserting key 0000070 ((TimedOutException))

Operation [71] retried 10 times - error inserting key 0000071 ((TimedOutException))

Operation [73] retried 10 times - error inserting key 0000073 ((TimedOutException))

Operation [72] retried 10 times - error inserting key 0000072 ((TimedOutException))

Operation [74] retried 10 times - error inserting key 0000074 ((TimedOutException))

25,0,0,238.9,263.2,263.3,21
FAILURE
automaton@ip-10-180-239-135:~$
{code}

node1 and node2 error the same way:
{code}
automaton@ip-10-180-230-58:~$ ./dsc-cassandra/tools/bin/cassandra-stress
Unable to create stress keyspace: Keyspace names must be case-insensitively unique (""Keyspace1"" conflicts with ""Keyspace1"")
total,interval_op_rate,interval_key_rate,latency/95th/99.9th,elapsed_time
Operation [9] retried 10 times - error inserting key 0000009 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [31] retried 10 times - error inserting key 0000031 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [27] retried 10 times - error inserting key 0000027 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [46] retried 10 times - error inserting key 0000046 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [11] retried 10 times - error inserting key 0000011 ((InvalidRequestException): unconfigured columnfamily Standard1)                                                                                              
                                                                                                              
Operation [39] retried 10 times - error inserting key 0000039 ((InvalidRequestException): unconfigured columnfamily Standard1)                                                                                              
                                                                                                              
Operation [43] retried 10 times - error inserting key 0000043 ((InvalidRequestException): unconfigured columnfamily Standard1)                                                                                              
                                                                                                              
Operation [2] retried 10 times - error inserting key 0000002 ((InvalidRequestException): unconfigured columnfamily Standard1)                                                                                               

Operation [13] retried 10 times - error inserting key 0000013 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [37] retried 10 times - error inserting key 0000037 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [49] retried 10 times - error inserting key 0000049 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [15] retried 10 times - error inserting key 0000015 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [19] retried 10 times - error inserting key 0000019 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [28] retried 10 times - error inserting key 0000028 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [21] retried 10 times - error inserting key 0000021 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [10] retried 10 times - error inserting key 0000010 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [17] retried 10 times - error inserting key 0000017 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [18] retried 10 times - error inserting key 0000018 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [30] retried 10 times - error inserting key 0000030 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [7] retried 10 times - error inserting key 0000007 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [42] retried 10 times - error inserting key 0000042 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [6] retried 10 times - error inserting key 0000006 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [25] retried 10 times - error inserting key 0000025 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [5] retried 10 times - error inserting key 0000005 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [44] retried 10 times - error inserting key 0000044 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [41] retried 10 times - error inserting key 0000041 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [16] retried 10 times - error inserting key 0000016 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [36] retried 10 times - error inserting key 0000036 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [14] retried 10 times - error inserting key 0000014 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [23] retried 10 times - error inserting key 0000023 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [29] retried 10 times - error inserting key 0000029 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [40] retried 10 times - error inserting key 0000040 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [1] retried 10 times - error inserting key 0000001 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [24] retried 10 times - error inserting key 0000024 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [8] retried 10 times - error inserting key 0000008 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [32] retried 10 times - error inserting key 0000032 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [4] retried 10 times - error inserting key 0000004 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [33] retried 10 times - error inserting key 0000033 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [48] retried 10 times - error inserting key 0000048 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [35] retried 10 times - error inserting key 0000035 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [45] retried 10 times - error inserting key 0000045 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [22] retried 10 times - error inserting key 0000022 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [34] retried 10 times - error inserting key 0000034 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [26] retried 10 times - error inserting key 0000026 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [47] retried 10 times - error inserting key 0000047 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [3] retried 10 times - error inserting key 0000003 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [20] retried 10 times - error inserting key 0000020 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [12] retried 10 times - error inserting key 0000012 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [38] retried 10 times - error inserting key 0000038 ((InvalidRequestException): unconfigured columnfamily Standard1)

Operation [0] retried 10 times - error inserting key 0000000 ((InvalidRequestException): unconfigured columnfamily Standard1)

0,0,0,0.0,0.0,0.0,1
FAILURE
automaton@ip-10-180-230-58:~$
{code}

I believe the answer to this is notabug and ""Don't do that.""  :)","07/Jan/14 18:21;jjordan;So sounds like the write failures aren't a bug.  Yes, schema changes are not supported in a mixed cluster, so you have to run stress (or make the tables) before upgrading.

But that fact that the other nodes are shown as DOWN while doing the upgrade is an issue.  So I would either keep this open for that, or make a new ticket for it.","07/Jan/14 18:25;brandon.williams;Jeremiah is right, up/down status is independent of schema.",07/Jan/14 19:07;brandon.williams;Can you bump o.a.c.gms up to TRACE and see what's going on with gossip/FD?,"07/Jan/14 20:20;mshuler;6554_trace_system.log is the complete system log from:
- 1.2.13 startup and stop
- grab 2.0.4, configure c*.yaml and set log4j.logger.org.apache.cassandra.gms=TRACE
- start 2.0.4
- run nodetool status:
{code}
automaton@ip-10-91-137-10:~$ nodetool status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address         Load       Owns (effective)  Host ID                               Token                                    Rack
DN  10.180.236.244  55.18 KB   66.7%             fbee6d5a-b5de-4bdb-81ac-0fa69bab9e7e  -3074457345618258603                     rack1
UN  10.91.137.10    63.04 KB   66.7%             d1161fd0-b612-4070-8a03-400d2c12c4d1  -9223372036854775808                     rack1
DN  10.182.208.161  63.22 KB   66.7%             03b1e32d-fdaf-4374-afad-6aeee1ffec0a  3074457345618258603                      rack1
{code}
- stop cassandra","07/Jan/14 20:32;mshuler;Just the relevant portion:
{code}
$ sudo echo ""== Begin =="" >> /var/log/cassandra/system.log ; nodetool status ; sudo echo ""== End =="" >> /var/log/cassandra/system.log
{code}
{code}
== Begin ==
TRACE [GossipTasks:1] 2014-01-07 20:29:59,731 Gossiper.java (line 133) My heartbeat is now 438
TRACE [GossipTasks:1] 2014-01-07 20:29:59,732 Gossiper.java (line 398) Gossip Digests are : /10.180.236.244:1389124753:2428 /10.91.137.10:1389126546:438 /10.182.208.161:1389124753:2265 
TRACE [GossipTasks:1] 2014-01-07 20:29:59,732 Gossiper.java (line 530) Sending a GossipDigestSyn to /10.182.208.161 ...
TRACE [GossipTasks:1] 2014-01-07 20:29:59,732 Gossiper.java (line 530) Sending a GossipDigestSyn to /10.180.236.244 ...
TRACE [GossipTasks:1] 2014-01-07 20:29:59,733 Gossiper.java (line 598) Performing status check ...
TRACE [GossipTasks:1] 2014-01-07 20:29:59,733 FailureDetector.java (line 215) PHI for /10.180.236.244 : 0.5008780617528535
TRACE [GossipTasks:1] 2014-01-07 20:29:59,733 FailureDetector.java (line 215) PHI for /10.182.208.161 : 0.6590114452533097
TRACE [GossipStage:1] 2014-01-07 20:29:59,734 GossipDigestAckVerbHandler.java (line 41) Received a GossipDigestAckMessage from /10.182.208.161
TRACE [GossipStage:1] 2014-01-07 20:29:59,735 GossipDigestAckVerbHandler.java (line 52) Received ack with 1 digests and 2 states
TRACE [GossipStage:1] 2014-01-07 20:29:59,735 FailureDetector.java (line 189) reporting /10.182.208.161
TRACE [GossipStage:1] 2014-01-07 20:29:59,735 Gossiper.java (line 932) /10.180.236.244local generation 1389124753, remote generation 1389124753
TRACE [GossipStage:1] 2014-01-07 20:29:59,735 Gossiper.java (line 978) Updating heartbeat state version to 2428 from 2428 for /10.180.236.244 ...
TRACE [GossipStage:1] 2014-01-07 20:29:59,749 Gossiper.java (line 932) /10.182.208.161local generation 1389124753, remote generation 1389124753
TRACE [GossipStage:1] 2014-01-07 20:29:59,749 Gossiper.java (line 978) Updating heartbeat state version to 2269 from 2265 for /10.182.208.161 ...
TRACE [GossipStage:1] 2014-01-07 20:29:59,751 Gossiper.java (line 730) local heartbeat version 438 greater than 433 for /10.91.137.10
TRACE [GossipStage:1] 2014-01-07 20:29:59,751 Gossiper.java (line 744) Adding state SEVERITY: 0.0
TRACE [GossipStage:1] 2014-01-07 20:29:59,751 Gossiper.java (line 744) Adding state SCHEMA: 9f0f4c3b-b568-3650-8ff7-39ae0d5be671
TRACE [GossipStage:1] 2014-01-07 20:29:59,752 GossipDigestAckVerbHandler.java (line 84) Sending a GossipDigestAck2Message to /10.182.208.161
TRACE [GossipStage:1] 2014-01-07 20:29:59,775 GossipDigestAckVerbHandler.java (line 41) Received a GossipDigestAckMessage from /10.180.236.244
TRACE [GossipStage:1] 2014-01-07 20:29:59,775 GossipDigestAckVerbHandler.java (line 52) Received ack with 1 digests and 2 states
TRACE [GossipStage:1] 2014-01-07 20:29:59,775 Gossiper.java (line 932) /10.180.236.244local generation 1389124753, remote generation 1389124753
TRACE [GossipStage:1] 2014-01-07 20:29:59,775 Gossiper.java (line 952) Ignoring remote version 2431 <= 2431 for /10.180.236.244
TRACE [GossipStage:1] 2014-01-07 20:29:59,776 Gossiper.java (line 932) /10.182.208.161local generation 1389124753, remote generation 1389124753
TRACE [GossipStage:1] 2014-01-07 20:29:59,776 Gossiper.java (line 952) Ignoring remote version 2269 <= 2270 for /10.182.208.161
TRACE [GossipStage:1] 2014-01-07 20:29:59,790 Gossiper.java (line 730) local heartbeat version 438 greater than 433 for /10.91.137.10
TRACE [GossipStage:1] 2014-01-07 20:29:59,791 Gossiper.java (line 744) Adding state SEVERITY: 0.0
TRACE [GossipStage:1] 2014-01-07 20:29:59,791 Gossiper.java (line 744) Adding state SCHEMA: 9f0f4c3b-b568-3650-8ff7-39ae0d5be671
TRACE [GossipStage:1] 2014-01-07 20:29:59,791 GossipDigestAckVerbHandler.java (line 84) Sending a GossipDigestAck2Message to /10.180.236.244
TRACE [GossipStage:1] 2014-01-07 20:29:59,971 GossipDigestSynVerbHandler.java (line 40) Received a GossipDigestSynMessage from /10.180.236.244
TRACE [GossipStage:1] 2014-01-07 20:29:59,971 GossipDigestSynVerbHandler.java (line 71) Gossip syn digests are : /10.182.208.161:1389124753:2269 /10.91.137.10:1389126546:438 /10.180.236.244:1389124753:2432 
TRACE [GossipStage:1] 2014-01-07 20:29:59,971 Gossiper.java (line 744) Adding state SCHEMA: 9f0f4c3b-b568-3650-8ff7-39ae0d5be671
TRACE [GossipStage:1] 2014-01-07 20:29:59,972 Gossiper.java (line 744) Adding state SCHEMA: 59adb24e-f3cd-3e02-97f0-5b395827453f
TRACE [GossipStage:1] 2014-01-07 20:29:59,972 GossipDigestSynVerbHandler.java (line 79) sending 1 digests and 2 deltas
TRACE [GossipStage:1] 2014-01-07 20:29:59,972 GossipDigestSynVerbHandler.java (line 84) Sending a GossipDigestAckMessage to /10.180.236.244
TRACE [GossipStage:1] 2014-01-07 20:29:59,986 GossipDigestAck2VerbHandler.java (line 38) Received a GossipDigestAck2Message from /10.180.236.244
TRACE [GossipStage:1] 2014-01-07 20:29:59,987 FailureDetector.java (line 189) reporting /10.180.236.244
TRACE [GossipStage:1] 2014-01-07 20:29:59,987 Gossiper.java (line 932) /10.180.236.244local generation 1389124753, remote generation 1389124753
TRACE [GossipStage:1] 2014-01-07 20:29:59,987 Gossiper.java (line 978) Updating heartbeat state version to 2432 from 2428 for /10.180.236.244 ...
TRACE [GossipStage:1] 2014-01-07 20:30:00,594 GossipDigestSynVerbHandler.java (line 40) Received a GossipDigestSynMessage from /10.182.208.161
TRACE [GossipStage:1] 2014-01-07 20:30:00,594 GossipDigestSynVerbHandler.java (line 71) Gossip syn digests are : /10.182.208.161:1389124753:2276 /10.91.137.10:1389126546:438 /10.180.236.244:1389124753:2432 
TRACE [GossipStage:1] 2014-01-07 20:30:00,594 Gossiper.java (line 744) Adding state SEVERITY: 0.0
TRACE [GossipStage:1] 2014-01-07 20:30:00,594 Gossiper.java (line 744) Adding state SCHEMA: 9f0f4c3b-b568-3650-8ff7-39ae0d5be671
TRACE [GossipStage:1] 2014-01-07 20:30:00,595 GossipDigestSynVerbHandler.java (line 79) sending 1 digests and 1 deltas
TRACE [GossipStage:1] 2014-01-07 20:30:00,595 GossipDigestSynVerbHandler.java (line 84) Sending a GossipDigestAckMessage to /10.182.208.161
TRACE [GossipStage:1] 2014-01-07 20:30:00,607 GossipDigestAck2VerbHandler.java (line 38) Received a GossipDigestAck2Message from /10.182.208.161
TRACE [GossipStage:1] 2014-01-07 20:30:00,607 FailureDetector.java (line 189) reporting /10.182.208.161
TRACE [GossipStage:1] 2014-01-07 20:30:00,607 Gossiper.java (line 932) /10.182.208.161local generation 1389124753, remote generation 1389124753
TRACE [GossipStage:1] 2014-01-07 20:30:00,607 Gossiper.java (line 978) Updating heartbeat state version to 2276 from 2269 for /10.182.208.161 ...
TRACE [GossipTasks:1] 2014-01-07 20:30:00,750 Gossiper.java (line 133) My heartbeat is now 445
TRACE [GossipTasks:1] 2014-01-07 20:30:00,751 Gossiper.java (line 398) Gossip Digests are : /10.91.137.10:1389126546:445 /10.180.236.244:1389124753:2432 /10.182.208.161:1389124753:2276 
TRACE [GossipTasks:1] 2014-01-07 20:30:00,751 Gossiper.java (line 530) Sending a GossipDigestSyn to /10.182.208.161 ...
TRACE [GossipTasks:1] 2014-01-07 20:30:00,751 Gossiper.java (line 530) Sending a GossipDigestSyn to /10.180.236.244 ...
TRACE [GossipTasks:1] 2014-01-07 20:30:00,752 Gossiper.java (line 598) Performing status check ...
TRACE [GossipTasks:1] 2014-01-07 20:30:00,752 FailureDetector.java (line 215) PHI for /10.180.236.244 : 0.503756137674087
TRACE [GossipTasks:1] 2014-01-07 20:30:00,753 FailureDetector.java (line 215) PHI for /10.182.208.161 : 0.09675013184828929
TRACE [GossipStage:1] 2014-01-07 20:30:00,753 GossipDigestAckVerbHandler.java (line 41) Received a GossipDigestAckMessage from /10.182.208.161
TRACE [GossipStage:1] 2014-01-07 20:30:00,754 GossipDigestAckVerbHandler.java (line 52) Received ack with 1 digests and 1 states
TRACE [GossipStage:1] 2014-01-07 20:30:00,754 Gossiper.java (line 932) /10.182.208.161local generation 1389124753, remote generation 1389124753
TRACE [GossipStage:1] 2014-01-07 20:30:00,754 Gossiper.java (line 978) Updating heartbeat state version to 2276 from 2276 for /10.182.208.161 ...
TRACE [GossipStage:1] 2014-01-07 20:30:00,762 Gossiper.java (line 730) local heartbeat version 445 greater than 444 for /10.91.137.10
TRACE [GossipStage:1] 2014-01-07 20:30:00,763 GossipDigestAckVerbHandler.java (line 84) Sending a GossipDigestAck2Message to /10.182.208.161
TRACE [GossipStage:1] 2014-01-07 20:30:00,763 GossipDigestAckVerbHandler.java (line 41) Received a GossipDigestAckMessage from /10.180.236.244
TRACE [GossipStage:1] 2014-01-07 20:30:00,763 GossipDigestAckVerbHandler.java (line 52) Received ack with 2 digests and 1 states
TRACE [GossipStage:1] 2014-01-07 20:30:00,764 Gossiper.java (line 932) /10.180.236.244local generation 1389124753, remote generation 1389124753
TRACE [GossipStage:1] 2014-01-07 20:30:00,764 Gossiper.java (line 978) Updating heartbeat state version to 2432 from 2432 for /10.180.236.244 ...
TRACE [GossipStage:1] 2014-01-07 20:30:00,765 Gossiper.java (line 730) local heartbeat version 445 greater than 440 for /10.91.137.10
TRACE [GossipStage:1] 2014-01-07 20:30:00,765 Gossiper.java (line 744) Adding state SEVERITY: 0.0
TRACE [GossipStage:1] 2014-01-07 20:30:00,765 Gossiper.java (line 744) Adding state SCHEMA: 9f0f4c3b-b568-3650-8ff7-39ae0d5be671
TRACE [GossipStage:1] 2014-01-07 20:30:00,766 Gossiper.java (line 730) local heartbeat version 2276 greater than 2272 for /10.182.208.161
TRACE [GossipStage:1] 2014-01-07 20:30:00,766 Gossiper.java (line 744) Adding state SEVERITY: 0.0
TRACE [GossipStage:1] 2014-01-07 20:30:00,766 Gossiper.java (line 744) Adding state SCHEMA: 59adb24e-f3cd-3e02-97f0-5b395827453f
TRACE [GossipStage:1] 2014-01-07 20:30:00,766 GossipDigestAckVerbHandler.java (line 84) Sending a GossipDigestAck2Message to /10.180.236.244
TRACE [GossipStage:1] 2014-01-07 20:30:00,961 GossipDigestSynVerbHandler.java (line 40) Received a GossipDigestSynMessage from /10.180.236.244
TRACE [GossipStage:1] 2014-01-07 20:30:00,962 GossipDigestSynVerbHandler.java (line 71) Gossip syn digests are : /10.91.137.10:1389126546:445 /10.180.236.244:1389124753:2441 /10.182.208.161:1389124753:2279 
TRACE [GossipStage:1] 2014-01-07 20:30:00,962 GossipDigestSynVerbHandler.java (line 79) sending 1 digests and 0 deltas
TRACE [GossipStage:1] 2014-01-07 20:30:00,962 GossipDigestSynVerbHandler.java (line 84) Sending a GossipDigestAckMessage to /10.180.236.244
TRACE [GossipStage:1] 2014-01-07 20:30:00,964 GossipDigestAck2VerbHandler.java (line 38) Received a GossipDigestAck2Message from /10.180.236.244
TRACE [GossipStage:1] 2014-01-07 20:30:00,964 FailureDetector.java (line 189) reporting /10.180.236.244
TRACE [GossipStage:1] 2014-01-07 20:30:00,964 Gossiper.java (line 932) /10.180.236.244local generation 1389124753, remote generation 1389124753
TRACE [GossipStage:1] 2014-01-07 20:30:00,964 Gossiper.java (line 978) Updating heartbeat state version to 2441 from 2432 for /10.180.236.244 ...
== End ==
{code}","07/Jan/14 20:48;brandon.williams;The strange this is, it does mark the other non-upgraded nodes as up:

{noformat}
TRACE [GossipStage:1] 2014-01-07 20:07:53,456 GossipDigestAck2VerbHandler.java (line 38) Received a GossipDigestAck2Message from /10.180.236.244
DEBUG [GossipStage:1] 2014-01-07 20:07:53,456 Gossiper.java (line 790) Clearing interval times for /10.180.236.244 due to generation change
TRACE [GossipStage:1] 2014-01-07 20:07:53,457 FailureDetector.java (line 189) reporting /10.180.236.244
DEBUG [GossipStage:1] 2014-01-07 20:07:53,467 Gossiper.java (line 790) Clearing interval times for /10.182.208.161 due to generation change
TRACE [GossipStage:1] 2014-01-07 20:07:53,468 FailureDetector.java (line 189) reporting /10.182.208.161
TRACE [GossipStage:1] 2014-01-07 20:07:53,468 Gossiper.java (line 932) /10.180.236.244local generation 0, remote generation 1389124753
TRACE [GossipStage:1] 2014-01-07 20:07:53,468 Gossiper.java (line 937) Updating heartbeat state generation to 1389124753 from 0 for /10.180.236.244
 INFO [GossipStage:1] 2014-01-07 20:07:53,468 Gossiper.java (line 868) Node /10.180.236.244 has restarted, now UP
TRACE [GossipStage:1] 2014-01-07 20:07:53,468 Gossiper.java (line 873) Adding endpoint state for /10.180.236.244
TRACE [GossipStage:1] 2014-01-07 20:07:53,506 Gossiper.java (line 815) Sending a EchoMessage to /10.180.236.244
 INFO [HANDSHAKE-/10.180.236.244] 2014-01-07 20:07:53,534 OutboundTcpConnection.java (line 386) Handshaking version with /10.180.236.244
TRACE [GossipStage:1] 2014-01-07 20:07:53,559 TokenSerializer.java (line 56) Reading token of 8 bytes
 INFO [GossipStage:1] 2014-01-07 20:07:53,562 StorageService.java (line 1445) Node /10.180.236.244 state jump to normal
TRACE [GossipStage:1] 2014-01-07 20:07:53,570 Gossiper.java (line 932) /10.182.208.161local generation 0, remote generation 1389124753
TRACE [GossipStage:1] 2014-01-07 20:07:53,571 Gossiper.java (line 937) Updating heartbeat state generation to 1389124753 from 0 for /10.182.208.161
 INFO [GossipStage:1] 2014-01-07 20:07:53,571 Gossiper.java (line 868) Node /10.182.208.161 has restarted, now UP
{noformat}

And never marks them down after that.  Can you get a gms trace from one of the other nodes too?",07/Jan/14 21:01;mshuler;Added full system.log from node1 and node2,"07/Jan/14 21:27;brandon.williams;Totally baffled, these logs show the same thing: everything's working normally.","09/Jan/14 18:52;brandon.williams;What I'm seeing is that on the first boot of 2.0, ring/status show the other nodes as down, even though the logs indicate everything is fine.  And indeed, it is, reads and writes work normally.  The other nodes on 1.2 see the 2.0 node as up.  If you restart the 2.0 node at this point, the cosmetic problem goes away and everything appears normal.  This appears to stem from CASSANDRA-3533 unconditionally sending echo commands and only adding the node to liveMembers if it responds, which cannot happen with nodes < 2.0.  Patch to conditionally mark anything < 2.0 up without the echo.",09/Jan/14 22:52;thobbs;+1,09/Jan/14 23:02;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Quickly restarted nodes can list others as down indefinitely,CASSANDRA-6571,12688344,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,kohlisankalp,rlow,rlow,10/Jan/14 22:18,12/Mar/19 14:17,13/Mar/19 22:29,13/Jan/14 15:17,2.0.5,,,,,,0,gossip,,,,"In a healthy cluster, if a node is restarted quickly, it may list other nodes as down when it comes back up and never list them as up.  I reproduced it on a small cluster running in Docker containers.

1. Have a healthy 5 node cluster:

{quote}
$ nodetool status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address          Load       Tokens  Owns (effective)  Host ID                               Rack
UN  192.168.100.1    40.88 KB   256     38.3%             92930ef6-1b29-49f0-a8cd-f962b55dca1b  rack1
UN  192.168.100.254  80.63 KB   256     39.6%             ef15a717-9d60-48fb-80a9-e0973abdd55e  rack1
UN  192.168.100.3    87.78 KB   256     40.8%             4e6765db-97ed-4429-a9f4-8e29de247f18  rack1
UN  192.168.100.2    75.22 KB   256     40.6%             e89bc581-5345-4abd-88ba-7018371940fc  rack1
UN  192.168.100.4    80.83 KB   256     40.8%             466a9798-d484-44f0-aae8-bb2b78d80331  rack1
{quote}

2. Kill a node and restart it quickly:

bq. kill -9 <pid> && start-cassandra

3. Wait for the node to come back and more often than not, it lists one or more other nodes as down indefinitely:

{quote}
$ nodetool status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address          Load       Tokens  Owns (effective)  Host ID                               Rack
UN  192.168.100.1    40.88 KB   256     38.3%             92930ef6-1b29-49f0-a8cd-f962b55dca1b  rack1
UN  192.168.100.254  80.63 KB   256     39.6%             ef15a717-9d60-48fb-80a9-e0973abdd55e  rack1
DN  192.168.100.3    87.78 KB   256     40.8%             4e6765db-97ed-4429-a9f4-8e29de247f18  rack1
DN  192.168.100.2    75.22 KB   256     40.6%             e89bc581-5345-4abd-88ba-7018371940fc  rack1
DN  192.168.100.4    80.83 KB   256     40.8%             466a9798-d484-44f0-aae8-bb2b78d80331  rack1
{quote}

From trace logging, here's what I think is going on:

1. The nodes are all happy gossiping
2. Restart node X. When it comes back up it starts gossiping with the other nodes.
3. Before node X marks node Y as alive, X sends an echo message (introduced in CASSANDRA-3533)
4. The echo message is received by Y. To reply, Y attempts to reuse a connection to X. The connection is dead, but the message is attempted anyway but fails.
5. X never receives the echo back, so Y isn't marked as alive.
6. X gossips to Y again, but because the endpoint isAlive() returns true, it never calls markAlive() to properly set Y as alive.

I tried to fix this by defaulting isAlive=false in the constructor of EndpointState. This made it less likely to mark a node as down but it still happens.

The workaround is to leave a node down for a while so the connections die on the remaining nodes.",,,,,,,,,,,,,,,,,,,,,,,,11/Jan/14 18:17;brandon.williams;6571.txt;https://issues.apache.org/jira/secure/attachment/12622507/6571.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-10 22:35:17.992,,,no_permission,,,,,,,,,,,,367363,,,Mon Jan 13 15:17:16 UTC 2014,,,,,,0|i1rbm7:,367672,2.0.0,,,,,,,brandon.williams,brandon.williams,,,2.0 beta 1,,,,,,,10/Jan/14 22:35;brandon.williams;Worth noting we recently had another problem with CASSANDRA-3533 in CASSANDRA-6554,"11/Jan/14 00:08;kohlisankalp;The problem is that we are adding the endpoint state of the newly discovered endpoint to endpointStateMap in handleMajorStateChange method. This endpoint state has isAlive=true because the endpoint is alive. 
So if echo fails, realMarkAlive(new refactored method in trunk) method will never run.

The fix is to make the isAlive=false before sending the echo message. The reason we should do it is because now we rely on echo message to mark anything alive. So it should be marked false even if we hear about it being alive from another node. 

Fix in code.
private void markAlive(final InetAddress addr, final EndpointState localState)
    {
        if (MessagingService.instance().getVersion(addr) < MessagingService.VERSION_20)
        {
            realMarkAlive(addr, localState);
            return;
        }
        +  localState.markDead();
        MessageOut<EchoMessage> echoMessage = new MessageOut<EchoMessage>(MessagingService.Verb.ECHO, new EchoMessage(),   EchoMessage.serializer);
        logger.trace(""Sending a EchoMessage to {}"", addr);
        IAsyncCallback echoHandler = new IAsyncCallback()","11/Jan/14 06:44;vijay2win@yahoo.com;Not sure if this will fix it, because the remote machine has not responded back (echo message response). 
1) I think we need to always mark the nodes as dead and mark it up only after we received the echo response
2) I think we need to check or reset the socket in the receiving side, may be need to markDead (or retry the message after x seconds?)

May be because we removed the hibernate during restarts, this issue shows up? (we are not resetting the states) <== [~brandon.williams]
I think the hang on the echo response (socket.write())",11/Jan/14 16:21;brandon.williams;Isn't 1) what Sankalp is proposing?  I'm not sure what you mean about hibernating on restarts.,11/Jan/14 18:17;brandon.williams;Patchifying Sankalp's idea for easy testing.,"11/Jan/14 18:51;vijay2win@yahoo.com;We had this discussion in IRC, we need to test this before...

To clarify, 
(1) is same as in the description 
{quote}
I tried to fix this by defaulting isAlive=false in the constructor of EndpointState.
{quote}
(2) we need to recover the receiving node from the hang state (while writing to the socket), by restarting the connections...","12/Jan/14 06:17;kohlisankalp;""I tried to fix this by defaulting isAlive=false in the constructor of EndpointState.""
This won't fix the problem as endpoint can be alive. 
I tried my fix and it seems to work but someone else should also test it. ","13/Jan/14 15:17;brandon.williams;This patch solves the issue outlined in the description for me.  I created a firewall between two nodes and they do mark each other as dead, despite being able to see each other through a third node.  What doesn't work though, if is the partition is temporary and heals, the nodes never mark each other up, even though the connection has been (re)established as in Vijay's point 2).  However, that is a separate problem that has always existed, so let's move that to another ticket.  Committed Sankalp's patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"TTL histogram compactions not triggered at high ""Estimated droppable tombstones"" rate",CASSANDRA-6563,12688107,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,pauloricardomg,cburroughs,cburroughs,09/Jan/14 17:30,12/Mar/19 14:17,13/Mar/19 22:29,27/May/14 08:53,2.0.9,,,Local/Compaction,,,1,,,,,"I have several column families in a largish cluster where virtually all columns are written with a (usually the same) TTL.  My understanding of CASSANDRA-3442 is that sstables that have a high ( > 20%) estimated percentage of droppable tombstones should be individually compacted.  This does not appear to be occurring with size tired compaction.

Example from one node:

{noformat}
$ ll /data/sstables/data/ks/Cf/*Data.db
-rw-rw-r-- 31 cassandra cassandra 26651211757 Nov 26 22:59 /data/sstables/data/ks/Cf/ks-Cf-ic-295562-Data.db
-rw-rw-r-- 31 cassandra cassandra  6272641818 Nov 27 02:51 /data/sstables/data/ks/Cf/ks-Cf-ic-296121-Data.db
-rw-rw-r-- 31 cassandra cassandra  1814691996 Dec  4 21:50 /data/sstables/data/ks/Cf/ks-Cf-ic-320449-Data.db
-rw-rw-r-- 30 cassandra cassandra 10909061157 Dec 11 17:31 /data/sstables/data/ks/Cf/ks-Cf-ic-340318-Data.db
-rw-rw-r-- 29 cassandra cassandra   459508942 Dec 12 10:37 /data/sstables/data/ks/Cf/ks-Cf-ic-342259-Data.db
-rw-rw-r--  1 cassandra cassandra      336908 Dec 12 12:03 /data/sstables/data/ks/Cf/ks-Cf-ic-342307-Data.db
-rw-rw-r--  1 cassandra cassandra     2063935 Dec 12 12:03 /data/sstables/data/ks/Cf/ks-Cf-ic-342309-Data.db
-rw-rw-r--  1 cassandra cassandra         409 Dec 12 12:03 /data/sstables/data/ks/Cf/ks-Cf-ic-342314-Data.db
-rw-rw-r--  1 cassandra cassandra    31180007 Dec 12 12:03 /data/sstables/data/ks/Cf/ks-Cf-ic-342319-Data.db
-rw-rw-r--  1 cassandra cassandra     2398345 Dec 12 12:03 /data/sstables/data/ks/Cf/ks-Cf-ic-342322-Data.db
-rw-rw-r--  1 cassandra cassandra       21095 Dec 12 12:03 /data/sstables/data/ks/Cf/ks-Cf-ic-342331-Data.db
-rw-rw-r--  1 cassandra cassandra       81454 Dec 12 12:03 /data/sstables/data/ks/Cf/ks-Cf-ic-342335-Data.db
-rw-rw-r--  1 cassandra cassandra     1063718 Dec 12 12:03 /data/sstables/data/ks/Cf/ks-Cf-ic-342339-Data.db
-rw-rw-r--  1 cassandra cassandra      127004 Dec 12 12:03 /data/sstables/data/ks/Cf/ks-Cf-ic-342344-Data.db
-rw-rw-r--  1 cassandra cassandra      146785 Dec 12 12:03 /data/sstables/data/ks/Cf/ks-Cf-ic-342346-Data.db
-rw-rw-r--  1 cassandra cassandra      697338 Dec 12 12:03 /data/sstables/data/ks/Cf/ks-Cf-ic-342351-Data.db
-rw-rw-r--  1 cassandra cassandra     3921428 Dec 12 12:03 /data/sstables/data/ks/Cf/ks-Cf-ic-342367-Data.db
-rw-rw-r--  1 cassandra cassandra      240332 Dec 12 12:03 /data/sstables/data/ks/Cf/ks-Cf-ic-342370-Data.db
-rw-rw-r--  1 cassandra cassandra       45669 Dec 12 12:03 /data/sstables/data/ks/Cf/ks-Cf-ic-342374-Data.db
-rw-rw-r--  1 cassandra cassandra    53127549 Dec 12 12:03 /data/sstables/data/ks/Cf/ks-Cf-ic-342375-Data.db
-rw-rw-r-- 16 cassandra cassandra 12466853166 Dec 25 22:40 /data/sstables/data/ks/Cf/ks-Cf-ic-396473-Data.db
-rw-rw-r-- 12 cassandra cassandra  3903237198 Dec 29 19:42 /data/sstables/data/ks/Cf/ks-Cf-ic-408926-Data.db
-rw-rw-r--  7 cassandra cassandra  3692260987 Jan  3 08:25 /data/sstables/data/ks/Cf/ks-Cf-ic-427733-Data.db
-rw-rw-r--  4 cassandra cassandra  3971403602 Jan  6 20:50 /data/sstables/data/ks/Cf/ks-Cf-ic-437537-Data.db
-rw-rw-r--  3 cassandra cassandra  1007832224 Jan  7 15:19 /data/sstables/data/ks/Cf/ks-Cf-ic-440331-Data.db
-rw-rw-r--  2 cassandra cassandra   896132537 Jan  8 11:05 /data/sstables/data/ks/Cf/ks-Cf-ic-447740-Data.db
-rw-rw-r--  1 cassandra cassandra   963039096 Jan  9 04:59 /data/sstables/data/ks/Cf/ks-Cf-ic-449425-Data.db
-rw-rw-r--  1 cassandra cassandra   232168351 Jan  9 10:14 /data/sstables/data/ks/Cf/ks-Cf-ic-450287-Data.db
-rw-rw-r--  1 cassandra cassandra    73126319 Jan  9 11:28 /data/sstables/data/ks/Cf/ks-Cf-ic-450307-Data.db
-rw-rw-r--  1 cassandra cassandra    40921916 Jan  9 12:08 /data/sstables/data/ks/Cf/ks-Cf-ic-450336-Data.db
-rw-rw-r--  1 cassandra cassandra    60881193 Jan  9 12:23 /data/sstables/data/ks/Cf/ks-Cf-ic-450341-Data.db
-rw-rw-r--  1 cassandra cassandra        4746 Jan  9 12:23 /data/sstables/data/ks/Cf/ks-Cf-ic-450350-Data.db
-rw-rw-r--  1 cassandra cassandra        5769 Jan  9 12:23 /data/sstables/data/ks/Cf/ks-Cf-ic-450352-Data.db
{noformat}

{noformat}
295562: Estimated droppable tombstones: 0.899035828535183
296121: Estimated droppable tombstones: 0.9135080937806197
320449: Estimated droppable tombstones: 0.8916766879896414
{noformat}

I've checked in on this example node several times and compactionstats has not shown any other activity that would be blocking the tombstone based compaction.  The TTL is in the 15-20 day range so an sstable from November should have had ample opportunities by January.",1.2.12ish,,,,,,,,,,,,,,,,,,,,,,,16/May/14 20:09;pauloricardomg;1.2.16-CASSANDRA-6563-v2.txt;https://issues.apache.org/jira/secure/attachment/12645316/1.2.16-CASSANDRA-6563-v2.txt,21/May/14 20:03;pauloricardomg;1.2.16-CASSANDRA-6563-v3.txt;https://issues.apache.org/jira/secure/attachment/12646087/1.2.16-CASSANDRA-6563-v3.txt,08/May/14 14:46;pauloricardomg;1.2.16-CASSANDRA-6563.txt;https://issues.apache.org/jira/secure/attachment/12643957/1.2.16-CASSANDRA-6563.txt,22/May/14 23:09;pauloricardomg;2.0-CASSANDRA-6563-v3.txt;https://issues.apache.org/jira/secure/attachment/12646410/2.0-CASSANDRA-6563-v3.txt,08/May/14 15:50;pauloricardomg;2.0.7-CASSANDRA-6563.txt;https://issues.apache.org/jira/secure/attachment/12643961/2.0.7-CASSANDRA-6563.txt,16/May/14 19:47;pauloricardomg;patch-v1-iostat.png;https://issues.apache.org/jira/secure/attachment/12645312/patch-v1-iostat.png,16/May/14 15:30;pauloricardomg;patch-v1-range1.png;https://issues.apache.org/jira/secure/attachment/12645241/patch-v1-range1.png,16/May/14 15:30;pauloricardomg;patch-v2-range3.png;https://issues.apache.org/jira/secure/attachment/12645240/patch-v2-range3.png,08/May/14 17:33;pauloricardomg;patched-droppadble-ratio.png;https://issues.apache.org/jira/secure/attachment/12643978/patched-droppadble-ratio.png,08/May/14 17:33;pauloricardomg;patched-storage-load.png;https://issues.apache.org/jira/secure/attachment/12643977/patched-storage-load.png,09/May/14 19:19;pauloricardomg;patched1-compacted-bytes.png;https://issues.apache.org/jira/secure/attachment/12644170/patched1-compacted-bytes.png,09/May/14 19:19;pauloricardomg;patched2-compacted-bytes.png;https://issues.apache.org/jira/secure/attachment/12644168/patched2-compacted-bytes.png,08/May/14 17:33;pauloricardomg;unpatched-droppable-ratio.png;https://issues.apache.org/jira/secure/attachment/12643979/unpatched-droppable-ratio.png,08/May/14 17:33;pauloricardomg;unpatched-storage-load.png;https://issues.apache.org/jira/secure/attachment/12643976/unpatched-storage-load.png,09/May/14 19:19;pauloricardomg;unpatched1-compacted-bytes.png;https://issues.apache.org/jira/secure/attachment/12644169/unpatched1-compacted-bytes.png,09/May/14 19:19;pauloricardomg;unpatched2-compacted-bytes.png;https://issues.apache.org/jira/secure/attachment/12644167/unpatched2-compacted-bytes.png,,,,16.0,,,,,,,,,,,,,,,,,,,2014-05-08 14:52:07.395,,,no_permission,,,,,,,,,,,,367113,,,Wed Aug 20 19:20:09 UTC 2014,,,,,,0|i1ra33:,367423,1.2.0 beta 2,,,,,,,krummas,krummas,,,,,,,,,,10/Jan/14 16:29;cburroughs;I think this may actually be CASSANDRA-6568,"08/May/14 14:52;pauloricardomg;I've also hit this bug and instrumented the code to identify what was happening. Below is a detailed explanation of the issues that lead to this:

* CASSANDRA-3442 introduced the automatic tombstone removal feature, which was one of the main new features of C* 1.2 (http://www.datastax.com/dev/blog/tombstone-removal-improvement-in-1-2).

* In CASSANDRA-4022 the hints column family entered a compaction loop due to the single sstable compaction introduced by CASSANDRA-3442 that was not clearing tombstones because compacted rows with a lower timestamp were present in other sstables. So, the same SSTable was being compated over and over due to a large tombstone ratio that never changed.
A new heuristic was introduced to estimate the number of keys of the candidate SSTable that overlaps with other SSTables to prevent CASSANDRA-4022. However, this heuristic uses the token range to estimate the key overlap, which can be an accurate estimation if an OrderedPartitioner is used, but not for a RandomPartitioner, since most SSTables will contain the whole node's range. The result is that this heuristic not only prevents CASSANDRA-4022 from happening, but in my opinion it also prevents any tombstone compaction to happen at all, since most of the times the token ranges of all SSTables overlap. Jonathan Ellis even noted in the ticket discussion, ""I'm worried that with SizeTiered compaction we'll have overlap in a lot of cases where we could still compact if we looked closer"", but this worry was not taken further and the fix was integrated.

* Even with the previous fix, the compaction bug reappeared in CASSANDRA-4781. Sylvain Lebresne noted that even with the estimation ""we could always end up in a case where the estimate thinks there is enough droppable tombstones, but in practice all the droppable tombstones are in overlapping ranges"", and suggested to skip the worthDroppingTombstone check for SStables that were compacted before tombstone_compaction_interval seconds. The estimation was update because it contained a minor bug and the check for ""tombstone_compaction_interval"" was added.

So, after this change the code has pretty much been untouched since 1.2.0 and hasn't caused any problems since then, in my opinion because the tombstone compaction was never being triggered because of the conservative estimation, as illustrated in this JIRA ticket and the following mailing list threads:

* https://www.mail-archive.com/user@cassandra.apache.org/msg29979.html
* http://www.mail-archive.com/user@cassandra.apache.org/msg36144.html
* https://www.mail-archive.com/user@cassandra.apache.org/msg31760.html
* https://www.mail-archive.com/user@cassandra.apache.org/msg35793.html

In my opinion, the tombstone_compaction_interval check is sufficient to prevent CASSANDRA-4022 and CASSANDRA-4781, since it will give time for rows to be merged and the tombstone compaction to execute succesfully, reducing the droppable tombstone ratio and avoiding the compaction loop. So, I propose removing altogether the heuristic to estimate the key overlap in order to decide if a sstable should be tombstone compacted, keeping only the droppable tombstone treshold and tombstone_compaction_interval to decide that.

I'm attaching the patch for 1.2, which I'm testing in one of our production servers, and will provide a patch for 2.0.X soon. 

We already gained about 10% disk space due to tombstone compactions that were not being triggered before the patch. I will post the graphs soon with the space savings and decrease in droppable tombstone ratios. So far we haven't noticed any compaction loop.","08/May/14 15:50;pauloricardomg;Attached patch for cassandra-2.0.7.

Removed check for ""CompactionController.getFullyExpiredSSTables(cfs, Collections.singleton(sstable), overlaps, gcBefore).size() > 0"" (added in CASSANDRA-5228), because it is no longer necessary, since it is sufficient for the droppable tombstone ratio to be above the treshold for the tombstone compaction to be triggered.",08/May/14 15:59;pauloricardomg;Related issue: CASSANDRA-6654,"08/May/14 17:46;pauloricardomg;The following graphs compare the droppable tombstone ratio vs storage load of an unpatched vs patched node until about 12 hours after the patch was applied. These are both for LCS and STCS CFs, so the bug is definitely affecting both, not only STCS.

*Droppable Tombstone Ratio (unpatched vs patched)*

!unpatched-droppable-ratio.png|align=left, height=""50%, width=""50%""! !patched-droppadble-ratio.png|align=right, height=""50%, width=""50%""!

*Live disk space used (unpatched vs patched)*

!unpatched-storage-load.png|align=left, height=""50%, width=""50%""! !patched-storage-load.png|align=right, height=""50%, width=""50%""!","09/May/14 08:45;krummas;have to say I don't really feel comfortable dropping these checks, we could start doing alot of extra unnecessary IO

A better solution would be to get a more accurate estimate on how much the sstables overlap, (CASSANDRA-6474)

What we could do now (in 2.0) is perhaps loosen checks a bit, for example, we should probably only check for overlap in sstables which contain data that is older than the data in one we want to compact, since those are the ones that can block dropping the tombstones.","09/May/14 19:37;pauloricardomg;You're right, dropping the checks increases necessary I/O but also unnecessary I/O. I made a comparison of the compacted bytes for 2 ranges (with and without patch). It's possible to see a huge increase of useful compactions, but also a relevant increase of useless compacted bytes (even though the compactions haven't stabilized yet).

Subtitle:

* compact+: compacted bytes from useful compactions (compaction ratio < 100%)
* tombstone+: compacted bytes from useful tombstone compactions (compaction ratio < 100%)
* compact-: compacted bytes from useless compactions (compaction ratio = 100%)
* tombstone-: compacted bytes from useless compactions (compaction ratio = 100%)

*Unpatched vs Patched compacted bytes* (range 1)

!unpatched1-compacted-bytes.png|align=left, height=""50%, width=""50%""! !patched1-compacted-bytes.png|align=right, height=""50%, width=""50%""!

*Unpatched vs Patched compacted bytes* (range 2)

!unpatched2-compacted-bytes.png|align=left, height=""50%, width=""50%""! !patched2-compacted-bytes.png|align=right, height=""50%, width=""50%""!

What we want here is to increase the number of useful tombstone compactions without increasing the number of useless ones. I will implement a new patch that only takes older sstables into account to check for overlap and compare the results with the previous patch to see if we can achieve good results.","16/May/14 20:29;pauloricardomg;Below I will present some live cluster analysis about 10 days after deploying the original patch that entirely removes the check for range overlap in worthDroppingTombstones(). 

*Analysis Description*

In our dataset we use both LCS and STCS, but most of the CFs are STCS. A significant portion of our dataset is comprised of append-only TTL-ed data, so a good match for tombstone compaction. Most of our large CFs with high droppable tombstone ratio use STCS, but there are a few that use LCS that also benefited from the patch. 

I deployed the patch in 2 different ranges with similar results. The metrics were collected between 1st of May and 16 of May, the nodes were patched on the 7th of May. Used cassandra version was 1.2.16.
  
In the analysis I compare the total space used (Cassandra Load), tombstone Ratio, disk utilization (system disk xvbd util), total bytes compacted and system load (linux cpu). For the last three metrics I also calculate the integral of the metric to make it easier to compare the total amount during the period.

*Analysis*

Graphs: https://issues.apache.org/jira/secure/attachment/12645241/patch-v1-range1.png

Each graph compares the metrics of the patched node with it's previous neighbor and next neighbor, no VNODES is used. So, the first row in the figure is node N-1, the second row is node N (the patched node, marked with asterisk), and the third row is node N+1.

* *Cassandra load*: In the patched node, it's possible to see a sudden decrease of 7% of disk space when the patch was applied, due to the execution of single SSTable compactions. The growth rate of disk usage is also decreased after the patch, since tombstone are cleared more often. In the whole period, there was a 1.2% disk space increase in the patched node, against about 10% growth on the unpatched nodes.

* *Tombstone ratio*: After the patch is applied, it's possible to see a decrease in the droppable tombstone ratio, that revolves around the default level of 20% after that. The droppable tombstone ratio of unpatched nodes remains high for most CFs, what indicates that tombstone compactions are not being triggered at all.

* *Disk utilization*: it's not possible to detect any change in the disk utilization pattern after the patch is applied, what might indicate the I/O is not affected by the patch, at least for our mixed dataset. I double checked the IOPS graph for the period and there was not even a slight sign of change in the I/O pattern after the patch was applied. (https://issues.apache.org/jira/secure/attachment/12645312/patch-v1-iostat.png)

* *Total Bytes compacted*: The number of compacted bytes in the patched node was about 17% higher in the period. About 7% due to the initial tombstones that were cleared and more 7% due to cleared tombstones after the patch was applied (the difference between the 2 nodes sizes). The remaining 3% can be attributed to unnecessary compactions + normal variations because of different node ranges.

* *System CPU Load*: Was not affected by the patch.

*Alternative Patch*

I implemented another version of the patch (v2) as suggested by [~krummas], that instead of dropping the overlap check entirely, it only performs the check for SSTables containing rows with smaller timestamp than the candidate SSTable (https://issues.apache.org/jira/secure/attachment/12645316/1.2.16-CASSANDRA-6563-v2.txt). 

One week ago I deployed this alternative patch on 2 of our production nodes, and unfortunately loosing the checks did not achieve significant results. I added some debugging log to the code and what I verified is that despite reducing the number of sstables to compare with, even if only one SSTable has a column with an equal or lower timestamp to the candidate SSTable, the token ranges of these sstables always overlap because of the Random Partitioner. So, this supports the claim that even with loosen checks, the single-sstable tombstone compaction is almost never being triggered. At least on the use cases that could benefit from it.

The graphs for the alternative patch analysis can be found here: https://issues.apache.org/jira/secure/attachment/12645240/patch-v2-range3.png","16/May/14 20:47;pauloricardomg;Based on the previous analysis I propose the following alternatives:

# Drop the sstable range overlap check and hope that the analysis can be generalized and I/O will not be significantly impacted by the dropping the check.
# Since the check almost never lets tombstone compactions run, we can drop the check add a new CF property: enable_tombstone_compactions: false (default), and at least allow admins to safely enable tombstone compaction for CFs they know is a good match without impacting I/O in other CFs.","20/May/14 12:01;krummas;[~pauloricardomg] I think making it configurable is the way to go, calling the config option something like ""aggressive_tombstone_compactions"" or something, with default off.","21/May/14 20:16;pauloricardomg;Please find the new version of the patch here: https://issues.apache.org/jira/secure/attachment/12646087/1.2.16-CASSANDRA-6563-v3.txt

Comments:
* I have initially implemented on 1.2-branch, since that's the version we're using currently. After it's reviewed and everything is fine I can port the patch 2.0-branch, since I don't think there'll be significant differences.
* I have called the property ""unchecked_tombstone_compaction"" (default: false), since aggressive_tombstone_compactions is too aggressive. :P
* I have added unit tests showing that tombstone compactions are not being properly executed (due to the check) when unchecked_tombstone_compaction is disabled, and are executed after the property is enabled, for both STCS and LCS.
* During the implementation, I found a minor bug in AbstractCompactionStrategy's constructor:
{code:java}
    protected AbstractCompactionStrategy(ColumnFamilyStore cfs, Map<String, String> options)
    {
        assert cfs != null;
        this.cfs = cfs;
        this.options = options;
    ...
{code}
The fact that AbstractCompactionStrategy.options has the same reference of CFMetadata.compactionStrategyOptions, means that ColumnFamilyStore.reload() does not reload the compaction strategy when a compaction strategy option changes, due to the following piece of code:
{code:java}
    private void maybeReloadCompactionStrategy()
    {
        // Check if there is a need for reloading
        if (metadata.compactionStrategyClass.equals(compactionStrategy.getClass()) 
            && metadata.compactionStrategyOptions.equals(compactionStrategy.options)) //metadata.compactionStrategyOptions == compactionStrategy.options, so compaction is never reloaded
            return;
{code}
I spotted this in my test, when I tried changing the value of ""unchecked_tombstone_compaction"" from false to true and calling ColumnFamilyStore.reload() was not reloading the compaction strategy. I don't know if ColumnFamilyStore.reload() is only called during tests, or also whenever the schema changes. In order to fix the bug, I made AbstractCompactionStrategy.options an ImmutableMap, so if CFMetadata.compactionStrategyOptions is updated, ColumnFamilyStore.maybeReloadCompactionStrategy will actually reload the compaction strategy:
{code:java}
    protected AbstractCompactionStrategy(ColumnFamilyStore cfs, Map<String, String> options)
    {
        assert cfs != null;
        this.cfs = cfs;
        this.options = ImmutableMap.copyOf(options);
    ...
{code}
Should I file a new issue for this fix?","22/May/14 18:02;krummas;bq. Should I file a new issue for this fix?
yes.

Thanks for the patch, could you rebase it on 2.0?","22/May/14 23:22;pauloricardomg;2.0 patch here: https://issues.apache.org/jira/secure/attachment/12646410/2.0-CASSANDRA-6563-v3.txt

Comments:
  * Due to the improvements to LCS in 2.0 I had to increase the TTL, sleep times and number of rows in testUncheckedTombstoneCompactionLeveledCompaction() to create an environment with overlapping-range sstables in multiple levels. Since this was mostly an ""educational"" test to show the benefits of the patch to LCS, and it was not totally deterministic, I decided to remove it in order not to burden the test suite. The main patch functionality is already well tested in testUncheckedTombstoneSizeTieredCompaction().","27/May/14 08:53;krummas;Committed as 367c741931c2a20eb2213650313dc238e8b0f3aa with added cqlsh support and documentation, also instead of Boolean.parseBoolean, I made sure the value is either 'true' or 'false'

thanks!","20/Aug/14 19:20;dhendry;I initially tried something similar to what was done here (adding the option to entirely skip the overlap check) but ran into the infinite compaction problem. Our cassandra nodes have a lot of data (4+TB/node) which churns quite quickly (< one month) and the simple sstable age based heuristic was burning IO without actually getting rid of expired data (largely because individual compactions take so long).

Based on how the CompactionController.shouldPurge() is implemented, which seems to be how the compaction process *actually* decides if data can be dropped, I stared running this patch in production: https://github.com/kikinteractive/cassandra/compare/cassandra-1.2 . Basically it checks the sampled keys agains the bloom filters of overlapping SSTables to estimate overlap instead of trying to calculate it based on tokens. 

So far it seems to have been highly effective. Every tombstone compaction which has happened so far has resulted in sstable size reduction of at least the specified threshold. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batchlog replays copy the entire batchlog table into the heap,CASSANDRA-6569,12688297,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,rbranson,rbranson,10/Jan/14 18:37,12/Mar/19 14:17,13/Mar/19 22:29,17/Jan/14 16:56,1.2.14,2.0.5,,,,,0,,,,,"The current batchlog replay path will read the entire batchlog table into the heap. This is pretty bad. This was compounded by CASSANDRA-5762, which caused the SELECT statement used by the batchlog replay to bring the entire row into memory instead of just the selected columns.",,,,,,,,,,,,,,,,,,,,,,,,13/Jan/14 00:57;iamaleksey;6569.txt;https://issues.apache.org/jira/secure/attachment/12622559/6569.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-10 18:43:37.929,,,no_permission,,,,,,,,,,,,367316,,,Fri Jan 17 16:56:31 UTC 2014,,,,,,0|i1rbbz:,367625,1.2.13,,,,,,,jbellis,jbellis,,,,,,,,,,"10/Jan/14 18:43;iamaleksey;FWIW, yesterday's quick patch: https://gist.github.com/iamaleksey/8346528",13/Jan/14 00:57;iamaleksey;Attaching a working patch (w/ a unit test).,13/Jan/14 15:31;rbranson;Looks like deleteBatch got switched off the CQL code-path. This seems inconsistent from the rest of this replay logic.,"13/Jan/14 15:32;rbranson;Also, would it be possible to get some INFO-level logging (like HH has) on batchlog replay while we're here?","13/Jan/14 16:11;iamaleksey;bq. Looks like deleteBatch got switched off the CQL code-path. This seems inconsistent from the rest of this replay logic.

Yes. It's in a 'hot path', and this took less code then keeping the prepared DELETE statement around. The two are equivalent though.","13/Jan/14 16:12;iamaleksey;bq. Also, would it be possible to get some INFO-level logging (like HH has) on batchlog replay while we're here?

Sure. What do you have in mind?",17/Jan/14 16:19;jbellis;+1,"17/Jan/14 16:56;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTable read meter sync not cancelled when reader is closed,CASSANDRA-6358,12679494,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thobbs,thobbs,thobbs,15/Nov/13 21:32,12/Mar/19 14:17,13/Mar/19 22:29,16/Nov/13 16:34,2.0.3,,,,,,0,,,,,We run a fixed-schedule task to sync the read meter for every SSTableReader periodically.  These tasks are not cancelled when the SSTR is closed.,,,,,,,,,,,,,,,,,,,,,,,,15/Nov/13 21:36;thobbs;0001-Cancel-SSTR-read-meter-syncer-on-close.patch;https://issues.apache.org/jira/secure/attachment/12614135/0001-Cancel-SSTR-read-meter-syncer-on-close.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-16 16:34:43.796,,,no_permission,,,,,,,,,,,,358854,,,Sat Nov 16 16:34:43 UTC 2013,,,,,,0|i1pv0n:,359144,2.0.2,,,,,,,jbellis,jbellis,,,2.0.2,,,,,,,15/Nov/13 21:36;thobbs;Attached patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-6358]) cancel the scheduled task when the SSTableReader is closed.,"16/Nov/13 16:34;jbellis;LGTM, committed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoSuchElementException in trunk,CASSANDRA-6671,12693798,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,benedict,thobbs,thobbs,07/Feb/14 01:25,12/Mar/19 14:17,13/Mar/19 22:29,07/Feb/14 02:29,2.1 beta1,,,,,,0,,,,,"When running {{cassandra-stress -o 10000000}} from {{cassandra-1.2}} against the latest trunk, I noticed this error in the system log:

{noformat}
ERROR 01:13:11 Exception in thread Thread[MutationStage:68,5,main]
java.lang.RuntimeException: java.util.NoSuchElementException
	at org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:2063) ~[main/:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_40]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_40]
	at java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_40]
Caused by: java.util.NoSuchElementException: null
	at java.util.concurrent.ConcurrentLinkedDeque.screenNullResult(ConcurrentLinkedDeque.java:812) ~[na:1.7.0_40]
	at java.util.concurrent.ConcurrentLinkedDeque.getLast(ConcurrentLinkedDeque.java:963) ~[na:1.7.0_40]
	at org.apache.cassandra.utils.concurrent.WaitQueue.signalAll(WaitQueue.java:122) ~[main/:na]
	at org.apache.cassandra.utils.concurrent.OpOrder$Group.unlink(OpOrder.java:254) ~[main/:na]
	at org.apache.cassandra.utils.concurrent.OpOrder$Group.finishOne(OpOrder.java:209) ~[main/:na]
	at org.apache.cassandra.db.commitlog.CommitLogSegment$Allocation.markWritten(CommitLogSegment.java:581) ~[main/:na]
	at org.apache.cassandra.db.commitlog.CommitLog.add(CommitLog.java:231) ~[main/:na]
	at org.apache.cassandra.db.commitlog.CommitLog.add(CommitLog.java:193) ~[main/:na]
	at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:349) ~[main/:na]
	at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:328) ~[main/:na]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:205) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy$7.runMayThrow(StorageProxy.java:997) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:2059) ~[main/:na]
	... 3 common frames omitted
{noformat}

I'm not sure what the implications are.",,,,,,,,,,,,,,,,,,,,,,,,07/Feb/14 02:04;benedict;tmp.patch;https://issues.apache.org/jira/secure/attachment/12627534/tmp.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-07 02:04:23.79,,,no_permission,,,,,,,,,,,,372307,,,Fri Feb 07 02:29:23 UTC 2014,,,,,,0|i1s5v3:,372611,,,,,,,,jbellis,jbellis,,,,,,,,,,"07/Feb/14 02:04;benedict;[~jbellis] I knew it was dangerous to replace NBQ with CLQ :-)

I will be dropping a major patch that would fix this within the next couple of days, however this may take some time to get merged. In the mean time I'm attaching a trivial fix: getLast() apparently isn't remotely concurrency safe, and will throw a NPE if the queue is empty. peekLast() is the correct method to use.

",07/Feb/14 02:29;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unwanted schema pull while upgrading nodes from 1.2 to 2.0,CASSANDRA-6678,12693890,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sbtourist,sbtourist,sbtourist,07/Feb/14 13:33,12/Mar/19 14:17,13/Mar/19 22:29,07/Feb/14 14:38,1.2.16,2.0.6,,,,,0,,,,,"While upgrading from 1.2 to 2.0, the 1.2 nodes are not supposed to pull schemas from upgraded 2.0 nodes to avoid conflicts.

This relies on network version checks between the two nodes, but there's a bit of a race between the Gossiper, which is activated first, and the MessagingService, which is activated after the Gossiper and handles network version exchange: if a 1.2 node Gossiper gets a gossip message from a newly 2.0 node *before* opening connections from the MessagingService, the version will still be 1.2, and the schema will be pulled from the new node.

A possible solution may be to have the Gossiper update the network version upon receiving the first gossip message of an upgraded node: thoughts?",,,,,,,,,,,,,,,,,,,,,,,,07/Feb/14 14:33;sbtourist;CASSANDRA-6678-1_2.patch;https://issues.apache.org/jira/secure/attachment/12627628/CASSANDRA-6678-1_2.patch,07/Feb/14 14:33;sbtourist;CASSANDRA-6678-2_0.patch;https://issues.apache.org/jira/secure/attachment/12627629/CASSANDRA-6678-2_0.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-02-07 13:48:27.582,,,no_permission,,,,,,,,,,,,372399,,,Fri Feb 07 15:53:05 UTC 2014,,,,,,0|i1s6fj:,372703,1.2.15,2.0.5,,,,,,brandon.williams,brandon.williams,,,,,,,,,,07/Feb/14 13:48;brandon.williams;Perhaps a MessagingService.knowsVersion check is what we need.,"07/Feb/14 14:08;sbtourist;Good point, way easier :)",07/Feb/14 14:33;sbtourist;Attached patches for 1.2 and 2.0.,07/Feb/14 14:38;brandon.williams;Committed.,07/Feb/14 15:53;pkolaczk;Is there any workaround that could be applied to version 2.0 *only* so it can join a non-patched 1.2 cluster?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid possible sstable overlaps with leveled compaction,CASSANDRA-6688,12694431,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,11/Feb/14 06:31,12/Mar/19 14:17,13/Mar/19 22:29,13/Feb/14 07:48,2.0.6,,,,,,0,lcs,,,,"Two cases where we can end up with overlapping sstables in the leveled manifest;

FIrst one is when we skip levels during compaction. Here we need to make sure we are not compacting in newLevel - 1 since if, for example, we are doing a L1 -> L2 compaction and then start a new L0 compaction where we decide to skip L1, we could have overlapping sstables in L2 when the compactions are done. This case is new in 2.0 since we check if we skip levels before the compaction starts.

Second case is where we try to include as many overlapping L0 sstables as possible, here we could add sstables that are not compacting, but overlap sstables that are.",,,,,,,,,,,,,,,,,,,,,,,,11/Feb/14 06:33;krummas;0001-6688.patch;https://issues.apache.org/jira/secure/attachment/12628154/0001-6688.patch,11/Feb/14 16:02;jbellis;6688-v2.txt;https://issues.apache.org/jira/secure/attachment/12628244/6688-v2.txt,12/Feb/14 08:03;krummas;6688-v3.patch;https://issues.apache.org/jira/secure/attachment/12628451/6688-v3.patch,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-02-11 15:41:56.82,,,no_permission,,,,,,,,,,,,372939,,,Thu Feb 13 07:48:30 UTC 2014,,,,,,0|i1s9q7:,373241,,,,,,,,jbellis,jbellis,,,2.0 beta 1,,,,,,,"11/Feb/14 15:41;jbellis;v2 attached that generalizes the skipLevel approach a bit, but I think we have a problem: what we really want to know is (as I edited the comment) whether sstables being compacted *into* newLevel might overlap, but I don't think we actually track that anywhere.  (Maybe we could walk the in-progress CompactionTasks but that is tricky since once we're out of getCompactionCandidates we don't have {{synchronized}} to keep things race-free.)  So, if we had two compactions skipping levels, we could still get into trouble if we're just checking newlevel-1.","11/Feb/14 15:42;jbellis;bq. Second case is where we try to include as many overlapping L0 sstables as possible, here we could add sstables that are not compacting, but overlap sstables that are.

This affects 1.2 as well right?","11/Feb/14 16:01;krummas;bq. This affects 1.2 as well right?
yeah, looks like it

Should we perhaps stop skipping levels? Can't really see it happening more than once in the lifetime of a cluster, right after bootstrap (or after dropping all the sstables to L0), first compaction might skip L1.",11/Feb/14 16:03;jbellis;that's definitely the simplest solution,12/Feb/14 08:03;krummas;1.2 is not affected since it repairs the level right after it adds the compacted files,12/Feb/14 08:03;krummas;v3 attached that removes skipLevels,12/Feb/14 15:53;jbellis;+1,13/Feb/14 07:48;krummas;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming session failures during node replace of same address,CASSANDRA-6622,12691296,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,ravilr,ravilr,26/Jan/14 22:25,12/Mar/19 14:17,13/Mar/19 22:29,11/Feb/14 23:44,1.2.16,2.0.6,,,,,0,,,,,"When using replace_address, Gossiper ApplicationState is set to hibernate, which is a down state. We are seeing that the peer nodes are seeing streaming plan request even before the Gossiper on them marks the replacing node as dead. As a result, streaming on peer nodes convicts the replacing node by closing the stream handler.  
I think, making the StorageService thread on the replacing node, sleep for BROADCAST_INTERVAL before bootstrapping, would avoid this scenario.


Relevant logs from peer node (see that the Gossiper on peer node mark the replacing node as down, 2 secs after  the streaming init request):

{noformat}
 INFO [STREAM-INIT-/x.x.x.x:46436] 2014-01-26 20:42:24,388 StreamResultFuture.java (line 116) [Stream #5c6cd940-86ca-11e3-90a0-411b913c0e88] Received streaming plan for Bootstrap
....
 INFO [GossipTasks:1] 2014-01-26 20:42:25,240 StreamResultFuture.java (line 181) [Stream #5c6cd940-86ca-11e3-90a0-411b913c0e88] Session with /x.x.x.x is complete
 WARN [GossipTasks:1] 2014-01-26 20:42:25,240 StreamResultFuture.java (line 210) [Stream #5c6cd940-86ca-11e3-90a0-411b913c0e88] Stream failed
 INFO [GossipStage:1] 2014-01-26 20:42:25,242 Gossiper.java (line 850) InetAddress /x.x.x.x is now DOWN
ERROR [STREAM-IN-/x.x.x.x] 2014-01-26 20:42:25,766 StreamSession.java (line 410) [Stream #5c6cd940-86ca-11e3-90a0-411b913c0e88] Streaming error occurred
java.lang.RuntimeException: Outgoing stream handler has been closed
        at org.apache.cassandra.streaming.ConnectionHandler.sendMessage(ConnectionHandler.java:175)
        at org.apache.cassandra.streaming.StreamSession.prepare(StreamSession.java:436)
        at org.apache.cassandra.streaming.StreamSession.messageReceived(StreamSession.java:358)
        at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:293)
        at java.lang.Thread.run(Thread.java:722)
 INFO [STREAM-IN-/x.x.x.x] 2014-01-26 20:42:25,768 StreamResultFuture.java (line 181) [Stream #5c6cd940-86ca-11e3-90a0-411b913c0e88] Session with /x.x.x.x is complete
 WARN [STREAM-IN-/x.x.x.x] 2014-01-26 20:42:25,768 StreamResultFuture.java (line 210) [Stream #5c6cd940-86ca-11e3-90a0-411b913c0e88] Stream failed
{noformat}","RHEL6, cassandra-2.0.4",,,,,,,,,,,,,,,,,,,,,,,01/Feb/14 20:40;brandon.williams;0001-don-t-signal-restart-of-dead-states.txt;https://issues.apache.org/jira/secure/attachment/12626494/0001-don-t-signal-restart-of-dead-states.txt,27/Jan/14 03:10;ravilr;6622-2.0.txt;https://issues.apache.org/jira/secure/attachment/12625312/6622-2.0.txt,11/Feb/14 22:48;brandon.williams;6622-v2.txt;https://issues.apache.org/jira/secure/attachment/12628354/6622-v2.txt,08/Feb/14 20:09;ravilr;6622_logs.tgz;https://issues.apache.org/jira/secure/attachment/12627830/6622_logs.tgz,03/Feb/14 20:04;ravilr;logs.tgz;https://issues.apache.org/jira/secure/attachment/12626729/logs.tgz,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2014-01-27 01:17:47.888,,,no_permission,,,,,,,,,,,,370041,,,Tue Feb 11 23:44:46 UTC 2014,,,,,,0|i1rs07:,370343,2.0.4,,,,,,,thobbs,thobbs,,,2.0.2,,,,,,,"27/Jan/14 01:17;brandon.williams;The failure detector shouldn't be tracking any nodes with a dead state (since they're already dead.)  It sounds like you're doing a replace on the same IP address, before the failure detector has marked the original node being replaced down.","27/Jan/14 03:09;ravilr;yes, i was replacing the node with same ip address, which was dead before.  Despite being dead before, since we set the state to hibernate and due to the generation change of the replacing node at startup, it gets marked down again.  StorageService thread already sleeps for broadcast_interval, if the replacing address is not same as broadcast address. the attached patch sleeps for same address also.","27/Jan/14 14:18;brandon.williams;If the node is already marked dead, the FailureDetector isn't going to convict() again just because it receives a dead state like hibernate.  It will call onDead again for subscribers, but StreamSession doesn't care about that.  What it does care about, however, is onRestart being called since there was a generation change, and that will fail the session.

That said, certainly delaying the stream until gossip has propagated should solve the issue, though I'm not sure streaming should be failing based on gossip/FD events (/cc [~yukim]).  However, instead of sleeping for BROADCAST_INTERVAL we can save half the time and sleep for RING_DELAY, since if gossip hasn't propagated fully by then there are bigger problems.  WDYT [~thobbs]?","31/Jan/14 23:43;thobbs;Your explanation seems reasonable, and after some quick testing the patch does seem to resolve the problem.  I agree that RING_DELAY should be fine.  I am interested to hear what [~yukim] has to say about the stream breaking, though.","31/Jan/14 23:54;yukim;Whether to fail streaming session on gossip/FD events is discussed in CASSANDRA-3569.

I'm fine with failing at gossip DOWN not to push too much data to, say, ""choked"" node. But thinking it again, I wonder about the frequency the case actually happens...",01/Feb/14 00:19;brandon.williams;Maybe our best bet here is to not call onRestart for dead states.,"01/Feb/14 20:40;brandon.williams;bq. Maybe our best bet here is to not call onRestart for dead states.

Let's give it shot.  [~ravilr] can you see if this patch solves the problem for you?","03/Feb/14 05:50;ravilr;bq. Maybe our best bet here is to not call onRestart for dead states.

Seeing the same error in original description, with the above patch on cassandra-2.0.",03/Feb/14 15:16;brandon.williams;Can you attach logs from both the replacing node and the node that is failing the stream session?,"03/Feb/14 20:11;ravilr;In attached logs, .72 was the replacing node, .73 is where the streaming session failed. I had trace logging turned on in .73 for org.apache.cassandra.gms.  Looks like, it is FailureDetector is convicting.  I have to mention that this was with '0001-don-t-signal-restart-of-dead-states.txt' applied on cassandra-2.0.4.","05/Feb/14 20:03;ravilr;I'm seeing FailureDetector notifying listeners every second invoked through GossiperTask's doStatusCheck(). Tested sleeping for RING_DELAY (instead of BROADCAST_INTERVAL) before bootstrap, works without any stream session closure. ","06/Feb/14 20:32;brandon.williams;Can you try the patch from CASSANDRA-6658?  I think that's the problem here, the incorrect FD signal is sent with an extremely high phi value, tripping StreamSession's convict with greater than two times the normal phi.","07/Feb/14 04:38;ravilr;bq. Can you try the patch from CASSANDRA-6658?
Didn't help. What i'm seeing is, the other nodes in the ring take around 2-3 seconds for PHI on the replacing node to drop below convict threshold. But, they also receive the stream plan from the replacing node with in 2 seconds of starting of replacing node. ","07/Feb/14 15:06;brandon.williams;bq. What i'm seeing is, the other nodes in the ring take around 2-3 seconds for PHI on the replacing node to drop below convict threshold.

You mean rise above it, so the node is still being convicted?  Can you add new logs?  Maybe now it actually is the restart event, so trying that patch with 6658 might work.

The problem with just sleeping for RING_DELAY is that the reason we do that during bootstrap is to announce the range, but with replacement that shouldn't be needed.  If we sleep and it works we're papering over the real problem without understand what it is.","08/Feb/14 20:09;ravilr;bq. You mean rise above it, so the node is still being convicted? Can you add new logs? Maybe now it actually is the restart event, so trying that patch with 6658 might work.

Tried with 6658 patch and 0001-don-t-signal-restart-of-dead-states.txt applied on cassandra-2.0.5 tag. Still see the same thing, where FD convicts the streaming session. I'm attaching the logs(6622_logs.tgz).  This should be easily reproducible when replacing a dead node in a cluster with same ip address. the issue is, the peer nodes could take 1-3 seconds to see the previously down node (now replacing) to be up(to reset the PHI score of the down node). Since, the streaming request arrives before this reset happens, they could be convicted leading to stream close. So, i think  a couple of seconds sleep time for gossip to settle, before the bootstrap/streaming starts is what is needed?

1.) node x.x.x.72 was dead
2.) node x.x.x.80's FD keeps notifying its listener to convict as PHI for .72 > threshold, every minute.
3.) node x.x.x.72 is restarted with replace_address=x.x.x.72 at 18:56:27,806
4.) node x.x.x.72 : Gossip thread started at 18:56:33,308 after shadow gossip round
5.) node x.xx.72:  Starts stream request at 18:56:35,443
 INFO [main] 2014-02-08 18:56:35,405 StorageService.java (line 947) JOINING: Starting to bootstrap...
 INFO [main] 2014-02-08 18:56:35,443 StreamResultFuture.java (line 82) [Stream #bb897500-90f2-11e3-9d67-d5d417af8653] Executing streaming plan for Bootstrap
6.) node x.x.x.80 : still hasn't seen the gossip from .72 with new generation at 18:56:35,031
TRACE [GossipTasks:1] 2014-02-08 18:56:35,031 FailureDetector.java (line 229) PHI for /x.x.x.72 : 36700.042810594234
TRACE [GossipTasks:1] 2014-02-08 18:56:35,032 FailureDetector.java (line 233) notifying listeners that /x.x.x.72 is down
7.) node x.x.x.80 : got the stream request at 18:56:35,450
 INFO [STREAM-INIT-/x.x.x.72:47408] 2014-02-08 18:56:35,450 StreamResultFuture.java (line 116) [Stream #bb897500-90f2-11e3-9d67-d5d417af8653] Received streaming plan for Bootstrap
8.) node x.x.x.80: at 18:56:36,090, still hasn't reset the interval times for .72
TRACE [GossipTasks:1] 2014-02-08 18:56:36,090 FailureDetector.java (line 229) PHI for /x.x.x.72 : 36700.87918907657
TRACE [GossipTasks:1] 2014-02-08 18:56:36,090 FailureDetector.java (line 233) notifying listeners that /x.x.x.72 is down
9.) node x.x.x.80:  closes the stream session due to convict() notification:
 INFO [GossipTasks:1] 2014-02-08 18:56:36,090 StreamResultFuture.java (line 181) [Stream #bb897500-90f2-11e3-9d67-d5d417af8653] Session with /x.x.x.72 is complete
 WARN [GossipTasks:1] 2014-02-08 18:56:36,091 StreamResultFuture.java (line 210) [Stream #bb897500-90f2-11e3-9d67-d5d417af8653] Stream failed
10.) node x.x.x.80:  at 18:56:36,097,  Gossiper thread on x.x.x.80 clears the interval times for .72, thereby resetting the PHI.
DEBUG [GossipStage:1] 2014-02-08 18:56:36,097 Gossiper.java (line 790) Clearing interval times for /x.x.x.72 due to generation change
TRACE [GossipStage:1] 2014-02-08 18:56:36,097 FailureDetector.java (line 203) reporting /x.x.x.72
11.) node x.x.x.80:  PHI score for .72 at 18:56:37,094
TRACE [GossipTasks:1] 2014-02-08 18:56:37,094 FailureDetector.java (line 229) PHI for /x.x.x.72 : 0.06483452387313912


","11/Feb/14 16:28;brandon.williams;So the problem isn't any gossip event breaking the stream, it's that streaming is subscribed to the FD directly in order to wait  for double the phi, but this notifies every second regardless of up/down state and ends up racing the stream request before the hibernate state has reached the source node.  We won't be able to fix that without CASSANDRA-3569, so we'll add the RING_DELAY sleep.  v2 conditionally determines sleep length based on whether the IP is the same or not.",11/Feb/14 23:10;thobbs;+1,11/Feb/14 23:44;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit user types to the keyspace they are defined in,CASSANDRA-6643,12692427,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,rhatch,rhatch,31/Jan/14 01:29,12/Mar/19 14:17,13/Mar/19 22:29,16/May/14 13:57,2.1 rc1,,,Legacy/CQL,,,0,,,,,"I'm not 100% certain this is a bug.

The current syntax for ""alter type rename"" requires the keyspace on the old and new table name (if a keyspace is not active). So, to rename the type 'foo' to 'bar', you have to issue this statement:
ALTER TYPE ks.foo rename to ks.bar .

As a result, this syntax will also allow renaming the type into another existing keyspace, which updates the metadata in system.schema_usertypes.

I'm wondering if perhaps we can omit the second keyspace prefix and implicitly rename into the same keyspace.

To reproduce:
{noformat}
cqlsh> create keyspace user_types with replication = {'class':'SimpleStrategy', 'replication_factor':3} ;
cqlsh> create keyspace user_types2 with replication = {'class':'SimpleStrategy', 'replication_factor':3} ;
cqlsh> CREATE TYPE user_types.simple_type (user_number int);
cqlsh> alter type user_types.simple_type rename to user_types2.simple_type;
{noformat}

Renaming to another keyspace is also possible when a keyspace is active, like so:
{noformat}
cqlsh:user_types> alter type simple_type rename to user_types2.simple_type;
{noformat}","java version ""1.7.0_51""
cassandra from trunk, 4b54b8...",,,,,,,,,,,,,,,,,,,,,,,15/May/14 14:06;slebresne;6643.txt;https://issues.apache.org/jira/secure/attachment/12645016/6643.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-31 03:03:40.78,,,no_permission,,,,,,,,,,,,371022,,,Fri May 16 13:57:12 UTC 2014,,,,,,0|i1rxzz:,371327,2.1 rc3,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"31/Jan/14 01:30;rhatch;I'm working on dtests for user types, so I can add one for this scenario when we know what the behavior ought to be.","31/Jan/14 03:03;mishail;[~rhatch] It doesn't work if there is a table which uses the type, does it?","31/Jan/14 13:19;slebresne;Just to make sure we're on the same page, are you saying that we can rename to another keyspace but that you wonder if we shouldn't refuse it? Or that it's allowed by the syntax but is broken? Something else?

but in general, I'd be fine limiting type rename to within the same keyspace if it turns out there is a technical difficulty to make it work across keyspaces, but skimming quickly over the code suggest it might be just be working alright (or rather, it was the intent that it would work but I don't think I've tested it) and I don't see why we'd limit it if it works. But again, I might me missing your point.",31/Jan/14 14:37;jbellis;limiting rename to the same ks would simplify the security concerns imo,"31/Jan/14 17:22;rhatch;[~slebresne]
yeah, I can currently rename to another keyspace but think maybe we shouldn't allow it (or at least implicitly name into the same ks implicitly when no ks prefix is on the rename's 'destination').

The rename to another ks works even if a table in the 'old' keyspace depends on the type, so I do think as-is it would be possible to get the types/tables into a weird or at least confusing state.

This scenario shows we can make a table depend on a type then rename that type into another keyspace:
{noformat}
cqlsh> create keyspace user_types with replication = {'class':'SimpleStrategy', 'replication_factor':3} ;
cqlsh> create keyspace user_types2 with replication = {'class':'SimpleStrategy', 'replication_factor':3} ;
cqlsh> CREATE TYPE user_types.simple_type (user_number int);
cqlsh> CREATE TABLE user_types.simple_table (
   ... id uuid PRIMARY KEY,
   ... number simple_type
   ... );
cqlsh> alter type user_types.simple_type rename to user_types2.simple_type;
{noformat}

We do still enforce the correct relationship when trying to drop the type in use though, but now there's a type in one ks dependent on a table in another ks:
{noformat}
cqlsh> drop type user_types2.simple_type;
Bad Request: Cannot drop user type user_types2.simple_type as it is still used by table user_types.simple_table
{noformat}
","31/Jan/14 20:15;slebresne;Thinking a bit more about that, I think the right question is whether we want to allow a table to depend on a type in another keyspace. Currently, you can (it has been designed for it), and you don't need type renames for that, it's enough to simply refer to a type using it's fullly qualified name at table creation. Now, this does present a problem if a keyspace is dropped but contains types used in another keyspace. I suppose we'd need to check for that case and refuse the drop in that case. It's not done right now though. And that does complicate security rules a bit.

But maybe having tables reference types in another keyspace is not that useful and it's better to keep things simpler and just forbid it. The reason I left that option initially however was for the case where someone have a type he uses in multiple keyspace, to avoid having to re-declare it multiple times (re-declaring the type multiple time is probably not a huge deal in itself, but that will mean the types will be different which could end up being slightly annoying on the client driver side if you really want those per-keyspace types to be the same).

So anyway, I don't think I have an extremely strong opinion on that one. But if we do let table reference type in another keyspace, it feels relatively natural to me to let type rename change the keyspace. But if we don't (let table reference type in another keyspace), then I agree that it's natural to forbid type rename to change the keyspace.

bq. or at least implicitly name into the same ks implicitly when no ks prefix is on the rename's 'destination'

When a type name is not fully qualified, it refers to the current keyspace everywhere else, so I really don't want to make a special case for that one. Would be confusing for no much value added imo.",31/Jan/14 20:19;jbellis;I think one keyspace should be isolated from others.,"31/Jan/14 20:59;slebresne;You're right, that's probably simpler that way. And we can always change it later if it turns out it would be really more useful, while the contrary is harder.

I'll workup a patch to limit it to one keyspace.","14/Mar/14 16:42;slebresne;Actually, just to make sure we agree here before I go ahead and remove code, do we agree that this means user types can never be fully qualified and always implicitly refer to the keyspace of whatever query the name is part of? Cause contrarily to what I said just above, if we do so, I don't think we'll be able to change our mind later and allow referencing other keyspace type. I can leave with that ""limitation"" but just wanted to double check we're fine with that.","14/Mar/14 16:55;rhatch;bq. can never be fully qualified and always implicitly refer to the keyspace of whatever query the name is part of

Does that mean user types couldn't be created or added like so? (from the global keyspace):
{noformat}
cqlsh> CREATE TYPE user_types.simple_type (user_number int);
cqlsh> CREATE TABLE user_types.simple_table (
   ... id uuid PRIMARY KEY,
   ... number simple_type
   ... );
{noformat}","14/Mar/14 17:22;slebresne;Right, right, we do need to allow fully qualified type names, my bad. But now that's means:
{noformat}
USE ks1;
CREATE TYPE mytype (f int);

USE ks2;
CREATE TABLE ks1.mytable (k mytype PRIMARY KEY);
{noformat}
would be invalid because mytype would refer to ks2 (since it's not fully qualified).
","25/Mar/14 16:31;slebresne;So I just realized that in the current code, a non qualified user type name refers to the keyspace of the statement it's in, not the keyspace logged into. Meaning that in
{noformat}
USE ks2;
CREATE TABLE ks1.mytable (k mytype PRIMARY KEY);
{noformat}
{{mytype}} refers to {{ks1}}, not {{ks2}}. Honestly, that's kind of an oversight, as imo that's somewhat inconsistent with how unqualified table name work (since they do refer to the logged keyspace). That being said, before I go into changing this all over the place (turns out it requires a slight refactoring), just wanted to check that we agree it's better to change so that non-qualified type name do refer to the keyspace logged in, not the ""statement"" keyspace?","26/Mar/14 00:49;rhatch;I don't have a strong opinion, but I think it seems reasonable when creating a table with user type to re-use the statement keyspace from the table (falling back to logged in keyspace if table has no prefix), since the tables/types will belong to the same keyspace anyway. But I think if the type keyspace is explicitly given we should use that, and not do anything surprising like switching it. So I guess I vote for leaving it the same.

So in the above scenario:
{noformat}
USE ks2;
CREATE TABLE ks1.mytable (k mytype PRIMARY KEY);
{noformat}
Effectively becomes:
{noformat}
USE ks2;
CREATE TABLE ks1.mytable (k [ks1].mytype PRIMARY KEY);
{noformat}
But if you try this you should get an error (differing table keyspace and type keyspace):
{noformat}
USE ks2;
CREATE TABLE ks1.mytable (k ks2.mytype PRIMARY KEY);
{noformat}

I assume this wouldn't have any impact on the CREATE TYPE syntax, so I'm assuming that would still be consistent with how CREATE TABLE figures out the keyspace.","26/Mar/14 09:05;slebresne;First attaching a patch to only allow referencing user types in the keyspace they are defined in.

Still not entirely convinced that the behavior described above is the best choice. It's not illogical, but I wonder if saying ""non qualified type and table names always refer to the logged keyspace"" wouldn't be simpler that ""non qualified table name refers to the logged keyspace, and so do type name, except when they are inside a statement in which case they refer to the statement keyspace"".","26/Mar/14 13:26;jbellis;apologies if this was covered above, but when is logged keyspace different from statement keyspace?","26/Mar/14 13:32;slebresne;In
{noformat}
USE ks2;
CREATE TABLE ks1.mytable (k mytype PRIMARY KEY);
{noformat}
The keyspace for mytype (where we're look for mytype to be precise) could {{ks2}} (the logged keyspace) or {{ks1}} (the statement keyspace). Of course this only matter if you mix some fully-qualified names and some non-fully-qualifed ones, which is probably a bad idea in the first place, but well, still need to handle it. ","24/Apr/14 13:56;slebresne;Forgot to it the 'patch available' button, so just did, but still would be nice to get confirmation regarding what's above. Don't want to end up committing a behavior we're not happy with, even if that's somewhat a detail.","08/May/14 21:22;jbellis;bq. contrarily to what I said just above, if we do so, I don't think we'll be able to change our mind later and allow referencing other keyspace type

Why is that?

bq. I wonder if saying ""non qualified type and table names always refer to the logged keyspace"" wouldn't be simpler 

I think ""type names always refer to the statement keyspace"" makes the most sense; if we don't allow referencing types in a different keyspace, then referring to the logged keyspace is an error.

If we want to help people out, we could raise an error if there is ambiguity and require explicitly specifying ks1.mytype if ks1 and ks2 both have a mytype defined.","14/May/14 08:45;slebresne;bq. Why is that?

 I don't remember :). But I think I confused myself a bit early on, so it should be ok.

bq. I think ""type names always refer to the statement keyspace"" makes the most sense

Yeah, I think I'm warming up to that too. So anyway, let's go with this then, patch is ready for review (bumping issue to major just to make sure we don't accidentally release 2.1 without that change since it would be breaking to change it post-release). ",15/May/14 14:06;slebresne;Rebased patch attached.,15/May/14 14:18;iamaleksey;+1,"16/May/14 13:57;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
upgradesstables causes NPE for secondary indexes without an underlying column family,CASSANDRA-6645,12692827,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sbtourist,sbtourist,sbtourist,03/Feb/14 11:02,12/Mar/19 14:17,13/Mar/19 22:29,06/Feb/14 16:19,1.2.16,2.0.6,,Feature/2i Index,,,0,,,,,"SecondaryIndex#getIndexCfs is allowed to return null by contract, if the index is not backed by a column family, but this causes an NPE as StorageService#getValidColumnFamilies and StorageService#upgradeSSTables do not check for null values.",,,,,,,,,,,,,,,,,,,,,,,,05/Feb/14 16:54;sbtourist;CASSANDRA-6645-1_2.patch;https://issues.apache.org/jira/secure/attachment/12627152/CASSANDRA-6645-1_2.patch,03/Feb/14 11:26;sbtourist;CASSANDRA-6645.patch;https://issues.apache.org/jira/secure/attachment/12626636/CASSANDRA-6645.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-02-04 05:44:21.731,,,no_permission,,,,,,,,,,,,371413,,,Thu Feb 06 16:19:19 UTC 2014,,,,,,0|i1s0dz:,371716,1.2.14,2.0.4,,,,,,jbellis,jbellis,,,,,,,,,,"04/Feb/14 05:44;jbellis;On the first hunk, should {{indexManager.getIndexForColumn(expression.column_name)}} even be including non-CF indexes?","04/Feb/14 17:04;sbtourist;I'd say so, it's still an index and it's useful to retrieve it from column names, whether it is backed by a CF or not.",05/Feb/14 16:54;sbtourist;Attached patch for 1.2 too.,06/Feb/14 16:19;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Java 7u51 breaks internode encryption,CASSANDRA-6613,12690986,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,sinner,sinner,24/Jan/14 08:31,12/Mar/19 14:17,13/Mar/19 22:29,17/Mar/14 17:12,1.2.16,2.0.7,2.1 beta2,,,,0,,,,,"With the latest update of the Oracle JRE (7u51) internode encryption no longer works, since none of the cipher suites (TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA) that Cassandra supports work anymore. 

I see this in the log file:
WARN  o.a.cassandra.security.SSLFactory - Filtering out TLS_RSA_WITH_AES_256_CBC_SHA as it isnt supported by the socket

See http://www.oracle.com/technetwork/java/javase/7u51-relnotes-2085002.html (all the way at the bottom) for more information.
",,,,,,,,,,,,,,,,,,,,,,,,24/Jan/14 15:01;brandon.williams;6613.txt;https://issues.apache.org/jira/secure/attachment/12625054/6613.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-24 14:37:15.206,,,no_permission,,,,,,,,,,,,369729,,,Mon Mar 17 17:12:58 UTC 2014,,,,,,0|i1rq2v:,370030,,,,,,,,sinner,sinner,,,1.2.12,,,,,,,24/Jan/14 14:37;brandon.williams;The release notes seem to indicate these ciphers work unless you're in FIPS 140 compliant mode instead of the default.  Does the 128 bit version also get filtered?,"24/Jan/14 14:49;sinner;Oh, yeah, I forgot to mention that. We are indeed running in FIPS 140 mode. All RSA versions are filtered.",24/Jan/14 15:01;brandon.williams;That explains it :)  Can you try this patch and use DHE/ECDHE?,24/Jan/14 15:11;sinner;Thank you for the quick response! I'll give the patch a shot early next week.,"13/Mar/14 21:33;jbellis;How did that work for you, Ray?","17/Mar/14 12:14;sinner;This patch works fine, thanks!",17/Mar/14 17:12;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix replaying old (1.2) commitlog in Cassandra 2.0,CASSANDRA-6714,12695522,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,pkolaczk,pkolaczk,17/Feb/14 14:08,12/Mar/19 14:17,13/Mar/19 22:29,18/Feb/14 22:41,2.0.6,2.1 beta1,,,,,0,qa-resolved,,,,"Our docs, and code, both explicitly say that you should drain a node before upgrading to a new major release.

If you don't do what the docs explicitly tell you to do, however, Cassandra won't scream at you. Also, we *do* currently have logic to replay 1.2 commitlog in 2.0, but it seems to be slightly broken, unfortunately.","Old node: Cassandra 1.2.15 (DSE)
New node: Cassandra 2.0.5.1 (DSE)",,,,,,,,,,,,,,,,,,,,,,,18/Feb/14 22:10;iamaleksey;6714.txt;https://issues.apache.org/jira/secure/attachment/12629648/6714.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-18 15:49:55.897,,,no_permission,,,,,,,,,,,,374030,,,Tue Feb 18 22:46:08 UTC 2014,,,,,,0|i1sgfb:,374330,2.0.5,2.1 rc3,,,,,,jbellis,jbellis,,,,,,,,,enigmacurry,"17/Feb/14 14:10;pkolaczk;Discovered while testing CASSANDRA-6707.

{quote}
However, I will note that after upgrading the first node to 2.0, the results of select count(*) were consistent but incorrect (I got 957815 as the result). After upgrading the second node to 2.0 I got another consistent but incorrect result (912873).
{quote}","18/Feb/14 15:49;enigmacurry;I've modified our dtest to run a test without flush/drain and I'm seeing the same thing (missing rows.)

Another oddity is the schema is getting modified. Here's my schema on 1.2:
{code}
CREATE KEYSPACE upgrade WITH replication = {
  'class': 'SimpleStrategy',
  'replication_factor': '2'
};

USE upgrade;

CREATE TABLE cf (
  k int PRIMARY KEY,
  v text
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};

CREATE INDEX vals ON cf (v);

CREATE TABLE countertable (
  k text PRIMARY KEY,
  c counter
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};
{code}

And after an upgrade to 2.0:

{code}
CREATE KEYSPACE upgrade WITH replication = {
  'class': 'SimpleStrategy',
  'replication_factor': '2'
};

USE upgrade;

CREATE TABLE cf (
  key int,
  v text,
  PRIMARY KEY (key)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='99.0PERCENTILE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};

CREATE INDEX vals ON cf (v);

CREATE TABLE countertable (
  key text,
  c counter,
  PRIMARY KEY (key)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='99.0PERCENTILE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};
{code}

Notice that the primary key column name has changed from 'k' to 'key' in both of my tables - there's nothing in my test code that does that, and it does not occur when I do the recommended flush/drain.",18/Feb/14 19:03;iamaleksey;Same reason.,"18/Feb/14 22:12;iamaleksey;We are currently comparing the segment's messaging version to a commit log descriptor version, which is a bug introduced by CASSANDRA-5764.

The trivial-ish patch attached fixes that.",18/Feb/14 22:17;jbellis;+1,"18/Feb/14 22:41;iamaleksey;Committed, thanks.

[~enigmacurry] leave the flush/drain off the dtest, let's have this scenario covered.","18/Feb/14 22:46;enigmacurry;Yea, we're testing both scenarios from now on.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Snapshot based repair does not send snapshot command to itself,CASSANDRA-6713,12695300,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,kohlisankalp,kohlisankalp,14/Feb/14 22:04,12/Mar/19 14:17,13/Mar/19 22:29,17/Feb/14 17:20,1.2.16,2.0.6,2.1 beta1,,,,0,,,,,"Due to this, the Merkle tree created will differ a lot causing lot of streaming to happen. ",,,,,,,,,,,,,,,,,,,,,,,,14/Feb/14 22:08;yukim;6713-1.2.txt;https://issues.apache.org/jira/secure/attachment/12629127/6713-1.2.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-14 22:08:36.076,,,no_permission,,,,,,,,,,,,373808,,,Mon Feb 17 17:20:41 UTC 2014,,,,,,0|i1sf1z:,374108,,,,,,,,kohlisankalp,kohlisankalp,,,,,,,,,,14/Feb/14 22:08;yukim;I think what we need for 1.2 is just send snapshot request to all node including coordinator itself.,14/Feb/14 22:16;kohlisankalp;The on liner change looks good. ,14/Feb/14 22:29;jbellis;Will it actually use this snapshot while building the tree with no other changes?,"14/Feb/14 23:00;yukim;Validation first tries to find snapshot with the name same as repair session ID and use it if exists(https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L716).
So attached change is sufficient to fix this.",14/Feb/14 23:33;kohlisankalp;+1,"17/Feb/14 17:20;yukim;Committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate rows returned when in clause has repeated values,CASSANDRA-6706,12695163,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,blerer,Mehitabel,Mehitabel,14/Feb/14 08:51,12/Mar/19 14:17,13/Mar/19 22:29,21/Jan/15 19:27,2.2.0 beta 1,,,Legacy/CQL,,,0,cql,,,,"If a value is repeated within an IN clause then repeated rows are returned for  the repeats:
cqlsh> create table t1(c1 text primary key);
cqlsh> insert into t1(c1) values ('A');
cqlsh> select * from t1;

 c1
----
  A

cqlsh> select * from t1 where c1 = 'A';

 c1
----
  A

cqlsh> select * from t1 where c1 in( 'A');

 c1
----
  A

cqlsh:dslog> select * from t1 where c1 in( 'A','A');

 c1
----
  A
  A
","Found on 
[cqlsh 4.1.0 | Cassandra 2.0.3-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.38.0]",,,,,,,,,,,,,,,,,,,,,,,21/Jan/15 11:25;blerer;CASSANDRA-6706-2.1.txt;https://issues.apache.org/jira/secure/attachment/12693568/CASSANDRA-6706-2.1.txt,21/Jan/15 13:43;blerer;CASSANDRA-6706-trunk-V2.txt;https://issues.apache.org/jira/secure/attachment/12693589/CASSANDRA-6706-trunk-V2.txt,21/Jan/15 11:25;blerer;CASSANDRA-6706-trunk.txt;https://issues.apache.org/jira/secure/attachment/12693569/CASSANDRA-6706-trunk.txt,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-02-14 13:19:14.923,,,no_permission,,,,,,,,,,,,373671,,,Wed Jan 21 19:27:04 UTC 2015,,,,,,0|i1se7r:,373971,,,,,,,,snazy,snazy,,,,,,,,,,"14/Feb/14 13:19;slebresne;That is kind of the intended behavior. Is it the best behavior? I don't know, though I'm not sure it matters much in practice tbh. But when there is an IN, we do order the resulting rows following the order of the values in the IN (unless there is an explicit ordering that takes precedence of course) which kind of suggest we consider the IN values as a list rather than a set, and from that perspective, it's probably not entirely crazy to return duplicate results in that case. In particular, if you use a prepared marker for an IN, the server will expect a list, not a set for the values (and changing now would really break users). It's easy enough to avoid the duplication client side if you don't want duplicates.

Don't get me wrong, I'm not saying not returning duplicate in that case would be inferior, but rather that I don't see a big problem with the current behavior and so that I'd rather not introduce a breaking change, even a small one, for no good reason.","14/Feb/14 13:38;Mehitabel;If I were performing a calculation summing the results then the answer would be wrong. 
I suppose it is arguable that I should know better than to put duplicates in an in clause but this bit me when I was generating a query by aggregating selection parameters from separate sources.
My background is much more extensive in relational databases and in Postgres (to take my preferred example) I would only get back one row for this query which in terms of correctness is absolutely what I would expect.","19/Sep/14 20:19;philipthompson;[~slebresne] Should this be closed as Won't Fix or Not a Problem, or left open?","19/Sep/14 20:46;philipthompson;IRC discussion shows a desire to fix this in 3.0, but only to warn in 2.1.","19/Sep/14 21:05;rcoli;No DBA or developer familiar with SQL will expect this behavior. 100% of them will find it unexpected, and many will conclude it is a bug.

I understand CQL is not SQL, but people expect similar semantics.",21/Jan/15 11:22;blerer;This behavior has apparently already been changed in trunk during the {{SelectStatement}} refactoring CASSANDRA-7981.,"21/Jan/15 11:25;blerer;The patch for 2.1 make sure that Cassandra will log a warning the first time a user execute a query containing an IN restriction with duplicate values on the primary key.

The patch for trunk add unit tests to check the behavior and fix the {{CHANGES.txt}} files.",21/Jan/15 11:27;blerer;[~snazy] can you review?,21/Jan/15 11:53;snazy;Sure :),"21/Jan/15 13:18;snazy;+1 on the 2.1 patch. It triggers the warning once for the example in the ticket description.

The trunk patch should have tests for duplicate IN values on the partition key.
It checks on duplicate IN values on the clustering key (even 2.1 does not return duplicates on CK duplicates).
You can simply add other assertions like {{SELECT * FROM %s WHERE k1 IN (?, ?) AND k2 = ?}} or {{SELECT * FROM %s WHERE k1 IN (?, ?) AND k2 IN (?, ?)}}.",21/Jan/15 13:43;blerer;Adds the missing tests.,"21/Jan/15 19:27;snazy;+1

committed as 0c2eaa9 (2.1) + 732986b (trunk, merge-commit)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use real node messaging versions for schema exchange decisions,CASSANDRA-6700,12695031,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,pkolaczk,pkolaczk,13/Feb/14 16:13,12/Mar/19 14:17,13/Mar/19 22:29,13/Feb/14 16:35,1.2.16,2.0.6,2.1 beta1,,,,0,,,,,"IncomingTcpConnection#handleModernVersion sets version to min(my version, version of the peer). This messes up schema pull/push.",,,,,,,,,,,,,,,,,,,,,,,,13/Feb/14 16:14;iamaleksey;6700.txt;https://issues.apache.org/jira/secure/attachment/12628777/6700.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-13 16:35:25.869,,,no_permission,,,,,,,,,,,,373539,,,Thu Feb 13 16:35:25 UTC 2014,,,,,,0|i1sdef:,373839,,,,,,,,pkolaczk,pkolaczk,,,,,,,,,,13/Feb/14 16:17;pkolaczk;+1 LGTM,"13/Feb/14 16:35;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cross DC writes not compatible 1.2->1.1 during rolling upgrade,CASSANDRA-6732,12695949,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jjordan,jjordan,jjordan,19/Feb/14 13:17,12/Mar/19 14:17,13/Mar/19 22:29,19/Feb/14 14:34,1.2.16,,,,,,0,,,,,"During a rolling upgrade from 1.1.12 to 1.2.15 one DC at a time only 1/3 of the writes to the first DC to be upgraded actually make it to the other DC, and LOTS of hints attempt to get made.

Looks like the header for forwarded writes changed from 1.1->1.2 so the 1.1 nodes can't read it.
",,,,,,,,,,,,,,,,,,,,,,,,19/Feb/14 13:20;jjordan;0001-Don-t-try-to-do-Cross-DC-forwarding-to-old-nodes.patch;https://issues.apache.org/jira/secure/attachment/12629762/0001-Don-t-try-to-do-Cross-DC-forwarding-to-old-nodes.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-19 14:34:53.519,,,no_permission,,,,,,,,,,,,374427,,,Wed Aug 20 01:29:19 UTC 2014,,,,,,0|i1sivb:,374727,1.2.15,,,,,,,jbellis,jbellis,,,,,,,,,,19/Feb/14 13:21;jjordan;Simple fix of not trying to Cross DC forwarding to old nodes attached.  If someone wants they can do a more complicated fix that uses the old 1.1 headers for 1.1 nodes.,19/Feb/14 14:34;jbellis;Committed.  (The regression was caused by CASSANDRA-3617 splitting FORWARD_HEADER into FORWARD_TO and FORWARD_FROM.),"20/Aug/14 01:29;ssmith;This week I ran into this problem testing upgrade from 1.1.12 to 1.2.18.  I want to document for others that, as far as I can tell, the attached patch fixes one case but not all.  When the local coordinator wants to relay cross-DC writes through a remote relay, there are 4 cases to consider (based on my tests and reading the code):

# Local coordinator is Cassandra 1.1, remote relay is Cassandra 1.1: \\ Everything works. The old ""FORWARD"" header is used as expected.
# Local coordinator is Cassandra 1.2, remote relay is Cassandra 1.1: \\ Everything works due to this patch.  The Cassandra 1.2 coordinator will send individual messages to all the remote DC servers.
# Local coordinator is Cassandra 1.2, remote relay is Cassandra 1.2: \\ All Cassandra 1.1 servers in the remote DC receive the write but ack to the remote relay instead of the coordinator.  This is because the Cassandra 1.1 servers ignore the new ""FWD_FRM"" header and, starting with Cassandra 1.2, the relay sets ""message.from"" to itself on the forwarded messages.  The net effect: writes get applied, but EACH_QUORUM, ALL writes fail due to lost acks.
# Local coordinator is Cassandra 1.1, remote relay is Cassandra 1.2: \\ The remote relay ignores the ""FORWARD"" header and does not forward messages.  The net effect: only the remote relay applies the write, other servers in the remote DC drop the write.  EACH_QUORUM, ALL writes fail and data is left inconsistent, repair is required if hints are lost.

Apologies if I have any mistakes here.  It's hard to test each combination independently.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodes never bootstrap if schema is empty,CASSANDRA-6685,12694253,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,rlow,rlow,10/Feb/14 12:46,12/Mar/19 14:17,13/Mar/19 22:29,10/Feb/14 15:41,1.2.16,,,,,,0,,,,,"Since 1.2.15, bootstrap never completes if the schema is empty. The bootstrapping node endlessly prints:

bq. {{INFO 12:37:44,863 JOINING: waiting for schema information to complete}}

until you add something to the schema (i.e. create a keyspace).

The problem looks to be caused by CASSANDRA-6648, where MigrationManager.isReadForBootstrap() was changed to:

bq. {{return Schema.instance.getVersion() != null && !Schema.emptyVersion.equals(Schema.instance.getVersion());}}

This is wrong since {{Schema.emptyVersion.equals(Schema.instance.getVersion())}} is always true if there is no schema.

We need some different logic for determining when the schema is propagated.

I haven't tested, but I expect this issue appears in 2.0.5 too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-02-10 14:54:17.802,,,no_permission,,,,,,,,,,,,372762,,,Sat Mar 22 20:36:02 UTC 2014,,,,,,0|i1s8nb:,373066,1.2.15,2.0.5,,,,,,rlow,rlow,,,1.2.15,,,,,,,"10/Feb/14 14:54;brandon.williams;Luckily bootstrapping without schema isn't a huge problem or very common, but I propose fixing this by reverting the change in question, since that puts everything back the way it was except for moving the MM events, which solved CASSANDRA-6648.","10/Feb/14 14:56;rlow;I tested and this isn't reproduced in 2.0.5, because the system_traces keyspace is added to the schema, but it appears to be added differently in 1.2.x so doesn't modify the schema ID.",10/Feb/14 15:04;rlow;Wasn't this change made to guard against the case when schema messages get delayed and responses aren't received before bootstrap begins?,"10/Feb/14 15:10;rlow;Also I think the use case of bootstrapping without a schema is quite common when setting up a new cluster. For more than a few nodes, people won't list all nodes as a seed and, with this issue, non-seed nodes won't join until a keyspace is created.","10/Feb/14 15:14;brandon.williams;bq. Wasn't this change made to guard against the case when schema messages get delayed and responses aren't received before bootstrap begins?

It was an extra guard, which is apparently slightly flawed.  The heart of the problem in 6648 was the event to check if pulling was needed firing too early, and not pulling because the remote node was not known to be part of the ring yet and thus considered a fat client.

bq. For more than a few nodes, people won't list all nodes as a seed and, with this issue, non-seed nodes won't join until a keyspace is created.

{noformat}auto_bootstrap = false{noformat} will not only solve this, but also skip the ring delay sleep and be much faster.
","10/Feb/14 15:21;rlow;OK, +1 on your proposed fix then.

Agreed auto_bootstrap is a workaround for the new cluster case.",10/Feb/14 15:41;brandon.williams;Committed.,12/Feb/14 01:12;appodictic;I am confused as to what was committed. No one mentioned a git hash or included a patch.,"13/Feb/14 16:45;rlow;Here's the git commit:

https://github.com/apache/cassandra/commit/c5627008a2c87ab960da98f4d5b8ca4aca6eebdd","19/Feb/14 19:08;rcoli;{quote}Luckily bootstrapping without schema isn't a huge problem or very common{quote}
In every cluster I have ever run, my initial setup process goes :

1) coalesce cluster
2) load schema

The above process seems ""very common"" to me; I am surprised to hear suggestion that anyone ever does anything else. A bug such as this one which would affect most noobs setting up My First Cassandra Cluster also seems ""very common"" to me, and a ""huge problem."" 

In my view, this bug is serious enough to warrant an accelerated 1.2.16 release schedule.",19/Feb/14 19:19;brandon.williams;Thanks for your input.,22/Mar/14 19:58;rbranson;Ran into this today trying to test out some new machines. It's not good that we've just been sitting on this for 6 weeks now.,"22/Mar/14 20:36;appodictic;Suggestion: if a machine starts up with no keyspaces it should attempt to create Keyspace1, Standard1....I miss that old keyspace :) jk. 

I think it would be nice to fix it. New users do not know how to create keyspaces yet, they likely join nodes and then attempt to use nodetool ring to see the topology. I could see someone grinding for a while attempting to determine why nodes are not seeing each other.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Never ending batch replay after dropping column family,CASSANDRA-6822,12699475,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,baldrick,baldrick,07/Mar/14 18:25,12/Mar/19 14:17,13/Mar/19 22:29,01/Apr/14 01:26,1.2.17,2.0.7,2.1 beta2,,,,0,,,,,"After dropping a column family, I got an infinite number of messages like this:

WARN [BatchlogTasks:1] 2014-03-07 17:50:08,962 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c

I say an infinite number but I only waited about 12 hours before I decided I had had enough and truncated system.batchlog.  So perhaps it would have gone away by itself.

Presumably the batch replay is skipped but not discarded, so will keep on turning up forever.",Cassandra 2.0.5,,,,,,,,,,,,,,,,,,,,,,,01/Apr/14 00:49;iamaleksey;6822.txt;https://issues.apache.org/jira/secure/attachment/12637967/6822.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-18 21:27:00.848,,,no_permission,,,,,,,,,,,,377822,,,Mon Sep 18 08:42:14 UTC 2017,,,,,,0|i1t3qf:,378114,,,,,,,,xedin,xedin,,,,,,,,,,"18/Mar/14 21:27;iamaleksey;[~baldrick] Right after that log line, BatchlogManager deleted the batch, for good. Those infinite messages of yours - were they LIKE this or exactly this? Have you seen the id in 'batch replay of <id>' repeat itself?","19/Mar/14 08:22;baldrick;$ grep bc3916e0-a533-11e3-9c41-55f3810000b5 system.log*
system.log.10: WARN [BatchlogTasks:1] 2014-03-07 17:32:44,430 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.10: WARN [BatchlogTasks:1] 2014-03-07 17:50:08,962 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.11: WARN [BatchlogTasks:1] 2014-03-07 17:16:01,702 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.12: WARN [BatchlogTasks:1] 2014-03-07 16:42:30,661 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.12: WARN [BatchlogTasks:1] 2014-03-07 16:59:16,837 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.13: WARN [BatchlogTasks:1] 2014-03-07 16:25:31,578 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.14: WARN [BatchlogTasks:1] 2014-03-07 15:51:00,713 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.14: WARN [BatchlogTasks:1] 2014-03-07 16:08:35,048 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.15: WARN [BatchlogTasks:1] 2014-03-07 15:34:07,253 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.16: WARN [BatchlogTasks:1] 2014-03-07 15:17:12,570 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.17: WARN [BatchlogTasks:1] 2014-03-07 14:43:43,369 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.17: WARN [BatchlogTasks:1] 2014-03-07 15:00:26,634 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.18: WARN [BatchlogTasks:1] 2014-03-07 14:26:15,338 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.19: WARN [BatchlogTasks:1] 2014-03-07 13:52:49,399 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.19: WARN [BatchlogTasks:1] 2014-03-07 14:09:32,664 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.20: WARN [BatchlogTasks:1] 2014-03-07 13:36:07,128 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.21: WARN [BatchlogTasks:1] 2014-03-07 13:02:31,492 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.21: WARN [BatchlogTasks:1] 2014-03-07 13:19:24,696 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.22: WARN [BatchlogTasks:1] 2014-03-07 12:45:49,411 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.23: WARN [BatchlogTasks:1] 2014-03-07 12:12:25,315 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.23: WARN [BatchlogTasks:1] 2014-03-07 12:29:07,386 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.24: WARN [BatchlogTasks:1] 2014-03-07 11:55:43,479 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.25: WARN [BatchlogTasks:1] 2014-03-07 11:22:18,243 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.25: WARN [BatchlogTasks:1] 2014-03-07 11:39:00,615 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.26: WARN [BatchlogTasks:1] 2014-03-07 11:05:36,232 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.27: WARN [BatchlogTasks:1] 2014-03-07 10:48:51,143 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.28: WARN [BatchlogTasks:1] 2014-03-07 10:15:20,859 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.28: WARN [BatchlogTasks:1] 2014-03-07 10:32:02,376 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.29: WARN [BatchlogTasks:1] 2014-03-07 09:58:36,394 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.30: WARN [BatchlogTasks:1] 2014-03-07 09:25:13,257 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.30: WARN [BatchlogTasks:1] 2014-03-07 09:41:54,872 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.31: WARN [BatchlogTasks:1] 2014-03-07 09:08:30,446 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.32: WARN [BatchlogTasks:1] 2014-03-07 08:34:58,201 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.32: WARN [BatchlogTasks:1] 2014-03-07 08:51:45,261 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.33: WARN [BatchlogTasks:1] 2014-03-07 08:18:16,738 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.34: WARN [BatchlogTasks:1] 2014-03-07 07:44:51,637 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.34: WARN [BatchlogTasks:1] 2014-03-07 08:01:33,711 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.35: WARN [BatchlogTasks:1] 2014-03-07 07:28:10,182 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.36: WARN [BatchlogTasks:1] 2014-03-07 06:54:47,093 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.36: WARN [BatchlogTasks:1] 2014-03-07 07:11:28,734 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.37: WARN [BatchlogTasks:1] 2014-03-07 06:38:05,595 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.38: WARN [BatchlogTasks:1] 2014-03-07 06:21:24,169 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.39: WARN [BatchlogTasks:1] 2014-03-07 05:48:01,066 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.39: WARN [BatchlogTasks:1] 2014-03-07 06:04:42,562 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.40: WARN [BatchlogTasks:1] 2014-03-07 05:31:19,390 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.41: WARN [BatchlogTasks:1] 2014-03-07 05:14:37,967 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.42: WARN [BatchlogTasks:1] 2014-03-07 04:40:06,539 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.43: WARN [BatchlogTasks:1] 2014-03-07 04:22:46,869 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
system.log.50: WARN [BatchlogTasks:1] 2014-03-06 18:26:31,876 BatchlogManager.java (line 235) Skipped batch replay of bc3916e0-a533-11e3-9c41-55f3810000b5 due to org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=2fdad07d-0cc3-373a-8aa0-15837cd9bc6c
","01/Apr/14 00:55;iamaleksey;Oh, well. BM#deleteBatch() uses millis for timestamps, so when batchlog is being replayed from the sstables, the replayed entries never go away.

This path is rarely hit - normally after a successful delete SP#asyncRemoveFromBatchlog() would clean up the batchlog, and that properly uses micros.

Furthermore, even in the unsuccessful case, if replaying a batch from the memtable, the entry will also go away b/c of this flush optimization - https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/Memtable.java#L357

Still, I'm surprised that this only surfaced now. Attaching a trivial patch that fixes the issue and modifies the unit test to flush the batchlog first.",01/Apr/14 01:15;xedin;+1,"01/Apr/14 01:26;iamaleksey;Committed, thanks.","04/Apr/14 05:08;exabytes18;FWIW, while it may be ""rarely hit"", we start starting to see this too.",19/Apr/14 10:13;crack2drop;Does this occur in 2.0.3.,20/Apr/14 14:09;iamaleksey;[~crack2drop] yes,"20/Apr/14 16:24;crack2drop;Will this patch help in 2.0.3 ? Don't see that version tagged for release.

Moreover , does this result in any data loss or long term impact if any ?","20/Apr/14 20:32;iamaleksey;[~crack2drop] no, it doesn't result in data loss. You can apply the patch to 2.0.3. The fix is tagged for 2.0.7, if you look more carefully - so you should just upgrade to 2.0.7. Running 2.0.3 at this point as an actively bad idea.","21/Apr/14 02:33;crack2drop;Just one doubt. We have been running 2.0.3 for so long and have not found any external issue so far other than a private link failure causing a stall in operation. 

Can you tell how stable is 2.0.7? Up gradation can be done on the run right ?(by just bringing down the node running 2.0.3 by nodetool drain and starting off new one with updates in cassandra.yaml? Shall I transfer the SSTables directly from 2.0.3 to 2.0.7 or allow it to sync its data after starting? Am actually using vnodes.",18/Sep/17 08:42;tjeubaoit;It still occur in 3.11 after dropping materialized view ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTable references not released if stream session fails before it starts,CASSANDRA-6818,12699420,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,rlow,rlow,07/Mar/14 14:08,12/Mar/19 14:17,13/Mar/19 22:29,03/Apr/14 21:20,1.2.16,2.0.7,2.1 beta2,,,,0,,,,,"I observed a large number of 'orphan' SSTables - SSTables that are in the data directory but not loaded by Cassandra - on a 1.1.12 node that had a large stream fail before it started. These orphan files are particularly dangerous because if the node is restarted and picks up these SSTables it could bring data back to life if tombstones have been GCed. To confirm the SSTables are orphan, I created a snapshot and it didn't contain these files. I can see in the logs that they have been compacted so should have been deleted.

The log entries for the stream are:

{{INFO [StreamStage:1] 2014-02-21 19:41:48,742 StreamOut.java (line 115) Beginning transfer to /10.0.0.1}}
{{INFO [StreamStage:1] 2014-02-21 19:41:48,743 StreamOut.java (line 96) Flushing memtables for [CFS(Keyspace='ks', ColumnFamily='cf1'), CFS(Keyspace='ks', ColumnFamily='cf2')]...}}
{{ERROR [GossipTasks:1] 2014-02-21 19:41:49,239 AbstractStreamSession.java (line 113) Stream failed because /10.0.0.1 died or was restarted/removed (streams may still be active in background, but further streams won't be started)}}
{{INFO [StreamStage:1] 2014-02-21 19:41:51,783 StreamOut.java (line 161) Stream context metadata [...] 2267 sstables.}}
{{INFO [StreamStage:1] 2014-02-21 19:41:51,789 StreamOutSession.java (line 182) Streaming to /10.0.0.1}}
{{INFO [Streaming to /10.0.0.1:1] 2014-02-21 19:42:02,218 FileStreamTask.java (line 99) Found no stream out session at end of file stream task - this is expected if the receiver went down}}

After digging in the code, here's what I think the issue is:

1. StreamOutSession.transferRanges() creates a streaming session, which is registered with the failure detector in AbstractStreamSession's constructor.
2. Memtables are flushed, potentially taking a long time.
3. The remote node fails, convict() is called and the StreamOutSession is closed. However, at this time StreamOutSession.files is empty because it's still waiting for the memtables to flush.
4. Memtables finish flusing, references are obtained to SSTables to be streamed and the PendingFiles are added to StreamOutSession.files.
5. The first stream fails but the StreamOutSession isn't found so is never closed and the references are never released.

This code is more or less the same on 1.2 so I would expect it to reproduce there. I looked at 2.0 and can't even see where SSTable references are released when the stream fails.

Some possible fixes for 1.1/1.2:

1. Don't register with the failure detector until after the PendingFiles are set up. I think this is the behaviour in 2.0 but I don't know if it was done like this to avoid this issue.
2. Detect the above case in (e.g.) StreamOutSession.begin() by noticing the session has been closed with care to avoid double frees.
3. Add some synchronization so closeInternal() doesn't race with setting up the session.",,,,,,,,,,,,,,CASSANDRA-7704,,,,,,,,,,10/Mar/14 22:57;yukim;6818-1.2.txt;https://issues.apache.org/jira/secure/attachment/12633805/6818-1.2.txt,22/Mar/14 00:03;yukim;6818-2.0-v2.txt;https://issues.apache.org/jira/secure/attachment/12636150/6818-2.0-v2.txt,01/Apr/14 23:19;yukim;6818-2.0-v3.txt;https://issues.apache.org/jira/secure/attachment/12638151/6818-2.0-v3.txt,03/Apr/14 03:16;yukim;6818-2.0-v4.txt;https://issues.apache.org/jira/secure/attachment/12638403/6818-2.0-v4.txt,10/Mar/14 22:57;yukim;6818-2.0.txt;https://issues.apache.org/jira/secure/attachment/12633806/6818-2.0.txt,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2014-03-07 14:19:56.301,,,no_permission,,,,,,,,,,,,377767,,,Thu Apr 03 21:20:34 UTC 2014,,,,,,0|i1t3e7:,378059,1.1.12,,,,,,,rlow,rlow,,,1.1.12,,,,,,,"07/Mar/14 14:19;jasobrown;We fixed this in CASSANDRA-6503, but that was only patched for 1.2 and above. As we're not really maintaining 1.1 any longer, I suggest you take the 1.2 patch from that ticket and apply to 1.1 (it should be reasonably easily).","07/Mar/14 14:37;rlow;We do see CASSANDRA-6503 as another source of orphans, but I think there are two separate issues. In particular, CASSANDRA-6503 creates orphans on failed stream in whereas this issue creates orphans on failed stream out. ","07/Mar/14 16:53;yukim;I think Richard is right, and the same thing can happen to 2.0+.
Considering fixing up to trunk, I think our option is No.2 on the above list.","10/Mar/14 22:57;yukim;Attaching patch for 1.2 and 2.0(and later).

For 1.2, I decided to go with No.1 of Richard's proposal, defer register to FD and Gossiper until files are set up. 2.0 already does this and it is easy to fix the problem.
the 
Although 2.0 defers FD/Gossiper registration, the way it releases reference can cause problem. Right now it releases immediately after the file is sent, so when the retry or session failure happens reference will not be released. So for 2.0, I changed to release reference when sender receives  ACK from receiver or when StreamSession fails for whatever reason.",11/Mar/14 05:15;jbellis;[~kohlisankalp] to review?,11/Mar/14 07:19;kohlisankalp;Sure. ,"11/Mar/14 10:19;rlow;I looked at the 1.2 patch, it looks fine. I'll see if I can reproduce the original issue to verify.

In StreamInSession.get, there is a minor memory leak - if another thread simultaneously creates the same session, the one that is discarded remains registered with the gossiper. This was present before, but we could easily fix it in this patch by delaying the registration until after the putIfAbsent succeeds.","14/Mar/14 14:31;rlow;I reproduced on 1.2 by inserting a sleep to delay the flushing. I killed a bootstrapping node during the sleep and references were leaked. With this patch, no references were leaked. So +1 for the 1.2 patch.","14/Mar/14 16:11;jbellis;Can you also look at the 2.0 patch, Richard?","14/Mar/14 19:29;rlow;Sure, I'll do a similar test next week.","19/Mar/14 22:28;rlow;I reproduced the original problem in the same way, and the patch fixed it.

But I'm concerned about moving the reference release to StreamTransferTask.complete(). This is only called when the receiver sends a completed message, which may never occur. In this case I think the streaming session hangs, so we would never release the reference. However, the original behavior was also wrong because a retry could be requested after a successful transfer. The reference will have been released so the file may have been removed.

We could add a timeout to catch the case when the completed message gets lost. If the node requests a retry after this time, there may be no reference so the stream would fail. That's probably OK in this rare case though.","22/Mar/14 00:03;yukim;Thanks, [~rlow].
I attached v2 to schedule timeout to release reference when not received ack after file is sent. I put hard coded 30min for timeout, but maybe we can set it longer.
",24/Mar/14 12:45;yukim;Committed 1.2. version to be released in 1.2.16.,"25/Mar/14 00:40;rlow;[~yukim] I think this is a good approach, but I think there are some issues:

1. If the timeout runnable races with complete() then the SSTable could be freed twice and cause it to be incorrectly deleted. This is unlikely, but the impact is pretty bad.
2. The timeout should remove the file from files (and files should become a thread-safe data structure) and call session.taskCompleted if files.isEmpty().
3. I'm not sure, but I think timeoutExecutor should be shutdown otherwise there is a risk the non-daemon thread will stop the JVM exiting.","01/Apr/14 23:19;yukim;[~rlow] Thanks for reviewing.
I attached updated version that I hope will clear concurrency issue your pointed out.",02/Apr/14 11:01;krummas;[~yukim] could this be the cause of CASSANDRA-6568 as well?,02/Apr/14 14:01;yukim;[~krummas] This can make SSTables that are compacted but not deleted. But for the SSTable in system/hints that cburroughs saw is different story since there should be no stream involved.,"02/Apr/14 16:58;rlow;[~yukim] looking better, a couple more issues though:

1. The timeoutExecutor shouldn't be static any more now it's per task
2. When the timeout occurs, inside StreamTransferTask.complete, timeoutTask.cancel is called while running the task. It's unclear from the java.util.concurrent code how safe this is. Did you test it? If it doesn't work a simple solution is to remove the task from timeoutTasks from within the runnable before calling complete.","03/Apr/14 03:16;yukim;[~rlow] Good catch.
I removed static from timeoutExecutor and moved task removal code.
I also added task cancel to createMessageForRetry since we want reschedule time out for that retry message.
Other change I made is set time out time to 12 hours instead of 30 min which is too short for sending large file.

Patch also includes simple unit test.","03/Apr/14 17:28;rlow;Great, +1.","03/Apr/14 21:20;yukim;Committed, thanks for review!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Collections should have a proper compare() method for UDT,CASSANDRA-6783,12697911,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,28/Feb/14 12:06,12/Mar/19 14:17,13/Mar/19 22:29,20/Mar/14 09:15,2.1 beta2,,,,,,0,,,,,"So far, ListType, SetType and MapType don't have a proper implementation of compare() (they throw UnsupportedOperationException) because we haven't need one since as far as the cell comparator is concenred, only parts of a collection ends up in the comparator and need to be compared, but the full collection itself does not.

But with UDT can nest a collection and that sometimes require to be able to compare them. Typically, I pushed a dtest [here|https://github.com/riptano/cassandra-dtest/commit/290e9496d1b2c45158c7d7f5487d09ba48897a7f] that ends up throwing:
{noformat}
java.lang.UnsupportedOperationException: CollectionType should not be use directly as a comparator
        at org.apache.cassandra.db.marshal.CollectionType.compare(CollectionType.java:72) ~[main/:na]
        at org.apache.cassandra.db.marshal.CollectionType.compare(CollectionType.java:37) ~[main/:na]
        at org.apache.cassandra.db.marshal.AbstractType.compareCollectionMembers(AbstractType.java:174) ~[main/:na]
        at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:101) ~[main/:na]
        at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:35) ~[main/:na]
        at java.util.TreeMap.compare(TreeMap.java:1188) ~[na:1.7.0_45]
        at java.util.TreeMap.put(TreeMap.java:531) ~[na:1.7.0_45]
        at java.util.TreeSet.add(TreeSet.java:255) ~[na:1.7.0_45]
        at org.apache.cassandra.cql3.Sets$DelayedValue.bind(Sets.java:205) ~[main/:na]
        at org.apache.cassandra.cql3.Sets$Literal.prepare(Sets.java:91) ~[main/:na]
        at org.apache.cassandra.cql3.UserTypes$Literal.prepare(UserTypes.java:60) ~[main/:na]
        at org.apache.cassandra.cql3.Operation$SetElement.prepare(Operation.java:221) ~[main/:na]
        at org.apache.cassandra.cql3.statements.UpdateStatement$ParsedUpdate.prepareInternal(UpdateStatement.java:201) ~[main/:na]
...
{noformat}
Note that this stack doesn't involve cell name comparison at all, it's just that CQL3 sometimes uses a SortedSet underneath to deal with set literals (since internal sets are sorted by their value), and so when a set contains UDT that has set themselves, we need the collection comparison. That being said, for some cases like having a UDT as a map key, we do would need collections to be comparable for the purpose of cell name comparison.

Attaching relatively simple patch. The patch is a bit bigger than it should be because while adding the 3 simple compare() method, I realized that we had methods to read a short length (2 unsigned short) from a ByteBuffer duplicated all over the place and that it was time to consolidate that in ByteBufferUtil where it should have been from day one (thus removing the duplication). I can separate that trivial refactor in a separate patch if we really need to, but really, the new stuff is the compare() method implementation in ListType, SetType and MapType and the rest is a bit of trivial cleanup. ",,,,,,,,,,,,,,,,,,,,,,,,19/Mar/14 10:18;slebresne;6783-2.txt;https://issues.apache.org/jira/secure/attachment/12635521/6783-2.txt,28/Feb/14 12:07;slebresne;6783.txt;https://issues.apache.org/jira/secure/attachment/12631726/6783.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-03-11 20:32:38.036,,,no_permission,,,,,,,,,,,,376385,,,Thu Mar 20 09:15:46 UTC 2014,,,,,,0|i1suwv:,376681,,,,,,,,thobbs,thobbs,,,2.0 beta 1,,,,,,,11/Mar/14 20:32;jbellis;([~thobbs] to review),"12/Mar/14 21:16;thobbs;In MapType.compare, and ListType.compareListOrSet you have:

{code}
        if (o1 == null || !o1.hasRemaining())                                                                                               
            return o2 == null || o2.hasRemaining() ? -1 : 0;
{code}
This should be:
{code}
        if (o1 == null || !o1.hasRemaining())                                                                                               
            return o2 == null || !o2.hasRemaining() ? 0 : -1;
{code}

Can you add some unit tests?

bq. I can separate that trivial refactor in a separate patch if we really need to

Just committing the ByteBuffer changes separately should be fine.",13/Mar/14 09:12;slebresne;I've committed the BB methods refactor separatly so attaching v2 that only do what this ticket is about. It fix the typo from above and adds a unit test too.,"18/Mar/14 19:07;thobbs;The v2 patch still doesn't use the correct condition.  Specifically, you need {{!o2.hasRemaining}}.  Perhaps add a null/empty case to the tests?","19/Mar/14 10:18;slebresne;Can't seem to get this one right, can I? Updated v2 to fix this.",19/Mar/14 17:13;thobbs;+1 with a minor nit: the {{java.util.*}} import is unused in CollectionTypeTest,"20/Mar/14 09:15;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
setting TTL on some columns seems to expire whole row,CASSANDRA-6782,12697821,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,rhatch,rhatch,27/Feb/14 22:53,12/Mar/19 14:17,13/Mar/19 22:29,04/Mar/14 10:53,2.0.6,,,,,,0,,,,,"I create a table with 4 columns, set a ttl on 2 of the columns and when the TTL is up, the entire row disappears.

{noformat}
cqlsh:myks> CREATE TABLE paging_test (
        ...   id int,
        ...   mytext text,
        ...   anothervalue text,
        ...   somevalue text,
        ...   PRIMARY KEY (id, mytext)
        ... );
cqlsh:myks> insert into paging_test (id, mytext, anothervalue, somevalue) values (1, 'foo', 'some', 'another');
cqlsh:myks> select * from paging_test;

 id | mytext | anothervalue | somevalue
----+--------+--------------+-----------
  1 |    foo |         some |   another

(1 rows)

cqlsh:myks> update paging_test using ttl 10
        ...   set somevalue='one', anothervalue='two'
        ...   where id = 1 and mytext = 'foo';
cqlsh:myks> select * from paging_test;

 id | mytext | anothervalue | somevalue
----+--------+--------------+-----------
  1 |    foo |          two |       one

(1 rows)

cqlsh:myks> -- wait for it....
cqlsh:myks> select * from paging_test;

(0 rows)
{noformat}","java version ""1.7.0_51""
cassandra from trunk: 9c4824d6c476",,,,,,,,,,,,,CASSANDRA-13127,CASSANDRA-8185,,,,,,,,,28/Feb/14 20:26;slebresne;6782.txt;https://issues.apache.org/jira/secure/attachment/12631823/6782.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-28 07:28:16.875,,,no_permission,,,,,,,,,,,,376295,,,Tue Mar 04 10:53:42 UTC 2014,,,,,,0|i1sucv:,376591,2.1 rc3,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"28/Feb/14 07:28;slebresne;Well, I would point to [my comment on CASSANDRA-6668|https://issues.apache.org/jira/browse/CASSANDRA-6668?focusedCommentId=13911416&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13911416] as this very much is the exact same thing here.

But to repeat the gist of it, the current semantic of an UPDATE is that it actually also update every column in the WHERE clause, and so the update above actually set a TTL on all the columns of the row. I don't disagree that this current semantic is rather intuitive though, but this is not a new semantic to 2.1 so changing it, while technically trivial, is a potential issue for backward compatibility. Though maybe CASSANDRA-6668 and this ticket suggests that said current semantic is unintuitive enough that nobody could seriously have been relying on it and that we'd better ""fix"" it. I sure would not hesitate to change it if there wasn't the risk of maybe breaking a few users.","28/Feb/14 18:24;rhatch;I'm not certain how folks may be using this behavior, but I'm inclined to think it's pretty surprising. I have seen one instance where someone mentioned that they are setting a TTL for an entire row and then issue updates after the fact to give shorter TTL's to specific data (CASSANDRA-6654). They would be unwittingly expiring the data they use to isolate a single row at the same time when they intend to keep that data.",28/Feb/14 19:19;jbellis;I'm curious why we did it this way since it's not what I'd expect either.,"28/Feb/14 19:51;brandon.williams;I have to agree, this syntax seems misleading.","28/Feb/14 20:26;slebresne;Ok, seems there is a relatively good agreement that this needs to change, so attaching trivial patch for that.",04/Mar/14 09:17;iamaleksey;+1,"04/Mar/14 10:53;slebresne;Alright, committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Static columns break IN clauses,CASSANDRA-6769,12697190,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,tupshin,tupshin,25/Feb/14 19:20,12/Mar/19 14:17,13/Mar/19 22:29,28/Feb/14 20:57,2.0.6,,,,,,0,,,,,"If you use static columns, as implemented in CASSANDRA-6561, then very simple SELECT...WHERE...IN queries fail with an internal NPE.

create table foo (x text, y text, s text static, primary key (x,y));
insert into foo (x,y,s) values ('a','b','c');
select * from foo where x='a' and y in ('b','c');
Request did not complete within rpc_timeout.

ERROR [ReadStage:190] 2014-02-25 14:19:16,400 CassandraDaemon.java (line 196) Exception in thread Thread[ReadStage:190,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1900)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.filter.ColumnSlice$NavigableMapIterator.computeNext(ColumnSlice.java:141)
	at org.apache.cassandra.db.filter.ColumnSlice$NavigableMapIterator.computeNext(ColumnSlice.java:162)
	at org.apache.cassandra.db.filter.ColumnSlice$NavigableMapIterator.computeNext(ColumnSlice.java:162)
	at org.apache.cassandra.db.filter.ColumnSlice$NavigableMapIterator.computeNext(ColumnSlice.java:117)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.filter.SliceQueryFilter$1.hasNext(SliceQueryFilter.java:148)
	at org.apache.cassandra.db.filter.QueryFilter$2.getNext(QueryFilter.java:157)
	at org.apache.cassandra.db.filter.QueryFilter$2.hasNext(QueryFilter.java:140)
	at org.apache.cassandra.utils.MergeIterator$OneToOne.computeNext(MergeIterator.java:200)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:185)
	at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:122)
	at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:80)
	at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:72)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:297)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1550)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1379)
	at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:327)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:65)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1341)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1896)
",,,,,,,,,,,,,,,,,,,,,,,,26/Feb/14 08:01;slebresne;6769.txt;https://issues.apache.org/jira/secure/attachment/12631171/6769.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-26 08:01:19.615,,,no_permission,,,,,,,,,,,,375664,,,Fri Feb 28 20:57:37 UTC 2014,,,,,,0|i1sqh3:,375960,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"26/Feb/14 08:01;slebresne;Oops, kind of a typo, trivial patch attached.",26/Feb/14 21:10;iamaleksey;+1,"28/Feb/14 20:57;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changing the IP of a node on a live cluster leaves gossip infos and throws Exceptions,CASSANDRA-6615,12691049,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,frousseau,frousseau,24/Jan/14 14:56,12/Mar/19 14:17,13/Mar/19 22:29,29/Jan/14 23:37,1.2.14,2.0.5,,,,,0,qa-resolved,,,,"Following this procedure : https://engineering.eventbrite.com/changing-the-ip-address-of-a-cassandra-node-with-auto_bootstrapfalse/  to change the IP of a node, we encountered an issue :

 - logs contains: ""java.lang.RuntimeException: Host ID collision between active endpoint /127.0.0.5 and /127.0.0.3""
 - logs also indicate that the old IP is being removed of the cluster (FatClient timeout), then added again...
 - nodetool gossipinfo still list old IP (even a few hours after...)
 - the old IP is still seen as ""UP"" in the cluster... (according to the logs...)


Below is a small shell script which allows to reproduce the scenario...
{noformat}
#! /bin/bash

CLUSTER=$1
ccm create $CLUSTER --cassandra-dir=.
ccm populate -n 2
ccm start

ccm add node3 -i 127.0.0.3 -j 7300 -b
ccm node3 start
ccm node3 ring
ccm node3 stop

sed -i 's/127.0.0.3/127.0.0.5/g' ~/.ccm/$CLUSTER/node3/node.conf 
sed -i 's/127.0.0.3/127.0.0.5/g' ~/.ccm/$CLUSTER/node3/conf/cassandra.yaml

ccm node3 start
sleep 3
nodetool --host 127.0.0.5 --port 7300 gossipinfo
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,29/Jan/14 03:53;brandon.williams;6615.txt;https://issues.apache.org/jira/secure/attachment/12625784/6615.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-24 16:05:19.266,,,no_permission,,,,,,,,,,,,369792,,,Sat Feb 01 00:28:35 UTC 2014,,,,,,0|i1rqh3:,370094,1.2.13,,,,,,,thobbs,thobbs,,,1.2.0 beta 1,,,,,,mshuler,"24/Jan/14 16:05;brandon.williams;What is happening is that the token moves correctly on the other nodes, but the node that moved gets the host ID collision adding the old ip which is still in gossip, with its own host ID.  Because of this, it never removes the old ip, and thus re-propagates it to the other nodes after their quarantine expires, causing them to now get the same collision.","29/Jan/14 03:53;brandon.williams;Host ID conflicts are roughly as important as token conflicts, and need to be handled the same way, decisively.  We can decide who has won a host id conflict, much like we do a token conflict.  Once the loser is removed from tMD we can just let the FD mark it dead and then it will be evicted as a fat client (which is how it worked before we added host IDs.)  However, post-4375 this will take quite a while, since the only sample the FD has is the seed value of 30s.  While this is actually ok as long as we've removed it from tMD, we can do better, so we call removeEndpoint, which in turn removes it from the FD, but doesn't mark the epstate as dead.  isFatClient began checking if the epstate was dead in CASSANDRA-5378, but this doesn't seem necessary since the timestamp is updated if the node is actually alive, and the duration check will prevent it from being expired, so this patch removes it.

One small bit of nuance here is that if the host IDs conflict and the loser is in tMD, then the token conflict check is basically useless, since we have to update the host ID before the tokens, and the token check relies on data in tMD.  This means if a host ID conflict occurs where the tokens are different, the loser's tokens may just vanish, but that's highly unlikely to occur without hand-editing the system table or crafting one specifically for this.",29/Jan/14 09:44;frousseau;Thanks Brandon for the detailed explanation. I think I'll be able to try the patch by the end of the week...,"29/Jan/14 20:46;mshuler;Patch looks good to me.

{noformat}
 INFO [HANDSHAKE-/127.0.0.2] 2014-01-29 14:41:43,388 OutboundTcpConnection.java (line 418) Handshaking version with /127.0.0.2
 INFO [HANDSHAKE-/127.0.0.1] 2014-01-29 14:41:43,388 OutboundTcpConnection.java (line 418) Handshaking version with /127.0.0.1
 INFO [GossipStage:1] 2014-01-29 14:41:44,031 Gossiper.java (line 843) Node /127.0.0.3 is now part of the cluster
 INFO [GossipStage:1] 2014-01-29 14:41:44,033 Gossiper.java (line 809) InetAddress /127.0.0.3 is now UP
 WARN [GossipStage:1] 2014-01-29 14:41:44,038 StorageService.java (line 1481) Not updating host ID 2b3a79b1-0c16-402a-a0cb-fa36c391a7c6 for /127.0.0.3 because it's mine
 INFO [GossipStage:1] 2014-01-29 14:41:44,038 StorageService.java (line 1572) Nodes /127.0.0.3 and /127.0.0.5 have the same token 4611686018427387903.  Ignoring /127.0.0.3
 INFO [GossipStage:1] 2014-01-29 14:41:44,039 StorageService.java (line 1778) Removing endpoint /127.0.0.3
 INFO [main] 2014-01-29 14:41:47,700 TServerCustomFactory.java (line 47) Using synchronous/threadpool thrift server on 127.0.0.5 : 9160
 INFO [Thread-2] 2014-01-29 14:41:47,701 ThriftServer.java (line 110) Listening for thrift clients...
 INFO [GossipTasks:1] 2014-01-29 14:42:45,414 Gossiper.java (line 622) FatClient /127.0.0.3 has been silent for 30000ms, removing from gossip
{noformat}",29/Jan/14 21:14;brandon.williams;I actually didn't mean for that 'removing endpoint' info line to leak through (it's already logged at debug in the gossiper) but I'll remove it on commit.,29/Jan/14 23:27;thobbs;+1 from me,29/Jan/14 23:37;brandon.williams;Committed.,31/Jan/14 11:07;frousseau;Works like a charm. Thanks!,01/Feb/14 00:28;rcoli;per [~brandon.williams] this was introduced in 1.2.0.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Query failing due to AssertionError,CASSANDRA-6612,12690431,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,ateesh@gmail.com,ateesh@gmail.com,22/Jan/14 23:20,12/Mar/19 14:17,13/Mar/19 22:29,07/Aug/14 16:37,2.0.10,,,,,,0,,,,,"I am trying out Cassandra for the first time and running it locally for simple session management db. [Cassandra-2.0.4, CQL3, datastax driver 2.0.0-rc2]

The following count query works fine when there is no data in the table:
{code}
select count(*) from session_data where app_name=? and account=? and last_access > ?
{code}

But after even a single row is inserted into the table, the query fails with the following error:
{code}
    java.lang.AssertionError
	at org.apache.cassandra.db.filter.ExtendedFilter$WithClauses.getExtraFilter(ExtendedFilter.java:258)
	at org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1719)
	at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1674)
	at org.apache.cassandra.db.PagedRangeCommand.executeLocally(PagedRangeCommand.java:111)
	at org.apache.cassandra.service.StorageProxy$LocalRangeSliceRunnable.runMayThrow(StorageProxy.java:1418)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1931)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{code}

Here is the schema I am using:

{code}
    CREATE KEYSPACE session WITH replication= {'class': 'SimpleStrategy', 'replication_factor': 1};

    CREATE TABLE session_data (
    username text,
    session_id text,
    app_name text,
    account text,
    last_access timestamp,
    created_on timestamp,
    PRIMARY KEY (username, session_id, app_name, account)
    );

    create index sessionIndex ON session_data (session_id);
    create index sessionAppName ON session_data (app_name);
    create index lastAccessIndex ON session_data (last_access);
{code}
","Cassandra-2.0.4, CQL3, datastax driver 2.0.0-rc2",,,,,,,,,,,,,,,,,,,,,,,05/Aug/14 15:21;slebresne;6612.txt;https://issues.apache.org/jira/secure/attachment/12659897/6612.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-31 14:07:38.19,,,no_permission,,,,,,,,,,,,369383,,,Thu Aug 07 16:37:00 UTC 2014,,,,,,0|i1rnz3:,369686,2.0.4,,,,,,,beobal,beobal,,,,,,,,,,"31/Jan/14 14:07;cgavin;There seem to be some issues with indexing on fields that are part of your primary key (see also https://issues.apache.org/jira/browse/CASSANDRA-6470).  You may want to try revising your schema and/or query to remove the need to index key columns.

I was also able to reproduce this error in cassandra 2.0.1 and 2.0.4 using cqlsh or the java driver (I tried two versions of the 2.0.0rc, reproduction code below).

public static void main(String[] args) {
	com.datastax.driver.core.Session session = null;
	try {
		String address = ""localhost"";
		session = new com.datastax.driver.core.Cluster.Builder().addContactPoint(address).build().connect();
		//create keyspace, table, and indices
		session.execute(""create keyspace if not exists testing  with replication = { 'class':'SimpleStrategy', 'replication_factor':1 }"");
		session.execute(""drop table if exists testing.timerangetest"");
		// testing queries by time range as described in https://stackoverflow.com/questions/4667040/storing-time-ranges-in-cassandra
		//creating the table using ""i_end"" as a partition key reproduces https://issues.apache.org/jira/browse/CASSANDRA-6612
		session.execute(""create table if not exists testing.timerangetest (id text, end timestamp, i_eq_dummy blob, i_start timestamp, i_end timestamp, primary key ((id, end)))"");
		//creating the table using ""i_end"" as a cluster key reproduces https://issues.apache.org/jira/browse/CASSANDRA-6470
		//session.execute(""create table if not exists testing.timerangetest (id text, i_eq_dummy blob, i_start timestamp, i_end timestamp, primary key (id, i_end))"");

		session.execute(""create index if not exists on testing.timerangetest (i_start)"");
		session.execute(""create index if not exists on testing.timerangetest (i_end)"");
		session.execute(""create index if not exists on testing.timerangetest (i_eq_dummy)"");
		//insert some values
		session.execute(""insert into testing.timerangetest (id, end, i_eq_dummy, i_start, i_end) values ('row1', 5, 0x00, 1, 5)"");
		session.execute(""insert into testing.timerangetest (id, end, i_eq_dummy, i_start, i_end) values ('row2', 13, 0x00, 3, 13)"");
		session.execute(""insert into testing.timerangetest (id, end, i_eq_dummy, i_start, i_end) values ('row3', 17, 0x00, 12, 17)"");
		session.execute(""insert into testing.timerangetest (id, end, i_eq_dummy, i_start, i_end) values ('row4', 22, 0x00, 16, 22)"");
		session.execute(""insert into testing.timerangetest (id, end, i_eq_dummy, i_start, i_end) values ('row5', 24, 0x00, 21, 24)"");
		session.execute(""insert into testing.timerangetest (id, end, i_eq_dummy, i_start, i_end) values ('row6', 23, 0x00, 4, 23)"");
		//query for everything
		System.out.println(""all records:"");
		for (com.datastax.driver.core.Row r : session.execute(""select * from testing.timerangetest"").all()) {
			System.out.println(""  "" + r.getString(""id"") + "" @ "" + r.getDate(""i_start"").getTime() + ""-"" + r.getDate(""i_end"").getTime());
		}
		//query for records that were active between 10 and 20
		System.out.println(""active records from 10-20:"");
		for (com.datastax.driver.core.Row r : session.execute(""select * from testing.timerangetest where i_eq_dummy=0x00 and i_end >= 10 and i_start <= 20 allow filtering"").all()) {
			System.out.println(""  "" + r.getString(""id"") + "" @ "" + r.getDate(""i_start"").getTime() + ""-"" + r.getDate(""i_end"").getTime());
		}
		//query for records that were active between 10 and 20 where id='row2'
		System.out.println(""active records from 10-20 where id='row2' and end=13:"");
		for (com.datastax.driver.core.Row r : session.execute(""select * from testing.timerangetest where id='row2' and end=13 and i_eq_dummy=0x00 and i_end >= 10 and i_start <= 20 allow filtering"").all()) {
			System.out.println(""  "" + r.getString(""id"") + "" @ "" + r.getDate(""i_start"").getTime() + ""-"" + r.getDate(""i_end"").getTime());
		}
	} finally {
		if (session != null) {
			com.datastax.driver.core.Cluster c = session.getCluster();
			session.close();
			c.close();
		}
	}
}","29/Jul/14 21:27;mshuler;[~enigmacurry] let's see if we can reproduce this in 2.0/2.1 HEAD with cqlsh and the latest java-driver, as above.","01/Aug/14 18:12;philipthompson;[~mshuler] I do not see this error on 2.0 HEAD with cqlsh, but the query instead hangs forever, even with only one row of data inserted/","05/Aug/14 15:21;slebresne;I do am able to reproduce with the example of the description (which I've pushed as a dtest).

There is actually two small problem in SecondaryIndexManager. The first one is in the {{hasIndexFor}} that clearly have a bogus logic when there is more than one searcher. It's this error that made the code take the wrong path and trigger the assertion. That said, once fixed, we still have a problem: the {{search}} is unhappy because we have more than one searcher. And indeed, we've never supported more than one searcher because even if we have multiple indexes, we should still have only one searcher for all of them (though we may have other searcher for custom indexes). That ""grouping"" of indexes is done by {{getIndexSearchersForQuery}}. However, that method was grouping using the class name, which didn't result in all internal index sharing the same searcher since we have different concrete implementation depending on whether the index is on a partition key column, clustering key one or regular one. Anyway, attaching a slightly hackish but simple solution that just make sure we group all internal indexes properly in {{getIndexSearchersForQuery}} as we should. We should probably overhaul the {{SecondaryIndexManager}} class at some point because it's pretty confusing imo but that's not for this ticket.",05/Aug/14 21:01;jbellis;[~beobal] to review,"07/Aug/14 15:11;beobal;I agree that SIM is in dire need of an overhaul and because of that this is a slightly hackish solution. That aside, the patch looks good to me. Just one thing to note,  the bogus logic in SIM.hasIndexFor is already fixed in 2.1 by CASSANDRA-7525","07/Aug/14 16:37;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batch CAS does not support LOCAL_SERIAL,CASSANDRA-6837,12700738,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,nff,nff,11/Mar/14 16:04,12/Mar/19 14:17,13/Mar/19 22:29,17/Mar/14 10:17,2.0.7,,,,,,0,LWT,,,,"The batch CAS feature introduced in Cassandra 2.0.6 does not support the LOCAL_SERIAL consistency level, and always uses SERIAL.

Create a cluster with 4 nodes with the following topology:

{code}
Datacenter: DC2
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens  Owns   Host ID                               Rack
UN  127.0.0.3  269 KB     256     26.3%  ae92d997-6042-42d9-b447-943080569742  RAC1
UN  127.0.0.4  197.81 KB  256     25.1%  3edc92d7-9d1b-472a-8452-24dddbc4502c  RAC1
Datacenter: DC1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens  Owns   Host ID                               Rack
UN  127.0.0.1  226.92 KB  256     24.8%  dbc17bd7-1ede-47a2-9b31-6063752d6eb3  RAC1
UN  127.0.0.2  179.27 KB  256     23.7%  bb0ad285-34d2-4989-a664-b068986ab6fa  RAC1
{code}

In cqlsh:
{code}
cqlsh> CREATE KEYSPACE foo WITH replication = {'class': 'NetworkTopologyStrategy', 'DC1': 2, 'DC2': 2};
cqlsh> USE foo;
cqlsh:foo> CREATE TABLE bar (x text, y bigint, z bigint, t bigint, PRIMARY KEY(x,y));
{code}

Kill nodes 127.0.0.3 and 127.0.0.4:

{code}
Datacenter: DC2
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens  Owns   Host ID                               Rack
DN  127.0.0.3  262.37 KB  256     26.3%  ae92d997-6042-42d9-b447-943080569742  RAC1
DN  127.0.0.4  208.04 KB  256     25.1%  3edc92d7-9d1b-472a-8452-24dddbc4502c  RAC1
Datacenter: DC1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens  Owns   Host ID                               Rack
UN  127.0.0.1  214.82 KB  256     24.8%  dbc17bd7-1ede-47a2-9b31-6063752d6eb3  RAC1
UN  127.0.0.2  178.23 KB  256     23.7%  bb0ad285-34d2-4989-a664-b068986ab6fa  RAC1
{code}

Connect to 127.0.0.1 in DC1 and run a CAS batch at CL.LOCAL_SERIAL+LOCAL_QUORUM:

{code}
        final Cluster cluster = new Cluster.Builder()
                .addContactPoint(""127.0.0.1"")
                .withLoadBalancingPolicy(new DCAwareRoundRobinPolicy(""DC1""))
                .build();

        final Session session = cluster.connect(""foo"");

        Batch batch = QueryBuilder.batch();
        batch.add(new SimpleStatement(""INSERT INTO bar (x,y,z) VALUES ('abc', 123, 1) IF NOT EXISTS""));
        batch.add(new SimpleStatement(""UPDATE bar SET t=2 WHERE x='abc' AND y=123""));

        batch.setConsistencyLevel(ConsistencyLevel.LOCAL_QUORUM);
        batch.setSerialConsistencyLevel(ConsistencyLevel.LOCAL_SERIAL);

        session.execute(batch);
{code}

The batch fails with:

{code}
Caused by: com.datastax.driver.core.exceptions.UnavailableException: Not enough replica available for query at consistency SERIAL (3 required but only 2 alive)
	at com.datastax.driver.core.Responses$Error$1.decode(Responses.java:44)
	at com.datastax.driver.core.Responses$Error$1.decode(Responses.java:33)
	at com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:182)
	at org.jboss.netty.handler.codec.oneone.OneToOneDecoder.handleUpstream(OneToOneDecoder.java:66)
	... 21 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,14/Mar/14 13:39;slebresne;6837.txt;https://issues.apache.org/jira/secure/attachment/12634715/6837.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-14 13:39:48.534,,,no_permission,,,,,,,,,,,,379081,,,Wed Mar 19 12:30:10 UTC 2014,,,,,,0|i1tbhr:,379373,,,,,,,,jbellis,jbellis,,,,,,,,,,"14/Mar/14 13:39;slebresne;For batches at the protocol level, unfortunately the serial CL is not shipped by the protocol so this will have to be part of CASSANDRA-6855 (the fact that the java driver silently send SERIAL is a separate problem, one that concerns the java driver).

That being said, as it happens, we don't even pass the right serial CL when you use non-protocol batches, and that can be fixed trivially, so attaching a patch (this will have to do as a work-around until CASSANDRA-6855).","14/Mar/14 19:24;jbellis;+1, but can we rename to commitCL instead of serialCL?","17/Mar/14 10:16;slebresne;Committed, thanks.

bq. but can we rename to commitCL instead of serialCL?

Hum, the thing is that serialCL is the CL for the paxos phase, not the one use during the commit (it corresponds to the consistencyForPaxos argument of SP.cas(), not to consistencyForCommit), so renaming it commitCL is probably a bad idea. I'm fine renaming it to paxosCL or something though, I don't really care much (though we use serialConsistency in other places already like the native protocol, QueryOptions and in thrift/CassandraServer so if it was just me, I'd leave it be).","17/Mar/14 10:17;slebresne;Closing but as noted above, this does not solve the issue of LOCAL_SERIAL for protocol batches, that part is left to CASSANDRA-6855.","19/Mar/14 04:11;jbellis;can we use paxosCL and commitCL then?

i don't like ""serialCL"" because serial is itself a CL value","19/Mar/14 10:46;slebresne;At the risk of sounding obtuse, we use ""serial_consistency"" both in thrift and in the native protocol spec. And truth is, while maybe not perfect, I do think it's better than paxos_consistency because that would be leaking implementation details and we've always said that the use of Paxos was an implementation detail we didn't wanted to leak. It also happen that I don't really see the problem with serial_consistency. The comment in cassandra.thrift describe it this way
{quote}
The first one, serial_consistency_level, simply indicates the level of serialization required. This can be either ConsistencyLevel.SERIAL or ConsistencyLevel.LOCAL_SERIAL
{quote}
and to me that sounds relatively sensible. Of course, we can call it ""serial consistency"" externally but ""paxos consistency"" internally, but that feels a bit inconsistent for no good reason.

Anyway, all this to say that imo serial_consistency is better than paxos_consistency at least externally because it doesn't leak implementation details, and that it follows to me that there is no point in making it different internally. Nor do I think that serial_consistency is so bad that we should bother finding something else. Those arguments and opinion being made, if you still think it's worth renaming, I won't fight it, and feel free to go ahead. It's orthogonal to this issue however, the patch here didn't introduced the serial_consistency naming.",19/Mar/14 12:30;jbellis;fair enough,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress (2.1) spams console with java.util.NoSuchElementException when run against nodes recently created,CASSANDRA-6848,12701120,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,benedict,rhatch,rhatch,12/Mar/14 22:28,12/Mar/19 14:17,13/Mar/19 22:29,14/Mar/14 00:52,2.1 beta2,,,,,,0,,,,,"I don't get any stack trace on the console, but I get two java.util.NoSuchElementException for each operation stress is doing.

This seems to occur when stress is being run against a recently created node (such as one from ccm).

To reproduce: create a ccm cluster, and run stress against it within a few minutes . Run a simple stress command like cassandra-stress write n=10 .",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-03-12 22:44:03.759,,,no_permission,,,,,,,,,,,,379466,,,Fri Mar 14 00:52:52 UTC 2014,,,,,,0|i1tdun:,379757,,,,,,,,xedin,xedin,,,,,,,,,,"12/Mar/14 22:34;rhatch;I should also mention that stress does still report statistics to the console, they are just buried in the error output. It would be helpful to know if the errors are impacting the performance stats or if they can safely be ignored.","12/Mar/14 22:44;jbellis;FTR I'm fine with saying ""2.1 stress requires 2.1 server"" but it should fail more gracefully than that.","12/Mar/14 22:53;brandon.williams;bq. I'm fine with saying ""2.1 stress requires 2.1 server""

I'm fine with that too if it comes to it, but when you're regularly using it against multiple branches with a real cluster, it's kind of nice to just have one build that works against all of them, which is how it's been in the past.  So if possible, I'd like to retain that behavior.","12/Mar/14 23:28;benedict;A quick update: Russ can only reproduce on one cluster, and I cannot reproduce locally, so going to introduce CASSANDRA-6849 as it seems the easiest way to track it down, and it's likely to be helpful in future anyway.

No need to worry about backwards compatibility. I've used it on 2.0 plenty, it definitely _can_ work. We'll figure out why it isn't here.","12/Mar/14 23:37;benedict;On a hunch, tried ""-mode thrift"" (i.e. with smart routing disabled) on the vague idea that it might be grabbing the topology/endpoint data before the cluster is ready to deliver it fully (or that maybe 2.0 can't). It fixes the problem.

So not sure exactly the cause, but it's probably not a major issue. If it's a race on cluster startup, it's probably not a big deal, but if it's an inherent limitation with 2.0 we should probably detect that and let the user know. Either way, should fail out more gracefully whatever the case.","13/Mar/14 01:38;rhatch;on second thought this doesn't seem to be purely timing as I suggested in the description above. I let my nodes wait for 10 minutes before sending stress commands and still got the flood of NoSuchElementException's. This is the command that triggered the problem after a 10 minute wait:
{noformat}
'cassandra-stress counterwrite n=5000000 CL=ONE -key populate=1..10000 -col n=FIXED\(1\) -schema replication\(factor=3\) -rate threads=50'
{noformat}
","13/Mar/14 11:06;benedict;Can you reproduce this consistently? Does this only occur on the remote cluster, or also when you run locally with ccm?","13/Mar/14 18:18;rhatch;happens every time on my austin test cluster (stress-2.1 targeting various versions)

happens more sporadically locally, but once it happens for a given cluster it seems to happen every time.

I'm getting it to reproduce today with stress from 2.1, and a ccm cluster built from 2.1. Happens using the stress counterwrite command I mentioned two comments above this one.","13/Mar/14 18:20;rhatch;To add 2 cents to the discussion above, it will be a great help if we can expect stress-2.1 to work against 2.0 clusters so we can make meaningful comparisons between these versions (otherwise it's going to be apples to oranges).

But I'm getting this to happen with stress-2.1 targeting a 2.1 cluster as well, so I think version may have nothing to do with it (or maybe two different problems are manifesting as the exceptions output to the console).","13/Mar/14 18:56;rhatch;I was able to isolate this a little bit more. Looks like if the first stress command I run is the 'counterwrite' above, then the cluster is no good for running stress after that (for even just a simple write n=10). Seems like there's some state being preserved by stress or the cluster since the outcome of a previous command is having an impact on subsequent runs.

However, if I run a simple write test first (write n=10), then follow with my counterwrite test the problem seems to go away.","13/Mar/14 19:04;benedict;Weird. Stress retains no state. Can you try running stress from the branch I uploaded for CASSANDRA-6849, and passing ""-log level=verbose"" ?","13/Mar/14 19:42;rhatch;here's the exception printed from your branch with verbose logging:
{noformat}
java.util.NoSuchElementException
	at com.google.common.collect.AbstractIndexedListIterator.next(AbstractIndexedListIterator.java:82)
	at org.apache.cassandra.stress.util.SmartThriftClient.get(SmartThriftClient.java:117)
	at org.apache.cassandra.stress.util.SmartThriftClient.batch_mutate(SmartThriftClient.java:137)
	at org.apache.cassandra.stress.operations.ThriftCounterAdder$1.run(ThriftCounterAdder.java:77)
	at org.apache.cassandra.stress.Operation.timeWithRetry(Operation.java:251)
	at org.apache.cassandra.stress.operations.ThriftCounterAdder.run(ThriftCounterAdder.java:72)
	at org.apache.cassandra.stress.StressAction$Consumer.run(StressAction.java:304)
{noformat}","13/Mar/14 20:00;benedict;[Patch|https://github.com/belliottsmith/cassandra/tree/iss-6848]

Was a bug when round-robining for smart routing, which is why disabling smart routing works. Not sure why it would _ever_ work for a RF3 cluster though. Basically, hadn't been run against RF>1 before I don't think.","13/Mar/14 20:05;benedict;[~xedin] this tree includes all of the minor stress modifications made recently. The last two mostly just a few lines, so if you're reviewing CASSANDRA-6824, might as well just look here.","14/Mar/14 00:28;xedin;Looks like I'm having a stress review day today, so I took a look at this and CASSANDRA-6849 and both look fine so if [~rhatch] gives a green light on this and I can commit both - this fix + 6849.",14/Mar/14 00:32;rhatch;I was able to run my performance tests today with [~benedict]'s branch iss-6848 and the issue was gone.,14/Mar/14 00:38;benedict;Thanks [~xedin]!,14/Mar/14 00:52;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"2.0.{5,6} node throws EOFExceptions on the Row mutation forwarding path during rolling upgrade from 1.2.15.",CASSANDRA-6840,12700927,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,fpiccinini,fpiccinini,12/Mar/14 10:27,12/Mar/19 14:17,13/Mar/19 22:29,14/Mar/14 17:55,2.0.7,,,,,,0,,,,,"During a rolling upgrade from 1.2.15 to 2.0.5 nodes running on 2.0.5 throw an EOFException:
{noformat}
ERROR [MutationStage:12] 2014-03-12 09:46:35,706 RowMutationVerbHandler.java (line 63) Error in row mutation
java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:197)
	at org.apache.cassandra.net.CompactEndpointSerializationHelper.deserialize(CompactEndpointSerializationHelper.java:37)
	at org.apache.cassandra.db.RowMutationVerbHandler.forwardToLocalNodes(RowMutationVerbHandler.java:81)
	at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:49)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:60)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{noformat}

In this specific context we have a setup with 3 datacenters, 3 nodes in each datacenter, NetworkTopologyStrategy as placement_strategy with 3 replicas in each DC. We noticed the issue on the only 2.0.5 node in the ring. All nodes run on Java7. We have tried to upgrade the node on 2.0.5 to 2.0.6 but that didn't solve the issue.

At a first glance it seems that the size of the size of the list of forward addresses in  org.apache.cassandra.db.RowMutationVerbHandler.forwardToLocalNodes() in inconsistent with the length of the InputStream, which causes the deserializer to try and read after the end of the InputStream.",,,,,,,,,,,,,,,,,,,,,,,,14/Mar/14 12:41;krummas;0001-Read-id-properly-from-older-versions.patch;https://issues.apache.org/jira/secure/attachment/12634705/0001-Read-id-properly-from-older-versions.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-14 17:11:24.212,,,no_permission,,,,,,,,,,,,379273,,,Fri Mar 21 21:48:59 UTC 2014,,,,,,0|i1tcnz:,379565,,,,,,,,jbellis,jbellis,,,2.0.0,,,,,,,14/Mar/14 17:11;jbellis;+1,"14/Mar/14 17:55;krummas;only committed to 2.0, not 2.1 since we require 2.0 before upgrading to 2.1","21/Mar/14 21:48;jjordan;Adding some follow up here for anyone hitting this issue.  Cross DC forwarding from C* 2.0.0-2.0.6 to C* 1.2.X is broken.  If you have multiple DC's and you already pushed through an upgrade, you will want to run repair to make sure everything is in sync.  Hinted Handoff should take care of the messed up forwards, but better safe than sorry, so I would run a repair.

Also EACH_QUORUM and ALL writes will fail during the upgrade.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition silently kills thrift server,CASSANDRA-6788,12698120,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,ccrolf,ccrolf,ccrolf,01/Mar/14 15:02,12/Mar/19 14:17,13/Mar/19 22:29,07/Jun/14 13:22,1.2.17,2.0.7,2.1 beta2,,,,0,,,,,"There's a race condition in CustomTThreadPoolServer that can cause the thrift server to silently stop listening for connections. 

It happens when the executor service throws a RejectedExecutionException, which is not caught.
 
Silent in the sense that OpsCenter doesn't notice any problem since JMX is still running fine.",,,,,,,,,,,,,,,,,,,,,,,,01/Mar/14 21:06;jbellis;6788-v2.txt;https://issues.apache.org/jira/secure/attachment/12631970/6788-v2.txt,03/Mar/14 18:27;ccrolf;6788-v3.txt;https://issues.apache.org/jira/secure/attachment/12632322/6788-v3.txt,13/Mar/14 17:59;jbellis;6793-v3-rebased.txt;https://issues.apache.org/jira/secure/attachment/12634495/6793-v3-rebased.txt,01/Mar/14 15:02;ccrolf;race_patch.diff;https://issues.apache.org/jira/secure/attachment/12631951/race_patch.diff,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2014-03-01 18:22:46.36,,,no_permission,,,,,,,,,,,,376589,,,Mon Mar 21 11:58:59 UTC 2016,,,,,,0|i1sw5z:,376884,1.2.16,2.0.4,2.0.5,,,,,jbellis,jbellis,,,,,,,,,,"01/Mar/14 18:22;jbellis;I don't understand how (a) a REE will ""cause the thrift server to silently stop listening for connections,"" nor (b) how closing transports fixes it.  Note that the executor is always a ThreadPoolExecutor so a dead worker thread will be replaced automatically.","01/Mar/14 19:53;ccrolf;Sorry, I should've been a more specific; this happens when the number of RPC threads is limited. We've been running a ring of 12 nodes with 2048 as max RPC threads for over a year without problems, but the past week we've been getting zombie nodes almost every day.

Basically, the active thread counter is decremented at line 216 (pre-patch) of CustomTThreadPoolServer.java, this can end the waiting loop at line 98. If a new connection is made before the run-method of old thread has completed, the execute() command at line 108 can cause a RejectedExecutionException.","01/Mar/14 21:06;jbellis;I see.  But that doesn't eliminate the window for a race, just reduces it.  (TPE.runWorker still needs to call afterExecute and do its own bookkeeping.)  v2 adds an explicit catch for REE.  This is better than dying, but it will accept connections and then drop them on the floor if necessary which is obviously sub optimal.  Moral is not to push right up to the edge of max connections. :)","03/Mar/14 18:27;ccrolf;True, given that the ThreadPoolExecutor.afterExecute is a noop, the exception should be rare. 

Here's an alternative solution...in the hope that you prefer overrides to factories and extra exception handling as much as I do :-)","13/Mar/14 17:58;jbellis;Hmm, doesn't this mean we're back to dying ignominiously if we still happen to get a REE?  I would prefer v2 for that reason.",13/Mar/14 17:59;jbellis;(attaching my rebase of v3 for posterity),"17/Mar/14 13:14;ccrolf;Having read through the (somewhat horrendous) code of java.util.concurrent.ThreadPoolExecutor, I agree whole-heartedly with your version. It's the only way to be completely safe from the race; the contract for afterExecute simply isn't clear enough to rely on.
I see I put myself as the assignee, what do I need to do to subtmit the patch?",17/Mar/14 15:52;jbellis;committed,"28/May/14 21:46;brandon.williams;This can happen on 1.2 as well, so we should backport this fix there.",28/May/14 21:49;kohlisankalp;+1,29/May/14 11:22;ccrolf;Thanks for reporting this. Looks like the patch (v2) is exactly the same for 1.2. ,"05/Jun/14 01:33;vmallet;+1 on the port to 1.2, we're hoping to grab your patch as soon as you feel comfortable with it and commit it for 1.2.17.
","05/Jun/14 16:20;brandon.williams;v2 Just Applies to 1.2, so if you're cool with that [~jbellis] let's just commit it there.",07/Jun/14 13:22;jbellis;committed to 1.2,"21/Mar/16 11:58;dpinol;With both Cassandra 1.2.19 and 2.2.5 (which should contain the patch) I can experience a similar problem, with both OSX and linux. I use thrift with scale7-pelops 1.3-1.1.x. This is my pseudocode. I use dynamic columns.
{noformat}
for(column=1..1000) 
{
  for(value=1..25) 
  {
    write(""CF1"", ""key1"", column, writtenValue);
    readValue = read(""CF1"", ""key1"", column);
    some times here readValue!=writtenValue. Once this happens, sleeping and reading again does not help  
  }
}
{noformat}
The only alternative ways to avoid the problem are:
* inserting a sleep (any duration) right after the put.
* replacing thrift with CQL
* This sounds crazy, but each value contains the previous one as prefix (1, 12, 123, 1234...) it never fails. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If null is explicitly set to a column, paging_state will not work",CASSANDRA-6748,12696454,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,na-ga,na-ga,21/Feb/14 03:09,12/Mar/19 14:17,13/Mar/19 22:29,25/Feb/14 09:46,2.0.6,,,,,,0,,,,,"If null is explicitly set to a column, paging_state will not work. My test procedure is as follows:

------
Create a table and insert 10 records using cqlsh. The query is as follows:

{code}
CREATE TABLE mytable (id int, range int, value text, PRIMARY KEY (id, range));
INSERT INTO mytable (id, range) VALUES (0, 0);
INSERT INTO mytable (id, range) VALUES (0, 1);
INSERT INTO mytable (id, range) VALUES (0, 2);
INSERT INTO mytable (id, range) VALUES (0, 3);
INSERT INTO mytable (id, range) VALUES (0, 4);
INSERT INTO mytable (id, range, value) VALUES (0, 5, null);
INSERT INTO mytable (id, range, value) VALUES (0, 6, null);
INSERT INTO mytable (id, range, value) VALUES (0, 7, null);
INSERT INTO mytable (id, range, value) VALUES (0, 8, null);
INSERT INTO mytable (id, range, value) VALUES (0, 9, null);
{code}

Select 10 records using datastax driver. The pseudocode is as follows:

{code}
Statement statement = QueryBuilder.select().from(""mytable"").setFetchSize(1);
ResultSet rs = session.execute(statement);
for(Row row : rs){
    System.out.println(String.format(""id=%d, range=%d, value=%s"",
        row.getInt(""id""), row.getInt(""range""), row.getString(""value"")));
}
{code}

The result is as follows:

{code}
id=0, range=0, value=null
id=0, range=1, value=null
id=0, range=2, value=null
id=0, range=3, value=null
id=0, range=4, value=null
id=0, range=5, value=null
id=0, range=7, value=null
id=0, range=9, value=null
{code}
------

Result is 8 records although 10 records were expected. I originally raised this issue in the mailing lists: http://www.mail-archive.com/user@cassandra.apache.org/msg34752.html","Cassandra 2.0.5
Ubuntu 12.04",,,,,,,,,,,,,,,,,,,,,,,24/Feb/14 09:02;slebresne;6748.txt;https://issues.apache.org/jira/secure/attachment/12630635/6748.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-24 09:02:33.666,,,no_permission,,,,,,,,,,,,374930,,,Tue Feb 25 09:46:42 UTC 2014,,,,,,0|i1slyv:,375229,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"24/Feb/14 09:02;slebresne;Paging requests a new page starting from the last cell of the previously returned result. Then, when it gets the new page, it checks if it starts with the previous last cell (likely, though not guaranteed), and if it does, it discards it (since it has been returned already). The problem is that the discarding part removes the first live CQL row, which is ok is that ""previous last cell"" is live, but ends up discarding the first ""valid"" row otherwise and that's the case here.

Simplest fix is probably just to make the check of whether the new page contains the previous last cell return false if said cell is deleted, since the rest of the paging code ignore deleted cells anyway, and attaching patch to do that (with a unit test).
","25/Feb/14 02:14;iamaleksey;LGTM, +1.","25/Feb/14 09:46;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"cqlsh DESCRIBE KEYSPACE returns ""'NoneType' object has no attribute 'get_usertypes_names'""",CASSANDRA-6741,12696082,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mishail,enigmacurry,enigmacurry,19/Feb/14 21:36,12/Mar/19 14:17,13/Mar/19 22:29,19/Feb/14 22:45,2.1 beta2,,,,,,0,,,,,"Start a fresh cluster on trunk and try to describe any keyspace :

{code}
ccm create -v git:trunk test
ccm populate -n 1
ccm start
ccm node1 cqlsh

cqlsh> DESCRIBE KEYSPACE system;

CREATE KEYSPACE system WITH replication = {
  'class': 'LocalStrategy'
};
'NoneType' object has no attribute 'get_usertypes_names'
{code}",,,,,,,,,,,,,,,,,,,,,,,,19/Feb/14 22:33;mishail;CASSANDRA-2.1-6741.patch;https://issues.apache.org/jira/secure/attachment/12629912/CASSANDRA-2.1-6741.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-19 22:33:58.201,,,no_permission,,,,,,,,,,,,374559,,,Wed Feb 19 22:45:40 UTC 2014,,,,,,0|i1sjon:,374859,2.1 rc3,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,19/Feb/14 22:33;mishail;Patch to return an empty object instead of None,19/Feb/14 22:37;brandon.williams;+1,19/Feb/14 22:45;mishail;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ColumnFamily opening race,CASSANDRA-5350,12637068,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,yukim,yukim,14/Mar/13 18:45,12/Mar/19 14:17,13/Mar/19 22:29,14/Mar/13 21:02,1.1.11,1.2.4,,,,,0,,,,,"(Moving from CASSANDRA-5151)

Currently, MeteredFlusher is scheduled inside static block of ColumnFamilyStore,  and it accesses all ColumnFamilyStore when it runs every 1 sec. Scheduling is done when JVM first load ColumnFamilyStore class, so after that, there is always a chance to open SSTables before doing scrub directory/remove compaction left overs.
We should move the content of static block at the end of CassandraDaemon setup.",,,,,,,,,,,,,,CASSANDRA-5469,,,,,,,,,,14/Mar/13 18:46;yukim;0001-move-scheduling-MeteredFlusher-to-CassandraDaemon.patch;https://issues.apache.org/jira/secure/attachment/12573749/0001-move-scheduling-MeteredFlusher-to-CassandraDaemon.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-14 19:22:37.368,,,no_permission,,,,,,,,,,,,317560,,,Thu Mar 14 21:02:19 UTC 2013,,,,,,0|i1isn3:,317901,,,,,,,,jbellis,jbellis,,,,,,,,,,14/Mar/13 18:46;yukim;Patch for 1.2 branch attached.,"14/Mar/13 18:48;yukim;Patch is for 1.2, but 1.1.x has the same problem, so maybe it is good to patch that as well.",14/Mar/13 19:22;jbellis;LGTM. +1 for 1.1.x and 1.2.x.,14/Mar/13 21:02;yukim;Committed to 1.1 branch and above,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Perform size-tiered compactions in L0 (""hybrid compaction"")",CASSANDRA-5371,12638256,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,21/Mar/13 15:22,12/Mar/19 14:17,13/Mar/19 22:29,08/Apr/13 18:24,2.0 beta 1,,,,,,0,lcs,,,,"If LCS gets behind, read performance deteriorates as we have to check bloom filters on man sstables in L0.  For wide rows, this can mean having to seek for each one since the BF doesn't help us reject much.

Performing size-tiered compaction in L0 will mitigate this until we can catch up on merging it into higher levels.",,,,,,,,,,,,,,,,,,,,,,,,21/Mar/13 20:52;tjake;HybridCompactionStrategy.java;https://issues.apache.org/jira/secure/attachment/12574880/HybridCompactionStrategy.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-21 19:32:33.519,,,no_permission,,,,,,,,,,,,318732,,,Sun Aug 30 16:23:25 UTC 2015,,,,,,0|i1izvj:,319073,,,,,,,,tjake,tjake,,,,,,,,,,"21/Mar/13 19:32;jjordan;I could see this possibly helping if you left the non L0 alone and went to STCS for L0 until the write load stopped. But you have to come up with a heuristic to know when to shut off LCS.  So a cluster with a periodic write load, which was too high for LCS's increased IO needs, would revert to STCS of L0 only until the load dropped.  You would then have to play catchup shoving all that L0 data into the other levels. I could see use cases where this would be useful, such as periodic large data dumps into a cluster. You would have to be careful that there was enough down time between dumps for LCS to catchup.",21/Mar/13 20:52;tjake;This initial version puts the newly flushed memtables into a queue and when there are 4 it size tiers them.  So you get 1/4 the sstables in L0.,"03/Apr/13 22:17;jbellis;Alternate implementation pushed to http://github.com/jbellis/cassandra/commits/5371 with the following improvements:

- Only applies STCS to L0 if L0 gets behind (defined as ""accumulates more than MAX_COMPACTING_L0 sstables"")
- Performs true STCS, rather than ""compact in sets of four and then never again""","04/Apr/13 02:02;tjake;Oh good, this is what I wanted the implementation to end up being.

In LeveledManifest.getCompactionCandidates:

I think there is a bug in the size tier candidate checks.  You seem to be size tiering across all the non-compacting sstables and not the level0 ones.  I think you mean't to intersect the level0 sstables with the non-compacting ones.  You should also add a check after that to make sure the non-compacting level0 sstables are still > MAX_COMPACTING_L0

Also, the code only checks for STCS when a higher level is ready to be compacted.  Maybe move this to the top before the higher level checks. We know the higher levels are seek bounded but the code should try to keep up with level 0 flushes as much as possible.","04/Apr/13 02:33;jbellis;bq. I think you mean't to intersect the level0 sstables with the non-compacting ones

Right.  Fix pushed.

bq. the code only checks for STCS when a higher level is ready to be compacted

The idea is, we'd prefer to do normal LCS compaction on L0.  So if the higher levels are okay, we'll treat L0 the same as before.  But if we do need to compact a higher level, we'll first check and see if L0 is far enough behind that we should do an STCS round there as a stop-gap.

bq. You should also add a check after that to make sure the non-compacting level0 sstables are still > MAX_COMPACTING_L0

I think it's more correct as written -- basically, we're doing L0 out-of-turn, since for max throughput we'd do the higher level next.  So, we'll do L0 STCS until it's under MCL0, then we'll go back to the higher levels until we catch up and can actually apply leveling to L0.","04/Apr/13 11:07;slebresne;For my own curiosity, do we have performance numbers for this (including, not only on SSD tests)?

A priori, I'm not fully sold on this being always a win (or even most of the time of ""L0 is being""). That is, I understand the reasoning that lots of SSTables in L0 is bad for reads, but at the same time, if you compact STCS things in L0, a lot of the work you've done you will redo when you compact your now bigger L0 sstable against L1. I.e. those STCS compactions don't help you make progress as far as leveling is concerned, so it seems like it waste work overall. Besides, in theory, our LCS is supposed to be able to compact large amount of L0 sstables into L1 to help with the ""I'm behind on L0 but it's just a pike in load"". Now I guess if you've pushed a lot of data in L1 and get behind again in L0, then it's not fun because all of L1 need to be including in L0 compaction. But if you are constantly behind, doesn't that mean you have bigger problems (and/or that you should just use STSC)?

Basically I wonder if there won't be a number of scenario where because you get a bit behind on L0 once, then the I/O you ""waste"" doing STSC in L0 will help you get even more and more behind on your leveling and you'd end up doing mostly STSC, while letting LCS do its job would have been fine overall.

That is, I'm happy with this if that makes things clearly better in practice more often than not, it's just that intellectually it's not obvious to me that it's the case (note that I'm not saying that it's obvious it's a bad idea either).

","04/Apr/13 12:02;tjake;[~jbellis] let me run some tests but the code looks good now.


[~slebresne]
bq. do we have performance numbers for this (including, not only on SSD tests)?

We only have SSD and I think LCS only ever makes sense on SSD.  If we want to support HDD then I agree this is def more IO overall.  The performance numbers for our use case went from all read timeouts using LCS to reads rarely timing out with the original patch.  The stress tool doesn't have a wide row scenario so it's hard to simulate out of the box.  

bq. doesn't that mean you have bigger problems (and/or that you should just use STSC)

You are right, this does require writes die down at somepoint otherwise you end up with STCS. ellis mentions this in the comments.

STCS isn't viable for the LCS use cases.  I don't see how having this (on SSD) would not help all LCS use cases since LCS is for wide row or heavy updates. The point of this is to avoid the situation where all sstables in L0 contain a portion of the row which requires reading them all.   One thing to keep in mind is if you do have a wide row and you end up with a STCS compacted row of 10MB and LCS has a 5MB limit you still end up with a 10MB sstable with a single row in it so the higher levels do benefit from STCS in this case.","04/Apr/13 12:51;slebresne;bq. The stress tool doesn't have a wide row scenario so it's hard to simulate out of the box

Agreed, and that's definitively lacking. I believe there is a few knobs that allow to do wideish rows, but that's probably not very realistic.

{quote}
I think LCS only ever makes sense on SSD
since LCS is for wide row or heavy updates
{quote}

I'm not sure I agree. Maybe there is some truth to it in our current implementation, but that would then be more of a quirk of the implementation that the goal. Typically, I'm not really sure why only wide rows would benefit it. There is certainly nothing in theory that makes it so. As for ""it's for heavy updates only"", I think that LCS has a number of nice properties (like avoiding huge files that require half of you disk in free space) that are nice even if you have a moderate to low update rate (and in that case you can definitively afford LCS on HDD). More concretely, I'm pretty sure we have tons on users on LCS on HDD.

Anyway, all this to say that I don't necessary agree on optimizing LCS for heavy writes + wide rows + SSD *if* that's done at the expense of all other type of workload (and I'm not saying that's what this patch is doing, just that discarding other type of workload as unimportant is not ok imo).

bq. LCS has a 5MB limit you still end up with a 10MB sstable with a single row

If having 10MB sstables being split due to row too wide is a problem, then you should either not use LCS or pick a 10MB limit for LCS, not 5MB.

Anyway, I'm not vetoing this or anything like this. Just trying to get a better understanding of why this is a good thing to do in general.","04/Apr/13 13:05;jbellis;bq. Basically I wonder if there won't be a number of scenario where because you get a bit behind on L0 once, then the I/O you ""waste"" doing STSC in L0 will help you get even more and more behind on your leveling

That's exact;y the case, which is why we only apply STCS to L0 when it's fairly badly behind, i.e., we can conclude two things:

# if the current workload continues, it's not going to magically catch up any time soon
# reads are starting to get into trouble

Note that #2 will cause a vicious cycle, slowing down compaction in turn.

So while I can hypothesize workloads that burst just long enough to cause STCS to kick in before stopping, thus ""wasting"" iops, I think for the vast majority this is a good ""safety valve,"" and specifically not worth adding a config option to disable.

However, I do think it's worth creating a ticket to allow STCS config options to be applied to the size-tiering done by LCS, and specifically allow configuring MAX_COMPACTION_L0 via the max sstables threshold, which I think may adequately address your concern.","08/Apr/13 15:05;tjake;I'm going to test this out to show how it helps our workload.  

In the meantime I think this is fine to commit for 2.0 if you'd like to get it in now.","08/Apr/13 18:24;jbellis;All right.  Rebased and committed, and created CASSANDRA-5439 for the options application.","03/May/13 15:36;rbranson;Is this just waiting on [~tjake]'s test to backport to 1.2? 

Yesterday we bootstrapped our first new node on our first LCS cluster where each node only had ~50GB of data, and it took 6 hours to complete the bootstrap, even after running the CPUs hot by bumping compaction throughput up to 64MB. We probably could have stood to raise this to 128MB/sec and pegged them, but I dread to think of what this would be like if we moved some larger, read-heavy data sets to Cassandra under LCS. Jake seems to think this patch will help with that.

http://i.imgur.com/LpdAKyc.png
http://i.imgur.com/ZsgEB9G.png

This is on an EC2 hi1.4xlarge, which is a 16-core box w/60GB RAM, 2TB of SSD storage, and 10GigE. 

We also have a cluster of m1.xlarges (4-core, 15G, 2TB rust) each with ~300GB of relatively cold data under STCS. Considering the spinning rust cluster w/1GigE and 16MB/s compaction throughput can bootstrap a new node in < 2 hours with 6x as much data we will definitely be trying this HCS on the SSD cluster running LCS at the moment.","03/May/13 15:46;jbellis;I'm not backporting this to a stable release.  It's a lot more involved than the 4KB proof of concept.  You can probably collaborate w/ Jake on a 1.2-appropriate alternative though.

That said, the main benefit of this is not that it magically makes LCS faster (it doesn't), but that when it does get behind your reads don't suffer so much.","03/May/13 15:58;tjake;Right, it takes a long time but it will keep reads happier.

Why do you need LCS on your dataset Rick? is it a wide row?","26/Dec/13 14:21;br1985;Hi,

We hit the same bug in production recently. We walked around it by switching to STCS for a few days, letting it stabilize and then going back to LCS. Quite long, but fully successful trip.

In our case we have a lot of sstables at L0 as a result of migration. Because of another bug in sstableloader (CASSANDRA-6527), we finally ended up simply copying all sstable files from the old cluster to the new one.

After the migration we had over 10k sstables (160MB per file) on each node. Of course, STCS-fallback activates automatically in that case.

I wonder if similar situation will happen after the classic bootstrap? Will streaming during bootstrapping put sstables at L0 or at the original level?

If it will put them all at L0 then I'm not sure if falling back to STCS is the best way to handle the situation. I've read the comment in the code and I'm aware why it is a good thing to do if we have to many sstables at L0 as a result of too many random inserts. We have a lot of sstables, each of them covers the whole ring, there's simply no better option. 

However, after the bootstrap situation looks a bit different. The loaded sstables already have vary small ranges! We just have to tidy up a bit and everything should be OK. STCS ignores that completely and after a while we have a bit less sstables but each of them covers the whole ring instead of just a small part. I believe that in that case letting LCS do the job is a better option that allowing STCS mix everything up before.

Is there a way to disable STCS fallback? I'll be glad to test this option the next time we do similar operation.
","25/Jan/14 03:20;ravilr;+1 on [~br1985 ] comment. 
Even during dead node replace (using replace_address), streaming puts all sstables in L0. 2.0.x switches to STCS, in doing so, also creates larger sstables, which means more free disk space to be left, in order for them to be compacted later into higher levels. LCS is known to lower the amount of free disk space (headroom) needed for compaction. this is no more true with LCS in above scenarios.
Is there a way to disable STCS fallback, please?
","25/Jan/14 11:46;br1985;Ravi, could you confirm that streaming puts all tables in L0? In that case I think we should open a separate issue instead of commenting on a closed one.",25/Jan/14 16:54;brandon.williams;They have to go to L0 since preserving the level across machines doesn't make any sense.  Please do open a new issue.,26/Jan/14 16:24;br1985;I've just created CASSANDRA-6621 describing the issue from the last comments.,"30/Aug/15 16:23;deag;I'm benchmarking our solution that uses LCS with C* 2.1.8 and we have the scenario here by [~jjordan] every week we need to burst C* with a batch job that takes 18-20h. The system behaves very well dureing the whole week, but during that time degrades considerably on the highest percentiles (>p99) were we get reads rocketing to latencies > 200ms. 

I'm working on understanding what is happening with the system since I don't see IO or CPU exhausted. Wonder what is the usual rate of compaction with LCS. In my system log I see that compaction tasks are only doing 2-3MB/s

{code}
INFO  [CompactionExecutor:93] 2015-08-30 17:06:22,114  CompactionTask.java:274 - Compacted 9 sstables to [/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20266,/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20274,/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20280,/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20286,/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20291,/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20297,/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20302,/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20307,].  1,383,544,305 bytes to 1,335,157,959 (~96% of original) in 450,225ms = 2.828154MB/s.  153,450 total partitions merged to 130,214.  Partition merge counts were {1:106978, 2:23236, }
{code}

Is this normal? ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Guava should be bumped to 13.0.1 in maven dependency declaration.,CASSANDRA-5364,12637890,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,xedin,xedin,20/Mar/13 04:28,12/Mar/19 14:17,13/Mar/19 22:29,20/Mar/13 21:42,1.2.4,,,,,,0,,,,,"Otherwise following error accours because generated pom says 12.0 and RateLimiter was introduced by 13.0

{noformat}
java.lang.NoClassDefFoundError: com/google/common/util/concurrent/RateLimiter
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:316)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:252)
        at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:89)
        at org.apache.cassandra.db.HintedHandOffManager$4.runMayThrow(HintedHandOffManager.java:459)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:679)
Caused by: java.lang.ClassNotFoundException: com.google.common.util.concurrent.RateLimiter
        at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:321)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:266)

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,20/Mar/13 04:28;xedin;CASSANDRA-5364.patch;https://issues.apache.org/jira/secure/attachment/12574486/CASSANDRA-5364.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-20 09:25:44.576,,,no_permission,,,,,,,,,,,,318370,,,Wed Mar 20 21:42:30 UTC 2013,,,,,,0|i1ixn3:,318711,,,,,,,,jbellis,jbellis,,,,,,,,,,20/Mar/13 09:25;slebresne;+1,20/Mar/13 21:42;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Clean up ColumnFamily, ISortedColumns heirarchy",CASSANDRA-5403,12639839,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,29/Mar/13 16:22,12/Mar/19 14:17,13/Mar/19 22:29,02/Apr/13 23:01,2.0 beta 1,,,,,,0,,,,,"CF wraps ISC but adds no real functionality of its own (post-supercolumn cleanup).

This means that we're wasting allocations every time we create a CF/ISC pair when really one would do.

Other things that would be nice to clean up:
# We often create an empty CF as a placeholder that should not be modified, and rely on convention to avoid such modification.  We could enforce this with a new CF/ISC subclass.
# Many places still use TMBSC where ABSC would be adequate and cheaper
# Other places still (ModificationStatement; others?) would be fine using a column container with no sorting requirement at all",,,,,,,,,,,,,,,,,,,,,,,,01/Apr/13 17:01;jbellis;Screen Shot 2013-04-01 at 12.00.09 PM.png;https://issues.apache.org/jira/secure/attachment/12576398/Screen+Shot+2013-04-01+at+12.00.09+PM.png,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-02 22:45:06.094,,,no_permission,,,,,,,,,,,,320308,,,Tue Apr 02 23:01:31 UTC 2013,,,,,,0|i1j9lr:,320649,,,,,,,,carlyeks,carlyeks,,,,,,,,,,"29/Mar/13 16:25;jbellis;Patchset pushed to https://github.com/jbellis/cassandra/tree/5403.  (To save me rebase pain, it builds on the patches for CASSANDRA-5395, which are not yet committed, but you don't need to re-review those.  So the first commit to review here is 20ebbe37867820e83f0fef63f95fc0cb9e7982ea: make getColumnNames return Iterable.)",01/Apr/13 16:18;jbellis;Rebased on top of trunk (post-5395-commit) and fixed some test errors: https://github.com/jbellis/cassandra/commits/5403-2,"01/Apr/13 16:50;jbellis;According to YourKit, this takes the time in CF.addColumn from 50% of mutationsForKey to 10%.",01/Apr/13 17:01;jbellis;Most of the rest of the time is taken up by CompositeType construction. :-|,"02/Apr/13 22:45;carlyeks;LGTM; ship it :)

For reference, merged in trunk and pushed to https://github.com/carlyeks/cassandra/tree/5403.",02/Apr/13 23:01;jbellis;Rebased + committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE during cql3 select with token(),CASSANDRA-5404,12639852,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,29/Mar/13 18:00,12/Mar/19 14:17,13/Mar/19 22:29,02/Apr/13 17:24,1.2.4,,,,,,0,,,,,"A query such as: select * from ""Standard1"" where token(key) > token(int(3030343330393233)) limit 1;

Produces:


{noformat}
 WARN 17:53:44,448 Inputing CLQ3 blobs as strings (like key = '') is now deprecated and will be removed in a future version. You should convert client code to use a blob constant (key = 0x) instead (see http://cassandra.apache.org/doc/cql3/CQL.html changelog section for more info).
ERROR 17:57:52,312 Error occurred during processing of message.
java.lang.NullPointerException
        at org.apache.cassandra.cql3.functions.FunctionCall$Raw.isAssignableTo(FunctionCall.java:135)
        at org.apache.cassandra.cql3.functions.Functions.validateTypes(Functions.java:131)
        at org.apache.cassandra.cql3.functions.Functions.get(Functions.java:92)
        at org.apache.cassandra.cql3.functions.FunctionCall$Raw.prepare(FunctionCall.java:103)
        at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.updateRestriction(SelectStatement.java:1246)
        at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:959)
        at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:271)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:140)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1726)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4074)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4062)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
        at java.lang.Thread.run(Thread.java:662)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,02/Apr/13 11:10;slebresne;5404.txt;https://issues.apache.org/jira/secure/attachment/12576541/5404.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-02 11:10:09.155,,,no_permission,,,,,,,,,,,,320321,,,Tue Apr 02 17:24:11 UTC 2013,,,,,,0|i1j9on:,320662,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"02/Apr/13 11:10;slebresne;Patch attached to handle the {{null}} correctly.

Let's note that the query itself is invalid because 'int' is not a function that exists.","02/Apr/13 17:17;brandon.williams;bq. Let's note that the query itself is invalid because 'int' is not a function that exists.

Yeah, I'll admit I was trying to do something whackass there.  +1","02/Apr/13 17:24;slebresne;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Windows 7 deleting/renaming files problem,CASSANDRA-5383,12638924,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,enigmacurry,enigmacurry,25/Mar/13 17:36,12/Mar/19 14:17,13/Mar/19 22:29,04/Oct/13 07:45,2.0.2,,,Legacy/Testing,,,0,qa-resolved,,,,"Two unit tests are failing on Windows 7 due to errors in renaming/deleting files:


org.apache.cassandra.db.ColumnFamilyStoreTest: 
{code}
    [junit] Testsuite: org.apache.cassandra.db.ColumnFamilyStoreTest
    [junit] Tests run: 27, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 13.904 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] ERROR 13:06:46,058 Unable to delete build\test\cassandra\data\Keyspace1\Indexed2\Keyspace1-Indexed2.birthdate_index-ja-1-Data.db (it will be removed on server restart; we'll also retry after GC)
    [junit] ERROR 13:06:48,508 Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
    [junit] java.lang.RuntimeException: Tried to hard link to file that does not exist build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-7-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.createHardLink(FileUtils.java:72)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:1057)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.run(DataTracker.java:168)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testSliceByNamesCommandOldMetatada(org.apache.cassandra.db.ColumnFamilyStoreTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-6-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-6-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-6-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-6-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:589)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStoreTest.testSliceByNamesCommandOldMetatada(ColumnFamilyStoreTest.java:885)
    [junit] 
    [junit] 
    [junit] Testcase: testRemoveUnifinishedCompactionLeftovers(org.apache.cassandra.db.ColumnFamilyStoreTest):	Caused an ERROR
    [junit] java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra\build\test\cassandra\data\Keyspace1\Standard3\Keyspace1-Standard3-ja-2-Data.db
    [junit] FSWriteError in build\test\cassandra\data\Keyspace1\Standard3\Keyspace1-Standard3-ja-2-Data.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:112)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:103)
    [junit] 	at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:139)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:507)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStoreTest.testRemoveUnifinishedCompactionLeftovers(ColumnFamilyStoreTest.java:1246)
    [junit] Caused by: java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra\build\test\cassandra\data\Keyspace1\Standard3\Keyspace1-Standard3-ja-2-Data.db
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.ColumnFamilyStoreTest FAILED
{code}


org.apache.cassandra.db.ScrubTest:
{code}
    [junit] Testcase: testScrubFile(org.apache.cassandra.db.ScrubTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\Super5\Keyspace1-Super5-f-2-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Super5\Keyspace1-Super5-f-2-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\Super5\Keyspace1-Super5-f-2-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Super5\Keyspace1-Super5-f-2-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:589)
    [junit] 	at org.apache.cassandra.db.ScrubTest.testScrubFile(ScrubTest.java:94)
    [junit] 
    [junit] 
    [junit] Testcase: testScubOutOfOrder(org.apache.cassandra.db.ScrubTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\Standard3\Keyspace1-Standard3-ia-1-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Standard3\Keyspace1-Standard3-ia-1-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\Standard3\Keyspace1-Standard3-ia-1-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Standard3\Keyspace1-Standard3-ia-1-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:589)
    [junit] 	at org.apache.cassandra.db.ScrubTest.testScubOutOfOrder(ScrubTest.java:201)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.ScrubTest FAILED
{code}

Reproduced in a Windows 7 VM:
java 1.6.0_43-b01
ant 1.9.0
C* trunk
run 'ant clean test'",,,,,,,,,,,,,,CASSANDRA-3613,CASSANDRA-6283,,,,,,,,,05/Jun/13 08:45;krummas;0001-CASSANDRA-5383-cant-move-a-file-on-top-of-another-fi.patch;https://issues.apache.org/jira/secure/attachment/12586278/0001-CASSANDRA-5383-cant-move-a-file-on-top-of-another-fi.patch,28/May/13 18:47;krummas;0001-CASSANDRA-5383-v2.patch;https://issues.apache.org/jira/secure/attachment/12585061/0001-CASSANDRA-5383-v2.patch,27/Mar/13 10:18;krummas;0001-use-Java7-apis-for-deleting-and-moving-files-and-cre.patch;https://issues.apache.org/jira/secure/attachment/12575685/0001-use-Java7-apis-for-deleting-and-moving-files-and-cre.patch,03/Oct/13 12:19;krummas;5383-v3.patch;https://issues.apache.org/jira/secure/attachment/12606575/5383-v3.patch,28/May/13 19:40;enigmacurry;5383_patch_v2_system.log;https://issues.apache.org/jira/secure/attachment/12585066/5383_patch_v2_system.log,06/Jun/13 21:46;enigmacurry;cant_move_file_patch.log;https://issues.apache.org/jira/secure/attachment/12586595/cant_move_file_patch.log,28/May/13 19:24;enigmacurry;test_log.5383.patch_v2.log.txt;https://issues.apache.org/jira/secure/attachment/12585065/test_log.5383.patch_v2.log.txt,06/Jun/13 21:00;enigmacurry;v2+cant_move_file_patch.log;https://issues.apache.org/jira/secure/attachment/12586585/v2%2Bcant_move_file_patch.log,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,2013-03-25 17:41:25.187,,,no_permission,,,,,,,,,,,,319394,,,Wed Jun 17 08:55:07 UTC 2015,,,,,,0|i1j3yf:,319735,,,,,,,,jbellis,jbellis,,,,,,,,,enigmacurry,25/Mar/13 17:41;jbellis;The mutateLevel problem looks like what I predicted -- Windows won't let you rename over an existing file.,"25/Mar/13 17:41;jbellis;(Not sure about CFST...  maybe we should take advantage of Java 7 and switch to using the Path api, which will give us better error messages.)","25/Mar/13 21:01;enigmacurry;I had a problem that prevented me from running the full test suite, I've resolved that and found several more errors that look similar to my eyes:

{code}
    [junit] Testcase: org.apache.cassandra.config.CFMetaDataTest:	Caused an ERROR
    [junit] java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\data
    [junit] FSWriteError in build\test\cassandra\data
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:112)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:340)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanup(SchemaLoader.java:399)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanupAndLeaveDirs(SchemaLoader.java:374)
    [junit] 	at org.apache.cassandra.SchemaLoader.loadSchema(SchemaLoader.java:65)
    [junit] 	at org.apache.cassandra.SchemaLoader.loadSchema(SchemaLoader.java:59)
    [junit] Caused by: java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\data
{code}

{code}
    [junit] Testcase: org.apache.cassandra.db.RemoveSubColumnTest:	Caused an ERROR
    [junit] java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\data\Keyspace4
    [junit] FSWriteError in build\test\cassandra\data\Keyspace4
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:112)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:340)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:336)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanup(SchemaLoader.java:399)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanupAndLeaveDirs(SchemaLoader.java:374)
    [junit] 	at org.apache.cassandra.SchemaLoader.loadSchema(SchemaLoader.java:65)
    [junit] 	at org.apache.cassandra.SchemaLoader.loadSchema(SchemaLoader.java:59)
    [junit] Caused by: java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\data\Keyspace4
    [junit] 
    [junit] 
    [junit] Testcase: org.apache.cassandra.db.RemoveSubColumnTest:	Caused an ERROR
    [junit] null
    [junit] java.lang.NullPointerException
    [junit] 	at org.apache.cassandra.gms.Gossiper.stop(Gossiper.java:1084)
    [junit] 	at org.apache.cassandra.SchemaLoader.stopGossiper(SchemaLoader.java:99)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.RemoveSubColumnTest FAILED
{code}

{code}
    [junit] Testsuite: org.apache.cassandra.db.compaction.LegacyLeveledManifestTest
    [junit] Tests run: 3, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 2.698 sec
    [junit] 
    [junit] Testcase: doMigrationTest(org.apache.cassandra.db.compaction.LegacyLeveledManifestTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\legacyleveled\Keyspace1-legacyleveled-hf-2-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\legacyleveled\Keyspace1-legacyleveled-hf-2-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\legacyleveled\Keyspace1-legacyleveled-hf-2-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\legacyleveled\Keyspace1-legacyleveled-hf-2-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.compaction.LegacyLeveledManifest.migrateManifests(LegacyLeveledManifest.java:102)
    [junit] 	at org.apache.cassandra.db.compaction.LegacyLeveledManifestTest.doMigrationTest(LegacyLeveledManifestTest.java:50)
    [junit] 
    [junit] 
    [junit] Testcase: validateSSTableMetadataTest(org.apache.cassandra.db.compaction.LegacyLeveledManifestTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\legacyleveled\Keyspace1-legacyleveled-hf-2-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\legacyleveled\Keyspace1-legacyleveled-hf-2-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\legacyleveled\Keyspace1-legacyleveled-hf-2-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\legacyleveled\Keyspace1-legacyleveled-hf-2-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.compaction.LegacyLeveledManifest.migrateManifests(LegacyLeveledManifest.java:102)
    [junit] 	at org.apache.cassandra.db.compaction.LegacyLeveledManifestTest.validateSSTableMetadataTest(LegacyLeveledManifestTest.java:74)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.LegacyLeveledManifestTest FAILED
{code}

{code}
    [junit] Testsuite: org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest
    [junit] Tests run: 3, Failures: 1, Errors: 1, Skipped: 0, Time elapsed: 20.217 sec
    [junit] 
    [junit] Testcase: testValidationMultipleSSTablePerLevel(org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest.testValidationMultipleSSTablePerLevel(LeveledCompactionStrategyTest.java:89)
    [junit] 
    [junit] 
    [junit] Testcase: testMutateLevel(org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\StandardLeveled\Keyspace1-StandardLeveled-ja-60-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\StandardLeveled\Keyspace1-StandardLeveled-ja-60-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\StandardLeveled\Keyspace1-StandardLeveled-ja-60-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\StandardLeveled\Keyspace1-StandardLeveled-ja-60-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest.testMutateLevel(LeveledCompactionStrategyTest.java:180)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest FAILED
{code}

{code}
    [junit] Testsuite: org.apache.cassandra.dht.OrderPreservingPartitionerTest
    [junit] Tests run: 7, Failures: 0, Errors: 6, Skipped: 0, Time elapsed: 1.716 sec
    [junit] 
    [junit] Testcase: testMidpoint(org.apache.cassandra.dht.OrderPreservingPartitionerTest):	Caused an ERROR
    [junit] java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] FSWriteError in build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:112)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:340)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:336)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanup(SchemaLoader.java:388)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanupAndLeaveDirs(SchemaLoader.java:374)
    [junit] 	at org.apache.cassandra.dht.OrderPreservingPartitionerTest.initPartitioner(OrderPreservingPartitionerTest.java:31)
    [junit] 	at org.apache.cassandra.dht.PartitionerTestCase.clean(PartitionerTestCase.java:39)
    [junit] Caused by: java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 
    [junit] 
    [junit] Testcase: testMidpointMinimum(org.apache.cassandra.dht.OrderPreservingPartitionerTest):	Caused an ERROR
    [junit] java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] FSWriteError in build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:112)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:340)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:336)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanup(SchemaLoader.java:388)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanupAndLeaveDirs(SchemaLoader.java:374)
    [junit] 	at org.apache.cassandra.dht.OrderPreservingPartitionerTest.initPartitioner(OrderPreservingPartitionerTest.java:31)
    [junit] 	at org.apache.cassandra.dht.PartitionerTestCase.clean(PartitionerTestCase.java:39)
    [junit] Caused by: java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 
    [junit] 
    [junit] Testcase: testMidpointWrapping(org.apache.cassandra.dht.OrderPreservingPartitionerTest):	Caused an ERROR
    [junit] java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] FSWriteError in build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:112)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:340)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:336)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanup(SchemaLoader.java:388)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanupAndLeaveDirs(SchemaLoader.java:374)
    [junit] 	at org.apache.cassandra.dht.OrderPreservingPartitionerTest.initPartitioner(OrderPreservingPartitionerTest.java:31)
    [junit] 	at org.apache.cassandra.dht.PartitionerTestCase.clean(PartitionerTestCase.java:39)
    [junit] Caused by: java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 
    [junit] 
    [junit] Testcase: testTokenFactoryBytes(org.apache.cassandra.dht.OrderPreservingPartitionerTest):	Caused an ERROR
    [junit] java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] FSWriteError in build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:112)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:340)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:336)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanup(SchemaLoader.java:388)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanupAndLeaveDirs(SchemaLoader.java:374)
    [junit] 	at org.apache.cassandra.dht.OrderPreservingPartitionerTest.initPartitioner(OrderPreservingPartitionerTest.java:31)
    [junit] 	at org.apache.cassandra.dht.PartitionerTestCase.clean(PartitionerTestCase.java:39)
    [junit] Caused by: java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 
    [junit] 
    [junit] Testcase: testTokenFactoryStrings(org.apache.cassandra.dht.OrderPreservingPartitionerTest):	Caused an ERROR
    [junit] java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] FSWriteError in build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:112)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:340)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:336)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanup(SchemaLoader.java:388)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanupAndLeaveDirs(SchemaLoader.java:374)
    [junit] 	at org.apache.cassandra.dht.OrderPreservingPartitionerTest.initPartitioner(OrderPreservingPartitionerTest.java:31)
    [junit] 	at org.apache.cassandra.dht.PartitionerTestCase.clean(PartitionerTestCase.java:39)
    [junit] Caused by: java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 
    [junit] 
    [junit] Testcase: testDescribeOwnership(org.apache.cassandra.dht.OrderPreservingPartitionerTest):	Caused an ERROR
    [junit] java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] FSWriteError in build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:112)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:340)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:336)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanup(SchemaLoader.java:388)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanupAndLeaveDirs(SchemaLoader.java:374)
    [junit] 	at org.apache.cassandra.dht.OrderPreservingPartitionerTest.initPartitioner(OrderPreservingPartitionerTest.java:31)
    [junit] 	at org.apache.cassandra.dht.PartitionerTestCase.clean(PartitionerTestCase.java:39)
    [junit] Caused by: java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.dht.OrderPreservingPartitionerTest FAILED
{code}

{code}
    [junit] Testsuite: org.apache.cassandra.io.sstable.SSTableReaderTest
    [junit] Tests run: 8, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 11.607 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit]  WARN 16:49:23,669 setting live ratio to maximum of 64.0 instead of 97.26984126984127
    [junit]  WARN 16:49:24,542 setting live ratio to maximum of 64.0 instead of 79.42857142857143
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testPersistentStatistics(org.apache.cassandra.io.sstable.SSTableReaderTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-2-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-2-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-2-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-2-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:589)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.clearAndLoad(SSTableReaderTest.java:170)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.testPersistentStatistics(SSTableReaderTest.java:163)
    [junit] 
    [junit] 
    [junit] Testcase: testPersistentStatisticsWithSecondaryIndex(org.apache.cassandra.io.sstable.SSTableReaderTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\Indexed1\Keyspace1-Indexed1.626972746864617465-ja-1-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Indexed1\Keyspace1-Indexed1.626972746864617465-ja-1-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\Indexed1\Keyspace1-Indexed1.626972746864617465-ja-1-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Indexed1\Keyspace1-Indexed1.626972746864617465-ja-1-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:589)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.clearAndLoad(SSTableReaderTest.java:170)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.assertIndexQueryWorks(SSTableReaderTest.java:324)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.testPersistentStatisticsWithSecondaryIndex(SSTableReaderTest.java:220)
    [junit] 
    [junit] 
    [junit] Testcase: testPersistentStatisticsFromOlderIndexedSSTable(org.apache.cassandra.io.sstable.SSTableReaderTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\Indexed1\Keyspace1-Indexed1.626972746864617465-ja-1-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Indexed1\Keyspace1-Indexed1.626972746864617465-ja-1-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\Indexed1\Keyspace1-Indexed1.626972746864617465-ja-1-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Indexed1\Keyspace1-Indexed1.626972746864617465-ja-1-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:589)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.clearAndLoad(SSTableReaderTest.java:170)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.assertIndexQueryWorks(SSTableReaderTest.java:324)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.testPersistentStatisticsFromOlderIndexedSSTable(SSTableReaderTest.java:248)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.io.sstable.SSTableReaderTest FAILED
{code}

{code}
    [junit] Testsuite: org.apache.cassandra.io.sstable.SSTableSimpleWriterTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 8.72 sec
    [junit] 
    [junit] Testcase: testSSTableSimpleUnsortedWriter(org.apache.cassandra.io.sstable.SSTableSimpleWriterTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\StandardInteger1\Keyspace1-StandardInteger1-ja-1-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\StandardInteger1\Keyspace1-StandardInteger1-ja-1-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\StandardInteger1\Keyspace1-StandardInteger1-ja-1-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\StandardInteger1\Keyspace1-StandardInteger1-ja-1-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:589)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableSimpleWriterTest.testSSTableSimpleUnsortedWriter(SSTableSimpleWriterTest.java:89)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.io.sstable.SSTableSimpleWriterTest FAILED
{code}

",26/Mar/13 22:13;enigmacurry;I cannot currently run a full 'ant test' on Windows due to a bug in ant/junit on windows #5388,"27/Mar/13 10:18;krummas;not yet tested on windows due to CASSANDRA-5388

",28/May/13 08:59;krummas;[~enigmacurry] - could you test this now that CASSANDRA-5388 is fixed?,"28/May/13 16:33;enigmacurry;[~krummas] I can't get this patch to apply on the current trunk (It did back in March when I first tried this.)

It does apply to cassandra-1.2, but it doesn't work there:

{code}
$ ant clean test -Dtest.name=ColumnFamilyStoreTest
[...]

    [junit] Testsuite: org.apache.cassandra.db.ColumnFamilyStoreTest
    [junit] Tests run: 27, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 9.162 sec
    [junit]
    [junit] Testcase: testSliceByNamesCommandOldMetatada(org.apache.cassandra.db.ColumnFamilyStoreTest):        Caused a
n ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ic-8-Index.db to build\te
st\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ic-9-Index.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standar
d1-ic-8-Index.db to build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ic-9-Index.db
    [junit]     at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:155)
    [junit]     at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:139)
    [junit]     at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.java:409)
    [junit]     at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:504)
    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testSliceByNamesCommandOldMetatada(ColumnFamilyStoreTes
t.java:925)
    [junit] Caused by: java.nio.file.FileSystemException: build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standa
rd1-ic-8-Index.db -> build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ic-9-Index.db: The process cannot
 access the file because it is being used by another process.
    [junit]
    [junit]     at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
    [junit]     at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
    [junit]     at sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:301)
    [junit]     at sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:286)
    [junit]     at java.nio.file.Files.move(Files.java:1345)
    [junit]     at org.apache.cassandra.io.util.FileUtils.atomicMoveWithFallback(FileUtils.java:169)
    [junit]     at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:151)
    [junit]
    [junit]
    [junit] Test org.apache.cassandra.db.ColumnFamilyStoreTest FAILED

{code}","28/May/13 18:47;krummas;rebased patch, probably wont work either though

the error is quite strange - had a quick look and the files should all be closed before renaming...","28/May/13 19:24;enigmacurry;Uploaded test_log.5383.patch_v2.log.txt

This log is the entire test suite run against trunk with the v2 patch. Windows 7, java 1.7.0_17 64bit","03/Jun/13 08:32;krummas;im stumped (and cant really debug since i dont own a windows machine)

anyone with windows skills want to take a look?","05/Jun/13 08:45;krummas;hacks around the problem by deleting the Statistics file on windows before moving

could in cases end up without a metadata file on windows","06/Jun/13 21:00;enigmacurry;[~krummas] Is the new patch (0001-CASSANDRA-5383-cant-move-a-file-on-top-of-another-fi.patch) meant to be applied on top of the v2 patch? 

I ran it that way and attached v2+cant-move-file-patch.log. I'm still seeing numberous FSWriteErrors.","06/Jun/13 21:16;krummas;[~enigmacurry] no the last patch should be applied alone, it is just a hack around the problem, not being able to replace files in windows","06/Jun/13 21:46;enigmacurry;OK, I reran it with just the 0001-CASSANDRA-5383-cant-move-a-file-on-top-of-another-fi.patch applied to trunk. Attached cant_move_file_patch.log. I'm still seeing FSWriteErrors.","17/Jun/13 06:57;krummas;those errors are not related to the mutateLevel issue anymore - mutatelevel only touches the *-Statistics.db files (and those are not mentioned in the logs with the latest patch it seems)

[~jbellis] got any idea what these are? seems we are not closing files before trying to rename them?",25/Sep/13 08:16;krummas;[~jbellis] wdyt?,"25/Sep/13 14:22;jbellis;bq. seems we are not closing files before trying to rename them?

That's what it looks like, but damned if I know why.

We should at least get the java7 refactoring committed though...","03/Oct/13 12:19;krummas;rebased against trunk, both starts using the java7 apis and removes a file before moving another file on top of it in windows",03/Oct/13 15:44;jbellis;+1,04/Oct/13 07:45;krummas;commited!,"28/Oct/13 17:59;Andie78;Release 2.0.2 still not working under Win7 32-bit with Java build 1.7.0_45-b18. system.log:
""SSTableDeletingTask.java (line 81) Unable to delete D:\Programme\cassandra\data\nieste\nfiles\nieste-nfiles-jb-2387-Data.db (it will be removed on server restart; we'll also retry after GC"". After heavy data changing (update data every 3 - 10 sec) up to 24.000 files in nfiles (data) folder and cassandra crashes. I tried LZ4Compressor, SnappyCompressor, DeflateCompressor and No Compression. I also tried Snapshot v2.1 last week without success. Is there a way to reduce frequency of writing sstables (new files) until issue is fixed? Rarely changed CFs are not affected.","14/May/14 16:01;cmessaoud;Hi, i am having the delete file problem on Windows 7, i downloaded your 5383-v3.patch fix, but don't know how to apply it onto my Cassandra platform. Anyone can shed some light on how to do that please ? Thanks","17/Jun/15 08:55;Andie78;I put it manually into the source:
/src/org/apache/cassandra/io/util/RandomAccessReader.java
Put (overwrite) the finalizer there.

To avoid unnecessary logs after patching u can un-comment the warning (logger.error) in /sstable/SSTableDeletingTask.java

To build u need maven (I use 3.0.5) and ant (1.9.2) and an internet-connection. I have to use ant on a dedicated server in front of the proxy since I couldn't get proxy-settings working in ant.
After the modifications u can type ""ant jar"" if u just need the patched jar(s). ""ant release"" builds u the compressed bin-package.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fat Client: No longer works in 1.2,CASSANDRA-5378,12638545,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,carlyeks,carlyeks,carlyeks,22/Mar/13 17:28,12/Mar/19 14:17,13/Mar/19 22:29,07/Aug/13 17:31,1.2.9,,,,,,0,client,,,,"The current client only example doesn't compile. After doing some updates, the fat client still won't work, mainly because the schema is not being pushed to the fat client.

I've made changes to the client to support CQL3 commands, to the ServiceManager to wait until a migration has completed before starting the client, and to the MigrationManager to not try to pull schemas from a fat client.",,,,,,,,,,,,,,,,,,,,,,,,22/Mar/13 18:50;carlyeks;5378-1.2.txt;https://issues.apache.org/jira/secure/attachment/12575068/5378-1.2.txt,15/Jun/13 01:52;carlyeks;5378-schema-writing.patch;https://issues.apache.org/jira/secure/attachment/12587943/5378-schema-writing.patch,29/Mar/13 15:50;carlyeks;5378-v2.txt;https://issues.apache.org/jira/secure/attachment/12576086/5378-v2.txt,22/Mar/13 17:29;carlyeks;5378.txt;https://issues.apache.org/jira/secure/attachment/12575045/5378.txt,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-03-29 13:54:33.647,,,no_permission,,,,,,,,,,,,319021,,,Wed Aug 07 17:31:18 UTC 2013,,,,,,0|i1j1nr:,319362,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"22/Mar/13 18:46;carlyeks;Was actually developing against trunk for this patch, so the fix version should be 2.0.

I'm fixing it for 1.2.4 as well, but this patch doesn't apply cleanly.",22/Mar/13 18:50;carlyeks;This patch is applied against the 1.2 branch - the unclean file wasn't needed.,"29/Mar/13 13:54;brandon.williams;Committed, thanks!","29/Mar/13 14:15;brandon.williams;http://buildbot.datastax.com:8020/builders/cassandra-1.2/builds/179/steps/shell/logs/stdio StorageServiceClientTest is failing, can you take a look?",29/Mar/13 15:50;carlyeks;This fixes the test.,"29/Mar/13 17:04;brandon.williams;Committed as well, thanks.","05/Apr/13 13:04;slebresne;This broke a dtests, namely cql_tests.py:TestCQL.bug_5240_test.

The reason is that the patch modifies SelectStatement.java and call CFMetadata.getKeyName(), but that method should not be called from CQL3 ever. I'm not sure I understand why fixing the fat client would involve making any kind of change in SelectStatement tbh.","05/Apr/13 17:57;carlyeks;The change to select statement is because the fat client can't use the CFS as it doesn't have the data. So, the only things that we have instead is the metadata. Since CQL3 is the way that most clients will want to interact with the service, I thought it made sense to try to update those statements to not use the CFS; any direction on how to fix this?","05/Apr/13 18:36;slebresne;Well looking at that code, what was the rational of adding the offending line was added in the first place:
{noformat}
+                indexedNames.add(cfm.getKeyName());
{noformat}
Seems to me it could just be removed.","05/Apr/13 18:51;carlyeks;If we don't have that, we won't be able to validate a query against the row key.

That is, if the schema is:
{code}
CREATE COLUMNFAMILY standard1 ( id ascii PRIMARY KEY , name ascii , value blob ) ;
SELECT * FROM standard1 WHERE id='abc';
{code}

That select statement will not be validated, as no column is indexed, but the row key is. It'll throw the ""No indexed columns present in by-columns clause with Equal operator"" exception.","08/Apr/13 09:40;slebresne;I'm not sure what you are talking about. The example above have always worked and does not require the row key to be added to the indexedNames (which is an addition of this patch). The row key is ""primarily"" indexed but it's not indexed by a secondary index. And adding a secondary index on the row key is not something supported (largely because it would be useless).

So anyway, I have removed that line in commit 69f05a704aafa90f2151db721312f3c5907abb2f if only because that restore the behavior of before that patch and let all cql dtests pass. If you still think there is a problem, please open a separate issue with a test case showing the problem you are talking about. ","01/May/13 20:34;tjake;The fix for the test breaks the fat client 5378-v2.txt

It can no longer get the schema from the non-fat clients and instead throws:

{code}
java.lang.NullPointerException
	at org.apache.cassandra.service.MigrationManager.maybeScheduleSchemaPull(MigrationManager.java:123)
	at org.apache.cassandra.service.MigrationManager.onAlive(MigrationManager.java:98)
	at org.apache.cassandra.gms.Gossiper.markAlive(Gossiper.java:773)
	at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:816)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:901)
	at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:50)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

{code}

Can we put the change back in or somehow fix this NPE?","13/May/13 10:40;carlyeks;The point of the second patch was to prevent us from using the file system. This means that the migration manager probably needs to be able to operate exclusively in memory.

I'll take a look at this issue later today.","15/Jun/13 01:52;carlyeks;I'm attaching a fix that adds back the Schema.instance.updateVersion(), but prevents writing out to disk by that call.

This fixes the test, and the startup of the fat client.",02/Aug/13 03:25;jbellis;Marking Patch Available.,"07/Aug/13 17:31;brandon.williams;Looks like I ninja'd this exact patch in right after committing the original, to fix the test.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-all 1.2.0 pom missing netty dependency,CASSANDRA-5392,12639348,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sgbridges,sgbridges,sgbridges,27/Mar/13 15:56,12/Mar/19 14:17,13/Mar/19 22:29,01/Apr/13 18:57,1.2.4,,,Packaging,,,0,,,,,"It seems that cassandra depends on netty now, however the pom excludes this dependency.  This was previously reported as CASSANDRA-5181, but the fix for 5181 added netty to the dependency-management section of the pom, not the depencies section",,,,,,,,,,,,,,,,,,,,,,,,27/Mar/13 15:58;sgbridges;CASSANDRA-5392.txt;https://issues.apache.org/jira/secure/attachment/12575723/CASSANDRA-5392.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-01 18:57:32.629,,,no_permission,,,,,,,,,,,,319818,,,Mon Apr 01 18:57:32 UTC 2013,,,,,,0|i1j6kv:,320159,,,,,,,,dbrosius,dbrosius,,,,,,,,,,"01/Apr/13 18:57;dbrosius;+1, committed to cassandra-1.2 as e06875ed2cd47f7bd77eaae9cc70dee5a3c0371a",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL Not Handling Descending Clustering Order On A timeuuid Correctly,CASSANDRA-5386,12639171,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,gcollins,gcollins,26/Mar/13 19:29,12/Mar/19 14:17,13/Mar/19 22:29,27/Mar/13 15:18,1.2.4,,,,,,0,,,,,"I raised this issue as a question in the mailing list:

http://www.mail-archive.com/user@cassandra.apache.org/msg28787.html

If I create a table (cqlsh) with the following schema:

CREATE TABLE mytable ( column1 text,
      column2 text,
      messageId timeuuid,
      message blob,
      PRIMARY KEY ((column1, column2), messageId));

I can quite happily add rows to this table:

insert into client_queue (column1,column2,messageId,message) VALUES
('string1','string2',now(),'ABCCDCC123');

If I however create a table with a desc clustering order on messageid:

CREATE TABLE mytable ( column1 text,
      column2 text,
      messageId timeuuid,
      message blob,
      PRIMARY KEY ((column1, column2), messageId)) WITH CLUSTERING
ORDER BY (messageId DESC);

Inserts are failing. I am getting the following error:

insert into client_queue2 (column1,column2,messageId,message) VALUES
('string1','string2',now(),'ABCCDCC123');

I get the following error:

Bad Request: Type error: cannot assign result of function now (type
timeuuid) to messageid (type
'org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.TimeUUIDType)')

","Apache Cassandra 1.2.3, Mac OS X (Lion), cql 3",,,,,,,,,,,,,,,,,,,,,,,27/Mar/13 10:27;slebresne;5386.txt;https://issues.apache.org/jira/secure/attachment/12575686/5386.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-27 10:27:28.97,,,no_permission,,,,,,,,,,,,319641,,,Wed Mar 27 15:18:57 UTC 2013,,,,,,0|i1j5hj:,319982,,,,,,,,jbellis,jbellis,,,,,,,,,,27/Mar/13 10:27;slebresne;Patch attached to fix (to ignore ReversedType when we do a type comparison).,27/Mar/13 12:49;jbellis;+1,"27/Mar/13 15:18;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CL regression in the presence of bootstrapping nodes,CASSANDRA-5354,12637479,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,jbellis,jbellis,18/Mar/13 04:08,12/Mar/19 14:17,13/Mar/19 22:29,18/Mar/13 13:46,1.2.4,,,,,,0,,,,,It looks like CASSANDRA-4858 broke CASSANDRA-833 again; pendingEndpoints is not provided to or accounted by blockFor.,,,,,,,,,,,,,,,,,,,,,,,,18/Mar/13 12:53;slebresne;5354.txt;https://issues.apache.org/jira/secure/attachment/12574137/5354.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-18 12:53:17.154,,,no_permission,,,,,,,,,,,,317970,,,Mon Mar 18 13:46:38 UTC 2013,,,,,,0|i1iv67:,318311,,,,,,,,jbellis,jbellis,,,,,,,,,,"18/Mar/13 12:53;slebresne;For the record and from what I can tell, this is no due to CASSANDRA-4858 that did not changed the prior behavior, but the fix for CASSANDRA-833 never really worked. What happened is that:
* CASSANDRA-833 was committed and reversed right away (we don't remember why).
* CASSANDRA-3979 was committed. That patch made {{blockFor}} a constant initialized in WriteResponseHandler ctor to consistencyLevel.blockFor() (and used to initialize the size of {{responses}} in particular). This was correct at the time, CASSANDRA-833 had been reverted.
* The rebase of CASSANDRA-833 was committed. That rebase changed blockFor() in AbstractWriteResponseHandler to be blockForCL() (the ""old"" blockFor) + the pending endpoints. However, WriteResponseHandler was not modify to use that blockFor() method to initial the {{responses}} variable.

As a result, CASSANDRA-833 never ""worked"".

Anyway, attaching patch to hopefully fix this. I note that it includes a small part of the original 833 patch in DatacenterSyncWriteResponseHandler that apparently didn't made it in the rebase but I think is needed.
","18/Mar/13 13:28;jbellis;LGTM, thanks for tracking that down!","18/Mar/13 13:46;slebresne;Alright, committed. Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PreparedStatements get mixed up between Keyspaces,CASSANDRA-5352,12637210,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,davedamoon,davedamoon,davedamoon,15/Mar/13 15:07,12/Mar/19 14:17,13/Mar/19 22:29,18/Mar/13 08:34,1.2.4,,,,,,0,,,,,"I found this behavior while running the same application using two different keyspaces connected to the same node.

The prepared statements uses the keyspace that was set while the statement was perpared (public final CFDefinition cfDef).
When reusing the Statement only the cql-query is used to create a key and the keyspace is ignored. When the same query is prepared and used for two different Keyspaces the wrong keyspace can be used.

The fix is not to ignore the keyspace when reusing the statement.",,,,,,,,,,,,,,,,,,,,,,,,15/Mar/13 15:13;davedamoon;CassandraServerCql3Test.java;https://issues.apache.org/jira/secure/attachment/12573884/CassandraServerCql3Test.java,15/Mar/13 15:16;davedamoon;patch.txt;https://issues.apache.org/jira/secure/attachment/12573885/patch.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-03-18 08:34:11.047,,,no_permission,,,,,,,,,,,,317702,,,Mon Mar 18 08:34:11 UTC 2013,,,,,,0|i1itin:,318043,,,,,,,,slebresne,slebresne,,,,,,,,,,15/Mar/13 15:13;davedamoon;the testcase for select querys,15/Mar/13 15:16;davedamoon;change to involve the keyspace while creating the key for statment storage,18/Mar/13 08:34;slebresne;Good catch. Committed with a slight modification to handle the case where the current keyspace is {{null}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changing column_index_size_in_kb on different nodes might corrupt files,CASSANDRA-5454,12642024,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,11/Apr/13 12:29,12/Mar/19 14:17,13/Mar/19 22:29,01/Jul/13 07:41,1.2.7,2.0 beta 1,,,,,0,,,,,"RangeTombstones requires that we sometimes repeat a few markers in the data file at index boundaries. Meaning that the same row with different column_index_size_in_kb will not have the same data size.

This is a problem for streaming, because if the column_index_size_in_kb is different in the source and the destination, the resulting row should have a different size on the destination, but streaming rely on the data size not changing in 1.2.

Now, while having different column_index_size on different nodes is probably not extremely useful in the long run, you may still have temporal discrepancies because there is no real way to change the setting on all node atomically. Besides, it's not to hard to get different setting on different nodes due to human error. And currently, the result is that if a file is stream while the setting is not consistent, then we'll end up corrupting the received file (due to the fix from CASSANDRA-5418 to be precise).

I don't see a good way to fix this in 1.2, so users will have to be careful not to have streaming happening while they change the column_index_size_in_kb setting. But in 2.0, once CASSANDRA-4180 is committed, we won't have the problem of having to respect the dataSize from the source on the destination anymore. So basically we should revert the fix from CASSANDRA-5418 (though we may still want to avoid repeating unneeded marker, but the tombstoneTracker can give us that easily).",,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4180,27/Jun/13 14:28;slebresne;5454.txt;https://issues.apache.org/jira/secure/attachment/12589912/5454.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-29 05:38:36.855,,,no_permission,,,,,,,,,,,,322438,,,Mon Jul 01 07:41:58 UTC 2013,,,,,,0|i1jmrj:,322783,,,,,,,,jbellis,jbellis,,,1.2.5,,,,,,,"27/Jun/13 14:28;slebresne;Attaching patch for this. As said above, this basically revert the changes from CASSANDRA-5418, which is ok now that we don't write the row size or column count at the start of the row.

I've checked that the test added by Yukim for CASSANDRA-5418 does pass with this patch. ",29/Jun/13 05:38;jbellis;+1,"01/Jul/13 07:41;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove PBSPredictor,CASSANDRA-5455,12642058,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,11/Apr/13 16:06,12/Mar/19 14:17,13/Mar/19 22:29,11/Apr/13 16:30,2.0 beta 1,,,,,,0,,,,,"It was a fun experiment, but it's unmaintained and the bar to understanding what is going on is high.  Case in point: PBSTest has been failing intermittently for some time now, possibly even since it was created.  Or possibly not and it was a regression from a refactoring we did.  Who knows?",,,,,,,,,,,,,,,,,,,,,,,,11/Apr/13 16:12;jbellis;5455.txt;https://issues.apache.org/jira/secure/attachment/12578235/5455.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-11 16:25:44.882,,,no_permission,,,,,,,,,,,,322472,,,Tue May 28 03:51:19 UTC 2013,,,,,,0|i1jmz3:,322817,,,,,,,,krummas,krummas,,,,,,,,,,"11/Apr/13 16:25;krummas;there is an ant target ""pbs-test"" that should go away as well

lgtm other than that",11/Apr/13 16:30;jbellis;fixed + committed,"10/May/13 17:58;pbailis;I am one of the original authors of CASSANDRA-4261 and was previously unaware of this change. I'm happy to make any changes to the tests, perform necessary code refactoring, or write additional documentation (but was unable to do so given the window between ticket creation and commit). That is, I will maintain this functionality given the opportunity to do so.

Could you please elaborate on what you'd like to see fixed? I suspect it'll be fairly straightforward, and, if anyone ""knows"" how to make the changes, I (and Shivaram) probably do.

If the answer is that ""we don't want this functionality,"" then that's a different case. But that's not what I'm getting from this ticket or CASSANDRA-4261 or am hearing from users.","10/May/13 18:11;jbellis;Honestly, it was probably a mistake (mine) to commit it in the first place.  In my defense, it's hard to say No when someone shows up with working code...  but I should have; it doesn't solve an actual pain point for our users, so none of the maintainers was motivated to get familiar enough with it to fix the kind of regressions we ran into.  I don't see that changing if we were to resurrect it.  My sincere apologies for the time you put into it.","10/May/13 18:46;rbranson;Correct me if I'm wrong, but it seems like only a small amount of the code in the original patch is actually necessary to have in core to satisfy the data requirements of doing the PBS prediction. I would love to be able to feed these metrics into our monitoring systems as well. Would it be acceptable to refactor the code to support the latency metric collection & expose them through a JMX call, and allow implementing the actual PBS logic in a third party tool?",10/May/13 19:24;jbellis;I don't see why not.  Aren't these the same latency numbers that we track in StorageProxy?,"10/May/13 19:32;pbailis;I don't believe that the StorageProxy tracks the latencies according to the same granularity. For example, the PBS latency tracking will record both how long it took for the request to reach a remote replica and be processed as well as how long the return trip takes.

That said, it shouldn't be too difficult to either 1.) simply expose the recorded latencies via an optional module providing a ""finer granularity tracing"" interface via JMX [thereby removing all actual prediction code but keeping the logging in place for folks who might want this] or 2.) modifying StorageProxy to log these latencies in addition to the coarser granularity measurements it already takes.

I can provide assistance with either.","10/May/13 19:51;jbellis;bq. the PBS latency tracking will record both how long it took for the request to reach a remote replica and be processed as well as how long the return trip takes.

Hmm, I don't see that happening on the 1.2 branch.  It looks to me like like PBS was trying to measure raw message RTT (slightly incorrectly since it was clicking Start when we enqueued the message rather than when we sent).  Which, granted, *is* different from the SP ""time to complete request"" metrics.

Would the latter be ""close enough?""  I'd rather only track one set of mostly similar metrics, given the choice.

bq. modifying StorageProxy to log these latencies in addition to the coarser granularity measurements it already takes

I'm fine with either ""provide it as SPMBean methods"" or ""create a separate MBean that we only kick off if individual data point collection is enabled.""

Should we make collection be a fraction (0..1) rather than on/off?  ISTM that 10% or 1% of requests could provide enough information on a busy system, and those CLQ objects could become a contention point w/ enough cores busy.","14/May/13 23:48;pbailis;I've thought some more about different options for enabling metrics that are useful to both PBS (in an external module, if committers prefer) and anyone else who would be interested in finer-grained tracing.

To start, I *do* think that there is interest in a PBS module: if an eventually consistent store is returning stale data, how stale *is* it? Especially given that many (most?) Cassandra client libraries (including the Datastax java-driver) choose CL=ONE by default, I'd expect most users would prefer to understand how their choice of N,R, and W affects their latency and consistency.

I've been contacted by several Cassandra users who are interested in and/or using this functionality and understand that several developers are interested in PBS for Riak (notably, Andy Gross highlighted PBS in his 2013 RICON East keynote as a useful feature Basho would like). We originally chose Cassandra based on our familiarity with the code base and on early discussions with Jonathan but we plan to integrate PBS functionality into Riak with the help of their committers in the near-term future. So I do think there is interest, and, if you're curious about *use cases* for this functionality, Shivaram and I will be demoing PBS in Cassandra 1.2 at the upcoming SIGMOD 2013 conference. Our demo proposal sketches three application vignettes, including the obvious integration with monitoring tools but also automatically tuning N,R, and W and and providing consistency and latency SLAs:
http://www.bailis.org/papers/pbs-demo-sigmod2013.pdf

So, on the more technical side, there are two statistics that aren't currently measured (in trunk) that are required for accurate PBS predictions. First, PBS requires per-server statistics. Currently, the ColumnFamily RTT read/write latency metrics are aggregated across all servers. Second, PBS requires a measure how how long a read/write request takes before it is processed (i.e., how long it took from a client sending  each read/write request to when it was performed). This requires knowledge of one-way request latencies as well as read/write request-specific logic.

The 1.2 PBS patch provided both of these, aggregating by server and measuring the delay until processing. As Jonathan notes above, the latter measurement was conservative; the remote replica recorded the time that it enqueued its response rather than the exact moment a read or write was performed, namely for simplicity of code. The coordinating server could then closely approximate the return time as RTT-(remote timestamp).

Given these requirements and the current state of trunk, there are a few ways forward to support an external PBS prediction module:

1a.) Modify Cassandra to store latency statistics on a per-server and per-ColumnFamily granularity. As Rick Branson has pointed out, this is actually useful for monitoring other than PBS and can be used to detect slower replicas.

1b.) Modify Cassandra to store local processing times for requests (i.e., expand StorageMetrics, which currently does not track the time required to, say, fulfill a local read stage). This also has the benefit of understanding whether a Cassandra node is slow due to network or disk.

2.) Use the newly developed tracing functionality to reconstruct latencies for selected requests. Performing any sort of profiling will require tracing to be enabled (this appears to be somewhat heavyweight given the amount of data that is logged for each request , and reconstructing latencies from the trace table may be expensive (i.e., amount to a many-way self-join).

3.) Use RTT/2 based on ColumnFamily LatencyMetrics as an inaccurate but already supported external predictor.

4.) Leave the PBS latency sampling as in 1.2 but remove the PBS predictor code. Expose the latency samples via an Mbean for users like Rick who would benefit from it.

Proposal #1 has benefits for many users and seems a natural extension to the existing metrics but requires changes to the existing code. Proposal #2 puts substantial burden on an end-user and, without a fixed schema for the trace table, may amount to a fair bit of code munging. Proposal #3 is inaccurate but works on trunk. Proposal #4 is essentially 1.2.0 without the requirement to maintain any PBS-specific code and is a reasonable stop-gap before proposal #1. All of these proposals are amenable to sampling.

I'd welcome your feedback on these proposals and next steps.","16/May/13 05:15;jbellis;I'd prefer option 1 but I'm also fine with option 3.

For option 1 we do need to distinguish between ""track latency stuff on a per-CF basis"" (which is universally useful as, say, an EstimatedHistogram) and ""keep a window of individual latency times"" which is pretty much only a PBS requirement.  As I said above, we'd want to keep the latter disabled unless specified otherwise.","25/May/13 01:06;pbailis;Okay. #1 will likely require more extensive code changes: basically, it'll require EstimatedHistograms for each of the servers acting as replicas for a given ColumnFamily and will require EstimatedHistogram tracing in the StorageProxy (to separate network-based latency from disk-based latency). Are these changes feasible?

re: ""a window of individual latency times,"" looking at the Metrics implementation of EstimatedHistogram, EstimatedHistogram.values() should provide a reasonable enough sample (especially since, as you mention, since it has other uses as well).

Perhaps the simplest strategy is to go with #3 for now but implement #1 in the future if there's interest. #3 is easy; I've already written an example external module to do RTT/2 predictions: https://github.com/pbailis/pbs-predictor/blob/9d31acd1667b08affa609278689b540d8e0380f5/pbspredictor/src/main/java/edu/berkeley/pbs/cassandra/CassandraLatencyTrace.java
","27/May/13 15:32;jbellis;bq. I've already written an example external module to do RTT/2 predictions

Do we need any core changes at all, then?  (Under the ""#3 for now"" plan.)","28/May/13 03:51;pbailis;bq. Do we need any core changes at all, then? (Under the ""#3 for now"" plan.)

Nope; the predictor I linked uses the per-CF latency metrics. The downside is accuracy.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Large number of bootstrapping nodes cause gossip to stop working,CASSANDRA-5456,12642102,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,okibirev,okibirev,okibirev,11/Apr/13 18:44,12/Mar/19 14:17,13/Mar/19 22:29,12/Apr/13 17:13,1.1.11,1.2.5,,,,,0,,,,,"Long running section of code in PendingRangeCalculatorService is synchronized on bootstrapTokens. This causes gossip to stop working as it waits for the same lock when a large number of nodes (hundreds in our case) are bootstrapping. Consequently, the whole cluster becomes non-functional. 

I experimented with the following change in PendingRangeCalculatorService.java and it resolved the problem in our case. Prior code had synchronized around the for loop.

synchronized(bootstrapTokens) {
    bootstrapTokens = new LinkedHashMap<Token, InetAddress>(bootstrapTokens);
}

for (Map.Entry<Token, InetAddress> entry : bootstrapTokens.entrySet())
{
   InetAddress endpoint = entry.getValue();

   allLeftMetadata.updateNormalToken(entry.getKey(), endpoint);
   for (Range<Token> range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))
   pendingRanges.put(range, endpoint);
   allLeftMetadata.removeEndpoint(endpoint);
}
 ",,,,,,,,,,,,,,,,,,,,,,,,11/Apr/13 20:07;okibirev;PendingRangeCalculatorService.patch;https://issues.apache.org/jira/secure/attachment/12578272/PendingRangeCalculatorService.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-12 17:13:38.505,,,no_permission,,,,,,,,,,,,322516,,,Fri Apr 12 17:13:38 UTC 2013,,,,,,0|i1jn8v:,322861,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,11/Apr/13 20:05;okibirev;Making a copy of bootstrapTokens rather than holding a lock on the same for entire time consuming loop.,11/Apr/13 20:07;okibirev;Making a copy of bootstrapTokens before a time consuming loop rather than holding a synchronized lock for the whole duration,"12/Apr/13 17:13;brandon.williams;Committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prepared statements from default keyspace are broken,CASSANDRA-5468,12642467,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,pchalamet,pchalamet,14/Apr/13 20:24,12/Mar/19 14:17,13/Mar/19 22:29,15/Apr/13 21:58,1.2.5,,,,,,0,,,,,"Tested under CQL 3 binary protocol.
Preparing a statement from the default keyspace of the connection (statement scoped with keyspace) and then running it will always throw the error ""no keyspace has been specified"".

{code}
Exec: CREATE KEYSPACE Tests WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 1}

Exec: CREATE TABLE Tests.AllTypes (a int, b int, primary key (a))

Prepare: insert into Tests.AllTypes (a, b) values (?, ?)
{code}

Exec prepared statement and exception ""no keyspace has been specified"" is thrown.

Doing a use Tests before preparing the statement solves the issue.
This used to work in 1.2.3.","Windows 8 x64, java 1.7.0_11 x64",,,,,,,,,,,,,,,,,,,,,,,14/Apr/13 23:04;iamaleksey;5468.txt;https://issues.apache.org/jira/secure/attachment/12578655/5468.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-14 23:06:45.12,,,no_permission,,,,,,,,,,,,322881,,,Mon Apr 15 21:58:58 UTC 2013,,,,,,0|i1jphr:,323226,,,,,,,,jbellis,jbellis,,,,,,,,,,"14/Apr/13 23:06;iamaleksey;CASSANDRA-5352 caused this. QueryProcessor.storePreparedStatement() handles keyspace==null case, but it'll never get to this point, because ClientState.getKeyspace() call in prepare() will throw when keyspace==null. 

The attached trivial fix replaces getKeyspace() call with getRawKeyspace(), that will return null without throwing.",15/Apr/13 20:33;pchalamet;works for me. Thanks.,15/Apr/13 21:02;jbellis;+1,"15/Apr/13 21:58;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exceptions in 1.1 nodes with 1.2 nodes in ring,CASSANDRA-5476,12642673,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,dctrwatson,dctrwatson,16/Apr/13 07:36,12/Mar/19 14:17,13/Mar/19 22:29,22/Jun/13 13:58,1.2.6,,,,,,0,,,,,"As 1.1.9 nodes were being upgraded to 1.2.3 nodes, the 1.1.9 nodes started having this exception:

{noformat}
    Exception in thread Thread[RequestResponseStage:19496,5,main]
    java.io.IOError: java.io.EOFException
            at org.apache.cassandra.service.AbstractRowResolver.preprocess(AbstractRowResolver.java:71)
            at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:155)
            at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:45)
            at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
            at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
            at java.lang.Thread.run(Thread.java:662)
    Caused by: java.io.EOFException
            at java.io.DataInputStream.readFully(DataInputStream.java:180)
            at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:100)
            at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:81)
            at org.apache.cassandra.service.AbstractRowResolver.preprocess(AbstractRowResolver.java:64)
            ... 6 more
{noformat}

As more 1.2.3 nodes were upgraded, the 1.2.3 nodes began logging for 1.1.9 node IPs:

{noformat}
    Unable to store hint for host with missing ID, /10.37.62.71 (old node?)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,22/Jun/13 13:18;sbtourist;0001.patch;https://issues.apache.org/jira/secure/attachment/12589252/0001.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-22 13:17:18.199,,,no_permission,,,,,,,,,,,,323087,,,Sat Jun 22 13:58:44 UTC 2013,,,,,,0|i1jqrj:,323432,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"22/Jun/13 13:17;sbtourist;I run into this one too: it is caused by ReadResponse not correctly computing the serialized payload size in case of digest reads; this is not a problem for 1.2, which doesn't really use the payload size to read from the stream, but actually is for 1.1 which uses it for deserializing the ReadResponse coming from a 1.2 node.
To reproduce, just run a replicated cluster with mixed 1.1 and 1.2 nodes, increment the read repair chance to 1, insert some data and query from a 1.1 node (as the problem is the response back from 1.2 nodes).",22/Jun/13 13:58;iamaleksey;Thanks. Committed in 57eb87b57cc7c69d99238ced08e50cc23b0127ba.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Native protocol sanity check,CASSANDRA-5422,12640517,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,danielnorberg,jbellis,jbellis,03/Apr/13 13:56,12/Mar/19 14:17,13/Mar/19 22:29,19/Jun/13 15:09,1.2.6,,,Legacy/CQL,,,1,,,,,"With MutationStatement.execute turned into a no-op, I only get about 33k insert_prepared ops/s on my laptop.  That is: this is an upper bound for our performance if Cassandra were infinitely fast, limited by netty handling the protocol + connections.

This is up from about 13k/s with MS.execute running normally.

~40% overhead from netty seems awfully high to me, especially for insert_prepared where the return value is tiny.  (I also used 4-byte column values to minimize that part as well.)",,,,,,,,,,,,,,,,,,,,,,,,03/Apr/13 13:59;jbellis;5422-test.txt;https://issues.apache.org/jira/secure/attachment/12576786/5422-test.txt,19/May/13 01:10;danielnorberg;ExecuteMessage Profiling - Call Tree.png;https://issues.apache.org/jira/secure/attachment/12583748/ExecuteMessage+Profiling+-+Call+Tree.png,19/May/13 01:10;danielnorberg;ExecuteMessage Profiling - Hot Spots.png;https://issues.apache.org/jira/secure/attachment/12583749/ExecuteMessage+Profiling+-+Hot+Spots.png,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-05-07 19:54:42.714,,,no_permission,,,,,,,,,,,,320977,,,Wed Jun 19 15:06:15 UTC 2013,,,,,,0|i1jdqf:,321318,,,,,,,,slebresne,slebresne,,,,,,,,,,"03/Apr/13 13:59;jbellis;Patch to disable MS.execute (and batch_mutate) attached.

To run the binary protocol stress test,

{{mvn install -Dmaven.test.skip=true}} from java-driver root (https://github.com/datastax/java-driver)

then {{mvn assembly:single}} from driver-example/stress

finally, {{java -jar target/cassandra-driver-examples-stress-1.0.0-beta2-APSHOT-jar-with-dependencies.jar insert_prepared --value-size 4}}","07/May/13 19:54;danielnorberg;The main issues I identified:

* Contention in the driver, i.e. per connection locks taken for every request
* Expensive serialization, i.e. multiple layers of ChannelBuffers used in the ExecuteMessage codec.
* No write batching, i.e. every message results in an expensive syscall.
* Contention in the stress application, bottlenecking on a shared work queue and spawning of one thread per asynchronous worker.

After eliminating contention in the driver and the stress application, optimizing serialization and adding write batching I get a throughput of 200k+ requests per second on my laptop (four core 2Ghz i7 mpb) when making asynchronous requests at a concurrency level of 500. This is with request execution and mutation disabled with the above patch and running both cassandra and the stress tool with Java 7. With this throughput, the benchmark uses a bandwidth of ~60 MB/sec so server grade hardware should be able to saturate 1 Gbit ethernet interfaces, especially with larger payloads.

https://github.com/danielnorberg/java-driver/tree/optimization
https://github.com/danielnorberg/cassandra/tree/transport-benchmark

\\
{noformat}
5/7/13 2:59:54 PM ==============================================================
com.datastax.driver.stress.Reporter:
  latencies:
             count = 352558280
         mean rate = 230848.13 calls/s
     1-minute rate = 223475.90 calls/s
     5-minute rate = 224159.41 calls/s
    15-minute rate = 190931.94 calls/s
               min = 0.27ms
               max = 124.37ms
              mean = 2.16ms
            stddev = 1.63ms
            median = 1.69ms
              75% <= 2.43ms
              95% <= 5.57ms
              98% <= 6.64ms
              99% <= 8.76ms
            99.9% <= 26.57ms

  requests:
             count = 352559217
         mean rate = 230848.12 requests/s
     1-minute rate = 223474.50 requests/s
     5-minute rate = 224159.75 requests/s
    15-minute rate = 190950.27 requests/s
{noformat}

Suggestions for further work:

* Use uniform histogram instead of biased (default) as the biased histogram takes expensive read-write locks for every update, i.e. every request. Or find some way to eliminate the read-write locking in the biased histogram.
* Make StorageProxy non-blocking and use the jsr166e ForkJoinPool instead of normal TPE for a nice throughput boost when working with a large volume of small messages.
* Change protocol to allow more than 128 outstanding requests per connection.

When running normally with request execution enabled I get ~24k rps. Quick profiling indicates that there's some contention points that could be removed, e.g. the ReentrantReadWriteLock (switchLock) in Table. We should be able to optimize the whole stack to the point where a cassandra node can achieve a sustained rate of 100k+ writes per second.

","07/May/13 20:01;jbellis;I have a branch that kills switchLock over on http://github.com/jbellis/cassandra/branches/5064, I'll see about dusting that off...",08/May/13 04:49;jbellis;Created CASSANDRA-5549 for switchLock removal.,"13/May/13 06:50;slebresne;bq. Make StorageProxy non-blocking

For info, CASSANDRA-5239 is open for this (and I have 3 quarters of a patch written, which I'm going to attach soonish).","17/May/13 14:02;slebresne;Thanks a lot Daniel for taking the time to look into that.

I was curious to understand from where the main benefits were coming from so I tried benching the optimizations separately.

First, the baseline on my machine (a quad-core i5 2.80Ghz) with the current java driver and C* 1.2 (with ModificationStatement execute commented out) is about 66K req/s (I'll note that I've already committed some patch to remove the contention on returnConnection in the Java driver and it's included in that baseline). That's with 50 threads (and it's slightly worth with 500 threads).

Long story short, the first bottleneck is the Java driver ""stress"" application.  Which I can't say is a surprise since it was a fairly quick hack primarily meant to check the driver wasn't crashing with more than one thread. [~danielnorberg], I'm happy committing your patch optimizing this, though the patch removes the Apache license from one file and adds some copyright, so wondering if the patches were meant for inclusion or not?

Anyway, even with the stress patch committed, I don't get much improvement yet.  More precisely, by default (synchronous mode, 50 threads) I get 74K, which is slightly better but not amazing. If I try the async mode with 500 threads (to compare with what's coming next), I actually get about 49K.

At this point, the main bottleneck by far seems to be the ArrayBlockingQueue used in the RequestThreadPoolExecutor. Changing it to LinkedBlockingQueue, we get 163K with stress in async mode and 500 threads (which is then the fastest mode: in synchronous mode, I get 95K with 50 threads and 117K with 500 threads).

So, I've committed that part (to 1.2) since that's such a trivial patch and is clearly the main bottleneck, at least Cassandra side. On trunk, I'll note that if we go ahead with CASSANDRA-5239, it'll remove RequestThreadPoolExecutor altogether which could improve things even more (though it's possible that once switched to LinkedBlockingQueue, it's not much of a bottleneck anymore).

bq. Expensive serialization, i.e. multiple layers of ChannelBuffers used in the ExecuteMessage codec.

The vague rational here was to avoid a copy of the values (when they are not trivially small). I did tried to quickly bench that patch separately (on top of the other optims) and didn't really saw a difference. Though I didn't saw much difference increasing the value size tbh (could be there is some other bottleneck, like the generation of bigger values for instance, I haven't checked). In any case, before changing the serialization of all messages it's probably worth some more thorough investigation. But I'm not sure we have a ton to win here, if any.

bq. No write batching

I agree that write batching is a good idea. That being said, write batching is often a trade-off between throughput and latency, so ideally I'd like to expose some of the tweaking knobs and/or test it on more realistic and varied scenario.

That being said, testing it (both client and server side) on top of the ABQ->LBQ patch, I get 180K req/s (versus 163K), so about 10% improvement on that test which ain't bad.

As a side note, same question on the license/copyright for the batching parts of the patch than above.
","19/May/13 01:16;danielnorberg;The patches were mostly to show the optimisations I applied to arrive at my benchmark results.

I've fixed the license headers, although I must admit that I'm unsure of what to do about the copyright notice, if something needs to be done.

The ExecuteMessage serialisation optimisation gives me a ~20% throughput boost on top of all other optimisations, which is not bad. I attached two screenshots of the VisualVM profiler highlighting the CompositeChannelBuffer.decompose() method, which gets called to flatten the serialised ExecuteMessage.

The write batcher will only batch writes if they arrive within less than a configurable time interval, default 100 microseconds, after another write. I.e., in low throughput scenarios no batching will be done, keeping latencies low. In high throughput scenarios the buffer is likely to be flushed quickly, providing a negligible latency impact. That said, more testing in order to better understand the behaviour and performance characteristics sounds like a good idea.

Anyway, 100k+ rps is a great improvement over the thrift interface. Great job on the new driver! I'm looking forward to using it =)
","19/May/13 03:36;jbellis;bq. I've fixed the license headers

I didn't see any changes to the github branches above; am I looking in the wrong place?

bq. although I must admit that I'm unsure of what to do about the copyright notice

The ASF requires copyright assignment to it.  So having a {{Copyright (c) 2012-2013 Spotify AB}} in there doesn't work for us.  If Spotify has a policy that they own everything you write unless specified otherwise, we should get them to sign a Corporate CLA: http://www.apache.org/licenses/cla-corporate.txt","19/May/13 10:36;danielnorberg;I amended the commits to restore/add the apache license in the source files and force pushed those branches.

Would Spotify signing the corporate CLA be enough or do we also need to assign copyright to the ASF?

","20/May/13 02:03;jbellis;It looks like I was wrong; they do not actually require copyright assignment, just the grant of a license covered in the CLA:

{quote}
Grant of Copyright License. Subject to the terms and conditions of this Agreement, You hereby grant to the Foundation and to recipients of software distributed by the Foundation a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare derivative works of, publicly display, publicly perform, sublicense, and distribute Your Contributions and such derivative works.
{quote}","20/May/13 02:08;danielnorberg;Ok, I'll try to get the corporate CLA signed asap.","20/May/13 02:09;jbellis;That said, the appropriate place for a Spotify copyright notice would be the NOTICES file:

{quote}
If [a] source file is submitted with a copyright notice included in it, the copyright owner (or owner's agent) must either:
# remove such notices, or
# move them to the NOTICE file associated with each applicable project release, or
# provide written permission for the ASF to make such removal or relocation of the notices.
{quote}

http://www.apache.org/legal/src-headers.html","20/May/13 02:14;jbellis;The rest of that page is interesting reading as well; basically, you retain copyright for your code whether or not it's actually explicitly specified, precisely since no copyright assignment is involved.  So while it's okay to add an entry to NOTICES, it's not actually useful for anything.

Very long thread about this at http://thread.gmane.org/gmane.comp.apache.legal.discuss/95","20/May/13 02:48;danielnorberg;Sounds reasonable. 

Interesting thread. Silly that there needs to be so much legal busywork in open source.",31/May/13 20:24;jbellis;Any progress?,"01/Jun/13 14:35;danielnorberg;I've gotten confirmation that my ICLA has been received by the ASF. Waiting for confirmation that our Corporate CLA has also been successfully submitted.

My changes to the message serialization can be merged at will as they should be covered by the ICLA.

The changes I made to the driver and stress application I have donated to the driver project so you can use them as you wish.

I believe the write batcher could be incorporated in org.apache.cassandra.transport and then used both in the driver and server without any legal difficulties.",03/Jun/13 23:07;danielnorberg;Our corporate CLA has also been submitted.,18/Jun/13 19:25;jbellis;/throws up the [~slebresne] signal,"19/Jun/13 15:06;slebresne;Let sum this up. The most important improvements (in term of performance gain) have already been committed to the Cassandra and the driver. So with the current code, I get up to 170K req/s for the ""sanity check"" (with MutationStatement.execute turned into a no-op). So imo this is saner: the stupid bottlenecks have been removed. So for the sake of a better tracking of changes, I move that we close this ticket as resolved and open specific tickets for the remaining improvements.

Talking of those remaining improvements, the 2 ones from Daniel's patches that remained to be considered/committed are:
# Write batching. I've opened CASSANDRA-5663. for this. I think we're pretty much good to go on that one, but [~danielnorberg], the batch code is in the commit that the other stuffs on you github branch. Would you mind extracting those bits and attach it to CASSANDRA-5663? Also, following what Jonathan pasted above, I think we'd want to move the Spotify copyright from the file header to the NOTICE file.
# Improving serialization. I've opened CASSANDRA-5664 for that one. I'd want to look that one further because 1) I haven't seen the improvement that Daniel has seen so I want to redo my testing, 2) the patch as it stands need a few modifications (it doesn't handle 'null' values correctly for instance) and 3) if we're going to do that change, I want to do it everywhere, not just for execute messages.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-shuffle with JMX usernames and passwords,CASSANDRA-5431,12641138,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,michalm,edong,edong,05/Apr/13 21:44,12/Mar/19 14:17,13/Mar/19 22:29,09/Apr/13 03:23,1.2.4,,,,,,0,,,,,"Unlike nodetool, cassandra-shuffle doesn't allow passing in a JMX username and password. This stops those who want to switch to vnodes from doing so if JMX access requires a username and a password.

Patch to follow.",,,,,,,,,,,,,,,,,,,,,,,,08/Apr/13 16:17;michalm;5431-v2.txt;https://issues.apache.org/jira/secure/attachment/12577556/5431-v2.txt,08/Apr/13 20:45;michalm;5431-v3.txt;https://issues.apache.org/jira/secure/attachment/12577615/5431-v3.txt,05/Apr/13 22:06;edong;CASSANDRA-5431-whitespace.patch;https://issues.apache.org/jira/secure/attachment/12577303/CASSANDRA-5431-whitespace.patch,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-04-08 08:12:46.082,,,no_permission,,,,,,,,,,,,321554,,,Tue Apr 09 03:23:37 UTC 2013,,,,,,0|i1jhb3:,321899,,,,,,,,dbrosius,dbrosius,,,,,,,,,,"05/Apr/13 22:06;edong;My intended changes touch files that have formatting issues relative to the Cassandra formatter settings for Eclipse, so I'm posting a whitespace patch first.","08/Apr/13 08:12;michalm;As far as I remember I wrote a patch for this some time ago when experimenting a bit with switching to vnodes. If you did not start to work on this task (so I won't ""double"" your work ;-) ), I'll check it later today. ","08/Apr/13 16:17;michalm;Yup, I have it (updated to make it apply on your whitespace patch). ","08/Apr/13 18:04;edong;Hi Michał,

Thanks for the information--- I have not started work on this, so please feel free to submit your patch!","08/Apr/13 20:45;michalm;OK. Attaching ""merged"" patch.",08/Apr/13 22:36;dbrosius;There's quite a bunch of miscellaneous formatting changes in 5431-v3.txt that obfuscate this patch. Mind reposting a patch with only the changes for the patch itself?,"08/Apr/13 22:48;jbellis;v2 is just the changes, v3 is changes squashed with the whitespace patch.","09/Apr/13 02:03;dbrosius;5431-v2 doesn't apply cleanly to cassandra-1.2. When fixes up are applied, patch looks good to me, on it's own.

I'd note that passing passwords as Strings is probably not a good idea as there's no way to flush the password from the server memory space, as there's no guarantee that gc will toss that string. Would be better if the interface was char[]'s that can be cleared.

But that's not this patch's problem. +1","09/Apr/13 03:23;dbrosius;thanks, committed as fe8939075999359b8a26a1a0dcf1a7c7bc4531bb to cassandra-1.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PerRowSecondaryIndex isn't notified of row-level deletes,CASSANDRA-5445,12641658,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,beobal,beobal,beobal,09/Apr/13 16:15,12/Mar/19 14:17,13/Mar/19 22:29,09/Apr/13 21:55,1.2.5,,,,,,0,,,,,"Following CASSANDRA-5297, the way PerRowSecondaryIndex updates are handled in AtomicSortedColumns is still not right as it doesn't cater for row level deletes properly. The key is only added MixedIndexUpdater's list of deferred updates if there's a column value being modified. Where an entire row is deleted, we never hit that code so the indexer.commit() in ASC.addAllWithSizeDelta becomes a no-op & the index is not correctly updated. 

Also, SecondaryIndexManager.updaterFor is not actually as efficient as it first seems. For a CF with no per-column indexes and a single per-row index defined, during compaction (when includeRowIndexes == false), we'd expect to be using nullUpdater to no-op the index update for each row being compacted given the updaterFor implementation:

{code}
	return (includeRowIndexes && !rowLevelIndexMap.isEmpty())
           		? new MixedIndexUpdater(key)
               		: indexesByColumn.isEmpty() ? nullUpdater : new PerColumnIndexUpdater(key);
{code}

However, this isn't the case as indexesByColumn is never empty, the reason being that any index *must* be attached to a column, there's just no way to register a PRSI without attaching it to one of the CF's columns. So where a PRSI is present, a new PCIU instance is created for each row during compaction regardless. With that in mind, I'd propose removing the includeRowLevelIndexes argument and just returning nullIndexer if no indexes (PRSI or PRCI) are configured (although I totally acknowledge that fixing index registration so we can register a PRSI without attaching it to a column would be desirable in the long-term).",,,,,,,,,,,,,,,,,,,,,,,,09/Apr/13 16:16;beobal;5445.txt;https://issues.apache.org/jira/secure/attachment/12577817/5445.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-09 21:55:39.958,,,no_permission,,,,,,,,,,,,322074,,,Tue Apr 09 21:55:39 UTC 2013,,,,,,0|i1jkin:,322419,,,,,,,,jbellis,jbellis,,,,,,,,,,09/Apr/13 16:16;beobal;patch against 1.2 attached,09/Apr/13 21:55;jbellis;LGTM; committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add retry mechanism to OTC for non-droppable_verbs,CASSANDRA-5393,12639405,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasobrown,jjordan,jjordan,27/Mar/13 17:59,12/Mar/19 14:17,13/Mar/19 22:29,18/Apr/13 20:56,1.1.12,1.2.5,2.0 beta 1,,,,1,,,,,"Can we add an Ack/Retry around passing merle tree's around in repair?  If the following fails, the repair hangs for ever on the coordinating node.

https://github.com/apache/cassandra/blob/cassandra-1.1.10/src/java/org/apache/cassandra/service/AntiEntropyService.java#L242

{noformat}
            Message message = TreeResponseVerbHandler.makeVerb(local, validator);
            if (!validator.request.endpoint.equals(FBUtilities.getBroadcastAddress()))
                logger.info(String.format(""[repair #%s] Sending completed merkle tree to %s for %s"", validator.request.sessionid, validator.request.endpoint, validator.request.cf));
            ms.sendOneWay(message, validator.request.endpoint);
{noformat}

If the message asking for merkle tree's gets lost, coordinating node hangs for ever as well.",,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5426,18/Apr/13 00:08;jasobrown;5393-v2.patch;https://issues.apache.org/jira/secure/attachment/12579245/5393-v2.patch,18/Apr/13 04:57;jasobrown;5393-v3.patch;https://issues.apache.org/jira/secure/attachment/12579269/5393-v3.patch,18/Apr/13 14:38;jbellis;5393-v4.txt;https://issues.apache.org/jira/secure/attachment/12579329/5393-v4.txt,18/Apr/13 00:02;jasobrown;5393.patch;https://issues.apache.org/jira/secure/attachment/12579244/5393.patch,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-04-02 07:34:06.585,,,no_permission,,,,,,,,,,,,319875,,,Thu Apr 18 20:56:01 UTC 2013,,,,,,0|i1j6xj:,320216,,,,,,,,jbellis,jbellis,,,,,,,,,,"02/Apr/13 07:34;jasobrown;We've got an idea we're testing out here, and will hopefully post a patch in a day or so.",03/Apr/13 23:48;jasobrown;Yuki's ticket is more comprehensive than this one,"18/Apr/13 00:01;jasobrown;At the end of the day, this is what I see happening:

{code}INFO [AntiEntropyStage:1] 2013-03-27 22:48:55,390 AntiEntropyService.java (line 239) repair #80fe25a0-9730-11e2-0000-ebe7011631ff Sending completed merkle tree to /54.246.XXX.YYY for (Geo,GeoCountryMetadata)
DEBUG [WRITE-/54.246.XXX.YYY] 2013-03-27 22:48:55,392 OutboundTcpConnection.java (line 165) error writing to ec2-54-246-XXX.YYY.eu-west-1.compute.amazonaws.com/54.246.XXX.YYY
java.net.SocketException: Connection timed out
at java.net.SocketOutputStream.socketWrite0(Native Method)
at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)
at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
at com.sun.net.ssl.internal.ssl.OutputRecord.writeBuffer(OutputRecord.java:358)
at com.sun.net.ssl.internal.ssl.OutputRecord.write(OutputRecord.java:346)
at com.sun.net.ssl.internal.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:781)
at com.sun.net.ssl.internal.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:753)
at com.sun.net.ssl.internal.ssl.AppOutputStream.write(AppOutputStream.java:100)
at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
at java.io.BufferedOutputStream.write(BufferedOutputStream.java:104)
at java.io.DataOutputStream.write(DataOutputStream.java:90)
at java.io.FilterOutputStream.write(FilterOutputStream.java:80)
at org.apache.cassandra.net.OutboundTcpConnection.write(OutboundTcpConnection.java:200)
at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:152)
at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:126)
{code}

The interesting thing is the ""Connection timed out"" exception message, rather than socket reset (or something similar). So, I'm thinking this might be to keepalive timing out after the connection is broken. I was able to reproduce this exception several times by having my test cluster setup in three ec2 regions (us-west-2, us-east-1, eu-west-1 - three nodes in each), and not sending any traffic for multiple hours. Basically, I'm waiting for the connection to get dropped. Thus, when I went to triggered repair on one of the nodes (usu. starting with us-west-2), I could see where the eu-west-1 nodes would get the request to build the merkle tree, but then failed on sending the tree response with the above exception. I was able to get similar problems when trying a schema update after many hours of cluster idleness.

The attached patch catches the exception when the socket is dead (for whatever reason), and attempts a simple retry by requeueing the message at the end of the backlog queue, with the hope that the next pass will successfully recreate the socket. Note that I'm excluding MessagingService.DROPPABLE_VERBS from retries as it's OK to drop reads/mutates, but it's really those AES and other schema-related messages that I think we'd want to retry.

Admittedly this is a simple mechanism that doesn't try to do anything fancy like exponential backoff, n-levels of configurable retrys, and so on. I'm open to discussion on that, but I'm not sure how much complexity we'd want to build in for that at this point. I think an incremental improvement would go a long way here as we're currently obscuring when messages can't be sent (which is OK for DROPPABLE_VERBS, but those other ones are ones are really important), so added visibility and a retry mechanism will help. 


 ",18/Apr/13 00:08;jasobrown;v2 addresses a potential race condition between disconnecting the bad socket and re-enqueueing the failed message.,18/Apr/13 00:42;krummas;+1,"18/Apr/13 01:41;jbellis;Can we make an Entry subclass instead of saddling each Entry with an extra field that will mostly be unused?

Also, space before open paren. :)","18/Apr/13 04:57;jasobrown;v3 includes Jonathan's suggestions. Created RetryableEntry as a subclass of Entry. Added method shouldRetry() to Entry; Entry will always return false, and RetryableEntry will check it's member boolean.
",18/Apr/13 14:31;jbellis;v4 attached -- easier to show than explain what I meant in English. :),"18/Apr/13 14:38;jbellis;sorry for the attachment churn, decided to improve the comments too :)","18/Apr/13 17:26;jasobrown;I think your patch and my patch are rather similar, but I'm game either way :). However, there is a small bug in Entry.shouldRetry(); you have

{code}return MessagingService.DROPPABLE_VERBS.contains(message.getVerb());{code}

but should be

{code}return !MessagingService.DROPPABLE_VERBS.contains(message.getVerb());{code}

Otherwise we would retry the DROPPABLE_VERBS, which we want to drop.

With that small fix, lgtm.",18/Apr/13 18:18;jbellis;Ship it!,"18/Apr/13 20:56;jasobrown;Changed name of ticket to better describe the change.

Committed to 1.1, 1.2, and trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamilyInputFormat demands OrderPreservingPartitioner when specifying InputRange with tokens,CASSANDRA-5536,12646032,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,lannyripple,lannyripple,03/May/13 19:53,12/Mar/19 14:17,13/Mar/19 22:29,27/May/13 16:28,1.2.6,,,,,,0,hadoop,,,,"When ColumnFamilyInputFormat starts getting splits (via getSplits(...) [ColumnFamilyInputFormat.java:101]) it checks to see if a `jobKeyRange` has been set.  If it has been set it attempts to set the `jobRange`.  However the if block (ColumnFamilyInputFormat.java:124) looks to see if the `jobKeyRange` has tokens but asserts that the OrderPreservingPartitioner must be in use.

This if block should be looking for keys (not tokens).  Code further down (ColumnFamilyInputFormat.java:147) already manages the range if tokens are used but can never be reached.",,,,,,,,,,,,,,,,,,,,,,,,06/May/13 21:53;jbellis;5536-v2.txt;https://issues.apache.org/jira/secure/attachment/12581966/5536-v2.txt,03/May/13 20:02;lannyripple;cassandra-1.2.3-5536.txt;https://issues.apache.org/jira/secure/attachment/12581742/cassandra-1.2.3-5536.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-05-06 21:53:29.17,,,no_permission,,,,,,,,,,,,326391,,,Mon May 27 16:28:12 UTC 2013,,,,,,0|i1kb4v:,326736,,,,,,,,alexliu68,alexliu68,,,,,,,,,,"03/May/13 19:56;lannyripple;Note that the assertion requiring OrderPreservingPartition mentions ""ConfigHelper.setInputKeyRange()"" which no longer exists.",03/May/13 20:02;lannyripple;Suggested changes.,"06/May/13 21:53;jbellis;Analysis LGTM, v2 attached with a bit more cleanup.","21/May/13 04:01;jbellis;Does v2 look good to you, Lanny?",27/May/13 15:47;jbellis;WDYT [~alexliu68]?,27/May/13 16:21;alexliu68;It looks good.,27/May/13 16:28;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Concurrent secondary index updates remove rows from the index,CASSANDRA-5540,12646193,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,beobal,alexeibakanov,alexeibakanov,06/May/13 11:04,12/Mar/19 14:17,13/Mar/19 22:29,09/May/13 22:42,1.2.5,,,Feature/2i Index,,,1,,,,,"Existing rows disappear from secondary index when doing simultaneous updates of a row with the same secondary index value.

Here is a little pycassa script that reproduces a bug. The script inserts 4 rows with same secondary index value, reads those rows back and check that there are 4 of them.
Please run two instances of the script simultaneously in two separate terminals in order to simulate concurrent updates:
{code}
-----scrpit.py START-----
import pycassa
from pycassa.index import *

pool = pycassa.ConnectionPool('ks123')
cf = pycassa.ColumnFamily(pool, 'cf1')

while True:
    for rowKey in xrange(4):
        cf.insert(str(rowKey), {'indexedColumn': 'indexedValue'})

    index_expression = create_index_expression('indexedColumn', 'indexedValue')
    index_clause = create_index_clause([index_expression])
    rows = cf.get_indexed_slices(index_clause)
    length = len(list(rows))
    if length == 4:
        pass
    else:
        print 'found just %d rows out of 4' % length

pool.dispose()

---script.py FINISH---

---schema cli start---
create keyspace ks123
  with placement_strategy = 'NetworkTopologyStrategy'
  and strategy_options = {datacenter1 : 1}
  and durable_writes = true;

use ks123;

create column family cf1
  with column_type = 'Standard'
  and comparator = 'AsciiType'
  and default_validation_class = 'AsciiType'
  and key_validation_class = 'AsciiType'
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and populate_io_cache_on_flush = false
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'
  and caching = 'KEYS_ONLY'
  and column_metadata = [
    {column_name : 'indexedColumn',
    validation_class : AsciiType,
    index_name : 'INDEX1',
    index_type : 0}]
  and compression_options = {'sstable_compression' : 'org.apache.cassandra.io.compress.SnappyCompressor'};
---schema cli finish---
{code}
Test cluster created with 'ccm create --cassandra-version 1.2.4 --nodes 1 --start testUpdate'",,,,,,,,,,,,,,,,,,,,,,,,07/May/13 21:59;beobal;0001-Use-different-index-updater-for-live-updates-compact.patch;https://issues.apache.org/jira/secure/attachment/12582179/0001-Use-different-index-updater-for-live-updates-compact.patch,09/May/13 12:10;beobal;5540.txt;https://issues.apache.org/jira/secure/attachment/12582457/5540.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-05-06 20:01:08.448,,,no_permission,,,,,,,,,,,,326551,,,Fri Sep 20 22:25:28 UTC 2013,,,,,,0|i1kc4f:,326896,,,,,,,,jbellis,jbellis,,,,,,,,,,"06/May/13 20:01;jbellis;Hmm.  It looks like this can happen when multiple inserts happen at the same timestamp, since we delete the existing entry with its own timestamp.  But if the replacement has the same timestamp, then the tombstone wins the tie.

Any clever ideas to fix this [~beobal]?","07/May/13 12:30;beobal;I don't think this is caused by the index updates in KeysSearcher. There, we only compare the values & since this test always writes the same values the index entry is never deemed stale, and so we don't ever write a tombstone. 

The test script does reproduce the issue completely reliably though, so I'll dig in and find the actual cause.","07/May/13 14:58;beobal;Sorry [~jbellis] I misunderstood, I see what you mean now. No clever ideas yet, but I'm working on it.","07/May/13 15:32;jbellis;Can we just special-case StandardUpdater.update to check for oldColumn.equals(column) and no-op that?

(NB I think the compaction code that calls {{remove}} would need to check for {{.equals}} instead of {{==}} as well.)",07/May/13 15:43;beobal;Can we just remove the deletion from StandardUpdater.update() altogether? That seems to be fine for realtime updates (I no longer see the missing rows & all unit tests are passing) but will it screw with compaction?,"07/May/13 15:48;jbellis;Right, the reason that's there is that compaction can't know to purge the stale index entries for values that never made it into an sstable.","07/May/13 21:59;beobal;Checking oldColumn.equals(column) in SU.update() isn't sufficient. I found that even with the short circuit, occasionally the test script would return only 3 of the 4 expected columns. My suspicion is that this is caused by the delete & subsequent insert in SU.update() being non-atomic, though I haven't proved this. Rather than go down that rabbit hole, I've split the Updater implementation into 2 subclasses - LiveUpdater & CompactionUpdater. The difference between them is that the CU behaves like SU and always purges old values, whereas LU just upserts into the index. SIM.updaterFor() now takes a second argument to determine whether the updater is for processing live updates or for use during compaction.

Unit tests pass & the test script runs without issue.
",08/May/13 15:32;jbellis;How does this avoid leaking index entries that will never be cleaned up by compaction?,"09/May/13 12:10;beobal;yes, you're right that's dumb sorry. 

It took me a while, but there's actually 2 issues here. The first, as you identified, is caused by overwrites with identical timestamps and is fixed by making the case where oldColumn.equals(newColumn) a no-op. The second is the window of inconsistency that I mentioned earlier. When the 2 instances of the test script are running, its possible for one to query the index while inbetween the old index entry being deleted & the new one inserted, leading to a ""missing"" result. To address that, I've reversed the order so that the new entry is added before the old one is removed. This should be safe for readers due to the checking for stale values in the index searcher. ",09/May/13 22:42;jbellis;LGTM; committed.,10/May/13 05:43;alexeibakanov;Fabulous! Thank you very much for the fix!,"20/Sep/13 18:38;rcoli;Is it a correct assessment that this issues ""affects"" begins with the initial implementation of 2i in 0.7? If not, when?","20/Sep/13 19:04;jbellis;No, affects 1.2.0+ as indicated.","20/Sep/13 22:25;rcoli;Oh, weird. I see ""affects"" in the history but don't see it in the ""Details"" section.

Probably I just need to configure my JIRA better, sorry for the noise.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop jobs assigns only one mapper in task,CASSANDRA-5544,12646363,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,alexliu68,shamim_ru,shamim_ru,07/May/13 07:00,12/Mar/19 14:17,13/Mar/19 22:29,29/May/13 17:55,1.2.6,,,,,,4,,,,,"We have got very strange beheviour of hadoop cluster after upgrading 
Cassandra from 1.1.5 to Cassandra 1.2.1. We have 5 nodes cluster of Cassandra, where three of them are hodoop slaves. Now when we are submitting job through Pig script, only one map assigns in task running on one of the hadoop slaves regardless of 
volume of data (already tried with more than million rows).
Configure of pig as follows:
export PIG_HOME=/oracle/pig-0.10.0
export PIG_CONF_DIR=${HADOOP_HOME}/conf
export PIG_INITIAL_ADDRESS=192.168.157.103
export PIG_RPC_PORT=9160
export PIG_PARTITIONER=org.apache.cassandra.dht.Murmur3Partitioner


Also we have these following properties in hadoop:
 <property>
 <name>mapred.tasktracker.map.tasks.maximum</name>
 <value>10</value>
 </property>
 <property>
 <name>mapred.map.tasks</name>
 <value>4</value>
 </property>","Red hat linux 5.4, Hadoop 1.0.3, pig 0.11.1",,,,,,,,,,,,,CASSANDRA-5604,,,,,,,,,,28/May/13 20:45;alexliu68;5544-1.txt;https://issues.apache.org/jira/secure/attachment/12585083/5544-1.txt,29/May/13 17:06;alexliu68;5544-2.txt;https://issues.apache.org/jira/secure/attachment/12585247/5544-2.txt,30/May/13 18:10;alexliu68;5544-3.txt;https://issues.apache.org/jira/secure/attachment/12585442/5544-3.txt,28/May/13 17:14;alexliu68;5544.txt;https://issues.apache.org/jira/secure/attachment/12585045/5544.txt,26/May/13 13:00;shamim_ru;Screen Shot 2013-05-26 at 4.49.48 PM.png;https://issues.apache.org/jira/secure/attachment/12584867/Screen+Shot+2013-05-26+at+4.49.48+PM.png,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2013-05-07 07:57:12.032,,,no_permission,,,,,,,,,,,,326721,,,Fri Jun 28 07:27:57 UTC 2013,,,,,,0|i1kd5z:,327066,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,07/May/13 07:57;cscetbon;same issue with Cassandra 1.2.3. I've tested with both RandomPartitioner and Murmur3Partitioner,"07/May/13 08:54;shamim_ru;For more information, here is some threads from mail archive
1) http://www.mail-archive.com/user@cassandra.apache.org/msg29663.html
2) http://www.mail-archive.com/user@cassandra.apache.org/msg28016.html
3) http://www.mail-archive.com/user@cassandra.apache.org/msg29425.html",07/May/13 19:45;brandon.williams;Does 1.1.11 have the same problem?,"26/May/13 12:56;shamim_ru;Cassandra version 1.1.11 have no such problem. I have test in single node cluster and it's created 15 map. 
See attach please.",27/May/13 08:30;cscetbon;So something goes wrong with 1.2.x version,"27/May/13 17:27;brandon.williams;Can you take a look, Alex?  Nothing changed in pig as far I know.","27/May/13 21:42;alexliu68;[~shamim] How many splits do you get for each hadoop node? You can set ConfigHelper.setInputSplitSize to a smaller number to get more mappers for your pig job. The existing CassandraStorage class doesn't set it, so it uses the defualt value of 64k. So if your nodes has less than 64k rows, it will have only one mapper.","27/May/13 21:44;alexliu68;Some changes had been made to CassandraColumnInputFormat class since 1.1.5

e.g.
add describe_splits_ex providing improved split size estimate
patch by Piotr Kolaczkowski; reviewed by jbellis for CASSANDRA-4803","27/May/13 22:08;cscetbon;[~alexliu68] I did some tests with more than 64k row and had only one mapper for the whole cluster. Even if we have less than 64k rows, why don't we have at least one mapper per node (in my case replication_factor=1) to work on rows using data locality. Vnodes are enabled on my cluster, can there be a relation with this option ?","27/May/13 22:13;alexliu68;Yes, if vnode is enale, it creates a lot of smaller splits (which is not preferred, we will fix the vnode hadoop too many small splits issue later), so can you test it with vnode disable.","27/May/13 22:19;cscetbon;But if there are many small splits it doesn't mean that we should have more mappers ? I'm saying that cause you propose to [~shamim_ru] to decrease ConfigHelper.setInputSplitSize exactly for that, right ?
I need one more day to test without vnodes.","27/May/13 22:31;alexliu68;Current implementation only matches one mapper to a split. Existing code doesn't set InputSplitSize (which means we can't change it to a smaller number unless we change the code at setLocation method to do it), so we need more than 64k rows to have more than one mapper per node.

For vnode we need to support a virtual split which combines multiple small splits. ","27/May/13 22:41;cscetbon;okay. I'll test without vnodes and give you a feedback except if [~shamim_ru] confirms that he didn't use vnodes, which I suppose as he upgraded from C* 1.1.5 to 1.2.1","28/May/13 07:04;shamim_ru;[~alexliu68]
1) I am using pig and actually don't know how many split i had (i am very curious to know how to calculate the split count). However i have had more than 30 million rows.
2) I didn't use VNODES.
3) SET mapred.min.split.size 12500000; 
SET mapred.max.split.size 12500000;
 doesn't help at all
4) SET pig.noSplitCombination true; - did some magic trick, we got more than 100 maps but 2 of them (always two maps) got very large Map input records and runs more than hours. 
5) Observe one very interesting thing when used SET pig.noSplitCombination true, a lot of maps created with  	
Map input records 	0 
","28/May/13 16:32;alexliu68;To get the splits for the node, call thrift API client.describe_splits_ex(cfName, range.start_token, range.end_token, splitsize) it returns the split for that node.

where range.start_token and range.end_token is the start and end token of the node, and splitsize is 64 *1024","28/May/13 17:04;alexliu68;[~shamim] I think you already found the answer, SET pig.noSplitCombination true, so Pig doesn't combine the small splits into one mapper. HBase internal code does it as well. I found that C*-1.2.1 update Pig from 0.9.0 version to 0.10.0 version which may cause the behavior changes.

As far as number 4) and number 5) concerns, I think the empty maps/big maps are due to data skewness. If you can first print out the splits, then you can check the rows for each split.

I will add the following code to CassandraStorage.java

job.getConfiguration().setBoolean(""pig.noSplitCombination"", true);",28/May/13 17:14;alexliu68;I attached the patch.,"28/May/13 19:46;cscetbon;AFAIK split combination is used to improve performance. Doesn't it mean the same for cassandra ?
And if performance decreases without split combination, will the performance decrease much more with vnodes ?","28/May/13 20:20;alexliu68;CassandraColumnInputFormat define the split size, so we don't want Pig to override it by combining splits. We can always tune the split size to tune the performance. Next step, we can open up a little bit so that Pig user can specify split size configuration.

Vnode hadoop performance generally decreases, we can do the split combination at Cassandra side to improve the performance, which could be another ticket.",28/May/13 20:45;alexliu68;Version 2 patch is attached. It allows user to define PIG_INPUT_SPLIT_SIZE in the system env,"29/May/13 07:06;shamim_ru;Alex, thank you very much for your quick response. 
However, i am afraid that above patch will not solve the problem i described ""we got more than 100 maps but 2 of them (always two maps) got very large Map input records and runs more than hours - point 4"" - this behavior is unexpected. This means Map input records is not evenly through cluster, most of the maps getting  Map input records = 10000 but only two of them getting more than millions.
Certainly i will do some test through thrift api as you described. 
One more things, would you kindly allows user to define PIG_INPUT_SPLIT_SIZE through cassandra store URL as ""STORE updated INTO 'cassandra://KEYSPACE/CF?allow_deletes=true&PIG_INPUT_SPLIT_SIZE=xxxxxx' USING CassandraStorage()"" instead of system environment.","29/May/13 14:10;cscetbon;Or maybe via a SET PIG_INPUT_SPLIT_SIZE in Pig script ?
[~alexliu68] I open the second ticket to improve performance with vnodes except if you prefer to open it, which could be better :)",29/May/13 17:06;alexliu68;Version 3 is attached. I add split_size as a parameter.,"29/May/13 17:08;alexliu68;[~cscetbon] please open it, someone else may already open it.","29/May/13 17:55;brandon.williams;Committed, with an update to the README to document split_size.","30/May/13 18:10;alexliu68;Version 4 is attached, it removes getting split size as system env",31/May/13 12:46;brandon.williams;I'm fine with leaving that in for now.,31/May/13 13:36;shamim_ru;I have a plan to do some test in weekend ,05/Jun/13 13:13;cscetbon;My tests confirm that I have multiple mappers (1025) and each mapper works on a range of my column family [http://pastebin.com/vL3uC5Ca]. Good job !,05/Jun/13 13:44;shamim_ru;did you run map reduce job through Pig?,"05/Jun/13 13:47;cscetbon;Yes. I used Pig 0.11.1, Hadoop 1.1.2 (as newer versions are not supported [CASSANDRA-5201|https://issues.apache.org/jira/browse/CASSANDRA-5201]) and cassandra 1.2.3 (I added the current patch from git commits and built sources) ","28/Jun/13 07:27;shamim_ru;At last , i could manage a few hours to try the fix. Definitely it's working now, every mapper works on their own range, however i have test in single node cluster with Hadoop 1.1.2 + Pig 0.11.1 and Cassandra 1.2.6. Thankx. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix memorySize bugs,CASSANDRA-5564,12647367,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,carlyeks,jbellis,jbellis,13/May/13 20:04,12/Mar/19 14:17,13/Mar/19 22:29,21/May/13 03:50,1.2.5,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/May/13 20:47;carlyeks;5564-v2.patch;https://issues.apache.org/jira/secure/attachment/12582992/5564-v2.patch,13/May/13 20:15;jbellis;5564.txt;https://issues.apache.org/jira/secure/attachment/12582976/5564.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-05-13 20:47:54.862,,,no_permission,,,,,,,,,,,,327723,,,Mon May 13 21:30:29 UTC 2013,,,,,,0|i1kjmf:,328067,,,,,,,,jbellis,jbellis,,,,,,,,,,"13/May/13 20:13;jbellis;It looks like getFieldSize and getArraySize are only meant to calculate *shallow* sizes.  Deep sizes must be added in separately.  Fixes attached, based on Carl's fix at the bottom of CASSANDRA-4860.","13/May/13 20:47;carlyeks;This seems to improve the readability of the memory size overall.

I thought that the ObjectSizes class could use some comments.

Also, adding comments so that it is easier to figure out where each of the additions is coming from. Also switched to using the TypeSizes instead of just adding seeming arbitrary numbers (plus adds a little self documentation since the field name is included).

There was also another small bug in the RowIndexEntry; it was adding a reference to the size of position. There is only a single long in the class other than a static. This wouldn't significantly overallocate, but just seemed like it should be fixed.","13/May/13 21:30;jbellis;Committed, w/ one final change to IndexedEntry to include the size of its superclass",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up backwards compatibility complexity for 2.0,CASSANDRA-5511,12644273,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,24/Apr/13 03:37,12/Mar/19 14:17,13/Mar/19 22:29,26/Apr/13 19:07,2.0 beta 1,,,,,,0,,,,,"We've supported rolling upgrades (network-compatible for read/write operations) for several releases, but both 1.0 -> 1.1 and 1.1 -> 1.2 required being on a recent release of the immediately prior major series for this to work as desired.

Meanwhile, we still support reading sstables at least back to 0.6 and possibly even earlier.  This makes dealing with changes to the sstable quite challenging; the recently written-and-reverted CASSANDRA-5487 comes to mind.

2.0 is a good place to drop support for sstables older than 1.2.5.  Our experience with network compatibility demonstrates that this is not an unreasonable burden to impose, and the major version number change suggests that this is a logical time to make such a change.",,,,,,,,,,,,,,,,,,,,,,,,24/Apr/13 20:54;krummas;CASSANDRA-5511-on_1.2.patch;https://issues.apache.org/jira/secure/attachment/12580377/CASSANDRA-5511-on_1.2.patch,24/Apr/13 20:54;krummas;CASSANDRA-5511-on_trunk.patch;https://issues.apache.org/jira/secure/attachment/12580376/CASSANDRA-5511-on_trunk.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-04-24 06:08:46.773,,,no_permission,,,,,,,,,,,,324640,,,Fri Apr 26 19:07:52 UTC 2013,,,,,,0|i1k0c7:,324985,,,,,,,,krummas,krummas,,,,,,,,,,"24/Apr/13 04:43;jbellis;Work-in-progress pushed to https://github.com/jbellis/cassandra/commits/5511.

Stuck on two things:

# Not sure how [~krummas] generated the test sstable for LegacyLeveledManifestTest; need to generate a new one at version ic
# Wanted to clean up messages too, but not sure why SuperColumns.java hardcodes VERSION_10.  [~slebresne]?","24/Apr/13 06:08;krummas;iirc i created a throwaway unit test that does a few RMs with a breakpoint to stop SchemaLoader from removing the sstable, then copied it into the legacy dir

want me to do it again for ic?","24/Apr/13 06:12;krummas;does this mean we will support upgrading 1.2 -> 2.1? would be nice if we required 2.0 as well, to get rid of the deprecated json leveled manifest code","24/Apr/13 06:35;slebresne;bq. not sure why SuperColumns.java hardcodes VERSION_10

That was an admittedly clumsy way to have just the DeletionTime of the DeletionInfo serialized (i.e. without serializing the range tombstones part at all (that should be empty anyway)). This can be replaced by a direct call to the DeletionTime serializer. ","24/Apr/13 12:33;jbellis;bq. want me to do it again for ic?

That would be great.  (Ideally I'd like to leave the code to generate it commented out like we do with LegacySSTableTest.)

bq. does this mean we will support upgrading 1.2 -> 2.1?

I guess we can see how much it bugs us by then. :)","24/Apr/13 20:54;krummas;adds an ignored test to 1.2 that generates sstables and refactors the test in trunk

- apply 1.2-patch
- merge 1.2 -> trunk
- apply trunk-patch

if the patches look ok, i can do the git-gymnastics (ie, they can be applied on current trunk/1.2 without breaking anything)","25/Apr/13 15:10;jbellis;Updates pushed to https://github.com/jbellis/cassandra/commits/5511-2.  Done except for LLMT, I think.","25/Apr/13 15:24;jbellis;Oops, missed Marcus's update somehow.  Will have a look.",25/Apr/13 15:52;jbellis;Marcus's patches LGTM.,25/Apr/13 19:41;jbellis;Rebased and pushed to https://github.com/jbellis/cassandra/commits/5511-3.  Tests pass.,"26/Apr/13 09:32;krummas;* we can remove Directories.sstablesNeedMigration(...) (and related)
* remove DefsTable.fixSchemaNanoTimestamps()
* SystemTable.upgradeSystemData() can be cleaned up (LocationInfo is gone in 1.2 right?)
* Remove SystemTable.OLD_STATUS_CF and SystemTable.OLD_HINTS_CF
* CFMetaData has a bunch of @Deprecated fields that can probably be removed.
* Nit: remove sstable.decorateKey(..) and use sstable.partitioner.decorateKey(...) everywhere
* why keep _SHA in FilterFactory.Type? (couldnt find any .ordinal() use)
* Do we use MURMUR2 anywhere?
* I guess we can remove everything testing ""version < MessagingService.VERSION_12"" (or throw appropriate exceptions etc)
","26/Apr/13 16:08;jbellis;Pushed commits to address above, thanks!",26/Apr/13 17:27;krummas;lgtm!,"26/Apr/13 19:07;jbellis;committed!

{{121 files changed, 370 insertions(+), 2257 deletions(-)}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
thrift_max_message_length_in_mb makes long-lived connections error out,CASSANDRA-5529,12645568,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,rtimpe,rtimpe,01/May/13 00:54,12/Mar/19 14:17,13/Mar/19 22:29,21/May/13 18:55,1.1.12,1.2.6,,Legacy/CQL,,,0,,,,,"When running mapreduce jobs that read directly from cassandra, the job will sometimes fail with an exception like this:

java.lang.RuntimeException: com.rockmelt.org.apache.thrift.TException: Message length exceeded: 40
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.maybeInit(ColumnFamilyRecordReader.java:400)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.computeNext(ColumnFamilyRecordReader.java:406)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.computeNext(ColumnFamilyRecordReader.java:329)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.getProgress(ColumnFamilyRecordReader.java:109)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.getProgress(MapTask.java:522)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:547)
	at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:771)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:375)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1132)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)
Caused by: com.rockmelt.org.apache.thrift.TException: Message length exceeded: 40
	at com.rockmelt.org.apache.thrift.protocol.TBinaryProtocol.checkReadLength(TBinaryProtocol.java:393)
	at com.rockmelt.org.apache.thrift.protocol.TBinaryProtocol.readBinary(TBinaryProtocol.java:363)
	at org.apache.cassandra.thrift.Column.read(Column.java:528)
	at org.apache.cassandra.thrift.ColumnOrSuperColumn.read(ColumnOrSuperColumn.java:507)
	at org.apache.cassandra.thrift.KeySlice.read(KeySlice.java:408)
	at org.apache.cassandra.thrift.Cassandra$get_range_slices_result.read(Cassandra.java:12422)
	at com.rockmelt.org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_get_range_slices(Cassandra.java:696)
	at org.apache.cassandra.thrift.Cassandra$Client.get_range_slices(Cassandra.java:680)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.maybeInit(ColumnFamilyRecordReader.java:362)
	... 16 more


In ColumnFamilyRecordReader#initialize, a TBinaryProtocol is created as follows:

TTransport transport = ConfigHelper.getInputTransportFactory(conf).openTransport(socket, conf);
TBinaryProtocol binaryProtocol = new TBinaryProtocol(transport, ConfigHelper.getThriftMaxMessageLength(conf));
client = new Cassandra.Client(binaryProtocol);

But each time a call to cassandra is made, checkReadLength(int length) is called in TBinaryProtocol, which includes this:

readLength_ -= length;
if (readLength_ < 0) {
   throw new TException(""Message length exceeded: "" + length);
}

The result is that readLength_ is decreased each time, until it goes negative and exception is thrown.  This will only happen if you're reading a lot of data and your split size is large (which is maybe why people haven't noticed it earlier).  This happens regardless of whether you use wide row support.

I'm not sure what the right fix is.  It seems like you could either reset the length of TBinaryProtocol after each call or just use a new TBinaryProtocol each time.",,,,,,,,,,,,,,,,,,,,,,,,01/May/13 04:58;jbellis;5529-1.1.txt;https://issues.apache.org/jira/secure/attachment/12581335/5529-1.1.txt,01/May/13 04:39;jbellis;5529.txt;https://issues.apache.org/jira/secure/attachment/12581334/5529.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-05-01 04:39:36.107,,,no_permission,,,,,,,,,,,,325929,,,Tue May 21 18:55:39 UTC 2013,,,,,,0|i1k8a7:,326274,,,,,,,,tjake,tjake,,,,,,,,,,"01/May/13 04:39;jbellis;Rob, your analysis looks spot on.

WTF.  Creating a new TBinaryProtocol for each message would be pretty ludicrous.

The genesis of this readLength_ business is hidden in the murky archives of the Thrift incubator svn repro.  It looks to me like it's kind of a really ugly hack for pre-Framed transports that could call setReadLength in between messages based on some kind of per-application knowledge.  Because I can't think of any use for ""expiring"" a connection after X bytes otherwise.

I don't think we should be using it at all.  Attached is a patch that rips it out, on the Cassandra server side as well.  I feel sorry for any poor bastard who ever pulled his hair out over Cassandra erroring out his connection apparently randomly...",01/May/13 04:58;jbellis;Thought I was on the 1.1 branch when I wrote that patch but it was really 1.2.  Here it is against 1.1 as well.,01/May/13 05:04;jbellis;Note to self: remove our hacked TBinaryProtocol entirely in trunk.,"01/May/13 07:06;rtimpe;Thanks for the patch and the quick turnaround.  Verified on the 1.1 branch that it fixes my problem.

I'm not really familiar with this api, hence my notes about TBinaryProtocol.  You solution makes way more sense :)",21/May/13 03:50;jbellis;[~tjake] Can you review above?,"21/May/13 16:43;tjake;Looks fine.  I filed THRIFT-1975 to get this issue fixed in general
",21/May/13 18:55;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BulkLoader fails with NoSuchElementException,CASSANDRA-5587,12648818,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,julien.ayme@gmail.com,julien.ayme@gmail.com,julien.ayme@gmail.com,22/May/13 05:49,12/Mar/19 14:17,13/Mar/19 22:29,02/Jun/13 03:46,1.2.6,,,Legacy/Tools,,,0,,,,,"When using BulkLoader tool (sstableloader command) to transfer data from a cluster to another, 
a java.util.NoSuchElementException is thrown whenever the directory contains a ""snapshot"" sub directory,
and the bulk load fails.

The fix should be quite simple:
Catch any NoSuchElementException thrown in {{SSTableLoader#openSSTables()}}

The directory structure:
{noformat}
user@cassandrasrv01:~$ ls /var/lib/cassandra/data/Keyspace1/CF1/
Keyspace1-CF1-ib-1872-CompressionInfo.db
Keyspace1-CF1-ib-1872-Data.db
Keyspace1-CF1-ib-1872-Filter.db
Keyspace1-CF1-ib-1872-Index.db
Keyspace1-CF1-ib-1872-Statistics.db
Keyspace1-CF1-ib-1872-Summary.db
Keyspace1-CF1-ib-1872-TOC.txt
Keyspace1-CF1-ib-2166-CompressionInfo.db
Keyspace1-CF1-ib-2166-Data.db
Keyspace1-CF1-ib-2166-Filter.db
Keyspace1-CF1-ib-2166-Index.db
Keyspace1-CF1-ib-2166-Statistics.db
Keyspace1-CF1-ib-2166-Summary.db
Keyspace1-CF1-ib-2166-TOC.txt
Keyspace1-CF1-ib-5-CompressionInfo.db
Keyspace1-CF1-ib-5-Data.db
Keyspace1-CF1-ib-5-Filter.db
Keyspace1-CF1-ib-5-Index.db
Keyspace1-CF1-ib-5-Statistics.db
Keyspace1-CF1-ib-5-Summary.db
Keyspace1-CF1-ib-5-TOC.txt
...
snapshots
{noformat}


The stacktrace: 
{noformat}
user@cassandrasrv01:~$ ./cassandra/bin/sstableloader -v --debug -d cassandrabck01 /var/lib/cassandra/data/Keyspace1/CF1/
null
java.util.NoSuchElementException
        at java.util.StringTokenizer.nextToken(StringTokenizer.java:349)
        at org.apache.cassandra.io.sstable.Descriptor.fromFilename(Descriptor.java:265)
        at org.apache.cassandra.io.sstable.Component.fromFilename(Component.java:122)
        at org.apache.cassandra.io.sstable.SSTable.tryComponentFromFilename(SSTable.java:194)
        at org.apache.cassandra.io.sstable.SSTableLoader$1.accept(SSTableLoader.java:71)
        at java.io.File.list(File.java:1087)
        at org.apache.cassandra.io.sstable.SSTableLoader.openSSTables(SSTableLoader.java:67)
        at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:119)
        at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:67)
{noformat}",,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,22/May/13 07:02;julien.ayme@gmail.com;cassandra-1.2-5587.txt;https://issues.apache.org/jira/secure/attachment/12584257/cassandra-1.2-5587.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-22 06:53:45.293,,,no_permission,,,,,,,,,,,,329148,,,Sun Jun 02 03:46:11 UTC 2013,,,,,,0|i1ksdz:,329488,,,,,,,,dbrosius,dbrosius,,,,,,,,,,"22/May/13 06:16;julien.ayme@gmail.com;The proposed patch, made against branch cassandra-1.2","22/May/13 06:53;dbrosius;the SSTableLoader.openSSTables filenameFilter could immediately ignore directories before even trying SSTable.tryComponentFromFilename. as well.

ie

if (new File(dir, name).isDirectory()) return false;","22/May/13 06:59;julien.ayme@gmail.com;Yes, this is also a valid solution, but there is one use case I could think of which would fail with the same symptom: if a user deliberatly adds other files (like metainfo, ...) in the directory (yes, this is a bad thing to do).

And nothing prevents us from doing both: immediatly ignore directories, and keep the second safe guard catch NSEE.",22/May/13 07:02;julien.ayme@gmail.com;Updated patch: both checks are done,"31/May/13 04:04;dbrosius;works as advertised, altho just my nitpick perhaps but not a big fan of catching RuntimeExceptions, i'd rather just prevent them, perhaps Descriptor.fromFilename could just use String.split and early exit if the number of components wasn't found rather than going thru the morass of using an old school StringTokenizer.",02/Jun/13 03:46;dbrosius;Committed as c0c1492666e44612ec4f3ca5b47ffcd68e85e210 to cassandra-1.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BulkLoader is broken in trunk,CASSANDRA-5542,12646297,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,yukim,yukim,06/May/13 21:20,12/Mar/19 14:17,13/Mar/19 22:29,18/Jul/13 14:28,2.0 beta 2,,,,,,0,,,,,"After CASSANDRA-5015 and CASSANDRA-5521, we need CFMetaData to open SSTable(Reader), especially to get bloom_filter_fp_chance and index_interval.

When using bulkloader, CFMetaData is not available, so we cannot open SSTable to be streamed.",,,,,,,,,,,,,,,,,,,,,,,,17/Jul/13 20:31;yukim;0001-change-streaming-procedure.patch;https://issues.apache.org/jira/secure/attachment/12592845/0001-change-streaming-procedure.patch,16/Jul/13 02:37;yukim;0002-make-BulkLoader-work-with-SSTableReader.patch;https://issues.apache.org/jira/secure/attachment/12592469/0002-make-BulkLoader-work-with-SSTableReader.patch,16/Jul/13 02:37;yukim;0003-update-for-post-CASSANDRA-5171.patch;https://issues.apache.org/jira/secure/attachment/12592468/0003-update-for-post-CASSANDRA-5171.patch,06/Jun/13 18:37;yukim;5542-fix-NPE.txt;https://issues.apache.org/jira/secure/attachment/12586548/5542-fix-NPE.txt,29/May/13 15:55;yukim;5542.txt;https://issues.apache.org/jira/secure/attachment/12585231/5542.txt,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2013-05-29 16:45:25.716,,,no_permission,,,,,,,,,,,,326655,,,Thu Jul 18 14:28:58 UTC 2013,,,,,,0|i1kcrj:,327000,,,,,,,,slebresne,slebresne,,,,,,,,,,"29/May/13 15:55;yukim;There are two classes that need to change:

* DatabaseDescriptor
  We need to set memoryAllocator in order to create IndexSummary.

* SSTableLoader
  Create dummy CFMetadata and pass it to SSTableReader.open, so it can use default values for bf_fp_chance and index_interval.

I'm not sure if that the best approach for second one though.","29/May/13 16:45;jbellis;I'm a little skeptical about the SSTL changes as well.  Seems dangerous to pass a default CFM that claims e.g. comparator that doesn't actually match the data.

I note that we're passing null back in 1.2 as well; what changed?","29/May/13 18:09;yukim;On trunk, we are using bloom_filter_fp_chance and index_interval from CFMetaData when opening.
If we pass null, we get NPE.
",29/May/13 22:03;jbellis;Pushed the DatabaseDescriptor change to trunk.  CASSANDRA-5555 will solve the SSTableLoader problem.,"06/Jun/13 18:36;yukim;Still, sstableloader throws NPE on the execution path at SSTableReader.",06/Jun/13 18:37;yukim;Attaching patch to switch using indexSummary instead of CFMetaData where affected.,"06/Jun/13 18:40;jbellis;sample key count is going to be nonsense, will that break anything?","06/Jun/13 18:45;yukim;hmm, you are right, but it is used to tell the receiving node the estimated key count for creating BF.
Maybe we should query schema first, then bulk load.
I need that anyway if we switch to use cf ID for new streaming protocol(https://issues.apache.org/jira/browse/CASSANDRA-5286?focusedCommentId=13671413&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13671413).","06/Jun/13 18:54;jbellis;It makes me sad to introduce a schema dependency, since that means you can't run sstableloader except on a cluster member.

What if we added key count and cfid to the stats/metadata sstable component?","06/Jun/13 19:06;yukim;bq. It makes me sad to introduce a schema dependency, since that means you can't run sstableloader except on a cluster member.

It does query schema even on 1.2(CASSANDRA-4755), so it is fairly easy to fetch info we need.

bq. What if we added key count and cfid to the stats/metadata sstable component?

This can also be done, but the change requires stats.db to be always present when using bulkloader, which we don't today.","06/Jun/13 19:27;jbellis;Oh, I'm fine with querying remotely a la 4755.  I thought we were talking about opening up the system keyspace locally.","08/Jul/13 22:58;yukim;I added ""querying schema"" part to bulk loader and successfully opened SSTables, but now I'm stuck on the change in streaming.
New streaming now connects from both endpoints, so bulk loader also needs to listen on storage port to receive message from the loading nodes. I think we don't want to make bulk loader a fat client again...
[~slebresne] Any suggestion for fixing this issue?","09/Jul/13 09:40;slebresne;Ahah, yeah that sucks, didn't though of that.

Well, one solution I think might be to change a bit the solution I used in CASSANDRA-5699. Instead of having the initiator node open one connection and the other one opening the other one afterwards, we could make the initiator node open both connections instead sequentially (sending a StreamInit on both, but with a ""isIncoming"" flag to say if the connection should be used by the remote as incoming or ongoing connection).","09/Jul/13 21:00;yukim;Thanks Sylvain.

Attaching 2 patches, 0001 changes streaming 2.0 procedure based on Sylvain's comment above(initiator opens 2 connections), and 0002 to query schema to open SSTableReader for streaming.
","12/Jul/13 14:23;slebresne;On the protocol changes, I'm not sure I'm a big fan of adding 2 new stream messages just for the purpose of setting up connections. I was also liking the idea of having a setup phase that creates both connections before starting to use it, instead of creating one and partially using it to synchronize the creation of the other one as done in the patch.

What I had imagined is that the initiator could just create 2 connections (without really waiting for anything from the other side), sending a StreamInit on each with a flag saying if the connection is the incoming or ongoing one. On receiving any of those connection, the other side would fetch the session and attach the connection if it's the second one, or create the session if it's the first one. The initiator could then start the prepare phase as soon as both connection are open (without waiting for any special message from the other side). Seems a bit simpler to me overall.

What do you think?

The 2nd patch lgtm.
","16/Jul/13 02:37;yukim;Uploaded update patches.

* 0001 changes so that streaming initiator makes two connections. It also changes how StreamManager manages current connection in order to distinguish plans inside the same JVM (for bulkload JMX op and unit test).
* 0002 is updated for sstableloader command output.
* 0003 introduces change in how we create socket from streaming session. CASSANDRA-5171 makes OutboundTCPConnection pool require system table which client like sstableloader doesn't have. Instead, I made newSocket static method so we just can obtain socket for specific host.","16/Jul/13 07:13;slebresne;Looks mostly good. A couple of minor remarks/nits:
 * In ConnectionHandler, initiateOnReceivingSide and the pair attach(Incoming/Outgoing)Socket are basically duplicates. We should keep only one of them.
 * We use to call the StreamSession.start(Socket, ...) (the one on the receiving side) on the streamExecutor because it was creating a connection (and was thus blocking on I/O). Since it's not the case anymore, it's probably overkill. So we could ditch that whole start function and do the attachSocket in SRF.initReceivingSide, which would improve the symmetry of that function.
 * Does separating initiated and receiving stream in StreamManager really buys us anything? (outside slightly complicating things)
 * On failure (StreamSession.onError), we could (and probably should) send a message as long as the outgoing connection is opened (even if the incoming one is broken), so the current isConnected() is possibly a bit restrictive.
 * For consistency sake, can't we rename forOutput to isOutgoing everywhere, and same for isFirst in ConnectionHandler.sendInitMessage.
 * The comment on StreamInitMessage.isForOutgoing is outdated (talk of ACK, INIT_ACK)
 * The comment on on top StreamSession still talks of InitCompleteMessage.
 * The comment in IncomingStreamingSession still mentions INIT_ACK
","17/Jul/13 20:31;yukim;Thanks for the review.
Updated 0001 patch.

bq. Does separating initiated and receiving stream in StreamManager really buys us anything? (outside slightly complicating things)

We need to keep StreamResultFuture of the same plan ID when we are doing streaming on the same JVM. This happens when we bulk load via JMX method, or running StreamingTransferTest unit test.

bq. On failure (StreamSession.onError), we could (and probably should) send a message as long as the outgoing connection is opened (even if the incoming one is broken), so the current isConnected() is possibly a bit restrictive.

You are right. I changed to put null check on outgoing message handler before sending message instead.
","18/Jul/13 08:32;slebresne;One last nit: In ConnectionHandler.sendMessage(), I don't like throwing messages on the floor without warning. I'd rather add some isOutgoingConnected() method in ConnectionHandler and have the caller check that before calling sendMessage. And otherwise, sendMessage() would throw a RuntimeException in that case too.

But other than that, lgtm, ship it!","18/Jul/13 14:28;yukim;Committed with above nit fix.
Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Growing pending compactions,CASSANDRA-5554,12646967,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,m0nstermind,m0nstermind,m0nstermind,10/May/13 06:25,12/Mar/19 14:17,13/Mar/19 22:29,13/May/13 18:45,1.2.5,,,,,,0,,,,,"I noticed on one of our new cassandra production server, that ""pending compactions"" number is steadily growing. The cluster is under low write load, so ""compactions are not keeping up"" was not the case.

A quick investigation shown, that compactions are stopping far before all pending tasks are completed. I also found, that if concurrent_compactors=1, background compactions are not happening at all.

The bug is in BackgroundCompactionTask rescheduling logic. The executor pool ""room control"" code in CompactionManager.submitBackground() does not reschedule next background cycle, if executor.getActiveCount reach maximun pool size, so it is lost forever.

So I patched it to always schedule single background cycle, regardless of the free room in executor pool.",,,,,,,,,,,,,,,,,,,,,,,,10/May/13 06:42;m0nstermind;patch.diff;https://issues.apache.org/jira/secure/attachment/12582588/patch.diff,10/May/13 06:42;m0nstermind;pending_compactions_fixed.png;https://issues.apache.org/jira/secure/attachment/12582587/pending_compactions_fixed.png,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-05-13 18:45:54.968,,,no_permission,,,,,,,,,,,,327324,,,Mon May 13 18:45:54 UTC 2013,,,,,,0|i1kh5r:,327668,,,,,,,,jbellis,jbellis,,,,,,,,,,"13/May/13 18:45;jbellis;committed; thanks!

(unable to add Oleg to the Contributor role, for some reason...)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL now() on prepared statements is evaluated at prepare time and not query execution time,CASSANDRA-5616,12651102,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,mpenet,mpenet,05/Jun/13 12:10,12/Mar/19 14:17,13/Mar/19 22:29,05/Jun/13 18:07,1.2.6,,,Legacy/CQL,,,0,cql3,,,,"insert into some_table (id,time) values (?,now())

On the example above now() will always have the same value, it should probably be evaluated at ""query"" time and not at prepare time. ",,,,,,,,,,,,,,,,,,,,,,,,05/Jun/13 15:12;slebresne;5616.txt;https://issues.apache.org/jira/secure/attachment/12586332/5616.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-05 14:15:32.891,,,no_permission,,,,,,,,,,,,331428,,,Wed Jun 05 18:07:29 UTC 2013,,,,,,0|i1l6dz:,331760,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,05/Jun/13 14:15;iamaleksey;Another option (that I'd prefer) is to reject now() in prepared statements altogether.,"05/Jun/13 15:12;slebresne;Being able to have now() (or any non-pure function really, it's just that now is only one we have so far) can be handy so I'm not in favor of rejecting it altogether.

Now I agree that the only user-visible semantic that make sense for functions is that they are evaluated at execution time. It just happens that if the function is pure and all it's argument are terminal it's more efficient to evaluate it at preparation time without failing that semantic (by definition of a pure function), and that's what the current implementation does. It's just an oversight that now() is not pure and so shouldn't be optimized that way.

Thus attaching a simple patch that correctly distinguishes pure and non pure functions.
","05/Jun/13 17:05;iamaleksey;All right. Given how small a change this is, I agree.

+1","05/Jun/13 18:07;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect handling of blob literals when the blob column is in reverse clustering order,CASSANDRA-5629,12652101,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,10/Jun/13 22:00,12/Mar/19 14:17,13/Mar/19 22:29,10/Jun/13 22:12,1.2.6,,,,,,0,cql3,,,,"Parsing goes through ReversedType.fromString() in this case, and that doesn't strip ""0x"" when calling BytesType.fromString().

The attached patch makes Constants.parsedValue() ReversedType-aware. ",,,,,,,,,,,,,,,,,,,,,,,,10/Jun/13 22:01;iamaleksey;5629.txt;https://issues.apache.org/jira/secure/attachment/12587140/5629.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-10 22:05:30.258,,,no_permission,,,,,,,,,,,,332425,,,Mon Jun 10 22:12:27 UTC 2013,,,,,,0|i1lchz:,332754,,,,,,,,slebresne,slebresne,,,,,,,,,,10/Jun/13 22:05;slebresne;+1,"10/Jun/13 22:12;iamaleksey;Thanks, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when creating column family shortly after multinode startup,CASSANDRA-5631,12652464,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,mserrano,mserrano,12/Jun/13 19:27,12/Mar/19 14:17,13/Mar/19 22:29,19/Feb/14 19:28,1.2.16,2.0.6,2.1 beta1,,,,2,,,,,"I'm testing a 2-node cluster and creating a column family right after the nodes startup.  I am using the Astyanax client.  Sometimes column family creation fails and I see NPEs on the cassandra server:

{noformat}
2013-06-12 14:55:31,773 ERROR CassandraDaemon [MigrationStage:1] - Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.db.DefsTable.addColumnFamily(DefsTable.java:510)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:444)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:354)
	at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:55)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)

{noformat}

{noformat}
2013-06-12 14:55:31,880 ERROR CassandraDaemon [MigrationStage:1] - Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:475)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:354)
	at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:55)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,19/Feb/14 08:47;iamaleksey;5631.txt;https://issues.apache.org/jira/secure/attachment/12629732/5631.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-12 19:29:58.188,,,no_permission,,,,,,,,,,,,332788,,,Wed Feb 19 19:28:18 UTC 2014,,,,,,0|i1leqf:,333117,,,,,,,,slebresne,slebresne,,,,,,,,,,12/Jun/13 19:29;jbellis;Please test 1.2.5,12/Jun/13 21:21;mserrano;Unfortunately we are using the Astyanax client which only supports up to 1.2.2.  Is there anything I can do to detect this case and retry?  or work around it?,"13/Jun/13 19:46;mserrano;I was incorrect regarding Astyanax support.  I just had a classpath issue.  Anyway, I have tested in 1.2.5 and can no longer reproduce.  Thanks!",13/Jun/13 19:49;jbellis;Great; thanks for the followup!,"24/Jan/14 14:10;rlow;I've seen this on Cassandra 1.2.11.  It happens if you create a keyspace, following quickly by creating a column family within that keyspace.  The NPE is thrown because Schema.instance.getTableDefinition returns null for the keyspace but it isn't checked.

In the case I saw, the node that threw the NPE had problems so it wasn't receiving many messages - it didn't get the create keyspace message but did get the create CF message.  Even if a node doesn't have any problems, the ordering of these messages is not guaranteed.  The node will get the create keyspace message some time later (probably about 60 seconds later when another node has noticed the schema version is wrong) but it won't attempt to recreate the CF unless there is a further CF change (create, update or delete) within that keyspace.  Only then is the current cached schema compared with the on disk schema (in DefsTable.mergeColumnFamilies).  It then notices the CF doesn't exist so creates it.  This could never happen, so the node won't ever create the CF (unless it is restarted).

I think a fix would be to catch the NPEs above, and then, on learning about a new keyspace, check to see if any CFs should have been created for that keyspace.

I haven't tried to repro this on 2.0 but the code looks almost identical so I would expect it to still be present.

Could someone reopen the ticket please?",30/Jan/14 18:37;Proggie;I'm just seeing the exact same thing on my 2 node cluster (cassandra 2.0.4).,"06/Feb/14 02:18;iamaleksey;bq. I think a fix would be to catch the NPEs above, and then, on learning about a new keyspace, check to see if any CFs should have been created for that keyspace.

This sounds reasonable to me. Another way would be to send the keyspace mutation serialized along with any column families created/altered messages, so that there will never be an NPE there in the first place. This had actually come up before. Will have a look.",18/Feb/14 20:26;jjordan;If the issue is about the node not having gotten the create KS yet.  Can you not just wait for schema agreement in your client before going on to the next create?  That is how I do things to avoid these kinds of issues.,"19/Feb/14 08:50;iamaleksey;The attached patch sends the serialized keyspace itself with any CF update/create/drop migration, making the NPE in question impossible - the keyspace will always be there now.","19/Feb/14 13:18;slebresne;Lgtm (nit: I'd rename serializeKeyspace to say addSerializedKeyspace).

bq. Can you not just wait for schema agreement in your client before going on to the next create?

For the record, Jeremiah is right that clients are supposed to wait for schema agreement if they want to guarantee the table creation won't fail just after the keyspace one (or alternatively make sure both creation goes through the same coordinator node). Of course, we shouldn't NPE internally if a user don't respect that and that's just what this ticket is about.","19/Feb/14 19:28;iamaleksey;Committed with a nit, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cross-DC bandwidth-saving broken,CASSANDRA-5632,12652525,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,12/Jun/13 23:37,12/Mar/19 14:17,13/Mar/19 22:29,20/Jun/13 20:54,1.2.6,,,,,,0,,,,,"We group messages by destination as follows to avoid sending multiple messages to a remote datacenter:

{code}
        // Multimap that holds onto all the messages and addresses meant for a specific datacenter
        Map<String, Multimap<Message, InetAddress>> dcMessages
{code}

When we cleaned out the MessageProducer stuff for 2.0, this code

{code}
                    Multimap<Message, InetAddress> messages = dcMessages.get(dc);
...
                    messages.put(producer.getMessage(Gossiper.instance.getVersion(destination)), destination);
{code}

turned into

{code}
                    Multimap<MessageOut, InetAddress> messages = dcMessages.get(dc);
...
                    messages.put(rm.createMessage(), destination);
{code}

Thus, we weren't actually grouping anything anymore -- each destination replica was stored under a separate Message key, unlike under the old CachingMessageProducer.",,,,,,,,,,,,,,,,,,,,,,,,18/Jun/13 17:27;jbellis;5632-v2.txt;https://issues.apache.org/jira/secure/attachment/12588416/5632-v2.txt,12/Jun/13 23:38;jbellis;5632.txt;https://issues.apache.org/jira/secure/attachment/12587523/5632.txt,18/Jun/13 15:02;hayato.shimizu;cassandra-topology.properties;https://issues.apache.org/jira/secure/attachment/12588382/cassandra-topology.properties,18/Jun/13 15:02;hayato.shimizu;fix_patch_bug.log;https://issues.apache.org/jira/secure/attachment/12588383/fix_patch_bug.log,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-06-13 22:28:00.649,,,no_permission,,,,,,,,,,,,332849,,,Fri Sep 20 22:02:54 UTC 2013,,,,,,0|i1lf3r:,333177,,,,,,,,dbrosius,dbrosius,,,,,,,,,hayato.shimizu,12/Jun/13 23:38;jbellis;Backport of 62f429337caf0aa83b68720a5904e8527b840c80 from 2.0 to fix.,13/Jun/13 22:28;hayato.shimizu;Tested and fixed the issue.,"18/Jun/13 15:04;hayato.shimizu;The patch fixes the issue of bandwidth-saving.

However, there seems to be two regressive issues being introduced.

1. DC2 coordinator selection by the DC1 coordinator is not equal across all available nodes in DC2. Some nodes in DC2 are unused as coordinators.
2. When using cqlsh, with EACH_QUORUM/ALL, with tracing on, on a row insert, RPC timeout occurs from a node that is not verifiable in the trace output.

Trace output has been attached for a 6 node cluster, DC1:3, DC2:3 replication factor configuration. network-topology configuration is also attached for clarity.","18/Jun/13 15:18;jbellis;bq. Secondary DC coordinator node is always the same node. This introduces a bottleneck in the secondary DC.

It's the same node for a given token range.  When all token ranges are considered, it is evenly spread.

bq. RPC timeout occurs from a node that is not verifiable in the trace output.

Well.  That's not a very useful error message, is it. :)","18/Jun/13 15:29;jbellis;.55 is the forwarding node in DC2.  It logs that it applies the mutation and acks it:

{noformat}
    Enqueuing response to /192.168.56.50 | 05:57:33,825 | 192.168.56.55 |          14785
{noformat}

But there is no ""Processing response from /192.168.56.55"" line logged by .50.  Hmm.","18/Jun/13 16:18;jbellis;You're not running with cross_node_timeout enabled, are you?  Because some of these clocks are minutes apart.

{noformat}
# Enable operation timeout information exchange between nodes to accurately
# measure request timeouts, If disabled cassandra will assuming the request
# was forwarded to the replica instantly by the coordinator
#
# Warning: before enabling this property make sure to ntp is installed
# and the times are synchronized between the nodes.
cross_node_timeout: false
{noformat}","18/Jun/13 17:25;jbellis;I note that .55 doesn't ever log ""Sending message"" to .50 either.  So the message is getting dropped somewhere inside .55's MessagingService.

cross_node_timeout is my best guess.  Next-best guess is that there's a reconnect somehow dropping the message a la CASSANDRA-5393.",18/Jun/13 17:27;jbellis;v2 attached that rebases and does some further cleanup to improve trace messages.,"19/Jun/13 01:59;dbrosius;other than simple FF, +LGTM

-import org.apache.cassandra.tracing.Tracing;
+import org.apache.cassandra.tracing.4Tracing;",19/Jun/13 03:42;jbellis;Committed.  That should give Hayato an easier way to test at least. :),"19/Jun/13 21:03;hayato.shimizu;It seems that issue 1. in my earlier comment was fixed with 1.2.5 by Yuki (CASSANDRA-5424), where in 1.2.4 NetworkTopologyStrategy.calculateNaturalEndpoints HashSet replicas was changed to LinkedHashSet, so please ignore.","20/Jun/13 03:34;enigmacurry;I've [written a dtest|https://github.com/riptano/cassandra-dtest/pull/13/files] that automates the testing of this issue.

This test clearly shows that the coordinator was talking to more than one node in a different datacenter, and the patch resolves that issue. It also verifies that [~hayato.shimizu]'s comment about using the same forwarder is not happening now.

[~jbellis] - I noticed in your [blog post about tracing|http://www.datastax.com/dev/blog/advanced-request-tracing-in-cassandra-1-2] you said not to rely on the activity field, well, that's exactly what I'm doing here. So, +1 to the idea of making those enums so this doesn't break in the future.","20/Jun/13 03:42;jbellis;Thanks, Ryan.  Go ahead and create a ticket for that and I'll put my next junior hire on it. :)","20/Sep/13 18:46;rcoli;Do you have an ""affects"" version for this issue? Description says it started when a re-write for 2.0 started, but it affects 1.2.x so I'm confused? :D",20/Sep/13 22:02;jeromatron;I believe for the issue that was fixed here it originated in 1.2 and was present up through 1.2.5.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix intersection-checking in CompositeType,CASSANDRA-5600,12649995,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,29/May/13 20:09,12/Mar/19 14:17,13/Mar/19 22:29,31/May/13 14:26,,,,,,,0,,,,,"CASSANDRA-5514 introduced the ability to skip entire sstables based on max/min column names in the sstable. This ticket aims to fix a few issues with it:

* dont use ACT.deconstruct on the hot path
* remove assert that compares type count with collected columns",,,,,,,,,,,,,,,,,,,,,,,,30/May/13 08:24;krummas;0001-CASSANDRA-5600-v1.patch;https://issues.apache.org/jira/secure/attachment/12585380/0001-CASSANDRA-5600-v1.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-30 20:16:23.553,,,no_permission,,,,,,,,,,,,330322,,,Fri May 31 14:26:49 UTC 2013,,,,,,0|i1kzl3:,330656,,,,,,,,slebresne,slebresne,,,,,,,,,,"30/May/13 20:16;slebresne;The changes look good. However, the methods in ColumnNameHelper basically assume that all column names (for a given CF) will have the same number of components. But that's not true (it's not true for CQL3 because of collections, but more importantly, since CompositeType doesn't force all components to be set, thrift users may have existing CF where the column names differ widely in number of components). So I believe we should have something along the lines of
{noformat}
List<ByteBuffer> components = Arrays.asList(ct.split(candidate));
int minSize = Math.min(maxSeen.size(), components.size());
for (int i = 0; i < minSize; i++)
    retList.add(ColumnNameHelper.max(maxSeen.get(i), components.get(i), ct.types.get(i)));

List<ByteBuffer> biggest = maxSeen().size() > components().size() ? maxSeen : components;
for (int i = minSize; i < biggest.size(); i++)
    retList.add(bigget.get(i));
{noformat}

Nit: In CompositeType.intersects, could be worth asserting is that minColumnNames.size() == maxColumnNames.size() (since the method does assume it). It's reasonable in that case since both are guaranteed to be the size of the longest column name in the sstable.
","31/May/13 14:26;krummas;thanks, pushed as 1ea5059fe3976ce8f660b46520859c31bb433fda with the comments fixed, and the addition to only at most collect typecount columns (typecount is the number CompositeType.types.size() or -1 if it is a collection)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support custom secondary indexes in CQL,CASSANDRA-5484,12642817,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,bcoverston,bcoverston,16/Apr/13 21:31,12/Mar/19 14:17,13/Mar/19 22:29,03/May/13 16:41,1.2.5,,,Feature/2i Index,,,0,cql3,index,,,"Through thrift users can add custom secondary indexes to the column metadata.

The following syntax is used in PLSQL, and I think we could use something similar.

CREATE INDEX <NAME> ON <TABLE> (<COLUMN>) [INDEXTYPE IS (<TYPENAME>) [PARAMETERS (<PARAM>[, <PARAM>])]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-04-16 21:39:59.959,,,no_permission,,,,,,,,,,,,323231,,,Fri May 03 16:41:46 UTC 2013,,,,,,0|i1jrnj:,323576,,,,,,,,slebresne,slebresne,,,,,,,,,,"16/Apr/13 21:39;jbellis;I have a mild preference for PostgreSQL syntax, which adds no additional keywords (we already have USING and WITH): 

{{CREATE INDEX [<name>] ON <table> [USING <method>] (<columns>) [WITH <parameters>]}}","16/Apr/13 21:45;jbellis;Actually if it wouldn't confuse the parser too much I'd prefer no keyword at all for the index implementation: 

{{CREATE [<method>] INDEX [<name>] ON <table> (<columns>) [WITH <parameters>]}}",16/Apr/13 21:53;jbellis;Let's also split out parameterization to another ticket -- I don't think we support that yet for any index type.,16/Apr/13 21:57;bcoverston;wfm,"22/Apr/13 10:55;slebresne;bq. Let's also split out parameterization to another ticket – I don't think we support that yet for any index type.

Actually, I think we do need it here because that's the way we pass the class name for CUSTOM indexes. So I think this could look like:
{noformat}
CREATE CUSTOM INDEX ON <table>(<column>) WITH class='MyCustomIndex'; 
{noformat}
(I do note that thrift uses 'class_name' in the index options, but for CQL3, using just 'class' would be somewhat more coherent with the compaction strategy.

I will also note that on the thrift side, we have 3 different IndexType: Keys, Composites and Custom. But for CQL3, I am not convinced that making the different between Keys and Composites is a good idea: CQL3 knows enough about the tables to know with one to use. In other words, I think the only <method> we should support here is 'CUSTOM' (or nothing).

And while in theory it is possible to create a KEYS index on a composite table on the thrift side, it's unclear whether it is useful or not for CQL3 and should be supported. So even if we do want to consider it, then I think that this part should be split to a separate ticket.","01/May/13 00:52;iamaleksey;I was going for
{noformat}
CREATE INDEX ON <table>(<column) WITH type = CUSTOM AND options = {'class_name': 'MyCostomIndex', ...};
{noformat}

to allow parametrization for KEYS/COMPOSITES index in the future, but I can't think of what we could actually want to parametrize with them, so scratch that. Let's only leave parametrization for CUSTOM.
But for the sake of consistency with CREATE KEYSPACE and compaction,compression,etc. options for CREATE TABLE, I'd like options to be a map (and replace 'class_name' key with 'class' internally, also for consistency), e.g.

{noformat}
CREATE INDEX ON <table>(<column>) WITH options = {'class': 'MyCustomIndex',...}; 
{noformat}

Not sure about CREATE CUSTOM INDEX vs. just treating CREATE INDEX with non-null options as custom, implicitly. I believe that every time we add a keyword to CQL, even an unreserved one, a kitten dies somewhere, so I'd like to avoid doing that.","01/May/13 07:48;slebresne;bq. I'd like options to be a map (and replace 'class_name' key with 'class' internally, also for consistency)

Make sense to me.

bq. just treating CREATE INDEX with non-null options as custom, implicitly

That would work now, but that slightly frighten me for the future because:
* what if we add some other type of non custom indexes, like say bitmap indexes.
* what if we want to add options for non custom indexes (while this is nice to avoid option when we can, it's not hard to imagine that future improvements to the 2ndary index code might require tweaking knobs for instance).

bq. every time we add a keyword to CQL, even an unreserved one, a kitten dies somewhere

I agree we should avoid new keyword when possible. But that being said, when we add unreserved ones I think there is no real downside for clients. Yes it add some marginal delta to the parser and it's definitively sad for the kitten, but typically I'm not sold that it's worth taking the risk of being blocked if we want to add options to non-custom index later just to avoid adding an unreserved keyword now.

",01/May/13 12:40;iamaleksey;Makes sense.,"03/May/13 00:52;iamaleksey;https://github.com/iamaleksey/cassandra/compare/5484 (https://github.com/iamaleksey/cassandra/compare/5484.patch)

Also refactored CreateIndexStatement to split validation into validate() instead of the mess in announceMigration().

Once this has been reviewed, but before resolving the issue, will need to:
1. Write a dtest
2. Update cqlsh completion
3. Update NEWS + cql docs","03/May/13 14:02;slebresne;lgtm, +1","03/May/13 16:41;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in net.OutputTcpConnection when tracing is enabled,CASSANDRA-5668,12653785,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,enigmacurry,enigmacurry,19/Jun/13 19:07,12/Mar/19 14:17,13/Mar/19 22:29,21/Jun/13 19:19,1.2.6,,,,,,0,pull-request-available,qa-resolved,,,"I get multiple NullPointerException when trying to trace INSERT statements.

To reproduce:
{code}
$ ccm create -v git:trunk
$ ccm populate -n 3
$ ccm start
$ ccm node1 cqlsh < 5668_npe_ddl.cql
$ ccm node1 cqlsh < 5668_npe_insert.cql
{code}

And see many exceptions like this in the logs of node1:
{code}
ERROR [WRITE-/127.0.0.3] 2013-06-19 14:54:35,885 OutboundTcpConnection.java (line 197) error writing to /127.0.0.3
java.lang.NullPointerException
        at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:182)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:144)
{code}


This is similar to CASSANDRA-5658 and is the reason that npe_ddl and npe_insert are separate files.",,"Github user dineshjoshi commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/253#discussion_r216105848
  
    --- Diff: src/java/org/apache/cassandra/net/MessageOut.java ---
    @@ -180,6 +199,73 @@ public String toString()
             return sbuf.toString();
         }
     
    +    /**
    +     * The main entry point for sending an internode message to a peer node in the cluster.
    +     */
    +    public void serialize(DataOutputPlus out, int messagingVersion, OutboundConnectionIdentifier destinationId, int id, long timestampNanos) throws IOException
    +    {
    +        captureTracingInfo(destinationId);
    +
    +        out.writeInt(MessagingService.PROTOCOL_MAGIC);
    +        out.writeInt(id);
    +
    +        // int cast cuts off the high-order half of the timestamp, which we can assume remains
    +        // the same between now and when the recipient reconstructs it.
    +        out.writeInt((int) NanoTimeToCurrentTimeMillis.convert(timestampNanos));
    +        serialize(out, messagingVersion);
    +    }
    +
    +    /**
    +     * Record any tracing data, if enabled on this message.
    +     */
    +    @VisibleForTesting
    +    void captureTracingInfo(OutboundConnectionIdentifier destinationId)
    +    {
    +        try
    +        {
    +            UUID sessionId =  (UUID)getParameter(ParameterType.TRACE_SESSION);
    +            if (sessionId != null)
    +            {
    +                TraceState state = Tracing.instance.get(sessionId);
    +                String logMessage = String.format(""Sending %s message to %s"", verb, destinationId.connectionAddress());
    +                // session may have already finished; see CASSANDRA-5668
    +                if (state == null)
    +                {
    +                    Tracing.TraceType traceType = (Tracing.TraceType)getParameter(ParameterType.TRACE_TYPE);
    +                    traceType = traceType == null ? Tracing.TraceType.QUERY : traceType;
    +                    Tracing.instance.trace(ByteBuffer.wrap(UUIDGen.decompose(sessionId)), logMessage, traceType.getTTL());
    +                }
    +                else
    +                {
    +                    state.trace(logMessage);
    +                    if (verb == MessagingService.Verb.REQUEST_RESPONSE)
    +                        Tracing.instance.doneWithNonLocalSession(state);
    +                }
    +            }
    +        }
    +        catch (Exception e)
    +        {
    +            logger.warn(""failed to capture the tracing info for an outbound message to {}, ignoring"", destinationId, e);
    +        }
    +    }
    +
    +    private Object getParameter(ParameterType type)
    +    {
    +        for (int ii = 0; ii < parameters.size(); ii += PARAMETER_TUPLE_SIZE)
    +        {
    +            if (((ParameterType)parameters.get(ii + PARAMETER_TUPLE_TYPE_OFFSET)).equals(type))
    --- End diff --
    
    Don't need the typecast to `ParameterType`
;07/Sep/18 23:09;githubbot;600","Github user jasobrown commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/253#discussion_r216157097
  
    --- Diff: src/java/org/apache/cassandra/net/MessageOut.java ---
    @@ -180,6 +199,73 @@ public String toString()
             return sbuf.toString();
         }
     
    +    /**
    +     * The main entry point for sending an internode message to a peer node in the cluster.
    +     */
    +    public void serialize(DataOutputPlus out, int messagingVersion, OutboundConnectionIdentifier destinationId, int id, long timestampNanos) throws IOException
    +    {
    +        captureTracingInfo(destinationId);
    +
    +        out.writeInt(MessagingService.PROTOCOL_MAGIC);
    +        out.writeInt(id);
    +
    +        // int cast cuts off the high-order half of the timestamp, which we can assume remains
    +        // the same between now and when the recipient reconstructs it.
    +        out.writeInt((int) NanoTimeToCurrentTimeMillis.convert(timestampNanos));
    +        serialize(out, messagingVersion);
    +    }
    +
    +    /**
    +     * Record any tracing data, if enabled on this message.
    +     */
    +    @VisibleForTesting
    +    void captureTracingInfo(OutboundConnectionIdentifier destinationId)
    +    {
    +        try
    +        {
    +            UUID sessionId =  (UUID)getParameter(ParameterType.TRACE_SESSION);
    +            if (sessionId != null)
    +            {
    +                TraceState state = Tracing.instance.get(sessionId);
    +                String logMessage = String.format(""Sending %s message to %s"", verb, destinationId.connectionAddress());
    +                // session may have already finished; see CASSANDRA-5668
    +                if (state == null)
    +                {
    +                    Tracing.TraceType traceType = (Tracing.TraceType)getParameter(ParameterType.TRACE_TYPE);
    +                    traceType = traceType == null ? Tracing.TraceType.QUERY : traceType;
    +                    Tracing.instance.trace(ByteBuffer.wrap(UUIDGen.decompose(sessionId)), logMessage, traceType.getTTL());
    +                }
    +                else
    +                {
    +                    state.trace(logMessage);
    +                    if (verb == MessagingService.Verb.REQUEST_RESPONSE)
    +                        Tracing.instance.doneWithNonLocalSession(state);
    +                }
    +            }
    +        }
    +        catch (Exception e)
    +        {
    +            logger.warn(""failed to capture the tracing info for an outbound message to {}, ignoring"", destinationId, e);
    +        }
    +    }
    +
    +    private Object getParameter(ParameterType type)
    +    {
    +        for (int ii = 0; ii < parameters.size(); ii += PARAMETER_TUPLE_SIZE)
    +        {
    +            if (((ParameterType)parameters.get(ii + PARAMETER_TUPLE_TYPE_OFFSET)).equals(type))
    --- End diff --
    
    done
;09/Sep/18 13:13;githubbot;600",,0,1200,,,0,1200,,,,,,,,,,,,,,19/Jun/13 22:15;jbellis;5668-assert-2.txt;https://issues.apache.org/jira/secure/attachment/12588704/5668-assert-2.txt,19/Jun/13 20:46;jbellis;5668-assert.txt;https://issues.apache.org/jira/secure/attachment/12588684/5668-assert.txt,21/Jun/13 15:25;jbellis;5668-followup.txt;https://issues.apache.org/jira/secure/attachment/12589085/5668-followup.txt,19/Jun/13 19:11;enigmacurry;5668-logs.tar.gz;https://issues.apache.org/jira/secure/attachment/12588666/5668-logs.tar.gz,20/Jun/13 15:25;jbellis;5668.txt;https://issues.apache.org/jira/secure/attachment/12588856/5668.txt,19/Jun/13 19:10;enigmacurry;5668_npe_ddl.cql;https://issues.apache.org/jira/secure/attachment/12588665/5668_npe_ddl.cql,19/Jun/13 19:10;enigmacurry;5668_npe_insert.cql;https://issues.apache.org/jira/secure/attachment/12588664/5668_npe_insert.cql,19/Jun/13 22:23;enigmacurry;system.log;https://issues.apache.org/jira/secure/attachment/12588710/system.log,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,2013-06-19 20:46:39.087,,,no_permission,,,,,,,,,,,,334062,,,Fri Jun 21 19:19:35 UTC 2013,,,,,,0|i1lmkv:,334388,,,,,,,,slebresne,slebresne,,,,,,,,,enigmacurry,"19/Jun/13 19:27;enigmacurry;I've tried to introduce sleeps in between INSERTs, even with 30s pauses between them I still see the same in the logs. I have also seen this fail on the very first INSERT.","19/Jun/13 19:32;enigmacurry;This is what one the traces looks like that failed:

{code}
127.0.0.1	78	Parsing statement	Thrift:1
127.0.0.1	606	Peparing statement	Thrift:1
127.0.0.1	949	Determining replicas for mutation	Thrift:1
127.0.0.1	3577	Acquiring switchLock read lock	MutationStage:1
127.0.0.1	3622	Appending to commitlog	MutationStage:1
127.0.0.1	3754	Adding to test memtable	MutationStage:1
127.0.0.1	4936	Sending message to /127.0.0.2	WRITE-/127.0.0.2
127.0.0.2	37	Message received from /127.0.0.1	Thread-4
127.0.0.2	562	Acquiring switchLock read lock	MutationStage:1
127.0.0.2	583	Appending to commitlog	MutationStage:1
127.0.0.1	32	Message received from /127.0.0.2	Thread-7
127.0.0.2	634	Adding to test memtable	MutationStage:1
127.0.0.2	1033	Enqueuing response to /127.0.0.1	MutationStage:1
127.0.0.2	1217	Sending message to /127.0.0.1	WRITE-/127.0.0.1
127.0.0.1	132	Processing response from /127.0.0.2	RequestResponseStage:3
{code}

With a replication factor of 3 it should be writing to all three nodes (127.0.0.1, 127.0.0.2, and 127.0.0.3) - 127.0.0.3 is conspicuously missing from that trace.",19/Jun/13 20:46;jbellis;attached additional assert to see where we are trying to trace without setting up the session first,"19/Jun/13 22:08;enigmacurry;I ran with the patch, but I did not see any assertion errors.",19/Jun/13 22:15;jbellis;Hmm.  Take 2.,"19/Jun/13 22:23;enigmacurry;OK, that prodcued the assertion errors. See attached system.log

Also, I saw a lot of this in the cqlsh terminal:
{code}
<stdin>:9:Request did not complete within rpc_timeout.
<stdin>:10:Request did not complete within rpc_timeout.
<stdin>:11:'NoneType' object is not iterable
<stdin>:12:Request did not complete within rpc_timeout.
<stdin>:13:Request did not complete within rpc_timeout.
<stdin>:14:'NoneType' object is not iterable
<stdin>:15:'NoneType' object is not iterable
<stdin>:16:Request did not complete within rpc_timeout.
<stdin>:17:'NoneType' object is not iterable
<stdin>:18:Request did not complete within rpc_timeout.
<stdin>:19:Request did not complete within rpc_timeout.
<stdin>:20:'NoneType' object is not iterable
<stdin>:21:'NoneType' object is not iterable
<stdin>:22:'NoneType' object is not iterable
{code} ","19/Jun/13 22:32;jbellis;So, .2 is sending two messages for this session, and the first is deleting the session when it's done.  (We know that the session is getting created correctly since the first patch, that checks at message send time, works fine.)

The part I don't understand is, why two messages from .2?  There should only be one (responding to .1).

(This could be problematic for cross-dc replication, although so far tracing seems to be working for CASSANDRA-5632.  But for same-dc, one message per replica should be straightforward.)","19/Jun/13 22:44;jbellis;Are we actually creating multiple sessions?

Edit: Yes.","20/Jun/13 04:49;jbellis;Okay, here's what's happening.  I added logging of session cleanup:

{noformat}
 INFO [Thrift:1] 2013-06-19 23:36:51,719 Tracing.java (line 176) session 0702a620-d963-11e2-832d-53376523a4a2 is complete

java.lang.AssertionError: Asked to trace TYPE:MUTATION VERB:MUTATION for session 0702a620-d963-11e2-832d-53376523a4a2 but that state does not exist
{noformat}

cqlsh is requesting QUORUM CL (or ONE?) so once that's achieved the coordinator sends success to the client and closes the tracing session.

if other messages have not yet gone out, then we error.

But it gets worse...

Once the coordinator's state is discarded, any late-arriving replies will create a new, ""non-local"" session.  Since the coordinator will not send any messages again for this session -- which is the trigger we use on replicas to indicate ""we're done"" -- the nonlocal session will persist indefinitely, ""leaking"" memory.

I think we can solve both of these:
# Make a static TraceState method that only needs the sessionid to be passed in to log an event.  OTC can use this to avoid having to look up tracestate at all; if it's cleared out, not a problem.
# Make Tracing.sessions an expiring map so sessions we don't clean up manually still get removed

Alternatively we could just go with #2 by itself and not try to cleanup manually at all.  Average case memory used will be worse, but maybe that is okay since we assume only a tiny fraction of requests are traced at all.

What do you think [~slebresne]?","20/Jun/13 08:38;slebresne;For what it's worth, I think that for the 2nd problem, another option might be to make Tracing.initializeMessage behave slightly differently depending on the message type. So if the state doesn't exist but the message type is a REQUEST_RESPONSE, we could create the state and set it in the threadLocal, but not save it in the global state map.

It's a bit of a hack though, but it slightly bother me to leave this to expiration either so .... ","20/Jun/13 15:25;jbellis;Good idea to check for REQUEST_RESPONSE, although it's not quite as easy as it sounds since we still need to be able to inject the TraceState into the executor stage.  Patch attached.

(Note that once the session is closed we won't know elapsed time anymore.  I don't see a good way around this.)","21/Jun/13 08:46;slebresne;I don't think this patch can be committed as is: in both TracingExecutorService and ExpiredTracingState, the package declaration is before the license header, and I can't let that slide.

But with that fixed, +1.",21/Jun/13 14:28;jbellis;Fixed and committed. :),"21/Jun/13 15:25;jbellis;followup patch to make session cleanup check for REQUEST_RESPONSE instead of local-ness, to keep from destroying session prematurely in RMVH forwarding case.",21/Jun/13 15:31;slebresne;+1 on that follow up.,21/Jun/13 15:42;jbellis;committed.,"21/Jun/13 15:44;enigmacurry;The NullPointerException is gone, however, I see an unexpected change in the trace after this commit.

Prior to the patch, I do this:
{code}
$ ccm create 5668
Current cluster is now: 5668
$ ccm populate -n 2:2
$ ccm start
$ ccm node1 cqlsh
Connected to 5668 at 127.0.0.1:9160.
[cqlsh 4.0.0 | Cassandra 2.0-SNAPSHOT | CQL spec 3.1.0 | Thrift protocol 19.37.0]
Use HELP for help.
cqlsh> CREATE KEYSPACE test WITH replication = {'class': 'NetworkTopologyStrategy', 'dc1':2, 'dc2':2};
cqlsh> CREATE TABLE test.test (id int PRIMARY KEY, value text);
cqlsh> tracing on;
Now tracing requests.
cqlsh> INSERT INTO test.test (id, value) VALUES ( 5, 'asdf');

Tracing session: 5d3bdaa0-da87-11e2-9eaa-35db2404c433

 activity                                                       | timestamp    | source    | source_elapsed
----------------------------------------------------------------+--------------+-----------+----------------
                                             execute_cql3_query | 11:29:29,423 | 127.0.0.1 |              0
 Parsing INSERT INTO test.test (id, value) VALUES ( 5, 'asdf'); | 11:29:29,424 | 127.0.0.1 |           1082
                                             Peparing statement | 11:29:29,425 | 127.0.0.1 |           2241
                              Determining replicas for mutation | 11:29:29,425 | 127.0.0.1 |           2617
                                  Sending message to /127.0.0.2 | 11:29:29,432 | 127.0.0.1 |           9321
                                 Acquiring switchLock read lock | 11:29:29,433 | 127.0.0.1 |          10376
                                         Appending to commitlog | 11:29:29,433 | 127.0.0.1 |          10424
                                        Adding to test memtable | 11:29:29,433 | 127.0.0.1 |          10544
                                  Sending message to /127.0.0.3 | 11:29:29,435 | 127.0.0.1 |          12658
                               Message received from /127.0.0.1 | 11:29:29,436 | 127.0.0.2 |            102
                               Message received from /127.0.0.1 | 11:29:29,438 | 127.0.0.3 |            103
                        Enqueuing forwarded write to /127.0.0.4 | 11:29:29,441 | 127.0.0.3 |           3596
                                 Acquiring switchLock read lock | 11:29:29,441 | 127.0.0.3 |           3686
                                         Appending to commitlog | 11:29:29,441 | 127.0.0.3 |           3704
                                 Acquiring switchLock read lock | 11:29:29,442 | 127.0.0.2 |           5768
                                        Adding to test memtable | 11:29:29,442 | 127.0.0.3 |           3759
                                         Appending to commitlog | 11:29:29,442 | 127.0.0.2 |           5821
                                  Sending message to /127.0.0.4 | 11:29:29,442 | 127.0.0.3 |           3834
                                        Adding to test memtable | 11:29:29,442 | 127.0.0.2 |           5896
                               Enqueuing response to /127.0.0.1 | 11:29:29,442 | 127.0.0.3 |           4307
                               Enqueuing response to /127.0.0.1 | 11:29:29,443 | 127.0.0.2 |           6602
                               Message received from /127.0.0.3 | 11:29:29,444 | 127.0.0.4 |             92
                                  Sending message to /127.0.0.1 | 11:29:29,444 | 127.0.0.2 |           8037
                               Message received from /127.0.0.2 | 11:29:29,445 | 127.0.0.1 |             36
                            Processing response from /127.0.0.2 | 11:29:29,445 | 127.0.0.1 |            147
                                 Acquiring switchLock read lock | 11:29:29,452 | 127.0.0.4 |           8791
                                         Appending to commitlog | 11:29:29,452 | 127.0.0.4 |           8835
                                        Adding to test memtable | 11:29:29,452 | 127.0.0.4 |           8897
                               Enqueuing response to /127.0.0.1 | 11:29:29,453 | 127.0.0.4 |           9542
                                  Sending message to /127.0.0.1 | 11:29:29,454 | 127.0.0.4 |          10021
                               Message received from /127.0.0.4 | 11:29:29,454 | 127.0.0.1 |           9660
                            Processing response from /127.0.0.4 | 11:29:29,455 | 127.0.0.1 |           9804
                                               Request complete | 11:29:29,435 | 127.0.0.1 |          12668
{code}

That's a 2x2 multi-dc cluster with a RF of 2 in each dc. I write a single INSERT and I see four nodes append to their commit log. All is well.

After the patch, I see this trace:

{code}
Tracing session: c3da4d00-da87-11e2-9a95-35db2404c433

 activity                                                       | timestamp    | source    | source_elapsed
----------------------------------------------------------------+--------------+-----------+----------------
                                             execute_cql3_query | 11:32:21,587 | 127.0.0.1 |              0
 Parsing INSERT INTO test.test (id, value) VALUES ( 5, 'asdf'); | 11:32:21,587 | 127.0.0.1 |            495
                                             Peparing statement | 11:32:21,588 | 127.0.0.1 |            962
                              Determining replicas for mutation | 11:32:21,588 | 127.0.0.1 |           1108
                                  Sending message to /127.0.0.2 | 11:32:21,590 | 127.0.0.1 |           3341
                                  Sending message to /127.0.0.3 | 11:32:21,590 | 127.0.0.1 |           3920
                               Message received from /127.0.0.1 | 11:32:21,592 | 127.0.0.2 |             91
                               Message received from /127.0.0.1 | 11:32:21,593 | 127.0.0.3 |             93
                                 Acquiring switchLock read lock | 11:32:21,593 | 127.0.0.2 |           1398
                                         Appending to commitlog | 11:32:21,593 | 127.0.0.2 |           1447
                                        Adding to test memtable | 11:32:21,593 | 127.0.0.2 |           1517
                        Enqueuing forwarded write to /127.0.0.4 | 11:32:21,594 | 127.0.0.3 |           1266
                               Enqueuing response to /127.0.0.1 | 11:32:21,594 | 127.0.0.2 |           2573
                                 Acquiring switchLock read lock | 11:32:21,594 | 127.0.0.3 |           1343
                                         Appending to commitlog | 11:32:21,594 | 127.0.0.3 |           1360
                                        Adding to test memtable | 11:32:21,594 | 127.0.0.3 |           1412
                               Enqueuing response to /127.0.0.1 | 11:32:21,595 | 127.0.0.3 |           2060
                               Message received from /127.0.0.2 | 11:32:21,595 | 127.0.0.1 |           null
                                  Sending message to /127.0.0.1 | 11:32:21,595 | 127.0.0.2 |           2850
                                  Sending message to /127.0.0.1 | 11:32:21,595 | 127.0.0.3 |           2442
                            Processing response from /127.0.0.2 | 11:32:21,595 | 127.0.0.1 |           null
                                  Sending message to /127.0.0.4 | 11:32:21,595 | 127.0.0.3 |           2442
                               Message received from /127.0.0.3 | 11:32:21,595 | 127.0.0.1 |           null
                            Processing response from /127.0.0.3 | 11:32:21,595 | 127.0.0.1 |           null
                               Message received from /127.0.0.3 | 11:32:21,597 | 127.0.0.4 |             95
                                 Acquiring switchLock read lock | 11:32:21,598 | 127.0.0.4 |           1040
                                         Appending to commitlog | 11:32:21,598 | 127.0.0.4 |           1087
                                        Adding to test memtable | 11:32:21,598 | 127.0.0.4 |           1391
                               Enqueuing response to /127.0.0.1 | 11:32:21,599 | 127.0.0.4 |           1885
                               Message received from /127.0.0.4 | 11:32:21,600 | 127.0.0.1 |           null
                                  Sending message to /127.0.0.1 | 11:32:21,600 | 127.0.0.4 |           3290
                            Processing response from /127.0.0.4 | 11:32:21,600 | 127.0.0.1 |           null
                                               Request complete | 11:32:21,592 | 127.0.0.1 |           5564
{code}

127.0.0.1 (which also happens to be the coordinator) didn't write anything to it's commit log (according to the trace at least). Reproducible on cassandra-1.2 and trunk.

",21/Jun/13 15:46;jbellis;Did you get the followup patch in that?  Should have addressed it.,"21/Jun/13 15:51;enigmacurry;I've retested with the second patch, it's the same.",21/Jun/13 16:13;jbellis;fix pushed in 110d283afd780774a44368b17177b5e8e781e37f,"21/Jun/13 16:21;enigmacurry;That works in cassandra-1.2 - if you commit to trunk, I'll test there too.",21/Jun/13 18:09;jbellis;Merged to trunk.,21/Jun/13 19:19;enigmacurry;+1 - all my tests are passing now. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CQL3 should not allow ranges on the partition key without the token() method, even for byte ordered partitioner.",CASSANDRA-5666,12653773,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,19/Jun/13 18:05,12/Mar/19 14:17,13/Mar/19 22:29,20/Jun/13 17:12,1.2.6,,,,,,0,,,,,"When the partition is an ordered one, CQL3 currently allows non-equal conditions on the partition key directly. I.e. we allow
{noformat}
CREATE TABLE t (k timeuuid PRIMARY KEY);
SELECT * FROM t WHERE k > ... AND k < ...;
{noformat}
but this is a bug because even ordered partitioner don't order following the type of the partition key. They order by bytes, always.

So that type of query doesn't do in general what it is supposed to do and we should disallow it. Even for ordered partitioner, the token() function should be used. ",,,,,,,,,,,,,,,,,,,,,,,,20/Jun/13 11:08;slebresne;5666.txt;https://issues.apache.org/jira/secure/attachment/12588825/5666.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-20 14:47:32.168,,,no_permission,,,,,,,,,,,,334050,,,Thu Jun 20 17:12:28 UTC 2013,,,,,,0|i1lmi7:,334376,,,,,,,,jbellis,jbellis,,,,,,,,,,"20/Jun/13 11:08;slebresne;Attached trivial patch for this (that include update of the documentation).

Just to illustrate the problem we currently have, consider (where BytesOrderingPartitioner is used):
{noformat}
cqlsh:ks> CREATE TABLE test ( k int PRIMARY KEY);
cqlsh:ks> INSERT INTO test(k) VALUES (1);
cqlsh:ks> INSERT INTO test(k) VALUES (0);
cqlsh:ks> INSERT INTO test(k) VALUES (-1);
cqlsh:ks> SELECT * FROM test;

 k
----
  0
  1
 -1

cqlsh:ks> SELECT * FROM test WHERE k >= -1 AND k < 1;
Bad Request: Start key must sort before (or equal to) finish key in your partitioner!

{noformat}",20/Jun/13 14:47;jbellis;+1,"20/Jun/13 17:12;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update debian packaging for 2.0,CASSANDRA-5688,12654306,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,blair,blair,blair,22/Jun/13 05:22,12/Mar/19 14:17,13/Mar/19 22:29,24/Jun/13 16:48,2.0 beta 1,,,Packaging,,,0,build,,,,"Building trunk on an Ubuntu Precise VM fails with the following output:

{code}
$ git describe 
cassandra-1.2.5-983-g96a1bb0
$ dpkg-buildpackage
...
...
gen-cql3-grammar:
     [echo] Building Grammar /home/blair/Code/Cassandra/cassandra-0.2.0.0.1.2.5.982/src/java/org/apache/cassandra/cql3/Cql.g  ...

build-project:
     [echo] apache-cassandra: /home/blair/Code/Cassandra/cassandra-0.2.0.0.1.2.5.982/build.xml
    [javac] Compiling 41 source files to /home/blair/Code/Cassandra/cassandra-0.2.0.0.1.2.5.982/build/classes/thrift
    [javac] javac: invalid target release: 1.7
    [javac] Usage: javac <options> <source files>
    [javac] use -help for a list of possible options

BUILD FAILED
{code}

I'm working on changes to the files in debian/ to support this.",Ubuntu precise,,,,,,,,,,,,,,,,,,,,,,,22/Jun/13 06:10;blair;trunk-5688.txt;https://issues.apache.org/jira/secure/attachment/12589235/trunk-5688.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-24 16:48:32.388,,,no_permission,,,,,,,,,,,,334583,,,Mon Jun 24 16:48:32 UTC 2013,,,,,,0|i1lpsn:,334909,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,24/Jun/13 16:48;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not query secondary index,CASSANDRA-5732,12656702,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,beobal,adanecito,adanecito,08/Jul/13 21:35,12/Mar/19 14:17,13/Mar/19 22:29,13/Oct/13 16:14,1.2.11,2.0.2,,Feature/2i Index,,,2,,,,,"Noticed after taking a column family that already existed and assigning to an IntegerType index_type:KEYS and the caching was already set to 'ALL' that the prepared statement do not return rows neither did it throw an exception. Here is the sequence.
1. Starting state query running with caching off for a Column Family with the query using the secondary index for te WHERE clause.
2, Set Column Family caching to ALL using Cassandra-CLI and update CQL. Cassandra-cli Describe shows column family caching set to ALL
3. Rerun query and it works.
4. Restart Cassandra and run query and no rows returned. Cassandra-cli Describe shows column family caching set to ALL
5. Set Column Family caching to NONE using Cassandra-cli and update CQL. Rerun query and no rows returned. Cassandra-cli Describe for column family shows caching set to NONE.
6. Restart Cassandra. Rerun query and it is working again. We are now back to the starting state.

Best Regards,
-Tony","Windows 8, Jre 1.6.0_45 32-bit",,,,,,,,,,,,,,,,,,,,,,,10/Oct/13 14:02;beobal;5732-1.2;https://issues.apache.org/jira/secure/attachment/12607811/5732-1.2,10/Oct/13 18:24;jbellis;5732-v2.txt;https://issues.apache.org/jira/secure/attachment/12607852/5732-v2.txt,10/Oct/13 19:49;jbellis;5732-v3.txt;https://issues.apache.org/jira/secure/attachment/12607870/5732-v3.txt,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-07-10 15:04:40.559,,,no_permission,,,,,,,,,,,,336925,,,Mon Feb 10 10:09:57 UTC 2014,,,,,,0|i1m46f:,337248,1.1.9,,,,,,,jbellis,jbellis,,,,,,,,,,"09/Jul/13 04:52;adanecito;Hi,

I tried the same set of tables, data, java code in Windows 7 jre 1.7.0_05 64-bit and it seems to not have the issue so far. Will try other environment using 1.7.0_x 32 then 64-bit to see if that solves the issue.

Regards,
-Tony","10/Jul/13 04:54;adanecito;Ok. I figured out it has to do with a config setting in cassandra.yaml file. If you set row_cache_size_in_mb to 200 instead of 0 and you are setting caching = ""ALL"" for a column family and using and secondary index for a query this issue occurs. If you set row_cache_size_in_mb this problem goes away.
Let me know why this is. It would be nice to use row caching and column family caching.",10/Jul/13 15:04;jalkanen;Is this the same as CASSANDRA-4785 and CASSANDRA-4973?,"10/Jul/13 15:19;adanecito;Hi Janne,
 
There are simularities. Mine though is a solid failure and I narrowed it down to what I said so the Cassandra team should be able to solve the issue.
 
Best Regards,
-Tony

From: Janne Jalkanen (JIRA) <jira@apache.org>
To: adanecito@yahoo.com 
Sent: Wednesday, July 10, 2013 9:05 AM
Subject: [jira] [Commented] (CASSANDRA-5732) Can not query secondary index



    [ https://issues.apache.org/jira/browse/CASSANDRA-5732?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13704633#comment-13704633 ] 

Janne Jalkanen commented on CASSANDRA-5732:
-------------------------------------------

Is this the same as CASSANDRA-4785?
                

--
This message is automatically generated by JIRA.
If you think it was sent incorrectly, please contact your JIRA administrators
For more information on JIRA, see: http://www.atlassian.com/software/jira
","19/Sep/13 16:54;dmeyer;This is definitly a bug in latest 1.2.  Just reproduced in 1.2.10

Repro is easy.  Create single node cluster off of latest 1.2 branch and do the following:

cqlsh:ks> CREATE TABLE test ( row text, name text, PRIMARY KEY (row) );
cqlsh:ks> ALTER TABLE test WITH caching='all';
cqlsh:ks> INSERT INTO test (row, name) VALUES ( 'row1', 'daniel' );
cqlsh:ks> INSERT INTO test (row, name) VALUES ( 'row2', 'ryan' );
cqlsh:ks> CREATE INDEX on test (name);
cqlsh:ks> select * from test where name='daniel';

 row  | name
------+--------
 row1 | daniel
#Notice how row is returned

Stop node and set:
row_cache_size_in_mb: 200
Start node again and follow this procedure:

cqlsh> use ks;
cqlsh:ks> select * from test where name='daniel';
#Nothing is returned from this query


Now stop the node and set:
row_cache_size_in_mb: 0

start the node and do the following:

cqlsh> use ks;
cqlsh:ks> select * from test where name='daniel';

 row  | name
------+--------
 row1 | daniel

#Notice how the row is returned.","19/Sep/13 16:57;jbellis;Suspect this might be up your alley, [~beobal].","10/Oct/13 14:02;beobal;The reason for the missing results is that in CFS.getColumnFamily() we look up the cfs id from Schema to calculate the cache key. However, 2i CFSes are never loaded into the Schema, so Schema.instance.getId always returns null. Simply fixing this by calling Schema.instance.load() with the 2i CFMD when the index is initialized uncovers another issue. The cfid is now retrievable, but the deserialization of a cached 2i row fails as it depends on the 2i CFMD being present in the enclosing KSMD for the eventual call to Schema.getCFMD(). Once we start adding index CFs to Schema they then become involved in schema migrations which makes everything very messy. So rather than adding them directly to KSMD like regular CFs, I added a separate cfId->CFMD map to Schema, so as far as most things are concerned nothing has changed, just we have one further place to look when retrieving CFMD for a given cfId.

The attached patch is against the 1.2 branch, CASSANDRA-4875 is a duplicate of this, but has a fixver of 1.1 [~jbellis], do you want me to submit a patch against 1.1 also?

I wrote a dtest for this, pull request for that here: https://github.com/riptano/cassandra-dtest/pull/22

Looking at this, I also uncovered what I think is an issue with the setup of the 2i cache config. In AbstractSimplePerColumnSecondaryIndex (in 1.2, the same code is in KeysIndex in 1.1), the estimated key and mean column counts are used to gauge the index's cardinality then use that to decide whether or not to enable row caching. This calculation is first performed prior to the index actually being built, so there are no SSTables to provide the estimates, which results in row caching always being disabled until the next time the index is initialized when C* is restarted (this appears to be why the repro steps require a restart). If this is a genuine problem, I'll create a separate JIRA to address it. 
","10/Oct/13 18:24;jbellis;Hmm, it's not actually necessary to look up your id through the schema here.  v2 is a simpler solution if that's the only problem.","10/Oct/13 18:50;beobal;Yeah, unfortunately that failed lookup was only masking the other problem I mentioned. Even with the v2 fix, without the index cfm in Schema, you fall foul (silently, except for debug) of https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/db/ColumnFamilySerializer.java#L183 and so the row cache is effectively bypassed for 2i cfs.","10/Oct/13 19:49;jbellis;I see.

I think your patch is probably fine, but I'm super nervous about messing with Schema internals this late in 1.2.x (let alone 1.1).

v3 just disables row cache entirely for 2i CFs.","10/Oct/13 19:59;beobal;Sure, that's reasonable and pretty much expected. I'll attach a patch for trunk/2.0","13/Oct/13 16:14;jbellis;Committed v3 then.

I don't think it's worth doing extra work for 2.0 since we're removing row cache for 2.1.","10/Feb/14 10:09;jeromatron;FWIW, appears to also be a problem in Cassandra 1.1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ALTER RENAME is broken in trunk,CASSANDRA-5702,12654893,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,26/Jun/13 09:02,12/Mar/19 14:17,13/Mar/19 22:29,04/Jul/13 13:33,2.0 beta 1,,,,,,0,,,,,"CASSANDRA-5125 has broken {{ALTER RENAME}} when the column is a default alias (for thrift column families where the PK columns haven't been renamed yet).

The problem is basically that while we assign default aliases to PK columns when they don't have one, we currently ""fake"" those default aliases and do not persist them. Concretely, CFDefinition is aware of them, but CFMetaData is not, which break renaming post CASSANDRA-5125.

We could fix rename punctually, but there is another related problem: for the same reason, if you try to create an index on a column that is a non-renamed default alias, this doesn't work with the arguably confusing message ""No column definition found for column X"". Here again, we could fix it punctually, but it starts to sound like we need a more general fix.

So I suggest stopping to ""fake"" those default aliases, but instead to just create ""real"" aliases (that are known of CFMetaData and persisted in the schema) when there is none. After all, from a user point of view, why should a default column name be any special. And on top of fixing the issues above, this also:
# fix CASSANDRA-5489 in a somewhat simpler way
# makes it easier for clients reading the schema CFs. They won't to infer the default aliases anymore.

The only theoretical downside is that we lose the information that a given CQL3 column name is one assigned by default versus one set up by the user, but given the user can rename those column names anyway, not sure this matters in any way.
",,,,,,,,,,,,,,,,,,,,,,,,26/Jun/13 13:22;slebresne;5702.txt;https://issues.apache.org/jira/secure/attachment/12589744/5702.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-26 10:44:36.724,,,no_permission,,,,,,,,,,,,335170,,,Thu Jul 04 13:33:44 UTC 2013,,,,,,0|i1ltd3:,335494,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,26/Jun/13 10:44;iamaleksey;Thought about this as well (in the context of CASSANDRA-5489) and came to the same conclusion.,"26/Jun/13 13:22;slebresne;Attaching patch for this. Note that persisting those default columns implied to only rebuild CQL3 metadata once the CFMetaData is basically complete, hence the patch moves the calls to said rebuild a bit (it's a good thing anyway, we were calling that rebuild way too often). Also:
* the patch fixes a small bug in CFMetaData.isThriftCompatible(). It was excluding way too much stuff post CASSANDRA-5125.
* this happen to lift the limitation introduced by CASSANDRA-5531 (not a big deal but not a bad thing either).
* I've also rewritten CFMetaData.isDense(). I don't think that was strictly necessary, but while trying to check it wasn't broken by the patch it felt it could be simplified/clarified.
",26/Jun/13 17:25;iamaleksey;+1,"26/Jun/13 17:37;slebresne;Committed, thanks","03/Jul/13 17:36;brandon.williams;This broke the wide_slice_test dtest (which admittedly is using cql2, but we shouldn't regress there)",04/Jul/13 13:33;slebresne;Had forgot a toUppercase() call when checking the CQL2 key alias. I've committed the trivial fix as b7e49b3.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrading to cassandra-1.2 with a dead LEFT state from 1.1 causes problems,CASSANDRA-5696,12654598,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,24/Jun/13 21:34,12/Mar/19 14:17,13/Mar/19 22:29,26/Jun/13 21:16,1.2.7,,,,,,0,qa-resolved,,,,"In 1.1, we wrote LEFT states as LEFT,token,expiretime in gossip.  However in 1.2, VersionValue serializes this to LEFT,expiretime,tokens and causes the upgrade 1.2 to try and parse it this way as well, causing it to try to parse the token as an expiretime.

Another wrinkle to this is assassinate still writes it the old way: LEFT,tokens,expiretime.",,,,,,,,,,,,,,,,,,,,,,,,26/Jun/13 19:19;brandon.williams;5696.txt;https://issues.apache.org/jira/secure/attachment/12589783/5696.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-26 20:37:58.342,,,no_permission,,,,,,,,,,,,334875,,,Wed Jun 26 21:16:07 UTC 2013,,,,,,0|i1lrjr:,335199,,,,,,,,jasobrown,jasobrown,,,,,,,,,enigmacurry,"26/Jun/13 19:19;brandon.williams;There are a few different ways this can be solved, but it's obvious that the most correct thing to do is correct the order in VV.  This too turns out to be workable though, as long as you either a) do a full ring restart (to wipe the dead states) or b) do not upgrade within 72 hours of decommissioning a node.  Almost everyone will likely fall into group b) but if they don't, and don't read the warning in NEWS.txt the possible things that can happen are:

* they receive a harmless NumberFormatException everywhere
* they carry a useless dead state around for a token that doesn't exist for an indeterminate amount of time (which assassinate could fix)
* the timestamp used for the decom expiration matches a node's token and it will need to be  shutdown and rebootstrapped

None of these seem world-ending, with the last one being the worst and also least likely to occur, so here's a patch where we just change the VV order and update NEWS.","26/Jun/13 20:37;jasobrown;After we make this change, won't the same problems apply to 1.2.\{0..6\} -> 1.2.7+/2.0 upgrades? The VV fields will again be inverted, although arguably in a more internally consistent manner (with 1.1)?
","26/Jun/13 20:45;jasobrown;Thought a little bit more, and I think this patch is fine. +1",26/Jun/13 21:16;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
shuffle disable subcommand not recognised,CASSANDRA-5756,12657559,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,dbrosius,hsn,hsn,13/Jul/13 01:52,12/Mar/19 14:17,13/Mar/19 22:29,13/Jul/13 17:58,2.0 beta 2,,,Legacy/Tools,,,0,,,,,"command disable listed in help but not recognized

C:\cassandra2\bin>shuffle.bat disable
Unknown subcommand: disable
Usage: shuffle [options] <sub-command>

Sub-commands:
 create           Initialize a new shuffle operation
 ls               List pending relocations
 clear            Clear pending relocations
 en[able]         Enable shuffling
 dis[able]        Disable shuffling
",,,,,,,,,,,,,,,,,,,,,,,,13/Jul/13 07:13;dbrosius;5756.txt;https://issues.apache.org/jira/secure/attachment/12592130/5756.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-13 07:13:29.077,,,no_permission,,,,,,,,,,,,337780,,,Sat Jul 13 17:58:27 UTC 2013,,,,,,0|i1m9fr:,338102,,,,,,,,jbellis,jbellis,,,,,,,,,,13/Jul/13 07:13;dbrosius;fix simple typo in 2.0,13/Jul/13 15:11;jbellis;+1,13/Jul/13 17:58;dbrosius;committed to trunk as commit c5bca30a0f8d21c3c3b59fde0b8e1bd0581c8905,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cql3 reader returns duplicate rows if the cluster column is reversed,CASSANDRA-5718,12655805,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,alexliu68,alexliu68,alexliu68,02/Jul/13 16:57,12/Mar/19 14:17,13/Mar/19 22:29,05/Aug/13 22:00,1.2.9,,,,,,0,,,,,"To reproduce it,

cqlsh:test>  select * from wordfreq;

 title   | occurances | word
---------+------------+-------
 alex123 |          4 |  liu3
   alex1 |      23456 |  liu2
  alex10 |         10 | liu10
  alex12 |         34 |  liu3
    alex |     123456 |  liu1
    alex |       1000 |   liu


CREATE TABLE wordfreq ( title text, word text, occurances int, PRIMARY KEY (title,occurances)) WITH CLUSTERING ORDER by (occurances DESC);

The hadoop job returns 7 rows instead of 6 rows. 

I will post a patch soon.
",,,,,,,,,,,,,,,,,,,,,,,,02/Jul/13 20:23;alexliu68;5718-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12590500/5718-1.2-branch.txt,01/Aug/13 18:04;alexliu68;5718-2-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12595459/5718-2-1.2-branch.txt,02/Aug/13 17:28;alexliu68;5718-3-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12595633/5718-3-1.2-branch.txt,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-07-11 15:13:28.775,,,no_permission,,,,,,,,,,,,336080,,,Sat Apr 26 04:40:33 UTC 2014,,,,,,0|i1lyz3:,336404,,,,,,,,jbellis,jbellis,,,,,,,,,,02/Jul/13 20:23;alexliu68;Patch for 1.2 branch is attached,11/Jul/13 15:13;jbellis;Looks to me like this doesn't handle reversed for non-composite cells.,"15/Jul/13 23:01;alexliu68;create a compact table

{code}
 CREATE TABLE wordfreq ( title text, word text, occurances int, PRIMARY KEY (title,occurances)) 
 WITH COMPACT STORAGE and CLUSTERING ORDER by (occurances DESC);
{code}

show the schema

{code}
  cqlsh:test> select key_aliases, column_aliases, key_validator, comparator from system.schema_columnfamilies where   keyspace_name='test';

 key_aliases | column_aliases | key_validator                            | comparator
-------------+----------------+------------------------------------------+-----------------------------------------------------------------------------------------
   [""title""] | [""occurances""] | org.apache.cassandra.db.marshal.UTF8Type | org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.Int32Type)
{code}

It has the clustering column and reversed type defined for the table in system.schema_columnfamilies table.

The patch covers the compact storage type tables. Is there any other type of thrift table I am missing? ","29/Jul/13 21:25;jbellis;Right; I'm saying that the only place this sets reversed away from its default of false is here:

{code}
+        if (comparatorValidator instanceof CompositeType)
+        {
+            for (int i = 0; i < clusterColumns.size(); i++)
+                clusterColumns.get(i).reversed = (((CompositeType) comparatorValidator).types.get(i) instanceof ReversedType);
{code}","01/Aug/13 18:04;alexliu68;Oh, I get it. 5718-2-1.2-branch.txt is attached to fix the none-composite reversed type of comparator",02/Aug/13 17:28;alexliu68;5718-3-1.2-branch.txt is attached as the latest patch.,"05/Aug/13 22:00;jbellis;LGTM, committed","26/Apr/14 04:40;rohitbrai;[~jbellis] [~alexliu68]

There is a regression in 2.0.x builds of Cassandra and the trunk.

The changes done for this issue were inadvertently overwritten in a merge from 1.2 to 2.0
https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=blobdiff;f=src/java/org/apache/cassandra/hadoop/cql3/CqlPagingRecordReader.java;h=d9b9a39cebe0ec50e5e80c6f8057d4cce0646b6a;hp=b6e793c7e46adf3694c71e365833960c650c8152;hb=3a4d6beb;hpb=8e7d7285cdeac4f2527c933280d595bbddd26935

Should I submit a patch against 2.0 and trunk?
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reverse slice queries can skip range tombstones,CASSANDRA-5712,12655335,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,28/Jun/13 13:45,12/Mar/19 14:17,13/Mar/19 22:29,01/Jul/13 07:38,1.2.7,,,,,,0,,,,,"On disk, we represent range tombstones by a marker at the beginning of the range covered. Since we repeat such markers when they overlap an index block and since an index block is always read in forward order (even in reverse queries), we are guaranteed to see a range tombstone before any column it covers. However, IndexedSliceReader returns the columns of an index block in reverse order and thus can return a range tombstone *after* columns it covers.

It follows that some range tombstone can be skipped during a reversed range slice. We need to fix IndexedSliceReader to always return range tombstone first (or at least before the first column covered by each range tombstone).",,,,,,,,,,,,,,,,,,,,,,,,28/Jun/13 13:48;slebresne;5712.txt;https://issues.apache.org/jira/secure/attachment/12590037/5712.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-28 15:54:48.101,,,no_permission,,,,,,,,,,,,335612,,,Mon Jul 01 07:38:23 UTC 2013,,,,,,0|i1lw33:,335936,,,,,,,,jbellis,jbellis,,,,,,,,,,"28/Jun/13 13:48;slebresne;Attaching patch with a unit test to demonstrate the bug as well as a fix. The fix simply make sure that for an index block we always return all the range tombstone first before returning any column. In theory this means we might sometimes return a few range tombstones the caller won't really need, but it's harmless and probably not worth bothering with.",28/Jun/13 15:54;jbellis;Do we really need rangeTombstonesReversed?  ISTM that we could just special case tombstones in addColumn (rename to addAtom) to always addFirst regardless of reversed-ness.,"28/Jun/13 16:31;slebresne;bq. ISTM that we could just special case tombstones in addColumn (rename to addAtom) to always addFirst regardless of reversed-ness.

I don't think that works :).

We get t(0), c(1), c(2), t(3), c(4) from the reader and we want to ultimately return them as t(3), t(0), c(4), c(2), c(1).

If we do addFirst for columns and addLast for tombstone, we'll get c(4), c(2), c(1), t(0), t(3), which gives us all tombstone last (and if we pollLast instead of pollFirst, the columns won't be reversed). If we do addLast for columns and addFirst for tombstone, we have the exact equivalent of pollLast in the previous case.

We can probably do some peekLast first and do pollLast as long as it's a tombstone and pollFirst if it's not, but is that really much of an improvement compared to the patch?
","29/Jun/13 05:34;jbellis;I'm not a huge fan of having two Collections of atoms involved, but you're right, it's not as simple as I thought.  +1 on the patch.","01/Jul/13 07:38;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming (2.0) can deadlock,CASSANDRA-5699,12654745,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,25/Jun/13 14:57,12/Mar/19 14:17,13/Mar/19 22:29,03/Jul/13 17:56,2.0 beta 1,,,,,,0,,,,,"The new streaming implementation (CASSANDRA-5286) creates 2 threads per host for streaming, one for the incoming stream and one for the outgoing one. However, both currently share the same socket, but since we use synchronous I/O, a read can block a write, which can result in a deadlock if 2 nodes are both blocking on a read a the same time, thus blocking their respective writes (this is actually fairly easy to reproduce with a simple repair).

So instead attaching a patch that uses one socket per thread.

The patch also correct the stream throughput throttling calculation that was 8000 times lower than what it should be.",,,,,,,,,,,,,,,,,,,,,,,,02/Jul/13 09:50;slebresne;5699.txt;https://issues.apache.org/jira/secure/attachment/12590400/5699.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-29 18:21:39.245,,,no_permission,,,,,,,,,,,,335022,,,Wed Jul 03 17:56:00 UTC 2013,,,,,,0|i1lsgf:,335346,,,,,,,,jbellis,jbellis,,,,,,,,,,"29/Jun/13 18:21;jbellis;Is the ""stream lifecycle"" documented anywhere the way we did in 1.2 StreamOut?

NB: this patches StreamingRepairTask, which does not exist in current trunk.","01/Jul/13 11:05;slebresne;bq. Is the ""stream lifecycle"" documented anywhere

Yuki had written https://gist.github.com/yukim/5672508. The one thing this patch changes compared to that ""design document"" is that the initialization phase is slightly more complex since we need to create 2 connection. So the first node sends a StreamInit to the other end to create the first connection (as was done previously), but then the remote side creates a connection back sending a StreamInit message of it's own. Then and only then do we go to the prepare phase.

In any case, we probably want that lifecycle to be documented in the javadoc or we'll lose track of it, so I've described this in relative detail at the head of StreamSession.

So updated the patch with those added comments and the modification to StreamRepairTask cleaned up.
","01/Jul/13 23:48;jbellis;It's somewhat confusing that we use StreamInit both as leader and as follower, to mean different things, but I get how the MessagingService architecture makes it difficult to do otherwise.  As a minor improvement, suggest renaming isInitiator to sentByInitiator.

What is going on with the switch from {{Set<UUID> ongoingSessions}} to {{Map<InetAddress, StreamSession> ongoingSessions}} in SRF?

Nit: onConnect could mean ""leader connects to follower"" or [what it actually means] ""stream is fully connected.""  Suggest renaming, e.g. onSessionEstablished.

Other comments on new Streaming:

Somewhat confused by logic in complete() -- ""I received a Complete message.  If I'm already waiting for a complete message, close the session.  Otherwise, wait for [another] Complete message"" ?

It looks like there may be synchronization issues with ""state;"" some accesses are synchronized and some are not.

Why is init broken out from construction?  Makes some things awkward, e.g. streamResult which is final-post-init but we have to null-check until then.

Why do we include a String description in SIM?

When does follower immediately have something to stream to leader, is this a Repair optimization?

","02/Jul/13 09:50;slebresne;bq. It's somewhat confusing that we use StreamInit both as leader and as follower, to mean different things

I guess, though if you see StreamInit as a way for both node to open/initialize their outgoing connection to the other node (which is what this is doing), it's not really that different. And it's somewhat consistent with the rest of the protocol, where each side will sent a Prepare and then finally a Complete.  Anyway, updated the patch with the sentByInitiator renaming.

bq. What is going on with the switch from Set<UUID> ongoingSessions to Map<InetAddress, StreamSession> ongoingSessions in SRF?

ongoingSession only role initialy was to track how many sessions for a StreamResultFuture were live to know when we're done (SRF.maybeComplete()). In the original patch of CASSANDRA-5286, it wasn't even there, and there was just an AtomicInteger decremented each time a session completed. I was slightly afraid that a race could have us counting the same session done twice, so I changed it to a Set<UUID> and added a simple UUID per session that was only used for that purpose (it was never send to the other side or anything). Anyway, that UUID addition was stupid in the first place since for a SRF, there is only one StreamSession per remote host, so it should have been a Set<InetAddress> to start with and we have no use for a StreamSession UUID.

But on top of that, this patch add the need to be able to find back a StreamSession in a SRF given the remote endpoint (in IncomingStreamConnection, when the initiator gets the other side StreamInit), so we really need a map now.

bq. Suggest renaming, e.g. onSessionEstablished

Agreed, though I've renamed to onInitializationComplete to be consistent with the wording of the javadoc.

bq. Somewhat confused by logic in complete() – ""I received a Complete message. If I'm already waiting for a complete message, close the session. Otherwise, wait for [another] Complete message"" ?

Each side will send a CompleteMessage to say ""I'm done on my side"", and the session will be considered really complete (and closed) when we got the Complete for both side. So WAIT_COMPLETE means ""I've seen only one complete message so far"". Hence the logic of complete() is ""I just received a complete from the other side, if I had already completed my side, close the session, otherwise move to the ""I'm waiting for the other complete"" state"".

bq. It looks like there may be synchronization issues with ""state;""

I think we're good, because the only case where we can race to set a state is during the completion phase, for WAIT_COMPLETE and COMPLETE, and those are protected. For other states, there are set sequentially on each node by construction.

bq. Why is init broken out from construction?

SRF takes his StreamSession in his ctor so we can't have StreamSessions take their SRF without some other refactor.  Now don't get me wrong, it bugs me too. And I do think there is a few things we can do to make that whole new streaming API simpler/cleaner (including probably renaming StreamRepairFuture). And I'm volonteering to give to a shot to that. But in another follow ticket because:
# I really think that kind of code cleanup shouldn't block beta1 (but this ticket, the fact that streaming deadlock, do is a blocker)
# I've already rewrote quite a bit of Yuki's code without him having the change to chime in. Would feel fair to wait to him to be back before refactoring parts that are not really crucial for beta1.

bq. Why do we include a String description in SIM?

This the description of what the operation is doing (""Repair"", ""Bootstrap"", ""Restore replica count"", ...)

bq. When does follower immediately have something to stream to leader

A StreamSession in the initiator handle both incoming and outgoing streams. Sometimes we only have outgoing ones (Unbootstrap/Restore replica count/Bulkloading), sometimes only
incoming ones (Bootstrap) and sometimes both (Repair indeed but also the SS.RangeRelocator for moves and vnodes tokens relocation). Does that answer your question?
",03/Jul/13 15:47;jbellis;Ship it!,"03/Jul/13 17:56;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The output of the describe command does not necessarily give you the right DDL to re-create the CF,CASSANDRA-5766,12658129,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,slowenthal,slowenthal,16/Jul/13 22:08,12/Mar/19 14:17,13/Mar/19 22:29,18/Jul/13 20:44,1.2.7,,,Legacy/Tools,,,0,cqlsh,describe,,,"If compression is not set for a CF, cqlsh omits the compression attribute.  When you replay that very same DDL, you get a CF with Snappy compression.  This may occur with other parameters.  Perhaps describe should always show every parameter in full.  The absence of a setting is a setting.  (Think of the arrow in the FedEx logo).

Create a CF with cassandra-stress.  cassandra-stress defaults to NO compression.

 ~/dse/resources/cassandra/tools/bin/cassandra-stress -S 100 -c 1 --num-keys 1

describe it
CREATE TABLE ""Standard1"" (
  key blob PRIMARY KEY,
  ""C0"" blob
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'};

replay it - I changed the cf name to standard2

describe the new CF:

CREATE TABLE standard2 (
  key blob PRIMARY KEY,
  ""C0"" blob
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};


",,,,,,,,,,,,,,,,,,,,,,,,18/Jul/13 20:37;iamaleksey;5766.txt;https://issues.apache.org/jira/secure/attachment/12593049/5766.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-18 20:39:41.697,,,no_permission,,,,,,,,,,,,338323,,,Thu Jul 18 20:44:50 UTC 2013,,,,,,0|i1mcrz:,338643,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,18/Jul/13 20:39;brandon.williams;+1,"18/Jul/13 20:44;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DateType (timestamp type in CQL3) does not sort pre-'unix epoch' dates correctly,CASSANDRA-5723,12656161,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,zhouhero,zhouhero,04/Jul/13 08:19,12/Mar/19 14:17,13/Mar/19 22:29,08/Jul/13 16:59,2.0 beta 1,,,,,,0,,,,,"- this bug can be confirmed by fellow:


1.create table like fellow:

create table test2 (
id varchar,
c varchar,
create_date timestamp,
primary key(id)
);

create index idx_test2_c on test2 (c);
create index idx_test2_create_date on test2 (create_date);


2.insert data like fellow;

cqlsh:pgl> update test2 set create_date='1950-01-01', c='1' where id='111';
cqlsh:pgl> update test2 set create_date='1917-01-01', c='1' where id='111';
cqlsh:pgl> update test2 set create_date='2013-01-01', c='1' where id='111';

3.select data :
cqlsh:pgl> select * from test2 where c='1' and create_date>'2011-01-01 12:00:01' ALLOW FILTERING ;

id | c | create_date
-----+---+--------------------------
111 | 1 | 2012-12-31 15:00:00+0000

4. add data:
update test2 set create_date='1917-05-01', c='1' where id='111';

5.select data:
cqlsh:pgl> select * from test2 where c='1' and create_date>'2011-01-01 12:00:01' ALLOW FILTERING ;

id | c | create_date
-----+---+--------------------------
111 | 1 | 1917-04-30 15:00:00+0000
↑
the search result is not right!
it should be fellow:

id | c | create_date
-----+---+--------------------------
111 | 1 | 2012-12-31 15:00:00+0000",,,,,,,,,,,,,,,,,,,,,,,,08/Jul/13 14:02;slebresne;5723.txt;https://issues.apache.org/jira/secure/attachment/12591217/5723.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-04 15:51:43.479,,,no_permission,,,,,,,,,,,,336436,,,Mon Jul 08 16:59:38 UTC 2013,,,,,,0|i1m15z:,336760,,,,,,,,jbellis,jbellis,,,,,,,,,,"04/Jul/13 15:51;slebresne;bq. it should be fellow

No, it should be empty, because you're overwriting the same row all the time, so after that update in 4, the row '111' should contain {{create_date='1917-05-01'}} and you could rightfully expect to not get that row back with your query.

The reason it's actually returned is that for some reason, DateType.compare() (the comparator used for the timestamp CQL3 type) use an unsigned comparison, and since 1917 is before the unix epoch, it's timestamp is negative and wrongfully sort after any post-epoch date. This is *not* a CQL3 specific bug in particular.

The simple fix would be to change the comparison to be signed, but there is obviously backward compatibility concerns (since DateType has done that for years). In any case, in the meantime, avoid pre-epoch dates.","05/Jul/13 13:28;slebresne;I'm slightly surprised that nobody ran into this with DateType and I would argue that sorting pre-epoch dates after post-epoch ones is definitively a bug, but at the same time changing DateType now is too risky imo as it could screw current DateType rather badly.

Instead, I would suggest adding a new, fixed, DateType (TimestampType or DateTimeType maybe?). And for CQL3, we would switch to that for the 'timestamp' type (which would not switch existing table however, since they would still be DateType internally). We can also make it so that switching (manually) from/to DateType to that new type is allowed (with maybe a warning in the log).

Opinions?
","06/Jul/13 01:09;jbellis;bq. Instead, I would suggest adding a new, fixed, DateType (TimestampType or DateTimeType maybe?). And for CQL3, we would switch to that for the 'timestamp' type (which would not switch existing table however, since they would still be DateType internally). We can also make it so that switching (manually) from/to DateType to that new type is allowed (with maybe a warning in the log)

Sounds reasonable.  What would we show for the old tables on the CQL side?  ""oldtimestamp?""  

(I would vote for TimestampType.)","06/Jul/13 12:44;slebresne;bq. What would we show for the old tables on the CQL side? ""oldtimestamp?

I was thinking of not having be any CQL3 specific type. I.e. it would be displayed as a custom type, or ""org.apache.cassandra.db.marshal.DateType"".",06/Jul/13 15:46;jbellis;That's reasonable.,"08/Jul/13 14:02;slebresne;Attaching a patch with that new TimestampType. The patch is against 1.2 right now, but I'm starting to wonder if 2.0 is not a more reasonable goal.

The basics of the patch is that the CQL3 timestamp type now default to that new type. For the native protocol however, we make both DateType and TimestampType be returned as 'timestamp'. Otherwise, if we were returning it as a custom type, this would likely break users since client driver wouldn't recognize it anymore. Besides, the actual sorting of a type in a ResultSet shouldn't matter for a CQL3 driver.

On the CQL-over-thrift side however, we return the full ""thrift"" comparator name, so in practice we'd have to update cqlsh so it continues to work with dates, but the patch doesn't do it.

The patch let user switch between DateType and TimestampType, but log a warning when you do so.","08/Jul/13 16:20;jbellis;+1, but I agree that this should be for 2.0.","08/Jul/13 16:59;slebresne;Alright, committed to trunk only then.

I've also created CASSANDRA-5719 to followup on the cqlsh changes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix broken streaming retry,CASSANDRA-5775,12658570,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,yukim,yukim,18/Jul/13 17:40,12/Mar/19 14:17,13/Mar/19 22:29,22/Jul/13 13:52,2.0 beta 2,,,,,,0,streaming,,,,Current streaming 2.0 retry is broken since the receiver would keep on reading file even when something bad happens and continue to throw exceptions.,,,,,,,,,,,,,,,,,,,,,,,,19/Jul/13 19:06;yukim;0001-fix-retry-streaming.patch;https://issues.apache.org/jira/secure/attachment/12593237/0001-fix-retry-streaming.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-19 08:26:52.632,,,no_permission,,,,,,,,,,,,338764,,,Mon Jul 22 13:52:26 UTC 2013,,,,,,0|i1mfhr:,339084,,,,,,,,slebresne,slebresne,,,,,,,,,,"18/Jul/13 17:43;yukim;Attaching patch for fix.
I introduced new message to ACK upon file receiving. Sender will keep the file reference until receiving ACK, and when it receives RETRY instead, it then sends the file again.","19/Jul/13 08:26;slebresne;When we do retry, shouldn't we move back the file from waitingForAck to files?  Feels wrong not to do it, if only for reporting sakes. And is there a reason why fileSent is guarded by 'if (files.containsKey(sequenceNumber))'? I mean, intuitively it seems that this should always be true, so we should assert it, not silently ignore it if it's not true.

Also, I think expecting that the file is in waitingForAck in STT.createMessageForRetry() is slightly racy. In theory, we could get the retry messge before the code has gone through fileSent() (it's very unlikely, but not totally impossible).

Nit: In (Compressed)StreamReader, I think we can remove the (duplicate) line before the 'while (toSkip > 0)'.","19/Jul/13 19:06;yukim;Updated patch attached.
I think I can just simplify the patch without waitForAck.
Also, StreamReader is refactored to remove duplicated codes.",22/Jul/13 06:37;slebresne;+1,22/Jul/13 13:52;yukim;Committed. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift cas() method crashes if input columns are not sorted.,CASSANDRA-5786,12659025,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,22/Jul/13 06:55,12/Mar/19 14:17,13/Mar/19 22:29,22/Jul/13 13:47,2.0 beta 2,,,,,,0,LWT,,,,"CassandraServer#cas() use UnsortedColumns for the ""updates"", which might result later to a
{noformat}
java.lang.AssertionError: Added column does not sort as the last column
        at org.apache.cassandra.db.ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:115)
        at org.apache.cassandra.db.ColumnFamily.addColumn(ColumnFamily.java:117)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:119)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:96)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:91)
        at org.apache.cassandra.service.paxos.Commit$CommitSerializer.deserialize(Commit.java:139)
        at org.apache.cassandra.service.paxos.Commit$CommitSerializer.deserialize(Commit.java:128)
        at org.apache.cassandra.net.MessageIn.read(MessageIn.java:99)
        at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:175)
        at org.apache.cassandra.net.IncomingTcpConnection.handleModernVersion(IncomingTcpConnection.java:135)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:82)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,22/Jul/13 08:02;slebresne;5786.txt;https://issues.apache.org/jira/secure/attachment/12593467/5786.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-22 13:40:40.913,,,no_permission,,,,,,,,,,,,339218,,,Mon Jul 22 13:47:49 UTC 2013,,,,,,0|i1mian:,339538,,,,,,,,jbellis,jbellis,,,,,,,,,,22/Jul/13 07:10;slebresne;Attaching simple patch to switch to TreeMap. I'm also including a minor optimization that switch to ArraySorted when we clone the updates in Commit#updatesWithPaxosTime.,22/Jul/13 08:02;slebresne;Disregard the comment above. For normal updates we keep columns unsorted until they are applied to the memtable and we should probably do that here too for consistency. So attaching patch that makes sure we don't assume the columns are sorted when deserializing for paxos.,22/Jul/13 13:40;jbellis;+1,"22/Jul/13 13:47;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException during streaming,CASSANDRA-5782,12658861,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,20/Jul/13 00:56,12/Mar/19 14:17,13/Mar/19 22:29,22/Jul/13 13:48,2.0 beta 2,,,,,,0,,,,,"During repair:

{noformat}
 INFO [STREAM-IN-/127.0.0.1] 2013-07-19 19:42:28,270 StreamSession.java (line 602) Flushing memtables for [CFS(Keyspace='ks', ColumnFamily='cf')]...
 INFO [STREAM-IN-/127.0.0.2] 2013-07-19 19:42:28,270 StreamSession.java (line 602) Flushing memtables for [CFS(Keyspace='ks', ColumnFamily='cf')]...
ERROR [STREAM-IN-/127.0.0.2] 2013-07-19 19:42:28,292 StreamSession.java (line 410) Streaming error occurred
java.util.ConcurrentModificationException
    at java.util.HashMap$HashIterator.nextEntry(HashMap.java:894)
    at java.util.HashMap$ValueIterator.next(HashMap.java:922)
    at org.apache.cassandra.streaming.ConnectionHandler.sendMessages(ConnectionHandler.java:169)
    at org.apache.cassandra.streaming.StreamSession.startStreamingFiles(StreamSession.java:624)
    at org.apache.cassandra.streaming.StreamSession.prepare(StreamSession.java:445)
    at org.apache.cassandra.streaming.StreamSession.messageReceived(StreamSession.java:358)
    at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:294)
    at java.lang.Thread.run(Thread.java:722)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,22/Jul/13 06:45;slebresne;5782.txt;https://issues.apache.org/jira/secure/attachment/12593457/5782.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-22 06:45:25.862,,,no_permission,,,,,,,,,,,,339054,,,Mon Jul 22 13:48:17 UTC 2013,,,,,,0|i1mha7:,339374,,,,,,,,yukim,yukim,,,,,,,,,,22/Jul/13 06:45;slebresne;I think racing between the qeuing of the messages and the completion of the first ones is what might lead to that. Attaching a trivial patch to fix that.,22/Jul/13 13:32;yukim;+1,"22/Jul/13 13:48;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commit#updatesWithPaxosTime should also update the row and range tombstones,CASSANDRA-5787,12659031,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,22/Jul/13 07:27,12/Mar/19 14:17,13/Mar/19 22:29,22/Jul/13 13:48,2.0 beta 2,,,,,,0,,,,,Rows and range tombstones should both also respect the paxos timestamp otherwise an update may contradict the serialization order decided by paxos.,,,,,,,,,,,,,,,,,,,,,,,,22/Jul/13 07:28;slebresne;5787.txt;https://issues.apache.org/jira/secure/attachment/12593465/5787.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-22 13:39:11.706,,,no_permission,,,,,,,,,,,,339224,,,Mon Jul 22 13:48:02 UTC 2013,,,,,,0|i1mibz:,339544,,,,,,,,jbellis,jbellis,,,,,,,,,,22/Jul/13 13:39;jbellis;+1,"22/Jul/13 13:48;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
assertionError in repair,CASSANDRA-5757,12657584,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,hsn,hsn,13/Jul/13 11:45,12/Mar/19 14:17,13/Mar/19 22:29,18/Jul/13 12:30,2.0 beta 2,,,,,,0,,,,,"i increased replication factor and run repair, some token ranges were repaired okay, but one failed with:

 INFO 13:03:52,234 [repair #dd7937a0-ebab-11e2-ba07-c38e7fba9d51] session completed successfully
ERROR 13:03:52,343 Exception in thread Thread[ValidationExecutor:2,1,main]
java.lang.AssertionError: (max(9099058114996150811),max(-5486100704702537010)]
        at org.apache.cassandra.db.DataRange.<init>(DataRange.java:50)
        at org.apache.cassandra.db.DataRange.forKeyRange(DataRange.java:74)
        at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReade
r.java:1033)
        at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScan
ners(AbstractCompactionStrategy.java:214)
        at org.apache.cassandra.db.compaction.CompactionManager$ValidationCompac
tionIterable.<init>(CompactionManager.java:751)
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationComp
action(CompactionManager.java:657)",,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5771,15/Jul/13 17:06;slebresne;5757.txt;https://issues.apache.org/jira/secure/attachment/12592354/5757.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-15 17:06:58.71,,,no_permission,,,,,,,,,,,,337805,,,Thu Jul 18 12:30:54 UTC 2013,,,,,,0|i1m9lb:,338127,,,,,,,,yukim,yukim,,,,,,,,,,"15/Jul/13 17:06;slebresne;The actual AssertionError is due to basically a misplaced assertion.  DataRange.Paging don't really support wrapping ranges but is only used in getRangeSlice which unwraps ranges first anyway.  However repair does use wrapping range so DataRange itself should allow wrapping ranges.

So fixing that is trivial, but this uncover the fact that SSTableScanner is broken on trunk if the range is wrapping. For the records, it has initially been broken by CASSANDRA-4180 (after this patch, for a wrapping range, only the part between the start of the range and then end of ring/file was returned by the scanner). CASSANDRA-4415 introduced DataRange but didn't really changed the logic there so the bug persisted.

Attaching a patch that move the assert and fix SSTableScanner to handle wrapping ranges correctly.
","17/Jul/13 15:14;yukim;Hmm, the patch broke SSTableReaderTest#testGetScannerForNoIntersectionRanges.

{code}
java.util.NoSuchElementException
	at com.google.common.collect.AbstractIterator.next(AbstractIterator.java:154)
	at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:169)
	at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:42)
	at org.apache.cassandra.io.sstable.SSTableReaderTest.testGetScannerForNoIntersectingRanges(SSTableReaderTest.java:302)
{code}","17/Jul/13 17:08;slebresne;Turns out that this is a bug in Range.intersects(Bound) that this test happens to run into. And since that bug actually affect 1.2 too, I've created CASSANDRA-5771 for that specific problem. With the patch from CASSANDRA-5771, the test passes correctly.",18/Jul/13 11:57;yukim;Confirmed it's passing. +1.,"18/Jul/13 12:30;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift tables are not supported from CqlPagingInputFormat,CASSANDRA-5752,12657535,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,alexliu68,jbellis,jbellis,12/Jul/13 22:07,12/Mar/19 14:17,13/Mar/19 22:29,09/Oct/13 14:32,1.2.11,,,,,,0,qa-resolved,,,,"CqlPagingInputFormat inspects the system schema to generate the WHERE clauses needed to page ""wide rows,"" but for a classic Thrift table there are no entries for the ""default"" column names of key, column1, column2, ..., value so CPIF breaks.",,,,,,,,,,,,,,,,,,,,,,,,17/Jul/13 21:58;alexliu68;5752-1-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12592870/5752-1-1.2-branch.txt,16/Jul/13 20:36;alexliu68;5752-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12592616/5752-1.2-branch.txt,31/Jul/13 21:22;alexliu68;5752-2-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12595275/5752-2-1.2-branch.txt,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-07-16 20:36:07.649,,,no_permission,,,,,,,,,,,,337756,,,Wed Jul 31 23:21:10 UTC 2013,,,,,,0|i1m9af:,338078,,,,,,,,jbellis,jbellis,,,,,,,,,enigmacurry,"12/Jul/13 22:10;jbellis;This was fixed Cassandra-side for 2.0 in CASSANDRA-5702 but the fix was pretty heavyweight and backporting to 1.2.x is not an option.

IMO the right fix for 1.2 is to call CFDefinition.get[Key,Column,Value]Id methods the way the CQL parser does.

/cc [~iamaleksey] [~pkolaczk]",16/Jul/13 20:36;alexliu68;Patch for 1.2 branch is attached.,"16/Jul/13 22:59;jbellis;-1 on using the exception for control flow; looks like the thrift logic should be merged w/ retrieveKeys.

Also, please use modern logging API instead of string concatenation; see b2daab711cd8dc03afb39a6464f21325ba0193ee",17/Jul/13 21:58;alexliu68;5752-1-1.2-branch.txt is attached. It doesn't use exception any more,22/Jul/13 18:28;jbellis;When do we expect partitionBoundColumns to be empty after a non-empty schema_columnfamilies resultset?,"29/Jul/13 21:57;alexliu68;If the key_aliases return as string [], the partitionBoundColumns size is zero. ","30/Jul/13 19:41;jbellis;But you're checking for key_aliases being null earlier.  Which is it going to be?  Both checks should not be necessary.

Also, I think the {{rows.size()==0}} check is bogus; there should always be an entry in schema_columnfamilies.  If there isn't, falling back to describe_columnfamilies isn't going to help.",31/Jul/13 21:22;alexliu68;5752-2-1.2-branch.txt is attached to clean up the code as suggested. It also includes the fix for CqlRecordWriter.,"31/Jul/13 23:21;jbellis;Committed to 1.2 only. (In 2.0 the system schema include the required information for Thrift tables, as mentioned above.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RowIndexEntry.deletionTime raises UnsupportedOperationException when upgrading to 1.2.7,CASSANDRA-5814,12660134,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,bretthoerner,bretthoerner,26/Jul/13 21:19,12/Mar/19 14:17,13/Mar/19 22:29,27/Jul/13 19:20,1.2.8,2.0 rc1,,,,,0,,,,,"Upgrading from 1.2.5 to 1.2.7 immediately caused the following exception. We stopped the node and reverted to 1.2.5.

I'll note that we also run a 1.2.6 cluster that is doing fine, so I think this is caused by a change in 1.2.7.

{code}
ERROR [MutationStage:59] 2013-07-26 19:35:10,460 CassandraDaemon.java (line 192) Exception in thread Thread[MutationStage:59,5,main]
java.lang.ExceptionInInitializerError
	at org.apache.cassandra.utils.CounterId.localIds(CounterId.java:49)
	at org.apache.cassandra.utils.CounterId.getLocalId(CounterId.java:54)
	at org.apache.cassandra.db.context.CounterContext.create(CounterContext.java:105)
	at org.apache.cassandra.db.CounterUpdateColumn.localCopy(CounterUpdateColumn.java:84)
	at org.apache.cassandra.db.CounterUpdateColumn.localCopy(CounterUpdateColumn.java:34)
	at org.apache.cassandra.db.CounterMutation.apply(CounterMutation.java:133)
	at org.apache.cassandra.service.StorageProxy$7.runMayThrow(StorageProxy.java:758)
	at org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:1630)
	at org.apache.cassandra.service.StorageProxy$3.apply(StorageProxy.java:142)
	at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:385)
	at org.apache.cassandra.service.StorageProxy.applyCounterMutationOnLeader(StorageProxy.java:733)
	at org.apache.cassandra.db.CounterMutationVerbHandler.doVerb(CounterMutationVerbHandler.java:53)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.UnsupportedOperationException
	at org.apache.cassandra.db.RowIndexEntry.deletionTime(RowIndexEntry.java:81)
	at org.apache.cassandra.db.columniterator.IndexedSliceReader.<init>(IndexedSliceReader.java:109)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:68)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:44)
	at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:101)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:68)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:272)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1390)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1213)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1125)
	at org.apache.cassandra.db.SystemTable.getCurrentLocalCounterId(SystemTable.java:595)
	at org.apache.cassandra.utils.CounterId$LocalCounterIdHistory.<init>(CounterId.java:194)
	at org.apache.cassandra.utils.CounterId$LocalIds.<clinit>(CounterId.java:42)
	... 16 more
{code} 

The code causing it seems to have changed in 1.2.7 via https://issues.apache.org/jira/browse/CASSANDRA-5677",,,,,,,,,,,,,,,,,,,,,,,,26/Jul/13 21:56;jbellis;5814.txt;https://issues.apache.org/jira/secure/attachment/12594458/5814.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-26 21:56:21.147,,,no_permission,,,,,,,,,,,,340326,,,Sat Jul 27 19:20:44 UTC 2013,,,,,,0|i1mp3z:,340644,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,26/Jul/13 21:56;jbellis;Patch to read the deletion time from the row header when dealing with old sstable formats.,27/Jul/13 15:58;iamaleksey;+1,27/Jul/13 19:20;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in HH metrics,CASSANDRA-5802,12659645,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thobbs,brandon.williams,brandon.williams,24/Jul/13 19:25,12/Mar/19 14:17,13/Mar/19 22:29,30/Jul/13 17:46,2.0 rc1,,,,,,0,,,,,"{noformat}
    [junit] Testcase: testCompactionOfHintsCF(org.apache.cassandra.db.HintedHandOffTest):	Caused an ERROR
    [junit] null
    [junit] java.lang.NullPointerException
    [junit] 	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:191)
    [junit] 	at com.google.common.cache.LocalCache.get(LocalCache.java:3989)
    [junit] 	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3994)
    [junit] 	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4878)
    [junit] 	at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4884)
    [junit] 	at org.apache.cassandra.metrics.HintedHandoffMetrics.incrCreatedHints(HintedHandoffMetrics.java:67)
    [junit] 	at org.apache.cassandra.db.HintedHandOffManager.hintFor(HintedHandOffManager.java:125)
    [junit] 	at org.apache.cassandra.db.HintedHandOffTest.testCompactionOfHintsCF(HintedHandOffTest.java:68)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.HintedHandOffTest FAILED
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,29/Jul/13 18:16;thobbs;0001-Handle-no-matching-endpoint-for-hint-target.patch;https://issues.apache.org/jira/secure/attachment/12594751/0001-Handle-no-matching-endpoint-for-hint-target.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-29 18:16:19.18,,,no_permission,,,,,,,,,,,,339837,,,Tue Jul 30 17:46:55 UTC 2013,,,,,,0|i1mm3r:,340156,,,,,,,,yukim,yukim,,,,,,,,,,29/Jul/13 18:16;thobbs;Patch 0001 gracefully handles the case where a matching endpoint cannot be found for the hint's targetID.,30/Jul/13 17:46;yukim;+1 and committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AE during validation compaction,CASSANDRA-5801,12659635,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,24/Jul/13 18:59,12/Mar/19 14:17,13/Mar/19 22:29,25/Jul/13 15:14,2.0 rc1,,,,,,0,,,,,"While repairing with vnodes enabled:

{noformat}
ERROR [ValidationExecutor:1] 2013-07-24 12:09:36,326 Validator.java (line 197) Failed creating a merkle tree for [repair #d13fd210-f483-11e2-b6fb-f1fe0a5dda64 on ks/cf, (9214460999857687863,-9209369219500956981]], /127.0.0.1 (see log for details)
ERROR [ValidationExecutor:1] 2013-07-24 12:09:36,328 CassandraDaemon.java (line 196) Exception in thread Thread[ValidationExecutor:1,1,main]
java.lang.AssertionError: -9191651187195735134 is not contained in (9214460999857687863,-9209369219500956981]
    at org.apache.cassandra.repair.Validator.add(Validator.java:136)
    at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:669)
    at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:64)
    at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:395)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
    at java.util.concurrent.FutureTask.run(FutureTask.java:166)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,25/Jul/13 14:32;slebresne;5801.txt;https://issues.apache.org/jira/secure/attachment/12594184/5801.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-25 14:32:14.433,,,no_permission,,,,,,,,,,,,339828,,,Thu Jul 25 15:14:22 UTC 2013,,,,,,0|i1mm1r:,340147,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"25/Jul/13 14:32;slebresne;Seems the SSTableScanner implementation was still not right for wrapping ranges. Basically the patch from CASSANDRA-5757 was correctly seeking at the end of the first part of a wrapping range, but it was still returning one wrong key before starting reading from where it had seek to. Attaching patch to fix.",25/Jul/13 15:04;brandon.williams;+1,"25/Jul/13 15:14;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AntiEntropySession fails when OutboundTcpConnection receives IOException,CASSANDRA-5804,12659845,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,awinters,awinters,awinters,25/Jul/13 17:15,12/Mar/19 14:17,13/Mar/19 22:29,26/Jul/13 15:59,1.2.8,2.0 rc1,,,,,0,,,,,"When requesting merkle trees for a repair, if the OutboundTcpConnection grabbed from the connection pool is reset (java.io.IOException: Connection reset by peer), the target node is not marked as dead, the TREE_REQUEST is not retried, and the repair does not fail. Instead, the repair stalls waiting for the merkle tree response which will never arrive.",CentOS 6.3 x86_64,,,,,,,,,,,,,,,,,,,,,,,25/Jul/13 19:00;awinters;5804-v1.patch;https://issues.apache.org/jira/secure/attachment/12594234/5804-v1.patch,25/Jul/13 18:08;awinters;ioexception.txt;https://issues.apache.org/jira/secure/attachment/12594217/ioexception.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-07-26 15:59:26.324,,,no_permission,,,,,,,,,,,,340037,,,Fri Jul 26 15:59:26 UTC 2013,,,,,,0|i1mnbz:,340355,,,,,,,,yukim,yukim,,,,,,,,,,25/Jul/13 18:08;awinters;TRACE level log showing the IOException thrown when sending TREE_REQUEST via MessagingService,"25/Jul/13 18:11;awinters;The dead connections are cross-datacenter, and I believe I have some network equipment _(naughty firewall)_ resetting idle connections. That'd be why gossip says the node is up, even though a pooled {{OutboundTcpConnection}} would be reset due to idleness.","25/Jul/13 18:52;awinters;// if the message was important, such as a repair acknowledgement, put it back on the queue
// to retry after re-connecting.  See CASSANDRA-5393
if (e instanceof SocketException && qm.shouldRetry())

CASSANDRA-5393 was insufficient to completely fix the issue, because {{e instanceof IOException}} should have been used, instead. Looks like a simple fix.","25/Jul/13 18:59;awinters;One-line fix to possibly retry for any IOException, not just SocketException.",25/Jul/13 19:00;awinters;Patch against cassandra-1.2 git,"26/Jul/13 15:59;yukim;+1 and committed.
Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Column can expire while lazy compacting it...,CASSANDRA-5799,12659571,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,frousseau,frousseau,24/Jul/13 13:37,12/Mar/19 14:17,13/Mar/19 22:29,26/Jul/13 18:15,1.2.7,,,,,,0,,,,,"Using TTL + range tombstones can lead to failure while lazy compacting rows.

Scenario to reproduce :
 - create an SSTable with one row and some columns and a TTL of 8 seconds
 - wait one second
 - create a second SSTable with the same rowkey as above, and add a range tombstone
 - start the first pass of the lazy compaction before the columns with TTL are expired
 - wait 10 seconds (enough for columns with TTL to expire)
 - continue lazy expiration
 - the following assertion will fail :
    [junit] junit.framework.AssertionFailedError: originally calculated column size of 1379 but now it is 1082
    [junit] 	at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:150)



",,,,,,,,,,,,,,CASSANDRA-5720,,,,,,,,,,24/Jul/13 14:28;slebresne;5799.txt;https://issues.apache.org/jira/secure/attachment/12593942/5799.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-24 14:20:08.779,,,no_permission,,,,,,,,,,,,339764,,,Thu Aug 22 07:58:06 UTC 2013,,,,,,0|i1mlnj:,340083,,,,,,,,jbellis,jbellis,,,,,,,,,,"24/Jul/13 13:45;frousseau;Note : it was initially reported here http://www.mail-archive.com/user@cassandra.apache.org/msg31277.html

The row size is around 153Mb, thus, compacting at 16Mb/s => around 9s to compact, which is an open window for columns to expire","24/Jul/13 14:20;jbellis;If it requires compaction to take longer than TTL to be a problem, I'm inclined to say ""don't do that in 1.2.""  2.0 has single-pass compaction so should not matter there.","24/Jul/13 14:28;slebresne;bq. If it requires compaction to take longer than TTL to be a problem

No, it only requires that a TTL expire between the first and second phase, which can happen whatever the TTL and compaction time is. For some reason, DeletionTime.isDeleted(), which is just supposed to check if the deletion time shadows a given column is completely broken (it checks if the columns is deleted which shouldn't even matter for that method) and depends on the current time. Attaching simple patch to fix.
",24/Jul/13 14:35;jbellis;+1,"24/Jul/13 14:45;slebresne;Alright, committed, thanks","27/Jul/13 20:34;efalcao;I was excited to see this fix in 1.2.7 but I fear it's still an issue. Here is a stack I just saw in my freshly upgraded 1.2.7 cluster:

{code}
ERROR [CompactionExecutor:17] 2013-07-27 17:24:31,132 CassandraDaemon.java (line 192) Exception in thread Thread[CompactionExecutor:17,1,RMI Runtime]
java.lang.AssertionError: originally calculated column size of 516898177 but now it is 516898234
	at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:162)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:355)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
{code}

Does it matter that my SSTables are mostly on version ic?

FWIW, I've been unable to compact this CF since we went to C* 1.2. I'll attach other JIRAs that may be related.
","27/Jul/13 20:36;efalcao;Here's my earlier JIRA that might be related. Any thoughts, Sylvain?","22/Aug/13 07:58;slebresne;bq. Does it matter that my SSTables are mostly on version ic?

I shouldn't matter, no.

bq. but I fear it's still an issue. Here is a stack I just saw in my freshly upgraded 1.2.7 cluster

It could be that there is still a problem. But I'm confident that we did fixed one issue with this ticket, so that would be a separate problem. Let's track that new problem in CASSANDRA-5720 then maybe. Can you indicate in that latter issue that you can reproduce on 1.2.7 btw (so we don't close it as a duplicate of something fixed).

bq. I've been unable to compact this CF since we went to C* 1.2.

If you have some sstables that always fail to compact with that error on 1.2.7, then would you be allowed to provide that set of sstables privately so I can check what's going on on my side?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix trigger directory detection code,CASSANDRA-5826,12660523,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,iamaleksey,iamaleksey,29/Jul/13 23:33,12/Mar/19 14:17,13/Mar/19 22:29,06/Aug/13 02:55,2.0 rc1,,,,,,0,triggers,,,,"At least when building from source, Cassandra determines the trigger directory wrong. C* calculates the trigger directory as 'build/triggers' instead of 'triggers'.

FBUtilities.cassandraHomeDir() is to blame, and should be replaced with something more robust.",OS X,,,,,,,,,,,,,,,,,,,,,,,31/Jul/13 19:04;vijay2win@yahoo.com;0001-5826.patch;https://issues.apache.org/jira/secure/attachment/12595243/0001-5826.patch,05/Aug/13 23:39;vijay2win@yahoo.com;0001-handle-trigger-non-existance-v2.patch;https://issues.apache.org/jira/secure/attachment/12596240/0001-handle-trigger-non-existance-v2.patch,05/Aug/13 20:18;vijay2win@yahoo.com;0001-handle-trigger-non-existance.patch;https://issues.apache.org/jira/secure/attachment/12596207/0001-handle-trigger-non-existance.patch,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-07-30 18:41:00.137,,,no_permission,,,,,,,,,,,,340714,,,Tue Aug 06 03:20:44 UTC 2013,,,,,,0|i1mrhr:,341032,,,,,,,,jbellis,jbellis,,,,,,,,,,"30/Jul/13 18:41;vijay2win@yahoo.com;Probably have to change the build.xml to copy the trigger directory to build like what we do with conf directory?
I will add the above and also add it to Debian package may be (in addition adding a property to override the trigger absolute path).","31/Jul/13 19:04;vijay2win@yahoo.com;Attached a small patch moves the trigger directory into conf directory, hope it is fine. that way we can just search for the triggers directory in the class path (which is Conf). Thanks!","02/Aug/13 17:53;jbellis;What is the Right Thing to do here?

I kind of like the original idea of locating lib/ and making triggers/ be a sibling of that by convention.

""Any directory named triggers on the classpath"" makes me nervous.

What would a Real Java Programmer do here [~zznate]?","02/Aug/13 18:30;zznate;bq. What would a Real Java Programmer do here Nate McCall?

Awww shucks. Capitalized and everything!

I would expect that it is on me to put my own dependencies in the existing ""lib"" directory. As a principle of least surprise: web containers, such as Tomcat, have a top level lib directory where you can put shared dependencies (http://tomcat.apache.org/tomcat-7.0-doc/appdev/deployment.html#Shared_Library_Files). 

This has the benefit of not requiring modification of cassandra.in.sh or similar init scripts already in the wild. 

As long as we are not trying to isolate classloaders or anything (not a bad idea for triggers in the future) there is no need technically or conventionally to have user binaries somewhere else. If folks want something different, they can hack the scripts to noodle the classpath to their hearts content. ","02/Aug/13 18:57;vijay2win@yahoo.com;{quote}
As long as we are not trying to isolate classloaders or anything 
{quote}

Actually we do it with triggers, similar to what solr does for Tokenizer code etc (but not the same). 

For the record: You can place all of your dependencies in the trigger directory except everything which Cassandra depends on.
If the user uses maven for building, all he needs to do is, and place the jars in the trigger directory.

{code}
    <dependency>
      <groupId>org.apache.cassandra</groupId>
      <artifactId>cassandra-all</artifactId>
      <version>2.0.0-beta2</version>
      <scope>provided</scope>
    </dependency>
{code}

My understanding is that, Java doesn't do nested class path scanning on sub directories, hence conf file was ok to do. 
But understand it is kind of scary if someone places in conf instead.","02/Aug/13 20:13;zznate;Ok, if the classloader is already isolated, a separate top-level ""triggers"" dir(s) would be fine. 

As a point of reference, and it's significantly more sophisticated than our current needs here, but Vert.x has a nice scanning and loading system for this.
Details:
http://vertx.io/mods_manual.html#module-classpath
Impl:
https://github.com/eclipse/vert.x/blob/master/vertx-platform/src/main/java/org/vertx/java/platform/impl/ModuleClassLoader.java

This approach has the benefit of being able load directly from dirs and bintray or maven repos (local, remote or central). This flexibility is way outside scope here, but just food for thought. 


","03/Aug/13 02:04;jbellis;What the hell, we're already labeling this Experimental.

Ship it!","03/Aug/13 18:32;vijay2win@yahoo.com;Committed to trunk, Thanks!

Nate: All we wanted to do was to separate/sandbox the ITriggers from already loaded Cassandra's classes, https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/triggers/CustomClassLoader.java.","05/Aug/13 18:33;brandon.williams;Reopening because first of all the directory detection code doesn't work when the directory does not exist:

{noformat}
ERROR 18:32:00,800 Exception in thread Thread[NonPeriodicTasks:1,5,main]
java.lang.ExceptionInInitializerError
        at org.apache.cassandra.service.StorageProxy.mutateWithTriggers(StorageProxy.java:535)
        at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithoutCondition(ModificationStatement.java:358)
        at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:342)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:101)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:117)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:130)
        at org.apache.cassandra.auth.Auth.setupDefaultSuperuser(Auth.java:209)
        at org.apache.cassandra.auth.Auth.access$000(Auth.java:44)
        at org.apache.cassandra.auth.Auth$1.run(Auth.java:144)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.utils.FBUtilities.cassandraTriggerDir(FBUtilities.java:351)
        at org.apache.cassandra.triggers.TriggerExecutor.<init>(TriggerExecutor.java:45)
        at org.apache.cassandra.triggers.TriggerExecutor.<clinit>(TriggerExecutor.java:41)
        ... 17 more
{noformat}

And secondly, I don't think we should throw if the directory doesn't exist, instead just move along without triggers (perhaps log a warning)","05/Aug/13 18:42;vijay2win@yahoo.com;Hi Brandon, Oooops... Isn't the directory found in conf? i can remove the RTE and make it log, if not found.","05/Aug/13 18:46;brandon.williams;The config is unmodified, but the conf/triggers dir itself is missing.","05/Aug/13 20:18;vijay2win@yahoo.com;Hi Brandon, Attached, handles un reachable trigger directory.","05/Aug/13 20:35;brandon.williams;This actually doesn't help since the NPE is occurring slightly sooner:

{noformat}
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.utils.FBUtilities.cassandraTriggerDir(FBUtilities.java:350)
        at org.apache.cassandra.triggers.TriggerExecutor.reloadClasses(TriggerExecutor.java:58)
        at org.apache.cassandra.triggers.TriggerExecutor.<init>(TriggerExecutor.java:49)
        at org.apache.cassandra.triggers.TriggerExecutor.<clinit>(TriggerExecutor.java:41)
        ... 17 more
{noformat}","05/Aug/13 20:42;vijay2win@yahoo.com;Hi Brandon did the patch apply clean? 

{code}
        File tiggerDirectory = FBUtilities.cassandraTriggerDir();
        if (tiggerDirectory == null)
            return;
{code}

should save a NPE, i did test it and worked fine for me.","05/Aug/13 21:39;brandon.williams;The NPE is on 350 (with the patch applied) in FBU, which is this:

{noformat}
            triggerDir = new File(FBUtilities.class.getClassLoader().getResource(DEFAULT_TRIGGER_DIR).getFile());
{noformat}","05/Aug/13 23:39;vijay2win@yahoo.com;Hi Brandon, fixed in v2 ","06/Aug/13 01:51;brandon.williams;+1, minor nit: s/Directory doesnt/directory doesn't/ in the warning.","06/Aug/13 02:55;vijay2win@yahoo.com;Committed, with nit Thanks!","06/Aug/13 03:11;jbellis;It looks like you only committed to trunk.  You should cherry-pick to 2.0.0, then merge to 2.0 and then to trunk again.","06/Aug/13 03:20;vijay2win@yahoo.com;Done, sorry for all the mess on a simple patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstableloader broken in 1.2.7/1.2.8,CASSANDRA-5820,12660414,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thobbs,nickmbailey,nickmbailey,29/Jul/13 16:41,12/Mar/19 14:17,13/Mar/19 22:29,30/Jul/13 20:08,1.2.9,,,Legacy/Tools,,,0,,,,,"I don't see this happen on 1.2.6.

To reproduce (on a fresh single node cluster):

{noformat}
[Nicks-MacBook-Pro:11:33:06 (cassandra-1.2.7)*] cassandra$ bin/cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 3.1.4 | Cassandra 1.2.7-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.36.0]
cqlsh> CREATE KEYSPACE test_backup_restore WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
cqlsh> use test_backup_restore;
cqlsh:test_backup_restore> CREATE TABLE cf0 (
                       ...   a text PRIMARY KEY,
                       ...   b text,
                       ...   c text
                       ... );
cqlsh:test_backup_restore> INSERT INTO cf0 (a, b, c) VALUES ( 'a', 'b', 'c');
cqlsh:test_backup_restore> select * from cf0;

 a | b | c
---+---+---
 a | b | c

cqlsh:test_backup_restore> ^D
[Nicks-MacBook-Pro:11:34:22 (cassandra-1.2.7)*] cassandra$ bin/nodetool snapshot
Requested creating snapshot for: all keyspaces
Snapshot directory: 1375115668449
[Nicks-MacBook-Pro:11:34:40 (cassandra-1.2.7)*] cassandra$ mkdir -p test_backup_restore/snapshots
[Nicks-MacBook-Pro:11:34:48 (cassandra-1.2.7)*] cassandra$ cp /var/lib/cassandra/data/test_backup_restore/cf0/snapshots/1375115668449/* test_backup_restore/snapshots/
[Nicks-MacBook-Pro:11:35:14 (cassandra-1.2.7)*] cassandra$ bin/sstableloader --debug -v -d 127.0.0.1 test_backup_restore/snapshots
Streaming revelant part of test_backup_restore/snapshots/test_backup_restore-cf0-ic-1-Data.db  to [/127.0.0.1]
org.apache.cassandra.io.util.CompressedSegmentedFile cannot be cast to org.apache.cassandra.io.util.CompressedPoolingSegmentedFile
java.lang.ClassCastException: org.apache.cassandra.io.util.CompressedSegmentedFile cannot be cast to org.apache.cassandra.io.util.CompressedPoolingSegmentedFile
	at org.apache.cassandra.io.sstable.SSTableReader.getCompressionMetadata(SSTableReader.java:574)
	at org.apache.cassandra.streaming.StreamOut.createPendingFiles(StreamOut.java:179)
	at org.apache.cassandra.streaming.StreamOut.transferSSTables(StreamOut.java:154)
	at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:145)
	at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:67)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,30/Jul/13 15:52;yukim;0001-Add-SSTableLoader-unit-test.patch;https://issues.apache.org/jira/secure/attachment/12594975/0001-Add-SSTableLoader-unit-test.patch,30/Jul/13 18:21;thobbs;0002-Create-CompressedFile-common-interface.patch;https://issues.apache.org/jira/secure/attachment/12595007/0002-Create-CompressedFile-common-interface.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-07-30 15:52:20.945,,,no_permission,,,,,,,,,,,,340606,,,Tue Jul 30 20:08:34 UTC 2013,,,,,,0|i1mqtr:,340924,,,,,,,,jbellis,jbellis,,,,,,,,,,"30/Jul/13 15:52;yukim;Looks like this is a regression from CASSANDRA-5555.
(I think workaround is to use sstableloader from 1.2.6).

Attached unit test for SSTableLoader. if run with 'ant test-compression -Dtest.name=SSTableLoaderTest', it fails as described above.",30/Jul/13 18:21;thobbs;Patch 0002 creates a common Interface for the two Compressed*File classes to allow access to the compression metadata without casting.,30/Jul/13 20:08;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix quoting in CqlPagingRecordReader and CqlRecordWriter,CASSANDRA-5824,12660496,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,alexliu68,alexliu68,alexliu68,29/Jul/13 21:16,12/Mar/19 14:17,13/Mar/19 22:29,30/Jul/13 19:32,1.2.9,2.0 rc1,,,,,0,,,,,"To support case sensitive in CQL, we need add double quotes to the name of columns and table.",,,,,,,,,,,,,,,,,,,,,,,,29/Jul/13 22:18;alexliu68;5824-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12594806/5824-1.2-branch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-29 21:18:36.198,,,no_permission,,,,,,,,,,,,340688,,,Tue Jul 30 19:30:56 UTC 2013,,,,,,0|i1mrbz:,341006,,,,,,,,jbellis,jbellis,,,,,,,,,,29/Jul/13 21:18;jbellis;done in CASSANDRA-5763,29/Jul/13 22:18;alexliu68;It fixes the CqlRecordWriter,"30/Jul/13 19:30;jbellis;To clarify: this fixes redundant quoting in CqlPRR of what keyString has already quoted for us, and adds quoting to CqlRW.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate classes in Cassandra-all package.,CASSANDRA-5833,12660810,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,dbrosius,samschumer,samschumer,30/Jul/13 23:08,12/Mar/19 14:17,13/Mar/19 22:29,14/Mar/14 04:45,2.0.7,,,Legacy/CQL,Packaging,,0,maven,,,,"As of Cassandra-All version 1.1.6 the classes org.apache.cassandra.thrift.ITransportFactory and org.apache.cassandra.thrift.TFramedTransportFactory are located in both the cassandra-thrift and the cassandra-all Maven JARS, and caasandra-thrift is imported by cassandra-all POM. This makes the cassandra-all package unbuildable when using the duplicate-finder Maven extension. The files were originally copied over due to [CASSANDRA-4668|https://issues.apache.org/jira/browse/CASSANDRA-4668]. All versions since have failed to build when using this maven extension.",,,,,,,,,,,,,,,,,,,,,,,,14/Mar/14 00:36;dbrosius;5833.txt;https://issues.apache.org/jira/secure/attachment/12634597/5833.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-31 13:01:39.889,,,no_permission,,,,,,,,,,,,340999,,,Fri Mar 14 04:45:53 UTC 2014,,,,,,0|i1mt93:,341317,1.2.7,,,,,,,jbellis,jbellis,,,1.1.6,,,,,,,"31/Jul/13 13:01;brandon.williams;It seems like the easiest thing to do would be not use the extension, or configure it to ignore cassandra-thrift.","31/Jul/13 13:10;jbellis;bq. cassandra-thrift is imported by cassandra-all POM

that also sounds like a problem to me.","31/Jul/13 17:20;__tango;Yes, the fact that cassandra-thrift imports cassandra-thrift means that you can't get away from this problem. Disabling the duplicate-finder extension is a sub-optimal way to fix the problem as having the exact same class in two separate jar files can lead to very odd bugs later in life if the files ever diverge.  ","13/Mar/14 22:03;jbellis;Is this still a problem, Dave?",14/Mar/14 00:23;dbrosius;The problem still exists on trunk. It seems we should remove those two classes from cassandra-all. I believe those are the only classes in conflict.,"14/Mar/14 00:36;dbrosius;patch is against trunk.

I can redo and apply to whatever branch you like assuming it is ok.",14/Mar/14 01:23;jbellis;LGTM; I'm okay with either 2.0+ or 2.1+.,14/Mar/14 04:45;dbrosius;committed to cassandra-2.0 as c49d33633aa07551af52e40277e284eb78bb73d4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Paxos loops endlessly due to faulty condition check,CASSANDRA-5830,12660728,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,soumava,soumava,soumava,30/Jul/13 17:52,12/Mar/19 14:17,13/Mar/19 22:29,05/Aug/13 21:44,2.0.0,,,,,,0,LWT,paxos,,,"Following is the code segment (StorageProxy.java:361) which causes the issue: 

Start is the start time of the paxos, is always less than the current system time, and therefore the negative difference is always less than the timeout. 

{code:title=StorageProxy.java|borderStyle=solid}
private static UUID beginAndRepairPaxos(long start, ByteBuffer key, CFMetaData metadata, List<InetAddress> liveEndpoints, int requiredParticipants, ConsistencyLevel consistencyForPaxos)
    throws WriteTimeoutException
    {
        long timeout = TimeUnit.MILLISECONDS.toNanos(DatabaseDescriptor.getCasContentionTimeout());

        PrepareCallback summary = null;
        while (start - System.nanoTime() < timeout)
        {
            long ballotMillis = summary == null
                              ? System.currentTimeMillis()
                              : Math.max(System.currentTimeMillis(), 1 + UUIDGen.unixTimestamp(summary.inProgressCommit.ballot));
            UUID ballot = UUIDGen.getTimeUUID(ballotMillis);
{code}

Here, the paxos gets stuck when PREPARE returns 'true' but with inProgressCommit. The code in StorageProxy.java:beginAndRepairPaxos() then tries to issue a PROPOSE and COMMIT for the inProgressCommit, and if it repeatedly receives 'false' as a PREPARE_RESPONSE it gets stuck in an endless loop until PREPARE_RESPONSE is true. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-08-05 21:44:40.811,,,no_permission,,,,,,,,,,,,340917,,,Mon Aug 05 21:44:40 UTC 2013,,,,,,0|i1msqv:,341235,,,,,,,,jbellis,jbellis,,,2.0 beta 1,,,,,,,"05/Aug/13 21:44;jbellis;Fixed condition check in 4b4ccc3ccfcc7be8fad0b25bde9a180f0016d520, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix 2i on composite components omissions,CASSANDRA-5851,12662147,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,06/Aug/13 16:03,12/Mar/19 14:17,13/Mar/19 22:29,12/Aug/13 10:58,2.0.0,,,,,,0,,,,,"There some edge-cases, not covered by CASSANDRA-5125, the attached patch fixes those:

(Assuming CREATE TABLE test (pk0 int, pk1 int, ck0 int, ck1 int, val int, PRIMARY KEY ((pk0, pk1), ck0, ck1)))

- could not create a 2i on the first part of a composite partition key (pk0)
- if created, it couldn't work because of getKeyBounds() returning non-empty bounds
- could create an index on the first clustering key column (ck0), but it would never actually be triggered on reads
- queries like SELECT * FROM test WHERE pk0 = x AND pk1 = y AND ck1 = z would throw an exception because COCK.makeIndexColumnNameBuilder() couldn't handle empty provided columnName
- cqlsh could not describe any of these indexes because it was taking column aliases and key aliases from schema_columnfamilies and not reading them directly from schema_columns (had to do the related refactoring).",,,,,,,,,,,,,,,,,,,,,,,,07/Aug/13 21:43;iamaleksey;5851-extra.txt;https://issues.apache.org/jira/secure/attachment/12596720/5851-extra.txt,06/Aug/13 16:04;iamaleksey;5851.txt;https://issues.apache.org/jira/secure/attachment/12596362/5851.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-08-07 05:27:03.725,,,no_permission,,,,,,,,,,,,342151,,,Mon Aug 12 10:58:32 UTC 2013,,,,,,0|i1n0a7:,342456,,,,,,,,jbellis,jbellis,,,,,,,,,,"07/Aug/13 05:27;duyleekun;I got this when I query on 2 indexed column. Is this expected?

SELECT * FROM gurugara.outgoing_edge WHERE source_id = 3 AND dest_id = 1 AND type = 'meh'  ALLOW FILTERING;
`dest_id` and `type` are secondary indexed column


ERROR 07:23:13,702 Exception in thread Thread[ReadStage:4,5,main]
java.lang.RuntimeException: java.lang.RuntimeException: Unable to search across multiple secondary index types
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1867)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.RuntimeException: Unable to search across multiple secondary index types
	at org.apache.cassandra.db.index.SecondaryIndexManager.search(SecondaryIndexManager.java:523)
	at org.apache.cassandra.db.ColumnFamilyStore.search(ColumnFamilyStore.java:1627)
	at org.apache.cassandra.db.RangeSliceCommand.executeLocally(RangeSliceCommand.java:135)
	at org.apache.cassandra.service.StorageProxy$LocalRangeSliceRunnable.runMayThrow(StorageProxy.java:1358)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1863)
	... 3 more
","07/Aug/13 05:31;iamaleksey;bq. I got this when I query on 2 indexed column. Is this expected?

Unfortunately, yes.",07/Aug/13 21:44;iamaleksey;Attached 5851-extra.txt that handles non-complete column names in CIOCK.makeIndexColumnNameBuilder() when used for searches.,08/Aug/13 03:07;jbellis;Can you add a unit test?,"08/Aug/13 23:49;iamaleksey;bq. Can you add a unit test?

https://github.com/riptano/cassandra-dtest/commit/26f9cc6ff5e6d95d44784f034a3d8ed974f415b3",11/Aug/13 05:11;jbellis;+1,"12/Aug/13 10:58;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stress reports invalid latencies,CASSANDRA-5896,12663892,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,dbrosius,enigmacurry,enigmacurry,15/Aug/13 17:44,12/Mar/19 14:17,13/Mar/19 22:29,16/Aug/13 12:40,2.1 beta1,,,Legacy/Tools,,,0,,,,,"I get this output from trunk:

{code}
$ ccm node1 stress
total,interval_op_rate,interval_key_rate,latency,95th,99th,elapsed_time
176678,17667,17667,153510.0,1322730.2,181065524.2,10
335566,15888,15888,145371.0,1220290.5,181065524.2,20
498488,16292,16292,145535.0,1227675.0,46032966.9,30
642012,14352,14352,143999.0,1130215.4,46004055.9,40
776083,13407,13407,145573.5,1737871.2,153211818.5,50
1000000,22391,22391,145128.0,1336373.6,148773096.0,60


Averages from the middle 80% of values:
interval_op_rate          : 15521
interval_key_rate         : 15521
latency median            : 146797.7
latency 95th percentile   : 1327756.5
latency 99.9th percentile : 121475978.0
Total operation time      : 00:01:00
END
{code}

Notice the wild latency values.

Whereas this is the output that I've come to expect (from cassandra-2.0):

{code}
$ ccm node1 stress
total,interval_op_rate,interval_key_rate,latency,95th,99th,elapsed_time
157972,15797,15797,0.1,1.2,119.1,10
346627,18865,18865,0.1,1.2,118.8,20
493937,14731,14731,0.1,1.2,119.2,30
663086,16914,16914,0.2,1.3,119.8,40
893083,22999,22999,0.1,1.1,65.6,50
1000000,10691,10691,0.1,1.1,65.6,55


Averages from the middle 80% of values:
interval_op_rate          : 17861
interval_key_rate         : 17861
latency median            : 0.1
latency 95th percentile   : 1.2
latency 99.9th percentile : 108.5
Total operation time      : 00:00:55
END
{code}",,,,,,,,,,,,,,,,,,,,,,,,16/Aug/13 05:26;dbrosius;5896.txt;https://issues.apache.org/jira/secure/attachment/12598363/5896.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-15 17:56:40.117,,,no_permission,,,,,,,,,,,,343893,,,Fri Aug 16 12:40:39 UTC 2013,,,,,,0|i1nazz:,344195,,,,,,,,jbellis,jbellis,,,2.1 rc3,,,,,,,15/Aug/13 17:56;jbellis;Could this be caused by the new metrics library [~dbrosius]?,"15/Aug/13 19:48;dbrosius;likely,.. i'll look","16/Aug/13 05:26;dbrosius;timer is now returning values as nanoseconds, so convert to milliseconds as was expected before.",16/Aug/13 06:47;jbellis;+1,16/Aug/13 06:50;jbellis;Nit: could use TimeUnit.NANOSECONDS.toMillis instead of rolling our own.,"16/Aug/13 06:59;dbrosius;Yeah i was going to use that, but that is only integer based, and the existing output was decimal.","16/Aug/13 07:16;jbellis;Ah, right.",16/Aug/13 12:40;dbrosius;committed to trunk (2.1) as 5dabd1cc0c65b329ff518d7ad3f09e4c11494f18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GossipingPropertyFileSnitch does not auto-reload local rack/dc,CASSANDRA-5897,12664029,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,daniels,jeromatron,jeromatron,16/Aug/13 09:38,12/Mar/19 14:17,13/Mar/19 22:29,06/Mar/14 17:29,2.1 rc2,,,,,,0,,,,,"With the property file snitch, I can change the rack and dc of a node in cassandra-topology.properties from DC1/RAC1 to DC2/RAC2 while the server is running and that configuration is reloaded a short time later.

With the gossiping property file snitch, I change the cassandra-rackdc.properties when the server is running from DC1/RAC1 to DC2/RAC2 while the server is running and it never reloads.  It requires a restart.",,,,,,,,,,,,,,,,,,,,,,,,27/Feb/14 07:37;daniels;trunk-5897-v2.txt;https://issues.apache.org/jira/secure/attachment/12631475/trunk-5897-v2.txt,27/Feb/14 07:41;daniels;trunk-5897-v3.txt;https://issues.apache.org/jira/secure/attachment/12631476/trunk-5897-v3.txt,03/Mar/14 00:10;daniels;trunk-5897-v4.txt;https://issues.apache.org/jira/secure/attachment/12632166/trunk-5897-v4.txt,26/Feb/14 05:42;daniels;trunk-5897.txt;https://issues.apache.org/jira/secure/attachment/12631156/trunk-5897.txt,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2014-02-25 04:13:58.014,,,no_permission,,,,,,,,,,,,344030,,,Thu Mar 06 17:29:36 UTC 2014,,,,,,0|i1nbuf:,344332,1.2.6,2.1 rc3,,,,,,thobbs,thobbs,,,,,,,,,,25/Feb/14 04:13;daniels;I'll see if I can make this work.  Should be relatively straightforward.,"26/Feb/14 05:44;daniels;Here is my stab at a fix for this issue.  Pretty much just following update listening logic that is already implemented in PropertyFileSnitch.

","26/Feb/14 20:47;thobbs;Thanks, Daniel!

Overall this looks pretty good.  I would still check if the dc, rack, or preferLocal have changed before invalidating cached rings and gossiping.  Although ResourceWatch only triggers a reload if the mod time on the file goes up, that doesn't necessarily mean the dc or rack have changed.

And a couple of nitpicks:
* You don't need curly braces for single-line ""if"" statements
* Name the constant {{DEFAULT_REFRESH_PERIOD_IN_SECONDS}} instead of {{DEFAULT_REFRESH_PERIOD_IN_S}}
* Although this case should be rare, I would log at error instead of debug if there's a ConfigurationException setting up the watcher.",27/Feb/14 07:37;daniels;Addressed CR feedback.  Note that I have previously omitted a couple of new test files from the patch (looks like git instruction on http://wiki.apache.org/cassandra/HowToContribute might be slightly off).,27/Feb/14 07:41;daniels;One more amendment:  elevating log level on configuration exception,"28/Feb/14 17:59;thobbs;bq. looks like git instruction on http://wiki.apache.org/cassandra/HowToContribute might be slightly off

You're right.  I'll get that fixed, thanks.

v3 is almost good to go, it just needs some minor cleanup on the test file:
* Remove unused imports
* The docstring for testBasic isn't really useful.  I would just delete it and name the method ""testAutoReloadConfig"" or something similar.
* You can shorten the reload period and sleep to 1 and 1.5 seconds, respectively.

Thanks!",03/Mar/14 00:10;daniels;addressed test issues,04/Mar/14 21:17;thobbs;+1,06/Mar/14 17:29;thobbs;Comitted,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OOM while loading key cache at startup,CASSANDRA-5706,12654953,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,frousseau,frousseau,frousseau,26/Jun/13 15:40,12/Mar/19 14:17,13/Mar/19 22:29,28/Jun/13 15:31,1.2.7,,,,,,0,,,,,"Steps to be able to reproduce it :
 - have a heap of 1Gb
 - have a saved key cache without the SSTables

When looking at KeyCacheSerializer.serialize : it always writes a Boolean
When looking at KeyCacheSerializer.deserialize : no Boolean is read if SSTable is missing...

In case of a promoted index, RowIndexEntry.serializer.skip(...) should be called rather than RowIndexEntry.serializer.skipPromotedIndex(...) (again for symmetry between serialization/deserialization)

Attached is a proposed patch",,,,,,,,,,,,,,,,,,,,,,,,26/Jun/13 15:41;frousseau;5706-OOM-while-loading-key-cache-at-startup.patch;https://issues.apache.org/jira/secure/attachment/12589764/5706-OOM-while-loading-key-cache-at-startup.patch,26/Jun/13 22:09;jbellis;5706-v2-txt;https://issues.apache.org/jira/secure/attachment/12589803/5706-v2-txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-06-26 22:00:45.31,,,no_permission,,,,,,,,,,,,335230,,,Fri Oct 18 08:47:42 UTC 2013,,,,,,0|i1ltqf:,335554,,,,,,,,jbellis,jbellis,,,,,,,,,,26/Jun/13 22:00;jbellis;(setting affects- to 1.2.5 b/c suspect was introduced by CASSANDRA-5492),"26/Jun/13 22:06;jbellis;(actually, annotate says this dates back to CASSANDRA-3762)",26/Jun/13 22:09;jbellis;Your analysis is correct; attached is slightly revised patch to emphasize that we already read exactly one boolean,27/Jun/13 23:58;jbellis;LGTY Fabien?,"28/Jun/13 07:56;frousseau;Sorry for the delay.

Yep, it looks good to me.
",28/Jun/13 15:31;jbellis;committed,18/Oct/13 08:47;tomvandenberge;I'm having this problem again since I've upgraded to 1.2.10.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
now() is being rejected in INSERTs when inside collections,CASSANDRA-5795,12659472,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,iamaleksey,iamaleksey,24/Jul/13 00:52,12/Mar/19 14:17,13/Mar/19 22:29,24/Jul/13 15:49,1.2.7,,,,,,0,,,,,"Lists, Sets and Maps reject NonTerminal terms in prepare:

{code}
    if (t instanceof Term.NonTerminal)
                    throw new InvalidRequestException(String.format(""Invalid list literal for %s: bind variables are not supported inside collection literals"", receiver));
{code}

and now() is instanceof NonTerminal since CASSANDRA-5616, hence

{noformat}
cqlsh:test> insert into demo (id, timeuuids) values (0, [now()]);
Bad Request: Invalid list literal for tus: bind variables are not supported inside collection literals
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,24/Jul/13 07:58;slebresne;5795.txt;https://issues.apache.org/jira/secure/attachment/12593881/5795.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-24 07:58:45.315,,,no_permission,,,,,,,,,,,,339665,,,Wed Jul 24 15:49:24 UTC 2013,,,,,,0|i1ml1z:,339985,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"24/Jul/13 07:58;slebresne;CASSANDRA-5616 was indeed slightly more work that I though for collections, my bad. Attaching patch to fix (on the plus side, if we ever want to support bind markers inside collections (not that I think it's useful), the code will be mostly there). I've pushed a dtest too.","24/Jul/13 15:28;iamaleksey;+1

nits:
- The copy-pasted DelayedValue code from Lists in Sets still references 'List' in bind (""List value too long"")
","24/Jul/13 15:49;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Buffer Underflow during streaming,CASSANDRA-5792,12659202,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,brandon.williams,brandon.williams,22/Jul/13 21:33,12/Mar/19 14:17,13/Mar/19 22:29,30/Jul/13 22:59,2.0 rc1,,,,,,0,,,,,"{noformat}
ERROR [STREAM-IN-/127.0.0.3] 2013-07-22 16:19:50,597 StreamSession.java (line 414) Streaming error occurred
java.nio.BufferUnderflowException
    at java.nio.Buffer.nextGetIndex(Buffer.java:492)
    at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:135)
    at org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:52)
    at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:288)
    at java.lang.Thread.run(Thread.java:722)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,23/Jul/13 15:41;yukim;5792.txt;https://issues.apache.org/jira/secure/attachment/12593712/5792.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-23 15:41:41.692,,,no_permission,,,,,,,,,,,,339395,,,Tue Jul 30 23:00:06 UTC 2013,,,,,,0|i1mjdz:,339715,,,,,,,,jbellis,jbellis,,,,,,,,,,"23/Jul/13 15:41;yukim;There is a chance we get nothing from socket,  like when socket gets closed.
Patch attached to check we actually read something from socket.",30/Jul/13 21:32;jbellis;+1,30/Jul/13 23:00;yukim;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The Pig CqlStorage/AbstractCassandraStorage classes don't handle collection types,CASSANDRA-5867,12662857,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,alexliu68,jeromatron,jeromatron,09/Aug/13 14:04,12/Mar/19 14:17,13/Mar/19 22:29,10/Sep/13 18:47,1.2.10,2.0.1,,,,,2,pig,,,,"The CqlStorage class gets the Pig data type for values from the AbstractCassandraStorage class, in the getPigType method.  If it isn't a known data type, it makes the value into a ByteArray.  Currently there aren't any cases there for lists, maps, and sets.
https://github.com/apache/cassandra/blob/cassandra-1.2.8/src/java/org/apache/cassandra/hadoop/pig/AbstractCassandraStorage.java#L336

See this describe output from the grunt shell:

{code}
grunt> describe listdata ;                                        
listdata: {id: (name: chararray,value: int),alist: (name: chararray,value: bytearray),amap: (name: chararray,value: bytearray),aset: (name: chararray,value: bytearray)}
{code}

where the cql data structures had this schema:

{code}
CREATE TABLE alltypes (
  id int PRIMARY KEY,
  alist list<text>,
  amap map<text, text>,
  aset set<text>
{code}

It turns out that if you cast the map in grunt to a pig map, then it sort of works, but I don't think we should probably use a pig map.  Lists don't appear to work at all, as there is no Pig analogue.  I *think* you could probably just do a UDF to cast these things, but we already have all of the type information, so we just need to change them to tuples or bags or whatever.",,,,,,,,,,,,,,,,,,,CASSANDRA-6073,,,,,15/Aug/13 02:08;alexliu68;5867-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12598137/5867-1.2-branch.txt,15/Aug/13 16:51;alexliu68;5867-2-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12598238/5867-2-1.2-branch.txt,19/Aug/13 20:14;alexliu68;5867-3-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12598812/5867-3-1.2-branch.txt,27/Aug/13 04:15;alexliu68;5867-4-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12600091/5867-4-1.2-branch.txt,28/Aug/13 23:56;alexliu68;5867-5-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12600509/5867-5-1.2-branch.txt,27/Aug/13 04:15;alexliu68;5867-bug-fix-filter-push-down-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12600090/5867-bug-fix-filter-push-down-1.2-branch.txt,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2013-08-09 14:26:56.699,,,no_permission,,,,,,,,,,,,342859,,,Tue Sep 10 18:47:28 UTC 2013,,,,,,0|i1n4mn:,343163,,,,,,,,brandon.williams,brandon.williams,,,1.2.7,,,,,,,"09/Aug/13 14:26;brandon.williams;A tuple is the obvious choice, since a bag is cumbersome and there's no need to spill to disk.","09/Aug/13 14:31;jeromatron;In the same vein as data types, as of PIG-2764 Pig has a BigInteger.  So once 0.12 is out and mainstream, we could look at using that instead of the Pig integer type to avoid overflow.  Just as a heads up.","15/Aug/13 02:08;alexliu68;Patch on 1.2 branch is attached, which map ListType and SetType to tuple, MapType to map","15/Aug/13 12:31;jeromatron;We may not want to use the MapType for maps.

In Pig, you can only have text based keys for maps.  From Programming Pig: ""A map in Pig is a chararray to data element mapping, where that element can be any Pig type, including a complex type.""

In Cassandra, you can have map keys of any type, like timestamp in the maps example on http://www.datastax.com/dev/blog/cql3_collections

So we may want to do a bag or tuple of tuples.","15/Aug/13 16:50;alexliu68;The key of Cassandra map has been converted to string into Pig map. We need to decide whether use tuple of tuples vs map for Cassandra map type. Tuple of tuples is more general than map, and map is more specific. HBase uses map to map its row. So which one to use for map? Map vs Tuple of tuples?","16/Aug/13 22:50;alexliu68;To store data to CQL3 table, we supports the following prepared statements

{code}
List
   e.g.
   UPDATE users SET top_places = ?
   UPDATE users SET top_places = [ 'rivendell', 'rohan' ] WHERE user_id = 'frodo';

   UPDATE users SET top_places = ? + top_places
   UPDATE users SET top_places = [ 'the shire' ] + top_places WHERE user_id = 'frodo';

   UPDATE users SET top_places = top_places - ?;
   UPDATE users SET top_places = top_places - ['riddermark'] WHERE user_id = 'frodo';

Set statements are similar to List

Map

  UPDATE users SET todo = ?
  UPDATE users
       SET todo = { '2012-9-24' : 'enter mordor',
                    '2012-10-2 12:00' : 'throw ring into mount doom' }
       WHERE user_id = 'frodo';


The following queries are handled as a regular value instead of tuples
   UPDATE users SET top_places[2] = ?
   UPDATE users SET top_places[2] = 'riddermark' WHERE user_id = 'frodo';

   DELETE top_places[3] FROM users;
   DELETE top_places[3] FROM users WHERE user_id = 'frodo';

   UPDATE users SET todo[?] = ?
   UPDATE users SET todo['2012-10-2 12:10'] = 'die' WHERE user_id = 'frodo';
{code}

The output schema for collections is as following

{code}
 (((name, value), (name, value)), (value ... value), (value...value)
 If a value of tuple (value...value) is a tuple of (inner_value ...inner_value) 
 and the first inner_value is the collection type. 
 it is either ""set"", ""list"" or ""map"".

 e.g. (value ... value) as (value1, value2, (set, riddermark, tom))
 map  (value1, value2, (map, (sfo, 12), (ny, 34))
{code}
",19/Aug/13 20:14;alexliu68;5867-3-1.2-branch.txt is attached to support storing collections to Cassandra,"22/Aug/13 20:13;alex.holmansky;If the key of C* map is converted to string (chararray) in a Pig map on read, how will the keys be handled on write?  Will the strings be auto-converted to appropriate C* types?  In the past, I've seen issues with that - especially with timestamps and UUID values.","22/Aug/13 20:49;alexliu68;The following is the data which will be auto converted to C* type, anything else will be bytes.

{code}
        if (o == null)
            return (ByteBuffer)o;
        if (o instanceof java.lang.String)
            return ByteBuffer.wrap(new DataByteArray((String)o).get());
        if (o instanceof Integer)
            return Int32Type.instance.decompose((Integer)o);
        if (o instanceof Long)
            return LongType.instance.decompose((Long)o);
        if (o instanceof Float)
            return FloatType.instance.decompose((Float)o);
        if (o instanceof Double)
            return DoubleType.instance.decompose((Double)o);
        if (o instanceof UUID)
            return ByteBuffer.wrap(UUIDGen.decompose((UUID) o));

        return ByteBuffer.wrap(((DataByteArray) o).get());
{code}

you need prepare the data for write different from read result.","26/Aug/13 22:16;jbellis;Alex, is this ready for review?","26/Aug/13 23:04;alexliu68;Yes, But I map the C* MapType to Pig map, we may need map it to tuples of tuples.","26/Aug/13 23:05;alexliu68;I also has a few bug fixes related to CqlStorage, so if it's possible, I can add those fix to this patch as well.","26/Aug/13 23:14;brandon.williams;Since pig's maps are limited to string keys, I think we'll need to use tuples. I'm ok with fixing bugs here, provided that's in a separate patch.","27/Aug/13 04:22;alexliu68;First apply 5867-bug-fix-filter-push-down-1.2-branch.txt to fix issue with filter push down. (schema changes), then apply 5867-4-1.2-branch.txt for collection supports in CqlStorage which map SetType/ListType to tuple, MapType to tuple of tuples.

",28/Aug/13 23:56;alexliu68;5867-5-1.2-branch.txt is attached to fix a bug related to writing map collection to C*. It should be applied after 5867-bug-fix-filter-push-down-1.2-branch.txt,"29/Aug/13 00:07;alexliu68;e.g.
{code}
   CREATE TABLE test(
     m text PRIMARY KEY,
     n map<text, text>
   ) 
   data format for insertion (((m,kk)),((map,(m,mm),(n,nn))))
   store recs into 'cql://test/test77?output_query=update+test.test+set+n+%3D+%3F' using CqlStorage();
   where output_query is url encoded
{code}","10/Sep/13 18:47;brandon.williams;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Speculative read performance data show unexpected results,CASSANDRA-5932,12665353,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,enigmacurry,enigmacurry,24/Aug/13 00:51,12/Mar/19 14:17,13/Mar/19 22:29,26/Sep/13 20:55,2.0.2,,,,,,1,,,,,"I've done a series of stress tests with eager retries enabled that show undesirable behavior. I'm grouping these behaviours into one ticket as they are most likely related.

1) Killing off a node in a 4 node cluster actually increases performance.
2) Compactions make nodes slow, even after the compaction is done.
3) Eager Reads tend to lessen the *immediate* performance impact of a node going down, but not consistently.

My Environment:
1 stress machine: node0
4 C* nodes: node4, node5, node6, node7

My script:
node0 writes some data: stress -d node4 -F 30000000 -n 30000000 -i 5 -l 2 -K 20
node0 reads some data: stress -d node4 -n 30000000 -o read -i 5 -K 20

h3. Examples:

h5. A node going down increases performance:

!node-down-increase-performance.png!

[Data for this test here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.eager_retry.node_killed.just_20.json&metric=interval_op_rate&operation=stress-read&smoothing=1]

At 450s, I kill -9 one of the nodes. There is a brief decrease in performance as the snitch adapts, but then it recovers... to even higher performance than before.

h5. Compactions make nodes permanently slow:

!compaction-makes-slow.png!
!compaction-makes-slow-stats.png!

The green and orange lines represent trials with eager retry enabled, they never recover their op-rate from before the compaction as the red and blue lines do.

[Data for this test here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.eager_retry.compaction.2.json&metric=interval_op_rate&operation=stress-read&smoothing=1]

h5. Speculative Read tends to lessen the *immediate* impact:

!eager-read-looks-promising.png!
!eager-read-looks-promising-stats.png!

This graph looked the most promising to me, the two trials with eager retry, the green and orange line, at 450s showed the smallest dip in performance. 

[Data for this test here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.eager_retry.node_killed.json&metric=interval_op_rate&operation=stress-read&smoothing=1]

h5. But not always:

!eager-read-not-consistent.png!
!eager-read-not-consistent-stats.png!

This is a retrial with the same settings as above, yet the 95percentile eager retry (red line) did poorly this time at 450s.

[Data for this test here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.eager_retry.node_killed.just_20.rc1.try2.json&metric=interval_op_rate&operation=stress-read&smoothing=1]",,,,,,,,,,,,,,,,,,,,,,,,29/Sep/13 21:27;enigmacurry;5932-6692c50412ef7d.png;https://issues.apache.org/jira/secure/attachment/12605828/5932-6692c50412ef7d.png,01/Oct/13 15:14;enigmacurry;5932.6692c50412ef7d.compaction.png;https://issues.apache.org/jira/secure/attachment/12606117/5932.6692c50412ef7d.compaction.png,01/Oct/13 20:15;enigmacurry;5932.6692c50412ef7d.rr0.png;https://issues.apache.org/jira/secure/attachment/12606179/5932.6692c50412ef7d.rr0.png,02/Oct/13 15:49;enigmacurry;5932.6692c50412ef7d.rr1.png;https://issues.apache.org/jira/secure/attachment/12606380/5932.6692c50412ef7d.rr1.png,29/Sep/13 16:16;enigmacurry;5932.ded39c7e1c2fa.logs.tar.gz;https://issues.apache.org/jira/secure/attachment/12605805/5932.ded39c7e1c2fa.logs.tar.gz,25/Sep/13 01:36;iamaleksey;5932.txt;https://issues.apache.org/jira/secure/attachment/12604942/5932.txt,27/Sep/13 14:55;enigmacurry;5933-128_and_200rc1.png;https://issues.apache.org/jira/secure/attachment/12605460/5933-128_and_200rc1.png,27/Sep/13 14:55;enigmacurry;5933-7a87fc11.png;https://issues.apache.org/jira/secure/attachment/12605459/5933-7a87fc11.png,27/Sep/13 14:55;enigmacurry;5933-logs.tar.gz;https://issues.apache.org/jira/secure/attachment/12605461/5933-logs.tar.gz,27/Sep/13 18:42;enigmacurry;5933-randomized-dsnitch-replica.2.png;https://issues.apache.org/jira/secure/attachment/12605518/5933-randomized-dsnitch-replica.2.png,27/Sep/13 21:13;enigmacurry;5933-randomized-dsnitch-replica.3.png;https://issues.apache.org/jira/secure/attachment/12605563/5933-randomized-dsnitch-replica.3.png,27/Sep/13 17:14;enigmacurry;5933-randomized-dsnitch-replica.png;https://issues.apache.org/jira/secure/attachment/12605480/5933-randomized-dsnitch-replica.png,24/Aug/13 01:04;enigmacurry;compaction-makes-slow-stats.png;https://issues.apache.org/jira/secure/attachment/12599771/compaction-makes-slow-stats.png,24/Aug/13 00:51;enigmacurry;compaction-makes-slow.png;https://issues.apache.org/jira/secure/attachment/12599762/compaction-makes-slow.png,24/Aug/13 01:04;enigmacurry;eager-read-looks-promising-stats.png;https://issues.apache.org/jira/secure/attachment/12599770/eager-read-looks-promising-stats.png,24/Aug/13 00:51;enigmacurry;eager-read-looks-promising.png;https://issues.apache.org/jira/secure/attachment/12599761/eager-read-looks-promising.png,24/Aug/13 01:04;enigmacurry;eager-read-not-consistent-stats.png;https://issues.apache.org/jira/secure/attachment/12599769/eager-read-not-consistent-stats.png,24/Aug/13 00:51;enigmacurry;eager-read-not-consistent.png;https://issues.apache.org/jira/secure/attachment/12599760/eager-read-not-consistent.png,24/Aug/13 00:51;enigmacurry;node-down-increase-performance.png;https://issues.apache.org/jira/secure/attachment/12599763/node-down-increase-performance.png,19.0,,,,,,,,,,,,,,,,,,,2013-09-16 16:03:55.944,,,no_permission,,,,,,,,,,,,345293,,,Mon Oct 07 23:01:59 UTC 2013,,,,,,0|i1njlz:,345594,2.0 rc1,2.0 rc2,,,,,,jbellis,jbellis,,,,,,,,,,"16/Sep/13 16:03;hubez;[~enigmacurry], what was your collection interval for the metrics you've collected?

I ask because I'm observing results similar to yours for ""3) Eager Reads tend to lessen the immediate performance impact of a node going down, but not consistently."". However, I've polled metrics @ a 1 second granularity to see that it's actually a multi-second stress-client outage - not just poor and inconsistent performance.

Polling metrics @ a 1 second interval, has observations that a ~20 second read operations starvation outage occurs for the stress client for all data in the cluster (even with the lowest phi_convict_threshold=6).

Analysis so far indicates that high-operations reads starve out all the C* client threads/connections, because they get stuck on awaiting for a server response whenever the key-space hits the node that is down (and by probability + high-operation reads, within 1 second each stress client thread will all hit the downed-node's key-space).



So I'm confirming that I'm also seeing this bug that Speculative reads (even with an ALWAYS setting). It isn't solving this outage for clients during high-operation reads, and based on what I understand of the feature, it should.

Thanks, guys!

","16/Sep/13 17:41;enigmacurry;I'm using 5 second intervals in these charts. 'multi-second stress-client outage' is a good way to put it, for both the case of speculative retry and not, the drop in performance after a node goes down is a duration of complete non-responsiveness (not degraded performance.) The addition of speculative retry consistently shortens this duration (it's always better), but this duration itself is inconsistent. ",17/Sep/13 23:39;kohlisankalp;SpeculateAlwaysExecutor - Here we are not reading from more endpoints than normal. We are only reading data from two endpoints. We should be reading from one more endpoint if possible. ,"18/Sep/13 19:28;hubez;Thanks, [~enigmacurry]. Sounds like you are seeing the same thing as us, so it's great to see it's getting attention!","24/Sep/13 21:53;lizou;Hello [~iamaleksey],

Thanks for the link to this jira and for your very detailed testing results. It confirms what we have seen in our lab testing for the Cassandra 2.0.0-rc2 ""Speculative Execution for Reads"".

We have a very simple data center setup consisting of four Cassandra nodes running on four server machines. A testing application (Cassandra client) is interacting with Cassandra nodes 1, 2 and 3. That is, the testing app does not directly connected to the Cassandra node 4.

The keyspace Replication Factor is set to 3 and the client requested Consistency Level is set to CL_TWO.

I have tested all of three configurations of the Speculative Execution for Reads ('ALWAYS', '85 PERCENTILE', '50 MS' / '100 MS'). It seems that none of them works as expected. From the test app log file point of view, they all give a 20-second window of outage immediately after the 4th node was killed. This behavior is consistent to Cassandra 1.2.4.

I have done a quick code reading of the Cassandra Server implementation (Cassandra 2.0.0 tarball) and I have noticed some design issues. I would like to discuss them with you.

*Issue 1* - StorageProxy.fetchRows() may still block for as long as conf.read_request_timeout_in_ms, though the speculative retry did fire correctly after the Cassandra node 4 was killed.

Take the speculative configuration of 'PERCENTILE' / 'CUSTOM' as example, after the Cassandra node 4 was killed, SpeculativeReadExecutor.speculate() would block for responses. If timed out, it would send out one more read request to an alternative node (from {{unfiltered}}) and increment the speculativeRetry counter. This part should work.

However, killing the 4th node would very likely cause inconsistency in the database and this will trigger the DigestMismatchException. In the fetchRows(), when handling DigestMismatchException, it uses handler.endpoints to send out digest mismatch retries and then block for responses. As we know that one of the endpoints was already killed, the handler.get() will block until it is timed out, which is 10 seconds.


{noformat}
                catch (DigestMismatchException ex)
                {
                    Tracing.trace(""Digest mismatch: {}"", ex);

                    ...

                    MessageOut<ReadCommand> message = exec.command.createMessage();
                    for (InetAddress endpoint : exec.handler.endpoints)
                    {
                        Tracing.trace(""Enqueuing full data read to {}"", endpoint);
                        MessagingService.instance().sendRR(message, endpoint, repairHandler);
                    }
                }
            }

            ...

            // read the results for the digest mismatch retries
            if (repairResponseHandlers != null)
            {
                for (int i = 0; i < repairCommands.size(); i++)
                {
                    ReadCommand command = repairCommands.get(i);
                    ReadCallback<ReadResponse, Row> handler = repairResponseHandlers.get(i);

                    Row row;
                    try
                    {
                        row = handler.get();
                    }
{noformat}



*Issue 2* - The speculative 'ALWAYS' does NOT send out any more read requests. Thus, in face of the failure of node 4, it will not help at all.

The SpeculateAlwaysExecutor.executeAsync() only sends out handler.endpoints.size() number of read requests and it blocks for the responses to come back. If one of the nodes is killed, say node 4, this speculative retry 'ALWAYS' will work the same way as Cassandra 1.2.4, i.e. it will block until timed out, which is 10 seconds.

??My understanding of this speculative retry 'ALWAYS' should ALWAYS send out ""handler.endpoints.size() + 1"" number of read requests and block for handler.endpoints.size() number of responses??.

*Issue 3* - Since the ReadRepairDecison is determined by a Random() number, this speculative retry may not work as the ReadRepairDecision may be ??ReadRepairDecision.GLOBAL??

*Issue 4* - For the ReadExecutor(s), the {{this.unfiltered}} and {{this.endpoints}} may not consistent. Thus, using {{this.unfiltered}} and {{this.endpoints}} for speculative retry may cause unexpected results. This is especially true when the Consistency Level is {{LOCAL_QUARUM}} and the ReadRepairDecision is {{DC_LOCAL}}.



","24/Sep/13 21:58;iamaleksey;Hey [~lizou]. Yeah, I've fixed most of these already (rewritten most of the ARE code, actually). Specifically issues 2,3,4. Will look into 1 too.

Thanks.","25/Sep/13 01:51;iamaleksey;Attaching 5932.txt that will hopefully fix this ([~enigmacurry] could you run the tests again, please, with the patch applied?)

1. As noted by [~lizou] and [~kohlisankalp], ALWAYS wasn't making an extra request, it was making an extra data request at the expense of one digest request. Fixed.

2. SpecRetry wasn't working correctly with RRD.DC_LOCAL, as noted by @lizou, because the two lists will be in different order, and a retry might be sent to a node that already had a request sent to it. (Please note that LOCAL_QUORUM here does not affect anything - CL.filterForQuery() sorts in place, so the two lists would be in the same order, everything was working correct). RRD.DC_LOCAL handling was a legit issue though. Fixed.

3. SpecRetry w/ RRD.GLOBAL is a noop, you can't speculate if you contact all the replicas in the first place. This is normal.

4. The DME issue is semi-legit. Killing a node shouldn't trigger DME or increase the likelihood of DME happening. *HOWEVER* when shooting requests for repair, we were not considering the case where one of the replies satisfying the original CL came from a SpecRetry attempt. The patch includes the extra replica in repair commands if SpecRetry had been triggered by the original request.


5. SP.getRangeSlice() is not SpectRetry-aware as of now. I don't know if this is an omission or by design, but for now, please don't include that in the benchmarks, since it would only be misleading. ","25/Sep/13 16:07;jbellis;Pushed some OCD of my own to https://github.com/jbellis/cassandra/commits/5932 on top of this.

bq. ALWAYS wasn't making an extra request, it was making an extra data request at the expense of one digest request

I'm not sure what the distinction is here.  Do you mean that if we weren't read-repairing, there would be no extra data request at all?

bq. SpecRetry w/ RRD.GLOBAL is a noop, you can't speculate if you contact all the replicas in the first place.

I dunno, I think we should turn a digest into a data for redundancy the way ALWAYS used to.","25/Sep/13 17:02;iamaleksey;Pushed even more OCD to https://github.com/iamaleksey/cassandra/commits/5932 on top of yours.

bq. I'm not sure what the distinction is here. Do you mean that if we weren't read-repairing, there would be no extra data request at all?

Instead of making, for example 1 data request + 2 digest requests, ALWAYS was making 2 data requests + 1 digest request, instead of making 2 data requests + 2 digest requests, not really helping to satisfy the CL in case of node's failure.

bq. I dunno, I think we should turn a digest into a data for redundancy the way ALWAYS used to.

Maybe.","25/Sep/13 23:03;iamaleksey;Force-pushed the 'final' version to https://github.com/iamaleksey/cassandra/commits/5932.

Among other things, properly handles RRD.GLOBAL and RRD.DC_LOCAL in 1-DC scenario.","26/Sep/13 17:54;jbellis;Pushed one more set of changes to mine, not forced: https://github.com/jbellis/cassandra/commits/5932.  Goal is to make SRE less fragile when doing RR.","26/Sep/13 20:30;iamaleksey;+1, I'm out of OCD juice.","26/Sep/13 20:43;lizou;Hello [~iamaleksey] and [~jbellis],

I took a quick look at the code changes. The new code looks very good to me. But I saw one potential issue in {{AlwaysSpeculatingReadExecutor.executeAsync()}}, in which it makes at least *two* data / digest requests. This will cause problems for a data center with only one Cassandra server node (e.g. bring up an embedded Cassandra node in JVM for JUnit test) or a deployed production data center of two Cassandra server nodes with one node shut down for maintenance. In the above mentioned two cases, {{AbstractReadExecutor.getReadExecutor()}} will return the {{AlwaysSpeculatingReadExecutor}} as condition {{(targetReplicas.size() == allReplicas.size())}} is met, though the tables may / may not be configured with ??Speculative ALWAYS??.

It is true for our legacy products we are considering to deploy each data center with only two Cassandra server nodes with RF = 2 and CL = 1.
","26/Sep/13 20:55;jbellis;The logic looks like this:

# Figure out how many replicas we need to contact to satisfy the desired consistencyLevel + Read Repair settings
# If that ends up being all the replicas, then use ASRE to get some redundancy on the data reads.  This will allow the read to succeed even if a digest for RR times out.  Of course if you are reading at CL.ALL and a replica times out there's nothing we can do.
# Otherwise, use SRE and make an ""extra"" request later, if it looks like one of the minimal set isn't going to respond in time

Note that performing extra data requests does not affect handler.blockfor -- just makes it possible for the request to proceed if it gets enough responses back, no matter which replicas they come from.","26/Sep/13 20:55;jbellis;(Committed after Aleksey's +1, incidentally.)","26/Sep/13 21:04;lizou;The logic for {{AlwaysSpeculatingReadExecutor}} is good. What I meant in my previous comment is that when {{targetReplicas.size() == allReplicas.size()}} and {{targetReplicas.size() == 1}}, then {{AlwaysSpeculatingReadExecutor.executeAsync()}} will throw an exception as there is only one endpoint in {{targetReplicas}}, but it tries to access two endpoints in {{targetReplicas}}.",26/Sep/13 21:10;jbellis;I see what you mean.  Fixed in 7a87fc1186f39678382cf9b3e1dd224d9c71aead.,"27/Sep/13 14:54;enigmacurry;The good news is that speculative read has improved across the board.

However, this new batch of testing introduces some new mysteries.

Here is all of the runs from 7a87fc1186f39678382cf9b3e1dd224d9c71aead:

!5933-7a87fc11.png!

All of the speculative retry runs are better than with 2.0.0-rc1. However, I can't explain why sr=NONE did better than ALWAYS and 95percentile. There is no visible indication that a node went down for sr=NONE. I have double checked the logs, and it did, in fact, go down. 

Compare this to the baseline of 1.2.8 and 2.0.0-rc1 (redone last night on same hardware as above):

!5933-128_and_200rc1.png!

All of these have clear indications of the node going down.

You can [see all the data here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.5933.node_killed.json&metric=interval_op_rate&operation=stress-read&smoothing=1] - you can double click the colored squares to toggle the visibility of the lines, as they do overlap.

I've uploaded logs from all these runs as 5933-logs.tar.gz.",27/Sep/13 15:00;jjordan;Yeah. The graphs for ALWAYS and NONE look swapped from what I would expect.,"27/Sep/13 15:02;enigmacurry;The other thing that I note, is that all of these runs are better than 1.2.8 :D (further evidence that CASSANDRA-5933 may be invalid)","27/Sep/13 15:07;jbellis;Hmm.

I wonder if it's just luck of the draw as to which replica dsnitch is preferring.  Here's a branch to randomize that, per-operation:

https://github.com/jbellis/cassandra/commits/5932-randomized",27/Sep/13 16:28;brandon.williams;Couldn't we just do a run with the dsnitch disabled?,27/Sep/13 16:31;jbellis;That still gives you luck-of-the-draw as to which replica it prefers.  (Unlikely to be evenly distributed.),"27/Sep/13 17:14;enigmacurry;This looks exactly like what I was expecting:

!5933-randomized-dsnitch-replica.png!

[data here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.5933.randomized-dsnitch.node_killed.json&metric=interval_op_rate&operation=stress-read&smoothing=1]","27/Sep/13 17:29;jbellis;That's awesome.

Can you test 90th and 75th percentile too?","27/Sep/13 18:44;enigmacurry;Seems like it's quite tunable, but not a lot of difference under 90%:

!5933-randomized-dsnitch-replica.2.png!

[data here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.5933.randomized-dsnitch.node_killed.json&metric=interval_op_rate&operation=stress-read&smoothing=1]","27/Sep/13 19:43;jbellis;Yeah, that makes sense.  70th..95th are all pretty damn close to median still.

What I'd like to do is get close to the 10ms performance hit (~none) as a percentile, and make that default in 2.1.  Try 99th and 99.9th?","27/Sep/13 19:54;jbellis;Also, did that stress patch work to get you failed request counts?  Would be good to get that too if we can show that even 10ms keeps requests from failing entirely.","27/Sep/13 21:14;enigmacurry;Here's 99th and 99.9th percentiles:

!5933-randomized-dsnitch-replica.3.png!

I'll look at that stress patch again, I seem to recall it not making a lot of sense to me when I last tried it, but will give it another go.",27/Sep/13 21:37;jbellis;Starting to think we still have a bug.  99.9 should be doing less retries than 10ms but the graph shows it doing more.,"27/Sep/13 21:47;lizou;I did some more code reading and noticed some potential issues and possible improvement. I've got to run now. I will get back to you guys Monday morning.

My guess is that the ??Speculative NONE?? is hit by the initial request reading path which is successfully resolved by the *Speculative Retry*. The observed throughput performance hit when Speculative Retry is enabled is caused by the ReadRepair path which has some coding / design issues. I will talk to you next Monday.","27/Sep/13 21:49;jbellis;Maybe the problem is that we're using CF-level latency instead of StorageProxy.

What does cfhistograms give for read latency?","27/Sep/13 22:40;jbellis;Pretty sure that's our smoking gun.  Pushed a commit to the -randomized branch that adds coordinator-level, per-cf latency tracking and uses that instead.

Can you repeat the last test with that?  (Maybe throw in ALL as well if you're feeling optimistic that we'll have a measurable difference between ALL and 90%. :)","29/Sep/13 16:15;enigmacurry;[~jbellis] Here's your two runs:

[dea27f84f40|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.5933.dea27f84f40.node_killed.json&metric=interval_op_rate&operation=stress-read&smoothing=1]
[ded39c7e1c2fa|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.5933.ded39c7e1c2fa.node_killed.json&metric=interval_op_rate&operation=stress-read&smoothing=1]

Logs for the second run are attached as 5932.ded39c7e1c2fa.logs.tar.gz","29/Sep/13 17:48;jbellis;The code to convert 99 into 0.99 was buggy and was actually converting to 0.0099.  Fix pushed, can you try it again?","29/Sep/13 21:26;enigmacurry;BINGO!

!5932-6692c50412ef7d.png!

[data here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.5933.6692c50412ef7d.node_killed.json&metric=interval_op_rate&operation=stress-read&smoothing=1]","30/Sep/13 01:25;hubez;Definitely the best results seen so far! Nice work!

In my mind, during the transition period right after the killing of the node, I expected ""ALWAYS"" to have negligible impact, or at least the smallest impact of all of the other values. However, red (90%), and purple (75%) is having a smaller impact. Seems fishy. Do I misunderstand the intention of the ""ALWAYS"" setting?

(Edited to clarify the period I'm talking about.)","30/Sep/13 03:10;jbellis;It looks to me like 75/90/Always are about the same, with Always dropping from a lower baseline.  Which makes sense; it's still doing a lot of unnecessary work compared to the others.","30/Sep/13 13:49;jbellis;[~enigmacurry], can you also re-test the uncapped compaction scenario with the same set of retry settings?","30/Sep/13 17:24;lizou;Hello [~iamaleksey] and [~jbellis],

It appears to me that the testing results have suggested that the ""_data read + speculative retry_"" path work as expected. This ""_data read + speculative retry_"" path has greatly minimized the throughput impact caused by the failure of one of Cassandra server nodes.

The observed small degradation of throughput performance when _speculative retry_ is enabled is very likely to be caused by the ""*_read repair_*"" path. I did the code reading of this path last Friday and noticed some design / coding issues. I would like to discuss them with you.

Please note that my code base is still the Cassandra 2.0.0 tarball, not updated with the latest code changes.

*Issue 1* -- When handling {{DigestMismatchException}} in {{StorageProxy.fetchRows()}}, all _data read requests_ are sent out using {{sendRR}} without distinguishing remote nodes from the local node.

Will this cause an issue, as {{MessagingService.instance().sendRR()}} will send out enqueued messages for a specified remote node via its pre-established TCP socket connection. For local node, this should be done via {{LocalReadRunnable}}, i.e. {{StageManager.getStage(Stage.READ).execute(new LocalReadRunnable(command, handler))}}.

If this may cause an issue, the following wait may block.

{noformat}
            // read the results for the digest mismatch retries
            if (repairResponseHandlers != null)
            {
                for (int i = 0; i < repairCommands.size(); i++)
                {
                    ReadCommand command = repairCommands.get(i);
                    ReadCallback<ReadResponse, Row> handler = repairResponseHandlers.get(i);

                    Row row;
                    try
                    {
                        row = handler.get();
                    }
{noformat}

For two reasons.
* The data read request for local node may never sent out
* As one of the nodes is down (which triggered the Speculative Retry) will cause one missing response.

*If missing two responses, this will block for 10 seconds*. 

*Issue 2* -- For _data repair_, {{RowDataResolver.resolve()}} has a similar issue as it calls  {{scheduleRepairs()}} to send out  messages using sendRR() without distinguishing remote nodes from the local node.

*Issue 3* -- When handling _data repair_, {{StorageProxy.fetchRows()}} blocks waiting for acks to all of {{data repair}} requests sent out using sendRR(). This may cause the thread to block.

For _data repair_ path, *data requests* are sent out and then compare / merge the received responses; send out the merged / diff version and then block for acks.

How do we handle the case for _local node_? Does the sendRR() and the corresponding receive part can handle the case for local node? If not, then this may block for 10 seconds.

{noformat}
            if (repairResponseHandlers != null)
            {
                for (int i = 0; i < repairCommands.size(); i++)
                {
                    ReadCommand command = repairCommands.get(i);
                    ReadCallback<ReadResponse, Row> handler = repairResponseHandlers.get(i);

                    Row row;
                    try
                    {
                        row = handler.get();
                    }
                    catch (DigestMismatchException e)
                    ...
                    RowDataResolver resolver = (RowDataResolver)handler.resolver;
                    try
                    {
                        // wait for the repair writes to be acknowledged, to minimize impact on any replica that's
                        // behind on writes in case the out-of-sync row is read multiple times in quick succession
                        FBUtilities.waitOnFutures(resolver.repairResults, DatabaseDescriptor.getWriteRpcTimeout());
                    }
                    catch (TimeoutException e)
                    {
                        Tracing.trace(""Timed out on digest mismatch retries"");
                        int blockFor = consistency_level.blockFor(Keyspace.open(command.getKeyspace()));
                        throw new ReadTimeoutException(consistency_level, blockFor, blockFor, true);
                    }
{noformat}

*Question for waiting for the ack* -- Do we really need to wait for the ack?

We should assume the best effort approach, i.e. do the data repair and then return. No need to block waiting for the acks for confirmation.

*Question for the Randomized approach* -- Since the end points are randomized, the first node in the list is no likely the local node. This may cause a higher possibility of data repair.

In the *Randomized Approach*, the end points are reshuffled. Then, the first node in the list used for _data read request_ is not likely the local node. If this node happens to be the *DOWN* node, then, we end with all digest responses without the data, which will block and eventually timed out.

","30/Sep/13 20:46;iamaleksey;First, let me thank you for your continued digging. Some of it helped. That said, you should probably look at the current cassandra-2.0 branch, and not the 2.0.0 tarball/branches here in the comments.

bq. Issue 1 – When handling DigestMismatchException in StorageProxy.fetchRows(), all data read requests are sent out using sendRR without distinguishing remote nodes from the local node.

This is not an issue, and it's not spec retry related. Using LRR for local read requests is merely an optimisation - there is nothing wrong with sendRR (not that it isn't worth optimising here - just noting that it's not an issue). This is also the answer to ""How do we handle the case for local node? Does the sendRR() and the corresponding receive part can handle the case for local node? If not, then this may block for 10 seconds."" Same goes for Issue 2 and Issue 3.

bq. The data read request for local node may never sent out. As one of the nodes is down (which triggered the Speculative Retry) will cause one missing response.

The former is not true, the latter won't, since the current cassandra-2.0 code will send requests to all the contacted replicas. So if a node triggered spec retry, that extra speculated replica will get the request as well, and we can still satisfy the CL.

{noformat}
                    for (InetAddress endpoint : exec.getContactedReplicas())
                    {
                        Tracing.trace(""Enqueuing full data read to {}"", endpoint);
                        MessagingService.instance().sendRR(message, endpoint, repairHandler);
                    }
{noformat}


bq. Question for the Randomized approach – Since the end points are randomized, the first node in the list is no likely the local node. This may cause a higher possibility of data repair.

I don't see how the possibility of data repair is correlated with the locality of a target node, but, it doesn't matter. The 'randomised approach' was an experiment, it wasn't committed as part of the fix. See the latest cassandra-2.0 branch code.

bq. In the Randomized Approach, the end points are reshuffled. Then, the first node in the list used for data read request is not likely the local node. If this node happens to be the DOWN node, then, we end with all digest responses without the data, which will block and eventually timed out.

See the above reply.

TLDR: None of these seem to be issues, but we could optimise RR to use LRR for local reads to get slightly better performance for local requests (and to be consistent with the regular reads code).","30/Sep/13 21:32;lizou;Thanks for the clarification of the sendRR issue.

Since the Randomized approach is not checked in, let us skip over it.

For the _data repair_, do we need to block waiting for the acks?","30/Sep/13 21:53;iamaleksey;bq. For the data repair, do we need to block waiting for the acks?

The reasons are listed in the comments, as you've seen:
{noformat}
// wait for the repair writes to be acknowledged, to minimize impact on any replica that's
// behind on writes in case the out-of-sync row is read multiple times in quick succession
{noformat}

To reach that goal - yes, it's necessary. Is that scenario worth optimizing for or should we reconsider? Dunno. We are only writing to the replicas that we got the result from, though, so a known down replica wouldn't affect it.

",30/Sep/13 22:00;iamaleksey;[~lizou] see CASSANDRA-4792 (TLDR: yes),"01/Oct/13 15:15;enigmacurry;I had to double the test length to get a good compaction graph. I'm not sure why it took so long, it didn't take as long in the original test.

!5932.6692c50412ef7d.compaction.png!

[data here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.5933.6692c50412ef7d.compaction.2.json&metric=interval_op_rate&operation=stress-read&smoothing=4]

([~iamaleksey] your read_repair 0 / 1 tests are in progress...)","01/Oct/13 20:15;enigmacurry;Node killed while read_repair_chance=0. I accidentally left the test run to be 60M rows, so I chopped off the uninteresting bit.

!5932.6692c50412ef7d.rr0.png!

[data here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.5933.6692c50412ef7d.node_killed.rr0.json&metric=interval_op_rate&operation=stress-read&smoothing=1]

(rr=1 is next..)","02/Oct/13 03:39;jbellis;The throughput is a wash in the compaction scenario, but the 99.9% latency looks a lot better with the retries.

Any theories on why the percentile settings are posting better latency numbers than ALWAYS though?","02/Oct/13 15:46;lizou;This testing result is reasonable and what is expected.

For PERCENTILE / CUSTOM configuration, the larger the {{cfs.sampleLatencyNanos}} the smaller the throughput impact for normal operations before the outage. However, during the outage period, the situation is reversed, i.e. the smaller {{cfs.sampleLatencyNanos}}, the smaller the throughput impact will be, as it times out quicker and triggers the speculative retries.

For the ALWAYS configuration, as it always sends out one speculative in addition to the usual read requests, the throughput performance should be lower than those of PERCENTILE / CUSTOM for normal operations before the outage. Since it always sends out the speculative retries, the throughput impact during the outage period should be the smallest. The testing result indicates that this is true.
","02/Oct/13 15:50;enigmacurry;With read_repair_chance = 1

!5932.6692c50412ef7d.rr1.png!

[data here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.5933.6692c50412ef7d.node_killed.rr1.json&metric=interval_op_rate&operation=stress-read&smoothing=1]","04/Oct/13 20:34;lizou;Have done some testing using today's trunk. Have observed following issues.

*Issue 1* -- The first method {{MessagingService.addCallback()}} (i.e. without the ConsistencyLevel argument) asserts.

Commenting out the assert statement seems to work. But the Cassandra servers themselves will produce 10-second outage (i.e. zero transactions from the client point of view) periodically.

*Issue 2* -- The Speculative Retry seems stop retrying during the outage window.

During the outage window triggered either by killing one of Cassandra nodes or produced by Cassandra servers themselves, the JConsole shows that the JMX stats, SpeculativeRetry counter stops incrementing until the gossip figures out the outage issue.

What is the reason for this? The Speculative Retry is meant to help during the outage period. This observed behavior is consistent with Cassandra 2.0.0-rc2.
","04/Oct/13 20:47;jbellis;bq. MessagingService.addCallback

Are you sure you have the latest code?  The only invocations of addCallback in 2.0/trunk include the consistencylevel argument as of late last night.","04/Oct/13 20:58;lizou;The trunk load I used for testing was pulled this noon. It has two addCallback() methods. One of them (i.e. without the ConsistencyLevel) asserts. 

I checked the MessagingService.java, there are two addCallback() methods.
* The one without ConsistencyLevel is called by sendRR()
* The one with ConsistencyLevel is called by sendMessageToNonLocalDC()
","04/Oct/13 21:06;lizou;As for yesterday's trunk load, there were two addCallback() methods. But the one with ConsistencyLevel was not called by anyone. The one without ConsistencyLevel asserts.","04/Oct/13 21:27;jbellis;Are you doing counter updates?  That's the only use of sendRR for updates I see.

Can you post the stack trace of the assertion error you're getting?",04/Oct/13 21:37;jbellis;(Pushed fix for mutateCounter in 3da10f469d6a328bad209d723a5997c932284344.),"07/Oct/13 18:56;lizou;[~jbellis], this morning's trunk load has a slightly different symptom, and is even more serious than last Friday's load, as this time just commenting out the assert statement in the {{MessagingService.addCallback()}} will not help.

I copy the {{/var/log/cassandra/system.log}} exception errors below.

{noformat}
ERROR [Thrift:12] 2013-10-07 14:42:39,396 Caller+0       at org.apache.cassandra.service.CassandraDaemon$2.uncaughtException(CassandraDaemon.java:134)
 - Exception in thread Thread[Thrift:12,5,main]
java.lang.AssertionError: null
        at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:543) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:591) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:571) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.service.StorageProxy.sendToHintedEndpoints(StorageProxy.java:869) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.service.StorageProxy$2.apply(StorageProxy.java:123) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:739) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:511) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.service.StorageProxy.mutateWithTriggers(StorageProxy.java:581) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithoutCondition(ModificationStatement.java:379) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:363) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:126) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:267) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.thrift.CassandraServer.execute_prepared_cql3_query(CassandraServer.java:2061) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_prepared_cql3_query.getResult(Cassandra.java:4502) ~[apache-cassandra-thrift-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_prepared_cql3_query.getResult(Cassandra.java:4486) ~[apache-cassandra-thrift-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.1.jar:0.9.1]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.1.jar:0.9.1]
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:194) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_25]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_25]
        at java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_25]

{noformat}

",07/Oct/13 19:35;jbellis;There's a ticket open for trunk over at CASSANDRA-6154.,"07/Oct/13 21:40;lizou;Hello,

As this ticket is already fixed in 2.0.2, where can I get the 2.0.2 source code?

Currently, my ""git tag"" only shows up to 2.0.1.
",07/Oct/13 21:48;jbellis;the cassandra-2.0 branch is what will become 2.0.2,"07/Oct/13 22:34;lizou;I even cannot see the cassandra-2.0 branch.
My ""git tag"" gives a list including following branches.

{noformat}
$ git tag
1.2.8
1.2.8-tentative
cassandra-0.3.0-final
cassandra-0.3.0-rc1
cassandra-0.3.0-rc2
...
cassandra-1.2.4
cassandra-1.2.5
cassandra-1.2.6
cassandra-1.2.7
cassandra-1.2.8
cassandra-1.2.9
cassandra-2.0.0
cassandra-2.0.0-beta1
cassandra-2.0.0-beta2
cassandra-2.0.0-rc1
cassandra-2.0.0-rc2
cassandra-2.0.1
drivers
list
{noformat}

There is no cassandra-2.0 branch. Where can I find it?
",07/Oct/13 23:01;jbellis;Under {{git branch}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commit Logs referencing deleted CFIDs not handled properly,CASSANDRA-5946,12665804,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,rbranson,rbranson,27/Aug/13 22:04,12/Mar/19 14:17,13/Mar/19 22:29,15/Oct/13 22:43,1.2.11,2.0.2,,,,,0,,,,,"ERROR 19:44:38,377 Exception in thread Thread[COMMIT-LOG-WRITER,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.db.commitlog.CommitLogAllocator.flushOldestTables(CommitLogAllocator.java:299)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator.fetchSegment(CommitLogAllocator.java:135)
        at org.apache.cassandra.db.commitlog.CommitLog.activateNextSegment(CommitLog.java:333)
        at org.apache.cassandra.db.commitlog.CommitLog.access$100(CommitLog.java:44)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:377)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:46)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.lang.Thread.run(Thread.java:679)

Working up a test case / patch for this.",,,,,,,,,,,,,,,,,,,,,,,,14/Oct/13 09:15;jbellis;5946-v2.txt;https://issues.apache.org/jira/secure/attachment/12608257/5946-v2.txt,09/Oct/13 14:58;jbellis;5946.txt;https://issues.apache.org/jira/secure/attachment/12607569/5946.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-10-02 17:28:31.436,,,no_permission,,,,,,,,,,,,345743,,,Tue Oct 15 22:43:44 UTC 2013,,,,,,0|i1nmdr:,346044,1.2.8,,,,,,,thobbs,thobbs,,,,,,,,,,02/Oct/13 17:28;jbellis;Still working on this [~rbranson]?,"09/Oct/13 14:57;jbellis;I don't see a clean way to fix the drop/append race, so here's my proposed workaround.",14/Oct/13 09:15;jbellis;v2 adds a debug line,"14/Oct/13 09:16;jbellis;(Rick emailed that he won't be available to review for a while, so switching to Tyler.)",15/Oct/13 16:01;thobbs;+1,15/Oct/13 22:43;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexSummary load fails when empty key exists in summary,CASSANDRA-5965,12666441,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,yukim,yukim,31/Aug/13 02:20,12/Mar/19 14:17,13/Mar/19 22:29,03/Sep/13 21:18,1.2.10,,,,,,0,,,,,"IndexSummary load fails with the following error when empty key is added to summary:

{code}
ERROR [SSTableBatchOpen:1] 2013-08-30 20:17:41,210 CassandraDaemon.java (line 192) Exception in thread Thread[SSTableBatchOpen:1,5,main]
java.lang.AssertionError
        at org.apache.cassandra.utils.ByteBufferUtil.readBytes(ByteBufferUtil.java:401)
        at org.apache.cassandra.io.sstable.IndexSummary$IndexSummarySerializer.deserialize(IndexSummary.java:124)
        at org.apache.cassandra.io.sstable.SSTableReader.loadSummary(SSTableReader.java:491)
        at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:388)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:198)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:157)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:262)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
{code}

I think this is typically caused by indexing empty column, and then the key in index columnfamily is added to its IndexSummary.",,,,,,,,,,,,,,,,,,,,,,,,31/Aug/13 02:20;yukim;0001-Add-IndexSummary-test.patch;https://issues.apache.org/jira/secure/attachment/12600913/0001-Add-IndexSummary-test.patch,31/Aug/13 02:49;yukim;5965-1.2.txt;https://issues.apache.org/jira/secure/attachment/12600914/5965-1.2.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-08-31 03:01:00.071,,,no_permission,,,,,,,,,,,,346380,,,Tue Sep 03 21:18:53 UTC 2013,,,,,,0|i1nqan:,346681,1.2.9,,,,,,,jbellis,jbellis,,,,,,,,,dmeyer,31/Aug/13 02:20;yukim;Attaching unit test case to reproduce.,31/Aug/13 02:49;yukim;Patch attached to handle key of length 0.,"31/Aug/13 03:01;jbellis;+1

Also suggest adding a {{: length}} to the assert, which will make diagnosing such oversights easier in the future.","03/Sep/13 19:19;dmeyer;Patch is good.  To repro the issue it is important to insert an empty string into an indexed column.  Leaving it null, will not cause a repro.

To repro:

set index_interval to 1

CREATE TABLE cf (
  name text PRIMARY KEY,
  val1 text,
  val2 text
) 

CREATE INDEX cf_val2_idx ON cf (val2);
INSERT INTO cf (name, val1, val2) VALUES ('dmeyer', 'testval', '');

Then run:
nodetool rebuild index ks cf cf.cf_val2_idx
flush
restart node
Observe the ERROR in system.log
After applying the patch and building and reproducing the above procedure the error did not occur.","03/Sep/13 21:18;yukim;Committed with new assert comment.
Note that 2.0.0 and above do not have this bug, so I just merged unit test and comment to cassandra-2.0 branch and trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dateOf() in 2.0 won't work with timestamp columns created in 1.2-,CASSANDRA-5928,12665231,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,23/Aug/13 14:55,12/Mar/19 14:17,13/Mar/19 22:29,26/Aug/13 15:54,2.0.0,,,,,,0,cql3,,,,"dateof() return type is TimestampType now, so it won't work with previously created DateType columns
(Type error: cannot assign result of function dateof (type timestamp) to value (type 'org.apache.cassandra.db.marshal.DateType')).",,,,,,,,,,,,,,,,,,,,,,,,23/Aug/13 15:22;iamaleksey;5928.txt;https://issues.apache.org/jira/secure/attachment/12599635/5928.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-26 15:33:20.643,,,no_permission,,,,,,,,,,,,345172,,,Mon Aug 26 15:54:41 UTC 2013,,,,,,0|i1niv3:,345473,2.0 rc1,,,,,,,slebresne,slebresne,,,,,,,,,,"23/Aug/13 15:24;iamaleksey;Actually, I think the fix is as simple as making DateType.asCQL3Type() return TIMESTAMP (since the assignability check is basically comparing the cql3 types).","26/Aug/13 15:33;slebresne;You're right, that fixes dateOf (and to be clear, I don't see a clearly much better fix). The one small concern I have is that because we use asCQL3Type() when printing error message, we will show ""timestamp"" when DateType is used even though ""timestamp"" is really TimestampType in practice. That being say, I ""think"" this has no real practical consequence so +1. ","26/Aug/13 15:54;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The native protocol server can deadlock,CASSANDRA-5926,12665214,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,23/Aug/13 11:57,12/Mar/19 14:17,13/Mar/19 22:29,30/Aug/13 11:32,1.2.9,,,,,,0,,,,,"Until CASSANDRA-5239 (i.e. since StorageProxy is blocking), the native protocol server needs to use a thread per request being processed. For that, it currently use a DebuggableThreadPoolExecutor, but with a limited queue. The rational being that we don't want to OOM if a client overwhelm the server. Rather, we prefer blocking (which DTPE gives us) on the submission of new request by the netty worker threads when all threads are busy.

However, as it happens, when netty sends back a response to a query, there is cases where some events (technically, InterestChanged and WriteComplete events) are send up the pipeline. And those event are submitted on the request executor as other requests. Long story short, a request thread can end blocking on the submission to its own executor, hence deadlocking.

The simplest solution is probably to reuse MemoryAwareThreadPoolExecutor from netty rather that our own DTPE as it also allow to block task submission when all threads are busy but knows not to block it's own internal events.
",,,,,,,,,,,,,,,,,,,,,,,,23/Aug/13 11:58;slebresne;5926.txt;https://issues.apache.org/jira/secure/attachment/12599611/5926.txt,28/Aug/13 18:45;mkjellman;stack;https://issues.apache.org/jira/secure/attachment/12600437/stack,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-08-23 14:09:05.61,,,no_permission,,,,,,,,,,,,345155,,,Fri Aug 30 11:32:34 UTC 2013,,,,,,0|i1nirb:,345456,1.2.9,,,,,,,jbellis,jbellis,,,,,,,,,,"23/Aug/13 11:58;slebresne;Attaching patch that switches to MemoryAwareThreadPoolExecutor. One interesting detail is that MATPE always forces corePoolSize == maxPoolSize, so doing that pretty much deprecates the native_transport_min_threads in the yaml. I'd say this is probably fine in practice though.
","23/Aug/13 14:09;jbellis;""serie"" typo, otherwise +1","26/Aug/13 15:43;slebresne;Committed, thanks","28/Aug/13 18:45;mkjellman;Ironically, I hit this deadlock after the proposed change. Thread dump attached.

IO Worker #11 is most interesting.",29/Aug/13 01:12;mkjellman;https://github.com/netty/netty/issues/1310,"29/Aug/13 17:55;slebresne;Yep, pretty sure that's what you've run into. We'll have to update your netty dependency.","30/Aug/13 11:32;slebresne;Let me re-close this since the committed did fix the original deadlock. It's obviously now unfortunate that we're running into a netty bug, but since 1.2.9 has shipped, I've opened a separate issue (CASSANDRA-5955) to upgrade our dependency.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in update lightweight transaction,CASSANDRA-5925,12665142,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,ppersad,ppersad,22/Aug/13 23:17,12/Mar/19 14:17,13/Mar/19 22:29,28/Aug/13 10:33,2.0.0,,,,,,0,LWT,,,,"I'm building some tests for a Cassandra PoC.  One scenario I need to test is consumption of 1 time tokens.  These tokens must be consumed exactly once.  The cluster involved is a 3 node cluster.  All queries are run with ConsistencyLevel.QUORUM. I'm using the following queries:

CREATE KEYSPACE IF NOT EXISTS test WITH replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 };

CREATE TABLE IF NOT EXISTS tkns (tkn blob, consumed boolean, PRIMARY KEY (tkn));

INSERT INTO tkns (tkn, consumed) VALUES (?,FALSE) USING TTL 30;

UPDATE tkns USING TTL 1 SET consumed = TRUE WHERE tkn = ? IF consumed = FALSE;

I use the '[applied]' column in the result set of the update statement to determine whether the token has been successfully consumed or if the token is being replayed.

My test involves concurrently executing many sets of 1 insert and 2 update statements (using Session#execute on BoundStatemnts) then checking to make sure that only one of the updates was applied.

When I run this test with relatively few iterations (~100) my results are  what I expect (exactly 1 update succeeds).  At ~1000 iterations, I start seeing both updates reporting success in 1-2% of cases.  While my test is running, I see corresponding error entries in the Cassandra log:

ERROR 15:34:53,583 Exception in thread Thread[MutationStage:522,5,main]
java.lang.NullPointerException
ERROR 15:34:53,584 Exception in thread Thread[MutationStage:474,5,main]
java.lang.NullPointerException
ERROR 15:34:53,584 Exception in thread Thread[MutationStage:536,5,main]
java.lang.NullPointerException
ERROR 15:34:53,729 Exception in thread Thread[MutationStage:480,5,main]
java.lang.NullPointerException
ERROR 15:34:53,729 Exception in thread Thread[MutationStage:534,5,main]
java.lang.NullPointerException


Thanks.

Update:

I'm not sure what's going on with the logging the the dev release.  I grabbed the rc2 source and built that.  The resultant log is a bit more informative:

ERROR 11:53:38,967 Exception in thread Thread[MutationStage:114,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.serializers.UUIDSerializer.deserialize(UUIDSerializer.java:32)
	at org.apache.cassandra.serializers.UUIDSerializer.deserialize(UUIDSerializer.java:26)
	at org.apache.cassandra.db.marshal.AbstractType.compose(AbstractType.java:142)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getUUID(UntypedResultSet.java:131)
	at org.apache.cassandra.db.SystemKeyspace.loadPaxosState(SystemKeyspace.java:785)
	at org.apache.cassandra.service.paxos.PaxosState.commit(PaxosState.java:118)
	at org.apache.cassandra.service.paxos.CommitVerbHandler.doVerb(CommitVerbHandler.java:34)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
",3 node Cassandra 2.0.0-rc2 cluster. Java driver 1.0.2.,,,,,,,,,,,,,,,,,,,,,,,27/Aug/13 13:53;slebresne;5925.txt;https://issues.apache.org/jira/secure/attachment/12600165/5925.txt,26/Aug/13 18:04;ppersad;TokenConsumptionTest.java;https://issues.apache.org/jira/secure/attachment/12599991/TokenConsumptionTest.java,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-08-23 07:43:19.559,,,no_permission,,,,,,,,,,,,345083,,,Wed Aug 28 10:33:54 UTC 2013,,,,,,0|i1nibj:,345384,2.0 rc2,,,,,,,jbellis,jbellis,,,,,,,,,,"23/Aug/13 07:43;slebresne;Don't you have more stack trace in the log? To have a NPE in the MutationStage is definitively a bug, but it would be immensely easier to find what's the problem with a stack trace. And there should be a trace. ","23/Aug/13 16:49;ppersad;I'm a little puzzled by that myself.  What I've posted is exactly what I see in the log output.  The exception seems to be being logged by the UncaughtExceptionHandler set at CassandraDaemon:129.  That's the only place in the code base where I can find a matching exception message.  That call looks like it should have a full stack trace, but I'm not seeing it in the log.",23/Aug/13 18:59;ppersad;I've updated the ticket with a more complete stack trace.,"26/Aug/13 09:36;slebresne;Thanks for the full stack.

Not completely sure what triggers that NPE however. I do see one scenario where the paxos state on-disk could not be empty but the there's no ""in_progress_ballot"" column (hence triggering that NPE): since savePaxosPromise only write the ""proposal"" and not the ""in_progress_ballot"", and since we set TTL on inserts, it sounds possible to end up in a case where a paxos row only has the ""proposal"" column but everything else has expired. I do am attaching a simple patch to always write ""in_progress_ballot"" to avoid that, but given that paxos TTL is at least 3 hours, I doubt that's the scenario you are running into in your test. So not sure what's going on ([~jbellis], if you have a brilliant idea...).

Phil, if you could check if the attached patch fixes it by any chance, that could be helpful. If it doesn't (likely), would you be able to provide a simple test script that reproduce this?
",26/Aug/13 18:04;ppersad;JUnit test to demonstrate the issue.,26/Aug/13 18:07;ppersad;I applied the patch and it did indeed make the NPEs go away.  However the double-consumption of tokens persists.  See the attached JUnit test class (the CassandraClient mentioned in the test is just a simple wrapper that creates a Cluster and Session and sets a default consistency of QUORUM on every query).,"27/Aug/13 13:53;slebresne;Thanks for the test.

There is indeed 2 problems:
# the NPE while loading the paxos state. Contrarily to my first reading, the fact that savePaxosProposal doesn't save the inProgress ballot is not only a problem due to expiration. We may call that method if our own inProgress is older than the proposal, so given a node received a proposal without having seen the prepare first (and without having a previous state), we'd end up with a state where just the 'proposal' column is set. Note that technically, I don't think it breaks Paxos not to update the ballot when saving a proposal so we could just fix loadPaxosState to not NPE in that case, but it feels saner/simpler to me to write the proposal ballot when we save a proposal value.
# the reason Phil test fails is different however. The problem is that when we were building the 'expected' ColumnFamily for the cas() call in CQL3, we were using the full parameters of the statement, including (which was the problem) the TTL. In that test, the TTL is 1 second, so it's possible (and even not that unlikely since we only have up to 1 second accuracy internally) that when we were comparing 'expected' to 'current' the expected column was considered deleted. So, if when the 2nd update to a row was processed the first one had expired (again, not unlikely given the 1 second ttl), the 2nd CAS update was comparing both a deleted 'expected' and a deleted 'current', thus succeeding.  So anyway, the fix is to not use the TTL for the conditions in ModificationStatement.

So attaching patch that fix both issue.
","27/Aug/13 18:26;ppersad;I applied the patch and it does seem to improve the situation.  Unfortunately, I'm still seeing double consumptions.  However, the occurrence has dropped from 1-2% to 0.002%-0.04%.  That rate is low enough that it may not show up with the sample size of 40,000 in the test I posted.  Increasing the iterations by 10-20 times should serve to demonstrate the issue.

For the sake of exploration, after I patched I tried running the test both with and without the TTL in the update statement an saw no appreciable difference in the number of failures. It looks like there may be yet a third problem.",27/Aug/13 18:32;jbellis;+1,"27/Aug/13 18:38;slebresne;bq. I applied the patch and it does seem to improve the situation. Unfortunately, I'm still seeing double consumptions. However, the occurrence has dropped from 1-2% to 0.002%-0.04%

Forgot to say, your test is actually broken in theory because it picks random keys. It is thus possible for an insert of one of the jobs to interleave with the updates of another one, thus making both update apply (and presumably both update of the other job fail, but the test don't check for that). So I'd suggest first re-running with guaranteed unique keys. If it still fails, I'm happy to look at it more deeply, though we did fixed 2 problem so let's commit those and open a separate ticket if a 3rd problem there is indeed (and if there is 3rd that take more than 40K iterations to manifest itself, chances are it will be tricky to track down, so it's not worth holding on the initial fixes). ","27/Aug/13 20:02;ppersad;I take your point and I'll tweak my test to check for duplicate keys.  That being said, even without the use of SecureRandom, I seriously doubt that I'm getting collisions on a 64 Byte key.

I'll create a new ticket once I've had time to update my test.",27/Aug/13 21:29;ppersad;I've updated my test to ensure uniqueness of the tokens and am still experiencing failures.  I've created CASSANDRA-5945 to track the issue so that the fixes here can be committed.,"28/Aug/13 10:33;slebresne;Alright, committed, thanks, we'll followup on CASSANDRA-5945.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Querying with an empty (impossible) range returns incorrect results,CASSANDRA-5573,12647879,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,mikeschrag,mikeschrag,16/May/13 02:50,12/Mar/19 14:17,13/Mar/19 22:29,12/Jul/13 15:17,1.2.7,2.0 beta 2,,,,,0,,,,,"SELECT * FROM cf WHERE token(key) > 2000 AND token(key) <= 2000 LIMIT 1000 ALLOW FILTERING;

This should return nothing, but instead appears to freak and return arbitrary token values.",,,,,,,,,,,,,,,,,,,,,,,,09/Jul/13 14:18;slebresne;5573.txt;https://issues.apache.org/jira/secure/attachment/12591437/5573.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-16 20:08:17.14,,,no_permission,,,,,,,,,,,,328235,,,Fri Jul 12 15:17:50 UTC 2013,,,,,,0|i1kms7:,328579,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"16/May/13 20:08;alexliu68; * The semantics of start keys and tokens are slightly different.
 * Keys are start-inclusive; tokens are start-exclusive.  Token
 * ranges may also wrap -- that is, the end token may be less
 * than the start one.  Thus, a range from keyX to keyX is a
 * one-element range, but a range from tokenY to tokenY is the
 * full ring.

So that query covers the whole ring. I think it's a bug in the hadoop code. I will fix it in CASSANDRA-4421","17/May/13 06:17;slebresne;Hum, that's not an hadoop query, that's a CQL3, so this is a valid issue.","17/May/13 06:53;slebresne;Let me maybe justify a bit more why I believe it's a valid issue.

It is true that in thrift, get_range_slide can take a range of tokens and this range is allowed to wrap, so the start token can be >= to the end token.
However, in CQL, the where clause expresses a condition that each returned record needs to fulfill to be returned. This is different. And it is impossible to have a key for which the token is both > 2000 and <= 2000, so such request must return nothing (or return an error thought that would be less convenient in practice).

Now, a related question could be, how do I return in CQL3 all the records whose tokens is in the wrapping (2000, 2000] token range? Well, you can't with just one request currently, you have to unwrap the range manually (and thus potentially do 2 queries since we don't have OR conditions). Is that a problem? I don't think so. If you have a not trivial amount of data, iterating over it all is going to take way more than one query anyway and if you care about performance, you'd better use hadoop for such things. Besides, if you do really want to iterate over the whole ring, just iterate from the min token to the max one and you don't even have to bother about wrapping.","09/Jul/13 14:18;slebresne;Attaching patch for this.

Now, it turns out that the current behavior of CQL when given unsatisfiable conditions is to throw an exception, so it would be somewhat to do it in that case too. However, the attached takes the other direction, of always returning an empty result set in that case because:
# the current behavior is not consistent anyway. If you do one of:
{noformat}
SELECT v FROM test WHERE k=0 AND v > 1 AND v <= 1
SELECT v FROM test WHERE k=0 AND v > 1 AND v < 2
{noformat}
(where {{v}} is a clustering column), then you get an exception, but if you do
{noformat}
SELECT v FROM test WHERE k=0 AND v >= 2 AND v < 1
{noformat}
then you actually get an empty result set.
# I think returning an empty result set is really more user friendly (and I've meant to change that for a while). As far as I know this is also how every SQL DB works.
","12/Jul/13 13:17;iamaleksey;LGTM, +1

nanonits:
- you can kill both ColumnSlice.validate() overloads (only used by QP.validateSLiceFilter) altogether if you ditch QueryProcessor.validateSliceFilter() (unused).
- SelectStatement.getKeyBounds() comment 'it will happilly return the whole' has a typo in 'happilly'.
",12/Jul/13 15:17;slebresne;Committed with nits addressed. Thanks.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changing column type from int to bigint or vice versa causes decoding errors.,CASSANDRA-5882,12663433,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,jblangston@datastax.com,jblangston@datastax.com,13/Aug/13 14:22,12/Mar/19 14:17,13/Mar/19 22:29,26/Aug/13 16:01,1.2.9,,,,,,0,,,,,"cqlsh:dbsite> create table testint (id uuid, bestof bigint, primary key (id) );
cqlsh:dbsite> insert into testint (id, bestof ) values (49d30f84-a409-4433-ad60-eb9c1a06b7bb, 1376399966);
cqlsh:dbsite> insert into testint (id, bestof ) values (6cab4798-ad29-4419-bd59-308f9ec3bc44, 1376389800);
cqlsh:dbsite> insert into testint (id, bestof ) values (685bb9ff-a4fe-4e47-95eb-f6a353d9e179, 1376390400);
cqlsh:dbsite> insert into testint (id, bestof ) values (a848f832-5ded-4ef7-bf4b-7db561564c57, 1376391000);
cqlsh:dbsite> select * from testint ;
 id                                   | bestof
--------------------------------------+------------
 a848f832-5ded-4ef7-bf4b-7db561564c57 | 1376391000
 49d30f84-a409-4433-ad60-eb9c1a06b7bb | 1376399966
 6cab4798-ad29-4419-bd59-308f9ec3bc44 | 1376389800
 685bb9ff-a4fe-4e47-95eb-f6a353d9e179 | 1376390400

cqlsh:dbsite> alter table testint alter bestof TYPE int;
cqlsh:dbsite> select * from testint ;
 id                                   | bestof
--------------------------------------+-----------------------------
 a848f832-5ded-4ef7-bf4b-7db561564c57 |  '\x00\x00\x00\x00R\n\x0fX'
 49d30f84-a409-4433-ad60-eb9c1a06b7bb |     '\x00\x00\x00\x00R\n2^'
 6cab4798-ad29-4419-bd59-308f9ec3bc44 | '\x00\x00\x00\x00R\n\n\xa8'
 685bb9ff-a4fe-4e47-95eb-f6a353d9e179 | '\x00\x00\x00\x00R\n\r\x00'

Failed to decode value '\x00\x00\x00\x00R\n\x0fX' (for column 'bestof') as int: unpack requires a string argument of length 4
Failed to decode value '\x00\x00\x00\x00R\n2^' (for column 'bestof') as int: unpack requires a string argument of length 4
2 more decoding errors suppressed.


I realize that going from BIGINT to INT would cause overflow if a column contained a number larger than 2^31-1, it is at least technically possible to go in the other direction.  I also understand that rewriting all the data in the correct format would be a very expensive operation on a large column family, but if that's not something we want to allow we should explicitly disallow changing data types if the table has any rows.",,,,,,,,,,,,,,,,,,,,,,,,21/Aug/13 14:11;slebresne;5882.txt;https://issues.apache.org/jira/secure/attachment/12599197/5882.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-21 14:11:19.512,,,no_permission,,,,,,,,,,,,343434,,,Mon Aug 26 16:01:12 UTC 2013,,,,,,0|i1n86f:,343738,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"21/Aug/13 14:11;slebresne;The reason is that we currently don't validate anything when a non-PK column type is altered, which is probably a mistake (at least imo). There's an historical reason for that: thrift doesn't validate such change either. So in a way, that's not a new thing: if you change a validator for an incompatible one (which is the case here, int (Int32Type) is not able to decode most value of bigint (IntegerType)), you'll likely break your client code (which is the case btw, it's a cqlsh error here, not a server side one). 
 
So I would personally be in favor of just refusing that kind of change pure and simple. Attaching a simple patch to do that (for CQL3 only since Thou Shalt Not Modify Thrift).","21/Aug/13 14:17;carlyeks;a couple of misspelled comments: s/let is fly/let it fly/; s/changet/change/

",26/Aug/13 15:56;iamaleksey;+1,"26/Aug/13 16:01;slebresne;Alright, committed (with typo fixed). Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Filtering on Secondary Index Takes a Long Time Even with Limit 1, Trace Log Filled with Looping Messages",CASSANDRA-5975,12666780,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,rspitzer,rspitzer,03/Sep/13 22:47,12/Mar/19 14:17,13/Mar/19 22:29,10/Sep/13 14:59,1.2.10,2.0.1,,Feature/2i Index,,,0,,,,,"After creating a table with 300,000 keys. Attempting to filter on a column with a secondary index causes an rpc timeout. Using a limit statement does not alleviate the problem. The tracing log appears to be filled with the same set of messages repeated over and over until the query times out. 

The data was created with the attached script and the command
{code}
python create_data.py --num-keys 300000 --num-columns 50 --keyspace 'ks' --columnfamily cf_300000_keys_50_cols --create-index y -v 3
{code}

The query causing the delay is
{code}
select * from cf_300000_keys_50_cols where color = 'green' limit 1;
{code}

An excerpt of the trace log
{code}
Tracing session: cedbead0-14d7-11e3-915e-999f6c86239a

 activity                                                                          | timestamp    | source       | source_elapsed
-----------------------------------------------------------------------------------+--------------+--------------+----------------
                                                                execute_cql3_query | 20:31:27,230 | 10.196.1.106 |              0
       Parsing select * from cf_300000_keys_50_cols where color = 'green' limit 1; | 20:31:27,230 | 10.196.1.106 |             31
                                                                Peparing statement | 20:31:27,230 | 10.196.1.106 |            219
                                                     Determining replicas to query | 20:31:27,230 | 10.196.1.106 |            563
 Executing indexed scan for [min(-9223372036854775808), min(-9223372036854775808)] | 20:31:27,232 | 10.196.1.106 |           1816
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,232 | 10.196.1.106 |           2036
                                                      Acquiring sstable references | 20:31:27,232 | 10.196.1.106 |           2201
                                                       Merging memtable tombstones | 20:31:27,232 | 10.196.1.106 |           2345
                                                       Key cache hit for sstable 3 | 20:31:27,232 | 10.196.1.106 |           2493
                                       Seeking to partition beginning in data file | 20:31:27,232 | 10.196.1.106 |           2555
                                                       Key cache hit for sstable 1 | 20:31:27,234 | 10.196.1.106 |           3742
                                       Seeking to partition beginning in data file | 20:31:27,234 | 10.196.1.106 |           3806
                                        Merging data from memtables and 2 sstables | 20:31:27,236 | 10.196.1.106 |           5805
                                                Read 3 live and 0 tombstoned cells | 20:31:27,236 | 10.196.1.106 |           5977
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,236 | 10.196.1.106 |           6166
                                                      Acquiring sstable references | 20:31:27,236 | 10.196.1.106 |           6319
                                                       Merging memtable tombstones | 20:31:27,236 | 10.196.1.106 |           6382
                                                       Key cache hit for sstable 3 | 20:31:27,236 | 10.196.1.106 |           6421
                                       Seeking to partition beginning in data file | 20:31:27,236 | 10.196.1.106 |           6423
                                            Bloom filter allows skipping sstable 2 | 20:31:27,237 | 10.196.1.106 |           7060
                                            Bloom filter allows skipping sstable 1 | 20:31:27,237 | 10.196.1.106 |           7218
                                        Merging data from memtables and 1 sstables | 20:31:27,237 | 10.196.1.106 |           7358
                                                Read 1 live and 0 tombstoned cells | 20:31:27,238 | 10.196.1.106 |           7644
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,238 | 10.196.1.106 |           7855
                                                      Acquiring sstable references | 20:31:27,238 | 10.196.1.106 |           8008
                                                       Merging memtable tombstones | 20:31:27,238 | 10.196.1.106 |           8072
                                            Bloom filter allows skipping sstable 3 | 20:31:27,238 | 10.196.1.106 |           8225
                                            Bloom filter allows skipping sstable 2 | 20:31:27,238 | 10.196.1.106 |           8284
                                                       Key cache hit for sstable 1 | 20:31:27,238 | 10.196.1.106 |           8367
                                       Seeking to partition beginning in data file | 20:31:27,238 | 10.196.1.106 |           8468
                                        Merging data from memtables and 1 sstables | 20:31:27,239 | 10.196.1.106 |           8968
                                                Read 1 live and 0 tombstoned cells | 20:31:27,239 | 10.196.1.106 |           9234
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,239 | 10.196.1.106 |           9405
                                                      Acquiring sstable references | 20:31:27,239 | 10.196.1.106 |           9547
                                                       Merging memtable tombstones | 20:31:27,240 | 10.196.1.106 |           9608
                                                       Key cache hit for sstable 3 | 20:31:27,240 | 10.196.1.106 |           9700
                                 Seeking to partition indexed section in data file | 20:31:27,240 | 10.196.1.106 |           9884
                                                       Key cache hit for sstable 1 | 20:31:27,240 | 10.196.1.106 |          10005
                                 Seeking to partition indexed section in data file | 20:31:27,240 | 10.196.1.106 |          10175
                                        Merging data from memtables and 2 sstables | 20:31:27,240 | 10.196.1.106 |          10323
                                                Read 3 live and 0 tombstoned cells | 20:31:27,249 | 10.196.1.106 |          19358
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,249 | 10.196.1.106 |          19516
                                                      Acquiring sstable references | 20:31:27,249 | 10.196.1.106 |          19580
                                                       Merging memtable tombstones | 20:31:27,250 | 10.196.1.106 |          19670
                                                       Key cache hit for sstable 3 | 20:31:27,250 | 10.196.1.106 |          19765
                                       Seeking to partition beginning in data file | 20:31:27,250 | 10.196.1.106 |          19884
                                            Bloom filter allows skipping sstable 2 | 20:31:27,250 | 10.196.1.106 |          20357
                                            Bloom filter allows skipping sstable 1 | 20:31:27,250 | 10.196.1.106 |          20514
                                        Merging data from memtables and 1 sstables | 20:31:27,250 | 10.196.1.106 |          20576
                                                Read 1 live and 0 tombstoned cells | 20:31:27,251 | 10.196.1.106 |          20864
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,251 | 10.196.1.106 |          21072
                                                      Acquiring sstable references | 20:31:27,251 | 10.196.1.106 |          21137
                                                       Merging memtable tombstones | 20:31:27,251 | 10.196.1.106 |          21315
                                                       Key cache hit for sstable 3 | 20:31:27,251 | 10.196.1.106 |          21461
                                 Seeking to partition indexed section in data file | 20:31:27,252 | 10.196.1.106 |          21599
                                                       Key cache hit for sstable 1 | 20:31:27,252 | 10.196.1.106 |          21761
                                 Seeking to partition indexed section in data file | 20:31:27,252 | 10.196.1.106 |          21909
                                        Merging data from memtables and 2 sstables | 20:31:27,252 | 10.196.1.106 |          21977
                                                Read 3 live and 0 tombstoned cells | 20:31:27,261 | 10.196.1.106 |          30678
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,261 | 10.196.1.106 |          31153
                                                      Acquiring sstable references | 20:31:27,261 | 10.196.1.106 |          31156
                                                       Merging memtable tombstones | 20:31:27,261 | 10.196.1.106 |          31221
                                                       Key cache hit for sstable 3 | 20:31:27,261 | 10.196.1.106 |          31259
                                       Seeking to partition beginning in data file | 20:31:27,261 | 10.196.1.106 |          31261
                                            Bloom filter allows skipping sstable 2 | 20:31:27,264 | 10.196.1.106 |          33808
                                            Bloom filter allows skipping sstable 1 | 20:31:27,264 | 10.196.1.106 |          33875
                                        Merging data from memtables and 1 sstables | 20:31:27,264 | 10.196.1.106 |          33877
                                                Read 1 live and 0 tombstoned cells | 20:31:27,264 | 10.196.1.106 |          34313
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,264 | 10.196.1.106 |          34488
                                                      Acquiring sstable references | 20:31:27,264 | 10.196.1.106 |          34552
                                                       Merging memtable tombstones | 20:31:27,265 | 10.196.1.106 |          34642
                                                       Key cache hit for sstable 3 | 20:31:27,265 | 10.196.1.106 |          34792
                                 Seeking to partition indexed section in data file | 20:31:27,265 | 10.196.1.106 |          34851
                                                       Key cache hit for sstable 1 | 20:31:27,265 | 10.196.1.106 |          35007
                                 Seeking to partition indexed section in data file | 20:31:27,265 | 10.196.1.106 |          35066
                                        Merging data from memtables and 2 sstables | 20:31:27,265 | 10.196.1.106 |          35272
                                                Read 3 live and 0 tombstoned cells | 20:31:27,274 | 10.196.1.106 |          44333
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,274 | 10.196.1.106 |          44529
                                                      Acquiring sstable references | 20:31:27,275 | 10.196.1.106 |          44686
                                                       Merging memtable tombstones | 20:31:27,275 | 10.196.1.106 |          44752
                                            Bloom filter allows skipping sstable 3 | 20:31:27,275 | 10.196.1.106 |          44766
                                            Bloom filter allows skipping sstable 2 | 20:31:27,275 | 10.196.1.106 |          45021
                                                       Key cache hit for sstable 1 | 20:31:27,275 | 10.196.1.106 |          45163
                                       Seeking to partition beginning in data file | 20:31:27,275 | 10.196.1.106 |          45241
                                        Merging data from memtables and 1 sstables | 20:31:27,276 | 10.196.1.106 |          45719
                                                Read 1 live and 0 tombstoned cells | 20:31:27,276 | 10.196.1.106 |          45985
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,276 | 10.196.1.106 |          46171
                                                      Acquiring sstable references | 20:31:27,276 | 10.196.1.106 |          46235
                                                       Merging memtable tombstones | 20:31:27,276 | 10.196.1.106 |          46325
                                                       Key cache hit for sstable 3 | 20:31:27,276 | 10.196.1.106 |          46501
                                 Seeking to partition indexed section in data file | 20:31:27,276 | 10.196.1.106 |          46561
                                                       Key cache hit for sstable 1 | 20:31:27,277 | 10.196.1.106 |          46652
                                 Seeking to partition indexed section in data file | 20:31:27,277 | 10.196.1.106 |          46853
                                        Merging data from memtables and 2 sstables | 20:31:27,277 | 10.196.1.106 |          46922
                                                Read 3 live and 0 tombstoned cells | 20:31:27,286 | 10.196.1.106 |          56025
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,286 | 10.196.1.106 |          56198
                                                      Acquiring sstable references | 20:31:27,286 | 10.196.1.106 |          56264
                                                       Merging memtable tombstones | 20:31:27,286 | 10.196.1.106 |          56352
                                            Bloom filter allows skipping sstable 3 | 20:31:27,286 | 10.196.1.106 |          56439
                                            Bloom filter allows skipping sstable 2 | 20:31:27,286 | 10.196.1.106 |          56543
                                                       Key cache hit for sstable 1 | 20:31:27,287 | 10.196.1.106 |          56631
                                       Seeking to partition beginning in data file | 20:31:27,287 | 10.196.1.106 |          56634
                                        Merging data from memtables and 1 sstables | 20:31:27,287 | 10.196.1.106 |          57194
                                                Read 1 live and 0 tombstoned cells | 20:31:27,287 | 10.196.1.106 |          57494
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,288 | 10.196.1.106 |          57672
                                                      Acquiring sstable references | 20:31:27,288 | 10.196.1.106 |          57736
                                                       Merging memtable tombstones | 20:31:27,288 | 10.196.1.106 |          57922
                                                       Key cache hit for sstable 3 | 20:31:27,288 | 10.196.1.106 |          58047
                                 Seeking to partition indexed section in data file | 20:31:27,288 | 10.196.1.106 |          58200
                                                       Key cache hit for sstable 1 | 20:31:27,288 | 10.196.1.106 |          58351
                                 Seeking to partition indexed section in data file | 20:31:27,289 | 10.196.1.106 |          58783
                                        Merging data from memtables and 2 sstables | 20:31:27,289 | 10.196.1.106 |          58790
                                                Read 3 live and 0 tombstoned cells | 20:31:27,299 | 10.196.1.106 |          69455
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,300 | 10.196.1.106 |          69641
                                                      Acquiring sstable references | 20:31:27,300 | 10.196.1.106 |          69707
                                                       Merging memtable tombstones | 20:31:27,300 | 10.196.1.106 |          69713
                                            Bloom filter allows skipping sstable 3 | 20:31:27,300 | 10.196.1.106 |          69806
                                            Bloom filter allows skipping sstable 2 | 20:31:27,300 | 10.196.1.106 |          69904
                                                       Key cache hit for sstable 1 | 20:31:27,300 | 10.196.1.106 |          69994
                                       Seeking to partition beginning in data file | 20:31:27,300 | 10.196.1.106 |          70297
                                        Merging data from memtables and 1 sstables | 20:31:27,301 | 10.196.1.106 |          70778
                                                Read 1 live and 0 tombstoned cells | 20:31:27,301 | 10.196.1.106 |          71044
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,301 | 10.196.1.106 |          71271
                                                      Acquiring sstable references | 20:31:27,301 | 10.196.1.106 |          71335
                                                       Merging memtable tombstones | 20:31:27,301 | 10.196.1.106 |          71424
                                                       Key cache hit for sstable 3 | 20:31:27,301 | 10.196.1.106 |          71583
                                 Seeking to partition indexed section in data file | 20:31:27,302 | 10.196.1.106 |          71645
                                                       Key cache hit for sstable 1 | 20:31:27,302 | 10.196.1.106 |          71882
                                 Seeking to partition indexed section in data file | 20:31:27,302 | 10.196.1.106 |          71940
                                        Merging data from memtables and 2 sstables | 20:31:27,302 | 10.196.1.106 |          72030
                                                Read 3 live and 0 tombstoned cells | 20:31:27,311 | 10.196.1.106 |          81217
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,311 | 10.196.1.106 |          81369
                                                      Acquiring sstable references | 20:31:27,311 | 10.196.1.106 |          81515
                                                       Merging memtable tombstones | 20:31:27,312 | 10.196.1.106 |          81661
                                                       Key cache hit for sstable 3 | 20:31:27,312 | 10.196.1.106 |          81812
                                       Seeking to partition beginning in data file | 20:31:27,312 | 10.196.1.106 |          81873
                                            Bloom filter allows skipping sstable 2 | 20:31:27,312 | 10.196.1.106 |          82409
                                            Bloom filter allows skipping sstable 1 | 20:31:27,312 | 10.196.1.106 |          82479
                                        Merging data from memtables and 1 sstables | 20:31:27,312 | 10.196.1.106 |          82481
                                                Read 1 live and 0 tombstoned cells | 20:31:27,313 | 10.196.1.106 |          82860
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,313 | 10.196.1.106 |          83035
                                                      Acquiring sstable references | 20:31:27,313 | 10.196.1.106 |          83099
                                                       Merging memtable tombstones | 20:31:27,313 | 10.196.1.106 |          83217
                                                       Key cache hit for sstable 3 | 20:31:27,313 | 10.196.1.106 |          83307
                                 Seeking to partition indexed section in data file | 20:31:27,313 | 10.196.1.106 |          83410
                                                       Key cache hit for sstable 1 | 20:31:27,314 | 10.196.1.106 |          83588
                                 Seeking to partition indexed section in data file | 20:31:27,314 | 10.196.1.106 |          83652
                                        Merging data from memtables and 2 sstables | 20:31:27,314 | 10.196.1.106 |          83742
                                                Read 3 live and 0 tombstoned cells | 20:31:27,338 | 10.196.1.106 |         108372
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,345 | 10.196.1.106 |         115180
                                                      Acquiring sstable references | 20:31:27,345 | 10.196.1.106 |         115183
                                                       Merging memtable tombstones | 20:31:27,345 | 10.196.1.106 |         115188
                                            Bloom filter allows skipping sstable 3 | 20:31:27,345 | 10.196.1.106 |         115194
                                            Bloom filter allows skipping sstable 2 | 20:31:27,345 | 10.196.1.106 |         115197
                                                       Key cache hit for sstable 1 | 20:31:27,345 | 10.196.1.106 |         115203
                                       Seeking to partition beginning in data file | 20:31:27,345 | 10.196.1.106 |         115205
                                        Merging data from memtables and 1 sstables | 20:31:27,347 | 10.196.1.106 |         116821
                                                Read 1 live and 0 tombstoned cells | 20:31:27,348 | 10.196.1.106 |         117631
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,348 | 10.196.1.106 |         117997
                                                      Acquiring sstable references | 20:31:27,348 | 10.196.1.106 |         118000
                                                       Merging memtable tombstones | 20:31:27,348 | 10.196.1.106 |         118007
                                                       Key cache hit for sstable 3 | 20:31:27,348 | 10.196.1.106 |         118014
                                 Seeking to partition indexed section in data file | 20:31:27,348 | 10.196.1.106 |         118017
                                                       Key cache hit for sstable 1 | 20:31:27,348 | 10.196.1.106 |         118023
                                 Seeking to partition indexed section in data file | 20:31:27,348 | 10.196.1.106 |         118025
                                        Merging data from memtables and 2 sstables | 20:31:27,348 | 10.196.1.106 |         118030
                                                Read 3 live and 0 tombstoned cells | 20:31:27,356 | 10.196.1.106 |         126375
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,356 | 10.196.1.106 |         126418
                                                      Acquiring sstable references | 20:31:27,356 | 10.196.1.106 |         126421
                                                       Merging memtable tombstones | 20:31:27,356 | 10.196.1.106 |         126427
                                            Bloom filter allows skipping sstable 3 | 20:31:27,357 | 10.196.1.106 |         126695
                                            Bloom filter allows skipping sstable 2 | 20:31:27,357 | 10.196.1.106 |         126699
                                                       Key cache hit for sstable 1 | 20:31:27,357 | 10.196.1.106 |         126705
                                       Seeking to partition beginning in data file | 20:31:27,357 | 10.196.1.106 |         126708
                                        Merging data from memtables and 1 sstables | 20:31:27,357 | 10.196.1.106 |         127265
                                                Read 1 live and 0 tombstoned cells | 20:31:27,357 | 10.196.1.106 |         127519
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,358 | 10.196.1.106 |         127686
                                                      Acquiring sstable references | 20:31:27,358 | 10.196.1.106 |         127694
                                                       Merging memtable tombstones | 20:31:27,358 | 10.196.1.106 |         127700
                                                       Key cache hit for sstable 3 | 20:31:27,358 | 10.196.1.106 |         127708
                                 Seeking to partition indexed section in data file | 20:31:27,358 | 10.196.1.106 |         127710
                                                       Key cache hit for sstable 1 | 20:31:27,358 | 10.196.1.106 |         127716
                                 Seeking to partition indexed section in data file | 20:31:27,358 | 10.196.1.106 |         127718
                                        Merging data from memtables and 2 sstables | 20:31:27,358 | 10.196.1.106 |         127722
                                                Read 3 live and 0 tombstoned cells | 20:31:27,366 | 10.196.1.106 |         135976
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,366 | 10.196.1.106 |         136018
                                                      Acquiring sstable references | 20:31:27,366 | 10.196.1.106 |         136020
                                                       Merging memtable tombstones | 20:31:27,366 | 10.196.1.106 |         136026
                                            Bloom filter allows skipping sstable 3 | 20:31:27,366 | 10.196.1.106 |         136032
                                            Bloom filter allows skipping sstable 2 | 20:31:27,366 | 10.196.1.106 |         136035
                                                       Key cache hit for sstable 1 | 20:31:27,366 | 10.196.1.106 |         136040
                                       Seeking to partition beginning in data file | 20:31:27,366 | 10.196.1.106 |         136043
                                        Merging data from memtables and 1 sstables | 20:31:27,367 | 10.196.1.106 |         136852
                                                Read 1 live and 0 tombstoned cells | 20:31:27,367 | 10.196.1.106 |         137046
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,367 | 10.196.1.106 |         137154
                                                      Acquiring sstable references | 20:31:27,367 | 10.196.1.106 |         137159
                                                       Merging memtable tombstones | 20:31:27,367 | 10.196.1.106 |         137165
                                                       Key cache hit for sstable 3 | 20:31:27,367 | 10.196.1.106 |         137172
                                 Seeking to partition indexed section in data file | 20:31:27,367 | 10.196.1.106 |         137175
                                                       Key cache hit for sstable 1 | 20:31:27,367 | 10.196.1.106 |         137181
                                 Seeking to partition indexed section in data file | 20:31:27,367 | 10.196.1.106 |         137183
                                        Merging data from memtables and 2 sstables | 20:31:27,367 | 10.196.1.106 |         137187
                                                Read 3 live and 0 tombstoned cells | 20:31:27,375 | 10.196.1.106 |         145161
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,375 | 10.196.1.106 |         145195
                                                      Acquiring sstable references | 20:31:27,375 | 10.196.1.106 |         145198
                                                       Merging memtable tombstones | 20:31:27,375 | 10.196.1.106 |         145203
                                                       Key cache hit for sstable 3 | 20:31:27,375 | 10.196.1.106 |         145212
                                       Seeking to partition beginning in data file | 20:31:27,375 | 10.196.1.106 |         145214
                                            Bloom filter allows skipping sstable 2 | 20:31:27,376 | 10.196.1.106 |         146059
                                            Bloom filter allows skipping sstable 1 | 20:31:27,376 | 10.196.1.106 |         146063
                                        Merging data from memtables and 1 sstables | 20:31:27,376 | 10.196.1.106 |         146066
                                                Read 1 live and 0 tombstoned cells | 20:31:27,376 | 10.196.1.106 |         146295
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,376 | 10.196.1.106 |         146416
                                                      Acquiring sstable references | 20:31:27,376 | 10.196.1.106 |         146419
                                                       Merging memtable tombstones | 20:31:27,376 | 10.196.1.106 |         146425
                                                       Key cache hit for sstable 3 | 20:31:27,376 | 10.196.1.106 |         146433
                                 Seeking to partition indexed section in data file | 20:31:27,376 | 10.196.1.106 |         146435
                                                       Key cache hit for sstable 1 | 20:31:27,376 | 10.196.1.106 |         146441
                                 Seeking to partition indexed section in data file | 20:31:27,376 | 10.196.1.106 |         146443
                                        Merging data from memtables and 2 sstables | 20:31:27,376 | 10.196.1.106 |         146447
                                                Read 3 live and 0 tombstoned cells | 20:31:27,384 | 10.196.1.106 |         153664
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,384 | 10.196.1.106 |         153708
                                                      Acquiring sstable references | 20:31:27,384 | 10.196.1.106 |         153711
                                                       Merging memtable tombstones | 20:31:27,384 | 10.196.1.106 |         153717
                                                       Key cache hit for sstable 3 | 20:31:27,384 | 10.196.1.106 |         153899
                                       Seeking to partition beginning in data file | 20:31:27,384 | 10.196.1.106 |         153902
                                            Bloom filter allows skipping sstable 2 | 20:31:27,384 | 10.196.1.106 |         154556
                                            Bloom filter allows skipping sstable 1 | 20:31:27,384 | 10.196.1.106 |         154566
                                        Merging data from memtables and 1 sstables | 20:31:27,384 | 10.196.1.106 |         154568
                                                Read 1 live and 0 tombstoned cells | 20:31:27,385 | 10.196.1.106 |         155041
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,385 | 10.196.1.106 |         155112
                                                      Acquiring sstable references | 20:31:27,385 | 10.196.1.106 |         155156
                                                       Merging memtable tombstones | 20:31:27,385 | 10.196.1.106 |         155163
                                                       Key cache hit for sstable 3 | 20:31:27,385 | 10.196.1.106 |         155171
                                 Seeking to partition indexed section in data file | 20:31:27,385 | 10.196.1.106 |         155173
                                                       Key cache hit for sstable 1 | 20:31:27,385 | 10.196.1.106 |         155179
                                 Seeking to partition indexed section in data file | 20:31:27,385 | 10.196.1.106 |         155181
                                        Merging data from memtables and 2 sstables | 20:31:27,385 | 10.196.1.106 |         155185
                                                Read 3 live and 0 tombstoned cells | 20:31:27,393 | 10.196.1.106 |         163321
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,393 | 10.196.1.106 |         163365
                                                      Acquiring sstable references | 20:31:27,393 | 10.196.1.106 |         163367
                                                       Merging memtable tombstones | 20:31:27,393 | 10.196.1.106 |         163373
                                            Bloom filter allows skipping sstable 3 | 20:31:27,393 | 10.196.1.106 |         163379
                                            Bloom filter allows skipping sstable 2 | 20:31:27,393 | 10.196.1.106 |         163382
                                                       Key cache hit for sstable 1 | 20:31:27,393 | 10.196.1.106 |         163387
                                       Seeking to partition beginning in data file | 20:31:27,393 | 10.196.1.106 |         163389
                                        Merging data from memtables and 1 sstables | 20:31:27,394 | 10.196.1.106 |         164206
                                                Read 1 live and 0 tombstoned cells | 20:31:27,394 | 10.196.1.106 |         164397
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,394 | 10.196.1.106 |         164562
                                                      Acquiring sstable references | 20:31:27,394 | 10.196.1.106 |         164570
                                                       Merging memtable tombstones | 20:31:27,394 | 10.196.1.106 |         164576
                                                       Key cache hit for sstable 3 | 20:31:27,394 | 10.196.1.106 |         164584
                                 Seeking to partition indexed section in data file | 20:31:27,394 | 10.196.1.106 |         164587
                                                       Key cache hit for sstable 1 | 20:31:27,395 | 10.196.1.106 |         164593
                                 Seeking to partition indexed section in data file | 20:31:27,395 | 10.196.1.106 |         164595
                                        Merging data from memtables and 2 sstables | 20:31:27,395 | 10.196.1.106 |         164599
                                                Read 3 live and 0 tombstoned cells | 20:31:27,403 | 10.196.1.106 |         172818
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,403 | 10.196.1.106 |         172860
                                                      Acquiring sstable references | 20:31:27,403 | 10.196.1.106 |         172862
                                                       Merging memtable tombstones | 20:31:27,403 | 10.196.1.106 |         172868
                                            Bloom filter allows skipping sstable 3 | 20:31:27,403 | 10.196.1.106 |         172967
                                            Bloom filter allows skipping sstable 2 | 20:31:27,403 | 10.196.1.106 |         172971
                                                       Key cache hit for sstable 1 | 20:31:27,403 | 10.196.1.106 |         172979
                                       Seeking to partition beginning in data file | 20:31:27,403 | 10.196.1.106 |         172982
                                        Merging data from memtables and 1 sstables | 20:31:27,404 | 10.196.1.106 |         173735
                                                Read 1 live and 0 tombstoned cells | 20:31:27,404 | 10.196.1.106 |         173923
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,404 | 10.196.1.106 |         174088
                                                      Acquiring sstable references | 20:31:27,404 | 10.196.1.106 |         174096
                                                       Merging memtable tombstones | 20:31:27,404 | 10.196.1.106 |         174102
                                                       Key cache hit for sstable 3 | 20:31:27,404 | 10.196.1.106 |         174110
                                 Seeking to partition indexed section in data file | 20:31:27,404 | 10.196.1.106 |         174112
                                                       Key cache hit for sstable 1 | 20:31:27,404 | 10.196.1.106 |         174119
                                 Seeking to partition indexed section in data file | 20:31:27,404 | 10.196.1.106 |         174121
                                        Merging data from memtables and 2 sstables | 20:31:27,404 | 10.196.1.106 |         174124
                                                Read 3 live and 0 tombstoned cells | 20:31:27,414 | 10.196.1.106 |         183917
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,414 | 10.196.1.106 |         183959
                                                      Acquiring sstable references | 20:31:27,414 | 10.196.1.106 |         183961
                                                       Merging memtable tombstones | 20:31:27,414 | 10.196.1.106 |         183967
                                            Bloom filter allows skipping sstable 3 | 20:31:27,414 | 10.196.1.106 |         183973
                                            Bloom filter allows skipping sstable 2 | 20:31:27,414 | 10.196.1.106 |         183976
                                                       Key cache hit for sstable 1 | 20:31:27,414 | 10.196.1.106 |         183982
                                       Seeking to partition beginning in data file | 20:31:27,414 | 10.196.1.106 |         183984
                                        Merging data from memtables and 1 sstables | 20:31:27,415 | 10.196.1.106 |         184807
                                                Read 1 live and 0 tombstoned cells | 20:31:27,415 | 10.196.1.106 |         184994
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,415 | 10.196.1.106 |         185105
                                                      Acquiring sstable references | 20:31:27,415 | 10.196.1.106 |         185108
                                                       Merging memtable tombstones | 20:31:27,415 | 10.196.1.106 |         185114
                                                       Key cache hit for sstable 3 | 20:31:27,415 | 10.196.1.106 |         185134
                                 Seeking to partition indexed section in data file | 20:31:27,415 | 10.196.1.106 |         185136
                                                       Key cache hit for sstable 1 | 20:31:27,415 | 10.196.1.106 |         185142
                                 Seeking to partition indexed section in data file | 20:31:27,415 | 10.196.1.106 |         185144
                                        Merging data from memtables and 2 sstables | 20:31:27,415 | 10.196.1.106 |         185148
                                                Read 3 live and 0 tombstoned cells | 20:31:27,423 | 10.196.1.106 |         193586
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,424 | 10.196.1.106 |         193635
                                                      Acquiring sstable references | 20:31:27,424 | 10.196.1.106 |         193638
                                                       Merging memtable tombstones | 20:31:27,424 | 10.196.1.106 |         193643
                                            Bloom filter allows skipping sstable 3 | 20:31:27,424 | 10.196.1.106 |         193649
                                            Bloom filter allows skipping sstable 2 | 20:31:27,424 | 10.196.1.106 |         193652
                                                       Key cache hit for sstable 1 | 20:31:27,424 | 10.196.1.106 |         193657
                                       Seeking to partition beginning in data file | 20:31:27,424 | 10.196.1.106 |         193660
                                        Merging data from memtables and 1 sstables | 20:31:27,424 | 10.196.1.106 |         194274
                                                Read 1 live and 0 tombstoned cells | 20:31:27,424 | 10.196.1.106 |         194483
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,425 | 10.196.1.106 |         194597
                                                      Acquiring sstable references | 20:31:27,425 | 10.196.1.106 |         194605
                                                       Merging memtable tombstones | 20:31:27,425 | 10.196.1.106 |         194611
                                                       Key cache hit for sstable 3 | 20:31:27,425 | 10.196.1.106 |         194619
                                 Seeking to partition indexed section in data file | 20:31:27,425 | 10.196.1.106 |         194621
                                                       Key cache hit for sstable 1 | 20:31:27,425 | 10.196.1.106 |         194627
                                 Seeking to partition indexed section in data file | 20:31:27,425 | 10.196.1.106 |         194629
                                        Merging data from memtables and 2 sstables | 20:31:27,425 | 10.196.1.106 |         194633
                                                Read 3 live and 0 tombstoned cells | 20:31:27,433 | 10.196.1.106 |         203042
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,433 | 10.196.1.106 |         203084
                                                      Acquiring sstable references | 20:31:27,433 | 10.196.1.106 |         203086
                                                       Merging memtable tombstones | 20:31:27,433 | 10.196.1.106 |         203092
                                            Bloom filter allows skipping sstable 3 | 20:31:27,433 | 10.196.1.106 |         203097
                                            Bloom filter allows skipping sstable 2 | 20:31:27,433 | 10.196.1.106 |         203100
                                                       Key cache hit for sstable 1 | 20:31:27,433 | 10.196.1.106 |         203106
                                       Seeking to partition beginning in data file | 20:31:27,433 | 10.196.1.106 |         203108
                                        Merging data from memtables and 1 sstables | 20:31:27,434 | 10.196.1.106 |         203598
                                                Read 1 live and 0 tombstoned cells | 20:31:27,434 | 10.196.1.106 |         203942
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,434 | 10.196.1.106 |         204112
                                                      Acquiring sstable references | 20:31:27,434 | 10.196.1.106 |         204120
                                                       Merging memtable tombstones | 20:31:27,434 | 10.196.1.106 |         204126
                                                       Key cache hit for sstable 3 | 20:31:27,434 | 10.196.1.106 |         204134
                                 Seeking to partition indexed section in data file | 20:31:27,434 | 10.196.1.106 |         204136
                                                       Key cache hit for sstable 1 | 20:31:27,434 | 10.196.1.106 |         204142
                                 Seeking to partition indexed section in data file | 20:31:27,434 | 10.196.1.106 |         204144
                                        Merging data from memtables and 2 sstables | 20:31:27,434 | 10.196.1.106 |         204148
                                                Read 3 live and 0 tombstoned cells | 20:31:27,441 | 10.196.1.106 |         211397
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,441 | 10.196.1.106 |         211439
                                                      Acquiring sstable references | 20:31:27,441 | 10.196.1.106 |         211441
                                                       Merging memtable tombstones | 20:31:27,441 | 10.196.1.106 |         211447
                                            Bloom filter allows skipping sstable 3 | 20:31:27,442 | 10.196.1.106 |         211546
                                            Bloom filter allows skipping sstable 2 | 20:31:27,442 | 10.196.1.106 |         211732
                                                       Key cache hit for sstable 1 | 20:31:27,442 | 10.196.1.106 |         211737
                                       Seeking to partition beginning in data file | 20:31:27,442 | 10.196.1.106 |         211739
                                        Merging data from memtables and 1 sstables | 20:31:27,442 | 10.196.1.106 |         212484
                                                Read 1 live and 0 tombstoned cells | 20:31:27,443 | 10.196.1.106 |         212809
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,443 | 10.196.1.106 |         212918
                                                      Acquiring sstable references | 20:31:27,443 | 10.196.1.106 |         212921
                                                       Merging memtable tombstones | 20:31:27,443 | 10.196.1.106 |         212927
                                                       Key cache hit for sstable 3 | 20:31:27,443 | 10.196.1.106 |         212934
                                 Seeking to partition indexed section in data file | 20:31:27,443 | 10.196.1.106 |         212936
                                                       Key cache hit for sstable 1 | 20:31:27,443 | 10.196.1.106 |         212943
                                 Seeking to partition indexed section in data file | 20:31:27,443 | 10.196.1.106 |         212945
                                        Merging data from memtables and 2 sstables | 20:31:27,443 | 10.196.1.106 |         212949
                                                Read 3 live and 0 tombstoned cells | 20:31:27,451 | 10.196.1.106 |         220911
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,451 | 10.196.1.106 |         220954
                                                      Acquiring sstable references | 20:31:27,451 | 10.196.1.106 |         220956
                                                       Merging memtable tombstones | 20:31:27,451 | 10.196.1.106 |         220962
                                            Bloom filter allows skipping sstable 3 | 20:31:27,451 | 10.196.1.106 |         220968
                                            Bloom filter allows skipping sstable 2 | 20:31:27,451 | 10.196.1.106 |         220971
                                                       Key cache hit for sstable 1 | 20:31:27,451 | 10.196.1.106 |         220976
                                       Seeking to partition beginning in data file | 20:31:27,451 | 10.196.1.106 |         220979
                                        Merging data from memtables and 1 sstables | 20:31:27,452 | 10.196.1.106 |         221774
                                                Read 1 live and 0 tombstoned cells | 20:31:27,452 | 10.196.1.106 |         221965
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,452 | 10.196.1.106 |         222093
                                                      Acquiring sstable references | 20:31:27,452 | 10.196.1.106 |         222101
                                                       Merging memtable tombstones | 20:31:27,452 | 10.196.1.106 |         222108
                                                       Key cache hit for sstable 3 | 20:31:27,452 | 10.196.1.106 |         222115
                                 Seeking to partition indexed section in data file | 20:31:27,452 | 10.196.1.106 |         222118
                                                       Key cache hit for sstable 1 | 20:31:27,452 | 10.196.1.106 |         222124
                                 Seeking to partition indexed section in data file | 20:31:27,452 | 10.196.1.106 |         222126
                                        Merging data from memtables and 2 sstables | 20:31:27,452 | 10.196.1.106 |         222130
                                                Read 3 live and 0 tombstoned cells | 20:31:27,460 | 10.196.1.106 |         230378
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,460 | 10.196.1.106 |         230420
                                                      Acquiring sstable references | 20:31:27,460 | 10.196.1.106 |         230422
                                                       Merging memtable tombstones | 20:31:27,460 | 10.196.1.106 |         230428
                                            Bloom filter allows skipping sstable 3 | 20:31:27,461 | 10.196.1.106 |         230527
                                            Bloom filter allows skipping sstable 2 | 20:31:27,461 | 10.196.1.106 |         230619
                                                       Key cache hit for sstable 1 | 20:31:27,461 | 10.196.1.106 |         230624
                                       Seeking to partition beginning in data file | 20:31:27,461 | 10.196.1.106 |         230627
                                        Merging data from memtables and 1 sstables | 20:31:27,461 | 10.196.1.106 |         231223
                                                Read 1 live and 0 tombstoned cells | 20:31:27,461 | 10.196.1.106 |         231419
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,461 | 10.196.1.106 |         231531
                                                      Acquiring sstable references | 20:31:27,461 | 10.196.1.106 |         231540
                                                       Merging memtable tombstones | 20:31:27,461 | 10.196.1.106 |         231545
                                                       Key cache hit for sstable 3 | 20:31:27,461 | 10.196.1.106 |         231553
                                 Seeking to partition indexed section in data file | 20:31:27,461 | 10.196.1.106 |         231555
                                                       Key cache hit for sstable 1 | 20:31:27,461 | 10.196.1.106 |         231562
                                 Seeking to partition indexed section in data file | 20:31:27,461 | 10.196.1.106 |         231564
                                        Merging data from memtables and 2 sstables | 20:31:27,461 | 10.196.1.106 |         231568
                                                Read 3 live and 0 tombstoned cells | 20:31:27,470 | 10.196.1.106 |         239976
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,470 | 10.196.1.106 |         240020
                                                      Acquiring sstable references | 20:31:27,470 | 10.196.1.106 |         240022
                                                       Merging memtable tombstones | 20:31:27,470 | 10.196.1.106 |         240028
                                            Bloom filter allows skipping sstable 3 | 20:31:27,470 | 10.196.1.106 |         240034
                                            Bloom filter allows skipping sstable 2 | 20:31:27,470 | 10.196.1.106 |         240037
                                                       Key cache hit for sstable 1 | 20:31:27,470 | 10.196.1.106 |         240054
                                       Seeking to partition beginning in data file | 20:31:27,470 | 10.196.1.106 |         240093
                                        Merging data from memtables and 1 sstables | 20:31:27,470 | 10.196.1.106 |         240585
                                                Read 1 live and 0 tombstoned cells | 20:31:27,471 | 10.196.1.106 |         240885
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,471 | 10.196.1.106 |         240993
                                                      Acquiring sstable references | 20:31:27,471 | 10.196.1.106 |         240997
                                                       Merging memtable tombstones | 20:31:27,471 | 10.196.1.106 |         241003
                                                       Key cache hit for sstable 3 | 20:31:27,471 | 10.196.1.106 |         241011
                                 Seeking to partition indexed section in data file | 20:31:27,471 | 10.196.1.106 |         241013
                                                       Key cache hit for sstable 1 | 20:31:27,471 | 10.196.1.106 |         241019
                                 Seeking to partition indexed section in data file | 20:31:27,471 | 10.196.1.106 |         241020
                                        Merging data from memtables and 2 sstables | 20:31:27,471 | 10.196.1.106 |         241028
                                                Read 3 live and 0 tombstoned cells | 20:31:27,479 | 10.196.1.106 |         249225
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,479 | 10.196.1.106 |         249265
                                                      Acquiring sstable references | 20:31:27,479 | 10.196.1.106 |         249267
                                                       Merging memtable tombstones | 20:31:27,479 | 10.196.1.106 |         249273
                                            Bloom filter allows skipping sstable 3 | 20:31:27,479 | 10.196.1.106 |         249279
                                            Bloom filter allows skipping sstable 2 | 20:31:27,479 | 10.196.1.106 |         249282
                                                       Key cache hit for sstable 1 | 20:31:27,479 | 10.196.1.106 |         249287
                                       Seeking to partition beginning in data file | 20:31:27,479 | 10.196.1.106 |         249289
                                        Merging data from memtables and 1 sstables | 20:31:27,479 | 10.196.1.106 |         249296
                                                Read 1 live and 0 tombstoned cells | 20:31:27,479 | 10.196.1.106 |         249558
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,480 | 10.196.1.106 |         249655
                                                      Acquiring sstable references | 20:31:27,480 | 10.196.1.106 |         249658
                                                       Merging memtable tombstones | 20:31:27,480 | 10.196.1.106 |         249755
                                                       Key cache hit for sstable 3 | 20:31:27,480 | 10.196.1.106 |         249763
                                 Seeking to partition indexed section in data file | 20:31:27,480 | 10.196.1.106 |         249765
                                                       Key cache hit for sstable 1 | 20:31:27,480 | 10.196.1.106 |         249772
                                 Seeking to partition indexed section in data file | 20:31:27,480 | 10.196.1.106 |         249774
                                        Merging data from memtables and 2 sstables | 20:31:27,480 | 10.196.1.106 |         249778
                                                Read 3 live and 0 tombstoned cells | 20:31:27,487 | 10.196.1.106 |         257542
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,487 | 10.196.1.106 |         257575
                                                      Acquiring sstable references | 20:31:27,488 | 10.196.1.106 |         257610
                                                       Merging memtable tombstones | 20:31:27,488 | 10.196.1.106 |         257616
                                            Bloom filter allows skipping sstable 3 | 20:31:27,488 | 10.196.1.106 |         257622
                                            Bloom filter allows skipping sstable 2 | 20:31:27,488 | 10.196.1.106 |         257625
                                                       Key cache hit for sstable 1 | 20:31:27,488 | 10.196.1.106 |         257631
                                       Seeking to partition beginning in data file | 20:31:27,488 | 10.196.1.106 |         257633
                                        Merging data from memtables and 1 sstables | 20:31:27,488 | 10.196.1.106 |         258473
                                                Read 1 live and 0 tombstoned cells | 20:31:27,489 | 10.196.1.106 |         259067
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,489 | 10.196.1.106 |         259197
                                                      Acquiring sstable references | 20:31:27,489 | 10.196.1.106 |         259200
                                                       Merging memtable tombstones | 20:31:27,489 | 10.196.1.106 |         259206
                                                       Key cache hit for sstable 3 | 20:31:27,489 | 10.196.1.106 |         259214
                                 Seeking to partition indexed section in data file | 20:31:27,489 | 10.196.1.106 |         259216
                                                       Key cache hit for sstable 1 | 20:31:27,489 | 10.196.1.106 |         259222
                                 Seeking to partition indexed section in data file | 20:31:27,489 | 10.196.1.106 |         259224
                                        Merging data from memtables and 2 sstables | 20:31:27,489 | 10.196.1.106 |         259319
                                                Read 3 live and 0 tombstoned cells | 20:31:27,497 | 10.196.1.106 |         267071
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,497 | 10.196.1.106 |         267135
                                                      Acquiring sstable references | 20:31:27,497 | 10.196.1.106 |         267137
                                                       Merging memtable tombstones | 20:31:27,497 | 10.196.1.106 |         267143
                                                       Key cache hit for sstable 3 | 20:31:27,497 | 10.196.1.106 |         267151
                                       Seeking to partition beginning in data file | 20:31:27,497 | 10.196.1.106 |         267154
                                            Bloom filter allows skipping sstable 2 | 20:31:27,498 | 10.196.1.106 |         267943
                                            Bloom filter allows skipping sstable 1 | 20:31:27,498 | 10.196.1.106 |         267957
                                        Merging data from memtables and 1 sstables | 20:31:27,498 | 10.196.1.106 |         267960
                                                Read 1 live and 0 tombstoned cells | 20:31:27,498 | 10.196.1.106 |         268176
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,498 | 10.196.1.106 |         268271
                                                      Acquiring sstable references | 20:31:27,498 | 10.196.1.106 |         268274
                                                       Merging memtable tombstones | 20:31:27,498 | 10.196.1.106 |         268280
                                                       Key cache hit for sstable 3 | 20:31:27,498 | 
...
{code}","Ubuntu, Single Node",,,,,,,,,,,,,,,,,,,,,,,05/Sep/13 22:05;alexliu68;5975-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12601701/5975-1.2-branch.txt,09/Sep/13 18:22;iamaleksey;5975-v2-extra-tracing.txt;https://issues.apache.org/jira/secure/attachment/12602178/5975-v2-extra-tracing.txt,06/Sep/13 23:57;iamaleksey;5975-v2.txt;https://issues.apache.org/jira/secure/attachment/12601927/5975-v2.txt,06/Sep/13 21:29;iamaleksey;5975-v2.txt;https://issues.apache.org/jira/secure/attachment/12601910/5975-v2.txt,03/Sep/13 22:54;rspitzer;create_data.py;https://issues.apache.org/jira/secure/attachment/12601259/create_data.py,08/Sep/13 00:36;alexliu68;patch.txt;https://issues.apache.org/jira/secure/attachment/12602024/patch.txt,03/Sep/13 22:54;rspitzer;trace.log;https://issues.apache.org/jira/secure/attachment/12601260/trace.log,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,2013-09-05 18:54:27.728,,,no_permission,,,,,,,,,,,,346718,,,Wed Jan 15 17:44:31 UTC 2014,,,,,,0|i1nsdj:,347019,1.2.6,1.2.9,,,,,,slebresne,slebresne,,,,,,,,,,05/Sep/13 18:54;alexliu68;It does return the result if we increase the timeout to big enough.,"05/Sep/13 20:34;alexliu68;Paging need be done for secondary index search with Limit clause. It retrieves all matched rows under the hood. If there are large number of matched rows (e.g. 30,000), it's time out.",05/Sep/13 22:01;alexliu68;5975-1.2-branch.txt patch is attached,"06/Sep/13 15:24;jjordan;Don't forget about:
{quote}
 // We shouldn't fetch only 1 row as this provides buggy paging in case the first row doesn't satisfy all clauses
{quote}

You don't want to strictly enforce the limit.",06/Sep/13 17:07;alexliu68;Can you provide a sample testing case to reproduce the issue? I can't reproduce it.,"06/Sep/13 17:18;jjordan;I think it would be anything that needs ""allow filtering"", such that results from the 2i lookup might be thrown out later...","06/Sep/13 17:38;alexliu68;Right, it needs ""allow filtering"" if there are more than one indexed columns in the query. But do you have any testing data to support your case. I tried a few ""allow filtering"" queries and all worked for me.","06/Sep/13 17:47;jjordan;I was just reading the comments in the code, which make sense.  If you do a limit X on something that then check another column through filters, if the index check only returns X rows, and the filter throws them all away, you will get 0 rows.","06/Sep/13 18:12;alexliu68;Based on tracing, I see it checks the first indexed column then do a scanning even though there is another indexed column until it hits the limit. The limit is on the number of returning Cql rows instead of a column. I am not sure the case you mentioned above can be reproduced or not, as along as my testing I can't reproduce it. ","06/Sep/13 21:27;iamaleksey;There are two issues here.

1. https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/db/ColumnFamilyStore.java#L1530 will never actually do anything, because filter.lastCounted(data) will always return 0, since CompositesSearcher does not use the original filter at all (except to get limit, which never gets updated), so the condition at https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/db/ColumnFamilyStore.java#L1500 is equivalent to {monospaced}while (rowIterator.hasNext()){monospaced}.

2. https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/db/index/composites/CompositesSearcher.java#L198 always resets columnsCount to 0, so the checks like https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/db/index/composites/CompositesSearcher.java#L205 and https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/db/index/composites/CompositesSearcher.java#L250 are basically useless, too.

with 1 and 2 there is nothing to stop the iteration, so it goes on until it exhausts all the matching indexed entries. The easiest fix seems to be to move limit and count one level up and treat the limit as immutable, and deal with it entirely in CompositesSearcher. Attaching a v2, with some unrelated minor cleanups that get rid of all the warnings, while at it.","06/Sep/13 22:27;alexliu68;I tested the version 2 patch, it doesn't fix the issue. It still does the full scan even limit 1

{code}
qlsh:ks> select * from cf_300000_keys_50_cols  where color = 'red' limit 1;                                    

 key      | color | column_0 | qty | size  | time
----------+-------+----------+-----+-------+--------------------------
 key_0002 |   red | column_0 | 124 | large | 2000-01-04 14:19:59-0500


Tracing session: 5182ef70-1743-11e3-a0e2-fb4a940b08ec

 activity                                                                          | timestamp    | source    | source_elapsed
-----------------------------------------------------------------------------------+--------------+-----------+----------------
                                                                execute_cql3_query | 18:26:04,904 | 127.0.0.1 |              0
        Parsing select * from cf_300000_keys_50_cols  where color = 'red' limit 1; | 18:26:04,904 | 127.0.0.1 |            101
                                                                Peparing statement | 18:26:04,904 | 127.0.0.1 |            500
                                                     Determining replicas to query | 18:26:04,905 | 127.0.0.1 |            966
 Executing indexed scan for [min(-9223372036854775808), min(-9223372036854775808)] | 18:26:04,906 | 127.0.0.1 |           1714
                  Executing single-partition query on cf_300000_keys_50_cols.color | 18:26:04,906 | 127.0.0.1 |           1962
                                                      Acquiring sstable references | 18:26:04,906 | 127.0.0.1 |           1987
                                                       Merging memtable tombstones | 18:26:04,906 | 127.0.0.1 |           2058
                                                       Key cache hit for sstable 5 | 18:26:04,906 | 127.0.0.1 |           2194
                                       Seeking to partition beginning in data file | 18:26:04,906 | 127.0.0.1 |           2232
                                        Merging data from memtables and 1 sstables | 18:26:04,906 | 127.0.0.1 |           2320
                                                Read 3 live and 0 tombstoned cells | 18:26:04,906 | 127.0.0.1 |           2466
                        Executing single-partition query on cf_300000_keys_50_cols | 18:26:04,907 | 127.0.0.1 |           2719
                                                      Acquiring sstable references | 18:26:04,907 | 127.0.0.1 |           2741
                                                       Merging memtable tombstones | 18:26:04,907 | 127.0.0.1 |           2819
                                                       Key cache hit for sstable 5 | 18:26:04,907 | 127.0.0.1 |           2950
                                       Seeking to partition beginning in data file | 18:26:04,907 | 127.0.0.1 |           2972
                                        Merging data from memtables and 1 sstables | 18:26:04,907 | 127.0.0.1 |           3040
                                                Read 1 live and 0 tombstoned cells | 18:26:04,907 | 127.0.0.1 |           3218
                        Executing single-partition query on cf_300000_keys_50_cols | 18:26:04,907 | 127.0.0.1 |           3545
                                                      Acquiring sstable references | 18:26:04,907 | 127.0.0.1 |           3563
                                                       Merging memtable tombstones | 18:26:04,908 | 127.0.0.1 |           3613
                                                       Key cache hit for sstable 5 | 18:26:04,908 | 127.0.0.1 |           3675
                                       Seeking to partition beginning in data file | 18:26:04,908 | 127.0.0.1 |           3688
                                        Merging data from memtables and 1 sstables | 18:26:04,908 | 127.0.0.1 |           3735
                                                Read 1 live and 0 tombstoned cells | 18:26:04,908 | 127.0.0.1 |           3874
                  Executing single-partition query on cf_300000_keys_50_cols.color | 18:26:04,908 | 127.0.0.1 |           4080
                                                      Acquiring sstable references | 18:26:04,908 | 127.0.0.1 |           4098
                                                       Merging memtable tombstones | 18:26:04,908 | 127.0.0.1 |           4143
                                                       Key cache hit for sstable 5 | 18:26:04,908 | 127.0.0.1 |           4211
                                 Seeking to partition indexed section in data file | 18:26:04,908 | 127.0.0.1 |           4234
                                        Merging data from memtables and 1 sstables | 18:26:04,908 | 127.0.0.1 |           4344
                                                Read 2 live and 0 tombstoned cells | 18:26:04,908 | 127.0.0.1 |           4406
                        Executing single-partition query on cf_300000_keys_50_cols | 18:26:04,909 | 127.0.0.1 |           4677
                                                      Acquiring sstable references | 18:26:04,909 | 127.0.0.1 |           4695
                                                       Merging memtable tombstones | 18:26:04,909 | 127.0.0.1 |           4751
                                                       Key cache hit for sstable 5 | 18:26:04,909 | 127.0.0.1 |           4816
                                       Seeking to partition beginning in data file | 18:26:04,909 | 127.0.0.1 |           4833
                                        Merging data from memtables and 1 sstables | 18:26:04,909 | 127.0.0.1 |           4869
                                                Read 1 live and 0 tombstoned cells | 18:26:04,909 | 127.0.0.1 |           5089
                  Executing single-partition query on cf_300000_keys_50_cols.color | 18:26:04,909 | 127.0.0.1 |           5276
                                                      Acquiring sstable references | 18:26:04,909 | 127.0.0.1 |           5290
                                                       Merging memtable tombstones | 18:26:04,909 | 127.0.0.1 |           5329
                                                       Key cache hit for sstable 5 | 18:26:04,909 | 127.0.0.1 |           5396
                                 Seeking to partition indexed section in data file | 18:26:04,909 | 127.0.0.1 |           5415
                                        Merging data from memtables and 1 sstables | 18:26:04,909 | 127.0.0.1 |           5541
                                                Read 1 live and 0 tombstoned cells | 18:26:04,910 | 127.0.0.1 |           5612
                                                      Scanned 3 rows and matched 3 | 18:26:04,910 | 127.0.0.1 |           5738
                                                                  Request complete | 18:26:04,910 | 127.0.0.1 |           6157

{code}","06/Sep/13 22:29;alexliu68;Cql query is a little different from thrift query, so the version 2 fix doesn't deal with the special case for cql query",06/Sep/13 23:18;iamaleksey;How is that a full scan? Are you sure you've pasted the right trace? (there is no way the query would complete in 6ms if this were a full scan).,"06/Sep/13 23:40;alexliu68;My testing only has three records with red color, and you see the tracing check three times the index even though we limit it to 1",06/Sep/13 23:43;alexliu68;You can also test it out with the attached data script.,"06/Sep/13 23:49;iamaleksey;Sorry, but 3 is not sufficient. Try with at least tens.","06/Sep/13 23:59;iamaleksey;Updated the v2 slightly to use >= instead of > in ""columnsCount > limit"", to make one redundant query less.",07/Sep/13 00:06;alexliu68;It's still the same. you can test your patch following the attached data script.,"07/Sep/13 00:18;iamaleksey;bq. It's still the same. you can test your patch following the attached data script.

Did so, obviously, with the original {noformat}python create_data.py --num-keys 300000 --num-columns 50 --keyspace 'ks' --columnfamily cf_300000_keys_50_cols --create-index y -v 3{noformat} and {noformat}select * from cf_300000_keys_50_cols where color = 'green' limit 1;{noformat}

Make sure you've got the patch applied and try that with tracing, too.","07/Sep/13 00:36;alexliu68;It's still the same for me. I am not sure how you set it up and test it. If it passes your testing, you can commit it. ",07/Sep/13 00:37;alexliu68;Can you post your tracing log?,"07/Sep/13 00:39;alexliu68;you need check how many times it 

{code}
  Executing single-partition query on cf_300000_keys_50_cols.color
{code}

if you understand the issue.","07/Sep/13 00:59;alexliu68;The correct behavior is that there is only one  ""Executing single-partition query on cf_300000_keys_50_cols.color"" in the tracing log for your test case.","07/Sep/13 08:56;iamaleksey;{noformat}
 activity                                                                          | timestamp    | source    | source_elapsed
-----------------------------------------------------------------------------------+--------------+-----------+----------------
                                                                execute_cql3_query | 11:55:57,938 | 127.0.0.1 |              0
       Parsing select * from cf_300000_keys_50_cols where color = 'green' limit 1; | 11:55:57,938 | 127.0.0.1 |             48
                                                                Peparing statement | 11:55:57,938 | 127.0.0.1 |            325
                                                     Determining replicas to query | 11:55:57,939 | 127.0.0.1 |            610
 Executing indexed scan for [min(-9223372036854775808), min(-9223372036854775808)] | 11:55:57,939 | 127.0.0.1 |           1276
                  Executing single-partition query on cf_300000_keys_50_cols.color | 11:55:57,940 | 127.0.0.1 |           1625
                                                      Acquiring sstable references | 11:55:57,940 | 127.0.0.1 |           1640
                                                       Merging memtable tombstones | 11:55:57,940 | 127.0.0.1 |           1696
                                                       Key cache hit for sstable 7 | 11:55:57,940 | 127.0.0.1 |           1785
                                       Seeking to partition beginning in data file | 11:55:57,940 | 127.0.0.1 |           1799
                                            Bloom filter allows skipping sstable 6 | 11:55:57,940 | 127.0.0.1 |           1855
                                                       Key cache hit for sstable 5 | 11:55:57,940 | 127.0.0.1 |           1901
                                       Seeking to partition beginning in data file | 11:55:57,940 | 127.0.0.1 |           1914
                                        Merging data from memtables and 2 sstables | 11:55:57,940 | 127.0.0.1 |           1954
                                                Read 3 live and 0 tombstoned cells | 11:55:57,940 | 127.0.0.1 |           2148
                        Executing single-partition query on cf_300000_keys_50_cols | 11:55:57,940 | 127.0.0.1 |           2295
                                                      Acquiring sstable references | 11:55:57,940 | 127.0.0.1 |           2308
                                                       Merging memtable tombstones | 11:55:57,940 | 127.0.0.1 |           2352
                                                       Key cache hit for sstable 7 | 11:55:57,940 | 127.0.0.1 |           2421
                                       Seeking to partition beginning in data file | 11:55:57,940 | 127.0.0.1 |           2434
                                            Bloom filter allows skipping sstable 6 | 11:55:57,940 | 127.0.0.1 |           2480
                                            Bloom filter allows skipping sstable 1 | 11:55:57,940 | 127.0.0.1 |           2511
                                        Merging data from memtables and 1 sstables | 11:55:57,940 | 127.0.0.1 |           2531
                                                Read 1 live and 0 tombstoned cells | 11:55:57,942 | 127.0.0.1 |           4260
                                                      Scanned 1 rows and matched 1 | 11:55:57,942 | 127.0.0.1 |           4492
                                                                  Request complete | 11:55:57,944 | 127.0.0.1 |           6000
{noformat}","08/Sep/13 00:39;alexliu68;I attach the patch I applied as patch.txt which is same as 5975-v2.txt.

The following is testing on my testing data

{code}
cqlsh:ks> select * from cf_300000_keys_50_cols where color='red' limit 1;

 key      | color | column_0 | qty | size  | time
----------+-------+----------+-----+-------+--------------------------
 key_0002 |   red | column_0 | 124 | large | 2000-01-04 14:19:59-0500


Tracing session: cef56100-181e-11e3-91e4-fb4a940b08ec

 activity                                                                          | timestamp    | source    | source_elapsed
-----------------------------------------------------------------------------------+--------------+-----------+----------------
                                                                execute_cql3_query | 20:37:15,154 | 127.0.0.1 |              0
           Parsing select * from cf_300000_keys_50_cols where color='red' limit 1; | 20:37:15,155 | 127.0.0.1 |            918
                                                                Peparing statement | 20:37:15,156 | 127.0.0.1 |           2176
                                                     Determining replicas to query | 20:37:15,159 | 127.0.0.1 |           5555
 Executing indexed scan for [min(-9223372036854775808), min(-9223372036854775808)] | 20:37:15,161 | 127.0.0.1 |           7353
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:37:15,162 | 127.0.0.1 |           7914
                                                      Acquiring sstable references | 20:37:15,162 | 127.0.0.1 |           7925
                                                       Merging memtable tombstones | 20:37:15,162 | 127.0.0.1 |           7973
                                                       Key cache hit for sstable 5 | 20:37:15,162 | 127.0.0.1 |           8055
                                       Seeking to partition beginning in data file | 20:37:15,162 | 127.0.0.1 |           8068
                                        Merging data from memtables and 1 sstables | 20:37:15,162 | 127.0.0.1 |           8334
                                                Read 3 live and 0 tombstoned cells | 20:37:15,162 | 127.0.0.1 |           8429
                        Executing single-partition query on cf_300000_keys_50_cols | 20:37:15,163 | 127.0.0.1 |           8734
                                                      Acquiring sstable references | 20:37:15,163 | 127.0.0.1 |           8745
                                                       Merging memtable tombstones | 20:37:15,163 | 127.0.0.1 |           8800
                                                       Key cache hit for sstable 5 | 20:37:15,163 | 127.0.0.1 |           8851
                                       Seeking to partition beginning in data file | 20:37:15,163 | 127.0.0.1 |           8860
                                        Merging data from memtables and 1 sstables | 20:37:15,163 | 127.0.0.1 |           9042
                                                Read 1 live and 0 tombstoned cells | 20:37:15,163 | 127.0.0.1 |           9124
                        Executing single-partition query on cf_300000_keys_50_cols | 20:37:15,163 | 127.0.0.1 |           9406
                                                      Acquiring sstable references | 20:37:15,163 | 127.0.0.1 |           9416
                                                       Merging memtable tombstones | 20:37:15,163 | 127.0.0.1 |           9443
                                                       Key cache hit for sstable 5 | 20:37:15,163 | 127.0.0.1 |           9487
                                       Seeking to partition beginning in data file | 20:37:15,163 | 127.0.0.1 |           9496
                                        Merging data from memtables and 1 sstables | 20:37:15,163 | 127.0.0.1 |           9522
                                                Read 1 live and 0 tombstoned cells | 20:37:15,163 | 127.0.0.1 |           9591
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:37:15,164 | 127.0.0.1 |           9689
                                                      Acquiring sstable references | 20:37:15,164 | 127.0.0.1 |           9705
                                                       Merging memtable tombstones | 20:37:15,164 | 127.0.0.1 |           9733
                                                       Key cache hit for sstable 5 | 20:37:15,164 | 127.0.0.1 |           9776
                                 Seeking to partition indexed section in data file | 20:37:15,164 | 127.0.0.1 |           9790
                                        Merging data from memtables and 1 sstables | 20:37:15,164 | 127.0.0.1 |           9868
                                                Read 2 live and 0 tombstoned cells | 20:37:15,164 | 127.0.0.1 |           9950
                        Executing single-partition query on cf_300000_keys_50_cols | 20:37:15,164 | 127.0.0.1 |          10095
                                                      Acquiring sstable references | 20:37:15,164 | 127.0.0.1 |          10104
                                                       Merging memtable tombstones | 20:37:15,164 | 127.0.0.1 |          10138
                                                       Key cache hit for sstable 5 | 20:37:15,164 | 127.0.0.1 |          10200
                                       Seeking to partition beginning in data file | 20:37:15,164 | 127.0.0.1 |          10209
                                        Merging data from memtables and 1 sstables | 20:37:15,164 | 127.0.0.1 |          10231
                                                Read 1 live and 0 tombstoned cells | 20:37:15,164 | 127.0.0.1 |          10305
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:37:15,164 | 127.0.0.1 |          10395
                                                      Acquiring sstable references | 20:37:15,164 | 127.0.0.1 |          10404
                                                       Merging memtable tombstones | 20:37:15,164 | 127.0.0.1 |          10429
                                                       Key cache hit for sstable 5 | 20:37:15,164 | 127.0.0.1 |          10471
                                 Seeking to partition indexed section in data file | 20:37:15,164 | 127.0.0.1 |          10479
                                        Merging data from memtables and 1 sstables | 20:37:15,164 | 127.0.0.1 |          10546
                                                Read 1 live and 0 tombstoned cells | 20:37:15,164 | 127.0.0.1 |          10594
                                                      Scanned 3 rows and matched 3 | 20:37:15,165 | 127.0.0.1 |          10713
                                                                  Request complete | 20:37:15,165 | 127.0.0.1 |          11030

cqlsh:ks> select * from cf_300000_keys_50_cols;                          

 key      | color | column_0  | qty | size  | time
----------+-------+-----------+-----+-------+--------------------------
 key_0002 |   red |  column_0 | 124 | large | 2000-01-04 14:19:59-0500
 key_0003 |   red |  column_0 | 125 | small |                     null
 key_0001 |   red | column__0 | 123 | small | 2000-01-04 14:19:59-0500
 key_0006 |  blue |  column_1 | 111 | small |                     null
 key_0005 |  blue |  column_1 | 111 | small |                     null


Tracing session: d35538b0-181e-11e3-91e4-fb4a940b08ec

 activity                                                                                        | timestamp    | source    | source_elapsed
-------------------------------------------------------------------------------------------------+--------------+-----------+----------------
                                                                              execute_cql3_query | 20:37:22,492 | 127.0.0.1 |              0
                                       Parsing select * from cf_300000_keys_50_cols LIMIT 10000; | 20:37:22,492 | 127.0.0.1 |             81
                                                                              Peparing statement | 20:37:22,492 | 127.0.0.1 |            322
                                                                   Determining replicas to query | 20:37:22,492 | 127.0.0.1 |            551
 Executing seq scan across 2 sstables for [min(-9223372036854775808), min(-9223372036854775808)] | 20:37:22,493 | 127.0.0.1 |            950
                                                                    Scanned 5 rows and matched 5 | 20:37:22,518 | 127.0.0.1 |          26549
                                                                                Request complete | 20:37:22,519 | 127.0.0.1 |          27123
{code}

It's broken.",08/Sep/13 00:39;alexliu68;It's easy to reproduce it using a smaller set of data.,"08/Sep/13 00:45;alexliu68;the testing table is 
{code}
qlsh:ks> describe table cf_300000_keys_50_cols;

CREATE TABLE cf_300000_keys_50_cols (
  key text PRIMARY KEY,
  color text,
  column_0 text,
  qty int,
  size text,
  time timestamp
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};

CREATE INDEX color ON cf_300000_keys_50_cols (color);

CREATE INDEX qty ON cf_300000_keys_50_cols (qty);

CREATE INDEX size ON cf_300000_keys_50_cols (size);

CREATE INDEX time ON cf_300000_keys_50_cols (time);
{code}","08/Sep/13 00:52;iamaleksey;I believe that that's what you are seeing, and at this point I'm really curious how it's possible that you see those results.

For the record, using a similar dataset to your last one:

{noformat}
cqlsh:ks> select * from cf_300000_keys_50_cols ;

 key   | color | column_0 | qty | size | time
-------+-------+----------+-----+------+--------------------------
 key_1 |   red |  value_0 |   0 |    P | 2000-01-01 00:00:01+0300
 key_3 |   red |  value_0 |   2 |    P | 2000-01-01 00:00:03+0300
 key_4 |   red |  value_0 |   3 |    P | 2000-01-01 00:00:04+0300
 key_2 |  blue |  value_0 |   1 |    P | 2000-01-01 00:00:02+0300
 key_5 |  blue |  value_0 |   4 |    P | 2000-01-01 00:00:05+0300

cqlsh:ks> select * FROM cf_300000_keys_50_cols where color = 'red' limit 1;

 key   | color | column_0 | qty | size | time
-------+-------+----------+-----+------+--------------------------
 key_1 |   red |  value_0 |   0 |    P | 2000-01-01 00:00:01+0300


Tracing session: 7913aec0-1820-11e3-a85f-070c076eda0a

 activity                                                                          | timestamp    | source    | source_elapsed
-----------------------------------------------------------------------------------+--------------+-----------+----------------
                                                                execute_cql3_query | 03:49:10,060 | 127.0.0.1 |              0
         Parsing select * FROM cf_300000_keys_50_cols where color = 'red' limit 1; | 03:49:10,060 | 127.0.0.1 |             58
                                                                Peparing statement | 03:49:10,061 | 127.0.0.1 |            289
                                                     Determining replicas to query | 03:49:10,061 | 127.0.0.1 |            485
 Executing indexed scan for [min(-9223372036854775808), min(-9223372036854775808)] | 03:49:10,061 | 127.0.0.1 |           1077
                  Executing single-partition query on cf_300000_keys_50_cols.color | 03:49:10,062 | 127.0.0.1 |           1175
                                                      Acquiring sstable references | 03:49:10,062 | 127.0.0.1 |           1190
                                                       Merging memtable tombstones | 03:49:10,062 | 127.0.0.1 |           1221
                                        Merging data from memtables and 0 sstables | 03:49:10,062 | 127.0.0.1 |           1273
                                                Read 3 live and 0 tombstoned cells | 03:49:10,062 | 127.0.0.1 |           1431
                        Executing single-partition query on cf_300000_keys_50_cols | 03:49:10,062 | 127.0.0.1 |           1577
                                                      Acquiring sstable references | 03:49:10,062 | 127.0.0.1 |           1587
                                                       Merging memtable tombstones | 03:49:10,062 | 127.0.0.1 |           1610
                                        Merging data from memtables and 0 sstables | 03:49:10,062 | 127.0.0.1 |           1636
                                                Read 1 live and 0 tombstoned cells | 03:49:10,062 | 127.0.0.1 |           1697
                                                      Scanned 1 rows and matched 1 | 03:49:10,062 | 127.0.0.1 |           1812
                                                                  Request complete | 03:49:10,062 | 127.0.0.1 |           2031
{noformat}",08/Sep/13 02:04;alexliu68;We need a third person to do the final testing as we get different results for the same patch.,"08/Sep/13 03:14;iamaleksey;It's just performing the cleanup of stale data. See CASSANDRA-2897. What you see in your tracing output is that CompositesSearcher is paging through the 2i partition, fetching rows from the base table, seeing stale entries, and moving on to the next index cell, until it finds enough non-stale base rows to satisfy the limit or exhausts all the cells in the 2i partition.","08/Sep/13 10:15;jbellis;So if Alex forces a major compaction he should see what you're seeing unless there's a bug in our indexing code, right?",08/Sep/13 10:17;jbellis;(Keeping in mind that CASSANDRA-5614 has only been fixed in the unreleased 2.0.1.),08/Sep/13 13:32;iamaleksey;Or if he runs the same query one more time (with the same or lower limit). Stale entries will be cleaned up during the first execution. (Assuming no updates/deletes between the two SELECTs).,"09/Sep/13 17:59;alexliu68;I run the same query with limit 1 for a few times, and I get the same results.","09/Sep/13 19:00;alexliu68;The patch is good on applied to Cassandra-1.2 branch. I had applied it to 1.2.6 which tells the difference between my testing and [~iamaleksey]'s

Some patches between 1.2.6 and 1.2.9 fix the issues that countCQL3Rows is somehow not set to true in the ExtendedFilter","10/Sep/13 11:59;slebresne;The most recently attached 5975-v2.txt lgtm, +1.","10/Sep/13 14:59;iamaleksey;Committed, thanks.",16/Sep/13 13:44;petter;I volunteer to test this fix in my environment/my design prior to 2.0.1 if you merge it.,16/Sep/13 13:46;iamaleksey;[~petter] It's already in 1.2.10 and 2.0.1,17/Sep/13 08:08;petter;[~iamaleksey] Oh. Sorry. Shouldn't 2.0.1 be added to this issues Fix Version/s field then and be part of the release notes? Maybe I'm jumping ahead of things here. I'll check out the code and have a go.,17/Sep/13 12:19;jbellis;Tagged 2.0.1.,19/Sep/13 14:04;petter;The problem I was seeing (using secondary indexes on fields part of a compound primary key) seems to be solved with this commit. Good work!,15/Jan/14 16:36;srrepaka;I think we have the similar issue on 1.2.5. Can a patch be applied to 1.2.5? Appreciate any response. thanks.,15/Jan/14 17:44;jbellis;You can try to apply the patch as easily as we can.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitLogDescriptor missing version 21,CASSANDRA-5984,12667450,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,cmolter,cmolter,07/Sep/13 14:29,12/Mar/19 14:17,13/Mar/19 22:29,08/Sep/13 17:24,,,,,,,0,,,,,"when running `ant clean test` I get:

    [junit] Testcase: testVersions(org.apache.cassandra.db.CommitLogTest):      FAILED
    [junit] expected:<8> but was:<7>
    [junit] junit.framework.AssertionFailedError: expected:<8> but was:<7>
    [junit]     at org.apache.cassandra.db.CommitLogTest.testVersions(CommitLogTest.java:224)

this seems to be caused by revision 14da6bca (CASSANDRA-5887) which introduced a new version for MessagingService without updating the versioning in CommitLogDescription
","MAC OSX 10.8
java 1.7.0_25
Trunk",,,,,,,,,,,,,,,,,,,,,,,07/Sep/13 14:45;cmolter;CASSANDRA-5984.patch;https://issues.apache.org/jira/secure/attachment/12601980/CASSANDRA-5984.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-08 17:24:04.796,,,no_permission,,,,,,,,,,,,347387,,,Sun Sep 08 17:24:04 UTC 2013,,,,,,0|i1nwhj:,347686,,,,,,,,krummas,krummas,,,,,,,,,,07/Sep/13 14:45;cmolter;Adding VERSION_21 to CommitLogDescriptor fix this issue.,"08/Sep/13 17:24;krummas;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make hint TTL customizable,CASSANDRA-5988,12667676,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,vkasar,okibirev,okibirev,09/Sep/13 21:45,12/Mar/19 14:16,13/Mar/19 22:29,28/Oct/13 17:02,1.2.12,2.0.3,,,,,0,patch,,,,"Currently time to live for stored hints is hardcoded to be gc_grace_seconds. This causes problems for applications using backdated deletes as a form of optimistic locking. Hints for updates made to the same data on which delete was attempted can persist for days, making it impossible to determine if delete succeeded by doing read(ALL) after a reasonable delay. We need a way to explicitly configure hint TTL, either through schema parameter or through a yaml file.

",,,,,,,,,,,,,,,,,,,,,,,,21/Oct/13 21:32;vkasar;5988.txt;https://issues.apache.org/jira/secure/attachment/12609531/5988.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-09 22:45:31.411,,,no_permission,,,,,,,,,,,,347613,,,Thu Oct 13 17:19:59 UTC 2016,,,,,,0|i1nxvr:,347912,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,brandon.williams,09/Sep/13 22:45;kohlisankalp;We also want to make sure that hint TTL is less than gc grace period. ,12/Sep/13 18:11;brandon.williams;So this would be per-cf? That's how hints are TTL'd currently.,"12/Sep/13 18:15;okibirev;That would be ideal, yes. ","12/Sep/13 18:27;jbellis;Hint TTL will be exactly gc grace for the reasons documented in CASSANDRA-5314.

""using backdated deletes as a form of optimistic locking"" sounds like a bad idea, would you care to defend it? :)","12/Sep/13 18:44;okibirev;Referencing the above JIRA, there is no harm in having smaller hint TTL.

As for optimistic locking, the only other form (CAS) is not available until cassandra 2.0 and even then does not provide performance for very high volume operations.

If the objective is to do a user facing operation and a background operation simultaneously and without external locking, and to have background operation lose if there is an intervening user operation, backdating the background operation by a certain amount is a good compromise between consistency and performance. This feature will safeguard against user operation failing, being only stored as a hint and then confusing the background process as to ultimate success or failure of the backdated operation.","21/Oct/13 20:35;vkasar;Index: branches/release/apache-cassandra-1.1.12/src/java/org/apache/cassandra/db/RowMutation.java
===================================================================
--- branches/release/apache-cassandra-1.1.12/src/java/org/apache/cassandra/db/RowMutation.java	(revision 105733)
+++ branches/release/apache-cassandra-1.1.12/src/java/org/apache/cassandra/db/RowMutation.java	(revision 105734)
@@ -123,6 +123,9 @@
      * }
      *
      */
+
+    static final int maxHintTTL = Integer.parseInt(System.getProperty(""ciedb.maxHintTTL"", String.valueOf(Integer.MAX_VALUE)));
+
     public static RowMutation hintFor(RowMutation mutation, ByteBuffer token) throws IOException
     {
         RowMutation rm = new RowMutation(Table.SYSTEM_TABLE, token);
@@ -131,7 +134,7 @@
         // determine the TTL for the RowMutation
         // this is set at the smallest GCGraceSeconds for any of the CFs in the RM
         // this ensures that deletes aren't ""undone"" by delivery of an old hint
-        int ttl = Integer.MAX_VALUE;
+        int ttl = maxHintTTL; //Integer.MAX_VALUE;
         for (ColumnFamily cf : mutation.getColumnFamilies())
             ttl = Math.min(ttl, cf.metadata().getGcGraceSeconds());
 
",21/Oct/13 21:33;vkasar;Attached the diff as a file 5988.txt,"28/Oct/13 16:54;brandon.williams;I couldn't get this patch to apply, but it was simple enough to just recreate, so I committed it to 1.2.  It looks like we need a separate patch for 2.0/trunk, however.",28/Oct/13 17:02;brandon.williams;Committed to 2.0/trunk.,"28/Oct/13 17:09;brandon.williams;I should note that I also changed the property name to ""cassandra.maxHintTTL""","12/Oct/16 04:12;kohlisankalp;[~iamaleksey] I could not search ""cassandra.maxHintTTL"" in 3.0.9. With new hints in 3.0, how can we change this?","12/Oct/16 10:36;iamaleksey;[~kohlisankalp] Will need to either modify {{HintsDispatcher}} logic to take 'maxhintttl' into account (compared to current time - hint's creationTime), or do the same even earlier, in {{HintsReader}}. The former is probably cleaner; the latter can be done a bit more efficiently - skipping hint body entirely if gcgs/creationTime/maxhinttl combination says the hint is basically dead.

Don't have time atm to do it, but [~bdeggleston] should be pretty familiar with that code, as he added compression logic - I can review.","13/Oct/16 00:07;kohlisankalp;Without hintTTL, if we replay data older than GC grace, that will bring back data right? If it is not there in 3.0, it should be fixed as Major if not blocker? ","13/Oct/16 15:59;iamaleksey;Nope, we are all good. We store creationTime and gcgs (at the time of hint's write), and check against current time (and current gcgs) before replaying. Essentially following the old behaviour, except even stricter (we use the min of gcgs at the time of writing the hint and the gcgs at the time of replay).

What changed is that you cannot *override* the *max* hintttl in 3.0 anymore to make it lower - or larger - than the calculated value.",13/Oct/16 17:19;kohlisankalp;Thanks [~iamaleksey]. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CAS transactions permitting multiple updates,CASSANDRA-5945,12665793,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,ppersad,ppersad,27/Aug/13 21:26,12/Mar/19 14:15,13/Mar/19 22:29,20/Nov/13 00:58,,,,,,,1,LWT,,,,"This bug is spawned off CASSANDRA-5925 to track an underlying issue not related to TTLs.  To reproduce:

Step 1:

CREATE TABLE IF NOT EXISTS tkns (tkn blob, consumed boolean, PRIMARY KEY (tkn));

Step 2:

INSERT INTO tkns (tkn, consumed) VALUES (?,FALSE);

Step 3:

UPDATE tkns SET consumed = TRUE WHERE tkn = ? IF consumed = FALSE;

Step 4:

UPDATE tkns SET consumed = TRUE WHERE tkn = ? IF consumed = FALSE;

Repeat steps 2-4 about 100,000 times.

Expectation:

For the '[applied]' column in the result sets for steps 3 and 4, exactly one should be true and one should be false.

Bug:

In a small number of cases (varying from 0.002% to 1%) both updates will report success.  See attached unit test.","3 node Cassandra 2.0.0-rc2 cluster
Java driver 1.0.2
Replication factor 3
Quorum consistency",,,,,,,,,,,,,CASSANDRA-5925,,,,,,,,,,27/Aug/13 21:27;ppersad;TokenConsumptionTest.java;https://issues.apache.org/jira/secure/attachment/12600258/TokenConsumptionTest.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-20 00:58:12.276,,,no_permission,,,,,,,,,,,,345732,,,Wed Nov 20 00:58:12 UTC 2013,,,,,,0|i1nmbb:,346033,,,,,,,,,,,,,,,,,,,20/Nov/13 00:58;jbellis;fixed in 2.0.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageProxy#cas() doesn't order columns names correctly when querying,CASSANDRA-5788,12659037,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,22/Jul/13 08:32,12/Mar/19 14:15,13/Mar/19 22:29,22/Jul/13 13:50,2.0 beta 2,,,,,,0,LWT,,,,"When querying columns for CAS, we build the SortedSet with:
{noformat}
new NamesQueryFilter(ImmutableSortedSet.copyOf(expected.getColumnNames())
{noformat}
but ImmutableSortedSet.copyOf() uses the natural order of keys unless a comparator is given, which is not what we want.",,,,,,,,,,,,,,,,,,,,,,,,22/Jul/13 08:43;slebresne;5788.txt;https://issues.apache.org/jira/secure/attachment/12593471/5788.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-22 13:41:11.215,,,no_permission,,,,,,,,,,,,339230,,,Tue Jul 23 16:07:34 UTC 2013,,,,,,0|i1midb:,339550,,,,,,,,jbellis,jbellis,,,,,,,,,,22/Jul/13 08:43;slebresne;Trivial patch attached.,22/Jul/13 13:41;jbellis;+1,"22/Jul/13 13:50;slebresne;Committed, thanks",23/Jul/13 15:51;appodictic;Can we re-open so I can put a unit test around this? I see we said this is trivial but I think a test could help prevent this from happening again.,"23/Jul/13 16:07;jbellis;We've rolled a release w/ this now so probably best to make a new ticket, sorry.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LOCAL_SERIAL doesn't work from Thrift,CASSANDRA-6584,12688865,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,nff,nff,14/Jan/14 21:55,12/Mar/19 14:15,13/Mar/19 22:29,15/Jan/14 03:24,2.0.5,,,Legacy/CQL,,,0,easyfix,LWT,,,"Calling ""cas"" from Thrift with CL.LOCAL_SERIAL fails with an AssertionError since ThriftConversion.fromThrift has no ""case"" statement for LOCAL_SERIAL.",,,,,,,,,,,,,,,,,,,,,,,,15/Jan/14 03:22;brandon.williams;6584.txt;https://issues.apache.org/jira/secure/attachment/12623049/6584.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-15 03:24:14.521,,,no_permission,,,,,,,,,,,,367832,,,Wed Jan 15 03:24:33 UTC 2014,,,,,,0|i1reh3:,368139,2.0.4,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,15/Jan/14 03:24;iamaleksey;+1,15/Jan/14 03:24;jjordan;LGTM,15/Jan/14 03:24;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
vnodes don't scale to hundreds of nodes,CASSANDRA-6127,12671666,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,tupshin,tupshin,01/Oct/13 21:19,12/Mar/19 14:15,13/Mar/19 22:29,03/Dec/13 14:39,,,,,,,0,,,,,"There are a lot of gossip-related issues related to very wide clusters that also have vnodes enabled. Let's use this ticket as a master in case there are sub-tickets.

The most obvious symptom I've seen is with 1000 nodes in EC2 with m1.xlarge instances. Each node configured with 32 vnodes.

Without vnodes, cluster spins up fine and is ready to handle requests within 30 minutes or less. 

With vnodes, nodes are reporting constant up/down flapping messages with no external load on the cluster. After a couple of hours, they were still flapping, had very high cpu load, and the cluster never looked like it was going to stabilize or be useful for traffic.",Any cluster that has vnodes and consists of hundreds of physical nodes.,,,,,,,,,,,,,CASSANDRA-6338,CASSANDRA-6345,CASSANDRA-6244,CASSANDRA-6297,CASSANDRA-4288,,,,,,07/Nov/13 15:14;qconner;2013-11-05_18-04-03_no_compression_cpu_time.png;https://issues.apache.org/jira/secure/attachment/12612626/2013-11-05_18-04-03_no_compression_cpu_time.png,07/Nov/13 15:12;qconner;2013-11-05_18-09-38_compression_on_cpu_time.png;https://issues.apache.org/jira/secure/attachment/12612625/2013-11-05_18-09-38_compression_on_cpu_time.png,24/Oct/13 16:44;qconner;6000vnodes.patch;https://issues.apache.org/jira/secure/attachment/12610105/6000vnodes.patch,24/Oct/13 16:51;qconner;AdjustableGossipPeriod.patch;https://issues.apache.org/jira/secure/attachment/12610107/AdjustableGossipPeriod.patch,12/Nov/13 21:30;qconner;cpu-vs-token-graph.png;https://issues.apache.org/jira/secure/attachment/12613430/cpu-vs-token-graph.png,24/Oct/13 17:46;qconner;delayEstimatorUntilStatisticallyValid.patch;https://issues.apache.org/jira/secure/attachment/12610117/delayEstimatorUntilStatisticallyValid.patch,12/Nov/13 21:25;qconner;flaps-vs-tokens.png;https://issues.apache.org/jira/secure/attachment/12613427/flaps-vs-tokens.png,20/Nov/13 15:21;qconner;vnodes & gossip flaps.png;https://issues.apache.org/jira/secure/attachment/12614903/vnodes+%26+gossip+flaps.png,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,2013-10-01 21:27:56.05,,,no_permission,,,,,,,,,,,,351376,,,Tue Dec 03 14:39:28 UTC 2013,,,,,,0|i1ol0v:,351668,1.2.6,1.2.9,,,,,,,,,,1.2.0 beta 1,,,,,,,"01/Oct/13 21:27;jbellis;bq. After a couple of hours, they were still flapping, had very high cpu load

To clarify, this is a bit of a mashup of multiple observations:

bq. When there was zero traffic on the cluster, we were seeing flapping without very high cpu. On smaller tests, we saw much higher cpu than expected when under load.","24/Oct/13 16:28;qconner;*Background and Reproduction*

The symptom is evident with the presence of ""is now DOWN"" messages in the Cassandra system.log file.  The recording of a node DOWN is often followed by a node UP a few seconds later.  Users have coined this phenomenon ""gossip flap"" and the occurence of ""Gossip flaps"" has a machine and a human consequence.

Humans react strongly to the (temporary) marking of a node down.  Automated monitoring may trigger SNMP traps, etc.  A ""busy"" node that doesn't transmit heartbeat gossip messages on time will be marked as ""down"" though it may still be performing useful work.

Machine reactions include other C* nodes buffering of row mutations and storage of hints on disk when another node is marked down.  I have not explored the machine reactions but imagine the endpointSnitch could also be affected from the client frame of reference.

One piece of good news is that I was able to reproduce two different use cases that elicit the ""is now DOWN"" message in Log4J log files.

Use Case #1 is as follows:
  provision 256 or 512 nodes in EC2
  install Cassandra 1.2.9
  take defaults except specify num_tokens=256 in c*.yaml
  start one node at a time

Use Case #2 is as follows:
  provision 32 nodes in EC2
  install Cassandra 1.2.9
  take defaults in c*.yaml
  configure rack & datacenter
  start one node at a time
  when all nodes are up create about 1GB of data
    e.g. ""tools/bin/cassandra-stress -c 20 -l 3 -n 1000000""
  provision a 33rdxtra node in EC2
  install Cassandra 1.2.9
  take defaults except specify num_tokens=256
  configure different datacenter than first 32 nodes
  start the node (auto_bootstrap=true)


","24/Oct/13 16:28;qconner;*Analysis*

My first experiments aimed to quantify the length of Gossip messages and determine what factors drive the message length.  I found the size of certain gossip messages increases proportionally with the number of vnodes (num_tokens in c.yaml).  I recorded message size over the num_tokens and number of nodes domains (64,128,256,512,...) for tokens and (32,64,128,256,512) for nodes.  I also made non-rigorous observation of User and Kernel CPU (Ubuntu 10.0.4 LTS).  My hunch is that both vnode count and node count have a mild effect on user CPU resource usage.

What is the rough estimate of bytes sent for certain Gossip messages and why does this matter?  The Phi Accrual Failure Detector (Hayashibara, et al) assumes fixed length heartbeat messages while Cassandra uses variable length messages.  I observed a correlation with larger messages, higher vnodes and false positive detections by the Gossip FailureDetector.  These observations, IMHO, are not explained by the research paper.  I formed a hypothesis that the false positives are due to jitter in the interval values.  I wondered if perhaps using a longer baseline to integrate over would reduce the jitter.  

I have a second theory to follow up on.  A newly added node will not have a long history of Gossip heartbeat interarrival times.  At least 40 samples are needed to compute mean, variance with any statistical significance.  It's possible the phi estimation algorithm is simply invalid for newly created nodes and that is why we see them flap shortly after creation.

In any case, the message of interest is the GossipDigestAck2 (GDA2) because it is the largest of the Gossip messages.  GDA2 contains the set of EndpointStateMaps (node metadata) for newly-discovered nodes, i.e. those nodes just added to an existing cluster.  When each node becomes aware of joining node, they Gossip it to three randomly-chosen other nodes.  The GDA2 message is tailored to contain the delta of new node metadata the receiving node is unaware of.

For a single node, the upper limit on GDA message size is roughly 3 * N * k * V
Where N is the number of nodes in the cluster,
V is the number of tokens (vnodes) per cluster,
k is a constant value, approximately 64 bytes, that represents a serialized token plus some other endpoint metadata.

If one is running hundreds of nodes in a cluster, the Gossip message traffic created when a node joins can be significant and increases with the number of nodes.  I believe this to be the first order effect and probably violates one of the assumptions of the PHI Accrual Failure Detection, that heartbeat messages are small enough not to consume a relevant amount of compute or communication resources.  The variable transmission time (due to variable length messages) is a clear violation of assumptions, if I've read the source code correctly.

On a related topic, there is a hard-coded limitation to the number of vnodes due to the serialization of the GDA messages.
No more than 1720 vnodes can be configured without creating a greater than 32K serialized String vnode message.  A patch is provided below for future use should this become an issue.

In clusters with hundreds of nodes, GDA2 messages can be 200 KB or 2 MB if many nodes join simultaneously.  This is not an issue if the computer experiences no latency from competing workloads.  In the real world, nodes are added because the cluster load has grown in terms of retained data, or in terms of a high transaction arrival rate.  This means node resources may be fully utilized when adding new nodes is typically attempted.

It occured to me that we have another use case to accomodate.  It is common to experience transient failure modes, even in modern data centers with disciplined maintenance practices.  Ethernet cables get moved, switches and routers rebooted.  BGP route errors and other temporary interruptions may occur with the network fabric in real world scenarios.  People make mistakes, plans change and preventative maintenance often causes short-lived interruptions occur with network, CPU and disk subsystems.
","24/Oct/13 16:29;qconner;*Feature Suggestion*

The current Gossip failure detector is characterized by a sliding window of elapsed time, a heartbeat message period and a PHI threshold used to make the continuous random variable (lower case phi) into a dichotomous (binary) random variable.  That PHI (uppercase) threshold is called phi_convict_threshold.

I don't have a better mathmatical theory or derivation at this writing, but I do have an easy workaround for your consideration.  While phi_convict_threshold is adjustable, the period (or frequency) of Gossip messages is not.  Adjusting the gossip period to integrate over a longer time baseline reduced false positives from the Gossip failure detector.  The side effect increases the elapsed time to detect a legitimately-failed node.

Depending on user workload characteristics, and the related sources of latency (CPU, disk and network activity or transient delays) cited above, a System Architect could present a reasonable use case for controlling the Gossip message period.

The goal would be to set a detection window that accomodates common occurences for a given deployment scenario.  Not all data centers are created equal.

Patches and results from implementation will follow in subsequent posts.

*Potential Next Steps*
  Explore concern about sensitivity to gossip period.  Do the vnode gossip messages exceed capacity for peers to ingest?
  Explore concern about phi estimates from un-filled (new) deque.  See Patch #3.
  Explore concern about assuming Gaussian PDF.  Networks (not computers) generally characterize expected arrival time by Poisson distribution, not Gaussian.
","24/Oct/13 16:44;qconner;Patch #1.  Increases number of allowed vnodes (num_tokens) from 1720 up to about 6000 with 512K stack size.

Because of CQL antlr grammar parser, stack is needed to parse token definitions.","24/Oct/13 16:51;qconner;This is patch #2.  It adds a new configuration item in cassandra.yaml, ""gossip_period"".  Set it in milliseconds.

Setting the gossip period by JMX will be lost if stop/restart gossip so needs more work.  Reading from JMX seems fine.

This also isn't DRY.  Should set the value for intervalInMillis, from config, in a static initializer.  Wasn't sure if config object is available in that scope.",24/Oct/13 17:46;qconner;Untested patch #3.  Delays output from FailureDetector until statistically valid number of samples have been obtained.,"24/Oct/13 17:49;qconner;First results with workaround patch #2.
No load.  No data.  Only system keyspace and Gossip on a 256 node m1.medium cluster in EC2.
Nodes started in rapid succession.

*phi=8, variable gossip_period*
1154 flaps for 1 sec
685 flaps for 2 sec
146 flaps for 3 sec
88 flaps for 4 sec
70 flaps for 5 sec
100 flaps for 10 sec

*phi=12*
1289 flaps for 1 sec
77 flaps for 2 sec
6 flaps for 3 sec
1 flaps for 4 sec
3 flaps for 5 sec
1 flaps for 6 sec
0 flaps for 8 sec
1 flaps for 10 sec
","24/Oct/13 18:56;brandon.williams;It would be helpful to dump the interval times for a node that is flapping (dumpInterArrivalTimes on the FD) so we can see how long the heartbeats are taking.  If some are excessively long, we need to get threads dumps/debugger timings from the gossiper to see if something is blocking it or taking a long time before changing any fundamentals (gossip interval, FD formula) that we already know work in principle without vnodes.  Increasing the payload size to >32k shouldn't cause these problems, since that is only sent during initial state synchronization and isn't all that large to begin with.",24/Oct/13 22:38;brandon.williams;Can you see if adding -Dcassandra.unsafesystem=true allows the cluster to stabilize at some point?,"25/Oct/13 12:29;cburroughs;> It would be helpful to dump the interval times for a node that is flapping (dumpInterArrivalTimes on the FD) so we can see how long the heartbeats are taking.

A per endpoint histogram of heartbeat arrival latency seems a worthwhile o.a.c.Metric to have all the time.

[~qconner]  On the topic of ""wait until there is enough data before doing stuff"" you might also be interested in the heuristic & report from CASSANDRA-4288","25/Oct/13 23:50;qconner;I grabbed some sample log files from 10 nodes of 256 in a run today.  
[flap-intervals.tar.gz|http://qconner.s3.amazonaws.com/flap-intervals.tar.gz]

Convictions are happening with only 1 to 5 intervals recorded.  Patch #3 is looking like the winner but we should do the math by hand to be sure (volunteers?).

Also, I just tested [Patch #3|https://issues.apache.org/jira/secure/attachment/12610117/delayEstimatorUntilStatisticallyValid.patch] and found 0 flaps for the same setup as yesterday (256 nodes, phi=8, normal 1000 ms gossip period).


","26/Oct/13 00:30;jbellis;Patch 1 will break things since later on we write the length of the string as two bytes.

I think we're fine with 1700 vnodes per machine TBH, although it would be better to limit that in the config instead of failing at an assert later on.","26/Oct/13 00:37;tupshin;I'd just set a max of 1024. No one could ever need more than that. (Famous
last words)

","26/Oct/13 01:22;brandon.williams;Patch #3 will make it take much longer for a rebooted node to know who's actually up or down, exacerbating CASSANDRA-4288.  I'd still like to know *why* things are taking longer with vnodes, and I'm especially hesitant to make any adjustments to the gossiper or FD since we know they work fine with single tokens, and also because they *have no knowledge about tokens*, it's just another opaque state to them.  I suspect something in StorageService is blocking the gossiper long enough to cause this, perhaps CASSANDRA-6244 or something similar.","26/Oct/13 02:46;jbellis;Couldn't we tie the thrift/native server startup to ""I have enough gossip data now?""","26/Oct/13 03:03;brandon.williams;That might confuse autodiscovery clients, at least without further changes.","28/Oct/13 12:04;cburroughs;bq.  I'd just set a max of 1024. No one could ever need more than that. (Famous last words)

Isn't that equivalent to saying no one will have a heterogeneous cluster with more  than a 1024/256 = 4 performance delta between physical nodes?  SSD vs spinny could account for more than that. ","28/Oct/13 14:16;jbellis;I'm okay with that limitation.  Intuitively it's reasonable that C* can't compensate for really ridiculous performance differences.

(Of course, you could also reduce the weak nodes below 256.)","29/Oct/13 18:17;qconner;Brandon,

You said Patch #3 will make it take much longer for a rebooted node to know who's actually up or down, exacerbating CASSANDRA-4288.  I've given this some thought and want to see if I understand your concern.

Patch #3 serves to send a zero value for phi, for newly-discovered nodes, until an accurate calculation of variance is complete.  This would be 40 seconds, applicable to new nodes only.

However (and this is what I'm looking for you to confirm) If a new node comes online, but is stopped again within 40 seconds of start-up, the FD will not ""convict"" it until the end of that 40 seconds.

I suspect this occurs less frequently than adding a node to a cluster, but probably depends on your use case (dev vs prod).

In my view, we can't escape the math, and the need to amass 40 samples.  That is why the bug exists today.  I agree we should look at tying thrift to a healthy startup as a compensating measure.

Instead of a fixed amount of time (gossip rounds), perhaps we should consider adding a hold-down timer based on a statistical measure?

This hold-down timer could be implemented for newly discovered nodes to suppress interaction until Gossip ""stabilizes"".  Just like we have a high-water mark for phi to denote failure, we could set a low-water mark and call it a trust threshold.  We wouldn't enable thrift communications to the new node until their phi value is below this low-water mark.

So the condition for ""recognizing"" a new node for thrift purposes could be two fold:
  1.  valid computation for variance (40 samples obtained in the 1000 sample window)
  2.  accurate phi value is indeed below the low-water mark","29/Oct/13 19:15;brandon.williams;Let's move that discussion to CASSANDRA-4288, since that change is orthogonal to the actual problem we have here, regardless of whether it fixes it or just papers over the problem.  What we need to do next on this ticket is either correlate a thread dump to what is burning up CPU, or attach a debugger and see where the time is being spent.","29/Oct/13 19:20;jbellis;bq. it would be better to limit that in the config instead of failing at an assert later on.

Split that out to CASSANDRA-6267.","02/Nov/13 01:07;qconner;Monday (11/4) I will be start getting the CPU profiling captured with a 256 or 512 node cluster.  Plan is to capture with internode compression and without.
I was able to get semi-reproduction this week in a 256 node cluster -- one node had twice the cpu utilization of the others (20% user versus 10% user).  But I had too much logging enabled and that skewed results.
","04/Nov/13 19:21;mstump;As another datapoint/use case create a 32 node ring with vnodes, decommission one of the nodes and observe the logs. Every node in the ring will be marked as down by the gossiper, then immediately be re-added again as up/available.","04/Nov/13 19:31;brandon.williams;bq. Every node in the ring will be marked as down by the gossiper

In which node's view? (or all of them?)","04/Nov/13 19:42;mstump;We're observing the logs of a random sample of nodes and on all nodes observed the entire ring is marked as down, so I assume it's for all nodes.",04/Nov/13 19:59;jbellis;How heavy is read/write load?,"04/Nov/13 20:10;mstump;Zero to minimal load. 177 writes/second, 0 reads against the entire ring. m2.4xlarge instances.","04/Nov/13 21:35;brandon.williams;With CASSANDRA-6244 and CASSANDRA-6297 in 1.2 head, I think we need to re-verify this is still a problem.","05/Nov/13 13:53;qconner;Good cpu profile results were obtained last night with the 1.2.9 code line.  Switching over to the cassandra-1.2 HEAD this morning for up-to-date analysis.
CPU profile of 1.2.9 showed bottleneck was computation of sum for the ArrivalWindow deque members (inter-arrival times of gossip messages).","06/Nov/13 21:00;jbellis;ISTM that FD processing Gossip updates synchronously is a fundamental problem.  Any hiccup in processing will cause FD false positives.  (And even if our own code is perfect, GC pauses can still do this to us.)

Wouldn't it be better if we:
- time heartbeats based on when they arrive instead of when Gossip processes them
- teach FD to recognize that its information is only good up to the most recently processed message -- the absence of messages after that doesn't mean everyone is down unless the Gossip stage is empty",06/Nov/13 21:10;tupshin;+1. Strongly agree with Jonathan's analysis and proposal.,"06/Nov/13 22:25;brandon.williams;At this point, I think we should:

* see if the flapping happens with vnodes in 1.2 head (maybe Quentin already knows from his last test)
* see if the flapping happens without vnodes in 1.2 head but the same number of nodes

Because if sum() in ArrivalWindow is burning the most CPU in the Gossiper task (note: not bottlenecking, each call was at most ~3ms, there were just lots of them) then that means that the problem is no longer tied to vnodes (if it ever was, since sum is per-node, not per-token) and we should probably open a new ticket (can't start a cluster of size >=X all at once, or similar) and discuss there.  We know that clusters much larger than any discussed on this ticket exist, but I don't think any of them have all rebooted at once.","07/Nov/13 15:12;qconner;Run against the following commit on cassandra-1.2 branch:
8e7d7285cdeac4f2527c933280d595bbddd26935

This profile measures CPU time, not elapsed time.

Internode compression turned on.  

256 nodes w/ num_tokens=256.  
GossipPeriod 1000 ms.
ArrivalWindow.size() = 1000.
","07/Nov/13 15:14;qconner;Run against the following commit on cassandra-1.2 branch:
8e7d7285cdeac4f2527c933280d595bbddd26935

This profile measures CPU time, not elapsed time.

Internode compression turned off.  

256 nodes w/ num_tokens=256.  
GossipPeriod 1000 ms.
ArrivalWindow.size() = 1000.
","07/Nov/13 15:18;qconner;Good morning.  We saw the same CPU usage profile with cassandra-1.2 8e7d7285cdeac4f2527c933280d595bbddd26935 (which included the patch to not flush peers CF).  

CPU time was spent in looking up EndpointState or spent in PHI calculation.  No surprises were found.  No race conditions, no deadlocks or mutex/monitor contention.

I do not know if flapping happens in 1.2 head without vnodes.  I will find out today, if I can get the nodes (having trouble this morning allocating from EC2).  Will keep trying (Fridays seem better) but could slip into the weekend...
","07/Nov/13 15:24;qconner;Tupshin, can you further quantify the CPU usage you observed, in terms of USER CPU and KERNEL CPU?
Also, can you confirm the number of nodes and vnodes for those observations.

I've seen about 25% user cpu @ 256 nodes and 60% @ 512 nodes.  Kernel cpu was under 5% for both in my trials.","12/Nov/13 21:25;qconner;Flapping occurs with vnodes or without.  Please see below.

!flaps-vs-tokens.png!

Using vnodes appears to exacerbate, possibly with longer messages, probably with higher cpu utilization.  Either would delay the timestamp for the Failure Detector interarrival time.","12/Nov/13 21:30;qconner;The num_tokens setting has a mild impact on average cpu utilization.  Please see the graph below for the trend with 256 nodes.

!cpu-vs-token-graph.png!

This graph does not characterize the bursty nature of any given node's CPU utilization.  It does average the utilization over a 200 second period, taken at 10 second intervals using ""sar -u"".

Since gossip heartbeat destinations are random, ""unlucky"" nodes will sometimes receive twice the gossip traffic and (10 second basis) CPU utilization has been casually observed @25% for N=256 and 60% for N=512.","12/Nov/13 23:43;jbellis;bq. ISTM that FD processing Gossip updates synchronously is a fundamental problem. Any hiccup in processing will cause FD false positives.

I've pulled a fix for this out to CASSANDRA-6338.","19/Nov/13 21:18;jbellis;bq. Untested patch #3. Delays output from FailureDetector until statistically valid number of samples have been obtained.

Did we ever find a scenario where we can demonstrate this patch making a difference?  Because I think it's a good idea in theory.","20/Nov/13 15:18;qconner;Yes, both use case 1 and use case 2 (detailed in early comment above) were cured by patch #3.  Zero flaps were recorded in multiple trials in both use cases.  Patch #3 cures the flaps, but does not address the cpu usage symptom.

This was tested against the cassandra-1.2 branch.  I am conducting the same test today against use case 2 today, but using the current cassandra-2.0 branch of source.","20/Nov/13 15:21;qconner;early results of testing with patch #3 were good

see
vnodes & gossip flaps.png

!vnodes\ \&\ gossip\ flaps.png!",03/Dec/13 14:39;jbellis;I think we've addressed the major problems in the related tickets above.  No single culprit.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
describe_ring hangs with hsha thrift server,CASSANDRA-6373,12679842,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,nickmbailey,nickmbailey,18/Nov/13 23:25,12/Mar/19 14:15,13/Mar/19 22:29,11/Jan/14 01:29,2.0.5,,,,,,1,,,,,"There is a strange bug with the thrift hsha server in 2.0 (we switched to lmax disruptor server).

The bug is that the first call to describe_ring from one connection will hang indefinitely when the client is not connecting from localhost (or it at least looks like the client is not on the same host). Additionally the cluster must be using vnodes. When connecting from localhost the first call will work as expected. And in either case subsequent calls from the same connection will work as expected. According to git bisect the bad commit is the switch to the lmax disruptor server:

https://github.com/apache/cassandra/commit/98eec0a223251ecd8fec7ecc9e46b05497d631c6

I've attached the patch I used to reproduce the error in the unit tests. The command to reproduce is: 

{noformat}
PYTHONPATH=test nosetests --tests=system.test_thrift_server:TestMutations.test_describe_ring
{noformat}

I reproduced on ec2 and a single machine by having the server bind to the private ip on ec2 and the client connect to the public ip (so it appears as if the client is non local). I've also reproduced with two different vms though.",,,,,,,,,,,,,,,,,,,,,,,,18/Nov/13 23:26;nickmbailey;describe_ring_failure.patch;https://issues.apache.org/jira/secure/attachment/12614510/describe_ring_failure.patch,19/Dec/13 17:33;nickmbailey;jstack.txt;https://issues.apache.org/jira/secure/attachment/12619604/jstack.txt,19/Dec/13 20:36;nickmbailey;jstack2.txt;https://issues.apache.org/jira/secure/attachment/12619654/jstack2.txt,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-12-11 20:05:44.006,,,no_permission,,,,,,,,,,,,359200,,,Sat Jan 11 01:29:13 UTC 2014,,,,,,0|i1px8n:,359499,2.0.0,,,,,,,,,,,,,,,,,,11/Dec/13 20:05;xedin;[~nickmbailey] Have you tried doing reads/writes with the same setup? I'm just wondering if it's *only* describe_ring or everything else too which displays the same behavior?...,11/Dec/13 23:35;nickmbailey;I can check tomorrow. I *think* i saw it with describe keyspaces as well but I'm not positive. I have the ec2 machine i recreated on as well which I could probably give you access to if you want.,13/Dec/13 16:53;nickmbailey;[~xedin] I've verified its *only* the describe_ring call. Other calls seem to work fine. Let me know if you want access to the machine I used to reproduce.,18/Dec/13 23:43;nickmbailey;[~xedin] any insight yet?,"19/Dec/13 01:59;xedin;I had a look at the code to figure out why only that particular command is not working but no success this far, it would be very helpful if you could attach output of jstack of the server side taken in the situation when describe_ring hangs...",19/Dec/13 17:33;nickmbailey;Attached the output of jstack.,"19/Dec/13 20:18;xedin;So from the thread stacks it looks like server side is just waiting for new packets from the client as both Acceptor and Selector threads are blocked on select() from the socket. Just to clarify, was this taken simultaneously with client being stuck waiting for describe_ring output?","19/Dec/13 20:36;nickmbailey;Yes, It should have been. Just to be sure I got another one which definitely is. They look about the same though.","19/Dec/13 20:44;xedin;Those looks the same, selector threads are waiting on the socket and all of the worker threads are waiting on the barrier for a new task... Server implementation could not distinguish between commands it's being sent so I don't see any reason why one would hang and another wouldn't, I think describe_ring itself should be a culprit in this case.","19/Dec/13 20:51;nickmbailey;Yeah, it's a very strange bug. Like I said it's definitely the commit where we changed the hsha server though. And it only happens on the first call to describe ring. For example if you change the patch I attached to do the following in the test function, the test passes fine after catching the first socket timeout.

{noformat}
    def test_describe_ring(self):
+       print 'running desc ring'
+       try:
+            list(client.describe_ring('Keyspace1'))[0].endpoints == ['127.0.0.1']
+       except Exception, e:
+            print e
+        print 'running again'
         assert list(client.describe_ring('Keyspace1'))[0].endpoints == ['127.0.0.1']
{noformat}","19/Dec/13 22:17;xedin;Is the same happening with vnodes turned off? I just don't get why if you send any other command it works fine but if you send describe_ring it fails, it shouldn't be any different from the server perspective... Can you also try increasing min number of rpc threads from 2 to 4, wonder if it would make any difference.","20/Dec/13 00:37;nickmbailey;It doesn't happen with vnodes off. In fact it doesn't happen with vnodes set to 128. So that would seem to indicate it has to do with the size of the response as more vnodes will make the describe_ring response much larger.

rpc_min_threads doesn't have any effect.","20/Dec/13 01:10;xedin;Does it have any ERROR messages in the server log starting with ""invalid frame size""? I think default frame size should be set to 15 mb which is enough for most of the things, but please check just in case.","20/Dec/13 01:21;nickmbailey;Nope, no errors in the logs at all.","09/Jan/14 23:24;xedin;[~nickmbailey] I have attached updated disruptor server jar to the CASSANDRA-6407, can you please try your test with update jar?","10/Jan/14 16:23;nickmbailey;If I use the patch I attached previously I get the timeout as I described before. When I copy your updated jar over the existing jar I get a different error:

{noformat}
automaton@ip-10-196-42-161:~/cassandra$ PYTHONPATH=test nosetests --tests=system.test_thrift_server:TestMutations.test_describe_ring
E
======================================================================
ERROR: system.test_thrift_server.TestMutations.test_describe_ring
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/nose/case.py"", line 381, in setUp
    try_run(self.inst, ('setup', 'setUp'))
  File ""/usr/local/lib/python2.7/dist-packages/nose/util.py"", line 469, in try_run
    return func()
  File ""/home/automaton/cassandra/test/system/__init__.py"", line 114, in setUp
    self.define_schema()
  File ""/home/automaton/cassandra/test/system/__init__.py"", line 183, in define_schema
    self.client.system_add_keyspace(ks)
  File ""/home/automaton/cassandra/interface/thrift/gen-py/cassandra/Cassandra.py"", line 1783, in system_add_keyspace
    return self.recv_system_add_keyspace()
  File ""/home/automaton/cassandra/interface/thrift/gen-py/cassandra/Cassandra.py"", line 1794, in recv_system_add_keyspace
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
  File ""/usr/lib/python2.7/dist-packages/thrift/protocol/TBinaryProtocol.py"", line 126, in readMessageBegin
    sz = self.readI32()
  File ""/usr/lib/python2.7/dist-packages/thrift/protocol/TBinaryProtocol.py"", line 203, in readI32
    buff = self.trans.readAll(4)
  File ""/usr/lib/python2.7/dist-packages/thrift/transport/TTransport.py"", line 58, in readAll
    chunk = self.read(sz-have)
  File ""/usr/lib/python2.7/dist-packages/thrift/transport/TTransport.py"", line 272, in read
    self.readFrame()
  File ""/usr/lib/python2.7/dist-packages/thrift/transport/TTransport.py"", line 276, in readFrame
    buff = self.__trans.readAll(4)
  File ""/usr/lib/python2.7/dist-packages/thrift/transport/TTransport.py"", line 58, in readAll
    chunk = self.read(sz-have)
  File ""/usr/lib/python2.7/dist-packages/thrift/transport/TSocket.py"", line 94, in read
    buff = self.handle.recv(sz)
error: [Errno 104] Connection reset by peer

----------------------------------------------------------------------
Ran 1 test in 8.528s

FAILED (errors=1)
{noformat}","10/Jan/14 18:56;xedin;Did you replace old one or just copied it over, does system log say anything? I'm going to try it myself today too.","10/Jan/14 19:08;nickmbailey;Actually I do see an error in the log:

{noformat}
ERROR [main] 2014-01-10 19:06:51,179 CassandraDaemon.java (line 478) Exception encountered during startup
java.lang.NoClassDefFoundError: com/lmax/disruptor/EventTranslator
        at com.thinkaurelius.thrift.TDisruptorServer.<init>(TDisruptorServer.java:192)
        at org.apache.cassandra.thrift.THsHaDisruptorServer.<init>(THsHaDisruptorServer.java:46)
        at org.apache.cassandra.thrift.THsHaDisruptorServer$Factory.buildTServer(THsHaDisruptorServer.java:90)
        at org.apache.cassandra.thrift.TServerCustomFactory.buildTServer(TServerCustomFactory.java:56)
        at org.apache.cassandra.thrift.ThriftServer$ThriftServerThread.<init>(ThriftServer.java:130)
        at org.apache.cassandra.thrift.ThriftServer.start(ThriftServer.java:56)
        at org.apache.cassandra.service.CassandraDaemon.start(CassandraDaemon.java:414)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:474)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:504)
Caused by: java.lang.ClassNotFoundException: com.lmax.disruptor.EventTranslator
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        ... 9 more
 INFO [StorageServiceShutdownHook] 2014-01-10 19:06:51,220 Gossiper.java (line 1238) Announcing shutdown
DEBUG [GossipTasks:1] 2014-01-10 19:06:51,722 DebuggableThreadPoolExecutor.java (line 245) Task cancelled
java.util.concurrent.CancellationException
        at java.util.concurrent.FutureTask.report(FutureTask.java:121)
        at java.util.concurrent.FutureTask.get(FutureTask.java:188)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.extractThrowable(DebuggableThreadPoolExecutor.java:237)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.logExceptionsAfterExecute(DebuggableThreadPoolExecutor.java:201)
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor.afterExecute(DebuggableScheduledThreadPoolExecutor.java:46)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
{noformat}

I copied your new jar over the existing disruptor jar:

{noformat}
cp disruptor-thrift-server-0.3.3-SNAPSHOT.jar lib/disruptor-3.0.1.jar
{noformat}",10/Jan/14 19:11;nickmbailey;Perhaps I was supposed to just copy the jar into the lib folder and not replace the old jar? I just tried that and it looks like everything is fixed.,"10/Jan/14 20:14;xedin;You have replaced the wrong jar :) there are two of them - disruptor and disruptor_thrift_server, can you bring disruptor jar back and replace disruptor_thrift_server instead?","10/Jan/14 21:36;nickmbailey;Got it:

{noformat}
mv disruptor-thrift-server-0.3.3-SNAPSHOT.jar lib/thrift-server-0.3.2.jar
{noformat}

That appears to fix the issue.",10/Jan/14 21:52;xedin;Thanks! I will release disruptor_thrift_server and close both issues once build.xml is changed.,11/Jan/14 01:29;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError exception in system.log,CASSANDRA-5453,12641987,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,winsdom,winsdom,11/Apr/13 06:23,12/Mar/19 14:15,13/Mar/19 22:29,15/Jan/14 09:41,,,,,,,0,,,,,"We've lot of error appeared in one node,
ERROR [Thrift:91817] 2013-03-24 03:26:17,528 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[Thrift:91817,5,main]
java.lang.AssertionError
        at org.apache.cassandra.db.SliceFromReadCommand.maybeGenerateRetryCommand(SliceFromReadCommand.java:78)
        at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:775)
        at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:605)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:115)
        at org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:270)
        at org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:354)
        at org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:314)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_slice.getResult(Cassandra.java:2847)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_slice.getResult(Cassandra.java:2835)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
After such error occur, it will crash. Restart cassandra will work temporary, but a month later, the case still happened. Not sure if it's due to data corrupted.",five nodes in cluster running on Cent O.S 5.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-01-15 09:33:45.113,,,no_permission,,,,,,,,,,,,322401,,,Wed Jan 15 09:41:07 UTC 2014,,,,,,0|i1jmjb:,322746,,,,,,,,,,,,,,,,,,,"15/Jan/14 09:33;lyubent;[~winsdom] Could you add more details about your environment, what version of C*, what RF, what load does your cluster handle? Were you doing anything in particular that triggered this?","15/Jan/14 09:41;winsdom;Hi Lyuben,
    The issue has fixed after we upgrade to 1.1.11. I'll mark the ticket as closed now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra2.1~beta1 Stall at Boot,CASSANDRA-6753,12696660,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,benedict,davychia,davychia,21/Feb/14 22:48,12/Mar/19 14:14,13/Mar/19 22:29,22/Feb/14 14:24,2.1 beta2,,,,,,0,,,,,"I was trying out the new release for several perf. improvements that I am very interested in. After upgrading my cassandra from 2.0.5 to the beta version, cassandra is stalled while init the column families.

I might misconfigure something, but it seems it is suck in a loop. I added a couple debug statements, but, on second thought, I think I should just leave it to the experts...

It's looping in the following over and over:

{code:title=src/java/org/apache/cassandra/utils/memory/Pool.java#needsCleaning}
0 >= -858993472 && true && true
{code}

{code:title=Log}
INFO  [HeapSlabPoolCleaner] 2014-02-21 22:28:40,073 Keyspace.java:77 - java.lang.Thread.getStackTrace(Unknown Source),
org.apache.cassandra.db.Keyspace$1.apply(Keyspace.java:77),
org.apache.cassandra.db.Keyspace$1.apply(Keyspace.java:74),
com.google.common.collect.Iterators$8.transform(Iterators.java:794),
com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48),
org.apache.cassandra.db.ColumnFamilyStore.all(ColumnFamilyStore.java:2278),
org.apache.cassandra.db.ColumnFamilyStore$FlushLargestColumnFamily.run(ColumnFamilyStore.java:1043),
org.apache.cassandra.utils.memory.PoolCleanerThread.run(PoolCleanerThread.java:70)
{code}

They may be totally unrelated or a normal behavior. Let me know if there is any other info I should provide.","Distributor ID:	Ubuntu
Description:	Ubuntu 12.04.3 LTS
Release:	12.04
Codename:	precise

AWS: i2.xlarge

{code}
INFO  22:34:40 JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.7.0
INFO  22:34:40 Heap size: 12777553920/12777553920
INFO  22:34:40 Code Cache Non-heap memory: init = 2555904(2496K) used = 621632(607K) committed = 2555904(2496K) max = 50331648(49152K)
INFO  22:34:40 Par Eden Space Heap memory: init = 859045888(838912K) used = 137447616(134226K) committed = 859045888(838912K) max = 859045888(838912K)
INFO  22:34:40 Par Survivor Space Heap memory: init = 107347968(104832K) used = 0(0K) committed = 107347968(104832K) max = 107347968(104832K)
INFO  22:34:40 CMS Old Gen Heap memory: init = 11811160064(11534336K) used = 1433816(1400K) committed = 11811160064(11534336K) max = 11811160064(11534336K)
INFO  22:34:40 CMS Perm Gen Non-heap memory: init = 21757952(21248K) used = 18654512(18217K) committed = 21757952(21248K) max = 85983232(83968K)
INFO  22:34:40 Classpath: /usr/share/cassandra/lib/airline-0.6.jar:/usr/share/cassandra/lib/antlr-3.2.jar:/usr/share/cassandra/lib/commons-cli-1.1.jar:/usr/share/cassandra/lib/commons-codec-1.2.jar:/usr/share/cassandra/lib/commons-lang3-3.1.jar:/usr/share/cassandra/lib/commons-math3-3.2.jar:/usr/share/cassandra/lib/compress-lzf-0.8.4.jar:/usr/share/cassandra/lib/concurrentlinkedhashmap-lru-1.4.jar:/usr/share/cassandra/lib/disruptor-3.0.1.jar:/usr/share/cassandra/lib/guava-16.0.jar:/usr/share/cassandra/lib/high-scale-lib-1.1.2.jar:/usr/share/cassandra/lib/jackson-core-asl-1.9.2.jar:/usr/share/cassandra/lib/jackson-mapper-asl-1.9.2.jar:/usr/share/cassandra/lib/jamm-0.2.6.jar:/usr/share/cassandra/lib/javax.inject.jar:/usr/share/cassandra/lib/jbcrypt-0.3m.jar:/usr/share/cassandra/lib/jline-1.0.jar:/usr/share/cassandra/lib/jna-4.0.0.jar:/usr/share/cassandra/lib/json-simple-1.1.jar:/usr/share/cassandra/lib/libthrift-0.9.1.jar:/usr/share/cassandra/lib/logback-classic-1.0.13.jar:/usr/share/cassandra/lib/logback-core-1.0.13.jar:/usr/share/cassandra/lib/lz4-1.2.0.jar:/usr/share/cassandra/lib/metrics-core-2.2.0.jar:/usr/share/cassandra/lib/netty-3.6.6.Final.jar:/usr/share/cassandra/lib/reporter-config-2.1.0.jar:/usr/share/cassandra/lib/slf4j-api-1.7.2.jar:/usr/share/cassandra/lib/snakeyaml-1.11.jar:/usr/share/cassandra/lib/snappy-java-1.0.5.jar:/usr/share/cassandra/lib/stream-2.5.2.jar:/usr/share/cassandra/lib/thrift-server-0.3.3.jar:/usr/share/cassandra/CustomAgent.jar:/usr/share/cassandra/apache-cassandra-2.1.0~beta1.jar:/usr/share/cassandra/apache-cassandra-thrift-2.1.0~beta1.jar:/usr/share/cassandra/apache-cassandra.jar:/usr/share/cassandra/jna.jar:/usr/share/cassandra/mx4j-tools.jar:/usr/share/cassandra/stress.jar:/usr/share/java/jna.jar:/etc/cassandra:/usr/share/java/commons-daemon.jar:/usr/share/cassandra/lib/jamm-0.2.6.jar:/usr/share/cassandra/CustomAgent.jar:/usr/local/jcollectd/jcollectd.jar
{code}

{code:title=Node configuration}
[authenticator=AllowAllAuthenticator; authorizer=AllowAllAuthorizer; auto_snapshot=true; batchlog_replay_throttle_in_kb=1024; cas_contention_timeout_in_ms=1000; client_encryption_options=<REDACTED>; cluster_name=sketchy_staging_test; column_index_size_in_kb=64; commitlog_directory=/mnt/cassandra/commitlog; commitlog_segment_size_in_mb=32; commitlog_sync=periodic; commitlog_sync_period_in_ms=10000; compaction_preheat_key_cache=true; compaction_throughput_mb_per_sec=64; concurrent_counter_writes=32; concurrent_reads=128; concurrent_writes=128; counter_cache_save_period=7200; counter_cache_size_in_mb=null; counter_write_request_timeout_in_ms=5000; cross_node_timeout=false; data_file_directories=[/mnt/cassandra/data]; disk_failure_policy=stop; dynamic_snitch_badness_threshold=0.1; dynamic_snitch_reset_interval_in_ms=600000; dynamic_snitch_update_interval_in_ms=100; endpoint_snitch=SimpleSnitch; flush_directory=/mnt/cassandra/flush; hinted_handoff_enabled=true; hinted_handoff_throttle_in_kb=1024; in_memory_compaction_limit_in_mb=64; incremental_backups=false; index_summary_capacity_in_mb=null; index_summary_resize_interval_in_minutes=60; inter_dc_tcp_nodelay=false; internode_compression=all; key_cache_save_period=14400; key_cache_size_in_mb=1024; listen_address=10.9.163.158; max_hint_window_in_ms=14400000; max_hints_delivery_threads=2; memtable_cleanup_threshold=0.4; memtable_total_space_in_mb=2048; native_transport_port=9042; num_tokens=256; partitioner=org.apache.cassandra.dht.Murmur3Partitioner; permissions_validity_in_ms=2000; preheat_kernel_page_cache=false; range_request_timeout_in_ms=10000; read_request_timeout_in_ms=5000; request_scheduler=org.apache.cassandra.scheduler.NoScheduler; request_timeout_in_ms=10000; row_cache_save_period=14400; row_cache_size_in_mb=1024; rpc_address=0.0.0.0; rpc_keepalive=true; rpc_port=9160; rpc_server_type=sync; saved_caches_directory=/mnt/cassandra/cache; seed_provider=[{class_name=org.apache.cassandra.locator.SimpleSeedProvider, parameters=[{seeds=10.71.141.38,10.218.142.35}]}]; server_encryption_options=<REDACTED>; snapshot_before_compaction=false; ssl_storage_port=7001; start_native_transport=true; start_rpc=true; storage_port=7000; thrift_framed_transport_size_in_mb=15; tombstone_failure_threshold=100000; tombstone_warn_threshold=1000; trickle_fsync=true; trickle_fsync_interval_in_kb=10240; truncate_request_timeout_in_ms=60000; write_request_timeout_in_ms=2000]
{code}

",,,,,,,,,,,,,,,,,,,,,,,22/Feb/14 10:38;benedict;patch.txt;https://issues.apache.org/jira/secure/attachment/12630476/patch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-22 00:12:56.2,,,no_permission,,,,,,,,,,,,375136,,,Sat Feb 22 14:24:47 UTC 2014,,,,,,0|i1sn87:,375433,2.1 beta1,,,,,,,,,,,,,,,,,,"22/Feb/14 00:12;benedict;Some obvious questions:
- Do you see any other errors?
- The 0 and -858993472 correspond to the used() and nextClean part of that method, respectively, correct? What about the limit and the cleanThreshold? What do they say?
- This is consistent, every time you start?

This is definitely not normal, and is almost certainly a bug, but it shouldn't ever stop Cassandra from starting. So, I wonder if there is a strange interaction going on with some other problem, which may be easier to track down if we can figure out if there is another such problem.

Could you attach the output from jstacking the process?

The easiest possibility to explain this is that somehow the memtable_cleanup_threshold is negative. We don't actually check this on startup, which is an oversight. The fact that the value for nextClean is exactly \-0.4 * 2Gb has me suspicious - with an 8Gb heap, we would default to a 2Gb limit, and default cleanup_threshold is 0.4. Is it possible you accidentally added a '\-' prefix to the line in the config file? Unlikely, I know, but it would explain it instantly :-)

","22/Feb/14 01:13;davychia;aha! good call!

{code:title=src/java/org/apache/cassandra/utils/memory/Pool.java#needsCleaning}
used(0) >= nextClean(-858993472) && updateNextClean(true) && cleanerThread(true) -- limit(-2147483648), cleanThreshold(0.400000)
{code}

It seems to be working after:
{code:title=src/java/org/apache/cassandra/config/DatabaseDescriptor.java:1385}
(long) conf.memtable_total_space_in_mb << 20
{code}

And failed with an exception (doh!)... but it is unrelated to this ticket.",22/Feb/14 10:38;benedict;Uploaded a simple patch to both correct the overflow and prevent provision of bad cleanup thresholds,22/Feb/14 14:24;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SecondaryIndexManager#deleteFromIndexes() doesn't correctly retrieve column indexes,CASSANDRA-6711,12695239,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sbtourist,sbtourist,sbtourist,14/Feb/14 16:23,12/Mar/19 14:14,13/Mar/19 22:29,14/Feb/14 17:57,1.2.16,2.0.6,2.1 beta1,,,,0,,,,,"SecondaryIndexManager#deleteFromIndexes() tries to retrieve the index class from indexesByColumn by using the raw column name, but indexes are inserted in indexesByColumn by using the ColumnDefinition name.",,,,,,,,,,,,,,,,,,,,,,,,14/Feb/14 16:54;sbtourist;CASSANDRA-6711-1.2.patch;https://issues.apache.org/jira/secure/attachment/12629057/CASSANDRA-6711-1.2.patch,14/Feb/14 16:54;sbtourist;CASSANDRA-6711-2.0.patch;https://issues.apache.org/jira/secure/attachment/12629058/CASSANDRA-6711-2.0.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-02-14 17:17:25.536,,,no_permission,,,,,,,,,,,,373747,,,Fri Feb 14 17:57:06 UTC 2014,,,,,,0|i1seon:,374047,1.2.15,2.0.5,2.1 rc3,,,,,beobal,beobal,,,,,,,,,,14/Feb/14 17:17;beobal;+1 LGTM,"14/Feb/14 17:57;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception on nodetool cfstats,CASSANDRA-6739,12696073,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mishail,mateusz.gajewski,mateusz.gajewski,19/Feb/14 20:46,12/Mar/19 14:14,13/Mar/19 22:29,20/Feb/14 03:42,2.1 beta2,,,,,,0,,,,,"While running nodetool cfstats exception is thrown:

javax.management.InstanceNotFoundException: org.apache.cassandra.metrics:type=ColumnFamily,keyspace=system_traces,scope=sessions,name=PendingTasks

VisualVM MBeans list attached

PendingTasks is not actually exposed.",,,,,,,,,,,,,,,,,,,,,,,,19/Feb/14 22:52;mishail;2014-02-19 14-51-43.png;https://issues.apache.org/jira/secure/attachment/12629920/2014-02-19+14-51-43.png,20/Feb/14 00:15;mishail;CASSANDRA-2.1-6739.patch;https://issues.apache.org/jira/secure/attachment/12629936/CASSANDRA-2.1-6739.patch,19/Feb/14 20:46;mateusz.gajewski;screenshot-1402196.jpg;https://issues.apache.org/jira/secure/attachment/12629885/screenshot-1402196.jpg,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-02-19 22:50:48.02,,,no_permission,,,,,,,,,,,,374550,,,Thu Feb 20 13:13:22 UTC 2014,,,,,,0|i1sjmn:,374850,2.1 rc3,,,,,,,yukim,yukim,,,,,,,,,,"19/Feb/14 22:50;mishail;There is an error in {{NodeProbe}}. There is no {{PendingTasks}} metric for a column family, It shall use {{PendingFlushes}} instead",19/Feb/14 23:06;mateusz.gajewski;Same happened to MemtableDataSize (should be MemtableLiveDataSize) that had changed its type from int to long. ,"19/Feb/14 23:07;mishail;Commit for CASSANDRA-5549 renamed several metrics in {{ColumnFamilyMetrics}}

https://github.com/apache/cassandra/commit/4b54b8acd21999ad4394feb93deb7cca1de445c0#diff-83",20/Feb/14 00:15;mishail;Patch to adjust {{nodetool}} to use new metrics,20/Feb/14 02:18;kohlisankalp;The patch seems to fix the problem. Need to do ant before running node tool. ,"20/Feb/14 02:23;yukim;Should we rename to ""Pending flushes"" in the output of nodetool also? Otherwise +1.","20/Feb/14 02:44;kohlisankalp;""Should we rename to ""Pending flushes"" in the output of node tool also""

+1 We should rename it at both Keyspace and CF level. ","20/Feb/14 03:01;mishail;bq. We should rename it at both Keyspace and CF level.

I'm not sure I follow you. ","20/Feb/14 03:42;mishail;Renamed to ""Pending flushes"" and committed. Thanks","20/Feb/14 08:04;mateusz.gajewski;LGTM. Thanks.

P.S. I think that nodetool implementation needs a little bit of refactoring. Can I contribute in this field? Just to explore Cassandra internals better?","20/Feb/14 12:34;jeromatron;I'm sure any contribution would be welcome, although trunk has CASSANDRA-6381 which did some cleanup of nodetool.  You might look at [the unresolved lhf (low hanging fruit) labeled tickets|https://issues.apache.org/jira/issues/?jql=project%20%3D%2012310865%20AND%20labels%20%3D%20lhf%20AND%20status%20!%3D%20resolved].","20/Feb/14 12:39;mateusz.gajewski;Sure, thanks for advice. Do I need to sign some kind of agreement before contributing?","20/Feb/14 13:13;jeromatron;I would take a look at http://wiki.apache.org/cassandra/HowToContribute but really the only thing you need to do is when you submit a patch, you are essentially giving rights to the patch to the apache software foundation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When making heavy use of counters, neighbor nodes occasionally enter spiral of constant memory consumpion",CASSANDRA-6405,12681233,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,alienth,alienth,26/Nov/13 09:43,12/Mar/19 14:14,13/Mar/19 22:29,10/Apr/14 18:16,2.1 beta2,,,,,,0,,,,,"We're randomly running into an interesting issue on our ring. When making use of counters, we'll occasionally have 3 nodes (always neighbors) suddenly start immediately filling up memory, CMSing, fill up again, repeat. This pattern goes on for 5-20 minutes. Nearly all requests to the nodes time out during this period. Restarting one, two, or all three of the nodes does not resolve the spiral; after a restart the three nodes immediately start hogging up memory again and CMSing constantly.

When the issue resolves itself, all 3 nodes immediately get better. Sometimes it reoccurs in bursts, where it will be trashed for 20 minutes, fine for 5, trashed for 20, and repeat that cycle a few times.

There are no unusual logs provided by cassandra during this period of time, other than recording of the constant dropped read requests and the constant CMS runs. I have analyzed the log files prior to multiple distinct instances of this issue and have found no preceding events which are associated with this issue.

I have verified that our apps are not performing any unusual number or type of requests during this time.

This behaviour occurred on 1.0.12, 1.1.7, and now on 1.2.11.

The way I've narrowed this down to counters is a bit naive. It started happening when we started making use of counter columns, went away after we rolled back use of counter columns. I've repeated this attempted rollout on each version now, and it consistently rears its head every time. I should note this incident does _seem_ to happen more rarely on 1.2.11 compared to the previous versions.

This incident has been consistent across multiple different types of hardware, as well as major kernel version changes (2.6 all the way to 3.2). The OS is operating normally during the event.


I managed to get an hprof dump when the issue was happening in the wild. Something notable in the class instance counts as reported by jhat. Here are the top 5 counts for this one node:

{code}
5967846 instances of class org.apache.cassandra.db.CounterColumn 
1247525 instances of class com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$WeightedValue 
1247310 instances of class org.apache.cassandra.cache.KeyCacheKey 
1246648 instances of class com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node 
1237526 instances of class org.apache.cassandra.db.RowIndexEntry 
{code}

Is it normal or expected for CounterColumn to have that number of instances?

The data model for how we use counters is as follows: between 50-20000 counter columns per key. We currently have around 3 million keys total, but this issue also replicated when we only had a few thousand keys total. Average column count is around 1k, and 90th is 18k. New columns are added regularly, and columns are incremented regularly. No column or key deletions occur. We probably have 1-5k ""hot"" keys at any given time, spread across the entire ring. R:W ratio is typically around 50:1. This is the only CF we're using counters on, at this time. CF details are as follows:

{code}
    ColumnFamily: CommentTree
      Key Validation Class: org.apache.cassandra.db.marshal.AsciiType
      Default column value validator: org.apache.cassandra.db.marshal.CounterColumnType
      Cells sorted by: org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.LongType,org.apache.cassandra.db.marshal.LongType,org.apache.cassandra.db.marshal.LongType)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 0.01
      DC Local Read repair chance: 0.0
      Populate IO Cache on flush: false
      Replicate on write: true
      Caching: KEYS_ONLY
      Bloom Filter FP chance: default
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.LeveledCompactionStrategy
      Compaction Strategy Options:
        sstable_size_in_mb: 160



                Column Family: CommentTree
                SSTable count: 30
                SSTables in each level: [1, 10, 19, 0, 0, 0, 0, 0, 0]
                Space used (live): 4656930594
                Space used (total): 4677221791
                SSTable Compression Ratio: 0.0
                Number of Keys (estimate): 679680
                Memtable Columns Count: 8289
                Memtable Data Size: 2639908
                Memtable Switch Count: 5769
                Read Count: 185479324
                Read Latency: 1.786 ms.
                Write Count: 5377562
                Write Latency: 0.026 ms.
                Pending Tasks: 0
                Bloom Filter False Positives: 2914204
                Bloom Filter False Ratio: 0.56403
                Bloom Filter Space Used: 523952
                Compacted row minimum size: 30
                Compacted row maximum size: 4866323
                Compacted row mean size: 7742
                Average live cells per slice (last five minutes): 39.0
                Average tombstones per slice (last five minutes): 0.0

{code}


Please let me know if I can provide any further information. I can provide the hprof if desired, however it is 3GB so I'll need to provide it outside of JIRA.","RF of 3, 15 nodes.
Sun Java 7 (also occurred in OpenJDK 6, and Sun Java 6).
Xmx of 8G.
No row cache.",,,,,,,,,,,,CASSANDRA-6506,,,,,,,,,,,11/Dec/13 03:34;alienth;threaddump.txt;https://issues.apache.org/jira/secure/attachment/12618172/threaddump.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-28 16:56:01.444,,,no_permission,,,,,,,,,,,,360498,,,Thu Apr 10 18:16:30 UTC 2014,,,,,,0|i1q57j:,360797,1.0.12,1.1.7,1.2.11,,,,,,,,,,,,,,,,"27/Nov/13 00:03;alienth;Just did some analysis under normal conditions. Typically, our nodes have less than 200k instances of org.apache.cassandra.db.CounterColumn. During this issue we had nearly 6 million instances, as shown above.","27/Nov/13 04:40;alienth;I have verified that an instance which exhibited the high instance count of CounterColumn classes returned to a lower count (from 5.5m to 180k) after the issue resolved itself, without a restart.",28/Nov/13 16:56;jbellis;It sounds like compaction tbh.,"02/Dec/13 23:43;alienth;[~jbellis] There are no compaction tasks pending during the incident. Additionally, on an earlier occurrence I disabled compaction on the CF to no avail.

Would a compaction pileup account for the huge number of class instances? I also find it somewhat unlikely that a compaction issue would appear on 3 nodes simultaneously, and then immediately resolve on 3 nodes simultaneously.","11/Dec/13 01:33;alienth;We're experiencing this issue right this moment (in fact, reddit is down as a result). Compactionstats on the three nodes that are spiking is as follows:

pending tasks: 0
Active compaction remaining time :        n/a
",11/Dec/13 02:29;mishail;[~alienth] can you take a thread dump when the issue happens?,11/Dec/13 03:34;alienth;Thread dump during incident.,"11/Dec/13 03:34;alienth;[~mishail] I have just attached a thread dump to this issue.

Thanks!","11/Dec/13 03:35;alienth;Also, GC details after a CMS occurred (immediately followedy by another CMS)

{code}
 Heap
  par new generation   total 276480K, used 31300K [0x00000005fae00000, 0x000000060da00000, 0x000000060da00000)
   eden space 245760K,   0% used [0x00000005fae00000, 0x00000005fae913e8, 0x0000000609e00000)
   from space 30720K, 100% used [0x000000060bc00000, 0x000000060da00000, 0x000000060da00000)
   to   space 30720K,   0% used [0x0000000609e00000, 0x0000000609e00000, 0x000000060bc00000)
  concurrent mark-sweep generation total 8081408K, used 1319539K [0x000000060da00000, 0x00000007fae00000, 0x00000007fae00000)
  concurrent-mark-sweep perm gen total 41060K, used 24529K [0x00000007fae00000, 0x00000007fd619000, 0x0000000800000000)
{code}","11/Dec/13 05:33;mishail;[~alienth] How big is your {{key_cache_size_in_mb}}? . And I assume you have {{compaction_preheat_key_cache: true}}, right?",03/Jan/14 01:03;alienth;[~mishail] 100M currently. preheat is turned on.,"03/Jan/14 01:20;alienth;I should note, [~brandon.williams] took a peek at the heap dump and it was unfortunately caught just after a CMS, so it doesn't tell us much. I've been unable to get a heap dump from when the memory is full. Despite the thing constantly CMSing, every dump I've taken is what the heap looked like just after a CMS.

Only solid clue still remaining is that instance count of CounterColumn.","03/Jan/14 08:16;slebresne;In and of itself, having lots of instances of CounterColumn is not abnormal when doing lots of counter operations as this the class for each counter value while inserting/reading them. If you do lots of normal operations, you'll similarly see a lot of Column object allocated. The behavior you are seeing is not particularly normal, but having very many CounterColumn objects is not a definitive sign of a problem.

That being said, do you do insertions at CL.ONE? If so, counters are kind of a time bomb in the sense that the read that is done as part of replication is done *after* we've answered the client. Which means that if you insert too fast, replication tasks will pill up behind the scenes, and those task will hold memory that cannot be GCed. In particular, one thing to look at is the replicate_on_write stage in JMX. If pendingTasks are accumulating, that's likely your problem (you're inserting faster than your cluster can actually handle). In which case the basic solution consists in rate limiting the insertions so pending tasks don't pill up.","03/Jan/14 08:58;alienth;[~slebresne] When the issue is occurring, we have no pending ReplicateOnWrite threads. All pending threads are either reads or writes.

When the crazy CounterColumn instance counts are reached, the number of reads/writes occurring on the table are drastically reduced they're all moving extremely slowly. 

If the CounterColumn instance count was legitimate, wouldn't we expect a huge number of reads/writes to be occurring, rather than a small few? Even during peak hours, we don't do more than 150 reads / 10 writes a second per cassandra node. When this issue occurs, that drops down to 2-3 reads and writes a second.

Additionally, we allow up to 128 read threads concurrently. Most of the counter column rows have around 1k columns, with the 95th percentile having 18k columns. Even if every single read thread was dedicated to reading our largest countercolumn row (which they're not), that accounts for a maximum of ~2-3m counter columns being concurrently accessed.","03/Jan/14 09:57;slebresne;bq. If the CounterColumn instance count was legitimate, wouldn't we expect a huge number of reads/writes to be occurring, rather than a small few?

I expressed myself badly. I'm not saying the exact number is normal, and you are definitively reaching a bad situtation. I was merely saying that it's likely a consequence, not a cause, and that it unfortunately does not allow to narrow what the cause may be a whole lot. I'm not suggesting to ignore that information though.

Now, sorry to insist, but are you doing CL.ONE inserts?

Because the fact is, if you do, we *know* that replicate on write tasks may easily pile up behind the scenes. Which would hold ColumnCounter objects in memory and might explain why neighboring nodes are affected together.

Granted the attached thread dump don't show a whole lot of activity on the ReplicateOnWriteStage, and the absence of pending task on that task would suggest it's not the problem. Nonetheless, it's the best lead I have to offer so far.
","03/Jan/14 10:17;alienth;[~slebresne] Woops, thought i included that. We are doing QUORUM writes.

Just dug through all of our logs and verified that we have never seen a pending count on ReplicateOnWriteStage during these incidents on any server. (Not only verified in thread dumps, but via periodic tpstats dumps).","03/Jan/14 10:25;alienth;I should also note a few other things I've tried on nodes experiencing this issue.

* Wiping the keycache with a restart.
* Disabling the keycache.
* Disabling thrift.
* Adjusting read thread concurrency down to 32 and up to 256.

All of these attempts resulted in no change on the affected nodes. They continued to operate in the manner described above until they randomly got better. I have tried all of the above methods with a restart on a single server, as well as a restart on all three ndoes.","03/Jan/14 10:32;alienth;Just had a thought. One thing I haven't tried is disabling hinted handoff on the affected nodes. When this issue is occurring, the constant CMSs result in a bunch of piled up hints. Perhaps something triggers this behaviour, and the hints keep it rolling until all hints have been handed off?

Bit of a stretch, but I'm grasping for anything at this point. I'll try this next time to see if it changes the behaviour at all.","05/Jan/14 00:43;alienth;Happened again a few more times today, taking the site down.

Pausing hinted handoff resulted in no change on the affected nodes.

I've also verified that there are no abnormal number of requests via the org.apache.cassandra.metrics:type=ClientRequest mbeans.","05/Jan/14 01:33;alienth;We've started abandoning the use of the counter columns. This issue has taken the site down for several hours in the past few days, so I could not allow it to continue.

Unfortunately this also means I won't have any place to reproduce this for continued troubleshooting.","07/Jan/14 20:28;iamaleksey;[~alienth] As Sylvain said, counters currently cause a lot of allocations. I can't say for sure if what I'm going to describe is the cause of your issues, but it definitely contributes to it.

One issue is that all the counter shards of a counter are stored in a single cell, as a blob with sorted tuples (see CounterContext class). And when we reconcile two counter cells, we have to allocate a third cell, large enough to hold the merged context. So unlike regular cells, where reconcile simply picks one of the two cells, reconcile for counter columns creates one more. This doesn't just affect reads, it also affects writes (to the memtable, including replication writes).

Another issues is that when we replicate the counter, we read, and then send, the whole thing to the neighbouring nodes, and not just the value local to the leader-node, and it makes issue #1 worse.

We are aware of it all, and will fix it in 2.1, with CASSANDRA-6506. The second issue is/will be fixed as part of CASSANDRA-6504.

Please note that while it was possible to deal with #2, partially, before, there was no way to make CASSANDRA-6506 happen - because of the supercolumns. However, with CASSANDRA-3237 resolved in 2.0, it is now possible, and I'm currently working on that ticket.","08/Jan/14 07:39;alienth;[~iamaleksey] Thanks for the details on those issues.

It definitely *feels* as though there is an allocation leak, since we go from ~200k allocations, up to 6 million when the issue is happening, and then immediately back down to ~200k when it goes away. Obviously very hard to determine exactly why that is :/

Is there any way to empirically determine if the issues you described are a contributing factor here?",08/Jan/14 16:15;iamaleksey;[~alienth] No built-in metrics that comes to mind. [~slebresne] any ideas?,"08/Jan/14 16:57;slebresne;Nothing coming to mind no, not by default at least. I suppose it wouldn't be too hard to add some instrumentation to count the number of times CounterColumn.reconcile() is called and see if the issue happening is linked to a sudden increase in those calls. That being said, that would still not tell us why there is a sudden increase of the calls... It's still mysterious to me why nodes would suddenly start allocating counters like crazy. ","21/Feb/14 12:59;jbellis;bq. when we reconcile two counter cells, we have to allocate a third cell, large enough to hold the merged context. So unlike regular cells, where reconcile simply picks one of the two cells, reconcile for counter columns creates one more. This doesn't just affect reads, it also affects writes (to the memtable, including replication writes).

Contention within a counter (as multiple writers race to merge cells) makes this worse, because you will get this allocation for failed merges (that is, that lost the CAS race) that need to retry as well.",21/Feb/14 13:00;jbellis;Closing as a duplicate of CASSANDRA-6506.  There's no reasonable way to fix this in earlier C* versions.,"10/Apr/14 18:16;iamaleksey;CASSANDRA-6506 has been delayed until 3.0, but this issues is now actually resolved in 2.1 by the combination of new memtable code and various counters++ commits (including, but not limited to, part of CASSANDRA-6506 and CASSANDRA-6953).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-stress failing in trunk,CASSANDRA-6631,12691935,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,mshuler,mshuler,28/Jan/14 23:24,12/Mar/19 14:14,13/Mar/19 22:29,12/Feb/14 17:57,2.1 beta1,,,Legacy/Tools,,,0,,,,,"Stress is failing in trunk.
- ant clean jar
- ./bin/cassandra -f
- ./tools/bin/cassandra-stress write
{noformat}
(trunk)mshuler@hana:~/git/cassandra$ ./tools/bin/cassandra-stress write
Created keyspaces. Sleeping 1s for propagation.
Warming up WRITE with 50000 iterations...
Exception in thread ""Thread-0"" java.lang.RuntimeException: java.lang.IllegalArgumentException: replicate_on_write is not a column defined in this metadata
        at org.apache.cassandra.stress.settings.StressSettings.getJavaDriverClient(StressSettings.java:142)
        at org.apache.cassandra.stress.settings.StressSettings.getSmartThriftClient(StressSettings.java:49)
        at org.apache.cassandra.stress.StressAction$Consumer.run(StressAction.java:273)
Caused by: java.lang.IllegalArgumentException: replicate_on_write is not a column defined in this metadata
        at com.datastax.driver.core.ColumnDefinitions.getAllIdx(ColumnDefinitions.java:273)
        at com.datastax.driver.core.ColumnDefinitions.getFirstIdx(ColumnDefinitions.java:279)
        at com.datastax.driver.core.Row.getBool(Row.java:117)
        at com.datastax.driver.core.TableMetadata$Options.<init>(TableMetadata.java:474)
        at com.datastax.driver.core.TableMetadata.build(TableMetadata.java:107)
        at com.datastax.driver.core.Metadata.buildTableMetadata(Metadata.java:128)
        at com.datastax.driver.core.Metadata.rebuildSchema(Metadata.java:89)
        at com.datastax.driver.core.ControlConnection.refreshSchema(ControlConnection.java:259)
        at com.datastax.driver.core.ControlConnection.tryConnect(ControlConnection.java:214)
        at com.datastax.driver.core.ControlConnection.reconnectInternal(ControlConnection.java:161)
        at com.datastax.driver.core.ControlConnection.connect(ControlConnection.java:77)
        at com.datastax.driver.core.Cluster$Manager.init(Cluster.java:890)
        at com.datastax.driver.core.Cluster$Manager.access$100(Cluster.java:806)
        at com.datastax.driver.core.Cluster.getMetadata(Cluster.java:217)
        at org.apache.cassandra.stress.util.JavaDriverClient.connect(JavaDriverClient.java:75)
        at org.apache.cassandra.stress.settings.StressSettings.getJavaDriverClient(StressSettings.java:135)
        ... 2 more
Exception in thread ""Thread-19"" java.lang.RuntimeException: java.lang.IllegalArgumentException: replicate_on_write is not a column defined in this metadata
        at org.apache.cassandra.stress.settings.StressSettings.getJavaDriverClient(StressSettings.java:142)
        at org.apache.cassandra.stress.settings.StressSettings.getSmartThriftClient(StressSettings.java:49)
        at org.apache.cassandra.stress.StressAction$Consumer.run(StressAction.java:273)
Caused by: java.lang.IllegalArgumentException: replicate_on_write is not a column defined in this metadata
        at com.datastax.driver.core.ColumnDefinitions.getAllIdx(ColumnDefinitions.java:273)
        at com.datastax.driver.core.ColumnDefinitions.getFirstIdx(ColumnDefinitions.java:279)
        at com.datastax.driver.core.Row.getBool(Row.java:117)
        at com.datastax.driver.core.TableMetadata$Options.<init>(TableMetadata.java:474)
        at com.datastax.driver.core.TableMetadata.build(TableMetadata.java:107)
        at com.datastax.driver.core.Metadata.buildTableMetadata(Metadata.java:128)
        at com.datastax.driver.core.Metadata.rebuildSchema(Metadata.java:89)
        at com.datastax.driver.core.ControlConnection.refreshSchema(ControlConnection.java:259)
        at com.datastax.driver.core.ControlConnection.tryConnect(ControlConnection.java:214)
        at com.datastax.driver.core.ControlConnection.reconnectInternal(ControlConnection.java:161)
        at com.datastax.driver.core.ControlConnection.connect(ControlConnection.java:77)
        at com.datastax.driver.core.Cluster$Manager.init(Cluster.java:890)
        at com.datastax.driver.core.Cluster$Manager.access$100(Cluster.java:806)
        at com.datastax.driver.core.Cluster.getMetadata(Cluster.java:217)
        at org.apache.cassandra.stress.util.JavaDriverClient.connect(JavaDriverClient.java:75)
        at org.apache.cassandra.stress.settings.StressSettings.getJavaDriverClient(StressSettings.java:135)
        ... 2 more
Exception in thread ""Thread-18"" java.lang.RuntimeException: java.lang.IllegalArgumentException: replicate_on_write is not a column defined in this metadata
        at org.apache.cassandra.stress.settings.StressSettings.getJavaDriverClient(StressSettings.java:142)
        at org.apache.cassandra.stress.settings.StressSettings.getSmartThriftClient(StressSettings.java:49)
        at org.apache.cassandra.stress.StressAction$Consumer.run(StressAction.java:273)
Caused by: java.lang.IllegalArgumentException: replicate_on_write is not a column defined in this metadata
<snip..>
{noformat}","Debian Stable ""Wheezy""
Oracle JDK 1.7.0_51-b13",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-01-28 23:32:15.279,,,no_permission,,,,,,,,,,,,370525,,,Thu Feb 13 14:52:23 UTC 2014,,,,,,0|i1ruzr:,370835,,,,,,,,,,,,,,,,,,,"28/Jan/14 23:32;jasobrown;I ran into this, today, as well. Looks like the java driver 2.0-rc expects the replicate_on_write field to be present in the table metadata: https://github.com/datastax/java-driver/blob/2.0/driver-core/src/main/java/com/datastax/driver/core/TableMetadata.java#L472.",29/Jan/14 02:49;mshuler;Opened java-driver bug:  https://datastax-oss.atlassian.net/browse/JAVA-254,"31/Jan/14 13:41;beobal;On a related note, because the java-driver jars aren't included in the binary distro, stress can only be run from a binary install if you supply those yourself. Not sure it's worth addressing that as they would add ~6Mb to the tarball","31/Jan/14 14:21;jasobrown;The simple hack to get around it is to simply set replicate_on_write to false (or true, depending on your disposition) and skip getting the column from the Row. But then you're hacking up a dependent jar, which, again, depends on your disposition....","31/Jan/14 14:23;beobal;The 6Mb I referred to earlier is due to requiring the jar containing all the driver dependencies, but it may not be the case that this is all required. Certainly if ClusterBuilder.withoutMetrics() is used when constructing stress's native client, some of the dependency issues go away. However, there are still some remaining - firstly, java-driver 2.0 uses a later jackson lib than C*. There may be more once that's resolved but I haven't had chance to dig any further yet.","31/Jan/14 14:27;benedict;Perhaps we could add a script to download the necessary driver binary from the interwebs when it's first run?

Not sure unpicking the dependencies is a job worth doing every release.","31/Jan/14 15:39;slebresne;bq. java-driver 2.0 uses a later jackson lib than C*

That can be arranged. In fact, it's been on todo list for a long time to kill that dependency in the driver. The only reason it's there is because we still have some silly json encoding in our schema table that the driver reads. This is trivial enough that it's worth doing the parsing manually to avoid the jackson dependency. I just somehow never found the time to do it. That can be easily fixed though.

If we exclude codahale metrics (as Sam mentioned), I don't think the other dependencies will be a burden: it's really just netty, guava (and snappy/lza but that are optional for the driver),
both of which are probably easy enough to keep on compatible version between the driver and C*.

bq. Perhaps we could add a script to download the necessary driver binary from the interwebs when it's first run?

I suspect downloading stuffs from the interwebs might annoy some sensibilities. It's also annoying if your box doesn't have access to the interwebs (possible for security reason or because on the plane). Lastly you need to host one jar-with-dependency of the driver for each C* release out there, which is not the case (and I'm not volunteering in making that happen). It's possible, but not convince it's simpler/better.","31/Jan/14 15:44;benedict;If we can keep the best compatible Java Driver release for a C* release on compatible dependencies, that would obviously be best. But I expect that could get tricky with the different release schedules. Not sure we want to try too hard to keep them in sync, but it's your domain, so I won't question it :-)

It's not absolutely necessary to keep a version per C* release. It could point to, say, a minor-version link that is always the latest build for that Java Driver version. But I'm not super worried about any of this, tbh, just making a suggestion. I think whatever is least maintenance overhead is best.","10/Feb/14 23:36;mshuler;Tried to drop in current java-driver jars, and c* stress-build fails to build with:
{noformat}
stress-build:
    [mkdir] Created dir: /home/mshuler/git/cassandra/build/classes/stress
    [javac] Compiling 74 source files to /home/mshuler/git/cassandra/build/classes/stress
    [javac] /home/mshuler/git/cassandra/tools/stress/src/org/apache/cassandra/stress/util/JavaDriverClient.java:145: error: cannot find symbol
    [javac]         FBUtilities.waitOnFuture(cluster.shutdown());
    [javac]                                         ^
    [javac]   symbol:   method shutdown()
    [javac]   location: variable cluster of type Cluster
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 1 error

BUILD FAILED
{noformat}

Since it took me a bit to figure out how to get both jars we needed from java-driver:
{noformat}
git clone https://github.com/datastax/java-driver.git
cd java-driver/driver-core/
mvn package assembly:single
{noformat}

replace the 2.0.0-rc2 jars in $gitdir/cassandra/tools/lib/ with
- target/cassandra-driver-core-2.0.0-rc3-SNAPSHOT.jar
- target/cassandra-driver-core-2.0.0-rc3-SNAPSHOT-jar-with-dependencies.jar","12/Feb/14 17:33;slebresne;Committed [a fix|https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commit;h=0d0acac6c59d3fa703a3d504f9cfd063e4d111b7] for that (with just the driver jar, not the whole jar-with-dependencies as we don't really need that). It works on my box but if someone else can confirm that, we'll just close this issue. ","12/Feb/14 17:39;mshuler;+1
Thanks a bunch!","12/Feb/14 17:43;mshuler;hmm.. write worked, read throws an error:
{noformat}
mshuler@hana:~$ cassandra-stress write
Created keyspaces. Sleeping 1s for propagation.
Warming up WRITE with 50000 iterations...
Connected to cluster: Test Cluster
Datatacenter: datacenter1; Host: localhost/127.0.0.1; Rack: rack1
Sleeping 2s...
Running with 4 threadCount
Running WRITE with 4 threads until stderr of mean < 0.02
ops       ,    op/s,adj op/s,   key/s,    mean,     med,     .95,     .99,    .999,     max,   time,   stderr
26745     ,   26744,   26744,   26744,     0.1,     0.1,     0.3,     0.5,     4.5,    20.3,    1.0,  0.00000
57881     ,   31076,   33453,   31076,     0.1,     0.1,     0.2,     0.3,     1.0,    71.4,    2.0,  0.00000
91605     ,   33636,   36535,   33636,     0.1,     0.1,     0.2,     0.2,     0.3,    79.7,    3.0,  0.07880
124152    ,   32496,   36416,   32496,     0.1,     0.1,     0.2,     0.2,     0.9,   108.0,    4.0,  0.07319
156085    ,   31879,   35501,   31879,     0.1,     0.1,     0.2,     0.2,     1.1,   102.3,    5.0,  0.05969
188464    ,   32319,   35774,   32319,     0.1,     0.1,     0.2,     0.2,     0.7,    96.9,    6.0,  0.04857
220734    ,   32204,   35836,   32204,     0.1,     0.1,     0.2,     0.2,     0.5,   101.7,    7.0,  0.04110
256342    ,   36149,   36149,   36149,     0.1,     0.1,     0.2,     0.2,     0.5,     2.4,    8.0,  0.03562
292091    ,   32251,   35674,   32251,     0.1,     0.1,     0.2,     0.2,     1.1,   106.5,    9.1,  0.03157
324068    ,   31898,   35569,   31898,     0.1,     0.1,     0.2,     0.2,     1.1,   103.7,   10.1,  0.02817
356260    ,   32107,   35942,   32107,     0.1,     0.1,     0.2,     0.2,     1.2,   107.1,   11.1,  0.02541
387422    ,   31078,   34824,   31078,     0.1,     0.1,     0.2,     0.2,     2.1,   108.0,   12.1,  0.02321
418991    ,   31480,   35631,   31480,     0.1,     0.1,     0.2,     0.2,     1.1,   117.0,   13.1,  0.02128
450483    ,   31379,   35289,   31379,     0.1,     0.1,     0.2,     0.2,     0.8,   111.5,   14.1,  0.01967
483436    ,   32627,   32810,   32627,     0.1,     0.1,     0.2,     0.4,     1.1,     6.3,   15.1,  0.01827
514382    ,   30655,   34541,   30655,     0.1,     0.1,     0.2,     0.2,     1.0,   113.7,   16.1,  0.01757
538549    ,   24004,   27020,   24004,     0.2,     0.1,     0.3,     0.5,     6.6,   112.6,   17.1,  0.01649
562287    ,   23517,   25586,   23517,     0.2,     0.1,     0.3,     0.7,     2.8,    81.8,   18.2,  0.02035
591465    ,   28864,   30857,   28864,     0.1,     0.1,     0.2,     0.3,     0.4,    65.5,   19.2,  0.02398
617315    ,   25577,   27332,   25577,     0.2,     0.1,     0.2,     0.3,     1.6,    65.1,   20.2,  0.02327
646406    ,   28872,   30783,   28872,     0.1,     0.1,     0.2,     0.3,     1.1,    62.7,   21.2,  0.02417
672760    ,   26076,   27894,   26076,     0.1,     0.1,     0.2,     0.3,     1.8,    66.1,   22.2,  0.02339
698728    ,   25745,   25745,   25745,     0.2,     0.1,     0.2,     0.3,     1.2,    70.0,   23.2,  0.02362
732390    ,   33250,   33250,   33250,     0.1,     0.1,     0.2,     0.3,     1.2,    11.4,   24.2,  0.02469
764852    ,   32198,   35910,   32198,     0.1,     0.1,     0.2,     0.2,     1.4,   104.4,   25.2,  0.02365
796679    ,   31590,   35180,   31590,     0.1,     0.1,     0.2,     0.2,     2.8,   102.9,   26.2,  0.02294
828361    ,   31457,   35103,   31457,     0.1,     0.1,     0.2,     0.2,     1.0,   104.8,   27.2,  0.02216
860567    ,   31942,   35573,   31942,     0.1,     0.1,     0.2,     0.2,     0.4,   103.1,   28.2,  0.02142
891884    ,   31051,   34680,   31051,     0.1,     0.1,     0.2,     0.2,     2.3,   105.8,   29.3,  0.02077
927193    ,   35447,   35548,   35447,     0.1,     0.1,     0.2,     0.2,     0.5,     3.1,   30.3,  0.02009
927550    ,    3017,   29011,    3017,     1.3,     0.1,     0.2,   106.0,   106.3,   106.3,   30.4,  0.01952


Results:
real op rate              : 30542
adjusted op rate          : 30649
adjusted op rate stderr   : 0
key rate                  : 30542
latency mean              : 0.1
latency median            : 0.1
latency 95th percentile   : 0.2
latency 99th percentile   : 0.3
latency 99.9th percentile : 1.2
latency max               : 117.0
Total operation time      : 00:00:30
Sleeping for 15s
^C
mshuler@hana:~$ cassandra-stress read
Warming up READ with 50000 iterations...
Connected to cluster: Test Cluster
Datatacenter: datacenter1; Host: localhost/127.0.0.1; Rack: rack1
java.io.IOException: Operation [283] retried 10 times - error executing for key 00000F1E45 

        at org.apache.cassandra.stress.Operation.error(Operation.java:189)
        at org.apache.cassandra.stress.Operation.timeWithRetry(Operation.java:170)
        at org.apache.cassandra.stress.operations.ThriftReader.run(ThriftReader.java:53)
        at org.apache.cassandra.stress.StressAction$Consumer.run(StressAction.java:300)
Sleeping 2s...
Running with 4 threadCount
Running READ with 4 threads until stderr of mean < 0.02
ops       ,    op/s,adj op/s,   key/s,    mean,     med,     .95,     .99,    .999,     max,   time,   stderr
java.io.IOException: Operation [102] retried 10 times - error executing for key 00000EDF9C 

        at org.apache.cassandra.stress.Operation.error(Operation.java:189)
        at org.apache.cassandra.stress.Operation.timeWithRetry(Operation.java:170)
        at org.apache.cassandra.stress.operations.ThriftReader.run(ThriftReader.java:53)
        at org.apache.cassandra.stress.StressAction$Consumer.run(StressAction.java:300)
36        ,    2099,    2099,    2099,     1.8,     1.3,     3.4,    10.9,    10.9,    10.9,    0.0,  0.00000
FAILURE
mshuler@hana:~$
{noformat}","12/Feb/14 17:48;benedict;I've improved the error messages with CASSANDRA-6691, but I think that error is that the data it expects to be present isn't. Had you fully populated the key range?",12/Feb/14 17:50;mshuler;I'm running again until completion - my fault.,"12/Feb/14 17:55;mshuler;+2  ;)
Working fine with a little patience.",12/Feb/14 17:57;slebresne;Let's close that then,"13/Feb/14 14:10;benedict;Why don;t we need the dependencies file? Do you want users to download it?

It breaks the cassandra-stress bin as it stands, anyway, which isn't great IMO","13/Feb/14 14:44;slebresne;bq. Why don;t we need the dependencies file? Do you want users to download it?

No, I meant that all the dependencies the driver jar I put it needs should be in C* already. This does mean that the stress should include the C* lib/ jars in the classpath, which I assumed it does since I was able to run stress on my box with what is in trunk (and Michael seemed to have confirmed it). What problem do you see?","13/Feb/14 14:49;benedict;bq. What problem do you see?

{quote}
Exception in thread ""Thread-22"" java.lang.NoClassDefFoundError: com/codahale/metrics/Metric
	at com.datastax.driver.core.Cluster$Manager.<init>(Cluster.java:940)
	at com.datastax.driver.core.Cluster$Manager.<init>(Cluster.java:885)
	at com.datastax.driver.core.Cluster.<init>(Cluster.java:88)
	at com.datastax.driver.core.Cluster.buildFrom(Cluster.java:144)
	at com.datastax.driver.core.Cluster$Builder.build(Cluster.java:850)
	at org.apache.cassandra.stress.util.JavaDriverClient.connect(JavaDriverClient.java:74)
	at org.apache.cassandra.stress.settings.StressSettings.getJavaDriverClient(StressSettings.java:134)
	at org.apache.cassandra.stress.settings.StressSettings.getSmartThriftClient(StressSettings.java:49)
	at org.apache.cassandra.stress.StressAction$Consumer.run(StressAction.java:277)
Caused by: java.lang.ClassNotFoundException: com.codahale.metrics.Metric
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
{quote}

bq. This does mean that the stress should include the C* lib/ jars in the classpath

Right, I'll have a look at the script and check what's happening","13/Feb/14 14:52;benedict;Ah, I just needed to rebuild the project. Thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError: Verb COUNTER_MUTATION should not legally be dropped,CASSANDRA-6604,12689697,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,br1985,br1985,19/Jan/14 10:46,12/Mar/19 14:14,13/Mar/19 22:29,19/Jan/14 19:18,1.2.14,2.0.5,,,,,0,,,,,"We're seeing the following errors in our logs from time to time (about once an hour):

{code}
ERROR [MutationStage:10] 2014-01-19 10:36:26,659 CassandraDaemon.java (line 187) Exception in thread Thread[MutationStage:10,5,main]
java.lang.AssertionError: Verb COUNTER_MUTATION should not legally be dropped
        at org.apache.cassandra.net.MessagingService.incrementDroppedMessages(MessagingService.java:779)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1925)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
{code}

They always appear in groups. About 50-100 errors in a row.

We've got a 12-nodes cluster recently upgraded from 1.2.10 to 2.0.4. It's under pretty heavy load.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-01-19 19:18:17.202,,,no_permission,,,,,,,,,,,,368664,,,Thu Feb 06 23:47:39 UTC 2014,,,,,,0|i1rjjz:,368968,,,,,,,,,,,,,,,,,,,19/Jan/14 19:18;iamaleksey;It's cosmetic - COUNTER_MUTATION wasn't added to MS.DROPPABLE_VERBS. Ninja-fixed in 9be437bd7b268c88901745b361aba271f2dbe5ec.,19/Jan/14 22:42;jbellis;The real problem is that your server is overloaded enough to drop writes on the floor.,"06/Feb/14 23:47;elubow;I'm not entirely sure this is cosmetic.  We have a 20 node cluster running 1.2.13.2 (DSE 3.2.4) and it is not under a heavy load at all.  We are seeing this simultaneously with tens of thousands of dropped mutations under higher traffic scenarios and just thousands of mutations dropped under lower traffic scenarios.  However, it is only happening on a single node.  There is nothing (visibly) wrong with that node other than higher CPU utilization that the other nodes.  Even with the higher CPU utilization, it is still not a stressed node and yet we are seeing the dropped MUTATIONS and this error message (again only on this node).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ALTER TYPE RENAME hangs,CASSANDRA-6582,12688833,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,rhatch,rhatch,14/Jan/14 20:16,12/Mar/19 14:14,13/Mar/19 22:29,15/Jan/14 18:16,,,,,,,0,,,,,"I can't rename a user defined type using 'ALTER TYPE RENAME'.

Steps to reproduce:

{noformat}
[cqlsh 4.1.1 | Cassandra 2.1-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.39.0]
Use HELP for help.
cqlsh> 
rhatch@whatup:~/git/cstar/cassandra$ ccm clear
rhatch@whatup:~/git/cstar/cassandra$ ccm remove
rhatch@whatup:~/git/cstar/cassandra$ ccm create test_cluster
Current cluster is now: test_cluster
rhatch@whatup:~/git/cstar/cassandra$ ccm populate -n 1
rhatch@whatup:~/git/cstar/cassandra$ ccm start
rhatch@whatup:~/git/cstar/cassandra$ ccm node1 cqlsh
Connected to test_cluster at 127.0.0.1:9160.
[cqlsh 4.1.1 | Cassandra 2.1-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.39.0]
cqlsh> create keyspace user_type_renaming with replication = {'class':'SimpleStrategy', 'replication_factor':1} ;
cqlsh> use user_type_renaming ;
cqlsh:user_type_renaming>               CREATE TYPE simple_type (
                      ...               user_number int
                      ...               );
cqlsh:user_type_renaming>               ALTER TYPE simple_type rename to renamed_type;
{noformat}

And here's the log contents after the failure:

{noformat}
INFO  [MigrationStage:1] 2014-01-14 13:11:21,521 DefsTables.java:410 - Loading org.apache.cassandra.db.marshal.UserType(user_type_renaming,73696d706c655f74797065,757365725f6e756d626572:org.apache.cassandra.db.marshal.Int32Type)
ERROR [Thrift:1] 2014-01-14 13:11:36,684 CassandraDaemon.java:139 - Exception in thread Thread[Thrift:1,5,main]
java.lang.AssertionError: null
	at org.apache.cassandra.config.Schema.getKSMetaData(Schema.java:228) ~[main/:na]
	at org.apache.cassandra.cql3.statements.AlterTypeStatement$TypeRename.makeUpdatedType(AlterTypeStatement.java:357) ~[main/:na]
	at org.apache.cassandra.cql3.statements.AlterTypeStatement.announceMigration(AlterTypeStatement.java:108) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:71) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:194) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:228) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:218) ~[main/:na]
	at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1966) ~[main/:na]
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4486) ~[thrift/:na]
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4470) ~[thrift/:na]
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.1.jar:0.9.1]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.1.jar:0.9.1]
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:194) ~[main/:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_45]
	at java.lang.Thread.run(Thread.java:744) ~[na:1.7.0_45]
{noformat}","cassandra trunk-4910ce8
java version ""1.7.0_45""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-01-15 18:16:36.765,,,no_permission,,,,,,,,,,,,367800,,,Fri Jan 17 19:52:28 UTC 2014,,,,,,0|i1re9z:,368107,2.1 rc3,,,,,,,,,,,,,,,,,,"15/Jan/14 18:16;slebresne;Thanks for the report. It's actually a simple oversight, the type name with which we were renaming was not ""prepared"" with the current keyspace. Since it's a simple fix and it's 2.1 anyway, took the liberty to commit directly (commit 4da83a3).

Btw [~rhatch], I saw you already committed a few dtests, so for things like that that are clearly bugs, if you just want to commit a test demonstrating the breakage to dtests first and pointing to that failing test in the issue, that would be awesome (avoids that we both write a dtest for the same issue and it's even faster to try to reproduce :) -- I did committed a dtest for that issue btw).","17/Jan/14 19:52;rhatch;I can see my dtest passing on trunk now. Sorry, next time around I'll mention if there's dtests so you don't burn any time making a new one.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C* should be able to throttle batchlog processing,CASSANDRA-6550,12687294,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,04/Jan/14 01:53,12/Mar/19 14:14,13/Mar/19 22:29,05/Jan/14 00:27,1.2.14,2.0.5,,,,,0,,,,,"Was going to do it in CASSANDRA-6134, but this is important enough to be handled separately, and in 1.2, too.",,,,,,,,,,,,,,,,,,,,,,,,04/Jan/14 23:18;iamaleksey;6550.txt;https://issues.apache.org/jira/secure/attachment/12621480/6550.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-04 23:57:08.657,,,no_permission,,,,,,,,,,,,366292,,,Sun Jan 05 00:27:04 UTC 2014,,,,,,0|i1r50v:,366603,,,,,,,,jbellis,jbellis,,,,,,,,,rbranson,"04/Jan/14 01:57;iamaleksey;There was no need for this originally, back when batchlog used to simply write hints, and never attempted to actually deliver anything. Post CASSANDRA-5314 lacking any throttling options creates some issues.","04/Jan/14 23:20;iamaleksey;It does change the default behavior, so including this in 1.2 is probably not ideal. But lacking throttle is more of a bug than a feature, so I'd rather have it in 1.2.",04/Jan/14 23:57;jbellis;+1,"05/Jan/14 00:27;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JAVA_HOME on cassandra-env.sh is ignored on Debian packages,CASSANDRA-6131,12671937,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,sebastianlacuesta,sebastianlacuesta,02/Oct/13 14:24,12/Mar/19 14:14,13/Mar/19 22:29,02/Jan/14 23:13,2.0.5,,,Packaging,,,0,qa-resolved,,,,"I've just got upgraded to 2.0.1 package from the apache repositories using apt. I had the JAVA_HOME environment variable set in /etc/cassandra/cassandra-env.sh but after the upgrade it only worked by setting it on /usr/sbin/cassandra script. I can't configure java 7 system wide, only for cassandra.
Off-toppic: Thanks for getting rid of the jsvc mess.",,,,,,,,,,,,,,,,,,,,,,,,20/Dec/13 04:40;mshuler;6131-2.patch;https://issues.apache.org/jira/secure/attachment/12619746/6131-2.patch,14/Oct/13 13:35;urandom;6131.patch;https://issues.apache.org/jira/secure/attachment/12608279/6131.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-10-03 15:33:11.22,,,no_permission,,,,,,,,,,,,351563,,,Wed Apr 30 18:24:20 UTC 2014,,,,,,0|i1om5r:,351852,,,,,,,,,,,,,,,,,,mshuler,03/Oct/13 15:33;urandom;Could you expound on this a bit?  Are you trying to _set_ {{JAVA_HOME}} from within {{cassandra-env.sh}}?,"03/Oct/13 16:32;sebastianlacuesta;Yes, I thought it was the way I should do this, in fact (due to time
constraints) I've just set it at /usr/sbin/cassandra (I know it's really
_awfull_). I used to set JAVA_HOME at /etc/cassandra/cassandra-env.sh and
it worked until 2.0.0. I need to use java 6 as the  default jvm on my
development machine, but jvm 7 just for cassandra, if I'm really wrong on
how I'm proceeding, I'd be glad to know what's the way to set it up.
Thanks a lot!


2013/10/3 Eric Evans (JIRA) <jira@apache.org>

","03/Oct/13 19:39;urandom;If setting {{JAVA_HOME}} from {{cassandra-env.sh}} ever worked before (from the Debian package), it was probably by accident, but there is no reason we can't support it going forward.

For what it's worth, I'd probably recommend using {{/etc/default/cassandra}} for Debian/Ubuntu, but it will work with either.

[~sebastianlacuesta], could you test the attached patch and let me know if this solves it for you?","07/Oct/13 19:04;sebastianlacuesta;Tried the patch with the 2.0.1 source tarball from the debian src file extracted from .dsc,
{code}
patching file debian/init
Hunk #4 FAILED at 53.
Hunk #5 FAILED at 95.
2 out of 5 hunks FAILED -- saving rejects to file debian/init.rej
{code}
content of debian/init.rej:
{code:title=debian/init.rej|borderStyle=solid}
--- debian/init
+++ debian/init
@@ -53,10 +29,6 @@
 # Depend on lsb-base (>= 3.0-6) to ensure that this file is present.
 . /lib/lsb/init-functions
 
-# If JNA is installed, add it to EXTRA_CLASSPATH
-#
-EXTRA_CLASSPATH=""/usr/share/java/jna.jar:$EXTRA_CLASSPATH""
-
 #
 # Function that returns 0 if process is running, or nonzero if not.
 #
@@ -95,7 +67,7 @@
     [ -e `dirname ""$PIDFILE""` ] || \
         install -d -ocassandra -gcassandra -m750 `dirname $PIDFILE`
 
-    export EXTRA_CLASSPATH
+
 
     start-stop-daemon -S -c cassandra -a /usr/sbin/cassandra -q -p ""$PIDFILE"" -t >/dev/null || return 1
{code}","14/Oct/13 13:35;urandom;Rebased to {{cassandra-2.0}} branch.
",14/Oct/13 13:37;urandom;[~sebastianlacuesta]: that patch is meant to apply to the 2.0 branch (where it will it land); are you able to test against the 2.0 branch?,19/Dec/13 23:17;jbellis;[~mshuler] can you test the above?,20/Dec/13 04:40;mshuler;6131-2.patch has the first hunk modified for current cassandra-2.0 branch (JVM_SEARCH_DIRS was different and failed),02/Jan/14 17:29;mshuler;Works fine for me with setting JAVA_HOME in the recommended location of /etc/default/cassandra or in cassandra-env.sh,02/Jan/14 23:13;urandom;thanks Michael; committed,"30/Apr/14 18:24;sebastianlacuesta;Works for me. Thanks!



",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cleanup ClassCastException,CASSANDRA-6462,12683593,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,Andie78,Andie78,09/Dec/13 12:24,12/Mar/19 14:14,13/Mar/19 22:29,10/Dec/13 04:35,2.0.4,,,Legacy/Tools,,,0,cleanup,compaction,,,"I enlarged the cluseter from 4 to 8 nodes. During cleaning up the ""old"" nodes with ""nodetool cleanup"" it breaks up with exception. I started cleanup from a different computer to manage them sequentially.
{panel:title=cmd.exe}
Error occurred during cleanup
java.util.concurrent.ExecutionException: java.lang.ClassCastException: org.apach
e.cassandra.io.sstable.SSTableReader$EmptyCompactionScanner cannot be cast to or
g.apache.cassandra.io.sstable.SSTableScanner
        at java.util.concurrent.FutureTask.report(Unknown Source)
        at java.util.concurrent.FutureTask.get(Unknown Source)
        at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTabl
eOperation(CompactionManager.java:227)
        at org.apache.cassandra.db.compaction.CompactionManager.performCleanup(C
ompactionManager.java:265)
        at org.apache.cassandra.db.ColumnFamilyStore.forceCleanup(ColumnFamilySt
ore.java:1054)
        at org.apache.cassandra.service.StorageService.forceKeyspaceCleanup(Stor
ageService.java:2038)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at sun.reflect.misc.Trampoline.invoke(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at sun.reflect.misc.MethodUtil.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown So
urce)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown So
urce)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(Unknown Source)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(Unknown Source)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(Unknown
Source)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(Unknown Sou
rce)
        at javax.management.remote.rmi.RMIConnectionImpl.access$300(Unknown Sour
ce)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run
(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(U
nknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at sun.rmi.server.UnicastServerRef.dispatch(Unknown Source)
        at sun.rmi.transport.Transport$1.run(Unknown Source)
        at sun.rmi.transport.Transport$1.run(Unknown Source)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(Unknown Sou
rce)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(Unknown Sour
ce)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.ClassCastException: org.apache.cassandra.io.sstable.SSTable
Reader$EmptyCompactionScanner cannot be cast to org.apache.cassandra.io.sstable.
SSTableScanner
        at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompact
ion(CompactionManager.java:563)
        at org.apache.cassandra.db.compaction.CompactionManager.access$400(Compa
ctionManager.java:62)
        at org.apache.cassandra.db.compaction.CompactionManager$5.perform(Compac
tionManager.java:274)
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(Compactio
nManager.java:222)
        at java.util.concurrent.FutureTask.run(Unknown Source)
        ... 3 more
{panel}",Windows 7 / Java 1.7.0.25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-12-10 04:33:40.124,,,no_permission,,,,,,,,,,,,362665,,,Thu Dec 12 16:47:18 UTC 2013,,,,,,0|i1qiin:,362959,2.0.3,,,,,,,,,,,2.0.1,,,,,,,"09/Dec/13 17:20;Andie78;Cleanup on that node not working anymore. There are already problems after restart of the cassandra service. Any ideas how to fix? If u don't have idea as well I will try to repair, but maybe cannot reproduce anymore.
{panel:title=system.log}
ERROR [CompactionExecutor:25] 2013-12-09 17:50:14,932 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:25,1,RMI Runtime]
java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@1dbf287 rejected from org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor@10f8a54[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 780]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.reject(Unknown Source)
	at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(Unknown Source)
	at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(Unknown Source)
	at java.util.concurrent.ScheduledThreadPoolExecutor.submit(Unknown Source)
	at org.apache.cassandra.io.sstable.SSTableDeletingTask.schedule(SSTableDeletingTask.java:66)
	at org.apache.cassandra.io.sstable.SSTableReader.releaseReference(SSTableReader.java:1105)
	at org.apache.cassandra.db.DataTracker.removeOldSSTablesSize(DataTracker.java:388)
	at org.apache.cassandra.db.DataTracker.postReplace(DataTracker.java:353)
	at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:347)
	at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:252)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:1078)
	at org.apache.cassandra.db.compaction.CompactionTask.replaceCompactedSSTables(CompactionTask.java:296)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:242)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
ERROR [ValidationExecutor:1] 2013-12-09 17:53:16,411 Validator.java (line 242) Failed creating a merkle tree for [repair #65efe6e0-60f2-11e3-ac47-eb1c24a59bb8 on nieste/nfiles, (3837863087520363054,3847126934337600104]], /10.9.9.240 (see log for details)
ERROR [ValidationExecutor:1] 2013-12-09 17:53:16,421 CassandraDaemon.java (line 187) Exception in thread Thread[ValidationExecutor:1,1,main]
FSWriteError in D:\Programme\cassandra\data\nieste\nfiles\snapshots\65efe6e0-60f2-11e3-ac47-eb1c24a59bb8\nieste-nfiles-jb-19878-Index.db
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:120)
	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:382)
	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:378)
	at org.apache.cassandra.db.Directories.clearSnapshot(Directories.java:416)
	at org.apache.cassandra.db.ColumnFamilyStore.clearSnapshot(ColumnFamilyStore.java:1801)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:810)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
	at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.nio.file.FileSystemException: D:\Programme\cassandra\data\nieste\nfiles\snapshots\65efe6e0-60f2-11e3-ac47-eb1c24a59bb8\nieste-nfiles-jb-19878-Index.db: Der Prozess kann nicht auf die Datei zugreifen, da sie von einem anderen Prozess verwendet wird.

	at sun.nio.fs.WindowsException.translateToIOException(Unknown Source)
	at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
	at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(Unknown Source)
	at sun.nio.fs.AbstractFileSystemProvider.delete(Unknown Source)
	at java.nio.file.Files.delete(Unknown Source)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:116)
	... 11 more
ERROR [CompactionExecutor:3] 2013-12-09 17:54:16,810 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:3,1,RMI Runtime]
java.lang.ClassCastException: org.apache.cassandra.io.sstable.SSTableReader$EmptyCompactionScanner cannot be cast to org.apache.cassandra.io.sstable.SSTableScanner
	at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:563)
	at org.apache.cassandra.db.compaction.CompactionManager.access$400(CompactionManager.java:62)
	at org.apache.cassandra.db.compaction.CompactionManager$5.perform(CompactionManager.java:274)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:222)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
{panel}",10/Dec/13 04:33;jbellis;Introduced by CASSANDRA-2524,10/Dec/13 04:35;jbellis;should be fixed in 2.0 HEAD by 5d24c55335e91fde5470ce94306fde26272e2b44,"12/Dec/13 12:27;Andie78;What can I do now? repair doesn't help and major compact doesn't help. Cleanup still stops with SSTableReader / ClassCastException.
Reset node (delete all and start new)? Wait for 2.0.4? Tx.","12/Dec/13 15:15;jbellis;That, or cherry pick the fix yourself.",12/Dec/13 16:47;Andie78;I will continue in the 2nd week of January. Company will close and I have just one day left... Marry X-Mas to @ ! ;-),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix dropping columns,CASSANDRA-6520,12685917,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mishail,rhatch,rhatch,20/Dec/13 23:06,12/Mar/19 14:14,13/Mar/19 22:29,03/Jan/14 22:54,2.1 beta1,,,Legacy/CQL,,,0,,,,,"Using ccqlsh, I issue a statement to drop a column from a table, and the session appears to disconnect.

The statement was:
{noformat}
cqlsh:taskapp> alter table user_task drop task_order;
{noformat}
Here's the full setup I used:
{noformat}
ccm create test_cluster
ccm populate -n 3
ccm start
ccm node1 cqlsh

CREATE KEYSPACE taskapp WITH replication = {
  'class': 'SimpleStrategy',
  'replication_factor': '3'
};

use taskapp;

create table user (
    user_id timeuuid PRIMARY KEY,
    first_name text,
    last_name text,
    email text
);

create table user_task (
    task_id timeuuid PRIMARY KEY,
    user_id timeuuid,
    task_order int,
    task_description text,
    is_complete boolean,
    is_top_level boolean,
    subtask_ids list<timeuuid>
);
{noformat}
and then the statement which triggers the disconnect:
{noformat}
cqlsh:taskapp> alter table user_task drop task_order;
TSocket read 0 bytes
TSocket read 0 bytes
cqlsh:taskapp> describe table user_task;

[Errno 32] Broken pipe
{noformat}

The log for the active node shows this INFO, followed immediately by an exception (included below). The other nodes show no relevant messages:
{noformat}
INFO  [Thrift:4] 2013-12-20 16:04:58,668 MigrationManager.java:263 - Update ColumnFamily 'taskapp/user_task' From org.apache.cassandra.config.CFMetaData@15e4ed88[cfId=df7153ac-c309-3bd2-92c2-e05bb53153fb,ksName=taskapp,cfName=user_task,cfType=Standard,comparator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.ColumnToCollectionType(7375627461736b5f696473:org.apache.cassandra.db.marshal.ListType(org.apache.cassandra.db.marshal.TimeUUIDType))),comment=,readRepairChance=0.1,dclocalReadRepairChance=0.0,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.TimeUUIDType,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=11 cap=11]=ColumnDefinition{name=subtask_ids, type=org.apache.cassandra.db.marshal.ListType(org.apache.cassandra.db.marshal.TimeUUIDType), kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=11 cap=11]=ColumnDefinition{name=is_complete, type=org.apache.cassandra.db.marshal.BooleanType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=12 cap=12]=ColumnDefinition{name=is_top_level, type=org.apache.cassandra.db.marshal.BooleanType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=7 cap=7]=ColumnDefinition{name=task_id, type=org.apache.cassandra.db.marshal.TimeUUIDType, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=task_order, type=org.apache.cassandra.db.marshal.Int32Type, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=7 cap=7]=ColumnDefinition{name=user_id, type=org.apache.cassandra.db.marshal.TimeUUIDType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=16 cap=16]=ColumnDefinition{name=task_description, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=0.01,memtable_flush_period_in_ms=0,caching=KEYS_ONLY,defaultTimeToLive=0,speculative_retry=99.0PERCENTILE,indexInterval=128,populateIoCacheOnFlush=false,droppedColumns={},triggers={}] To org.apache.cassandra.config.CFMetaData@3568f812[cfId=df7153ac-c309-3bd2-92c2-e05bb53153fb,ksName=taskapp,cfName=user_task,cfType=Standard,comparator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.ColumnToCollectionType(7375627461736b5f696473:org.apache.cassandra.db.marshal.ListType(org.apache.cassandra.db.marshal.TimeUUIDType))),comment=,readRepairChance=0.1,dclocalReadRepairChance=0.0,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.TimeUUIDType,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=11 cap=11]=ColumnDefinition{name=subtask_ids, type=org.apache.cassandra.db.marshal.ListType(org.apache.cassandra.db.marshal.TimeUUIDType), kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=11 cap=11]=ColumnDefinition{name=is_complete, type=org.apache.cassandra.db.marshal.BooleanType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=12 cap=12]=ColumnDefinition{name=is_top_level, type=org.apache.cassandra.db.marshal.BooleanType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=7 cap=7]=ColumnDefinition{name=task_id, type=org.apache.cassandra.db.marshal.TimeUUIDType, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=7 cap=7]=ColumnDefinition{name=user_id, type=org.apache.cassandra.db.marshal.TimeUUIDType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=16 cap=16]=ColumnDefinition{name=task_description, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=0.01,memtable_flush_period_in_ms=0,caching=KEYS_ONLY,defaultTimeToLive=0,speculative_retry=99.0PERCENTILE,indexInterval=128,populateIoCacheOnFlush=false,droppedColumns={task_order=1387580698664000},triggers={}]
ERROR [Thrift:4] 2013-12-20 16:04:58,672 CustomTThreadPoolServer.java:212 - Error occurred during processing of message.
java.lang.ClassCastException: java.lang.Long cannot be cast to java.util.Map
	at org.apache.cassandra.serializers.MapSerializer.serialize(MapSerializer.java:27) ~[main/:na]
	at org.apache.cassandra.db.marshal.AbstractType.decompose(AbstractType.java:71) ~[main/:na]
	at org.apache.cassandra.db.CFRowAdder.add(CFRowAdder.java:78) ~[main/:na]
	at org.apache.cassandra.db.CFRowAdder.addMapEntry(CFRowAdder.java:65) ~[main/:na]
	at org.apache.cassandra.config.CFMetaData.toSchemaNoColumnsNoTriggers(CFMetaData.java:1610) ~[main/:na]
	at org.apache.cassandra.config.CFMetaData.toSchemaUpdate(CFMetaData.java:1483) ~[main/:na]
	at org.apache.cassandra.service.MigrationManager.announceColumnFamilyUpdate(MigrationManager.java:264) ~[main/:na]
	at org.apache.cassandra.cql3.statements.AlterTableStatement.announceMigration(AlterTableStatement.java:217) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:71) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:194) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:228) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:218) ~[main/:na]
	at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1966) ~[main/:na]
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4486) ~[thrift/:na]
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4470) ~[thrift/:na]
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.1.jar:0.9.1]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.1.jar:0.9.1]
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:194) ~[main/:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]
	at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]
{noformat}","C* from trunk -- cassandra-2.0.3-709-g486f079
java 1.7.0_45 (on linux 64 bit)
[cqlsh 4.1.0 | Cassandra 2.1-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.39.0]

3 node cluster built on my machine using ccm",,,,,,,,,,,,,,,,,,,,,,,21/Dec/13 04:08;mishail;trunk-6520.patch;https://issues.apache.org/jira/secure/attachment/12619961/trunk-6520.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-21 04:08:16.288,,,no_permission,,,,,,,,,,,,364986,,,Fri Jan 03 22:54:09 UTC 2014,,,,,,0|i1qwuf:,365295,,,,,,,,iamaleksey,iamaleksey,,,2.1 rc3,,,,,,,21/Dec/13 04:08;mishail;I guess we should use {{valueComparator}} for collection types in this case. Patch attached,"03/Jan/14 22:54;iamaleksey;Committed, thanks again [~mishail]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig support for hadoop CqlInputFormat,CASSANDRA-6454,12682961,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,alexliu68,alexliu68,alexliu68,05/Dec/13 18:12,12/Mar/19 14:14,13/Mar/19 22:29,23/Jul/14 15:32,2.0.10,2.1.1,,,,,1,,,,,"CASSANDRA-6311 adds new CqlInputFormat, we need add the Pig support for it",,,,,,,,,,,,,,CASSANDRA-6311,,,,,,,,,,10/Dec/13 03:22;alexliu68;6454-2.0-branch.txt;https://issues.apache.org/jira/secure/attachment/12617971/6454-2.0-branch.txt,19/May/14 22:38;alexliu68;6454-v2-2.0-branch.txt;https://issues.apache.org/jira/secure/attachment/12645661/6454-v2-2.0-branch.txt,11/Jul/14 20:25;alexliu68;6454-v3-2.0-branch.txt;https://issues.apache.org/jira/secure/attachment/12655290/6454-v3-2.0-branch.txt,11/Jul/14 20:15;alexliu68;6454-v3-2.1-branch.txt;https://issues.apache.org/jira/secure/attachment/12655287/6454-v3-2.1-branch.txt,22/Jul/14 18:29;alexliu68;6454-v4-2.0-branch.txt;https://issues.apache.org/jira/secure/attachment/12657153/6454-v4-2.0-branch.txt,22/Jul/14 18:29;alexliu68;6454-v4-2.1-branch.txt;https://issues.apache.org/jira/secure/attachment/12657152/6454-v4-2.1-branch.txt,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2014-05-16 12:09:02.621,,,no_permission,,,,,,,,,,,,362218,,,Wed Jul 23 15:32:19 UTC 2014,,,,,,0|i1qfrr:,362513,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,10/Dec/13 03:22;alexliu68;The patch is on top of CASSANDRA-6311. It creates CqlNativeStorage for new CqlInputFormat.,"16/May/14 12:09;ShridharB;[~alexliu68] I  tried to run pig script to load data with CqlNativeStorage but getting problems, looks like jar conflicts or may be something else can you please let me know the required jars and their version needed to run CqlNativeStorage . Below are the things i tried.
1.Applied this patch on top of Cassandra 2.0.07. 
2.When i tried to run pig script with CqlNativeStorage it threw me ""ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2998: Unhandled internal error. com/datastax/driver/core/policies/LoadBalancingPolicy"" exception. 
3.Then  added ""cassandra-driver-core-2.0.1.jar"" in my classpath after this i got an exception ""ERROR org.apache.pig.tools.pigstats.SimplePigStats - ERROR: com.codahale.metrics.Metric .......
....
Caused by: java.lang.ClassNotFoundException: com.codahale.metrics.Metric"" 
4.Then  added ""metrics-core-3.0.2.jar"". After adding this jar file i was able to run the job but failed and my hadoop log shows me this exception
""Caused by: com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: slave1.tpgsi.com/xxxx.yyy.zzz.aa (com.datastax.driver.core.TransportException: [slave1/xxxx.yyy.zzz.aa] Error writing))
NOTE: metrics-core-2.2.0.jar already exists in C* 2.0.7 lib folder.","16/May/14 18:53;alexliu68;The patch is a little of off, I will update it to the latest code. ","19/May/14 18:21;alexliu68;[~minishri] Do you enable native port 9042 on all the nodes? If the native port is different from 9042, set it in url native_port=<native_port>","19/May/14 22:44;alexliu68;The latest patch is attached. To use CqlNativeStorage, the following parameters need to be specified
{code}
input_cql=     (It must in the following format 
 * 1) select clause must include partition key columns (to calculate the progress based on the actual CF row processed)
 * 2) where clause must include token(partition_key1, ...  , partition_keyn) > ? and 
 *       token(partition_key1, ... , partition_keyn) <= ?  (in the right order) 

native_port=  (If it's not default port)

other parameters are
[&native_port=<native_port>][&core_conns=<core_conns>]
[&max_conns=<max_conns>][&min_simult_reqs=<min_simult_reqs>][&max_simult_reqs=<max_simult_reqs>]
[&native_timeout=<native_timeout>][&native_read_timeout=<native_read_timeout>][&rec_buff_size=<rec_buff_size>]
[&send_buff_size=<send_buff_size>][&solinger=<solinger>][&tcp_nodelay=<tcp_nodelay>]
[&reuse_address=<reuse_address>]
[&keep_alive=<keep_alive>][&auth_provider=<auth_provider>][&trust_store_path=<trust_store_path>]
[&key_store_path=<key_store_path>][&trust_store_password=<trust_store_password>]
[&key_store_password=<key_store_password>][&cipher_suites=<cipher_suites>][&input_cql=<input_cql>]]
{code}
","28/May/14 15:25;ShridharB;[~alexliu68] I have applied the new patch. After this am not able to load data from Cassandra. It throws me an exception 

Caused by: InvalidRequestException(why:Keyspace '' does not exist)
        at org.apache.cassandra.thrift.Cassandra$set_keyspace_result$set_keyspace_resultStandardScheme.read(Cassandra.java:8906)
        at org.apache.cassandra.thrift.Cassandra$set_keyspace_result$set_keyspace_resultStandardScheme.read(Cassandra.java:8892)
        at org.apache.cassandra.thrift.Cassandra$set_keyspace_result.read(Cassandra.java:8842)
        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_set_keyspace(Cassandra.java:599)
        at org.apache.cassandra.thrift.Cassandra$Client.set_keyspace(Cassandra.java:586)
        at org.apache.cassandra.hadoop.pig.AbstractCassandraStorage.initSchema(AbstractCassandraStorage.java:493)


Table schema :
CREATE TABLE dogs (
  block_id int,
  breed text,
  color text,
  short_hair boolean,
  PRIMARY KEY ((block_id, breed), color, short_hair))

My pig load function would be like this
data =  load 'cql://excelsior/dogs' using org.apache.cassandra.hadoop.pig.CqlNativeStorage();

Also tried with this 
a = load 'cql://excelsior/dogs?input_cql=select block_id,breed from excelsior.dogs where token(block_id,breed) > ? and token(block_id,breed) <= ? and block_id=5 and breed='Bulldog' ' using org.apache.cassandra.hadoop.pig.CqlNativeStorage();



Still same exception. ","28/May/14 16:11;alexliu68;There are a few unit testing examples at CqlTableTest. input_cql needs encoded. 

{code}
using org.apache.cassandra.hadoop.pig.CqlNativeStorage();
should be using CqlNativeStorage();
{code}","10/Jul/14 15:28;brandon.williams;Looks good, though some of those params feel like YAGNI to me, but I guess if we already have them we might as well put them in.  Can you post a 2.1 branch too?","10/Jul/14 17:58;brandon.williams;For 2.1, it's probably a good idea to switch all the tests to native.","11/Jul/14 16:49;alexliu68;I got some test errors for compact tables using CqlStorage in CASSANDRA-7059, I hope change them to CqlNativeStorage fixes the issue.","11/Jul/14 16:51;brandon.williams;Yes, that's a bug in CPRR, I'm fine with moving away from it.","11/Jul/14 20:02;alexliu68;I also got a lib conflict issue as following

{code}
   [junit] ------------- ---------------- ---------------
    [junit] Testcase: testCassandraStorageCompositeColumnCF(org.apache.cassandra.pig.ThriftColumnFamilyTest):	Caused an ERROR
    [junit] org.antlr.runtime.tree.BaseTree.insertChild(ILjava/lang/Object;)V
    [junit] java.lang.NoSuchMethodError: org.antlr.runtime.tree.BaseTree.insertChild(ILjava/lang/Object;)V
    [junit] 	at org.apache.pig.parser.QueryParser.paren_expr(QueryParser.java:17532)
    [junit] 	at org.apache.pig.parser.QueryParser.cast_expr(QueryParser.java:17005)
    [junit] 	at org.apache.pig.parser.QueryParser.multi_expr(QueryParser.java:15679)
    [junit] 	at org.apache.pig.parser.QueryParser.expr(QueryParser.java:15568)
    [junit] 	at org.apache.pig.parser.QueryParser.unary_cond(QueryParser.java:15324)
    [junit] 	at org.apache.pig.parser.QueryParser.not_cond(QueryParser.java:14951)
    [junit] 	at org.apache.pig.parser.QueryParser.and_cond(QueryParser.java:14828)
    [junit] 	at org.apache.pig.parser.QueryParser.cond(QueryParser.java:14728)
    [junit] 	at org.apache.pig.parser.QueryParser.filter_clause(QueryParser.java:10509)
    [junit] 	at org.apache.pig.parser.QueryParser.op_clause(QueryParser.java:7092)
    [junit] 	at org.apache.pig.parser.QueryParser.general_statement(QueryParser.java:2314)
    [junit] 	at org.apache.pig.parser.QueryParser.statement(QueryParser.java:1579)
    [junit] 	at org.apache.pig.parser.QueryParser.query(QueryParser.java:395)
    [junit] 	at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:236)
    [junit] 	at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:179)
    [junit] 	at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1678)
    [junit] 	at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1625)
    [junit] 	at org.apache.pig.PigServer.registerQuery(PigServer.java:577)
    [junit] 	at org.apache.pig.PigServer.registerQuery(PigServer.java:590)
    [junit] 	at org.apache.cassandra.pig.ThriftColumnFamilyTest.testCassandraStorageCompositeColumnCF(ThriftColumnFamilyTest.java:624)
{code}

Pig 0.12 is built with antlr 3.4.
Cassandra uses antler 3.2

Other than that, CqlNativeStorage passes all the tests","11/Jul/14 20:26;alexliu68;I comment out that test case, we can fix it later  in another ticket",18/Jul/14 17:59;brandon.williams;Do we actually need cassandra_pig.yaml?  It seems like all we're doing is turning on the native proto which should be fine in the standard config.,21/Jul/14 22:36;alexliu68;cassandra_pig.yaml uses Murmur3Partitioner instead of ByteOrderedPartitioner. ByteOrderedPartitioner is used for other unit tests.,21/Jul/14 22:45;brandon.williams;Is there a reason we need to switch to it for this? (not that I disagree with it at all),"22/Jul/14 00:52;alexliu68;Most of Cassandra deployment uses Murmur3Partitioner, testing on Murmur3Partitioner covers the general case. Some unit tests still use old ByteOrderedPartitioner, so just update the cassanra.yaml to Murmur3Partitioner breaks other unit tests. That's the reason I create a new yaml file.","22/Jul/14 06:47;jbellis;I'm -0 on adding extra yaml files, fwiw.","22/Jul/14 14:40;brandon.williams;I'm +1 on switching to M3 in principle, but it seems highly unlikely that we're going to catch any problems here with the handful of records we use in the pig tests.

bq. I'm -0 on adding extra yaml files, fwiw.

I agree, this doesn't seem like the time or place to be worried about BOP vs M3.  Let's just take that out and use the default yaml for now.

",22/Jul/14 18:29;alexliu68;V4 rollback cassandra_pig.yaml,23/Jul/14 15:32;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Loss of secondary index entries if nodetool cleanup called before compaction,CASSANDRA-6517,12685801,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,beobal,awerrch,awerrch,20/Dec/13 12:40,12/Mar/19 14:14,13/Mar/19 22:29,31/Jan/14 15:45,2.0.5,,,Feature/2i Index,Legacy/CQL,,1,,,,,"From time to time we had the feeling of not getting all results that should have been returned using secondary indexes. Now we tracked down some situations and found out, it happened:

1) To primary keys that were already deleted and have been re-created later on

2) After our nightly maintenance scripts were running

We can reproduce now the following szenario:

- create a row entry with an indexed column included
- query it and use the secondary index criteria -> Success
- delete it, query again -> entry gone as expected
- re-create it with the same key, query it -> success again

Now use in exactly that sequence

nodetool cleanup
nodetool flush
nodetool compact

When issuing the query now, we don't get the result using the index. The entry is indeed available in it's table when I just ask for the key. Below is the exact copy-paste output from CQL when I reproduced the problem with an example entry on on of our tables.

mwerrch@mstc01401:/opt/cassandra$ current/bin/cqlsh Connected to 14-15-Cluster at localhost:9160.
[cqlsh 4.1.0 | Cassandra 2.0.3 | CQL spec 3.1.1 | Thrift protocol 19.38.0] Use HELP for help.
cqlsh> use mwerrch;
cqlsh:mwerrch> desc tables;

B4Container_Demo

cqlsh:mwerrch> desc table ""B4Container_Demo"";

CREATE TABLE ""B4Container_Demo"" (
  key uuid,
  archived boolean,
  bytes int,
  computer int,
  deleted boolean,
  description text,
  doarchive boolean,
  filename text,
  first boolean,
  frames int,
  ifversion int,
  imported boolean,
  jobid int,
  keepuntil bigint,
  nextchunk text,
  node int,
  recordingkey blob,
  recstart bigint,
  recstop bigint,
  simulationid bigint,
  systemstart bigint,
  systemstop bigint,
  tapelabel bigint,
  version blob,
  PRIMARY KEY (key)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='demo' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=604800 AND
  index_interval=128 AND
  read_repair_chance=1.000000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='NONE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};

CREATE INDEX mwerrch_Demo_computer ON ""B4Container_Demo"" (computer);

CREATE INDEX mwerrch_Demo_node ON ""B4Container_Demo"" (node);

CREATE INDEX mwerrch_Demo_recordingkey ON ""B4Container_Demo"" (recordingkey);

cqlsh:mwerrch> INSERT INTO ""B4Container_Demo"" (key,computer,node) VALUES (78c70562-1f98-3971-9c28-2c3d8e09c10f, 50, 50); cqlsh:mwerrch> select key,node,computer from ""B4Container_Demo"" where computer=50;

 key                                  | node | computer
--------------------------------------+------+----------
 78c70562-1f98-3971-9c28-2c3d8e09c10f |   50 |       50

(1 rows)

cqlsh:mwerrch> DELETE FROM ""B4Container_Demo"" WHERE key=78c70562-1f98-3971-9c28-2c3d8e09c10f;
cqlsh:mwerrch> select key,node,computer from ""B4Container_Demo"" where computer=50;

(0 rows)

cqlsh:mwerrch> INSERT INTO ""B4Container_Demo"" (key,computer,node) VALUES (78c70562-1f98-3971-9c28-2c3d8e09c10f, 50, 50); cqlsh:mwerrch> select key,node,computer from ""B4Container_Demo"" where computer=50;

 key                                  | node | computer
--------------------------------------+------+----------
 78c70562-1f98-3971-9c28-2c3d8e09c10f |   50 |       50

(1 rows)

**********************************
Now we execute (maybe from a different shell so we don't have to close this session) from /opt/cassandra/current/bin directory:
./nodetool cleanup
./nodetool flush
./nodetool compact


Going back to our CQL session the result will no longer be available if queried via the index:
*********************************

cqlsh:mwerrch> select key,node,computer from ""B4Container_Demo"" where computer=50;

(0 rows)
",Ubuntu 12.0.4 with 8+ GB RAM and 40GB hard disk for data directory.,,,,,,,,,,,,,,,,,,,,,,,13/Jan/14 15:32;beobal;0001-CASSANDRA-6517-Use-column-timestamp-to-check-for-del.patch;https://issues.apache.org/jira/secure/attachment/12622641/0001-CASSANDRA-6517-Use-column-timestamp-to-check-for-del.patch,13/Jan/14 15:32;beobal;repro.sh;https://issues.apache.org/jira/secure/attachment/12622640/repro.sh,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-12-20 14:45:01.179,,,no_permission,,,,,,,,,,,,364987,,,Fri Jan 31 15:45:41 UTC 2014,,,,,,0|i1qwun:,365296,,,,,,,,slebresne,slebresne,,,,,,,,,,20/Dec/13 14:45;jbellis;Can you reproduce [~mshuler]?,"20/Dec/13 15:17;awerrch;Some technical details about our environment: We use Murmur3Partitioner and NetworkTopologyStrategy with 3 data centers consisting each of 2 computers. We use VNodes and 256 tokens each. The column family from the above example has an RF=1 per data center. The server log files did not show any obvious errors (level = INFO, we might increase this) except this during a nodetool repair -pr job some days ago (so I don't think this is the main reason for something reproducible today):

""Comparison method violates its general contract.""

The machine kept running stable afterwards and was fully available for further requests. Still we restarted it to make sure there are no broken internal threads or something that nasty.","20/Dec/13 18:20;mshuler;Thanks for the extra details, Christoph - I will see if I can reproduce.
","06/Jan/14 17:55;mshuler;I reproduced on 2.0.4 with a simple 3-node RF=3 ccm cluster.  Repair did not change the results and querying a different node has the same result.
{code}
cqlsh:mwerrch> select key,node,computer from ""B4Container_Demo"" where computer=50;

(0 rows)

cqlsh:mwerrch> select * from ""B4Container_Demo"";

 key                                  | archived | bytes | computer | deleted | description | doarchive | filename | first | frames | ifversion | imported | jobid | keepuntil | nextchunk | node | recordingkey | recstart | recstop | simulationid | systemstart | systemstop | tapelabel | version
--------------------------------------+----------+-------+----------+---------+-------------+-----------+----------+-------+--------+-----------+----------+-------+-----------+-----------+------+--------------+----------+---------+--------------+-------------+------------+-----------+---------
 78c70562-1f98-3971-9c28-2c3d8e09c10f |     null |  null |       50 |    null |        null |      null |     null |  null |   null |      null |     null |  null |      null |      null |   50 |         null |     null |    null |         null |        null |       null |      null |    null

(1 rows)

cqlsh:mwerrch>
{code}
----
Update1: full cluster restart shows the same results
----
Update2: debug logs, query run on node1 ""... where computer=50"" resulting in (0 rows)
{code}
DEBUG [Thrift:1] 2014-01-06 12:11:42,122 CassandraServer.java (line 1954) execute_cql3_query
DEBUG [Thrift:1] 2014-01-06 12:11:42,136 AbstractReplicationStrategy.java (line 86) clearing cached endpoints
DEBUG [WRITE-/127.0.0.2] 2014-01-06 12:11:42,270 OutboundTcpConnection.java (line 290) attempting to connect to /127.0.0.2
 INFO [HANDSHAKE-/127.0.0.2] 2014-01-06 12:11:42,271 OutboundTcpConnection.java (line 386) Handshaking version with /127.0.0.2

==> .ccm/test/node2/logs/system.log <==
DEBUG [ACCEPT-/127.0.0.2] 2014-01-06 12:11:42,271 MessagingService.java (line 850) Connection version 7 from /127.0.0.1
DEBUG [Thread-7] 2014-01-06 12:11:42,272 IncomingTcpConnection.java (line 107) Upgrading incoming connection to be compressed
DEBUG [Thread-7] 2014-01-06 12:11:42,274 IncomingTcpConnection.java (line 115) Max version for /127.0.0.1 is 7
DEBUG [Thread-7] 2014-01-06 12:11:42,274 MessagingService.java (line 743) Setting version 7 for /127.0.0.1
DEBUG [Thread-7] 2014-01-06 12:11:42,274 IncomingTcpConnection.java (line 124) set version for /127.0.0.1 to 7
DEBUG [ReadStage:1] 2014-01-06 12:11:42,283 KeysSearcher.java (line 69) Most-selective indexed predicate is 'B4Container_Demo.computer EQ 50'
DEBUG [ReadStage:1] 2014-01-06 12:11:42,285 FileCacheService.java (line 70) Evicting cold readers for /home/mshuler/.ccm/test/node2/data/system/schema_columns/system-schema_columns-jb-7-Data.db
DEBUG [ReadStage:1] 2014-01-06 12:11:42,286 FileCacheService.java (line 115) Estimated memory usage is 11033316 compared to actual usage 0
DEBUG [ReadStage:1] 2014-01-06 12:11:42,286 FileCacheService.java (line 115) Estimated memory usage is 11164665 compared to actual usage 131349
DEBUG [ReadStage:1] 2014-01-06 12:11:42,286 FileCacheService.java (line 115) Estimated memory usage is 11296014 compared to actual usage 262698

==> .ccm/test/node1/logs/system.log <==
DEBUG [Thrift:1] 2014-01-06 12:11:42,287 Tracing.java (line 159) request complete
{code}

'select * ..."" query debug log is just:
{code}
DEBUG [Thrift:1] 2014-01-06 12:18:07,087 CassandraServer.java (line 1954) execute_cql3_query
DEBUG [Thrift:1] 2014-01-06 12:18:07,122 Tracing.java (line 159) request complete
{code}
----
Update4: went back to fresh cluster, inserted the data and queried ""... where computer=50""
{code}
DEBUG [Thrift:1] 2014-01-06 12:28:01,995 CassandraServer.java (line 1954) execute_cql3_query

==> .ccm/test/node2/logs/system.log <==
DEBUG [ReadStage:33] 2014-01-06 12:28:02,015 KeysSearcher.java (line 69) Most-selective indexed predicate is 'B4Container_Demo.computer EQ 50'

==> .ccm/test/node1/logs/system.log <==
DEBUG [Thrift:1] 2014-01-06 12:28:02,019 Tracing.java (line 159) request complete
{code}","06/Jan/14 18:54;jbellis;Do you have time to take a look, Sam?",07/Jan/14 11:02;beobal;I should have some time to look at this in the next few days,"09/Jan/14 09:10;awerrch;I just tried to reproduce the problem on an old cluster here that is still running Cassandra 1.2.3 version using RandomPartitioner. There everything succeeded properly. My guess is, the above problem is a side effect of the changes made for the 2.x versions.","13/Jan/14 15:32;beobal;Cleanup is irrelevant here and I was able to repro on a single node. The root cause is the incorrect use of CompactionManager.NO_GC (aka Long.MIN_VALUE) as a timestamp in PreCompactedRow.merge. The sequence of events is like so:

* update 0 inserts the row, so indexes the column value 
* update 1 deletes the row with a RangeTombstone, which deletes the value from the 2i, but leaves the original columns in the main cf's memtable
* update 2 re-inserts the row, now the main cf memtable still has the old col (which was being shadowed by the RT) so it calls SIM.update - which inserts the new col into the 2i and tries to delete the old column (which was already removed by update 1) - this results in another tombstone being written to the 2i memtable, but this has the timestamp of the column from the update 0 (the one who's 2i entry has already been removed) so it has no negative effect. This is why the 2i query contines to work as expected until we flush/compact.

When we flush, the RT is written to the sstable. This means that at compaction time, when we come to process the live column value from the sstable it is checked against the RT and ends up being removed from the 2i because of the incorrect timestamp passed into deletionInfo.isDeleted in PCR.merge. This index removal only hits the 2i memtable though, so although it prevents queries working correctly it only does so until the node is restarted (clearing the 2i memtable).

I've attached a bash script which repros the problem & a patch to fix it. The patch includes a new unit test and all the existing unit tests are still passing (though I didn't check any dtests).","15/Jan/14 09:17;awerrch;Thanks Sam, for working this out and for the provided patch. I took the 2.0.4 source code and modified the PrecompactedRow class as suggested. Now we are running this patched version on some testing machines and there the described problem is gone. Have to do more testing and finally give it a try on our productive system, but at this moment things are looking very promising.","31/Jan/14 15:45;slebresne;+1, committed (with s/{{isDeleted(column.name(), column.timestamp())}}/{{isDeleted(column)}} since that's equivalent), thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't drop local mutations without a hint,CASSANDRA-6510,12685590,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,19/Dec/13 13:33,12/Mar/19 14:14,13/Mar/19 22:29,19/Dec/13 13:58,1.2.14,2.0.4,,,,,0,,,,,"SP.insertLocal() uses a regular DroppableRunnable, thus timed out local mutations get dropped without leaving a hint. SP.insertLocal() should be using LocalMutationRunnable instead.

Note: hints are the context here, not consistency.",,,,,,,,,,,,,,,,,,,,,,,,19/Dec/13 13:34;iamaleksey;6510.txt;https://issues.apache.org/jira/secure/attachment/12619553/6510.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-19 13:38:17.013,,,no_permission,,,,,,,,,,,,364665,,,Mon Feb 03 21:58:44 UTC 2014,,,,,,0|i1qutb:,364965,,,,,,,,jbellis,jbellis,,,1.2.1,,,,,,,19/Dec/13 13:38;jbellis;History: this was supposed to be fixed by CASSANDRA-4753 but inexplicably I only changed the counters write path.,19/Dec/13 13:39;jbellis;+1,"19/Dec/13 13:58;iamaleksey;Committed, thanks.","19/Dec/13 14:26;slebresne;bq. but inexplicably I only changed the counters write path.

We all know that it's because you love that path so much.","03/Feb/14 21:30;rcoli;This bug seems to have the implication that no ConsistencyLevel has had its supposed meaning for the duration of the bug, because there is no guarantee that the acknowledged-to-the-client local write actually succeeds? Is that correct?

If so, this issue seems quite fundamental and serious; why did automated testing not surface it? Is there now a test which covers this case?

What is the ""since"" for this issue? Looks like at least 1.2.0?","03/Feb/14 21:40;jbellis;Nope, that's not the implication.  You can see from the code that {{responseHandler.response}} only gets called after {{rm.apply}}.  That is, no write is acknowledged if it hasn't actually been applied.","03/Feb/14 21:58;rcoli;Thanks for the clarification. Others who look to JIRA to understand impact will appreciate not having to try to deduce it from reading the patch.

What is the ""since"" for this issue? Looks like at least 1.2.0?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MoveTest fails in 1.2+,CASSANDRA-6416,12681619,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mshuler,jbellis,jbellis,27/Nov/13 18:01,12/Mar/19 14:14,13/Mar/19 22:29,05/Dec/13 17:06,1.2.13,2.0.4,,Legacy/Testing,,,0,qa-resolved,,,,"One test fails in 1.2, two in 2.0/trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-12-05 17:05:47.883,,,no_permission,,,,,,,,,,,,360883,,,Thu Dec 05 17:19:15 UTC 2013,,,,,,0|i1q7kv:,361182,,,,,,,,,,,,,,,,,,,"05/Dec/13 17:05;mshuler;trunk looks OK:
    [junit] Testsuite: org.apache.cassandra.service.MoveTest
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 11.137 sec

2.0 looks OK:
    [junit] Testsuite: org.apache.cassandra.service.MoveTest
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 7.376 sec

and 1.2:
    [junit] Testsuite: org.apache.cassandra.service.MoveTest
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 6.52 sec

Fix looks like a winner:
----
commit d8c4e89b3e85e8cb41a438963845cb10a923a3d6
Author: Marcus Eriksson <marcuse@spotify.com>
Date:   Wed Dec 4 20:17:30 2013 +0100

    fix MoveTest","05/Dec/13 17:19;krummas;oops, didnt see this",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
gossip performance improvement at node startup,CASSANDRA-6409,12681470,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,qconner,qconner,27/Nov/13 01:29,12/Mar/19 14:14,13/Mar/19 22:29,27/Nov/13 17:38,1.2.13,2.0.4,,,,,0,,,,,"With large clusters (> 500 nodes) and num_tokens > 255 we sometimes see a node have trouble starting up.  CPU usage for one thread is pegged.

We see this concurrent with Gossip flaps on the node trying to learn the ring topology.  Other nodes on the ring, that are already at steady state do not seem to suffer.  It is the node joining the large ring that has trouble.",,,,,,,,,,,,CASSANDRA-6127,,,,,,,,,,,,27/Nov/13 01:33;qconner;2013-11-26_17-40-08.png;https://issues.apache.org/jira/secure/attachment/12615967/2013-11-26_17-40-08.png,27/Nov/13 02:33;jbellis;endpointToTokenMapCPU.txt;https://issues.apache.org/jira/secure/attachment/12615981/endpointToTokenMapCPU.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-11-27 02:33:49.621,,,no_permission,,,,,,,,,,,,360735,,,Wed Nov 27 17:38:40 UTC 2013,,,,,,0|i1q6nz:,361034,1.2.11,,,,,,,,,,,,,,,,,qconner,27/Nov/13 01:31;qconner;sub ticket for the cpu peg at startup symptom.,27/Nov/13 01:33;qconner;Taken about 10 minutes after node startup.  Gossip should have settled down by now.,27/Nov/13 02:33;jbellis;Patch attached to move the Multimap computation into only the block where it is used and not each gossip update.,"27/Nov/13 15:23;qconner;+1

Moving getEndpointToTokenMapForReading() did the trick.  I tested the patch against cassandra-1.2 branch and the persistent flapping node and high cpu use symptom disappeared in my 500 node cluster setup (num_tokens=512).
",27/Nov/13 17:38;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BlacklistingCompactionsTest fails in trunk,CASSANDRA-6414,12681615,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,jbellis,jbellis,27/Nov/13 17:16,12/Mar/19 14:14,13/Mar/19 22:29,16/Jan/14 14:29,2.1 beta1,,,Legacy/Testing,,,0,,,,,Passes in 2.0 HEAD.  Bisect should be relatively easy.,,,,,,,,,,,,,,,,,,,,,,,,05/Dec/13 17:33;mshuler;6414_out.txt;https://issues.apache.org/jira/secure/attachment/12617193/6414_out.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-05 17:08:03.046,,,no_permission,,,,,,,,,,,,360879,,,Thu Jan 16 16:39:22 UTC 2014,,,,,,0|i1q7jz:,361178,,,,,,,,,,,,2.1 rc3,,,,,,,27/Nov/13 17:17;jbellis;I also note that {{ant test -Dtest.name=BlacklistingCompactionsTest}} dumps a TON of crap to stdout in trunk but not in 2.0.  Is this logback's fault [~dbrosius]?,"05/Dec/13 17:08;mshuler;quick fix for logback spam:
{code}
sed -i 's/commitlog_sync_batch_window_in_ms: 1.0/commitlog_sync_batch_window_in_ms: 100/' test/conf/cassandra.yaml
{code}","05/Dec/13 17:32;mshuler;The difference between the output without the above conf edit (first in 6414_out attachment) and with the edit (ran it a couple times at the end of the file) is interesting and hopefully helpful with regards to the java.io.EOFException errors being the cause, I think.  Setting the batch_window to 1s results in the test simply timing out, which isn't very helpful.

Bisecting next","05/Dec/13 19:35;mshuler;a few different good/bad strategies - this one seems to be closest:

my last chain of bisect:
...
((99b5040...)|BISECTING)mshuler@hana:~/git/cassandra$ git bisect bad
Bisecting: 32 revisions left to test after this (roughly 5 steps)
[6c5c12de6bd6f9b035501ea3e4dc53497a20b9fe] Merge branch 'cassandra-2.0' into trunk
...
((6c5c12d...)|BISECTING)mshuler@hana:~/git/cassandra$ git bisect good
Bisecting: 16 revisions left to test after this (roughly 4 steps)
[88a390064425171c617b66280a0a5961b234599b] Merge branch 'cassandra-2.0' into trunk
...
((88a3900...)|BISECTING)mshuler@hana:~/git/cassandra$ git bisect bad
Bisecting: 7 revisions left to test after this (roughly 3 steps)
[5dabd1cc0c65b329ff518d7ad3f09e4c11494f18] fix latency displays caused by metrics-core now using nanoseconds
...
((5dabd1c...)|BISECTING)mshuler@hana:~/git/cassandra$ git bisect bad
Bisecting: 3 revisions left to test after this (roughly 2 steps)
[e890b1f2ccee051d27ff3bec9b6e8ef63e7ff508] increase -Xss to 256k
...
((e890b1f...)|BISECTING)mshuler@hana:~/git/cassandra$ git bisect bad
Bisecting: 1 revision left to test after this (roughly 1 step)
[873ce0cb3f05d55753f205092e681c963cd20fc4] clarify error messages for zero/multiple PKs patch by Lyben Todorov; reviewed by jbellis for CASSANDRA-5875
...
compile fail
...
((873ce0c...)|BISECTING)mshuler@hana:~/git/cassandra$ git bisect skip
Bisecting: 1 revision left to test after this (roughly 1 step)
[29605aedd9e19f2f07042cd0aa6b31b6c94a4aea] switch logging from log4j to logback patch by dbrosius reviewed by jbellis for cassandra-5883
...
compile fail
...
((29605ae...)|BISECTING)mshuler@hana:~/git/cassandra$ git bisect skip
Bisecting: 1 revision left to test after this (roughly 1 step)
[fa0b7cf8b279c568aa7b2fcbaad91510797d838f] add logback-classic jar
...
((fa0b7cf...)|BISECTING)mshuler@hana:~/git/cassandra$ git bisect bad
There are only 'skip'ped commits left to test.
The first bad commit could be any of:
29605aedd9e19f2f07042cd0aa6b31b6c94a4aea
873ce0cb3f05d55753f205092e681c963cd20fc4
fa0b7cf8b279c568aa7b2fcbaad91510797d838f
We cannot bisect more!","05/Dec/13 23:44;mshuler;(edit: added commit, since it's a one-liner)

I misunderstood the logback spam as the WARN entries from CASSANDRA-3578.  So ignoring all the ERROR traces in the output (which I was marking bad above), just straight SUCESSFUL/FAILED test gets me:

((f3dc188...)|BISECTING)mshuler@hana:~/git/cassandra$ git bisect bad
f3dc188e203b3db980ee81df05390968043cb601 is the first bad commit
commit f3dc188e203b3db980ee81df05390968043cb601
Author: Jonathan Ellis <jbellis@apache.org>
Date:   Fri Nov 22 17:33:07 2013 -0600

    move setting lastCompactedKey to before the return-if-nothing-added

:040000 040000 0c616036c3aac24861a0f43b21f5812faecf2c92 33fb0a0761168d605a224dc87522785afe8cb5a1 M      src
----
$ git show f3dc188e203b3db980ee81df05390968043cb601
commit f3dc188e203b3db980ee81df05390968043cb601 (HEAD, refs/bisect/bad)
Author: Jonathan Ellis <jbellis@apache.org>
Date:   Fri Nov 22 17:33:07 2013 -0600

    move setting lastCompactedKey to before the return-if-nothing-added

{code}
diff --git a/src/java/org/apache/cassandra/db/compaction/LeveledManifest.java b/src/java/org/apache/cassandra/
index 76f51d1..232d1f7 100644
--- a/src/java/org/apache/cassandra/db/compaction/LeveledManifest.java
+++ b/src/java/org/apache/cassandra/db/compaction/LeveledManifest.java
@@ -142,6 +142,7 @@ public class LeveledManifest
             int thisLevel = remove(sstable);
             minLevel = Math.min(minLevel, thisLevel);
         }
+        lastCompactedKeys[minLevel] = SSTableReader.sstableOrdering.max(added).last;
 
         // it's valid to do a remove w/o an add (e.g. on truncate)
         if (added.isEmpty())
@@ -152,7 +153,6 @@ public class LeveledManifest
 
         for (SSTableReader ssTableReader : added)
             add(ssTableReader);
-        lastCompactedKeys[minLevel] = SSTableReader.sstableOrdering.max(added).last;
     }
 
     public synchronized void repairOverlappingSSTables(int level)
{code}","06/Dec/13 02:43;jbellis;That should be purely cosmetic, and indeed reverting f3dc188e203b3db980ee81df05390968043cb601 still leaves BCT erroring out.","15/Jan/14 20:31;mshuler;Multiple git bisections for purely BUILD SUCCESSFUL / FAILED arrive at the above commit as the first bad commit.  When I am on commit f3dc188 and revert it (ending up on 6164a83), 'ant clean jar && ant test -Dtest.name=BlacklistingCompactionsTest' gives me BUILD SUCCESSFUL.  Reverting f3dc188 from current trunk also fixes this test for me.

That said, I *do* have tons of traceback output, even when successful at that point.  I'm going to see if there is a common initial trace message (maybe ""org.apache.cassandra.io.sstable.CorruptSSTableException: java.io.EOFException"") and see if I can find where those are starting.

git bisect run script:
{code}
#!/bin/bash
ant realclean
ant jar || exit 125
for i in {1..3}; do
  ant test -Dtest.name=BlacklistingCompactionsTest
  RET=$?
  if [ ""$RET"" -eq ""0"" ]; then
    break
  fi
done
exit $RET
{code}

consistently arrives at:

{noformat}
# bad: [63c490af9cbe392ba025f15854173eb4f7fbbfad] encapsulate SecondaryIndex.estimateResultRows patch by Miguel Angel Fernandez Diaz; reviewed by Sam Tunnicliffe for CASSANDRA-6498
# good: [7514e61b48e9456cf6591abaf6dbf17b52217883] update changes
git bisect start 'trunk' 'cassandra-2.0'
# good: [6c379343561766724e48d1d7cf98e282e8ec91dd] Merge branch 'cassandra-2.0' into trunk
git bisect good 6c379343561766724e48d1d7cf98e282e8ec91dd
# skip: [a13c6dcbb7a6ba74b27e50eef4bfd0f80eaea121] Merge branch 'cassandra-2.0' into trunk
git bisect skip a13c6dcbb7a6ba74b27e50eef4bfd0f80eaea121
# bad: [351d49b06bd6887447eecb149b3cdf1697ab41ac] validate directory permissions on startup Patch by Lyuben Todorov and Koray Sariteke; reviewed by Mikhail Stepura for CASSANDRA-5818
git bisect bad 351d49b06bd6887447eecb149b3cdf1697ab41ac
# good: [5c21711e76731a6f48d67ca956ea865fda4574b4] merge from 2.0
git bisect good 5c21711e76731a6f48d67ca956ea865fda4574b4
# good: [52cc7efb2bcd47285148da85c089d796cb20734a] Merge branch 'cassandra-2.0' into trunk
git bisect good 52cc7efb2bcd47285148da85c089d796cb20734a
# good: [40598efa6344333d4d4deee2c1ec3e71bd931066] Merge branch 'cassandra-2.0' into trunk
git bisect good 40598efa6344333d4d4deee2c1ec3e71bd931066
# skip: [1bfd062fdc9daa35fbabcebb3ac31e726504f1ff] Merge branch 'cassandra-2.0' into trunk
git bisect skip 1bfd062fdc9daa35fbabcebb3ac31e726504f1ff
# bad: [7ea5b40b7172ec9f3fdecca533e19d8a165de7df] don't leave replaced SSTRs around to break other tests patch by Tyler Hobbs
git bisect bad 7ea5b40b7172ec9f3fdecca533e19d8a165de7df
# bad: [9f3a7f8a698aaaa9be44bed01aaf526567df8aca] simple naming fixes
git bisect bad 9f3a7f8a698aaaa9be44bed01aaf526567df8aca
# bad: [8732fe938659232b3f0c63933b125131fa50fd2e] Merge branch 'cassandra-2.0' into trunk
git bisect bad 8732fe938659232b3f0c63933b125131fa50fd2e
# good: [87b39c8af3477c3b80f124da23b46de350a259e7] Merge branch 'cassandra-2.0' into trunk
git bisect good 87b39c8af3477c3b80f124da23b46de350a259e7
# good: [a10150542c662a4cc69ce1b88f48636d1e6884f7] merge from 2.0
git bisect good a10150542c662a4cc69ce1b88f48636d1e6884f7
# bad: [f3dc188e203b3db980ee81df05390968043cb601] move setting lastCompactedKey to before the return-if-nothing-added
git bisect bad f3dc188e203b3db980ee81df05390968043cb601
{noformat}","15/Jan/14 21:31;mshuler;git bisect run, looking just for the loads of trace output does looks like it's from the logback addition
{code}
#!/bin/bash
ant realclean
ant jar || exit 125
if ant test -Dtest.name=BlacklistingCompactionsTest | grep ""org.apache.cassandra.io.sstable.CorruptSSTableException: java.io.EOFException""; then
  RET=1
else
  RET=0
fi
exit $RET
{code}

{noformat}
# bad: [63c490af9cbe392ba025f15854173eb4f7fbbfad] encapsulate SecondaryIndex.estimateResultRows patch by Miguel Angel Fernandez Diaz; reviewed by Sam Tunnicliffe for CASSANDRA-6498
# good: [7514e61b48e9456cf6591abaf6dbf17b52217883] update changes
git bisect start 'trunk' 'cassandra-2.0'
# bad: [6c379343561766724e48d1d7cf98e282e8ec91dd] Merge branch 'cassandra-2.0' into trunk
git bisect bad 6c379343561766724e48d1d7cf98e282e8ec91dd
# bad: [5102e8d748289876964305292d75f5eba28a40dd] Merge branch 'cassandra-2.0' into trunk
git bisect bad 5102e8d748289876964305292d75f5eba28a40dd
# bad: [cd6aa2d1ef16a8af2e5e3de20e2389575d8021e1] switch to use parameterized logging
git bisect bad cd6aa2d1ef16a8af2e5e3de20e2389575d8021e1
# bad: [5dabd1cc0c65b329ff518d7ad3f09e4c11494f18] fix latency displays caused by metrics-core now using nanoseconds
git bisect bad 5dabd1cc0c65b329ff518d7ad3f09e4c11494f18
# good: [e718467ec471bc5c952c3eaeac8bdc2b3db9eac5] Merge branch 'cassandra-2.0' into trunk
git bisect good e718467ec471bc5c952c3eaeac8bdc2b3db9eac5
# good: [a69457c14a36c5eb3db523b3eb42b6d399993177] Merge branch 'cassandra-2.0' into trunk
git bisect good a69457c14a36c5eb3db523b3eb42b6d399993177
# skip: [873ce0cb3f05d55753f205092e681c963cd20fc4] clarify error messages for zero/multiple PKs patch by Lyben Todorov; reviewed by jbellis for CASSANDRA-5875
git bisect skip 873ce0cb3f05d55753f205092e681c963cd20fc4
# bad: [fa0b7cf8b279c568aa7b2fcbaad91510797d838f] add logback-classic jar
git bisect bad fa0b7cf8b279c568aa7b2fcbaad91510797d838f
# good: [6c5c12de6bd6f9b035501ea3e4dc53497a20b9fe] Merge branch 'cassandra-2.0' into trunk
git bisect good 6c5c12de6bd6f9b035501ea3e4dc53497a20b9fe
# skip: [29605aedd9e19f2f07042cd0aa6b31b6c94a4aea] switch logging from log4j to logback patch by dbrosius reviewed by jbellis for cassandra-5883
git bisect skip 29605aedd9e19f2f07042cd0aa6b31b6c94a4aea


# There are only 'skip'ped commits left to test.
# The first bad commit could be any of:
# 29605aedd9e19f2f07042cd0aa6b31b6c94a4aea
# 873ce0cb3f05d55753f205092e681c963cd20fc4
# fa0b7cf8b279c568aa7b2fcbaad91510797d838f
# We cannot bisect more!
# bisect run cannot continue any more
{noformat}","15/Jan/14 22:30;mshuler;running 10 iterations, pass 10x to get a ""good""
{code}
#!/bin/bash
ant realclean
ant jar || exit 125
for i in {1..10}; do
  ant test -Dtest.name=BlacklistingCompactionsTest
  RET=$?
  if [ ""$RET"" -ne ""0"" ]; then
    break
  fi
done
exit $RET
{code}","15/Jan/14 23:42;brandon.williams;I get a very consistent pointer to a552b305f3d1b17e394744b18efd7f40599f3c2e (CASSANDRA-5590).  A single iteration won't pass there, but all of them pass on the commit before.","16/Jan/14 13:15;slebresne;bq. That should be purely cosmetic, and indeed reverting f3dc188e203b3db980ee81df05390968043cb601 still leaves BCT erroring out.

I'll call your bluff!

As far as I can tell, the actual error that makes the test error out is
{noformat}
    [junit] java.lang.RuntimeException: java.util.NoSuchElementException
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.runWithCompactionsDisabled(ColumnFamilyStore.java:2076)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.truncateBlocking(ColumnFamilyStore.java:2020)
    [junit] 	at org.apache.cassandra.db.compaction.BlacklistingCompactionsTest.testBlacklisting(BlacklistingCompactionsTest.java:153)
    [junit] 	at org.apache.cassandra.db.compaction.BlacklistingCompactionsTest.testBlacklistingWithLeveledCompactionStrategy(BlacklistingCompactionsTest.java:67)
    [junit] Caused by: java.util.NoSuchElementException
    [junit] 	at java.util.Collections$EmptyIterator.next(Collections.java:3006)
    [junit] 	at com.google.common.collect.Ordering.max(Ordering.java:536)
    [junit] 	at com.google.common.collect.Ordering.max(Ordering.java:555)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.replace(LeveledManifest.java:142)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:162)
    [junit] 	at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:469)
    [junit] 	at org.apache.cassandra.db.DataTracker.markObsolete(DataTracker.java:246)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.markObsolete(ColumnFamilyStore.java:1097)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.discardSSTables(ColumnFamilyStore.java:2359)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore$10.run(ColumnFamilyStore.java:2004)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.runWithCompactionsDisabled(ColumnFamilyStore.java:2072)
{noformat}
and as it happens, due to f3dc188e203b3db980ee81df05390968043cb601, {{SSTableReader.sstableOrdering.max(added).last}} will be called even when {{added}} is empty, which is clearly documented as wrong for Ordering.max(). And while BCT fails every time for me on trunk, it does work fine with f3dc188e203b3db980ee81df05390968043cb601 reverted (it does log a bunch of ERROR because compaction complains about the sstable being corrupted, which is correct, and I do have to bump the test timeout on my box to not have the test timeout, but it does pass).

Haven't reverted f3dc188e203b3db980ee81df05390968043cb601 yet because I'm not sure what was the initial intent of that commit tbh.","16/Jan/14 14:29;jbellis;You're right, f3dc188e203b3db980ee81df05390968043cb601 is a regression.  Reverted.

And it passes now, so I guess there must have been two problems before, although I wouldn't rule out ""Jonathan smoking crack"" either.","16/Jan/14 16:39;brandon.williams;I think what happened with a552b305f is, we did have a problem at some point, and later fixed it.  I didn't know how far back to go to find a 'good' spot for bisect, so I bisected from 2.0 to trunk and arrived there, since the bisect script couldn't tell the difference between NPE (f3dc188e) and timeout (a552b305) so I think we're fine.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exceptions when a second Datacenter is Added,CASSANDRA-6493,12685014,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,rspitzer,rspitzer,16/Dec/13 18:38,12/Mar/19 14:14,13/Mar/19 22:29,18/Dec/13 17:04,,,,,,,0,,,,,"On adding a second datacenter several exceptions were raised.

Test outline:
Start 25 Node DC1
Keyspace Setup Replication 3
Begin insert against DC1 Using Stress
While the inserts are occuring
Start up 25 Node DC2
Alter Keyspace to include Replication in 2nd DC
Run rebuild on DC2
Wait for stress to finish
Run repair on Cluster
... Some other operations

At the point when the second datacenter is added several warnings go off because nodetool status is not functioning, and a few moments later the start operation reports a failure because a node has not successfully turned on. 

The first start attempt yielded the following exception on a node in the second DC.

{code}
CassandraDaemon.java (line 464) Exception encountered during startup
java.lang.AssertionError: -7560216458456714666 not found in -9222060278673125462, -9220751250790085193, ..... ALL THE TOKENS ...,  9218575851928340117, 9219681798686280387
at org.apache.cassandra.locator.TokenMetadata.getPredecessor(TokenMetadata.java:752)
at org.apache.cassandra.locator.TokenMetadata.getPrimaryRangesFor(TokenMetadata.java:696)
at org.apache.cassandra.locator.TokenMetadata.getPrimaryRangeFor(TokenMetadata.java:703)
at org.apache.cassandra.locator.AbstractReplicationStrategy.getRangeAddresses(AbstractReplicationStrategy.java:187)
at org.apache.cassandra.dht.RangeStreamer.getAllRangesWithSourcesFor(RangeStreamer.java:147)
at org.apache.cassandra.dht.RangeStreamer.addRanges(RangeStreamer.java:121)
at org.apache.cassandra.dht.BootStrapper.bootstrap(BootStrapper.java:81)
at org.apache.cassandra.service.StorageService.bootstrap(StorageService.java:979)
at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:745)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:586)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:483)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
{code}

The test automatically tries to restart nodes if they fail during startup, The second attempt for this node succeeded but a 'nodetool status' still failed and a different node in the second DC logged the following and failed to start up.

{code}
ERROR [main] 2013-12-16 18:02:04,869 CassandraDaemon.java (line 464) Exception encountered during startup
java.util.ConcurrentModificationException
	at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1115)
	at java.util.TreeMap$KeyIterator.next(TreeMap.java:1169)
	at org.apache.commons.lang.StringUtils.join(StringUtils.java:3382)
	at org.apache.commons.lang.StringUtils.join(StringUtils.java:3444)
	at org.apache.cassandra.locator.TokenMetadata.getPredecessor(TokenMetadata.java:752)
	at org.apache.cassandra.locator.TokenMetadata.getPrimaryRangesFor(TokenMetadata.java:696)
	at org.apache.cassandra.locator.TokenMetadata.getPrimaryRangeFor(TokenMetadata.java:703)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getRangeAddresses(AbstractReplicationStrategy.java:187)
	at org.apache.cassandra.dht.RangeStreamer.getAllRangesWithSourcesFor(RangeStreamer.java:147)
	at org.apache.cassandra.dht.RangeStreamer.addRanges(RangeStreamer.java:121)
	at org.apache.cassandra.dht.BootStrapper.bootstrap(BootStrapper.java:81)
	at org.apache.cassandra.service.StorageService.bootstrap(StorageService.java:979)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:745)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:586)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:483)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
ERROR [StorageServiceShutdownHook] 2013-12-16 18:02:04,876 CassandraDaemon.java (line 191) Exception in thread Thread[StorageServiceShutdownHook,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.stopNativeTransport(StorageService.java:358)
	at org.apache.cassandra.service.StorageService.shutdownClientServers(StorageService.java:373)
	at org.apache.cassandra.service.StorageService.access$000(StorageService.java:89)
	at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:551)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.lang.Thread.run(Thread.java:724)
{code}

","Ubuntu, EC2 M1.large",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-12-17 00:21:50.687,,,no_permission,,,,,,,,,,,,364091,,,Wed Dec 18 16:53:11 UTC 2013,,,,,,0|i1qra7:,364391,1.2.13,,,,,,,,,,,,,,,,,,"16/Dec/13 18:43;rspitzer;https://cassci.datastax.com/job/cassandra-addremovedc/25/console

The ""Node down Detected"" are messages from a thread which runs nodetool status every ~2 seconds and counts how many nodes report themselves as up, the lack of a command line output shows the command failed. ","16/Dec/13 23:20;rspitzer;I was able to get the same results repeating the test.
https://cassci.datastax.com/job/cassandra-addremovedc/26/console","17/Dec/13 00:21;jbellis;From chat, this does not reproduce when CASSANDRA-6488 is reverted.","17/Dec/13 00:31;rspitzer;Correct I didn't see this over several runs over the weekend testing on the pre-6488 build. Head of the git log from that build

{code}
commit c133ff88982948fdb12669bf766e9848102a3496
Author: Russell Spitzer <Russell.Spitzer@gmail.com>
Date:   Fri Dec 13 12:00:53 2013 -0800

    Patch to fix NPE ( this is patch a3d91dc9d67572e16d9ad92f22b89eb969373899)

commit 11455738fa61c6eb02895a5a8d3fbbe4d8cb24b4
Author: Brandon Williams <brandonwilliams@apache.org>
Date:   Fri Dec 13 12:10:47 2013 -0600

    Pig: don't assume all DataBags are DefaultDataBags
    Patch by Mike Spertus, reviewed by brandonwilliams for CASSANDRA-6420
{code}",18/Dec/13 16:53;rspitzer;Fixed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractQueryPager.DiscardFirst is still broken,CASSANDRA-6555,12687650,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,07/Jan/14 14:33,12/Mar/19 14:14,13/Mar/19 22:29,29/Jan/14 09:10,2.0.5,,,,,,1,,,,,"See https://datastax-oss.atlassian.net/browse/JAVA-243 for an example failure.

This is my bad, I messed up while testing the fix for CASSANDRA-6447. Attaching fix for that. I've (correctly) tested that this fixes the issue but also added a few specific unit tests for discardFirst/discardLast to make sure they work correctly this time.",,,,,,,,,,,,,,,,,,,,,,,,07/Jan/14 14:33;slebresne;6555.txt;https://issues.apache.org/jira/secure/attachment/12621793/6555.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-28 23:12:57.356,,,no_permission,,,,,,,,,,,,366650,,,Wed Jan 29 09:10:49 UTC 2014,,,,,,0|i1r787:,366961,,,,,,,,thobbs,thobbs,,,,,,,,,,28/Jan/14 23:12;thobbs;+1,"29/Jan/14 09:10;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Heavy write load exhausts heap space,CASSANDRA-6056,12669278,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,arnaud-lb,arnaud-lb,18/Sep/13 16:17,12/Mar/19 14:14,13/Mar/19 22:29,20/Nov/13 00:10,,,,,,,0,,,,,"Issuing many INSERT or UPDATE queries cause cassandra to exhaust the heap space in a few minutes. It then gets stuck in full GCs, failing to recover.

Observed with the default config from Datastax Debian packages, with a 8GB heap. Query rate is around 35K queries per second, with 16 concurrent queries.","Debian, Cassandra 2.0",,,,,,,,,,,,,,,,,,,,,,,19/Sep/13 10:18;arnaud-lb;dominator-tree.png;https://issues.apache.org/jira/secure/attachment/12604021/dominator-tree.png,19/Sep/13 09:35;arnaud-lb;jvisualvm-class-instances.png;https://issues.apache.org/jira/secure/attachment/12604013/jvisualvm-class-instances.png,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-09-18 16:49:15.357,,,no_permission,,,,,,,,,,,,349210,,,Wed Nov 20 00:10:31 UTC 2013,,,,,,0|i1o7pj:,349508,2.0.0,,,,,,,,,,,,,,,,,,"18/Sep/13 16:49;jbellis;This sounds a lot like CASSANDRA-5982.  You should have a look at that, especially if you're throwing large blobs around.  If not, you should put some effort into describing what makes your workload special.","19/Sep/13 09:37;arnaud-lb;I can reproduce this with only UPDATES on two columns with small values (timestamp, int). Those columns were previously null. The entire rows are moderately small too, less than 200 bytes in average. This happens with only 4 concurrent queries after some times.

Attached a screenshot of jvisualvm's classes view: [^jvisualvm-class-instances.png]

The table being updated looks like this:

{code}
id timeuuid primary key
str1 text
str2 text
str3 text
str4 text
time1 timestamp (the column being updated)
time2 timestamp (mostly null)
time3 timestamp (mostly null)
n int           (the second column being updated)
time4 timestamp
time5 timestamp (mostly null)
{code}

Looking at CASSANDRA-5982, it could very well be related. I'm willing to try with the fix applied, is this merged in any 2.0 branch?","19/Sep/13 10:18;arnaud-lb;Attached a screenshot of eclipse memory analyzer tool, showing the dominator tree report",19/Sep/13 13:55;jbellis;5982 is applied to the only 2.0 branch in the repository.,20/Nov/13 00:10;jbellis;between CASSANDRA-5982 and CASSANDRA-6059 I think we're good.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Endless L0 LCS compactions,CASSANDRA-6496,12685087,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,ngrigoriev,ngrigoriev,17/Dec/13 03:00,12/Mar/19 14:14,13/Mar/19 22:29,17/Dec/13 20:35,2.0.4,,,,,,0,compaction,lcs,,,"I have first described the problem here: http://stackoverflow.com/questions/20589324/cassandra-2-0-3-endless-compactions-with-no-traffic

I think I have really abused my system with the traffic (mix of reads, heavy updates and some deletes). Now after stopping the traffic I see the compactions that are going on endlessly for over 4 days.

For a specific CF I have about 4700 sstable data files right now.  The compaction estimates are logged as ""[3312, 4, 0, 0, 0, 0, 0, 0, 0]"". sstable_size_in_mb=256.  3214 files are about 256Mb (+/1 few megs), other files are smaller or much smaller than that. No sstables are larger than 256Mb. What I observe is that LCS picks 32 sstables from L0 and compacts them into 32 sstables of approximately the same size. So, what my system is doing for last 4 days (no traffic at all) is compacting groups of 32 sstables into groups of 32 sstables without any changes. Seems like a bug to me regardless of what did I do to get the system into this state...
","Cassandra 2.0.3, Linux, 6 nodes, 5 disks per node",,,,,,,,,,,,,,,,,,,,,,,17/Dec/13 14:39;jbellis;6496.txt;https://issues.apache.org/jira/secure/attachment/12619110/6496.txt,17/Dec/13 03:28;ngrigoriev;system.log.1.gz;https://issues.apache.org/jira/secure/attachment/12619027/system.log.1.gz,17/Dec/13 03:28;ngrigoriev;system.log.gz;https://issues.apache.org/jira/secure/attachment/12619028/system.log.gz,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-12-17 03:14:26.983,,,no_permission,,,,,,,,,,,,364164,,,Tue Dec 17 20:35:58 UTC 2013,,,,,,0|i1qrqf:,364464,,,,,,,,krummas,krummas,,,2.0 beta 1,,,,,,,17/Dec/13 03:14;jbellis;Can you enable debug logging in o.a.c.db.compaction and post a log sample?,17/Dec/13 03:28;ngrigoriev;Attaching the logs. I have enabled the compaction logging this morning to get a slight idea of what was going on. ,17/Dec/13 08:26;krummas;i think this might be a duplicate of CASSANDRA-6284,"17/Dec/13 11:47;ngrigoriev;One thing I forgot to mention about the logs - I have reduced the number of compactors to one when enabling the debugging. Since at that point it was clear that something was wrong I was looking for clarity, not performance :)","17/Dec/13 14:39;jbellis;Patch to remove sstable output size limit when we're supposed to be doing STCS in L0.

- Chose to use LCT w/ unlimited size instead of normal CT since that seems less fragile (e.g. if we decide CT.level() should return -1)
- Some churn to standardize on limiting in Bytes over MB
","17/Dec/13 17:16;ngrigoriev;Cool!!!!! I have got the source tagged 2.0.3, applied the patch, recompiled, restarted the node. Clearly now it compacts the groups of 32 L0 sstables into  large ones. I see that it just did one round and created 8Gb sstable from 32 256Mb ones.

Thanks a lot for the patch! I will revert the compaction settings to give it enough resources and let it complete its job to see the end results before I restart the test traffic.","17/Dec/13 18:51;krummas;[~ngrigoriev] thanks for testing

and patch lgtm, +1",17/Dec/13 20:35;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IOException in MessagingService.run() causes orphaned storage server socket,CASSANDRA-6349,12679323,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mishail,cl1cker,cl1cker,15/Nov/13 01:47,12/Mar/19 14:14,13/Mar/19 22:29,16/Nov/13 16:26,2.0.3,,,,,,0,,,,,"The refactoring of reading the message header in MessagingService.run() vs IncomingTcpConnection seems to mishandle IOException as the loop is broken and MessagingService.SocketThread never seems to get reinitialized.

To reproduce: telnet to port 7000 and send random data. This then prevents any new or restarting node in the cluster from handshaking with this defunct storage port.",cassandra 2.0+,,,,,,,,,,,,,CASSANDRA-10816,,,,,CASSANDRA-6468,,,,,16/Nov/13 03:49;mishail;CASSANDRA-2.0-6349.patch;https://issues.apache.org/jira/secure/attachment/12614197/CASSANDRA-2.0-6349.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-16 03:49:37.172,,,no_permission,,,,,,,,,,,,358685,,,Wed Dec 11 00:57:15 UTC 2013,,,,,,0|i1ptz3:,358975,2.0.2,,,,,,,jbellis,jbellis,,,,,,,,,,16/Nov/13 03:49;mishail;One of the options is to close the socket on IOException and continue,"16/Nov/13 16:26;jbellis;LGTM, committed","09/Dec/13 16:40;oseiler;I suspect these changes introduced an infinite loop if the ServerSocket gets closed (not sure how that is happening though). We've been seeing some major problems with Cassandra 2.0.3 when a new cluster is coming up for the first time, and it seems to be a result of this. With logging set to debug, system.log is getting pummelled with these exception messages:

{noformat}
DEBUG [ACCEPT-localhost-grid/10.96.99.178] 2013-12-06 22:55:39,759 MessagingService.java (line 905) Error reading the socket null
java.net.SocketException: Socket closed
        at java.net.PlainSocketImpl.socketAccept(Native Method)
        at java.net.AbstractPlainSocketImpl.accept(Unknown Source)
        at java.net.ServerSocket.implAccept(Unknown Source)
        at sun.security.ssl.SSLServerSocketImpl.accept(Unknown Source)
        at org.apache.cassandra.net.MessagingService$SocketThread.run(MessagingService.java:865)
{noformat}

It looks like once in this state, nothing will break it out; prior to this change the IOException catch block was throwing another exception, now it just keeps looping, using the (seemingly closed) ServerSocket. Restarting Cassandra seems to be the only way to resolve this. I'll probably be recommending we drop back to 2.0.2 until this problem is fixed (or we can understand why the ServerSocket is closed...)
","09/Dec/13 18:41;mishail;The current SocketThread's code detects whether the ServerSocket is closing by catching AsynchronousCloseException/ClosedChannelException and breaking the endless loop.
And it looks like SSLServerSocketImpl throws a different exception (SocketException) which the thread doesn't handle

Do we really need {{while(true)}} there? Why can't we use {{while (!server.isClosed())}} instead?
",10/Dec/13 07:01;mishail;CASSANDRA-6468,"11/Dec/13 00:57;cl1cker;Please also note that handling the protocol magic and version handshake in the while loop allows an attacker to open a connection and not send any data, preventing any further connections. Prior revisions handled all the handshaking in the resulting thread where it might be more appropriate.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bisect unit test failures on 2.0 branch,CASSANDRA-6365,12679586,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mshuler,jbellis,jbellis,16/Nov/13 20:14,12/Mar/19 14:14,13/Mar/19 22:29,06/Dec/13 00:12,,,,Legacy/Testing,,,0,qa-resolved,,,,"Unit tests pass in 2.0.1.

They do not in 2.0.2.

Let's find where the failures were introduced.",,,,,,,,,,,,,,,,,,,,,,,,17/Nov/13 01:52;jbellis;2.0.1-utest.txt;https://issues.apache.org/jira/secure/attachment/12614263/2.0.1-utest.txt,16/Nov/13 21:29;mshuler;C-2.0.1_tag_utests.txt;https://issues.apache.org/jira/secure/attachment/12614249/C-2.0.1_tag_utests.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-11-16 21:29:33.189,,,no_permission,,,,,,,,,,,,358946,,,Fri Dec 06 00:11:00 UTC 2013,,,,,,0|i1pvl3:,359236,,,,,,,,,,,,,,,,,,,"16/Nov/13 21:29;mshuler;Unit Test results from c-2.0.1 git tag (full output attached):

{code}
((cassandra-2.0.1) *)mshuler@mana:~/DataStax/repos/cassandra$ egrep '.*Testcase.*ERROR' ../../tmp/C-2.0.1_tag_utests.txt
    [junit] Testcase: org.apache.cassandra.cli.CliTest:testCli: Caused an ERROR
    [junit] Testcase: org.apache.cassandra.service.EmbeddedCassandraServiceTest:BeforeFirstTest:        Caused an ERROR
    [junit] Testcase: org.apache.cassandra.streaming.StreamingTransferTest:testRandomSSTableTransfer:   Caused an ERROR
{code}","17/Nov/13 01:52;jbellis;You may be seeing a heisenbug or two.  Passing test run attached.

","17/Nov/13 03:12;mishail;http://buildbot.datastax.com:8020/builders/cassandra-2.0/builds/210 is the latest ""green"" build for 2.0.x
http://buildbot.datastax.com:8020/waterfall?last_time=1381334941&show=cassandra-2.0

It was triggered by https://github.com/apache/cassandra/commit/01a57eea841e51fb4a97329ab9fa0f59d0b826f6 which is *after* 2.0.1 but before 2.0.2","17/Nov/13 03:58;jbellis;01a57eea841e51fb4a97329ab9fa0f59d0b826f6 passes for me too :-| [Edit: twice in a row]

My takeaway is we've had heisenbugs for a while but at some point we introduced some that reproduce more readily because 2.0 HEAD hasn't passed in a long time.","06/Dec/13 00:11;mshuler;Latest several 2.0 branch unit test builds have passed successfully - good work!
http://cassci.datastax.com/job/cassandra-2.0_test/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Collateral damage from killing a node,CASSANDRA-6620,12691244,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,appodictic,appodictic,26/Jan/14 05:09,12/Mar/19 14:14,13/Mar/19 22:29,26/Jan/14 17:23,,,,,,,0,,,,,"I have designed a new scenario with farsandra: 
3 nodes with Replication factor = 2 a Counter column family. I perform 10,000 inserts to node 1. I kill off node 2, do 10000 more inserts. restart node 2. Sometimes I made it completely though this test. However sometimes I do not. I have seen the client throw time out exceptions. it seems like the death of node 2 greatly upsets node 1 and it times out a request. Since the default is ready 1 should this be happening?",,,,,,,,,,,,,,,,,,,,,,,,26/Jan/14 05:10;appodictic;nodelogs.txt;https://issues.apache.org/jira/secure/attachment/12625239/nodelogs.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-26 05:24:08.192,,,no_permission,,,,,,,,,,,,369989,,,Mon Jan 27 07:19:45 UTC 2014,,,,,,0|i1rron:,370291,,,,,,,,,,,,2.0.4,,,,,,,26/Jan/14 05:10;appodictic;Trace of the incident.,"26/Jan/14 05:11;appodictic;https://github.com/edwardcapriolo/farsandra/blob/master/src/test/java/io/teknek/farsandra/TestFarsandra.java. This trace happens periodically while running the ""3nodeTest"" method.",26/Jan/14 05:24;brandon.williams;Sounds like (and logs seem to agree) that the FD doesn't know the node is dead yet.,26/Jan/14 05:26;brandon.williams;There are a bunch of tests doing something similar except waiting for the FD here: https://github.com/riptano/cassandra-dtest/blob/master/consistency_test.py,"26/Jan/14 15:53;appodictic;Dtests may not be revealing the problem for two reasons.
1) You doing 100 operations not 10000
   for n in xrange(100, 200):
            insert_c1c2(cursor1, n, ""QUORUM"")
            query_c1c2(cursor2, n, ""QUORUM"")
2) Every test in the suite has wait_other_notice=True
        # shutdown another node and test we get unavailabe exception
        node2.stop(wait_other_notice=True)

My test is not waiting for others to notice.",26/Jan/14 16:00;appodictic;Right yes. I did not correlate FD with failure detector in your message. I would guess that is the problem.  ,26/Jan/14 16:21;appodictic;Why resolve this? It is a problem I think. Cassandra should not time out in this scenario?,26/Jan/14 16:25;appodictic;It is a problem. Writes at ONE should not timeout with RF=2 cluster=3 and only one node down. The user should not have to re-try this write operation.,"26/Jan/14 16:35;brandon.williams;That depends on the consistency level.  Can you point to where your code is setting that?  This should be easily reproducible from the cli/cqlsh, in any case.","26/Jan/14 16:39;appodictic;According to the CQL docs the default consistency level is ONE. Look at the exception:

{quote}
TimedOutException(acknowledged_by:0)
{quote}

acknoledged_by:0 shows that of the natural endpoints for the key it was writing to increment, one was down, the other should have responded. It is not a socket time out, so a GC does not explain. It is an application level time out.","26/Jan/14 16:56;brandon.williams;Can you show me where the CF is created in 'theeNodeTest' here? https://github.com/edwardcapriolo/farsandra/blob/master/src/test/java/io/teknek/farsandra/TestFarsandra.java#L12  Maybe I'm missing something, but it looks like it mostly just sleeps for 10s after it has started.","26/Jan/14 17:03;appodictic;My bad. I pushed latest.
https://github.com/edwardcapriolo/farsandra/blob/master/src/test/java/io/teknek/farsandra/TestFarsandra.java#L102
",26/Jan/14 17:04;appodictic;I have set it to ALL. Just for argument sake.,26/Jan/14 17:12;appodictic;I guess at ALL you could see a TimedOutException or an UnavailableException. At ONE this should always work.,"26/Jan/14 17:14;brandon.williams;bq. According to the CQL docs the default consistency level is ONE

Not if you're passing it over thrift, which obviously requires the parameter passed outside of the query.

bq. I have set it to ALL

Why?  This is certainly going to fail with either TOE before the FD sees it, or UE after it does.","26/Jan/14 17:23;appodictic;Right that makes sense. The strange thing about ALL is you end up in situations with counts of like 11785 before you get a timeout exception. When you would assume, but I guess that is just the instance you killed taking a while to die.",26/Jan/14 17:23;appodictic;My bad,26/Jan/14 17:24;appodictic;Question. Do system_* methods ignore the consistency level?,"27/Jan/14 07:19;jbellis;If you mean system_ tables, then they do effectively ignore CL because they are stored with LocalStrategy, i.e. each node stores its own copy with no replication.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Writing wide row causes high CPU usage after compaction,CASSANDRA-5534,12645855,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,enigmacurry,enigmacurry,02/May/13 18:53,12/Mar/19 14:14,13/Mar/19 22:29,05/Jul/13 08:44,2.0.0,,,,,,0,,,,,"Introduced in commit -e74c13ff08663d306dcc5cdc99c07e9e6c12ca21- (see below) there is a significant slow down when creating a wide row with cassandra-stress:

Testing with the prior (good) commit I used this to write a single wide row, which completed rather quickly:
{code}
$ ccm create -v git:60f09f0121e0801851b9ab017eddf7e326fa05fb wide-row
Fetching Cassandra updates...
Cloning Cassandra (from local cache)
Checking out requested branch (60f09f0121e0801851b9ab017eddf7e326fa05fb)
Compiling Cassandra 60f09f0121e0801851b9ab017eddf7e326fa05fb ...
Current cluster is now: wide-row
$ ccm populate -n 1
$ ccm start
$ time ccm node1 stress -c 10000 -S 1000 -n 1
Created keyspaces. Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,latency/95th/99th,elapsed_time
1,0,0,273.3,273.3,273.3,0
END

real	0m7.106s
user	0m1.710s
sys	0m0.120s
{code}

Using the bugged commit (e74c13ff08663d306dcc5cdc99c07e9e6c12ca21) I get a significant slow down:
{code}
02:42 PM:~$ ccm create -v git:e74c13ff08663d306dcc5cdc99c07e9e6c12ca21 wide-row
Fetching Cassandra updates...
Current cluster is now: wide-row
02:42 PM:~$ ccm populate -n 1
02:42 PM:~$ ccm start
02:42 PM:~$ time ccm node1 stress -c 10000 -S 1000 -n 1
Created keyspaces. Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,latency,95th,99th,elapsed_time
1,0,0,423.2,423.2,423.2,0
Total operation time      : 00:00:00
END

real	4m16.394s
user	0m2.230s
sys	0m0.137s

{code}

Interestingly, the commit in question just says it's a merge from cassandra-1.2, but I do not see this same slowdown using that branch, this only occurs in trunk.",,,,,,,,,,,,,,,,,,,,,,,,02/May/13 18:54;enigmacurry;wide_row_stress.trunk.log.txt.gz;https://issues.apache.org/jira/secure/attachment/12581573/wide_row_stress.trunk.log.txt.gz,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-02 22:38:22.863,,,no_permission,,,,,,,,,,,,326214,,,Fri Jul 05 14:12:32 UTC 2013,,,,,,0|i1ka1j:,326559,,,,,,,,,,,,,,,,,,alexzar,"02/May/13 18:59;enigmacurry;The reason I titled this bug with the phrase ""after compaction"" is that the very last line of the log file that talked about compaction is what it stayed on for the majority of that 4 minutes that it ran for. I don't have any other clues that this is related to compaction. ",02/May/13 22:38;jbellis;It takes multiple minutes for me on e74c13ff08663d306dcc5cdc99c07e9e6c12ca21^ as well.  Are you sure that's the problem?,"03/May/13 00:21;enigmacurry;No, you're right. I think my git-fu was failing me, I was just going by git log, but I think I need to do a full bisect. I'll update when I find the real cause.",03/May/13 01:13;enigmacurry;The bisect brought me back to a950b9257f4c92d067eb5e1d437096699123ac9b which is from [CASSANDRA-5125|https://issues.apache.org/jira/browse/CASSANDRA-5125] - that's the first instance where this slowdown occurs.,"03/May/13 01:22;jbellis;It looks like there is slowdown in the schema creation as well as the inserts.

(I note that stress is creating 10,000 named columns which is not a normal ""wide row.""  There is no way to configure stress to create ""real"" wide rows, unfortunately.)",26/Jun/13 17:56;slebresne;Would you mind testing that again? I just tried the same test with current trunk and it's fast (< 2s for the whole stress).,"05/Jul/13 02:19;alexzar;latest test on cassandra-1.2:

$> ccm create -v git:cassandra-1.2 test-1.2.6
Fetching Cassandra updates...
Cloning Cassandra (from local cache)
Checking out requested branch (cassandra-1.2)
Compiling Cassandra cassandra-1.2 ...
Current cluster is now: test-1.2.6

$> ccm populate -n 1
$> ccm start

$> time ccm node1 stress -c 10000 -S 1000 -n 1
Created keyspaces. Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,latency/95th/99th,elapsed_time
1,0,0,233.6,233.6,233.6,0
END

real    0m6.539s
user    0m1.714s
sys     0m0.137s

latest test on trunk:

$> ccm create -v git:trunk test-trunk
Fetching Cassandra updates...
Cloning Cassandra (from local cache)
Checking out requested branch (trunk)
Compiling Cassandra trunk ...
Current cluster is now: test-trunk

$> ccm populate -n 1
$> ccm start

$> time ccm node1 stress -c 10000 -S 1000 -n 1
Created keyspaces. Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,latency,95th,99th,elapsed_time
1,0,0,343.1,343.1,343.1,0


Total operation time      : 00:00:00
END

real    0m7.333s
user    0m1.945s
sys     0m0.136s
","05/Jul/13 08:44;slebresne;So closing since both our experience seems to show that this has been fixed somehow: it doesn't take ""multiple minutes"" anymore.","05/Jul/13 14:12;enigmacurry;Alex, I think it would be useful to know what the root cause of this was, so we can track it going forward. Running a 'git bisect' should tell us.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cluster : Host ID collision,CASSANDRA-5450,12641826,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,nivance,nivance,10/Apr/13 12:39,12/Mar/19 14:14,13/Mar/19 22:29,11/Apr/13 02:14,1.2.0,,,Local/Config,,,0,cluster,,,,"I follow the guide ""Initializing a multiple node cluster"" in url:
http://www.datastax.com/docs/1.2/initialize/cluster_init

the exception occurs below when start the second node:

java.lang.RuntimeException: Host ID collision between active endpoint /192.168.0.193 and /192.168.0.194 (id=4ebdbeea-2712-475b-a3e3-64b6e6b099a9)
	at org.apache.cassandra.locator.TokenMetadata.updateHostId(TokenMetadata.java:227)
	at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1296)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1157)
	at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:1895)
	at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:805)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:883)
	at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:43)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)


I track the source code. the function ""getHostId"" in class org.apache.cassandra.gms.Gossiper, diff endpoint it returns the same value,like:
/192.168.0.194	4ebdbeea-2712-475b-a3e3-64b6e6b099a9
/192.168.0.88	c375010a-c464-40c9-a7db-61de319a4cba
/192.168.3.21	4ebdbeea-2712-475b-a3e3-64b6e6b099a9
/192.168.0.193	4ebdbeea-2712-475b-a3e3-64b6e6b099a9
so, the exception occurs.


Is it a bug or is my Configuration wrong?
","centos-6-x86_64, jdk1.6.0_30, cassandra1.2.0
4 nodes, 2 datacenter, 2 nodes for each datacenter",,,60,60,,0%,60,60,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-04-10 13:23:01.455,,,no_permission,,,,,,,,,,,,322241,,,Thu Apr 11 02:18:13 UTC 2013,,,,,,0|i1jljr:,322586,,,,,,,,,,,,,,,,,,,"10/Apr/13 13:23;brandon.williams;bq. Is it a bug or is my Configuration wrong?

Are these virtual machines you cloned or in any way copied the system tables between?","11/Apr/13 01:42;nivance;yes, I make one node configured, then tar the dir of cassandra, scp to the other three nodes.","11/Apr/13 02:13;nivance;It's my fault. 

The system tables of Every node is the same.

The cluster run well after I delete the system tables in each node.

Thks Williams.",11/Apr/13 02:18;brandon.williams;You're welcome.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Paging filter empty rows a bit too agressively,CASSANDRA-6040,12669007,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,17/Sep/13 12:58,12/Mar/19 14:14,13/Mar/19 22:29,17/Sep/13 13:37,2.0.1,,,,,,0,,,,,See the attached patch.,,,,,,,,,,,,,,,,,,,,,,,,17/Sep/13 12:59;slebresne;0001-Correctly-filter-empty-rows-during-paging.txt;https://issues.apache.org/jira/secure/attachment/12603586/0001-Correctly-filter-empty-rows-during-paging.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-17 13:31:15.728,,,no_permission,,,,,,,,,,,,348939,,,Tue Sep 17 13:37:21 UTC 2013,,,,,,0|i1o61j:,349237,,,,,,,,iamaleksey,iamaleksey,,,2.0.0,,,,,,,17/Sep/13 13:31;iamaleksey;+1,"17/Sep/13 13:37;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ISE during short reads,CASSANDRA-5440,12641485,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,08/Apr/13 23:32,12/Mar/19 14:14,13/Mar/19 22:29,09/Apr/13 22:05,2.0 beta 1,,,,,,0,,,,,"On trunk:

{noformat}
ERROR [Thrift:2] 2013-04-08 15:06:56,468 ProcessFunction.java (line 41) Internal error processing execute_cql3_query
java.lang.IllegalStateException
    at java.util.AbstractList$Itr.remove(AbstractList.java:356)
    at org.apache.cassandra.db.filter.SliceQueryFilter.trim(SliceQueryFilter.java:187)
    at org.apache.cassandra.db.SliceFromReadCommand.maybeTrim(SliceFromReadCommand.java:101)
    at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:902)
    at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:831)
    at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:128)
    at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:56)
    at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:130)
    at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:141)
    at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1836)
    at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4232)
    at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4216)
    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
    at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
    at java.lang.Thread.run(Thread.java:662)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,09/Apr/13 21:22;jbellis;5440.txt;https://issues.apache.org/jira/secure/attachment/12577894/5440.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-09 21:22:35.807,,,no_permission,,,,,,,,,,,,321901,,,Tue Apr 09 22:05:25 UTC 2013,,,,,,0|i1jjg7:,322246,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,08/Apr/13 23:32;brandon.williams;I'll bisect this.,"09/Apr/13 18:08;brandon.williams;Bisect says:

{noformat}
There are only 'skip'ped commits left to test.
The first bad commit could be any of:
aa76394764bcb4af54150f12528fed9ddfa66044
fbe99b711aa9be3fcfd1e5e0f37c4d45a1717864
{noformat}

Obviously between these it has to be aa76394764bcb4af54150f12528fed9ddfa66044 which is CASSANDRA-5403",09/Apr/13 21:22;jbellis;patch attached.,09/Apr/13 21:35;brandon.williams;+1,09/Apr/13 22:05;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'null' error when running sstablesplit on valid sstable,CASSANDRA-6027,12668517,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,dmeyer,dmeyer,13/Sep/13 17:49,12/Mar/19 14:14,13/Mar/19 22:29,17/Sep/13 13:34,2.0.1,,,,,,0,,,,,"ccm create --cassandra-version git:cassandra-2.0 test
ccm populate -n 1
ccm start

ccm node1 stress -n 10000000 -o insert
ccm node1 compact
cd ~/.ccm/test/node1/data
../bin/sstablesplit -s 100 ./Keyspace1/Standard1/Keyspace1-Standard1-jb-12-Data.db

Expected: single sstable should be split into multiple sstables

Got:
Pre-split sstables snapshotted into snapshot pre-split-1379088385051
Error splitting SSTableReader(path='./Keyspace1/Standard1/Keyspace1-Standard1-jb-12-Data.db'): null

running du -h on the large compacted sstable showed it to be 2.4GB

This is probably related to CASSANDRA-6026; however, it is different.  In this bug the split does not occur, whereas in 6026 the split does occur though an error is thrown.","Environment: java version ""1.7.0_40""
Java(TM) SE Runtime Environment (build 1.7.0_40-b43)
Java HotSpot(TM) 64-Bit Server VM (build 24.0-b56, mixed mode)",,,,,,,,,,,,,,,,,,,,,,,17/Sep/13 09:08;slebresne;6027.txt;https://issues.apache.org/jira/secure/attachment/12603572/6027.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-17 11:24:50.561,,,no_permission,,,,,,,,,,,,348451,,,Tue Sep 17 13:34:09 UTC 2013,,,,,,0|i1o30v:,348748,2.0.0,,,,,,,iamaleksey,iamaleksey,,,,,,,,,dmeyer,17/Sep/13 11:24;iamaleksey;+1 (<SSTableReader> is not needed here though).,"17/Sep/13 13:34;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The native protocol server can trigger a Netty bug,CASSANDRA-5955,12666304,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,30/Aug/13 11:30,12/Mar/19 14:14,13/Mar/19 22:29,02/Sep/13 16:17,1.2.10,,,,,,0,,,,,"The patch from CASSANDRA-5926 did fix the original deadlock, but unfortunately we can now run into a netty bug (with MemoryAwareThreadPoolExecutor): https://github.com/netty/netty/issues/1310.

That bug has been fixed in netty 3.6.6 but we're currently using an older version (3.5.9). So we should just upgrade our dependency to 3.6.6. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-08-30 14:42:09.945,,,no_permission,,,,,,,,,,,,346243,,,Mon Sep 02 16:17:53 UTC 2013,,,,,,0|i1npgf:,346544,,,,,,,,jbellis,jbellis,,,,,,,,,,30/Aug/13 14:42;jbellis;+1 on principle,"02/Sep/13 16:17;slebresne;Alright, dependency updated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in Pig CassandraStorage,CASSANDRA-6072,12669761,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,alexliu68,jjordan,jjordan,20/Sep/13 22:35,12/Mar/19 14:14,13/Mar/19 22:29,25/Sep/13 21:11,1.2.11,2.0.2,,,,,0,,,,,"key_alias can be null for tables created from thrift.

Which causes an NPE here:
https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/hadoop/pig/AbstractCassandraStorage.java#L633",,,,,,,,,,,,,,,,,,,,,,,,23/Sep/13 19:42;alexliu68;6072-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12604655/6072-1.2-branch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-23 19:39:49.73,,,no_permission,,,,,,,,,,,,349689,,,Wed Sep 25 21:11:02 UTC 2013,,,,,,0|i1oanz:,349987,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"23/Sep/13 19:39;alexliu68;It's the issue when upgrade from 1.1 to 1.2.6, but the issue has been fixed by CASSANDRA-5800, so I use this ticket to clean up the code. We don't need check key_alias any more.","23/Sep/13 20:35;iamaleksey;[~alexliu68] 5800 migrates the key_alias to key_aliases. The issue here is that there might not be a key_alias in the first place, so there is nothing to migrate.

Basically, the code should handle tables without a key alias, and it doesn't. 5800 wasn't covering that.",23/Sep/13 20:49;alexliu68;The clean up patch just removes key_alias which should fix NPE as well.,25/Sep/13 21:11;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Timeuuid with CLUSTERING ORDER DESC cannot be used with the dateOf CQL3 function,CASSANDRA-5472,12642541,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,gcollins,gcollins,15/Apr/13 14:04,12/Mar/19 14:13,13/Mar/19 22:29,29/Apr/13 07:32,1.2.5,,,,,,0,,,,,"I originally raised this issue in the mailing lists:

http://www.mail-archive.com/user@cassandra.apache.org/msg29185.html

Here is what I tried:

cqlsh:location> create table test_y (message_id timeuuid, name text,
PRIMARY KEY (name,message_id));
cqlsh:location> insert into test_y (message_id,name) VALUES (now(),'foo');
cqlsh:location> insert into test_y (message_id,name) VALUES (now(),'foo');
cqlsh:location> insert into test_y (message_id,name) VALUES (now(),'foo');
cqlsh:location> insert into test_y (message_id,name) VALUES (now(),'foo');
cqlsh:location> select dateOf(message_id) from test_y;

 dateOf(message_id)
--------------------------
 2013-04-13 00:33:42-0400
 2013-04-13 00:33:43-0400
 2013-04-13 00:33:43-0400
 2013-04-13 00:33:44-0400

cqlsh:location> create table test_x (message_id timeuuid, name text,
PRIMARY KEY (name,message_id)) WITH CLUSTERING ORDER BY (message_id DESC);
cqlsh:location> insert into test_x (message_id,name) VALUES (now(),'foo');
cqlsh:location> insert into test_x (message_id,name) VALUES (now(),'foo');
cqlsh:location> insert into test_x (message_id,name) VALUES (now(),'foo');
cqlsh:location> insert into test_x (message_id,name) VALUES (now(),'foo');
cqlsh:location> insert into test_x (message_id,name) VALUES (now(),'foo');
cqlsh:location> select dateOf(message_id) from test_x;
Bad Request: Type error:
org.apache.cassandra.cql3.statements.Selection$SimpleSelector@1e7318 cannot
be passed as argument 0 of function dateof of type timeuuid

It should be possible to use dateOf on message_id in table test_x",,,,,,,,,,,,,,,,,,,,,,,,24/Apr/13 09:15;slebresne;5472.txt;https://issues.apache.org/jira/secure/attachment/12580263/5472.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-24 09:15:35.871,,,no_permission,,,,,,,,,,,,322955,,,Mon Apr 29 07:32:37 UTC 2013,,,,,,0|i1jpy7:,323300,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"24/Apr/13 09:15;slebresne;Attaching patch to fix. The main problem is that the selector was still using the AbstractType to check for type equality, so the ReversedType was messing things up. Fixed to use the CQL3 type instead as in other places. Other that that, the patch also adds toString() methods so that the error looks readable. ",28/Apr/13 21:45;iamaleksey;Makes sense and lgtm. +1,"29/Apr/13 07:32;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTableMetadata.min(max)ColumnNames keep whole SlabAllocatior region referenced; wasting memory,CASSANDRA-6077,12670016,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,m0nstermind,m0nstermind,m0nstermind,23/Sep/13 09:58,12/Mar/19 14:13,13/Mar/19 22:29,23/Sep/13 14:14,2.0.2,,,,,,0,,,,,".. which could be a problem when there is a lot of sstables, when using LCS for example.

SSTableWriter calls SSTableMetadata.Collector.updateMin(Max)ColumnNames passing List of ByteByffers which reference a small byte array in slab region.
ColumnNameHelper.mergeMin(Max) just returns a reference of column name back to SSTableMetadata. So the latter keeps whole slab region referenced, preventing it from being GCed. 

Fixed it by making copies of column name bytebuffer, if its size more than column name itself.",,,,,,,,,,,,,,,,,,,,,,,,23/Sep/13 11:27;m0nstermind;ColumnNameHelper.diff;https://issues.apache.org/jira/secure/attachment/12604566/ColumnNameHelper.diff,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-23 14:14:10.051,,,no_permission,,,,,,,,,,,,349846,,,Mon Sep 23 14:14:10 UTC 2013,,,,,,0|i1obmv:,350144,,,,,,,,jbellis,jbellis,,,2.0.0,,,,,,,23/Sep/13 14:14;jbellis;Nice catch; committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Discard pooled readers for cold data,CASSANDRA-5661,12653702,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,jbellis,jbellis,19/Jun/13 13:31,12/Mar/19 14:13,13/Mar/19 22:29,17/Sep/13 21:58,2.0.1,,,,,,0,,,,,"Reader pooling was introduced in CASSANDRA-4942 but pooled RandomAccessReaders are never cleaned up until the SSTableReader is closed.  So memory use is ""the worst case simultaneous RAR we had open for this file, forever.""

We should introduce a global limit on how much memory to use for RAR, and evict old ones.",,,,,,,,,,,,,,,,,,,,,,,,21/Jul/13 04:50;xedin;CASSANDRA-5661-global-multiway-cache.patch;https://issues.apache.org/jira/secure/attachment/12593385/CASSANDRA-5661-global-multiway-cache.patch,05/Jul/13 19:22;xedin;CASSANDRA-5661.patch;https://issues.apache.org/jira/secure/attachment/12591034/CASSANDRA-5661.patch,26/Jun/13 19:50;jjordan;DominatorTree.png;https://issues.apache.org/jira/secure/attachment/12589788/DominatorTree.png,26/Jun/13 19:50;jjordan;Histogram.png;https://issues.apache.org/jira/secure/attachment/12589787/Histogram.png,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-06-25 18:54:42.037,,,no_permission,,,,,,,,,,,,333979,,,Tue Sep 17 22:19:51 UTC 2013,,,,,,0|i1lm2f:,334305,,,,,,,,jbellis,jbellis,,,,,,,,,dmeyer,"19/Jun/13 13:32;jbellis;One possibility would be to have a {{CLHM<SSTableReader, Queue<RandomAccessReader>>}} that PooledSegmentedFile looks itself up in.

[~xedin], do you have time to take a stab at this?","25/Jun/13 18:39;jbellis;This has bitten someone in production now...  LCS w/ 4000 sstables, using 2GB of heap (16k CRAR buffers).",25/Jun/13 18:54;xedin;I'm starting to think that it would be just easier for everybody to just start using mmap buffers for compressed buffers all the time instead before we make caching too complex. I will start working in that direction to see if there is any promise in that.,25/Jun/13 18:57;jjordan;Saw some issues caused by these never getting cleared out.  On a cluster using LCS CompressedRandomAccessReader objects are using 2 GB of heap space per node.,"25/Jun/13 21:46;jbellis;bq. I'm starting to think that it would be just easier for everybody to just start using mmap buffers for compressed buffers all the time

Good idea, but probably too big a change for 1.2.7?","25/Jun/13 21:55;xedin;As a brain dump, I see 3 ways to resolve this:

1. Introduce queue total memory limit and evict based on that, but there is no guarantee that we won't be evicting incorrect instances.

2. Introduce liveness per instance (e.g. 20-30 seconds) before evictor considers it as ""old"", that solves the problem with #1 but are relying on eviction thread to be robust and run constantly, any delays in such manual GC could result in the same memory bloat as described in the issue.

3. Remove caching and go with mmap'ed segments instead, the problem with that is that we need to create direct byte buffer every time we decompress data which I'm not sure if could be GC'ed reliably, so for example, if particular JVM implementation only cleans up such buffers only on CMS or full GC process can effectively OOM because we are actually trying to avoid any significant GC activity as much as possible.

I like #2 the most. Thoughts?","25/Jun/13 23:05;jbellis;bq. Introduce queue total memory limit and evict based on that, but there is no guarantee that we won't be evicting incorrect instances.

I don't follow -- surely LRU is a better heuristic than ""idle for N seconds"" as in #2?","26/Jun/13 01:08;xedin;LRU is more of a replacement strategy when you have distinct objects and fixed upper limit (num items, memory), but in here we deal with mostly duplicate objects and the states where num of items hardly overgrows queue so the problem is not handling replacement but rather expiring items after some period of idling. I was thinking of doing something like ExpiringQueue with the one (or set of timers) similar to Guava Cache. Which solves the problem of [C]RAR instances being stuck in the cache for long periods of time even when read pattern have changed and they are not useful anymore.","26/Jun/13 03:28;jbellis;It still looks like an LRU problem to me: I have room for N objects, when I try to allocate N+1 I throw away the LRU.

This means we will use more memory than a timer approach if N is actually more than we need, but it will do better than timers if we are crunched for space, which seems like the more important scenario to optimize.

It also means that we have a very clear upper bound on memory use, rather than depending on workload, which history shows is tough for users to tune.","26/Jun/13 03:55;xedin;It seems like we are trying to address different problems of what is in description to the ticket and what Jeremia pointed out. Let me describe what I'm trying to solve: When reading from multiple SSTables for a while and then pattern changes and load is switched to the different subset of SSTables, previous [C]RAR instances are returned to the appropriate queues and stuck there until each SSTable is deallocated (by compaction) which creates memory pressure on stale workloads or when compaction is running behind.

LRU could solve that problem when we have limit on total amount of memory that we can use so it would start kicking in only after we reach that limit and create a jitter in the queue and processing latencies. 

What I propose adds minimal booking overhead per queue and expires items quicker than LRU and more precise, also I'm not really worried about max number of items in the queue per SSTable as it's organically limited to the number of concurrent readers.",26/Jun/13 04:45;jbellis;LCS makes number of sstables x concurrent readers a problem.,26/Jun/13 06:13;xedin;What is LCS?,"26/Jun/13 09:46;jjordan;[~xedin] LCS == LeveledCompactionStrategy.  While in theory I like the idea of the searchers in most cases expiring quicker with timers, in practice since these are on the heap, and LCS defaults to only 5 MB files per levels, you can have A LOT of sstables, 200 per GB...  Which as [~jbellis] mentioned makes # of sstables * concurrent readers a big problem, which was the problem hit above.  So we really need to bound memory usage with a hard cap, not the fuzzy cap of ""how many can I open in X seconds"".

1k reqs/sec hitting a 4 level deep LCS CF could mean 4k Reader's (~500 MB) created per second.  Now that I say that out loud, I almost think we should go back to not caching these at all so they always just get recycled in young gen and never have a chance to hit old gen.  I guess it could be a config parameter which defaults on for STCS and off for LCS.  Since STCS work loads have no where near as many sstables.","26/Jun/13 14:20;jbellis;I'm not as pessimistic -- I think pooling will still be useful for most workloads -- but if we add a configurable ceiling on memory use, we could certainly add that zero = no pooling at all.

Would also be useful to track the pool ""hit rate.""","26/Jun/13 19:00;xedin;bq. While in theory I like the idea of the searchers in most cases expiring quicker with timers, in practice since these are on the heap, and LCS defaults to only 5 MB files per levels, you can have A LOT of sstables, 200 per GB... Which as Jonathan Ellis mentioned makes # of sstables * concurrent readers a big problem, which was the problem hit above. So we really need to bound memory usage with a hard cap, not the fuzzy cap of ""how many can I open in X seconds""

When I was talking about concurrent readers I actually meant the max number of threads in READ stage (concurrent_reads in yaml, default 32) that could be running in the same time, that is the worst case limit of cache queue per SSTable.
 
I'm not convinced that LRU on SSTable would work better then [C]RAR instance expiration for our use case because due to replacement mechanism it creates jitter in latencies grater than p75 once global threshold is reached, which in production systems create situation when people can't explain why latencies suddenly degraded without increasing amount of data.

I'm not sure that I follow what you mean by last part, we are not trying to limit number of open for period of time but rather expire items that been returned to the queue after time period. I agree that we can add upper limit on memory usage just like we did for key/row caches, which in combination with expiring would yield predictable behavior.

bq. 1k reqs/sec hitting a 4 level deep LCS CF could mean 4k Reader's (~500 MB) created per second. Now that I say that out loud, I almost think we should go back to not caching these at all so they always just get recycled in young gen and never have a chance to hit old gen. I guess it could be a config parameter which defaults on for STCS and off for LCS. Since STCS work loads have no where near as many sstables.

It sounds like memory overhead caused by compression for your use-case is too big with or without caching (it's around 79KB = 64KB default chunk + snappy overhead) per file, even if you drop caching your allocation rate would cause [C]RAR buffers to be promoted and cause fragmentation in the old gen which could cause long ""remark"" phrase of CMS (can be battled with CMSScavengeBeforeRemark option) and as CMS is not compacting you can suffer from FullGC more frequently. That is what we are trying to battle with caching [C]RAR instances (and their buffers) as if we allocate those close in time and reuse, it's more luckily that they are going to be laid out closely plus removes portion of work from young gen collector.","26/Jun/13 19:56;jjordan;Screen shots attached to show the issue.
Histogram.png show my histogram.  Over 2 GB is being retained by CRAR objects on a node with only 80 GB of data, most of that in LCS CF's with 20 MB sstable size, and some ine LCS CF's with 10 MB sstable size.
!Histogram.png!

Here is the dominator tree for that same heap dump.
!DominatorTree.png!

The problem is that most of those sstables have 1 MB or more of CRAR buffers sitting there waiting to be used.  If we do a timer with an absolute cap, even better, but we need some kind of cap.

The compression overhead itself isn't really an issue, the issue is the cached CRAR objects/buffers.","26/Jun/13 22:05;xedin;bq. The problem is that most of those sstables have 1 MB or more of CRAR buffers sitting there waiting to be used. If we do a timer with an absolute cap, even better, but we need some kind of cap.

1MB actually means that it's around 12-14 caches reads which is not even half of default concurrency limit, we can start with expiring for 1.2.7 as it doesn't require adding options (and maybe disable caching with LCS), then for 2.0 we can add cap limit per CF or even global into yaml.

bq. The compression overhead itself isn't really an issue, the issue is the cached CRAR objects/buffers.

It is for such a small files because it's not just buffers but also compression metadata per SSTable although you can't see it in histogram/tree as it was moved off heap, I guess it's an artifact of LCS that we can't do anything about.

This all got me wondering what is BF space overhead LCS vs. STCS, can you please check?","26/Jun/13 22:12;jbellis;bq. it's not just buffers but also compression metadata per SSTable although you can't see it in histogram/tree as it was moved off heap

The on-heap part is bad enough :)","26/Jun/13 23:17;xedin;After thinking about this more as LCS produces similar (same) sized files there is actually no benefit of caching [C]RAR instances. I think we should do caching only with STCS + expiring as it introduces less memory overhead per file for [C]RAR.

I also want to share my worries about LCS using 5MB file size with mmap and no compression: default vm.max_map_count is 65536 (maximum number of memory mappings per process) which is around 322GB of data, we need to mention somewhere that if somebody wants to run with mmap + LCS they need to adjust that setting. Another thing that worries me is that performance of mmap, munmap and similar + minor/major page faults would degrade logarithmically as memory mappings are handled by red-black tree and with constant compaction we would be burning a lot more cpu on tree balancing as dataset grows.","27/Jun/13 01:36;jbellis;I don't see what similar-sized files has to do with it.  CRAR buffer size is independent of file size, and that's what causes the fragmentation.  (Zeroing out large buffers isn't free, either.)",27/Jun/13 01:54;xedin;I'm not sure what you mean. I was trying to say that caching doesn't deliver good trade-off in terms of memory usage on small files. On the other hand caching makes more sense with STCS especially on higher tiers as number of blocks handled by one buffer would be few orders of magnitude higher.,"27/Jun/13 04:03;xedin;In other words, my initial idea didn't take into account leveled compaction, it was simple - by paying overall small memory price (one chunk size or less depending on size of file) per 1GB we can minimize GC work, memory allocation and number of syscalls per read. With leveled compaction that strategy doesn't work as price per 1 GB is pretty big as most of the files are very small which increases allocation rate to cache and GC activity by very frequent compactions.","27/Jun/13 13:43;tjake;We use LCS and haven't seen any heap pressure here, though we set our files to 128M.  Would it make more sense to change this default from 5mb.  No one successfully using LCS has it set to 5mb.","27/Jun/13 14:16;jbellis;Agreed that we should evaluate LCS defaults (and we have Daniel Meyer working on this now), but that just kicks the can down the road; if you're in trouble with 50GB of data and 5MB sstables, you'll be in equal trouble at 1TB of data and 100MB sstables.","27/Jun/13 19:28;xedin;Well it depends on how do you define equal, having 1TB of data would definitely require bigger heap and physical memory configuration. 

Let's calculate (where each file have one buffer in memory at all times):

5MB   files (each 79KB decompression buffer) for 1GB of such files in memory would be: 204 (num files in 1GB) * 79KB = *16MB* buffers
128MB files (-//-) require 25.5 times less buffers per 1GB than 5MB files: 16MB (buffers per 1GB in case of 5MB files) / 25.5 = *643KB* buffers

So for 1TB with 5MB files we need 1024 * 16MB = *16GB* of heap and for 128MB files it's 25.5 times less = *643MB*, if each of the files is going to have at least 8 caches items in the same time with 128MB files we are going to have around 5GB of heap but I do think this scenario is a worst case, normal mode would be 2-3GB. If you go with 14-16GB heap and 1TB of data, 2GB of cache is the least of your problems as it's around 10% of total heap size which is still good trade-off to allocation rate if those buffers are allocated per call.","29/Jun/13 17:32;jbellis;It occurs to me that we may be approaching the problem the wrong way.  If we just pool the buffers rather than the CRAR objects, we would only need (concurrent readers x max sstables for a given partition) which is going to be much lower than the total sstable count.","29/Jun/13 20:18;xedin;I tried that before going with cached instances which is per CF map<int, queue<ByteBuffer>> as one CF could have different chunk sizes, it actually performs worse because of queue contention and we would still have to pay the price of ""open"" call on each read of file.","29/Jun/13 20:31;jbellis;We don't need exact chunk size matches though -- i.e., we can use a larger buffer.  So if we just pool max chunk size buffers we'll probably come out ahead.",29/Jun/13 20:58;xedin;It's waste of memory and doesn't solve contention problem.,29/Jun/13 23:59;jbellis;It's a lot less memory used than the status quo.  I'd take a little contention over OOMing people.,"30/Jun/13 00:42;xedin;I think we are trying solve the consequence instead of actual problem of adjusting max sstable size as jake pointed out, I think [~vijay2win@yahoo.com] was also doing so in production. Caching in the current state does the job for STCS and LCS with bigger files, expiring would be a good addition tho.","30/Jun/13 01:32;jbellis;As I explained, increasing sstable size helps but does not solve the problem; we're supposed to be supporting up to 5-10TB of data in 1.2.","30/Jun/13 02:00;xedin;What I am just trying to say is that expiring with global limit as good enough even for LCS with bigger files, but useless with 5MB besides all other problems. And as I pointed in on of the comment for 5-10 terabytes even with 128MB files are too small and affect system performance without taking into account indexing/bf overhead.","30/Jun/13 03:33;jbellis;bq. What I am just trying to say is that expiring with global limit as good enough even for LCS with bigger files

I don't see how that follows at all.  The expiring approach is broken at any dataset size where memory pressure is a problem, since in the worst case it will not evict quickly enough.","30/Jun/13 04:44;xedin;Right, that's what max memory size cap is for, to make eviction more intelligent in times of memory pressure, and concurrency as limited so as dataset grows there wouldn't be a lot if items in each the queue anyway.",05/Jul/13 19:22;xedin;Attached patch adds FileCacheService (+ metrics) which can expire instances after they are not accessed for time period and has a global memory usage limit (set to 512MB).,"07/Jul/13 02:25;jbellis;I talked this over a bit with [~ben.manes] (author of CLHM).  Here's his take:

{quote}
[This] sounds less like a multimap cache than a multi-way object pool. To me a multimap cache would be like any other cache where entries are read frequently, rarely explicitly invalidated, and evicted by a boundary condition. An object pool has instances checked in and out, so care needs be taken to make sure the transfer overhead is cheap. I think you want a more advanced pool with global boundary conditions and is multi-way, so more complex than a traditional database connection pool. For that, actually, a few years ago I advised the author of BoneCP to use a LinkedTransferQueue to leverage elimination to avoid contention which provided the performance improvements to make his library the fastest available.
{quote}

Ben put together an implementation at https://github.com/ben-manes/multiway-pool:

{quote}
 * A concurrent object pool that supports pooling multiple resources that are associated with a
* single key. A resource is borrowed from the pool, used exclusively, and released back for reuse
* by another caller. This implementation can optionally be bounded by a maximum size, time-to-live,
* or time-to-idle policies.
* 
* A traditional object pool is homogeneous; all of the resources are identical in the data and
* capabilities offered. For example a database connection pool to a shared database instance. A
* multiway object pool is heterogeneous; resources may differ in the data and capabilities offered.
* For example a flat file database may pool random access readers to the database table files. The
* relationship of a single-way to a multi-way object pool is similar to that of a map to a
* multimap.
* 
* When this pool is bounded any resource is eligible for eviction regardless of the key that it is
* associated with. A size based bound will evict resources by a best-effort LRU policy and a time
* based policy will evict by either a time-to-idle and/or time-to-live policy. The resource's life
* cycle can be instrumented, such as when cleaning up after eviction, by using the appropriate
* ResourceLifecycle method.
{quote}","07/Jul/13 03:15;xedin;The only point I disagree with is ""rarely explicitly invalidated"", this is not true especially with LCS and small files, that work already done for us by compaction, things we need to care are - expiry after access + total memory limit (as concurrency is also limited by read stage size so all of the queues are implicitly bounded). My implementation is fairly simple and takes advantage of all services provided by upper levels (compaction for eviction, read stage for concurrently limiting) as well as build-in guava cache compatibilities of expiring items and handling removals.","07/Jul/13 03:35;ben.manes;""rarely explicitly invalidated"" is in regards to a cache, as Jonathan originally described the problem as a multimap cache instead of as an object pool. He also expressed concern with evicting a block of buffers at once when he conceived of the same model that you implemented.

I am intimately familiar with Guava's cache as I designed the algorithms, ported and wrote code for it, and advised on the api. Unfortunately I am not familiar with Cassandra's needs and its code, so the pool was implemented based on a brief description of the problem and ideal behavior.

It was a fun exercise for a long weekend. I'd recommend writing tests and benchmarks, which unfortunately appears to be missing with the patch in its current form. Of couse use whatever makes the most sense.","07/Jul/13 03:46;xedin;It's more of object pool, where each key has limited number of ""equal"" objects, so if caller wants to read any portion of the file it would get an instance which represents whole file, seek to appropriate position and do reading, returning that instance to the pull when done. evicting block of buffers is still required when files are compacted out which would be more frequent than eviction by timer because use-cases usually don't drift in pattern. I also raised question about what I think is a major problem here - max size of a file being 5MB by default for CLS.","07/Jul/13 04:21;jbellis;bq. It's more of object pool, where each key has limited number of ""equal"" objects

Yes.  This is what the javadoc means by ""supports pooling multiple resources that are associated with a single key.""

bq. evicting block of buffers is still required when files are compacted out

Sure, and this is supported, but this is not a case of contention which was the concern I raised with Ben.","07/Jul/13 04:24;jbellis;bq. I also raised question about what I think is a major problem here - max size of a file being 5MB by default for CLS

I mentioned that Daniel Meyer is working on this, but I've formally opened CASSANDRA-5727 in the hopes that that helps restrict our scope of discussion here. :)","07/Jul/13 05:00;xedin;bq. Sure, and this is supported, but this is not a case of contention which was the concern I raised with Ben.

What contention are we talking? If that is memory pressure, then expiring items in bulk is good and leaves room for new files. In case of LCS instances mostly are going to be invalidated by compaction vs access time expiry, STCS observes the same effect but better in terms of memory usage as smaller files are going to be compacted into bigger one so each compaction reduces number of instances and buffers. 

I think the original problem in here was that with default settings LCS eats a lot of memory because files are too small and even if minor portion of dataset is read memory, overhead for caching is still unacceptable, this is why I was talking about adding global memory cap and as a good bonus - expiry of unused instances, because if file is expired by timer, it means that reads have turned away from it or it's read in bursts so deallocating all of the cached instances is the way to go, where with LRU we would only replace when new instances are returned to the pull (as we don't pre-allocate) which could create a problem when there is a burst of requests to the same file after long internal of inactivity. But even after so many comments that we had here and people ([~tjake]) reporting that even current setup works for them on bigger files, it still looks like the everybody is trying to solve different issues.","07/Jul/13 05:39;jbellis;Perhaps it will help if I recap:

# CRAR pooling + LCS OOMs us at relatively small amounts of data (20GB in Jeremiah's example)
# worst case pool size proportional to sstables * concurrent readers means that memory usage increases linearly with disk size; increasing sstable size decreases it linearly, so we expect 100MB sstables [which is on the high side of what is reasonable IMO] to get us to ~4TB of space managed which is good but not ""problem solved"" territory. 
# thus, we need to bound CRAR pooling and not just say ""it's working as designed, go use larger sstables or STCS"" [although that is probably adequate for 1.2, so I am tagging this for 2.0]
# The most common operations in an object pool are the borrow/return.  Ben's multiway pool optimizes for this, with particular attention to possible contention from multiple reader threads.
# The multiway pool supports expiring objects after they have been idle for a given time, as well as a total pool size.
","07/Jul/13 05:59;xedin;bq. worst case pool size proportional to sstables * concurrent readers means that memory usage increases linearly with disk size; increasing sstable size decreases it linearly, so we expect 100MB sstables [which is on the high side of what is reasonable IMO] to get us to ~4TB of space managed which is good but not ""problem solved"" territory.

Let's clarify that it's a really-really worse case which means that your reads touch whole dataset all the time and it's a good show case for side affect of having fixed size files of small size so even if you don't do caching it would create allocation rate proportional to the dataset size as well as increased syscall rate to open/seek/close those files so it's frequent GC/FullGC vs OOM in here (when run without memory cap).

bq. The most common operations in an object pool are the borrow/return. Ben's multiway pool optimizes for this, with particular attention to possible contention from multiple reader threads.

If I understand correctly by borrow you mean allocate object in the pool and give it way to the caller, this is not how I would like it to behave instead we can allocate object without assigning to the queue when it runs short (as all of the objects are short lived) and then decide if we want it back when caller it done with it, disadvantages of allocating to the pool I have already described in my previous comments.

Bottom line for me being, I'm tired of arguing about this so I will let mighty people to decide what they see fit as in all other cases (e.g. fadvic'ing whole file on reads, preheating page cache etc.). 

","07/Jul/13 07:33;xedin;[~ben.manes] I briefly looked through the code and I think there is an important component missing - we need to weight size of the cache based on internal structure of each object allocated into it (borrowed) as the biggest part would be in memory buffer that each instance holds, so simple number of entries wouldn't do...

I'm worried that getResourceHandler has to go through few queues and allocation, I wanted to avoid polling + atomic CAS on read path, which is most critical, as much as possible because it adds additional undesired latency especially to high cardinality requests.

Also I don't think using just weak references to remove unused queues is a good idea (if I interpreted comment at the top correctly), we need something more agressive, because ParNew+CMS only processes those in FullGC phrase even when objects effectively die in young gen (sun/open jdk) and G1 doesn't even have any guarantees on when they are going to be processed, not mentioning that it requires double pass.","07/Jul/13 08:38;ben.manes;Weights are trivial to add and I wanted to avoid adding non-critical features without more details. In your patch, it appears assumed that every queue as a single entry with the same size buffer and privately Jonathan's description of the problem stated 128KB per CRAR. If the weight is constant than they are merely a convenience mapping as it really is the number of entries.

Uncontended CAS is cheap, short lived allocations are trivial, and Doug Lea describes LTQ as lower overhead than CLQ (especially under load).

The use of weak references was an easy way to avoid race conditions when flushing out the primary structure. It could be replaced with lazy clean-up passes, which is what I originally started with. At this point it seemed unwise to complicate things without more information so I simplified it. The number of queues is probably going to be quite small, on the order of dozens, so the reference cost in this case is quite small.

You're trying to compare approaches, which is valid but better oriented towards discussing with Jonathan. The challenge presented to me is as described in the class's JavaDoc: an multiway object pool bounded by the total number of entries. I took a more general approach due to not knowing the trade-offs one could make with context to Cassandra's behavior.","07/Jul/13 09:03;xedin;I understand that you didn't account for any specific nuances, I just wanted to point those things out for subscribers of that ticket...

bq. In your patch, it appears assumed that every queue as a single entry with the same size buffer and privately Jonathan's description of the problem stated 128KB per CRAR. If the weight is constant than they are merely a convenience mapping as it really is the number of entries.

I'm not sure if Jonathan wants to use it per CF or globally but different column families are going to have different buffer settings especially with CRAR, my patch accounts each queue buffer as being more or less a constant size but doesn't assume very key introduces the same overhead.

bq. The number of queues is probably going to be quite small, on the order of dozens, so the reference cost in this case is quite small.

It's actually going to be pretty high especially with LCS (default size of 5 MB), it creates 204 files for 1GB, so it's order of thousands without taking into account compaction process.

bq. Uncontended CAS is cheap

Well if 8-32 threads (default number of threads) are going to request the same row from the same number of files, there could be contention especially it we throw LRU in the mix. But I'm more concerned about 1 ms polling timeout in there tho... This is why I'm concerned with tries to make cache first class citizen instead of lucky shot especially when we don't really need that.
","07/Jul/13 10:28;ben.manes;I solved the LRU problem years ago, which gave you CLHM and Guava's Cache. It scales very well without degrading due to LRU management under higher thread count, limited primarily by the hash table usage. Previous approaches didn't scale to 4-8 threads, but 32+ is limited by the chosen hash table design.

In neither approaches will there be significant contention or overhead. The difference is about the level of granularity to bound the resources by and how to evict them.

You seem to be focusing on tuning parameters, minute details, etc. for a class written in a few evenings as a favor, knowing that those things are trivial to change. There's not much of a point debating it with me as I don't care and have no stake or interest in what is decided. Especially when you're comparing it against a simplistic usage relying on another class I wrote much of, Guava's. In the end something I wrote will be used to solve this bug. ;)

","07/Jul/13 19:17;xedin;You got me wrong, I'm not trying to debate anything with you and minute details are proven to be the hardest sometimes... I'm just trying so say, to Jonathan most of all, if we go with borrowing approach and put cache in front of each row read we would suffer additional latency on every read (even if it's 1-3 ms, that adds to every file read on each request) of the row where people already report order of magnitude worse latencies on >= p99, am I right [~tjake]?

P.S. am I sorry I forgot to bow before I darred to speak up to such master who solved all your problems and who's modesty doesn't know it's limits...","07/Jul/13 19:36;jbellis;I'm confused.  We already have a borrowing approach, what part of the proposal do you see adding 1-3ms?","07/Jul/13 19:55;xedin;We don't borrow if there is no items in the cache, we just allocate new instance and then decide if we want to add it on recycle. 1-3 ms I'm talking is in Ben's multiway pool in getResourceHandler has to poll few times (once of those from blocking queue with 1 ms timeout) + atomic CAS.","09/Jul/13 08:28;xedin;I think this discussion already outgrown proportions of the original problem, I want to suggest we try currently implemented queueing approach with expiry and maximum memory size cap as fix for problem related to memory usage (with LCS in particular) without introducing any additional complexity on getSegment (e.g. borrowing) at least for next 1.2 release. And in the meantime I'm open for discussion of other ways of handling caching which could be borrowing and gradual control over each instance in the queue, like proposed multiway pool, without adding any significant overhead on such critical path as reading file segments.  ","09/Jul/13 16:44;jbellis;As mentioned above (but apparently I didn't actually touch fixver, oops) I'd rather just tweak default sstable size for 1.2.x and do a deeper fix in 2.0.  1.2 is 6 months old at this point and it seems like we have a pretty good workaround available without making deep code changes.  WDYT?","09/Jul/13 19:04;xedin;I'm fine with that, I can work on a patch that would use multiway pool, [~dmeyer] Will you be able to test and compare expiry + mem limit to multiway borrowing, once I'm done with second patch, to check performance/latenties of reads and memory usage?","10/Jul/13 00:38;xedin;I have started working on integrating multiway pool and it looks like we have two problems:

#1. As each SegmentedFile has to return unique instance using ""createReader(String)"" LoadingCache won't do for us, as we need get(K, Callable) per SSTableReader.
#2. as MultiWay returns a handle I changed RAR to have a setHandle method instead of passing SegmentedFile into constructor, which seems a bit hacky to me as we need to be careful in maintaining that relationship...

I did some performance testing (with attached patch) where MultiwayPool allocated per instance because we can't specify loader in borrow(...) yet, which should be a best case for it, not in terms of memory usage but contention. I loaded 5,000,000 keys with following stress command (./tools/bin/cassandra-stress -n 5000000 -S 512 -C 20 -Z LeveledCompactionStrategy) for initial data and then I made it run in a loop and was doing reads in parallel.

With writes:

Average read performance for MultiwayPool: median 6.2, 95th 11.4, 99.9th 78.8 
Average read performance for FileCacheService: median: 5.3, 95th 9.6, 99.9th 73.1

No writes, no compaction:

Average read performance for MultiwayPool: median 2.3, 95th 3.2, 99.9th 21.3 
Average read performance for FileCacheService: median: 1.7, 95th 2.9, 99.9th 19.2

I tried doing range_slice but due to timeouts I couldn't really complete test on any of the implementations, median latenties on average different by 3-4 ms.

Edit: I forgot to mention that I hardcoded maxSize per MultiwayPool instance which was fine for that test, but we really need a way to weight items if we are going to use it globally.
","10/Jul/13 01:29;ben.manes;I think part of the problem is that idle caching is not overly efficient in this version. That can be improved upon, but maximum size might be better to verify as a baseline with first. 

Weights are supported as of the 7th.","10/Jul/13 01:51;xedin;Good to know that weight is supported now, i must have overlooked it... Anyhow since we can't make pool global yet maxSize was good baseline, as you mentioned. ","12/Jul/13 21:50;jbellis;bq. As each SegmentedFile has to return unique instance using ""createReader(String)"" LoadingCache won't do for us, as we need get(K, Callable) per SSTableReader.

Ben has added a borrow(K, Callable) method.

bq. as MultiWay returns a handle I changed RAR to have a setHandle method instead of passing SegmentedFile into constructor, which seems a bit hacky to me as we need to be careful in maintaining that relationship

Can you elaborate?","13/Jul/13 04:16;ben.manes;I rewrote the time-to-idle policy, so it should be faster when enabled. 

Details (if interested)
------------------------
For prototyping purposes, I previously used a secondary Guava Cache to track idle resources. Unlike a cache's time-to-idle, which is reset when an entry is read, an object pool's concept of idle time is when a resource resides unused and ready to be borrowed. The use of a secondary Guava Cache meant that the resource had to be added and removed frequently, resulting in locking on the hashtable segments and incurring other maintenance overhead.

In Guava's cache we observed that expiration policies mirrored maximum size policies, but time based. Thus time-to-live is a FIFO queue and time-to-idle is an LRU queue. That let us leverage the amortization technique in CLHM to be used for expiration with O(1) reorder costs.

The new implementation strips off the unnecessary work by maintaining a time ordered queue that only supports adds and removals. For our definition of idle there is no need to reorder so it is effectively a FIFO. A tryLock guards the policy operations, draining a queue of pending operations if acquired. I decided to allow this to be proactively drained whenever possible, though if we see a need then we can buffer the operations for longer like the caches do.","13/Jul/13 05:27;xedin;bq. Can you elaborate?

Please that a look to v2 patch PoolingSegmentFile.getSegment() method, I have added comment about that.

v2 patch makes cache global and adds per borrow loading of missing items. The problem I encountered is that system is unable to start because of the following error:

I'm not sure if I'm doing something wrong or it's a bug in multiway pool

{noformat}
 INFO 22:19:07,254 Opening /var/lib/cassandra/data/system/local/system-local-ja-21 (520 bytes)
ERROR 22:19:07,298 Exception encountered during startup
com.google.common.util.concurrent.UncheckedExecutionException: java.lang.NullPointerException
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2258)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3990)
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4793)
	at com.github.benmanes.multiway.TransferPool.newResourceHandle(TransferPool.java:206)
	at com.github.benmanes.multiway.TransferPool.tryToGetResourceHandle(TransferPool.java:186)
	at com.github.benmanes.multiway.TransferPool.getResourceHandle(TransferPool.java:167)
	at com.github.benmanes.multiway.TransferPool.borrow(TransferPool.java:152)
	at com.github.benmanes.multiway.TransferPool.borrow(TransferPool.java:143)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:64)
	at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1040)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.createFileDataInput(SSTableNamesIterator.java:96)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:109)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:62)
	at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:87)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
	at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:124)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1458)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1284)
	at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:332)
	at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:55)
	at org.apache.cassandra.cql3.statements.SelectStatement.readLocally(SelectStatement.java:227)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:245)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:56)
	at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:154)
	at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:456)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:237)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
Caused by: java.lang.NullPointerException
	at com.github.benmanes.multiway.TransferPool$2.call(TransferPool.java:209)
	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4796)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3589)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2374)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2337)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2252)
	... 28 more
com.google.common.util.concurrent.UncheckedExecutionException: java.lang.NullPointerException
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2258)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3990)
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4793)
	at com.github.benmanes.multiway.TransferPool.newResourceHandle(TransferPool.java:206)
	at com.github.benmanes.multiway.TransferPool.tryToGetResourceHandle(TransferPool.java:186)
	at com.github.benmanes.multiway.TransferPool.getResourceHandle(TransferPool.java:167)
	at com.github.benmanes.multiway.TransferPool.borrow(TransferPool.java:152)
	at com.github.benmanes.multiway.TransferPool.borrow(TransferPool.java:143)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:64)
	at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1040)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.createFileDataInput(SSTableNamesIterator.java:96)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:109)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:62)
	at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:87)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
	at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:124)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1458)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1284)
	at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:332)
	at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:55)
	at org.apache.cassandra.cql3.statements.SelectStatement.readLocally(SelectStatement.java:227)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:245)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:56)
	at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:154)
	at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:456)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:237)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
Caused by: java.lang.NullPointerException
	at com.github.benmanes.multiway.TransferPool$2.call(TransferPool.java:209)
	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4796)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3589)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2374)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2337)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2252)
	... 28 more
Exception encountered during startup: java.lang.NullPointerException
{noformat}",13/Jul/13 05:33;xedin;v2 updated with a small thing - added recordStats() to the builder so we have statistics information available on demand. Exception listed above still accours.,"13/Jul/13 06:12;ben.manes;I think I just fixed this issue in my last push. Sorry I didn't check my email earlier, as I found it when writing more test cases. The problem is that I forgot to default the lifecycle to a discarding instance if not used, after I made it an optional setting.","13/Jul/13 06:26;xedin;I have pulled/rebuild your code and now there is another error:

{noformat}
 INFO 23:24:33,367 Opening /var/lib/cassandra/data/system/local/system-local-ja-21 (520 bytes)
ERROR 23:24:33,412 Exception encountered during startup
java.lang.ClassCastException: com.github.benmanes.multiway.ResourceKey$LinkedResourceKey cannot be cast to java.lang.String
	at org.apache.cassandra.io.util.PoolingSegmentedFile$1.weigh(PoolingSegmentedFile.java:35)
	at com.google.common.cache.LocalCache$Segment.setValue(LocalCache.java:2219)
	at com.google.common.cache.LocalCache$Segment.storeLoadedValue(LocalCache.java:3196)
	at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2375)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2337)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2252)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3990)
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4793)
	at com.github.benmanes.multiway.TransferPool.newResourceHandle(TransferPool.java:215)
	at com.github.benmanes.multiway.TransferPool.tryToGetResourceHandle(TransferPool.java:195)
	at com.github.benmanes.multiway.TransferPool.getResourceHandle(TransferPool.java:176)
	at com.github.benmanes.multiway.TransferPool.borrow(TransferPool.java:156)
	at com.github.benmanes.multiway.TransferPool.borrow(TransferPool.java:147)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:65)
	at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1040)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.createFileDataInput(SSTableNamesIterator.java:96)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:109)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:62)
	at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:87)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
	at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:124)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1458)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1284)
	at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:332)
	at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:55)
	at org.apache.cassandra.cql3.statements.SelectStatement.readLocally(SelectStatement.java:227)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:245)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:56)
	at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:154)
	at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:456)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:237)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
{noformat}","13/Jul/13 06:44;ben.manes;okay, fixed. thanks for catching this. The tests no longer use raw keys, which should catch this from occurring again.","13/Jul/13 07:31;xedin;Thanks, it worked that time, I did a quick test on local machine with global cache backed by MultiwayPool.

MultiwayPool average run: median 3.1, 95th 4.7, 99.9th 21.9
FileCacheService (+ LinkedTranferQueue) average run: median 2.4, 95th 3.0, 99.9th 17.1

It's the same setup that I used in my previous test with no writes and no compaction.

I will try to experiment with ArrayBlockingQueue as we know upper bound on concurrency and update my FileCacheService patch with either of them (LTQ vs. ABQ) soon.","13/Jul/13 07:49;ben.manes;LTQ is best when you allow there to be some spin between producers and consumers, as its optimized for message passing scenarios. In your usage you don't allow any delay, so the likelihood of a successful transfer is low. When transfers are common, the overhead is less due to fewer contented CAS operations.

If desired, I can make the pool parameterized to take a supplier of queues to produce so you can parameterize that as well.

The pool will always be slower than the FileCacheService patch, since it does more. The decision is whether the performance degradation is acceptable and if the rational for the pool is to provide a finer grained eviction policy is still desired.","13/Jul/13 08:07;xedin;I understand what is ideal use-case for LTQ is, I wanted to try it out since it was mentioned couple of times to have better results than CLQ under load.

I expected FileCacheService to be faster, I was just trying to check how much latency it actually adds even on such synthetic scenario as stress with no writes nor compaction. I strongly think (and I explained why multiple times) that we can't allow any degradation more than 0.5 ms in percentile on such critical path and why expiring items in bulk is okey for us since, in steady state, eviction would be driven by compactions cleaning all of open file descriptors per compacted sstable, where timed expiry would be very infrequent as read pattern doesn't change frequently in production systems.","14/Jul/13 06:58;ben.manes;In a simple single-threaded benchmark, LTQ is relatively on par within the object pool.

Currently I have a finalizer on the handle as a safety net, both to catch my bugs and usage mistakes. This includes a note on the performance impact, which appears to have add 2.5x overhead. I had intended to replace this with phantom references instead, though now I'm wondering if I should not put any safety net in whatsoever.

# Finalizer
queueType  ns linear runtime
      ABQ 489 =========================
      SAQ 545 ============================
      CLQ 535 ===========================
      LBQ 578 ==============================
      LTQ 555 ============================
      LBD 490 =========================

# No finalizer
queueType  ns linear runtime
      ABQ 176 =========================
      SAQ 159 ======================
      CLQ 166 =======================
      LBQ 210 ==============================
      LTQ 183 ==========================
      LBD 181 =========================
","14/Jul/13 08:48;xedin;Finalizers are never a good idea, because it requires double pass though GC and secondary actual finalization processing is single threaded with global queue, all of which creates additional CPU overhead as well as memory pressure. Also as as far as I remember CMS/G1 both process all types of references on FullGC phase even if refs to underlying objects where lost in young gen they are still promoted to old gen.","14/Jul/13 08:59;ben.manes;yes, I understand that and that was documented. It was correct to add it early on, due to prototyping to help catch my bugs if there were race conditions. When looking at performance then it became appropriate to remove it as tests have baked the code.

The only aspect I'm grudgingly punting on is that I prefer warning developers when they have resource leaks, when possible without overhead, instead of silently letting production environments crash. This can be done with phantom references, but I dislike having libraries spawn its own threads (e.g. MapMaker did) and prefer amortizing it (e.g. CacheBuilder). There's no free hook in my pool to tie into, so I'm not providing that warning given you don't need it atm.","14/Jul/13 09:01;ben.manes;anyways, this is now removed so hopefully your performance tests will see a favorable impact like mine do.","14/Jul/13 09:52;xedin;As default queue size is set to 512MB the tests I did weren't actually using expiry on replacement (as there were no compaction nor dataset was been enough) so I shouldn't have touched the finalization path. I re-run with updated MultiwayPool code and saw the same results as previously stated in the steady state (which is expected), but when I add compaction in the mix (by overwriting data), latencies become shaky with MultiwayPool most notably on 99.9th percentile where random spikes by 10-15 ms are observed, FileCacheService 99.9th stays almost constant.",14/Jul/13 20:11;ben.manes;Can you test without time-to-idle? Most likely there are bursts of expirations and the penalty is now better spread out. ,"15/Jul/13 02:39;ben.manes;Profiled to reduce allocations, cutting out about 10ms in a caliper benchmark. The dominating factor in a profile are reads from the cache.","20/Jul/13 09:00;ben.manes;I replaced the external handle with directly providing the resource and maintaining the association in a threadlocal. This should better match your usage and resolve your concern above.

The primary motivation was to reduce object churn, as a handle was created per borrow. This reduced the hot spot time from an average invocation time of 1001us to 704us, when summing up the worst offenders.

This may remove the random spiked that you observed if they were caused by garbage collection.

98% of the overhead is now due to usage of other collections (Guava's Cache, LTQ, CLQ).","20/Jul/13 10:41;ben.manes;Switching from LTQ to a custom elimination backoff stack appears to have dropped the 98% to 179us. The single threaded benchmark improves by 30ns. A significant gain was also observed when using an EBS instead of an array of CLQs in the time-to-idle policy.

I'm surprised by how much of a gain occurs, so I'll have to experiment further to understand if its factual. LTQ/CLQ are hindered by having to honor FIFO with j.u.c. interfaces, and LIFO elimination is the ideal strategy for an object pool. The more frequently successful exchanges may reduce down to eden-space GC, resulting in major net wins. That, or I'm prematurely believing that its working correctly.","20/Jul/13 20:51;xedin;attaching patch with supports the latest version of multiway pool (and latest trunk) and adds (C)RAR deallocation on object removal. Sorry I didn't run the test sooner, I was quiet busy with other things... So I see that with this version 95-99.9 have degraded comparing to previous by ~3 ms but they don't shake as they used too - it's around ~5 ms now (I'm testing with expireAfterAccess). ","20/Jul/13 21:03;ben.manes;Thanks Pavel.

I'm not sure why it got worse recently, except that you did turn on recordStats() in the last few runs. That can incur significant overhead by maintaining multiple LongAdders. Since you did not turn it on in the FileCache patch, which would provide similar stats, it may be an unfair comparison.

I'll try to wrap up my EBS prototype and push those changes soon. Those aren't on github yet.","20/Jul/13 22:14;xedin;Maybe that's side effect of theadlocals, i am not sure. We need recordStats() to expose some via JMX, FileCache uses separate metrics service as we track explicitly every get/release which are exposed via JMX the same way as the rest of Cassandra, i didn't do that for multiway yet but that would be a final step.

Edit: I have also added onRemoval listener with the latest patch which does syscall to close the file among other things, which could have affected borrow/release latencies somehow.","21/Jul/13 02:31;ben.manes;I was able to reduce the EBS version down to 120us. I probably won't have it on github until tomorrow, though.","21/Jul/13 04:42;xedin;Ok, let me know and I will try to test it tomorrow.",21/Jul/13 04:50;xedin;this is updated patch which actually includes onRemoval callback.,"22/Jul/13 05:28;ben.manes;Since the EBS version is still in progress, the code is shared below. It uses a treiber stack with backoff to an elimination array, mixing in optimizations borrowed from j.u.c.Exchanger. It performs superior to the queues by not to honor FIFO ordering, making cancellation easy to achieve.

While all tests pass, I think that the time-to-idle policy is corrupted as it assumed fifo ordering. I can make it tolerant of running out-of-order. I may try writing an elimination queue (LTQ uses a dual queue design).

https://github.com/ben-manes/multiway-pool/tree/elimination","08/Aug/13 15:17;jbellis;[~xedin], have you had a chance to get back to this?",08/Aug/13 18:34;xedin;I was actually destructed with other things and kind of waiting until code is done so I can test the version of multipool with all of the changes.,"08/Aug/13 18:50;ben.manes;My benchmark had a bug and EBS may only be on par with LTQ performance wise. I need to investigate that again, though.

I shifted focus to fixing the performance bottleneck in Guava's cache. The way we tracked usage history (e.g. LRU) was focused on common usage, but is a bottleneck on synthetic benchmarks. I made the fixes to CLHM (v1.4) and offered them upstream (issue 1487). I'll experiment with using CLHM instead to see if that removes the hotspot.",10/Aug/13 04:02;ben.manes;EBS is 28% faster than LTQ. There might be opportunities to make it slightly faster with some tuning. I could probably add blocking methods if there was interest in experimenting with it for CASSANDRA-4718.,"10/Aug/13 05:01;xedin;Sounds good, let me know when it's available so I can run stress tests again.","29/Aug/13 08:27;ben.manes;Sorry that I haven't had the time to work on this for a while. I've been playing with writing a queue using a combining arena, similar to how the stack has an elimination arena, and how to incorporate thread locals to reduce contention. That made me think about the flat combining technique, so after a little digging I uncovered a conversation I had with Chris Vest. At the time he was starting on an object pool, which he's released as Stormpot (http://chrisvest.github.io/stormpot). The implementation has some excellent ideas that are worth borrowing and mixing in. While it is not multi-way, he might be inclined to add that capability after playing with my prototype.","17/Sep/13 21:58;jbellis;We're now beginning 2.0's ""stabilization"" period, so even if Ben's multiway cache were available tomorrow I'd be uneasy about pushing it out to our ""stable"" branch.

So I've committed Pavel's original queue-based cache with light revisions (primarily adding a conf setting to allow overriding the size).

Longer term I think CASSANDRA-6045 may be a better approach overall.",17/Sep/13 22:19;ben.manes;sounds good to me.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Paxos replay of in progress update is incorrect,CASSANDRA-5985,12667496,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,08/Sep/13 23:07,12/Mar/19 14:13,13/Mar/19 22:29,09/Sep/13 22:11,2.0.1,,,,,,0,LWT,,,,"When we replay {{inProgress}}, we need to refresh it with the newly prepared ballot, or it will be (correctly) rejected.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-09-09 13:20:15.334,,,no_permission,,,,,,,,,,,,347433,,,Mon Sep 09 22:11:34 UTC 2013,,,,,,0|i1nwrr:,347732,2.0.0,,,,,,,slebresne,slebresne,,,,,,,,,,"08/Sep/13 23:47;jbellis;Fix pushed to https://github.com/jbellis/cassandra/tree/5985, along with a patch to clean up the trace messages.  Also added a sleep to the collision-in-replay branch to match the collision-in-prepare.","09/Sep/13 13:20;slebresne;One ""nit"": I'm not sure sleeping in the case where the propose work (and we commit) is really useful, the commit itself probably play a good enough sleep if we're contending (which, for what its worth, is confirmed by my unscientific test: moving the sleep only if the propose fails perform slightly better without making threads retry more often).

But overall, +1 on the fix (a quick test (that I'll commit to dtests) does confirm the current CAS timeout as soon a 2 inserts contend a bit too much).

",09/Sep/13 22:11;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Any exception hangs repair,CASSANDRA-5758,12657585,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,hsn,hsn,13/Jul/13 11:50,12/Mar/19 14:13,13/Mar/19 22:29,26/Jul/13 15:42,2.0 rc1,,,,,,0,,,,,"If there is any exception during repair, it hangs and never complete. This is far away from optimal error handling.

CASSANDRA-5646 and CASSANDRA-5757 and probably much other issues can be used to reproduce this problem for testing purposes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-07-13 21:18:40.808,,,no_permission,,,,,,,,,,,,337806,,,Fri Jul 26 15:42:18 UTC 2013,,,,,,0|i1m9lj:,338128,,,,,,,,,,,,,,,,,,,13/Jul/13 21:18;jbellis;Repair was updated for 2.0b1 in CASSANDRA-5426.  Please provide more details about what you see there since it is no longer similar to the 1.2 design.,"13/Jul/13 22:59;hsn;Actually any time I see exception during repair then repair hangs and you can not run other repair until you restart node.

if you want example for 2.0 then CASSANDRA-5757 hangs repair.","25/Jul/13 23:30;hsn;still not fixed in 2.0b2. 

But i guess i am just wasting time with this for 1 year. Lets scrap this ticket, you will never agree that exception handling during repair is broken and fix that exception instead of exception handling, right?

I am not saying that fixing exception which caused this hang is bad thing.",26/Jul/13 15:42;brandon.williams;I'm pretty sure if nothing else 0cc0d8ded34051bf94e936dc4564b634a68ea864 fixed this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updating primary key only fails,CASSANDRA-5846,12661904,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,brandon.williams,brandon.williams,05/Aug/13 15:27,12/Mar/19 14:13,13/Mar/19 22:29,05/Aug/13 16:18,,,,,,,0,,,,,"""UPDATE test SET PRIMARY KEY WHERE k = 0"" is now invalid syntax, but shouldn't be.  This bisected to CASSANDRA-5125.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-08-05 16:18:57.424,,,no_permission,,,,,,,,,,,,341908,,,Mon Aug 05 16:18:57 UTC 2013,,,,,,0|i1mysf:,342214,,,,,,,,,,,,,,,,,,,05/Aug/13 16:18;iamaleksey;Removed the dtest in question.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't return internal StreamState objects from streaming mbeans,CASSANDRA-5859,12662499,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,nickmbailey,nickmbailey,07/Aug/13 21:37,12/Mar/19 14:13,13/Mar/19 22:29,12/Aug/13 16:15,2.0.0,,,,,,0,streaming,,,,The stream manager mbean returns StreamState objects. We want to avoid returning internal C* objects over jmx. We should switch to a map or something similar that can represent streaming state.,,,,,,,,,,,,,,,,,,,,,,,,08/Aug/13 19:40;yukim;5859.txt;https://issues.apache.org/jira/secure/attachment/12596919/5859.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-08 19:40:14.87,,,no_permission,,,,,,,,,,,,342502,,,Mon Aug 12 16:15:05 UTC 2013,,,,,,0|i1n2fz:,342807,,,,,,,,thobbs,thobbs,,,,,,,,,,"08/Aug/13 19:40;yukim;Initial patch attached. (Also on github: https://github.com/yukim/cassandra/commits/5859)

Since StreamState is complex object, I used JMX's [CompositeData|http://docs.oracle.com/javase/7/docs/api/javax/management/openmbean/CompositeData.html] to convert StreamState to be transferred over JMX.
Patch also adds support for exposing JMX notification support for stream events to StreamManagerMBean, so one can just monitor start/progress/completion of streaming.","09/Aug/13 19:43;thobbs;In {{ProgressInfoCompositeData}}, the direction is being reported as a single byte, 0 or 1, which can't be interpreted without the internal enum.  Just using a string ""in"" or ""out"" or a boolean with name ""outbound"" would be fine.

There are several alignment issues around one-arg-per-line lists in all of the new streaming.management classes.  For example:
{noformat}
            COMPOSITE_TYPE = new CompositeType(ProgressInfo.class.getName(),
                                                      ""ProgressInfo"",
                                                      ITEM_NAMES,
                                                      ITEM_DESCS,
                                                      ITEM_TYPES);
{noformat}

Other than those two minor issues, +1.","12/Aug/13 16:15;yukim;Committed with above fix(string instead of byte/coding style).
Thanks for review!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update scrub and scrubtest for single-pass compaction format,CASSANDRA-5429,12641060,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasobrown,jbellis,jbellis,05/Apr/13 15:41,12/Mar/19 14:13,13/Mar/19 22:29,08/Jul/13 17:05,2.0 beta 1,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Jun/13 23:14;jasobrown;5429-v1.diff;https://issues.apache.org/jira/secure/attachment/12589812/5429-v1.diff,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-29 12:25:50.117,,,no_permission,,,,,,,,,,,,321476,,,Mon Jul 08 17:05:20 UTC 2013,,,,,,0|i1jgtr:,321821,,,,,,,,slebresne,slebresne,,,,,,,,,,"28/May/13 22:38;jbellis;Have you had a chance to start on this, Jason?",29/May/13 12:25;jasobrown;Will try to knock it out this week.,26/Jun/13 16:32;slebresne;[~jasobrown] ping. Is this still on your radar? Cause it does sound like something we need to fix for 2.0.,"26/Jun/13 23:14;jasobrown;Attached patch fixes Scrubber to read/scrub Descriptor versions ic and ja. 

I tested by hand using tables created from both 1.2.6 and 2.0, and was able to successfully scrub both. It'll take a day or two to sort out the test (revive ScrubTest and get some relevant test sstables in place), but the core work that we'll need in 2.0 is in this patch.","27/Jun/13 16:01;slebresne;Nit: We can move the ""Index doublecheck"" log line in the else of the ""if(!sstable.descriptor.version.hasRowSizeAndColumnCount)"" since it's not really meaningful anymore in 2.0.

But otherwise lgtm, +1.

It'll be nice to resurrect the ScrubTest indeed, though it's clearly not a blocker for beta1.",27/Jun/13 16:51;jasobrown;Committed to trunk (with nit addressed). Will work on the tests today.,"08/Jul/13 17:05;slebresne;Don't want to hold back beta1, so created CASSANDRA-5730 as a follow up to add back scrubTest.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HHOM.countPendingHints is a trap for the unwary,CASSANDRA-5746,12657166,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thobbs,jbellis,jbellis,10/Jul/13 22:54,12/Mar/19 14:13,13/Mar/19 22:29,18/Jul/13 20:27,2.0 beta 2,,,Legacy/Tools,,,0,,,,,"countPendingHints can OOM the server fairly easily since it does a per-target seq scan without paging.

More generally, countPendingHints is far too slow to be useful for routine monitoring.",,,,,,,,,,,,,,,,,,,,,,,,17/Jul/13 21:20;thobbs;0001-Remove-HHOM.countPendingHints.patch;https://issues.apache.org/jira/secure/attachment/12592857/0001-Remove-HHOM.countPendingHints.patch,17/Jul/13 21:20;thobbs;0002-Report-created-hint-count-by-endpoint.patch;https://issues.apache.org/jira/secure/attachment/12592858/0002-Report-created-hint-count-by-endpoint.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-07-16 19:00:54.634,,,no_permission,,,,,,,,,,,,337389,,,Thu Jul 18 20:27:57 UTC 2013,,,,,,0|i1m713:,337712,,,,,,,,jbellis,jbellis,,,,,,,,,,"10/Jul/13 22:58;jbellis;If we want it to be fast enough for monitoring use, we need to denormalize the count.  So the question is, do we use a Counters table, a normal int table with manual lock-and-read-before-write, or just one-off it with AtomicInteger and sync to disk occasionally? 

Personally I'd be inclined towards the last; it's okay if under- or over-count (because of periodic CL sync for instance), as long as we reliably distinguish between zero and non-zero hints for a given target.  We could sanity check that with an approach like listEndpointsPendingHints on startup, which would be fairly low-overhead.","16/Jul/13 19:00;thobbs;Doesn't the TTL on hints cells complicate all of those strategies?  I can't think of a cheap way to schedule all of those future decrements to a counter.

Instead, I suppose we could recount on demand if the time since the last count is greater than the smallest TTL we've seen, but without throttling of some sort recounts would still happen frequently under some circumstances.

Alternatively, it seems like a fair amount of work, but perhaps a get_range_counts() implementation with internal auto-paging (like get_count) is a decent option?","16/Jul/13 19:34;jbellis;What if we redefine the problem to be ""how many hints have I generated for node X?"" and get rid of countPendingHints entirely?

ISTM that's a more important indicator of cluster health than how many hints X expects to get when he comes back up.  I can live without that.","16/Jul/13 20:07;thobbs;> What if we redefine the problem to be ""how many hints have I generated for node X?"" and get rid of countPendingHints entirely?

That seems like a reasonable replacement metric for most purposes, but I can see where countPendingHints() might still be useful.  However, in its current state, it seems too dangerous to leave in.  Maybe pull it out in this ticket and open a new ticket for a better implementation if there's interest?

I'm assuming we don't want to persist this one to disk. (Most RRD-style metric systems handle normally monotonically increasing metrics resetting to 0 occasionally, so just counting hints created since the node has been up should be fine.)",16/Jul/13 20:33;jbellis;Sounds good to me.  Let's try to get this into b2 so we don't rip it out of a stable release.,"17/Jul/13 21:20;thobbs;0001 removes countPendingHints().

0002 adds a per-endpoint count through the new metrics system. I made HHOM.hintFor() non-static, as I couldn't see why the singleton couldn't be used.  Let me know if there was some motivation for that.","18/Jul/13 20:27;jbellis;LGTM, committed.  (Tweaked to use LoadingCache.getUnchecked instead of manually throwing RTE ourselves, also in the existing incrPastWindow.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrub does not understand compound primary key created in CQL 3 beta,CASSANDRA-5855,12662401,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thobbs,jblangston@datastax.com,jblangston@datastax.com,07/Aug/13 15:02,12/Mar/19 14:13,13/Mar/19 22:29,08/Aug/13 22:10,1.2.9,2.0.0,,Legacy/Tools,,,0,,,,,"We have a customer who was using the beta version of CQL 3 in DSE 3.0 which includes Cassandra 1.1.9 plus patches backported from later versions.  They've now upgraded to DSE 3.1, which includes Cassandra 1.2.6 plus patches.

When restarting for the first time after running upgradesstables, they noticed the following error in the log:

{noformat}
Thread[SSTableBatchOpen:2,5,main]
java.lang.AssertionError
        at org.apache.cassandra.utils.ByteBufferUtil.readBytes(ByteBufferUtil.java:401)
        at org.apache.cassandra.io.sstable.IndexSummary$IndexSummarySerializer.deserialize(IndexSummary.java:124)
        at org.apache.cassandra.io.sstable.SSTableReader.loadSummary(SSTableReader.java:426)
        at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:360)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:201)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:154)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:241)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

This error was also reported on CASSANDRA-5703.  The comments suggested it was caused by an empty row key, so I had them run scrub on it.  When they did, scrub reported the following warning almost 4 million times:

{noformat}
 WARN [CompactionExecutor:27] 2013-08-02 10:13:13,041 OutputHandler.java (line 52) Row at 530332255 is unreadable; skipping to next
 WARN [CompactionExecutor:27] 2013-08-02 10:13:13,041 OutputHandler.java (line 57) Non-fatal error reading row (stacktrace follows)
java.lang.RuntimeException: Error validating row DecoratedKey(139154446688383793922009760478335751546, 735fc9da503b11e2844b123140ff209f)
 at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:243)
 at org.apache.cassandra.db.compaction.PrecompactedRow.merge(PrecompactedRow.java:114)
 at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:98)
 at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:160)
 at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:166)
 at org.apache.cassandra.db.compaction.Scrubber.scrub(Scrubber.java:173)
 at org.apache.cassandra.db.compaction.CompactionManager.scrubOne(CompactionManager.java:529)
 at org.apache.cassandra.db.compaction.CompactionManager.doScrub(CompactionManager.java:518)
 at org.apache.cassandra.db.compaction.CompactionManager.access$400(CompactionManager.java:73)
 at org.apache.cassandra.db.compaction.CompactionManager$3.perform(CompactionManager.java:283)
 at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:253)
 at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
 at java.util.concurrent.FutureTask.run(FutureTask.java:138)
 at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
 at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.cassandra.db.marshal.MarshalException: String didn't validate.
 at org.apache.cassandra.db.marshal.UTF8Type.validate(UTF8Type.java:66)
 at org.apache.cassandra.db.Column.validateFields(Column.java:292)
 at org.apache.cassandra.db.ColumnFamily.validateColumnFields(ColumnFamily.java:382)
 at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:239)
 ... 15 more
{noformat}

The customer did some testing and they've determined that the issue only exists when taking a Cassandra 1.1 sstable with compound primary keys and running scrub on it in either Cassandra 1.1 or 1.2.

It appears that the scrub does not understand the 1.1 compound primary key so is invalidating the row.

The customer provided a cassandra data directory that's from DSE 3.0. Running ""nodetool scrub"" in either DSE 3.0 or 3.1 generates all sorts of exceptions.

If you fire up cassandra before running the scrub, running this query:

{noformat}
select count(*) from b_projectsubscription_project;
{noformat}

will return 88.

After the scrub, it returns 0.

When I discussed this with Alexey Yeschenko, he said that if he recalls correctly, the beta didn't have row markers, and did not use a composite comparator for simple primary keys. Whereas CQL3 final used CompositeType(UTF8Type), the beta would just use UTF8Type. I asked him if this could cause these errors, and he said he didn't think so because after upgrading your schema created under the beta would still have UTF8Type, so Cassandra would know how to handle it correctly. 

Based on the customer's investigation, it sounds like this may be true of the normal read/write path but not for scrub. However, given the error that occurred at startup, this may be causing some issues aside from just scrub.  My theory is that scrub is looking at what's just a UTF8 string and trying to interpret the first few bytes as the sentinels for a composite type.  When it then tries to interpret the remaining bytes, if only part of a multi-byte UTF8 character was in the remaining byte array, it might cause the UTF8 validation errors above.",,,,,,,,,,,,,,,,,,,,,,,,08/Aug/13 19:39;thobbs;0001-Correctly-validate-sparse-composite-columns.patch;https://issues.apache.org/jira/secure/attachment/12596918/0001-Correctly-validate-sparse-composite-columns.patch,08/Aug/13 21:16;jbellis;5855-followup.txt;https://issues.apache.org/jira/secure/attachment/12596943/5855-followup.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-08-07 17:02:19.038,,,no_permission,,,,,,,,,,,,342405,,,Thu Aug 08 22:10:55 UTC 2013,,,,,,0|i1n1uf:,342710,,,,,,,,jbellis,jbellis,,,,,,,,,,"07/Aug/13 17:02;jbellis;Can you get us instructions to reproduce along these lines?

# Start C* 1.1
# CREATE TABLE ...
# INSERT ...
# Start C* 1.2
# Scrub and watch the error","07/Aug/13 21:30;jblangston@datastax.com;The error will occur whether scrub is run under 1.1 or 1.2.

1) Start Cassandra 1.1.9

2) Run the following in cqlsh -3

CREATE TABLE b_projectsubscription_project (
  project uuid,
  uuid uuid,
  created_on bigint,
  deleted boolean,
  hsn_deleted boolean,
  send_notifications boolean,
  user uuid,
  PRIMARY KEY (project, uuid)
);

insert into b_projectsubscription_project (project,uuid,created_on,deleted,hsn_deleted,send_notifications,user) values (d7ea9931-3e70-4ec3-b7a8-a7a7535473f2,00388f1e-5b59-41d8-92c7-16b6cedf25a5,1375831887947529,'False','False','True',d3151d4d-873c-4c8e-9445-dabd7f0660ef);

4) run nodetool flush
5) optional: upgrade to Cassandra 1.2.6
6) run nodetool scrub
7) check system.log for ""RuntimeException: Error validating row""
8) in cqlsh -3 run 'select * from b_projectsubscription_project;' and notice the row is gone.",07/Aug/13 21:52;jblangston@datastax.com;One more observation: the bug may only happen if the pk fields are uuid. I tried to reproduce it before with pk fields that are text and wasn't able to.,"07/Aug/13 23:07;thobbs;At least with a 1.1.9 scrub, it's trying to look up the column value validator with the full composite column name, but CfMetadata is expecting to only see the last component (e.g. ""created_on"").  So, it ends up using the default validator, which is UTF8Type, to try to validate all of the column values.","07/Aug/13 23:34;xcbsmith;Tyler, that makes sense... does that mean if you kept the composite key fields all the same type, it would work properly?

It's kind of weird that you'd still see the problem with the table after it had been upgraded to 1.2. Really, after an upgrade, I would expect scrubs of a table to behave *exactly the same* as if the table had been originally created in 1.2. Is the upgrade process perhaps less complete than I imagined?",08/Aug/13 02:43;jbellis;Usually upgrades only fix bugs that we knew we had. :),"08/Aug/13 06:53;xcbsmith;Hehe.

I figured the idea would be to make it the _same_ as the same column family created with the new tool. So for example, if the problem was the missing markers, while a bug might exist with the 1.1.9 scrub tool with 1.1.9 column family, it'd be a different story after upgrade. Since the bug shows up you upgrade the column family, but not if you always created it in 1.2, that would imply the markers still weren't there after the upgrade, which you'd think would be noticed by someone just checking to make sure upgrade actually upgraded, even if nobody realized this tickled an actual bug in scrub.","08/Aug/13 12:36;jbellis;The problem is that it can't read the existing data file.  If it could, it would indeed write out a 1.2-format file.","08/Aug/13 16:20;thobbs;bq. Tyler, that makes sense... does that mean if you kept the composite key fields all the same type, it would work properly?

If all of the column values were UTF8Type, it would work properly, but that's definitely not a feasible workaround.  I'm looking into a proper fix, which would allow scrub to lookup the correct column validators.","08/Aug/13 19:39;thobbs;Patch 0001 should apply to 1.1 or 1.2.  It's not the most general technique, so I welcome any suggestions, but it does fix this particular issue.  If a sparse composite schema is being used, only the last composite component is used to try to look up the column validator.

I'll also note that this code path is currently only used by scrub, so it should be safe to test.","08/Aug/13 20:20;jbellis;It looks to me like we actually need to handle 3 cases:

# classic one-cell-per-column, no composites; the existing code
# CQL-style one-cell-per-column; the code you added
# COMPACT style, multiple columns in one cell; need to split them up and check each value

Nit: prefer assigning variables only once, e.g. we could write the validationName code [without addressing case 3 above] as

{code}
        ByteBuffer validationName;
        if (cfdef.isComposite && !cfdef.isCompact)
        {
            AbstractCompositeType comparator = (AbstractCompositeType) metadata.comparator;
            List<AbstractCompositeType.CompositeComponent> components = comparator.deconstruct(name);
            validationName = components.get(components.size() - 1).value;
        }
        else
        {
            validationName = name;
        }
{code}
","08/Aug/13 20:21;jbellis;Actually, COMPACT case is handled by validateName so we're good.  I'll fix the nit and commit.",08/Aug/13 21:16;jbellis;Attached followup patch to make it collection-aware.,08/Aug/13 21:31;thobbs;+1 on 5585-followup.txt,08/Aug/13 22:10;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update CqlRecordWriter to conform to RecordWriter API,CASSANDRA-5622,12651522,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,alexliu68,jbellis,jbellis,06/Jun/13 22:09,12/Mar/19 14:13,13/Mar/19 22:29,17/Jun/13 22:43,1.2.6,,,,,,0,,,,,"{{RecordWriter<K, V>}} is supposed to write values V that can be uniquely identified by keys K.

Currently CqlRW requires the user to give it all the bind variables for a complete statement in V, and effectively ignores K.
",,,,,,,,,,,,,,,,,,,,,,,,07/Jun/13 21:08;alexliu68;5622-1-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12586799/5622-1-1.2-branch.txt,07/Jun/13 20:45;alexliu68;5622-1-trunk.txt;https://issues.apache.org/jira/secure/attachment/12586794/5622-1-trunk.txt,17/Jun/13 17:56;alexliu68;5622-2-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12588169/5622-2-1.2-branch.txt,17/Jun/13 18:02;alexliu68;5622-2-trunk-branch.txt;https://issues.apache.org/jira/secure/attachment/12588172/5622-2-trunk-branch.txt,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-06-07 00:13:12.948,,,no_permission,,,,,,,,,,,,331848,,,Mon Jun 17 22:43:29 UTC 2013,,,,,,0|i1l8yn:,332177,,,,,,,,,,,,,,,,,,,"06/Jun/13 22:15;jbellis;Two solutions suggest themselves:
# Embrace ignoring of K and just pass new Object()
# Treat K as designed and accept responsibility for generating appropriate bind variables

I'm not a huge fan of #1 since it forces the user to do more work to generate bind variables for the PK values.  I'd rather automate that.

Let me give an example of what I mean in #2.  Suppose we have word counts as follows:
{code}
CREATE TABLE word_counts (
  file_name text,
  counted_at timestamp,
  word text,
  count int,
  PRIMARY KEY ((file_name, counted_at), word)
);
{code}

Then the user would configure CQL such as ""UPDATE word_counts SET count = ?"".  CqlRW would inspect the table definition and realize it needs to add "" WHERE file_name = ? AND counted_at = ? AND word = ?"".

Then when it gets a Map<String, BB> as K, it combines those with the List<BB> variables.  It knows the ordering of the pk columns since it generated it itself.

One downside: this doesn't lend itself to INSERT statements, since that does not separate SET/WHERE the way UPDATE does.  This is a limitation I can live with.","07/Jun/13 00:13;alexliu68;Agreed, #2 is more concise and easy to use.",07/Jun/13 20:45;alexliu68;patch for trunk is attached.,07/Jun/13 21:08;alexliu68;Patch for 1.2 branch is attached,12/Jun/13 22:19;jbellis;Is it worth supporting INSERT at all since it's semantically identical to UPDATE?  I think I'd rather just support UPDATE only and avoid the confusion of two different ways of working.,"13/Jun/13 21:23;alexliu68;I will disable INSERT query, only allow UPDATE or DELETE statement.","17/Jun/13 14:09;jbellis;How is that coming, Alex?  Would like to ship this in 1.2.6.","17/Jun/13 15:24;alexliu68;I was busy with other project, I will get it done today.","17/Jun/13 18:04;alexliu68;[~jbellis] I attach the version 2 patches which throws 
{code}
   IOException(""INSERT statement is depreciated and not supported, please use UPDATE/DELETE statement.""); 
{code}
if the INSERT type query is used for writter.",17/Jun/13 20:44;jbellis;Pushed a bunch of changes to https://github.com/jbellis/cassandra/commits/5622.,"17/Jun/13 21:24;alexliu68;+1 for the changes, [~jbellis] I can create the final patch.","17/Jun/13 22:43;jbellis;Not necessary, I merge --squashed and committed.  Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Silently failing messages in case of schema not fully propagated,CASSANDRA-5725,12656254,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sureshsajja,sbtourist,sbtourist,04/Jul/13 13:28,12/Mar/19 14:13,13/Mar/19 22:29,09/Oct/13 12:36,1.2.11,2.0.2,,,,,0,,,,,"When a new keyspace and/or column family is created on a multi nodes cluster (at least three), and then a mutation is executed on such new column family, the operations sometimes silently fails by timing out.

I tracked this down to the schema not being fully propagated to all nodes. Here's what happens:
1) Node 1 receives the create keyspace/column family request.
2) The same node receives a mutation request at CL.QUORUM and sends to other nodes too.
3) Upon receiving the mutation request, other nodes try to deserialize it and fail in doing so if the schema is not fully propagated, i.e. because they don't find the mutated column family.
4) The connection between node 1 and the failed node is dropped, and the request on the former hangs until timing out.

Here is the underlying exception, I had to tweak several log levels to get it: 
{noformat}
INFO 13:11:39,441 IOException reading from socket; closing
org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=a31c7604-0e40-393b-82d7-ba3d910ad50a
	at org.apache.cassandra.db.ColumnFamilySerializer.deserializeCfId(ColumnFamilySerializer.java:184)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:94)
	at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:397)
	at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:407)
	at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:367)
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:94)
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:207)
	at org.apache.cassandra.net.IncomingTcpConnection.handleModernVersion(IncomingTcpConnection.java:139)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:82)
{noformat}

Finally, there's probably a correlated failure happening during repairs of newly created/mutated column family, causing the repair process to hang forever as follows:
{noformat}
""AntiEntropySessions:1"" daemon prio=5 tid=7fe981148000 nid=0x11abea000 in Object.wait() [11abe9000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <7c6200840> (a org.apache.cassandra.utils.SimpleCondition)
	at java.lang.Object.wait(Object.java:485)
	at org.apache.cassandra.utils.SimpleCondition.await(SimpleCondition.java:34)
	- locked <7c6200840> (a org.apache.cassandra.utils.SimpleCondition)
	at org.apache.cassandra.service.AntiEntropyService$RepairSession.runMayThrow(AntiEntropyService.java:695)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:680)

""http-8983-1"" daemon prio=5 tid=7fe97d24d000 nid=0x11a5c8000 in Object.wait() [11a5c6000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <7c620db58> (a org.apache.cassandra.utils.SimpleCondition)
	at java.lang.Object.wait(Object.java:485)
	at org.apache.cassandra.utils.SimpleCondition.await(SimpleCondition.java:34)
	- locked <7c620db58> (a org.apache.cassandra.utils.SimpleCondition)
	at org.apache.cassandra.service.StorageService$4.runMayThrow(StorageService.java:2442)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at org.apache.cassandra.service.StorageService.forceTableRepairRange(StorageService.java:2409)
	at org.apache.cassandra.service.StorageService.forceTableRepair(StorageService.java:2387)
	at com.datastax.bdp.cassandra.index.solr.SolrCoreResourceManager.repairResources(SolrCoreResourceManager.java:693)
	at com.datastax.bdp.cassandra.index.solr.SolrCoreResourceManager.createCore(SolrCoreResourceManager.java:255)
	at com.datastax.bdp.cassandra.index.solr.CassandraCoreAdminHandler.handleCreateAction(CassandraCoreAdminHandler.java:121)
	at org.apache.solr.handler.admin.CoreAdminHandler.handleRequestBody(CoreAdminHandler.java:144)
	at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135)
	at org.apache.solr.servlet.SolrDispatchFilter.handleAdminRequest(SolrDispatchFilter.java:615)
	at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:206)
{noformat}

I wasn't able to track any exception as I can't reproduce it reliably enough, but I believe it's correlated to schema propagation as based on log messages the merkle tree request on node 1 happens concurrently to schema installation on other nodes.",,,,,,,,,,,,,,,,,,,,,,,,07/Jul/13 09:47;sbtourist;5725-0001.patch;https://issues.apache.org/jira/secure/attachment/12591120/5725-0001.patch,09/Oct/13 05:35;sureshsajja;5725_V2.patch;https://issues.apache.org/jira/secure/attachment/12607512/5725_V2.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-07-04 16:13:11.231,,,no_permission,,,,,,,,,,,,336477,,,Wed Oct 09 12:36:50 UTC 2013,,,,,,0|i1m1f3:,336801,,,,,,,,sbtourist,sbtourist,,,,,,,,,,04/Jul/13 16:13;jbellis;This is working as designed.  What do you think should happen instead?,"04/Jul/13 16:33;sbtourist;Well, in an ideal world, given C* has the notion of schema, mutations should be validated with the schema of the coordinator node and associated to such schema version, which should be unique and monotonic (we are the former, not the latter): this way, replica nodes could understand if they're missing a schema update and request it (which would solve this bug), as well as recognize if a partition is ongoing and react accordingly.
By the way, this probably translates in using vector clocks for schema updates, and I understand C* has not been designed this way, so let's forget about the ideal world.

A more pragmatic solution may be to implement a consistency level for schema updates too: right now we only wait for the schema to be applied on the local node, while supporting all consistency levels would allow subsequent updates to succeed under the same CL specification: i.e., applying a schema update at CL.QUORUM would allow subsequent updates at the same CL to succeed too.

Finally, a trivial one may just be to make the schema problem explicit with a specific exception.

Certainly, in my opinion, masking a schema problem with a timeout exception is pretty much confusing, and may lead to several hours spent in debugging/testing or (if the user isn't that smart to do that) increasing the timeouts, which is a bad solution to a wrong problem.

Unless I'm missing something in the current design/implementation, which may well be :)","04/Jul/13 16:56;jbellis;bq. Finally, a trivial one may just be to make the schema problem explicit with a specific exception.

This is not trivial since replicas only ack writes on success.

Here's how it's supposed to work: you perform your schema change, then you check for schema agreement before starting to write to the new table.","04/Jul/13 17:05;sbtourist;bq. Here's how it's supposed to work: you perform your schema change, then you check for schema agreement before starting to write to the new table.

Sure, you can do that, but doesn't look like a great solution to me :)

By the way, if any change to fix this is too big at the moment, or really not worth, feel free to close this as won't fix, we'll live with this.","04/Jul/13 17:40;jbellis;IMO the fix here is to special case UnknownColumnFamilyException so that it gets logged at INFO or WARN instead of being swallowed by the default IOException handler, which it currently subclasses.","05/Jul/13 09:31;slebresne;bq. the fix here is to special case UnknownColumnFamilyException so that it gets logged at INFO or WARN instead of being swallowed by the default IOException handler

Agreed, at least it's by far the simplest fix and it's probably good enough in practice.

I don't think getting more fancy is worth the complexity that it would add. ""you perform your schema change, then you check for schema agreement before starting to write to the new table"" is not that hard a rule to follow, and a good client driver will do that for you under the hood anyway :) ","07/Jul/13 09:46;sbtourist;Attaching patch.

I think it's better to catch a generic Throwable in IncomingTcpConnection, as special casing to UnknownColumnFamilyException means we may find ourselves in the same situation if some other unexpected exception is thrown.","07/Jul/13 19:46;jbellis;This doesn't work on two levels:

# UCFE will still be caught and logged at debug by the IOE clause
# There's no need to add a logger.warn for ""everything else"" because fatal exceptions are logged by the default uncaught exception handler (that is set up in CassandraDaemon)","07/Jul/13 20:15;sbtourist;bq. UCFE will still be caught and logged at debug by the IOE clause

Oh, UCFE actually being an IOE is a bit unexpected. By the way, silly ranting, my fault I didn't check for it.

bq. There's no need to add a logger.warn for ""everything else"" because fatal exceptions are logged by the default uncaught exception handler

Got it, but my point still stands: by special casing for UCFE, we possibly miss other IOE-derived exceptions and make the IncomingTcpConnection catch block pretty ugly (and wrong actually, as an implementation detail about an exception thrown by an opaque message leaks through).

At this point, I'd rather log IOEs at info level: what do you think? Or are there too many IOEs during normal C* operations, which would mess up the logs?","07/Jul/13 20:47;jbellis;bq. are there too many IOEs during normal C* operations, which would mess up the logs?

Exactly (every time you bounce a node, for instance); that's why we special cased it.",07/Jul/13 21:10;sbtourist;Then I guess there's only one thing left ... I'll update the patch.,"02/Aug/13 03:26;jbellis;Still working on this, [~sbtourist]?","02/Aug/13 18:05;sbtourist;I'm sorry I didn't have the time to get back to this: if anyone else wants to pick it up, feel free, otherwise I'll try to update the PR in the next few days.","09/Oct/13 05:35;sureshsajja;Based on above comments, UnknownColumnFamilyException is handled separately and logged as WARN.

Patch is attached here for 1.2 branch","09/Oct/13 07:53;sbtourist;Apologize for not getting back to this one myself.
Patch looks good to me.","09/Oct/13 12:36;jbellis;Committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support thrift tables in Pig CqlStorage,CASSANDRA-5847,12661940,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,alexliu68,alexliu68,alexliu68,05/Aug/13 19:03,12/Mar/19 14:13,13/Mar/19 22:29,11/Sep/13 15:20,1.2.10,2.0.1,,,,,0,,,,,This is fix for Pig side of CASSANDRA-5752,,,,,,,,,,,,,,,,,,,,,,,,10/Sep/13 21:13;alexliu68;001_5847_patch.txt;https://issues.apache.org/jira/secure/attachment/12602422/001_5847_patch.txt,10/Sep/13 21:13;alexliu68;002_5847_patch.txt;https://issues.apache.org/jira/secure/attachment/12602423/002_5847_patch.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-09-11 15:20:00.097,,,no_permission,,,,,,,,,,,,341944,,,Wed Sep 11 15:20:00 UTC 2013,,,,,,0|i1mz0f:,342250,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"05/Aug/13 19:07;alexliu68;001 patch fixes counter issue.
002 patch to support thrift tables in CqlStorage",11/Sep/13 15:20;brandon.williams;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair exception when getPositionsForRanges returns empty iterator,CASSANDRA-5407,12639981,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,31/Mar/13 13:44,12/Mar/19 14:12,13/Mar/19 22:29,03/Apr/13 20:23,,,,,,,0,,,,,"CASSANDRA-5250 broke repair, this re-adds the code from CASSANDRA-5249",,,,,,,,,,,,,,,,,,,,,,,,02/Apr/13 09:30;krummas;0001-CASSANDRA-5407-v2.patch;https://issues.apache.org/jira/secure/attachment/12576528/0001-CASSANDRA-5407-v2.patch,31/Mar/13 13:44;krummas;0001-Fix-repair-bug-where-getPositionsForRanges-returns-e.patch;https://issues.apache.org/jira/secure/attachment/12576283/0001-Fix-repair-bug-where-getPositionsForRanges-returns-e.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-04-01 17:08:52.161,,,no_permission,,,,,,,,,,,,320449,,,Wed Apr 03 20:23:47 UTC 2013,,,,,,0|i1jah3:,320790,,,,,,,,jbellis,jbellis,,,,,,,,,,01/Apr/13 17:08;jbellis;What is causing the breakage?  Is it possible to add a test that exposes the problem?,"01/Apr/13 17:28;krummas;my fix in CASSANDRA-5250 just fixed it for LCS (tests if intersecting sstables is empty in LeveledScanner), and this re-broke it for STCS

i'll try to write a unit test for this",02/Apr/13 09:30;krummas;adds a unit test that would have found the bug,"03/Apr/13 20:23;jbellis;LGTM, committed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamOut doesn't correctly handle wrapped ranges,CASSANDRA-5948,12665973,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sbtourist,sbtourist,sbtourist,28/Aug/13 18:19,12/Mar/19 14:12,13/Mar/19 22:29,29/Aug/13 15:21,1.2.10,2.0.1,,,,,0,,,,,"StreamOut doesn't normalize ranges, causing AbstractViewSSTableFinder to miss sstables when the requested range is wrapped, and hence breaking node bootstrapping/unbootstrapping on such ranges.",,,,,,,,,,,,,,,,,,,,,,,,28/Aug/13 18:21;sbtourist;5948-0001.patch;https://issues.apache.org/jira/secure/attachment/12600426/5948-0001.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-28 19:28:16.779,,,no_permission,,,,,,,,,,,,345912,,,Wed Sep 04 22:54:39 UTC 2013,,,,,,0|i1nnfb:,346213,1.2.8,,,,,,,yukim,yukim,,,1.2.6,,,,,,,28/Aug/13 18:22;sbtourist;Attached patch with StreamOut fix and a related test case.,"28/Aug/13 19:28;yukim;+1.
This is regression caused by CASSANDRA-5569.","29/Aug/13 15:21;yukim;Committed, thanks!","04/Sep/13 19:28;rbranson;Curious what the impact of this would have been? Has streaming effectively been broken since 1.2.6, since every cluster has wrapping ranges?","04/Sep/13 22:54;yukim;[~rbranson] It only affects bootstrap/decommission/move of wrapping range, since only those use transferRanges. If you do repair after bootstrap or move, then it will sync unfetched data.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit tests fail due to ant/junit problem,CASSANDRA-5388,12639209,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,enigmacurry,enigmacurry,26/Mar/13 22:09,12/Mar/19 14:12,13/Mar/19 22:29,27/May/13 16:35,,,,,,,0,,,,,"Intermittently, but more often than not I get the following error when running 'ant test' on Windows 7 (also encountered on Linux now):

{code}
BUILD FAILED
c:\Users\Ryan\git\cassandra3\build.xml:1121: The following error occurred while executing this line:
c:\Users\Ryan\git\cassandra3\build.xml:1064: Using loader AntClassLoader[C:\Program Files\Java\apache-ant-1.9.0\lib\ant-launcher.jar;c:\Program Files\Java\apache-ant-1.9.0\lib\ant.jar;c:\Program Files\Java\apache-ant-1.9.0\lib\ant-junit.jar;c:\Program Files\Java\apache-ant-1.9.0\lib\ant-junit4.jar;c:\Users\Ryan\git\cassandra3\build\classes\main;c:\Users\Ryan\git\cassandra3\build\classes\thrift;c:\Users\Ryan\git\cassandra3\lib\antlr-3.2.jar;c:\Users\Ryan\git\cassandra3\lib\avro-1.4.0-fixes.jar;c:\Users\Ryan\git\cassandra3\lib\avro-1.4.0-sources-fixes.jar;c:\Users\Ryan\git\cassandra3\lib\commons-cli-1.1.jar;c:\Users\Ryan\git\cassandra3\lib\commons-codec-1.2.jar;c:\Users\Ryan\git\cassandra3\lib\commons-lang-2.6.jar;c:\Users\Ryan\git\cassandra3\lib\compress-lzf-0.8.4.jar;c:\Users\Ryan\git\cassandra3\lib\concurrentlinkedhashmap-lru-1.3.jar;c:\Users\Ryan\git\cassandra3\lib\guava-13.0.1.jar;c:\Users\Ryan\git\cassandra3\lib\high-scale-lib-1.1.2.jar;c:\Users\Ryan\git\cassandra3\lib\jackson-core-asl-1.9.2.jar;c:\Users\Ryan\git\cassandra3\lib\jackson-mapper-asl-1.9.2.jar;c:\Users\Ryan\git\cassandra3\lib\jamm-0.2.5.jar;c:\Users\Ryan\git\cassandra3\lib\jbcrypt-0.3m.jar;c:\Users\Ryan\git\cassandra3\lib\jline-1.0.jar;c:\Users\Ryan\git\cassandra3\lib\json-simple-1.1.jar;c:\Users\Ryan\git\cassandra3\lib\libthrift-0.9.0.jar;c:\Users\Ryan\git\cassandra3\lib\log4j-1.2.16.jar;c:\Users\Ryan\git\cassandra3\lib\lz4-1.1.0.jar;c:\Users\Ryan\git\cassandra3\lib\metrics-core-2.0.3.jar;c:\Users\Ryan\git\cassandra3\lib\netty-3.5.9.Final.jar;c:\Users\Ryan\git\cassandra3\lib\servlet-api-2.5-20081211.jar;c:\Users\Ryan\git\cassandra3\lib\slf4j-api-1.7.2.jar;c:\Users\Ryan\git\cassandra3\lib\slf4j-log4j12-1.7.2.jar;c:\Users\Ryan\git\cassandra3\lib\snakeyaml-1.11.jar;c:\Users\Ryan\git\cassandra3\lib\snappy-java-1.0.4.1.jar;c:\Users\Ryan\git\cassandra3\lib\snaptree-0.1.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\apache-rat-0.6.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\apache-rat-core-0.6.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\apache-rat-tasks-0.6.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\asm-3.2.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\avro-1.3.2.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-beanutils-1.7.0.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-beanutils-core-1.8.0.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-cli-1.2.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-codec-1.4.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-collections-3.2.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-configuration-1.6.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-digester-1.8.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-el-1.0.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-httpclient-3.0.1.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-lang-2.4.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-logging-1.1.1.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-math-2.1.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-net-1.4.1.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\core-3.1.1.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\hadoop-core-1.0.3.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\hsqldb-1.8.0.10.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\jackson-core-asl-1.0.1.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\jackson-mapper-asl-1.0.1.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\jasper-compiler-5.5.12.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\jasper-runtime-5.5.12.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\jets3t-0.7.1.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\jetty-6.1.26.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\jetty-util-6.1.26.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\jna-3.2.7.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\jopt-simple-3.2.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\jsp-2.1-6.1.14.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\jsp-api-2.1-6.1.14.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\junit-4.6.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\kfs-0.3.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\oro-2.0.8.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\paranamer-2.2.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\paranamer-ant-2.1.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\paranamer-generator-2.1.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\pig-0.10.0.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\qdox-1.10.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\servlet-api-2.5-20081211.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\servlet-api-2.5-6.1.14.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\slf4j-api-1.5.11.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\xmlenc-0.52.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\ant-1.6.5-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\apache-rat-core-0.6-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\apache-rat-tasks-0.6-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\avro-1.3.2-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-beanutils-1.7.0-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-cli-1.2-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-codec-1.4-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-collections-3.2-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-configuration-1.6-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-digester-1.8-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-el-1.0-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-httpclient-3.0.1-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-lang-2.4-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-logging-1.1.1-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-math-2.1-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-net-1.4.1-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\jackson-core-asl-1.0.1-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\jackson-mapper-asl-1.0.1-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\jets3t-0.7.1-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\jetty-6.1.26-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\jetty-util-6.1.26-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\jna-3.2.7-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\jopt-simple-3.2-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\jsp-2.1-6.1.14-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\jsp-api-2.1-6.1.14-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\junit-4.6-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\oro-2.0.8-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\paranamer-2.2-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\paranamer-ant-2.1-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\paranamer-generator-2.1-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\pig-0.10.0-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\qdox-1.10-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\servlet-api-2.5-20081211-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\servlet-api-2.5-6.1.14-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\slf4j-api-1.5.11-sources.jar;c:\Users\Ryan\git\cassandra3\build\test\classes;c:\Users\Ryan\git\cassandra3\test\conf] on class org.apache.tools.ant.taskdefs.optional.junit.XMLJUnitResultFormatter: java.lang.NoClassDefFoundError: junit/framework/TestListener
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:791)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:423)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:356)
	at org.apache.tools.ant.AntClassLoader.findBaseClass(AntClassLoader.java:1385)
	at org.apache.tools.ant.AntClassLoader.loadClass(AntClassLoader.java:1064)
	at org.apache.tools.ant.util.SplitClassLoader.loadClass(SplitClassLoader.java:58)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:356)
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:791)
	at org.apache.tools.ant.AntClassLoader.defineClassFromData(AntClassLoader.java:1128)
	at org.apache.tools.ant.AntClassLoader.getClassFromStream(AntClassLoader.java:1299)
	at org.apache.tools.ant.AntClassLoader.findClassInComponents(AntClassLoader.java:1354)
	at org.apache.tools.ant.AntClassLoader.findClass(AntClassLoader.java:1315)
	at org.apache.tools.ant.util.SplitClassLoader.loadClass(SplitClassLoader.java:52)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:356)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:266)
	at org.apache.tools.ant.taskdefs.optional.junit.FormatterElement.createFormatter(FormatterElement.java:286)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.logVmExit(JUnitTask.java:1653)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.logTimeout(JUnitTask.java:1606)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeAsForked(JUnitTask.java:1096)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:851)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1899)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:800)
	at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)
	at org.apache.tools.ant.Task.perform(Task.java:348)
	at org.apache.tools.ant.taskdefs.Sequential.execute(Sequential.java:68)
	at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)
	at org.apache.tools.ant.Task.perform(Task.java:348)
	at org.apache.tools.ant.taskdefs.MacroInstance.execute(MacroInstance.java:396)
	at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)
	at org.apache.tools.ant.Task.perform(Task.java:348)
	at org.apache.tools.ant.Target.execute(Target.java:435)
	at org.apache.tools.ant.Target.performTasks(Target.java:456)
	at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1393)
	at org.apache.tools.ant.Project.executeTarget(Project.java:1364)
	at org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)
	at org.apache.tools.ant.Project.executeTargets(Project.java:1248)
	at org.apache.tools.ant.Main.runBuild(Main.java:851)
	at org.apache.tools.ant.Main.startAnt(Main.java:235)
	at org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)
	at org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)
Caused by: java.lang.ClassNotFoundException: junit.framework.TestListener
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:423)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:356)
	... 62 more
{code}

This isn't a specific unit test failing, this is ant itself crashing. The log specifies that junit-4.6.jar is on the classpath in the build/lib/jars directory and this file exists on disk, and the md5sum is the same as the official jar (37dc57962c1275ebc572726a6f5cdd13), so I cannot understand why the class cannot be found.

Steps to reproduce:
* Use Windows 7. (Reproduced on Linux now too)
* clone C* trunk
* run 'ant clean test'
* See the error in the log.

Interestingly, this is not 100% reproducible. While attempting to debugging this, I deleted my entire checkout, did a fresh 'git clone' and ran 'ant test' and *one time* I ran all the tests to completion, but I cannot reproduce this again, it fails every time I try now with the error above.","Windows 7 or Linux
java 1.7.0_17
ant 1.9.0",,,,,,,,,,,CASSANDRA-5383,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-04-05 15:49:34.693,,,no_permission,,,,,,,,,,,,319679,,,Mon May 27 16:35:16 UTC 2013,,,,,,0|i1j5pz:,320020,,,,,,,,,,,,,,,,,,,05/Apr/13 15:49;jbellis;I wonder if this could be related to pulling a second copy of ant down to build/lib/jars.  Maybe try manually removing build/lib/jars/ant-1.6.5.jar and run ant test -Dwithout.maven ?,"09/Apr/13 18:39;krummas;i have seen this error with ant 1.9.0 on linux as well, downgrading to 1.8.2 fixed it (on linux)","12/Apr/13 14:45;enigmacurry;I'm seeing this on linux now too, albeit intermittently.

Unfortunately, I can't get the cobertura code coverage stuff to work at all with -Dwithout.maven set.

I haven't seen this on ant 1.8.2, so that seems to be a solution for me anyway.","06/May/13 08:51;antoine@apache.org;Can you check whether it helps if you replace the 
c:\Users\Ryan\git\cassandra3\build\lib\jars\junit-4.6.jar by a junit-4.11.jar ?

Also, can you build the current ant 1.9.1-alpha from source by checking it out from subversion ? There are instructions on this page : http://ant.apache.org/manual/install.html#buildingant

Alternatively, you can use the file [ant1.9.1-alpha.tgz|http://people.apache.org/~antoine/dist/ant1.9.1-alpha.tgz]  
 
","07/May/13 16:04;enigmacurry;Upgrading to junit-4.11 did not have an effect.

Upgrading to ant-1.9.1-alpha from the tarball you uploaded, worked! I'm able to get all the way through 'ant clean test' repeatably. Thanks.",07/May/13 16:20;antoine@apache.org;Happy about that. This will be an incentive to release ant 1.9.1 fast.,27/May/13 15:39;jbellis;Ant 1.9.1 was released last week.,27/May/13 16:35;enigmacurry;Tested with ant 1.9.1 - issue resolved!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rebuilding secondary indexes leaks SSTable references,CASSANDRA-6635,12692095,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sbtourist,sbtourist,sbtourist,29/Jan/14 18:32,12/Mar/19 14:12,13/Mar/19 22:29,29/Jan/14 20:11,2.0.5,,,Feature/2i Index,,,0,,,,,"SecondaryIndex#buildIndexBlocking doesn't release SSTable references, which is generally no good, and most notably leaves sstable files hanging on the file system even after the column family is dropped.",,,,,,,,,,,,,,,,,,,,,,,,29/Jan/14 19:09;sbtourist;CASSANDRA-6635.patch;https://issues.apache.org/jira/secure/attachment/12625929/CASSANDRA-6635.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-29 20:11:26.763,,,no_permission,,,,,,,,,,,,370685,,,Wed Jan 29 20:11:26 UTC 2014,,,,,,0|i1rvyn:,370995,2.0.4,,,,,,,iamaleksey,iamaleksey,,,2.0.4,,,,,,,"29/Jan/14 20:11;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra startup fails when reported Java version contains alphanumeric characters,CASSANDRA-5380,12638846,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,dynamind,dynamind,25/Mar/13 10:31,12/Mar/19 14:12,13/Mar/19 22:29,10/Aug/13 21:32,,,,,,,0,,,,,"There is a new Java version check in the CassandraDaemon.java setup method that triggers a NumberFormatException when parsing a Java version containing alphanumeric characters such as ""1.7.0_12-ea""","OSX 10.8.3, Java version ""1.7.0_12-ea""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-03-25 12:50:29.871,,,no_permission,,,,,,,,,,,,319320,,,Mon Mar 25 12:50:29 UTC 2013,,,,,,0|i1j3i7:,319661,,,,,,,,,,,,,,,,,,,25/Mar/13 12:50;jbellis;Added a catch in 4fa3418c0bb8008ab2d2f1e2b4a2044596a47a4a; should fix it better to be able to actually parse that.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cant migrate json manifest with multiple data directories,CASSANDRA-6093,12670431,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,25/Sep/13 08:06,12/Mar/19 14:12,13/Mar/19 22:29,25/Sep/13 12:59,2.0.2,,,,,,0,,,,,"http://mail-archives.apache.org/mod_mbox/cassandra-user/201309.mbox/%3C002401ceb980%2485a26d10%2490e74730%24%40struq.com%3E

most likely due to having multiple data dirs",,,,,,,,,,,,,,,,,,,,,,,,25/Sep/13 08:50;krummas;0001-create-snapshot-directory-if-it-does-not-exist-when-.patch;https://issues.apache.org/jira/secure/attachment/12604981/0001-create-snapshot-directory-if-it-does-not-exist-when-.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-25 12:06:11.742,,,no_permission,,,,,,,,,,,,350260,,,Wed Sep 25 12:59:21 UTC 2013,,,,,,0|i1oe5r:,350553,,,,,,,,,,,,2.0 beta 1,,,,,,,"25/Sep/13 08:36;krummas;nope, works fine with multiple data dirs, need more info","25/Sep/13 08:49;krummas;found it, if the data dir is empty except for the json file, there is no directory to store the hardlink in

attached patch creates the dir",25/Sep/13 12:06;jbellis;+1,25/Sep/13 12:59;krummas;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool compact throws an error after importing data with sstableloader,CASSANDRA-6262,12676233,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,jblangston@datastax.com,jblangston@datastax.com,28/Oct/13 22:27,12/Mar/19 14:12,13/Mar/19 22:29,22/Jan/14 23:08,1.2.14,,,,,,0,,,,,"Exception when running nodetool compact:

{code}
Error occurred during compaction
java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: index (2) must be less than size (2)
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
	at java.util.concurrent.FutureTask.get(FutureTask.java:111)
	at org.apache.cassandra.db.compaction.CompactionManager.performMaximal(CompactionManager.java:331)
	at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1691)
	at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:2198)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1487)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:848)
	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at sun.rmi.transport.Transport$1.run(Transport.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.IndexOutOfBoundsException: index (2) must be less than size (2)
	at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:305)
	at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:284)
	at com.google.common.collect.RegularImmutableList.get(RegularImmutableList.java:81)
	at org.apache.cassandra.db.marshal.CompositeType.getComparator(CompositeType.java:94)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:76)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:31)
	at org.apache.cassandra.db.ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:128)
	at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:119)
	at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:114)
	at org.apache.cassandra.db.ColumnFamily.addAtom(ColumnFamily.java:219)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumnsFromSSTable(ColumnFamilySerializer.java:149)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:234)
	at org.apache.cassandra.db.compaction.PrecompactedRow.merge(PrecompactedRow.java:114)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:98)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:160)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:76)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:57)
	at org.apache.cassandra.utils.MergeIterator$OneToOne.computeNext(MergeIterator.java:203)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:145)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:352)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	... 3 more
{code}

Schema:

{code}
create table test (
 a uuid,
 b int,
 c timestamp,
 d text,
 e uuid,
 f blob,
 g uuid,
 h boolean,
 primary key (a, b)
);
{code}",,,,,,,,,,,,,,,,,,,,,,,,20/Jan/14 01:48;slebresne;6262.patch;https://issues.apache.org/jira/secure/attachment/12623879/6262.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-28 22:35:18.273,,,no_permission,,,,,,,,,,,,355730,,,Wed Jan 22 23:08:05 UTC 2014,,,,,,0|i1pbrj:,356018,,,,,,,,brandon.williams,brandon.williams,,,1.2.4,,,,,,,28/Oct/13 22:35;jbellis;What do I need to insert to reproduce?  I assume it doesn't repro just after CREATE TABLE.,"28/Oct/13 22:38;brandon.williams;Stack without compaction:

{noformat}
ERROR [ReadStage:98] 2013-10-23 22:15:25,991 CassandraDaemon.java (line 191) Exception in thread Thread[ReadStage:98,5,main]
java.lang.IndexOutOfBoundsException: index (2) must be less than size (2)
	at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:305)
	at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:284)
	at com.google.common.collect.RegularImmutableList.get(RegularImmutableList.java:81)
	at org.apache.cassandra.db.marshal.CompositeType.getComparator(CompositeType.java:94)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:76)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:31)
	at org.apache.cassandra.db.marshal.AbstractType$3.compare(AbstractType.java:69)
	at org.apache.cassandra.db.marshal.AbstractType$3.compare(AbstractType.java:66)
	at org.apache.cassandra.utils.MergeIterator$Candidate.compareTo(MergeIterator.java:151)
	at org.apache.cassandra.utils.MergeIterator$Candidate.compareTo(MergeIterator.java:128)
	at java.util.PriorityQueue.siftUpComparable(PriorityQueue.java:582)
	at java.util.PriorityQueue.siftUp(PriorityQueue.java:574)
	at java.util.PriorityQueue.offer(PriorityQueue.java:274)
	at java.util.PriorityQueue.add(PriorityQueue.java:251)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.advance(MergeIterator.java:123)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:96)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:157)
	at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:136)
	at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:84)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:291)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1391)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1214)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1126)
	at org.apache.cassandra.db.Table.getRow(Table.java:347)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:70)
	at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:44)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
{noformat}","28/Oct/13 23:19;brandon.williams;Confirmed that this was a case of schema mismatch.  It seems would could communicate this better to save time in the future, however.","19/Nov/13 11:09;slebresne;bq. Confirmed that this was a case of schema mismatch.

You meant, the sstables weren't valid sstables for the schema they were loaded into? If so, I'm not sure the loader can validate that easily (short of reading the sstable just for that matter) since there is no information on the schema in the sstable itself.","17/Jan/14 18:30;brandon.williams;I'm not saying we need to validate the schema per se (and I realize that's not easy) but throwing IOOBE doesn't make the problem immediately clear, some kind of hint about a possible schema mismatch could be useful.","20/Jan/14 01:48;slebresne;Not against improving the error message. Attaching a tentative patch for this. I'll note though that CompositeType seems like the only logical place to catch this and rethrow with more info, but that also mean we can't be absolutely sure of what the problem is when we're there, so I've attempted to throw an helpful error message without being too specific that it would be confusing in some other cases (than the one of this issue). It's also possible that an incompatible schema would throw another kind of exception, so this may not be enough, but well, that's progress so we can start there I guess.",20/Jan/14 02:25;brandon.williams;+1,22/Jan/14 23:08;brandon.williams;Sylvain committed this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CLASSPATH logic from init script is unused, JNA isn't loaded",CASSANDRA-6240,12675560,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,paravoid,paravoid,24/Oct/13 20:47,12/Mar/19 14:12,13/Mar/19 22:29,25/Oct/13 20:19,,,,Packaging,,,0,,,,,"The init script has a classpath() function that collects all the jars and even includes this piece of code to work with the standard Debian/Ubuntu libjna-jar:
{code:none}
    # use JNA if installed in standard location
    [ -r /usr/share/java/jna.jar ] && cp=""$cp:/usr/share/java/jna.jar""
{code}

This seems very nice and correct, however the classpath() function is never called and is entirely unused :) Instead, /usr/bin/cassandra is called, which in turn includes /usr/share/cassandra/cassandra.in.sh, which has basically similar code to collect the jars for CLASSPATH but a) without the JNA standard path trick b) without using EXTRA_CLASSPATH (from /etc/default/cassandra) at all, so Cassandra boots without either JNA nor EXTRA_CLASSPATH, contrary to expectations.

There are various suggestions on the web to do ""ln -s /usr/share/java/jna.jar /usr/share/cassandra/lib/""; I suspect this bug to be the reason for that.

 /usr/share/cassandra/cassandra.in.sh seems smart enough to append but not overwrite CLASSPATH, so fixing the init script's classpath() to only include JNA + EXTRA_CLASSPATH (and making sure it's actually getting called :)) should be enough for a fix.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-10-25 20:18:48.208,,,no_permission,,,,,,,,,,,,355137,,,Fri Oct 25 20:18:48 UTC 2013,,,,,,0|i1p83z:,355425,2.0.1,,,,,,,,,,,,,,,,,,"25/Oct/13 20:18;urandom;This was fixed in the 2.0 branch as part of CASSANDRA-6101, and will be released as part of 2.0.2.

There is one remaining issue with the init script that has gotten hung in review.  [~paravoid] if you could have a look at CASSANDRA-6131 and comment on it there, I'd be very grateful (I'll buy you a cheesesteak sandwich in Portland next summer :)).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add more data mappings for Pig,CASSANDRA-6128,12671672,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,alexliu68,alexliu68,alexliu68,01/Oct/13 22:05,12/Mar/19 14:12,13/Mar/19 22:29,07/Oct/13 19:07,1.2.11,2.0.2,,,,,0,,,,,"We need add more data mappings for
{code}
 DecimalType
 InetAddressType
 LexicalUUIDType
 TimeUUIDType
 UUIDType
{code}

Existing implementation throws exception for those data type",,,,,,,,,,,,,,,,,,,,,,,,01/Oct/13 22:19;alexliu68;6128-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12606207/6128-1.2-branch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-07 15:28:02.128,,,no_permission,,,,,,,,,,,,351382,,,Tue Oct 08 11:18:20 UTC 2013,,,,,,0|i1ol27:,351674,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"01/Oct/13 22:21;alexliu68;Map

{code}
 DecimalType
 InetAddressType
 LexicalUUIDType
 TimeUUIDType
 UUIDType
{code}

to CHARARRAY",07/Oct/13 15:28;brandon.williams;Shouldn't DecimalType map to a float or double instead of a string?,"07/Oct/13 18:27;alexliu68;Decimal has different precision than float/double, we will lose precision if we convert a decimal to a float/double. It is explained in this link http://stackoverflow.com/questions/5749615/losing-precision-converting-from-java-bigdecimal-to-double

If we don't need preserve the precision, we can use a double instead of a string. ","07/Oct/13 19:07;brandon.williams;Well, crap: PIG-2764

I guess we'll have to use a string for now, otherwise we box people into the corner of precision loss with no way out.  At least with strings they can do something in a UDF, so +1 and committed.","08/Oct/13 05:32;jeromatron;true, though we've been doing that all along anyway - we've always mapped Cassandra's BigInteger type to Pig's int and just accepted any possible loss.  That Pig ticket is nice to have for the future though.","08/Oct/13 11:18;brandon.williams;Actually, we just let BigInteger overflow:

{noformat}
        else if (type instanceof IntegerType || type instanceof Int32Type) // IntegerType will overflow at 2**31, but is kept for compatibility until pig has a BigInteger
{noformat}

but no one has ever noticed that, which doesn't surprise me.  On the other hand I think losing precision in DecimalType is likely to be a bigger, more subtle problem, when encountered. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade of 1.2.11 to 2.0.5 make IllegalArgumentException in Buffer.limit on read of a super column family,CASSANDRA-6733,12695964,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,hibou,hibou,19/Feb/14 14:18,12/Mar/19 14:12,13/Mar/19 22:29,04/Mar/14 10:56,2.0.6,,,,,,0,,,,,"We have a super column family which was first created with a 1.0.x. Then upgraded to 1.1.x, then to 1.2.11, and now to 2.0.5.
{noformat}
cqlsh:QaUser> desc table user_view;
CREATE TABLE user_view (
  key bigint,
  column1 varint,
  column2 text,
  value counter,
  PRIMARY KEY (key, column1, column2)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=1.000000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='99.0PERCENTILE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};
{noformat}

With cqlsh, the following query was doing a timeout:
{noformat}
select * from user_view where key = 3 and column1 = 1 and column2 = '20130218';
{noformat}

In the log of cassandra, we could read:
{noformat}
ERROR [ReadStage:1385] 2014-02-19 14:45:19,549 CassandraDaemon.java (line 192) Exception in thread Thread[ReadStage:1385,5,main]
java.lang.IllegalArgumentException
        at java.nio.Buffer.limit(Buffer.java:267)
        at org.apache.cassandra.db.marshal.AbstractCompositeType.getBytes(AbstractCompositeType.java:55)
        at org.apache.cassandra.db.marshal.AbstractCompositeType.getWithShortLength(AbstractCompositeType.java:64)
        at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:82)
        at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:35)
        at org.apache.cassandra.db.marshal.AbstractType$1.compare(AbstractType.java:63)
        at org.apache.cassandra.db.marshal.AbstractType$1.compare(AbstractType.java:60)
        at java.util.Collections.indexedBinarySearch(Collections.java:377)
        at java.util.Collections.binarySearch(Collections.java:365)
        at org.apache.cassandra.io.sstable.IndexHelper.indexFor(IndexHelper.java:144)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader$IndexedBlockFetcher.setNextSlice(IndexedSliceReader.java:262)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader$IndexedBlockFetcher.<init>(IndexedSliceReader.java:255)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader.<init>(IndexedSliceReader.java:91)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:65)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:42)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:167)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:250)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1560)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1379)
        at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:327)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:65)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:47)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:60)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
{noformat}

I tried launching repair on our 2 nodes, nothing improved.
I tried launching a major compaction on this column family, the query doesn't fail anymore and return expected results;

This happens on our cluster which is used for integration and test purpose, not much activity on it. There are only 2 nodes and the replication factor is at 1. Since it is our test cluster, I have a quite small (2 x ~500K) snapshot done before the upgrade of the cluster I could share, if needed.",,,,,,,,,,,,,,,,,,,,,,,,27/Feb/14 12:37;slebresne;6733.txt;https://issues.apache.org/jira/secure/attachment/12631518/6733.txt,20/Feb/14 19:27;hibou;QaUser_user_view_node1.tgz;https://issues.apache.org/jira/secure/attachment/12630125/QaUser_user_view_node1.tgz,20/Feb/14 19:27;hibou;QaUser_user_view_node2.tgz;https://issues.apache.org/jira/secure/attachment/12630126/QaUser_user_view_node2.tgz,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-02-20 08:28:54.595,,,no_permission,,,,,,,,,,,,374442,,,Wed Mar 19 12:35:37 UTC 2014,,,,,,0|i1siyn:,374742,2.0.5,,,,,,,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,,"20/Feb/14 08:28;slebresne;bq.  I have a quite small (2 x ~500K) snapshot done before the upgrade of the cluster I could share, if needed.

If it's no problem for you (and feel free to send it to me privately if you prefer), that would definitively simplify checking what's wrong (if you can share the original thrift table definition for completeness sake, that would be perfect).","20/Feb/14 19:27;hibou;See attached the compressed tar of the snapshot on the failing column family for both nodes.

For the creation of the column family, it was done via Hector:
{noformat}
ColumnFamilyDefinition cfViewDef = HFactory.createColumnFamilyDefinition(keyspaceName, columnFamilyName);
cfViewDef.setColumnType(ColumnType.SUPER);
cfViewDef.setComparatorType(ComparatorType.INTEGERTYPE);
cfViewDef.setSubComparatorType(ComparatorType.UTF8TYPE);
cfViewDef.setDefaultValidationClass(ComparatorType.COUNTERTYPE.getClassName());
cluster.getCluster().addColumnFamily(cfViewDef, true);
{noformat}

Then along the life of this column family, some manual operations were done via the cassandra-cli, like changing the compression, and setting a key validation class.","27/Feb/14 12:37;slebresne;Thanks Nicolas for the sstables and repro steps.

The main problem is here that while we do translate super-columns from pre-2.0 sstable to their new composite encoding on the fly, we don't handle the index for those sstable (more precisely, we don't handle the ""promoted"" column index part of the index).

I'm attaching a relatively simple patch to fix that. It's not excessively pretty, but we really only need that code for 2.0 since it's a mandatory stop before 2.1 and once the old sstable are rewritten we don't need that anymore (this is why compaction, which rewrite the sstables, fix that).

I'll note that even with that patch and using the sstables attached above, the query mentioned in the description ({{select * from user_view where key = 3 and column1 = 1 and column2 = '2013-02-18'}}) still doesn't work properly. It doesn't timeout anymore but return no results while it shouldn't. The reason for that is CASSANDRA-6778 because it happens that the super column names in the sstable are all 8 bytes long even when they wouldn't need to.

","27/Feb/14 15:41;hibou;Thank you for the feedback. I'll revert my test cluster to the data of the snapshot before the upgrade, and revert to 1.2.11. I'll try an upgrade again as soon as a fix is released.",03/Mar/14 11:18;slebresne;[~vijay2win@yahoo.com] marking you as reviewer since you were reviewer on CASSANDRA-3237 which this is basically a left-over. Feel free to complain if you don't have time to look at it (but it's not really a big patch).,"04/Mar/14 07:32;vijay2win@yahoo.com;Hi Sylvain, No Problem and would love to do so.

+1

Not sure why we do redundant checks on metadata.isSuper() inside SuperColumns.getComparatorFor()... (Not introduced by this patch though).
Nit: looks like isAfterSliceFinish is not used any more...","04/Mar/14 09:25;iamaleksey;bq. I'm attaching a relatively simple patch to fix that. It's not excessively pretty, but we really only need that code for 2.0 since it's a mandatory stop before 2.1 and once the old sstable are rewritten we don't need that anymore (this is why compaction, which rewrite the sstables, fix that).

Do we? We don't seem to require full upgradesstables on 2.0 for the 2.1 migration, or at least it's not there in NEWS.txt.",04/Mar/14 09:30;iamaleksey;(Should at least require it for super-CFs),"04/Mar/14 10:11;slebresne;bq. We don't seem to require full upgradesstables on 2.0 for the 2.1 migration

Oh, you're right, for some reason I though we had got rid of all SC compat code in 2.1 but that's just the MessagingService stuff. That being said, since we do require users to stop to 2.0 before into 2.1, I wonder if requiring them to upgrade their sstable is a huge deal, which would let us get rid of said compatibility code. Not a big deal though and we can decide that in another ticket (the patch here is not excessively pretty because it's a special case, but it's rather simple really, so not the end of the world if we have it in 2.1 too).","04/Mar/14 10:22;iamaleksey;bq. I wonder if requiring them to upgrade their sstable is a huge deal, which would let us get rid of said compatibility code.

Not a huge deal as long as it's limited to super columnfamilies only, since it wouldn't affect most anyone, relatively. That said, I prefer this to go into both 2.0 and 2.1, now that I realised that this affects CASSANDRA-6506 as well, which I now have to update (once this is pushed to 2.1). And not, it's not ugly.","04/Mar/14 10:56;slebresne;Committed, thanks (I've merge the code to 2.1, we can decide when we want to drop compatibility with pre-2.0 sstables another time).

bq. Not sure why we do redundant checks on metadata.isSuper() inside SuperColumns.getComparatorFor()

They are not redundant in most call of this method, and we could have inlined the relevant part of this method for the sake of this patch, but it felt like reuse that method was cleaner and it's not like the redundant test will matter in practice.

bq. Nit: looks like isAfterSliceFinish is not used any more

Removed in commit, thanks.","19/Mar/14 12:35;hibou;I tried again the upgrade but with 2.0.6 on our test cluster, it was not straight forward (I'll ask for some insights on cassandra-user), but it was successful.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`service cassandra status` fails in Ubuntu 13.04 RUssian,CASSANDRA-6163,12672841,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,nsv,nsv,08/Oct/13 16:22,12/Mar/19 14:12,13/Mar/19 22:29,29/Jul/14 18:01,,,,,,,0,,,,,"> sudo service cassandra status

xss =  -ea -javaagent:/usr/share/cassandra/lib/jamm-0.2.5.jar -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms4G -Xmx4G -Xmn800M -XX:+HeapDumpOnOutOfMemoryError -Xss256k
 * Cassandra is not running

> sudo netstat -anltp | grep 7199
tcp        0      0 0.0.0.0:7199            0.0.0.0:*               LISTEN      7589/java

Maybe it's linked to https://issues.apache.org/jira/browse/CASSANDRA-6162 - incomplete startup, some information not recorded (PID file ?), or again failure to parse localized output of system utilities.
",Ubuntu 13.04 RUssian (language seems important),,,,,,,,,,,,CASSANDRA-6162,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-10-08 16:45:08.5,,,no_permission,,,,,,,,,,,,352464,,,Tue Oct 08 16:45:08 UTC 2013,,,,,,0|i1oron:,352751,2.0.1,,,,,,,,,,,,,,,,,,08/Oct/13 16:45;brandon.williams;Probably CASSANDRA-6101 and CASSANDRA-6090,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Native protocol event don't respect the protocol version,CASSANDRA-5778,12658754,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,19/Jul/13 14:25,12/Mar/19 14:12,13/Mar/19 22:29,19/Jul/13 15:23,2.0 beta 2,,,,,,0,,,,,"Currently, the protocol version is on a per-message basis only. When we get a request, we respond with a message on the same protocol version. This is however broken for server events, that are responses to no request and currently always default to version 2 (on trunk), even if the client is on version 1.

So instead, we need to force a version per connection, and event messages should use that.",,,,,,,,,,,,,,,,,,,,,,,,19/Jul/13 14:27;slebresne;5578.txt;https://issues.apache.org/jira/secure/attachment/12593201/5578.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-19 15:11:18.193,,,no_permission,,,,,,,,,,,,338948,,,Fri Jul 19 15:23:33 UTC 2013,,,,,,0|i1mgmn:,339268,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,19/Jul/13 15:11;iamaleksey;+1,"19/Jul/13 15:23;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CREATE/DROP TRIGGER in CQL,CASSANDRA-5576,12648170,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,jbellis,jbellis,17/May/13 12:24,12/Mar/19 14:12,13/Mar/19 22:29,16/Jun/13 23:00,2.0 beta 1,,,Legacy/CQL,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-05-29 05:29:17.243,,,no_permission,,,,,,,,,,,,328526,,,Mon Jun 17 02:13:20 UTC 2013,,,,,,0|i1kokv:,328870,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"29/May/13 05:29;vijay2win@yahoo.com;Just to clarify is this what we are looking for?

CREATE TRIGGER <trigger_name> ON <table_name> FOR EACH MUTATION <trigger_class>;

or are we expecting something like this?

CREATE TRIGGER <trigger_name> FOR EACH MUTATION <trigger_class>;
ALTER TABLE ADD TRIGGER <trigger_name>;",29/May/13 14:16;jbellis;Whichever is easier. :),"29/May/13 16:18;iamaleksey;I would drop 'FOR EACH MUTATION' for now - until we have other trigger types (if we ever do at all).

Also, we might (and probably will) need to store additional info - or at least add the ability to parametrize triggers. So a column in schema_columnfamilies won't do (even a map, to keep the names).

We should add another system schema cf, something like this:

{noformat}
CREATE TABLE schema_triggers (
  keyspace_name text,
  columnfamily_name text,
  trigger_name text,
  trigger_options map<text, text>,
  PRIMARY KEY (keyspace_name, columnfamily_name, trigger_name)
);
{noformat}

And for consistency with CREATE CUSTOM INDEX and other CREATE's, use the following syntax for CREATE TRIGGER:

{noformat}
CREATE TRIGGER <name> ON <cfname> WITH options = {'class': …, ..}
{noformat}

(Support only 'class' for now).

This makes CASSANDRA-5578 semi-irrelevant, since we are not going to keep triggers info in schema_columnfamilies anymore.","02/Jun/13 22:20;vijay2win@yahoo.com;Alright i have pushed the changes to https://github.com/Vijay2win/cassandra/commits/5576

The trigger schematics is as follows:
{code}
cqlsh>CREATE TRIGGER index_logger ON ""Keyspace1"".""Standard1"" EXECUTE ('org.apache.cassandra.triggers.InvertedIndex', 'org.apache.cassandra.triggers.LogColumnUpdates'); 
cqlsh>DROP TRIGGER index_logger ON ""Keyspace1"".""Standard1"";
{code}

CLI
{code}
update column family Standard1 with triggers='{""test"":[""org.apache.cassandra.triggers.InvertedIndex"",""org.apache.cassandra.triggers.LogColumnUpdates""]}';
{code}","03/Jun/13 20:58;iamaleksey;Was there an issue with any of the suggestions in https://issues.apache.org/jira/browse/CASSANDRA-5576?focusedCommentId=13669381&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13669381?

CQL is part of public API, we *try* not to change it too often, and to keep it consistent.

One issue with
{noformat}
CREATE TRIGGER index_logger ON ""Keyspace1"".""Standard1"" EXECUTE ('org.apache.cassandra.triggers.InvertedIndex', 'org.apache.cassandra.triggers.LogColumnUpdates');
{noformat}
Is that it closes the door to future parametrization of trigger classes (which is not planned for 2.0, but will probably happen eventually). Another is that I just see no value in being able to bundle several trigger classes under one name - what exactly does it buy us?

So, ideally, I'd rather see
{noformat}
CREATE TRIGGER indexer ON ""Keyspace1"".""Standard1"" WITH options = {'class': 'org.apache.cassandra.triggers.InvertedIndex'}
{noformat}

(Use map syntax, even if we are only going to support one option - 'class', for now. You can look at CREATE CUSTOM INDEX implementation to see what exactly I mean).

We also *don't* want to serialize anything as JSON in schema columns anymore (this is what CASSANDRA-5578 was about). The original plan was to use a set<text> field for the triggers, but now that we can attach names to them, and potentially want to be able to parametrize them, neither a set<text> or a map<text, text> will do. So we need a separate schema-table - I listed one example schema in the comment before this one.","04/Jun/13 03:26;vijay2win@yahoo.com;{quote}
Was there an issue with any of the suggestions
{quote}
Didn't we just drop the options (Map<String, String>) style configuration on CF in CASSANDRA-4795? Future extension can do simple match on the input parameter which i think is not that bad...

{quote}
what exactly does it buy us?
{quote}
It allows the user to group things so they can drop and manage a set of triggers than individual ones... I am fine dropping that and having one to one relationship (name to class names).

{quote}
We also don't want to serialize anything as JSON in schema columns anymore
{quote}

Well it was not just simple type specification in table definition the announce method has to change, let me spend more time troubleshooting. Hope everything else is alright.


","04/Jun/13 13:47;iamaleksey;bq. Didn't we just drop the options (Map<String, String>) style configuration on CF in CASSANDRA-4795?

We didn't. We made the validation stricter, that's all.","06/Jun/13 04:28;jbellis;bq. I just see no value in being able to bundle several trigger classes under one name 

+1

bq. CREATE TRIGGER indexer ON ""Keyspace1"".""Standard1"" WITH options = {'class': 'org.apache.cassandra.triggers.InvertedIndex'}

-0 -- I see the value of ""options"" when we introduce parameterization, but having the actual trigger class be part of that seems odd, since that's something every trigger needs (i.e. a requirement, not an option or parameter).

So I think I'd prefer to have the EXECUTE syntax, with a single target.  We can add {{WITH options}} later.  (But I agree that it's a good idea to split the triggers into a separate system table to make that easier.)","06/Jun/13 06:38;iamaleksey;{quote} 
-0 – I see the value of ""options"" when we introduce parameterization, but having the actual trigger class be part of that seems odd, since that's something every trigger needs (i.e. a requirement, not an option or parameter).

So I think I'd prefer to have the EXECUTE syntax, with a single target. We can add WITH options later. (But I agree that it's a good idea to split the triggers into a separate system table to make that easier.)
{quote}

I would agree with you if there were no precedent. But that's the syntax we already use for CREATE CUSTOM INDEX (CASSANDRA-5484), and it's worth reusing it just for consistency's sake, IMO.","06/Jun/13 13:46;jbellis;I don't buy ""we made a mistake in 5484 so we should keep perpetuating the mistake in other places"" argument.  We should do it right here and update CREATE CUSTOM INDEX syntax for 2.0 to be consistent with this, instead of the other way around.","06/Jun/13 15:52;iamaleksey;I don't think it was a mistake in case of 5484. Do you have better suggestions?

But, all right, I'm okay with your suggestion for CREATE TRIGGER.

[~slebresne] any ideas re: changing 5484 and syntax for CREATE TRIGGER?","06/Jun/13 16:07;jbellis;bq. I don't think it was a mistake in case of 5484.

Hmm.  I see what you mean wrt compaction, compression configuration.  Damn it.  All right, I guess I'm sold on ""everything in options map.""","06/Jun/13 16:17;slebresne;I don't think I care too much which syntax we pick, though I'm definitely in favor of more consistency over less.

It does slightly bug me to change the syntax for custom indexes after it's been release. But, given that nobody is probably using the custom index syntax at this point (it's not really properly documented yet typically), I'm fine with it if you guys think it measurably improve the syntax (I have no real opinion for once on that question). But I'd *really* prefer doing it now rather than leaving the current syntax in 1.2 and changing it in 2.0.  

Though if we do change the custom index syntax, what are we talking about? Something like:
{noformat}
CREATE CUSTOM INDEX ON <table>(<column>) USING <classname>
{noformat}
with a trigger syntax that would be
{noformat}
CREATE TRIGGER <name> ON <table> EXECUTE <classname>
{noformat}
Knowing that for both we could add support for {{WITH options=\{...\} }} whenever we actually have options?

","06/Jun/13 16:28;iamaleksey;btw, if we do go with USING for CREATE CUSTOM INDEX, a more consistent
{noformat}
CREATE TRIGGER <name> on <table> USING <class> WITH options
{noformat}
also sounds okay to me (and avoids adding an extra keyword, too).

But I'm equally fine with 'everything in options map'. As long as we are consistent, at least betweetn CREATE CUSTOM INDEX and CREATE TRIGGER.",06/Jun/13 18:52;jbellis;If everyone is fine with both options then I move we leave the final decision to Vijay who is coding it up. :),"11/Jun/13 05:39;vijay2win@yahoo.com;It took longer than expected to troubleshoot CFMetaData, 
https://github.com/Vijay2win/cassandra/commits/5576-v2 has the following semantics...

{code}
cqlsh> create TRIGGER test2 ON ""Keyspace1"".""Standard1"" using 'org.apache.cassandra.triggers.InvertedIndex';
cqlsh> SELECT * FROM system.schema_triggers WHERE keyspace_name='Keyspace1' AND column_family='Standard1';

 keyspace_name | column_family | trigger_classes
---------------+---------------+------------------------------------------------------
     Keyspace1 |     Standard1 | {test2: org.apache.cassandra.triggers.InvertedIndex}

cqlsh> drop trigger test2 on ""Keyspace1"".""Standard1"";                           
{code}

The trigger data is now stored in schema_triggers and cli is as follows
{code}
update column family Standard1 with triggers= '{""test"":""org.apache.cassandra.triggers.InvertedIndex""}';
{code}","12/Jun/13 19:52;iamaleksey;Yep, that's the CQL3 syntax we want. I'll open a ticket for me to update CREATE CUSTOM INDEX to switch to USING as well.

I'm afraid that schema_triggers schema like this is not enough - won't allow options in the future (and changing this schema is gonna be *a lot* more painful than adding WITH to the CREATE statement).

Can't we use

{noformat}
CREATE TABLE schema_triggers (
  keyspace_name text,
  columnfamily_name text,
  trigger_name text,
  trigger_options map<text, text>,
  PRIMARY KEY (keyspace_name, columnfamily_name, trigger_name)
);
{noformat}
, as it was suggested in https://issues.apache.org/jira/browse/CASSANDRA-5576?focusedCommentId=13669381&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13669381 initially?

Your example would translate to
keyspace_name=Keyspace1, column_family=Standard1, trigger_name=test2, trigger_options={class:org.apache.cassandra.triggers.InvertedIndex}

Will also need to update the cli syntax to something column_metadata-like

{noformat}
update column family Standard1 with triggers = [
{trigger_name: test2, class: org.apache.cassandra.triggers.InvertedIndex}
];
{noformat}","14/Jun/13 06:23;vijay2win@yahoo.com;Done with all the above changes and pushed to the same branch. Let me know... Thanks!

{code}
          update column family Standard1 with triggers =
              '[{""name"":""test"",""class"":""org.apache.cassandra.triggers.InvertedIndex""}]';
{code}",14/Jun/13 16:02;jbellis;I don't see Aleksey's schema change; am I missing something?,"14/Jun/13 16:28;vijay2win@yahoo.com;Yep, We do have it in https://github.com/Vijay2win/cassandra/commit/ee36fe0fe99252f7ca7268863f33a9bc585a4c58#L0R165 i separated into 2 commits but will squash before merging to trunk.","14/Jun/13 16:40;jjordan;[~vijay2win@yahoo.com] I think the change he is refering to is using:

{noformat}
  trigger_options map<text, text>,
{noformat}

instead of

{noformat}
trigger_class text,
{noformat}

So that adding more parameters later doesn't require a schema change.","14/Jun/13 17:05;vijay2win@yahoo.com;[~jjordan] the options can be additional columns which can be defined later (same way we do SCHEMA_COLUMNFAMILIES_CF) when we add them we need to validate them anyways.... 
If they are dynamic then they can be additional columns like list<map<String, map<String, String>> i believe this schema provides a lot of flexibility for now.

I really dont have a lot of opinion on it, it just was more logical with the rest of the code (Specially with querying and dropping etc)... 
if you want to change it, i can change it. Then we just need to special case trigger name from the map etc..","14/Jun/13 23:59;jbellis;bq. the options can be additional columns which can be defined later

I think compaction options is a better example -- valid options are defined by the implementing class, so we just want a Map<option, value> that we can pass to the constructor.","15/Jun/13 04:52;iamaleksey;Yes, options should definitely be a map<text, text>. Question is, should class be a separate column or should it be simply one of the options in that map.

Cases where class is part of the options and not a separate column:
- custom 2i (schema_columns.index_options)
- compression (schema_columnfamilies.compression_parameters)

Cases where class is a separate column:
- replication (schema_keyspaces.strategy_class)
- compaction (schema_columnfamilies.compaction_strategy_class)

In the last two cases, even though they are stored separately, in CQL3 CREATE/ALTER the class is still conceptually part of the options map:
- CREATE KEYSPACE <name> WITH replication = {'class': 'path.to.strategy.class', 'option1': 'value1', 'optionX': ...}
- ALTER TABLE <name> WITH compaction = {'class': 'SizeTieredCompactionStrategy', 'option1': 'value1', 'optionX': ...}

Now, once we add parameters support to triggers, we could just add trigger_options map<text, text> and to keep all but class there. Adding columns to system tables is okay, dropping them isn't. But adding trigger_options map<text,text> right now and storing the class there (with 'class' key) will let us to leave schema_triggers schema like this forever, future-proofing it (this is what I suggested in https://issues.apache.org/jira/browse/CASSANDRA-5576?focusedCommentId=13681562&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13681562 and in https://issues.apache.org/jira/browse/CASSANDRA-5576?focusedCommentId=13669381&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13669381 before that).","15/Jun/13 05:03;vijay2win@yahoo.com;{quote}
 dropping them isn't.
{quote}
With range tombstone we just delete all the Composite columns with a prefix ""where trigger_name='xyz'""... thats what the patch does :)","15/Jun/13 05:08;iamaleksey;I'm talking about removing columns from schema of system tables themselves, not the values (say, getting rid of schema_columnfamilies.compaction_strategy_class column altogether). Values, sure.","15/Jun/13 05:17;iamaleksey;Mentioned it to highlight how important getting the schema of system tables right the first time is. Changing it is hard. Removing columns - almost impossible. If we can have one less from the start, I'd much rather prefer it. And we *will* add trigger parametrization in the future - so why not add trigger_options map<text,text> right now and keep the class there, instead of having a separate trigger_class column? Not having to touch schema_triggers schema in the future = win.",15/Jun/13 06:10;vijay2win@yahoo.com;Pushed the schema change to the same branch. Thanks!,"15/Jun/13 07:29;iamaleksey;Thanks! Everything looks good except some 1-line-fixable things and thrift/cli issue:
- DropTriggerStatement.checkAccess() should check for ALTER - we are altering a table here, not dropping it
- DropTriggerStatement.changeType() should return UPDATED, for the same reason
- in Cql.g should probably add K_TRIGGER to unreserved_function_keyword to not break people's queries if they have a column named 'trigger' somewhere. The less reserved keywords we got, the better
- (nit) in CFMetaData, SchemaTriggerCf = compile(5, ...) - there is no need for oldCfId argument here (5), since the table has never existed in 1.1
- (nit) CFMetaData.getTriggerClass() should be renamed to CFMetaData.getTriggerClasses() - it returns a collection, after all

Thrift/cli issue:
{noformat}
43: optional list<map<string, string>> triggers,
{noformat}
Is not future-parametrization proof.

Also, cli syntax is using json, instead of using arrayConstruct from Cli.g (with nested hashConstruct). The former requires json where there shouldn't be, and quotes around key names (and quotes around the whole thing), which it shouldn't.

*HOWEVER* I suggest not fixing it at all and removing triggers entirely from CLI and thrift APIs. CLI is deprecated. And with Thrift you won't be able to add/remove triggers from CQL3 tables, anyway, because you can't even see them. With CQL3 CREATE TRIGGER/DROP TRIGGER, however, you can modify both CQL3 and old-style tables, easily. So let's just rely on them/cqlsh for modifying triggers.

I'm open to debate about thirft/cli here. But if we are going to support trigger modification from cli/thrift, it'll be done right, and I can't justify wasting any more of your time on that.","16/Jun/13 18:24;vijay2win@yahoo.com;Removed cli support for it, but thrift i am not sure there is a lot of automation in testing which relies on it... 

As a user i use thrift for various reasons like performance and i am not sure if i would switch that soon, specially with the cross platforms we have to deal with.

v4 addresses the nits too. https://github.com/Vijay2win/cassandra/commits/5576-v4
","16/Jun/13 18:56;iamaleksey;bq. As a user i use thrift for various reasons like performance and i am not sure if i would switch that soon, specially with the cross platforms we have to deal with.

You'd only have to use CQL3 for DROP and CREATE TRIGGER (via thfit.execute_cql3_query()) and ignore it otherwise. Speed not affected (unless you are talking about the speed of creating a trigger (: ). Though I can see some value of being able to modify triggers via thrift.system_update_column_family() if one has no CQL3 tables at all and doesn't want to bother with it, ever.

Can you make it map<string, map<string, string>> (index name -> index options, including class) then? So that once we add parametrization support, thrift wouldn't have to change at all?

Other than this, everything looks fine to me.

(tiny nit: FBUtilities.fromJsonListObject() is now unused and can be removed)","16/Jun/13 19:17;iamaleksey;Also, TriggerOptions should be moved to org.apache.cassandra.config (and ideally renamed to just ""Triggers"").

This is all.","16/Jun/13 22:59;vijay2win@yahoo.com;{quote}
Speed not affected
{quote}
I am talking about libraries that are written on thrift which has to special case just for triggers if we dont support, though it is not required per say.

{quote}
renamed to just ""Triggers""
{quote}
Triggers class name will cause confusion and hence left it alone and moved it to config, though I am not sure if thats the right place for it.

Committed to trunk, Glad it is done... and has map<string, map<string, string> now :-), Thanks!",16/Jun/13 23:01;iamaleksey;Good (:,"17/Jun/13 02:13;jbellis;\o/

On Sun, Jun 16, 2013 at 6:03 PM, Aleksey Yeschenko (JIRA)



-- 
Jonathan Ellis
Project Chair, Apache Cassandra
co-founder, http://www.datastax.com
@spyced
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add existing sstables to leveled manifest on startup,CASSANDRA-5908,12664817,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,21/Aug/13 12:14,12/Mar/19 14:12,13/Mar/19 22:29,21/Aug/13 12:55,2.0.0,,,,,,0,lcs,,,,"we need to add all sstables to the leveled manifest on startup, looks like this was introduced in 6968f68cd7c",,,,,,,,,,,,,,,,,,,,,,,,21/Aug/13 12:15;krummas;0001-on-startup-add-all-sstables-to-the-leveled-manifest.patch;https://issues.apache.org/jira/secure/attachment/12599179/0001-on-startup-add-all-sstables-to-the-leveled-manifest.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-21 12:43:44.916,,,no_permission,,,,,,,,,,,,344760,,,Wed Aug 21 12:55:43 UTC 2013,,,,,,0|i1ngbj:,345060,,,,,,,,,,,,2.0 beta 1,,,,,,,21/Aug/13 12:43;jbellis;+1,"21/Aug/13 12:55;krummas;cool, committed as 8f367fdf92c03ee4bbcb0daa2272bd5155cf4174",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memtable flushing should write any columns shadowed by partition/range tombstones if any 2i are present,CASSANDRA-6112,12671288,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,29/Sep/13 22:14,12/Mar/19 14:12,13/Mar/19 22:29,29/Sep/13 23:21,1.2.11,2.0.2,,,,,0,,,,,"We shouldn't be dropping any columns obsoleted by partition and/or range tombstones in case the table has secondary indexes, or else the stale entries wouldn't be cleaned up during compaction, and will only be dropped during 2i query read-repair, if that happens.",,,,,,,,,,,,,,,,,,,,,,,,29/Sep/13 22:15;iamaleksey;6112.txt;https://issues.apache.org/jira/secure/attachment/12605830/6112.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-29 23:10:44.818,,,no_permission,,,,,,,,,,,,350994,,,Sun Sep 29 23:30:25 UTC 2013,,,,,,0|i1oinr:,351285,,,,,,,,jbellis,jbellis,,,,,,,,,,29/Sep/13 23:10;jbellis;+1,"29/Sep/13 23:21;iamaleksey;Committed, thanks.",29/Sep/13 23:30;iamaleksey;Backported to 1.2.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
system.peers table not updated after decommissioning nodes in C* 2.0,CASSANDRA-6053,12669215,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thobbs,gumuz,gumuz,18/Sep/13 09:06,12/Mar/19 14:11,13/Mar/19 22:29,10/Jan/14 18:52,1.2.14,2.0.5,,,,,9,,,,,"After decommissioning my cluster from 20 to 9 nodes using opscenter, I found all but one of the nodes had incorrect system.peers tables.

This became a problem (afaik) when using the python-driver, since this queries the peers table to set up its connection pool. Resulting in very slow startup times, because of timeouts.

The output of nodetool didn't seem to be affected. After removing the incorrect entries from the peers tables, the connection issues seem to have disappeared for us. 

Would like some feedback on if this was the right way to handle the issue or if I'm still left with a broken cluster.

Attached is the output of nodetool status, which shows the correct 9 nodes. Below that the output of the system.peers tables on the individual nodes.
",Datastax AMI running EC2 m1.xlarge instances,,,,,,,,,,,,,,,,,,,,,,,31/Dec/13 22:49;thobbs;6053-v1.patch;https://issues.apache.org/jira/secure/attachment/12620992/6053-v1.patch,18/Sep/13 09:07;gumuz;peers;https://issues.apache.org/jira/secure/attachment/12603794/peers,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-09-26 21:28:41.175,,,no_permission,,,,,,,,,,,,349147,,,Tue Nov 03 14:30:09 UTC 2015,,,,,,0|i1o7bj:,349445,1.2.9,2.0.3,,,,,,brandon.williams,brandon.williams,,,2.0.0,,,,,,,18/Sep/13 09:07;gumuz;Output of nodetool status and the contents of the system.peers tables.,18/Sep/13 14:35;gumuz;Just a heads up: I've just removed a broken node using 'nodetool removenode' and encountered the same problem again. The node wasn't removed from the other nodes' system.peers.,26/Sep/13 21:28;cjbottaro;We're seeing this on a 1.2.9 cluster as well.,27/Sep/13 18:56;cjbottaro;Is there a way to manually fix the system.peers table once it's in the messed up state?  Like a nodetool command or simply using CQL to delete the bad rows?,"27/Sep/13 19:11;brandon.williams;You can simply delete them all from system.peers if you want, they'll re-populate correctly.","27/Sep/13 19:15;brandon.williams;Not sure how this can happen, given this code:

{noformat}
    private void removeEndpoint(InetAddress endpoint)
    {
        Gossiper.instance.removeEndpoint(endpoint);
        if (!isClientMode)
            SystemTable.removeEndpoint(endpoint);
    }
{noformat}

which is what decom and removetoken end up calling.","27/Sep/13 21:16;cjbottaro;I did a ""truncate peers"" and it hasn't repopulated (I waited about 10 mins).  Do I need to restart one or more nodes to trigger repopulating of the table?",27/Sep/13 23:32;brandon.williams;Yes.,"03/Oct/13 11:12;jeromatron;The load_ring_state=false directive should probably also clear out the peers table because otherwise, the state that you're trying to get rid of is still persisted there.","07/Oct/13 16:56;brandon.williams;If a user knows enough to disable loading the state and that fixes the problem, they can clear the peers table manually.",18/Dec/13 17:02;jbellis;[~enigmacurry] can you reproduce?,"18/Dec/13 18:03;enigmacurry;First attempt appears to work correctly on cassandra-2.0 HEAD and 1.2.9 : 

{code}
12:53 PM:~$ ccm create -v git:cassandra-1.2.9 t
Fetching Cassandra updates...
Current cluster is now: t
12:53 PM:~$ ccm populate -n 5
12:54 PM:~$ ccm start
12:54 PM:~$ ccm node1 stress
Created keyspaces. Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,latency/95th/99th,elapsed_time
24994,2499,2499,9.5,55.2,179.0,10
103123,7812,7812,2.8,27.2,134.7,20
236358,13323,13323,1.7,15.4,134.7,30
329477,9311,9311,1.7,9.8,109.8,40
405667,7619,7619,1.8,9.2,6591.9,50
558989,15332,15332,1.5,6.6,6591.1,60
^C12:55 PM:~$ ccm node1 cqlsh
Connected to t at 127.0.0.1:9160.
[cqlsh 3.1.7 | Cassandra 1.2.9-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.36.0]
Use HELP for help.
cqlsh> select peer from system.peers;

 peer
-----------
 127.0.0.3
 127.0.0.2
 127.0.0.5
 127.0.0.4

cqlsh>
12:55 PM:~$ ccm node2 decommission
12:57 PM:~$ ccm node1 cqlsh
Connected to t at 127.0.0.1:9160.
[cqlsh 3.1.7 | Cassandra 1.2.9-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.36.0]
Use HELP for help.
cqlsh> select peer from system.peers;

 peer
-----------
 127.0.0.3
 127.0.0.5
 127.0.0.4

cqlsh>
12:58 PM:~$
{code}

All nodes show equivalent peers table.","18/Dec/13 18:29;enigmacurry;OK, reproduced this by killing -9 one of the nodes and then doing a 'nodetool removenode':

{code}
01:20 PM:~$ kill -9 18961    (PID of node1)
01:21 PM:~$ ccm node1 status
Failed to connect to '127.0.0.1:7100': Connection refused
01:21 PM:~$ ccm node2 status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Owns   Host ID                               Token                                    Rack
DN  127.0.0.1  62.93 KB   20.0%  896644af-8640-4be6-a3ff-e8ed559d851c  -9223372036854775808                     rack1
UN  127.0.0.2  51.17 KB   20.0%  d3801466-d36d-428c-b4e5-05ff69fe36c0  -5534023222112865485                     rack1
UN  127.0.0.3  62.78 KB   20.0%  cb36c3ad-df45-4f77-bff5-ca93c504ec08  -1844674407370955162                     rack1
UN  127.0.0.4  51.17 KB   20.0%  89031a05-a3f6-4ac7-9d29-6caa0c609dbc  1844674407370955161                      rack1
UN  127.0.0.5  51.27 KB   20.0%  4909d856-a86e-493a-a7d0-7570d71eb9d8  5534023222112865484                      rack1

# Issue removenode on node3 :
01:21 PM:~$ ~/.ccm/t/node1/bin/nodetool -p 7300 removenode 896644af-8640-4be6-a3ff-e8ed559d851c

01:22 PM:~$ ccm node3 cqlsh
Connected to t at 127.0.0.3:9160.
[cqlsh 4.1.0 | Cassandra 2.0.3-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.39.0]
Use HELP for help.
cqlsh> select * from system.peers;

 peer      | data_center | host_id                              | preferred_ip | rack  | release_version | rpc_address | schema_version                       | tokens
-----------+-------------+--------------------------------------+--------------+-------+-----------------+-------------+--------------------------------------+--------------------------
 127.0.0.2 | datacenter1 | d3801466-d36d-428c-b4e5-05ff69fe36c0 |         null | rack1 |  2.0.3-SNAPSHOT |   127.0.0.2 | d133398f-f287-3674-83af-a1b04ee29f1f | {'-5534023222112865485'}
 127.0.0.5 | datacenter1 | 4909d856-a86e-493a-a7d0-7570d71eb9d8 |         null | rack1 |  2.0.3-SNAPSHOT |   127.0.0.5 | d133398f-f287-3674-83af-a1b04ee29f1f |  {'5534023222112865484'}
 127.0.0.4 | datacenter1 | 89031a05-a3f6-4ac7-9d29-6caa0c609dbc |         null | rack1 |  2.0.3-SNAPSHOT |   127.0.0.4 | d133398f-f287-3674-83af-a1b04ee29f1f |  {'1844674407370955161'}

(3 rows)

# Check node2 peers table:

01:23 PM:~$ ccm node2 cqlsh
Connected to t at 127.0.0.2:9160.
[cqlsh 4.1.0 | Cassandra 2.0.3-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.39.0]
Use HELP for help.
cqlsh> select * from system.peers;

 peer      | data_center | host_id                              | preferred_ip | rack  | release_version | rpc_address | schema_version                       | tokens
-----------+-------------+--------------------------------------+--------------+-------+-----------------+-------------+--------------------------------------+--------------------------
 127.0.0.3 | datacenter1 | cb36c3ad-df45-4f77-bff5-ca93c504ec08 |         null | rack1 |  2.0.3-SNAPSHOT |   127.0.0.3 | d133398f-f287-3674-83af-a1b04ee29f1f | {'-1844674407370955162'}
 127.0.0.1 |        null | 896644af-8640-4be6-a3ff-e8ed559d851c |         null |  null |            null |   127.0.0.1 |                                 null |                     null
 127.0.0.5 | datacenter1 | 4909d856-a86e-493a-a7d0-7570d71eb9d8 |         null | rack1 |  2.0.3-SNAPSHOT |   127.0.0.5 | d133398f-f287-3674-83af-a1b04ee29f1f |  {'5534023222112865484'}
 127.0.0.4 | datacenter1 | 89031a05-a3f6-4ac7-9d29-6caa0c609dbc |         null | rack1 |  2.0.3-SNAPSHOT |   127.0.0.4 | d133398f-f287-3674-83af-a1b04ee29f1f |  {'1844674407370955161'}

(4 rows)

# oh noes!... node2 still has an entry for node1 in peers table.

01:23 PM:~$ ccm node2 status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Owns   Host ID                               Token                                    Rack
UN  127.0.0.2  51.17 KB   40.0%  d3801466-d36d-428c-b4e5-05ff69fe36c0  -5534023222112865485                     rack1
UN  127.0.0.3  62.78 KB   20.0%  cb36c3ad-df45-4f77-bff5-ca93c504ec08  -1844674407370955162                     rack1
UN  127.0.0.4  51.17 KB   20.0%  89031a05-a3f6-4ac7-9d29-6caa0c609dbc  1844674407370955161                      rack1
UN  127.0.0.5  51.27 KB   20.0%  4909d856-a86e-493a-a7d0-7570d71eb9d8  5534023222112865484                      rack1

{code}

By issuing the removenode on node3, node3 seems to know about the node being removed and it's peers table is correct. node2, although it's status output shows node1 going away, it's peers table has not been updated.","18/Dec/13 21:57;jbellis;Thanks, Ryan.",30/Dec/13 13:54;brandon.williams;Can you provide debug logs from node2?,"31/Dec/13 18:39;thobbs;I was able to repro with Ryan's steps, so I can upload the logs if you'd like, but I should be able to figure this one out.","31/Dec/13 22:49;thobbs;The problem was that state changes for the removed node were being handled after the system.peers row was deleted.  These state changes would result in update to the system.peers row, partially reviving it.

6053-v1.patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-6053]) avoids updating the system.peers table if the node is unknown or is in one of the ""dead"" states and adds some basic unit test coverage.",10/Jan/14 18:52;brandon.williams;Committed.,"03/Nov/15 14:30;kenfailbus;[~brandon.williams] FYI - Even though, this was fixed it surfaced in 2.0.14 release that we have in production. We are going to follow the work-around as mentioned above.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool getsstables converts key from String incorrectly,CASSANDRA-6803,12699010,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,zznate,zznate,zznate,05/Mar/14 23:27,12/Mar/19 14:11,13/Mar/19 22:29,13/Mar/14 18:07,1.2.16,2.0.7,,Tool/nodetool,,,0,,,,,"Trivial fix, just need to get the bytebuffer from the CfMetaData's key validator as opposed to just calling String#getBytes (which is broken for most data types).  ",,,,,,,,,,,,,,CASSANDRA-11337,,,,,,,,,,05/Mar/14 23:28;zznate;sstables_for_key_blob_support.txt;https://issues.apache.org/jira/secure/attachment/12632959/sstables_for_key_blob_support.txt,05/Mar/14 23:28;zznate;sstables_for_key_blob_support_2.0.txt;https://issues.apache.org/jira/secure/attachment/12632958/sstables_for_key_blob_support_2.0.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-03-13 18:07:04.909,,,no_permission,,,,,,,,,,,,377357,,,Mon May 19 15:15:58 UTC 2014,,,,,,0|i1t0w7:,377652,1.2.15,2.0.5,,,,,,jbellis,jbellis,,,,,,,,,,05/Mar/14 23:28;zznate;patches for 2.0 and 1.2. ,13/Mar/14 18:07;jbellis;committed; thanks!,"18/May/14 00:41;pauloricardomg;hm, not cool. I have a CF with a BytesType key validator, and now I'm getting an exception when trying to call nodetool getsstables <keyspace> <cf> <rowKeyStr>.

{code:java}
Exception in thread ""main"" org.apache.cassandra.db.marshal.MarshalException: cannot parse 'foobar' as hex bytes
	at org.apache.cassandra.db.marshal.BytesType.fromString(BytesType.java:69)
	at org.apache.cassandra.db.ColumnFamilyStore.getSSTablesForKey(ColumnFamilyStore.java:1373)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1487)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:848)
	at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at sun.rmi.transport.Transport$1.run(Transport.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.NumberFormatException: An hex string representing bytes must have an even length
	at org.apache.cassandra.utils.Hex.hexToBytes(Hex.java:52)
	at org.apache.cassandra.db.marshal.BytesType.fromString(BytesType.java:65)
	... 36 more
{code}

Used to work before this fix. Works if I convert my String to hexBytes before. Is this a bug or expected behavior?","19/May/14 15:08;zznate;Your case was working because of this ""String.getBytes()"" bug. CfMetaData's key validator is used everywhere else in the code - essentially why you have to use the ""assume"" commands in cqlsh and the ""hex()"" function on the cassandra-cli. 

If you convert your string to the hex representation to pass to nodetool as an argument, it should work fine. ","19/May/14 15:15;pauloricardomg;yep, makes sense. worked when I used the hex bytes representation. thanks for the clarification!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in system.log,CASSANDRA-6211,12674276,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,ash2k,ash2k,17/Oct/13 08:06,12/Mar/19 14:10,13/Mar/19 22:29,23/Oct/13 12:30,2.0.2,,,,,,0,npe,nullpointerexception,,,"I wrote a stresstest to test C* and my code that uses CAS heavily. I see strange exception messages in logs:
{noformat}
ERROR [MutationStage:320] 2013-10-17 13:59:10,710 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:320,5,main]
java.lang.NullPointerException
ERROR [MutationStage:328] 2013-10-17 13:59:10,718 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:328,5,main]
java.lang.NullPointerException
ERROR [MutationStage:327] 2013-10-17 13:59:10,732 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:327,5,main]
java.lang.NullPointerException
ERROR [MutationStage:325] 2013-10-17 13:59:10,750 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:325,5,main]
java.lang.NullPointerException
ERROR [MutationStage:326] 2013-10-17 13:59:10,762 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:326,5,main]
java.lang.NullPointerException
ERROR [MutationStage:330] 2013-10-17 13:59:10,768 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:330,5,main]
java.lang.NullPointerException
ERROR [MutationStage:331] 2013-10-17 13:59:10,775 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:331,5,main]
java.lang.NullPointerException
ERROR [MutationStage:334] 2013-10-17 13:59:10,789 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:334,5,main]
java.lang.NullPointerException
ERROR [MutationStage:329] 2013-10-17 13:59:10,803 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:329,5,main]
java.lang.NullPointerException
ERROR [MutationStage:335] 2013-10-17 13:59:10,812 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:335,5,main]
java.lang.NullPointerException
ERROR [MutationStage:333] 2013-10-17 13:59:10,826 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:333,5,main]
java.lang.NullPointerException
ERROR [MutationStage:332] 2013-10-17 13:59:10,834 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:332,5,main]
java.lang.NullPointerException
ERROR [MutationStage:337] 2013-10-17 13:59:10,842 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:337,5,main]
java.lang.NullPointerException
ERROR [MutationStage:336] 2013-10-17 13:59:10,859 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:336,5,main]
java.lang.NullPointerException
ERROR [MutationStage:338] 2013-10-17 13:59:10,870 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:338,5,main]
java.lang.NullPointerException
ERROR [MutationStage:339] 2013-10-17 13:59:10,884 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:339,5,main]
java.lang.NullPointerException
ERROR [MutationStage:341] 2013-10-17 13:59:10,894 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:341,5,main]
java.lang.NullPointerException
ERROR [MutationStage:340] 2013-10-17 13:59:10,910 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:340,5,main]
java.lang.NullPointerException
ERROR [MutationStage:344] 2013-10-17 13:59:10,920 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:344,5,main]
java.lang.NullPointerException
{noformat}","java version ""1.7.0_25""
Java(TM) SE Runtime Environment (build 1.7.0_25-b15)
Java HotSpot(TM) 64-Bit Server VM (build 23.25-b01, mixed mode)

Linux hostname 2.6.32-279.el6.x86_64 #1 SMP Thu Jun 21 15:00:18 EDT 2012 x86_64 x86_64 x86_64 GNU/Linux
",,,,,,,,,,,,,,,,,,,,,,,23/Oct/13 08:08;slebresne;6211.txt;https://issues.apache.org/jira/secure/attachment/12609820/6211.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-21 04:03:54.505,,,no_permission,,,,,,,,,,,,353898,,,Wed Oct 23 12:30:35 UTC 2013,,,,,,0|i1p0in:,354190,2.0.1,,,,,,,jbellis,jbellis,,,,,,,,,,21/Oct/13 04:03;jbellis;I'm not sure how you've gotten an NPE logged without the rest of the stacktrace.  Are you doing anything unusual in deploying or configuring the log?,"21/Oct/13 04:30;ash2k;No, nothing special. Just a fresh deployment. By the way, client got no exceptions. All requests succeded and data was readable after that. So probably these exceptions are happening asynchronously to the request processing.","21/Oct/13 04:32;ash2k;Also, I can add that these exceptions happen periodically in ""chunks"" of ~20.","21/Oct/13 09:21;benedict;I've seen this before - especially with NPE the VM can optimise away the stack trace in certain cases (helpful, right?)   - seen it especially a problem in small highly parallelized workloads.

Try running with -XX:-OmitStackTraceInFastThrow to see if it helps
","23/Oct/13 04:13;ash2k;{noformat}
ERROR [MutationStage:27] 2013-10-23 10:11:35,507 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:27,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.serializers.UUIDSerializer.deserialize(UUIDSerializer.java:32)
	at org.apache.cassandra.serializers.UUIDSerializer.deserialize(UUIDSerializer.java:26)
	at org.apache.cassandra.db.marshal.AbstractType.compose(AbstractType.java:142)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getUUID(UntypedResultSet.java:127)
	at org.apache.cassandra.db.SystemKeyspace.loadPaxosState(SystemKeyspace.java:812)
	at org.apache.cassandra.service.paxos.PaxosState.propose(PaxosState.java:94)
	at org.apache.cassandra.service.paxos.ProposeVerbHandler.doVerb(ProposeVerbHandler.java:34)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
{noformat}","23/Oct/13 08:08;slebresne;When we load the paxos state, we assume that if the state is not empty then it must have an in_progress_ballot. But that's not guaranteed (at least not with the current code) since a node could get a commit without ever having seen the prepare/propose (it was dead for them or is just lagging behind). Besides, another reason (for having a non-empty state but no in_progress_ballot) is that we use TTL and the in_progress_ballot is the first thing that would TTL.

Anyway, attaching simple patch that test for the presence of in_progress_ballot.",23/Oct/13 12:18;jbellis;+1,"23/Oct/13 12:30;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
replace doesn't clean up system.peers if you have a new IP,CASSANDRA-6217,12674536,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,jjordan,jjordan,18/Oct/13 14:46,12/Mar/19 14:10,13/Mar/19 22:29,18/Oct/13 16:14,1.2.12,2.0.2,,,,,0,,,,,"When you use replace_token (or replace_node or replace_address) if the new node has a different IP, the old node will still be in system.peers",,,,,,,,,,,,,,,,,,,,,,,,18/Oct/13 15:52;brandon.williams;6217.txt;https://issues.apache.org/jira/secure/attachment/12609161/6217.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-18 15:52:39.332,,,no_permission,,,,,,,,,,,,354158,,,Fri Oct 18 16:14:31 UTC 2013,,,,,,0|i1p24f:,354450,1.2.10,,,,,,,jjordan,jjordan,,,1.2.0 beta 1,,,,,,,"18/Oct/13 15:52;brandon.williams;Existing nodes will already have the correct peers state due to the token conflict(s) from the replacer, but the replacer will still have the dead node in its own peers table.  The simplest thing to do is finish replacing by removing the replace_address from the table, since either it will be our own (which should not appear there) or it will be the old node.  Trivial patch to do so.",18/Oct/13 16:12;jjordan;+1 LGTM,18/Oct/13 16:14;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CqlParser throws StackOverflowError on bigger batch operation,CASSANDRA-5893,12663873,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,vmallet,vmallet,15/Aug/13 15:36,12/Mar/19 14:10,13/Mar/19 22:29,21/Aug/13 16:00,1.2.9,,,,,,0,,,,,"We are seeing a problem with CQL3/Cassandra 1.2.8 where a large batch operation causes the CqlParser to throw a StackOverflowError (-Xss180k initially, then -Xss325k).

Shouldn't a batch be processed iteratively to avoid having to bump stack sizes to unreasonably large values?

Here is more info from the original problem description:


<<<
It looks like the CqlParser in 1.2.8 (probably 1.2.x, but i didn't look) is implemented recursively in such a way that large batch statements blow up the stack. We, of course on a Friday night, have a particular piece of code that's hitting a degenerate case that creates a batch of inserts with a VERY large number of collection items, and it manifests as a StackOverflow coming out the cass servers:

java.lang.StackOverflowError
       at org.apache.cassandra.cql3.CqlParser.value(CqlParser.java:5266)
       at org.apache.cassandra.cql3.CqlParser.term(CqlParser.java:5627)
       at org.apache.cassandra.cql3.CqlParser.set_tail(CqlParser.java:4807)
       at org.apache.cassandra.cql3.CqlParser.set_tail(CqlParser.java:4813)
       at org.apache.cassandra.cql3.CqlParser.set_tail(CqlParser.java:4813)
       at org.apache.cassandra.cql3.CqlParser.set_tail(CqlParser.java:4813)
       at org.apache.cassandra.cql3.CqlParser.set_tail(CqlParser.java:4813)
       at org.apache.cassandra.cql3.CqlParser.set_tail(CqlParser.java:4813)
       at org.apache.cassandra.cql3.CqlParser.set_tail(CqlParser.java:4813)
	...
	
I think in the short term I can give up the atomicity of a batch in this code and kind of suck it up, but obviously I'd prefer not to. I'm also not sure if I kept a single batch, but split this into smaller pieces in each statement, whether that would still fail. I'm guessing I could also crank the hell out of the stack size on the servers, but that feels pretty dirty.

It seems like the CqlParser should probably be implemented in a way that isn't quite so vulnerable to this, though I fully accept that this batch is koo-koo-bananas.
>>>

Thanks!

 ",,,,,,,,,,,,,,,,,,,,,,,,18/Aug/13 14:44;iamaleksey;5893.txt;https://issues.apache.org/jira/secure/attachment/12598664/5893.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-15 15:49:02.878,,,no_permission,,,,,,,,,,,,343874,,,Mon Aug 26 19:05:58 UTC 2013,,,,,,0|i1navr:,344176,1.2.8,,,,,,,slebresne,slebresne,,,1.2.0,,,,,,,"15/Aug/13 15:49;jbellis;bq. Shouldn't a batch be processed iteratively

Probably, although batch is not intended for making ""bulk load"" atomic or other Really Large Sets of Rows.  Specifically, it's quite possible that this batch would time out even if it were parsed without issue.","17/Aug/13 04:49;jbellis;Hmm.

Can we at least catch the overflow and return InvalidRequest?","18/Aug/13 00:18;iamaleksey;Never mind what I said earlier, sorry. A misunderstanding on my part. The size of the batch doesn't matter, only the size of any set/map literal.

Stack overflow happens when parsing huge set and map literals (list literals are not affected). Starting at around 24k elements with the default 1.2 -Xss. Now, while you probably shouldn't be using literals this big, this *fits* within the 64k limit and should be supported.

And yeah, it can be done non-recursively. Will fix.",21/Aug/13 15:06;slebresne;+1,"21/Aug/13 16:00;iamaleksey;Committed, thanks.","26/Aug/13 19:05;vmallet;Great, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assertion error in 2.0.1 at db.ColumnSerializer.serialize(ColumnSerializer.java:56),CASSANDRA-6152,12672552,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,ThinkerFeeler,ThinkerFeeler,06/Oct/13 20:54,12/Mar/19 14:10,13/Mar/19 22:29,14/Oct/13 17:06,1.2.11,,,,,,0,,,,,"{noformat}
ERROR [COMMIT-LOG-WRITER] 2013-10-06 12:12:36,845 CassandraDaemon.java (line 185) Exception in thread Thread[COMMIT-LOG-WRITER,5,main]
java.lang.AssertionError
        at org.apache.cassandra.db.ColumnSerializer.serialize(ColumnSerializer.java:56)
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:77)
        at org.apache.cassandra.db.RowMutation$RowMutationSerializer.serialize(RowMutation.java:268)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:229)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:352)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.lang.Thread.run(Thread.java:722)
{noformat}","CentOS release 6.2 (Final)
With default set up on single node.
I also saw this exception in 2.0.0 on a three node cluster.",,,,,,,,,,,,,,,,,,,,,,,14/Oct/13 07:39;slebresne;6152.txt;https://issues.apache.org/jira/secure/attachment/12608250/6152.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-06 22:12:24.945,,,no_permission,,,,,,,,,,,,352175,,,Mon Oct 14 17:06:11 UTC 2013,,,,,,0|i1opwn:,352463,1.2.10,2.0.1,,,,,,jbellis,jbellis,,,1.2.0,,,,,,,06/Oct/13 20:56;ThinkerFeeler;The exception seems to happen first during a delete.  Let me know if you need more info.,"06/Oct/13 22:12;jbellis;That says you have an empty cell name, which is Not Supposed To Be Allowed.  Definitely need to know how you're doing this.","06/Oct/13 22:29;ThinkerFeeler;I was running a functional test suite, which populates some tables after deleting the old rows for the same keys. 

I ran it by a command like:
{noformat}
    repeat 10 ./run-test.sh 
{noformat}
So, it was deleting and writing rows in quick succession.  

If you want to see more detail than that, I'll see what I can provide.
","06/Oct/13 23:53;jbellis;As you can guess, we do have deletes in the unit and system tests, so yeah, details like ""here's the test script"" would be good!","09/Oct/13 18:59;ThinkerFeeler;The test just runs our test suite repeatedly. After a few runs it gets the following, with TRACE level. I'll document more later.
{noformat}
DEBUG [Native-Transport-Requests:11] 2013-10-09 11:40:49,459 Message.java (line 302) Received: PREPARE INSERT INTO as_reports.data_report_details(report_id,item_name,item_value) VALUES (7bc2a570-a42b-4632-b245-f0db9255ccc3,?,?);, v=1
TRACE [Native-Transport-Requests:11] 2013-10-09 11:40:49,460 QueryProcessor.java (line 208) Stored prepared statement ef4c655e042ffab2c1f0eef1e53a573e with 2 bind markers
DEBUG [Native-Transport-Requests:11] 2013-10-09 11:40:49,460 Tracing.java (line 157) request complete
DEBUG [Native-Transport-Requests:11] 2013-10-09 11:40:49,460 Message.java (line 309) Responding: RESULT PREPARED ef4c655e042ffab2c1f0eef1e53a573e [item_name(as_reports, data_report_details), org.apache.cassandra.db.marshal.UTF8Type][item_value(as_reports, data_report_details), org.apache.cassandra.db.marshal.UTF8Type] (resultMetadata=[0 columns]), v=1
DEBUG [Native-Transport-Requests:13] 2013-10-09 11:40:49,464 Message.java (line 302) Received: EXECUTE ef4c655e042ffab2c1f0eef1e53a573e with 2 values at consistency ONE, v=1
TRACE [Native-Transport-Requests:13] 2013-10-09 11:40:49,464 QueryProcessor.java (line 232) [1] 'java.nio.HeapByteBuffer[pos=0 lim=0 cap=0]'
TRACE [Native-Transport-Requests:13] 2013-10-09 11:40:49,464 QueryProcessor.java (line 232) [2] 'java.nio.HeapByteBuffer[pos=36 lim=41 cap=43]'
TRACE [Native-Transport-Requests:13] 2013-10-09 11:40:49,464 QueryProcessor.java (line 97) Process org.apache.cassandra.cql3.statements.UpdateStatement@321baa4a @CL.ONE
ERROR [COMMIT-LOG-WRITER] 2013-10-09 11:40:49,465 CassandraDaemon.java (line 185) Exception in thread Thread[COMMIT-LOG-WRITER,5,main]
java.lang.AssertionError
        at org.apache.cassandra.db.ColumnSerializer.serialize(ColumnSerializer.java:56)
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:77)
        at org.apache.cassandra.db.RowMutation$RowMutationSerializer.serialize(RowMutation.java:268)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:229)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:352)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.lang.Thread.run(Thread.java:722)
DEBUG [Native-Transport-Requests:13] 2013-10-09 11:40:49,466 Tracing.java (line 157) request complete
DEBUG [Native-Transport-Requests:13] 2013-10-09 11:40:49,466 Message.java (line 309) Responding: EMPTY RESULT, v=1
{noformat}",11/Oct/13 17:19;jbellis;I assume your test suite involves dropping and recreating the same tables?,"11/Oct/13 17:56;ThinkerFeeler;No, the test suite does *not* drop and create new tables (i.e., it does not call ""DROP TABLE"" and ""CREATE TABLE"").  It deletes rows from tables and re-inserts. I'm working right now on submitting a focused example that reproduces the bug.","11/Oct/13 20:36;ThinkerFeeler;I found a *simple* example of the bug.  

If I insert an empty string ("""") into the table it causes the AssertionError. If I insert a non-empty string there's no AssertionError!

{noformat}
create keyspace if not exists bug with replication = {'class':'SimpleStrategy', 'replication_factor':1};


create table if not exists bug.bug_table ( -- compact; column values are ordered by item_name
        report_id   uuid,
        item_name   text,
        item_value  text,
primary key (report_id, item_name)) with compact storage;
}
{noformat}

BugMain.java:
{noformat}
package bug;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

public class BugMain {
    private static String CASSANDRA_HOST = System.getProperty(""cassandraServer"",""172.17.1.169""); //""donalds01lx.uscorp.audsci.com"";
    private static BugInterface dao = new BugImpl(CASSANDRA_HOST);

    public static void bug() throws IOException {
        List<BugItem> items = new ArrayList<BugItem>();
        items.add(new BugItem("""",1,2,3));   // if you change the empty string """" to a non-empty string, the AssertionError goes away!
        items.add(new BugItem(""twp"",2,2,3));
        items.add(new BugItem(""three"",3,2,3));
        items.add(new BugItem(""four"",4,2,3));
        dao.saveReport(items);
    }
    
    public static void main(String [] args) throws IOException { 
       try {
           for(int i=0;i<1000;i++) {
               System.out.println(""\ndas: iteration "" + i + ""\n"");
               bug();
           }
       } finally {
           dao.shutdown();
       }
    }
}
{noformat}

BugItem.java:
{noformat}
package bug;

public class BugItem {
    public String name;
    public long long1; 
    public long long2;
    public long long3; 
    public BugItem(String string, long i, long j, long k) {
        name=string;
        long1 = i;
        long2= j;
        long3 = k;
    }
    public String toString() {return ""Item with name = "" + name + "", long1 = "" + long1 + "", long2 = "" + long2 + "", long3 = "" + long3;}
}
{noformat}

BugInterface.java:
{noformat}
package bug;

import java.util.List;


public interface BugInterface {
        public static final String VALUE_DELIMITER = "":"";
        public static final String HIERARCHY_DELIMITER = "" > "";
	void saveReport(List<BugItem> item);

	void connect();
	void shutdown();
}
{noformat}

BugImpl.java:
{noformat}
package bug;

import java.text.NumberFormat;
import java.util.List;
import java.util.UUID;

import org.apache.log4j.Logger;

import com.datastax.driver.core.Cluster;
import com.datastax.driver.core.PreparedStatement;
import com.datastax.driver.core.Session;
import com.datastax.driver.core.querybuilder.Insert;
import com.datastax.driver.core.querybuilder.QueryBuilder;

public class BugImpl implements BugInterface {
	private static final String CASSANDRA_NODE_PROPERTY=""CASSANDRA_NODE"";
	private static final Logger L = Logger.getLogger(new Throwable()
			.getStackTrace()[0].getClassName());
	private static final String KEYSPACE_NAME = ""bug"";
	private static final String REPORT_DATA_TABLE_NAME = ""bug_table"";
	private static NumberFormat numberFormat = NumberFormat.getInstance();
	private Cluster m_cluster;
	private Session m_session;
	private int m_writeBatchSize = 64;
	private String m_cassandraNode = ""<your cassandra hostname here>"";
	
	static {
		numberFormat.setMaximumFractionDigits(1);
	}

	public BugImpl() {
		m_cassandraNode=System.getProperty(CASSANDRA_NODE_PROPERTY, m_cassandraNode); // Get from command line
	}
	public BugImpl(String cassandraNode) {
		m_cassandraNode=cassandraNode;
	}
	@Override
	public void shutdown() {
		if (m_session!=null) {m_session.shutdown();}
		if (m_cluster!=null) {m_cluster.shutdown();}
	}
	@Override
	public void connect() {
		 m_cluster = Cluster.builder().addContactPoint(m_cassandraNode).build();
	     m_session = m_cluster.connect();
	}
	// ---------------------------------------------------------------------------------
	@Override
	public void saveReport(List<BugItem> items) {
		final long time1 = System.currentTimeMillis();
		if (m_session==null) {
			connect();
		}
		UUID reportId = UUID.randomUUID(); 
		saveReportAux(items,reportId);
		final long time2 = System.currentTimeMillis();
		L.info(""saveReport: t="" + numberFormat.format((double)(time2-time1) * 0.001) + "" seconds"");
	}
	
    public void saveReportAux(List<BugItem> items, UUID reportId) {
		int index = 0;
		int reportSize = items.size();
		int maxBatchedIndex = (int) (reportSize / m_writeBatchSize) * m_writeBatchSize;

		// prepare the batch statement
		final StringBuilder sb = new StringBuilder();
		sb.append(""BEGIN UNLOGGED BATCH "");
		for (int i = 0; i < m_writeBatchSize; i++) {
			sb.append(""insert into "" + KEYSPACE_NAME + "".""
					+ REPORT_DATA_TABLE_NAME
					+ "" (report_id, item_name, item_value) values ("" + reportId
					+ "",?,?);"");
		}
		sb.append(""APPLY BATCH;"");
		PreparedStatement insertBatchPrep = m_session.prepare(sb.toString());

		// prepare the single-insert statement for ""the rest"" that does not fit
		// in a whole batch
		final Insert writeReportData = QueryBuilder
				.insertInto(KEYSPACE_NAME, REPORT_DATA_TABLE_NAME)
				.value(""report_id"", reportId)
				.value(""item_name"", QueryBuilder.bindMarker())
				.value(""item_value"", QueryBuilder.bindMarker());
		final PreparedStatement writeReportDataPrep = m_session
				.prepare(writeReportData);

		// write the rows
		Object[] args = new Object[2 * m_writeBatchSize];
		int batchIndex = 0;
		int argsIndex = 0;
		for (final BugItem item : items) {
			if (index < maxBatchedIndex) {
				args[argsIndex++] = item.name;
				args[argsIndex++] = item.long1 + VALUE_DELIMITER
						+ item.long2 + VALUE_DELIMITER + item.long3;
				batchIndex++;
				if (batchIndex == m_writeBatchSize) {
					argsIndex = 0;
					batchIndex = 0;
					m_session.execute(insertBatchPrep.bind(args));
				}
			} else {
				m_session.execute(writeReportDataPrep.bind(item.name,
						item.long1 + VALUE_DELIMITER + item.long2 + VALUE_DELIMITER + item.long3));
			}
			index++;
		}
    }
   
    public void setWriteBatchSize(int size) {m_writeBatchSize=size;}
    public void setCassandraNode(String node) {m_cassandraNode=node;}
}
{noformat}","11/Oct/13 20:44;jbellis;Thanks, we'll have a look.","11/Oct/13 21:49;ThinkerFeeler;I have a hunch that when the column name is """" and the Memtable flushes to an SSTable is when this bug bites.   I notice it happens at about the same iteration of the *for* loop in BugMain.java.","14/Oct/13 07:42;slebresne;For some reason we weren't properly validating the cell names on inserts. In this case, it's a compact table with just one clustering column, so we cannot allow that clustering column to be the empty value. Patch attached to fix validation. This is not specific to 2.0 so the patch is against 1.2 (I've pushed a dtest for it too).","14/Oct/13 08:44;jbellis;Tactically, I think renaming column to cell in 1.2 is a bit out of place w/ the rest of the code, but the fix LGTM.","14/Oct/13 17:06;slebresne;Committed (with renaming only for 2.0), thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate pre-2.0 key/value/column aliases to system.schema_columns,CASSANDRA-6009,12668231,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,ksaritek,ksaritek,12/Sep/13 10:53,12/Mar/19 14:10,13/Mar/19 22:29,18/Sep/13 12:15,2.0.1,,,Legacy/Tools,,,0,,,,,"upgrade cassandra from 1.2.6 to 1.2.9 first,
then upgrade from 1.2.9 to 2.0.0 as documented at cassandra upgrade doc

describe command is not giving table definition properly

cqlsh:datadb> DESCRIBE KEYSPACE demoks ;

CREATE KEYSPACE demoks WITH replication = {
  'class': 'SimpleStrategy',
  'replication_factor': '2'
};

USE demoks;

CREATE TABLE demodb (
  description text,
  symbol text,
list index out of range
  PRIMARY KEY (cqlsh:datadb>","cassandra 2.0, jdk 7",,,,,,,,,,,,,CASSANDRA-8000,,,,,,,,,,17/Sep/13 17:51;iamaleksey;6009.txt;https://issues.apache.org/jira/secure/attachment/12603630/6009.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-12 16:58:21.645,,,no_permission,,,,,,,,,,,,348165,,,Wed Sep 18 12:15:55 UTC 2013,,,,,,0|i1o19r:,348461,2.0.0,,,,,,,slebresne,slebresne,,,,,,,,,,12/Sep/13 16:58;iamaleksey;[~ksaritek] Need the original schema to reproduce this issue.,"13/Sep/13 07:31;ksaritek;here it is,
	
CREATE KEYSPACE demoks WITH replication = {
  'class': 'SimpleStrategy',
  'replication_factor': '1'
};

USE demoks;

CREATE TABLE demodb (
  id int,
  time text,
  price double,
  volume double,
  PRIMARY KEY (id, time)
) WITH compaction =
    { 'class' : 'LeveledCompactionStrategy',  'sstable_size_in_mb' : 10 };
","13/Sep/13 19:49;iamaleksey;All right, this is legit. Fortunately, it's not a schema/migration issue. It's just cqlsh assuming a little too much. system.schema_columns doesn't have the values for clustering columns and partition key columns, if the tables are from pre-2.0 era.

DESCRIBE works fine with anything created in 2.0+, though.

Will fix.","14/Sep/13 15:25;ksaritek;is there a workaround to get schema properly, like get snapshot and create table & keyspace then bulk import via sstable","18/Sep/13 12:01;slebresne;+1 (but let's not commit in 2.1 since 2.0 will be an obligatory stop; startup is slow enough as it is)

[~ksaritek] Altering the tables on which cqlsh is unhappy should do the trick (you could alter the 'comment' property for instance).","18/Sep/13 12:15;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException on running instances,CASSANDRA-5673,12653871,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,analyst,analyst,20/Jun/13 06:39,12/Mar/19 14:10,13/Mar/19 22:29,12/Sep/13 07:34,1.2.6,,,,,,0,,,,,"Hallo,
We are having sporadic NullPointerException in some of the cassandra nodes in cluster (See stacktrace). 
We are having two Datacenter, each having 15 nodes with RF = 2, OS is SLES with java-1_6_0-ibm-1.6.0_sr12.0-0.5.1. 
At present only  workaround is to stop the application running on same node and run repair tool on cassandra. We are unable to identify the cause of error.

1)
INFO|ScheduledTasks:1|org.apache.cassandra.service.GCInspector|GC for MarkSweepCompact: 347 ms for 1 collections, 138398568 used; ma
x is 1051721728
2013-06-19T16:25:50:843|ERROR|ReplicateOnWriteStage:115|org.apache.cassandra.service.CassandraDaemon|Exception in thread Thread[ReplicateOnWriteStage:115,5,m
ain]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1582)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:897)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:919)
        at java.lang.Thread.run(Thread.java:738)
Caused by: java.lang.NullPointerException
        at java.util.TreeSet.iterator(TreeSet.java:230)
        at org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:163)
        at org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:64)
        at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:81)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:68)
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:274)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1357)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1214)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1126)
        at org.apache.cassandra.db.Table.getRow(Table.java:347)
        at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:64)
        at org.apache.cassandra.db.CounterMutation.makeReplicationMutation(CounterMutation.java:90)
        at org.apache.cassandra.service.StorageProxy$7$1.runMayThrow(StorageProxy.java:796)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1578)
        ... 3 more
2013-06-19T16:26:01:001|ERROR|ReadStage:4833|org.apache.cassandra.service.CassandraDaemon|Exception in thread Thread[ReadStage:4833,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1582)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:897)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:919)
        at java.lang.Thread.run(Thread.java:738)
Caused by: java.lang.NullPointerException
        at java.util.TreeSet.iterator(TreeSet.java:230)
        at org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:163)
        at org.

2)
2013-06-19T08:38:23:436| INFO|Thread-2447|org.apache.cassandra.service.StorageService|Starting repair command #2, repairing 1 ranges for keyspace system_auth
2013-06-19T08:58:25:685|ERROR|ReadStage:9270|org.apache.cassandra.service.CassandraDaemon|Exception in thread Thread[ReadStage:9270,5,main]
java.lang.NullPointerException
        at java.util.TreeSet.iterator(TreeSet.java:230)
        at org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:163)
        at org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:64)
        at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:81)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:68)
        at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:133)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1357)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1214)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1126)
        at org.apache.cassandra.db.Table.getRow(Table.java:347)
        at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:64)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:44)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:908)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:931)
        at java.lang.Thread.run(Thread.java:738)

Best Regards
Sanjay",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-06-20 16:27:08.541,,,no_permission,,,,,,,,,,,,334148,,,Thu Sep 12 07:34:23 UTC 2013,,,,,,0|i1ln3z:,334474,,,,,,,,,,,,,,,,,,,"20/Jun/13 16:27;jbellis;Please test 1.2.5, SSTableNamesIterator has changed substantially due to CASSANDRA-5492.",12/Sep/13 07:34;analyst;After Upgrading the problem seems to be fixed. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL/Thrift request hangs forever when querying more than certain amount of data,CASSANDRA-6407,12681357,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,ngrigoriev,ngrigoriev,26/Nov/13 19:55,12/Mar/19 14:10,13/Mar/19 22:29,11/Jan/14 01:28,2.0.5,,,,,,1,,,,,"I have a table like this (slightly simplified for clarity):

{code}
CREATE TABLE my_test_table (
		uid  uuid,
		d_id	 uuid,
		a_id	 uuid,	
		c_id	text,
		i_id	blob,	
		data	text,
		PRIMARY KEY ((uid, d_id, a_id), c_id, i_id)
);
{code}

I have created about over a hundred (117 to be specific) of sample entities with the same row key and different clustering keys. Each has a blob of approximately 4Kb.

I have tried to fetch all of them with a query like this via CQLSH:

{code}
select * from my_test_table where uid=44338526-7aac-4640-bcde-0f4663c07572 and a_id=00000000-0000-4000-0000-000000000002 and d_id=00000000-0000-1e64-0000-000000000001 and c_id='list-2'
{code}

This query simply hangs in CQLSH, it does not return at all until I abort it.

Then I started playing with LIMIT clause and found that this query returns instantly (with good data) when I use LIMIT 55 but hangs forever when I use LIMIT 56.

Then I tried to just query all ""i_id"" values like this:

{code}
select i_id from my_test_table where uid=44338526-7aac-4640-bcde-0f4663c07572 and a_id=00000000-0000-4000-0000-000000000002 and d_id=00000000-0000-1e64-0000-000000000001 and c_id='list-2'
{code}

And this query returns instantly with the complete set of 117 values. So I started thinking that it must be something about the total size of the response, not the number of results or the number of columns to be fetches in slices. And I have tried another test:

{code}
select cdata from my_test_table where uid=44338526-7aac-4640-bcde-0f4663c07572 and a_id=00000000-0000-4000-0000-000000000002 and d_id=00000000-0000-1e64-0000-000000000001 and c_id='list-2' LIMIT 63
{code}

This query returns instantly but if I change the limit to 64 it hangs forever. Since my blob is about 4Kb for each entity it *seems* like the query hangs when the total size of the response exceeds 252..256Kb. Looks quite suspicious especially because 256Kb is such a particular number. I am wondering if this has something to do with the result paging.

I did not test if the issue is reproducible outside of CQLSH but I do recall that I observed somewhat similar behavior when fetching relatively large data sets.

I can consistently reproduce this problem on my cluster. I am also attaching the jstack output that I have captured when CQLSH was hanging on one of these queries.","Oracle Linux 6.4, JDK 1.7.0_25-b15, Cassandra 2.0.2",,,,,,,,,,,,,,,,,,,,,,,26/Nov/13 19:58;ngrigoriev;cassandra.jstack.gz;https://issues.apache.org/jira/secure/attachment/12615897/cassandra.jstack.gz,09/Jan/14 03:00;ngrigoriev;cassandra.yaml;https://issues.apache.org/jira/secure/attachment/12622094/cassandra.yaml,09/Jan/14 02:59;ngrigoriev;cassandra6407test.cql.gz;https://issues.apache.org/jira/secure/attachment/12622093/cassandra6407test.cql.gz,09/Jan/14 23:23;xedin;disruptor-thrift-server-0.3.3-SNAPSHOT.jar;https://issues.apache.org/jira/secure/attachment/12622289/disruptor-thrift-server-0.3.3-SNAPSHOT.jar,09/Jan/14 03:19;ngrigoriev;system.log.gz;https://issues.apache.org/jira/secure/attachment/12622098/system.log.gz,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2014-01-09 01:18:53.29,,,no_permission,,,,,,,,,,,,360622,,,Sat Jan 11 01:28:46 UTC 2014,,,,,,0|i1q5yv:,360921,2.0.4,,,,,,,,,,,2.0.2,,,,,,,26/Nov/13 19:58;ngrigoriev;jstack output for Cassandra server process on the host where I run CQLSH,"09/Jan/14 00:38;ngrigoriev;Some additional details.

I can confirm that the problem is not limited to CQLSH, it can be reproduced via CQL/Thrift. Which does not surprise me, I was assuming that's what CQLSH is using today.

One of my coworkers has pointed out that he did not observe this problem in his small single-node cluster, even with larger amounts of data in one response. I was curious enough to try it so I have configured a single-node Cassandra 2.0.4 cluster on a spare Linux machine, loaded my schema there and generated the ""problematic"" test data set. I could not reproduce the problem, i.e. I was getting back much larger result set than in my larger cluster. After that I took my ""production"" cassandra.yaml, changed the cluster name to a dummy one, reinitialized that single-node cluster with new config, reloaded the data and I could immediately reproduce the problem. To keep long story short, I was comparing the parameters I changed in my config with the defaults and finally found THE parameter that is clearly responsible for this issue: rpc_server_type. If set to ""sync"", then I can query larger data set. If set to ""hsha"" - I can only query up to ~256Kb of data and then the connection gets stuck forever.

Anything obvious that I am missing about the limitations of hsha? ","09/Jan/14 00:48;ngrigoriev;It sounds somewhat related to:

CASSANDRA-4573

CASSANDRA-6373

",09/Jan/14 01:18;jbellis;/cc [~xedin],09/Jan/14 01:59;xedin;[~ngrigoriev] Is it possible to you to give us at least part of the data that you have been testing with? It sounds like it could be a bug in hsha implementation of the thrift server.,"09/Jan/14 03:09;ngrigoriev;[~xedin] I have prepared a simple test that does demonstrate the problem even in a small single-node cluster. Interestingly enough, with this test and such a small cluster with no load at all sometimes it actually works.

So, here is how I use it:

1. Set the RPC server type to hsha
2. Load the attached CQL ile
3. Use CQLSH
   use cassandra6407test ;
   select * from my_test_table ;

In most of the cases this SELECT gets stuck forever. Sometimes if you interrupt it (after a while) and do it again it actually returns all the data on the second attempt. Sometimes it does not. If you restart CQLSH and do it again - it will get stuck again. Specifying a LIMIT above 24-25 demonstrates similar behavior.

If you switch  RPC server type to ""sync"" and restart, then ""select * from my_test_table ;"" works all the time.

It almost feels like some sort of race condition or a timing issue somewhere between the part that produces the query result and the part that streams it back to the client.

The server config I have attached is simplified, I have disabled JNA, JEMalloc etc to have a configuration that is as close as possible to the default installation.","09/Jan/14 03:19;ngrigoriev;this is the DEBUG log -  I have tried that ""select *"" request 3 times after restarting the server with RPC server type set to hsha.","09/Jan/14 03:57;xedin;Thank you, [~ngrigoriev]! I will start working on this asap.",09/Jan/14 06:02;nickmbailey;This definitely sounds like the same thing as CASSANDRA-6373 given that it happens with vnode clusters which would make describe_ring return a fairly large response.,"09/Jan/14 06:11;xedin;Yeah, they look similar but at lease we can reproduce, can you try one more thing please while you are on it - disable vnodes and try hsha with your data?",09/Jan/14 06:17;nickmbailey;I'm not sure if you are asking me to try that in CASSANDRA-6373. I've verified it does not happen with vnodes disabled. The steps I described there should also let you reproduce the issue in a unit test.,"09/Jan/14 07:25;xedin;I was asking you to test your queries with hsha and vnodes disabled on a single node, but you mentioned right now that it works without vnodes so we are good, I will take it from here, thanks!","09/Jan/14 21:49;xedin;I identified the problem and fixing it right now, you won't need to wait for next Cassandra release as it's a problem with disruptor_thrift_server, so as a temporary solution you will be just need to drop in the updated jar into lib/.","09/Jan/14 22:32;ngrigoriev;[~xedin] Source patch will be OK too, whichever is simpler for you. We are building our Cassandra from source with two patches that are scheduled for 2.0.5. I do not mind rebuilding another dependency :) Thanks!","09/Jan/14 23:23;xedin;There is the updated jar, simply replace 0.3.2 (or remove 0.3.2) with this one in your Cassandra lib/.",10/Jan/14 00:46;ngrigoriev;I have tested the updated Thrift server with a single-node cluster using my test case and in my larger cluster with my original test - it seems to be working correctly now with large responses! Thanks!!!,"10/Jan/14 01:12;xedin;Great! I will wait for confirmation from CASSANDRA-6373, I think Nick is on it, and bump disruptor_thrift_server version in build.xml",11/Jan/14 01:28;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup fails with assertion error after stopping previous run,CASSANDRA-6774,12697419,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,keithwrightbos,keithwrightbos,26/Feb/14 18:50,12/Mar/19 14:10,13/Mar/19 22:29,18/Mar/14 08:48,2.1 beta2,,,,,,4,,,,,"I am stress testing a new 2.0.5 cluster and did the following:

- start decommission during heavy write, moderate read load
- trigger cleanup on non-decommissioning node (nodetool cleanup)
- Started to see higher GC load stop stopped cleanup via nodetool stop CLEANUP
- attempt to launch cleanup now fails with the following message in console.

Cassandra log shows: http://aep.appspot.com/display/cKmlMcDuKD72iYAcBykDuVZkRWY/",2.0.5,,,,,,,,,,,,,,,,,,,,,,,17/Mar/14 12:12;krummas;0001-6774-wip.patch;https://issues.apache.org/jira/secure/attachment/12635066/0001-6774-wip.patch,13/Mar/14 08:26;krummas;0001-Dont-continue-after-failing-to-cancel-in-progress-co.patch;https://issues.apache.org/jira/secure/attachment/12634393/0001-Dont-continue-after-failing-to-cancel-in-progress-co.patch,14/Mar/14 18:05;jbellis;6774-v2.txt;https://issues.apache.org/jira/secure/attachment/12634766/6774-v2.txt,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-03-12 08:40:56.019,,,no_permission,,,,,,,,,,,,375893,,,Tue Mar 18 08:48:53 UTC 2014,,,,,,0|i1srvz:,376189,,,,,,,,,,,,,,,,,,,"26/Feb/14 18:54;keithwrightbos;Additional info:

cqlsh:system> select * from compactions_in_progress limit 10;

 id                                   | columnfamily_name      | inputs                                                                                                                                                                                                                           | keyspace_name
--------------------------------------+------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------
 9a20d760-9f15-11e3-bfe7-fb46089edbb0 |            global_user | {16796, 17068, 17073, 17077, 17118, 17135, 17164, 17193, 17219, 17281, 17293, 17365, 17553, 17575, 17606, 17668, 17695, 17701, 17906, 17983, 18016, 18017, 18069, 18089, 18098, 18099, 18108, 18114, 18119, 18123, 18188, 18449} |         users
 93778f70-9f16-11e3-bfe7-fb46089edbb0 | client_end_user_lookup |                                                                                             {3280, 3281, 3283, 3284, 3286, 3293, 3297, 3298, 3301, 3302, 3303, 3306, 3323, 3324, 3326, 3328, 3329, 3330, 3331, 3332, 3333, 3334} |         users
 96d23e10-9f14-11e3-bfe7-fb46089edbb0 |     cookie_user_lookup |                                                                                                                     {9072, 9078, 9083, 9088, 9094, 9100, 9106, 9112, 9118, 9124, 9131, 9146, 9369, 9370, 9371, 9372, 9373, 9393} |         users

(3 rows)",26/Feb/14 19:14;keithwrightbos;Looks like restarting the node corrected the issue thus far as cleanup is running.,"12/Mar/14 08:40;vjevdokimov;Same issue on LCS, but restarting doesn't help. The only way to remove data after decreasing RF is Cleanup, but it fails with the same exception.
Is there a way to temporarily disable compactions on LCS?","12/Mar/14 09:11;dimchansky;Same issue here on LCS - after decreasing RF cleanup fails with the same stacktrace. Restart didn't help, after running ""nodetool disableautocompaction"" command and waiting for compaction get finished cleanup fails with the following stacktrace:
{quote}
Error occurred during cleanup
java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
        at java.util.concurrent.FutureTask.report(FutureTask.java:122)
        at java.util.concurrent.FutureTask.get(FutureTask.java:188)
        at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:227)
        at org.apache.cassandra.db.compaction.CompactionManager.performCleanup(CompactionManager.java:265)
        at org.apache.cassandra.db.ColumnFamilyStore.forceCleanup(ColumnFamilyStore.java:1115)
        at org.apache.cassandra.service.StorageService.forceKeyspaceCleanup(StorageService.java:2152)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
        at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1487)
        at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:848)
        at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at sun.rmi.transport.Transport$1.run(Transport.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:556)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:811)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:670)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
        at java.util.ArrayList.rangeCheck(ArrayList.java:635)
        at java.util.ArrayList.get(ArrayList.java:411)
        at org.apache.cassandra.db.compaction.CompactionManager.needsCleanup(CompactionManager.java:502)
        at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:540)
        at org.apache.cassandra.db.compaction.CompactionManager.access$400(CompactionManager.java:62)
        at org.apache.cassandra.db.compaction.CompactionManager$5.perform(CompactionManager.java:274)
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:222)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        ... 3 more
{quote}

*Update:*
IndexOutOfBoundsException exception issue has been fixed: [CASSANDRA-6845|https://issues.apache.org/jira/browse/CASSANDRA-6845]","13/Mar/14 08:25;krummas;Looks like the cause here is that the node is so heavily overloaded that cassandra could not cancel in-progress compactions quickly enough, the exception is harmless, but perhaps not very nice. 

Attaching patch to not continue with the cleanup (or, all operations that need all sstables marked as compacting) after failing to cancel in-progress compactions.

The IndexOutOfBoundsException error is unrelated and fixed in CASSANDRA-6845 as mentioned in a previous comment.","14/Mar/14 18:05;jbellis;I don't think we want to drop an exception into the log for what is to some degree an expected situation.  v2 refactors scrub/upgrade/cleanup to log an info message if they have to abort, instead.  (Also runs unmarkCompacting in a finally block, which was not the case before.)","17/Mar/14 08:25;krummas;hmm, i actually think we should throw an exception if we fail, so that the nodetool user gets some feedback without having to check logs, attached",17/Mar/14 08:35;krummas;or.. maybe we should return a status code from scrub/upgradesstables/cleanup instead... i'll fix,"17/Mar/14 12:12;krummas;Attaching patch that;
* Introduces an enum with the status for the operation (currently only aborted/successful)
* Makes cfs.markAllCompacting() return empty list if there are no sstables to execute the operation on and null if we failed cancelling
* Make nodetool output an error message and set exit code to 1 if we fail","17/Mar/14 13:16;jbellis;LGTM.

Are you still comfortable with 2.0 for this or should we do it in 2.1?","17/Mar/14 18:03;krummas;hmm i guess we shouldn't change the signatures of the exposed mbean methods in a minor rev

lets go 2.1 and leave 2.0 with the assertion, as it is mostly a cosmetic change

",18/Mar/14 08:48;krummas;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible Collections.sort assertion failure in STCS.filterColdSSTables,CASSANDRA-6483,12684385,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thobbs,graham sanderson,graham sanderson,13/Dec/13 00:42,12/Mar/19 14:10,13/Mar/19 22:29,13/Dec/13 23:13,2.0.4,,,,,,0,compaction,,,,"We have observed the following stack trace periodically:

{code}
java.lang.IllegalArgumentException: Comparison method violates its general contract!
        at java.util.TimSort.mergeLo(TimSort.java:747)
        at java.util.TimSort.mergeAt(TimSort.java:483)
        at java.util.TimSort.mergeCollapse(TimSort.java:410)
        at java.util.TimSort.sort(TimSort.java:214)
        at java.util.TimSort.sort(TimSort.java:173)
        at java.util.Arrays.sort(Arrays.java:659)
        at java.util.Collections.sort(Collections.java:217)
        at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.filterColdSSTables(SizeTieredCompactionStrategy.java:94)
        at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundSSTables(SizeTieredCompactionStrategy.java:59)
        at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundTask(SizeTieredCompactionStrategy.java:229)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:191)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
{code}

The comparator ant SizeTieredCompactionStrategy line 94 breaks the assertions in the new JDK7 default sort algorithm, because (I think just) the hotness value (based on meter) may be modified concurrently by another thread

This bug appears to have been introduced in CASSANDRA-6109
",,,,,,,,,,,,,,CASSANDRA-6109,CASSANDRA-8885,,,,,,,,,13/Dec/13 21:31;thobbs;6483-2.0-v1.patch;https://issues.apache.org/jira/secure/attachment/12618692/6483-2.0-v1.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-13 21:31:38.293,,,no_permission,,,,,,,,,,,,363457,,,Fri Dec 13 23:13:53 UTC 2013,,,,,,0|i1qnfb:,363763,,,,,,,,jbellis,jbellis,,,2.0.3,,,,,,,"13/Dec/13 00:43;graham sanderson;Note the java option java.util.Arrays.useLegacyMergeSort could be used as a workaround, but it is unclear to me if that would produce desirable results","13/Dec/13 00:46;graham sanderson;The simplest fix is probably just to precompute an IdentityMap to any of the mutable data, and use it from within the comparator (since the comparator happens to be non static)

Alternatively use a List of a new wrapper type and sort that instead.","13/Dec/13 03:53;graham sanderson;Adding my questions from dev-email thread

Note that the CASSANDRA-6109 feature claims to be “off” by default, however it isn’t immediately clear to me from that patch how “off” is implemented, and whether it is supposed to go down that code path even when “off""

I’m guessing there is no actual downside (other than ERROR level messages in the logs which cause alerts), since it just fails a subset of compactions?",13/Dec/13 21:31;thobbs;Attached patch 6483-2.0-v1.patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-6483]) builds a map of hotness values prior to the sort and uses that for comparisons.  I also made {{filterColdSSTables()}} skip unneeded work if we won't be able to filter anything anyway.,"13/Dec/13 21:37;thobbs;bq. Note that the CASSANDRA-6109 feature claims to be “off” by default, however it isn’t immediately clear to me from that patch how “off” is implemented, and whether it is supposed to go down that code path even when “off""

I answered this on the dev ML, but I'll repeat it here for others who are interested.  The default max_cold_reads_ratio is 0.0, so {{filterColdSSTables()}} shouldn't filter any SSTables.  When writing this patch, I realized that even with that set to 0.0, SSTables that have no read activity at all would still be filtered out.  However, after this patch, that's no longer true, and a setting of 0.0 will prevent any filtering at all.

bq. I’m guessing there is no actual downside (other than ERROR level messages in the logs which cause alerts), since it just fails a subset of compactions?

That's correct, this shouldn't cause any other problems, only delay some compactions.","13/Dec/13 23:13;jbellis;LGTM, committed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Division by zero Exception in HintedHandoff and CompactionExecutor,CASSANDRA-6403,12680902,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,gwicke,gwicke,23/Nov/13 21:53,12/Mar/19 14:10,13/Mar/19 22:29,13/Mar/14 21:49,2.0.4,,,,,,0,,,,,"In write load testing I'm getting division by zero exceptions after running for a while:

ERROR [HintedHandoff:2] 2013-11-23 20:44:41,411 CassandraDaemon.java (line 187) Exception in thread Thread[HintedHandoff:2,1,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.ArithmeticException: / by zero
        at org.apache.cassandra.db.HintedHandOffManager.doDeliverHintsToEndpoint(HintedHandOffManager.java:464)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:309)
        at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:92)
        at org.apache.cassandra.db.HintedHandOffManager$4.run(HintedHandOffManager.java:530)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
Caused by: java.util.concurrent.ExecutionException: java.lang.ArithmeticException: / by zero
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.HintedHandOffManager.doDeliverHintsToEndpoint(HintedHandOffManager.java:460)
        ... 6 more

ERROR [CompactionExecutor:8] 2013-11-23 21:34:01,493 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:8,1,RMI Runtime]
java.lang.ArithmeticException: / by zero
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable.<init>(ParallelCompactionIterable.java:59)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:126)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
        at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:296)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)


The nodes that encounter this error seem to hold onto a lot of memory which is not freed even after the write load is stopped. With the write load continuing they eventually run out of heap. nodetool compact dies with the same exception.","Cassandra 2.0.3 RC, Linux (Ubuntu Precise), OpenJDK 7",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-11-24 01:32:46.614,,,no_permission,,,,,,,,,,,,360167,,,Sun Nov 24 01:44:57 UTC 2013,,,,,,0|i1q36f:,360466,2.0.3,,,,,,,,,,,,,,,,,,"24/Nov/13 01:32;jbellis;Fixed in cassandra-2.0 HEAD.

Note that you probably shouldn't use multithreaded compaction.  (CASSANDRA-6142)","24/Nov/13 01:44;gwicke;Ahh, good to know. I suspected something along those lines, so started a run with multithreaded_compaction disabled. So far (four hours in) it is looking good.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BADNESS_THRESHOLD does not working correctly with DynamicEndpointSnitch,CASSANDRA-6683,12694216,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thobbs,kirill.sc,kirill.sc,10/Feb/14 08:11,12/Mar/19 14:10,13/Mar/19 22:29,25/Feb/14 16:45,2.0.6,2.1 beta2,,,,,0,snitch,,,,"There is a problem in *DynamicEndpointSnitch.java* in sortByProximityWithBadness()

Before calling sortByProximityWithScore we comparing each nodes score ratios to the badness threshold.
{code}
if ((first - next) / first >  BADNESS_THRESHOLD)
            {
                sortByProximityWithScore(address, addresses);
                return;
            }
{code}

This is not always the correct comparison because *first* score can be less than *next*  score and in that case we will compare a negative number with positive.

The solution is to compute absolute value of the ratio:
{code}
if (Math.abs((first - next) / first) > BADNESS_THRESHOLD)
{code}

This issue causing an incorrect sorting of DCs based on their performance and affects performance of the snitch.

Thanks.
 ",Linux 3.8.0-33-generic,,,,,,,,,,,,,,,,,,,,,,,20/Feb/14 22:44;thobbs;6683.patch;https://issues.apache.org/jira/secure/attachment/12630177/6683.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-10 13:05:46.14,,,no_permission,,,,,,,,,,,,372725,,,Tue Feb 25 16:45:36 UTC 2014,,,,,,0|i1s8f3:,373029,2.0.5,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,10/Feb/14 13:05;cburroughs;Earlier DES problems reported in CASSANDRA-6465,"10/Feb/14 15:00;brandon.williams;I don't think this should happen after CASSANDRA-6465, wdyt [~thobbs]?","11/Feb/14 21:14;thobbs;CASSANDRA-6465 doesn't affect what [~kirill.sc] is referring to.  However, when the score for {{first}} is less than {{next}}, the DES is behaving correctly.  Although {{first - next / first}} results in a negative number, that result is less than {{BADNESS_THRESHOLD}}, which results in the DES saying that {{first}} should be used.  This is the correct behavior because {{first}} has a lower score (less ""badness"") than {{next}}.

The whole point of this check is that the DES should only use something other than {{first}} if the score for {{next}} is lower (less bad) than the score for {{first}} by a certain margin (BADNESS_THRESHOLD).","12/Feb/14 08:46;kirill.sc;Thank you for your answer.

I have came across this part of code in DES because I have observed suboptimal choice of nodes in my configuration and started to investigate it. 

This is my config: 
* PropertyFileSnitch
* dynamic_snitch_badness_threshold 0.1, 
* 4 DCs, 
* keyspace with replication quota 1 for each DC. 
* Read repair and speculative_retry are disabled for my tables.
* Performing read operations with consistency TWO

I am observing that local DC that serves read request has about the same probability of asking any of the 3 remote replicas to confirm consistency TWO regardless of their score (is that correct?). 
Since all nodes are in different DCs {{subsnitch.sortByProximity}} places local node at the start of the list (first) but does not sort other remote DCs.
After {{subsnitch.sortByProximity}} addresses list with scores may look something like that:
- DC1: 0.1 (first)
- DC2: 0.7 
- DC3: 0.2
- DC4: 0.2

Since we are not calling {{sortByProximityWithScore}} we returning this list to {{AbstractReadExecutor getReadExecutor}} where {{consistencyLevel.filterForQuery}} (based on consistency TWO) picks up first 2 addresses from the list. As a result we are sending read request to suboptimal DC2.

By implementing my change ({{Math.abs()}}) I am seeing ~15% read throughput improvement in my setup with cassandra stress tool.

Due to my limited knowledge of Cassandra internals I am probably wrong to blame DES and BADNESS_THRESHOLD, but I would greatly appreciate if you could point out what is the correct behaviour in the situation above and which module is responsible for sorting nodes by the scores.

Thank you.",12/Feb/14 14:11;brandon.williams;You could test by disabling the dynamic snitch with {noformat}dynamic_snitch: false{noformat} so the sorting is always the same.,"12/Feb/14 16:58;thobbs;[~brandon.williams] was our motivation for not calling {{sortByProximityWithScore}} every time just the overhead of that operation? It seems like it shouldn't have a large impact unless the RF is high.  If we want to handle the high-RF case more efficiently, perhaps we could add a parameter that specifies how many of the replicas will be used (based on the consistency level) and just move the N lowest scores to the front if the first N scores aren't within BADNESS_THRESHOLD.","12/Feb/14 17:16;brandon.williams;bq. was our motivation for not calling sortByProximityWithScore every time just the overhead of that operation

I'm not sure what you mean exactly, we always end up calling it, we just don't check badness when it's set to zero.

bq. perhaps we could add a parameter that specifies how many of the replicas will be used (based on the consistency level)

Possible, but it'd be a lot of work, because it would change the snitch interface and we'd still need the old call because not all uses of it have a consistency level available.","12/Feb/14 18:18;thobbs;bq. I'm not sure what you mean exactly, we always end up calling it, we just don't check badness when it's set to zero.

I was a little confused on my last comment, but let me try again. Right now we only call {{sortByProximityWithScore()}} if {{BADNESS_THRESHOLD != 0}} and two neighbors in the list returned by the subsnitch differ by BADNESS_THRESHOLD.  I think it would make more sense (and fix Kirill's case) to always call {{sortByProximityWithScore()}} and then compare that ordering against the subsnitch list.  Something like this:

{noformat}
defaultOrder = subsnitch.sort(address, addresses);
scoredOrder = sortByProximityWithScore(address, addresses);  // make this return a new list instead of sorting in place
for (int i = 0; i < defaultOrder.size(); i++)
{
    if (scores.get(defaultOrder.get(i)) > scores.get(scoredOrder.get(i)) * (1 + BADNESS_THRESHOLD))
        return scoredOrder;
}
return defaultOrder;
{noformat}

bq. Possible, but it'd be a lot of work, because it would change the snitch interface and we'd still need the old call because not all uses of it have a consistency level available.

It looks like there aren't too many callers, so it shouldn't be that much work.  I would just make the arg optional and default it to the length of {{addresses}}.","12/Feb/14 18:29;brandon.williams;I see what you meant.  Yeah, I think it was done the way it is was as an optimization, though as you said it's probably not a huge one.","13/Feb/14 14:18;cburroughs;bq. I think it would make more sense (and fix Kirill's case) to always call sortByProximityWithScore() and then compare that ordering against the subsnitch list. 

FWIW Everyone I have shown this code to thought that's what it did based on the description and then spent a lot of time being puzzled when they realized it didn't.",20/Feb/14 22:44;thobbs;6683.patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-6683]) sorts the scores and compares each of them against the scores in the subsnitch ordering.,25/Feb/14 16:45;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Slice Queries Can Skip Intersecting SSTables,CASSANDRA-6825,12699517,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thobbs,wtmitchell3,wtmitchell3,07/Mar/14 21:57,12/Mar/19 14:10,13/Mar/19 22:29,02/Apr/14 20:03,2.0.7,2.1 beta2,,,,,0,,,,,"Investigating another problem, I needed to do COUNT(*) on the several partitions of a table immediately after a test case ran, and I discovered that count(*) on the full table and on each of the partitions returned different counts.  

In particular case, SELECT COUNT(*) FROM sr LIMIT 1000000; returned the expected count from the test 99999 rows.  The composite primary key splits the logical row into six distinct partitions, and when I issue a query asking for the total across all six partitions, the returned result is only 83999.  Drilling down, I find that SELECT * from sr WHERE s = 5 AND l = 11 AND partition = 0; returns 30,000 rows, but a SELECT COUNT(*) with the identical WHERE predicate reports only 14,000. 

This is failing immediately after running a single small test, such that there are only two SSTables, sr-jb-1 and sr-jb-2.  Compaction never needed to run.  

In selectrowcounts.txt is a copy of the cqlsh output showing the incorrect count(*) results.","quad core Windows7 x64, single node cluster
Cassandra 2.0.5",,,,,,,,,,,,,,,,,,,,,,,02/Apr/14 19:14;thobbs;6825-2.0-part2.txt;https://issues.apache.org/jira/secure/attachment/12638312/6825-2.0-part2.txt,02/Apr/14 16:16;thobbs;6825-2.0-v2.txt;https://issues.apache.org/jira/secure/attachment/12638280/6825-2.0-v2.txt,01/Apr/14 20:06;thobbs;6825-2.0.txt;https://issues.apache.org/jira/secure/attachment/12638122/6825-2.0.txt,07/Mar/14 22:08;wtmitchell3;cassandra.log;https://issues.apache.org/jira/secure/attachment/12633471/cassandra.log,07/Mar/14 22:11;wtmitchell3;selectpartitions.zip;https://issues.apache.org/jira/secure/attachment/12633472/selectpartitions.zip,07/Mar/14 21:57;wtmitchell3;selectrowcounts.txt;https://issues.apache.org/jira/secure/attachment/12633470/selectrowcounts.txt,21/Mar/14 08:09;wtmitchell3;testdb_1395372407904.zip;https://issues.apache.org/jira/secure/attachment/12635975/testdb_1395372407904.zip,21/Mar/14 03:55;wtmitchell3;testdb_1395372407904.zip;https://issues.apache.org/jira/secure/attachment/12635952/testdb_1395372407904.zip,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,2014-03-07 23:00:17.16,,,no_permission,,,,,,,,,,,,377864,,,Wed Apr 02 20:03:29 UTC 2014,,,,,,0|i1t3zr:,378156,,,,,,,,slebresne,slebresne,,,2.0.3,,,,,,,07/Mar/14 22:08;wtmitchell3;I've also attached the cassandra.log from the period during which the test was running.  ,"07/Mar/14 22:11;wtmitchell3;In selectpartitions.txt, one can see the full selects for each of the separate partition values.",07/Mar/14 23:00;jbellis;Is this a single-node cluster?,"07/Mar/14 23:25;wtmitchell3;Yes.  I've added that to the environment description.  

The partitioning is in the schema and in the code for when we move this to a non-trivial cluster.","08/Mar/14 02:04;jbellis;Can your team reproduce, Ryan?","08/Mar/14 13:43;wtmitchell3;If it helps in reproducing it, unlike my earlier report in CASSANDRA-6736, this failure and that of CASSANDRA-6826 appear in a small volume test, less than 100,000 rows total.  This lower number was being run in a JUnit test as part of a maven build of a complete product, such that the test keyspace and tables were created, but the row insertion did not begin until 9 minutes later.  So Cassandra is not noting these as high-volume activity, and the row width is not large enough to provoke incremental compaction, or in fact any compaction whatsoever.  ","10/Mar/14 19:33;rhatch;[~wtmitchell3] -- I'm working to reproduce this issue. To get as close to the mark can you provide me with a full schema for the table? (Mainly I'm interested in which columns are part of the primary key -- aside from the siteid, listid, and partition).","10/Mar/14 23:36;wtmitchell3;After shortening the column names, the schema is: CREATE TABLE sr (s bigint, l bigint, partition int, cd timestamp, ec text, ea text, properties text, rd timestamp, PRIMARY KEY ((s, l, p), cd, ec)) WITH CLUSTERING ORDER BY (cd DESC, ec ASC).","20/Mar/14 19:48;rhatch;Unfortunately I was not able to reproduce this issue. I tried cassandra 2.0.5 on linux, and also on Win7 (tried with the python driver, and cqlsh for checking the counts). I was using java 1.7.0_51.","20/Mar/14 20:24;wtmitchell3;I can confirm the problem is still there in 2.0.6.  As I was verifying that I could still reproduce CASSANDRA-6826, I checked for the COUNT(*) issue too.  In one of the tables six partitions, a COUNT(*) reported 10000 rows, but if I did a SELECT * in either ascending or descending order, cqlsh printed 20000 rows.  Would it help if I zipped up the data directory containing the table after the problem appeared?  Or would you need other information from the system directory, too, to see how the data is recorded? That might help in isolating how the problem arises.","20/Mar/14 22:39;thobbs;The overcounting problem seems to be limited to overwrites that end up in different SSTables (or a memtable).   If you write once, flush, and then overwrite, your count will be exactly 2x.","20/Mar/14 23:18;thobbs;Scratch that, I made a mistake in my test case (facepalm).  After fixing that, I'm not able to reproduce.  

[~billmichell]  A zip of your sstables could be useful if you can still provide that.","21/Mar/14 04:01;wtmitchell3;I've attached a testdb_1395372407904.zip of the data/testdb_1395372407904 directory after the test ran.  After the test completed, I did select * from sr and it returned 100000 rows:

cqlsh:testdb_1395372407904> select count(*) from sr limit 100000;

 count
--------
 100000

(1 rows)

When I did a select count(*) for each of the six partitions, they total only 90000:
cqlsh:testdb_1395372407904> select count(*) from sr where siteID = '4CA4F79E-3AB
2-41C5-AE42-C7009736F1D5' and listID = 24 and partition = 0 LIMIT 100000;

 count
-------
 20000

(1 rows)

cqlsh:testdb_1395372407904> select count(*) from sr where siteID = '4CA4F79E-3AB
2-41C5-AE42-C7009736F1D5' and listID = 24 and partition = 1 LIMIT 100000;

 count
-------
 20000

(1 rows)

cqlsh:testdb_1395372407904> select count(*) from sr where siteID = '4CA4F79E-3AB
2-41C5-AE42-C7009736F1D5' and listID = 24 and partition = 2 LIMIT 100000;

 count
-------
 10000

(1 rows)

cqlsh:testdb_1395372407904> select count(*) from sr where siteID = '4CA4F79E-3AB
2-41C5-AE42-C7009736F1D5' and listID = 24 and partition = 3 LIMIT 100000;

 count
-------
 10000

(1 rows)

cqlsh:testdb_1395372407904> select count(*) from sr where siteID = '4CA4F79E-3AB
2-41C5-AE42-C7009736F1D5' and listID = 24 and partition = 4 LIMIT 100000;

 count
-------
 10000

(1 rows)

cqlsh:testdb_1395372407904> select count(*) from sr where siteID = '4CA4F79E-3AB
2-41C5-AE42-C7009736F1D5' and listID = 24 and partition = 5 LIMIT 100000;

 count
-------
 20000

(1 rows)

As it turns out, the 10000 rows not counted were all from partition=2, and have a createDate identical except in the milliseconds to 10000 rows that do appear.  The common key values of the presumably uncounted rows (as they are the rows that did not return on the SELECT query, CASSANDRA-6826) are siteID=4CA4F79E-3AB2-41C5-AE42-C7009736F1D5,listID=24,partition=2,createDate=2014-03-20T22:27:26.457-0500. 
","21/Mar/14 08:21;wtmitchell3;Tyler, you used an interesting word, ""flush"".  After running a test with a different database name, I went back and looked at the first keyspace, as I did not drain the node before zipping the file the first time.  A third SSTable had now been written.  See the larger .zip file I have attached.  When I try the same statements through cqlsh, a SELECT * FROM sr WHERE ... AND partition = 2 now shows 20000 rows, but SELECT COUNT(*) FROM sr WHERE ... AND partition=2 still returns a count of 10000.  So the count is still incorrect.  ","21/Mar/14 20:16;thobbs;[~wtmitchell3] what type is the siteid column supposed to be?  So far I've tried varint, uuid, and text and had problems with each.   Just pasting ""DESCRIBE KEYSPACE testdb_xxxx"" from cqlsh would also work.","21/Mar/14 21:31;wtmitchell3;As it happens, I have that info handy as my JUnit testcase includes it in the log4j output:


CREATE TABLE testdb_1395374703023.sr (
    siteid text,
    listid bigint,
    partition int,
    createdate timestamp,
    emailcrypt text,
    emailaddr text,
    properties text,
    removedate timestamp,
    PRIMARY KEY ((siteid, listid, partition), createdate, emailcrypt)
) WITH CLUSTERING ORDER BY (createdate DESC, emailcrypt ASC)
   AND read_repair_chance = 0.1
   AND dclocal_read_repair_chance = 0.0
   AND replicate_on_write = true
   AND gc_grace_seconds = 864000
   AND bloom_filter_fp_chance = 0.01
   AND caching = 'KEYS_ONLY'
   AND comment = ''
   AND compaction = { 'class' : 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy' }
   AND compression = { 'sstable_compression' : 'org.apache.cassandra.io.compress.SnappyCompressor' };

(siteID was a BIGINT until recently when the schema was changed to TEXT to match the use of siteID elsewhere in the product.  I had not thought to represent our Java String as a Cassandra UUID.)","28/Mar/14 18:37;thobbs;[~wtmitchell3] thanks! I can reproduce the issue now, so I should be able to track down what's going on.","28/Mar/14 21:35;thobbs;It loos like CASSANDRA-6327 is the cause for this.  The logic for testing sstables for inclusion when there's a composite comparator and multiple components in the slice filter is off.  This showed up for {{count(\*)}} because counting queries are always paged internally; the second page was erroneously skipping an sstable.  If the {{select *}} query has the same page size (10k), it will also omit results.",01/Apr/14 15:49;slebresne;[~thobbs] Any insights on what it off in the logic exactly?,"01/Apr/14 16:28;thobbs;[~slebresne] the logic is primarily broken because it continues checking latter components after it knows that the first component intersects.  For example, suppose you have a slice of {{((1, 1), """")}}, min column names of {{(0, 2)}}, and max column names of {{(2, 3)}}.  The first component of the slice start falls within the min/max range; the second component does not.  Although the slice is _starting_ outside of the min/max range for the second component, it should be considered intersecting because we'll accept other values for the second component (for higher values of the first component).  The current logic sees that the second component doesn't fall within min/max and considers it non-intersecting.","01/Apr/14 20:06;thobbs;The attached patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-6825]) fixes {{CompositeType.intersects()}} and adds unit tests.  I also added some basic debug logging around query paging, since there wasn't anything there.","02/Apr/14 13:35;slebresne;I'm not sure the example above is really the one you meant (because I believe that the current code does return that there is intersection for that example), I think you meant something along the lines of a slice of say {{((1, 3), """")}} with min={{(0, 1)}} and max={{(3, 2)}}, but I agree there's a problem.

The patch looks mostly good, but regarding the ""optimization"" inner loop in CompositeType.intersects, I believe it could be simplified to:
{noformat}
for (int i = 0; i < Math.min(Math.min(start.length, finish.length), minColumnNames.size()); i++)
{
    AbstractType<?> t = types.get(i);
    if (t.compare(start[i], finish[i]) != 0)
        break;

    // we already know the first component falls within its min/max range (otherwise we wouldn't get here)
    if (i > 0 && !t.intersects(minColumnNames.get(i), maxColumnNames.get(i), start[i], finish[i]))
        continue outer;
}
{noformat}
Note in particular that we shouldn't assume that minColumnNames/maxColumnNames length is always bigger or equal that the slice bounds. But +1 if we agree on that change.
","02/Apr/14 16:16;thobbs;bq. I think you meant something along the lines of a slice of say {{((1, 3), """")}} with min={{(0, 1)}} and max={{(3, 2)}}

Oops, you're correct there.

Your simplification is good, except that the equality check for the start and finish needs to happen after the min/max intersects check.  Non-equal start/finish can still fall outside the min/max range.

The v2 patch uses the simplification with that correction.  (The branch is also updated.)","02/Apr/14 16:30;slebresne;bq. except that the equality check for the start and finish needs to happen after the min/max intersects check

Hum, you're right, it just felt less natural for me to check the equals afterwards for some reason, but fair enough, +1 on v2","02/Apr/14 18:21;thobbs;Thanks, committed.","02/Apr/14 19:14;thobbs;Looks like I missed one case, which ended up breaking KeyspaceTest.testLimitSSTableComposites().  In particular, the sstable-skipping optimization can't stop at {{min(min(start.length, finish.length), minColumnNames.size())}}, because it needs to handle starts and finishes of different lengths.

6825-2.0-part2.txt corrects that and adds another case to CompositeTypeTest to explicitly cover this.","02/Apr/14 19:22;slebresne;Good catch, +1","02/Apr/14 20:03;thobbs;Thanks, part2 patch committed as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
server side ClassCastException using compact storage,CASSANDRA-6813,12699273,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,pkendall,pkendall,06/Mar/14 21:56,12/Mar/19 14:10,13/Mar/19 22:29,07/Mar/14 12:24,2.0.6,,,,,,0,,,,,"The following snippet fails on the current 2.0 branch and succeeds on the 2.1 branch.

{code}
create KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};

USE test ;

CREATE TABLE lock (
	partition text,
	key text,
	owner text,
	PRIMARY KEY ((partition), key)
) WITH COMPACT STORAGE;

INSERT INTO lock(partition,key,owner) VALUES ('a','b',null) ;

UPDATE lock SET owner='z' WHERE partition='a' AND key='b' IF owner=null;
{code}

On the 2.0 branch a ClassCastException is logged in the server log file.

{code}
ERROR [ReadStage:244] 2014-03-07 09:38:47,227 CassandraDaemon.java (line 196) Exception in thread Thread[ReadStage:244,5,main]
java.lang.RuntimeException: java.lang.ClassCastException: org.apache.cassandra.db.marshal.BytesType cannot be cast to org.apache.cassandra.db.marshal.CompositeType
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1900)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.ClassCastException: org.apache.cassandra.db.marshal.BytesType cannot be cast to org.apache.cassandra.db.marshal.CompositeType
	at org.apache.cassandra.db.filter.SliceQueryFilter.columnCounter(SliceQueryFilter.java:231)
{code}",,,,,,,,,,,,,,,,,,,,,,,,07/Mar/14 11:03;slebresne;6813.txt;https://issues.apache.org/jira/secure/attachment/12633356/6813.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-07 11:03:06.78,,,no_permission,,,,,,,,,,,,377620,,,Fri Mar 07 12:24:50 UTC 2014,,,,,,0|i1t2hz:,377913,,,,,,,,iamaleksey,iamaleksey,,,2.0.5,,,,,,,"07/Mar/14 01:00;pkendall;BTW, this also works in 2.0 if not using compact storage.","07/Mar/14 11:03;slebresne;We indeed were not handling compact tables properly for CAS, relatively trivial patch attached. I've pushed a dtest for this too.",07/Mar/14 11:08;iamaleksey;+1,"07/Mar/14 12:24;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Please delete old releases from mirroring system,CASSANDRA-6490,12684870,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,tjake,sebb@apache.org,sebb@apache.org,16/Dec/13 00:21,12/Mar/19 14:09,13/Mar/19 22:29,22/Jun/15 16:10,,,,,,,0,,,,,"To reduce the load on the ASF mirrors, projects are required to delete old releases [1]

Please can you remove all non-current releases?
Thanks!
[Note that older releases are always available from the ASF archive server]

Any links to older releases on download pages should first be adjusted to point to the archive server.

[1] http://www.apache.org/dev/release.html#when-to-archive",http://www.apache.org/dist/cassandra/,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-12-16 10:39:35.796,,,no_permission,,,,,,,,,,,,363942,,,Wed Jun 24 14:55:04 UTC 2015,,,,,,0|i1qqef:,364248,,,,,,,,,,,,,,,,,,,"16/Dec/13 00:23;sebb@apache.org;The download page [1] shows that the only supported releases are 2.0.x 1.2.x and 1.1.x

Please delete all the older releases from

http://www.apache.org/dist/cassandra/

[1] http://cassandra.apache.org/download/","16/Dec/13 10:39;slebresne;Done ([~urandom], can you check the debian/dists/ directory and delete the 06x and 07x directories? I don't seem to have the right to do so and they don't point at anything existing anymore).","16/Dec/13 14:24;sebb@apache.org;There is a problem with the directory protections:

drwxrwxr-x  3 eevans     eevans     6 Aug 20  2012 06x
drwxrwxr-x  3 eevans     eevans     6 Aug 20  2012 07x
drwxr-xr-x  3 slebresne  cassandra  6 May 27  2013 11x
drwxr-xr-x  3 slebresne  cassandra  6 Nov 25 08:11 12x
drwxr-xr-x  3 slebresne  cassandra  6 Nov 25 08:40 20x
drwxrwxr-x  3 apbackup   cassandra  6 Sep 10  2012 sid
drwxrwxr-x  3 eevans     eevans     6 Aug 20  2012 unstable

Only 'sid' above is correct.

The file group should be cassandra, and files should be group-writable otherwise only the owner can change things.
Which is awkward when the individual is temporarily unavailable.

However please note that Infra are moving towards all projects using svnpubsub [1] for releases - which avoids all such issues.
I suggest you file an Infra request now so you are ready for the next release.

[1] http://www.apache.org/dev/release-publishing.html#distribution_dist","16/Dec/13 15:39;urandom;bq. Done (Eric Evans, can you check the debian/dists/ directory and delete the 06x and 07x directories? I don't seem to have the right to do so and they don't point at anything existing anymore).

Done.","16/Dec/13 16:00;sebb@apache.org;There's still a problem with some of the protections:

drwxr-xr-x  3 slebresne  cassandra  6 May 27  2013 11x
drwxr-xr-x  3 slebresne  cassandra  6 Nov 25 08:11 12x
drwxr-xr-x  3 slebresne  cassandra  6 Nov 25 08:40 20x

These should be changed - by slebresne - to allow group-write","16/Dec/13 16:37;slebresne;Right, right, fixed.","21/Jun/15 13:45;sebb@apache.org;There are several old releases still on the mirror system under

https://dist.apache.org/repos/dist/release/cassandra/

Please delete the following:

2.0.14
2.1.4
2.1.5

Also, please can you update the release process so that older releases are removed from the mirror system when a new release has been uploaded?
Deletes should be done a few days after the new release has been announced.","22/Jun/15 16:10;tjake;I've cleaned up the system but I will say, I like to include the latest and previous releases since when a release happens we wait to update the website till the mirrors are updated (otherwise users get a 404). If we push delete the old release and add the new then users get the worst of both worlds during the transition. 404 on the old versions and 404 on the new till all mirrors are updated.  ","24/Jun/15 14:55;sebb@apache.org;The recommended way to do this is as follows:

Following a successful release vote:
- publish the release artifacts (add to dist/release/cassandra)
- publish Maven artifacts via Nexus (if relevant)
- wait a day for mirrors to catch up (most will do so in less than a day)
- update the website download page to point to the new releases; older releases links should point to the archive server (this is published immediately)
- send the announce message
- a few days later, delete the older releases from dist/release/cassandra

That should not result in any 404s.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexHelper.skipBloomFilters won't skip non-SHA filters,CASSANDRA-5385,12639161,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,carlyeks,carlyeks,carlyeks,26/Mar/13 19:03,12/Mar/19 14:08,13/Mar/19 22:29,03/Apr/13 23:17,1.2.4,2.0 beta 1,,,,,0,,,,,"Currently, if the bloom filter is not of SHA type, we do not properly skip the bytes. We need to read out the number of bytes, as happens in the Murmur deserializer, then skip that many bytes instead of just skipping the hash size. The version needs to be passed into the method as well, so that it knows what type of index it is, and does the appropriate skipping.",,,,,,,,,,,,CASSANDRA-4885,,,,,,,,,,,,03/Apr/13 04:19;carlyeks;5385-v2.patch;https://issues.apache.org/jira/secure/attachment/12576713/5385-v2.patch,26/Mar/13 19:04;carlyeks;5385.patch;https://issues.apache.org/jira/secure/attachment/12575563/5385.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-03-26 19:16:46.997,,,no_permission,,,,,,,,,,,,319631,,,Sun Apr 21 22:50:06 UTC 2013,,,,,,0|i1j5fb:,319972,,,,,,,,jasobrown,jasobrown,,,,,,,,,,"26/Mar/13 19:16;jasobrown;LGTM. Nice catch, will commit in a few minutes. Thanks!

EDIT: actually, looking into a side question that carl raised about OOM problem.","27/Mar/13 01:40;jasobrown;After talking on the IRC with [~carlyeks], this does affect 1.2 (and starts there) so resetting the affects/fix versions","02/Apr/13 18:51;jasobrown;There is a problem with ScrubTest, and we decided, on the IRC, to can it for now. It will be rewritten as part of #4180.

Committed to cassandra-1.2 and trunk. Thanks, Carl!","03/Apr/13 04:19;carlyeks;The problem with the OOM is actually something that shows what is wrong (it's not a symptom). We can add back the scrub test, as it should pass now.

Before sstable version ia, the size of the bloom filter was written in the column header, along with the bloom filter. In 1.2, the Murmur3 partitioner was added, which dropped the size of the bloom filter as the first parameter. If we tried to read a bloom filter that was encoded in a pre-ia table, we would fail, skip it (using index helper), and it would skip the correct number of bytes. Now, we need to distinguish between the bloom filter stored in the sstable and the one stored in the filter component.

This patch fixes the LegacySSTableTest by distinguishing between the sstables written in the ia format (which didn't write the size of the bloom filter up front) and the pre-ia format.","03/Apr/13 13:39;jasobrown;Committed to trunk, and ScrubTest and LegacySSTableTest now pass. [~carlyeks], does this need to be applied to 1.2, as well? Seems like it should, but the ScrubTest and LegacySSTableTest pass on 1.2 without this.","03/Apr/13 23:16;jasobrown;Backported and committed to cassandra-1.2. Thanks for the effort, Carl!","19/Apr/13 15:34;jbellis;Hmm, looks like the backport blew away scrubtest on 1.2 as well.  Oversight?","19/Apr/13 17:41;jbellis;I'm confused by this whole ticket.  Which of these is incorrect?

# skipBloomFilters is only called on data in the Data componenet
# skipBloomFilters is only called against a 1.1 sstable
# 1.1 sstables only use Murmur2 Bloom filters

(Because if all of these are correct, then we shouldn't need the changes to IndexHelper introduced here.)","21/Apr/13 18:48;carlyeks;{quote}

Hmm, looks like the backport blew away scrubtest on 1.2 as well. Oversight?

{quote}
Yes, was an oversight.

Just for clarification, skipBloomFilters is old.

#1 and #2 are incorrect.

1: The skipBloomFilters gets called on the index file in RowIndexEntry line 104.
2: In 2.0, the skipBloomFilters will be called on a 1.2 sstable; also, the skipBloomFilters currently is called against any sstable when scrubbing.","21/Apr/13 22:50;jbellis;Ah, I see.  So, my assertions are correct for 1.2, which is what I've been working in lately.  I'll make sure it stays that way as I merge CASSANDRA-5497 and CASSANDRA-5492 forward.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integer overflow in OffHeapBitSet when bloomfilter > 2GB,CASSANDRA-5903,12664583,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,tdevelioglu,tdevelioglu,20/Aug/13 08:29,12/Mar/19 14:08,13/Mar/19 22:29,21/Aug/13 17:32,1.2.9,,,,,,0,patch,,,,"In org.apache.cassandra.utils.obs.OffHeapBitSet.

byteCount overflows and causes an IllegalArgument exception in Memory.allocate when bloomfilter is > 2GB.

Suggest changing byteCount to long.

{code:title=OffHeapBitSet.java}
    public OffHeapBitSet(long numBits)
    {
        // OpenBitSet.bits2words calculation is there for backward compatibility.
        int byteCount = OpenBitSet.bits2words(numBits) * 8;
        bytes = RefCountedMemory.allocate(byteCount);
        // flush/clear the existing memory.
        clear();
    }

{code}",,,,,,,,,,,,,,,,,,,,,,,,21/Aug/13 20:58;vijay2win@yahoo.com;0001-CASSANDRA-5903-check.patch;https://issues.apache.org/jira/secure/attachment/12599276/0001-CASSANDRA-5903-check.patch,20/Aug/13 20:33;vijay2win@yahoo.com;0001-CASSANDRA-5903.patch;https://issues.apache.org/jira/secure/attachment/12599022/0001-CASSANDRA-5903.patch,21/Aug/13 16:04;tdevelioglu;0002-CASSANDRA-5903.patch;https://issues.apache.org/jira/secure/attachment/12599214/0002-CASSANDRA-5903.patch,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-08-20 18:29:58.637,,,no_permission,,,,,,,,,,,,344526,,,Thu Aug 22 02:26:50 UTC 2013,,,,,,0|i1nevz:,344826,1.2.8,,,,,,,tdevelioglu,tdevelioglu,,,,,,,,,,"20/Aug/13 11:03;tdevelioglu;Added relevant stacktrace

DEBUG [CompactionExecutor:116] 2013-08-19 17:06:07,543 CompactionTask.java (line 115) Expected bloom filter size : 1830440832
ERROR [CompactionExecutor:116] 2013-08-19 17:06:07,584 CassandraDaemon.java (line 192) Exception in thread Thread[CompactionExecutor:116,1,main]
java.lang.IllegalArgumentException
        at org.apache.cassandra.io.util.Memory.allocate(Memory.java:58)
        at org.apache.cassandra.utils.obs.OffHeapBitSet.<init>(OffHeapBitSet.java:40)
        at org.apache.cassandra.utils.FilterFactory.createFilter(FilterFactory.java:143)
        at org.apache.cassandra.utils.FilterFactory.getFilter(FilterFactory.java:137)
        at org.apache.cassandra.utils.FilterFactory.getFilter(FilterFactory.java:126)
        at org.apache.cassandra.io.sstable.SSTableWriter$IndexWriter.<init>(SSTableWriter.java:446)
        at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:92)
        at org.apache.cassandra.db.ColumnFamilyStore.createCompactionWriter(ColumnFamilyStore.java:1983)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:143)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:211)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
","20/Aug/13 18:29;vijay2win@yahoo.com;I can change the byte count to long, 

As a side note, i am not sure if we are addressing the right issue. From the stack trace the byteCount should be 228805104 which is 228 MB (OpenBitSet.bits2words(1830440832L) * 8L) / ((1830440832L/64) * 8) which should fit in a integer.",20/Aug/13 19:14;jbellis;Hmm.  Is bits2words overflowing somehow?,"20/Aug/13 19:44;vijay2win@yahoo.com;Not sure yet, still trying to figure it out (Since i am more curious)... A simple test shows it might run out after 17B to 18B keys in a single SSTable (thats a giant SST) :)

{code}
        for (int i = 0; i < 30; i++) {
            long items = (i * 1000000000L);
            System.out.println(""Items: "" + items + "" byteCount: "" + (OpenBitSet.bits2words(items) * 8));
        }
{code}

{noformat}
Items: 0 byteCount: 0
Items: 1000000000 byteCount: 125000000
Items: 2000000000 byteCount: 250000000
Items: 3000000000 byteCount: 375000000
Items: 4000000000 byteCount: 500000000
Items: 5000000000 byteCount: 625000000
Items: 6000000000 byteCount: 750000000
Items: 7000000000 byteCount: 875000000
Items: 8000000000 byteCount: 1000000000
Items: 9000000000 byteCount: 1125000000
Items: 10000000000 byteCount: 1250000000
Items: 11000000000 byteCount: 1375000000
Items: 12000000000 byteCount: 1500000000
Items: 13000000000 byteCount: 1625000000
Items: 14000000000 byteCount: 1750000000
Items: 15000000000 byteCount: 1875000000
Items: 16000000000 byteCount: 2000000000
Items: 17000000000 byteCount: 2125000000
Items: 18000000000 byteCount: -2044967296
Items: 19000000000 byteCount: -1919967296
Items: 20000000000 byteCount: -1794967296
...
{noformat}","20/Aug/13 19:54;vijay2win@yahoo.com;Actually my calculations where wrong it does use 2 GB for 1830440832

long numElements = 1830440832L;
FilterFactory.getFilter(numElements, 0.01d, true);

fixing it.","20/Aug/13 20:33;vijay2win@yahoo.com;Simple fix for 1.2, it also catches for native OOM (I am neutral, i can also remove it so we fail fast) and throws a RTE to pause the compaction etc.",20/Aug/13 20:45;jbellis;+1,20/Aug/13 22:55;vijay2win@yahoo.com;Committed to 1.2 and merged into 2.0.0 -> 2.0 -> trunk. Thanks!,20/Aug/13 22:59;jbellis;Can you also add a CHANGES entry?,20/Aug/13 23:33;vijay2win@yahoo.com;Done! Thanks.,"21/Aug/13 15:07;tdevelioglu;Sadly that wasn't sufficient, there's another overflow in OffHeapBitSet.deserialize:

{code}
    public static OffHeapBitSet deserialize(DataInput dis) throws IOException
    {
        int byteCount = dis.readInt() * 8;
        Memory memory = RefCountedMemory.allocate(byteCount);
        for (int i = 0; i < byteCount;)
        {
            long v = dis.readLong();
            memory.setByte(i++, (byte) (v >>> 0));
            memory.setByte(i++, (byte) (v >>> 8));
            memory.setByte(i++, (byte) (v >>> 16));
            memory.setByte(i++, (byte) (v >>> 24));
            memory.setByte(i++, (byte) (v >>> 32));
            memory.setByte(i++, (byte) (v >>> 40));
            memory.setByte(i++, (byte) (v >>> 48));
            memory.setByte(i++, (byte) (v >>> 56));
        }
        return new OffHeapBitSet(memory);
    }
{code}","21/Aug/13 15:09;tdevelioglu;ERROR [SSTableBatchOpen:6] 2013-08-21 15:29:51,799 CassandraDaemon.java (line 192) Exception in thread Thread[SSTableBatchOpen:6,5,main]
java.lang.IllegalArgumentException
        at org.apache.cassandra.io.util.Memory.allocate(Memory.java:58)
        at org.apache.cassandra.utils.obs.OffHeapBitSet.deserialize(OffHeapBitSet.java:123)
        at org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:46)
        at org.apache.cassandra.utils.Murmur2BloomFilter$Murmur2BloomFilterSerializer.deserialize(Murmur2BloomFilter.java:40)
        at org.apache.cassandra.utils.FilterFactory.deserialize(FilterFactory.java:71)
        at org.apache.cassandra.io.sstable.SSTableReader.loadBloomFilter(SSTableReader.java:365)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:195)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:153)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:258)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
","21/Aug/13 16:06;tdevelioglu;Patch against 1.2, that fixes overflow in OffHeapBitSet.deserialize.

With both patches I was succesfully able to load a 2.3GB bloomfilter.","21/Aug/13 17:27;vijay2win@yahoo.com;Thanks Taylan, I will writeup a test case for it... The patch on 1.2 (0002) should handle up to 2GB * 8 over which we might want to serialize and deserialize into long for 2.1.","21/Aug/13 17:32;jbellis;LGTM, committed","21/Aug/13 17:33;jbellis;Oops, comment race condition w/ Vijay. :)  Test would still be nice.

16GB filter?  Well, maybe that's where we decide that just making bigger and bigger sstables is a bad idea...","21/Aug/13 20:55;vijay2win@yahoo.com;Not sure if we still need this patch, attaching it just in case :) Ignored the test since we need 4 GB to test it function.",21/Aug/13 21:36;jbellis;+1,"22/Aug/13 02:26;vijay2win@yahoo.com;Done, Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2.0 read performance is slower than 1.2,CASSANDRA-5933,12665355,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,enigmacurry,enigmacurry,24/Aug/13 00:56,12/Mar/19 14:08,13/Mar/19 22:29,14/Nov/13 05:28,,,,,,,0,,,,,"Over the course of several tests I have observed that 2.0 read performance is noticeably slower than 1.2

Example:

Blue line is 1.2, the rest are various forms of 2.0 rc1 (I've also seen this on rc2, just don't have a good graph handy)

!1.2-faster-than-2.0.png!
!1.2-faster-than-2.0-stats.png!

[See test data here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.eager_retry.node_killed.json&metric=interval_op_rate&operation=stress-read&smoothing=1]",,,,,,,,,,,,,,,,,,,,,,,,24/Aug/13 00:59;enigmacurry;1.2-faster-than-2.0-stats.png;https://issues.apache.org/jira/secure/attachment/12599767/1.2-faster-than-2.0-stats.png,24/Aug/13 00:56;enigmacurry;1.2-faster-than-2.0.png;https://issues.apache.org/jira/secure/attachment/12599766/1.2-faster-than-2.0.png,25/Sep/13 13:50;enigmacurry;5933-new-hardware.png;https://issues.apache.org/jira/secure/attachment/12605008/5933-new-hardware.png,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-09-03 02:46:24.652,,,no_permission,,,,,,,,,,,,345295,,,Thu Nov 14 05:28:47 UTC 2013,,,,,,0|i1njmf:,345596,2.0 rc1,2.0 rc2,,,,,,,,,,,,,,,,,"03/Sep/13 02:46;vijay2win@yahoo.com;Ryan, Do you mind testing the custom with 5 to 10 ms... 
I am thinking, we might need enough sample for Percentiles to make more sense (if conformed we might want to wait till the samples arrive etc).","03/Sep/13 03:02;enigmacurry;Hi [~vijay2win@yahoo.com], I'm not sure what you meant by 'custom with 5 to 10 ms'. Can you please clarify the test scenario you'd like me to run?
","03/Sep/13 03:15;vijay2win@yahoo.com;Hi Ryan, You can set a custom speculative execution like the below...
{code}
update column family Standard1 with speculative_retry=10ms;
{code}","03/Sep/13 03:21;enigmacurry;Ah, OK, I can run that test, that also applies to CASSANDRA-5932. However, in this case, I don't believe speculative retry can account for all the difference. The red line has none enabled.","25/Sep/13 13:49;enigmacurry;I'm seeing very contradictory results on different hardware.

!5933-new-hardware.png!

[data|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.20_read_regression_4.json&metric=interval_op_rate&operation=stress-read&smoothing=1]",14/Nov/13 05:10;jbellis;So did we bottom out at this not actually being a thing?,"14/Nov/13 05:28;enigmacurry;Yea, I haven't seen this behavior since I switched machines.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
move IndexSummary off heap,CASSANDRA-5521,12645079,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,jbellis,jbellis,28/Apr/13 14:42,12/Mar/19 14:08,13/Mar/19 22:29,03/May/13 08:42,2.0 beta 1,,,,,,0,,,,,IndexSummary can still use a lot of heap for narrow-row sstables.  (It can also contribute to memory fragmentation because of the large arrays it creates.),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-04-28 20:21:45.381,,,no_permission,,,,,,,,,,,,325441,,,Fri May 03 08:42:43 UTC 2013,,,,,,0|i1k59z:,325786,,,,,,,,jbellis,jbellis,,,,,,,,,,"28/Apr/13 20:21;vijay2win@yahoo.com;Pushed the changes to https://github.com/Vijay2win/cassandra/commits/5521

The idea is as mentioned in CASSANDRA-5506 comments, Since this ticket is marked for 2.0 i took the liberty of changing the index summary file format.
I am going to spend sometime on testing the performance difference (If any). Thanks!","29/Apr/13 03:23;jbellis;Seems inefficient to make the ""get pair"" the unit of fetching memory, since usually you want the key or the position but not both.  (You wouldn't have to change the signature of getKey either, if you just fetched the key into a byte[] directly instead of wrapping half of it.)

What's the upgrade path?  Would prefer ""automatically rebuilds old summaries"" to ""user has to manually blow away summaries or it dies trying to start.""","29/Apr/13 03:37;vijay2win@yahoo.com;{quote}
Seems inefficient to make the ""get pair"" the unit of fetching memory,
{quote}
Ahaa good point will fix it.

{quote}
What's the upgrade path? Would prefer ""automatically rebuilds old summaries"" to ""user has to manually blow away summaries or it dies trying to start.""
{quote}
It is automatic, but until the user runs scrub or until the new SST's are created the startup will be slow.","30/Apr/13 07:51;vijay2win@yahoo.com;Pushed an update to https://github.com/Vijay2win/cassandra/commits/5521_v2

This update doesn't use int[] which v1 used, v2 uses kind of offheap index over index summary which is stored at the beginning of the memory block... Moves index summary completely offheap... :)

I was not able to see any difference in performance between trunk and v2 using stress tool. 
Micro benchmark shows v2 is 6 seconds slower for 20 Bilion get's hence it is was not noticeable in stress tool, but the real benefit is less pauses and more data a node can hold...","01/May/13 03:53;xedin;Have we considered using vint encoding on those arrays as we keep them in memory anyway to minimize space consumption?

Edit: i remember now why that is not a good idea :) I wonder though how what could memory footprint be if we use TreeMap inside and keys and offsets (in vint encoding) saved in native memory...","01/May/13 06:17;xedin;I see v2 does byte[] allocation on every getKey(int) call which would be happening very frequently due to binary search which happens on very index lookup. So I don't think there is any real benefit in terms of GC friendliness from moving off-heap in this case as we have to copy data over multiple times anyway.

As an alternative to Unsafe we can try hybrid approach - identify if JNA is present and put summaries off-heap (using JNA's Memory) in combination with Pointer.getByteBuffer() which doesn't copy any data around but instead creates direct ByteBuffer, otherwise have IndexSummary on-heap but split byte[][] and long[] into pages so we don't have to allocate contiguous space for big SSTables which would be much GC friendlier. 

","01/May/13 13:46;jbellis;As Pavel notes, the most important use of getKey is the one in binarySearch.  But here we only care about the comparison, we don't actually need the artifact of a ByteBuffer.  So why not compare directly without creating a buffer first?  No buffer at all is even cheaper than a native buffer.

(This would also mean that we only need to look at as many bytes as it takes before the first difference is found.)","01/May/13 16:12;jbellis;bq. So why not compare directly without creating a buffer first?

That doesn't quite work, since we need to compare the Token, not the key itself.

So, revised suggestion: allow the partitioner to decorate a Memory location, instead of forcing us to create a ByteBuffer first.","01/May/13 23:54;xedin;if we do so we would have to change partitioner interface to decorateKey and getToken, DecoratedKey to have two ""key"" fields and change at least MurmurHash to accept both BB and M at the same time. I'm my opinion it's just too many changes just because we don't require JNA but with hybrid approach we don't have to do any of that work. Besides mentioned users would want to run with JNA in production anyway.","02/May/13 01:58;jbellis;Just add a IPartitioner.compareToken method that does what we need, then.  Much better than creating extra objects that we don't care about.

I like where we are now, with JNA being mostly optional (more so in 2.0 where we require Java7, so we don't need JNA for snapshot).  Remember, we don't support JNA at all on Windows.  I'd rather use less JNA than more.","02/May/13 02:11;xedin;But don't you still have to generate token from Memory and make changes to getToken(BB) and underlying methods or was is proposed interface for compareToken?

bq. I like where we are now, with JNA being mostly optional (more so in 2.0 where we require Java7, so we don't need JNA for snapshot). Remember, we don't support JNA at all on Windows. I'd rather use less JNA than more.

I just to clarify, the only thing we need from JNA in this case is Pointer.getByteBuffer() which is actually a JNI method but unfortunately Unsafe doesn't have it somehow :(

I agree tho that we should rely less on JNA but in this case we still pay the price of memory usage even putting off-heap so in non-JNA case we make it GC/allocation friendly it still makes a good improvement overall with almost no code changes.","02/May/13 02:37;jbellis;Thinking about it, {{getToken(Memory, offset, length)}} is probably the right thing to add to IPartitioner.  The rest can live in IndexSummary.  That doesn't sound like a huge burden to me.  And as I said, creating no Buffer (or DK) objects is better than creating native ones. :)","02/May/13 03:08;xedin;So tokens that keep bb around right now would have to keep Memory and offset, size references? I'm not against this, just trying to clarify to myself if we would rather want to keep some kind of ROBuffer container for DK and Token to unify interface...","02/May/13 03:23;jbellis;No, the Token wouldn't share any bytes with the key (with the exception of BOP, and I don't care about optimizing for that case), so there's no reason to not create a regular, on-heap Token.

Edit: keeping in mind that we don't actually need a full DK object, we just need a Token.","02/May/13 04:03;vijay2win@yahoo.com;Honestly, glad to see the thread going in the same thinking process which i went though.... 

Changing the Partitioner is a bigger change... but before we go there, wondering if this optimization is going to help us? 
For BB is not cheap, but it is going to be good garbage which will live and die in young generation.

I can think of 2 other options...
1) We can serialize and deserialize Token in IndexSummary we still need additional function to serialize and deserialize from memory (for BOP we can serialize the key/byte[], we have also removed the token calculation overhead) so we can also try and compare incrementally.
2) We can use MMappedFile instead and get ByteBuffer (this could work in our favor, for the new SST's which is never queried there is zero overhead in memory ) :)","02/May/13 04:56;xedin;bq. For BB is not cheap, but it is going to be good garbage which will live and die in young generation.

Indeed, those are just containers so actual data is not copied and those buffers pretty GC friendly as you mentioned. But I think that option with using Memory with token is ok if we can encapsulate it properly.","02/May/13 16:17;jbellis;bq. Changing the Partitioner is a bigger change

It does get ugly since you'd need to reimplement Murmur3.hash3_x64_128 on Memory objects.  (Not for the first time, I'm pissed that ByteBuffer isn't an interface...)

Let's go ahead and move forward with v2 and optimize later if we need to.

Nits:
# rename hasSummaries to offHeapSummaries
# InputStream.read has an overload that takes a length parameter, you don't need to realloc the buffer
# The comment doesn't match the code here.  Also, getIndex should be private.
{code}
.       // multiply by 4 and add the block start
        return bytes.getInt(index << 2);
{code}
# We can easily inline DK.compareTo instead of actually creating a DK object (i.e., call partitioner.getToken instead, then compare the tokens and keys without the DK wrapper)

The rest LGTM.
","03/May/13 08:42;vijay2win@yahoo.com;Committed to Trunk!

Added following function to DK to support RowPosition

{code}
public static int compareTo(IPartitioner partitioner, ByteBuffer key, RowPosition position)
    {
        // delegate to Token.KeyBound if needed
        if (!(position instanceof DecoratedKey))
            return -position.compareTo(partitioner.decorateKey(key));

        DecoratedKey otherKey = (DecoratedKey) position;
        int cmp = partitioner.getToken(key).compareTo(otherKey.getToken());
        return cmp == 0 ? ByteBufferUtil.compareUnsigned(key, otherKey.key) : cmp;
    }
{code}

Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updating Pig to 0.11.1 breaks the existing Pig driver,CASSANDRA-6213,12674372,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,alexliu68,alexliu68,alexliu68,17/Oct/13 18:45,12/Mar/19 14:08,13/Mar/19 22:29,17/Oct/13 19:06,,,,,,,0,,,,,"Current trunk upgrades Pig to 0.11.1 which causes the Pig storages code can't compile. Pig storages need implement the new API method cleanupOnSuccess(String,Job).",,,,,,,,,,,,,,,,,,,,,,,,17/Oct/13 18:55;alexliu68;6213-trunk.txt;https://issues.apache.org/jira/secure/attachment/12608999/6213-trunk.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-17 19:06:19.972,,,no_permission,,,,,,,,,,,,353994,,,Tue Jan 07 15:09:17 UTC 2014,,,,,,0|i1p13z:,354286,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,17/Oct/13 18:56;alexliu68;Fixed compiling error in trunk,17/Oct/13 19:06;brandon.williams;Committed,07/Jan/14 10:28;jeromatron;[~alexliu68] [~brandon.williams] Is there a reason this can't get merged into 2.0 so that 2.0 can use Pig 0.11.1 and 0.12.0?,"07/Jan/14 14:25;brandon.williams;We aren't going to change a lib version in a minor release.  We could backport this, but since we build with pig you'd have to roll your own custom release to use 0.11 or 0.12.",07/Jan/14 14:42;jeromatron;ok.  unfortunate but makes sense since we've had issues with updating dependency libs in a minor release in the past.,07/Jan/14 14:52;jeromatron;Can we update our dependency to 0.12 so that we're current with the pig stable releases before 2.1 is released?,07/Jan/14 15:09;jeromatron;I created a separate ticket so we can revisit upgrading before going to beta with 2.1: CASSANDRA-6556.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Don't exchange schema between nodes with different versions (no pull, no push)",CASSANDRA-6695,12694757,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,pkolaczk,pkolaczk,12/Feb/14 14:57,12/Mar/19 14:08,13/Mar/19 22:29,12/Feb/14 16:42,1.2.16,2.0.6,2.1 beta1,,,,0,,,,,"Subject. Don't push schema to unknown-, or differently major-versioned nodes, and don't pull schema from them, either.

Since we don't support schema altering during upgrade, and adding nodes during cluster upgrades is also a non-recommended thing, this is what we are going to do.

Until CASSANDRA-6038, that is.
",,,,,,,,,,,,,,,,,,,,,,,,12/Feb/14 16:19;iamaleksey;6695-v2.txt;https://issues.apache.org/jira/secure/attachment/12628509/6695-v2.txt,12/Feb/14 15:03;pkolaczk;6695.patch;https://issues.apache.org/jira/secure/attachment/12628497/6695.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-02-12 16:27:08.847,,,no_permission,,,,,,,,,,,,373265,,,Wed Feb 12 16:42:21 UTC 2014,,,,,,0|i1sbqf:,373566,,,,,,,,pkolaczk,pkolaczk,,,,,,,,,,"12/Feb/14 16:27;iamaleksey;Hijacking this issue to make broader changes, and apply them to 1.2 as well. Also reclassifying this as an improvement, just because.",12/Feb/14 16:28;pkolaczk;+1 LGTM :),"12/Feb/14 16:42;iamaleksey;Committed, thanks (:",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool repair is taking a long time. ,CASSANDRA-6616,12691072,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,dharsanl,dharsanl,24/Jan/14 16:47,12/Mar/19 14:07,13/Mar/19 22:29,19/Aug/14 15:27,,,,,,,0,,,,,"We have a two  nodes cluster with the replication factor of 2.   The db has more than 2500 column families(tables).   The 'nodetool -pr -par' repair on an empty database(one or table has a litter data) takes about 30 hours to complete.  
 
",Cassandra Version 2.0.4 running on Redhat Version 6,,,,,,,,,,,,,,,,,,,CASSANDRA-6566,CASSANDRA-6455,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-01-24 21:00:37.162,,,no_permission,,,,,,,,,,,,369815,,,Tue Aug 19 15:27:24 UTC 2014,,,,,,0|i1rqm7:,370117,2.0.9,,,,,,,,,,,,,,,,,,"24/Jan/14 21:00;brandon.williams;This is because the CFs are being processed sequentially. CASSANDRA-6566 may help some, but if all of these CFs did have data, processing them in parallel would be detrimental to IO.  It would be better to have much less CFs.","27/Jan/14 19:38;dharsanl;Hi Brandon,

In our DB 99% of the tables are empty. Can Cassandra ignore the empty tables and run the repair only on the tables that contain data?. 

Thanks
Dharsan
 

",27/Jan/14 19:52;brandon.williams;Pass the ones that have data explicitly to nodetool.,"27/Jan/14 20:30;dharsanl;Thanks Brandon,

Is there any  easy way in C* finding out which tables are empty. I don't want run count on each table to find out which ones are empty.

Dharsan

 

",27/Jan/14 20:58;brandon.williams;cfstats will tell you,"27/Jan/14 21:08;dharsanl;Is the cfstats accurate?.  

Thanks
Dharsan


",27/Jan/14 21:38;brandon.williams;Accurate within index_interval.  For zero it's perfectly accurate.,"14/Feb/14 23:31;kohlisankalp;CASSANDRA-6455 will fix this problem as the JIRA will allow multiple CFs to run in parallel. 
","19/Aug/14 15:27;dharsanl;This is not resolved,",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expired cells not converted to deleted cells during cleanup/scrub/compaction (sometimes),CASSANDRA-6844,12700975,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,12/Mar/14 13:59,12/Mar/19 14:07,13/Mar/19 22:29,12/Mar/14 17:12,2.0.7,2.1 beta2,,,,,0,,,,,"On read path, we intentionally read expiring cells as expiring cells, never converting them to tombstones, to return consistent results to the clients.

For compaction/cleanup/scrub we don't care about that, and are supposed to make the conversion as an optimization. However, somewhere along the way it got lost, and SSTII doesn't always do it now in 2.0 (and never does it in 2.1).

SSTI.getColumnFamilyWithColumns() passes the correct expireBefore(), but SSTI.next() does not, and while we use both in 2.0, depending on the some factors, in 2.1 we only use SSTI.next(), and SSTI.getColumnFamilyWithColumns() is actually dead code (I will remove it during the 2.1 merge).",,,,,,,,,,,,,,,,,,,,,,,,12/Mar/14 14:00;iamaleksey;6844.txt;https://issues.apache.org/jira/secure/attachment/12634169/6844.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-12 15:01:19.319,,,no_permission,,,,,,,,,,,,379321,,,Wed Mar 12 17:12:40 UTC 2014,,,,,,0|i1tcyn:,379613,,,,,,,,slebresne,slebresne,,,,,,,,,,12/Mar/14 14:01;iamaleksey;Trivial patch attached.,"12/Mar/14 15:01;slebresne;+1, good catch.","12/Mar/14 17:12;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup fails with IndexOutOfBoundsException exception,CASSANDRA-6845,12701001,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,dimchansky,dimchansky,dimchansky,12/Mar/14 16:05,12/Mar/19 14:07,13/Mar/19 22:29,12/Mar/14 17:19,2.0.7,,,,,,0,,,,,"Originally we had Cassandra cluster with the following configuration:
{quote}
Initial tokens:
10.1.11.51: -9223372036854775808
10.1.11.52: 0
10.2.11.51: -9223372036854775807
10.2.11.52: 1

Endpoint snitch: PropertyFileSnitch
10.1.11.51=DC1:RAC1
10.1.11.52=DC1:RAC1 
10.2.11.51=DC2:RAC1
10.2.11.52=DC2:RAC1
{quote}
We have created keyspace with network topology strategy and replication factor 2:
{code}
CREATE KEYSPACE our_keyspace WITH replication = {
  'class': 'NetworkTopologyStrategy',
  'DC2': '2',
  'DC1': '2'
};
{code}
All column families have been created with SizeTiered compaction strategy. Аfter a while column families have been migrated to LeveledCompactionStrategy and then replication factor was decreased to 1.

We started cleanup, but on non-seed nodes (10.1.11.52,10.2.11.52) it failed with assertion error. Assertion stacktrace was the same as described in [CASSANDRA-6774|https://issues.apache.org/jira/browse/CASSANDRA-6774] issue.
Then we've stopped all compactions, started cleanup again and got another exception:
{quote}
Error occurred during cleanup
java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
        at java.util.concurrent.FutureTask.report(FutureTask.java:122)
        at java.util.concurrent.FutureTask.get(FutureTask.java:188)
        at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:227)
        at org.apache.cassandra.db.compaction.CompactionManager.performCleanup(CompactionManager.java:265)
        at org.apache.cassandra.db.ColumnFamilyStore.forceCleanup(ColumnFamilyStore.java:1115)
        at org.apache.cassandra.service.StorageService.forceKeyspaceCleanup(StorageService.java:2152)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
        at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1487)
        at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:848)
        at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at sun.rmi.transport.Transport$1.run(Transport.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:556)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:811)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:670)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
        at java.util.ArrayList.rangeCheck(ArrayList.java:635)
        at java.util.ArrayList.get(ArrayList.java:411)
        at org.apache.cassandra.db.compaction.CompactionManager.needsCleanup(CompactionManager.java:502)
        at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:540)
        at org.apache.cassandra.db.compaction.CompactionManager.access$400(CompactionManager.java:62)
        at org.apache.cassandra.db.compaction.CompactionManager$5.perform(CompactionManager.java:274)
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:222)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        ... 3 more
{quote}
It seems there is typo in *needsCleanup* method of *CompactionManager* class and the supplied patch should fix the issue.
Get method on the line *502* (*CompactionManager.java*) throws exception, because of the wrong comparison on the line *496*: there should be *sortedRanges* size check, not *ownedRanges*.","CentOS Linux 2.6.32-431.el6.x86_64
Datastax Cassandra 2.0.5-1",,,,,,,,,,,,,,,,,,,,,,,12/Mar/14 16:05;dimchansky;0001-fixed-bug-in-CompactionManager.needsCleanup.patch;https://issues.apache.org/jira/secure/attachment/12634193/0001-fixed-bug-in-CompactionManager.needsCleanup.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-12 16:09:47.867,,,no_permission,,,,,,,,,,,,379347,,,Wed Mar 12 17:19:39 UTC 2014,,,,,,0|i1td4f:,379639,2.0.5,,,,,,,krummas,krummas,,,,,,,,,,12/Mar/14 16:09;jbellis;([~krummas] to review),"12/Mar/14 17:19;krummas;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
counters upgrade test fails,CASSANDRA-5448,12641689,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,09/Apr/13 19:00,12/Mar/19 14:07,13/Mar/19 22:29,09/May/14 01:40,,,,,,,0,,,,,"{noformat}
Test for bug of #4436 ... FAIL

======================================================================
FAIL: Test for bug of #4436
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/srv/cassandra-dtest/counter_tests.py"", line 108, in upgrade_test
    check(1)
  File ""/srv/cassandra-dtest/counter_tests.py"", line 96, in check
    assert row[1] == i * updates, ""Unexpected value %s"" % str(row)
AssertionError: Unexpected value [23, 5011]

----------------------------------------------------------------------
{noformat}

I tested as far back as 1.2.0, so this is pretty old.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-04-10 15:49:32.073,,,no_permission,,,,,,,,,,,,322105,,,Fri May 09 01:40:42 UTC 2014,,,,,,0|i1jkpj:,322450,,,,,,,,,,,,,,,,,,,"10/Apr/13 15:49;slebresne;I think the test is to blame. It uses 2 nodes with RF=2, but uses CL.ONE to do his inserts and checks, which looks wrong. I pushed a fix to that test to use QUORUM instead. With that, I was able to get the test go past line 108 ... until line 109. But then it broke because I was using the current 1.2 branch and CASSANDRA-5187 has broken ccm, so I'll need to fix ccm. I'll test on some older C* version but the test is so damn long that fell free to test it on your side.",30/Sep/13 22:22;jbellis;How does this look now that ccm is fixed?,09/May/14 01:40;brandon.williams;Fixed as of CASSANDRA-7036,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FBUtilities.singleton() should use the CF comparator,CASSANDRA-6778,12697674,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,27/Feb/14 12:33,12/Mar/19 14:07,13/Mar/19 22:29,06/Mar/14 09:06,2.0.6,,,,,,0,,,,,"We sometimes use FBUtilities.singleton() to created a SortedSet for NamesQueryFilter. However, the set created by that method does not use the CF comparator, so that it use ByteBuffer comparison/equality for methods like contains(). And this might not be ok if it turns that the comparator is so that 2 column name can be equal without their binary representation being equal, and as it turns out at least IntegerType, DecimalType (because they let you put arbitrary many zeros in front of the binary encoding) have such property (BooleanType should also have that property though it doesn't in practice which I think that's a bug, but that's for another ticket).

I'll note that CASSANDRA-6733 contains an example where this matter.  However, in practice, only SELECT on compact tables that select just one column can ever ran into that and you'd only run into it if your client insert useless zeros in its IntegerType/DecimalType binary representation, which ought to be not common in the first place. It's still wrong and should be fixed.

Patch attached to include the comparator in FBUtilities.singleton. I also found 2 other small places where we were using ByteBuffer.equals() where the comparator should be used instead and attaching a 2nd patch for those.",,,,,,,,,,,,,,,,,,,,,,,,27/Feb/14 12:36;slebresne;0001-Proper-comparison-for-singleton-sorted-set.txt;https://issues.apache.org/jira/secure/attachment/12631516/0001-Proper-comparison-for-singleton-sorted-set.txt,27/Feb/14 12:36;slebresne;0002-Use-comparator-instead-of-BB.equals.txt;https://issues.apache.org/jira/secure/attachment/12631517/0002-Use-comparator-instead-of-BB.equals.txt,05/Mar/14 18:59;thobbs;6778-test.txt;https://issues.apache.org/jira/secure/attachment/12632881/6778-test.txt,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-03-05 18:59:44.952,,,no_permission,,,,,,,,,,,,376148,,,Thu Mar 06 09:06:10 UTC 2014,,,,,,0|i1stg7:,376444,,,,,,,,thobbs,thobbs,,,,,,,,,,"27/Feb/14 12:36;slebresne;I'll note that the patch are against 2.0 because well, I wrote them while working on CASSANDRA-6733, but we might want to put that in 1.2 too, I just wonder how much more 1.2 release we want to do (and in the end, that's relatively corner-casy). But I'll gladly rebase to 1.2 before committing if we want it here and that's necessary.","05/Mar/14 18:59;thobbs;+1

The attached 6778-text.txt adds a unit test to exercise this patch.

As for 1.2, I agree that this is a pretty rare corner case, so it's probably safer to only apply this patch to 2.0.

By the way, it looks like you already did most of this work on 2.1 as part of CASSANDRA-5417, so make sure the conflicts get resolved properly.","06/Mar/14 09:06;slebresne;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
gossip and tokenMetadata get hostId out of sync on failed replace_node with the same IP address,CASSANDRA-5916,12664877,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,21/Aug/13 17:12,12/Mar/19 14:07,13/Mar/19 22:29,18/Oct/13 02:08,1.2.11,2.0.2,,,,,0,,,,,"If you try to replace_node an existing, live hostId, it will error out.  However if you're using an existing IP to do this (as in, you chose the wrong uuid to replace on accident) then the newly generated hostId wipes out the old one in TMD, and when you do try to replace it replace_node will complain it does not exist.  Examination of gossipinfo still shows the old hostId, however now you can't replace it either.",,,,,,,,,,,,CASSANDRA-5571,,,,,,,,,,,,08/Oct/13 15:25;brandon.williams;5916-v2.txt;https://issues.apache.org/jira/secure/attachment/12607367/5916-v2.txt,16/Oct/13 20:26;brandon.williams;5916-v3.txt;https://issues.apache.org/jira/secure/attachment/12608795/5916-v3.txt,17/Oct/13 21:12;brandon.williams;5916-v4.txt;https://issues.apache.org/jira/secure/attachment/12609025/5916-v4.txt,24/Sep/13 23:40;brandon.williams;5916.txt;https://issues.apache.org/jira/secure/attachment/12604915/5916.txt,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-09-29 22:45:16.632,,,no_permission,,,,,,,,,,,,344820,,,Fri Oct 18 02:08:38 UTC 2013,,,,,,0|i1ngov:,345120,1.2.8,,,,,,,thobbs,thobbs,,,1.2.0,,,,,,,"21/Aug/13 17:25;brandon.williams;The problem runs a little deeper, too: even if you specify the right uuid, and the replace fails for whatever reason, now they're out of sync again and you can't do the replace at all.",21/Aug/13 19:13;brandon.williams;This same behavior also occurs with replace_token.,"22/Aug/13 20:21;brandon.williams;This isn't so much a problem with retrying the replace, as it is with the same IP address (which won't work at all currently.) The reason for this is that by using the same IP address, the replacing node itself changes the HOST_ID, and then can't find the old one.  It's not just as simple as not advertising a new HOST_ID either, since by not having one but modifying STATUS we wipe out any existing HOST_ID as well.","24/Sep/13 23:40;brandon.williams;Here's my first (working) attempt at solving this.  This patch disables replace_[token,node] and adds a new replace_address.  In some ways replace_address seems more intuitive, but really we have to do it this way because we're going to pull everything else we need out of gossip, and endpoints are keyed by address.

We use a special gossip operation I'm calling 'shadow gossip' where we use a generation of zero and only do a single, half-round.  This means we send an empty SYN with our own blank digest to a seed, accept one ACK and then stop the gossip round there, so as not to perturb any existing state.

From there we extract the original HOST_ID and tokens, and use those for the replacement process.  A catch here though is once our gossiper actually starts, we'll knock both the TOKENS state and the existing STATUS state (for single token replacements) out with our newer, real generation, so if the replace fails past this point, we can't retry.  It may be possible to stay in shadow gossip mode through all of the process to get around that (and just remove the hibernate state), but I haven't tried this.","29/Sep/13 22:45;ravilr;Tested the patch applied against 1.2.10 and it works. Hints replay also works now after replace/bootstrap.  Regarding the corner case, where replace fails to finish after gossiper started with new generation, hence knocking out the TOKENS state,  does it make sense to allow the operator to specify replace_token with the token(s) along with the replace_address to recover from such scenario. the token list is logged during the first attempt already.
I think remaining in shadow mode may not work optimally well for cases where the node being replaced was down for more than hint window. So, all the nodes would have stopped hinting, and after replace, it would require repair to be ran to get the new data fed during the replace.
","07/Oct/13 20:22;brandon.williams;First, thanks for testing, [~ravilr]!

bq. does it make sense to allow the operator to specify replace_token with the token(s) along with the replace_address to recover

That could work, but I find it a bit ugly and confusing, especially since replace_token alone is supposed to work right now, but does not.

bq. I think remaining in shadow mode may not work optimally well for cases where the node being replaced was down for more than hint window. So, all the nodes would have stopped hinting, and after replace, it would require repair to be ran to get the new data fed during the replace.

That is true regardless of shadow mode though, since hibernate is a dead state and the node doesn't go live to reset the hint timer until the replace has completed.","07/Oct/13 21:03;ravilr;bq. That is true regardless of shadow mode though, since hibernate is a dead state and the node doesn't go live to reset the hint timer until the replace has completed.

my understanding is, due to the generation change of the replacing node, gossiper.handleMajorStateChange marks the node as dead, as hibernate is one of the DEAD_STATES. So, the other nodes marks the replacing node as dead before the token bootstrap starts, hence should be storing hints to the replacing node from that point.  Am i reading it wrong? ","07/Oct/13 21:53;brandon.williams;You're right, it will change the endpoint's expire time and reset the window.  That said, once the bootstrap has started the node should be receiving any incoming writes for the range it owns, so 'new' hints shouldn't matter in the common case where it succeeds.","07/Oct/13 23:36;ravilr;bq. once the bootstrap has started the node should be receiving any incoming writes for the range it owns, so 'new' hints shouldn't matter in the common case where it succeeds.

Is this true for node bootstrapping in hibernate state? From what i have observed, writes to hibernate'd node during its bootstrap are not sent to it, as gossip marks that node down right. 

","08/Oct/13 15:25;brandon.williams;It's not true for replacing, not only because we're down but also because we don't do any pending range announcement since there's no point.

I'd be fine with telling people they need to have a large enough hint window to complete the replace to avoid needing to repair, but we have to spin up 'real' gossip to get the schema anyway, so staying in shadow mode the entire time won't work.

However, there is a relatively simple way to have our cake (automatically extended hint window) and eat it too (be able to retry on failure and not have to specify anything new.)  As soon as we receive the tokens via shadow gossip, we can set them ourselves along with the hibernate state.  When we spin up the full gossip mode to get the schema, we'll be using the same HOST_ID and TOKENS that we grabbed, so if anything goes wrong at that point we can just grab them again next time.

This just leaves the issue of checking that the host is really dead, but this doesn't make any sense when replacing with the same IP anyway, so we can skip it when the addresses match.

v2 does all of this and includes a few other minor cleanups.","08/Oct/13 21:42;thobbs;I'm testing this out with a three-node ccm cluster.  If I do the following:
# (optional) stop node3
# add a blank node4
# start node4 with replace_address=127.0.0.3

I'll get the following:
{noformat}
ERROR 16:29:02,689 Exception encountered during startup
java.lang.RuntimeException: Cannot replace_address /127.0.0.3because it doesn't exist in gossip
    at org.apache.cassandra.service.StorageService.prepareReplacementInfo(StorageService.java:421)
    at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:623)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:604)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:501)
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
    at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
java.lang.RuntimeException: Cannot replace_address /127.0.0.3because it doesn't exist in gossip
    at org.apache.cassandra.service.StorageService.prepareReplacementInfo(StorageService.java:421)
    at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:623)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:604)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:501)
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
    at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
Exception encountered during startup: Cannot replace_address /127.0.0.3because it doesn't exist in gossip
ERROR 16:29:02,692 Exception in thread Thread[StorageServiceShutdownHook,5,main]
java.lang.NullPointerException
    at org.apache.cassandra.service.StorageService.stopRPCServer(StorageService.java:321)
    at org.apache.cassandra.service.StorageService.shutdownClientServers(StorageService.java:370)
    at org.apache.cassandra.service.StorageService.access$000(StorageService.java:88)
    at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:569)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at java.lang.Thread.run(Thread.java:724)
{noformat}

This happens whether node3 is up or down.  It seems like this problem occurs any time replace_address doesn't match the broadcast address.","16/Oct/13 20:26;brandon.williams;There are two distinct cases here: we replace 'ourself' with the same IP, or we replace a dead node with a new IP (ala ec2.)  We can't know which one we're doing _a priori_, so we shadow gossip.  If we're replacing the same IP, our shadow SYN will contain it, and the remote node will ACK with what we need.

If we're _not_ replacing with the same IP, there's a problem: an ACK will only contain what was present in the SYN digest list.  One could argue this is the sender being naive, since it obviously knows the node that sent the SYN doesn't have some states that it does, but I think at scale this makes sense since it's possible a third node has begun gossiping with the SYN sender, too.  In any case, I don't want to change that behavior at this point.

The other problem is, we can't just sit around and wait for someone to send us a populated SYN either, since we're not a part of gossip and we're new.  But we don't know we're new yet, and can't insert ourselves into gossip either, or we'll break the case of using the same IP.

So, we'll create a special case for shadow gossip, and redefine it a bit.  Instead of sending a SYN with our own endpoint and a generation of zero, we'll send a completely empty SYN (digest-wise, we still populate the cluster name and partioner, since those checks still make sense.)  This won't ever normally occur in gossip, because a node always knows about and adds itself.  When we see an empty SYN, we can know that the node that sent it is asking for everything we've got, and we can ACK with just that, allowing the replacement node to have whatever it needs for either the same or different IP cases.

v3 does this.","17/Oct/13 19:14;thobbs;That strategy sounds good to me in principle.

I'm seeing a few problems when testing, though.

If I start node4 with replace_address=node3 (while node3 is either up or down), I get an NPE:

{noformat}
DEBUG 14:01:33,359 Node /127.0.0.4 state normal, token [6564349027099416762]
 INFO 14:01:33,362 Node /127.0.0.4 state jump to normal
ERROR 14:01:33,363 Exception encountered during startup
java.lang.NullPointerException
	at org.apache.cassandra.gms.Gossiper.usesHostId(Gossiper.java:682)
	at org.apache.cassandra.gms.Gossiper.getHostId(Gossiper.java:694)
	at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1382)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1250)
	at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:973)
	at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1187)
	at org.apache.cassandra.service.StorageService.setTokens(StorageService.java:214)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:824)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:584)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:481)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
java.lang.NullPointerException
	at org.apache.cassandra.gms.Gossiper.usesHostId(Gossiper.java:682)
	at org.apache.cassandra.gms.Gossiper.getHostId(Gossiper.java:694)
	at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1382)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1250)
	at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:973)
	at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1187)
	at org.apache.cassandra.service.StorageService.setTokens(StorageService.java:214)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:824)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:584)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:481)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
Exception encountered during startup: null
ERROR 14:01:33,368 Exception in thread Thread[StorageServiceShutdownHook,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.stopRPCServer(StorageService.java:321)
	at org.apache.cassandra.service.StorageService.shutdownClientServers(StorageService.java:370)
	at org.apache.cassandra.service.StorageService.access$000(StorageService.java:88)
	at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:549)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.lang.Thread.run(Thread.java:724)
{noformat}

If I do replace_address with a non-existent node, after the ring delay sleep, I'll see:
{noformat}
java.lang.RuntimeException: Unable to gossip with any seeds
{noformat}
which is misleading, as that's not the actual problem.  Perhaps we should explicitly check for presence of the address to replace?

I've also seen that the node to replace can be the seed selected to gossip with, which results in this:
{noformat}
 INFO 14:12:58,298 Gathering node replacement information for /127.0.0.3
 INFO 14:12:58,302 Starting Messaging Service on port 7000
DEBUG 14:12:58,316 attempting to connect to /127.0.0.3
ERROR 14:13:29,320 Exception encountered during startup
java.lang.RuntimeException: Unable to gossip with any seeds
	at org.apache.cassandra.gms.Gossiper.doShadowRound(Gossiper.java:1123)
	at org.apache.cassandra.service.StorageService.prepareReplacementInfo(StorageService.java:396)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:603)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:584)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:481)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
{noformat}","17/Oct/13 21:12;brandon.williams;v4 fixes the NPE and throws when autobootstrap is disabled.  The second issue wasn't because of the replace_address, but because of checks in sendGossip.  v4 just manually sends the message to all seeds.  Depending on how many seeds you had, that may also fix the last issue (if the node being replaced is the only seed, obviously that can't work.)","17/Oct/13 21:38;thobbs;Minor nitpick: you're missing a space before ""because"" in:
{noformat}
throw new RuntimeException(""Cannot replace_address "" + DatabaseDescriptor.getReplaceAddress() + ""because it doesn't exist in gossip"");
{noformat}

Other than that, +1","18/Oct/13 02:08;brandon.williams;Committed.  I will note for ops folks, you can use replace_address in a mixed minor version 1.2 cluster, as long as one seed is also upgraded.  If no seeds are upgraded there will be no harm, the replace will simply fail.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Query returns different number of results depending on fetchsize,CASSANDRA-6826,12699535,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,wtmitchell3,wtmitchell3,07/Mar/14 23:40,12/Mar/19 14:07,13/Mar/19 22:29,28/Apr/14 07:51,,,,,,,0,,,,,"I issue a query across the set of partitioned wide rows for one logical row, where s, l, and partition specify the composite primary key for the row:
SELECT ec, ea, rd FROM sr WHERE s = ? and partition IN ? and l = ? ALLOW FILTERING;

If I set fetchSize to only 1000 when the Cluster is configured, the query sometimes does not return all the results.  In the particular case I am chasing, it returns a total of 98586 rows.  If I increase the fetchsize to 100000, all the 99999 actual rows are returned.  This suggests there is some problem with fetchsize re-establishing the position on the next segment of the result set, at least when multiple partitions are being accessed.  ","quad-core Windows 7 x64, single node cluster
Cassandra 2.0.5",,,,,,,,,,,,CASSANDRA-6825,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-03-12 09:19:29.608,,,no_permission,,,,,,,,,,,,377882,,,Mon Apr 28 07:51:00 UTC 2014,,,,,,0|i1t43r:,378174,,,,,,,,,,,,,,,,,,,"08/Mar/14 03:54;wtmitchell3;It is conceivable that this problem and CASSANDRA-6825 are related, in that they were uncovered together.  I came across the behavior described in CASSANDRA-6825 trying to analyze the test failure caused by this problem.  ","12/Mar/14 09:19;slebresne;[~wtmitchell3] Did you have a change to test against 2.0.6 yet, and if not, would it be possible for you to give it a shot? I'm wondering if that couldn't be CASSANDRA-6748.","12/Mar/14 12:00;wtmitchell3;Sylvain, merci de me l'avoir fait remarquer.  La dernière fois que j'ai cherché la nouvelle version, je ne l'ai pas trouvée.  Je vais la télécharger sur-le-champ.  Sans doute celui-ci c'est le même problème que CASSANDRA-6748.  ","12/Mar/14 21:20;wtmitchell3;Much as I found Sylvain's suggestion plausible, no, it does not explain this problem.  After installing the Apache Cassandra 2.0.6 build, the first time I tried this, it still failed.  

Unfortunately, the problem is data or timing dependent.  After seeing the failure on 2.0.6, I changed the test case to write all the rows into one partition, and that worked, so I changed it back to distributing the rows over 6 partitions, and this time that worked, too.  So we were ""lucky"" that the first time I tried this, the failure did appear.  

(I should have noticed that CASSANDRA-6748 appeared only when a column was explicitly set to null.  That was the behavior of my code about two weeks ago, before I discovered the issues around having a large number of tombstones in a wide row.)  ","19/Mar/14 03:09;wtmitchell3;Following Sylvain's suggestion that something about the null's might be affecting the problem, I tried changing the schema.  On my dual-core laptop, where the final column is null but not set explicitly null on INSERT, the SELECT * is returning a total of 90000 rows where 100000 are expected.  Changing the name of the column to begin with an a, so the nullable column is no longer last, the SELECT * is returning a total of 80000 rows, where 100000 are expected.  If I try the same query from cqlsh, where there is no limit on fetchSize, all the expected rows are returned.  

So, at least in this one experiment, changing the schema by changing the order of the columns affected the behavior.  This could, of course, be merely coincidental, some timing issue.  ","19/Mar/14 03:16;wtmitchell3;I tried a different experiment.  I used a different algorithm to compute the partition value when the rows are INSERTed.  In the failing case, I was inserting a block of 10000 rows with an identical partition values (in 20 batches of 500 each), then choosing another partition value for the next block of 10000.  

I changed the partition calculation to randomly assign the partition value, so that rows were written across all the partition values in each block.  With this algorithm, no failure was observed, even though internally I grouped the inserts by partition value into distinct batches, to take advantage of CASSANDRA-6737.  Because of the random assignment of partition values, odds are the partition boundaries no longer align with the fetchSize.   ","19/Mar/14 08:34;slebresne;If you can reproduce easily enough, some code that reproduce would truly be the best thing to help with this.","20/Mar/14 20:43;wtmitchell3;No doubt.  At the moment, though, the test case is embedded in a full application, as I mentioned to Joshua (CASSANDRA-6736).  Stripping that application down so that the test case did not carry with it so much proprietary code is a couple of days of work, and I'm not sure when I will get to it.  Even worse, when I first encountered this problem, it appeared only in a maven remove clean install of the whole project and not when the test case was run by itself.  This last week, though, it would intermittently appear and disappear when I repeated the test unchanged, without doing the maven complete build.  So it may be that a reduced version, when I have a chance to strip it down, will show the same anomaly.    ","21/Mar/14 04:03;wtmitchell3;The data files i just attached to CASSANDRA-6825 may help here, too.  ","21/Mar/14 13:11;wtmitchell3;It is worth noting that, when I first reported this problem, the difference between the two expected and actual number of rows returned was 1413, a rather odd number.  So far, on 2.0.6, I have seen differences that are always a multiple of 10,000, matching the behavior in CASSANDRA-6825.  So it may indeed be, as Sylvain suggested, that CASSANDRA-6748 fixed one problem, that I was seeing when I first reported this, but that the one test was hitting two problems, depending on timing and other issues, and now only CASSANDRA-6825 remains.    ","29/Mar/14 02:04;wtmitchell3;I started working on a smaller testcase, but competing time pressures at work put that effort on hold.  In the meantime, I was able to work around this problem by using LIMIT instead of fetch, iterating over the partitions, and using a compound comparison in the WHERE clause to establish position for the next query.  This prompted me to open JAVA-295, as I had to abandon the QueryBuilder in order to construct this WHERE clause.  

When Cassandra 2.0.7 comes out, I will check if the fix to CASSANDRA-6825 also fixes all the issue I found with the SELECT.","24/Apr/14 13:53;slebresne;[~wtmitchell3] Did you time to check if you could reproduce on 2.0.7, now that it's out?","28/Apr/14 00:41;wtmitchell3;Thank you for calling my attention to its release; the last time I checked the DataStax site, I did not yet see it, and once again I forgot to check the Apache site directly.  

Although in my first 2.0.7 tests I saw a failure, I was trying something new, to use fetchMoreResults now that fetchSize was supposed to be fixed.  Further testing has convinced me that these failures are new issues, different from this report.  The specific test that failed for me above works in 2.0.7, so, yes, I believe this problem is fixed. ","28/Apr/14 07:51;slebresne;Thanks for the feedback. Closing as dup of CASSANDRA-6825 then (for the driver issue, I saw your issue and will look it up there).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when upgrading a mixed version 1.1/1.2 cluster fully to 1.2,CASSANDRA-5612,12650803,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,enigmacurry,enigmacurry,04/Jun/13 02:55,12/Mar/19 14:07,13/Mar/19 22:29,16/Aug/13 02:09,,,,,,,0,qa-resolved,,,,"See the attached upgrade_through_versions_test.py upgrade_test_mixed().

Conceptually this method does the following:

* Instantiates a 3 node 1.1.9 cluster
* Writes some data
* Shuts down node 1 and upgrades it to 1.2 (HEAD)
* Brings the node1 back up, making the cluster a mixed version 1.1/1.2
* Brings down node2 and node3 and does the same upgrade making it all the same version.
* At this point, I would run upgradesstables on each of the nodes, but there is already an error on node3 directly after it's upgrade:

{code}
INFO [FlushWriter:1] 2013-06-03 22:49:46,543 Memtable.java (line 461) Writing Memtable-peers@1023263314(237/237 serialized/live bytes, 14 op
s)
 INFO [FlushWriter:1] 2013-06-03 22:49:46,556 Memtable.java (line 495) Completed flushing /tmp/dtest-YqMtHN/test/node3/data/system/peers/syst
em-peers-ic-2-Data.db (291 bytes) for commitlog position ReplayPosition(segmentId=1370314185862, position=58616)
 INFO [GossipStage:1] 2013-06-03 22:49:46,568 StorageService.java (line 1330) Node /127.0.0.2 state jump to normal
ERROR [MigrationStage:1] 2013-06-03 22:49:46,655 CassandraDaemon.java (line 192) Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.db.DefsTable.addColumnFamily(DefsTable.java:511)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:445)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:355)
        at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:55)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
{code}

This error is repeatable, but inconsistent. Interestingly, it is always node3 with the error.",,,,,,,,,,,,,,CASSANDRA-5811,,,,,,,,,,04/Jun/13 02:56;enigmacurry;logs.tar.gz;https://issues.apache.org/jira/secure/attachment/12586032/logs.tar.gz,04/Jun/13 02:56;enigmacurry;upgrade_through_versions_test.py;https://issues.apache.org/jira/secure/attachment/12586033/upgrade_through_versions_test.py,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-08-15 21:46:36.861,,,no_permission,,,,,,,,,,,,331130,,,Fri Aug 16 02:09:11 UTC 2013,,,,,,0|i1l4jz:,331463,,,,,,,,,,,,,,,,,,enigmacurry,"15/Aug/13 21:46;iamaleksey;So, what's happening:

1. node2 comes up, slightly earlier than node3
2. node2 calls Auth.setupAuthKeyspace(); but node3 is not alive yet, so this change is *not* pushed to node3
3. node3 comes up and gets noticed by node2
4. node2 calls Auth.setupUsersTable(); the new CF is pushed to all the nodes, including node3. However, node3 does *not* have the keyspace itself, so NPE is thrown at https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/db/DefsTable.java#L514 - because ksm is null","15/Aug/13 21:51;iamaleksey;This is entirely harmless - node3's own auth setup code will recreate both the keyspace and the table in the same second or so.
 
This is not even auth-specific. Can happen with any new ks/cf if some node becomes visible between the CREATE KEYSPACE and CREATE TABLE calls. And it's also harmless - the node would still get the changes when a different schema version is detected via gossip.","15/Aug/13 22:06;iamaleksey;The general solution would be to push the seriazlied keyspace RM when announcing create/alter table migration. Or to do it just for auth, to make the test not cause that error in the logs.","16/Aug/13 02:09;iamaleksey;Actually, even that wouldn't help with the dtest, since it would be replaced with another error - AE on the 1.1 side from the 1.2 nodes querying the default superuser presence.

Committed ea712bbbe8bb66ceb439dad0ddbb10da18a19c22 to simply skip setting up auth keyspace/users table when AllowAll is used.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
failure to decode multiple UDT,CASSANDRA-6770,12697196,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mishail,jbellis,jbellis,25/Feb/14 19:28,12/Mar/19 14:07,13/Mar/19 22:29,25/Feb/14 19:58,2.1 beta2,,,Legacy/Tools,,,0,,,,,"{code}
CREATE TYPE address (
    city text,
    address text,
    zip text
);

CREATE TYPE phone_number (
    country text,
    number text
);

CREATE TABLE users (
    login text PRIMARY KEY,
    name text,
    addresses set<address>,
    phone_numbers set<phone_number>
);

insert into users (login, name, addresses, phone_numbers)
values ('jbellis',
        'jonathan ellis',
        {{city: 'Austin', address: '902 East 5th St. #202', zip: '78702'},
         {city: 'Sunnyvale', address: '292 Gibraltar Drive #107', zip: '94089'}},
        {{country: '+44', number: '208 622 3021'}, 
         {country: '+1', number: '512-537-7809'}});

select * from users;
{code}

Result:
{code}
 login   | addresses                                                                                                                                                        | name           | phone_numbers
---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+---------------------------------------------------------------------------------------------------------
 jbellis | '\x00\x02\x00)\x00\x06Austin\x00\x00\x15902 East 5th St. #202\x00\x00\x0578702\x00\x00/\x00\tSunnyvale\x00\x00\x18292 Gibraltar Drive #107\x00\x00\x0594089\x00' | jonathan ellis | '\x00\x02\x00\x14\x00\x02+1\x00\x00\x0c512-537-7809\x00\x00\x15\x00\x03+44\x00\x00\x0c208 622 3021\x00'

(1 rows)

Failed to decode value '\x00\x02\x00)\x00\x06Austin\x00\x00\x15902 East 5th St. #202\x00\x00\x0578702\x00\x00/\x00\tSunnyvale\x00\x00\x18292 Gibraltar Drive #107\x00\x00\x0594089\x00' (for column 'addresses') as set<address>: unhashable type: 'list'
Failed to decode value '\x00\x02\x00\x14\x00\x02+1\x00\x00\x0c512-537-7809\x00\x00\x15\x00\x03+44\x00\x00\x0c208 622 3021\x00' (for column 'phone_numbers') as set<phone_number>: unhashable type: 'list'
{code}",,,,,,,,,,,,,,,,,,,,,,,,25/Feb/14 19:52;mishail;CASSANDRA-2.1-6770.diff;https://issues.apache.org/jira/secure/attachment/12631022/CASSANDRA-2.1-6770.diff,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-25 19:52:09.931,,,no_permission,,,,,,,,,,,,375670,,,Tue Feb 25 20:36:25 UTC 2014,,,,,,0|i1sqif:,375966,2.1 beta1,,,,,,,,,,,,,,,,,,"25/Feb/14 19:52;mishail;Patch to return a tuple (which can be hashed) instead of a list

{code}
cqlsh> select * FROM test.users ;

 login   | addresses                                                                                                                                  | name           | phone_numbers
---------+--------------------------------------------------------------------------------------------------------------------------------------------+----------------+-------------------------------------------------------------------------------------
 jbellis | {{city: 'Austin', address: '902 East 5th St. #202', zip: '78702'}, {city: 'Sunnyvale', address: '292 Gibraltar Drive #107', zip: '94089'}} | jonathan ellis | {{country: '+1', number: '512-537-7809'}, {country: '+44', number: '208 622 3021'}}

(1 rows)

cqlsh>
{code}",25/Feb/14 19:58;mishail;Committed,25/Feb/14 20:36;brandon.williams;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra-shuffle causes NumberFormatException,CASSANDRA-5995,12667824,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,lyubent,willymontaz,willymontaz,10/Sep/13 15:10,12/Mar/19 14:07,13/Mar/19 22:29,03/Apr/14 23:00,1.2.17,2.0.7,2.1 beta2,Legacy/Tools,,,0,,,,,"Using Cassandra-shuffle create, then Cassandra-shuffle en causes a NumbertFormatException :

Extract from output.log

 INFO 15:01:28,935 Enabling scheduled transfers of token ranges
 INFO 15:01:28,957 Initiating transfer of 3059156119944164299 (scheduled at Tue Sep 10 15:01:19 UTC 2013)
 WARN 15:01:28,962 Token 3059156119944164299 changing ownership from /10.36.194.173 to /10.39.67.29
 WARN 15:01:28,967 Token 3059156119944164299 changing ownership from /10.36.194.173 to /10.39.67.29
 INFO 15:01:28,968 RELOCATING: relocating [3059156119944164299] to 10.39.67.29
 INFO 15:01:28,968 RELOCATING: Sleeping 30000 ms before start streaming/fetching ranges
ERROR 15:01:29,331 Exception in thread Thread[GossipStage:8,5,main]
java.lang.NumberFormatException: For input string: """"
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        at java.lang.Long.parseLong(Long.java:453)
        at java.lang.Long.valueOf(Long.java:540)
        at org.apache.cassandra.dht.Murmur3Partitioner$1.fromString(Murmur3Partitioner.java:183)
        at org.apache.cassandra.service.StorageService.handleStateRelocating(StorageService.java:1490)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1180)
        at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:956)
        at org.apache.cassandra.gms.Gossiper.applyNewStates(Gossiper.java:947)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:905)
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:57)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
ERROR 15:01:30,098 Exception in thread Thread[GossipStage:9,5,main]
java.lang.NumberFormatException: For input string: """"
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        at java.lang.Long.parseLong(Long.java:453)
        at java.lang.Long.valueOf(Long.java:540)
        at org.apache.cassandra.dht.Murmur3Partitioner$1.fromString(Murmur3Partitioner.java:183)
        at org.apache.cassandra.service.StorageService.handleStateRelocating(StorageService.java:1490)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1180)
        at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:956)
        at org.apache.cassandra.gms.Gossiper.applyNewStates(Gossiper.java:947)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:905)
        at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
",Amazon EC2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-09-10 22:26:24.599,,,no_permission,,,,,,,,,,,,347760,,,Thu Apr 03 23:00:21 UTC 2014,,,,,,0|i1nysf:,348059,,,,,,,,,,,,1.2.0,,,,,,,10/Sep/13 22:26;aledsage;Looks similar to https://issues.apache.org/jira/browse/CASSANDRA-5874,"11/Sep/13 08:03;willymontaz;It seems that the same method is involved in that issue. In this particular case, I had no problem on startup and it is really the shuffle operation that led to the exception.","13/Mar/14 22:00;jbellis;Can you have a look, Lyuben?",14/Mar/14 00:19;lyubent;Will do.,02/Apr/14 23:26;lyubent;[~willymontaz][~hsn] What exact version of Cassandra are you running and approximately how much load is the node that is being shuffled store? ,"03/Apr/14 22:35;lyubent;We [carry out a check|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/StorageService.java#L3234] when moving tokens in {{StorageService#relocateTokens}} that removes the token to be moved if the source and destination addresses are the same. This means that the token is never sent to other nodes when carrying out {{shuffle enable}} leading to the empty string error in the coordinator and the NumberFormattingException in other involved nodes. I'm closing as not a problem because the only way I could reproduce is by shuffling twice, feel free to reopen if this can be reproduced on the first shuffle.  ",03/Apr/14 22:40;brandon.williams;Can't we just put an else clause after the tokens.size check and fix it by making it a no-op?,03/Apr/14 23:00;brandon.williams;Fixed this in 514ce33bc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in CompactionExecutor,CASSANDRA-5720,12656062,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,efalcao,efalcao,03/Jul/13 16:34,12/Mar/19 14:07,13/Mar/19 22:29,18/Mar/14 14:36,,,,,,,2,compaction,ttl,,,"Seeing this on all 4 of my nodes during a major compaction.

{code}
ERROR [CompactionExecutor:57927] 2013-07-03 13:41:27,591 CassandraDaemon.java (line 175) Exception in thread Thread[CompactionExecutor:57927,1,RMI Runtime]
java.lang.AssertionError: originally calculated column size of 443395646 but now it is 443395712
        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:162)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
        at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:355)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
{code}

A little bit about this CF. It stores data with a TTL of up to 30 days. These rows are wide. We run major compactions to remove expired data. This has been our setup for almost 2 years and this issue only started cropping up after upgrading to 1.2.5 (from 1.1.5)

I've been running scrub in the meantime to remove expired data. Now, I'm ending up with lots of similarly sized SSTables that C* is trying constantly to compact. These minor compactions of the bigger SSTables are failing also.","4 nodes
RF 2",,,,,,,,,,,,,CASSANDRA-4206,CASSANDRA-5359,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-03-18 14:36:05.382,,,no_permission,,,,,,,,,,,,336337,,,Tue Mar 18 14:36:05 UTC 2014,,,,,,0|i1m0jz:,336661,1.2.7,,,,,,,,,,,,,,,,,,"03/Jul/13 16:37;efalcao;Although it may be the same root issue, I created a new ticket because this is not exclusively in the realm of HintedHandoff (as in the other linked issues).","18/Mar/14 14:36;jbellis;As with the other errors from 2-pass compaction, the fix is to upgrade to 2.0.x.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tracing should deal with write failure,CASSANDRA-6133,12671944,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,02/Oct/13 14:34,12/Mar/19 14:21,13/Mar/19 22:29,02/Oct/13 16:10,1.2.11,2.0.2,,Legacy/Tools,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Oct/13 14:35;jbellis;6133.txt;https://issues.apache.org/jira/secure/attachment/12606366/6133.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-02 15:15:21.012,,,no_permission,,,,,,,,,,,,351570,,,Wed Oct 02 16:10:21 UTC 2013,,,,,,0|i1om7b:,351859,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,02/Oct/13 14:35;jbellis;Attached.,"02/Oct/13 15:15;iamaleksey;Hmm. Shall we ever get UAE when writing with CL.ANY? I don't think so. Same re WTE, once CASSANDRA-6132 is resolved.

Only OE should be possible. I'd replace all those catch-es with a single catch (REE) with assert e instanseof OE and the log warning for 'too many nodes are overloaded to save trace events'.

Other than that +1.",02/Oct/13 16:10;jbellis;committed w/ suggested changes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prepared statements are broken in native protocol v2,CASSANDRA-6035,12668840,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,blair,blair,16/Sep/13 18:48,12/Mar/19 14:21,13/Mar/19 22:29,17/Sep/13 13:48,2.0.1,,,,,,0,,,,,"Pulling this ticket over from: https://datastax-oss.atlassian.net/browse/JAVA-177

Using Java Driver 2.0.0-beta1, this fails:

{code}
Session#prepareStatement(""CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3}"")
{code}

This works with Java Driver 1.0.3.

The client side stack:

{code}
Caused by: com.datastax.driver.core.exceptions.DriverInternalError: An unexpected error occured server side on /127.0.0.3: java.lang.ArrayIndexOutOfBoundsException: 30
        at com.datastax.driver.core.Responses$Error.asException(Responses.java:85)
        at com.datastax.driver.core.Session.toPreparedStatement(Session.java:281)
        at com.datastax.driver.core.Session.prepare(Session.java:187)
{code}

The server side stack:

{code}
 INFO [FlushWriter:16] 2013-09-14 00:41:02,882 Memtable.java (line 422) Completed flushing /home/blair/.ccm/cassandra-2.0.0-168-gd2c67a1/node3
/data/system/local/system-local-jb-127-Data.db (80 bytes) for commitlog position ReplayPosition(segmentId=1379132442754, position=712508)
 INFO [CompactionExecutor:69] 2013-09-14 01:20:42,701 AutoSavingCache.java (line 250) Saved KeyCache (325 items) in 255 ms
 INFO [CompactionExecutor:70] 2013-09-14 05:20:42,665 AutoSavingCache.java (line 250) Saved KeyCache (325 items) in 218 ms
 INFO [CompactionExecutor:71] 2013-09-14 09:20:42,678 AutoSavingCache.java (line 250) Saved KeyCache (325 items) in 217 ms
ERROR [Native-Transport-Requests:3009] 2013-09-14 10:53:38,724 ErrorMessage.java (line 222) Unexpected exception during request
java.lang.ArrayIndexOutOfBoundsException: 30
        at org.jboss.netty.buffer.BigEndianHeapChannelBuffer.setInt(BigEndianHeapChannelBuffer.java:98)
        at org.jboss.netty.buffer.AbstractChannelBuffer.writeInt(AbstractChannelBuffer.java:422)
        at org.apache.cassandra.cql3.ResultSet$Metadata$Codec.encode(ResultSet.java:368)
        at org.apache.cassandra.cql3.ResultSet$Metadata$Codec.encode(ResultSet.java:320)
        at org.apache.cassandra.transport.messages.ResultMessage$Prepared$1.encode(ResultMessage.java:261)
        at org.apache.cassandra.transport.messages.ResultMessage$Prepared$1.encode(ResultMessage.java:239)
        at org.apache.cassandra.transport.messages.ResultMessage$1.encode(ResultMessage.java:49)
        at org.apache.cassandra.transport.messages.ResultMessage$1.encode(ResultMessage.java:39)
        at org.apache.cassandra.transport.Message$ProtocolEncoder.encode(Message.java:279)
        at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:66)
        at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
        at org.jboss.netty.handler.execution.ExecutionHandler.handleDownstream(ExecutionHandler.java:186)
        at org.jboss.netty.channel.Channels.write(Channels.java:704)
        at org.jboss.netty.channel.Channels.write(Channels.java:671)
        at org.jboss.netty.channel.AbstractChannel.write(AbstractChannel.java:248)
        at org.apache.cassandra.transport.Message$Dispatcher.messageReceived(Message.java:311)
        at org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:43)
        at org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:67)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
{code}
",Ubuntu 13.04 with JDK 7u40 using ccm and the the cassandra-2.0 branch at d2c67a1cb95264e8a839bdcfc0a727c892f1fc1d.,,,,,,,,,,,,,,,,,,,,,,,17/Sep/13 07:07;slebresne;6035.txt;https://issues.apache.org/jira/secure/attachment/12603556/6035.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-17 07:07:13.856,,,no_permission,,,,,,,,,,,,348774,,,Tue Sep 17 13:47:06 UTC 2013,,,,,,0|i1o50v:,349072,2.0.1,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"17/Sep/13 07:07;slebresne;That's, well, a typo. Attaching trivial patch to fix.",17/Sep/13 09:05;slebresne;PS: that does mean prepared statement are kind of broken with the v2 protocol. So updating the description to reflect that.,"17/Sep/13 10:20;slebresne;Actually, this doesn't affect 2.0.0 (I was pretty surprised since I'm sure I've tested prepared statement there), it's just there on the current 2.0 branch.",17/Sep/13 11:22;iamaleksey;+1,"17/Sep/13 13:47;iamaleksey;Anonymous people, please stop moving issues to Testing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"No ""echo off"" in cqlsh.bat",CASSANDRA-6324,12678346,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mishail,mishail,mishail,08/Nov/13 23:33,12/Mar/19 14:21,13/Mar/19 22:29,11/Nov/13 17:56,1.2.12,,,Legacy/Tools,,,0,cqlsh,,,,https://github.com/apache/cassandra/commit/08a22729afedb53dfa988b3e2db34b7a743977de mistakenly deleted {{@echo off}} from {{cqlsh.bat}},,,,,,,,,,,,,,,,,,,,,,,,08/Nov/13 23:37;mishail;CASSANDRA-1.2-6324.patch;https://issues.apache.org/jira/secure/attachment/12612929/CASSANDRA-1.2-6324.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-09 00:20:07.081,,,no_permission,,,,,,,,,,,,357721,,,Mon Nov 11 17:56:43 UTC 2013,,,,,,0|i1po1b:,358011,,,,,,,,brandon.williams,brandon.williams,,,1.2.11,,,,,,,08/Nov/13 23:37;mishail;{{ECHO OFF}},09/Nov/13 00:20;iamaleksey;(this should be ninja-d),11/Nov/13 17:56;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra document errata,CASSANDRA-6342,12678969,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lyubent,knight76,knight76,13/Nov/13 09:05,12/Mar/19 14:21,13/Mar/19 22:29,25/Nov/13 23:32,1.2.13,2.0.3,,,,,0,,,,,"Hi
I think a sample cql statement of cassandra document (http://cassandra.apache.org/doc/cql3/CQL.html) is wrong. Please change it.
------------------------------------------------------
Note that TTLs are allowed for both INSERT and UPDATE, but in both case the TTL set only apply to the newly inserted/updated values. In other words,

// Updating (or inserting)
UPDATE users USING TTL 10 SET favs['color'] = 'green' WHERE id = 'jsmith'
will only apply the TTL to the { 'color' : 'green' } record, the rest of the map remaining unaffected.

Deleting a map record is done with:

DELETE favs['author'] FROM plays WHERE id = 'jsmith'




upper DELETE cql statement is changed to below. On context of document,  'plays' table might be changed  'users' table. 

DELETE favs['author'] FROM users WHERE id = 'jsmith'
",,,,,,,,,,,,,,,,,,,,,,,,25/Nov/13 21:57;lyubent;6342.patch;https://issues.apache.org/jira/secure/attachment/12615683/6342.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-25 21:56:37.505,,,no_permission,,,,,,,,,,,,358335,,,Mon Nov 25 23:32:18 UTC 2013,,,,,,0|i1prtb:,358625,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,25/Nov/13 21:56;lyubent;Just renamed the table in the docs. Patch is for 1.2,"25/Nov/13 23:32;iamaleksey;Ninja-d, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool getendpoints doesn't validate key arity,CASSANDRA-6458,12683140,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,philipthompson,yaitskov,yaitskov,06/Dec/13 15:40,12/Mar/19 14:21,13/Mar/19 22:29,15/Apr/15 17:48,2.1.5,,,Tool/nodetool,,,0,lhf,,,,"I have a complex row key.

$ create table b (x int, s text, ((x,s)) primary key);

In cqlsh I cannot fill row key partially:

{noformat}
$ insert into b (x) values(4);
Bad Request: Missing mandatory PRIMARY KEY part s
{noformat}

But nodetool can find hosts by incomplete key
{noformat}
$ nodetool -h cas3 getendpoints anti_portal b 12
192.168.4.4
192.168.4.5
192.168.4.6
{noformat}

No error is reported.

I found that columns are separated by "":"".
And If I pass to many elements then the error happens.

{noformat}
$ nodetool -h cas3 getendpoints anit_portal b 12:dd:dd
Exception in thread ""main"" org.apache.cassandra.serializers.MarshalException: unable to make int from '12:dd:dd'
    at org.apache.cassandra.db.marshal.Int32Type.fromString(Int32Type.java:69)
    at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:2495)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
    at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
    at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
    at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
    at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
    at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
    at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
    at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
    at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
    at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1487)
    at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
    at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
    at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
    at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:848)
    at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
    at sun.rmi.transport.Transport$1.run(Transport.java:177)
    at sun.rmi.transport.Transport$1.run(Transport.java:174)
    at java.security.AccessController.doPrivileged(Native Method)
    at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
    at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:556)
    at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:811)
    at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:670)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NumberFormatException: For input string: ""12:dd:dd""
    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
    at java.lang.Integer.parseInt(Integer.java:492)
    at java.lang.Integer.parseInt(Integer.java:527)
    at org.apache.cassandra.db.marshal.Int32Type.fromString(Int32Type.java:65)
    ... 36 more
{noformat}

I think showing huge stack trace is not proper behavior.
Error message should be printer if arity of passed key and table key are not equal.",,,,,,,,,,,,,,,,,,,,,,,,18/Mar/15 20:31;philipthompson;6458-2.1.txt;https://issues.apache.org/jira/secure/attachment/12705427/6458-2.1.txt,15/Apr/15 17:16;philipthompson;6458-trunk-v2.txt;https://issues.apache.org/jira/secure/attachment/12725629/6458-trunk-v2.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-03-14 04:22:14.497,,,no_permission,,,,,,,,,,,,362392,,,Wed Apr 15 17:48:16 UTC 2015,,,,,,0|i1qgu7:,362686,,,,,,,,jjordan,jjordan,,,2.0.2,,,,,,,14/Mar/14 04:22;jjordan;getendpoints operates on the partition key only.  we might want to update the help text for it.,"10/Jun/14 08:53;xapharius;Hello!
I started working on this issue, but would like to have some feedback on how to proceed. I am quite new to Cassandra and might be making some wrong assumptions about it's internals.
In order for NodeTool to check the arity, it has to get the number of columns in the partition key of a cf. As far as I understand it, I would have to get that from CFMetaData.
The problem is that NodeTool can only connect through NodeProbe, which has the proxies for the MBeans.
The closest I got was through the StorageServerProxy and ColumnFamilyStore proxy, but none of them specify a method in their interface to access a cfMetaData.

The options I see so far are to either add a method to ColumnFamilyStoreMBean or try to validate the key arity somewhere lower.

Or is there an easier way?

Thanks!
",18/Mar/15 20:31;philipthompson;Attached patches for 2.1 and trunk,"26/Mar/15 15:35;philipthompson;Upon reflection, I think only the documentation needed expanded. The error message explains pretty sufficiently that the key passed in is unacceptable, and there is no earlier point in the stack to validate it.",15/Apr/15 16:49;jjordan;+1,15/Apr/15 17:11;jbellis;I think trunk patch fails to apply now.,15/Apr/15 17:14;philipthompson;I'll rebase.,15/Apr/15 17:17;philipthompson;New patch for trunk attached.,"15/Apr/15 17:48;thobbs;Committed as {{dac54976}}, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executing a prepared CREATE KEYSPACE multiple times doesn't work,CASSANDRA-6471,12683854,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,10/Dec/13 15:26,12/Mar/19 14:21,13/Mar/19 22:29,18/Dec/13 10:15,1.2.14,,,,,,0,,,,,"See user reports on the java driver JIRA: https://datastax-oss.atlassian.net/browse/JAVA-223. Preparing CREATE KEYSPACE queries is not particularly useful but there is no reason for it to be broken.

The reason is that calling KSPropDef/CFPropDef.validate() methods are not idempotent. Attaching simple patch to fix.",,,,,,,,,,,,,,,,,,,,,,,,10/Dec/13 15:27;slebresne;6471.txt;https://issues.apache.org/jira/secure/attachment/12618049/6471.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-12 08:11:48.603,,,no_permission,,,,,,,,,,,,362926,,,Wed Dec 18 10:15:47 UTC 2013,,,,,,0|i1qk67:,363232,,,,,,,,jbellis,jbellis,,,,,,,,,,12/Dec/13 08:11;amichai;Does this apply to 2.0.x as well?,12/Dec/13 08:19;slebresne;It does.,17/Dec/13 23:13;jbellis;+1,"18/Dec/13 10:15;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
removenode outputs confusing non-error,CASSANDRA-6397,12680789,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,kirktrue,enigmacurry,enigmacurry,22/Nov/13 17:58,12/Mar/19 14:21,13/Mar/19 22:29,12/Jun/14 22:34,2.0.9,2.1 rc2,,Legacy/Tools,,,0,lhf,,,,"*{{nodetool removenode force}}* outputs a slightly confusing error message when there is nothing for it to do.

* Start a cluster, then kill one of the nodes.
* run *{{nodetool removenode}}* on the node you killed.
* Simultaneously, in another shell, run *{{nodetool removenode force}}*, see that it outputs a simple message regarding it's status.
* Run *{{nodetool removenode force}}* again after the firsrt removenode command finishes, you'll see this message and traceback:

{code}
$ ~/.ccm/test/node1/bin/nodetool -p 7100 removenode force
RemovalStatus: No token removals in process.
Exception in thread ""main"" java.lang.UnsupportedOperationException: No tokens to force removal on, call 'removetoken' first
	at org.apache.cassandra.service.StorageService.forceRemoveCompletion(StorageService.java:3140)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:235)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:250)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:791)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1486)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:96)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1327)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1419)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:847)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at sun.rmi.transport.Transport$1.run(Transport.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
{code}

Two issues I see with this traceback:

* ""No tokens to force removal on"" is telling me the same thing that the message before it tells me: ""RemovalStatus: No token removals in process."", So the entire traceback is redundant.
* ""call 'removetoken' first"" - removetoken has been deprecated according to the message output by removenode, so there is inconsistency in directions to the user.",,,,,,,,,,,,,,,,,,,,,,,,13/May/14 01:07;kirktrue;trunk-6397.txt;https://issues.apache.org/jira/secure/attachment/12644530/trunk-6397.txt,23/May/14 16:50;kirktrue;trunk-6397.v2.txt;https://issues.apache.org/jira/secure/attachment/12646542/trunk-6397.v2.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-11-22 19:25:50.873,,,no_permission,,,,,,,,,,,,360054,,,Thu Jun 12 22:34:58 UTC 2014,,,,,,0|i1q2hb:,360353,1.2.12,2.0.3,,,,,,brandon.williams,brandon.williams,,,,,,,,,,22/Nov/13 19:25;brandon.williams;Changed all removetoken references to removenode in da5ff080550.  The rest should be fairly easy in nodetool.,"23/Apr/14 17:06;kirktrue;Sorry, I'm a little confused by the steps to reproduce. The first step says to ""kill one of the nodes"" but the second step uses nodetool on that same node. When I attempt step 2:

{noformat}
$ ~/.ccm/CASSANDRA-6397/node1/bin/nodetool -p 7100 removenode
{noformat}

I get this:

{noformat}
nodetool: Failed to connect to '127.0.0.1:7100' - ConnectException: 'Connection refused'.
{noformat}

Is there a missing step or something I'm missing?

Thanks.",23/Apr/14 17:19;brandon.williams;You have some kind of JMX problem.,23/Apr/14 17:23;kirktrue;I killed the node using {{kill -9 <PID of node1>}}. Thus the JVM and its JMX hooks aren't around anymore. Previous to the kill I was able to access the node via JMX OK.,"23/Apr/14 17:34;brandon.williams;It doesn't mean run nodetool *against* the same node, it means 'remove the node you just killed'",16/May/14 14:16;jbellis;[~brandon.williams] to review,16/May/14 14:35;jbellis;[~brandon.williams] to review,16/May/14 16:58;brandon.williams;Why switch to a boolean that we never check?,"20/May/14 16:58;kirktrue;Yes, I agree that's ugly.

My concern is that this change turns an error case (forcing when nothing is being removed) into a non-error case. Do users of either nodetool or JMX rely on getting an error? Do they need to know that their request was a no-op when they ask to force a removenode?

If so, they can check the boolean, kind of like File.delete(). But if you want, I can just remove the boolean FYI flag altogether.","20/May/14 17:05;brandon.williams;I think we're conflating a couple issues here.  I think Ryan's point is there shouldn't be a stacktrace printed, but if there is a problem we should report that and exit with a non-zero code.  Programmatically though, the boolean is completely unused as the patch stands, so we don't need that unless it's to enable solving the first problem.",23/May/14 16:51;kirktrue;Removed boolean return value.,"12/Jun/14 22:34;brandon.williams;I don't think any has been relying on catching an exception here, and if they have they've been doing it wrong.  If anyone needs this they can open a new one.  Committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction timestamp captured incorrectly in Compaction History table,CASSANDRA-6784,12697934,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,chander,chander,chander,28/Feb/14 14:29,12/Mar/19 14:21,13/Mar/19 22:29,28/Feb/14 16:10,2.0.6,,,,,,0,patch,,,,"Compaction finish time is incorrectly captured using System.nanoTime() before the compaction is triggered. The finish time should be captured using System.currentTimeMillis() as per #4432.  The suggested patch captures using  FBUtilities.timestampMicros() after the compaction completes.
 
This however doesn't fix the formatting of timestamp values by
  cqlsh which throws decoding errors as below:
   - Failed to format value NNN.. as timestamp: timestamp out of range for platform time_t

 
Reference : https://issues.apache.org/jira/browse/CASSANDRA-4432",,,,,,,,,,,,,,,,,,,,,,,,28/Feb/14 14:31;chander;trunk-6784.txt;https://issues.apache.org/jira/secure/attachment/12631748/trunk-6784.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-28 16:10:09.583,,,no_permission,,,,,,,,,,,,376408,,,Fri Feb 28 16:10:09 UTC 2014,,,,,,0|i1sv1z:,376704,,,,,,,,jbellis,jbellis,,,,,,,,,,28/Feb/14 16:10;jbellis;Since it's a cell of type timestamp (not an internal cell timestamp) it actually needs to be millis-since-epoch.  This makes cqlsh happy.  Committed w/ that change.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Require specifying rows_per_partition_to_cache,CASSANDRA-6745,12696331,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,krummas,jbellis,jbellis,20/Feb/14 19:16,12/Mar/19 14:21,13/Mar/19 22:29,12/Mar/14 13:12,2.1 beta2,,,,,,0,,,,,"We should require specifying rows_to_cache_per_partition for new tables or newly ALTERed when row caching is enabled.

Pre-upgrade should be grandfathered in as ALL to match existing semantics.",,,,,,,,,,,,,,,,,,,,,,,,04/Mar/14 13:45;krummas;0001-wip-caching-options.patch;https://issues.apache.org/jira/secure/attachment/12632503/0001-wip-caching-options.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-21 07:50:06.96,,,no_permission,,,,,,,,,,,,374807,,,Thu Mar 13 08:41:40 UTC 2014,,,,,,0|i1sl7j:,375106,,,,,,,,slebresne,slebresne,,,2.1 beta1,,,,,,,"21/Feb/14 07:50;slebresne;On that note, I have wondered if it wouldn't be cleaner to group the caching options into a map like we do for say compression options. Something like:
{noformat}
ALTER t WITH caching = { 'keys' : 'false', 'rows' : 'true', 'rows_per_partition' : '100' }
{noformat}
This feels cleaner to me, and would provide a clear home for future other caching options (or if say, we start moving some of your global ones to per-table ones).","22/Feb/14 19:42;krummas;sounds good, I'll do that","27/Feb/14 09:08;krummas;pushed to https://github.com/krummas/cassandra/commits/marcuse/6745

I changed the option format a bit to make it more clear (might not be for existing users though), wdyt?
{code}
 CQL: caching = { 'keys' : 'ALL|NONE', 'rows_per_partition': '200|NONE|ALL' }
 Thrift: caching = { 'keys' : 'ALL|NONE', 'cells_per_row': '200|NONE|ALL' }
{code}

also note that this breaks beta1 -> beta2 upgrades, but i guess that is ok","03/Mar/14 13:44;slebresne;Hum, I kind of like my version more :). Basically, my goal is to make things as future proof as possible since it's not at all unlikely that in the future we might add per-table setting for both cache. So my idea was to have a simple on/off option switch for each cache (and I kind of like for that to be a boolean) + any number of options that control the behavior. Typically, having rows_per_partition being the option that control whether the row cache is enabled or not feels weird to me, feeling that we're trying to cram too much info in that option. But I don't know, maybe it's just me trying to justify what is ultimately a personal preference?

Other than that, I do would rather not change the thrift interface. We definitively can't change an existing option like caching like that as this would break clients which we don't do for thrift, and while we could probably deprecate 'caching' to replace it with some 'caching_options', this would still be borderline in term of backward compatibility, so I propose to leave things as they are. In fact, I would even argue that this whole issue is kind of breaking if done for thrift and so we may want to stick to CQL only here.","03/Mar/14 17:43;krummas;My reasoning was that it made no real sense to first have a boolean and then state how many rows, felt like i was repeating myself when doing it, but I can buy the argument that if we want to extend this in the future, having the bools might make sense

And the thrift part, sure",03/Mar/14 18:11;jbellis;I rather like the way Marcus did it.,"04/Mar/14 13:45;krummas;Attaching patch, thrift side is kept as-is, no generated files this time

CQL syntax is kept as in previous patch.",11/Mar/14 20:33;jbellis;([~slebresne] to review),"12/Mar/14 09:04;slebresne;Mostly look good to me, though one last point is that since the new ''caching"" syntax beaks the existing one, I think we'd need to still support the old syntax too at least for 2.1 (in CQL3 I mean), with maybe a warning in the log if said old syntax is used. Otherwise this could be annoying for rolling upgrades.

Another nit is that I'd leave CQL2 alone. It will be removed in 3.0 so the only thing someone that still use CQL2 in 2.1 is update to CQL3. So changing the syntax of ""caching"" there would just be an annoyance, and I wouldn't even bother with supporting the new syntax: again, if you want to use the new caching features and you're on CQL2, it's high time for you to upgrade to CQL3.","12/Mar/14 11:59;krummas;pushed a fix on top of the patch: https://github.com/krummas/cassandra/commits/marcuse/6745

cql2 already supports old syntax",12/Mar/14 12:44;slebresne;+1,"12/Mar/14 13:12;krummas;committed, thanks","13/Mar/14 03:00;mishail;Guys, I've made a couple of tweaks for CQLSH https://github.com/Mishail/cassandra/compare/apache:cassandra-2.1...CASSANDRA-6745-caching
* Autocomplete with {{{'keys': '}} right after {{caching}}
* {{'ALL'}} and {{'NONE'}} are values, not hints

I'll push them if you're OK with those changes",13/Mar/14 08:41;krummas;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Delimiter not working for special characters in COPY command from CQLSH,CASSANDRA-6773,12697354,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,Shiti,Shiti,Shiti,26/Feb/14 12:13,12/Mar/19 14:21,13/Mar/19 22:29,13/Mar/14 21:11,1.2.16,2.0.7,2.1 beta2,,,,0,CQL,CQL3,,,"COPY command from CQLSH which can be used to load data from CSV files is not working with option delimiter='\t'

cqlsh>CREATE TABLE airplanes (
name text PRIMARY KEY,
manufacturer ascii,
year int,
mach float
);

cqlsh>INSERT INTO airplanes   (name, manufacturer, year, mach)   VALUES ('P38-Lightning', 'Lockheed', 1937, 7);

cqlsh> SELECT * FROM airplanes;   
name          | mach | manufacturer | year 
--------------+------+--------------+------  
P38-Lightning |  0.7 |     Lockheed | 1937

cqlsh> COPY airplanes (name, manufacturer, year, mach) TO 'temp.tsv' WITH DELIMITER = '\t';
""delimiter"" must be an 1-character string

cqlsh> COPY airplanes (name, manufacturer, year, mach) FROM 'temp.csv' WITH DELIMITER = '\t';
""delimiter"" must be an 1-character string",,,,,,,,,,,,,,,,,,,,,,,,10/Mar/14 05:01;Shiti;trunk-6773.txt;https://issues.apache.org/jira/secure/attachment/12633629/trunk-6773.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-03 00:08:22.537,,,no_permission,,,,,,,,,,,,375828,,,Thu Mar 13 21:11:03 UTC 2014,,,,,,0|i1srhj:,376124,,,,,,,,mishail,mishail,,,,,,,,,,"03/Mar/14 00:08;mishail;Unfortunately the patch doesn't work with {ESCAPE}
{code}
cqlsh:test> COPY ttable TO 'test.csv' WITH ESCAPE = '\';
ValueError: Trailing \ in string
{code}","10/Mar/14 05:15;Shiti;Hi [~mishail],

I assumed that one would use double ""\"" when they had to specify backslash as a value since its escape character.
{code}
cqlsh:test> COPY ttable TO 'test.csv' WITH ESCAPE = '\\';
{code}

Is this wrong? Should we handle cases where user sets the value using a single ""\"" or throw an error?
If this should be handled internally, are there any other cases we need to handle? ","10/Mar/14 19:36;mishail;[~Shiti] My fault, you're right. I'll take a look again when I'll have a time","13/Mar/14 21:11;mishail;Committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CQL: disallow counter update with ""USING TIMESTAMP"" and ""USING TTL""",CASSANDRA-6649,12693160,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,slebresne,slebresne,04/Feb/14 17:45,12/Mar/19 14:21,13/Mar/19 22:29,11/Feb/14 23:06,1.2.16,2.0.6,2.1 beta1,,,,0,,,,,"Timestamps are not used by counters and TTL are not supported, but it appears we don't reject counter updates that have ""USING TIMESTAMP X"" or ""USING TTL X"". We should since both are non-sensical (the value is completely ignored currently).

Note: we should also refuse ""USING TIMESTAMP"" on ""DELETE"" statements on counters table: even though we kind of do use a timestamp internally, it's more of an implementation detail and in fact may go away with CASSANDRA-6506 (there is also nothing clever you can do with it by providing it client side).

Note bis: strictly speaking doing that could break a few users that where setting those thinking it does something. I think that the lack of validation is more of a bug and that user that think it's doing something probably ought to know it's not sooner than later, but I could be fine with just warning in the log file for 1.2 and 2.0, and only rejecting in 2.1 if someone thinks it's safer.",,,,,,,,,,,,,,,,,,,,,,,,09/Feb/14 00:53;iamaleksey;6649-2.0.txt;https://issues.apache.org/jira/secure/attachment/12627842/6649-2.0.txt,09/Feb/14 00:43;iamaleksey;6649-2.1.txt;https://issues.apache.org/jira/secure/attachment/12627841/6649-2.1.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-02-04 18:34:05.34,,,no_permission,,,,,,,,,,,,371746,,,Tue Feb 11 23:06:34 UTC 2014,,,,,,0|i1s2fb:,372046,,,,,,,,slebresne,slebresne,,,,,,,,,,"04/Feb/14 18:34;iamaleksey;Probably should leave 1.2 alone. Maybe 2.0 as well, since some might be using it (foolishly), and introduce the change as part of the 'counters 2.0'.","11/Feb/14 18:25;slebresne;For the 2.0 version, I really think we should have a longer, more explanatory message, since users will get that out of context in the log. Something like: ""Detected use of 'USING TIMESTAMP' in counter UPDATE. This is invalid since counters do not use timestamps and the timestamp has been ignored. Such query will be rejected starting from Cassandra 2.1 so you should fix your query."". But lgtm otherwise.

Note: I would also commit that in 1.2 too because really, that's just a warning and it's never too early to warn user that it's not doing what they probably think it does (if we end up not doing any more 1.2 release, there is still not harm having committed it). I'm not extremely strong on that though.","11/Feb/14 23:06;iamaleksey;Committed with a better warning + moved the validation to validate(), where it belongs. Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Config param [compaction_throughput_mb_per_sec] unsafe calculation.,CASSANDRA-6647,12693092,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,platon-vai,platon-vai,platon-vai,04/Feb/14 12:23,12/Mar/19 14:21,13/Mar/19 22:29,05/Feb/14 01:12,1.2.15,,,,,,0,,,,,"In cassandra.yaml i set 
compaction_throughput_mb_per_sec: 2048

and got en exception

ERROR 16:15:41,705 Exception in thread Thread[CompactionExecutor:3,1,main]
java.lang.IllegalArgumentException: rate must be positive
        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:93)
        at com.google.common.util.concurrent.RateLimiter.setRate(RateLimiter.java:355)
        at org.apache.cassandra.db.compaction.CompactionManager.getRateLimiter(CompactionManager.java:119)
        at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:241)
        at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:250)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:126)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)

After trivial exploration i found this code

CompactionManager.java:114

double currentThroughput = DatabaseDescriptor.getCompactionThroughputMbPerSec() * 1024 * 1024;

Main reason - result of multiplication out of range integer type

I think simple fix
double currentThroughput = DatabaseDescriptor.getCompactionThroughputMbPerSec() * 1024 * 1024L;
","RHEL 6.3 x64, jdk 1.7u51 64 bit (server mode),apache-cassandra 2.0.4",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-02-05 01:12:30.717,,,no_permission,,,,,,,,,,,,371678,,,Wed Feb 05 01:12:30 UTC 2014,,,,,,0|i1s207:,371978,,,,,,,,dbrosius,dbrosius,,,,,,,,,,"05/Feb/14 01:12;dbrosius;Thanks!

committed as 511787d75d19e778123bc0c1384c3fd80f1bf32e to cassandra-1.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dead code: net/HeaderTypes.java,CASSANDRA-6743,12696132,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,daniels,daniels,daniels,20/Feb/14 01:32,12/Mar/19 14:21,13/Mar/19 22:29,20/Feb/14 05:34,2.0.6,,,,,,0,,,,,"Class o.a.c.net.HeaderTypes is not used anywhere, and, even though public, is of dubious value, because it only contains a couple of constants and no code. Googling resulted in no hits other than from this source file.

Searching commit history didn't uncover any reasons for its continued existence, so that whole file can be safely removed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-02-20 05:34:04.838,,,no_permission,,,,,,,,,,,,374609,,,Thu Feb 20 05:34:04 UTC 2014,,,,,,0|i1sjzr:,374909,,,,,,,,jbellis,jbellis,,,,,,,,,,20/Feb/14 05:34;jbellis;Removed!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect use of System.nanoTime(),CASSANDRA-5584,12648683,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,ash2k,ash2k,ash2k,21/May/13 18:04,12/Mar/19 14:21,13/Mar/19 22:29,21/May/13 19:19,1.2.6,,,,,,0,,,,,"From System.nanoTime() JavaDoc:
{noformat}
For example, to measure how long some code takes to execute:
 long startTime = System.nanoTime();
 // ... the code being measured ...
 long estimatedTime = System.nanoTime() - startTime; 

To compare two nanoTime values
 long t0 = System.nanoTime();
 ...
 long t1 = System.nanoTime();
one should use t1 - t0 < 0, not t1 < t0, because of the possibility of numerical overflow.
{noformat}
I found one place with such incorrect use that can result in overflow and in incorrect timeout handling. See attached patch.",,,,,,,,,,,,,,,,,,,,,,,,21/May/13 18:05;ash2k;trunk-5584.txt;https://issues.apache.org/jira/secure/attachment/12584046/trunk-5584.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-21 19:19:18.51,,,no_permission,,,,,,,,,,,,329038,,,Tue May 21 19:19:18 UTC 2013,,,,,,0|i1krpz:,329380,,,,,,,,jbellis,jbellis,,,,,,,,,,21/May/13 19:19;jbellis;LGTM; committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DESCRIBE TABLES is empty after case insensitive use,CASSANDRA-5567,12647581,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,mbulman,mbulman,14/May/13 19:33,12/Mar/19 14:21,13/Mar/19 22:29,15/May/13 13:09,1.2.5,,,Legacy/Tools,,,0,,,,,"Using trunk @ 02547747198c0d14f9a4102a920b914bcfd57a23

{noformat}
trunk*:~/src/cassandra$ bin/cqlsh -3
Connected to Test Cluster at localhost:9160.
[cqlsh 3.0.2 | Cassandra 2.0-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.37.0]
Use HELP for help.
cqlsh> create keyspace Test with replication={'class':'SimpleStrategy', 'replication_factor':1};
cqlsh> use Test;
cqlsh:Test> CREATE TABLE users (   user_name varchar PRIMARY KEY,   password varchar,   gender varchar,   session_token varchar,   state varchar,   birth_year bigint );
cqlsh:Test> describe tables;

<empty>

cqlsh:Test> use test;
cqlsh:test> describe tables;

users

cqlsh:test>
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,15/May/13 00:08;iamaleksey;5567.txt;https://issues.apache.org/jira/secure/attachment/12583249/5567.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-15 01:31:07.174,,,no_permission,,,,,,,,,,,,327937,,,Wed May 15 13:09:03 UTC 2013,,,,,,0|i1kkxz:,328281,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,15/May/13 01:31;brandon.williams;+1,"15/May/13 13:09;iamaleksey;Thanks, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The native protocol server is not correctly stopped on shutdown,CASSANDRA-5507,12644081,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,23/Apr/13 09:57,12/Mar/19 14:21,13/Mar/19 22:29,23/Apr/13 11:52,1.2.5,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Apr/13 09:59;slebresne;5507.txt;https://issues.apache.org/jira/secure/attachment/12580015/5507.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-23 11:21:39.517,,,no_permission,,,,,,,,,,,,324448,,,Tue Apr 23 11:52:45 UTC 2013,,,,,,0|i1jz5j:,324793,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,23/Apr/13 09:59;slebresne;Trivial patch attached (we were only stopping the RPC server).,23/Apr/13 11:21;iamaleksey;+1,"23/Apr/13 11:52;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"remove dead classes (ArrayUtil.java, CreationTimeAwareFuture.java)",CASSANDRA-5475,12642643,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,dbrosius,dbrosius,16/Apr/13 01:13,12/Mar/19 14:21,13/Mar/19 22:29,16/Apr/13 02:40,1.2.5,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Apr/13 01:14;dbrosius;5475.txt;https://issues.apache.org/jira/secure/attachment/12578849/5475.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-16 01:31:26.402,,,no_permission,,,,,,,,,,,,323057,,,Tue Apr 16 02:40:43 UTC 2013,,,,,,0|i1jqkv:,323402,,,,,,,,jbellis,jbellis,,,,,,,,,,16/Apr/13 01:31;iamaleksey;+1,16/Apr/13 02:40;dbrosius;committed to cassandra-1.2 as 40e7aba6b2f694017df5fbba90fd44caa0d43fc9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BlacklistingCompactionTest missing Apache license,CASSANDRA-5627,12651946,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,carlyeks,carlyeks,carlyeks,09/Jun/13 20:16,12/Mar/19 14:21,13/Mar/19 22:29,12/Jun/13 05:16,2.0 beta 1,,,,,,0,,,,,BlacklistingCompactionsTest is missing the Apache license header,,,,,,,,,,,,,,,,,,,,,,,,09/Jun/13 20:19;carlyeks;5627.patch;https://issues.apache.org/jira/secure/attachment/12586970/5627.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-12 05:16:06.472,,,no_permission,,,,,,,,,,,,332270,,,Wed Jun 12 05:16:06 UTC 2013,,,,,,0|i1lbjr:,332599,,,,,,,,jbellis,jbellis,,,,,,,,,,12/Jun/13 05:16;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ThriftServer.stop() hangs forever,CASSANDRA-5635,12652678,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,snazy,snazy,13/Jun/13 15:00,12/Mar/19 14:21,13/Mar/19 22:29,13/May/14 00:14,2.1 rc1,,,,,,0,,,,,"I've written a very small main() method just to start to test ""how to embed Cassandra"". But the code hangs while executing CassandraDaemon.stop()...
I've used a default {{cassandra.yaml}} file.

{noformat}
cassandraDaemon = new CassandraDaemon();
cassandraDaemon.init(null);
cassandraDaemon.start();
cassandraDaemon.stop();
{noformat}

{{CassandraDaemon.stop()}} calls {{ThriftServer.stop()}, which ends somehow in {{TCustomServerSocket.close()}}, which sets its field {{serverSocket=null}}. This causes {{CustomTThreadPoolServer.server()}} to loop forever, because it's {{stopped}} field is still {{false}} - {{TServerTransport.accept()}} immediatly throws a {{TTransportException}} because {{TCustomServerSocket}}'s {{serverSocket}} is {{null}}.
",,,,,,,,,,,,,,,,,,,,,,,,12/May/14 02:31;dbrosius;5635.txt;https://issues.apache.org/jira/secure/attachment/12644368/5635.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-04-27 22:28:03.402,,,no_permission,,,,,,,,,,,,333002,,,Tue May 13 00:14:09 UTC 2014,,,,,,0|i1lg1r:,333330,,,,,,,,jbellis,jbellis,,,,,,,,,,"27/Apr/14 22:28;dbrosius;On trunk at least, what's keeping it alive is the threads

COMMIT-LOG-ALLOCATOR  (CommitLogSegmentManager.java)
PERIODIC-COMMIT-LOG-SYNCER (PeriodicCommitLogService.java)
ACCEPT-(ip) (MessagingService)

not of which are terminated by CassandraDaemon.stop()

might need a StorageService.instance.drain(); afterwards.","12/May/14 02:31;dbrosius;make SlabPoolCleaner thread, a daemon.",12/May/14 19:57;jbellis;+1,13/May/14 00:14;dbrosius;committed to cassandra-2.1 as commit 81bf2b08895336b7f650624b7a581ccc5e2dbf26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
error in help text of cassandra-cli (--tspw option shows SSL: full path to truststore),CASSANDRA-5643,12653019,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,metskem,metskem,metskem,15/Jun/13 12:43,12/Mar/19 14:21,13/Mar/19 22:29,15/Jun/13 15:30,1.2.6,,,Legacy/Tools,,,0,,,,,"When you issue the cmdline ""cassandra-cli -?"" you get the following:

{noformat}
cssndra@ubuntu2:/opt/cassandra/conf$ cassandra-cli -?
usage: cassandra-cli
 -?,--help                                           usage help
 -alg,--ssl-alg <ALGORITHM>                          SSL: algorithm
                                                     (default: SunX509)
 -B,--batch                                          enabled batch mode
                                                     (suppress output; errors are fatal)
 -ciphers,--ssl-ciphers <CIPHER-SUITES>              SSL: comma-separated
                                                     list of encryption suites to use
    --debug                                          display stack-traces
                                                     (NOTE: We print strack-traces in the places where it makes sense even
                                                     without --debug)
 -f,--file <FILENAME>                                load statements from
                                                     the specific file
 -h,--host <HOSTNAME>                                cassandra server's
                                                     host name
    --jmxpassword <JMX-PASSWORD>                     JMX service password
    --jmxport <JMX-PORT>                             JMX service port
    --jmxusername <JMX-USERNAME>                     JMX service username
 -k,--keyspace <KEYSPACE>                            cassandra keyspace
                                                     user is authenticated against
 -p,--port <PORT>                                    cassandra server's
                                                     thrift port
 -prtcl,--ssl-protocol <PROTOCOL>                    SSL: connections
                                                     protocol to use (default: TLS)
 -pw,--password <PASSWORD>                           password for
                                                     cassandra authentication
    --schema-mwt <TIME>                              Schema migration wait
                                                     time (secs.), default is 10 secs
 -st,--store-type <STORE-TYPE>                       SSL: type of store
 -tf,--transport-factory <TRANSPORT-FACTORY>         Fully-qualified
                                                     TTransportFactory class name for creating a connection to cassandra
 -ts,--truststore <TRUSTSTORE>                       SSL: full path to
                                                     truststore
 -tspw,--truststore-password <TRUSTSTORE-PASSWORD>   SSL: full path to
                                                     truststore
 -u,--username <USERNAME>                            user name for
                                                     cassandra authentication
 -v,--verbose                                        verbose output when
                                                     using batch mode
{noformat}

As you see, the help text for the --tspw switch has the same as the --ts switch.
","N/A
(Ubuntu Linux, OpenJDK 7)",,,,,,,,,,,,,,,,,,,,,,,15/Jun/13 12:45;metskem;CASSANDRA-5634.patch;https://issues.apache.org/jira/secure/attachment/12587973/CASSANDRA-5634.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-15 15:30:15.856,,,no_permission,,,,,,,,,,,,333342,,,Sat Jun 15 15:30:15 UTC 2013,,,,,,0|i1li5b:,333670,,,,,,,,dbrosius,dbrosius,,,,,,,,,,15/Jun/13 12:45;metskem;proposed patch,"15/Jun/13 15:30;dbrosius;Thanks!

committed to cassandra-1.2 as d87ed95c0fbffc978cbcaec5d02cb65fdd4b0ea4",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SnitchProperties uses GossipingPropertyFileSnitch log factory,CASSANDRA-5602,12650161,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,cburroughs,cburroughs,cburroughs,30/May/13 15:26,12/Mar/19 14:21,13/Mar/19 22:29,02/Jun/13 04:00,1.2.6,,,,,,0,,,,,"{noformat} 
 $ git diff
diff --git a/src/java/org/apache/cassandra/locator/SnitchProperties.java b/src/java/org/apache/cassandra/locator/SnitchProperties.java
index ae27fbe..809a180 100644
--- a/src/java/org/apache/cassandra/locator/SnitchProperties.java
+++ b/src/java/org/apache/cassandra/locator/SnitchProperties.java
@@ -26,7 +26,7 @@ import org.slf4j.LoggerFactory;
 
 public class SnitchProperties
 {
-    private static final Logger logger = LoggerFactory.getLogger(GossipingPropertyFileSnitch.class);
+    private static final Logger logger = LoggerFactory.getLogger(SnitchProperties.class);
     public static final String RACKDC_PROPERTY_FILENAME = ""cassandra-rackdc.properties"";
     private static Properties properties = new Properties();
{noformat}  
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-06-02 04:00:53.334,,,no_permission,,,,,,,,,,,,330488,,,Sun Jun 02 04:00:53 UTC 2013,,,,,,0|i1l0lr:,330822,,,,,,,,dbrosius,dbrosius,,,,,,,,,,02/Jun/13 04:00;dbrosius;committed as 456e91d182bf8d2cbcb82746cfff6c62af6f0150 to cassandra-1.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitLogReplayer should calculate checksums differently for < 2.0,CASSANDRA-5764,12657926,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius,dbrosius,16/Jul/13 04:42,12/Mar/19 14:21,13/Mar/19 22:29,17/Jul/13 01:09,2.0 beta 2,,,,,,0,,,,,code uses the wrong version to check whether to use old style or new style checksumming.,,,,,,,,,,,,,,,,,,,,,,,,16/Jul/13 04:43;dbrosius;5764.txt;https://issues.apache.org/jira/secure/attachment/12592487/5764.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-16 21:54:20.644,,,no_permission,,,,,,,,,,,,338120,,,Wed Jul 17 01:09:12 UTC 2013,,,,,,0|i1mbj3:,338441,,,,,,,,jbellis,jbellis,,,,,,,,,,"16/Jul/13 21:54;jbellis;Good catch, +1",17/Jul/13 01:09;dbrosius;committed to trunk as faa9d7ebf8c2376ab23d11fd68e2850ef3966288,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose setters for consistency level in Hadoop config helper,CASSANDRA-5827,12660553,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mkmainali,mkmainali,mkmainali,30/Jul/13 04:50,12/Mar/19 14:21,13/Mar/19 22:29,30/Jul/13 14:24,1.2.9,2.0 rc1,,,,,0,,,,,"ConfigHelper exposes the getters for read and write consistency, which defaults to the consistency level of ""ONE"" if one is not defined. However, setters are missing.",,,,,,,,,,,,,,,,,,,,,,,,30/Jul/13 04:51;mkmainali;trunk-CASSANDRA-5827.patch;https://issues.apache.org/jira/secure/attachment/12594868/trunk-CASSANDRA-5827.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-30 14:24:59.991,,,no_permission,,,,,,,,,,,,340744,,,Tue Jul 30 14:24:59 UTC 2013,,,,,,0|i1mrof:,341062,,,,,,,,jbellis,jbellis,,,,,,,,,,30/Jul/13 04:51;mkmainali;Attaching the patch,30/Jul/13 14:24;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when you mistakenly set listen_address to 0.0.0.0,CASSANDRA-5865,12662741,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,mfiguiere,mfiguiere,08/Aug/13 23:41,12/Mar/19 14:21,13/Mar/19 22:29,25/Sep/13 15:10,1.2.11,,,,,,0,,,,,"It's clearly stated that setting {{listen_address}} to {{0.0.0.0}} is always wrong. But if you mistakenly do it anyway you end up with an NPE on 1.2.8 while it's not the case on 2.0.0-rc1. See bellow:

{code}
 INFO 16:34:43,598 JOINING: waiting for ring information
 INFO 16:34:44,505 Handshaking version with /127.0.0.1
 INFO 16:34:44,533 Handshaking version with /0.0.0.0
 INFO 16:35:13,626 JOINING: schema complete, ready to bootstrap
 INFO 16:35:13,631 JOINING: getting bootstrap token
ERROR 16:35:13,633 Exception encountered during startup
java.lang.RuntimeException: No other nodes seen!  Unable to bootstrap.If you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  Otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  Usually, this can be solved by giving all nodes the same seed list.
	at org.apache.cassandra.dht.BootStrapper.getBootstrapSource(BootStrapper.java:154)
	at org.apache.cassandra.dht.BootStrapper.getBalancedToken(BootStrapper.java:135)
	at org.apache.cassandra.dht.BootStrapper.getBootstrapTokens(BootStrapper.java:115)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:666)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:554)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:451)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
java.lang.RuntimeException: No other nodes seen!  Unable to bootstrap.If you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  Otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  Usually, this can be solved by giving all nodes the same seed list.
	at org.apache.cassandra.dht.BootStrapper.getBootstrapSource(BootStrapper.java:154)
	at org.apache.cassandra.dht.BootStrapper.getBalancedToken(BootStrapper.java:135)
	at org.apache.cassandra.dht.BootStrapper.getBootstrapTokens(BootStrapper.java:115)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:666)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:554)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:451)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
Exception encountered during startup: No other nodes seen!  Unable to bootstrap.If you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  Otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  Usually, this can be solved by giving all nodes the same seed list.
ERROR 16:35:13,668 Exception in thread Thread[StorageServiceShutdownHook,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.stopRPCServer(StorageService.java:321)
	at org.apache.cassandra.service.StorageService.shutdownClientServers(StorageService.java:370)
	at org.apache.cassandra.service.StorageService.access$000(StorageService.java:88)
	at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:519)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.lang.Thread.run(Thread.java:724)
{code}",Cassandra 1.2.8,,,,,,,,,,,,,,,,,,,,,,,13/Sep/13 19:02;brandon.williams;5865.txt;https://issues.apache.org/jira/secure/attachment/12603079/5865.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-09 14:48:14.629,,,no_permission,,,,,,,,,,,,342743,,,Wed Sep 25 15:10:05 UTC 2013,,,,,,0|i1n3wv:,343047,,,,,,,,dbrosius,dbrosius,,,,,,,,,,"09/Aug/13 14:48;brandon.williams;You basically can never set listen_address to an unspecified IP address, because gossip needs a way to communicate with the node.",09/Aug/13 15:01;jbellis;Shouldn't we make it reject invalid addresses instead of starting up and NPEing later?,"09/Aug/13 20:05;brandon.williams;We can do that, though the only invalid IP is 0.0.0.0","11/Aug/13 06:32;dbrosius;255.*.*.* and multicast addresses are bad as well, altho probably unlikely.",13/Sep/13 19:02;brandon.williams;Trivial patch to throw when listen_address is 0.0.0.0.,24/Sep/13 13:37;jbellis;Any reason not to check the cases Dave mentioned?,"24/Sep/13 15:16;brandon.williams;There are just too many possibilities, if someone screws up their subnetting.  192.168.0.255 would be invalid in a /24 due to being the broadcast address, but so would 192.168.0.3 in a /30.  255.* is just a special case of this problem.  I don't think anyone is really going to do this, or use multicast, so it doesn't seem worth bothering with.",25/Sep/13 03:20;dbrosius;+1,25/Sep/13 15:10;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicated error messages on directory creation error at startup,CASSANDRA-5818,12660297,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lyubent,mfiguiere,mfiguiere,28/Jul/13 20:47,12/Mar/19 14:21,13/Mar/19 22:29,16/May/14 03:28,2.1 rc2,,,,,,0,,,,,"When I start Cassandra without the appropriate OS access rights to the default Cassandra directories, I get a flood of {{ERROR}} messages at startup, whereas one per directory would be more appropriate. See bellow:

{code}
ERROR 13:37:39,792 Failed to create /var/lib/cassandra/data/system/schema_triggers directory
ERROR 13:37:39,797 Failed to create /var/lib/cassandra/data/system/schema_triggers directory
ERROR 13:37:39,798 Failed to create /var/lib/cassandra/data/system/schema_triggers directory
ERROR 13:37:39,798 Failed to create /var/lib/cassandra/data/system/schema_triggers directory
ERROR 13:37:39,799 Failed to create /var/lib/cassandra/data/system/schema_triggers directory
ERROR 13:37:39,800 Failed to create /var/lib/cassandra/data/system/batchlog directory
ERROR 13:37:39,801 Failed to create /var/lib/cassandra/data/system/batchlog directory
ERROR 13:37:39,801 Failed to create /var/lib/cassandra/data/system/batchlog directory
ERROR 13:37:39,802 Failed to create /var/lib/cassandra/data/system/batchlog directory
ERROR 13:37:39,802 Failed to create /var/lib/cassandra/data/system/peer_events directory
ERROR 13:37:39,803 Failed to create /var/lib/cassandra/data/system/peer_events directory
ERROR 13:37:39,803 Failed to create /var/lib/cassandra/data/system/peer_events directory
ERROR 13:37:39,804 Failed to create /var/lib/cassandra/data/system/compactions_in_progress directory
ERROR 13:37:39,805 Failed to create /var/lib/cassandra/data/system/compactions_in_progress directory
ERROR 13:37:39,805 Failed to create /var/lib/cassandra/data/system/compactions_in_progress directory
ERROR 13:37:39,806 Failed to create /var/lib/cassandra/data/system/compactions_in_progress directory
ERROR 13:37:39,807 Failed to create /var/lib/cassandra/data/system/compactions_in_progress directory
ERROR 13:37:39,808 Failed to create /var/lib/cassandra/data/system/hints directory
ERROR 13:37:39,809 Failed to create /var/lib/cassandra/data/system/hints directory
ERROR 13:37:39,809 Failed to create /var/lib/cassandra/data/system/hints directory
ERROR 13:37:39,811 Failed to create /var/lib/cassandra/data/system/hints directory
ERROR 13:37:39,811 Failed to create /var/lib/cassandra/data/system/hints directory
ERROR 13:37:39,812 Failed to create /var/lib/cassandra/data/system/schema_keyspaces directory
ERROR 13:37:39,812 Failed to create /var/lib/cassandra/data/system/schema_keyspaces directory
ERROR 13:37:39,813 Failed to create /var/lib/cassandra/data/system/schema_keyspaces directory
ERROR 13:37:39,814 Failed to create /var/lib/cassandra/data/system/schema_keyspaces directory
ERROR 13:37:39,814 Failed to create /var/lib/cassandra/data/system/schema_keyspaces directory
ERROR 13:37:39,815 Failed to create /var/lib/cassandra/data/system/range_xfers directory
ERROR 13:37:39,816 Failed to create /var/lib/cassandra/data/system/range_xfers directory
ERROR 13:37:39,817 Failed to create /var/lib/cassandra/data/system/range_xfers directory
ERROR 13:37:39,817 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,818 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,818 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,820 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,821 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,821 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,822 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,822 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,823 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,824 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,824 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,825 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,825 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,827 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,828 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,828 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,829 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,830 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,831 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,831 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,832 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,833 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,834 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,834 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,835 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,836 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,836 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,838 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,838 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,839 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,840 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,840 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,841 Failed to create /var/lib/cassandra/data/system/NodeIdInfo directory
ERROR 13:37:39,849 Failed to create /var/lib/cassandra/data/system/NodeIdInfo directory
ERROR 13:37:39,850 Failed to create /var/lib/cassandra/data/system/NodeIdInfo directory
ERROR 13:37:39,851 Failed to create /var/lib/cassandra/data/system/NodeIdInfo directory
ERROR 13:37:39,854 Failed to create /var/lib/cassandra/data/system/paxos directory
ERROR 13:37:39,855 Failed to create /var/lib/cassandra/data/system/paxos directory
ERROR 13:37:39,857 Failed to create /var/lib/cassandra/data/system/paxos directory
ERROR 13:37:39,859 Failed to create /var/lib/cassandra/data/system/paxos directory
ERROR 13:37:39,859 Failed to create /var/lib/cassandra/data/system/paxos directory
ERROR 13:37:39,860 Failed to create /var/lib/cassandra/data/system/paxos directory
ERROR 13:37:39,865 Failed to create /var/lib/cassandra/data/system/paxos directory
ERROR 13:37:39,866 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 13:37:39,867 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 13:37:39,867 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 13:37:39,868 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 13:37:39,869 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 13:37:39,869 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 13:37:39,870 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 13:37:39,870 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 13:37:39,871 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 13:37:39,872 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 13:37:39,872 Failed to create /var/lib/cassandra/data/system/IndexInfo directory
ERROR 13:37:39,873 Failed to create /var/lib/cassandra/data/system/IndexInfo directory
ERROR 13:37:39,873 Failed to create /var/lib/cassandra/data/system/IndexInfo directory
ERROR 13:37:39,874 Failed to create /var/lib/cassandra/data/system/IndexInfo directory
ERROR 13:37:39,874 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 13:37:39,875 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 13:37:39,876 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 13:37:39,876 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 13:37:39,877 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 13:37:39,877 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 13:37:39,878 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 13:37:39,879 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 13:37:39,879 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 13:37:39,880 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 13:37:39,880 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,881 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,881 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,882 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,883 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,883 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,884 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,885 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,885 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,886 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,887 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,888 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,889 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,889 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,890 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,894 Fatal error: java.io.IOException: Failed to mkdirs /var/lib/cassandra/data
Failed to mkdirs /var/lib/cassandra/data; unable to start server
{code}",,,,,,,,,,,,,,,,,,,,,,,,27/Nov/13 13:02;lyubent;5818_v2.patch;https://issues.apache.org/jira/secure/attachment/12616046/5818_v2.patch,13/May/14 09:55;lyubent;cassandra-2.0-5818_v2.diff;https://issues.apache.org/jira/secure/attachment/12644594/cassandra-2.0-5818_v2.diff,13/Oct/13 14:02;ksaritek;patch.diff;https://issues.apache.org/jira/secure/attachment/12608206/patch.diff,06/Oct/13 04:23;mishail;trunk-5818.patch;https://issues.apache.org/jira/secure/attachment/12607052/trunk-5818.patch,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-07-29 00:21:32.115,,,no_permission,,,,,,,,,,,,340489,,,Fri May 16 03:28:40 UTC 2014,,,,,,0|i1mq3r:,340807,2.0.1,,,,,,,mishail,mishail,,,,,,,,,,"29/Jul/13 00:21;dbrosius;c* should consider switching to logback

http://logback.qos.ch/apidocs/ch/qos/logback/classic/turbo/DuplicateMessageFilter.html",21/Aug/13 23:29;iamaleksey;Moving this to 2.1 (to be able to use logback),"27/Aug/13 05:24;dbrosius;the issue is, you can look for duplicates in two ways:

1) look at duplicates in the format string -> logbacks DuplicateMessageFilter class
2) look at duplicates in the total message -> catch only the pathological runaway logger case (c* potentially supplied filter)

obviously for the this case, you would want 1.

However there are lots of message that are like this, like 

INFO  05:09:36 Initializing system.IndexInfo
INFO  05:09:36 Initializing system.peers
INFO  05:09:36 Initializing system.local
....

etc.

So, i'm not sure c* wants to set anything in logback.xml by default. Certainly the Op can add it, (perhaps we just comment it into the lockback.xml class so they can uncomment if they so desire).","27/Aug/13 07:33;slebresne;Honestly, I think we've been looking at that issue the wrong way. I don't think we should do anything with the general logging here (and I'm -1 on DuplicateMessageFilter given how it works).

What we should do is, in the code that creates the directories at startup, add a check for permissions on the data directory (the same could be done for the commitlog directory btw) that stops startup right away with just one message (which would be more friendly as it would tell what's the problem). On top of that, if we really want to be thorough, we could try creating the data directory first (if it doesn't exist already) and check for errors, and then create the CF directories (instead of trying to create all the CF directories upfront and logging a message for each one when the problem is actually on the top-level data directory).",06/Oct/13 04:23;mishail;There is already a logic which checks _existing_ dirs on start-up. We can extend it to try to create non existing directories and check the result. Attaching the patch just in case,"06/Oct/13 07:03;dbrosius;Not sure how this addresses the original issue? Is this to fix something else?

BTW, this bug report seems to require the removal of assertion exceptions... ie, remove -ea from cassandra-env.sh

","06/Oct/13 17:45;mishail;{quote}
 if we really want to be thorough, we could try creating the data directory first (if it doesn't exist already) and check for errors, and then create the CF directories (instead of trying to create all the CF directories upfront and logging a message for each one when the problem is actually on the top-level data directory)
{quote}

The patch address the issue in the way proposed by [~slebresne]. It tries to create data/commit log/saved caches directories if they don't exist and stops the startup in case of errors.",07/Oct/13 20:11;ksaritek;+1,"08/Oct/13 07:12;slebresne;We should not use asserts here because some people disallow them. The fact that we currently check the directory permissions in an assert is a bug, we should use this ticket to fix it. Nit: while we're at it, could make sense to move that directory checking code in a static method of Directories (like is done with Directories.migrateSSTables()).",08/Oct/13 17:20;ksaritek;attached a patch. Check for  filepermission then trigger exit to not to go whole proccess.,09/Oct/13 01:02;dbrosius;please fix formatting -- curlies on their own lines,"09/Oct/13 03:03;mishail;On Windows 7 (JDK 7u40) with [^patch.diff] applied I get 
{{ERROR 02:59:36 Has no permission for /var/lib/cassandra/data directory}} 
even though dir exists and writable/readable.

I made a quick test and I see that {{AccessController.checkPermission(perm);}} always throws an exception","10/Oct/13 19:59;ksaritek;Mikhail, that was my fault, i traced and see that just the directory that classes are loaded are readonly permitted. Just put an enum and a simple method at Directories. If that approach is ok, will go on it. 

And one more thing, attached patch name is also same as before : patch.diff","11/Oct/13 00:50;dbrosius;I don't understand why hasPrivilege takes a varargs of actions, considering, you are only considering the last one passed in, the way the code is now. Where they supposed to be privilege &= .... ?","11/Oct/13 06:46;ksaritek;You are right, sorry for crappy code.

Could we approach like chmod staff instead of varargs? 
then we will check that have permission like:
7 = 4+2+1 (read/write/execute)
6 = 4+2 (read/write)
5 = 4+1 (read/execute)
4 = 4 (read)
3 = 2+1 (write/execute)
2 = 2 (write)
1 = 1 (execute)


","13/Oct/13 14:03;ksaritek;attached a new path under patch.diff, am I at wrong way?","20/Oct/13 01:10;jbellis;How does that look to you, Mikhail?","20/Oct/13 21:04;mishail;Few comments 
* Permissions in a 1-7 format don't seem very informative for me. I would rather stick with R. X, W, RW, RWX etc combination.
* For existing file objects the patch does not check whether the file can be read and executed, and whether it's directory at all. It only checks if file is writable ({{Directories.FileAction._2}}).
* There is no need to create a new {{File}} object in {{!Directories.FileAction.hasPrivilege(new File(dataDir), Directories.FileAction._2)}} since we already {{dir}} object.","18/Nov/13 23:33;jbellis;Are you still working on this, [~ksaritek]?","27/Nov/13 13:01;lyubent;all the nits pointed out by [~mishail] are corrected in v2. Also changed the logic to:
{code}
check if dir exists
    if not try to create it
if dir does exist
    check permissions
{code}

Moved the directory checking function to o.a.c.db.Directores#hasFullPermissions

Also patch is for cassandra-2.0",27/Nov/13 18:30;mishail;+1,27/Nov/13 18:37;jbellis;committed,02/May/14 15:49;iamaleksey;Should back port to 2.0.,"13/May/14 09:55;lyubent;Patch for cassandra-2.0 on 453a07430c3ebce938047f9d5d0339ff90c6bfcc
",13/May/14 15:22;iamaleksey;[~mishail] can you review/commit the 2.0 backport patch?,16/May/14 03:28;mishail;Commited into 2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Equals without hashcode in SpeculativeRetry,CASSANDRA-6712,12695265,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,14/Feb/14 19:27,12/Mar/19 14:21,13/Mar/19 22:29,17/Feb/14 05:41,2.1 beta1,,,,,,0,,,,,This could cause problems if we were to start using supposed-to-be-equal SR objects in a Hashmap.,,,,,,,,,,,,,,,,,,,,,,,,14/Feb/14 19:27;jbellis;6712.txt;https://issues.apache.org/jira/secure/attachment/12629086/6712.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-14 23:44:28.258,,,no_permission,,,,,,,,,,,,373773,,,Mon Feb 17 05:41:27 UTC 2014,,,,,,0|i1seuf:,374073,,,,,,,,vijay2win@yahoo.com,vijay2win@yahoo.com,,,2.0.0,,,,,,,"14/Feb/14 19:27;jbellis;As near as I can tell, SR objects are never compared with .equals so the simplest solution is to just go back to using the default implementation.",14/Feb/14 23:44;vijay2win@yahoo.com;+1,15/Feb/14 16:07;jbellis;committed.,"17/Feb/14 03:00;iamaleksey;They are compared, in CFMetaData.equals(). I think this revert is the commit that has broken CFMetaData.testConversionsInverses().",17/Feb/14 05:41;jbellis;added back equals and a hashcode implementation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"UTMetaData is missing equals() definition on trunk, breaking KSMetaData and CFMetaData equality",CASSANDRA-6634,12692093,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,benedict,benedict,benedict,29/Jan/14 18:25,12/Mar/19 14:21,13/Mar/19 22:29,30/Jan/14 07:54,2.1 beta1,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Jan/14 18:26;benedict;utmetadata.patch;https://issues.apache.org/jira/secure/attachment/12625912/utmetadata.patch,30/Jan/14 01:19;benedict;utmetadata2.patch;https://issues.apache.org/jira/secure/attachment/12626035/utmetadata2.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-01-29 18:37:11.033,,,no_permission,,,,,,,,,,,,370683,,,Thu Jan 30 07:54:40 UTC 2014,,,,,,0|i1rvy7:,370993,,,,,,,,slebresne,slebresne,,,,,,,,,,"29/Jan/14 18:26;benedict;Attached is a simple fix, that also changes the CFMetaData and KSMetaData equals definitions to make debugging the exact cause of a failure much easier. In particular the EqualsBuilder is completely opaque without downloading the source jars. It doesn't seem to add enough to warrant this annoyance.","29/Jan/14 18:37;slebresne;Wow, testing for nulls every time bugs me *a lot* more that having ""EqualsBuilder opaque"". Wouldn't mind changing to use guava's Objects.Equal() though (if only because that's probably what we use more often in the rest of the code so it brings consistency), but let's not manually repeat the handling of nulls on every line like that.","29/Jan/14 18:42;benedict;I don't see why it's a problem. It's not pretty, but it is more useful. I don't want to find/replace when I want to find out why equality is failing, and that class has a monstrous number of fields. I spent a long time staring at it, saying, it's definitely equal. And it was, as far as you would practically be able to determine with your eyes. Checking the ObjectID of every item isn't something you want (or expect) to do.","29/Jan/14 18:45;benedict;Oh, nevermind, I see what you mean. Sure, no problem with that.",30/Jan/14 01:19;benedict;updated version with Guava Objects.equal(),"30/Jan/14 07:54;slebresne;For what is worth, I still find it less readable and personally think better readability should trump ""someone may need to take 10 seconds to do a quick search&replace to help locate a bug in that equals() when we introduce one every 2 years or so"", but I'm nitpicking and can admit it's probably a personal preference, so +1, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
example pig script doesn't run,CASSANDRA-6329,12678547,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jeromatron,jeromatron,jeromatron,11/Nov/13 10:44,12/Mar/19 14:21,13/Mar/19 22:29,20/Nov/13 16:26,1.2.13,,,,,,0,pig,,,,"The following line in the examples/pig/example-script.pig will not run (using Pig 0.9.2):
{code}
state_footage = FOREACH state_grouped GENERATE GROUP AS State, SUM(state_flat.SquareFeet) AS TotalFeet:int;
{code}

It needs to have a lowercase 'group' because that's the variable name after doing the GROUP operation in the previous line:
{code}
state_footage = FOREACH state_grouped GENERATE group AS State, SUM(state_flat.SquareFeet) AS TotalFeet:int;
{code}

Also, I wonder if it would be good to separate the CassandraStorage and CqlStorage into two different scripts, just so you can run the CqlStorage example out of the box without having to comment out the CassandraStorage example.",,,,,,,,,,,,,,,,,,,,,,,,12/Nov/13 13:41;jeromatron;CASSANDRA-6329.txt;https://issues.apache.org/jira/secure/attachment/12613369/CASSANDRA-6329.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-11 17:00:21.47,,,no_permission,,,,,,,,,,,,357914,,,Wed Nov 20 16:26:16 UTC 2013,,,,,,0|i1pp7z:,358204,1.2.11,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,11/Nov/13 17:00;brandon.williams;Care to submit a patch?,12/Nov/13 13:27;jeromatron;Yep will do.  I just wanted to make a ticket so I didn't forget.,12/Nov/13 13:40;jeromatron;Adding a patch for 1.2 branch.,20/Nov/13 16:26;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in error message in cqlsh (CQL3) when using more than one relation with a IN,CASSANDRA-5647,12653074,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,ruivieira,ruivieira,16/Jun/13 16:38,12/Mar/19 14:21,13/Mar/19 22:29,16/Jun/13 17:10,1.2.6,,,Legacy/Tools,,,0,,,,,"When performing a query such as

bq. select * from foo where bucket in (1) and id in ('O1', 'O2') and bucket in (2) and id in ('O3', 'O4') ;

the error message in cqlsh is:

bq. Bad Request: bucket cannot be restricted by more than one *reation* if it includes a IN

Expected:

bq. Bad Request: bucket cannot be restricted by more than one *relation* if it includes a IN
","Linux 3.8.0-25-generic #37-Ubuntu 13.04 SMP
[cqlsh 3.0.2 | Cassandra 1.2.5-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.36.0] (installed via ccm)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-06-16 17:10:15.742,,,no_permission,,,,,,,,,,,,333397,,,Sun Jun 16 17:10:15 UTC 2013,,,,,,0|i1lihj:,333725,,,,,,,,dbrosius,dbrosius,,,,,,,,,,"16/Jun/13 17:10;dbrosius;Thanks

committed as 155afa1b7f22dedad3fb9c0d7270a101e72be814 to cassandra-1.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL version is wrong either in doc or in code,CASSANDRA-5570,12647754,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,jeromatron,jeromatron,15/May/13 14:49,12/Mar/19 14:21,13/Mar/19 22:29,15/May/13 14:56,,,,,,,0,,,,,"The textile doc in 1.2.2+ for cql3 says it's version 3.0.2.  The code says it's 3.0.1.

In 1.2-branch and trunk it says it is 3.0.2 even though it should be 3.0.3 and 3.1.0 respectively.

https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/cql3/QueryProcessor.java#L44
https://github.com/apache/cassandra/blob/cassandra-1.2/doc/cql3/CQL.textile

https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/cql3/QueryProcessor.java#L43
https://github.com/apache/cassandra/blob/trunk/doc/cql3/CQL.textile",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-05-15 14:56:39.912,,,no_permission,,,,,,,,,,,,328110,,,Wed May 15 14:56:39 UTC 2013,,,,,,0|i1km0f:,328454,,,,,,,,,,,,,,,,,,,15/May/13 14:56;iamaleksey;ninja-resolved,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spelling and grammar errors in cassandra.yaml,CASSANDRA-5471,12642535,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jeromatron,jeromatron,jeromatron,15/Apr/13 13:27,12/Mar/19 14:21,13/Mar/19 22:29,19/Apr/13 23:47,1.2.5,,,,,,0,,,,,There are various spelling and grammar errors in cassandra.yaml.,,,,,,,,,,,,,,,,,,,,,,,,15/Apr/13 13:29;jeromatron;5471.txt;https://issues.apache.org/jira/secure/attachment/12578722/5471.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-15 15:42:32.517,,,no_permission,,,,,,,,,,,,322949,,,Mon Apr 22 15:34:03 UTC 2013,,,,,,0|i1jpwv:,323294,,,,,,,,dbrosius,dbrosius,,,,,,,,,,15/Apr/13 13:29;jeromatron;Also tried to clarify a bit in the row cache description and standardize on \_word_ instead of both that and \*word*.  Patch is against trunk.,15/Apr/13 15:42;dbrosius;+1,19/Apr/13 23:47;dbrosius;committed (all but CAS documentation) to cassandra-1.2 as d5c0c7ae301116e9be8dcb84b91fc948ee931778,22/Apr/13 15:34;dbrosius;committed v2.0 specific changes to trunk as commit c85d4722bee1952756fd1c1f70f45bb6ddba53b2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bloom filter will be loaded when SSTable is opened for batch,CASSANDRA-5938,12665535,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,26/Aug/13 16:04,12/Mar/19 14:21,13/Mar/19 22:29,26/Aug/13 16:44,2.0.0,,,,,,0,,,,,"We are setting bf and then load bf again.

{code}
sstable.bf = FilterFactory.ALWAYS_PRESENT;
sstable.load(true, false); // should be false for 1st arg
{code}
",,,,,,,,,,,,,,,,,,,,,,,,26/Aug/13 16:05;yukim;5983-2.0.0.txt;https://issues.apache.org/jira/secure/attachment/12599963/5983-2.0.0.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-26 16:22:25.492,,,no_permission,,,,,,,,,,,,345475,,,Mon Aug 26 16:44:34 UTC 2013,,,,,,0|i1nkq7:,345776,,,,,,,,jbellis,jbellis,,,2.0 beta 2,,,,,,,"26/Aug/13 16:05;yukim;Also, I think we don't need to create instance for every AlwaysPresentFilter.","26/Aug/13 16:22;jbellis;+1

Nit: prefer to keep ALL_CAPS for primitives; camel case fine for static final instances",26/Aug/13 16:44;yukim;Committed with nit fix.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"nodetool help removenode man page missing status, force, ID options description",CASSANDRA-6617,12691082,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,khahn,khahn,24/Jan/14 17:59,12/Mar/19 14:21,13/Mar/19 22:29,24/Jan/14 18:12,2.1 beta1,,,Legacy/Documentation and Website,,,0,,,,,"Man page OPTIONS section looks like you were interrupted before finishing the description:

OPTIONS
 . . .
        <status>|<force>|<ID>
            The keyspace and column family name",2.1-SNAPSHOT,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-01-24 18:12:12.208,,,no_permission,,,,,,,,,,,,369825,,,Fri Jan 24 18:12:12 UTC 2014,,,,,,0|i1rqof:,370127,,,,,,,,,,,,,,,,,,,24/Jan/14 18:12;brandon.williams;Fixed in b38a905c6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig Storage classes should use IOException instead of RuntimeException,CASSANDRA-6099,12670574,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,alexliu68,alexliu68,alexliu68,25/Sep/13 21:02,12/Mar/19 14:21,13/Mar/19 22:29,25/Sep/13 21:31,1.2.11,,,,,,0,,,,,"If there is anything wrong in the script, a RuntimeException will results the following scripts to error out even though the following scripts are correct.

e.g.
{code}
grunt> newtable1 = LOAD 'cql://test_not_exist/test' USING CqlStorage();
2013-09-25 14:33:38,916 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2999: Unexpected internal error. InvalidRequestException(why:Keyspace 'test_not_exist' does not exist)
Details at logfile: /home/automaton/pig_1380119534487.log
grunt> newtable2 = LOAD 'cql://test_exist/test' USING CqlStorage(); 
2013-09-25 14:33:48,083 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2999: Unexpected internal error. InvalidRequestException(why:Keyspace 'test_not_exist' does not exist)
Details at logfile: /home/automaton/pig_1380119534487.log
{code}
",,,,,,,,,,,,,,,,,,,,,,,,25/Sep/13 21:18;alexliu68;6099-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12605105/6099-1.2-branch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-25 21:31:16.504,,,no_permission,,,,,,,,,,,,350403,,,Wed Sep 25 21:31:16 UTC 2013,,,,,,0|i1of1b:,350696,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"25/Sep/13 21:03;alexliu68;To fix it, we need switch all RuntimeException to IOException",25/Sep/13 21:31;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix typo in HintedHandoffMetrics.java,CASSANDRA-5976,12666782,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,rcoli,rcoli,rcoli,03/Sep/13 23:01,12/Mar/19 14:21,13/Mar/19 22:29,03/Sep/13 23:15,,,,,,,0,,,,,"Summary : There's a typo in this file, ""diffrence"" instead of ""difference"". Attached patch changes all instances of ""diffrence"" to ""difference"".

",,,,,,,,,,,,,,,,,,,,,,,,03/Sep/13 23:01;rcoli;HintedHandoffMetrics.diffrence.to.difference.patch;https://issues.apache.org/jira/secure/attachment/12601262/HintedHandoffMetrics.diffrence.to.difference.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-03 23:15:05.025,,,no_permission,,,,,,,,,,,,346720,,,Tue Sep 03 23:15:05 UTC 2013,,,,,,0|i1nsdz:,347021,,,,,,,,,,,,,,,,,,,03/Sep/13 23:15;iamaleksey;Ninja-committed. Shouldn't to into CHANGES.txt.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should use index_interval from already loaded index summary,CASSANDRA-5731,12656637,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,08/Jul/13 17:35,12/Mar/19 14:21,13/Mar/19 22:29,08/Jul/13 18:53,2.0 beta 1,,,,,,0,,,,,"Since index_interval can be changed anytime in 2.0, SSTableReader should use index interval from loaded summary.",,,,,,,,,,,,,,,,,,,,,,,,08/Jul/13 17:36;yukim;5731.txt;https://issues.apache.org/jira/secure/attachment/12591234/5731.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-08 17:45:56.341,,,no_permission,,,,,,,,,,,,336860,,,Mon Jul 08 18:53:37 UTC 2013,,,,,,0|i1m3rz:,337183,,,,,,,,jbellis,jbellis,,,,,,,,,,08/Jul/13 17:36;yukim;Fix attached.,08/Jul/13 17:45;jbellis;+1,08/Jul/13 18:53;yukim;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StandaloneScrubber assumes old-style json leveled manifest,CASSANDRA-6005,12667986,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,krummas,krummas,krummas,11/Sep/13 06:17,12/Mar/19 14:21,13/Mar/19 22:29,13/Sep/13 07:20,2.0.1,,,,,,0,lcs,,,,"With standalone scrubber in 2.0 we can encounter both the old-style json manifest and the new way, StandaloneScrubber needs to handle this.",,,,,,,,,,,,,,,,,,,,,,,,11/Sep/13 06:18;krummas;0001-Make-StandaloneScrubber-handle-new-leveled-manifest.patch;https://issues.apache.org/jira/secure/attachment/12602536/0001-Make-StandaloneScrubber-handle-new-leveled-manifest.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-12 14:25:11.663,,,no_permission,,,,,,,,,,,,347921,,,Fri Sep 13 07:20:08 UTC 2013,,,,,,0|i1nzrj:,348217,,,,,,,,,,,,,,,,,,,12/Sep/13 14:25;jbellis;+1,"13/Sep/13 07:20;krummas;committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update version in build.xml to reflect 2.1,CASSANDRA-6015,12668295,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,enigmacurry,enigmacurry,12/Sep/13 16:05,12/Mar/19 14:21,13/Mar/19 22:29,12/Sep/13 16:16,,,,,,,0,,,,,"The build.xml still marks the base.version as 2.0.0 in trunk.

This [breaks ccm|https://github.com/pcmanus/ccm/issues/73], as it's checking for this version number when it's doing things with 2.1 features (like logback logging, instead of log4j)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-09-12 16:16:02.415,,,no_permission,,,,,,,,,,,,348229,,,Thu Sep 12 16:16:02 UTC 2013,,,,,,0|i1o1nr:,348525,,,,,,,,,,,,,,,,,,,12/Sep/13 16:16;brandon.williams;Done.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
use slf4j not commons-logging,CASSANDRA-5464,12642363,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius,dbrosius,13/Apr/13 04:02,12/Mar/19 14:21,13/Mar/19 22:29,13/Apr/13 05:05,1.1.11,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Apr/13 04:03;dbrosius;5464.txt;https://issues.apache.org/jira/secure/attachment/12578578/5464.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-13 04:51:00.906,,,no_permission,,,,,,,,,,,,322777,,,Sat Apr 13 05:05:34 UTC 2013,,,,,,0|i1jouv:,323122,,,,,,,,jbellis,jbellis,,,,,,,,,,13/Apr/13 04:51;jbellis;+1,13/Apr/13 05:05;dbrosius;committed to cassandra-1.1 as commit 822bda77aba24fda8b234590bedb292e32443aa5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
realclean does not show up with ant -p,CASSANDRA-5356,12637542,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,cburroughs,cburroughs,cburroughs,18/Mar/13 14:24,12/Mar/19 14:21,13/Mar/19 22:29,23/Mar/13 04:07,1.2.4,,,Packaging,,,0,,,,,"This has bothered me for years!

-    <target name=""realclean"" depends=""clean"">
+    <target name=""realclean"" depends=""clean"" description=""Remove the entire build directory and downloaded artifacts"">
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-03-23 04:07:21.087,,,no_permission,,,,,,,,,,,,318033,,,Sat Mar 23 04:07:21 UTC 2013,,,,,,0|i1ivk7:,318374,,,,,,,,,,,,,,,,,,,"23/Mar/13 04:07;jbellis;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Suppress custom exceptions thru jmx,CASSANDRA-5652,12653349,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius,dbrosius,18/Jun/13 00:03,12/Mar/19 14:20,13/Mar/19 22:29,18/Jun/13 12:48,1.2.6,,,Legacy/Tools,,,0,,,,,"startNativeTransport, can send back 

org.jboss.netty.channel.ChannelException

which causes jconsole to puke with a bad message such as

Problem invoking startNativeTransport: java.rmi.UnmarshalException: Error unmarshaling return header; nested exception is: java.io.EOFException


convert to RuntimeException so you get something like:

org.jboss.netty.channel.ChannelException: Failed to bind to: localhost/127.0.0.1:9042",,,,,,,,,,,,,,,,,,,,,,,,18/Jun/13 00:04;dbrosius;5652.txt;https://issues.apache.org/jira/secure/attachment/12588262/5652.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-18 11:35:48.036,,,no_permission,,,,,,,,,,,,333627,,,Tue Jun 18 20:52:50 UTC 2013,,,,,,0|i1ljwn:,333955,,,,,,,,slebresne,slebresne,,,,,,,,,,"18/Jun/13 11:35;slebresne;lgtm, though provided jconsole can handle it, I'd replace the exception by something like:
{noformat}
new RuntimeException(""Error starting native transport: "" + e.getMessage(), e);
{noformat}
so we don't lose the original stack trace (but if it's still too hard to handle for JMX, let's just ship it as in you patch).","18/Jun/13 12:39;dbrosius;yea, unfortunately jmx will still CNFE if you pass e as the initcause, but i will change the error message.",18/Jun/13 12:48;dbrosius;committed to cassandra-1.2 as f30015c862eb913d1f0cf8c10d201de5698a6dda,"18/Jun/13 20:52;dbrosius;really, imo mbeans should be seperate objects that impl the interface and just call into the real objects, so that these mbean objects can do the exception sanitization stuff (and other cleaning) outside of real code. but, probably just being pendantic.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in error msg in cqlsh: Bad Request: Only superusers are allowed to perfrom CREATE USER queries,CASSANDRA-6195,12673629,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,harisekhon,harisekhon,13/Oct/13 21:47,12/Mar/19 14:20,13/Mar/19 22:29,13/Oct/13 22:56,1.2.11,2.0.2,,,,,0,,,,,"Typo in error message ""perfrom"" instead of ""perform"":

cqlsh
Connected to MyCluster1 at x.x.x.x:9160.
[cqlsh 4.0.1 | Cassandra 2.0.1 | CQL spec 3.0.0 | Thrift protocol 19.37.0]
Use HELP for help.
cqlsh> create user hari with password 'mypass';
Bad Request: Only superusers are allowed to perfrom CREATE USER queries",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-10-13 22:56:22.978,,,no_permission,,,,,,,,,,,,353252,,,Sun Oct 13 22:56:22 UTC 2013,,,,,,0|i1owkf:,353545,2.0.1,,,,,,,,,,,,,,,,,,13/Oct/13 22:56;brandon.williams;Fixed in 4284d98,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool fails with java.lang.UnsatisfiedLinkError (cannot find libjemalloc.so) when JEMalloc is configured,CASSANDRA-6301,12677619,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,ngrigoriev,ngrigoriev,05/Nov/13 18:30,12/Mar/19 14:20,13/Mar/19 22:29,06/Nov/13 00:56,2.0.3,,,Tool/nodetool,,,0,,,,,"{code}
>/opt/apache-cassandra-2.0.2/bin/nodetool ring
Note: Ownership information does not include topology; for complete information, specify a keyspace

Datacenter: DC1
==========
Address      Rack        Status State   Load            Owns                Token
                                                                            9208241795664305161
10.3.45.160  r1          Up     Normal  125.88 GB       16.41%              -9222548266947385654
10.3.45.160  r1          Up     Normal  125.88 GB       16.41%              -9177629719965963707
10.3.45.160  r1          Up     Normal  125.88 GB       16.41%              -9039272433194428886
10.3.45.160  r1          Up     Normal  125.88 GB       16.41%              -9037742357058937987
...
Exception in thread ""main"" java.lang.UnsatisfiedLinkError: Unable to load library 'jemalloc': libjemalloc.so: cannot open shared object file: No such file or directory
        at com.sun.jna.NativeLibrary.loadLibrary(NativeLibrary.java:164)
        at com.sun.jna.NativeLibrary.getInstance(NativeLibrary.java:237)
        at com.sun.jna.Library$Handler.<init>(Library.java:140)
        at com.sun.jna.Native.loadLibrary(Native.java:375)
        at com.sun.jna.Native.loadLibrary(Native.java:359)
        at org.apache.cassandra.io.util.JEMallocAllocator.<init>(JEMallocAllocator.java:36)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at java.lang.Class.newInstance(Class.java:374)
        at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:488)
        at org.apache.cassandra.utils.FBUtilities.newOffHeapAllocator(FBUtilities.java:438)
        at org.apache.cassandra.config.DatabaseDescriptor.applyConfig(DatabaseDescriptor.java:442)
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:104)
        at org.apache.cassandra.tools.NodeCmd.printRing(NodeCmd.java:286)
        at org.apache.cassandra.tools.NodeCmd.main(NodeCmd.java:1092)
{code}

In my conf/cassandra-env.sh I have:

{code}
# Configure the following for JEMallocAllocator and if jemalloc is not available in the system
# library path (Example: /usr/local/lib/). Usually ""make install"" will do the right thing.
export LD_LIBRARY_PATH=/usr/local/lib
JVM_OPTS=""$JVM_OPTS -Djava.library.path=/usr/local/lib/""
{code}

I believe this file is not sourced by the tools, this is why a tool that might need that library cannot find it.
","Linux
Cassandra 2.0.2
libjemalloc.so in /usr/local/lib",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-11-05 19:07:12.404,,,no_permission,,,,,,,,,,,,356994,,,Wed Nov 06 00:55:08 UTC 2013,,,,,,0|i1pjk7:,357284,2.0.2,,,,,,,,,,,,,,,,,,"05/Nov/13 19:07;brandon.williams;It is sourced by the tools, however the don't use JVM_OPTS (and it probably doesn't make sense to.)  I don't think we need a fancy allocator for any of the tools.","05/Nov/13 20:27;ngrigoriev;Hmmm...I do not think it is sourced by nodetool.

{code}
/opt/apache-cassandra-2.0.2 >grep cassandra-env.sh bin/* conf/*
bin/cassandra:if [ -f ""$CASSANDRA_CONF/cassandra-env.sh"" ]; then
bin/cassandra:    . ""$CASSANDRA_CONF/cassandra-env.sh""
bin/debug-cql:if [ -f ""$CASSANDRA_CONF/cassandra-env.sh"" ]; then
bin/debug-cql:    . ""$CASSANDRA_CONF/cassandra-env.sh""
conf/cassandra-env.sh:        echo ""please set or unset MAX_HEAP_SIZE and HEAP_NEWSIZE in pairs (see cassandra-env.sh)""
conf/cassandra-topology.properties:# in cassandra-env.sh
conf/cassandra.yaml:# modify cassandra-env.sh as directed in the file.
{code}

I see that nodetool invokes JAVA directly, so it is sourced only by cassandra itself and debug-cql. And it seems to me that sourcing cassandra-env.sh from the tools would not be appropriate - it contains a number of settings that are server-specific all packed in JVM_OPTS. Or I am missing something...

The allocator is probably not needed, but, in general, a tool might need a native library that is used by the server too. Maybe I am using wrong example but something like native snappy library...so probably it may make sense to use the same set of native libraries for the tools as for the server itself. Even if not really needed now this may be more useful in the future. Just my two cents.","05/Nov/13 20:45;brandon.williams;Actually it does after CASSANDRA-6273, so you might see if that patch helps.","06/Nov/13 00:55;ngrigoriev;[~brandon.williams]

Indeed, your fix indirectly fixes this problem because by sourcing cassandra-end.sh it effectively does this:

{code}
export LD_LIBRARY_PATH=/usr/local/lib
{code}

I have applied your patch to test it to confirm that it does help. I took the liberty of closing this issue and linking it to CASSANDRA-6273. Thanks!
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool clearsnapshot incorrectly reports to have requested a snapshot,CASSANDRA-5478,12642699,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,cathodion,cathodion,16/Apr/13 10:34,12/Mar/19 14:20,13/Mar/19 22:29,25/May/13 17:02,1.2.6,,,Tool/nodetool,,,0,exception-reporting,lhf,nodetool,,"When running ""nodetool clearsnapshot"" all existing snapshots are removed, but the following message is printed:

./nodetool clearsnapshot
Requested snapshot for: all keyspaces 

Instead it should just print a single line stating that all snapshots have been removed.","MacOS, Datastax Cassandra 1.2.2",,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,25/May/13 00:54;dbrosius;5478.txt;https://issues.apache.org/jira/secure/attachment/12584805/5478.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-25 00:54:18.997,,,no_permission,,,,,,,,,,,,323113,,,Sat May 25 17:02:06 UTC 2013,,,,,,0|i1jqxb:,323458,,,,,,,,jbellis,jbellis,,,,,,,,,,"25/May/13 00:54;dbrosius;changed to

Requested creating snapshot for:

or

Requested clearing snapshot for:",25/May/13 13:33;jbellis;+1,25/May/13 17:02;dbrosius;committed to cassandra-1.2 as 7c9f17f9525c7736010be31f596888cccd398cfd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compression chunk_length shouldn't be mandatory,CASSANDRA-5707,12654959,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,26/Jun/13 16:20,12/Mar/19 14:20,13/Mar/19 22:29,27/Jun/13 07:57,2.0 beta 1,,,,,,0,,,,,CASSANDRA-5693 introduced a minor regression on trunk in that we refuse compression parameters that don't have the chunk_length option. This is not the case in 1.2 where we just use the default.,,,,,,,,,,,,,,,,,,,,,,,,26/Jun/13 16:21;slebresne;5707.txt;https://issues.apache.org/jira/secure/attachment/12589770/5707.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-26 18:04:17.223,,,no_permission,,,,,,,,,,,,335236,,,Thu Jun 27 07:57:08 UTC 2013,,,,,,0|i1ltrr:,335560,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,26/Jun/13 16:21;slebresne;Trivial patch attached.,26/Jun/13 18:04;iamaleksey;+1,"27/Jun/13 07:57;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix help text for stress counterwrite,CASSANDRA-6824,12699500,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,benedict,rhatch,rhatch,07/Mar/14 20:45,12/Mar/19 14:20,13/Mar/19 22:29,13/Mar/14 21:57,2.1 rc2,,,,,,0,,,,,"the help output for counterwrite shows 'counteradd' in the syntax instead of 'counterwrite'.
{noformat}
rhatch@whatup:~/git/cstar/cassandra/tools$ ./bin/cassandra-stress help counterwrite

Usage: counteradd [err<?] [n>?] [n<?] [tries=?] [ignore_errors] [cl=?]
 OR 
Usage: counteradd n=? [tries=?] [ignore_errors] [cl=?]

  err<? (default=0.02)                     Run until the standard error of the mean is below this fraction
  n>? (default=30)                         Run at least this many iterations before accepting uncertainty convergence
  n<? (default=200)                        Run at most this many iterations before accepting uncertainty convergence
  tries=? (default=9)                      Number of tries to perform for each operation before failing
  ignore_errors                            Do not print/log errors
  cl=? (default=ONE)                       Consistency level to use
  n=?                                      Number of operations to perform
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,07/Mar/14 23:09;benedict;6824-2.txt;https://issues.apache.org/jira/secure/attachment/12633482/6824-2.txt,07/Mar/14 20:47;rhatch;trunk-6824.txt;https://issues.apache.org/jira/secure/attachment/12633455/trunk-6824.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-03-07 23:09:34.354,,,no_permission,,,,,,,,,,,,377847,,,Thu Mar 13 21:57:38 UTC 2014,,,,,,0|i1t3vz:,378139,2.1 rc3,,,,,,,xedin,xedin,,,,,,,,,,07/Mar/14 20:47;rhatch;attaching patch,"07/Mar/14 23:09;benedict;Actually it looks like this was a minor bug - it should accept both commands. Had a quick look and this was only functioning for printing of help, not for running the commands, which was a bit of a mistake. 

Attached a patch that fixes this, and also adds a line stating ""help <command>"" right at the top, as I had a number of queries from people about how on earth it worked that were mostly solved by giving them that command; it's not obvious from its position in the normal print out that it would be so useful.",13/Mar/14 18:09;jbellis;([~xedin] to review),"13/Mar/14 18:37;benedict;[~xedin] I've pushed a [branch|https://github.com/belliottsmith/cassandra/tree/iss-6824] merged with CASSANDRA-6835 to make things easier for you, as they would conflict.

",13/Mar/14 21:57;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transposed KS/CF arguments,CASSANDRA-5362,12637842,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,j.casares,j.casares,19/Mar/13 22:19,12/Mar/19 14:20,13/Mar/19 22:29,20/Mar/13 08:43,1.2.4,,,,,,0,datastax_qa,,,,"*Reproduction*
Using https://github.com/joaquincasares/java-driver's integrationtests branch, run `mvn test` from the root directory.

*Issue*
The test will fail due to https://github.com/joaquincasares/java-driver/blob/integrationtests/driver-core/src/main/java/com/datastax/driver/core/ResultSetFuture.java being swapped here:
{CODE}
case ALREADY_EXISTS:
    org.apache.cassandra.exceptions.AlreadyExistsException aee = (org.apache.cassandra.exceptions.AlreadyExistsException)te;
    return new AlreadyExistsException(aee.ksName, aee.cfName);
{CODE}

*Error*
{CODE}
repeatSchemaDefinition(com.datastax.driver.core.ExceptionsTest)  Time elapsed: 0.501 sec  <<< FAILURE!
org.junit.ComparisonFailure: expected:<Table repeatschema[ks.repeatschemacf] already exists> but was:<Table repeatschema[cf.repeatschemaks] already exists>
{CODE}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-03-20 08:43:08.619,,,no_permission,,,,,,,,,,,,318322,,,Wed Mar 20 08:43:08 UTC 2013,,,,,,0|i1ixcf:,318663,,,,,,,,,,,,,,,,,,,20/Mar/13 08:43;slebresne;The transposition was in MigrationManager.announceNewColumnFamily. Took the liberty to commit (c1332ef) directly without review as this is a trivial fix.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-shuffle is not available for windows,CASSANDRA-5753,12657550,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,hsn,hsn,13/Jul/13 00:35,12/Mar/19 14:20,13/Mar/19 22:29,13/Jul/13 21:15,1.2.7,2.0 beta 2,,Legacy/Tools,,,0,,,,,windows version (.BAT file) of cassandra-shuffle utility is missing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-07-13 21:15:38.496,,,no_permission,,,,,,,,,,,,337771,,,Sat Jul 13 21:15:38 UTC 2013,,,,,,0|i1m9dr:,338093,,,,,,,,,,,,,,,,,,,13/Jul/13 11:40;hsn;easy to fix. copy cassandra-cli.bat and change name of main class,13/Jul/13 21:15;jbellis;Done in fa327aa699d4c6a72ea0a194c129d40e48d43024,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Link for latest bleeding edge releases is broken,CASSANDRA-5621,12651470,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,anssssss,anssssss,06/Jun/13 21:03,12/Mar/19 14:20,13/Mar/19 22:29,13/Mar/14 23:08,,,,Legacy/Documentation and Website,,,0,,,,,"This issue is just like [CASSANDRA-2736|https://issues.apache.org/jira/browse/CASSANDRA-2736]. The builds link on this [page|https://cassandra.apache.org/download/] points to [Jenkins continuous integration|https://builds.apache.org/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/] which reports a 404 status.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,331796,,,2013-06-06 21:03:46.0,,,,,,0|i1l8n3:,332125,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add additional limits in cassandra.conf provided by Debian package,CASSANDRA-6104,12670767,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,jblangston@datastax.com,jblangston@datastax.com,26/Sep/13 16:39,12/Mar/19 14:20,13/Mar/19 22:29,19/Mar/14 14:06,1.2.16,2.0.7,2.1 beta2,Packaging,,,0,,,,,"/etc/security/limits.d/cassandra.conf distributed with DSC deb/rpm packages should contain additional settings. We have found these limits to be necessary for some customers through various support tickets.

{code}
cassandra - memlock  unlimited
cassandra - nofile  100000
cassandra - nproc 32768
cassandra - as unlimited
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-09-26 16:54:53.966,,,no_permission,,,,,,,,,,,,350596,,,Wed Mar 19 14:06:46 UTC 2014,,,,,,0|i1og87:,350889,,,,,,,,,,,,,,,,,,,"26/Sep/13 16:54;brandon.williams;We already have these two:

{noformat}
cassandra  -  memlock  unlimited
cassandra  -  nofile   100000
{noformat}

32k threads seems a little overboard.","19/Mar/14 14:06;brandon.williams;Committed the address space change as unlimited, and nproc at the reasonable bound of 8096, since going beyond that would suggest that either you need HSHA, or mask a bug on our side.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift validation refuses row markers on CQL3 tables,CASSANDRA-6081,12670045,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,23/Sep/13 12:35,12/Mar/19 14:20,13/Mar/19 22:29,23/Sep/13 14:43,2.0.2,,,,,,0,,,,,CASSANDRA-5138 don't let row markers pass. It should.,,,,,,,,,,,,,,,,,,,,,,,,23/Sep/13 12:36;slebresne;6081.txt;https://issues.apache.org/jira/secure/attachment/12604579/6081.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-23 13:36:23.904,,,no_permission,,,,,,,,,,,,349875,,,Mon Sep 23 14:43:06 UTC 2013,,,,,,0|i1obtb:,350173,,,,,,,,jbellis,jbellis,,,2.0.0,,,,,,,23/Sep/13 12:36;slebresne;Trivial patch attached.,23/Sep/13 13:36;jbellis;+1,"23/Sep/13 14:43;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstablemetadata{.bat} location inconsistency,CASSANDRA-6562,12688104,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,cburroughs,cburroughs,09/Jan/14 17:19,12/Mar/19 14:20,13/Mar/19 22:29,07/May/14 14:33,1.2.17,2.0.9,2.1 rc1,Legacy/Tools,,,0,lhf,,,,"tools/bin has  sstablemetadata and  sstablemetadata.bat https://github.com/apache/cassandra/tree/cassandra-2.0/bin

while bin has only  sstablemetadata.bat https://github.com/apache/cassandra/tree/cassandra-2.0/bin

This is confusing.  Not sure which is intended.","1,2,0,trunk",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-02-24 14:54:31.916,,,no_permission,,,,,,,,,,,,367110,,,Wed May 07 14:33:52 UTC 2014,,,,,,0|i1ra2f:,367420,,,,,,,,,,,,,,,,,,,24/Feb/14 14:54;jbellis;[~sdelmas]?,07/May/14 14:33;brandon.williams;I fixed this in 64394b25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation cites three but only two rpc_server_types provided,CASSANDRA-6159,12672718,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,rektide,rektide,rektide,07/Oct/13 23:02,12/Mar/19 14:20,13/Mar/19 22:29,09/Oct/13 14:05,2.0.2,,,Legacy/Documentation and Website,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Oct/13 23:05;rektide;0001-Out-of-the-box-rpc_server_type-set-is-sync-hsha-thes.patch;https://issues.apache.org/jira/secure/attachment/12607262/0001-Out-of-the-box-rpc_server_type-set-is-sync-hsha-thes.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-08 00:32:55.415,,,no_permission,,,,,,,,,,,,352341,,,Wed Oct 09 14:05:31 UTC 2013,,,,,,0|i1oqxj:,352629,,,,,,,,jbellis,jbellis,,,,,,,,,,"07/Oct/13 23:06;rektide;jira, how does one work you? typically submit patch includes a form to link or include a patch. what are you jira?","07/Oct/13 23:08;rektide;Patch attached and pull request submitted.  https://github.com/apache/cassandra/pull/22 . JIRA's ""submit patch"" button didn't give me any apparent ways to attach or link to any specific diffs. I have no idea wtf, whatever.","08/Oct/13 00:32;brandon.williams;More->Attach files will let you attach files.  ""Submit patch"" is purely a workflow state, it's confusing I know.",09/Oct/13 14:05;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 Batch statement memory leak,CASSANDRA-6107,12670953,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lyubent,cowardlydragon,cowardlydragon,27/Sep/13 15:18,12/Mar/19 14:05,13/Mar/19 22:29,06/Oct/13 16:29,1.2.11,,,Legacy/CQL,,,0,,,,,"We are doing large volume insert/update tests on a CASS via CQL3. 


Using 4GB heap, after roughly 750,000 updates create/update 75,000 row keys, we run out of heap, and it never dissipates, and we begin getting this infamous error which many people seem to be encountering:

WARN [ScheduledTasks:1] 2013-09-26 16:17:10,752 GCInspector.java (line 142) Heap is 0.9383457210434385 full.  You may need to reduce memtable and/or cache sizes.  Cassandra will now flush up to the two largest memtables to free up memory.  Adjust flush_largest_memtables_at threshold in cassandra.yaml if you don't want Cassandra to do this automatically
 INFO [ScheduledTasks:1] 2013-09-26 16:17:10,753 StorageService.java (line 3614) Unable to reduce heap usage since there are no dirty column families


8 and 12 GB heaps appear to delay the problem by roughly proportionate amounts of 75,000 - 100,000 rowkeys per 4GB. Each run of 50,000 row key creations sees the heap grow and never shrink again. 

We have attempted to no effect:
- removing all secondary indexes to see if that alleviates overuse of bloom filters 
- adjusted parameters for compaction throughput
- adjusted memtable flush thresholds and other parameters 

By examining heapdumps, it seems apparent that the problem is perpetual retention of CQL3 BATCH statements. We have even tried dropping the keyspaces after the updates and the CQL3 statement are still visible in the heapdump, and after many many many CMS GC runs. G1 also showed this issue.

The 750,000 statements are broken into batches of roughly 200 statements.","- CASS version: 1.2.8 or 2.0.1, same issue seen in both
- Running on OSX MacbookPro
- Sun JVM 1.7
- Single local cassandra node
- both CMS and G1 GC used
- we are using the cass-JDBC driver to submit our batches

",,,,,,,,,,,,,,,,,,CASSANDRA-6293,,,,,04/Oct/13 19:18;jbellis;6107-v4.txt;https://issues.apache.org/jira/secure/attachment/12606863/6107-v4.txt,02/Oct/13 23:30;lyubent;6107.patch;https://issues.apache.org/jira/secure/attachment/12606480/6107.patch,03/Oct/13 14:39;lyubent;6107_v2.patch;https://issues.apache.org/jira/secure/attachment/12606584/6107_v2.patch,04/Oct/13 15:15;lyubent;6107_v3.patch;https://issues.apache.org/jira/secure/attachment/12606805/6107_v3.patch,03/Oct/13 15:01;lyubent;Screen Shot 2013-10-03 at 17.59.37.png;https://issues.apache.org/jira/secure/attachment/12606586/Screen+Shot+2013-10-03+at+17.59.37.png,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2013-09-27 16:08:11.12,,,no_permission,,,,,,,,,,,,350782,,,Thu Jan 30 16:14:44 UTC 2014,,,,,,0|i1ohcv:,351073,1.2.10,,,,,,,jbellis,jbellis,,,,,,,,,,"27/Sep/13 15:22;cowardlydragon;- Further examination of several point-in-time heap dumps show that ALL cql statement batches are retained in the heap. Each statement has multiple collections such as ConcurrentHashMap and other data structures which will obviously consume huge amounts of resources.

- We have run a smaller run that does NOT batch our updates. It is obviously much slower, but the heap dumps show over time objects being garbage collected propertly.

","27/Sep/13 15:58;cowardlydragon;It appears that since we are sending preparedStatements (this allows us to prep the statement and then set the consistency level), that the preparedStatements are never evicted from the prepared statement cache in org.apache.cassandra.cql3.QueryProcessor

There are no removes ever done to preparedStatements or thriftPreparedStatements...

This may technically be our fault for preparing every single batch statement, but shouldn't there be a limit on stored prep statements with LRU eviction?","27/Sep/13 16:08;slebresne;bq. but shouldn't there be a limit on stored prep statements with LRU eviction?

There is (from QueryProcessor.java):
{noformat}
    public static final int MAX_CACHE_PREPARED = 100000; // Enough to keep buggy clients from OOM'ing us
    private static final Map<MD5Digest, CQLStatement> preparedStatements = new ConcurrentLinkedHashMap.Builder<MD5Digest, CQLStatement>()
                                                                               .maximumWeightedCapacity(MAX_CACHE_PREPARED)
                                                                               .build();
{noformat}
but it's possible that this limit was too high to prevent you from OOM'ing in that case. And maybe that hard-coded is too high

But really, you should not prepare every single statement, it is a client error.","27/Sep/13 16:10;cowardlydragon;It appears you are using a MAX_CACHE_PREPARED of 100,000, and COncurrentLinkedHashMap should use that as an evictor.

If the individual keys for 200 line batch statements are large (say, 10k, which I think based on the heap dump they consist of 1 map per statement in the batch possibly, so that is easily possible). So 100000 x 100000 bytes per statement = 10 gigabytes... uhoh. 

I think 600,000 updates, which are 3000 batches of 200 statements each popped the heap for a 4GB. I figure 1 GB of that heap is used for filters/sstables/memtables/etc, so 3000 batches popped 3GB of heap, so a megabyte per batch.

Can we expose the MAX_CACHE_PREPARED as a config parameter?","27/Sep/13 16:14;cowardlydragon;I agree, there is client dysfunction here... we're going to stop prepping the statements, if possible (I think the cass jdbc project may have required prepping to set the consistency level, which sucks, but let me verify).
","27/Sep/13 16:22;cowardlydragon;We're using SpringJDBC on top of the cass-jdbc driver. In order to intercept the update and specify consistency, that is only convenient with a PreparedStatementCreator...

so we will not use SpringJDBC/PreparedStatementCreator and instead do a more manual JDBC call...

sorry for the ""critical"" spam...",27/Sep/13 16:30;jbellis;Could we fix the OOM by adding a weight to the Map entry instead of assuming all entries are equal?,"27/Sep/13 16:48;slebresne;Given that prepared is not performance sensitive, I suppose we could even use jmeter to get the precise in-memory size, and then cap the prepared statements to some percentage of the heap.","27/Sep/13 16:53;jbellis;That sounds reasonable.

I'd actually just pin it as something pretty small like 1MB; with CASSANDRA-4693 we shouldn't need to prepare monstrous batches.","27/Sep/13 17:00;cowardlydragon;Yep, removing statement preparation looks good! Heap is GC'ing, and multiple runs can be done.",02/Oct/13 23:29;lyubent;Added a MemoryMeter to QueryProcessor#getStatement to track the size of each ParsedStatement that is Prepared. If the statement is more than 1MB (1048576 bytes) than an IllegalArgumentsException is thrown. Also added the size of the prepared statement in the tracing line inside of QueryProcessor#getStatement. ,"02/Oct/13 23:42;iamaleksey;I don't think the issue here is (just) large individual prepared statements. It's the total size that all the prepared statements are occupying. That's what should be tracked and limited, not just the individual ones.",02/Oct/13 23:59;jbellis;Right.  Use the size you're calculating as the weight in the cache Map.,"03/Oct/13 14:39;lyubent;Changed the MemoryMeter to measure the full map rather than enforcing restrictions on individual prepared statements. The hardcoded maximum is 100MB for the thrift cache and 100MB for the CQL cache. 

","03/Oct/13 15:01;jbellis;Use the cache weigher/weightedCapacity api instead of re-measuring the entire cache each time.  then the cache will take care of evicting old ones to make room as needed.

suggest making the capacity 1/256 of heap size.

should probably have a separate setting for maximum single statement size.  if a single statement is under this threshold but larger than the cache, execute it but do not cache it.

finally, statementid size should be negligible, i'd leave that out.
",03/Oct/13 15:01;lyubent;Memory Usage graph for batches of 'insert' statements with varying numbers of columns.  ,"04/Oct/13 15:15;lyubent;Set the total cache for the statements to 1/256 of the heap (for thrift and cql), set the individual statement limit to 1MB. If a statement is less than 1MB but too large for the cache, it's executed but not stored in the cache.","04/Oct/13 19:18;jbellis;On second thought, rejecting really huge statements should be done at the protocol level.  I'll follow up with Sylvain to see if we're already doing that.

v4 attached that just does the weighing as discussed.  WDYT?",06/Oct/13 10:18;lyubent;LGTM. But i was able to build some pretty big batch statements ( >4MB ) so I'm not sure about the rejection of large statements at protocol level.,"06/Oct/13 15:32;jbellis;bq. I'm not sure about the rejection of large statements at protocol level.

I think that's what CASSANDRA-5981 is open for, actually.",06/Oct/13 16:29;jbellis;commented,"07/Oct/13 07:54;slebresne;CASSANDRA-5981 is indeed about limiting the size at the protocol level. However it's a global frame limitation. In particular this is the hard limit for queries with their values and for that reason the current hard-coded limit is relatively high (256MB). And we can bikeshed on the exact default to user and CASSANDRA-5981 will probably allow the user to play with that limit, but in any case, it will definitively have to be higher than the 1MB. The other detail is that the limit done by CASSANDRA-5981 is on the sent bytes, not the in-memory size of the query, but that probably don't matter much.

Anyway, provided that a prepared statement doesn't include values, it wouldn't be absurd to have a specific, lower limit on their size. Though my own preference would be to just leave it to a global limit on the preparedStatements cache map (but it could make sense to reject statements that blow up the entire limit on their own, so as to make sure to respect it). Too many hard-coded limitations make me nervous.",03/Nov/13 16:48;mikeoz;This change appears to break code that uses EmbeddedCassandraService and PreparedStatements in version 1.2.11.  Please see CASSANDRA-6293 for details.,30/Jan/14 16:14;jbellis;Note: this was reverted in 1.2.14 because of CASSANDRA-6592.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig with widerows=true and batch size = 1 works incorrectly,CASSANDRA-6114,12671448,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,alexliu68,alexliu68,alexliu68,30/Sep/13 21:29,12/Mar/19 14:05,13/Mar/19 22:29,20/Oct/13 01:10,1.2.12,2.0.2,,,,,0,,,,,"If I run the demo pig scripts, I end up with a column family with 6 fairly wide rows.  If I load and dump those rows with widerows=true or set the cassandra.range.batch.size=1, the dump returns the correct values.  However, if I set both of those, it does not.  So in the case of a batch size of 1, wide rows support is broken.

So it's relatively simple to reproduce from the demo data:
{code}
grunt> SET cassandra.range.batch.size 1                                                
grunt> rows = LOAD 'cassandra://PigDemo/Scores' using CassandraStorage();                
grunt> dump rows;
...
(sylvain,{(4,),(7,),(10,),(21,),(24,),(46,),(47,),(49,),(51,),(52,),(67,),(68,),(72,),(73,),(82,),(83,),(86,),(98,),(101,),(105,),(108,),(112,),(114,),(124,),(125,),(136,),(139,),(145,),(150,),(151,),(153,),(165,),(167,),(171,),(178,),(182,),(202,),(211,),(212,),(215,),(226,),(237,),(242,),(243,),(255,),(261,),(273,),(282,),(300,),(307,),(308,),(311,),(312,),(313,),(316,),(317,),(332,),(337,),(338,),(348,),(355,),(360,),(361,),(373,),(375,),(377,),(384,),(401,),(404,),(412,),(418,),(429,),(436,),(441,),(451,),(453,),(461,),(473,),(478,),(483,),(485,),(486,),(489,),(509,),(511,),(516,),(517,),(521,),(536,),(541,),(543,),(545,),(550,),(583,),(587,),(592,),(611,),(613,),(622,),(625,),(627,),(633,),(648,),(649,),(651,),(659,),(665,),(668,),(670,),(672,),(679,),(688,),(692,),(700,),(703,),(707,),(709,),(730,),(731,),(738,),(740,),(744,),(750,),(759,),(764,),(766,),(768,),(774,),(776,),(778,),(779,),(788,),(795,),(796,),(813,),(821,),(825,),(830,),(831,),(835,),(843,),(846,),(847,),(848,),(851,),(862,),(863,),(872,),(878,),(881,),(883,),(884,),(888,),(905,),(906,),(916,),(921,),(926,),(928,),(944,),(946,),(947,),(952,),(954,),(972,),(973,),(974,),(976,),(978,),(982,),(991,)})
(brandon,{(6,),(7,),(14,),(15,),(25,),(36,),(37,),(38,),(46,),(53,),(57,),(65,),(74,),(75,),(84,),(91,),(104,),(120,),(128,),(137,),(148,),(159,),(171,),(174,),(176,),(179,),(183,),(192,),(195,),(201,),(205,),(210,),(216,),(222,),(223,),(243,),(255,),(264,),(271,),(287,),(290,),(308,),(309,),(326,),(343,),(347,),(356,),(359,),(360,),(363,),(367,),(368,),(378,),(398,),(400,),(402,),(410,),(412,),(419,),(427,),(429,),(447,),(449,),(462,),(464,),(468,),(470,),(472,),(480,),(482,),(506,),(511,),(520,),(521,),(522,),(524,),(535,),(548,),(553,),(565,),(569,),(571,),(573,),(575,),(583,),(584,),(595,),(597,),(606,),(608,),(634,),(646,),(650,),(654,),(667,),(673,),(677,),(686,),(690,),(692,),(713,),(715,),(721,),(723,),(736,),(737,),(752,),(753,),(758,),(759,),(764,),(766,),(767,),(776,),(778,),(786,),(812,),(816,),(818,),(823,),(826,),(832,),(838,),(842,),(860,),(873,),(879,),(918,),(919,),(935,),(941,),(942,),(948,),(956,),(961,),(966,),(973,),(974,),(977,),(979,),(983,),(984,),(986,),(995,),(997,)})
(jake,{(1,),(7,),(10,),(14,),(29,),(52,),(54,),(65,),(67,),(78,),(82,),(83,),(89,),(97,),(100,),(115,),(126,),(140,),(141,),(145,),(214,),(221,),(230,),(231,),(232,),(241,),(245,),(247,),(265,),(266,),(269,),(271,),(282,),(286,),(288,),(299,),(316,),(323,),(331,),(332,),(335,),(338,),(348,),(353,),(355,),(364,),(367,),(371,),(379,),(398,),(409,),(420,),(428,),(429,),(439,),(443,),(450,),(454,),(467,),(477,),(482,),(488,),(490,),(502,),(503,),(512,),(520,),(521,),(535,),(536,),(541,),(548,),(552,),(557,),(560,),(596,),(600,),(604,),(606,),(611,),(613,),(621,),(624,),(630,),(635,),(641,),(647,),(655,),(660,),(665,),(674,),(676,),(690,),(693,),(694,),(704,),(719,),(720,),(724,),(731,),(749,),(751,),(763,),(765,),(767,),(771,),(779,),(782,),(784,),(789,),(793,),(797,),(798,),(801,),(802,),(806,),(820,),(825,),(839,),(845,),(848,),(856,),(865,),(866,),(867,),(870,),(876,),(887,),(891,),(901,),(905,),(908,),(922,),(929,),(944,),(960,),(964,),(980,),(988,),(996,)})
(eric,{(14,),(17,),(23,),(25,),(26,),(34,),(42,),(43,),(57,),(64,),(68,),(80,),(88,),(93,),(100,),(114,),(131,),(132,),(134,),(143,),(146,),(147,),(156,),(157,),(170,),(171,),(172,),(177,),(186,),(197,),(198,),(206,),(209,),(223,),(224,),(233,),(236,),(241,),(251,),(252,),(255,),(263,),(266,),(267,),(268,),(272,),(277,),(280,),(289,),(293,),(294,),(297,),(301,),(306,),(310,),(312,),(321,),(326,),(333,),(334,),(335,),(345,),(357,),(362,),(363,),(370,),(380,),(389,),(392,),(393,),(401,),(420,),(431,),(462,),(464,),(465,),(471,),(484,),(486,),(490,),(493,),(504,),(505,),(509,),(515,),(521,),(534,),(538,),(547,),(554,),(557,),(561,),(564,),(572,),(573,),(578,),(582,),(584,),(590,),(598,),(599,),(603,),(605,),(609,),(618,),(634,),(636,),(639,),(648,),(656,),(661,),(667,),(671,),(674,),(675,),(687,),(713,),(721,),(733,),(736,),(763,),(767,),(776,),(785,),(787,),(809,),(813,),(826,),(829,),(830,),(832,),(840,),(841,),(844,),(846,),(854,),(855,),(876,),(890,),(892,),(902,),(910,),(930,),(934,),(938,),(940,),(943,),(955,),(959,),(965,),(966,),(968,),(972,),(980,),(985,),(989,)})
(jonathan,{(17,),(18,),(31,),(34,),(37,),(40,),(67,),(69,),(75,),(93,),(111,),(124,),(127,),(128,),(137,),(142,),(168,),(178,),(190,),(193,),(194,),(207,),(211,),(216,),(221,),(229,),(237,),(242,),(252,),(253,),(264,),(265,),(267,),(270,),(272,),(274,),(276,),(278,),(280,),(283,),(297,),(299,),(300,),(302,),(303,),(309,),(311,),(318,),(323,),(329,),(330,),(332,),(344,),(346,),(351,),(354,),(358,),(361,),(363,),(366,),(367,),(374,),(378,),(379,),(386,),(389,),(392,),(395,),(398,),(404,),(424,),(426,),(429,),(434,),(439,),(443,),(445,),(448,),(472,),(477,),(494,),(500,),(504,),(522,),(525,),(538,),(539,),(541,),(548,),(553,),(557,),(560,),(563,),(566,),(567,),(578,),(591,),(593,),(595,),(599,),(605,),(610,),(626,),(635,),(636,),(640,),(642,),(644,),(649,),(660,),(662,),(663,),(667,),(674,),(690,),(706,),(708,),(712,),(716,),(723,),(733,),(741,),(747,),(758,),(765,),(797,),(798,),(801,),(822,),(827,),(828,),(837,),(850,),(863,),(867,),(894,),(895,),(896,),(904,),(911,),(917,),(932,),(949,),(951,),(952,),(958,),(969,),(974,),(983,),(985,),(988,),(989,),(996,),(1000,)})
(gary,{(3,),(13,),(21,),(23,),(33,),(36,),(44,),(45,),(48,),(62,),(65,),(68,),(75,),(80,),(81,),(90,),(111,),(113,),(119,),(123,),(137,),(149,),(152,),(153,),(157,),(161,),(166,),(178,),(179,),(180,),(183,),(184,),(188,),(189,),(191,),(197,),(199,),(200,),(204,),(212,),(221,),(229,),(239,),(265,),(270,),(272,),(276,),(279,),(282,),(295,),(296,),(304,),(305,),(314,),(326,),(329,),(335,),(342,),(345,),(346,),(362,),(370,),(371,),(375,),(380,),(382,),(387,),(389,),(390,),(393,),(399,),(403,),(406,),(414,),(417,),(424,),(428,),(445,),(458,),(462,),(486,),(490,),(492,),(495,),(499,),(500,),(507,),(514,),(520,),(542,),(550,),(551,),(570,),(571,),(572,),(574,),(577,),(588,),(604,),(614,),(619,),(626,),(634,),(640,),(648,),(659,),(663,),(684,),(687,),(690,),(694,),(715,),(741,),(750,),(765,),(772,),(776,),(781,),(782,),(783,),(785,),(789,),(802,),(806,),(812,),(816,),(820,),(829,),(836,),(843,),(850,),(855,),(868,),(873,),(875,),(889,),(900,),(904,),(922,),(928,),(929,),(935,),(946,),(949,),(954,),(956,),(959,),(960,),(962,),(992,)})
{code}

{code}
grunt> SET cassandra.range.batch.size 1                                                
grunt> rows = LOAD 'cassandra://PigDemo/Scores?widerows=true' using CassandraStorage();
grunt> dump rows;
...
(jonathan,{(17,)})
(sylvain,{(4,)})
{code}

When I try with set batch size to something higher than 1 and it works fine.",,,,,,,,,,,,,,,,,,,,,,,,30/Sep/13 21:53;alexliu68;6114.txt;https://issues.apache.org/jira/secure/attachment/12605984/6114.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-20 01:10:35.166,,,no_permission,,,,,,,,,,,,351154,,,Sun Oct 20 01:10:35 UTC 2013,,,,,,0|i1ojnj:,351446,,,,,,,,jbellis,jbellis,,,,,,,,,,"30/Sep/13 21:35;alexliu68;if widerows=true, then it uses get_paged_slice which set the page size(number of columns per page) to {code} cassandra.range.batch.size {code}, so it's not good to make it to one. For efficiency, it needs to be at least 100 which is the default number of columns per page.

We shouldn't use it combining with widerows=true. it should be at least 100.

The reason why it ends with one column is that at ColumnFamilyRecordReader
{code}
                if (wideColumns.hasNext() && wideColumns.peek().right.keySet().iterator().next().equals(lastColumn))
                    wideColumns.next();
                if (!wideColumns.hasNext())
                    rows = null;
{code}

if batch size set to 1, then wideColumns returns 1 column, which then end the iterator.

I will throw exception if the widerows=true and batch size set to 1",20/Oct/13 01:10;jbellis;I think batchSize==1 is broken for non-wide-rows as well.  Updated to throw InvalidArgument for either and committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong memtable size estimation: liveRatio is not honored in edge cases,CASSANDRA-6078,12670024,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,m0nstermind,m0nstermind,23/Sep/13 11:08,12/Mar/19 14:05,13/Mar/19 22:29,25/Sep/13 01:51,1.2.11,2.0.2,,,,,0,,,,,"Memtable.getLiveSize does not honours liveRatio the correct way: 
allocator.get**Size() return sizes allocated only by name and columns data (i.e. no liveRatio applied); but conditions, which cap estimated size, compare it with estimatedSize, already multiplied by liveRatio.
If liveRatio is big enough (i've seen >11 on our dataset), this leads to huge estimation errors and even to OutOfMemory, because MeteredFlusher  underestimates memtables sizes.
",,,,,,,,,,,,,,,,,,,,,,,,23/Sep/13 15:03;jbellis;6078-v2.txt;https://issues.apache.org/jira/secure/attachment/12604603/6078-v2.txt,23/Sep/13 11:08;m0nstermind;Memtable-getLiveSize.diff;https://issues.apache.org/jira/secure/attachment/12604558/Memtable-getLiveSize.diff,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-09-23 15:03:25.345,,,no_permission,,,,,,,,,,,,349854,,,Wed Sep 25 01:51:48 UTC 2013,,,,,,0|i1obon:,350152,,,,,,,,jasobrown,jasobrown,,,1.1.12,,,,,,,"23/Sep/13 15:03;jbellis;This isn't 100% correct, either.  Consider the case where we've allocated a single 1-byte column from a 1MB slab: multiplying the 1MB by liveRatio is clearly incorrect.

The intent (CASSANDRA-5497) is to bound our error when liveRatio is incorrect.  I think where we get into trouble is with the upper bound, and I don't see a good way to fix that.  (On the bright side, overestimating the size is a lot less dangerous than underestimating it.)

Patch attached to remove the upper bound.",24/Sep/13 13:38;jbellis;WDYT [~jasobrown]?,"24/Sep/13 23:11;jasobrown;I think this is a legit change to protect against the upper bound problem. I'd rather flush more frequently than OOM more frequently. 

lgtm",25/Sep/13 01:51;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Memtables flush is delayed when having a lot of batchlog activity, making node OOM",CASSANDRA-6079,12670028,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,m0nstermind,m0nstermind,m0nstermind,23/Sep/13 11:20,12/Mar/19 14:05,13/Mar/19 22:29,23/Sep/13 14:22,1.2.11,2.0.2,,,,,0,,,,,"Both MeteredFlusher and BatchlogManager share the same OptionalTasks thread. So, when batchlog manager processes its tasks no flushes can occur. Even more, batchlog manager waits for batchlog CF compaction to finish.

On a lot of batchlog activity this prevents memtables from flush for a long time, making the node OOM.

Fixed this by moving batchlog to its own thread and not waiting for batchlog compaction to finish.",,,,,,,,,,,,,,,,,,,,,,,,23/Sep/13 11:20;m0nstermind;NoWaitBatchlogCompaction.diff;https://issues.apache.org/jira/secure/attachment/12604564/NoWaitBatchlogCompaction.diff,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-23 14:22:17.84,,,no_permission,,,,,,,,,,,,349858,,,Wed Sep 25 10:09:02 UTC 2013,,,,,,0|i1obpj:,350156,,,,,,,,jbellis,jbellis,,,,,,,,,,"23/Sep/13 14:22;jbellis;Committed the executorservice change.  I'm less convinced by making cleanup not block for the compaction -- if batchlog is really behind, you don't want it to have to re-scan all the tombstoned rows you just replayed.","23/Sep/13 14:32;jbellis;bq. Committed the executorservice change

... also to 1.2.11.","23/Sep/13 16:42;m0nstermind;From the other hand, if all compaction pool threads are occupied by large CFs, batchlog manager stops replaying batches for a long time, which may not be acceptable.
Rescanning tombstones looks more acceptable to me than delaying of mutations. 

Ideally, batchlog should ensure compaction really happens before waiting. Either running it in its own executor or making some communication with CompactionManager. And if compaction cannot be done at the moment - just go and rescan tobmstones, trading efficiency for consistency.","23/Sep/13 18:18;jbellis;Remember, this is the *replay* path.  If it's running, the mutation is already delayed, probably because the target node was down.  So delaying for compactions doesn't seem like a dealbreaker to me.  Especially since that leaves more iops to keep the rest of the system healthy and not go down ourselves. :)","23/Sep/13 20:13;m0nstermind;Not the target, but coordinator. This could be a target node at the same time of course, but even if it is, at least 2 other targets are ready to accept this mutation. Delaying mutation to quorum for more than rpc write timeout practically means it is lost.

",23/Sep/13 20:33;jbellis;The coordinator doesn't block for compaction.  Only replayAllFailedBatches.,"23/Sep/13 20:43;m0nstermind;Exactly.
The case here could be:
1. Coordinator writes batchlog to chosen 2 nodes within the same dc
2. Coordinator crashes
3. Those 2 nodes supposed to run replayAllFailedBatches to replay this batch of crashed coordinator
3.1 but they cannot, because compaction pool is busy with ongoing large CF compaction and they are waiting on submitUsedDefined().get()

","23/Sep/13 20:48;jbellis;Right.  I don't think it's worth sacrificing throughput in the common case, to reduce delay in the exceptional case.","23/Sep/13 20:49;jbellis;(But, I'd be fine with avoiding the cleanup call unless there was actually something replayed.)","23/Sep/13 21:21;m0nstermind;Yeah, this makes stall on get() less probable. 

I could measure how big the impact on common case throughout due to rescan of thombstones, if you did not performed this measurement earlier.

Yet another idea is to reserve a special thread in CompactionManager for doing compactions on batchlog (and possibly hints) CF. This way it at least wouldn't delay for a long time.","23/Sep/13 22:30;jbellis;I'm not a huge fan of making CompactionManager more complex, either. :)","24/Sep/13 16:29;m0nstermind;Well, I did some quick test. As i suspected there is no measurable performance impact on this.

3 node cluster, RF=3, separated client, writing batches of 3 inserts each to 3 different CFs. 8 core (not so) recent iron servers.
(no batches were replayed actually, so this pure common case). 

I measured this for 1 hour run in both cases.

Here is performance  WITH wait on get():
avg 3052 batches/sec, st deviance 131

WITHOUT waiting in get()
avg 3030 per sec, deviance 106

","24/Sep/13 17:07;jbellis;I think you're missing that we need to replay not just for coordinator failure but for replica failure.  So ""only scan for last couple seconds"" is not correct; you need to scan every partition since the replica went down.

(Note that for typical deployments, replica failure will be 3x more likely than coordinator failure.)","24/Sep/13 17:28;m0nstermind;Do you mean the replica, which is target for one of batched mutations ? I see a hint is written by BM if it cannot deliver one of mutations serialized to batchlog record (in replaySerializedMutation and attemptDirectDelivery) , so as i understood HintManager bothers further about delivering it to failed replica when it is back online. So as soon as BM writes a hint(s) for (all of) failed replica(s), BM has nothing to do with batchlog record, so no all partitions scanning neccessary.

Am I missing something again ?","24/Sep/13 20:14;jbellis;bq. as soon as BM writes a hint(s) for (all of) failed replica(s), BM has nothing to do with batchlog record, so no all partitions scanning neccessary

You are right.  Carry on!",25/Sep/13 10:09;m0nstermind;ok then. started writing code ;-),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad metadata returned for SELECT COUNT,CASSANDRA-6080,12670041,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,23/Sep/13 12:20,12/Mar/19 14:05,13/Mar/19 22:29,25/Sep/13 12:01,2.0.2,,,,,,0,,,,,"The native protocol v2 returns the result metadata with a prepared statement, but for count queries this is currently incorrect, we return the metadata corresponding to a 'SELECT *' instead.",,,,,,,,,,,,,,,,,,,,,,,,23/Sep/13 12:22;slebresne;6080.txt;https://issues.apache.org/jira/secure/attachment/12604576/6080.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-24 13:40:33.964,,,no_permission,,,,,,,,,,,,349871,,,Wed Sep 25 12:01:12 UTC 2013,,,,,,0|i1obsf:,350169,,,,,,,,iamaleksey,iamaleksey,,,2.0 beta 1,,,,,,,23/Sep/13 12:22;slebresne;Trivial patch attached,24/Sep/13 13:40;iamaleksey;+1,"25/Sep/13 12:01;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
client_only example does not work,CASSANDRA-6089,12670347,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mihasya,mihasya,mihasya,24/Sep/13 20:10,12/Mar/19 14:05,13/Mar/19 22:29,25/Sep/13 19:12,1.2.11,2.0.2,,,,,0,,,,,"client_only example fails out of the box with:

{code}
13/09/24 11:44:53 INFO gms.Gossiper: Node /127.0.0.1 is now part of the cluster
13/09/24 11:44:53 INFO gms.Gossiper: InetAddress /127.0.0.1 is now UP
13/09/24 11:44:53 ERROR concurrent.DebuggableThreadPoolExecutor: Error in ThreadPoolExecutor
java.lang.NullPointerException
  at org.apache.cassandra.service.MigrationManager.maybeScheduleSchemaPull(MigrationManager.java:113)
	at org.apache.cassandra.service.MigrationManager.onAlive(MigrationManager.java:95)
	at org.apache.cassandra.gms.Gossiper.markAlive(Gossiper.java:803)
	at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:846)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:931)
	at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:50)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:680)
Exception in thread ""GossipStage:1"" java.lang.NullPointerException
	at org.apache.cassandra.service.MigrationManager.maybeScheduleSchemaPull(MigrationManager.java:113)
	at org.apache.cassandra.service.MigrationManager.onAlive(MigrationManager.java:95)
	at org.apache.cassandra.gms.Gossiper.markAlive(Gossiper.java:803)
	at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:846)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:931)
	at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:50)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:680)
{code}
I've tested this both on the 1.2.8 tag and with 1.2 latest.

The line in question is 

{code:java}
if (Schema.instance.getVersion().equals(theirVersion) || !shouldPullSchemaFrom(endpoint))
{code}

Seems that {{Schema.instance.getVersion()}} returns null in the client_only case. Adding a {{Schema.instance.getVersion() == null}} as another {{||}} in the conditional appears to fix it, but I don't remember the codepaths well enough to confidently say that that's the correct thing to do.",,,,,,,,,,,,,,,,,,,,,,,,25/Sep/13 18:45;mihasya;cassandra-1.2-6089.txt;https://issues.apache.org/jira/secure/attachment/12605074/cassandra-1.2-6089.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-25 19:12:59.949,,,no_permission,,,,,,,,,,,,350176,,,Wed Sep 25 19:12:59 UTC 2013,,,,,,0|i1odnb:,350470,1.2.10,1.2.8,,,,,,iamaleksey,iamaleksey,,,,,,,,,mihasya,"25/Sep/13 18:37;mihasya;Actually, the change I mention above stops the errors, but also short-circuits schema propagation :D I've sorted out what the code actually does and made a change that properly fixes the issue. Patch incoming, please hold.","25/Sep/13 18:44;mihasya;This patch correctly checks if ""own"" schema version is null before trying to compare versions.",25/Sep/13 18:45;mihasya;Here's the patch..,"25/Sep/13 19:12;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool repair randomly hangs.,CASSANDRA-6097,12670530,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,jblangston@datastax.com,jblangston@datastax.com,25/Sep/13 17:54,12/Mar/19 14:05,13/Mar/19 22:29,09/Oct/13 15:40,1.2.11,,,,,,0,,,,,"nodetool repair randomly hangs. This is not the same issue where repair hangs if a stream is disrupted. This can be reproduced on a single-node cluster where no streaming takes place, so I think this may be a JMX connection or timeout issue. Thread dumps show that nodetool is waiting on a JMX response and there are no repair-related threads running in Cassandra. Nodetool main thread waiting for JMX response:

{code}
""main"" prio=5 tid=7ffa4b001800 nid=0x10aedf000 in Object.wait() [10aede000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <7f90d62e8> (a org.apache.cassandra.utils.SimpleCondition)
	at java.lang.Object.wait(Object.java:485)
	at org.apache.cassandra.utils.SimpleCondition.await(SimpleCondition.java:34)
	- locked <7f90d62e8> (a org.apache.cassandra.utils.SimpleCondition)
	at org.apache.cassandra.tools.RepairRunner.repairAndWait(NodeProbe.java:976)
	at org.apache.cassandra.tools.NodeProbe.forceRepairAsync(NodeProbe.java:221)
	at org.apache.cassandra.tools.NodeCmd.optionalKSandCFs(NodeCmd.java:1444)
	at org.apache.cassandra.tools.NodeCmd.main(NodeCmd.java:1213)
{code}

When nodetool hangs, it does not print out the following message:

""Starting repair command #XX, repairing 1 ranges for keyspace XXX""

However, Cassandra logs that repair in system.log:

1380033480.95  INFO [Thread-154] 10:38:00,882 Starting repair command #X, repairing X ranges for keyspace XXX

This suggests that the repair command was received by Cassandra but the connection then failed and nodetool didn't receive a response.

Obviously, running repair on a single-node cluster is pointless but it's the easiest way to demonstrate this problem. The customer who reported this has also seen the issue on his real multi-node cluster.

Steps to reproduce:

Note: I reproduced this once on the official DataStax AMI with DSE 3.1.3 (Cassandra 1.2.6+patches).  I was unable to reproduce on my Mac using the same version, and subsequent attempts to reproduce it on the AMI were unsuccessful. The customer says he is able is able to reliably reproduce on his Mac using DSE 3.1.3 and occasionally reproduce it on his real cluster. 

1) Deploy an AMI using the DataStax AMI at https://aws.amazon.com/amis/datastax-auto-clustering-ami-2-2

2) Create a test keyspace
{code}
create keyspace test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
{code}
3) Run an endless loop that runs nodetool repair repeatedly:

{code}
while true; do nodetool repair -pr test; done
{code}

4) Wait until repair hangs. It may take many tries; the behavior is random.",DataStax AMI,,,,,,,,,,,,,,,,,,,,,,,03/Oct/13 16:34;yukim;6097-1.2.txt;https://issues.apache.org/jira/secure/attachment/12606597/6097-1.2.txt,01/Oct/13 14:08;jblangston@datastax.com;dse.stack;https://issues.apache.org/jira/secure/attachment/12606104/dse.stack,01/Oct/13 14:08;jblangston@datastax.com;nodetool.stack;https://issues.apache.org/jira/secure/attachment/12606103/nodetool.stack,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-09-25 20:39:54.521,,,no_permission,,,,,,,,,,,,350359,,,Sun Oct 13 17:04:37 UTC 2013,,,,,,0|i1oerr:,350652,,,,,,,,slebresne,slebresne,,,,,,,,,,"25/Sep/13 20:39;brandon.williams;It took a few thousands of iterations to reproduce, but it did happen (on my own machine, not in AWS or using the AMI)  Netstat shows one connection from nodetool to the jmx port that stays connected, and another one that connects, gets disconnected after a short while, then the process repeats.  This sounds like jmx doing it's double connection dance, but whatever reason the server rejects the second one.  I'm not sure there's anything we can do about this except say what we already know: jmx kinda sucks.

One odd thing I did notice occasionally get printed straight to stdout/err while running the loop:
{noformat}
Sep 25, 2013 7:55:52 PM ServerCommunicatorAdmin reqIncoming
WARNING: The server has decided to close this client connection.
{noformat}

However it didn't correlate to the iteration that hung.","29/Sep/13 21:07;jblangston@datastax.com;If I'm reading [this|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/tools/NodeProbe.java#L1036-L1039] correctly, the condition that nodetool repair is waiting on won't get signaled if the status returned to the NotificationListener is SESSION_FAILED. Could that explain why it's hanging?",29/Sep/13 22:25;yukim;Repair should send back FINISHED at the end of the repair even if session failed. https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/StorageService.java#L2414,"30/Sep/13 23:08;jblangston@datastax.com;I think the ease with which this can be reproduced is dependent on the number of keyspaces.  I started up a stock 3.1.3 AMI in hadoop mode so that DSE would create the cfs/HiveMetaStore/dse_system keyspaces and also created an additional keyspace using the customer's schema. Now I am able to reproduce the issue very readily by running nodetool repair -pr in a loop.  I thought that it might have been something to do with having hadoop enabled, so i disabled it again, but I am still able to reproduce the issue.  On the other hand, if I give repair a specific keyspace name, it takes much longer to reproduce, if at all.",01/Oct/13 00:33;mishail;[~jblangston@datastax.com] do you have a full thread dump for the problem?,01/Oct/13 14:08;jblangston@datastax.com;Stack trace for nodetool and cassandra attached.,"01/Oct/13 17:10;mishail;I suspect it could be infamous hang at {{SocketInputStream.socketRead0}}. And the only way I know to work around is to limit {{sun.net.client.defaultReadTimeout}}
{code}

""ClientNotifForwarder-5"" daemon prio=10 tid=0x00007f133c2e4000 nid=0x7549 runnable [0x00007f13389de000]
   java.lang.Thread.State: RUNNABLE
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:129)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
	- locked <0x00000000ff562dc0> (a java.io.BufferedInputStream)
	at java.io.DataInputStream.readByte(DataInputStream.java:248)
	at sun.rmi.transport.StreamRemoteCall.executeCall(StreamRemoteCall.java:195)
	at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:142)
	at com.sun.jmx.remote.internal.PRef.invoke(Unknown Source)
	at javax.management.remote.rmi.RMIConnectionImpl_Stub.fetchNotifications(Unknown Source)
	at javax.management.remote.rmi.RMIConnector$RMINotifClient.fetchNotifs(RMIConnector.java:1306)
	at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.fetchNotifs(ClientNotifForwarder.java:554)
	at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.doRun(ClientNotifForwarder.java:437)
	at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.run(ClientNotifForwarder.java:418)
	at com.sun.jmx.remote.internal.ClientNotifForwarder$LinearExecutor$1.run(ClientNotifForwarder.java:88)
{code}","01/Oct/13 20:05;jblangston@datastax.com;The JMX documentation [states|http://www.oracle.com/technetwork/java/javase/tech/best-practices-jsp-136021.html#mozTocId387765] that notifications are not guaranteed to always be delivered.  The API only guarantees that a client either receives all notifications for which it is listening, or can discover that notifications may have been lost. A client can discover when notifications are lost by registering a listener using JMXConnector.addConnectionNotificationListener. It looks like nodetool isn't doing this last part. Seems like we should register a ConnectionNotificationListener and if a notification fails, signal the condition so that nodetool doesn't hang. Maybe have nodetool query for the status of the repair at that point via separate JMX call, or just print a warning that ""The status of the repair command can't be determined, please check the log."" or something like that.

I would disagree with prioritizing this as trivial. It's not critical but I have had many customers express frustration with the  nodetool repair's proclivity for hanging.  It makes automating repairs painful because they can't count on nodetool to ever return.","03/Oct/13 16:34;yukim;JB is right. We need to handle cases where connection got lost.
Attaching patch to listen to JMXConnectionNotification.

There are 2 cases handled. One is for ""notification lost"". In this case, We cannot tell if repair finished, so log message to check server log and go on to next keyspace for repair. The other one is for ""connection closed"", and in this case we throw exception to stop further processing of repair.",05/Oct/13 05:37;jbellis;Tagging Sylvain for review since he also reviewed CASSANDRA-4767.,"09/Oct/13 14:47;slebresne;I've tried reproducing but lost patience when it didn't reproduce after > 1K iterations, so can't say I've ""tested"" the fix, but the JMX documentation makes it pretty clear we should handle those notifications and that it's very likely the source of the problem here. A few minor nits on the patch though:
* Is there a reason JMXConnectionNotification.FAILED is not handled? Feels like it would be a good to catch it too just in case.
* May want to mention in the error message for CLOSED/FAILED that we're not starting repair for the other keyspaces not yet started.

But lgtm otherwise","09/Oct/13 15:40;yukim;Thanks!
Committed with above nits fix.",09/Oct/13 20:49;jblangston@datastax.com;Customer compiled Cassandra from git and ran the resulting nodetool against his DSE installation. He reported that the hang is still reproducible.  I haven't tried to duplicate this myself yet.,"13/Oct/13 16:52;yukim;I think the issue they are having is platform specific.
Try setting sun.net.client.defaultReadTimeout system property (-Dsun.net.client.defaultReadTimeout=<timeout in millisec>) as suggested by Mikhail above to avoid stuck on socket read.
(http://docs.oracle.com/javase/6/docs/technotes/guides/net/properties.html)","13/Oct/13 17:00;brandon.williams;After this patch, I could not reproduce the problem after 10K iterations.","13/Oct/13 17:04;jbellis;(Unless this was already fine in 2.0, we should tag fixversion appropriately.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in Tracing,CASSANDRA-6041,12669008,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,17/Sep/13 13:01,12/Mar/19 14:05,13/Mar/19 22:29,17/Sep/13 13:34,2.0.1,,,,,,0,,,,,"Tracing in 2.0.0 can throw the following:
{noformat}
ERROR [TracingStage:1] 2013-09-17 14:38:41,486 CassandraDaemon.java (line 185) Exception in thread Thread[TracingStage:1,5,main]
java.lang.AssertionError: Added column does not sort as the last column
        at org.apache.cassandra.db.ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:115)
        at org.apache.cassandra.db.ColumnFamily.addColumn(ColumnFamily.java:116)
        at org.apache.cassandra.tracing.Tracing.addParameterColumns(Tracing.java:101)
        at org.apache.cassandra.tracing.Tracing$2.runMayThrow(Tracing.java:210)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)

{noformat}

That's because addParamaterColumns will insert cell with names we don't know in advance, so we can't use ArrayBackedSortedColumns. Attaching patch to switch to TreeMapBackedSortedColumns.",,,,,,,,,,,,,,,,,,,,,,,,17/Sep/13 13:10;slebresne;0002-Fix-AssertionError-in-Tracing.txt;https://issues.apache.org/jira/secure/attachment/12603589/0002-Fix-AssertionError-in-Tracing.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-17 13:23:17.156,,,no_permission,,,,,,,,,,,,348940,,,Tue Sep 17 13:34:34 UTC 2013,,,,,,0|i1o61r:,349238,,,,,,,,iamaleksey,iamaleksey,,,2.0.0,,,,,,,"17/Sep/13 13:10;slebresne;I'll note that it would be possible technically to keep ArrayBackedSortedColumns, but we'd need to ensure addParameterColumns is a SortedMap with the correct order. So not sure it's worth bothering, especially since it feels using ABSC is a bit error prone in that code.",17/Sep/13 13:23;iamaleksey;+1,"17/Sep/13 13:34;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parens around WHERE condition break query,CASSANDRA-6037,12668957,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,nsv,nsv,17/Sep/13 07:32,12/Mar/19 14:05,13/Mar/19 22:29,20/Sep/13 14:50,1.2.11,2.0.2,,,,,0,cql3,,,,"SELECT * FROM user WHERE (key=<UUID>);
Bad Request: line 1:25 no viable alternative at input '('

SELECT * FROM user WHERE key=<UUID>; -- No parens
-- Normal output

The example provided is minimal, bug was discovered with AND logic on indexed columns.

Parens-enclosed conditions is good SQL and so is produced by database abstraction layers in complex queries to avoid operation precedence problems.

Fixing this at application side is no option - this will open the can of logic bugs.","cqlsh, pdo_cassandra",,,,,,,,,,,,,,,,,,,,,,,20/Sep/13 05:29;dbrosius;6037.txt;https://issues.apache.org/jira/secure/attachment/12604200/6037.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-20 05:33:02.981,,,no_permission,,,,,,,,,,,,348889,,,Fri Sep 20 14:50:55 UTC 2013,,,,,,0|i1o5qf:,349187,,,,,,,,slebresne,slebresne,,,,,,,,,,"20/Sep/13 05:33;dbrosius;being as the conditions are 'and' conditions, probably not that important, but harmless.",20/Sep/13 14:27;slebresne;+1,20/Sep/13 14:50;dbrosius;committed as a0fa69715f7913804fbd55c1280e0d35edd3bf0f to cassandra-1.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool status should issue a warning when no keyspace is specified,CASSANDRA-6168,12672921,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,patricioe,patricioe,08/Oct/13 23:03,12/Mar/19 14:05,13/Mar/19 22:29,22/Mar/14 20:59,1.2.16,2.0.7,2.1 beta2,Tool/nodetool,,,1,lhf,,,,"Seen in 1.2.10.

Apologies if this is expected behavior. Nodetool status reports 0% ownership unless I add a keyspace name.

nodetool help docs says:
..."" status                 - Print cluster information (state, load, IDs, ...)""...

output without keyspace name
{code}
Datacenter: DC1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address      Load       Tokens  Owns   Host ID                               Rack
UN  10.x.x.146  81.96 GB   256     0.0%   a70c59b3-a667-4d76-ba5b-ba849ad672da  r1
UN  10.x.x.63   95.32 GB   256     0.0%   f8cb7b10-4ebe-484a-a1c0-6cb2d053901b  r1
UN  10.x.x.184  89.54 GB   256     0.1%   cd86c420-55e2-4d99-8ed9-d9ee8d6a9d9c  r1
UN  10.x.x.190  79.68 GB   256     0.0%   544c3906-bc02-400d-9fd2-1e39ecadd6ff  r1
UN  10.x.x.168  93.44 GB   256     0.7%   33be316f-1276-475d-90cf-2667950d3a2c  r1
UN  10.x.x.132  84.4 GB    256     0.0%   b327d9f1-cab0-4583-8e5e-95c50b4074fd  r1
Datacenter: DCOFFLINE
=====================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address      Load       Tokens  Owns   Host ID                               Rack
UN  10.x.x.62   56.09 GB   256     32.4%  c8994d27-767b-431f-bdc2-9196eeeb6f44  r1
UN  10.x.x.131  60.11 GB   256     32.8%  0b9d3314-039e-4f88-8ba6-d0f2885d9a30  r1
UN  10.x.x.167  56.45 GB   256     34.0%  ba76f4fe-4250-4839-a37d-c1a7c24e585d  r1
{code}

and with keyspace. Example: nodetool status MYKSPS

{code}
Datacenter: DC1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address      Load       Tokens  Owns (effective)  Host ID                               Rack
UN  10.x.x.184  89.51 GB   256     50.0%             cd86c420-55e2-4d99-8ed9-d9ee8d6a9d9c  r1
UN  10.x.x.146  81.96 GB   256     50.0%             a70c59b3-a667-4d76-ba5b-ba849ad672da  r1
UN  10.x.x.168  93.44 GB   256     50.0%             33be316f-1276-475d-90cf-2667950d3a2c  r1
UN  10.x.x.63   95.32 GB   256     50.0%             f8cb7b10-4ebe-484a-a1c0-6cb2d053901b  r1
UN  10.x.x.190  79.68 GB   256     50.0%             544c3906-bc02-400d-9fd2-1e39ecadd6ff  r1
UN  10.x.x.132  84.4 GB    256     50.0%             b327d9f1-cab0-4583-8e5e-95c50b4074fd  r1
Datacenter: DCOFFLINE
=====================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address      Load       Tokens  Owns (effective)  Host ID                               Rack
UN  10.x.x.131  60.11 GB   256     32.8%             0b9d3314-039e-4f88-8ba6-d0f2885d9a30  r1
UN  10.x.x.167  56.45 GB   256     34.7%             ba76f4fe-4250-4839-a37d-c1a7c24e585d  r1
UN  10.x.x.62   56.09 GB   256     32.5%             c8994d27-767b-431f-bdc2-9196eeeb6f44  r1
{code}
",,,,,,,,,,,,,,,,,,,,,,,,22/Mar/14 04:26;vijay2win@yahoo.com;0001-CASSANDRA-6168.patch;https://issues.apache.org/jira/secure/attachment/12636175/0001-CASSANDRA-6168.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-21 04:09:15.841,,,no_permission,,,,,,,,,,,,352544,,,Sat Mar 22 20:59:59 UTC 2014,,,,,,0|i1os6f:,352831,,,,,,,,brandon.williams,brandon.williams,,,1.2.10,,,,,,,"21/Oct/13 04:09;jbellis;Since we need KS to pick the right replication strategy, I vote we just leave Owns out for no-KS situation.  WDYT [~brandon.williams]?","21/Oct/13 04:11;brandon.williams;Well, we do issue a warning when no keyspace is specified.","20/Mar/14 05:07;jjordan;we actually don't for status, we need to make it act like ring","20/Mar/14 05:14;brandon.williams;Care to take a stab, Vijay?","20/Mar/14 15:36;vijay2win@yahoo.com;Hi Brandon, Sure, Thanks!",22/Mar/14 04:26;vijay2win@yahoo.com;One line change.,22/Mar/14 12:51;brandon.williams;+1,22/Mar/14 20:59;vijay2win@yahoo.com;Committed Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove multithreaded compaction (and precompactedrow),CASSANDRA-6142,12672169,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,03/Oct/13 15:38,12/Mar/19 14:05,13/Mar/19 22:29,30/Oct/13 15:20,2.1 beta1,,,,,,0,,,,,"There is at best a very small sweet spot for multithreaded compaction (ParallelCompactionIterable).  For large rows, we stall the pipeline and fall back to a single LCR pass.  For small rows, the overhead of the coordination outweighs the benefits of parallelization (45s to compact 2x1M stress rows with multithreading enabled, vs 35 with it disabled).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-10-03 16:32:10.224,,,no_permission,,,,,,,,,,,,351795,,,Wed Oct 30 15:20:24 UTC 2013,,,,,,0|i1onkv:,352083,,,,,,,,krummas,krummas,,,,,,,,,,"03/Oct/13 15:41;jbellis;I tried parallelizing at the OnDiskAtomIterator level instead (thread-per-iterator-per-partition, buffering into a queue) and for small partitions the performance is ridiculously bad, easily 100x worse than single threaded mode.

Any better ideas [~krummas] [~yukim] [~iamaleksey] [~slebresne]?  If not I will post a patch to rip out PCI.","03/Oct/13 16:32;krummas;i tried improving it a while back as well, got basically the same results, yes, we should remove it

concluded that the best way to improve the speed was to do more compactions in parallel (CASSANDRA-5936 - i should finish that up..)","03/Oct/13 22:36;jbellis;Pushed removal to https://github.com/jbellis/cassandra/commits/6142.

Also removes PrecompactedRow, which is no longer necessary, and fixes a couple existing bugs in LCR and Scrub that this revealed (last two commits).",13/Oct/13 17:40;jbellis;Belated clicked Submit Patch.,"16/Oct/13 07:50;krummas;CompactionsPurgeTest fails:
{noformat}
    [junit] Testsuite: org.apache.cassandra.db.compaction.CompactionsPurgeTest
    [junit] Tests run: 6, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 9.365 sec
    [junit] 
    [junit] Testcase: testMinTimestampPurge(org.apache.cassandra.db.compaction.CompactionsPurgeTest):   FAILED
    [junit] expected:<2> but was:<1>
    [junit] junit.framework.AssertionFailedError: expected:<2> but was:<1>
    [junit]     at org.apache.cassandra.db.compaction.CompactionsPurgeTest.testMinTimestampPurge(CompactionsPurgeTest.java:185)
    [junit] 
    [junit] 
    [junit] Testcase: testCompactionPurgeTombstonedRow(org.apache.cassandra.db.compaction.CompactionsPurgeTest):        FAILED
    [junit] expected:<10> but was:<5>
    [junit] junit.framework.AssertionFailedError: expected:<10> but was:<5>
    [junit]     at org.apache.cassandra.db.compaction.CompactionsPurgeTest.testCompactionPurgeTombstonedRow(CompactionsPurgeTest.java:313)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.CompactionsPurgeTest FAILED
{noformat}
and a few nits:
in LazilyCompactedRow:
* make reducer and merger final
* remove comment about reducer being null on row 123
","17/Oct/13 21:49;jbellis;Damn, not sure how I missed that.  Suspect another existing bug.  Will investigate.",19/Oct/13 20:02;jbellis;Pushed fixes for these to the same branch.  (They are indeed existing bugs in LCR.),"22/Oct/13 06:58;krummas;looks good to me

regarding saveOutOfOrderRows, i guess a solution would be to flush a new sstable from the TreeSet when its size exceeds some limit? Unsure how common this is though.","22/Oct/13 16:41;jbellis;I'm guessing not super common because the existing code will just break if it hits that case.  (A LCR object will throw errors if you try to use it after advancing the underlying stream to another row.)

I guess the next step is probably for me to pull the fixes out for application to 2.0.","25/Oct/13 00:40;jbellis;Posted 2.0 fixes to https://github.com/jbellis/cassandra/commits/6142-2.0.  Note that b959e8ff3bccd3437de70d33da91307ab9c12a19 is a different, less-invasive approach than the one taken for trunk.","30/Oct/13 07:40;krummas;ok, looks good to me

how about 1.2?","30/Oct/13 15:10;jbellis;The 2.0 backport was bad enough, I don't even want to think about 1.2.  They're all pretty rare corner cases, so I'm fine with telling people to upgrade to 2.0 if they care.",30/Oct/13 15:13;jbellis;Split that out to CASSANDRA-6274 to keep CHANGES clean when i tag it 2.0.3.,30/Oct/13 15:20;jbellis;Committed the MT and PCR removal to 2.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL should not allow an empty string as column identifier,CASSANDRA-6136,12672018,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,mfiguiere,mfiguiere,02/Oct/13 20:32,12/Mar/19 14:05,13/Mar/19 22:29,03/Oct/13 14:31,2.0.2,,,,,,0,,,,,"CQL currently allows users to create a table with an empty string as column identifier:

{code}
CREATE TABLE t (k int primary key, """" int);
{code}

Which results in the following table:

{code}
CREATE TABLE t (
  k int,
  """" int,
  PRIMARY KEY (k)
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='NONE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};
{code}

Empty strings are not allowed for keyspace and table identifiers though.

I guess it's just a case that we haven't covered. Of course making it illegal in a future version would be a breaking change, but nobody serious would manually have chosen such an identifier...",,,,,,,,,,,,,,,,,,,,,,,,03/Oct/13 01:51;dbrosius;6136.txt;https://issues.apache.org/jira/secure/attachment/12606505/6136.txt,03/Oct/13 02:55;dbrosius;6136_v2.txt;https://issues.apache.org/jira/secure/attachment/12606512/6136_v2.txt,03/Oct/13 12:11;dbrosius;6136_v3.txt;https://issues.apache.org/jira/secure/attachment/12606573/6136_v3.txt,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-10-03 02:55:54.853,,,no_permission,,,,,,,,,,,,351644,,,Thu Oct 03 14:31:03 UTC 2013,,,,,,0|i1omnj:,351932,,,,,,,,slebresne,slebresne,,,,,,,,,,"02/Oct/13 20:36;mfiguiere;Looks like it's used in {{system.""IndexInfo""}} actually:

{code}
cqlsh> DESC TABLE system.""IndexInfo""

CREATE TABLE ""IndexInfo"" (
  table_name text,
  index_name text,
  """" blob,
  PRIMARY KEY (table_name, index_name)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='indexes that have been completed' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=0 AND
  index_interval=128 AND
  read_repair_chance=0.000000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='NONE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};
{code}

Would it be the reason for that to be allowed?","03/Oct/13 02:55;dbrosius;version 1 disallows empty column names everywhere
version 2 disallows empty column names everywhere but in the system keyspace","03/Oct/13 09:02;slebresne;Concerning IndexInfo, this is really a bug of the describe command of cqlsh. We internally use a empty column name to represent COMPACT tables that have not column outside the PK (which is generally allowed). So cqlsh should be not display that empty column.

Now the fact that we use it internally for that purpose is probably a good idea to refuse it otherwise indeed (the fact that it's not allowed for table and keyspace identifiers is less so, those are a lot more restricted than column names already).

But if we do it, I'd rather directly do it at the grammar level, and disallow empty quoted names altogether (again, that's *not* a problem for IndexInfo).",03/Oct/13 12:10;dbrosius;version3 disallows at the parse layer.,"03/Oct/13 14:22;slebresne;Last patch lgtm, +1. I've also created CASSANDRA-6139 to fix cqlsh DESC command.",03/Oct/13 14:31;dbrosius;committed to cassandra-2.0 as commit 27f4ea2bfd8831ee147ee1ed7a59be9c3308a558,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add compaction, compression to cqlsh tab completion for CREATE TABLE",CASSANDRA-6196,12673630,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mishail,jbellis,jbellis,13/Oct/13 22:25,12/Mar/19 14:05,13/Mar/19 22:29,21/Oct/13 19:18,1.2.12,2.0.2,,Legacy/Tools,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Oct/13 20:07;mishail;cassandra-1.2-6196.patch;https://issues.apache.org/jira/secure/attachment/12609512/cassandra-1.2-6196.patch,21/Oct/13 19:03;mishail;cassandra-2.0-6196.patch;https://issues.apache.org/jira/secure/attachment/12609504/cassandra-2.0-6196.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-10-14 05:40:55.996,,,no_permission,,,,,,,,,,,,353253,,,Tue Oct 22 06:31:35 UTC 2013,,,,,,0|i1owkn:,353546,1.2.10,2.0.1,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"14/Oct/13 05:40;mishail;The completion for CREATE TABLE options is already there, but doesn't work because there are duplicate completers for Properties related stuff.

Attached the patch to remove static Keyspace-only completers, so the dynamic ones (keyspace/columnfamily) will work","21/Oct/13 05:30;iamaleksey;[~mishail] The issue is present in 1.2, too, so the patch should be 1.2-based. Also, the patch does not fix auto completion for ALTER TABLE. Could you please handle that as well?","21/Oct/13 19:03;mishail;New patch for 2.0 to address ""ALTER TABLE"" as well.

Fixing it for 1.2 will be trickier for me as 1.2 doesn't contain https://github.com/apache/cassandra/commit/7f6ac19efb9a9d51a3ebdb58197c8fe35476034f","21/Oct/13 19:18;iamaleksey;Committed as is, thanks.

Re: 1.2 - if you break completion for CQL2/CQL3-beta while fixing it for CQL3-proper, I wouldn't object. So if that's the only thing that's stopping you, feel free to break it.","21/Oct/13 20:07;mishail;Patch for 1.2. 
* Removed static KS-only completers
* Fix ""ALTER TABLE""","22/Oct/13 06:31;iamaleksey;Committed the back port, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw errror when attempting to create a secondary index against counter,CASSANDRA-6160,12672782,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,ahattrell,ahattrell,08/Oct/13 10:24,12/Mar/19 14:05,13/Mar/19 22:29,09/Oct/13 14:53,1.2.11,,,Feature/2i Index,Legacy/CQL,,0,,,,,"Using CQL you can create a secondary index against a counter which is then non-functional.  

{code}
cqlsh:test> create table test2 (col1 int, col2 counter, primary key (col1)) ;
cqlsh:test> create index dodgy on test2(col2) ;
cqlsh:test> update test2 set col2 = col2 + 0 where col1 = 1 ;
cqlsh:test> select * from test2 ;

 col1 | col2
------+------
    1 |    0

cqlsh:t7088> select * from test2 where col2 = 0 ;
{code}

We should return an error to let users know they are in unsupported territory.
",,,,,,,,,,,,,,,,,,,,,,,,09/Oct/13 13:21;slebresne;6160.txt;https://issues.apache.org/jira/secure/attachment/12607554/6160.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-09 13:21:23.03,,,no_permission,,,,,,,,,,,,352405,,,Wed Jan 28 20:02:28 UTC 2015,,,,,,0|i1orbj:,352692,1.2.10,,,,,,,jbellis,jbellis,,,,,,,,,,09/Oct/13 13:21;slebresne;Trivial patch atttached,09/Oct/13 13:29;jbellis;+1,"09/Oct/13 14:53;slebresne;Committed, thanks","28/Jan/15 19:15;nff;Unfortunately this breaks secondary indexes on primary key dimensions:

{code}
cqlsh:ks1> create table test3 (col1 int, col2 int, col3 counter, primary key((col1, col2)));
cqlsh:ks1> update test3 set col3 = col3 + 3 where col1 = 1 and col2 = 2;
cqlsh:ks1> select * from test3;

 col1 | col2 | col3
------+------+------
    1 |    2 |    3

(1 rows)

cqlsh:ks1> create index expected on test3(col2);
Bad Request: Secondary indexes are not supported on counter tables
{code}

With RMW counters, we should be able to add both the counter write and the secondary index update in the same commit log entry. I couldn't find a JIRA for this; is there one?","28/Jan/15 20:02;brandon.williams;Actually, I don't think we should be allowing mixing counters in the same table, but go ahead and make a new ticket [~nff] and [~slebresne] can decide.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Too many splits causes a ""OutOfMemoryError: unable to create new native thread"" in AbstractColumnFamilyInputFormat",CASSANDRA-6169,12672932,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,patricioe,patricioe,patricioe,09/Oct/13 00:28,12/Mar/19 14:05,13/Mar/19 22:29,09/Oct/13 18:36,1.2.11,2.0.2,,,,,0,hadoop,,,,"The problem is caused by having 2300+ tokens due to vnodes.

In the client side I get this exception

{code}
Exception in thread ""main"" java.lang.OutOfMemoryError: unable to create new native thread
	at java.lang.Thread.start0(Native Method)
	at java.lang.Thread.start(Thread.java:691)
	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:943)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1336)
	at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:132)
	at org.apache.cassandra.hadoop.AbstractColumnFamilyInputFormat.getSplits(AbstractColumnFamilyInputFormat.java:187)
	at org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:1054)
	at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:1071)
	at org.apache.hadoop.mapred.JobClient.access$700(JobClient.java:179)
	at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:983)
	at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:936)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:936)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:550)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:580)
	at com.relateiq.hadoop.cassandra.etl.CassandraETLJob.run(CassandraETLJob.java:58)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at com.relateiq.hadoop.cassandra.etl.CassandraETLJob.main(CassandraETLJob.java:149)
{code}

The problem seem to be in AbstractColumnFamilyInputFormat line ~180 which has an unbounded upper limit (actually it is Integer.MAX_INT)
{code}
ExecutorService executor = Executors.newCachedThreadPool();
{code}

Followed by:
{code}
            for (TokenRange range : masterRangeNodes)
            {
                if (jobRange == null)
                {
                    // for each range, pick a live owner and ask it to compute bite-sized splits
                    splitfutures.add(executor.submit(new SplitCallable(range, conf)));
                }
                else
                .....
{code}

which gets called one time per token and creates one thread just as many times.

The easy fix unless there is a longer term fix I'm unaware of would be to set an upper limit to the thread pool.

Something like this:
{code}
ExecutorService executor = new ThreadPoolExecutor(0, ConfigHelper.getMaxConcurrentSplitsResolution(), 60L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>());
{code}


Shall I proceed with a patch ?","1.2.10
vnodes (server side)
Mac OS x (client)",,,,,,,,,,,,,,,,,,,,,,,09/Oct/13 17:33;patricioe;CASSANDRA-6169.diff;https://issues.apache.org/jira/secure/attachment/12607604/CASSANDRA-6169.diff,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-09 01:08:22.383,,,no_permission,,,,,,,,,,,,352555,,,Wed Oct 09 18:36:32 UTC 2013,,,,,,0|i1os8v:,352842,,,,,,,,jbellis,jbellis,,,1.2.10,,,,,,,"09/Oct/13 01:08;jbellis;Makes sense to me, although I'm not sure it needs to be configurable.  Picking something reasonable like 128 should be fine.",09/Oct/13 18:36;jbellis;LGTM; committed!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't update int column to blob type.,CASSANDRA-6185,12673491,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,nickmbailey,nickmbailey,11/Oct/13 20:45,12/Mar/19 14:05,13/Mar/19 22:29,21/Oct/13 08:16,1.2.12,2.0.2,,,,,0,,,,,"Patch for dtests:

{noformat}
diff --git a/cql_tests.py b/cql_tests.py
index 11461e4..405c998 100644
--- a/cql_tests.py
+++ b/cql_tests.py
@@ -1547,35 +1547,35 @@ class TestCQL(Tester):
             CREATE TABLE test (
                 k text,
                 c text,
-                v text,
+                v int,
                 PRIMARY KEY (k, c)
             )
         """""")

-        req = ""INSERT INTO test (k, c, v) VALUES ('%s', '%s', '%s')""
+        req = ""INSERT INTO test (k, c, v) VALUES ('%s', '%s', %d)""
         # using utf8 character so that we can see the transition to BytesType
-        cursor.execute(req % ('ɸ', 'ɸ', 'ɸ'))
+        cursor.execute(req % ('ɸ', 'ɸ', 1))

         cursor.execute(""SELECT * FROM test"")
         cursor.execute(""SELECT * FROM test"")
         res = cursor.fetchall()
-        assert res == [[u'ɸ', u'ɸ', u'ɸ']], res
+        assert res == [[u'ɸ', u'ɸ', 1]], res

         cursor.execute(""ALTER TABLE test ALTER v TYPE blob"")
         cursor.execute(""SELECT * FROM test"")
         res = cursor.fetchall()
         # the last should not be utf8 but a raw string
-        assert res == [[u'ɸ', u'ɸ', 'ɸ']], res
+        assert res == [[u'ɸ', u'ɸ', '\x00\x00\x00\x01']], res

         cursor.execute(""ALTER TABLE test ALTER k TYPE blob"")
         cursor.execute(""SELECT * FROM test"")
         res = cursor.fetchall()
-        assert res == [['ɸ', u'ɸ', 'ɸ']], res
+        assert res == [['ɸ', u'ɸ', '\x00\x00\x00\x01']], res

         cursor.execute(""ALTER TABLE test ALTER c TYPE blob"")
         cursor.execute(""SELECT * FROM test"")
         res = cursor.fetchall()
-        assert res == [['ɸ', 'ɸ', 'ɸ']], res
+        assert res == [['ɸ', 'ɸ', '\x00\x00\x00\x01']], res

     @since('1.2')
     def composite_row_key_test(self):
{noformat}

",,,,,,,,,,,,,,,,,,,,,,,,14/Oct/13 08:30;slebresne;6185.txt;https://issues.apache.org/jira/secure/attachment/12608254/6185.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-11 20:58:27.057,,,no_permission,,,,,,,,,,,,353114,,,Mon Oct 21 08:16:46 UTC 2013,,,,,,0|i1ovof:,353401,,,,,,,,iamaleksey,iamaleksey,,,1.2.9,,,,,,,11/Oct/13 20:58;brandon.williams;CASSANDRA-5882 is responsible.,"14/Oct/13 08:30;slebresne;Currently, our type compatibility is based on comparison compatibility, and since 'blob' does not compare like 'int', this is refused. That being said, except for clustering columns, we can use a more permissive compatibilty check that don't take comparison order into account. Attaching patch that does that (the patch also slightly clean-up/improve the validation made by AlterTableStatement).
","14/Oct/13 08:45;jbellis;Let's stick with 2.0 for this since it's new functionality and a bit involved.

Edit: or is this actually a regression?","14/Oct/13 09:18;slebresne;bq. or is this actually a regression?

Kind of. In CASSANDRA-5882, we decided on purpose to stop allowing type changes that were not making sense. In doing so, we've been too restrictive, disallowing some type changes that stricly speaking make sense, which you could say is a restriction (of CASSANDRA-5882).

I'll note that the patch really only change code related to altering type, which is hardly mission critical, so I don't think there is too much risk in pushing it in 1.2. But personally I don't really care either way.
","14/Oct/13 09:21;jbellis;All right, I'll set it back to 1.2 (1.2.12, which I just created).","14/Oct/13 12:19;iamaleksey;+1

Comment typos: ATS.announceMigration(), the COLUMN_METADATA case comment ""so CFMetaData.validateCompatility ..."" - Complatility + the same typo in the COLUMN_ALIAS case.",21/Oct/13 07:26;iamaleksey;^,"21/Oct/13 08:16;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skip mutations that pass CRC but fail to deserialize,CASSANDRA-6183,12673465,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,11/Oct/13 17:22,12/Mar/19 14:05,13/Mar/19 22:29,14/Oct/13 19:26,1.2.11,2.0.2,,,,,1,,,,,"We've had a couple reports of CL replay failure that appear to be caused by dropping and recreating the same table with a different schema, e.g. CASSANDRA-5905.  While CASSANDRA-5202 is the ""right"" fix for this, it's too involved for 2.0 let alone 1.2, so we need a stopgap until then.",,,,,,,,,,,,,,,,,,,,,,,,14/Oct/13 18:16;jbellis;6183-v2.txt;https://issues.apache.org/jira/secure/attachment/12608312/6183-v2.txt,11/Oct/13 17:27;jbellis;6183.txt;https://issues.apache.org/jira/secure/attachment/12608034/6183.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-10-14 18:32:29.011,,,no_permission,,,,,,,,,,,,353088,,,Mon Oct 14 19:26:29 UTC 2013,,,,,,0|i1ovin:,353375,,,,,,,,lyubent,lyubent,,,,,,,,,,11/Oct/13 17:27;jbellis;Patch will save skipped mutations out to separate files in case user wishes to perform forensics.,"11/Oct/13 17:29;jbellis;Lyuben, can you review?

To test, I suggest something like this:

# Flush, so only the new row is in the CL.
# Create a table and insert a row.  Copy the active CL segment.
# Drop table, recreate w/ incompatible schem (e.g., int -> double)
# Shutdown, move saved CL segment in; restart",14/Oct/13 18:16;jbellis; v2 attached to validate cell names against comparator,14/Oct/13 18:32;lyubent;v2 LGTM. Gist of the [test and output|https://gist.github.com/lyubent/6979885] ,14/Oct/13 19:26;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CL.ANY writes can still time out,CASSANDRA-6132,12671942,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,02/Oct/13 14:33,12/Mar/19 14:05,13/Mar/19 22:29,03/Oct/13 00:12,2.0.2,,,,,,0,,,,,"If we know that all replicas are down at the beginning of a mutation, we will write a hint and return success.

But if we do not, we will attemp to write to replicas, time out, return failure, and then write a hint, violating our contract that (unless the coordinator goes down), writes at CL.ANY should always succeed.",,,,,,,,,,,,,,,,,,,,,,,,02/Oct/13 19:11;jbellis;6132-v2.txt;https://issues.apache.org/jira/secure/attachment/12606418/6132-v2.txt,02/Oct/13 17:03;jbellis;6132.txt;https://issues.apache.org/jira/secure/attachment/12606386/6132.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-10-02 18:53:13.82,,,no_permission,,,,,,,,,,,,351568,,,Fri Oct 04 21:33:26 UTC 2013,,,,,,0|i1om6v:,351857,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,02/Oct/13 17:03;jbellis;Patch attached against 1.2.,"02/Oct/13 18:53;iamaleksey;Huh, are you sure? It doesn't build for me (WriteCallbackInfo?).

",02/Oct/13 19:11;jbellis;v2,02/Oct/13 23:38;iamaleksey;+1,03/Oct/13 00:12;jbellis;committed w/ additional comment,04/Oct/13 21:33;jbellis;This was messier than I thought and (four ninja commits later) I'm not confident all the corner cases are fixed.  Reverted from 1.2.x; will fix in 2.0.y only.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
COPY TO command doesn't escape single quote in collections,CASSANDRA-6172,12673002,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mishail,IvanMykhailov,IvanMykhailov,09/Oct/13 10:51,12/Mar/19 14:05,13/Mar/19 22:29,24/Nov/13 18:58,1.2.13,2.0.4,,Legacy/Tools,,,0,,,,,"{code}
CREATE TABLE test (key text PRIMARY KEY , testcollection set<text>) ;
INSERT INTO test (key, testcollection ) VALUES ( 'test', {'foo''bar'});
COPY test TO '/tmp/test.csv';
COPY test FROM '/tmp/test.csv';

Bad Request: line 1:73 mismatched character '<EOF>' expecting '''
Aborting import at record #0 (line 1). Previously-inserted values still present.
{code}

Content of generated '/tmp/test.csv':
{code}
test,{'foo'bar'}
{code}

Unfortunately, I didn't find workaround with any combination of COPY options ","Cassandra 2.0.1, Linux",,,,,,,,,,,,,,,,,,,,,,,22/Nov/13 21:51;mishail;CASSANDRA-2.0-6172.patch;https://issues.apache.org/jira/secure/attachment/12615393/CASSANDRA-2.0-6172.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-22 21:51:23.965,,,no_permission,,,,,,,,,,,,352625,,,Sun Nov 24 18:58:14 UTC 2013,,,,,,0|i1osof:,352912,2.0.2,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,22/Nov/13 21:51;mishail;Patch: keep single quotes in a text value,24/Nov/13 18:58;iamaleksey;Committed a *slightly* altered version (that only quotes the 's if we are formatting a collection). Thanks.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SERIAL consistency in errors to v1 protocol driver,CASSANDRA-6270,12676559,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,iconara,iconara,30/Oct/13 06:47,12/Mar/19 14:05,13/Mar/19 22:29,30/Oct/13 16:24,2.0.3,,,,,,0,LWT,,,,"I'm the author of the Ruby driver for CQL, and I got a bug report about strange errors when running on C* 2.0 and using lightweight transaction queries. The bug report can be found here: https://github.com/iconara/cql-rb/issues/53

The client sent {{UPDATE table SET val = 42 WHERE row_id = 5 IF val = 41}} and when C* couldn't fulfill SERIAL consistency it sent an error back saying ""Operation timed out - received only -1 responses"".

So far so good, but it also set the {{consistency}} field in the error response to 8, corresponding to {{SERIAL}} in v2 of the binary protocol, even if the communication with the client was over v1 of the protocol. Since my driver doesn't yet support v2 it doesn't think that 8 is a valid consistency, and fails to parse the frame.

Is this the intended behaviour of C*, or an oversight in how that error is formulated? I could easily add {{SERIAL}} and accept it even if the communication is over v1 of the protocol, but the bigger issue is how C* handles drivers that do not speak the latest version of the protocol. People should be able to use a driver that worked correctly with C* X with C* X+1, right?

Do drivers have to be accepting in what they receive from C* because they might get consistencies, data types, etc. that are from future versions of the protocol, or does C* guarantee that frames will conform to the protocol that the driver says it understands?",Cassandra 2.0,,,,,,,,,,,,,,,,,,,,,,,30/Oct/13 15:34;slebresne;6270.txt;https://issues.apache.org/jira/secure/attachment/12611098/6270.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-30 10:36:12.07,,,no_permission,,,,,,,,,,,,355991,,,Wed Oct 30 16:24:57 UTC 2013,,,,,,0|i1pddj:,356279,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"30/Oct/13 10:36;iamaleksey;v1 native protocol is not intended to support all the Cassandra 2.0 features. (If it could, we wouldn't need v2 native protocol in the first place).

To use 2.0 CAS with native proto you should be using the v2 protocol.","30/Oct/13 13:22;iconara;In that case Cassandra should fail the request because it uses a CQL syntax that is not supported over that version of the protocol. Cassandra has accepted that the connection will use CQL v3.0.0 and the frame was sent with v1 of the protocol. The right thing to do is to fail the request because it uses features that are not in CQL v3.0.0, but instead it sends a frame back that a v1 driver cannot parse.

Drivers can't tell if the CQL is valid because they don't parse it, they can't tell if the CQL uses features not available in the version of the binary protocol or not. Cassandra has to say that the request is not valid, and it can't send frames back that are not valid in the protocol specified by the driver.","30/Oct/13 15:07;adstage-david;I originally reported the bug on the Ruby project, just thought it might be useful to clarify a bit:

I could _sometimes_ successfully use lightweight transactions successfully over the v1 binary protocol the ruby driver is using (specifying a consistency of :quorum on writes), but sometimes it would throw this error. ","30/Oct/13 15:34;slebresne;I agree that sending frames that are not valid is a tad anti-social. Attaching simple patch that throw an InvalidRequestException if the user tries to use CAS with the protocol v1.
","30/Oct/13 16:00;iamaleksey;LGTM, although I'd slightly prefer protocol version be a enum (UNDEFINED, V1, V2), but not strongly.","30/Oct/13 16:24;slebresne;Committed, thanks.

bq. I'd slightly prefer protocol version be a enum (UNDEFINED, V1, V2)

Bah, we use int for the encoding/decoding methods and perhaps more importantly, I'm lazy :)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstableloader data distribution is broken since 1.2.7,CASSANDRA-6272,12676594,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,beobal,beobal,beobal,30/Oct/13 10:23,12/Mar/19 14:05,13/Mar/19 22:29,30/Oct/13 19:12,1.2.12,,,Legacy/Tools,,,0,,,,,"Running sstableloader on 1.2.10+ results in radically different distribution compared with earlier versions. It looks as though the 'bare-bones' IndexSummary created in SSTR.loadForBatch is the cause (CASSANDRA-5555); because it contains only a single entry, we end up with the wrong segment of the index file in SSTR.getPosition (its position is always 0),  so only the first segment of the index file is considered when searching for the range's position.

This doesn't affect  2.0/trunk",,,,,,,,,,,,,,,,,,,,,,,,30/Oct/13 18:46;beobal;0001-CASSANDRA-6272-Load-or-build-a-real-IndexSummary-in-.patch;https://issues.apache.org/jira/secure/attachment/12611138/0001-CASSANDRA-6272-Load-or-build-a-real-IndexSummary-in-.patch,30/Oct/13 21:08;beobal;SSTableReaderTest-for-2.0.patch;https://issues.apache.org/jira/secure/attachment/12611176/SSTableReaderTest-for-2.0.patch,30/Oct/13 21:08;beobal;SSTableReaderTest-for-trunk.patch;https://issues.apache.org/jira/secure/attachment/12611175/SSTableReaderTest-for-trunk.patch,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-10-30 19:12:10.076,,,no_permission,,,,,,,,,,,,356026,,,Wed Oct 30 21:12:53 UTC 2013,,,,,,0|i1pdlb:,356314,1.2.7,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"30/Oct/13 18:46;beobal;Sorry, generated the original patch against an older version, so it doesn't apply to the current 1.2 branch. Replaced with an updated version","30/Oct/13 18:57;beobal;Actually, this problem is present in earlier versions than 1.2.10, as CASSANDRA-5555 was actually included in 1.2.7",30/Oct/13 19:12;brandon.williams;Committed.,"30/Oct/13 20:46;brandon.williams;Sam, can you rewrite the test for trunk?",30/Oct/13 21:08;beobal;Patches with SSTableReaderTest updated for 2.0 & trunk,"30/Oct/13 21:12;brandon.williams;Pushed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add snapshot disk space to cfstats,CASSANDRA-6231,12675154,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mishail,jbellis,jbellis,22/Oct/13 23:18,12/Mar/19 14:05,13/Mar/19 22:29,26/Nov/13 21:21,2.1 beta1,,,,,,0,lhf,,,,"As discussed in CASSANDRA-6179, this could help avoid some user confusion, especially when snapshots are autocreated for drop/truncate.",,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6179,,25/Nov/13 21:55;mishail;CASSANDRA-2.0-6231-v2.patch;https://issues.apache.org/jira/secure/attachment/12615682/CASSANDRA-2.0-6231-v2.patch,25/Nov/13 23:54;mishail;CASSANDRA-2.0-6231-v3.patch;https://issues.apache.org/jira/secure/attachment/12615731/CASSANDRA-2.0-6231-v3.patch,23/Nov/13 20:54;mishail;CASSANDRA-2.0-6231.patch;https://issues.apache.org/jira/secure/attachment/12615472/CASSANDRA-2.0-6231.patch,26/Nov/13 20:01;mishail;trunk-6231-v3.patch;https://issues.apache.org/jira/secure/attachment/12615898/trunk-6231-v3.patch,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-11-23 20:54:55.837,,,no_permission,,,,,,,,,,,,354774,,,Tue Nov 26 21:21:15 UTC 2013,,,,,,0|i1p5vj:,355063,,,,,,,,jbellis,jbellis,,,,,,,,,,"23/Nov/13 20:54;mishail;Patch: added a method to {{Directories}} to calculate an allocated size in ""snapshots"" directory","25/Nov/13 18:03;jbellis;How hard would it be to break out ""space used by sstables that aren't live anymore?""  Could be confusing to double-count (or 10x-count or more with lots of snapshots).","25/Nov/13 21:55;mishail;Patch v2. Separately report a space used by obsolete sstables. Assuming ""obsolete"" are those not reported by {{SSTableLister}}","25/Nov/13 22:19;jbellis;- Both numbers need to be ""de-duplicated"" across snapshots right?
- Looks like we could cut down on the file walking if we restricted it to the subdirectory of the CFS in question","25/Nov/13 22:37;mishail;bq. Both numbers need to be ""de-duplicated"" across snapshots right?
I will fix that

bq. Looks like we could cut down on the file walking if we restricted it to the subdirectory of the CFS in question
We start walking from ""snapshots"" subdirectory of the CFS in question. Not sure how it can be reduced/restricted further.","25/Nov/13 22:51;jbellis;Ah, right.  Then isn't the prefix check redundant?","25/Nov/13 22:56;mishail;bq. Ah, right. Then isn't the prefix check redundant?

That is to separate the CF in question and its ""index"" CFs, they are reported separately in cfstats.  ","25/Nov/13 23:06;mishail;And it seems that we only have to report that ""obsolete"" number. Because it makes no sense to count those from snapshots which are just links to existing files.",25/Nov/13 23:12;jbellis;Makes sense to me.,"25/Nov/13 23:54;mishail;Patch v3. Count only the size of snapshot files (once for each file) which are not links for ""live"" SSTables.","26/Nov/13 05:21;jbellis;LGTM!

Last thing, the size methods in CFSMBean are deprecated because we're moving to CFMetrics.  Not much reason to add this to both places; let's just do it in metrics.  (So it's probably easier to just do this for trunk since otherwise you'd have to do support both in nodecmd.)","26/Nov/13 19:22;nickmbailey;I'd just like to make a note that we've (opscenter) have seen issues with inspecting snapshots for LCS column families. LCS can create a very large number of sstable files (anywhere from 10k to 100k+ range) , and just storing strings for all the file names was giving us some issues. In our case we were dealing with much smaller heap sizes though. Also this is already handling duplicate sstables, but it doesn't sound unreasonable that the number of distinct files could get extremely large, even by just taking daily snapshots.","26/Nov/13 19:37;jbellis;100k distinct filenames would be about 50MB of heap at 500 bytes each, and 16TB worth of data on disk.  I'm okay with those numbers.  (If you're still using 5MB sstables you should probably fix that before calling this, but you probably already have more important reasons to fix that.)",26/Nov/13 20:01;mishail;V3 of the patch with changes for trunk. (Use o.a.c.metrcis),26/Nov/13 21:21;jbellis;committed!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL LIST USERS does nothing after a user is created.,CASSANDRA-6242,12675836,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mishail,bensykes,bensykes,25/Oct/13 16:11,12/Mar/19 14:05,13/Mar/19 22:29,26/Oct/13 12:33,2.0.3,,,Legacy/Tools,,,0,,,,,"After using CREATE USER to create a new user, the LIST USERS command returns nothing to the console.
After removing this user again, the command works as expected.

{code}
$ ./cqlsh -u cassandra -p cassandra
Connected to Test Cluster at localhost:9160.
[cqlsh 4.0.1 | Cassandra 2.0.1 | CQL spec 3.1.1 | Thrift protocol 19.37.0]
Use HELP for help.
cqlsh> LIST USERS;

 name      | super
-----------+-------
 cassandra |  True

cqlsh> CREATE USER bob WITH PASSWORD 'example' NOSUPERUSER;
cqlsh> LIST USERS;
cqlsh> SELECT * FROM system_auth.users;

 name      | super
-----------+-------
       bob | False
 cassandra |  True

(2 rows)

cqlsh> DROP USER bob;
cqlsh> LIST USERS;

 name      | super
-----------+-------
 cassandra |  True

cqlsh>
{code}","cqlsh 4.0.1 | Cassandra 2.0.1 | CQL spec 3.1.1 | Thrift protocol 19.37.0
java version ""1.6.0_43""
Java(TM) SE Runtime Environment (build 1.6.0_43-b01)
Java HotSpot(TM) 64-Bit Server VM (build 20.14-b01, mixed mode)
Windows 7 - CQL running in Cygwin.
Python 2.7.3",,,,,,,,,,,,,,,,,,,,,,,26/Oct/13 05:44;mishail;cassandra-2.0-6242.patch;https://issues.apache.org/jira/secure/attachment/12610444/cassandra-2.0-6242.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-26 05:38:35.074,,,no_permission,,,,,,,,,,,,355334,,,Sat Oct 26 12:33:44 UTC 2013,,,,,,0|i1p9br:,355622,2.0.1,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,26/Oct/13 05:38;mishail;Issue in cqlsh,"26/Oct/13 05:44;mishail;Patch to print out results for ""LIST"" commands as well","26/Oct/13 12:33;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception count not incremented on OutOfMemoryError (HSHA),CASSANDRA-6255,12676130,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mishail,dhendry,dhendry,28/Oct/13 14:49,12/Mar/19 14:05,13/Mar/19 22:29,17/Jan/14 16:09,1.2.14,,,,,,1,,,,,"One of our nodes decided to stop listening on 9160 (netstat -l was showing nothing and telnet was reporting connection refused). Nodetool status showed no hosts down and on the offending node nodetool info gave the following:

{noformat}
nodetool info
Token            : (invoke with -T/--tokens to see all 256 tokens)
ID               : (removed)
Gossip active    : true
Thrift active    : true
Native Transport active: false
Load             : 2.05 TB
Generation No    : 1382536528
Uptime (seconds) : 432970
Heap Memory (MB) : 8098.05 / 14131.25
Data Center      : DC1
Rack             : RAC2
Exceptions       : 0
Key Cache        : size 536854996 (bytes), capacity 536870912 (bytes), 41383646 hits, 1710831591 requests, 0.024 recent hit rate, 0 save period in seconds
Row Cache        : size 0 (bytes), capacity 0 (bytes), 0 hits, 0 requests, NaN recent hit rate, 0 save period in seconds
{noformat}

After looking at the cassandra log, I saw a bunch of the following:

{noformat}
ERROR [Selector-Thread-16] 2013-10-27 17:36:00,370 CustomTHsHaServer.java (line 187) Uncaught Exception: 
java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:691)
        at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1371)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.execute(DebuggableThreadPoolExecutor.java:145)
        at org.apache.cassandra.thrift.CustomTHsHaServer.requestInvoke(CustomTHsHaServer.java:337)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.handleRead(CustomTHsHaServer.java:281)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.select(CustomTHsHaServer.java:224)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.run(CustomTHsHaServer.java:182)
ERROR [Selector-Thread-7] 2013-10-27 17:36:00,370 CustomTHsHaServer.java (line 187) Uncaught Exception: 
java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:691)
        at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1371)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.execute(DebuggableThreadPoolExecutor.java:145)
        at org.apache.cassandra.thrift.CustomTHsHaServer.requestInvoke(CustomTHsHaServer.java:337)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.handleRead(CustomTHsHaServer.java:281)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.select(CustomTHsHaServer.java:224)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.run(CustomTHsHaServer.java:182)
{noformat}

There wasn't anything else overtly suspicious in the logs except for the occasional 
{noformat}
ERROR [Selector-Thread-0] 2013-10-27 17:35:58,662 TNonblockingServer.java (line 468) Read an invalid frame size of 0. Are you using TFramedTransport on the client side?
{noformat}
but  that periodically comes up - I have looked into it before but it has never seemed to have any serious impact.

This ticket is not about *why* an OutOfMemoryError occurred - which is bad but I don't think I have enough information to reproduce or speculate on a cause. This ticket is about the fact that an OutOfMemoryError occurred and nodetool info was reporting Thrift active : true and Exceptions : 0. 

Our monitoring systems and investigation processes are both starting to rely on on the exception count. The fact that it was not accurate here is disconcerting.","Oracle java version ""1.7.0_15""

rpc_server_type: hsha",,,,,,,,,,,,,,,,,,,,,,,17/Jan/14 05:42;mishail;CASSANDRA-1.2-6255.patch;https://issues.apache.org/jira/secure/attachment/12623583/CASSANDRA-1.2-6255.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-16 22:48:00.687,,,no_permission,,,,,,,,,,,,355627,,,Fri Jan 17 16:09:45 UTC 2014,,,,,,0|i1pb4n:,355915,1.2.13,,,,,,,brandon.williams,brandon.williams,,,1.2.10,,,,,,,"16/Jan/14 22:48;luapmahp;We've experienced almost precisely the same issue, both in the sense that cassandra stopped listening on 9160 due to an apparent out-of-memory problem, and also in observing the ""invalid frame size of 0"" error, which we've also observed to a lesser degree in the past (but trending up as of late) and haven't been able to root cause.

One other interesting data point is that all 3 of our nodes in our main datacenter experienced this same issue at the same time. Our working theory is that the trigger was the starting of many applications simultaneously was too much for the current cluster to handle (with both reads and writes using a consistency level of TWO). 

Relevant log snippet:
{code}
INFO [ScheduledTasks:1] 2014-01-16 19:01:45,208 GCInspector.java (line 119) GC for ConcurrentMarkSweep: 921 ms for 1 collections, 1389728032 used; max is 2105540608
ERROR [Selector-Thread-0] 2014-01-16 19:01:48,813 CustomTHsHaServer.java (line 187) Uncaught Exception:
java.lang.OutOfMemoryError: Java heap space
        at java.nio.HeapByteBuffer.<init>(Unknown Source)
        at java.nio.ByteBuffer.allocate(Unknown Source)
        at org.apache.thrift.server.TNonblockingServer$FrameBuffer.read(TNonblockingServer.java:491)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.handleRead(CustomTHsHaServer.java:273)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.select(CustomTHsHaServer.java:224)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.run(CustomTHsHaServer.java:182)
 INFO [ScheduledTasks:1] 2014-01-16 19:01:48,822 GCInspector.java (line 119) GC for ConcurrentMarkSweep: 3125 ms for 2 collections, 1390753448 used; max is 2105540608
{code}

Strangely, you can see that we're not really close to our max heap limit, although it was inching close to our configured ""CMSInitiatingOccupancyFraction"" which is effectively 1.5/2GB.

We were ready to chalk this up to an increased load on the cassandra cluster (and a need for beefier/more nodes). But I'd also echo Dan's concern that the health reporting is not sufficient/accurate for this particular issue (which I think is the main ask for this bug). We were only able to detect this issue because we had a monit check checking for expected listening ports.

","17/Jan/14 05:08;mishail;The problem is that {{CustomTHsHaServer.SelectorThread}} swallows all Throwables (including {{Error}} ) in its {{run}} method. Probably that was done for a reason. 
I guess it would be reasonable to increment the exceptions counter in that {{catch}} block, since the exception will be swallowed, and won't be handled by global {{UncaughtExceptionHandler}}
","17/Jan/14 05:14;jbellis;If it's an easy fix let's go ahead and do it; otherwise, that class is replaced in 2.0 with the Disruptor server.",17/Jan/14 05:18;jbellis;(Should probably rethrow the OOM so the global handler can shut down the server.),"17/Jan/14 05:42;mishail;Patch. 
* Don't swallow OOM
* Increase the exception counter in all other cases",17/Jan/14 16:09;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Murmur3Partitioner doesn't yield proper ownership calculation,CASSANDRA-6289,12677192,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,cywjackson,cywjackson,02/Nov/13 01:47,12/Mar/19 14:05,13/Mar/19 22:29,21/Nov/13 20:19,2.0.4,,,Legacy/Tools,,,0,,,,,"In a new 1.2 install with Murmur3 as default, I setup a test cluster with N=RF=3 for the cluster size and RF for a keyspace

but when I look at the ring output (with the keyspace name), to my surprise it shows RF=2.

Further investigate shows the ""total replica"" is an addition of the float value from the effectiveOwnership. But that results in < 1 for the setup:
{panel}
#bean is set to org.apache.cassandra.db:type=StorageService
$>run effectiveOwnership Keyspace1
#calling operation effectiveOwnership of mbean org.apache.cassandra.db:type=StorageService
#operation returns: 
\{ 
  /127.0.0.1 = 0.9999989;
  /127.0.0.2 = 0.9999989;
  /127.0.0.3 = 0.9999989;
 \}
{panel}

{panel}
$ ./bin/nodetool -h 0 -p 7100 ring Keyspace1

Datacenter: datacenter1
==========
Replicas: 2

Address    Rack        Status State   Load            Owns                Token                                       
                                                                          3074457345618258602                         
127.0.0.1  rack1       Up     Normal  1.02 GB         100.00%             -9223372036854775808                        
127.0.0.2  rack1       Up     Normal  996.38 MB       100.00%             -3074457345618258603                        
127.0.0.3  rack1       Up     Normal  980.55 MB       100.00%             3074457345618258602 
{panel}

{panel}
Keyspace: Keyspace1:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:3]
{panel}
The println would simply class the float value to int, so i guess that's round down.

When using RandomPartitioner, the effectiveOwnership will return 1.0 

So I guess the real question is, is the Murmur3 calculation correct? Or is it losing precision? If it is correct, then I guess we need to force the float -> int to round up? (is that even the right thing to do?)",,,,,,,,,,,,,,,,,,,,,,,,12/Nov/13 14:45;jbellis;6289-v2.txt;https://issues.apache.org/jira/secure/attachment/12613374/6289-v2.txt,12/Nov/13 05:46;mishail;cassandra-1.2-6289.patch;https://issues.apache.org/jira/secure/attachment/12613324/cassandra-1.2-6289.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-11-12 05:46:58.043,,,no_permission,,,,,,,,,,,,356567,,,Thu Nov 21 20:19:03 UTC 2013,,,,,,0|i1pgx3:,356855,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,12/Nov/13 05:46;mishail;Use {{Math.round}} instead of truncating,12/Nov/13 13:31;jeromatron;Might this be a similar problem in RandomPartitioner?  Just wondering if CASSANDRA-6320 might have a similar fix.,12/Nov/13 14:26;jbellis;6320 is about ownership changes which is not the case here.,"12/Nov/13 14:45;jbellis;I think the real problem is that displaying replica count derived from sum(ownership) is fundamentally broken.  Consider using SimpleStrategy instead of NTS -- you'll just get nonsense.

I think the right solution is to rip this out instead of trying to compensate for one inaccuracy by applying more on an ad hoc basis until it looks right in some cases.  But, I don't want to break anyone parsing nodetool output in 1.2.x.  Patch attached for 2.0.",21/Nov/13 20:15;brandon.williams;+1,21/Nov/13 20:19;jbellis;committed after IRC +1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
json2sstable breaks on RangeTombstone,CASSANDRA-6316,12678197,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lyubent,exabytes18,exabytes18,08/Nov/13 11:08,12/Mar/19 14:05,13/Mar/19 22:29,28/Nov/13 02:14,1.2.13,2.0.3,,,,,0,,,,,"It seems that sstable2json writes out json which json2sstable has trouble reading.

{code}
java.lang.NumberFormatException: An hex string representing bytes must have an even length
	at org.apache.cassandra.utils.Hex.hexToBytes(Hex.java:52)
	at org.apache.cassandra.utils.ByteBufferUtil.hexToBytes(ByteBufferUtil.java:503)
	at org.apache.cassandra.tools.SSTableImport.stringAsType(SSTableImport.java:572)
	at org.apache.cassandra.tools.SSTableImport.access$000(SSTableImport.java:66)
	at org.apache.cassandra.tools.SSTableImport$JsonColumn.<init>(SSTableImport.java:158)
	at org.apache.cassandra.tools.SSTableImport.addColumnsToCF(SSTableImport.java:229)
	at org.apache.cassandra.tools.SSTableImport.addToStandardCF(SSTableImport.java:212)
	at org.apache.cassandra.tools.SSTableImport.importUnsorted(SSTableImport.java:361)
	at org.apache.cassandra.tools.SSTableImport.importJson(SSTableImport.java:318)
	at org.apache.cassandra.tools.SSTableImport.main(SSTableImport.java:537)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)
ERROR: An hex string representing bytes must have an even length
12345:2:!
{code}",1.2.10,,,,,,,,,,,,,,,,,,,,,,,11/Nov/13 15:38;lyubent;6316.diff;https://issues.apache.org/jira/secure/attachment/12613167/6316.diff,14/Nov/13 03:24;lyubent;6316_cassandra-2.0.diff;https://issues.apache.org/jira/secure/attachment/12613776/6316_cassandra-2.0.diff,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-11-08 14:06:30.308,,,no_permission,,,,,,,,,,,,357572,,,Thu Nov 28 02:14:40 UTC 2013,,,,,,0|i1pn47:,357862,1.2.11,2.0.2,,,,,,iamaleksey,iamaleksey,,,,,,,,,,08/Nov/13 14:06;lyubent;[~exabytes18] Could you please post the schema of the cf you are trying to export/import? ,"08/Nov/13 19:38;exabytes18;Pretty trivial schema:

{code}
cqlsh> CREATE KEYSPACE test_range_tombstones WITH replication = {'class':'SimpleStrategy', 'replication_factor':1};
cqlsh> use test_range_tombstones;
cqlsh:test_range_tombstones> CREATE TABLE tbl (
                         ...     a INT,
                         ...     b INT,
                         ...     c INT,
                         ...     d TIMESTAMP,
                         ...     e BOOLEAN,
                         ...     PRIMARY KEY(a, b, c)
                         ... ) WITH compaction = { 'class' : 'LeveledCompactionStrategy' };
cqlsh:test_range_tombstones> insert into tbl (a, b, c, d, e) values (1, 2, 3, 4, false);
cqlsh:test_range_tombstones> select * from tbl;

 a | b | c | d                        | e
---+---+---+--------------------------+-------
 1 | 2 | 3 | 1969-12-31 16:00:00-0800 | False

cqlsh:test_range_tombstones> delete from tbl where a = 5 and b = 2;
{code}",11/Nov/13 15:37;lyubent;When SSTableImport#JsonColumn tries to find the comparator for the tombstoned range it fails and defaults to a BytesType comparator. I special-cased the tombstoned columns to get their comparator from the 1st component of the cf. [Gist of the schemas used for verification|https://gist.github.com/lyubent/7415048].,12/Nov/13 17:44;iamaleksey;[~lyubent] Could you attach a patch for 2.0? Keep in mind that CASSANDRA-5435 exists.,14/Nov/13 03:24;lyubent;[~iamaleksey] This patch is fundamentally the same (maybe I'm missing something) but as far as my testing shows it works for thrift and cql in C* 2.0. ,19/Nov/13 22:24;jbellis;So... whose ball is this? :),"19/Nov/13 22:30;iamaleksey;Mine, although Sylvain has already merged 1.2 into 2.0 and trunk some time ago.","28/Nov/13 02:13;iamaleksey;Oh. My bad.

The original 1.2 patch was incorrect - we cannot/should not be getting comparator from the first component b/c
a) It does not import range tombstones with 2+ components correctly and
b) there is no need to, we already know the comparator from the metadata, and simple comparator.fromString() is enough.

Backported Sylvain's merge-commit version from 2.0 instead.",28/Nov/13 02:14;iamaleksey;(69c1ee96025dd35ed37bce2a2ccc0c2ca5a3dfed),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make CqlPagingRecordReader more robust to failures,CASSANDRA-6302,12677638,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,05/Nov/13 20:22,12/Mar/19 14:05,13/Mar/19 22:29,05/Nov/13 21:29,1.2.12,2.0.3,,,,,0,,,,,"CPPR currently bails if the first location fails for any reason, and generates invalid CQL if only the row key is specified in the column list.",,,,,,,,,,,,,,,,,,,,,,,,05/Nov/13 20:23;brandon.williams;0001-Try-connecting-to-more-than-the-first-location.txt;https://issues.apache.org/jira/secure/attachment/12612241/0001-Try-connecting-to-more-than-the-first-location.txt,05/Nov/13 20:23;brandon.williams;0002-avoid-generating-broken-cql-when-only-the-row-key-is-s.txt;https://issues.apache.org/jira/secure/attachment/12612242/0002-avoid-generating-broken-cql-when-only-the-row-key-is-s.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-11-05 21:19:23.152,,,no_permission,,,,,,,,,,,,357013,,,Tue Nov 05 21:29:56 UTC 2013,,,,,,0|i1pjo7:,357303,,,,,,,,jbellis,jbellis,,,,,,,,,,05/Nov/13 21:19;jbellis;LGTM modulo brace-on-newline.,05/Nov/13 21:29;brandon.williams;Committed with brace fix.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thread leak caused in creating OutboundTcpConnectionPool,CASSANDRA-6308,12677834,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,timiblossom,timiblossom,timiblossom,06/Nov/13 18:32,12/Mar/19 14:05,13/Mar/19 22:29,06/Nov/13 20:34,1.2.12,2.0.3,,,,,0,leak,thread,,,"We have seen in one of our large clusters that there are many OutboundTcpConnection threads having the same names.  From a thread dump, OutboundTcpConnection threads have accounted for the largest shares of the total threads (65%+) and kept growing.

Here is a portion of a grep output for threads in which names start with ""WRITE-"":

""WRITE-/10.28.131.195"" daemon prio=10 tid=0x00002aaac4022000 nid=0x2cb5 waiting on condition [0x00002acfbacda000]
""WRITE-/10.28.131.195"" daemon prio=10 tid=0x00002aaac42fe000 nid=0x2cb4 waiting on condition [0x00002acfbacad000]
""WRITE-/10.30.142.49"" daemon prio=10 tid=0x0000000040840000 nid=0x2cb1 waiting on condition [0x00002acfbac80000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004083e000 nid=0x2cb0 waiting on condition [0x00002acfbac53000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004083b800 nid=0x2caf waiting on condition [0x00002acfbac26000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040839800 nid=0x2cae waiting on condition [0x00002acfbabf9000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040837800 nid=0x2cad waiting on condition [0x00002acfbabcc000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000404a3800 nid=0x2cac waiting on condition [0x00002acfbab9f000]
""WRITE-/10.30.142.49"" daemon prio=10 tid=0x00000000404a1800 nid=0x2cab waiting on condition [0x00002acfbab72000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004049f800 nid=0x2caa waiting on condition [0x00002acfbab45000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004049e000 nid=0x2ca9 waiting on condition [0x00002acfbab18000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004049c800 nid=0x2ca8 waiting on condition [0x00002acfbaaeb000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x000000004049a800 nid=0x2ca7 waiting on condition [0x00002acfbaabe000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040498800 nid=0x2ca6 waiting on condition [0x00002acfbaa91000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040496800 nid=0x2ca5 waiting on condition [0x00002acfbaa64000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040717800 nid=0x2ca4 waiting on condition [0x00002acfbaa37000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040716000 nid=0x2ca3 waiting on condition [0x00002acfbaa0a000]
""WRITE-/10.30.146.195"" daemon prio=10 tid=0x0000000040714800 nid=0x2ca2 waiting on condition [0x00002acfba9dd000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040712800 nid=0x2ca1 waiting on condition [0x00002acfba9b0000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040710800 nid=0x2ca0 waiting on condition [0x00002acfba983000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004070e800 nid=0x2c9f waiting on condition [0x00002acfba956000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004070d000 nid=0x2c9e waiting on condition [0x00002acfba929000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004070b800 nid=0x2c9d waiting on condition [0x00002acfba8fc000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004070a000 nid=0x2c9c waiting on condition [0x00002acfba8cf000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040827000 nid=0x2c9b waiting on condition [0x00002acfba8a2000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040825000 nid=0x2c9a waiting on condition [0x00002acfba875000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00002aaac488e000 nid=0x2c99 waiting on condition [0x00002acfba848000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040823000 nid=0x2c98 waiting on condition [0x00002acfba81b000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040821800 nid=0x2c97 waiting on condition [0x00002acfba7ee000]
""WRITE-/10.30.146.195"" daemon prio=10 tid=0x000000004081f000 nid=0x2c96 waiting on condition [0x00002acfba7c1000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004081d000 nid=0x2c95 waiting on condition [0x00002acfba794000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004081b000 nid=0x2c94 waiting on condition [0x00002acfba767000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00002aaac488b000 nid=0x2c93 waiting on condition [0x00002acfba73a000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040819000 nid=0x2c92 waiting on condition [0x00002acfba70d000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407f9000 nid=0x2c91 waiting on condition [0x00002acfba6e0000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407f7000 nid=0x2c90 waiting on condition [0x00002acfba6b3000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407f5000 nid=0x2c8f waiting on condition [0x00002acfba686000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407f3000 nid=0x2c8d waiting on condition [0x00002acfba659000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407f1800 nid=0x2c8c waiting on condition [0x00002acfba62c000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000407ef000 nid=0x2c8b waiting on condition [0x00002acfba5ff000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000407ed800 nid=0x2c8a waiting on condition [0x00002acfba5d2000]
""WRITE-/10.28.131.195"" daemon prio=10 tid=0x00000000407ec000 nid=0x2c89 waiting on condition [0x00002acfba5a5000]
""WRITE-/10.30.161.144"" daemon prio=10 tid=0x00000000407e9800 nid=0x2c88 waiting on condition [0x00002acfba578000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405f5000 nid=0x2c87 waiting on condition [0x00002acfba54b000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405f3000 nid=0x2c86 waiting on condition [0x00002acfba51e000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405f1000 nid=0x2c85 waiting on condition [0x00002acfba4f1000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405ef000 nid=0x2c83 waiting on condition [0x00002acfba4c4000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405ed800 nid=0x2c82 waiting on condition [0x00002acfba497000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405eb800 nid=0x2c81 waiting on condition [0x00002acfba46a000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405ea000 nid=0x2c80 waiting on condition [0x00002acfba43d000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405e8800 nid=0x2c7f waiting on condition [0x00002acfba40f000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405e7800 nid=0x2c7e waiting on condition [0x00002acfba3e2000]
""WRITE-/10.30.161.144"" daemon prio=10 tid=0x0000000040607000 nid=0x2c7d waiting on condition [0x00002acfba3b5000]
""WRITE-/10.30.161.144"" daemon prio=10 tid=0x0000000040605800 nid=0x2c7c waiting on condition [0x00002acfba388000]
""WRITE-/10.30.142.49"" daemon prio=10 tid=0x0000000040604000 nid=0x2c7b waiting on condition [0x00002acfba35b000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x0000000040602000 nid=0x2c7a waiting on condition [0x00002acfba32e000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405ff800 nid=0x2c79 waiting on condition [0x00002acfba301000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405fe000 nid=0x2c78 waiting on condition [0x00002acfba2d4000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405fc000 nid=0x2c77 waiting on condition [0x00002acfba2a7000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405fa800 nid=0x2c75 waiting on condition [0x00002acfba27a000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x0000000040af9800 nid=0x2c74 waiting on condition [0x00002acfba24d000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x0000000040af8000 nid=0x2c73 waiting on condition [0x00002acfba220000]
""WRITE-/10.30.161.144"" daemon prio=10 tid=0x0000000040af6000 nid=0x2c72 waiting on condition [0x00002acfba1f3000]
""WRITE-/10.28.131.195"" daemon prio=10 tid=0x0000000040af4000 nid=0x2c71 waiting on condition [0x00002acfba1c6000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x0000000040af2000 nid=0x2c70 waiting on condition [0x00002acfba199000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040af0800 nid=0x2c6f waiting on condition [0x00002acfba16c000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040aef000 nid=0x2c6e waiting on condition [0x00002acfba13f000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040aed000 nid=0x2c6d waiting on condition [0x00002acfba112000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040aeb800 nid=0x2c6b waiting on condition [0x00002acfba0b8000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00002aaac46b9000 nid=0x2c6a waiting on condition [0x00002acfba08b000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407b3000 nid=0x2c69 waiting on condition [0x00002acfba05e000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407b1800 nid=0x2c68 waiting on condition [0x00002acfba031000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407af800 nid=0x2c66 waiting on condition [0x00002acfba004000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407ae000 nid=0x2c65 waiting on condition [0x00002acfb9fd7000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407ab800 nid=0x2c64 waiting on condition [0x00002acfb9faa000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407a9800 nid=0x2c63 waiting on condition [0x00002acfb9f7d000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407a8000 nid=0x2c62 waiting on condition [0x00002acfb9f50000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407a6800 nid=0x2c61 waiting on condition [0x00002acfb9f23000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000408d2800 nid=0x2c60 waiting on condition [0x00002acfb9ef6000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000408d1000 nid=0x2c5f waiting on condition [0x00002acfb9ec9000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000408cf800 nid=0x2c5d waiting on condition [0x00002acfb9e9c000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000408cd800 nid=0x2c5c waiting on condition [0x00002acfb9e6f000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000408cc000 nid=0x2c5b waiting on condition [0x00002acfb9e42000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004088d800 nid=0x2c5a waiting on condition [0x00002acfb9e15000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004088b000 nid=0x2c59 waiting on condition [0x00002acfb9de8000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x0000000040889000 nid=0x2c58 waiting on condition [0x00002acfb9dbb000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040887800 nid=0x2c57 waiting on condition [0x00002acfb9d8e000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040720000 nid=0x2c56 waiting on condition [0x00002acfb9d61000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004071f000 nid=0x2c55 waiting on condition [0x00002acfb9d34000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407c3000 nid=0x2c54 waiting on condition [0x00002acfb9d07000]
""WRITE-/10.78.95.30"" daemon prio=10 tid=0x00000000407c1800 nid=0x2c53 waiting on condition [0x00002acfb9cda000]
""WRITE-/10.28.131.195"" daemon prio=10 tid=0x00000000407c0000 nid=0x2c52 waiting on condition [0x00002acfb9cac000]
""WRITE-/10.28.131.195"" daemon prio=10 tid=0x00000000407be000 nid=0x2c51 waiting on condition [0x00002acfb9c7f000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000405cc000 nid=0x2c50 waiting on condition [0x00002acfb9c52000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000405ca800 nid=0x2c4f waiting on condition [0x00002acfb9c24000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000405c8800 nid=0x2c4e waiting on condition [0x00002acfb9bf7000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000405c6800 nid=0x2c4d waiting on condition [0x00002acfb9bca000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00002aaac5010800 nid=0x2c4c waiting on condition [0x00002acfb9b9c000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00002aaac4cd9800 nid=0x2c4b waiting on condition [0x00002acfb9b6f000]
""WRITE-/10.11.15.209"" daemon prio=10 tid=0x0000000040756800 nid=0x2c4a waiting on condition [0x00002acfb9b42000]
""WRITE-/10.11.15.209"" daemon prio=10 tid=0x0000000040754800 nid=0x2c49 waiting on condition [0x00002acfb9b15000]

 

We have patched this https://issues.apache.org/jira/browse/CASSANDRA-5175 but I don't this fix solves the issue totally.  I will attach  a patch soon. 

",,,,,,,,,,,,,,,,,,,,,,,,06/Nov/13 20:26;timiblossom;patch.txt;https://issues.apache.org/jira/secure/attachment/12612434/patch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-06 20:34:58.58,,,no_permission,,,,,,,,,,,,357209,,,Wed Nov 06 20:34:58 UTC 2013,,,,,,0|i1pkvj:,357499,1.2.10,,,,,,,jbellis,jbellis,,,,,,,,,,06/Nov/13 20:34;jbellis;LGTM; committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError on startup reading saved Serializing row cache,CASSANDRA-6325,12678380,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mishail,cburroughs,cburroughs,09/Nov/13 05:29,12/Mar/19 14:05,13/Mar/19 22:29,04/Feb/15 18:27,1.2.12,2.0.3,,,,,0,,,,,"I don't see any reason what this could have to do with the upgrade, but don't have a large enough non-prod cluster to just keep restarting on.  Occurred on roughly 2 out of 100 restarted nodes. 

{noformat}
ERROR [main] 2013-11-08 14:40:13,535 CassandraDaemon.java (line 482) Exception encountered during startup
java.lang.AssertionError
        at org.apache.cassandra.cache.SerializingCacheProvider$RowCacheSerializer.serialize(SerializingCacheProvider.java:41)
        at org.apache.cassandra.cache.SerializingCacheProvider$RowCacheSerializer.serialize(SerializingCacheProvider.java:37)
        at org.apache.cassandra.cache.SerializingCache.serialize(SerializingCache.java:118)
        at org.apache.cassandra.cache.SerializingCache.put(SerializingCache.java:176)
        at org.apache.cassandra.cache.InstrumentingCache.put(InstrumentingCache.java:44)
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:156)
        at org.apache.cassandra.db.ColumnFamilyStore.initRowCache(ColumnFamilyStore.java:444)
        at org.apache.cassandra.db.Table.open(Table.java:114)
        at org.apache.cassandra.db.Table.open(Table.java:87)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:278)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:465)
{noformat}

I have the files if there is any useful analysis that can be run.  Looked 'normal' to a cursory `less` inspection.

Possibly related: CASSANDRA-4463",upgrade from 1.2.9ish to 1.2.11ish,,,,,,,,,,,,,,,,,,,,,,,11/Nov/13 16:10;jbellis;6325-v2.txt;https://issues.apache.org/jira/secure/attachment/12613172/6325-v2.txt,10/Nov/13 01:02;mishail;CASSANDRA-1.2-6325.patch;https://issues.apache.org/jira/secure/attachment/12613022/CASSANDRA-1.2-6325.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-11-10 01:02:15.193,,,no_permission,,,,,,,,,,,,357755,,,Wed Feb 04 18:27:33 UTC 2015,,,,,,0|i1po8v:,358045,,,,,,,,jbellis,jbellis,,,,,,,,,,10/Nov/13 01:02;mishail;_SerializingCache.serialize_ will return {{null}} for a {{null}} value,"11/Nov/13 16:10;jbellis;Hmm, that would work but since we're already using serialize() == null to mean ""out of memory"" I'd rather avoid using it to mean ""you passed a null value"" as well.

v2 attached to avoid passing a null value instead.","12/Nov/13 14:57;jbellis;WDYT, Mikhail?",12/Nov/13 17:49;mishail;+1,12/Nov/13 21:51;jbellis;committed,"04/Feb/15 18:23;ngrigoriev@gmail.com;I have started seeing it recently. Not sure from which version but now it happens relatively often one some of my nodes.

{code}
 INFO [main] 2015-02-04 18:18:09,253 ColumnFamilyStore.java (line 249) Initializing duo_xxxxxxxxxxx
 INFO [main] 2015-02-04 18:18:09,254 AutoSavingCache.java (line 114) reading saved cache /var/lib/cassandra/saved_caches/duo_xxxxxxxxxxx-RowCach
e-b.db
ERROR [main] 2015-02-04 18:18:09,256 CassandraDaemon.java (line 513) Exception encountered during startup
java.lang.AssertionError
        at org.apache.cassandra.cache.SerializingCacheProvider$RowCacheSerializer.serialize(SerializingCacheProvider.java:41)
        at org.apache.cassandra.cache.SerializingCacheProvider$RowCacheSerializer.serialize(SerializingCacheProvider.java:37)
        at org.apache.cassandra.cache.SerializingCache.serialize(SerializingCache.java:118)
        at org.apache.cassandra.cache.SerializingCache.put(SerializingCache.java:177)
        at org.apache.cassandra.cache.InstrumentingCache.put(InstrumentingCache.java:44)
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:130)
        at org.apache.cassandra.db.ColumnFamilyStore.initRowCache(ColumnFamilyStore.java:592)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:119)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:92)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:305)
        at com.datastax.bdp.server.DseDaemon.setup(DseDaemon.java:419)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:496)
        at com.datastax.bdp.server.DseDaemon.main(DseDaemon.java:659)
 INFO [Thread-2] 2015-02-04 18:18:09,259 DseDaemon.java (line 505) DSE shutting down...
ERROR [Thread-2] 2015-02-04 18:18:09,279 CassandraDaemon.java (line 199) Exception in thread Thread[Thread-2,5,main]
java.lang.AssertionError
        at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1274)
        at com.datastax.bdp.gms.DseState.setActiveStatus(DseState.java:171)
        at com.datastax.bdp.server.DseDaemon.stop(DseDaemon.java:506)
        at com.datastax.bdp.server.DseDaemon$1.run(DseDaemon.java:408)
 INFO [main] 2015-02-04 18:18:49,144 CassandraDaemon.java (line 135) Logging initialized
 INFO [main] 2015-02-04 18:18:49,169 DseDaemon.java (line 382) DSE version: 4.6.0
{code}


Cassandra version: 2.0.11.83 (DSE 4.6.0)",04/Feb/15 18:27;brandon.williams;Please open in a new ticket instead of one that's over a year old.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh should handle 'null' as session duration,CASSANDRA-6317,12678204,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mishail,exabytes18,exabytes18,08/Nov/13 11:36,12/Mar/19 14:05,13/Mar/19 22:29,09/Nov/13 00:08,1.2.12,2.0.3,,,,,0,,,,,"Mysteriously, tracing doesn't fail all the time. If I run the query multiple times at different consistency levels, tracing sometimes starts working.

{code}
cqlsh:some_keyspace> TRACING on;
cqlsh:some_keyspace> select * from appservers;

 key     | status
---------+--------
 server1 |      1
 server2 |      1
 server3 |      1

unsupported operand type(s) for /: 'NoneType' and 'float'
{code}",1.2.10,,,,,,,,,,,,,,,,,,,,,,,08/Nov/13 23:49;mishail;CASSANDRA-1.2-6317.patch;https://issues.apache.org/jira/secure/attachment/12612931/CASSANDRA-1.2-6317.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-08 13:20:08.827,,,no_permission,,,,,,,,,,,,357579,,,Sat Nov 09 00:08:57 UTC 2013,,,,,,0|i1pn5r:,357869,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,08/Nov/13 13:20;jbellis;Sounds like cqlsh isn't handling an elapsed time of null/None which is what get logged when a replica responds after the result has been sent to the client.,"08/Nov/13 13:34;iamaleksey;bq. Sounds like cqlsh isn't handling an elapsed time of null/None which is what get logged when a replica responds after the result has been sent to the client.

Yup.",08/Nov/13 23:49;mishail;Handle the case when {{duration == None}},"09/Nov/13 00:08;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig tests are failing,CASSANDRA-6376,12680000,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,alexliu68,slebresne,slebresne,19/Nov/13 17:32,12/Mar/19 14:05,13/Mar/19 22:29,14/Mar/14 19:55,1.2.16,2.0.7,2.1 beta2,,,,0,,,,,"On my box, all pig tests are failing with the following stack:
{noformat}
    [junit] Testcase: org.apache.cassandra.pig.CqlTableDataTypeTest:	Caused an ERROR
    [junit] null
    [junit] java.lang.ExceptionInInitializerError
    [junit] 	at org.apache.cassandra.pig.PigTestBase.startHadoopCluster(PigTestBase.java:104)
    [junit] 	at org.apache.cassandra.pig.CqlTableDataTypeTest.setup(CqlTableDataTypeTest.java:198)
    [junit] Caused by: java.lang.NullPointerException
    [junit] 	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:422)
    [junit] 	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:280)
    [junit] 	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:124)
    [junit] 	at org.apache.pig.test.MiniCluster.setupMiniDfsAndMrClusters(MiniCluster.java:50)
    [junit] 	at org.apache.pig.test.MiniGenericCluster.<init>(MiniGenericCluster.java:49)
    [junit] 	at org.apache.pig.test.MiniCluster.<init>(MiniCluster.java:31)
    [junit] 	at org.apache.pig.test.MiniGenericCluster.<clinit>(MiniGenericCluster.java:45)
{noformat}
On CASSANDRA-6375, Brandon reported that it was the case on his box too, so I don't think it's a local to my machine. Seems to be a relatively basic setup thing though, not an actual test failure.

I'll also note that we have a specific target for pig tests and that this target uses a longer timeout than the one for the test target. If that's because pig tests typically don't finish within the test timeout, then it would be nice to exclude them from the normal test target (and maybe include them in the long-test target).",,,,,,,,,,,,,,,,,,,,,,,,20/Nov/13 23:43;alexliu68;6376-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12615034/6376-1.2-branch.txt,14/Mar/14 17:38;alexliu68;6376-v2.txt;https://issues.apache.org/jira/secure/attachment/12634759/6376-v2.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-11-20 23:41:41.788,,,no_permission,,,,,,,,,,,,359357,,,Fri Mar 14 19:55:06 UTC 2014,,,,,,0|i1py7b:,359656,1.2.11,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"20/Nov/13 23:41;alexliu68;Add ""exclude"" attribute to testmacro, default exclude pig test.","21/Nov/13 08:21;slebresne;I'll note that while I'm +1 on this exclude patch, the pig test fails with that ExceptionInInitializerError above even if run alone through the pig-test target, which is the main problem I meant we should fix here.","21/Nov/13 15:21;brandon.williams;Hmm, pig-test passes for me just fine.","21/Nov/13 15:32;slebresne;bq. pig-test passes for me just fine.

Interesting. But you didn't saw the same stacktrace than in the description when running it with the main test target did you? If you did, that doesn't make a whole lot of sense because the pig-test target does nothing more than only running the pig tests with a different timeout.

I wonder if it's not some dependency that needs to be installed that I don't have but you two guys have because you've used pig and I never have. In any case, if it's just a weird local thing of my machine, then I suppose I don't really care, I'll just never run the pig-test. But can at least one more people check the pig-test target and say what he got?","21/Nov/13 15:50;brandon.williams;I think the root cause originally wasn't an NPE, if you look at the log output slightly above that, is a NCDF error in hadoop itself.  I think this is because Cassandra is built against a different hadoop version than hadoop.minicluster is, and that's why running the pig tests in isolation works.",13/Mar/14 19:40;jbellis;where does that leave us here?,"13/Mar/14 20:09;brandon.williams;Now they seem to execute fine, but have an error:

{noformat}
    [junit] Testcase: testCqlStorageCollectionColumnTable(org.apache.cassandra.pig.CqlTableTest):       FAILED
    [junit] expected:<3> but was:<2>
    [junit] junit.framework.AssertionFailedError: expected:<3> but was:<2>
    [junit]     at org.apache.cassandra.pig.CqlTableTest.testCqlStorageCollectionColumnTable(CqlTableTest.java:186)
{noformat}",14/Mar/14 17:38;alexliu68;Fix the failed test case. v2 is attached.,14/Mar/14 19:55;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make gossip tolerate slow Gossip tasks,CASSANDRA-6338,12678906,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,12/Nov/13 23:38,12/Mar/19 14:05,13/Mar/19 22:29,13/Nov/13 15:39,2.0.3,,,,,,0,gossip,,,,"Currently if a single gossip task bogs down the gossip Stage, Gossip will mark everyone down because it hasn't seen updates from them (since they are all queued behind the slow one).

This means that full GCs can cause gossip ""flapping"" as well as any actually problematic tasks such as recomputing pending ranges.",,,,,,,,,,,,,,,,,,,,,,,,12/Nov/13 23:42;jbellis;6338.txt;https://issues.apache.org/jira/secure/attachment/12613464/6338.txt,20/Nov/13 13:12;jbellis;gossip-slowdown.txt;https://issues.apache.org/jira/secure/attachment/12614884/gossip-slowdown.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-11-13 13:05:25.404,,,no_permission,,,,,,,,,,,,358272,,,Wed Nov 20 13:12:29 UTC 2013,,,,,,0|i1prfb:,358562,,,,,,,,jasobrown,jasobrown,,,,,,,,,,"12/Nov/13 23:42;jbellis;Attached.

Of course this doesn't help if we have too much for one thread to handle and it just gets farther and farther behind, but I don't think we've seen a failure scenario like that yet (at least post-CASSANDRA-6244).  And even then, at least this gives users something obvious in the log to alert them as to the cause of the problem instead of ""your cluster mysteriously marked everyone down and started serving up UAE.""","13/Nov/13 13:05;jasobrown;overall, lgtm. I'd be a bit worried about over-logging but that's probably better than the opposite (what we have now). ",13/Nov/13 13:57;cburroughs;Is the fix version intended to be 2.x only?,"13/Nov/13 14:02;jbellis;bq. I'd be a bit worried about over-logging

Me too, but like I said, I've only seen transitory spikes and not a persistent condition.  If overlogging exposes that we have a problem with the latter, then we can work on fixing that.

bq. Is the fix version intended to be 2.x only?

Yes, I'm pretty nervous about causing regressions here because something subtle depended on the existing behavior.","13/Nov/13 14:08;jasobrown;bq. If overlogging exposes that we have a problem with the latter ....

Totally agreed, and that's the reason for this ticket (to point problems) :)
",13/Nov/13 15:39;jbellis;committed,20/Nov/13 13:12;jbellis;Attached patch should make it easy to test this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
debian init searches for jdk6 explicitly,CASSANDRA-6396,12680783,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,22/Nov/13 17:41,12/Mar/19 14:05,13/Mar/19 22:29,22/Nov/13 18:05,1.2.13,2.0.4,,Packaging,,,0,,,,,"When JAVA_HOME isn't set, the init looks for jdk6 explicitly.  Obviously for 2.0+ this can cause problems.",,,,,,,,,,,,,,,,,,,,,,,,22/Nov/13 17:44;brandon.williams;6396.txt;https://issues.apache.org/jira/secure/attachment/12615361/6396.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-22 18:00:50.41,,,no_permission,,,,,,,,,,,,360048,,,Fri Nov 22 18:05:13 UTC 2013,,,,,,0|i1q2fz:,360347,,,,,,,,jbellis,jbellis,,,,,,,,,,"22/Nov/13 17:44;brandon.williams;I think the best thing to do is just search the default-jvm path.  If the user hasn't set up alternatives correctly, they can fix that.  If they want to use a specific non-default jvm, they can override JAVA_HOME in /etc/default/cassandra.",22/Nov/13 18:00;jbellis;+1,22/Nov/13 18:05;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"After executing abnormal cql statement, not working  (hang)",CASSANDRA-6341,12678966,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,knight76,knight76,13/Nov/13 08:44,12/Mar/19 14:05,13/Mar/19 22:29,14/Nov/13 17:49,1.2.12,2.0.3,,,,,0,,,,," I use a set type in table like sample, but if awkward cql statement failed, cqlsh is not worked like belows.


serkeyspace> CREATE TABLE images (
                ...     name text PRIMARY KEY,
                ...     owner text,
                ...     date timestamp,
                ...     tags set<text>
                ... );

cqlsh:userkeyspace> delete tags['cuddly'] from images where name = 'cat.jpg'; // not allowd cql statement
(hang)
^C 
cqlsh:userkeyspace> select * from plays;
(hang)
^C
cqlsh:userkeyspace> describe table plays;
(hang)
^C
cqlsh:userkeyspace> quit


---------------------------------

cassandra log when hang is occured.

ERROR 16:59:57,653 Exception in thread Thread[Thrift:8,5,main]
java.lang.AssertionError
	at org.apache.cassandra.cql3.Lists$Discarder.execute(Lists.java:414)
	at org.apache.cassandra.cql3.statements.DeleteStatement.updateForKey(DeleteStatement.java:82)
	at org.apache.cassandra.cql3.statements.ModificationStatement.getMutations(ModificationStatement.java:506)
	at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithoutCondition(ModificationStatement.java:377)
	at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:363)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:101)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:117)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:108)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1933)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4394)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4378)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)


",2.0.1,,,,,,,,,,,,,,,,,,,,,,,13/Nov/13 16:31;slebresne;6341.txt;https://issues.apache.org/jira/secure/attachment/12613636/6341.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-13 16:31:52.052,,,no_permission,,,,,,,,,,,,358332,,,Thu Nov 14 17:49:27 UTC 2013,,,,,,0|i1prsn:,358622,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"13/Nov/13 16:31;slebresne;That query is actually valid, but the fact that it triggers an AssertionError is obviously not and due to some bad typo. Attaching trivial patch to fix (I've pushed a dtests too).","13/Nov/13 16:33;slebresne;Actually, seems to affect 1.2 too, so the patch is against 2.0 but it probably apply to 1.2 as well anyway and I'll make sure to commit to 1.2 first in any case.",13/Nov/13 19:33;iamaleksey;+1,"14/Nov/13 17:49;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When dropping a CF, row cache is not invalidated",CASSANDRA-6351,12679404,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,frousseau,frousseau,frousseau,15/Nov/13 14:48,12/Mar/19 14:05,13/Mar/19 22:29,16/Nov/13 17:11,1.2.12,2.0.3,,,,,0,,,,,"When dropping a ColumnFamily with row cache enabled, then row cache is not invalidated for this CF.

This can be a bit annoying if the ColumnFamily is recreated because it will be empty, but row cache won't.
Note : this is similar to a ""TRUNCATE"" command (and TRUNCATE does invalidate the cache...)

Attached is patch which removes the rows of the currently dropped CF from row cache.",,,,,,,,,,,,,,,,,,,,,,,,15/Nov/13 14:49;frousseau;0001-invalidate-row-cache-when-dropping-CF.patch;https://issues.apache.org/jira/secure/attachment/12614067/0001-invalidate-row-cache-when-dropping-CF.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-16 17:11:20.669,,,no_permission,,,,,,,,,,,,358764,,,Sat Nov 16 17:11:20 UTC 2013,,,,,,0|i1pugn:,359054,1.2.10,,,,,,,jbellis,jbellis,,,,,,,,,,"16/Nov/13 17:11;jbellis;LGTM, committed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FD phi estimator initial conditions,CASSANDRA-6385,12680322,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,qconner,qconner,20/Nov/13 16:51,12/Mar/19 14:05,13/Mar/19 22:29,21/Nov/13 02:58,1.2.13,2.0.3,,,,,0,,,,,"phi estimates are calculated for newly discovered nodes from an un-filled (new, uninitialized) deque.

The inter-arrival time (elapsed time between gossip heartbeats) is stored in the o.a.c.gms.ArrivalWindow.arrivalIntervale deque for each received heartbeat, up to the maximum window size of 1000 samples.

In the o.a.c.gms.FailureDetector.interpret() method, phi is calculated for the node which uses a statistical measure called variance.  Like mean, variance on a population (a set of numbers or measurements) is not statistically relevant unless the population set size is 30 or greater. 

When a new node is discovered, the calculated variance is higher than normal, and causes phi to be higher than normal, resulting in a false positive failure detection.",,,,,,,,,,,,CASSANDRA-6127,,,,,,,,,,,,20/Nov/13 17:49;jbellis;6385-v2.txt;https://issues.apache.org/jira/secure/attachment/12614933/6385-v2.txt,20/Nov/13 18:59;jbellis;6385-v3.txt;https://issues.apache.org/jira/secure/attachment/12614949/6385-v3.txt,20/Nov/13 17:21;qconner;6385.txt;https://issues.apache.org/jira/secure/attachment/12614921/6385.txt,20/Nov/13 22:11;jbellis;6835-v4.txt;https://issues.apache.org/jira/secure/attachment/12615010/6835-v4.txt,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-11-20 17:49:21.132,,,no_permission,,,,,,,,,,,,359587,,,Thu Nov 21 02:58:20 UTC 2013,,,,,,0|i1pzmf:,359886,1.2.11,1.2.9,2.0.2,,,,,brandon.williams,brandon.williams,,,,,,,,,,20/Nov/13 16:52;qconner;breaking out FD phi initial condition to related issue,20/Nov/13 17:21;qconner;initial condition patch that paints a rosy picture until 30 rounds of gossip have completed and a statistically valid sample set has accumulated.,"20/Nov/13 17:49;jbellis;Thinking about it more, I'm not comfortable with saying that a dead node will *never* be detected if it dies before it hits the cutoff.  v2 changes it to sqrt(phi) until it hits 30.

We could probably do something more sophisticated that narrows the fudge factor as we approach our threshold of confidence.

(Both of these break ArrivalWindowTest, btw.)","20/Nov/13 18:59;jbellis;Thinking about it more, I think the main problem is using too low of an initial value to seed the Window.  Interval / 2 is always smaller then the actual mean will be, and it will be increasingly too small as the cluster size grows.

Picking a nice large value there gives us the ""large fudge to start that ""decays"" (by being averaged with real values) as we get more data"" behavior that we want.

v3 attached.","20/Nov/13 19:15;brandon.williams;I like v3's logic much better, however on startup you can trip the assert (infinitely, if you kill a node very quickly after a round a gossip):

{noformat}
ERROR 19:10:48,249 Exception in thread Thread[GossipTasks:1,5,main]
java.lang.AssertionError
        at org.apache.cassandra.gms.ArrivalWindow.phi(FailureDetector.java:331)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:220)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:612)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:57)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:163)
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:75)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
{noformat}","20/Nov/13 19:24;jbellis;What am I missing?  We got rid of clear, and I added code so we don't put the AW into the Map until it has at least one data point.","20/Nov/13 19:33;brandon.williams;bq. We got rid of clear

Well, not in v3 :)",20/Nov/13 19:46;jbellis;Time to pull :),"20/Nov/13 20:09;brandon.williams;Well crap, I pulled and still get it. :/","20/Nov/13 20:19;brandon.williams;I think the clue lies here:

{noformat}
DEBUG 20:16:42,475 Ignoring interval time of 30000.0
 INFO 20:16:42,475 Node /10.179.65.102 is now part of the cluster
DEBUG 20:16:42,482 removing expire time for endpoint : /10.179.65.102
 INFO 20:16:42,482 InetAddress /10.179.65.102 is now UP
ERROR 20:16:43,377 Exception in thread Thread[GossipTasks:1,5,main]
java.lang.AssertionError
        at org.apache.cassandra.gms.ArrivalWindow.phi(FailureDetector.java:319)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:213)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:612)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:57)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:163)
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:75)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
 INFO 20:16:43,377 Handshaking version with cassandra-1/10.179.65.102
{noformat}

Which I guess we need a backdoor method to inject, because I was going to go with 10s on CASSANDRA-4375.",20/Nov/13 22:11;jbellis;v4,20/Nov/13 22:54;brandon.williams;+1,21/Nov/13 02:58;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra 2.0.1 OutOfMemoryError:  Requested array size exceeds VM limit,CASSANDRA-6260,12676213,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mishail,prateek@bloomreach.com,prateek@bloomreach.com,28/Oct/13 20:57,12/Mar/19 14:05,13/Mar/19 22:29,31/Oct/13 13:43,2.0.3,,,,,,0,,,,,"I am running cassandra 2.0.1 server with cascading client https://github.com/ifesdjeen/cascading-cassandra/ (v 1.0.0-rc6). I am running a problem on restarting one of the nodes in the cassandra cluster. All other nodes in the cluster started properly without any issues. I had originally assigned 8G of RAM to heap space. I tried starting the node with 12G of RAM but it still fails with the following error. This is currently blocking a production release so appreciate your quick response.

[(bloomreach-ami) ubuntu@ip-10-179-26-169 :/mnt/cassandra_latest]# ERROR 20:55:58,738 Exception encountered during startup
java.lang.OutOfMemoryError: Requested array size exceeds VM limit
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:394)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:355)
        at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:352)
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:119)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:267)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:411)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:383)
        at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:314)
        at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:268)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:110)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:88)
        at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:474)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:226)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:442)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:485)
 INFO 20:55:58,739 Initializing system.schema_triggers
java.lang.OutOfMemoryError: Requested array size exceeds VM limit
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:394)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:355)
        at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:352)
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:119)

",Cassandra server 2.0.1,,,,,,,,,,,,,,,,,,,,,,,31/Oct/13 05:42;mishail;cassandra-2.0-6260-keyLength.patch;https://issues.apache.org/jira/secure/attachment/12611255/cassandra-2.0-6260-keyLength.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-29 15:51:18.712,,,no_permission,,,,,,,,,,,,355710,,,Thu Oct 31 13:43:21 UTC 2013,,,,,,0|i1pbn3:,355998,2.0.1,,,,,,,jbellis,jbellis,,,,,,,,,,"29/Oct/13 15:51;iflatness;I have gotten the same error with the nodes on my cluster.

The cluster is running 2.0.1, and the error started on 28.10.2013 for me as well.
My machines are each running with heaps of 512mb, and that was consistent with how they were before the restart. On initialization system memory drops extremely low right before the error output (after which java stops and the memory is freed). There are 4 machines in my cluster and the partitioner is Murmur3.

-On 2 of the nodes, a different error is thrown- Upgrading to 2.0.2 fixed the other error. The heap error is unchanged with the upgrade to 2.0.2

","29/Oct/13 20:48;jbellis;Since the problem is reading the key cache, deleting it is an easy workaround.",30/Oct/13 06:38;mishail;Patch to check if the length of array from DataInput doesn't exceed the limit,"30/Oct/13 15:26;jbellis;Seriously, people are corrupting their key cache files with a length between maxint and maxint-8?  Isn't that ridiculously unlikely?",30/Oct/13 19:16;mishail;[~jbellis] Should we validate that key size is under 64K? (http://wiki.apache.org/cassandra/FAQ#max_key_size) ,30/Oct/13 19:31;brandon.williams;Sounds reasonable to me.,31/Oct/13 05:42;mishail;Validate that {{keyLength <= FBUtilities.MAX_UNSIGNED_SHORT}},31/Oct/13 13:43;jbellis;committed; thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setting max_hint_window_in_ms explicitly to null causes problems with JMX view,CASSANDRA-6419,12681794,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,zznate,zznate,zznate,28/Nov/13 17:04,12/Mar/19 14:05,13/Mar/19 22:29,19/Dec/13 23:44,1.2.14,2.0.4,,Local/Config,,,0,,,,,"Setting max_hint_window_in_ms to null in cassandra.yaml makes the StorageProxy mbean inaccessable. 

Stack trace when trying to view the bean through MX4J:
{code}
Exception during http request
javax.management.RuntimeMBeanException: java.lang.NullPointerException
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrow(DefaultMBeanServerInterceptor.java:839)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrowMaybeMBeanException(DefaultMBeanServerInterceptor.java:852)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:651)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)
	at mx4j.tools.adaptor.http.MBeanCommandProcessor.createMBeanElement(MBeanCommandProcessor.java:119)
	at mx4j.tools.adaptor.http.MBeanCommandProcessor.executeRequest(MBeanCommandProcessor.java:56)
	at mx4j.tools.adaptor.http.HttpAdaptor$HttpClient.run(HttpAdaptor.java:980)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.config.DatabaseDescriptor.getMaxHintWindow(DatabaseDescriptor.java:1161)
	at org.apache.cassandra.service.StorageProxy.getMaxHintWindow(StorageProxy.java:1506)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647)
	... 4 more
Exception during http request
{code}",,,,,,,,,,,,,,,,,,,,,,,,28/Nov/13 17:13;zznate;6419-1.2.patch;https://issues.apache.org/jira/secure/attachment/12616288/6419-1.2.patch,28/Nov/13 17:13;zznate;6419-2.0.patch;https://issues.apache.org/jira/secure/attachment/12616289/6419-2.0.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-12-19 23:44:52.247,,,no_permission,,,,,,,,,,,,361058,,,Mon Dec 30 21:33:23 UTC 2013,,,,,,0|i1q8nb:,361357,1.2.12,2.0.2,,,,,,jbellis,jbellis,,,,,,,,,,28/Nov/13 17:05;zznate;Attaching a trivial patch to throw a config exception with a 'don't do that' message. ,28/Nov/13 17:12;zznate;Trivial patches for 1.2 and 2.0 (only line numbers differ),"19/Dec/13 23:44;jbellis;LGTM, committed","30/Dec/13 21:33;rcoli;FWIW, this looks like a single case of the systemic issue described in CASSANDRA-4967.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
auto_snapshots are not removable via 'nodetool clearsnapshot',CASSANDRA-6418,12681657,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lyubent,jre,jre,27/Nov/13 20:57,12/Mar/19 14:05,13/Mar/19 22:29,17/Mar/14 18:05,2.0.5,,,Tool/nodetool,,,0,,,,,"Snapshots of deleted CFs created via the ""auto_snapshot"" configuration parameter appear to not be tracked.  The result is that 'nodetool clearsnapshot <keyspace with deleted CFs>' does nothing, and short of manually removing the files from the filesystem, deleted CFs remain indefinitely taking up space.

I'm not sure if this is intended, but it seems pretty counter-intuitive.  I haven't found any documentation that indicates ""auto_snapshots"" would be ignored by 'nodetool clearsnapshot'.",auto_snapshot: true,,,,,,,,,,,,,,,,,,,,,,,29/Nov/13 13:12;lyubent;6418_cassandra-2.0.patch;https://issues.apache.org/jira/secure/attachment/12616383/6418_cassandra-2.0.patch,21/Dec/13 01:37;lyubent;6418_cassandra-2.0_v5.patch;https://issues.apache.org/jira/secure/attachment/12619946/6418_cassandra-2.0_v5.patch,05/Dec/13 12:18;lyubent;6418_v2.patch;https://issues.apache.org/jira/secure/attachment/12617156/6418_v2.patch,12/Dec/13 01:38;lyubent;6418_v3_cassandra-2.0.patch;https://issues.apache.org/jira/secure/attachment/12618337/6418_v3_cassandra-2.0.patch,21/Dec/13 00:05;mishail;CASSANDRA-2.0-6418_v4.patch;https://issues.apache.org/jira/secure/attachment/12619937/CASSANDRA-2.0-6418_v4.patch,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2013-11-29 13:11:24.42,,,no_permission,,,,,,,,,,,,360921,,,Mon Mar 17 18:05:06 UTC 2014,,,,,,0|i1q7t3:,361220,2.0.2,,,,,,,mishail,mishail,,,,,,,,,,"29/Nov/13 13:11;lyubent;Because cassandra uses CFStores to clear the snapshots once the CF is dropped, it goes out of scope. Changed the responsibility of clearing the snapshot from CFStore to o.a.c.db.Keyspace where the KS will use DatabaseDescriptor#getAllDataFileLocations to find the data directories then append the KS name and find all the CF dirs inside that KS directory instead of using the sstables' directories. This can be extended to be more granular (per CF) where instead of using all the dirs inside the KS directory, the user can provide a list of CFs to be removed. ",29/Nov/13 19:23;jbellis;Can you review [~mishail]?,"05/Dec/13 05:16;mishail;{code}    [javac] C:\Users\mishail\workspace\cassandra\src\java\org\apache\cassandra\db\compaction\CompactionManager.java:810: error: cannot find symbol
    [javac]                 cfs.clearSnapshot(snapshotName);
    [javac]                    ^
    [javac]   symbol:   method clearSnapshot(String)
    [javac]   location: variable cfs of type ColumnFamilyStore
{code}

For *getAllKSDirectories*
* {{List<File> snapshotDirs = new ArrayList();}} .It's a raw data type.  You probably meant either {{newArrayList()}} or {{new ArrayList<>()}}
* {{new File(dataDirectory + ""/""  + ksName)}} and {{File(dataDirectory + ""/""  + ksName + ""/"" + cfDir)}} - you should use {{org.apache.cassandra.db.Directories.join(String...)}} instead that concatenation.
* I think it would be better to use one of {{File.listFiles}} methods instead of {{File.list()}} ","05/Dec/13 12:18;lyubent;v2 adds a CFS#clearSnapshot function, and addresses all the nits.","06/Dec/13 06:45;mishail;* {{getCFDirectory}} assumes that CF resides in a single location. If I'm correct, a CF can be in multiple dirs from {{DatabaseDescriptor.getAllDataFileLocations()}}
* I suspect {{File.listFiles()}} can return NULL if a directory doesn't exist

Minor remarks
* {{cfDir.exists() && cfDir.isDirectory()}} can be reduced to just {{cfDir.isDirectory()}}
* You can use {{dataFileLocations}} instead of {{DatabaseDescriptor.getAllDataFileLocations()}}",12/Dec/13 01:38;lyubent;Changed getCFDirectory to return a list of directories and added a check to ensure that CFs exist in v3.,"21/Dec/13 00:05;mishail;Looking at that duplicated code in {{getKSDirectories}} and {{getCFDirectory}}, and in {{getCFDirectory}} and {{Directories}}'s constructor, it occured to me that we don't need to call {{getCFDirectory}} in a static context. The only place it's used is {{doValidationCompaction}} where we always have a reference to CFS.

I'm attaching V4 of the patch. [~lyubent] what do you think?","21/Dec/13 01:37;lyubent;LGTM, attaching v5 which adds a space to the for statement in {{Directories#getKSChildDirectories}}... ocd ftw.",21/Dec/13 02:25;mishail;+1,21/Dec/13 06:36;jbellis;committed,14/Mar/14 17:41;nickmbailey;This doesn't handle the entire keyspace no longer existing. Due to calling 'getValidKeyspace' in storage service.,"17/Mar/14 18:00;nickmbailey;Re-opening, let me know if I should create a separate ticket.",17/Mar/14 18:05;mishail;Let's use CASSANDRA-6821 fro that,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Counter writes shouldn't be resubmitted after timeouts,CASSANDRA-6427,12682005,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,iamaleksey,iamaleksey,01/Dec/13 15:44,12/Mar/19 14:05,13/Mar/19 22:29,06/Dec/13 15:29,1.2.14,2.0.4,,,,,0,,,,,"CASSANDRA-4753 made SP.counterWriteTask() return a LocalMutationRunnable instead of the usual DroppableRunnalbe, and LMR resubmits the original runnable in case of timing out instead of simply dropping it.

For counters this is not the right option since it would lead to overcounting if the mutation got dropped-then-resubmitted and then retried by the user.",,,,,,,,,,,,,,,,,,,,,,,,01/Dec/13 15:47;iamaleksey;6427.txt;https://issues.apache.org/jira/secure/attachment/12616476/6427.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-02 00:17:59.491,,,no_permission,,,,,,,,,,,,361264,,,Fri Dec 06 15:29:03 UTC 2013,,,,,,0|i1q9x3:,361563,,,,,,,,jbellis,jbellis,,,1.2.1,,,,,,,"02/Dec/13 00:17;jbellis;But if it's dropped-then-resubmitted then the user would only retry if the resubmit times out, right?  So we're better off than we would be in at least some situations (dropped, resubmitted, not retried) and no worse off in others (succeeded after timeout, but retried).","02/Dec/13 21:17;iamaleksey;But the coordinator (and thus the user) has no way of telling if the timeout was caused, by a leader node death or if the runnable had been resubmitted and successfully applied eventually.

It's subtle, and it's about control.

Consider the two possible timeout handling strategies:
1. Never lose a counter write - prefer overcounting to undercounting. In this scenario one would always retry, and with the current behavior there will be a lot more overcounting than there would've been had Cassandra not resubmitted them under the cover.

2. Never repeat a counter write - prefer undercounting to overcounting. One would never retry, and with the current behavior would actually get more accurate results than with pre-1.2.1 C* (no resubmitting).

So on balance LocalMutationRunnable and DroppableRunnable do yield the same score, but I expect 1) to be more common, and LMR here makes things worse (and it's probably not something people are aware of and expect).

So it's not a big deal, and I'm fine with not-a-problem'ing it, but I do believe that going back to DroppableRunnable and not resubmitting is less surprising to users and more preferable in most cases.","02/Dec/13 21:21;iamaleksey;Also, timeout due to node death is probably less common than timing out b/c of overload, and that makes always-retrying less accurate in general.",06/Dec/13 03:27;jbellis;+1,"06/Dec/13 15:29;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Saved KeyCache prints success to log; but no file present,CASSANDRA-6413,12681604,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mishail,cburroughs,cburroughs,27/Nov/13 15:53,12/Mar/19 14:05,13/Mar/19 22:29,05/Dec/13 15:44,1.2.13,2.0.4,,,,,0,,,,,"Cluster has a single keyspace with 3 CFs.  All used to have ROWS_ONLY, two were switched to KEYS_ONLY about 2 days ago.  Row cache continues to save fine, but there is no saved key cache file present on any node in the cluster.

{noformat}
6925: INFO [CompactionExecutor:12] 2013-11-27 10:12:02,284 AutoSavingCache.java (line 289) Saved RowCache (50000 items) in 118 ms
6941:DEBUG [CompactionExecutor:14] 2013-11-27 10:17:02,163 AutoSavingCache.java (line 233) Deleting old RowCache files.
6942: INFO [CompactionExecutor:14] 2013-11-27 10:17:02,310 AutoSavingCache.java (line 289) Saved RowCache (50000 items) in 146 ms
8745:DEBUG [CompactionExecutor:6] 2013-11-27 10:37:25,140 AutoSavingCache.java (line 233) Deleting old RowCache files.
8746: INFO [CompactionExecutor:6] 2013-11-27 10:37:25,283 AutoSavingCache.java (line 289) Saved RowCache (50000 items) in 143 ms
8747:DEBUG [CompactionExecutor:6] 2013-11-27 10:37:25,283 AutoSavingCache.java (line 233) Deleting old KeyCache files.
8748: INFO [CompactionExecutor:6] 2013-11-27 10:37:25,625 AutoSavingCache.java (line 289) Saved KeyCache (21181 items) in 342 ms
8749:DEBUG [CompactionExecutor:6] 2013-11-27 10:37:25,625 AutoSavingCache.java (line 233) Deleting old RowCache files.
8750: INFO [CompactionExecutor:6] 2013-11-27 10:37:25,759 AutoSavingCache.java (line 289) Saved RowCache (50000 items) in 134 ms
8751:DEBUG [CompactionExecutor:6] 2013-11-27 10:37:25,759 AutoSavingCache.java (line 233) Deleting old RowCache files.
8752: INFO [CompactionExecutor:6] 2013-11-27 10:37:25,893 AutoSavingCache.java (line 289) Saved RowCache (50000 items) in 133 ms
8753:DEBUG [CompactionExecutor:6] 2013-11-27 10:37:25,893 AutoSavingCache.java (line 233) Deleting old RowCache files.
8754: INFO [CompactionExecutor:6] 2013-11-27 10:37:26,026 AutoSavingCache.java (line 289) Saved RowCache (50000 items) in 133 ms
9915:DEBUG [CompactionExecutor:18] 2013-11-27 10:42:01,851 AutoSavingCache.java (line 233) Deleting old KeyCache files.
9916: INFO [CompactionExecutor:18] 2013-11-27 10:42:02,185 AutoSavingCache.java (line 289) Saved KeyCache (22067 items) in 334 ms
9917:DEBUG [CompactionExecutor:17] 2013-11-27 10:42:02,279 AutoSavingCache.java (line 233) Deleting old RowCache files.
9918: INFO [CompactionExecutor:17] 2013-11-27 10:42:02,411 AutoSavingCache.java (line 289) Saved RowCache (50000 items) in 131 ms
{noformat}

{noformat}
$ ll ~/shared/saved_caches/
total 3472
-rw-rw-r-- 1 cassandra cassandra 3551608 Nov 27 10:42 Foo-Bar-RowCache-b.db

{noformat}",1.2.11,,,,,,,,,,,,,,,,,,,,,,,04/Dec/13 23:55;jbellis;6413-v2.txt;https://issues.apache.org/jira/secure/attachment/12617081/6413-v2.txt,04/Dec/13 06:42;mishail;CASSANDRA-1.2-6413.patch;https://issues.apache.org/jira/secure/attachment/12616946/CASSANDRA-1.2-6413.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-11-27 21:05:48.32,,,no_permission,,,,,,,,,,,,360868,,,Thu Dec 12 12:08:58 UTC 2013,,,,,,0|i1q7hj:,361167,1.2.12,,,,,,,jbellis,jbellis,,,,,,,,,,27/Nov/13 21:05;mishail;[~cburroughs] what's your {{key_cache_size_in_mb}} in _cassandra.yaml_ ?,"27/Nov/13 21:10;cburroughs;{noformat}
key_cache_size_in_mb: 48
key_cache_save_period: 900

# Number of keys from the key cache to save
# Disabled by default, meaning all keys are going to be saved
# key_cache_keys_to_save: 100
{noformat}",28/Nov/13 07:15;mishail;That's weird. Does the directory contain only that single file? Are there any suspicious messages in the logs?,"02/Dec/13 16:33;cburroughs; * Yes, on every node in the cluster `saved_caches_directory` contains only a single file.
 * I don't see anything else relevant in the logs.  The log clip in the ticket is with AutoSavingCache set to DEBUG logging.","04/Dec/13 06:01;mishail;Silly me :) The bug is so obvious, but only manifests itself if both Key cache AND row cache are enabled

{code:title=org.apache.cassandra.cache.AutoSavingCache.java|borderStyle=solid}
        private void deleteOldCacheFiles()
        {
            File savedCachesDir = new File(DatabaseDescriptor.getSavedCachesLocation());

            if (savedCachesDir.exists() && savedCachesDir.isDirectory())
            {
                for (File file : savedCachesDir.listFiles())
                {
                    if (file.isFile() && file.getName().endsWith(cacheType.toString()))
                    {
                        if (!file.delete())
                            logger.warn(""Failed to delete {}"", file.getAbsolutePath());
                    }

                    if (file.isFile() && file.getName().endsWith(CURRENT_VERSION + "".db""))
                    {
                        if (!file.delete())
                            logger.warn(""Failed to delete {}"", file.getAbsolutePath());
                    }
                }
            }
        }
{code}

So, each cache deletes FILES FROM ALL CACHES from the {{saved_caches}} and then happily writes its files.
The last save wins, ",04/Dec/13 06:42;mishail;Patch: each cache should delete only its own files,04/Dec/13 06:53;mishail;Looks like it was introduced in https://github.com/apache/cassandra/commit/cfe585c2c420c6e8445eb4c3309b09db8cf134ac for CASSANDRA-3762,"04/Dec/13 22:31;jbellis;Hmm...  Maybe the 3762 line was supposed to be

{code}
if (file.isFile() && !file.getName().endsWith(CURRENT_VERSION + "".db""))
{code}

to clean out obsolete cache files?","04/Dec/13 23:29;mishail;[~jbellis]
In that case the obsolete files  would be never deleted. Their filenames end with ""<cacheType>-CURRENT_VERSION.db"", so they would fall through those two conditions.",04/Dec/13 23:55;jbellis;v2 attached to clean out both old- and new- format files of the correct type.,"05/Dec/13 00:03;mishail;[~jbellis]
Do you think there will be problems with the simple {{contains(cacheType.toString())}} approach?",05/Dec/13 05:43;jbellis;Wouldn't that match CF or KS names or even other parts of the path?,"05/Dec/13 06:03;mishail;That will match for sure. But what are chances?
Nevertheless, I'm ok with your patch.","05/Dec/13 14:27;cburroughs;Jira's priority guide thingy for minor says ""easy workaround is present"".  Is there a way to get both caches to persist before upgrading with the fix?","05/Dec/13 15:41;jbellis;No.  ""Minor"" also means ""doesn't seriously affect cluster stability or ability to respond to client requests.""",05/Dec/13 15:44;jbellis;committed v2,"12/Dec/13 12:08;cburroughs;As a followup note, this bug appears to have prevented any of the system-* KeyCaches from being saved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool cfstats doesn't handle index CFs,CASSANDRA-6406,12681334,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mishail,mishail,mishail,26/Nov/13 19:12,12/Mar/19 14:05,13/Mar/19 22:29,26/Nov/13 21:42,,,,,,,0,,,,,"After CASSANDRA-5871 values cfstats are read from the metrics, the problem is that metrics for Index column families have different JMX type ( {{type=IndexColumnFamily}} vs {{type=ColumnFamily}} for regular ones)

{code}
$ bin/nodetool.bat cfstats stress
Starting NodeTool
Keyspace: stress
Exception in thread ""main"" java.lang.reflect.UndeclaredThrowableException
        at com.sun.proxy.$Proxy16.getCount(Unknown Source)
        at org.apache.cassandra.tools.NodeCmd.printColumnFamilyStats(NodeCmd.java:829)
        at org.apache.cassandra.tools.NodeCmd.main(NodeCmd.java:1123)
Caused by: javax.management.InstanceNotFoundException: org.apache.cassandra.metrics:type=ColumnFamily,keyspace=stress,scope=t1.t1_num_idx,name=WriteLatency
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:643)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1464)
        at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:657)
        at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at sun.rmi.transport.Transport$1.run(Transport.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:556)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:811)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:670)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
        at sun.rmi.transport.StreamRemoteCall.exceptionReceivedFromServer(StreamRemoteCall.java:275)
        at sun.rmi.transport.StreamRemoteCall.executeCall(StreamRemoteCall.java:252)
        at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:161)
        at com.sun.jmx.remote.internal.PRef.invoke(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl_Stub.getAttribute(Unknown Source)
        at javax.management.remote.rmi.RMIConnector$RemoteMBeanServerConnection.getAttribute(RMIConnector.java:902)
        at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:267)
        ... 3 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,26/Nov/13 19:16;mishail;Oracle_Java_Mission_Control_2013-11-26_11-15-02.png;https://issues.apache.org/jira/secure/attachment/12615886/Oracle_Java_Mission_Control_2013-11-26_11-15-02.png,26/Nov/13 19:47;mishail;trunk-6406.patch;https://issues.apache.org/jira/secure/attachment/12615895/trunk-6406.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-11-26 21:42:06.623,,,no_permission,,,,,,,,,,,,360599,,,Tue Nov 26 21:42:06 UTC 2013,,,,,,0|i1q5tr:,360898,2.1 rc3,,,,,,,yukim,yukim,,,2.1 rc3,,,,,,,26/Nov/13 19:47;mishail;Patch to use a different JMX type for index CFs,"26/Nov/13 21:42;yukim;Thanks for catching this!
Committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DroppableTombstoneRatio JMX value is 0.0 for all CFs,CASSANDRA-6522,12686076,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,krummas,dkador,dkador,22/Dec/13 19:59,12/Mar/19 14:05,13/Mar/19 22:29,06/Feb/14 07:59,2.0.6,,,,,,0,,,,,"We're seeing that the JMX value for DroppableTombstoneRatio for all our CFs is 0.0. On the face of it that seems wrong since we've definitely issued a ton of deletes for row keys to expire some old data that we no longer need (and it definitely hasn't been reclaimed from disk yet). Am I misunderstanding what this means / how to use it? We're on 1.2.8 and using leveled compaction for all our CFs.

gc_grace_seconds is set to 1 day and we've issued a series of deletes over a day ago, so gc_grace has elapsed.

Cluster is 18 nodes.  Two DCs, so 9 nodes in each DC.  Each node has capacity for 1.5TB or so and is sitting with about 1TB under management.  That's why we wanted to do deletes, obviously.  Most of that 1TB is a single CF (called ""events"") which represents intermediate state for us that we can delete.

Happy to provide any more info, just let me know.","Ubuntu 12.04 LTS, Cassandra 1.2.8",,,,,,,,,,,,,,,,,,,,,,,08/Jan/14 08:24;krummas;0001-account-for-range-and-row-tombstones-in-tombstone-dr.patch;https://issues.apache.org/jira/secure/attachment/12621942/0001-account-for-range-and-row-tombstones-in-tombstone-dr.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-26 23:11:27.505,,,no_permission,,,,,,,,,,,,365033,,,Thu Feb 06 13:03:43 UTC 2014,,,,,,0|i1qx4n:,365341,1.2.8,,,,,,,jbellis,jbellis,,,,,,,,,,26/Dec/13 23:11;jbellis;/cc [~yukim],"02/Jan/14 10:42;krummas;This metric only counts deleted columns, I guess you have been doing entire row deletes?

[~jbellis] [~yukim] Should we try to estimate how many columns are affected by a row/range tombstone and include in this metric? ",02/Jan/14 15:20;jbellis;Yes.,"02/Jan/14 17:18;dkador;Marcus Eriksson: Yeah, we were doing row deletes, not column deletes.

Thanks for getting this scheduled.","08/Jan/14 08:24;krummas;Patch that accounts for row and range tombstones in the histogram. Also prints the histogram in tools/bin/sstablemetadata.

Note that it is very hard to cheaply know how many live columns a range tombstone shadows, so this metric is only the tombstone count / number of columns, which in this case would be pretty pointless.

CQL deletes were not accounted for before this, so it should also help for tombstone-only compaction in those cases.",31/Jan/14 15:47;jbellis;+1,"06/Feb/14 07:59;krummas;Committed, thanks","06/Feb/14 12:38;cburroughs;This a ""jmx value reporting"" bug or a ""sstables not getting compacted"" bug?","06/Feb/14 13:03;krummas;Both

It is the histogram used for deciding if we should do a tombstone-only compaction that is exposed over JMX",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Unable to contact any seeds!"" with multi-DC cluster and listen != broadcast address",CASSANDRA-6523,12686197,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,cburroughs,cburroughs,23/Dec/13 19:48,12/Mar/19 14:05,13/Mar/19 22:29,09/Jun/14 19:29,2.0.9,2.1 rc2,,,,,0,,,,,"New cluster:
 * Seeds: list of 6 internal IPs
 * listen address: internal ip
 * broadcast: external ip

Two DC cluster, using GPFS where the external IPs are NATed.  Clusters fails to start with ""Unable to contact any seeds!""

 * Fail: Try to start a seed node
 * Fail: Try to start two seed nodes at the same time in the same DC
 * Success: Start two seed nodes at the same time in different DCs.

Presumably related to CASSANDRA-5768",1.2.13ish,,,,,,,,,,,,,,,,,,,,,,,28/May/14 15:54;brandon.williams;6523.txt;https://issues.apache.org/jira/secure/attachment/12647154/6523.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-13 21:43:36.932,,,no_permission,,,,,,,,,,,,365170,,,Mon Jun 09 19:29:19 UTC 2014,,,,,,0|i1qxyf:,365475,,,,,,,,jasobrown,jasobrown,,,,,,,,,,13/Mar/14 21:43;jbellis;Is this still a problem with 5768 fixed?,31/Mar/14 15:37;cburroughs;I believe it's a regression caused by CASSANDRA-5768,"22/May/14 14:43;cburroughs;Still present in 2.0.x series.  There are a variety of stack overflow and mailing list threads threads with ""Unable to contact any seeds"".

I think the problem is that CASSANDRA-5768 isn't checking ""have I contacted a seed"" but ""am I connected to one of these IP addresses"".  That ends up being  requirement that the seeds/broadcast/listen address line up in a particular way.","22/May/14 21:44;brandon.williams;It's comparing the actual IP address it received contact from to the seed list, so if it's being contacted by the broadcast address, but the seed is listed with the internal address, there won't be a match.  So, you'd probably want a different seed list for DC 1 and DC 2, with the listen/broadcast switched for the seeds that aren't local.","23/May/14 13:14;cburroughs;I don't think that's an unreasonable requirement and I can roll with it but if there is an easy way the check could be made contact based instead of string equality based that would be nice.  Starting a cluster is often the first thing someone will do with Cassandra and NATs, firewalls hijacking DNS requests, reconnecting snitches all make this complicated (Top results on stack overflow disagree for example [1] [2]).

Barring a more clever check I suggest changing the message to something like ""Seed check failed!  At least one boardcast_address must match IPs from seed list"".  Seeing ""Handshaking with foo"" from the seed list and then ""Unable to contact any seeds!"" is a confusing experience.

[1] http://stackoverflow.com/questions/21261098/cassandra-unable-to-contact-seeds-if-using-aws-elastic-ip-address-only-works
[2] http://stackoverflow.com/questions/20690987/apache-cassandra-unable-to-gossip-with-any-seeds","23/May/14 15:36;brandon.williams;Part of the problem, is we can't know a seed's broadcast_address without, of course, them gossiping it to us.  We could possibly extend the 'talked to a seed' check to loop through the endpoints we know about and look for a matching broadcast_address, though that's a bit expensive and is kind of cheating since we want to know we talked to a seed directly, not that we were told about a seed, but I don't think that actually matters.  I'll have to think about it some.

bq. Barring a more clever check I suggest changing the message to something like ""Seed check failed! At least one boardcast_address must match IPs from seed list"".

Well, that's not entirely accurate either, since if you aren't using BCA at all, it's confusing.  We could print out the seeds with the error though, so at least between that and the handshaking messages you'd get a clue.","28/May/14 15:53;brandon.williams;Patch to iterate through the endpoint map and compare both the endpoint address and INTERNAL_IP (if present) to the seeds list instead of tracking it on a per-contact basis.  Possibly expensive in a large cluster, but not called from the gossiper itself so that should be fine.

I don't actually have a NAT setup and don't really want to make one (give myself the disease to test the cure), so if you could test this out Chris that'd help.","30/May/14 22:27;jasobrown;[~cburroughs] Not sure how usable this suggestion is, but you could use the external addresses in the seeds list instead of the internal (or mixed per-dc). As GPFS is a 'reconnecting snitch', once the connection of the IP addr is established, the nodes in the same DC will switch over to the internal IP (listen_address). The length of time from node startup to switching the connection to internal IP should be rather short, so your traffic shouldn't be 'unusual' for much time. However, what happens during that 'unusual' time may be a deal breaker.

I bring this up as that's what we did when running multi-region in ec2, and is what EC2MRS facilitates. ","31/May/14 14:04;garo5;I have a similar issue described in CASSANDRA-7292 and in there I use the public ip addresses in the seeds list, but it does not help in my case. These two issues might or might not be connected.","06/Jun/14 20:10;cburroughs;Jason, thanks for the suggestion.  I think there are a very different ways I could work around it.  At this point though I'd like to see it cleared up so others don't trip over it too.

Tested driftx's patch on top of 2.0.8 with two DCs and the same NAT setup.

 * Started a node in DC A with auto_bootrap:false and it came up happy.
 * Started a second node in DC A, and it successfully bootstrapped.
 * Stopped all DC A nodes, started a node in DC B with auto_bootrap:true.  It failed with ""Unable to contact any seeds!'
 * Started all DC A nodes, nodes in B could bootstrap.

So it looks like it works from my point of view.",09/Jun/14 18:58;jasobrown;+1 to @driftx's patch,09/Jun/14 19:29;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstableloader shows no progress or errors when pointed at a bad directory,CASSANDRA-6529,12686445,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,krummas,thobbs,thobbs,26/Dec/13 18:34,12/Mar/19 14:05,13/Mar/19 22:29,07/Jan/14 05:52,2.0.5,,,Legacy/Tools,,,0,,,,,"With sstableloader, the source directory is supposed to be in the format {{<keyspace_name>/<table_name>/}}.  If you incorrectly just put the sstables in a {{<keyspace_name>/}} directory, the sstableloader process will not show any progress, errors, or other output, it will simply hang.

This was initially reported on the user ML here: http://www.mail-archive.com/user@cassandra.apache.org/msg33916.html",,,,,,,,,,,,,,,,,,,,,,,,03/Jan/14 08:02;krummas;0001-verify-that-the-keyspace-exists-in-describeRing-and-.patch;https://issues.apache.org/jira/secure/attachment/12621275/0001-verify-that-the-keyspace-exists-in-describeRing-and-.patch,02/Jan/14 12:14;krummas;0001-verify-that-the-keyspace-exists-in-describeRing.patch;https://issues.apache.org/jira/secure/attachment/12621061/0001-verify-that-the-keyspace-exists-in-describeRing.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-01-02 12:14:22.337,,,no_permission,,,,,,,,,,,,365435,,,Tue Jan 07 05:52:15 UTC 2014,,,,,,0|i1qzkn:,365737,2.0.3,,,,,,,thobbs,thobbs,,,,,,,,,,"26/Dec/13 18:42;thobbs;Server side, you'll see something like this in the logs:

{noformat}
ERROR 12:28:28,053 Exception in thread Thread[Thrift:1,5,main]
java.lang.AssertionError: Unknown keyspace foo
	at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:262)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:110)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:88)
	at org.apache.cassandra.service.StorageService.describeRing(StorageService.java:1181)
	at org.apache.cassandra.service.StorageService.describeRing(StorageService.java:1168)
	at org.apache.cassandra.thrift.CassandraServer.describe_ring(CassandraServer.java:1389)
	at org.apache.cassandra.thrift.Cassandra$Processor$describe_ring.getResult(Cassandra.java:4094)
	at org.apache.cassandra.thrift.Cassandra$Processor$describe_ring.getResult(Cassandra.java:4078)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
{noformat}",02/Jan/14 12:14;krummas;verify that the given keyspace exists in describeRing,"02/Jan/14 17:05;thobbs;It would be nice to have an error message instead of a stack trace:

{noformat}
~/cassandra $ bin/sstableloader /foo/bar -d 127.0.0.1
Exception in thread ""main"" java.lang.RuntimeException: Could not retrieve endpoint ranges: 
	at org.apache.cassandra.tools.BulkLoader$ExternalClient.init(BulkLoader.java:239)
	at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:149)
	at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:79)
Caused by: InvalidRequestException(why:No such keyspace: foo)
	at org.apache.cassandra.thrift.Cassandra$describe_ring_result$describe_ring_resultStandardScheme.read(Cassandra.java:34055)
	at org.apache.cassandra.thrift.Cassandra$describe_ring_result$describe_ring_resultStandardScheme.read(Cassandra.java:34022)
	at org.apache.cassandra.thrift.Cassandra$describe_ring_result.read(Cassandra.java:33964)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_describe_ring(Cassandra.java:1251)
	at org.apache.cassandra.thrift.Cassandra$Client.describe_ring(Cassandra.java:1238)
	at org.apache.cassandra.tools.BulkLoader$ExternalClient.init(BulkLoader.java:215)
{noformat}

Can we wrap the describe_ring() call in a try/catch for InvalidRequestException and just print the ""why"" attribute and a brief message that suggests checking the usage?","03/Jan/14 08:02;krummas;adds a nicer message

we exited with a stack trace for certain errors before (like giving a host without c* running there), the errors wont be as scary now","03/Jan/14 21:40;thobbs;+1, thanks very much for the change.",07/Jan/14 05:52;krummas;committed!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prepared Statement on Defunct CF Can Impact Cluster Availability,CASSANDRA-6535,12686876,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,aholmber,aholmber,aholmber,31/Dec/13 21:18,12/Mar/19 14:05,13/Mar/19 22:29,02/Jan/14 13:18,1.2.14,,,,,,0,,,,,"*Synopsis:* misbehaving clients can cause DoS on a cluster with a defunct prepared statement

*Scenario:* 
1.) Create prepared INSERT statement on existing table X
2.) Table X is dropped
3.) Continue using prepared statement from (1)

*Result:* 
a.) on coordinator node: COMMIT-LOG-WRITER + MutationStage errors
b.) on other nodes: ""UnknownColumnFamilyException reading from socket; closing""  --> leads to thrashing inter-node connections
c.) Other clients of the cluster suffer from I/O timeouts, presumably a result of (b)

*Other observations:*
* On single-node clusters, clients return from insert without error because mutation errors are swallowed.
* On multiple-node clusters, clients receive a confounded 'read timeout' error because the closed internode connections do not propagate the error back.
* With prepared SELECT statements (as opposed to INSERT described above). A NullPointerException is caused on the server, and no meaninful error is returned to the client.

Besides the obvious ""don't do that"" to the integrator, it would be good if the cluster could handle this error case more gracefully and avoid undue impact.","Cassandra 1.2.12
CentOS 6.4",,,,,,,,,,,,,,,,,,,,,,,31/Dec/13 21:25;aholmber;6535.txt;https://issues.apache.org/jira/secure/attachment/12620984/6535.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-02 13:18:10.374,,,no_permission,,,,,,,,,,,,365871,,,Thu Jan 02 13:18:10 UTC 2014,,,,,,0|i1r2en:,366177,1.2.12,,,,,,,slebresne,slebresne,,,,,,,,,,"31/Dec/13 21:25;aholmber;6535.txt - a simple patch that adds CF validation to ClientState.hasColumnFamilyAccess. This buttons up the error pathology I was observing, preventing cluster impact and returning meaningful errors to the client.","02/Jan/14 13:18;slebresne;+1, committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DES scores fluctuate too much for cache pinning,CASSANDRA-6465,12683652,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thobbs,cburroughs,cburroughs,09/Dec/13 18:23,12/Mar/19 14:05,13/Mar/19 22:29,14/Jan/14 21:18,2.0.5,,,,,,0,gossip,,,,"To quote the conf:

{noformat}
# if set greater than zero and read_repair_chance is < 1.0, this will allow
# 'pinning' of replicas to hosts in order to increase cache capacity.
# The badness threshold will control how much worse the pinned host has to be
# before the dynamic snitch will prefer other replicas over it.  This is
# expressed as a double which represents a percentage.  Thus, a value of
# 0.2 means Cassandra would continue to prefer the static snitch values
# until the pinned host was 20% worse than the fastest.
dynamic_snitch_badness_threshold: 0.1
{noformat}

An assumption of this feature is that scores will vary by less than dynamic_snitch_badness_threshold during normal operations.  Attached is the result of polling a node for the scores of 6 different endpoints at 1 Hz for 15 minutes.  The endpoints to sample were chosen with `nodetool getendpoints` for row that is known to get reads.  The node was acting as a coordinator for a few hundred req/second, so it should have sufficient data to work with.  Other traces on a second cluster have produced similar results.
 * The scores vary by far more than I would expect, as show by the difficulty of seeing anything useful in that graph.
 * The difference between the best and next-best score is usually > 10% (default dynamic_snitch_badness_threshold).

Neither ClientRequest nor ColumFamily metrics showed wild changes during the data gathering period.

Attachments:
 * jython script cobbled together to gather the data (based on work on the mailing list from Maki Watanabe a while back)
 * csv of DES scores for 6 endpoints, polled about once a second
 * Attempt at making a graph

","1.2.11, 2 DC cluster",,,,,,,,,,,,,,,,,,,,,,,14/Jan/14 18:10;thobbs;6465-v1.patch;https://issues.apache.org/jira/secure/attachment/12622904/6465-v1.patch,14/Jan/14 17:59;thobbs;99th_latency.png;https://issues.apache.org/jira/secure/attachment/12622899/99th_latency.png,09/Dec/13 18:24;cburroughs;des-score-graph.png;https://issues.apache.org/jira/secure/attachment/12617871/des-score-graph.png,14/Jan/14 21:04;thobbs;des-scores-with-penalty.csv;https://issues.apache.org/jira/secure/attachment/12622977/des-scores-with-penalty.csv,14/Jan/14 21:04;thobbs;des-scores-without-penalty.csv;https://issues.apache.org/jira/secure/attachment/12622976/des-scores-without-penalty.csv,09/Dec/13 18:24;cburroughs;des.sample.15min.csv;https://issues.apache.org/jira/secure/attachment/12617870/des.sample.15min.csv,09/Dec/13 18:24;cburroughs;get-scores.py;https://issues.apache.org/jira/secure/attachment/12617869/get-scores.py,14/Jan/14 17:59;thobbs;throughput.png;https://issues.apache.org/jira/secure/attachment/12622900/throughput.png,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,2013-12-17 00:51:52.029,,,no_permission,,,,,,,,,,,,362724,,,Tue Jan 14 21:18:45 UTC 2014,,,,,,0|i1qivr:,363018,,,,,,,,brandon.williams,brandon.williams,,,1.2.0,,,,,,,"17/Dec/13 00:51;rcoli;Are we sure that this mechanism of producing cache pinning is worth the complexity here, especially given speculative execution? ","03/Jan/14 00:34;thobbs;I can reproduce Chris's results, and in my experimentation it looks like almost all of the variation is due to the ""timePenalty"", which is basically how long it has been since the last entry from an endpoint.  I can see why something like the time penalty might be useful for the phi FD, which expects messages on a periodic basis, but it doesn't make sense to me to use it in a load balancing measure.  My suggestion would be to remove the time penalty.

bq. Are we sure that this mechanism of producing cache pinning is worth the complexity here, especially given speculative execution?

Effective cache utilization is extremely important, so I would say it's well worth the additional complexity.  I don't think speculative execution should affect this greatly, but I might be missing something; care to expand on that?","03/Jan/14 01:34;jbellis;This was introduced by CASSANDRA-3722.  It's not clear to me what that code is trying to do.  Or maybe I'm still grumpy about calling i/o activity ""severity.""","06/Jan/14 19:36;ianbarfield;I believe the purpose of time penalty was to more quickly detect problematic nodes. If a node was suddenly suffering severe issues, that wouldn't be reflected in its latency metric until the current outstanding queries resolved. That might take until the maximum duration timeout which can be arbitrarily long, and in many cases is a lot longer than you'd like. By using timeDelay, the snitch can somewhat immediately penalize problem nodes since the queries do not have to timeout first. That said, it has numerous flaws both conceptually and in its implementation.

I was working on this problem a couple weeks ago, but have been distracted since, so I might not be able to give the best summary. Here's a couple issues off the top of my head though:
- if the time delay values are low, then high jitter throws the scores way off. It isn't unreasonable to expect situations where the time delay shifts semi-randomly between 0 and 1 ms. This means very little in terms of whether a node is a suitable target but can cause a drastic difference in scores if there is no slow node to anchor the scores.
- if the node response periods aren't low; say they average around 50 ms. Then by definition they are highly random since the score could be calculated at any point along 0 to 50 ms.
- it has a lot of complex interactions outside of its original scope of detecting bad nodes
- when calculating scores, if there is no lastReceived value for a node (eg. the node has just been added to the cluster), then the logic defaults to using the current time (essentially 0 or maximum 'good'). You might instead take the view that an unproven, cache-cold node would be a bad selection.
- sensitive to local noise. Each time the score is calculated, the timePenalty is calculated fresh. Since there is no concept of persistance or scope, events that corrupt the scoring process are extra harmful. eg. GC, CPU load / thread scheduling, and concurrency shenanigans occuring between the lastReceived.get() and System.currentTimeMillis()

Some of these issues are somewhat alleviated by the switch to using nanos, and I've been tempted to back port that for this class at least for testing, but this logic fails in complex ways. I think at some point I was able to confirm some wildly fluctuating values of the subcomponents to the scores (specifically timePenalty) by checking the mbeans and working under the assumption that timePenalty was likely the only component to well rounded scores -- if you have at least one node with >> timePenalty then it gets cut off to UPDATE_INTERVAL_IN_MS which as a divisor makes for nicely formed floating point numbers.

There are also a lot of issues with the other score components, and some of the overall logic, but... some other time. Apologies if i've gotten something quite wrong; I've never really used Cassandra.","10/Jan/14 20:08;thobbs;[~ianbarfield] thanks for the analysis, you make some excellent observations.

From the discussion in CASSANDRA-3722, it seems like the two motivations for the time penalty were these:
# When a node dies, the FD will not mark it down for a while; in the meantime, we'd like to stop sending queries to it
# In a multi-DC setup, we would like to penalize the remote DC, but not so much that we won't ever use it when local nodes become very slow

I suspect that rapid read protection (CASSANDRA-4705) does a good job of mitigating the #1 case until the FD marks the node down.  I'll do some testing to confirm this.

I don't feel like the #2 case needs special treatment from the dynamic snitch, especially with the badness_threshold in effect.  Latency to the remote DC should prevent it from being used under normal circumstances.  If users really want to guarantee that, the LOCAL consistency levels are always available.",10/Jan/14 20:22;brandon.williams;The best way to test #1 is to run in foreground mode and then suspend (^Z) the JVM.,"14/Jan/14 17:59;thobbs;Attached are two graphs of throughput and 99th percentile latencies for four runs of stress.  Two runs kept the time penalty in DES, and two had it removed.  There was a normal stress read of 3 million rows with and without the time penalty, and a second run where one of the three nodes was suspended 30 seconds into the run and resumed 60 seconds into the run.

In short, there's no difference in throughput or median/95th/99th latencies when a node goes down with the time penalty removed, so it looks like rapid read protection does indeed dominate there.",14/Jan/14 18:10;thobbs;6465-v1.patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-6465]) removes the timePenalty component from the DES score.,14/Jan/14 20:09;brandon.williams;Can we get some numbers on score fluctuation with the time penalty removed to be certain this fixes it?,"14/Jan/14 21:04;thobbs;Attached are the DES scores from a run with and without the time penalty.  This was done with a three node CCM cluster. node1 coordinated all reads, and node2 and node3 were the replicas for all reads.  In both runs, node2 served most of the reads (as reported by cfstats).","14/Jan/14 21:18;brandon.williams;Committed, with mentions of time penalty removed from comments.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Runaway thread for SSL socket,CASSANDRA-6468,12683783,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mishail,mishail,mishail,10/Dec/13 07:00,12/Mar/19 14:05,13/Mar/19 22:29,17/Dec/13 22:39,2.0.4,,,,,,1,,,,,"* Start Cassandra with {{internode_encryption}} turned on. For example {{server_encryption_options:   internode_encryption: dc}}
* Open a telnet session to port 7001. Don't type anything. I used PuTTY in a RAW mode
* Shutdown the MessageService (for ex. by calling {{drain}} on {{StorageSevice}} MBean )
* Now type anything in the telnet session and press enter
* Observe an endless flood of ""Error reading the socket null. java.net.SocketException: Socket closed""",,,,,,,,,,,,,,,,,,,,,,,,10/Dec/13 18:44;mishail;CASSANDRA-2.0-6468.patch;https://issues.apache.org/jira/secure/attachment/12618074/CASSANDRA-2.0-6468.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-17 22:39:36.962,,,no_permission,,,,,,,,,,,,362855,,,Tue Dec 17 22:39:36 UTC 2013,,,,,,0|i1qjqf:,363161,2.0.3,,,,,,,jbellis,jbellis,,,2.0.3,,,,,,,"10/Dec/13 18:44;mishail;Patch: don't rely on exceptions to check the socket's state, use socket's {{isClosed()}} instead",17/Dec/13 22:39;jbellis;Committed.  Also added (in a separate commit) some more cleanup of SocketThread.run.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
avoid maybeResolveForRepair twice,CASSANDRA-6606,12689945,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,wy96f,wy96f,wy96f,21/Jan/14 04:33,12/Mar/19 14:05,13/Mar/19 22:29,29/Jan/14 03:55,2.0.5,,,,,,0,,,,,"In ReadCallback maybeResolveForRepair maybe executed twice. For example, endpoints.size = 3, blockfor = 2. At the moment the second response triggers maybeResolveForRepair, the third one coming increments the received variable which equals 3. As a result, two repairs are produced. This is a tiny problem we can fix easily.",,,,,,,,,,,,,,,,,,,,,,,,21/Jan/14 05:06;wy96f;0001-avoid-maybeResolveForRepair-twice.patch;https://issues.apache.org/jira/secure/attachment/12624070/0001-avoid-maybeResolveForRepair-twice.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-29 03:55:49.887,,,no_permission,,,,,,,,,,,,368911,,,Wed Jan 29 03:55:49 UTC 2014,,,,,,0|i1rl2v:,369215,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"29/Jan/14 03:55;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""hung"" repair results in drain hanging",CASSANDRA-6603,12689535,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jasobrown,cburroughs,cburroughs,17/Jan/14 18:26,12/Mar/19 14:05,13/Mar/19 22:29,17/Jan/14 23:14,1.2.14,,,,,,0,,,,,"A ""hung"" repair (pile of outstanding streams with no visible progress) can result in drain never completing of run.  This is a problem because restarting is a reasonable thing to do with a node that has a hung repair, and drain is a standard part of the restart procedure.  I have had this happen > 20 times.

{noformat}
 WARN [RMI TCP Connection(7752)-10.20.6.115] 2014-01-17 12:56:51,162 StorageService.java (line 288) Stopping gossip by operator request
 INFO [RMI TCP Connection(7752)-10.20.6.115] 2014-01-17 12:56:51,162 Gossiper.java (line 1194) Announcing shutdown
 INFO [RMI TCP Connection(7754)-10.20.6.115] 2014-01-17 12:57:09,217 StorageService.java (line 942) DRAINING: starting drain process
 INFO [RMI TCP Connection(7754)-10.20.6.115] 2014-01-17 12:57:09,217 ThriftServer.java (line 116) Stop listening to thrift clients
 INFO [RMI TCP Connection(7754)-10.20.6.115] 2014-01-17 12:57:09,251 Gossiper.java (line 1194) Announcing shutdown
 INFO [RMI TCP Connection(7754)-10.20.6.115] 2014-01-17 12:57:11,252 MessagingService.java (line 694) Waiting for messaging service to quiesce
 INFO [ACCEPT-ldc1e.clearspring.local/10.20.6.115] 2014-01-17 12:57:11,253 MessagingService.java (line 904) MessagingService shutting down server thread.
 ...
wait 10 minutes with nothing happening
{noformat}

",1.2.12 w/ 1.2.13 patches,,,,,,,,,,,,,,,,,,,,,,,17/Jan/14 21:36;jasobrown;6603_v1.diff;https://issues.apache.org/jira/secure/attachment/12623730/6603_v1.diff,17/Jan/14 18:28;cburroughs;CassandraDaemon.stack;https://issues.apache.org/jira/secure/attachment/12623698/CassandraDaemon.stack,17/Jan/14 18:28;cburroughs;CassandraDaemon.stack2;https://issues.apache.org/jira/secure/attachment/12623699/CassandraDaemon.stack2,17/Jan/14 18:28;cburroughs;drain.stack;https://issues.apache.org/jira/secure/attachment/12623700/drain.stack,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2014-01-17 18:42:50.043,,,no_permission,,,,,,,,,,,,368502,,,Fri Jan 17 23:14:23 UTC 2014,,,,,,0|i1rijz:,368806,,,,,,,,yukim,yukim,,,,,,,,,,17/Jan/14 18:28;cburroughs;netstats has > 150 streams where visual inspection did not reveal any forward progress,"17/Jan/14 18:42;jasobrown;Well, the good news is that's not hung - it's waiting for up to 24 hours for streams to complete :/ . In MS.waitForStreaming (https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/net/MessagingService.java#L675) you can see it. ","17/Jan/14 18:48;jasobrown;Note: I didn't mean my previous comment to sound flippant, but this is the current state of the code in c-1.2. Doing my homework and checking 2.0/trunk now...","17/Jan/14 18:56;jasobrown;Looks like these lines are removed in c* 2.0 from MS.drain()

{code}setMode(Mode.DRAINING, ""waiting for streaming"", false);
MessagingService.instance().waitForStreaming();{code}

So, at least in 2.0, I think we won't block on streaming during drain. [~yukim] can you comment on this? Thanks

","17/Jan/14 19:01;brandon.williams;Hmm, I don't see the 'waiting for streaming' line in the description, is that just oversight [~cburroughs]?","17/Jan/14 19:11;jasobrown;[~brandon.williams] The last arg to setMode() is a boolean if we should log at INFO the msg (2nd arg), else if gets logged at DEBUG.","17/Jan/14 19:14;brandon.williams;Ah, you're right.  setMode is so whack.",17/Jan/14 19:29;yukim;waitFoStreaming is removed in 36f54f939caeeb57f4583bb526fe3b9f73296cf6.,17/Jan/14 19:36;brandon.williams;Any reason why we should wait for it on drain to begin with? Obviously the operator is looking to restart/shutdown when draining.,17/Jan/14 21:16;jasobrown;[~yukim] was hoping you could explain why it was changed in 2.0 :),17/Jan/14 21:36;jasobrown;patch for c* 1.2 attached for skipping the block for streaming during drain. Looks like it's already gone in 2.0,"17/Jan/14 23:01;yukim;I think it's ok to remove wait for streaming completely.

Looks like wait for streaming had been around at least since 0.6, at first for [5 sec|https://github.com/apache/cassandra/blob/cassandra-0.6/src/java/org/apache/cassandra/net/MessagingService.java#L341] and then changed to 24 hours (CASSANDRA-3679).

Since 2.0, streaming does not use streaming executor, thus removed.",17/Jan/14 23:14;jasobrown;committed to 1.2. thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossiper failed with ArrayIndexOutOfBoundsException,CASSANDRA-6564,12688121,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thobbs,shaochuan,shaochuan,09/Jan/14 19:56,12/Mar/19 14:05,13/Mar/19 22:29,24/Jan/14 18:54,1.2.14,2.0.5,2.1 beta1,,,,0,gossip,,,,"We have 30 nodes in one DC, 25 nodes in another. We are running 2.0.1.

Two nodes are joining the ring, but one of them failed with ArrayIndexOutOfBoundsException:

{noformat}
java.lang.ArrayIndexOutOfBoundsException: 2
        at org.apache.cassandra.service.StorageService.extractExpireTime(StorageService.java:1594)
        at org.apache.cassandra.service.StorageService.handleStateRemoving(StorageService.java:1550)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1174)
        at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:1887)
        at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:844)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:922)
        at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,18/Jan/14 00:17;thobbs;6564-v1.patch;https://issues.apache.org/jira/secure/attachment/12623754/6564-v1.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-09 21:57:13.678,,,no_permission,,,,,,,,,,,,367127,,,Fri Jan 24 18:54:02 UTC 2014,,,,,,0|i1ra67:,367437,2.0.1,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"09/Jan/14 21:57;jbellis;Is this a new node you are adding to the ring, or an existing node you restarted?",10/Jan/14 04:34;shaochuan;This is a new node that I added to join the ring.,10/Jan/14 14:11;brandon.williams;Was this cluster upgraded from a previous version? Can you attach the output from 'nodetool gossipinfo' from one of the nodes?,"10/Jan/14 23:58;shaochuan;The cluster was upgraded from 1.2.8, but that was long time ago. Noted I removed the data folder, and started joining from scratch, and after several retries, they finally joined the cluster. Hopefully, the following information is useful.

Here is the gossip info:

/10.215.114.239
  LOAD:6.42133986848E11
  RACK:us-east-1c
  HOST_ID:d13374d8-e4ae-466d-ad5a-44229a2fa190
  RPC_ADDRESS:0.0.0.0
  SEVERITY:0.0
  SCHEMA:4909e1b6-7f1d-3882-bee2-d345d2d3df17
  DC:pagedb-frontend
  STATUS:NORMAL,-1305722878370288637
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.122.218.80
  LOAD:2.70118657429E11
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  HOST_ID:7746a875-1e53-4ebe-aef6-ad59fadd6ea7
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1796000685025988745
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.178.13.230
  LOAD:2.17601876297E11
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  HOST_ID:160d83bf-4cc0-4872-aec6-7908446eccbf
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1646882803236172485
  DC:pagedb-backend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.71.141.179
  LOAD:2.20270818508E11
  RACK:us-east-1c
  HOST_ID:baa81888-6417-4e7c-8d7b-2bb79c38ca40
  SEVERITY:0.0
  RPC_ADDRESS:0.0.0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  DC:pagedb-frontend
  STATUS:NORMAL,-1277850891915823266
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.95.128.214
  LOAD:1.46145062779E11
  RACK:us-east-1e
  HOST_ID:3e5e6e75-56cb-4664-abf4-de8c2801bd5d
  SEVERITY:0.0
  RPC_ADDRESS:0.0.0.0
  SCHEMA:ab0684d8-b50f-3d99-bc2d-7aa021db8131
  DC:pagedb-backend
  STATUS:NORMAL,-1896996972976095280
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.251.114.31
  LOAD:2.95507989207E11
  RACK:us-east-1b
  HOST_ID:a71e7037-c4f8-4ff9-864d-e83b6d3037d7
  SEVERITY:0.0
  RPC_ADDRESS:0.0.0.0
  SCHEMA:70359e7a-d66e-3e72-a83d-c65b2b0db916
  DC:pagedb-frontend
  STATUS:NORMAL,-1866613386904756042
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.228.26.240
  LOAD:1.45711328732E11
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  HOST_ID:8df816c9-7875-4106-aeda-b372b9f1fdc9
  SEVERITY:1.7857143878936768
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1359255731430490566
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.185.67.195
  LOAD:4.5836673634E10
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  HOST_ID:37fe7cc7-3481-48b3-96ff-c92df17a4132
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1295072457639851651
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.220.195.198
  LOAD:7.7243382968E10
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  HOST_ID:0bd7f3ab-d347-4c59-9f1a-1b7104e34a6b
  SEVERITY:8.875740051269531
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1514821029440891529
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.62.39.130
  LOAD:1.67888679362E11
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  HOST_ID:13b12ee3-36a4-462c-9013-ccc1b83a70ff
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1089815038805941976
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.137.11.197
  LOAD:1.63784332779E11
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  HOST_ID:76d339f4-ac4a-4f95-bada-1ec17f67f4e3
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1708596056219631840
  DC:pagedb-backend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.87.145.85
  LOAD:5.78900708105E11
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  HOST_ID:bdad1b28-3fb7-4788-8f34-01caace03ca9
  SEVERITY:3.8759689331054688
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-2223601356751123985
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.97.135.36
  LOAD:9.1222284614E10
  RACK:us-east-1c
  HOST_ID:40fb13d6-0803-45cc-9b4f-45b3ad7194fa
  SEVERITY:0.0
  RPC_ADDRESS:0.0.0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  DC:pagedb-frontend
  STATUS:NORMAL,-1146411778648768253
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.95.128.6
  LOAD:2.86435047103E11
  RACK:us-east-1e
  HOST_ID:80b583a5-8f7d-4fee-91db-b948c090d055
  SEVERITY:0.0
  RPC_ADDRESS:0.0.0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  DC:pagedb-backend
  STATUS:NORMAL,-1025262993714352739
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.154.136.39
  LOAD:1.68092055557E11
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  HOST_ID:0e391fea-e4e9-4a46-b9af-87948459876c
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-2690692580263318876
  DC:pagedb-backend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.185.9.84
  LOAD:1.65537034566E11
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  HOST_ID:080241a9-eadd-48b4-8f94-efbe35bfd6e1
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1468213189211385280
  DC:pagedb-backend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.251.114.15
  LOAD:2.86426383655E11
  RACK:us-east-1b
  RPC_ADDRESS:0.0.0.0
  SEVERITY:0.0
  HOST_ID:99e3e672-8410-4850-8257-edc7b814a776
  SCHEMA:27ac2baa-c2ff-309b-b480-e96bea58be95
  STATUS:NORMAL,-1117031336072704821
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.97.135.32
  LOAD:2.30702093528E11
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  SEVERITY:0.0
  HOST_ID:e9ac8ad9-c3cc-44ff-81e7-af28a4d5f576
  SCHEMA:20a2d44f-9d12-35d2-b372-67c6d3c57282
  STATUS:NORMAL,-1176341277720804881
  DC:pagedb-backend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.99.144.60
  LOAD:8.7791334967E10
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  HOST_ID:01b323c1-3f0e-40de-8ef7-3e1d33b391c5
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1695890443918261320
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.121.13.216
  LOAD:3.69965110407E11
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  HOST_ID:05f06b6f-ddb7-42a9-9e56-8efd41cf1545
  SEVERITY:0.2531645596027374
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1081159627867819835
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.99.144.97
  LOAD:1.15579406939E11
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  HOST_ID:7a84853a-2c0a-448c-84e2-cceaa98c7523
  SEVERITY:0.2525252401828766
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  DC:pagedb-backend
  STATUS:NORMAL,-1188304416367949186
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.95.132.5
  LOAD:9.4871423158E10
  RACK:us-east-1e
  RPC_ADDRESS:0.0.0.0
  HOST_ID:6480a3a2-cbbe-44f4-b67b-7310f885b307
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1168449340133292844
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.178.2.177
  LOAD:3.82427624094E11
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  HOST_ID:aa7fd551-8a47-4e59-869e-ae0afaadf27e
  SEVERITY:0.25
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1337943409437095727
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.201.206.166
  LOAD:2.00848067233E11
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  HOST_ID:f9651587-8f34-4dbc-af5c-64f19bc85ad6
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-2302424866842171501
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.93.6.87
  LOAD:5.55658195833E11
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  HOST_ID:2ca596b1-fa0a-40a8-ace1-bc7b21e82c45
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1767037199861003446
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.123.74.248
  LOAD:3.09054824961E11
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  HOST_ID:94daf57d-838d-4285-8048-e60f97629dba
  SEVERITY:0.2493765652179718
  REMOVAL_COORDINATOR:REMOVER,76d339f4-ac4a-4f95-bada-1ec17f67f4e3
  SCHEMA:b2c3e6ec-3b3f-3ad9-baef-e9e311a24eb9
  STATUS:removed,94daf57d-838d-4285-8048-e60f97629dba,1389650189822
  DC:pagedb-backend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.71.141.12
  LOAD:1.91108857556E11
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  SEVERITY:0.0
  HOST_ID:4b9f63e5-8561-4e59-8011-e11fb3e8d627
  REMOVAL_COORDINATOR:REMOVER,c7a0edfd-629e-47c3-b79c-32a9c2c98801
  SCHEMA:26cf5f29-f7f9-321d-b5c4-ac9314d90653
  DC:pagedb-frontend
  STATUS:removed,4b9f63e5-8561-4e59-8011-e11fb3e8d627,1389562451886
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.185.0.80
  LOAD:2.2200005E11
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  HOST_ID:9885866e-af4d-4129-ad33-d465c8a7cd58
  SEVERITY:0.0
  SCHEMA:548308e6-bd1c-388f-ad1e-5ecf39f994fd
  STATUS:LEAVING,-1327521511834457537
  DC:pagedb-backend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.185.47.50
  LOAD:2.19304461069E11
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  HOST_ID:73c27c92-1f3e-4628-9c91-a1e0a03a9e21
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:LEAVING,-1275697304680139394
  DC:pagedb-backend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.93.49.164
  LOAD:1.02638093298E11
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  HOST_ID:19f401cb-b418-4ce3-b0e0-ea1530ab2121
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1102475930390387446
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.206.97.184
  LOAD:1.61110374415E11
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  SEVERITY:1.159420371055603
  HOST_ID:8d3d9e79-5a72-4561-99f0-0817cd6ad6cc
  SCHEMA:9b172508-3803-31cd-bd9a-4b82c7695fea
  STATUS:NORMAL,-1397201050646007911
  DC:pagedb-backend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.71.141.42
  LOAD:5.9084879531E10
  RACK:us-east-1c
  HOST_ID:2113b135-8566-4ed9-b74b-5e302d240c42
  RPC_ADDRESS:0.0.0.0
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  DC:pagedb-frontend
  STATUS:NORMAL,-1008058669507434673
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.154.148.5
  LOAD:4.90437848228E11
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  HOST_ID:86642aef-f625-4561-be7b-ab8f58066294
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1261154574760184756
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.183.153.179
  LOAD:3.16807499528E11
  RACK:us-east-1e
  RPC_ADDRESS:0.0.0.0
  HOST_ID:5988d85d-663c-4597-9422-7f5bc195ec5b
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1486604450347152783
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.97.131.247
  LOAD:1.12362968132E11
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  HOST_ID:811a2966-b6f0-41fd-ba29-fc980763a69c
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  DC:pagedb-backend
  STATUS:NORMAL,-1087382214101550882
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.99.144.54
  LOAD:8.9541174805E10
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  HOST_ID:675e8de7-641f-4914-beeb-fad29942381f
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-3327961952466683302
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.210.91.83
  LOAD:3.17740980893E11
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  HOST_ID:b0726b39-1587-4443-853e-6e668658b1cc
  SEVERITY:0.0
  SCHEMA:548308e6-bd1c-388f-ad1e-5ecf39f994fd
  STATUS:NORMAL,-1375725999837634215
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.71.141.158
  LOAD:1.78152183005E11
  RACK:us-east-1c
  HOST_ID:31c7690b-30fa-455c-9f69-e98f11d4c235
  RPC_ADDRESS:0.0.0.0
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  DC:pagedb-backend
  STATUS:NORMAL,-1306761067597910068
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.71.141.7
  LOAD:2.05956307253E11
  RACK:us-east-1c
  HOST_ID:adbee379-f7eb-45d3-a259-58e35a32526f
  SEVERITY:0.0
  RPC_ADDRESS:0.0.0.0
  REMOVAL_COORDINATOR:REMOVER,13b12ee3-36a4-462c-9013-ccc1b83a70ff
  SCHEMA:26cf5f29-f7f9-321d-b5c4-ac9314d90653
  DC:pagedb-frontend
  STATUS:removed,adbee379-f7eb-45d3-a259-58e35a32526f,1389487070232
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.38.169.32
  LOAD:1.8436403005E11
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  HOST_ID:67ec4857-c867-46c8-9c62-778fd26360eb
  SEVERITY:0.0
  SCHEMA:b2c3e6ec-3b3f-3ad9-baef-e9e311a24eb9
  STATUS:NORMAL,-137637605179813169
  DC:pagedb-backend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.87.94.230
  LOAD:3.25766553603E11
  RACK:us-east-1a
  HOST_ID:604b0268-187a-4a5f-a51e-b94608111825
  SEVERITY:0.0
  RPC_ADDRESS:0.0.0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1519665921822534263
  DC:pagedb-backend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.99.144.91
  LOAD:1.09636372331E11
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  HOST_ID:010613b8-9d0b-487d-bdd1-64e80ddc60e6
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  DC:pagedb-backend
  STATUS:NORMAL,-3103769032004455685
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.71.141.130
  LOAD:1.44144056816E11
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  SEVERITY:0.0
  HOST_ID:3c30b5fb-bf8c-4169-a60f-6ffad2c28598
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  DC:pagedb-frontend
  STATUS:NORMAL,-1099965132011727160
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.99.146.244
  LOAD:7.7127649757E10
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  HOST_ID:6a96c2e0-6376-461d-bac5-0dcbe3cd1eb5
  SEVERITY:0.5102040767669678
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  DC:pagedb-backend
  STATUS:NORMAL,-1404826323347485057
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.95.129.124
  LOAD:1.58532861482E11
  RACK:us-east-1e
  RPC_ADDRESS:0.0.0.0
  HOST_ID:2a987ea1-4037-4b9e-a403-27a6d7995621
  SEVERITY:0.0
  SCHEMA:ab0684d8-b50f-3d99-bc2d-7aa021db8131
  STATUS:NORMAL,-2498420976414553971
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.122.50.31
  LOAD:2.36075305112E11
  RACK:us-east-1a
  HOST_ID:b9e40521-bf2e-4833-8f6a-a8e3297df042
  RPC_ADDRESS:0.0.0.0
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  DC:pagedb-backend
  STATUS:NORMAL,-1277185879546496035
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.4.197.53
  LOAD:3.17042001421E11
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  HOST_ID:9f76061b-43a7-432b-9e6d-612a628534cf
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1388643925813439514
  DC:pagedb-backend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.44.183.111
  LOAD:2.2734300494E11
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  HOST_ID:1e0e3870-5026-452b-8050-dad7fcd5bd88
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-283520605333269313
  DC:pagedb-backend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.238.138.32
  LOAD:1.91421972811E11
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  HOST_ID:e225c748-ffd8-4905-91bf-4e745b26fa44
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:LEAVING,-1437879538859760580
  DC:pagedb-backend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.95.129.103
  LOAD:1.48505882147E11
  RACK:us-east-1e
  RPC_ADDRESS:0.0.0.0
  HOST_ID:18031026-e20c-40eb-8bb3-d771b641fb26
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1389777754931898083
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.125.10.14
  LOAD:7.2479153089E10
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  HOST_ID:5764492f-31f8-4c3f-8bec-d8a2c1af4b4d
  SEVERITY:1.0723861455917358
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1822845329295703853
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.4.202.48
  LOAD:2.63093299094E11
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  HOST_ID:76f98456-2029-4bb6-b3de-e08157220fc9
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-2184837513740789816
  DC:pagedb-backend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.95.132.44
  LOAD:2.1847224734E11
  RACK:us-east-1e
  HOST_ID:c7a0edfd-629e-47c3-b79c-32a9c2c98801
  RPC_ADDRESS:0.0.0.0
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  DC:pagedb-backend
  STATUS:NORMAL,-243162178282959980
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.96.31.80
  LOAD:2.28377954207E11
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  HOST_ID:fa3ea99e-f9b6-4f83-9234-21d8059e6eb4
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1236217677726412210
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.40.245.193
  LOAD:1.27846265043E11
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  HOST_ID:d2383010-fd55-4364-8868-5dc8a937778f
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1237220198988914368
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
/10.99.144.75
  LOAD:8.284603516E10
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  HOST_ID:c085c62d-74ea-45e5-9ed6-6a26a72ec3bb
  SEVERITY:0.0
  SCHEMA:c85aa256-c8a3-3122-bc16-a4cd5c9032d4
  STATUS:NORMAL,-1022810197871922800
  DC:pagedb-frontend
  RELEASE_VERSION:2.0.1
  NET_VERSION:7
","18/Jan/14 00:17;thobbs;It looks like the third patch on CASSANDRA-5216 caused this.  Basically, there's only an expireTime for REMOVED statuses and not for REMOVING statuses, but when dealing with non-member endpoints we didn't make that check.  (I have to admit that I'm not sure how we would get a REMOVING status for a non-member endpoint.)

6564-v1.patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-6564]) only looks for an expire time for REMOVED states and adds a basic unit test.  I wrote this against 2.0, but I think it should apply to 1.2 and trunk as well.","24/Jan/14 18:54;brandon.williams;I'm not sure how we get here either, unless gossip propagation is way behind, but the check certainly won't hurt.  Committed to 1.2 and up.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Slower shutdown on trunk than previous versions,CASSANDRA-6578,12688567,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,benedict,brandon.williams,brandon.williams,13/Jan/14 16:19,12/Mar/19 14:05,13/Mar/19 22:29,13/Jan/14 20:15,2.1 beta1,,,,,,0,,,,,"Trunk takes a good bit longer to shudown than 2.0.  A stack trace reveals it's hanging for a bit here:

{noformat}
""StorageServiceShutdownHook"" prio=10 tid=0x000000000276a800 nid=0x7fbc in Object.wait() [0x00007f31e0a69000]
   java.lang.Thread.State: WAITING (on object monitor)
    at java.lang.Object.wait(Native Method)
    - waiting on <0x00000000ca581b08> (a java.lang.Thread)
    at java.lang.Thread.join(Thread.java:1258)
    - locked <0x00000000ca581b08> (a java.lang.Thread)
    at java.lang.Thread.join(Thread.java:1332)
    at org.apache.cassandra.db.commitlog.AbstractCommitLogService.awaitTermination(AbstractCommitLogService.java:171)
    at org.apache.cassandra.db.commitlog.CommitLog.shutdownBlocking(CommitLog.java:309)
    at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:571)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at java.lang.Thread.run(Thread.java:722)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,13/Jan/14 19:46;benedict;tmp.patch;https://issues.apache.org/jira/secure/attachment/12622687/tmp.patch,13/Jan/14 19:46;benedict;tmp2.patch;https://issues.apache.org/jira/secure/attachment/12622688/tmp2.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-01-13 19:46:01.963,,,no_permission,,,,,,,,,,,,367586,,,Mon Jan 13 20:15:34 UTC 2014,,,,,,0|i1rczb:,367894,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,13/Jan/14 17:44;brandon.williams;Bisect says CASSANDRA-3578 is the cause.,13/Jan/14 19:46;benedict;I've attached two trivial patches. Both should fix the bug. I'm comfortable with either.,"13/Jan/14 20:15;brandon.williams;LGTM, commited the first patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool Refresh / CFS.loadNewSSTables() can Lose New SSTables,CASSANDRA-6514,12685696,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thobbs,thobbs,thobbs,19/Dec/13 22:40,12/Mar/19 14:05,13/Mar/19 22:29,18/Mar/14 23:17,2.0.7,2.1 beta2,,Tool/nodetool,,,0,,,,,"When nodetool refresh / CFS.loadNewSSTables() renames the newly loaded SSTables, it doesn't check to make sure the new name doesn't already exist.  It's easy for one of the newly loaded files themselves to have one of these names, so the rename will wipe out one of the SSTables you intended to load.

For example, if you create a new, empty table, move two SSTables with generations 1 and 2 into the data directory, and then call {{nodetool refresh}}, you might see this:

{noformat}
INFO 15:37:42,587 Loading new SSTables for duration_test/ints...
 INFO 15:37:42,601 Renaming new SSTable /var/lib/cassandra/data/duration_test/ints/duration_test-ints-jb-2 to /var/lib/cassandra/data/duration_test/ints/duration_test-ints-jb-1
 INFO 15:37:42,605 Opening /var/lib/cassandra/data/duration_test/ints/duration_test-ints-jb-1 (424005 bytes)
 INFO 15:37:42,614 Renaming new SSTable /var/lib/cassandra/data/duration_test/ints/duration_test-ints-jb-1 to /var/lib/cassandra/data/duration_test/ints/duration_test-ints-jb-2
 INFO 15:37:42,615 Opening /var/lib/cassandra/data/duration_test/ints/duration_test-ints-jb-2 (424005 bytes)
 INFO 15:37:42,617 Loading new SSTables and building secondary indexes for duration_test/ints: [SSTableReader(path='/var/lib/cassandra/data/duration_test/ints/duration_test-ints-jb-1-Data.db'), SSTableReader(path='/var/lib/cassandra/data/duration_test/ints/duration_test-ints-jb-2-Data.db')]
 INFO 15:37:42,618 Done loading load new SSTables for duration_test/ints
ERROR 15:38:09,428 Exception in thread Thread[ReadStage:40,5,main]
java.lang.RuntimeException: java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/cassandra/data/duration_test/ints/duration_test-ints-jb-1-Data.db (No such file or directory)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1939)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)

{noformat}",,,,,,,,,,,,,,CASSANDRA-6245,,,,,,,,,,18/Mar/14 22:21;thobbs;6514-2.0-v2.patch;https://issues.apache.org/jira/secure/attachment/12635417/6514-2.0-v2.patch,19/Dec/13 22:43;thobbs;6514-2.0.patch;https://issues.apache.org/jira/secure/attachment/12619682/6514-2.0.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-12-19 22:50:14.609,,,no_permission,,,,,,,,,,,,364771,,,Tue Mar 18 23:17:53 UTC 2014,,,,,,0|i1qvgn:,365071,,,,,,,,jbellis,jbellis,,,,,,,,,,19/Dec/13 22:43;thobbs;Attached patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-6514]) checks for conflicts when renaming the loaded SSTables and includes a unit test that exercises this.,"19/Dec/13 22:50;jbellis;I'm not sure it's worth special-casing the rename, since flush/compaction can still error out here.  Either we need to solve it everywhere or tell people, ""come up with generation ids high enough that they won't conflict with new sstables until long after loading is done.""","19/Dec/13 22:57;jbellis;Maybe it would be best to add some kind of ""staging"" directory so we don't need to worry about conflicts at all.  Fundamentally it's kind of broken to encourage people to manually edit the live data directories.","19/Dec/13 22:59;thobbs;bq. Maybe it would be best to add some kind of ""staging"" directory so we don't need to worry about conflicts at all. Fundamentally it's kind of broken to encourage people to manually edit the live data directories.

Yeah, I agree that that would be the proper fix, this was just a quick solution to avoid a (probably) more common case.","19/Dec/13 23:01;thobbs;bq.  ""come up with generation ids high enough that they won't conflict with new sstables until long after loading is done.""

I did think about that, but in this case, the generations didn't conflict with newly flushed sstables, so it was particularly unexpected. ","12/Mar/14 20:35;jbellis;+1, although I'd caution not to make it sounds like there is no race at all anyore in CHANGES",18/Mar/14 22:21;thobbs;I ended up needing to adjust the tests a bit to ensure SSTables were cleared off disk and there were new duplicate hardlinks due to incremental backups.  The v2 patch includes those [changes|https://github.com/thobbs/cassandra/commit/d060d9ec9dc460c21aa151f37f59badf9d25150a].,18/Mar/14 22:44;jbellis;+1,"18/Mar/14 23:17;thobbs;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstableloader does not free off-heap memory for index summary,CASSANDRA-6359,12679499,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thobbs,thobbs,thobbs,15/Nov/13 21:45,12/Mar/19 14:05,13/Mar/19 22:29,16/Nov/13 16:31,2.0.3,,,Legacy/Tools,,,0,,,,,"Although sstableloader tells {{SSTableReaders}} to release their references to the {{IndexSummary}} objects, the summary's {{Memory}} is never {{free()}}'d, causing an off-heap memory leak.",,,,,,,,,,,,,,,,,,,,,,,,15/Nov/13 21:48;thobbs;0001-Free-off-heap-memory-when-releasing-index-summary.patch;https://issues.apache.org/jira/secure/attachment/12614140/0001-Free-off-heap-memory-when-releasing-index-summary.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-16 16:31:45.431,,,no_permission,,,,,,,,,,,,358859,,,Sat Nov 16 16:31:45 UTC 2013,,,,,,0|i1pv1r:,359149,,,,,,,,jbellis,jbellis,,,2.0.0,,,,,,,15/Nov/13 21:48;thobbs;Attached patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-6359]) properly closes the {{IndexSummary}} before releasing the reference.,"16/Nov/13 16:31;jbellis;LGTM, committed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null in a cell caused by expired TTL does not work with IF clause (in CQL3),CASSANDRA-6623,12691349,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,scs95,scs95,27/Jan/14 10:47,12/Mar/19 14:05,13/Mar/19 22:29,06/Feb/14 07:50,2.0.6,,,Legacy/Testing,,,0,,,,,"IF onecell=null clause does not work if the onecell has got its null value from an expired TTL. If onecell is updated with null value (UPDATE) then IF onecell=null works fine.
This bug is not present when you create a table with COMPACT STORAGE directive.
",One cluster with two nodes on a Linux and a Windows system. cqlsh 4.1.0 | Cassandra 2.0.4 | CQL spec 3.1.1 | Thrift protocol 19.39.0. CQL3 Column Family,,,,,,,,,,,,,,,,,,,,,,,06/Mar/14 06:26;pkendall;0001-Fix-for-expiring-columns-used-in-cas-conditions.patch;https://issues.apache.org/jira/secure/attachment/12633056/0001-Fix-for-expiring-columns-used-in-cas-conditions.patch,29/Jan/14 17:58;slebresne;6623.txt;https://issues.apache.org/jira/secure/attachment/12625907/6623.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-01-27 12:32:29.812,,,no_permission,,,,,,,,,,,,370094,,,Thu Mar 06 09:38:21 UTC 2014,,,,,,0|i1rsbz:,370396,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"27/Jan/14 12:32;slebresne;Can you provide a simple reproduction test case? (I'm not contesting there is a problem, just want to make sure what you are running into exactly first)","27/Jan/14 14:14;scs95;Dear Sylvain Lebresne,

I created a table without COMPACT STORAGE directive:
cqlsh> CREATE TABLE astyanaxks.cf2 (name varchar PRIMARY KEY, lock varchar, something varchar);
Then inserted a row:
cqlsh> INSERT INTO astyanaxks.cf2 (name, lock , something ) VALUES ( 'name1', 'lock1', 'som1');
Updated the lock column with a new TTL value:
cqlsh> UPDATE astyanaxks.cf2 USING TTL 10 SET lock='lock2' WHERE name='name1';
cqlsh> SELECT * FROM astyanaxks.cf2;

 name  | lock  | something
-------+-------+-----------
 name1 | lock2 |      som1

(1 rows)

After 10 seconds:

cqlsh> SELECT * FROM astyanaxks.cf2;

 name  | lock | something
-------+------+-----------
 name1 | null |      som1

(1 rows)

Then I wanted to update the row if lock is null:
cqlsh> UPDATE astyanaxks.cf2 USING TTL 10 SET lock='lock2' WHERE name='name1' IF lock=null;

 [applied]
-----------
     False

It was unsuccessful.

cqlsh> SELECT * FROM astyanaxks.cf2;

 name  | lock | something
-------+------+-----------
 name1 | null |      som1

(1 rows)

cqlsh> 

On the other hand, if the null value is set by an update, then IF clause works. On the same Column Family:
cqlsh> UPDATE astyanaxks.cf2 SET lock=null WHERE name='name1';
cqlsh> UPDATE astyanaxks.cf2 USING TTL 10 SET lock='lock2' WHERE name='name1' IF lock=null;

 [applied]
-----------
      True

cqlsh> SELECT * FROM astyanaxks.cf2;

 name  | lock  | something
-------+-------+-----------
 name1 | lock2 |      som1

Now lock column is set.

Best Regards,

Csaba Seres
 


","27/Jan/14 15:17;slebresne;Thanks Csaba.

So this is kind of due to CASSANDRA-5619 patch, with a soupçon of the problem from CASSANDRA-5762 thrown in.

When we generate conditions on a row internally, we currently include the row marker in the 'expected' CF. The reason is that for CASSANDRA-5619, we wanted to distinguish the cases of CAS failure because the row doesn't exist, from CAS failure because all columns on which we have conditions are null (but the row exists). So including the row marker in 'expected', makes us query the row marker, which in turns allows to say if the row does exists or not on CAS failure.

But this also means that 'UPDATE IF' checks for the row existence. Which doesn't really matter, unless you have only 'null' conditions. Namely, doing
{noformat}
CREATE TABLE test (k int PRIMARY KEY, v int)
UPDATE test SET v = 1 WHERE k = 0 IF v = null
{noformat}
the last update won't work, because the row doesn't exist prior to the update.  And I think this is actually the first question to ask here: do we want that update not to work? I could see arguments for either side tbh. On the one side, since 'null' means the column doesn't exist, if the row don't exist then the column doesn't either and in that sense the update should work. On the other side, making it not work is somewhat more expressive since it allow to check for 'row exists but has null value' separately of 'row doesn't exists' (which you can already check with an 'INSERT IF NOT EXISTS'). Also, this reinforce the notion that with conditions, UPDATE work really more like a SQL UPDATE.

So anyway, the problem Csaba is having here is a bit different. Namely, it's due to the fact that TTL ends up removing the row marker, even if the TTL was only applied to one of the columns and is not a proper marker of row existence.  The result is that the row marker expires in Casba example, but since the CAS expects it to be there, it fails (and this, even though the row actually does still exist).

I think the solution here is basically the same than in CASSANDRA-5762, we should query the full CQL row as soon as we have a condition on it to be able to reliably say if the row exists or not. But while we're at it, it's worth deciding if we want to preserve the current 'UPDATE IF always checks for row existence' behavior or not.
","29/Jan/14 08:32;scs95;Dear Sylvain Lebresne,

Sorry for the late reply but we had much work to finish. Thank you for your respond. We wanted to use lightweight transaction as a semaphore (to lock a row or a Column Family). We would have used the TTL to guarantee a time limit for processes. As the null problem is only in CQL3 table, we use Compact Storage table for timed semaphores.

Best Regards,

Csaba Seres


","29/Jan/14 17:58;slebresne;Attaching patch that fetch the full CQL row for CAS as we do for normal selects. The patch did abstract a bit the conditions in SP.cas() so that we can deal with the details of thrift and CQL separatly, which I think is a lot cleaner anyway and is actually needed to lift a few of the current limitation of the current patch on CASSANDRA-6561.

I'll note that the patch preserves the current behavior that a 'UPDATE IF' implicitely check for the row existence, even if the conditions are all testing for {{null}}. As said above, I don't know if that is the actual semantic we want to preserve or if we want to change it.
","05/Feb/14 21:17;iamaleksey;LGTM

Nits:
- I'd rather see {code}if (!(c != null && c.isLive(now) && c.value().equals(e.value()))){code} rewritten as {code}if (c == null || c.isMarkedForDelete(now) || !c.value().equals(e.value())){code} in ThriftCASConditions (as it is more or less in ColumnsConditions, apparently)
- hasLiveColumns() in ColumnsCondition is dead code
- ByteBufferUtil in ModificationStatement; ColumnNameBuilder, NamesQueryFilter, and SliceQueryFilter in SP are now unused imports","06/Feb/14 07:50;slebresne;Committed (with nits fixed), thanks","06/Mar/14 05:47;pkendall;This problem is not fixed. I am trying to exactly as the steps as in comment #2 above and get exactly the same problems using the trunk version from git.

The expiry time of a column is in seconds
The call time passed to isLive is in milliseconds
The queryTimestamp is in microseconds (not seconds like the comment in the patch says)
There are 3 calls to the CQL3CasConditions constructor passing the queryTimestamp and the one in ModificationStatement.executeWithCondition is the only one that changes the scale of the time value.

From my testing the best solution is to remove the scaling done here and apply a divide by 1000 in the constructor of CQL3CasConditions.

Attached patch [^0001-Fix-for-expiring-columns-used-in-cas-conditions.patch]","06/Mar/14 09:38;slebresne;That's correct, pushed that fix, thanks (the worst part is that there is a dtest but it was currently skipped, so activated it too).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition issue during upgrading 1.1 to 1.2,CASSANDRA-6619,12691198,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,timiblossom,timiblossom,timiblossom,25/Jan/14 11:20,12/Mar/19 14:05,13/Mar/19 22:29,29/Jan/14 15:53,1.2.14,,,,,,0,,,,,"There is a race condition during upgrading a C* 1.1x cluster to C* 1.2.
One issue is that OutboundTCPConnection can't establish from a 1.2 node to some 1.1x nodes.  Because of this, a live cluster during the upgrading will suffer in high read latency and be unable to fulfill some write requests.  It won't be a problem if there is a small cluster but it is a problem in a large cluster (100+ nodes) because the upgrading process takes 10+ hours to 1+ day(s) to complete.


Acknowledging about CASSANDRA-5692, however, it does not fully fix the issue.  We already have a patch for this and will attach shortly for feedback.


",,,,,,,,,,,,,,,,,,,,,,,,27/Jan/14 22:18;timiblossom;patch.txt;https://issues.apache.org/jira/secure/attachment/12625455/patch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-28 14:14:08.454,,,no_permission,,,,,,,,,,,,369941,,,Tue Feb 04 19:39:13 UTC 2014,,,,,,0|i1rre7:,370243,,,,,,,,jbellis,jbellis,,,,,,,,,,"27/Jan/14 22:10;timiblossom;As posted in other tickets, 1.1 and 1.2 have different message protocols.  Hence, it is important to set the right target version when making outbound connections rather than depending on the inbound connections to set a version value.  Thus, race condition in setting the version values is solved.

Attachment is the patch to make sure the code does that when an outbound connection is open and an exchange for versioning information in the hankshake fails.

As discussed with Jason Brown here at Netflix, we came up with a solution that during the upgrade, the upgraded nodes have in the environment the variable cassandra.prev_version = 5 (for 1.1.7 or 4 for 1.1) to help out the handshakes in a mixed version cluster.

Once a cluster is fully upgraded to 1.2, cassadra.prev_version is removed from all nodes' environment and a C* rolling restart across nodes is required.  This step ensures that the new patch won't penalize the 1.2 cluster where all outbound connections are from 1.2 to 1.2.

 

","28/Jan/14 14:14;jbellis;Before we add a workaround, do we understand why the CASSANDRA-5692 approach ""reconnect and use the version that the other side sent us in the meantime"" doesn't work?  1.1 and 1.2 are compatible enough to see each others' version, by design.  Specifically, in both versions the first two ints sent are MAGIC (a sanity check) and ""header"" (compression + version flags).","28/Jan/14 16:39;timiblossom;Jonathan,  you are right that both 1.2 and 1.1 are designed to read out the versions from the headers of the other.  However, 1.2, as a sender in opening the outbound socket, expects to receive back immediately the version int as soon as it sends out its own.  1.1, as a receiver, can read 1.2 header but does not send the version int back.

Here is the piece of code in 1.2 in IncomingTcpConnection.java that sends back the version int:

private void handleModernVersion(int version, int header) throws IOException
{
        DataOutputStream out = new DataOutputStream(socket.getOutputStream());
        out.writeInt(MessagingService.current_version);
        out.flush();
        ......
}


Because 1.1 does not send this back immediately, 1.2 OutboundTcpConnection will be timed out on the read and the socket gets disconnected.  The whole cycle will get repeated again and again until some code sets the target version right.  In the lucky case, IncomingTcpConnection sets the right target version.  However,
it takes a while for the other 1.1 nodes to know that there is a new 1.2 node, especially if the new 1.2 node can't connection to any 1.1 nodes first.

The version convergence will eventually settle down.  However, in a large cluster, this would take some time causing some side effects during this time such as high read latencies, and more hints being stored.

",29/Jan/14 15:53;jbellis;Changed the property name to cassandra.default_messaging_version and commited.,"04/Feb/14 19:39;rcoli;As a note to those coming here from the changelog and trying to estimate potential impact on their 1.1 -> 1.2 upgrade, this particular upgrade edge case has so far only been reproduced with the EC2MultiRegionSnitch, and is likely to only affect deploys using a reconnecting snitch like this one.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException while stopping/draining if native transport wasn't started,CASSANDRA-6618,12691109,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,ravilr,ravilr,ravilr,24/Jan/14 20:23,12/Mar/19 14:05,13/Mar/19 22:29,24/Jan/14 20:25,2.0.5,,,,,,0,,,,,"if using a custom Authenticator, native transport server wouldn't be started.
ERROR [main] 2014-01-24 03:04:40,876 Server.java (line 131) Not starting native transport as the configured IAuthenticator is not capable of SASL authentication

But, while stopping cassandra/'nodetool drain', this results in NullpointerException being thrown currently:
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.cassandra.transport.Server.close(Server.java:177)
	at org.apache.cassandra.transport.Server.stop(Server.java:116)
	at org.apache.cassandra.service.StorageService.stopNativeTransport(StorageService.java:349)
	at org.apache.cassandra.service.StorageService.shutdownClientServers(StorageService.java:364)
	at org.apache.cassandra.service.StorageService.drain(StorageService.java:3288)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:235)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:792)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1486)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:96)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1327)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1419)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:847)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at sun.rmi.transport.Transport$1.run(Transport.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
","RHEL6, cassandra-2.0.4",,,,,,,,,,,,,,,,,,,,,,,24/Jan/14 21:45;ravilr;6618-2.0.txt;https://issues.apache.org/jira/secure/attachment/12625115/6618-2.0.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-24 20:25:12.03,,,no_permission,,,,,,,,,,,,369852,,,Fri Jan 24 23:05:50 UTC 2014,,,,,,0|i1rquf:,370154,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,24/Jan/14 20:25;brandon.williams;Was already fixed in 0d38b25f6fa3b,"24/Jan/14 21:45;ravilr;nativeServer.isRunning could be set to true, even when it wasn't started due to custom authenticator not supporting sasl.","24/Jan/14 21:46;ravilr;Brandon, there still seems to be a logic error, which could wrongly set nativeServer.isRunning to true, even when it wasn't started. Can you please check.",24/Jan/14 22:08;brandon.williams;I don't see how setting it to true after calling run() or calling it as the last step inside run() makes any difference.,"24/Jan/14 23:00;ravilr;if the authenticator is not Saslaware, the run() returns without starting the nativeServer and we still set isRunning to true, in that case:
https://github.com/apache/cassandra/blob/cassandra-2.0/src/java/org/apache/cassandra/transport/Server.java#L133","24/Jan/14 23:05;brandon.williams;I see, I was looking at 1.2.  Thanks, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New versions of Hotspot create new Class objects on every JMX connection causing the heap to fill up with them if CMSClassUnloadingEnabled isn't set.,CASSANDRA-6541,12687035,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,jlacefie,jlacefie,02/Jan/14 20:38,12/Mar/19 14:05,13/Mar/19 22:29,26/Mar/15 20:50,1.2.16,2.0.6,2.1 beta2,Local/Config,,,4,,,,,"Newer versions of Oracle's Hotspot JVM , post 6u43 (maybe earlier) and 7u25 (maybe earlier), are experiencing issues with GC and JMX where heap slowly fills up overtime until OOM or a full GC event occurs, specifically when CMS is leveraged.  Adding:

{noformat}
JVM_OPTS=""$JVM_OPTS -XX:+CMSClassUnloadingEnabled""
{noformat}

The the options in cassandra-env.sh alleviates the problem.",,,,,,,,,,,,,,,,,,,,,,,,28/Feb/15 05:17;rekhajoshm;dse_systemlog;https://issues.apache.org/jira/secure/attachment/12701548/dse_systemlog,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-02 20:45:11.806,,,no_permission,,,,,,,,,,,,366030,,,Mon Jul 02 09:45:54 UTC 2018,,,,,,0|i1r3e7:,366337,,,,,,,,mshuler,mshuler,,,,,,,,,,"02/Jan/14 20:45;mstump;I've seen at least 3 users in the field hit this issue, in each case enabling CMSClassUnloadingEnabled solved the issue.","02/Jan/14 21:21;jasobrown;I think we've been seeing the same issue, as well, with JMX. Do you have any links or refs about the issue/resolution?",03/Jan/14 15:51;jjordan;[~benedict] tracked it down to most likely being this code change: http://hg.openjdk.java.net/icedtea/jdk7/jdk/rev/985b53122cf8,"03/Jan/14 16:03;jbellis;A bit more context:

# We have heap dumps full of javax.management.remote.rmi.RMIConnectionImpl$CombinedClassLoader
# CombinedClassLoader appears to be sandboxing classes used by RMI/JMX and thus instantiates its own copies for each connection
# If you only use a stable JMX connection pool you are fine, otherwise you ""leak"" CCL roots
# If you have bad enough old-gen fragmentation to force periodic STW GC, you are also fine; perversely, this will only bite you if CMS is working as designed","03/Jan/14 16:04;jbellis;[~nickmbailey] did you ever file an openjdk bug, or are we convinced this is the new Working As Designed?","03/Jan/14 16:35;jasobrown;[~jbellis] Yes, that is exactly what we have been seeing, as well. However, I'm not sure if our JVM is that 'recent' (/me weeps silently) - but, I'll try to confirm today across several of our clusters. ","03/Jan/14 16:38;jeromatron;[~jasobrown] fwiw, even though the diff is from openjdk 1.7, we've seen it as far back as jdk 1.6.0 update 45 thus far.","03/Jan/14 16:51;jasobrown;Yeah, a brute force check of some clusters confirms some are on 1.6.0_41, _37, and _45 - and, of course, they are mixed within the cluster, depending on when the instance was launched (if it was with an older AMI with older JVM).

I haven't narrowed the problem down to which instances had troubles with which JVM, but some clusters with < u45 did not have any problems, so I suspect it's at least > u41 (our most highest version), but probably [~benedict] is correct with u45.","03/Jan/14 18:54;nickmbailey;I attempted to file a bug with oracle, but the end result was a notification that I would be emailed with either a bug number or further questions. I haven't heard anything from them yet though. It's been a little less than a month.","09/Jan/14 14:05;jjordan;This goes back at least as far as 1.6u43, just had someone hit it there.","09/Jan/14 14:33;benedict;We could try emailing dmitry.samersoff@oracle.com who contributed the change, and andrew.gross@oracle.com who reviewed it, directly?","09/Jan/14 15:45;benedict;FYI, I don't think I asserted this was u45. Only a specific changeset, which appears to have been in OpenJDK 7u12. It's not possible to see when this was included in Java 6, but could have been as early as u39 judging from release dates on Wikipedia.","09/Jan/14 15:55;jjordan;Yeah, I was adding 1.6 versions where we've seen it, since we can't know when they put that into 1.6.","09/Jan/14 16:34;jbellis;I have emailed dmitry and andrew, ccing cassandra-dev.","24/Feb/14 15:01;benedict;I emailed jmx-dev a few weeks ago, and it's been tumbleweed. I intend to follow up soon, and try the core-dev list (see if I can get any response from a higher traffic list), but I expect this will be slow going. Nobody seems interested.",24/Feb/14 15:05;jbellis;Will {{-XX:+CMSClassUnloadingEnabled}} cause older VMs to bitch at us?  If not let's add it as a default to 1.1+.,"24/Feb/14 18:30;brandon.williams;We already do java version detection in shell and add flags based on that, so adding this shouldn't be a problem.","24/Feb/14 19:25;jbellis;Well, then someone needs to figure out when it was introduced. :)",25/Feb/14 13:41;jbellis;I volunteer [~mshuler] :),"28/Feb/14 20:52;mshuler;I get no complaints using -XX:+CMSClassUnloadingEnabled with 1.5.0_22-b03 and found an old doc that lists this VM option available in 1.4.1 1.4.2 1.5.0 1.6.0 - http://www.reins.altervista.org/java/A_Collection_of_JVM_Options_MP.html
I'd say it's probably pretty safe to add it.",28/Feb/14 21:19;brandon.williams;I committed this.,"01/Mar/14 05:30;mshuler;since I was there..
$ /opt/jdk1.8.0/bin/java -XX:+CMSClassUnloadingEnabled -XX:+PrintFlagsFinal -version |grep CMSClassUnloadingEnabled
     bool CMSClassUnloadingEnabled                 := true            {product}
java version ""1.8.0""
Java(TM) SE Runtime Environment (build 1.8.0-b128)
Java HotSpot(TM) 64-Bit Server VM (build 25.0-b69, mixed mode)","11/Mar/14 01:08;rcoli;Is Cassandra ""since"" on this issue ""the dawn of time""?","11/Mar/14 05:16;jbellis;It's a HotSpot regression that we're working around, not a Cassandra bug.","21/Mar/14 15:01;jjordan;Just adding some more java versions here for people wondering if they are hitting this.  Just saw this on 1.6.0_38, so it goes back at least that far on 1.6","24/Mar/14 18:48;rcoli;{quote}It's a HotSpot regression that we're working around, not a Cassandra bug.{quote}
Yes, so the statement ""this HotSpot regression is not-worked-around in all extant versions of Cassandra since the dawn of time"" is correct. Thanks for the clarification!","24/Mar/14 18:49;brandon.williams;Well, not really, since the problematic JVM versions didn't exist back then.","24/Mar/14 18:52;benedict;bq. Yes, so the statement ""this HotSpot regression is not-worked-around in all extant versions of Cassandra since the dawn of time"" is correct. Thanks for the clarification!

No, it isn't, since the hot spot bug did not exist since the dawn of time. As such the from-version is ill-defined, but probably the most sensible definition requires determining the from-version-for-hotspot, which we have yet to manage, and aligning that with C* releases.","24/Mar/14 19:22;jjordan;C* from version really has no meaning here.  But yes, if you run C* 0.3 (if that did JMX monitoring) with 1.6_45 you will hit this issue.  Unless of course you add the given setting to your cassandra-env.sh.  Which can be done without upgrading your C*.","28/Feb/15 05:16;rekhajoshm;On DSE 4.5.1, Java 1.7, linux ami on aws  encountered long running GC before dse was shutdown.This was a ring of 6 nodes, and no unexpected processes /load were run.The error log attached.

I suspect to have hit that hotspot/java GC issue.But in this case the class unloading is enabled.
JVM_OPTS=""$JVM_OPTS -XX:+CMSClassUnloadingEnabled”
XX:CMSInitiatingOccupancyFraction=75

Now configured to have MAX_HEAP_SIZE=“8G”, HEAP_NEWSIZE=“2G” and GC logging enabled and is stable.However if there is a heap leak, it needs a better fix.
Suggestion: If the CPU utilization is going up, or heap size leak/long running GC is sensed, the nodetool -h localhost flush can auto kick in?

Thanks
Rekha
",28/Feb/15 05:17;rekhajoshm;Dse system log with long running GC before DSE shutdown,20/Sep/15 08:22;Mark Curtis;On newer systems with G1 GC is this option ignored? I'm assuming it would be.,"29/Jun/18 19:05;githubbot;Github user jasobrown commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/236#discussion_r199253772
  
    --- Diff: conf/jvm11.options ---
    @@ -0,0 +1,89 @@
    +###########################################################################
    +#                            jvm11.options                                #
    +#                                                                         #
    +# See jvm.options. This file is specific for Java 11 and newer.           #
    +###########################################################################
    +
    +#################
    +#  GC SETTINGS  #
    +#################
    +
    +
    +
    +### CMS Settings
    +#-XX:+UseParNewGC
    +#-XX:+UseConcMarkSweepGC
    +#-XX:+CMSParallelRemarkEnabled
    +#-XX:SurvivorRatio=8
    +#-XX:MaxTenuringThreshold=1
    +#-XX:CMSInitiatingOccupancyFraction=75
    +#-XX:+UseCMSInitiatingOccupancyOnly
    +#-XX:CMSWaitDuration=10000
    +#-XX:+CMSParallelInitialMarkEnabled
    +#-XX:+CMSEdenChunksRecordAlways
    +## some JVMs will fill up their heap when accessed via JMX, see CASSANDRA-6541
    +#-XX:+CMSClassUnloadingEnabled
    +
    +
    +
    +### G1 Settings
    +## Use the Hotspot garbage-first collector.
    +-XX:+UseG1GC
    +-XX:+ParallelRefProcEnabled
    +
    +#
    +## Have the JVM do less remembered set work during STW, instead
    +## preferring concurrent GC. Reduces p99.9 latency.
    +-XX:G1RSetUpdatingPauseTimePercent=5
    +#
    +## Main G1GC tunable: lowering the pause target will lower throughput and vise versa.
    +## 200ms is the JVM default and lowest viable setting
    +## 1000ms increases throughput. Keep it smaller than the timeouts in cassandra.yaml.
    +-XX:MaxGCPauseMillis=500
    +
    +## Optional G1 Settings
    +# Save CPU time on large (>= 16GB) heaps by delaying region scanning
    +# until the heap is 70% full. The default in Hotspot 8u40 is 40%.
    +#-XX:InitiatingHeapOccupancyPercent=70
    +
    +# For systems with > 8 cores, the default ParallelGCThreads is 5/8 the number of logical cores.
    +# Otherwise equal to the number of cores when 8 or less.
    +# Machines with > 10 cores should try setting these to <= full cores.
    +#-XX:ParallelGCThreads=16
    +# By default, ConcGCThreads is 1/4 of ParallelGCThreads.
    +# Setting both to the same value can reduce STW durations.
    +#-XX:ConcGCThreads=16
    +
    +
    +### JPMS
    +
    +-Djdk.attach.allowAttachSelf=true
    +--add-exports java.base/jdk.internal.misc=ALL-UNNAMED
    +--add-opens java.base/jdk.internal.module=ALL-UNNAMED
    +--add-exports java.base/jdk.internal.ref=ALL-UNNAMED
    +--add-exports java.base/sun.nio.ch=ALL-UNNAMED
    +--add-exports java.management.rmi/com.sun.jmx.remote.internal.rmi=ALL-UNNAMED
    +--add-exports java.rmi/sun.rmi.registry=ALL-UNNAMED
    +--add-exports java.rmi/sun.rmi.server=ALL-UNNAMED
    +--add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED
    +
    +
    +### GC logging options -- uncomment to enable
    +
    +# Java 11 (and newer) GC logging options:
    +# See description of https://bugs.openjdk.java.net/browse/JDK-8046148 for details about the syntax
    +# The following is the equivalent to -XX:+PrintGCDetails -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=10M
    +#-Xlog:gc=info,heap*=trace,age*=debug,safepoint=info,promotion*=trace:file=/var/log/cassandra/gc.log:time,uptime,pid,tid,level:filecount=10,filesize=10240
    --- End diff --
    
    minor nit: `filesize=10240` is the file size in bytes. change to `10485760` if you actually want 10MB files.
","02/Jul/18 09:45;githubbot;Github user snazy commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/236#discussion_r199440821
  
    --- Diff: conf/jvm11.options ---
    @@ -0,0 +1,89 @@
    +###########################################################################
    +#                            jvm11.options                                #
    +#                                                                         #
    +# See jvm.options. This file is specific for Java 11 and newer.           #
    +###########################################################################
    +
    +#################
    +#  GC SETTINGS  #
    +#################
    +
    +
    +
    +### CMS Settings
    +#-XX:+UseParNewGC
    +#-XX:+UseConcMarkSweepGC
    +#-XX:+CMSParallelRemarkEnabled
    +#-XX:SurvivorRatio=8
    +#-XX:MaxTenuringThreshold=1
    +#-XX:CMSInitiatingOccupancyFraction=75
    +#-XX:+UseCMSInitiatingOccupancyOnly
    +#-XX:CMSWaitDuration=10000
    +#-XX:+CMSParallelInitialMarkEnabled
    +#-XX:+CMSEdenChunksRecordAlways
    +## some JVMs will fill up their heap when accessed via JMX, see CASSANDRA-6541
    +#-XX:+CMSClassUnloadingEnabled
    +
    +
    +
    +### G1 Settings
    +## Use the Hotspot garbage-first collector.
    +-XX:+UseG1GC
    +-XX:+ParallelRefProcEnabled
    +
    +#
    +## Have the JVM do less remembered set work during STW, instead
    +## preferring concurrent GC. Reduces p99.9 latency.
    +-XX:G1RSetUpdatingPauseTimePercent=5
    +#
    +## Main G1GC tunable: lowering the pause target will lower throughput and vise versa.
    +## 200ms is the JVM default and lowest viable setting
    +## 1000ms increases throughput. Keep it smaller than the timeouts in cassandra.yaml.
    +-XX:MaxGCPauseMillis=500
    +
    +## Optional G1 Settings
    +# Save CPU time on large (>= 16GB) heaps by delaying region scanning
    +# until the heap is 70% full. The default in Hotspot 8u40 is 40%.
    +#-XX:InitiatingHeapOccupancyPercent=70
    +
    +# For systems with > 8 cores, the default ParallelGCThreads is 5/8 the number of logical cores.
    +# Otherwise equal to the number of cores when 8 or less.
    +# Machines with > 10 cores should try setting these to <= full cores.
    +#-XX:ParallelGCThreads=16
    +# By default, ConcGCThreads is 1/4 of ParallelGCThreads.
    +# Setting both to the same value can reduce STW durations.
    +#-XX:ConcGCThreads=16
    +
    +
    +### JPMS
    +
    +-Djdk.attach.allowAttachSelf=true
    +--add-exports java.base/jdk.internal.misc=ALL-UNNAMED
    +--add-opens java.base/jdk.internal.module=ALL-UNNAMED
    +--add-exports java.base/jdk.internal.ref=ALL-UNNAMED
    +--add-exports java.base/sun.nio.ch=ALL-UNNAMED
    +--add-exports java.management.rmi/com.sun.jmx.remote.internal.rmi=ALL-UNNAMED
    +--add-exports java.rmi/sun.rmi.registry=ALL-UNNAMED
    +--add-exports java.rmi/sun.rmi.server=ALL-UNNAMED
    +--add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED
    +
    +
    +### GC logging options -- uncomment to enable
    +
    +# Java 11 (and newer) GC logging options:
    +# See description of https://bugs.openjdk.java.net/browse/JDK-8046148 for details about the syntax
    +# The following is the equivalent to -XX:+PrintGCDetails -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=10M
    +#-Xlog:gc=info,heap*=trace,age*=debug,safepoint=info,promotion*=trace:file=/var/log/cassandra/gc.log:time,uptime,pid,tid,level:filecount=10,filesize=10240
    --- End diff --
    
    nice catch!
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disablethrift results in unclosed file descriptors,CASSANDRA-6546,12687183,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mishail,alienth,alienth,03/Jan/14 13:23,12/Mar/19 14:05,13/Mar/19 22:29,01/May/14 22:25,1.2.17,2.0.8,2.1 rc1,,,,0,,,,,"Disabling thrift results in unclosed thrift sockets being left around.


Steps to reproduce and observe:

1. Have a handful of clients connect via thrift.
2. Disable thrift.
3. Enable thrift, have the clients reconnect.
4. Observe netstat or lsof, and you'll find a lot of thrift sockets in CLOSE_WAIT state, and they'll never go away.
  * Also verifiable from org.apache.cassandra.metrics:type=Client,name=connectedThriftClients MBean.


What's extra fun about this is the leaked sockets still count towards your maximum RPC thread count. As a result, toggling thrift enough times will result in an rpc_max_threads number of CLOSED_WAIT sockets, with no new clients able to connect.

This was reproduced with HSHA. I haven't tried it in sync yet.
",,,,,,,,,,,,,,,,,,,,,,,,30/Apr/14 20:54;mishail;CASSANDRA-6546-1.2.patch;https://issues.apache.org/jira/secure/attachment/12642719/CASSANDRA-6546-1.2.patch,30/Apr/14 23:17;mishail;CASSANDRA-6546-2.x.patch;https://issues.apache.org/jira/secure/attachment/12642765/CASSANDRA-6546-2.x.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-03-14 17:24:04.986,,,no_permission,,,,,,,,,,,,366178,,,Thu May 01 22:25:04 UTC 2014,,,,,,0|i1r4bj:,366489,1.2.11,2.0.7,2.1 beta1,,,,,brandon.williams,brandon.williams,,,,,,,,,,03/Jan/14 23:18;alienth;Might be a regression of CASSANDRA-3867,"14/Mar/14 17:24;enigmacurry;Tested the following:

Scenario 1:
 * rpc_max_threads: 16
 * rpc_server_type: sync
 * Create 16 pycassa thrift connections, try to create one more, it blocks due to rpc_max_threads reached. Seems like correct behavior.

Scenario 2:
 * rpc_max_threads: 16
 * rpc_server_type: sync
 * Create 16 pycassa thrift connections, disable thrift, reenable thrift, try to create 16 more thrift connections, it works, and connectedThriftClients mbean counts up to ~32. This can be looped as many times as you want, until maximum sockets allowed by ulimit is reached (1024 on my machine). netstat shows all of those connections in ESTABLISHED state on my machine.

Scenario 3:
  * rpc_max_threads: 16
  * rpc_server_type: hsha
  * Create as many connections as you want, it never blocks until Linux ulimit is reached (1024 on my machine). With hsha enabled, rpc_max_threads does not cap the number of connections, though that may be to due the asynchronous design. I haven't figured out how to check how many actual threads are running.

Scenario 4:
  * rpc_max_threads: 16
  * rpc_server_type: hsha
  * Same scenario as 3 but introduce thrift disable/reenable between 16 connection starts. Same effect, create as many connections as you want up to ulimit.

Scenario 2 seems like a definite bug, I'm not sure about 3 and 4.

[Here's the script I'm using to test|https://github.com/riptano/cassandra-dtest/blob/262b2918250aa53b8010804023a35f288d9e9073/thrift_hsha_test.py#L19]","22/Apr/14 21:46;jbellis;Do you want to take a stab at this, [~mishail]?",22/Apr/14 21:53;mishail;[~jbellis] will do,"30/Apr/14 19:25;mishail;I've reproduced the problem on 1.2 with HSHA, for some reason {{ThriftSessionManager.connectionComplete(SocketAddress)}} is never called in this case.
{code}
mstepura-mac:logs mikhail$ lsof -p 84299 | grep CLOSE_WAIT | wc -l
     150
{code}
And all those 150 connections are still sitting in ThriftSessionManager

!2014-04-30 12-22-17.png!
!2014-04-30 12-22-58.png!","30/Apr/14 20:54;mishail;So, we have to cleanup slector keys in HSHA server before closing the selector itself.

There is no such issue with the SYNC server, and behavior observed by [~enigmacurry] in ""Scenario 2"" happens because his script didn't close the connections from *the client side*, and all those ESTABLISHED connections are actually held by the client. As soon as the client(s) close the connections (using {{pool.dispose()}} in dtest for ex.) those ESTABLISHED will be gone. And that is correct.

In case of HSHA those ESTABLISHED would become CLOSE_WAIT, and attached patch cleans them up.",30/Apr/14 21:25;mishail;I'm observing the same CLOSE_WAIT behavior with for 2.0 (using THsHaDisruptorServer). Need to figure out how to fix that,"30/Apr/14 22:47;mishail;To fix this for 2.x we have to use the latest build of {{thrift-server}} - https://github.com/xedin/disruptor_thrift_server/pull/7
","01/May/14 21:55;brandon.williams;+1, but we shouldn't call it 0.3.4 unless it's actually an 0.3.4 release.","01/May/14 22:25;mishail;Commited that as {{thrift-server-internal-only-0.3.3.jar}}
Pushed dtest as well https://github.com/riptano/cassandra-dtest/pull/47",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StackOverflowError with big IN list,CASSANDRA-6567,12688259,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,shohou,shohou,10/Jan/14 15:17,12/Mar/19 14:05,13/Mar/19 22:29,13/Jan/14 09:37,1.2.14,2.0.5,,,,,0,,,,,"Cassandra throws StackOverflowError when binding big list in prepared query  in IN parameter

Stack trace:
java.lang.StackOverflowError
        at org.apache.cassandra.utils.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer.compareTo(FastByteComparisons.java:110)
        at org.apache.cassandra.utils.FastByteComparisons.compareTo(FastByteComparisons.java:41)
        at org.apache.cassandra.utils.FBUtilities.compareUnsigned(FBUtilities.java:216)
        at org.apache.cassandra.utils.ByteBufferUtil.compareUnsigned(ByteBufferUtil.java:89)
        at org.apache.cassandra.db.marshal.LongType.compareLongs(LongType.java:54)
        at org.apache.cassandra.db.marshal.LongType.compare(LongType.java:36)
        at org.apache.cassandra.db.marshal.LongType.compare(LongType.java:28)
        at org.apache.cassandra.db.ArrayBackedSortedColumns.binarySearch(ArrayBackedSortedColumns.java:170)
        at org.apache.cassandra.db.ArrayBackedSortedColumns.binarySearch(ArrayBackedSortedColumns.java:152)
        at org.apache.cassandra.db.ArrayBackedSortedColumns.getColumn(ArrayBackedSortedColumns.java:89)
        at org.apache.cassandra.cql3.statements.SelectStatement$1$1.computeNext(SelectStatement.java:825)
        at org.apache.cassandra.cql3.statements.SelectStatement$1$1.computeNext(SelectStatement.java:826)
        at org.apache.cassandra.cql3.statements.SelectStatement$1$1.computeNext(SelectStatement.java:826)
        at org.apache.cassandra.cql3.statements.SelectStatement$1$1.computeNext(SelectStatement.java:826)
        at .... many more same line stack elements

Would be nice to change the logic to exclude manual paging in such cases",Cassandra 2.0.4 Java Driver 2.0 rc2,,,,,,,,,,,,,,,,,,,,,,,10/Jan/14 18:10;slebresne;6567.txt;https://issues.apache.org/jira/secure/attachment/12622411/6567.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-10 18:10:09.323,,,no_permission,,,,,,,,,,,,367278,,,Mon Jan 13 09:37:06 UTC 2014,,,,,,0|i1rb3j:,367587,,,,,,,,jbellis,jbellis,,,,,,,,,,"10/Jan/14 18:10;slebresne;Attaching trivial patch to avoid the recursion. The patch is against 1.2: it's probably less likely in 1.2 since you can't easily prepare IN, but it's still possible if you try hard enough and the patch is trivial so ...",10/Jan/14 18:15;jbellis;+1,"13/Jan/14 09:37;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstables from stalled repair sessions become live after a reboot and can resurrect deleted data,CASSANDRA-6503,12685433,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jasobrown,jjordan,jjordan,18/Dec/13 18:52,12/Mar/19 14:05,13/Mar/19 22:29,31/Jan/14 13:31,1.2.14,2.0.5,,,,,1,,,,,"The sstables streamed in during a repair session don't become active until the session finishes.  If something causes the repair session to hang for some reason, those sstables will hang around until the next reboot, and become active then.  If you don't reboot for 3 months, this can cause data to resurrect, as GC grace has expired, so tombstones for the data in those sstables may have already been collected.",,,,,,,,,,,,,,CASSANDRA-10797,,,,,,,,,,31/Jan/14 01:59;yukim;6503-2.0-followup.txt;https://issues.apache.org/jira/secure/attachment/12626245/6503-2.0-followup.txt,23/Jan/14 13:42;jasobrown;6503_2.0-v2.diff;https://issues.apache.org/jira/secure/attachment/12624797/6503_2.0-v2.diff,29/Jan/14 21:07;yukim;6503_2.0-v3.diff;https://issues.apache.org/jira/secure/attachment/12625969/6503_2.0-v3.diff,06/Jan/14 14:48;jasobrown;6503_c1.2-v1.patch;https://issues.apache.org/jira/secure/attachment/12621600/6503_c1.2-v1.patch,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-12-18 22:06:01.414,,,no_permission,,,,,,,,,,,,364510,,,Fri Jan 31 23:44:46 UTC 2014,,,,,,0|i1qtv3:,364810,1.1.7,,,,,,,yukim,yukim,,,0.7.0,,,,,,,"18/Dec/13 18:53;jjordan;One thing I was thinking that might help with this is if we could leave the sstables named ""-tmp"" until we are ready to make them become active.  That way when you reboot, any files hanging around will get removed on restart, instead of becoming active.","18/Dec/13 22:06;jbellis;I don't think we should be messing with repair code in 1.2.14, not for something that it took 3 years for someone to run across.  Suggest targetting 2.0.",19/Dec/13 23:10;jbellis;(Not actually sure this is still an issue in 2.0.x but it seems possible.),"19/Dec/13 23:42;yukim;In 1.2 as well as 2.0, every time the node receives SSTable file, it starts writing as 'tmp', but at the end of receiving, it calls SSTableWriter#closeAndOpenReader that moves received file out of 'tmp'. At this point, SSTables are still not added to ColumnFamilyStore, though if we shutdown the node and restart, the node would recognize received files even though streaming session was not finished successfully.

We need little tweak to defer renaming of SSTable files to do so after receiving all streamed files.
 ","06/Jan/14 14:48;jasobrown;Attached patch, 6503_c1.2-v1, defers the release of the sstables to the CFS until the session is complete. Note: that patch is only for 1.2.

For c* 2.0, I'd like [~yukim]'s advice. I have a WIP here: https://github.com/jasobrown/cassandra/tree/6503_c2.0. The problem I'm running into is that FileMessage.sstable is of type SSTableReader, which we need on the sender side, but on the receiver side we want SSTableWriter (if we are going to defer the release of the sstables. For hacking things up sake, I've just changed FileMessage.sstable to a plain SSTable and let the users do the casting - which is only in two places, one of which is the FileMessage.Serializer.serailize() method. Not very extensive, but perhaps a bit sloppy.

Yuki, do you think it's worthwhile to split up the FileMessage object into two classes like OutFileMessage (which has a SSTR) and InFileMessage (which has SSTW)? ","06/Jan/14 16:32;yukim;[~jasobrown] What I'm thinking to do is to closeAndOpenReader without renaming from tmp as we receive, and rename them at once at the end. And for renaming multiple files at once we probably want some kind of lockfile(also resolve CASSANDRA-2900?).

Though finalizing SSTable write in closeAndOpenReader takes some time, so completely defer finalize as you do may be a good idea. I think splitting FileMessage is better than casting.","07/Jan/14 01:28;jasobrown;bq. we probably want some kind of lockfile

Interesting. What problems do you see this solving (I'm probably missing something in my understanding)?

bq. splitting FileMessage is better than casting

Yeah, I knew you were gonna say that :)","07/Jan/14 15:01;jasobrown;[~yukim] I can see how modifying closeAndOpenReader() would help here, but there's one wrinkle (I think): if we defer renaming the files from closeAndOpenReader(), we would need to rename the *open* SSTR, and there's a note in FileUtils.renameWithConfirm():

{code}        // this is not FSWE because usually when we see it it's because we didn't close the file before renaming it,
        // and Windows is picky about that.{code}

So I'm not sure about the deferred rename of the SSTR, assuming we care about Windows.
","10/Jan/14 18:53;yukim;I think you are right. We don't want to close SSTR again for renaming.
Let's stick with deferring closeAndOpenReader as you did.
For the patch, I think it would be nice to call SSTW.abort() to discard already received file when something bad happened.

bq. Interesting. What problems do you see this solving (I'm probably missing something in my understanding)?

If the node goes down during we are doing closeAndOpenReader to received files, there is a chance we have already renamed files.
So I wanted to make sure the node won't read those files when the node come up.
","10/Jan/14 19:30;jasobrown;bq. call SSTW.abort() ...

Makes sense, will add that in.

bq. If the node goes down during we are doing closeAndOpenReader to received files, there is a chance we have already renamed files

Ok, got it. Will come up with an idea and drop into the next patch. Thanks","10/Jan/14 21:24;rcoli;What is the ""since"" version for this ticket?

It seems rather serious that users were still exposed to zombie data even if following repair best practice, if a repair session ever stalled... especially because repair sessions tend to stall. In a way we are fortunate that the only way to restart a failed repair session is to restart the nodes (CASSANDRA-3486) because this workaround has protected some users from this bug.

{quote}I don't think we should be messing with repair code in 1.2.14, not for something that it took 3 years for someone to run across. Suggest targetting 2.0.{quote}
The analysis of impact here seems confused. What has happened is not that it took 3 years for someone to encounter this bug; rather, it took 3 years for someone who ran across this very serious bug to notice and care enough to file a JIRA.

Bugs which violate guarantees in the way this one does (""I repaired once every gc_grace_seconds and still ended up with zombie data!"") should IMO be considered Major or Critical and should be candidates for fixes at any point in the release cycle. If we can entirely re-write a broken feature like replace_node in 1.2.12, I fail to see what keeps us from fixing buggy code that erroneously treats temporary files as permanent, in 1.2.14.","10/Jan/14 21:52;jasobrown;bq. What is the ""since"" version for this ticket?

I suspect it's been around for a while - I only (knowingly) ran into the problem with 1.1.7.

bq. ""3 years""

The original descriptions says ""3 months"", and that is how long we took to run into the problem (at a serious level). I think perhaps [~jbellis] misspoke when he mentioned ""3 years"" in an earlier comment. 

As for fix version, I would like to see this in 1.2, as well, but it depends on the changes needed. At this point, though, the patch I have now (still WIP) is not too dramatic, so it could reasonably go into 1.2, but we should judge when it's ready/accepted.",10/Jan/14 22:29;jbellis;I'm counting 3 years since 0.7 which is at least as long as repair has worked this way.,"10/Jan/14 22:48;jasobrown;[~jbellis] Ahh, OK. Thank you for clarifying.","10/Jan/14 22:52;jjordan;I think most people notice they have hung repairs, and restart the nodes to clear them, before it becomes a problem.","23/Jan/14 13:42;jasobrown;Attached v2 patch has the following changes:

- Changed StreamReceiveTask to keep a collection of SSTW rather than SSTR. This allows us to do the conversion of SSTW to SSTR all together after we've gotten all the streamed files. Also fixed up the code paths to here so they pass SSTW.

- Also in StreamReceiveTask, added an abort() method, which will discard the SSTWs it has buffered up. Changed StreamSession so that when a session ends in failure, it calls the new STR.abort() method.

- Split FileMessage out into IncomingFileMessage and OutgoingFileMessage. I needed to do this since as each one has a different subclass of SSTable, but also because java generics doesn't allow me to return different subclasses from StreamMessage.Serializer<V extends StreamMessage>. This necessitated the changes in StreamMessage as I couldn't have one serializer for both IncomingFileMessage and OutgoingFileMessage.  As it didn't seem best to create a new StreamMessage.Type (something like FILE_IN and FILE_OUT) just to represent the FILE message type's behavior on inbound vs. outbound, I instead split the SM.Type.serializer into two variables: inSerializer and outSerializer. For all the other Type's, the in and out serializers are the same class; in the case of Type.FILE, this is where I'm referencing IncomingFileMessage.serializer and OutgoingFileMessage.serializer, respectively. This seemed the cleanest way to introduce the now-bifurcated life of Type.FILE/FileMessage.

- added StreamLockfile to satisfy [~yukim]'s request for a mechanism to remove, on restart, the subset of SSTRs that were successfully converted when others from it's stream session failed. Assumes the process crashed in the middle of converting the SSTWs to SSTRs.

In the first patch, I chose to write the lockfile out to the commitlog directory. I did this as it seems like overkill to add another yaml setting (and Config/DD change) just for this value. Thus, I wanted to piggyback off something else that we already have, and DD.getCommitLogDirectory seemed the least worst. I'm open to suggestions on this.

Once these changes are incorporated into 2.0 and trunk, I would still like to do something for 1.2 but I do not think we need to be as extensive as what we're doing for 2.0+. Perhaps leave out the lockfile and the abort(), and just leave the deferring of converting SSTW to SSTR until the end of the session (basically what the current 1.2 patch does, but I'll check it out again after the 2.0 stuff is good).
","29/Jan/14 21:07;yukim;bq. In the first patch, I chose to write the lockfile out to the commitlog directory. 

How about using SSTable directory itself? You can access it using Directories object so it is easy to perform delete if we do it inside CFS.scrubDataDirectories. Attaching patch for this.

Other than that, it seems good.

bq.  ...I would still like to do something for 1.2 but I do not think we need to be as extensive as what we're doing for 2.0+. Perhaps leave out the lockfile and the abort(), and just leave the deferring of converting SSTW to SSTR until the end of the session...

I agree. We can commit attached 1.2 patch as well.","30/Jan/14 18:01;jasobrown;Committed the 1.2 patch to 1.2, have a few questions for [~yukim] about 2.0 patch that I'll add here in a minute (after coffee)","30/Jan/14 19:32;jasobrown;bq. How about using SSTable directory itself?

I think that is legit as StreamReceiveTask is specific to a CF.","31/Jan/14 01:59;yukim;[~jasobrown] Looks like we need to make a little tweak around complete message since we moved adding received SSTables to different thread.
It is breaking StreamingTransferTest.

Patch attach for fix.","31/Jan/14 10:58;krummas;[~yukim] i committed the followup patch to 2.0, revert if that was a bad idea :)","31/Jan/14 14:23;jasobrown;The followup patch is fine, but I'm not quite sure why it is needed. How does being in a different thread affect the sending of the CompleteMessage, which doesn't look like it was there before?","31/Jan/14 23:44;yukim;[~jasobrown] Complete message exchange was actually fragile before and could leave streaming session to WAIT_COMPLETE state on one side.

Only bellow pattern worked, and it worked because we were sending complete as we receive FileMessage in the same thread.

{code}
(A) ---> File     ---> (B) ...1
(A) <--- Complete <--- (B) ...2
(A) ---> Complete ---> (B) ...3
{code}

But now finalizing all received files moved to another thread. So sending receiving complete from A(3) gets first and B terminates it session without sending back complete, leaving A as WAIT_COMPLETE. Thus we needed to make sure to send complete message.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LOCAL_SERIAL  use QUORUM consistency level to validate expected columns,CASSANDRA-6495,12685086,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,kohlisankalp,kohlisankalp,kohlisankalp,17/Dec/13 02:59,12/Mar/19 14:05,13/Mar/19 22:29,14/Jan/14 17:54,2.0.5,,,,,,0,LWT,,,,"If CAS is done at LOCAL_SERIAL consistency level, only the nodes from the local data center should be involved. 
Here we are using QUORAM to validate the expected columns. This will require nodes from more than one DC. 
We should use LOCAL_QUORAM here when CAS is done at LOCAL_SERIAL. 

Also if we have 2 DCs with DC1:3,DC2:3, a single DC down will cause CAS to not work even for LOCAL_SERIAL. 

",,,,,,,,,,,,,,,,,,,,,,,,14/Jan/14 08:07;kohlisankalp;trunk_6495.diff;https://issues.apache.org/jira/secure/attachment/12622812/trunk_6495.diff,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-19 09:25:11.804,,,no_permission,,,,,,,,,,,,364163,,,Tue Jan 14 17:54:19 UTC 2014,,,,,,0|i1qrq7:,364463,,,,,,,,jbellis,jbellis,,,,,,,,,,"19/Dec/13 06:38;kohlisankalp;[~jbellis]
For this should we use LOCAL_QUORAM for LOCAL_SERIAL or use the consistency level of commit. I think adding a third CL for this will be too confusing. So I think we can use CL of commit for validating columns.  ",19/Dec/13 09:25;slebresne;Pretty sure we *must* use LOCAL_QUORUM. LOCAL_SERIAL should still provided serializability within the local data-center and this require doing a LOCAL_QUORUM. Using the CL of the commit would be incorrect in most case.,"19/Dec/13 13:29;jbellis;I think that ""cas"" CL of SERIAL -> read at Q, LOCAL_SERIAL -> LQ.",20/Dec/13 04:57;kohlisankalp;OK. Let me do that,"14/Jan/14 17:54;jbellis;committed the StorageProxy change.

Leaving the AbstractPaxosCallback timeout alone; the intention is that CasContentionTimeout is for ""the replicas are responding normally, but I couldn't get a ballot accepted within X seconds because I'm competing with other transactions.""  So using WriteTimeout for a response to a single prepare or propose is correct.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Loss of precision under concurrent access on o.a.c.utils.EstimatedHistogram methods,CASSANDRA-6682,12694177,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,paulogaspar7,paulogaspar7,paulogaspar7,09/Feb/14 21:18,12/Mar/19 14:05,13/Mar/19 22:29,10/Feb/14 06:46,2.0.6,,,,,,0,,,,,"On method getBuckets(), under concurrent use of an instance, the value of a ""buckets"" variable element might change between its access on the 1st for cycle and its access on the 2nd, when it is reset to 0. This means that, if one collects metrics by repeatedly calling estHist.getBuckets(true), (e.g.: to sum its values) it will miss counting some values added to ""buckets"" entries between that 1st and 2nd access.

On method mean(), if the i-th entry of ""buckets"" changes value between the 1st and 2nd access inside the for cycle, than the ""elements"" and ""sum"" accumulators are not working with the same values for that entry. It is more precise (and faster) to use a local variable to read the value just once.

Not an error but a semantic improvement: at my initial read of this class, I thought the ""buckets"" and ""bucketOffsets"" fields could change length. Such perception can be avoided by making the ""bucketOffsets"" field final.",,,,,,,,,,,,,,,,,,,,,,,,09/Feb/14 21:23;paulogaspar7;6682.txt;https://issues.apache.org/jira/secure/attachment/12627895/6682.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-10 06:46:09.941,,,no_permission,,,,,,,,,,,,372686,,,Mon Feb 10 06:46:09 UTC 2014,,,,,,0|i1s86n:,372990,,,,,,,,jbellis,jbellis,,,,,,,,,,09/Feb/14 21:23;paulogaspar7;The attached files contains all necessary changes.,10/Feb/14 06:46;jbellis;committed; thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstableloader fails when attempting to load data from a single node into a multi-node cluster,CASSANDRA-6636,12692155,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,rhatch,rhatch,29/Jan/14 22:04,12/Mar/19 14:05,13/Mar/19 22:29,28/Feb/14 21:35,2.0.6,,,,,,0,,,,,"I'm running into this exception when trying to use sstableloader to bring in data from another cluster:
{noformat}
rhatch@whatup:~/.ccm/test_cluster_1391031988/node1$ bin/sstableloader -d 127.0.0.1 ~/tmp/Keyspace1/Standard1
Established connection to initial hosts
Opening sstables and calculating sections to stream
Streaming relevant part of /home/rhatch/tmp/Keyspace1/Standard1/Keyspace1-Standard1-jb-5-Data.db /home/rhatch/tmp/Keyspace1/Standard1/Keyspace1-Standard1-jb-6-Data.db to [/127.0.0.1, /127.0.0.2, /127.0.0.3]
Exception in thread ""STREAM-OUT-/127.0.0.1"" java.lang.NullPointerException
	at org.apache.cassandra.streaming.ConnectionHandler$MessageHandler.signalCloseDone(ConnectionHandler.java:249)
	at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.run(ConnectionHandler.java:375)
	at java.lang.Thread.run(Thread.java:744)
{noformat}

This is what I see in the node system.log:
{noformat}
==> ./test_cluster_1391031988/node1/logs/system.log <==
 INFO [STREAM-INIT-/127.0.0.1:60971] 2014-01-29 14:57:25,375 StreamResultFuture.java (line 116) [Stream #564ded70-8930-11e3-84e9-2766c3cc4197] Received streaming plan for Bulk Load
 INFO [STREAM-IN-/127.0.1.1] 2014-01-29 14:57:25,375 StreamResultFuture.java (line 168) [Stream #564ded70-8930-11e3-84e9-2766c3cc4197] Prepare completed. Receiving 2 files(91047224 bytes), sending 0 files(0 bytes)

{noformat}

steps to reproduce:

create a 3 node cluster with ccm

run stress on one node with 'ccm node1 stress'

copy the node's data from the data/Keyspace1/Standard1 directory to save it for re-loading (preserve the directory structure for sstableloader)

remove the cluster, and create a new 3 node cluster

pick a node and run bin/nodetool -d localhost ~/saved_data_location/Keyspace1/Standard1","java version ""1.7.0_51""
cassandra from cassandra-2.0 branch (0be424...)",,,,,,,,,,,,,,,,,,,,,,,22/Feb/14 16:15;yukim;6636-2.0.txt;https://issues.apache.org/jira/secure/attachment/12630482/6636-2.0.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-29 22:47:41.957,,,no_permission,,,,,,,,,,,,370746,,,Fri Feb 28 21:35:26 UTC 2014,,,,,,0|i1rwbb:,371054,2.0.5,,,,,,,kohlisankalp,kohlisankalp,,,,,,,,,,"29/Jan/14 22:05;rhatch;I reproduced this with the latest code on the cassandra-2.0 branch.

I should also note that the sstableloader command succeeds if I attempt importing into a single node cluster (instead of into a 3 node cluster).","29/Jan/14 22:40;rhatch;One other strange thing to note: I was able to successfully load data into node2 and node3. After that, the load also worked on node1.",29/Jan/14 22:47;yukim;I can reproduced in my box. I will investigate.,"30/Jan/14 21:33;yukim;So the root cause I tracked down is actually NPE when showing progress.
BulkLoader expects to receive StreamEvent PREPARE to setup progress indicator. But when you get the exception, event listener is registered after streaming emits PREPARE event. I think this is more likely to happen in dev environment like ccm.
",13/Feb/14 23:31;jbellis;How involved would fixing this be?,"22/Feb/14 16:15;yukim;bq. How involved would fixing this be?

Not much. I attached patch to add event handler through StreamPlan so that BulkLoader can attach progress handler before receiving any stream event.","22/Feb/14 17:18;jbellis;Can you review, [~kohlisankalp]",24/Feb/14 17:54;kohlisankalp;Sure.,"28/Feb/14 19:50;kohlisankalp;The StreamPlan.handlers is not thread safe but here it is set by same thread so it is good.
So looks good.  ","28/Feb/14 21:35;yukim;Right, StreamPlan is not intended to be used from multiple threads.

Thanks for the review. Committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disk Failure Policy ignores CorruptBlockException,CASSANDRA-6646,12692930,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,krummas,kohlisankalp,kohlisankalp,03/Feb/14 19:24,12/Mar/19 14:05,13/Mar/19 22:29,18/Mar/14 07:16,2.0.7,2.1 beta2,,,,,0,,,,,"If Cassandra is using compression and has a bad drive or stable, it will throw an CorruptBlockException. 
Disk Failure Policy only works if it is an FSError and does not work for IOExceptions like this. 
We need to better handle such exceptions as it causes nodes to not respond to the co-ordinator causing the client to timeout. ",,,,,,,,,,,,,,,,,,,,,,,,17/Mar/14 17:45;krummas;0001-add-paranoid-disk-failure-option.patch;https://issues.apache.org/jira/secure/attachment/12635124/0001-add-paranoid-disk-failure-option.patch,14/Mar/14 08:17;krummas;0001-make-CorruptSSTableException-an-FSError-to-be-able-t.patch;https://issues.apache.org/jira/secure/attachment/12634674/0001-make-CorruptSSTableException-an-FSError-to-be-able-t.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-03-14 08:17:03.699,,,no_permission,,,,,,,,,,,,371516,,,Tue Mar 18 07:16:10 UTC 2014,,,,,,0|i1s107:,371818,,,,,,,,kohlisankalp,kohlisankalp,,,,,,,,,,"14/Mar/14 08:17;krummas;[~kohlisankalp] do you have a stack trace? It looks to me like we catch all CorruptBlockExceptions and either rethrow them as FSError or CorruptSSTableException

Attached patch makes CorruptSSTableException extend FSError to make the uncaught exception handler call handleFSError for them as well.","14/Mar/14 18:14;kohlisankalp;Here is the stack trace. It is CorruptSSTableException which is a problem. 
ERROR [ReadStage:2257] xxxxx CassandraDaemon.java (line 191) Exception in thread Thread[ReadStage:2257,5,main]
org.apache.cassandra.io.sstable.CorruptSSTableException: org.apache.cassandra.io.compress.CorruptBlockException: (/xx/data/Cassandra/xxxxx/xxxxx/xxxxxxx-166-Data.db): corruption detected, chunk at 148883342 of length 23271.
       at org.apache.cassandra.io.compress.CompressedRandomAccessReader.reBuffer(CompressedRandomAccessReader.java:89)
       at org.apache.cassandra.io.util.RandomAccessReader.seek(RandomAccessReader.java:312)
       at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:42)
       at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1048)
       at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:63)
       at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:68)
       at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:44)
       at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:104)
       at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:68)
       at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:272)
       at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
       at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1391)
       at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1207)
       at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1123)
       at org.apache.cassandra.db.Table.getRow(Table.java:347)
       at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:70)
       at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:44)
       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
       at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.cassandra.io.compress.CorruptBlockException: (xxxx-Data.db): corruption detected, chunk at 148883342 of length 23271.
       at org.apache.cassandra.io.compress.CompressedRandomAccessReader.decompressChunk(CompressedRandomAccessReader.java:120)
       at org.apache.cassandra.io.compress.CompressedRandomAccessReader.reBuffer(CompressedRandomAccessReader.java:85)
       ... 20 more
",14/Mar/14 18:22;jbellis;[~iamaleksey] were you deliberately distinguishing CorruptSSTableException from FSError in CASSANDRA-2118?,"14/Mar/14 18:24;kohlisankalp;The patch looks straightforward. The only thing is that now with default setting of stop for disk failure policy, Cassandra will stop on any bad stable. 
Also work done in CASSANDRA-2261 will not matter here as Cassandra will stop on any bad stable. 

I would vote for the behavior in this ticket as you should replace your drive on corruption.  ","14/Mar/14 18:58;jbellis;If anything I think we should add another setting (""paranoid?"") that would stop on corrupt sstables, because really that's a distinct scenario from actual FS errors.","14/Mar/14 21:19;kohlisankalp;+1
We might also need to count errors from unique stable and have a threshold for it. ","15/Mar/14 02:16;iamaleksey;CorruptBlockException is not an FSError for a reason, yes. +1 for adding another option to stop on those.",17/Mar/14 17:45;krummas;add paranoid DiskFailurePolicy for CorruptSSTableExceptions,"17/Mar/14 18:04;jbellis;sorry to bikeshed myself, maybe rename to stop_paranoid to emphasize that it shares behavior with stop?

where did stopTransports get introduced?  i don't see it on trunk.","17/Mar/14 18:08;krummas;bq. rename to stop_paranoid
sure

it was introduced in CASSANDRA-6364 which was a 2.0-only fix, ill add it when i commit this",17/Mar/14 18:15;kohlisankalp;Let me look at the patch. ,"18/Mar/14 01:15;kohlisankalp;The patch looks quite straight forward. This will work for us. 
We might also want to add another option to stop cassandra after we see error in x different stables. ",18/Mar/14 02:13;jbellis;I'm having trouble thinking of a scenario where one error is not enough but two or three is.  Let's keep it simple?,18/Mar/14 02:29;kohlisankalp;Sure. We can keep it simple. ,"18/Mar/14 07:16;krummas;committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodes flap once at startup,CASSANDRA-6658,12693394,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,05/Feb/14 17:29,12/Mar/19 14:05,13/Mar/19 22:29,06/Feb/14 21:08,2.0.6,,,,,,0,,,,,"Upon initially seeing each other, a node will mark another UP, then DOWN, then UP again.",,,,,,,,,,,,,,,,,,,,,,,,05/Feb/14 20:25;brandon.williams;6658-v2.txt;https://issues.apache.org/jira/secure/attachment/12627203/6658-v2.txt,05/Feb/14 18:09;brandon.williams;6658.txt;https://issues.apache.org/jira/secure/attachment/12627171/6658.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-02-05 19:18:39.744,,,no_permission,,,,,,,,,,,,371979,,,Thu Feb 06 21:08:18 UTC 2014,,,,,,0|i1s3un:,372283,,,,,,,,jbellis,jbellis,,,2.0.3,,,,,,,"05/Feb/14 18:07;brandon.williams;At least one problem is that in CASSANDRA-6385 we never converted the initial value to nanos, so when we divide by the mean when the only sample is the initial value and thus equivalent to it, the resulting phi is inflated.",05/Feb/14 19:18;jbellis;Nit: shouldn't we use TimeUnit to convert instead of hardcoding extra constants?,05/Feb/14 20:25;brandon.williams;v2 removes the constant CASSANDRA-4925 added and uses TimeUnit instead.,06/Feb/14 20:59;jbellis;+1,06/Feb/14 21:00;jbellis;Looks like this affects 1.2 as well,"06/Feb/14 21:03;brandon.williams;1.2 doesn't use nanos, so I don't think so.","06/Feb/14 21:05;jbellis;Ah, you're right.  It was a bad merge.",06/Feb/14 21:08;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setting -Dcassandra.fd_initial_value_ms Results in NPE,CASSANDRA-6751,12696631,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,thobbs,thobbs,21/Feb/14 21:01,12/Mar/19 14:05,13/Mar/19 22:29,28/Apr/14 23:55,1.2.17,2.0.8,,,,,0,,,,,"Start Cassandra with {{-Dcassandra.fd_initial_value_ms=1000}} and you'll get the following stacktrace:

{noformat}
 INFO [main] 2014-02-21 14:45:57,731 StorageService.java (line 617) Starting up server gossip
ERROR [main] 2014-02-21 14:45:57,736 CassandraDaemon.java (line 464) Exception encountered during startup
java.lang.ExceptionInInitializerError
    at org.apache.cassandra.gms.Gossiper.<init>(Gossiper.java:178)
    at org.apache.cassandra.gms.Gossiper.<clinit>(Gossiper.java:71)
    at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:618)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:583)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:480)
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
    at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
Caused by: java.lang.NullPointerException
    at org.apache.cassandra.gms.FailureDetector.getInitialValue(FailureDetector.java:81)
    at org.apache.cassandra.gms.FailureDetector.<clinit>(FailureDetector.java:48)
    ... 8 more
ERROR [StorageServiceShutdownHook] 2014-02-21 14:45:57,754 CassandraDaemon.java (line 191) Exception in thread Thread[StorageServiceShutdownHook,5,main]
java.lang.NoClassDefFoundError: Could not initialize class org.apache.cassandra.gms.Gossiper
    at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:550)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at java.lang.Thread.run(Thread.java:724)
{noformat}

Glancing at the code, this is because the FailureDetector logger isn't initialized when the static initialization of {{INITIAL_VALUE}} happens.",,,,,,,,,,,,,,,,,,,,,,,,28/Apr/14 04:07;dbrosius;6751.txt;https://issues.apache.org/jira/secure/attachment/12642183/6751.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-04-28 04:07:20.224,,,no_permission,,,,,,,,,,,,375107,,,Mon Jul 21 03:37:24 UTC 2014,,,,,,0|i1sn1r:,375404,1.2.15,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"28/Apr/14 04:07;dbrosius;move logger creating above the call to getInitialValue()

against branch 1.2",28/Apr/14 15:55;brandon.williams;+1,28/Apr/14 23:55;dbrosius;committed to cassandra-1.2 as commit 7f019804ca82c727062c2a787fe534690e9dbf6d,21/Jul/14 03:37;rlow;This issue also affects 2.0 branch and was fixed in 2.0.8.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrading node uses the wrong port in gossiping,CASSANDRA-6702,12695102,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,bdeggleston,timiblossom,timiblossom,13/Feb/14 23:17,12/Mar/19 14:05,13/Mar/19 22:29,12/May/15 15:57,2.0.15,,,,,,0,,,,,"When upgrading a node in 1.1.7 (or 1.1.11) cluster to 1.2.15 and inspecting the gossip information on port/Ip, I could see that the upgrading node (1.2 version) communicates to one other node in the same region using Public IP and non-encrypted port.

For the rest, the upgrading node uses the correct ports and IPs to communicate in this manner:
   Same region: private IP and non-encrypted port 
   and
   Different region: public IP and encrypted port

Because there is one node like this (or 2 out of 12 nodes cluster in which nodes are split equally on 2 AWS regions), we have to modify Security Group to allow the new traffics.

Without modifying the SG, the 95th and 99th latencies for both reads and writes in the cluster are very bad (due to RPC timeout).  Inspecting closer, that upgraded node (1.2 node) is contributing to all of the high latencies whenever it acts as a coordinator node. 






 

","1.1.7, AWS, Ec2MultiRegionSnitch",,,,,,,,,,,,,,,,,,,,,,,05/May/15 22:17;bdeggleston;C6702-1.2.txt;https://issues.apache.org/jira/secure/attachment/12730624/C6702-1.2.txt,05/May/15 22:17;bdeggleston;C6702-2.0.txt;https://issues.apache.org/jira/secure/attachment/12730625/C6702-2.0.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-02-14 05:36:07.882,,,no_permission,,,,,,,,,,,,373610,,,Tue May 12 15:52:17 UTC 2015,,,,,,0|i1sdu7:,373910,1.2.15,,,,,,,aweisberg,aweisberg,,,1.2.13,,,,,,,14/Feb/14 05:36;jjordan;[~jasobrown] is this expected?  you mention that scenario here https://issues.apache.org/jira/browse/CASSANDRA-5669?focusedCommentId=13703209&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13703209  (publicIP on non-SSL port),19/Jun/14 21:37;jasobrown;[~timiblossom] Do you remember if the nodes being connected to (over the publicIP/non-secure port) were already upgraded to 1.2? Were they seeds?,"20/Jun/14 02:03;timiblossom;If I recalled correctly, this happened on C* 1.2 nodes while the cluster was still in a mixed mode and the target nodes were seed nodes (C* 1.1.x).  After a while, gossips seemed to settle down correctly on the right IPs and Ports.  However, this took some significant time depending on the size of the cluster.","27/Mar/15 17:08;philipthompson;Has this been reproduced on an upgrade from 1.2 -> 2.0, or only from 1.1 -> 1.2?","27/Mar/15 20:12;jjordan;Yes, I believe this still happens from 1.2->2.0 as well","30/Apr/15 19:50;bdeggleston;Same problem upgrading from 1.1.x to 2.0.x.

The crux of the issue is here: https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/net/IncomingTcpConnection.java#L129

When an upgraded node connects to a non-upgraded node, that call to Gossiper.instance.addSavedNode marks the connecting node as down and forces it to reconnect. That's usually not a problem, except when using the Ec2MultiRegionSnitch, since the ReconnectableSnitchHelper reconnects onJoin/onChange/onAlive, which starts a cycle of reconnection. So this was worked around in CASSANDRA-5669 by just skipping the reconnect step if the cluster was upgrading here: https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/locator/ReconnectableSnitchHelper.java#L65

However for upgrades from versions >= 1.2 to versions >= 1.2, I think we should be able to skip that gossip call, since MS protocol version is no longer tracked by gossip as of 1.2. If the call was just making sure we reconnected, we should be able to replace it with a call to MessagingService.instance.getConnectionPool for the same effect.

With that out of the way, we should be able to remove the version check from ReconnectableSnitchHelper. That should probably go behind a -D flag until 3.0 though, since all hell will break loose if someone using the Ec2MultiRegionSnitch upgrades from a release without that fix","05/May/15 22:17;bdeggleston;I've attached a patch against 1.2, and one for 2.0 when it's merged forward. 

After reading through the connection code in ITC and OTC again, and being unable to reach it in an upgrading cluster, I don't think this code can ever be reached: https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/net/IncomingTcpConnection.java#L126.

To reach that if statement, ITC would need to read the maxVersion from OTC on line 111. However, OTC performs the same check here: https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/net/OutboundTcpConnection.java#L361, and disconnects before it can send it's version to the ITC node on line 377.

I've replaced that block with an assertion in the patch, and updated the ReconnectableSnitchHelper to only skip reconnecting during an upgrade if it's talking to a node with a version < 1.2, which will fix this issue when upgrading from 1.2. The 2.0 patch removes the check in ReconnectableSnitchHelper altogether, since it doesn't support talking to pre 1.2 nodes.",05/May/15 22:36;jbellis;[~aweisberg] to review,06/May/15 21:51;aweisberg;[~bdeggleston] Can you get set up to run in cassci and get this branch tested?,08/May/15 00:57;aweisberg;I think I understand... +1.,08/May/15 21:12;aweisberg;[~enigmacurry] I moved this one to testing just in case it's worth looking at how this might effect other scenarios. Blake did manual testing of the fix and there are some dtests so it's not completely untested. I am not sure how we feel about the dtest coverage and whether that is enough.,"12/May/15 15:52;jbellis;(Missing one of the commits for 2.0 from https://github.com/bdeggleston/cassandra/commits/C6702-2.0, committing based on that.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ALTER TYPE <type> RENAME <field> fails sometime with java.lang.AssertionError: null,CASSANDRA-6705,12695142,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,mishail,mishail,14/Feb/14 05:14,12/Mar/19 14:05,13/Mar/19 22:29,24/Feb/14 09:06,2.1 beta2,,,Legacy/CQL,,,0,,,,,"Here are the steps w

{code}
cqlsh> create KEYSPACE bug WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
cqlsh>
cqlsh>
cqlsh> use bug;
cqlsh:bug> create type first_type (first_field int);
cqlsh:bug> create type second_type (second_field list<first_type >);
cqlsh:bug> alter type first_type RENAME first_field TO first_fieldd;
TSocket read 0 bytes
{code}

And here is from the C* side: 
{code}
NFO  05:11:54 Loading org.apache.cassandra.db.marshal.UserType(bug,7365636f6e645f74797065,7365636f6e645f6669656c64:org.apache.cassandra.db.marshal.ListType(org.apache.cassandra.db.marshal.UserType(bug,66697273745f74797065,66697273745f6669656c6464:org.apache.cassandra.db.marshal.Int32Type)))
INFO  05:11:54 Compacted 4 sstables to [/var/lib/cassandra/data/system/schema_usertypes-3aa752254f82350b8d5c430fa221fa0a/system-schema_usertypes-ka-5,].  908 bytes to 425 (~46% of original) in 12ms = 0.033776MB/s.  4 total partitions merged to 1.  Partition merge counts were {4:1, }
ERROR 05:11:54 Exception in thread Thread[MigrationStage:1,5,main]
java.lang.AssertionError: null
	at org.apache.cassandra.config.UTMetaData.addType(UTMetaData.java:145) ~[main/:na]
	at org.apache.cassandra.db.DefsTables.addType(DefsTables.java:412) ~[main/:na]
	at org.apache.cassandra.db.DefsTables.mergeTypes(DefsTables.java:365) ~[main/:na]
	at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:182) ~[main/:na]
	at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:299) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_51]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_51]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_51]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]
	at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]
ERROR 05:11:54 Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411) ~[main/:na]
	at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:281) ~[main/:na]
	at org.apache.cassandra.service.MigrationManager.announceNewType(MigrationManager.java:216) ~[main/:na]
	at org.apache.cassandra.service.MigrationManager.announceTypeUpdate(MigrationManager.java:247) ~[main/:na]
	at org.apache.cassandra.cql3.statements.AlterTypeStatement.announceMigration(AlterTypeStatement.java:139) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:71) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:180) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:214) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:204) ~[main/:na]
	at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1973) ~[main/:na]
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4486) ~[thrift/:na]
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4470) ~[thrift/:na]
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.1.jar:0.9.1]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.1.jar:0.9.1]
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:194) ~[main/:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_51]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]
	at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.7.0_51]
	at java.util.concurrent.FutureTask.get(FutureTask.java:188) ~[na:1.7.0_51]
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407) ~[main/:na]
	... 17 common frames omitted
Caused by: java.lang.AssertionError: null
	at org.apache.cassandra.config.UTMetaData.addType(UTMetaData.java:145) ~[main/:na]
	at org.apache.cassandra.db.DefsTables.addType(DefsTables.java:412) ~[main/:na]
	at org.apache.cassandra.db.DefsTables.mergeTypes(DefsTables.java:365) ~[main/:na]
	at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:182) ~[main/:na]
	at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:299) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_51]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_51]
	... 3 common frames omitted
{code}",trunk,,,,,,,,,,,,,,,,,,,,,,,21/Feb/14 12:59;slebresne;6705-2.txt;https://issues.apache.org/jira/secure/attachment/12630300/6705-2.txt,17/Feb/14 06:41;mishail;trunk-6705.patch;https://issues.apache.org/jira/secure/attachment/12629319/trunk-6705.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-02-21 12:59:22.059,,,no_permission,,,,,,,,,,,,373650,,,Mon Feb 24 09:06:15 UTC 2014,,,,,,0|i1se33:,373950,2.1 beta1,,,,,,,mishail,mishail,,,,,,,,,,"14/Feb/14 22:23;mishail;In the current implementation {{org.apache.cassandra.db.marshal.ListType(org.apache.cassandra.db.marshal.UserType(bug,66697273745f74797065,66697273745f6669656c64:org.apache.cassandra.db.marshal.Int32Type))}} in not compatible with {{org.apache.cassandra.db.marshal.ListType(org.apache.cassandra.db.marshal.UserType(bug,66697273745f74797065,66697273745f6669656c6464:org.apache.cassandra.db.marshal.Int32Type))}}. Because {{ListType}} uses {{isCompatibleWith}} impementation from {{AbstractType}}

{code:title=AbstractType.java}
    public boolean isCompatibleWith(AbstractType<?> previous)
    {
        return this.equals(previous);
    }
{code}","14/Feb/14 22:28;mishail;What if we'll override {{isCompatibleWith}} for Collections as follows:

{code:title=CollectionType.java}
    @Override
    public boolean isCompatibleWith(AbstractType<?> previous)
    {
        if (this == previous)
            return true;

        if (!(previous instanceof CollectionType))
            return false;
        
        CollectionType tprev = (CollectionType) previous;
        return this.valueComparator().isCompatibleWith(tprev.valueComparator())
                && this.nameComparator().isCompatibleWith(tprev.nameComparator());
    }
{code}

Is it a valid approach?",17/Feb/14 06:41;mishail;Patch: Change the way how collections check their compatibility,"21/Feb/14 12:59;slebresne;We definitively should add a proper isCompatibleWith method for CollectionType.  If we do so however, it means we'll allow altering the type of a collection column, which is a good thing, but is not handle properly currently because collection type are aliases in the comparator and so we must make sure we propagate a change to the comparator too. That case (altering a collection and not propagating it to the comparator) is also not handled by AlterTypeStatement currently. This also revealed a bug in ColumnToCollectionType.isCompatibleWith which has arguments inverted when calling isCompatibleWith on the collection it contains (it didn't matter so far since isCompatibleWith was equality until this patch).

On the added isCompatibleWith, 2 remarks:
# we shouldn't consider 2 different collection compatible, so previous should not just be a CollectionType but really of the same class than {{this}}.
# for the valueComparator, we can use isValueCompatibleWith since we know it's always applied to a cell value and thus preserving the sort order is not required.

Attaching a v2 that handle the remarks above. I've pushed [some dtests|https://github.com/riptano/cassandra-dtest/commit/b208cd6462b823d400e131339ab10911ab570297] for this too.
",22/Feb/14 21:24;mishail;LGTM as far as I comprehend,"24/Feb/14 09:06;slebresne;Committed then, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exceptions during memtable flushes on shutdown hook prevent process shutdown,CASSANDRA-6735,12696019,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,sbtourist,sbtourist,sbtourist,19/Feb/14 17:41,12/Mar/19 14:05,13/Mar/19 22:29,19/Feb/14 22:16,1.2.16,2.0.6,,,,,0,,,,,"If an exception occurs while flushing memtables during the shutdown hook, the process is left hanging due to non-daemon threads still running.",,,,,,,,,,,,,,,,,,,,,,,,19/Feb/14 17:49;sbtourist;CASSANDRA-6735-1.2.patch;https://issues.apache.org/jira/secure/attachment/12629820/CASSANDRA-6735-1.2.patch,19/Feb/14 17:44;sbtourist;CASSANDRA-6735.patch;https://issues.apache.org/jira/secure/attachment/12629819/CASSANDRA-6735.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-02-19 22:16:58.8,,,no_permission,,,,,,,,,,,,374496,,,Wed Feb 19 22:16:58 UTC 2014,,,,,,0|i1sjan:,374796,1.2.15,2.0.5,,,,,,jbellis,jbellis,,,,,,,,,,"19/Feb/14 17:43;sbtourist;Attaching patch that simply catches the exception and logs it, as we're shutting down and there's nothing more we can do.",19/Feb/14 22:16;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improvements and FIxes to Stress,CASSANDRA-6691,12694718,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,benedict,benedict,benedict,12/Feb/14 11:28,12/Mar/19 14:05,13/Mar/19 22:29,13/Feb/14 01:33,2.1 beta1,,,Legacy/Tools,,,0,,,,,"There were a couple of minor issues with the new stress:

1) The warmup period did not scale up as the cluster size increased
2) The mixed workload did not work with CQL

At the same time, I have introduced a change in behaviour in the way the default column values are generated so that they are deterministically based on the key. I have then modified read operations to verify that the data they fetch is the same as should have been inserted, so that stress does some degree of data quality checking at the same time. For the moment the values generated never vary for a given key, so this does nothing to test consistency, it only tests for corruption.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-02-12 16:57:02.695,,,no_permission,,,,,,,,,,,,373226,,,Thu Feb 13 19:48:11 UTC 2014,,,,,,0|i1sbhr:,373527,,,,,,,,xedin,xedin,,,,,,,,,,12/Feb/14 11:37;benedict;Patch available [here|https://github.com/belliottsmith/cassandra/tree/iss-6691],12/Feb/14 16:57;jbellis;Can you review [~xedin]?,"13/Feb/14 01:33;xedin;+1, committed with minor changes.","13/Feb/14 16:39;benedict;Hi [~xedin],

Unfortunately I screwed up this commit slightly (like most of those I submitted yesterday...)

The new feature of checking the data returned was correct was simply no-op'ing. I've updated my repository with the simple fix (and merged with your changes to trunk).","13/Feb/14 19:15;xedin;Ok, [~benedict] I have merged your changed, so everything is in trunk now.",13/Feb/14 19:48;benedict;Awesome - thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using static with a counter column is broken,CASSANDRA-6827,12699561,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,jjordan,jjordan,08/Mar/14 01:15,12/Mar/19 14:05,13/Mar/19 22:29,12/Mar/14 12:09,2.0.7,,,,,,0,,,,,"From a client:
Looks like there is a problem with serialisation//deserialisation of static counters. 
It reproduces on the 2.0 branch, but doesn't appear to be a problem on trunk.

Here is a small cql script which reproduces the problem:

CREATE KEYSPACE ks1 WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}; 
use ks1; 
CREATE COLUMNFAMILY problematic ( stat_cnt counter static, norm_cnt counter, pk text, ck text, PRIMARY KEY (pk, ck) ); 
UPDATE problematic SET stat_cnt = stat_cnt + 1 WHERE pk = 'foo'; 
SELECT * FROM problematic ;

and an example of the error from cqlsh: 
cqlsh:ks1> SELECT * FROM problematic ;

pk | ck | stat_cnt | norm_cnt 
-----+------+--------------------------------------------------------------------------------------------------------------------------------------+---------- 
foo | null | '\x00\x01\x00\x00\xf1$\t`\xa6\x1d\x11\xe3\x82\x9a\x9fqi\x80\x1f\xec\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x01' | null

(1 rows)

Failed to decode value '\x00\x01\x00\x00\xf1$\t`\xa6\x1d\x11\xe3\x82\x9a\x9fqi\x80\x1f\xec\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x01' (for column 'stat_cnt') as counter: unpack requires a string argument of length 8

It seems like the problem lies in SelectStatement, it doesn't resolve the counter and ends up sending the entire 36 bytes of raw counter rather than the 8 bytes.

In select statement we seem to keep the bytebuffer and then in two different places we end up calling the following method:

Selection.ResultSetBuilder.add(ByteBuffer v ) 
rather than: add(Column c) 
(Which in turn calls value() and invokes the CounterContext)

I suppose the fix is just to ensure that the CounterContext is invoked for static counters. 
I can see various ways and places of putting it in place. 
Presumably you'd want to reconcile the static counters just once, earlier and keep the resolved value around to be ouput in several rows. 
I suppose somewhere here either by extending the ternary or changing getSimpleValue : 
staticValues.put(name, name.type.isCollection() ? getCollectionValue(name, group) : getSimpleValue(name, group));

ColumnGroupMap seems to contain the relevant byte buffers and has methods: 
getCollection 
getSimple 
which could be extended to contain some counter related call wrapped in SelectStatement similarly.",,,,,,,,,,,,,,,,,,,,,,,,08/Mar/14 06:49;iamaleksey;6827.txt;https://issues.apache.org/jira/secure/attachment/12633528/6827.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-12 09:23:07.176,,,no_permission,,,,,,,,,,,,377908,,,Wed Mar 12 12:09:57 UTC 2014,,,,,,0|i1t49j:,378200,2.0.5,,,,,,,slebresne,slebresne,,,,,,,,,,12/Mar/14 09:23;slebresne;+1,"12/Mar/14 12:09;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updates to COMPACT STORAGE tables via cli drop CQL information,CASSANDRA-6831,12699814,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,devdazed,devdazed,10/Mar/14 17:47,12/Mar/19 14:05,13/Mar/19 22:29,05/May/14 07:31,1.2.17,2.0.8,2.1 beta2,,,,0,,,,,"If a COMPACT STORAGE table is altered using the CLI all information about the column names reverts to the initial ""key, column1, column2"" namings.  Additionally, the changes in the columns name will not take effect until the Cassandra service is restarted.  This means that the clients using CQL will continue to work properly until the service is restarted, at which time they will start getting errors about non-existant columns in the table.

When attempting to rename the columns back using ALTER TABLE an error stating the column already exists will be raised.  The only way to get it back is to ALTER TABLE and change the comment or something, which will bring back all the original column names.

This seems to be related to CASSANDRA-6676 and CASSANDRA-6370

In cqlsh
{code}
Connected to cluster1 at 127.0.0.3:9160.
[cqlsh 3.1.8 | Cassandra 1.2.15-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.36.2]
Use HELP for help.
cqlsh> CREATE KEYSPACE test WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 };
cqlsh> USE test;
cqlsh:test> CREATE TABLE foo (bar text, baz text, qux text, PRIMARY KEY(bar, baz) ) WITH COMPACT STORAGE;
cqlsh:test> describe table foo;

CREATE TABLE foo (
  bar text,
  baz text,
  qux text,
  PRIMARY KEY (bar, baz)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};
{code}

Now in cli:
{code}

  Connected to: ""cluster1"" on 127.0.0.3/9160
Welcome to Cassandra CLI version 1.2.15-SNAPSHOT

Type 'help;' or '?' for help.
Type 'quit;' or 'exit;' to quit.

[default@unknown] use test;
Authenticated to keyspace: test
[default@test] UPDATE COLUMN FAMILY foo WITH comment='hey this is a comment';
3bf5fa49-5d03-34f0-b46c-6745f7740925
{code}

Now back in cqlsh:
{code}
cqlsh:test> describe table foo;

CREATE TABLE foo (
  bar text,
  column1 text,
  value text,
  PRIMARY KEY (bar, column1)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='hey this is a comment' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};

cqlsh:test> ALTER TABLE foo WITH comment='this is a new comment';
cqlsh:test> describe table foo;

CREATE TABLE foo (
  bar text,
  baz text,
  qux text,
  PRIMARY KEY (bar, baz)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='this is a new comment' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};
{code}",,,,,,,,,,,,,,,,,,,,,,,,01/May/14 15:28;slebresne;0001-Properly-recompute-denseness-of-the-table-on-thrift-up.txt;https://issues.apache.org/jira/secure/attachment/12642861/0001-Properly-recompute-denseness-of-the-table-on-thrift-up.txt,30/Apr/14 03:21;mishail;6831-1.2.patch;https://issues.apache.org/jira/secure/attachment/12642588/6831-1.2.patch,25/Apr/14 10:09;slebresne;6831-2.0-v2.txt;https://issues.apache.org/jira/secure/attachment/12641903/6831-2.0-v2.txt,30/Apr/14 03:21;mishail;6831-2.1.patch;https://issues.apache.org/jira/secure/attachment/12642587/6831-2.1.patch,02/May/14 21:47;iamaleksey;6831-nits.txt;https://issues.apache.org/jira/secure/attachment/12643132/6831-nits.txt,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2014-04-04 20:27:58.75,,,no_permission,,,,,,,,,,,,378160,,,Mon May 05 07:31:30 UTC 2014,,,,,,0|i1t5tb:,378452,1.2.16,2.0.6,2.1 beta1,,,,,iamaleksey,iamaleksey,,,,,,,,,,"04/Apr/14 20:27;mishail;Here's what I've seen on the 2.1 branch.
The table was created in ``cqlsh``
{code:title=cqlsh}
[cqlsh 5.0.0 | Cassandra 2.1.0-beta1-SNAPSHOT | CQL spec 3.1.5 | Native protocol v2]
Use HELP for help.
cqlsh>
cqlsh>  CREATE KEYSPACE test WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };
cqlsh> use test;
cqlsh:test> CREATE TABLE foo (bar text, baz text, qux text, PRIMARY KEY(bar, baz) ) WITH COMPACT STORAGE;
cqlsh:test> DESCRIBE TABLE foo;

CREATE TABLE test.foo (
    bar text,
    baz text,
    qux text,
    PRIMARY KEY (bar, baz)
) WITH COMPACT STORAGE
    AND CLUSTERING ORDER BY (baz ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""NONE""}'
    AND comment = ''
    AND compaction = {'min_threshold': '4', 'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND populate_io_cache_on_flush = false
    AND read_repair_chance = 0.1
    AND speculative_retry = '99.0PERCENTILE'
{code}

Then I did the stuff in ``cassandra-cli``
{code:title=cassandra-cli}
mstepura-mac:cassandra mikhail$ bin/cassandra-cli
Connected to: ""Test Cluster"" on 127.0.0.1/9160
Welcome to Cassandra CLI version 2.1.0-beta1-SNAPSHOT

The CLI is deprecated and will be removed in Cassandra 3.0.  Consider migrating to cqlsh.
CQL is fully backwards compatible with Thrift data; see http://www.datastax.com/dev/blog/thrift-to-cql3

Type 'help;' or '?' for help.
Type 'quit;' or 'exit;' to quit.

[default@unknown] use test;
Authenticated to keyspace: test
[default@test] UPDATE COLUMN FAMILY foo WITH comment='hey this is a comment';
org.apache.thrift.transport.TTransportException
{code}

Meanwhile in the logs
{code}
ERROR 20:14:17 Exception in thread Thread[MigrationStage:1,5,main]
java.lang.AssertionError: There shouldn't be more than one compact value defined: got ColumnDefinition{name=qux, type=org.apache.cassandra.db.marshal.UTF8Type, kind=COMPACT_VALUE, componentIndex=null, indexName=null, indexType=null} and ColumnDefinition{name=value, type=org.apache.cassandra.db.marshal.UTF8Type, kind=COMPACT_VALUE, componentIndex=null, indexName=null, indexType=null}
	at org.apache.cassandra.config.CFMetaData.rebuild(CFMetaData.java:1981) ~[main/:na]
	at org.apache.cassandra.config.CFMetaData.fromSchemaNoTriggers(CFMetaData.java:1751) ~[main/:na]
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1791) ~[main/:na]
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:320) ~[main/:na]
	at org.apache.cassandra.db.DefsTables.mergeColumnFamilies(DefsTables.java:306) ~[main/:na]
	at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:181) ~[main/:na]
	at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:306) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_51]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_51]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_51]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]
	at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]
ERROR 20:14:17 Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: There shouldn't be more than one compact value defined: got ColumnDefinition{name=qux, type=org.apache.cassandra.db.marshal.UTF8Type, kind=COMPACT_VALUE, componentIndex=null, indexName=null, indexType=null} and ColumnDefinition{name=value, type=org.apache.cassandra.db.marshal.UTF8Type, kind=COMPACT_VALUE, componentIndex=null, indexName=null, indexType=null}
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411) ~[main/:na]
	at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:288) ~[main/:na]
	at org.apache.cassandra.service.MigrationManager.announceColumnFamilyUpdate(MigrationManager.java:242) ~[main/:na]
	at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:1676) ~[main/:na]
	at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:4430) ~[thrift/:na]
	at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:4414) ~[thrift/:na]
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.1.jar:0.9.1]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.1.jar:0.9.1]
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:201) ~[main/:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_51]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]
	at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: There shouldn't be more than one compact value defined: got ColumnDefinition{name=qux, type=org.apache.cassandra.db.marshal.UTF8Type, kind=COMPACT_VALUE, componentIndex=null, indexName=null, indexType=null} and ColumnDefinition{name=value, type=org.apache.cassandra.db.marshal.UTF8Type, kind=COMPACT_VALUE, componentIndex=null, indexName=null, indexType=null}
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.7.0_51]
	at java.util.concurrent.FutureTask.get(FutureTask.java:188) ~[na:1.7.0_51]
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407) ~[main/:na]
	... 11 common frames omitted
Caused by: java.lang.AssertionError: There shouldn't be more than one compact value defined: got ColumnDefinition{name=qux, type=org.apache.cassandra.db.marshal.UTF8Type, kind=COMPACT_VALUE, componentIndex=null, indexName=null, indexType=null} and ColumnDefinition{name=value, type=org.apache.cassandra.db.marshal.UTF8Type, kind=COMPACT_VALUE, componentIndex=null, indexName=null, indexType=null}
	at org.apache.cassandra.config.CFMetaData.rebuild(CFMetaData.java:1981) ~[main/:na]
	at org.apache.cassandra.config.CFMetaData.fromSchemaNoTriggers(CFMetaData.java:1751) ~[main/:na]
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1791) ~[main/:na]
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:320) ~[main/:na]
	at org.apache.cassandra.db.DefsTables.mergeColumnFamilies(DefsTables.java:306) ~[main/:na]
	at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:181) ~[main/:na]
	at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:306) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_51]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_51]
	... 3 common frames omitted
{code}

Then I quit ``cqlsh`` and start in again, and now I have
{code:title=cqlsh after cli}
Connected to Test Cluster at 127.0.0.1:9042.
[cqlsh 5.0.0 | Cassandra 2.1.0-beta1-SNAPSHOT | CQL spec 3.1.5 | Native protocol v2]
Use HELP for help.
cqlsh> use test;
cqlsh:test> DESCRIBE TABLE foo;

CREATE TABLE test.foo (
    bar text,
    column1 text,
    value text,
    baz text,
    qux text,
    PRIMARY KEY (bar, column1)
) WITH COMPACT STORAGE
    AND CLUSTERING ORDER BY (column1 ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""NONE""}'
    AND comment = 'hey this is a comment'
    AND compaction = {'min_threshold': '4', 'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND populate_io_cache_on_flush = false
    AND read_repair_chance = 0.1
    AND speculative_retry = '99.0PERCENTILE';
cqlsh:test>
{code}","09/Apr/14 04:18;mishail;So, {{CFMetaData}} created _fromThrift_ has no information about column/value aliases. I guess we can just copy those aliases from the _old_ meta data.

Attaching the patch for that.

",09/Apr/14 04:19;mishail;I still need to figure out how to fix those AssertionError's on 2.0/2.1. ,"10/Apr/14 19:31;iamaleksey;[~mishail] So, do you want to delay review until then or not? (if yes, cancel the patch, please).","10/Apr/14 19:36;mishail;The patch is for 1.2 only, and it does fix the described problem. The patch is not applicable for 2.0/2.1. 

So, yes, the patch can be reviewed for 1.2, and we can track changes for 2.0/2.1 under different JIRA issue. ","10/Apr/14 19:52;iamaleksey;TBH I think the right way to fix this is to reject any attempts from CLI/Thrift in general if a table has non-default aliases.

You should not be mixing schema changes from CLI and cqlsh, should stick to one.","10/Apr/14 19:54;iamaleksey;That is, treat these tables (that have CQL metadata with them) as CQL3 tables, even if they have WITH COMPACT STORAGE, and extend CASSANDRA-6370 to them.",10/Apr/14 19:55;mishail;That's the easiest way to fix the problem,"11/Apr/14 20:25;mishail;I'm trying to understand changes made by this commit (for CASSANDRA-5702): https://github.com/apache/cassandra/commit/67435b528dd474bd25fc90eaace6e6786f75ce04#diff-75146ba408a51071a0b19ffdfbb2bb3cL1965

Before that, only tables with REGULAR columns with {{componentIndex == null}}  were Thrift-*compatible*, i.e. tables WITH COMPACT STORAGE weren't compatible.
After those changes, only tables which have a REGULAR column with {{componentIndex != null}} became Thrift *incompatible*, i.e. tables WITH COMPACT STORAGE became compatible

[~slebresne] [~iamaleksey] could you guys shed more light on that? Can we switch back to the previous behavior?
","14/Apr/14 21:28;iamaleksey;Hmm. There are two compatibility levels:

1. Using Thrift API, can I safely perform write requests against a column family if I only have access to its Thrift metadata? (true for all column families except those with compound sparse comparators, aka 'CQL3 tables')
2. Using Thrift API, can I update the column family without potentially losing extra metadata added via CQL? Now this depends strictly on whether or not such metadata (aliases) have been added via CQL3.

WITH COMPACT STORAGE tables are always 1-compatible (including tools that auto-generate code and ORM-style libraries), but not always 2-compatible (depends on whether or not the the table has been created via CQL3 or altered via CQL3 to rename default aliases).

Now that I've thought about this more, I think we should go with your original approach (copy the aliases from the original CFMetaData), but handle that assertion in this very ticket.","23/Apr/14 21:15;mishail;I'm attaching patches for 1.2 and 2.0, to copy aliases from ""old"" column family","24/Apr/14 07:09;slebresne;I definitively get why 2.0 is broken and I'm good with copying aliases there, but for 1.2, this problem is supposed to be handled by https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/config/CFMetaData.java#L805-L814. Can we check why this isn't working anymore first (and I'm fine removing this code in favor of the attached patch, but we need to make sure we understand what's wrong with it first).",24/Apr/14 16:45;mishail;I'll take a look,"24/Apr/14 18:59;mishail;So, here is how I see what's happening on 1.2:

* We create CFMetaData from Thrift (i.e. no aliases)
* We create a mutation from that Thrift-based metadata ( {{newState.toSchemaNoColumns(rm, modificationTimestamp);}} ). This mutation will delete all column/value aliases from {{schema_columnfamilies}}
* We apply the mutation ({{DefsTable.mergeSchema}}). *Damage done*. 
* We reload in-memory presentation. Now the {{apply(CFMetaData cfm)}} merges the both definitions, correctly keeping column/value aliases, but they are now in-memory only (in {{Schema.instance}}), there are no aliases in the {{schema_columnfamilies}} and this information will be lost on a next restart.

And the patch populates the columns/value aliases for that Thrift-based metadata, hence the mutation doesn't erase them.","25/Apr/14 10:09;slebresne;Makes sense, thanks for having looked at it. Then I guess for 1.2 we should remove the lines mentioned above in favor of the alias copy of your patch.

Regarding the 2.0 patch (the 1.2 patch if fine imo, modulo the lines deletion I just mentioned), I'd prefer dealing with this somewhat earlier, in the {{fromThrift}} method: no need to build the CQL metadata with default values to replace them afterwards. So attaching an alternative that feels a little bit better encapsulated.
","28/Apr/14 18:00;iamaleksey;1.2 patch - LGTM (w/ those lines removed, and I'd rather inline copyAliasesFrom(), since it's not reused and is tiny).
2.0 patch (v2) LGTM","30/Apr/14 03:21;mishail;Attaching the new patch for 1.2 (adjusted as per comments), and patch for 2.1 based on Sylvain's patch.
I give up trying to commit those changes, I have no enough git powers to do that.

I'm able to apply them and merge the branches locally, but {{git pull}} becomes a nightmare after that. I have no guts to push them in the wild.","30/Apr/14 09:36;slebresne;Ok, committed the patches then, thanks.","30/Apr/14 21:41;thobbs;It looks like the 2.1 patch broke CF secondary index creation through Thrift.  The following error shows up when when adding a column definition with an index on it:

{noformat}
INFO  [MigrationStage:1] 2014-04-30 16:34:11,766 DefsTables.java:388 - Loading org.apache.cassandra.config.CFMetaData@1547508d[cfId=2b5c99b0-d0af-11e3-a938-6b09a6cc3d5a,ksName=PycassaTestKeyspace,cfName=Indexed1,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,comment=,readRepairChance=0.1,dclocalReadRepairChance=0.0,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=3 cap=3]=ColumnDefinition{name=key, type=org.apache.cassandra.db.marshal.BytesType, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=5 cap=5]=ColumnDefinition{name=value, type=org.apache.cassandra.db.marshal.BytesType, kind=COMPACT_VALUE, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=7 cap=7]=ColumnDefinition{name=column1, type=org.apache.cassandra.db.marshal.BytesType, kind=CLUSTERING_COLUMN, componentIndex=null, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionParameters={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=<null>,memtableFlushPeriod=0,caching={""keys"":""ALL"", ""rows_per_partition"":""NONE""},defaultTimeToLive=0,minIndexInterval=128,maxIndexInterval=2048,speculativeRetry=NONE,droppedColumns={},triggers={}]
INFO  [MigrationStage:1] 2014-04-30 16:34:11,771 ColumnFamilyStore.java:285 - Initializing PycassaTestKeyspace.Indexed1
ERROR [Thrift:1] 2014-04-30 16:34:11,810 CustomTThreadPoolServer.java:219 - Error occurred during processing of message.
java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
    at java.util.ArrayList.rangeCheck(ArrayList.java:635) ~[na:1.7.0_40]
    at java.util.ArrayList.set(ArrayList.java:426) ~[na:1.7.0_40]
    at org.apache.cassandra.config.CFMetaData.rebuild(CFMetaData.java:2037) ~[main/:na]
    at org.apache.cassandra.config.CFMetaData.fromThriftForUpdate(CFMetaData.java:1036) ~[main/:na]
    at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:1668) ~[main/:na]
    at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:4430) ~[thrift/:na]
    at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:4414) ~[thrift/:na]
    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.1.jar:0.9.1]
    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.1.jar:0.9.1]
    at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:201) ~[main/:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_40]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_40]
    at java.lang.Thread.run(Thread.java:724) [na:1.7.0_40]
{noformat}.","01/May/14 15:28;slebresne;[~mishail] You might not be testing the right commit (or use the right test) because I do get the test failure Tyler mentions.

The problem is a tad subtle, and not directly related to the patch (which mainly uncovered something that was wrong before).  What happens is that when we build the comparator in fromThrit, we don't take the copied CQL metadata to decide if the table is dense or not. This result in the addition of the index, which adds column_metadata where there was none, to switch the comparator from an initially dense one to a sparse one. But that's what confuse the rebuild() since we end up with a non-composite sparse comparator and a CLUSTERING_KEY definition, which incompatible.

Now the reason I said this is not directly related to the patch is that before this patch, fromThrift was already returning a sparse comparator when it shouldn't have (since the table was dense initially), but because we were not copying the CQL metadata, no exception was triggered and the mistake was ""repaired"" as soon as the update was written on disk since then previous CQL metadata were picked up so that the table was left dense as should be. In other words, the code was fishy and not doing what we meant it to do, but that had no visible consequence out of sheer luck.

Anyway I'm attaching a patch that slightly refactor the fromThrift so that we do take all metadata into account when computing isDense(). The patch is 2.1 only, 2.0 is not affected because we don't compute isDense at the same places.

Now I'll note that while that make the pycassa tests run, there is 3 failures.  One of them is actually due to the fact that the CASSANDRA-6738 patch was incomplete so attaching a simple 2nd patch to fix that (I can create a separate issue for that if people prefer but well, that's a simple fix). The other two failure are super-columns related but I haven't yet looked into them (but it's likely they are not related to this ticket).
","01/May/14 16:50;thobbs;bq. One of them is actually due to the fact that the CASSANDRA-6738 patch was incomplete so attaching a simple 2nd patch to fix that (I can create a separate issue for that if people prefer but well, that's a simple fix).

I created CASSANDRA-7112 a couple of days ago for that.  Want to move the patch and review there?

bq. The other two failure are super-columns related but I haven't yet looked into them (but it's likely they are not related to this ticket).

On 7112 I mentioned that the failing super column tests may be related, but if they're not, we can create a new ticket for those.","02/May/14 09:25;slebresne;Oh, I had missed CASSANDRA-7112, my bad. That's definitively where we should handle this so moved the patch there. Let's decide what we do for the super columns problem there too.

So to summarize, as far as this ticket goes, the fix is just the 0001-Properly-recompute-denseness-of-the-table-on-thrift-up patch which is ready for review.","02/May/14 21:01;iamaleksey;LGTM

Attaching 6831-nits.txt that fixes the ColumnDefinitionTest and cleans up unused imports in CFMetaData.

(Also some unrelated stuff that probably might belong to a separate commit - normalizes if-formatting in CFMetaData#internalFromThrift() and removes the left-over empty populate_io_cache_on_flush setting branch).","05/May/14 07:31;slebresne;Committed with nits, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 docs don't mention BATCH UNLOGGED,CASSANDRA-6816,12699388,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thobbs,baldrick,baldrick,07/Mar/14 10:37,12/Mar/19 14:05,13/Mar/19 22:29,19/Mar/14 16:49,1.2.16,2.0.7,2.1 beta2,Legacy/Documentation and Website,,,0,,,,,Here http://cassandra.apache.org/doc/cql3/CQL.html#batchStmt there is no mentioned of BATCH UNLOGGED or BATCH COUNTER.  The datastax documentation has it: http://www.datastax.com/documentation/cql/3.0/cql/cql_reference/batch_r.html,,,,,,,,,,,,,,,,,,,,,,,,19/Mar/14 00:02;thobbs;6816.txt;https://issues.apache.org/jira/secure/attachment/12635439/6816.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-19 00:02:24.192,,,no_permission,,,,,,,,,,,,377735,,,Wed Mar 19 16:49:16 UTC 2014,,,,,,0|i1t373:,378027,,,,,,,,slebresne,slebresne,,,,,,,,,,"19/Mar/14 00:02;thobbs;The attached patch is against cassandra-1.2.  After a +1, I'll patch 2.0 as well and update the website.","19/Mar/14 08:28;slebresne;+1, though we could maybe add that the logged batch are only slower when multiple partition are involved (for completeness sake).","19/Mar/14 16:49;thobbs;Committed with the suggested change, thanks.  The website has also been updated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Triggers are broken in trunk because of imutable list,CASSANDRA-6790,12698145,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,beobal,appodictic,appodictic,01/Mar/14 23:34,12/Mar/19 14:05,13/Mar/19 22:29,11/Mar/14 12:34,2.0.7,,,,,,1,,,,,"The trigger code is uncovered by any tests (that I can find). When inserting single columns an immutable list is created. When the trigger attempts to edit this list the operation fails.
Fix coming shortly.
{noformat}
    java.lang.UnsupportedOperationException
            at java.util.AbstractList.add(AbstractList.java:148)
            at java.util.AbstractList.add(AbstractList.java:108)
            at java.util.AbstractCollection.addAll(AbstractCollection.java:342)
            at org.apache.cassandra.service.StorageProxy.mutateWithTriggers(StorageProxy.java:522)
            at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:1084)
            at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:1066)
            at org.apache.cassandra.thrift.CassandraServer.internal_insert(CassandraServer.java:676)
            at org.apache.cassandra.thrift.CassandraServer.insert(CassandraServer.java:697)
            at org.apache.cassandra.triggers.TriggerTest.createATriggerWithCqlAndReadItBackFromthrift(TriggerTest.java:108)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:606)
            at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
            at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
            at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
            at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
            at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
            at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
            at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
            at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:44)
            at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180)
            at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41)
            at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173)
            at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
            at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
            at org.junit.runners.ParentRunner.run(ParentRunner.java:220)
            at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
            at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
            at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
            at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
            at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
            at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,11/Mar/14 11:20;beobal;0001-Apply-trigger-mutations-when-base-mutation-list-is-i.patch;https://issues.apache.org/jira/secure/attachment/12633893/0001-Apply-trigger-mutations-when-base-mutation-list-is-i.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-11 11:19:35.162,,,no_permission,,,,,,,,,,,,376614,,,Tue Mar 11 14:40:54 UTC 2014,,,,,,0|i1swbj:,376909,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,01/Mar/14 23:44;appodictic;https://github.com/edwardcapriolo/cassandra/compare/trigger_coverage?expand=1,01/Mar/14 23:50;appodictic;I added a trigger implementing class to src/main rather then src/test because when we add python tests we need a compiled trigger already present on the classpath.,02/Mar/14 20:23;appodictic;BatchMutation works but added coverage anyway.,04/Mar/14 01:36;appodictic;I cleaned up the tests significantly. Without this patch triggers are only functional from batch mutation. This is a pretty small patch. The majority is tests.,11/Mar/14 11:19;beobal;This actually affects CQL batches too (due to CASSANDRA-6737),11/Mar/14 11:20;beobal;Attached patch applies to 2.0 branch.,"11/Mar/14 12:34;iamaleksey;Committed, thanks.",11/Mar/14 14:40;appodictic;Sweet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Triggers can not be added from thrift,CASSANDRA-6789,12698139,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,beobal,appodictic,appodictic,01/Mar/14 22:27,12/Mar/19 14:05,13/Mar/19 22:29,11/Mar/14 12:34,2.0.7,,,,,,0,,,,,"While playing with groovy triggers, I determined that you can not add triggers from thrift, unless I am doing something wrong. (I see no coverage of this feature from thrift/python)

https://github.com/edwardcapriolo/cassandra/compare/trigger_coverage?expand=1

{code}
package org.apache.cassandra.triggers;


import java.io.IOException;
import java.net.InetSocketAddress;
import java.nio.ByteBuffer;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import junit.framework.Assert;

import org.apache.cassandra.SchemaLoader;
import org.apache.cassandra.config.Schema;
import org.apache.cassandra.service.EmbeddedCassandraService;
import org.apache.cassandra.thrift.CassandraServer;
import org.apache.cassandra.thrift.CfDef;
import org.apache.cassandra.thrift.ColumnParent;
import org.apache.cassandra.thrift.KsDef;
import org.apache.cassandra.thrift.ThriftSessionManager;
import org.apache.cassandra.thrift.TriggerDef;
import org.apache.cassandra.utils.ByteBufferUtil;
import org.apache.thrift.TException;
import org.junit.BeforeClass;
import org.junit.Test;

public class TriggerTest extends SchemaLoader
{
    private static CassandraServer server;
    
    @BeforeClass
    public static void setup() throws IOException, TException
    {
        Schema.instance.clear(); // Schema are now written on disk and will be reloaded
        new EmbeddedCassandraService().start();
        ThriftSessionManager.instance.setCurrentSocket(new InetSocketAddress(9160));
        server = new CassandraServer();
        server.set_keyspace(""Keyspace1"");
    }
    
    @Test
    public void createATrigger() throws TException
    {
        TriggerDef td = new TriggerDef();
        td.setName(""gimme5"");
        Map<String,String> options = new HashMap<>();
        options.put(""class"", ""org.apache.cassandra.triggers.ITriggerImpl"");
        td.setOptions(options);
        CfDef cfDef = new CfDef();
        cfDef.setKeyspace(""Keyspace1"");
        cfDef.setTriggers(Arrays.asList(td));
        cfDef.setName(""triggercf"");
        server.system_add_column_family(cfDef);
        
        KsDef keyspace1 = server.describe_keyspace(""Keyspace1"");
        CfDef triggerCf = null;
        for (CfDef cfs :keyspace1.cf_defs){
          if (cfs.getName().equals(""triggercf"")){
            triggerCf=cfs;
          }
        }
        Assert.assertNotNull(triggerCf);
        Assert.assertEquals(1, triggerCf.getTriggers().size());
    }
}
{code}

junit.framework.AssertionFailedError: expected:<1> but was:<0>

",,,,,,,,,,,,,,CASSANDRA-6790,,,,,,,,,,11/Mar/14 11:34;beobal;0001-Include-trigger-defs-in-CFMetaData.toSchema.patch;https://issues.apache.org/jira/secure/attachment/12633896/0001-Include-trigger-defs-in-CFMetaData.toSchema.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-11 11:34:11.077,,,no_permission,,,,,,,,,,,,376608,,,Tue Mar 11 12:34:10 UTC 2014,,,,,,0|i1swa7:,376903,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,04/Mar/14 01:09;appodictic;My mistake. This works fine. I have some testing for this in 6790 which is still a valid bug,04/Mar/14 01:27;appodictic;Now I am waffling. Issue does exist.,11/Mar/14 11:34;beobal;Attaching patch against 2.0 branch,"11/Mar/14 12:34;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in Hadoop Word count example,CASSANDRA-6793,12698188,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,chander,chander,chander,02/Mar/14 13:40,12/Mar/19 14:05,13/Mar/19 22:29,17/Mar/14 21:48,2.0.7,2.1 beta2,,,,,0,hadoop,,,,"The partition keys requested in WordCount.java do not match the primary key set up in the table output_words. It looks this patch was not merged properly from [CASSANDRA-5622|https://issues.apache.org/jira/browse/CASSANDRA-5622].The attached patch addresses the NPE and uses the correct keys defined in #5622.

I am assuming there is no need to fix the actual NPE like throwing an InvalidRequestException back to user to fix the partition keys, as it would be trivial to get the same from the TableMetadata using the driver API.

java.lang.NullPointerException
	at org.apache.cassandra.dht.Murmur3Partitioner.getToken(Murmur3Partitioner.java:92)
	at org.apache.cassandra.dht.Murmur3Partitioner.getToken(Murmur3Partitioner.java:40)
	at org.apache.cassandra.client.RingCache.getRange(RingCache.java:117)
	at org.apache.cassandra.hadoop.cql3.CqlRecordWriter.write(CqlRecordWriter.java:163)
	at org.apache.cassandra.hadoop.cql3.CqlRecordWriter.write(CqlRecordWriter.java:63)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:587)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at WordCount$ReducerToCassandra.reduce(Unknown Source)
	at WordCount$ReducerToCassandra.reduce(Unknown Source)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:260)",,,,,,,,,,,,,,,,,,,,,,,,13/Mar/14 08:49;chander;trunk-6793-v2.txt;https://issues.apache.org/jira/secure/attachment/12634400/trunk-6793-v2.txt,15/Mar/14 10:33;chander;trunk-6793-v3.txt;https://issues.apache.org/jira/secure/attachment/12634910/trunk-6793-v3.txt,02/Mar/14 13:42;chander;trunk-6793.txt;https://issues.apache.org/jira/secure/attachment/12632012/trunk-6793.txt,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-03-11 17:50:38.021,,,no_permission,,,,,,,,,,,,376657,,,Mon Mar 17 21:48:27 UTC 2014,,,,,,0|i1swl3:,376952,2.0.0,2.1 beta1,,,,,,alexliu68,alexliu68,,,,,,,,,,"11/Mar/14 17:50;jbellis;I confess that I'm mystified by the schema introduced in CASSANDRA-4421:

{noformat}
/**
 * This counts the occurrences of words in ColumnFamily
 *   cql3_worldcount ( user_id text,
 *                   category_id text,
 *                   sub_category_id text,
 *                   title  text,
 *                   body  text,
 *                   PRIMARY KEY (user_id, category_id, sub_category_id))
 *
 * For each word, we output the total number of occurrences across all body texts.
 *
 * When outputting to Cassandra, we write the word counts to column family
 *  output_words ( row_id1 text,
 *                 row_id2 text,
 *                 word text,
 *                 count_num text,
 *                 PRIMARY KEY ((row_id1, row_id2), word))
 * as a {word, count} to columns: word, count_num with a row key of ""word sum""
 */
{noformat}

Both the input and output tables look far more complex than necessary.  

My preferred solution would be to just strip the output down to {{(word text primary key, count int)}}, and make a similar simplification for the input.

Can you shed any light [~alexliu68]?","11/Mar/14 18:38;alexliu68;(word text primary key, count int), and make a similar simplification for the input.
---------------------
This should work.

Original implementation is to show how to use composite primary key, so it has PRIMARY KEY ((row_id1, row_id2), word)","11/Mar/14 20:09;jbellis;IMO we should come up with a separate example for that, otherwise people are going to get the wrong idea since word count really shouldn't be that complicated.","13/Mar/14 08:49;chander; patch v2 addresses the following:
 
    * Simplify the schema for both input and output tables. Traditionally word count example uses a line as input, so input can just be changed to {noformat} ( id uuid, line text, PRIMARY KEY (id)) {noformat} .  Removed category, sub category, and title from input table and using UUID instead and a line to represent a line of text. Changed output schema to {noformat}(word text primary key, count int) {noformat} as suggested earlier.
    * Remove toString method and printing as it adds to unnecessary clutter in the mapper.
    * Remove the filter clauses as its not relevant to the word count example

However I still see thrift interfaces in WordCountSetup class and runtime dependencies on cassandra in the bin folder. I didn't go into details, but can someone shed some light on the importance of having this. I  think having clearly defined client API's/dependencies will be useful for the end user.",14/Mar/14 18:27;jbellis;Can you review [~alexliu68]?,"14/Mar/14 19:04;alexliu68;Hey, I ran the example. I got the file permission issue, I change it to executable and test it . The final result looks wrong

{code}
more /tmp/word_count_counters/part-r-00000 
total_count     6400
total_count     11600
{code}

There are two total_counts","15/Mar/14 10:33;chander;attached version 3, which adds a reducer to WordCountCounter class. The original example always mapped the input keys to a single node, so this was never needed before. ",17/Mar/14 16:53;alexliu68;+1 except for change bin/* file permission to executable.,17/Mar/14 21:48;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2.1-beta1 missing cassandra-driver-core jar in -bin.tar.gz,CASSANDRA-6762,12697040,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mshuler,mshuler,mshuler,25/Feb/14 00:58,12/Mar/19 14:05,13/Mar/19 22:29,25/Feb/14 21:55,2.1 beta2,,,Packaging,,,0,qa-resolved,,,,"{noformat}
(cassandra-2.1)mshuler@hana:~/git/cassandra$ tar tzvf build/apache-cassandra-2.1.0-beta1-SNAPSHOT-bin.tar.gz |grep 'tools/lib'
drwxr-xr-x 0/0               0 2014-02-24 18:53 apache-cassandra-2.1.0-beta1-SNAPSHOT/tools/lib/
-rw-r--r-- 0/0          257908 2014-02-24 18:53 apache-cassandra-2.1.0-beta1-SNAPSHOT/tools/lib/stress.jar
(cassandra-2.1)mshuler@hana:~/git/cassandra$ ls -l tools/lib/
total 504
-rw-r--r-- 1 mshuler mshuler 515357 Feb 24 17:14 cassandra-driver-core-2.0.0-rc3-SNAPSHOT.jar
(cassandra-2.1)mshuler@hana:~/git/cassandra$
{noformat}

It looks like once this gets copied from tools/lib/ to build/tools/lib it should be included in the tar.gz (correct me, if that's wrong)",,,,,,,,,,,,,,,,,,,,,,,,25/Feb/14 21:49;mshuler;0001-Add-cassandra-driver-core-jar-to-artifacts-and-deb.patch;https://issues.apache.org/jira/secure/attachment/12631057/0001-Add-cassandra-driver-core-jar-to-artifacts-and-deb.patch,25/Feb/14 01:07;mshuler;debian_stress_jars.diff;https://issues.apache.org/jira/secure/attachment/12630850/debian_stress_jars.diff,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-02-25 16:48:07.696,,,no_permission,,,,,,,,,,,,375515,,,Tue Feb 25 21:55:16 UTC 2014,,,,,,0|i1spk7:,375811,,,,,,,,brandon.williams,brandon.williams,,,2.1 beta1,,,,,,,25/Feb/14 01:07;mshuler;After java driver jar is in build/tools/lib/ the debian packaging should install both the stress.jar and driver jar; patch attached.,25/Feb/14 16:48;brandon.williams;Committed.,25/Feb/14 19:42;mshuler;Needs copy to build/tools/lib/,"25/Feb/14 20:47;mshuler;The artifacts target looks like the right place to add this, to me. Included a patch for just the build.xml addition.","25/Feb/14 21:48;mshuler;Removed my previous patch, as it's included in this one.

Since previous debian patch was committed, this patch goes back to specifying stress.jar, and adds a dh_install line for any jars in tools/lib/.  The artifacts jar copy to the -bin.tar.gz is also in this patch.
","25/Feb/14 21:55;brandon.williams;LGTM, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BooleanType is not too boolean,CASSANDRA-6779,12697677,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,27/Feb/14 12:57,12/Mar/19 14:05,13/Mar/19 22:29,12/Mar/14 21:45,2.0.7,2.1 beta2,,,,,0,,,,,"The BooleanType validator accepts any byte (it only checks it's one byte long) and the comparator actually uses the ByteBuffer.compareTo() method on it's input. So that BooleanType is really ByteType and accepts 256 values.

Note that in practice, it's likely no-one or almost no-one has ever used BooleanType as a comparator, and almost surely the handful that might have done it have stick to sending only 0 for false and 1 for true. Still, it's probably worth fixing before it actually hurt someone. ",,,,,,,,,,,,,,,,,,,,,,,,27/Feb/14 13:01;slebresne;6779.txt;https://issues.apache.org/jira/secure/attachment/12631522/6779.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-11 20:35:12.957,,,no_permission,,,,,,,,,,,,376151,,,Wed Mar 12 21:45:29 UTC 2014,,,,,,0|i1stgv:,376447,,,,,,,,thobbs,thobbs,,,,,,,,,,"27/Feb/14 13:01;slebresne;I'll note that we could either fix compare or fix validation to only accept 0 or 1, not any byte, but I suggest fixing compare because fixing validation could have a slightly bigger chance of breaking someone.",11/Mar/14 20:35;jbellis;([~thobbs] to review),"12/Mar/14 21:45;thobbs;+1, committed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2.1 w/java-driver 2.0 and stress write using thrift results in ArithmeticException / by zero errors,CASSANDRA-6777,12697588,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,djatnieks,djatnieks,djatnieks,27/Feb/14 03:43,12/Mar/19 14:05,13/Mar/19 22:29,04/Mar/14 11:10,2.1 rc2,,,Legacy/Tools,,,0,stress,,,,"Running stress write (thrift) on 2.1 branch is resulting in the following.

Note: this is after working around [JAVA-276|https://datastax-oss.atlassian.net/browse/JAVA-276] causing stress to fail to connect to 2.1.

{noformat}
$ ./tools/bin/cassandra-stress write n=5000000

Unable to create stress keyspace: Keyspace names must be case-insensitively unique (""Keyspace1"" conflicts with ""Keyspace1"")
Warming up WRITE with 50000 iterations...
Connected to cluster: Test Cluster
Datatacenter: datacenter1; Host: localhost/127.0.0.1; Rack: rack1
java.lang.ArithmeticException: / by zero
java.lang.ArithmeticException: / by zero
java.lang.ArithmeticException: / by zero
java.lang.ArithmeticException: / by zero
...
java.lang.Arithmjava.io.IOException: Operation [220] x10 key 00000000DD Error executing: (ArithmeticException): / by zero
eticException: / by zero

java.lang.ArithmeticException: / by zero
	at org.apache.cassandra.stress.Operation.error(Operation.java:237)
java.lang.ArithmeticException: / by zero	at org.apache.cassandra.stress.Operation.timeWithRetry(Operation.java:216)

	at org.apache.cassandra.stress.operations.ThriftInserter.run(ThriftInserter.java:72)
	at org.apache.cassandra.stress.StressAction$Consumer.run(StressAction.java:302)java.lang.ArithmeticException: / by zero

java.lang.ArithmeticException: / by zero
java.io.IOException: Operation [200] x10 key 00000000C9 Error executing: (ArithmeticException): / by zero

	at org.apache.cassandra.stress.Operation.error(Operation.java:237)
java.lang.ArithmeticException: / by zero	at org.apache.cassandra.stress.Operation.timeWithRetry(Operation.java:216)

java.lang.ArithmeticException: / by zero	at org.apache.cassandra.stress.operations.ThriftInserter.run(ThriftInserter.java:72)

	at org.apache.cassandra.stress.StressAction$Consumer.run(StressAction.java:302)
...
{noformat}

Seems to be just a thrift issue, as running stress write using the native protocol works:

{noformat}
$ ./tools/bin/cassandra-stress write n=5000000 -mode native cql3

Unable to create stress keyspace: Keyspace names must be case-insensitively unique (""Keyspace1"" conflicts with ""Keyspace1"")
Warming up WRITE with 50000 iterations...
Connected to cluster: Test Cluster
Datatacenter: datacenter1; Host: localhost/127.0.0.1; Rack: rack1
Sleeping 2s...
Running WRITE with 50 threads  for 5000000 iterations
ops       ,    op/s,adj op/s,   key/s,    mean,     med,     .95,     .99,    .999,     max,   time,   stderr
29342     ,   29340,   30903,   29340,     1.7,     1.4,     3.1,     5.7,    51.5,    54.4,    1.0,  0.00000
56353     ,   26968,   28523,   26968,     1.8,     1.5,     3.6,     7.0,    56.5,    57.2,    2.0,  0.00000
...
5000000   ,   29358,   29358,   29358,     1.7,     1.3,     3.6,     8.4,    10.1,    11.6,  168.8,  0.00828


Results:
real op rate              : 29620
adjusted op rate          : 29629
adjusted op rate stderr   : 0
key rate                  : 29620
latency mean              : 1.7
latency median            : 1.4
latency 95th percentile   : 3.0
latency 99th percentile   : 5.4
latency 99.9th percentile : 56.5
latency max               : 305.7
Total operation time      : 00:02:48
END

{noformat}

Attaching stress write and system logs as well.
","Mac OSX, java 1.7.0_51",,,,,,,,,,,,,,,,,,,,,,,27/Feb/14 23:23;djatnieks;Quote_keyspace_name.patch;https://issues.apache.org/jira/secure/attachment/12631620/Quote_keyspace_name.patch,27/Feb/14 03:43;djatnieks;logs.tar.gz;https://issues.apache.org/jira/secure/attachment/12631447/logs.tar.gz,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-03-04 11:10:33.125,,,no_permission,,,,,,,,,,,,376062,,,Tue Mar 04 11:10:33 UTC 2014,,,,,,0|i1ssxj:,376358,,,,,,,,slebresne,slebresne,,,2.1 rc3,,,,,,,"27/Feb/14 05:08;djatnieks;Ah. To get around JAVA-276, I had dropped in a modified java driver that could parse the Cassandra version ""2.1.0-beta1-SNAPSHOT"". However doing that on the head of the java driver 2.0 branch is somehow causing these ArithmeticException / by zero errors. 

Restoring the {{cassandra-driver-core-2.0.0-rc3-SNAPSHOT.jar}} in tools/lib and instead building C* with {{ant -Dbase.version=2.1.0}} works around JAVA-276 and doesn't cause any problems for stress.

So closing this issue as it seems to be caused by java-driver.
","27/Feb/14 06:04;djatnieks;A recent change to java-driver (revision f6a179d, for [JAVA-269|https://datastax-oss.atlassian.net/browse/JAVA-269]) caused this.

In stress, {{SmartThriftClient#get()}} gets a set of hosts by calling {{metadata.getReplicas(keyspace, pk)}}.

However, in java-driver, {{Metadata#getReplicas}} has changed and now calls {{handleId(keyspace)}} which essentially forces values to lowercase unless they are quoted.

This likely means the caller of {{Metadata#getReplicas}} is responsible to first quote case-sensitive keyspace names, such as the default ""Keyspace1"" used by stress, e.g. {{metadata.getReplicas(metadata.quote(keyspace), pk)}}
","27/Feb/14 06:21;djatnieks;Re-opening as there does appear to be something to fix in stress to make it work with java-driver 2.0.

At the moment the 2.1 branch is using {{cassandra-driver-core-2.0.0-rc3}} but I think when this is updated to the release version of 2.0 this error will occur.

See my previous comment for the proposed stress fix to quote the keyspace name when calling java-driver {{Metadata#getReplicas}}.
","27/Feb/14 23:23;djatnieks;Attaching patch that should work with released version of java-driver 2.0.

Note: this also fixes a minor issue with using the {{\-schema ks=}} option.","04/Mar/14 11:10;slebresne;Patch lgtm, committed along with an update of the java driver jar from rc3 to 2.0 final.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
File handle leak in StreamWriter.java,CASSANDRA-6832,12699834,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,JoshuaMcKenzie,JoshuaMcKenzie,JoshuaMcKenzie,10/Mar/14 18:43,12/Mar/19 14:05,13/Mar/19 22:29,13/Mar/14 18:14,2.0.7,,,Legacy/Local Write-Read Paths,,,0,,,,,"Reference CASSANDRA-6283 where this first came up.  nodetool.bat repair -par on 2.0.5 pops up the following stack:

ERROR [Finalizer] 2014-02-17 09:21:52,922 RandomAccessReader.java (line 399) LEAK finalizer had to clean up
java.lang.Exception: RAR for C:\var\lib\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-jb-41-CRC.db allocated
        at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:66)
        at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:106)
        at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:98)
        at org.apache.cassandra.io.util.DataIntegrityMetadata$ChecksumValidator.<init>(DataIntegrityMetadata.java:53)
        at org.apache.cassandra.io.util.DataIntegrityMetadata.checksumValidator(DataIntegrityMetadata.java:40)
        at org.apache.cassandra.streaming.StreamWriter.write(StreamWriter.java:76)
        at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:59)
        at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:42)
        at org.apache.cassandra.streaming.messages.StreamMessage.serialize(StreamMessage.java:45)
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.sendMessage(ConnectionHandler.java:383)
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.run(ConnectionHandler.java:355)
        at java.lang.Thread.run(Thread.java:744)

This leak doesn't look like it's breaking anything but is still worth fixing.","Windows, 2.0.5, leakdetect patch",,,,,,,,,,,,,,,,,,,,,,,10/Mar/14 18:45;JoshuaMcKenzie;6832_v1.patch;https://issues.apache.org/jira/secure/attachment/12633747/6832_v1.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-13 18:14:59.51,,,no_permission,,,,,,,,,,,,378180,,,Thu Mar 13 18:23:55 UTC 2014,,,,,,0|i1t5xr:,378472,2.0.5,,,,,,,jbellis,jbellis,,,,,,,,,,10/Mar/14 18:54;JoshuaMcKenzie;Attaching patch - this came up during testing on CASSANDRA-6283 and there's no sense having this fix blocked on that.,13/Mar/14 18:14;jbellis;committed to 2.0.7 (assuming it doesn't affect 1.2),"13/Mar/14 18:23;JoshuaMcKenzie;It's part of ""streaming 2.0"" so we should be clear.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The binary transport doesn't load truststore file,CASSANDRA-6847,12701086,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mishail,mishail,mishail,12/Mar/14 21:00,12/Mar/19 14:05,13/Mar/19 22:29,13/Mar/14 20:09,1.2.16,2.0.7,2.1 beta2,,,,0,ssl,,,,"{code:title=org.apache.cassandra.transport.Server.SecurePipelineFactory}
this.sslContext = SSLFactory.createSSLContext(encryptionOptions, false);
{code}

{{false}} there means that {{truststore}} file won't be loaded in any case. 
And that means that the file will not be used to validate clients when {{require_client_auth==true}}, making http://www.datastax.com/documentation/cassandra/2.0/cassandra/security/secureNewTrustedUsers_t.html meaningless.

The only way to workaround that currently is to start C* with {{-Djavax.net.ssl.trustStore=conf/.truststore}}

I believe we should load  {{truststore}} when {{require_client_auth==true}},
",,,,,,,,,,,,,,,,,,,,,,,,13/Mar/14 18:29;mishail;cassandra-2.0-6847.patch;https://issues.apache.org/jira/secure/attachment/12634504/cassandra-2.0-6847.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-13 06:34:57.264,,,no_permission,,,,,,,,,,,,379432,,,Mon Apr 21 07:48:28 UTC 2014,,,,,,0|i1tdn3:,379723,2.0.7,2.1 beta1,,,,,,slebresne,slebresne,,,,,,,,,,"12/Mar/14 23:52;mishail;[~slebresne] [~jasobrown] what do you think guys?

I suppose we should consider {{require_client_auth}} as well when deciding to load/don't load a truststore.","13/Mar/14 06:34;jasobrown;Beyond the discussion in the original ticket, I have no recollection of why we bailed on the trust store for the native protocol. We quite clearly do it in thrift: CustomTThreadPoolServer.Factory.buildTServer() passes true as the second arg to SSLFactory.createSSLContext(), so I'm not sure why we left it out of the native protocol.

Barring any further insight from [~slebresne], I think we should go ahead and change this to respect require_client_auth.","13/Mar/14 08:22;slebresne;bq.  I have no recollection of why we bailed on the trust store for the native protocol.

I suspect that this has to do with the fact that CASSANDRA-5120, which adds the require_client_auth for client connections, is newer than CASSANDRA-5031.",13/Mar/14 18:29;mishail;Attaching a trivial patch,"13/Mar/14 18:53;jasobrown;Yup, that was gonna be my patch :). +1",13/Mar/14 20:09;mishail;Committed,"17/Apr/14 14:26;jjordan;Can we put a line in changes.txt for this?  I spent 2 days pulling my hair out from this one, and yes I probably should have done a full JIRA search, but I would expect ""require_client_auth"" being completely broken to show up in changes.txt :/","21/Apr/14 07:48;slebresne;bq. Can we put a line in changes.txt for this?

Just did, but it won't be retroactive so probably mildly useful only.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cross-partition ordering should have warning or be disallowed when paging,CASSANDRA-6722,12695794,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,rhatch,rhatch,18/Feb/14 21:29,12/Mar/19 14:05,13/Mar/19 22:29,21/Feb/14 13:07,2.0.6,,,,,,0,,,,,"consider this schema/data/query:
{noformat}
CREATE TABLE paging_test (
    id int,
    value text,
    PRIMARY KEY (id, value)
) WITH CLUSTERING ORDER BY (value ASC)

            |id|value|
            |1 |a    |
            |2 |b    |
            |1 |c    |
            |2 |d    | 
            |1 |e    | 
            |2 |f    | 
            |1 |g    | 
            |2 |h    |
            |1 |i    |
            |2 |j    |

select * from paging_test where id in (1,2) order by value asc;
{noformat}
When paging the above query I get the sorted results from id=1 first, then the sorted results from id=2 after that. I was testing this because I was curious if the paging system could somehow globally sort the results but it makes sense that we can't do that, since that would require all results to be collated up front.",,,,,,,,,,,,,,,,,,,,,,,,19/Feb/14 15:34;slebresne;6722.txt;https://issues.apache.org/jira/secure/attachment/12629787/6722.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-19 15:34:24.503,,,no_permission,,,,,,,,,,,,374301,,,Fri Feb 21 13:07:51 UTC 2014,,,,,,0|i1si3b:,374601,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"19/Feb/14 15:34;slebresne;Good catch. We can't indeed properly do post-query reordering in general if we page. Note that currently, there is 2 cases where we do post-query reordering:
# with a IN on the partition key and an ORDER BY (the example above).
# if there is an IN on the last clustering column of a compact table.

The 2nd case is actually a bit weird. The reason we order post-query is so the resultset follows the order of the IN in the query, i.e. if you have the table above but COMPACT and do
{noformat}
SELECT value FROM paging_test WHERE id=1 AND value IN ('b', 'a', 'c')
{noformat}
you will get [ 'b', 'a', 'c' ] in that order as a result set. So far, why not, but there is 2 problems in practice:
* we only do that for compact tables. For non-compact ones, we just return results in clustering order (we don't do the post-query ordering). That inconsistency is an historical accident.
* this actually take precedence over ORDER BY, which is completely broken. I.e. even if you add 'ORDER BY value' in the query above, the results will not be properly ordered.
* this is even more broken than that: CASSANDRA-6701.
This case is a mess. So because it's a problem in the context of this ticket and because there is no reason to guarantee any special ordering of the result set if there is no ORDER BY, I suggest we just remove the behavior for compact storage (it was an implementation detail so far) to make it in line with the non-compact case, thus avoiding us to have to deal with it here.

Back to the more interesting case of the example in the description: a IN on the partition key with ORDER BY. In that case, we could almost support paging properly: the reason it's broken is that the pager queries partitions of the IN one after the other.  But the pager could, in theory, page over each partition simultaneously, querying them all little by little and doing a merge sort.

It is however not all that easy in practice. If you query a full page of each partition and there is many partitions in the IN, you'll load tons of data in memory, defeating in large parts the goal of paging. If you instead query less than the page size of each partition, you now may need to re-query some of the partitions depending on what the merge sort yield on those first pages. Not only would that require serious refactor of the current code to properly handle, but it's rather unclear how efficient this would really be in general. I'm not sure it's really worth it in the end. 

But in any case, such solution is way out of scope for 2.0 (and probably even for 2.1 at this point). So we need a quick solution for now and it probably mean that we should throw an IRE if the query requires post-query paging and paging is on.

Taking a step back, I wonder if allowing post-query reordering by default was a good idea, and if the same way we have ALLOW FILTERING, we shouldn't have required an ALLOW IN-MEMORY REORDERING for such queries. Of course, it's harder to change that now, but maybe we could still do it by adding such flag, deprecate queries that don't use it by logging a warning like we've done in CASSANDRA-6649 for 1 or 2 versions, and forbid it completely afterwards?
","19/Feb/14 15:34;slebresne;Attaching a patch that does 2 things:
# remove the ordering of results for compact tables when a IN is on the last clustering column (see discussion above) so the only case where we do post-query reordering is with IN on partition key + ORDER BY.
# throw IRE if the query needs post-query reordering and paging is on.
","20/Feb/14 20:39;iamaleksey;LGTM, +1.","21/Feb/14 13:07;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException in commit-log-writer after local schema reset,CASSANDRA-6841,12700930,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,benedict,pasthelod,pasthelod,12/Mar/14 10:50,12/Mar/19 14:05,13/Mar/19 22:29,22/Apr/14 14:18,1.2.17,2.0.7,2.1 beta2,,,,0,,,,,"{code}
 INFO [RMI TCP Connection(38)-192.168.36.171] 2014-03-12 11:37:54,013 MigrationManager.java (line 329) Starting local schema reset...
 INFO [RMI TCP Connection(38)-192.168.36.171] 2014-03-12 11:37:54,016 ColumnFamilyStore.java (line 785) Enqueuing flush of Memtable-local@394448776(114/1140 serialized/live bytes, 3 ops)
 INFO [FlushWriter:6] 2014-03-12 11:37:54,016 Memtable.java (line 331) Writing Memtable-local@394448776(114/1140 serialized/live bytes, 3 ops)
 INFO [FlushWriter:6] 2014-03-12 11:37:54,182 Memtable.java (line 371) Completed flushing /var/lib/cassandra/data/system/local/system-local-jb-398-Data.db (145 bytes) for commitlog position ReplayPosition(segmentId=1394620057452, position=33159822)
 INFO [RMI TCP Connection(38)-192.168.36.171] 2014-03-12 11:37:54,185 ColumnFamilyStore.java (line 785) Enqueuing flush of Memtable-local@1087210140(62/620 serialized/live bytes, 1 ops)
 INFO [FlushWriter:6] 2014-03-12 11:37:54,185 Memtable.java (line 331) Writing Memtable-local@1087210140(62/620 serialized/live bytes, 1 ops)
 INFO [FlushWriter:6] 2014-03-12 11:37:54,357 Memtable.java (line 371) Completed flushing /var/lib/cassandra/data/system/local/system-local-jb-399-Data.db (96 bytes) for commitlog position ReplayPosition(segmentId=1394620057452, position=33159959)
 INFO [RMI TCP Connection(38)-192.168.36.171] 2014-03-12 11:37:54,361 ColumnFamilyStore.java (line 785) Enqueuing flush of Memtable-local@768887091(62/620 serialized/live bytes, 1 ops)
 INFO [FlushWriter:6] 2014-03-12 11:37:54,361 Memtable.java (line 331) Writing Memtable-local@768887091(62/620 serialized/live bytes, 1 ops)
 INFO [FlushWriter:6] 2014-03-12 11:37:54,516 Memtable.java (line 371) Completed flushing /var/lib/cassandra/data/system/local/system-local-jb-400-Data.db (96 bytes) for commitlog position ReplayPosition(segmentId=1394620057452, position=33160096)
 INFO [CompactionExecutor:38] 2014-03-12 11:37:54,517 CompactionTask.java (line 115) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-jb-398-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-jb-400-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-jb-399-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-jb-397-Data.db')]
 INFO [RMI TCP Connection(38)-192.168.36.171] 2014-03-12 11:37:54,519 ColumnFamilyStore.java (line 785) Enqueuing flush of Memtable-local@271993477(62/620 serialized/live bytes, 1 ops)
 INFO [FlushWriter:6] 2014-03-12 11:37:54,519 Memtable.java (line 331) Writing Memtable-local@271993477(62/620 serialized/live bytes, 1 ops)
 INFO [FlushWriter:6] 2014-03-12 11:37:54,794 Memtable.java (line 371) Completed flushing /var/lib/cassandra/data/system/local/system-local-jb-401-Data.db (96 bytes) for commitlog position ReplayPosition(segmentId=1394620057452, position=33160233)
 INFO [RMI TCP Connection(38)-192.168.36.171] 2014-03-12 11:37:54,799 MigrationManager.java (line 357) Local schema reset is complete.
 INFO [CompactionExecutor:38] 2014-03-12 11:37:54,848 CompactionTask.java (line 275) Compacted 4 sstables to [/var/lib/cassandra/data/system/local/system-local-jb-402,].  6,099 bytes to 5,821 (~95% of original) in 330ms = 0.016822MB/s.  4 total partitions merged to 1.  Partition merge counts were {4:1, }
 INFO [OptionalTasks:1] 2014-03-12 11:37:55,110 ColumnFamilyStore.java (line 785) Enqueuing flush of Memtable-schema_columnfamilies@106276050(181506/509164 serialized/live bytes, 3276 ops)
 INFO [FlushWriter:6] 2014-03-12 11:37:55,110 Memtable.java (line 331) Writing Memtable-schema_columnfamilies@106276050(181506/509164 serialized/live bytes, 3276 ops)
 INFO [OptionalTasks:1] 2014-03-12 11:37:55,110 ColumnFamilyStore.java (line 785) Enqueuing flush of Memtable-schema_columns@252242773(185191/630698 serialized/live bytes, 3614 ops)
ERROR [COMMIT-LOG-WRITER] 2014-03-12 11:37:55,111 CassandraDaemon.java (line 196) Exception in thread Thread[COMMIT-LOG-WRITER,5,main]
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)
        at java.util.HashMap$KeyIterator.next(HashMap.java:960)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator.flushOldestKeyspaces(CommitLogAllocator.java:309)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator.fetchSegment(CommitLogAllocator.java:147)
        at org.apache.cassandra.db.commitlog.CommitLog.activateNextSegment(CommitLog.java:299)
        at org.apache.cassandra.db.commitlog.CommitLog.access$100(CommitLog.java:49)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:350)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:51)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.lang.Thread.run(Thread.java:744)

{code}","Linux 3.2.0 (Debian Wheezy) Cassandra 2.0.6, Oracle JVM 1.7.0_51

Almost default cassandra.yaml (IPs and cluster name changed)

This is the 2nd node in a 2-node ring. It has ~2500 keyspaces and very low traffic. (Only new keyspaces see reads and writes.)",,,,,,,,,,,,,,,,,,,,,,,07/Apr/14 18:54;benedict;6841.12.txt;https://issues.apache.org/jira/secure/attachment/12639038/6841.12.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-12 15:46:25.292,,,no_permission,,,,,,,,,,,,379276,,,Fri Apr 25 15:02:50 UTC 2014,,,,,,0|i1tcon:,379568,2.0.6,,,,,,,carlyeks,carlyeks,,,,,,,,,,12/Mar/14 15:46;jbellis;Can you give more details regarding how you did this?,"12/Mar/14 15:58;benedict;Without investigating _thoroughly_ to rule out anything else, I would guess the problem is with Keyspace.cfIdMap, which is not threadsafe. We should probably create it with two ConcurrentHashMap, instead of the default (plain HashMap)","12/Mar/14 17:04;pasthelod;To give more context. There was a single node ring used for running test of an application that uses, among other stores, Cassandra as a backend. It has a lot of small keyspaces (each with 1 or 2 CFs - supercolumn based), total data on the node was/is less than 300G. We are in the process of migrating away from supercolumns, so we're moving to CQL and composite primary keys and whatnot, so there are a few keyspaces already with this new schema.

Then we tried to add a new node, setting its seed property to the old one. It failed to bootstrap with errors ( https://issues.apache.org/jira/browse/CASSANDRA-6565?focusedCommentId=13931732&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13931732 ), but finally after a few tries it started and went from BOOT to NORMAL. Then we noticed that the new node has a different schema UUID (as indicated by gossipinfo), and tried to do resetlocalschema. (Which happened, according to system.log, but then we have observed no improvement, so we tried it again then the gossip stage appeared to have been deadlocked; but a restart solved it. See also https://issues.apache.org/jira/browse/CASSANDRA-6799?focusedCommentId=13931690&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13931690 )

","12/Mar/14 19:21;benedict;Patch available [here|https://github.com/belliottsmith/cassandra/tree/iss-6841]

Looking at the stack trace, it looks like this problem is actually caused by the same basic problem I found with cfIdMap, but in another spot: CLS.cfLastWrite is a regular HashMap, and markClean can be called at any time by other threads (although in a critical section, so the map won't be left in a dodgy state), so I've fixed both of these issues.

Regrettably we can't just pass a pair of ConcurrentHashMaps to HashBiMap, as the constructor is private. So I've created a new ConcurrentBiMap which synchronises on modification but uses ConcurrentHashMap so that we can permit unsynchronized access for reads. I've replaced cfIdMap with this.","12/Mar/14 20:42;carlyeks;The patch LGTM, but it might help to have the two IllegalStateExceptions in the ConcurrentBiMap.put method commented.",13/Mar/14 12:42;benedict;Updated the tree. Also converted one of those ISE to an IAE check prior to modifying the map,13/Mar/14 12:52;carlyeks;+1,14/Mar/14 18:18;jbellis;committed.  might want to doublecheck my merge to 2.1; i kept the changes to Schema but the CL changes looked irrelevant,"14/Mar/14 23:25;benedict;LGTM as far as I can tell, but a bit tricky as git diff includes some of the 2.0 patches in the 2.1 git log.

The CL changes weren't necessary for 2.1 anyway, you're right, and it looks like they're missing from 2.1 so I'm happy.",01/Apr/14 19:38;jblangston@datastax.com;Reopening to get a backport for 1.2.,"07/Apr/14 18:54;benedict;Applied mostly cleanly to 1.2

I've attached a version stripped of the ConcurrentBiMap change, as that's probably a much rarer bug and a more invasive change, so probably best to leave it for 2.x",22/Apr/14 14:18;jbellis;committed,"25/Apr/14 15:02;wtmitchell3;Although not the author, I ran into this symptom of the ConcurrentModificationException at line 309 in the CommitLogAllocator more than once this week on 2.0.6, on an 8-core Windows desktop.  After downloading and installing 2.0.7 yesterday, I have not seen this problem reappear.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential problem with GarbageCollectorMXBean,CASSANDRA-5345,12637020,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,JoshuaMcKenzie,mbyrd,mbyrd,14/Mar/13 13:45,12/Mar/19 14:05,13/Mar/19 22:29,18/Jul/14 23:15,2.0.10,2.1.1,,Legacy/Observability,,,1,,,,,"I am not certain this is definitely a bug, but I thought it might be worth posting to see if someone with more JVM//JMX knowledge could disprove my reasoning. Apologies if I've failed to understand something.

We've seen an intermittent problem where there is an uncaught exception in the scheduled task of logging gc results in GcInspector.java:

{code}
...
 ERROR [ScheduledTasks:1] 2013-03-08 01:09:06,335 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[ScheduledTasks:1,5,main]
java.lang.reflect.UndeclaredThrowableException
        at $Proxy0.getName(Unknown Source)
        at org.apache.cassandra.service.GCInspector.logGCResults(GCInspector.java:95)
        at org.apache.cassandra.service.GCInspector.access$000(GCInspector.java:41)
        at org.apache.cassandra.service.GCInspector$1.run(GCInspector.java:85)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: javax.management.InstanceNotFoundException: java.lang:name=ParNew,type=GarbageCollector
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1094)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:662)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
        at com.sun.jmx.mbeanserver.MXBeanProxy$GetHandler.invoke(MXBeanProxy.java:106)
        at com.sun.jmx.mbeanserver.MXBeanProxy.invoke(MXBeanProxy.java:148)
        at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:248)
        ... 13 more
...
{code}

I think the problem, may be caused by the following reasoning:

In GcInspector we populate a list of mxbeans when the GcInspector instance is instantiated:

{code}
...
List<GarbageCollectorMXBean> beans = new ArrayList<GarbageCollectorMXBean>();
        MBeanServer server = ManagementFactory.getPlatformMBeanServer();
        try
        {
            ObjectName gcName = new ObjectName(ManagementFactory.GARBAGE_COLLECTOR_MXBEAN_DOMAIN_TYPE + "",*"");
            for (ObjectName name : server.queryNames(gcName, null))
            {
                GarbageCollectorMXBean gc = ManagementFactory.newPlatformMXBeanProxy(server, name.getCanonicalName(), GarbageCollectorMXBean.class);
                beans.add(gc);
            }
        }
        catch (Exception e)
        {
            throw new RuntimeException(e);
        }
...
{code}

Cassandra then periodically calls:

{code}
...
    private void logGCResults()
    {
        for (GarbageCollectorMXBean gc : beans)
        {
            Long previousTotal = gctimes.get(gc.getName());
...
{code}

In the oracle javadocs, they seem to suggest that these beans could disappear at any time.(I'm not sure why when or how this might happen)
http://docs.oracle.com/javase/6/docs/api/
See: getGarbageCollectorMXBeans

{code}
...
public static List<GarbageCollectorMXBean> getGarbageCollectorMXBeans()
Returns a list of GarbageCollectorMXBean objects in the Java virtual machine. The Java virtual machine may have one or more GarbageCollectorMXBean objects. It may add or remove GarbageCollectorMXBean during execution.
Returns:
a list of GarbageCollectorMXBean objects.
...
{code}

Correct me if I'm wrong, but do you think this might be causing the problem? That somehow the JVM decides to remove the GarbageCollectorMXBean temporarily or permanently (causing said exception) and if this is expected behaviour, should it be handled in some way?
Also I'd like to point out that this may be an issue on other versions as well as I don't believe this code has changed in quite a long time.
Unfortunately I haven't been able to reproduce this outside of the production environment, if you have any tips, questions or are able to explain//disprove my concerns, I'd be very grateful.

Thanks,
Matt","JVM:JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.6.0_30  typical 6 node 2 availability zone Mutli DC cluster on linux vms with
and mx4j-tools.jar and jna.jar both on path. Default configuration bar token setup(equispaced), sensible cassandra-topology.properties file and use of said snitch.",,,,,,,,,,,,,,,,,,,,,,,26/Jun/14 15:33;JoshuaMcKenzie;5345_v1.txt;https://issues.apache.org/jira/secure/attachment/12652626/5345_v1.txt,18/Jul/14 15:15;JoshuaMcKenzie;5345_v2.txt;https://issues.apache.org/jira/secure/attachment/12656521/5345_v2.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-03-14 19:46:54.037,,,no_permission,,,,,,,,,,,,317512,,,Fri Jul 18 23:15:20 UTC 2014,,,,,,0|i1iscf:,317853,1.0.7,,,,,,,jbellis,jbellis,,,,,,,,,,"14/Mar/13 19:46;brandon.williams;Personally, I doubt this theory, the JVM has no reason to make PN disappear, and this code has been around for a long time with no similar reports.  I think you might have a build problem.","25/Mar/14 01:11;arya;Running 1.2.14 on Ubuntu 12.04 with HotSpot Java 1.7.0_21-b11. 

For the past couple of days our nodes started to produce this exception every second. This exception started to get produced after we decommissioned one DC, and it continued to get produced until the nodes started producing client errors as we got alerts. No suspicious other log entries were found and no full GCs were recorded in the GC logs. The GC logs looked normal however system.log was getting filled with this exception. 

We decommissioned the DC by first altering our keyspace to not replication to that DC as we are using NetworkTopologyStrategy. Then we issued nodetool decommission on each node in that DC. 

Why this exception started to get produced is not known. However, restarting the nodes fixed the problem. Another observation was that the nodetool info command which we use to collect heap size statistics was not working as well and was tossing this exception:

Exception in thread ""main"" java.lang.IllegalArgumentException: javax.management.InstanceNotFoundException: java.lang:type=Memory
        at java.lang.management.ManagementFactory.newPlatformMXBeanProxy(ManagementFactory.java:610)
        at org.apache.cassandra.tools.NodeProbe.connect(NodeProbe.java:175)
        at org.apache.cassandra.tools.NodeProbe.<init>(NodeProbe.java:116)
        at org.apache.cassandra.tools.NodeCmd.main(NodeCmd.java:1138)
Caused by: javax.management.InstanceNotFoundException: java.lang:type=Memory
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.isInstanceOf(DefaultMBeanServerInterceptor.java:1401)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.isInstanceOf(JmxMBeanServer.java:1082)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1492)
        at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:96)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1327)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1419)
        at javax.management.remote.rmi.RMIConnectionImpl.isInstanceOf(RMIConnectionImpl.java:957)
        at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)","25/Mar/14 01:13;arya;Matt Byrd, please assign version and increase the priority as this did in fact bring our cluster to halt. ","25/Mar/14 03:51;brandon.williams;Ryan, can your team repro?","25/Mar/14 06:30;mbyrd;The cluster in question was running on 1.0.7, however the code in question has remained static since well before that and doesn't look to have changed since. (though admittedly the problem could somehow be being caused elsewhere, jvm maybe?) 
I've upped the priority to major.

Have you been able to reproduce? or seen the problem anywhere else?
Any further details about your environment is set up and how you deploy may also help those trying to reproduce.
Some common but perhaps co-incidental things about the two occurrences:
1. virtual machines (though not both AWS)
2. multi D.C , wouldn't have though this would be relevant but Arya does seem to see the problem after removing a d.c.
3. Slightly old Jvm versions...

I no longer have access to the cluster where we saw this previously but let me know if I can help in any other way.","25/Mar/14 19:56;arya;To give you more details about our setup:

AWS
us-east-1 3 nodes. 
us-west-2 3 nodes.
Classic EC2 hi1.4xlarge machines
NetworkTopologyStrategy us-east-1:3, us-west-2:3
Effective load on each machine is 100% about 360Gb
We run a 24Gb Heap with MaxTenuringThreshold = 20

Average read 4k/sec on each node
Average write 1.5k/sec on each node

We have a mix of SizeTiered and Leveled CFs.
We turned off read repair. 
We use mmap_index_only","25/Jun/14 21:30;JoshuaMcKenzie;[~enigmacurry] - did you or your team have any luck reproducing this?

It should be trivial to throw a gc.isValid() check in the logGCResults loop and if invalid, flag to rebuild the List<GarbageCollectorMXBean> we're iterating across in that function as well as to introduce some exception handling for the UndeclaredThrowableException.  I'm not finding much on the logic behind *when* these MXBeans can be removed from the system and I agree with Brandon that it seems incredibly odd for a JVM to punt and re-init a garbage collector MXBean on the fly with such infrequency that we haven't seen this more often.

The fact that Arya had a nodetool failure connecting to the Memory subsystem:
{code:title=Memory failure}
Exception in thread ""main"" java.lang.IllegalArgumentException: javax.management.InstanceNotFoundException: java.lang:type=Memory
at java.lang.management.ManagementFactory.newPlatformMXBeanProxy(ManagementFactory.java:610)
at org.apache.cassandra.tools.NodeProbe.connect(NodeProbe.java:175)
{code}
is concerning as it would seem to imply some deeper problems w/the JMX integration of the Memory subsystem on these JVM's given that we're querying the factory by String there rather than trying to use an invalid reference.","25/Jun/14 21:51;enigmacurry;[~JoshuaMcKenzie] no, I haven't reproduced it. :(","26/Jun/14 15:33;JoshuaMcKenzie;Attaching a v1 that tries to gracefully check for GC MXBean validity on log and if an invalid GC is found, skip it and rebuild the list of cached MXBean's for next logging.

Given that I can't find any info on *when* these things are getting recycled, I also put in some exception handling in-case the change happens in the middle of our logging process.

This doesn't address the failure of nodetool info that Arya saw with nodetool info; that issue seems like a related but perhaps different (and less critical) effort than this ticket.",17/Jul/14 21:59;jbellis;I couldn't get this to apply to the repo as of Jun 26.  Can you rebase against current 2.0 branch?,18/Jul/14 15:15;JoshuaMcKenzie;rebased against cassandra-2.0,18/Jul/14 23:15;jbellis;Added a comment pointing to this issue and committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Collection values size is not validated.,CASSANDRA-5355,12637512,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,18/Mar/13 10:46,12/Mar/19 14:05,13/Mar/19 22:29,21/Mar/13 12:58,1.2.4,,,,,,0,,,,,"Collections values are currently limited to 64K because the serialized form used uses shorts to encode the elements length (and for sets elements and key map, because they are part of the internal column name that is itself limited to 64K).

However, there is no check on the collection elements size currently so we don't refuse values > 64K (except for sets elements and map keys because we check internal column name sizes), even though they can't be decoded correctly client side.",,,,,,,,,,,,,,,,,,,,,,,,21/Mar/13 11:24;slebresne;5355.txt;https://issues.apache.org/jira/secure/attachment/12574776/5355.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-18 13:30:24.33,,,no_permission,,,,,,,,,,,,318003,,,Thu Mar 21 12:58:27 UTC 2013,,,,,,0|i1ivdj:,318344,,,,,,,,jbellis,jbellis,,,,,,,,,,"18/Mar/13 10:55;slebresne;I note that 64k might sound arbitrary low and we might want to lift the limitation. However, as said above, for sets elements we are constrained by the current internal column name size. We could of course lift that limitation, but it's not trivial. Of course, we could lift the limitation only for map and list values, but even that would unfortunately break the current format currently used to send collections to the client, so at a minimum it would require a binary protocol version bump.","18/Mar/13 13:30;jbellis;I'm fine with the 64k limit since I've always thought of collections as places to denormalize small amounts of data, not a replacement for creating a new table.","21/Mar/13 11:24;slebresne;Patch attached to do 2 things:
# validate the 64K limit on inserts. Note that the patch does the validation for set values and map keys too so as to provide a more meaningful error message, but in practice slightly slower value may be rejected if the column name limit is reached.
# fix a small ""bug"" in the collection types {{compose}} method that was not reading the size unsigned, thus artificially limiting the size to 32K.
",21/Mar/13 12:41;jbellis;+1,"21/Mar/13 12:58;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClosedChannelException on shutdown,CASSANDRA-5368,12638011,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,20/Mar/13 16:23,12/Mar/19 14:05,13/Mar/19 22:29,20/Mar/13 16:40,1.2.4,,,,,,0,,,,,"Catching AsynchronousCloseException isn't enough:

{noformat}
ERROR [ACCEPT-/127.0.0.3] 2013-03-20 11:10:38,087 CassandraDaemon.java (line 169) Exception in thread Thread[ACCEPT-/127.0.0.3,5,main]
java.lang.RuntimeException: java.nio.channels.ClosedChannelException
    at org.apache.cassandra.net.MessagingService$SocketThread.run(MessagingService.java:892)
Caused by: java.nio.channels.ClosedChannelException
    at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:135)
    at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:84)
    at org.apache.cassandra.net.MessagingService$SocketThread.run(MessagingService.java:881)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,20/Mar/13 16:24;brandon.williams;5368.txt;https://issues.apache.org/jira/secure/attachment/12574565/5368.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-20 16:37:10.344,,,no_permission,,,,,,,,,,,,318489,,,Wed Mar 20 16:40:57 UTC 2013,,,,,,0|i1iydj:,318830,,,,,,,,jbellis,jbellis,,,,,,,,,,20/Mar/13 16:24;brandon.williams;Trivial patch to catch this but only mention it at debug.,20/Mar/13 16:37;jbellis;+1,20/Mar/13 16:40;brandon.williams;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 don't validate that collections haven't more than 64K elements,CASSANDRA-5428,12641019,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,05/Apr/13 10:56,12/Mar/19 14:04,13/Mar/19 22:29,03/Dec/13 13:56,1.2.13,,,,,,0,,,,,"This is somewhat similar to CASSANDRA-5355 but with a twist. When we serialize collections, not only does the size of the elements is limited to 64K, but the number of elements is too because it is also an unsigned short.

Now the same argument than in CASSANDRA-5355 that collections are ""places to denormalize small amounts of data"" is true here too. So the fact that collections are limited to 64K elements is something I could live with. However, we don't validate that no more than 64K elements are inserted. And in fact, we can't validate it if the elements are added one by one.

So in practice, you can insert more than 64K elements, but if you try to read it, you will only get back some subset of the collection. And the number of elements returned will correspond to the 2 last bytes of the real size (so a collection of 65536 elements will be returned as 1 element). Imo, that's more problematic.

So since unfortunately we can't validate this at insertion, I suggest that as a first step we:
# document that limitation (in http://cassandra.apache.org/doc/cql3/CQL.html typically)
# when we read a collection that has > 64K elements, we detect it and when serializing that for the client, we:
** return as much as we can, i.e. the 64K first ones
** log a warning that something is wrong

On the longer term, for 2.0, maybe we should just change the serialization format and use an int for the collection size, using an unsigned short was probably misguided. Of course that changes said serialization format so we have to bump the native protocol version for that (and thus can't do that in 1.2).",,,,,,,,,,,,,,,,,,,,,,,,02/Dec/13 12:10;slebresne;5428.txt;https://issues.apache.org/jira/secure/attachment/12616542/5428.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-05 13:37:44.216,,,no_permission,,,,,,,,,,,,321441,,,Tue Dec 03 13:56:55 UTC 2013,,,,,,0|i1jglz:,321786,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,05/Apr/13 13:37;jbellis;Sounds like a reasonable plan.,"02/Dec/13 10:54;slebresne;Attaching patch that does the part 2. above, detecting when we're about to serialize a collection with > 64K elements, logging an error and sending only the first 64K elements. On the documentation part, I've already ninja-committed a new paragraph to the doc for the limitations (no pushed online yet though).

As already said above, this is obviously not perfect and next version of the native protocol we do, we should probably switch to an int to encode collection sizes.","02/Dec/13 12:10;slebresne;First version of the patch was incorrect, attaching corrected version.",02/Dec/13 12:35;iamaleksey;+1,"03/Dec/13 13:56;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificantionException on server when multiple CQL3 read requests received on single column family simultaneously.,CASSANDRA-5382,12638905,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,rettet181,rettet181,25/Mar/13 16:24,12/Mar/19 14:04,13/Mar/19 22:29,27/Mar/13 10:42,1.2.4,,,,,,0,,,,,"The exception below is thrown on the server when two reads are performed at the exact same time on the same column family. This causes the query to fail. 

The problem appears to be caused by the 'name' list in org.apache.cassandra.cql3.ResultSet$Metadata. The reference is passed in to the constructor and iterated without copying and without a synch block. When two of these ResultSet instances are created from the same metadata list at the same time, a ConcurrentModificationException is thrown.

The error:

ERROR [Thrift:860] 2013-03-25 09:27:39,467 CustomTThreadPoolServer.java (line 217) Error occurred during processing of message.
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.cql3.ResultSet$Metadata.allInSameCF(ResultSet.java:237)
        at org.apache.cassandra.cql3.ResultSet$Metadata.<init>(ResultSet.java:219)
        at org.apache.cassandra.cql3.ResultSet.<init>(ResultSet.java:47)
        at org.apache.cassandra.cql3.statements.Selection$ResultSetBuilder.<init>(Selection.java:239)
        at org.apache.cassandra.cql3.statements.Selection$ResultSetBuilder.<init>(Selection.java:221)
        at org.apache.cassandra.cql3.statements.Selection.resultSetBuilder(Selection.java:211)
        at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:655)
        at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:147)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:136)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:62)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:132)
        at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:254)
        at org.apache.cassandra.thrift.CassandraServer.execute_prepared_cql3_query(CassandraServer.java:1851)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_prepared_cql3_query.getResult(Cassandra.java:4166)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_prepared_cql3_query.getResult(Cassandra.java:4154)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
        at java.lang.Thread.run(Thread.java:662)
","SLES, Sun JDK 1.6.0_43",,,,,,,,,,,,,,,,,,,,,,,26/Mar/13 11:44;slebresne;0001-Copy-ResultSet-in-makeCountResult.txt;https://issues.apache.org/jira/secure/attachment/12575508/0001-Copy-ResultSet-in-makeCountResult.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-26 11:16:08.377,,,no_permission,,,,,,,,,,,,319375,,,Wed Mar 27 10:42:56 UTC 2013,,,,,,0|i1j3u7:,319716,,,,,,,,jbellis,jbellis,,,,,,,,,,"26/Mar/13 11:16;slebresne;Have you identified which queries have triggered this exactly? Is it possible some ""SELECT count(*) ..."" is involved?","26/Mar/13 11:44;slebresne;From a quick check the only place I see the metadata list being modified after Metadata creation is in makeCountResult. Even if I've missed another spot, not copying in makeCountResult was bad so attaching a simple patch to fix that.","26/Mar/13 15:26;rettet181;Thanks for the patch, yes the query does involve a count, specifically:

""SELECT count(*) FROM node WHERE state_identifier = 1""",26/Mar/13 15:37;jbellis;+1,"27/Mar/13 10:42;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CQL3: IN clause on last key not working when schema includes set,list or map",CASSANDRA-5376,12638518,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,voodooless,voodooless,22/Mar/13 14:48,12/Mar/19 14:04,13/Mar/19 22:29,27/Mar/13 10:43,1.2.4,,,,,,0,,,,,"This is an exception on the fix of https://issues.apache.org/jira/browse/CASSANDRA-5230

Looks like any schema using map,list or set won't work with IN clauses on the last key (in this example c)

Schema:
{code}
CREATE TABLE foo2 (
  key text,
  c bigint,
  v text,
  x set<text>,
  PRIMARY KEY (key, c)
);
{code}

Query:
{code}select * from foo2 where key = 'foo' and c in (1,3,4) ;{code}

This will lead to an assertion error on the nodes:

{code}java.lang.AssertionError
        at org.apache.cassandra.cql3.statements.SelectStatement.buildBound(SelectStatement.java:540)
        at org.apache.cassandra.cql3.statements.SelectStatement.getRequestedBound(SelectStatement.java:568)
        at org.apache.cassandra.cql3.statements.SelectStatement.makeFilter(SelectStatement.java:308)
        at org.apache.cassandra.cql3.statements.SelectStatement.getSliceCommands(SelectStatement.java:219)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:132)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:62)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:132)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:143)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1726)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4074)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4062)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)

{code}",,,,,,,,,,,,,,,,,,,,,,,,25/Mar/13 12:39;slebresne;5376.txt;https://issues.apache.org/jira/secure/attachment/12575310/5376.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-25 12:39:09.386,,,no_permission,,,,,,,,,,,,318994,,,Mon Jun 03 16:07:38 UTC 2013,,,,,,0|i1j1hr:,319335,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"25/Mar/13 12:39;slebresne;Unfortunately, handling collections is slightly harder than what CASSANDRA-5230 aimed for, because we can't do a name query. So this will have to wait for CASSANDRA-4762. In the meantime, we should obviously not throw an assertion error so attaching a patch to improve validation. The patch also slightly optimize matters as it uses name queries as much as possible if the select doesn't include a collection (versus, never using them on CF with collections even if no selection is selected). 
",27/Mar/13 09:39;iamaleksey;+1,"27/Mar/13 10:43;slebresne;Committed, thanks","03/Jun/13 16:00;sh123;how to apply patch ?
","03/Jun/13 16:07;iamaleksey;bq. how to apply patch ?

It's been already applied to 1.2.4. Also, make sure to read Sylvain's comment.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction doesn't remove index entries as designed,CASSANDRA-5395,12639433,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,27/Mar/13 19:32,12/Mar/19 14:04,13/Mar/19 22:29,04/Apr/13 15:48,1.2.4,,,,,,0,qa-resolved,,,,"PerColumnIndexUpdater ignores updates where the new value is a tombstone.  It should still remove the index entry on oldColumn.

(Note that this will not affect user-visible correctness, since KeysSearcher/CompositeSearcher will issue deletes against stale index entries, but having more stale entries than we ""should"" could affect performance.)",,,,,,,,,,,,,,,,,,,,,,,,27/Mar/13 20:02;jbellis;5395-2.txt;https://issues.apache.org/jira/secure/attachment/12575756/5395-2.txt,27/Mar/13 19:34;jbellis;5395.txt;https://issues.apache.org/jira/secure/attachment/12575751/5395.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-03-28 20:04:55.997,,,no_permission,,,,,,,,,,,,319903,,,Fri Mar 29 17:08:24 UTC 2013,,,,,,0|i1j73r:,320244,,,,,,,,beobal,beobal,,,,,,,,,enigmacurry,"27/Mar/13 20:02;jbellis;second patch attached (on top of the first) that also avoids creating duplicate index entries during PrecompactedRow.merge.  (Calling indexer.update(A, B) would remove the entry for A and add one for B, but since we're compacting we know that an entry for B already exists.)

switched to a merge-column-at-a-time approach similar to what LCR uses.

also tweaked LCR's reducer to short-circuit the column lookup if there is no index involved.","27/Mar/13 22:06;jbellis;Fixed a test failure and pushed to http://github.com/jbellis/cassandra/tree/5395.  (Original patches were also half 1.2, half trunk.  Now both against 1.2.)","28/Mar/13 01:58;jbellis;... And pushed a 3rd commit applying the fix in patch to to ParallelCompactionIterable.

This also has the side effect of switching from TSM to ABSM for PrecompactedRow and ParallelCompactionIterable, so there may be a performance improvement.","28/Mar/13 20:04;beobal;lgtm, just have 2 trivial queries:

In LCR & PCR, if the purpose of the additional clauses is to omit unnecessary column lookups, should the column lookup be the last of the &&'d  conditions?

{code}
 if (indexer != SecondaryIndexManager.nullUpdater
                    && !column.isMarkedForDelete()
                    && container.getColumn(column.name()) != column)
{code}


Class documentation in IdentityQueryFilter states ""Only for use in testing; will read entire CF into memory."" Seeing as its being used in non-test code we should probably amend the docstring
",29/Mar/13 17:08;jbellis;Committed with suggested improvements.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updates to PerRowSecondaryIndex don't use most current values,CASSANDRA-5397,12639634,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,beobal,beobal,beobal,28/Mar/13 17:28,12/Mar/19 14:04,13/Mar/19 22:29,04/Apr/13 18:24,1.2.4,,,,,,0,,,,,"The way that updates to secondary indexes are performed using  SecondaryIndexManager.Updater is flawed for PerRowSecondaryIndexes.  Unlike PerColumnSecondaryIndexes, which only require the old & new values for a single column,  the expectation is that a PerRow indexer can be given just a key which it will use to retrieve the entire row (or as many columns as it requires) and perform its indexing on those columns.  As the indexes are updated before the memtable atomic swap occurs, a per-row indexer may only read the previous values for the row, not the new ones that are being written. In the case of an insert, there is no previous value and so nothing is added to the index.",,,,,,,,,,,,,,,,,,,,,,,,04/Apr/13 15:56;jbellis;5397-1.2-v3.txt;https://issues.apache.org/jira/secure/attachment/12576992/5397-1.2-v3.txt,04/Apr/13 17:03;beobal;5397-1.2-v4.txt;https://issues.apache.org/jira/secure/attachment/12577011/5397-1.2-v4.txt,28/Mar/13 17:30;beobal;5397.txt;https://issues.apache.org/jira/secure/attachment/12575897/5397.txt,04/Apr/13 09:35;beobal;5397_12.txt;https://issues.apache.org/jira/secure/attachment/12576954/5397_12.txt,04/Apr/13 09:35;beobal;5397_trunk.txt;https://issues.apache.org/jira/secure/attachment/12576955/5397_trunk.txt,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2013-04-01 17:15:50.178,,,no_permission,,,,,,,,,,,,320103,,,Fri Apr 05 08:57:28 UTC 2013,,,,,,0|i1j8c7:,320444,,,,,,,,jbellis,jbellis,,,,,,,,,,"28/Mar/13 17:30;beobal;Patch extends the SecondaryIndexManager.Updater interface to add a commit() method.  Behaviour for PerColumn indexes remains unchanged, with updates applied to the index immediately and so the commit is a no-op. For PerRow indexes, the updates are deferred until after the memtable update occurs, then actioned via the call to commit.  I missed this in CASSANDRA-2897 partially because there are no implementations of PerRowSecondaryIndex in the codebase, so I've also added a unit test with a dummy implementation.","01/Apr/13 17:15;jbellis;TBH my preferred fix here would be to make PerRowSI also do ""lazy updates"" a la CASSANDRA-2897.  Is that possible?","02/Apr/13 13:56;beobal;I don't really see how we can do that, especially as PRSI is typically implemented outside of C* and the contract we give it is expressed in the method signature {{public abstract void index(ByteBuffer rowKey);}} So the assumption is that an update to a PRSI will be able to access the entire row at index time. 

Changes to AtomicSortedColumns are applied a column at a time so MixedIndexUpdater has a guard to ensure that even when a mutation changes multiple columns in the row,  the index is only updated once. Obviously though, until the last of these column updates occurs, the row is not fully updated. So I see 2 options, defer the per-row indexing until we've finished updating the row (as in my first patch), or remove the guard and apply the per-row update as each column is updated. The second option has the benefit of not changing the SIM.Updater api, but is potentially very inefficient.",03/Apr/13 20:14;jbellis;Is this intended for 1.2 or 2.0?  I'm getting lots of conflicts on both.,"04/Apr/13 09:34;beobal;The patch was originally against 1.2, but it needed rebasing after CASSANDRA-5395 was committed. I'm attaching 2 new versions, one each for 1.2 and trunk.","04/Apr/13 15:56;jbellis;v3 against 1.2 fixes some formatting and removes the {{if (column.isMarkedForDelete()) return}} from MIU.update, since it would re-introduce one of the problems fixed in CASSANDRA-5395.

Not sure if it should actually be moved to the ""not instanceof PCSI"" block -- if so, how does PRSI remove stale entries?","04/Apr/13 15:59;jbellis;Also, remove is only called by compaction, so there will be no commit (so adding to deferred is a bad idea).

If we're assuming that PRSI always keeps the index exactly up to date, remove can be a no-op.","04/Apr/13 17:03;beobal;Yes, you're right about the {{if( column.isMarkedForDelete()) return}} being a regression.

Its down to the PRSI implementation to figure out whether an update is actually an update or whether it actually calls for a delete. As the PRSI only has the key to work with & is going to be inspecting the whole row anyway this shouldn't be difficult, but it does make the whole SI/PCSI/PRCI hierarchy a bit ugly. 

Also, we do/should assume that PRSI always keeps the index exactly up to date, so I'm +1 with making remove a no-op there.  

attached v4 for 1.2 (v3 + the no-op remove for PRSI)
","04/Apr/13 18:24;jbellis;Odd, I'm seeing the following with v4:

{noformat}
formite:git johnathanellis$ patch -p1 < ~/.JIRAClient/download/5397-1.2-v4.txt 
patching file src/java/org/apache/cassandra/db/AtomicSortedColumns.java
patching file src/java/org/apache/cassandra/db/index/SecondaryIndexManager.java
patch: **** malformed patch at line 159: diff --git a/test/unit/org/apache/cassandra/SchemaLoader.java b/test/unit/org/apache/cassandra/SchemaLoader.java
{noformat}

I committed what I think is the same code based on v3, please doublecheck it.","04/Apr/13 19:44;beobal;lgtm, thanks.",04/Apr/13 20:24;jjordan;Does this effect 1.1?  Or is it a problem with the new faster 1.2 indexes?,05/Apr/13 08:57;krummas;pushed a build fix to trunk (basically only fixed the test case): 6afbed371c0d12a15a969e4f52ba670998bab282 RowMutations do not take QueryPath in trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken default values for min/max timestamp,CASSANDRA-5372,12638321,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,21/Mar/13 17:33,12/Mar/19 14:04,13/Mar/19 22:29,25/Mar/13 14:33,1.1.11,1.2.4,,,,,0,,,,,"When the SStableMetadata are not present (or are too hold), the default for the min and max timestamp used is not always correct. Namely, the default (i.e. when we don't know anything) for the min timestamp should be MIN_VALUE and the max timestamp should be MAX_VALUE.

And there is 2 places where we need to apply those default:
* if the metadata is an old one that don't have the info
* if we don't have any metadata component at all

The only default that is correct is the case fixed by CASSANDRA-5153, but even then it missed a number of occurrences of the problem.",,,,,,,,,,,,,,,,,,,,,,,,21/Mar/13 17:37;slebresne;5372.txt;https://issues.apache.org/jira/secure/attachment/12574837/5372.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-25 13:12:08.528,,,no_permission,,,,,,,,,,,,318797,,,Mon Mar 25 14:33:38 UTC 2013,,,,,,0|i1j09z:,319138,,,,,,,,jasobrown,jasobrown,,,,,,,,,,"21/Mar/13 17:37;slebresne;Patch attached to apply the correct default. The patch is against 1.2 though part of it affect 1.1 too (but 1.1 doesn't track the min timestamp). I'll happily commit the relevant part to 1.1 but attaching against 1.2 to have the whole fix.

The patch also remove SSTableMetadata.defaultInstance() because the deserializer already know what to do when the stat component is not here.",25/Mar/13 13:12;jasobrown;LGTM,"25/Mar/13 14:33;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing files in debian /etc/cassandra/conf folder,CASSANDRA-5363,12637883,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,sdelmas,sdelmas,20/Mar/13 02:42,12/Mar/19 14:04,13/Mar/19 22:29,20/Mar/13 19:04,1.2.4,,,Packaging,,,0,,,,,"The standard debian installation puts;

cassandra-env.sh cassandra.yaml log4j-server.properties

into /etc/cassandra. However there seem to be additional files that might make sense there:

commitlog_archiving.properties cassandra-rackdc.properties log4j-tools.properties cassandra-topology.properties

(these are at least installed by the DSC rpm installer). So should those be added in debian or removed in rpms?",Debian,,,,,,,,,,,,,,,,,,,,,,,20/Mar/13 16:05;brandon.williams;5363.txt;https://issues.apache.org/jira/secure/attachment/12574562/5363.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-20 16:05:34.536,,,no_permission,,,,,,,,,,,,318363,,,Thu Mar 21 12:23:06 UTC 2013,,,,,,0|i1ixlj:,318704,,,,,,,,urandom,urandom,,,,,,,,,,"20/Mar/13 16:05;brandon.williams;We already had rackdc properties, but here's a patch for the others.",20/Mar/13 18:59;urandom;LGTM; +1,20/Mar/13 19:04;brandon.williams;Committed,21/Mar/13 12:23;sdelmas;Thanks for the quick turnaround.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disallow renaming columns one at a time when when the table don't have CQL3 metadata yet,CASSANDRA-5531,12645635,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,01/May/13 14:21,12/Mar/19 14:04,13/Mar/19 22:29,01/May/13 14:27,1.2.5,,,,,,0,,,,,"As noted in CASSANDRA-5489, if you have a ""thrift"" CF, say:
{noformat}
[default@ks] create column family test with comparator='CompositeType(Int32Type, Int32Type, Int32Type)' and key_validation_class=UTF8Type and default_validation_class=UTF8Type;
{noformat}

And that trying to use it in CQL3 you rename the columns one at a time, you can get:
{noformat}
cqlsh:ks> DESC COLUMNFAMILY test;

CREATE TABLE test (
  key text,
  column1 int,
  column2 int,
  column3 int,
  value text,
  PRIMARY KEY (key, column1, column2, column3)
) WITH COMPACT STORAGE ...
cqlsh:ks> ALTER TABLE test RENAME column2 TO foo;
TSocket read 0 bytes
{noformat}

No, it happens that renaming the columns one at a time is a bad idea anyway as it can confuse the CQL3 code in some cases. So I suggest to disallow that and to force renaming all columns in one request the first you use a thrift CF from CQL3.

To be clear, you will still be able to rename columns one at a time in general, it's just for the first time you rename on a metadata-less CF. So overall that's a very small limitation and it simplify our lives code-wise.

See CASSANDRA-5489 for a bit more context here.",,,,,,,,,,,,,,,,,,,,,,,,01/May/13 14:25;slebresne;5531.txt;https://issues.apache.org/jira/secure/attachment/12581358/5531.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,325996,,,Wed May 01 14:25:50 UTC 2013,,,,,,0|i1k8p3:,326341,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"01/May/13 14:25;slebresne;To be clear, that ticket is just to track the bits that go into 1.2 discussed on CASSANDRA-5489. I'm attaching the patch committed for info.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
forceTablePrimaryRange fails with nullpointer exception,CASSANDRA-5512,12644433,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,etobgra,etobgra,24/Apr/13 20:21,12/Mar/19 14:04,13/Mar/19 22:29,25/Apr/13 17:38,1.2.5,,,Legacy/Tools,,,0,,,,,"Running JMX operation forceTableRepairRange fails with nullpointer exception.
Three node cluster with one keyspace and one large columnfamily.

Works when running nodetool -pr but not over JMX.
Stacktrace:

ERROR [MiscStage:1] 2013-04-24 09:53:02,884 CassandraDaemon.java (line 164) Exception in thread Thread[MiscStage:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.service.SnapshotVerbHandler.doVerb(SnapshotVerbHandler.java:38)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
        at java.lang.Thread.run(Thread.java:662)","Linux red hat, java 1.6.43
three nodes",,,,,,,,,,,,,,,,,,,,,,,24/Apr/13 21:39;yukim;5512-1.2.txt;https://issues.apache.org/jira/secure/attachment/12580389/5512-1.2.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-24 21:18:49.661,,,no_permission,,,,,,,,,,,,324800,,,Thu Apr 25 17:38:58 UTC 2013,,,,,,0|i1k1bz:,325146,,,,,,,,jbellis,jbellis,,,,,,,,,,"24/Apr/13 20:40;etobgra;UPDATE
======
The jmx operation is successfully invoked on node 1 according to system.log (AntiEntropySession started) but the other two nodes generates nullpointer exception.

","24/Apr/13 21:18;jbellis;Unclear to me how you can generate a null snapshot message.  Can you test with 1.2 git branch?  May be fixed by CASSANDRA-5424.

In the meantime you can also pass isSequential=false to avoid creating snapshots.","24/Apr/13 21:39;yukim;forceTablePrimaryRange's second boolean parameter is for sequential repair using snapshot(you can do this with nodetool repair -snapshot too), and when it's on, looks like it fails as described.

We somehow omitted registering Snapshot message's serializer in 1.2 branch. Trivial patch attached.",24/Apr/13 22:12;jbellis;Good eyes! +1,"25/Apr/13 05:26;etobgra;Great!
Thank you very much for the quick response.
","25/Apr/13 17:38;yukim;Committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use allocator information to improve memtable memory usage estimate,CASSANDRA-5497,12643299,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,18/Apr/13 18:31,12/Mar/19 14:04,13/Mar/19 22:29,19/Apr/13 13:46,1.1.12,1.2.5,,,,,0,,,,,"A user reported that Cassandra's estimate of memtable space used was off by a factor of between 3 and 10 for his counter columnfamilies.

We may or may not be able to fix the counter estimate (counter merging is a cranky best, and unlike normal merging can involve allocating new objects), but we can definitely use the SlabAllocator information to cap the error in our estimate at a fairly low amount.",,,,,,,,,,,,,,,,,,,,,,,,18/Apr/13 18:48;jbellis;5497-v2.txt;https://issues.apache.org/jira/secure/attachment/12579389/5497-v2.txt,18/Apr/13 18:33;jbellis;5497.txt;https://issues.apache.org/jira/secure/attachment/12579385/5497.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-04-18 18:49:04.837,,,no_permission,,,,,,,,,,,,323709,,,Fri Apr 19 13:46:44 UTC 2013,,,,,,0|i1julr:,324054,,,,,,,,jasobrown,jasobrown,,,,,,,,,,18/Apr/13 18:33;jbellis;attached.,18/Apr/13 18:48;jbellis;v2 attached to track allocations larger than MAX_CLONED_SIZE,18/Apr/13 18:49;jasobrown;lgtm. This should bound the estimate reasonably well. EDIT: [~jbellis] attached v2 while i was typing. will review that one now,18/Apr/13 19:16;jasobrown;v2 lgtm,19/Apr/13 13:46;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix SemanticVersion.isSupportedBy patch/minor handling,CASSANDRA-5496,12643271,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,iamaleksey,iamaleksey,18/Apr/13 16:32,12/Mar/19 14:04,13/Mar/19 22:29,18/Apr/13 16:48,1.2.5,,,,,,0,,,,,"Currently we use the following logic:
{noformat}
return major == version.major && minor <= version.minor && patch <= version.patch;
{noformat}

This requires both minor and patch vers to be less or equal, which means that, for example, '3.0.3' is not supported by '3.1.2', because, while 0 <= 1 (minor), 3 > 2 (patch). This is clearly not the intent since 3.1.2 > 3.0.3.

CQL3 doc: ""Minor version increments occur when new, but backward compatible, functionality is introduced"". Hence '3.0.3' is supposed to be supported by '3.1.0'.

This doesn't really affect 1.2, but breaks a lot of trunk dtests post CASSANDRA-3919.",,,,,,,,,,,,,,,,,,,,,,,,18/Apr/13 16:33;iamaleksey;5496.txt;https://issues.apache.org/jira/secure/attachment/12579359/5496.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-18 16:40:07.097,,,no_permission,,,,,,,,,,,,323681,,,Thu Apr 18 16:48:58 UTC 2013,,,,,,0|i1jufj:,324026,,,,,,,,jbellis,jbellis,,,,,,,,,,18/Apr/13 16:40;jbellis;+1,"18/Apr/13 16:48;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid serializing keyspace redundantly in RowMutation,CASSANDRA-5458,12642150,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,11/Apr/13 22:31,12/Mar/19 14:04,13/Mar/19 22:29,12/Apr/13 02:07,2.0 beta 1,,,,,,0,,,,,"We can infer the table from the CFID, so there's no need to de/serialize it.",,,,,,,,,,,,,,,,,,,,,,,,11/Apr/13 22:31;jbellis;5458.txt;https://issues.apache.org/jira/secure/attachment/12578301/5458.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-12 01:33:27.965,,,no_permission,,,,,,,,,,,,322564,,,Fri Apr 12 02:07:19 UTC 2013,,,,,,0|i1jnjj:,322909,,,,,,,,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,,12/Apr/13 01:33;vijay2win@yahoo.com;+1 LGTM,12/Apr/13 02:07;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraStorage throws NullPointerException (NPE) when widerows is set to 'true',CASSANDRA-5488,12643010,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,sgosrani,sgosrani,sgosrani,17/Apr/13 19:51,12/Mar/19 14:04,13/Mar/19 22:29,22/May/13 16:21,1.1.12,1.2.6,,,,,0,cassandra,hadoop,pig,,"CassandraStorage throws NPE when widerows is set to 'true'. 

2 problems in getNextWide:
1. Creation of tuple without specifying size
2. Calling addKeyToTuple on lastKey instead of key

java.lang.NullPointerException
    at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
    at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:124)
    at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:73)
    at org.apache.cassandra.cql.jdbc.JdbcUTF8.compose(JdbcUTF8.java:93)
    at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:34)
    at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:26)
    at org.apache.cassandra.hadoop.pig.CassandraStorage.addKeyToTuple(CassandraStorage.java:313)
    at org.apache.cassandra.hadoop.pig.CassandraStorage.getNextWide(CassandraStorage.java:196)
    at org.apache.cassandra.hadoop.pig.CassandraStorage.getNext(CassandraStorage.java:224)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:194)
    at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:532)
    at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
    at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
    at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
    at org.apache.hadoop.mapred.Child.main(Child.java:249)
2013-04-16 12:28:03,671 INFO org.apache.hadoop.mapred.Task: Runnning cleanup for the task","Ubuntu 12.04.1 x64, Cassandra 1.2.4",,,,,,,,,,,,,,,,,,,,,,,21/May/13 16:11;jeromatron;5488-2.txt;https://issues.apache.org/jira/secure/attachment/12584029/5488-2.txt,17/Apr/13 22:08;sgosrani;5488.txt;https://issues.apache.org/jira/secure/attachment/12579212/5488.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-05-01 12:29:30.728,,,no_permission,,,,,,,,,,,,323420,,,Wed May 22 16:21:42 UTC 2013,,,,,,0|i1jstj:,323765,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,17/Apr/13 20:04;sgosrani;This patch (5488.txt) fixes the issue.,01/May/13 12:29;brandon.williams;Can you add a test to examples/pig/test/test_storage.pig that demonstrates the problem?,17/May/13 13:58;jeromatron;I've reproduced this with 1.1.9 as well.,17/May/13 16:16;jeromatron;Looks like it's from CASSANDRA-5098,21/May/13 16:11;jeromatron;An alternative way to do it with consolidating the two methods and checking for null in that method.,"21/May/13 16:15;brandon.williams;Committed v2, and also flipped the copy test to use widerow mode as a smoke test.","22/May/13 10:45;jeromatron;There ended up being a secondary problem that was hidden by the first NPE.  It seems to be related to getting the AbstractType.  The NPE was for this line: https://github.com/apache/cassandra/blob/cassandra-1.1/src/java/org/apache/cassandra/hadoop/pig/CassandraStorage.java#L307 which I decomposed to find out what it was NPEing on, and got this:
{code}
            List<AbstractType> atList = getDefaultMarshallers(cfDef);
            AbstractType at = atList.get(2);
            Object o = at.compose(key); //NPE from this line
            setTupleValue(tuple, 0, o);
            //setTupleValue(tuple, 0, getDefaultMarshallers(cfDef).get(2).compose(key));
{code}

So it seems unrelated to the original NPE, but still matches the description of this ticket.

To reproduce, here is my schema:
{code}
CREATE KEYSPACE circus
with placement_strategy = 'SimpleStrategy'
and strategy_options = {replication_factor:1};

use circus;

CREATE COLUMN FAMILY acrobats
WITH comparator = UTF8Type
AND key_validation_class=UTF8Type
AND default_validation_class = UTF8Type;
{code}

Here is a pycassa script to create the data:
{code}
from pycassa.pool import ConnectionPool
from pycassa.columnfamily import ColumnFamily

pool = ConnectionPool('circus')
col_fam = pycassa.ColumnFamily(pool, 'acrobats')

for i in range(1, 10):
    for j in range(1, 200000):
        col_fam.insert('row_key' + str(i), {str(j): 'val'})
{code}

Here is the pig (0.9.2) that I'm running in local mode:
{code}
rows = LOAD 'cassandra://circus/acrobats?widerows=true&limit=200000' USING CassandraStorage();
filtered = filter rows by key == 'row_key1';
columns = foreach filtered generate flatten(columns);
counted = foreach (group columns all) generate COUNT($1);
dump counted;
{code}",22/May/13 16:21;brandon.williams;v2 was a little too aggressive in function consolidation.  I reverted it and applied v1.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
allow STCS options to apply to the L0 compaction performed by LCS,CASSANDRA-5439,12641419,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,carlyeks,jbellis,jbellis,08/Apr/13 18:21,12/Mar/19 14:04,13/Mar/19 22:29,21/Apr/13 23:08,2.0 beta 1,,,,,,0,lcs,,,,(See CASSANDRA-5371),,,,,,,,,,,,,,,,,,,,,,,,14/Apr/13 02:54;jbellis;5439-v2.txt;https://issues.apache.org/jira/secure/attachment/12578632/5439-v2.txt,13/Apr/13 20:43;carlyeks;5439.patch;https://issues.apache.org/jira/secure/attachment/12578617/5439.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-04-13 20:43:18.756,,,no_permission,,,,,,,,,,,,321835,,,Sun Apr 21 23:08:36 UTC 2013,,,,,,0|i1jj1j:,322180,,,,,,,,jbellis,jbellis,,,,,,,,,,"13/Apr/13 20:43;carlyeks;Patch adds a new file, SizeTieredCompactionStrategyOptions, which is used by both the SizeTiered and the Leveled compaction strategies.",14/Apr/13 02:53;jbellis;v2 attached to pass the options from LCS constructor into the manifest.,21/Apr/13 19:09;carlyeks;Changes to the original patch look good.,21/Apr/13 23:08;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InputStreams not closed,CASSANDRA-5580,12648337,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,ash2k,ash2k,ash2k,19/May/13 06:32,12/Mar/19 14:04,13/Mar/19 22:29,20/May/13 22:11,2.0 beta 1,,,Local/Config,,,0,,,,,InputStreams are not closed properly in few places.,,,,,,,,,,,,,,,,,,,,,,,,19/May/13 06:34;ash2k;trunk-5580.txt;https://issues.apache.org/jira/secure/attachment/12583762/trunk-5580.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-20 22:11:20.405,,,no_permission,,,,,,,,,,,,328693,,,Mon May 20 22:11:20 UTC 2013,,,,,,0|i1kplr:,329036,,,,,,,,jbellis,jbellis,,,,,,,,,,19/May/13 06:34;ash2k;Patch.,20/May/13 22:11;jbellis;LGTM; committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Write row markers when serializing columnfamilies and columns schema,CASSANDRA-5572,12647866,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,iamaleksey,iamaleksey,16/May/13 00:20,12/Mar/19 14:04,13/Mar/19 22:29,16/May/13 13:51,1.2.6,,,,,,0,,,,,"ColumnDefinition.toSchema() and CFMetaData.toSchemaNoColumns() currently don't write the row markers, which leads to certain queries not returning the expected results, e.g.

select keyspace_name, columnfamily_name from system.schema_columnfamilies where keyspace_name = 'system' and columnfamily_name = 'hints' -> [].",,,,,,,,,,,,,,,,,,,,,,,,16/May/13 00:21;iamaleksey;5572.txt;https://issues.apache.org/jira/secure/attachment/12583410/5572.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-16 08:08:41.563,,,no_permission,,,,,,,,,,,,328222,,,Thu May 16 13:51:02 UTC 2013,,,,,,0|i1kmpb:,328566,,,,,,,,slebresne,slebresne,,,,,,,,,,"16/May/13 08:08;slebresne;lgtm, +1.

Makes me think it might simplify things a bit to use range tombstones in dropFromSchema. And maybe we could start using CQL3 queries (with processInternal) to avoid having to deal with row markers manually? But anyway, we can definitively do that later. ","16/May/13 13:39;iamaleksey;Yeah, thought about range tombstones here as well yesterday. But, later.",16/May/13 13:51;iamaleksey;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Maven package installation broken by recent build changes,CASSANDRA-5532,12645685,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,beobal,beobal,beobal,01/May/13 19:22,12/Mar/19 14:04,13/Mar/19 22:29,02/May/13 03:50,2.0 beta 1,,,Legacy/Tools,,,0,,,,,"CASSANDRA-3818 provides the ability to disable maven ant tests during the build. Part of the change is to refactor the maven-ant-tasks-retrieve-build target to wrapped by an antcall, with a guard condition to check if m-a-t should be used. The use of antcall causes the target & those targets it depends on to be called in a separate scope to the main build, which unfortunately means that the repository refs and ant macros which get defined there are not available once the antcall is completed. The final effect is that the mvn-install task fails as the install macro is not defined in its scope, and the artifacts task fails due to the repository refs being similarly undefined. I haven't tried it, but I suspect the publish task would be affected in the same way.",,,,,,,,,,,,,,,,,,,,,,,,01/May/13 19:23;beobal;0001-Remove-antcall-around-m-a-t-retrieve-build.patch;https://issues.apache.org/jira/secure/attachment/12581399/0001-Remove-antcall-around-m-a-t-retrieve-build.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-02 00:35:52.749,,,no_permission,,,,,,,,,,,,326046,,,Thu May 02 03:50:28 UTC 2013,,,,,,0|i1k907:,326391,,,,,,,,dbrosius,dbrosius,,,,,,,,,,01/May/13 19:23;beobal;Patch to revert the antcall wrapping around maven-ant-tasks-retrieve-build. I've added the check on without.maven directly to the target which seems to me to have the desired effect as the original patch,02/May/13 00:35;dbrosius;+1,02/May/13 03:50;dbrosius;committed to trunk as 2bc79a07474e48d57d9c17d2e597048006ff7bf2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
intersects the bounds not right,CASSANDRA-5551,12646780,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,wy96f,wy96f,wy96f,09/May/13 10:01,12/Mar/19 14:04,13/Mar/19 22:29,13/May/13 10:14,1.1.12,1.2.5,,,,,0,,,,,intersecs the bound includes the left of bound instead of right,,,,,,,,,,,,,,,,,,,,,,,,10/May/13 05:50;wy96f;0001-correctly-intersects-the-bounds.patch;https://issues.apache.org/jira/secure/attachment/12582579/0001-correctly-intersects-the-bounds.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-09 13:22:03.546,,,no_permission,,,,,,,,,,,,327138,,,Tue May 28 18:38:31 UTC 2013,,,,,,0|i1kg0f:,327482,,,,,,,,slebresne,slebresne,,,,,,,,,,09/May/13 13:22;jbellis;You're going to need to get a *lot* more specific.,10/May/13 05:50;wy96f;the intersects(Bounds<T> that) function in Range.java  should contains(that.left) instead of right,"13/May/13 10:14;slebresne;Make sense. +1, committed, thanks.",13/May/13 14:05;jbellis;Is this also a problem in 1.1?,13/May/13 14:12;slebresne;It is apparently. Forgot to look sorry. I'll backport there.,"28/May/13 17:27;rcoli;What impact does/did this bug have, on what code paths?","28/May/13 18:38;jbellis;Basically nil.  The effect is that cleanup will take not take the fast ""discard entire sstable path"" occasionally.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Every stream operation requires checking indexes in every SSTable,CASSANDRA-5569,12647639,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,rbranson,rbranson,rbranson,14/May/13 23:46,12/Mar/19 14:04,13/Mar/19 22:29,16/May/13 16:46,1.2.6,,,,,,0,streaming,,,,"It looks like there's a streaming performance issue when leveled compaction and vnodes get together. To get the candidate set of chunks to stream, the streaming system gets references to every SSTable for a CF. This is probably a perfectly reasonable assumption for non-vnode cases, because the data being streamed is likely distributed across the full SSTable set. This is also probably a perfectly reasonable assumption for size-tiered compaction, because the data is, again, likely distributed across the full SSTable set. However, for each vnode repair performed on LCS CF's, this scan across potentially tens of thousands of SSTables is wasteful considering that only a small percentage of them will actually have data for a given range.

This manifested itself as ""hanging"" repair operations with tasks backing up on the MiscStage thread pool.

The attached patch changes the streaming code so that for a given range, only SSTables for the requested range are checked to be included in streaming.",,,,,,,,,,,,,,,,,,,,,,,,15/May/13 04:57;rbranson;5569-v2.txt;https://issues.apache.org/jira/secure/attachment/12583280/5569-v2.txt,15/May/13 16:05;rbranson;5569-v3.txt;https://issues.apache.org/jira/secure/attachment/12583333/5569-v3.txt,14/May/13 23:46;rbranson;5569.txt;https://issues.apache.org/jira/secure/attachment/12583242/5569.txt,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-05-15 00:58:28.337,,,no_permission,,,,,,,,,,,,327995,,,Thu May 16 16:46:08 UTC 2013,,,,,,0|i1klav:,328339,,,,,,,,yukim,yukim,,,,,,,,,,"15/May/13 00:58;jbellis;This could cause a lot of reference churn, can you make DataTracker.markReferenced take a list of Bounds objects instead so we can uniquify the set before acquiring references?","15/May/13 01:30;rbranson;How do you feel about the safety of refactoring the markReferenced(RowPosition start, RowPosition end) signature call the new markReferenced to cut down on the duplication between these methods?",15/May/13 01:35;jbellis;+1,"15/May/13 04:57;rbranson;v2 patch attached.

Decided not to rock the boat a bunch in terms of allocation & copying on the markReferenced() for the single range common case, which it looks like is the path for a bunch of read operations. This patch contains a refactor that DRYs up the markReferenced() calls.",15/May/13 07:17;rbranson;It looks like StreamingRepairTask is also suffering this problem. Going to work up a v3 to include that as well.,"15/May/13 15:15;rbranson;http://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/streaming/StreamingRepairTask.java#L133

^^^ is there a good reason we're calling StreamOut.transferSSTables directly instead of StreamOut.transferRanges?",15/May/13 15:28;yukim;I think we can use transferRanges there even though we do blocking flush before streaming.,"15/May/13 15:35;rbranson;What I'm working with now is adding a second method signature with a ""flush"" boolean that allows the behavior to be turned off for StreamingRepairTask.","15/May/13 16:05;rbranson;v3 patch (rebased against current 1.2 branch).

Adds the same optimization to StreamingRepairTask. Note that the only apparent difference is that StreamingRepairTask wasn't doing a blocking flush, so I made that behavior optional in StreamOut.transferRanges(). Also took the opportunity to run through and fix some naming irregularities from the v2 patch.","15/May/13 22:14;rbranson;FWIW -- we're running the v3 patch in production on our vnodes + LCS cluster, and repairs are something like 25x faster.","16/May/13 16:46;yukim;+1 and committed.
Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"java/org/apache/cassandra/dht/BytesToken.java returns invalid string token (with prefix   Token(bytes["" + Hex.bytesToHex(token) + ""]))",CASSANDRA-5566,12647517,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,juras,juras,juras,14/May/13 15:05,12/Mar/19 14:04,13/Mar/19 22:29,14/May/13 19:03,1.2.5,,,,,,0,,,,,"BytesToken java class method toString returns invalid token. 
Changing java/org/apache/cassandra/dht/BytesToken.java
line 44 from:
return ""Token(bytes["" + Hex.bytesToHex(token) + ""])"";
to:
return Hex.bytesToHex(token);
shuffle works fine.

when you run:
./cassandra-shuffle -h localhost create
cassandra throws:
Exception in thread ""main"" java.lang.NumberFormatException: Non-hex characters in Token(bytes[d439e5e5ad484679a0c1f045fea7b2a3])
        at org.apache.cassandra.utils.Hex.hexToBytes(Hex.java:60)
        at org.apache.cassandra.dht.AbstractByteOrderedPartitioner$1.fromString(AbstractByteOrderedPartitioner.java:168)
        at org.apache.cassandra.tools.Shuffle.createShuffleBatchInsert(Shuffle.java:580)
        at org.apache.cassandra.tools.Shuffle.shuffle(Shuffle.java:358)
        at org.apache.cassandra.tools.Shuffle.main(Shuffle.java:678)
","debian x86_64 GNU/Linux, eff3c455ac73ac482a6fcf5cec111abc  apache-cassandra-1.2.4-src.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-05-14 18:37:23.813,,,no_permission,,,,,,,,,,,,327873,,,Tue May 14 18:37:23 UTC 2013,,,,,,0|i1kkjr:,328217,,,,,,,,jbellis,jbellis,,,,,,,,,,"14/May/13 18:37;jbellis;Kind of bizarre that we're passing String representations of tokens around, but this is probably the easiest fix.  Committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra crashes at start with segmentation fault,CASSANDRA-5517,12644819,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,sknaumov,sknaumov,26/Apr/13 11:57,12/Mar/19 14:04,13/Mar/19 22:29,15/Aug/13 19:15,,,,,,,0,,,,,"Sometimes Cassandra fails at start with segmentation fault:

# /usr/sbin/cassandra -f
xss =   -ea -javaajent:/usr/share/cassandra/lib/jamm-0.2.5.jar -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms1024M -Xmx1024M -Xmn100M -XX:+HeapDumpOnOutOfMemoryError -Xss180k
Segmentation fault

It seems that not only me encountered this bug: http://snapwebsites.org/known-issues/cassandra-crashes-java-segmentation-fault

Solution proposed on this link works.","VirtualBox 4.2.6 VM with 4GB RAM, Xubuntu 12.10 as host and guest OS.
Cassandra 1.2.4 installed on guest as Debian package.",,,,,,,,,,,,,CASSANDRA-2441,,,,,,,,,,08/Jul/13 18:31;urandom;0001-CASSANDRA-5517-up-xss-to-256k-for-openjdk-1.6.patch;https://issues.apache.org/jira/secure/attachment/12591246/0001-CASSANDRA-5517-up-xss-to-256k-for-openjdk-1.6.patch,08/Jul/13 18:31;urandom;0002-optional-upgrade-dependency-to-OpenJDK-7.patch;https://issues.apache.org/jira/secure/attachment/12591247/0002-optional-upgrade-dependency-to-OpenJDK-7.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-04-26 12:52:54.184,,,no_permission,,,,,,,,,,,,325182,,,Wed Nov 27 17:04:16 UTC 2013,,,,,,0|i1k3of:,325527,,,,,,,,jbellis,jbellis,,,,,,,,,,26/Apr/13 12:52;jeromatron;What version of Java are you running?  Can you paste the output of running java -version?,"29/Apr/13 05:56;sknaumov;# java -version
java version ""1.6.0_27""
OpenJDK Runtime Environment (IcedTea6 1.12.3) (6b27-1.12.3-0ubuntu1~12.10.1)
OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)","29/Apr/13 11:15;jeromatron;Can you try to see if you can reproduce the problem while running with a Sun/Oracle 1.6 JDK (non-openjdk), one of the more recent versions?  In the 1.6 line, openjdk is pretty behind the Sun/Oracle JDK.  I wouldn't be surprised if it was just an oddity with openjdk.","08/Jul/13 18:36;urandom;The attached patch, 0001-CASSANDRA-5517-up-xss-to-256k-for-openjdk-1.6, raises -Xss to 256k for OpenJDK 1.6 only.

Optionally, 0002-optional-upgrade-dependency-to-OpenJDK-7 raises the Debian package dependency to OpenJDK 1.7.  You could argue it unwise to change this in a point release, but OpenJDK 1.7 seems to be a _much_ better default at this point.","10/Aug/13 21:23;jbellis;According to https://github.com/apache/cassandra/pull/18, recent Oracle JDK7 also needs a larger stack.  Maybe we should just increase it to 256 across the board; with a fairly solid native protocol + HSHA story, keeping per-thread memory down isn't quite as important as it was when we started.

+1 on switching to OpenJDK 1.7 for Debian.","15/Aug/13 13:39;urandom;
{quote}
According to https://github.com/apache/cassandra/pull/18, recent Oracle JDK7 also needs a larger stack. Maybe we should just increase it to 256 across the board; with a fairly solid native protocol + HSHA story, keeping per-thread memory down isn't quite as important as it was when we started.
{quote}

That sounds reasonable to me, are we confident enough in this change to land it in cassandra-2.0.0? Or cassandra-2.0 (my comfort sweet-spot, I think)?",15/Aug/13 15:00;jbellis;2.0 WFM.,15/Aug/13 19:15;urandom;committed to cassandra-2.0 and trunk; closing,"27/Nov/13 17:04;cscetbon;I think you should update JVM_OPTS in test/cassandra.in.sh too and the comment message in conf/cassandra-env.sh to :
u45 and greater need need 256k",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Crash caused by insufficient disk space to flush,CASSANDRA-5605,12650385,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,dhendry,dhendry,31/May/13 19:08,12/Mar/19 14:04,13/Mar/19 22:29,18/Sep/13 17:45,1.2.10,2.0.1,,,,,2,,,,,"A few times now I have seen our Cassandra nodes crash by running themselves out of memory. It starts with the following exception:

{noformat}
ERROR [FlushWriter:13000] 2013-05-31 11:32:02,350 CassandraDaemon.java (line 164) Exception in thread Thread[FlushWriter:13000,5,main]
java.lang.RuntimeException: Insufficient disk space to write 8042730 bytes
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:42)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
{noformat} 

After which, it seems the MemtablePostFlusher stage gets stuck and no further memtables get flushed: 

{noformat} 
INFO [ScheduledTasks:1] 2013-05-31 11:59:12,467 StatusLogger.java (line 68) MemtablePostFlusher               1        32         0
INFO [ScheduledTasks:1] 2013-05-31 11:59:12,469 StatusLogger.java (line 73) CompactionManager                 1         2
{noformat} 

What makes this ridiculous is that, at the time, the data directory on this node had 981GB free disk space (as reported by du). We primarily use STCS and at the time the aforementioned exception occurred, at least one compaction task was executing which could have easily involved 981GB (or more) worth of input SSTables. Correct me if I am wrong but but Cassandra counts data currently being compacted against available disk space. In our case, this is a significant overestimation of the space required by compaction since a large portion of the data being compacted has expired or is an overwrite.

More to the point though, Cassandra should not crash because its out of disk space unless its really actually out of disk space (ie, dont consider 'phantom' compaction disk usage when flushing). I have seen one of our nodes die in this way before our alerts for disk space even went off.","java version ""1.7.0_15""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-05-31 19:27:20.182,,,no_permission,,,,,,,,,,,,330712,,,Wed Sep 18 17:45:39 UTC 2013,,,,,,0|i1l1zj:,331046,1.2.4,1.2.6,1.2.8,,,,,yukim,yukim,,,1.2.0 beta 1,,,,,,,"31/May/13 19:27;brandon.williams;Do you have multiple data directories?  If you only have one, it may have been blacklisted and marked read-only by a previous issue, can you check the logs for anything like that?",31/May/13 19:53;dhendry;We have only one data directory. There is nothing in the log about it being blacklisted.,"11/Jul/13 03:56;agundabattula;Am not sure if the following information helps but we too hit this issue in production today. We were running with cassandra 1.2.4 and two patches CASSANDRA-5554 & CASSANDRA-5418. 

We were running with RF=3 and LCS. 

We ran into this issue while using sstablelaoder to push data from  remote 1.2.4 cluster nodes to another cluster

We cross checked using JMX if blacklisting is the cause of this bug and it looks like it is definitely not the case. 

We however saw a pile up of pending compactions ~ 1800 pending compactions per node when node crashed. Surprising thing is that the ""Insufficient disk space to write xxxx bytes"" appears much before the node crashes. For us it started appearing aprrox 3 hours before the node crashed. 

The cluster which showed this behavior was having loads of writes occurring ( We were using multiple SSTableLoaders to stream data into this cluster. ). We pushed in almost 15 TB worth data ( including the RF =3 ) in a matter of 16 hours. We were not serving any reads from this cluster as we were still migrating data to it. 

Another interesting behavior observed that nodes were neighbors in most of the time. 

Am not sure if the above information helps but wanted to add it to the context of the ticket.  ","07/Aug/13 15:27;daubman;Apologies if this isn't directly relevant, but I seem to be experiencing the same issue using 1.2.8 launched via ccm for integration testing. One differentiating feature here is that this happened the first time the node was ever brought up (0 data). The integration tests attempting to use the ccm cluster never completed due to this hang, but all they do is test creation of a fairly simple schema and then attempt to write and then read back a single row. There is plenty of free disk space available...

Here's what was in the log:
 INFO [main] 2013-08-07 14:56:46,763 CassandraDaemon.java (line 118) Logging initialized
 INFO [main] 2013-08-07 14:56:46,807 CassandraDaemon.java (line 145) JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.7.0_25
 INFO [main] 2013-08-07 14:56:46,808 CassandraDaemon.java (line 183) Heap size: 8248098816/8248098816
 INFO [main] 2013-08-07 14:56:46,808 CassandraDaemon.java (line 184) Classpath: /opt/ccmlib_cassandra/ccm/lwcdbng_test_cluster/node1/conf:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/build/classes/main:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/build/classes/thrift:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/antlr-3.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/avro-1.4.0-fixes.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/avro-1.4.0-sources-fixes.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/commons-cli-1.1.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/commons-codec-1.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/commons-lang-2.6.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/compress-lzf-0.8.4.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/concurrentlinkedhashmap-lru-1.3.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/guava-13.0.1.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/high-scale-lib-1.1.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jackson-core-asl-1.9.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jackson-mapper-asl-1.9.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jamm-0.2.5.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jbcrypt-0.3m.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jline-1.0.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/json-simple-1.1.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/libthrift-0.7.0.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/log4j-1.2.16.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/lz4-1.1.0.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/metrics-core-2.0.3.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/netty-3.5.9.Final.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/servlet-api-2.5-20081211.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/slf4j-api-1.7.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/slf4j-log4j12-1.7.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/snakeyaml-1.6.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/snappy-java-1.0.5.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/snaptree-0.1.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jamm-0.2.5.jar
 INFO [main] 2013-08-07 14:56:46,822 CLibrary.java (line 65) JNA not found. Native methods will be disabled.
 INFO [main] 2013-08-07 14:56:46,891 DatabaseDescriptor.java (line 132) Loading settings from file:/opt/ccmlib_cassandra/ccm/lwcdbng_test_cluster/node1/conf/cassandra.yaml
 INFO [main] 2013-08-07 14:56:47,821 DatabaseDescriptor.java (line 150) Data files directories: [/opt/ccmlib_cassandra/ccm/lwcdbng_test_cluster/node1/data]
 INFO [main] 2013-08-07 14:56:47,822 DatabaseDescriptor.java (line 151) Commit log directory: /opt/ccmlib_cassandra/ccm/lwcdbng_test_cluster/node1/commitlogs
 INFO [main] 2013-08-07 14:56:47,822 DatabaseDescriptor.java (line 191) DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO [main] 2013-08-07 14:56:47,822 DatabaseDescriptor.java (line 205) disk_failure_policy is stop
 INFO [main] 2013-08-07 14:56:48,000 DatabaseDescriptor.java (line 273) Global memtable threshold is enabled at 2622MB
 INFO [main] 2013-08-07 14:56:49,142 DatabaseDescriptor.java (line 401) Not using multi-threaded compaction
 INFO [main] 2013-08-07 14:56:52,610 CacheService.java (line 111) Initializing key cache with capacity of 100 MBs.
 INFO [main] 2013-08-07 14:56:52,675 CacheService.java (line 140) Scheduling key cache save to each 14400 seconds (going to save all keys).
 INFO [main] 2013-08-07 14:56:52,678 CacheService.java (line 154) Initializing row cache with capacity of 0 MBs and provider org.apache.cassandra.cache.SerializingCacheProvider
 INFO [main] 2013-08-07 14:56:52,698 CacheService.java (line 166) Scheduling row cache save to each 0 seconds (going to save all keys).
 INFO [main] 2013-08-07 14:56:57,163 DatabaseDescriptor.java (line 535) Couldn't detect any schema definitions in local storage.
 INFO [main] 2013-08-07 14:56:57,164 DatabaseDescriptor.java (line 540) To create keyspaces and column families, see 'help create' in cqlsh.
 INFO [main] 2013-08-07 14:56:57,258 CommitLog.java (line 120) No commitlog files found; skipping replay
 INFO [main] 2013-08-07 14:56:57,715 StorageService.java (line 456) Cassandra version: 1.2.8-SNAPSHOT
 INFO [main] 2013-08-07 14:56:57,715 StorageService.java (line 457) Thrift API version: 19.36.0
 INFO [main] 2013-08-07 14:56:57,716 StorageService.java (line 458) CQL supported versions: 2.0.0,3.0.5 (default: 3.0.5)
 INFO [main] 2013-08-07 14:56:57,795 StorageService.java (line 483) Loading persisted ring state
 INFO [main] 2013-08-07 14:56:57,805 StorageService.java (line 564) Starting up server gossip
 WARN [main] 2013-08-07 14:56:57,823 SystemTable.java (line 573) No host ID found, created 6abf69c6-8472-4607-96b7-4a532ed8e2a8 (Note: This should happen exactly once per node).
 INFO [main] 2013-08-07 14:56:57,852 ColumnFamilyStore.java (line 630) Enqueuing flush of Memtable-local@2038679449(367/367 serialized/live bytes, 15 ops)
ERROR [FlushWriter:1] 2013-08-07 14:56:57,860 CassandraDaemon.java (line 192) Exception in thread Thread[FlushWriter:1,5,main]
java.lang.RuntimeException: Insufficient disk space to write 452 bytes
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:42)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,243 GCInspector.java (line 119) GC for ParNew: 2786 ms for 1 collections, 230275096 used; max is 8248098816
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,250 StatusLogger.java (line 53) Pool Name                    Active   Pending   Blocked
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,291 StatusLogger.java (line 68) MemtablePostFlusher               1         1         0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,292 StatusLogger.java (line 68) FlushWriter                       0         0         0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,293 StatusLogger.java (line 68) commitlog_archiver                0         0         0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,294 StatusLogger.java (line 73) CompactionManager                 0         0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,522 StatusLogger.java (line 85) MessagingService                n/a       0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,523 StatusLogger.java (line 95) Cache Type                     Size                 Capacity               KeysToSave                                                         Provider
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,524 StatusLogger.java (line 96) KeyCache                          0                104857600                      all                                                                 
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,525 StatusLogger.java (line 102) RowCache                          0                        0                      all              org.apache.cassandra.cache.SerializingCacheProvider
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,525 StatusLogger.java (line 109) ColumnFamily                Memtable ops,data
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,526 StatusLogger.java (line 112) system.local                              0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,526 StatusLogger.java (line 112) system.peers                              0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,527 StatusLogger.java (line 112) system.batchlog                           0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,527 StatusLogger.java (line 112) system.NodeIdInfo                         0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,528 StatusLogger.java (line 112) system.LocationInfo                       0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,528 StatusLogger.java (line 112) system.Schema                             0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,529 StatusLogger.java (line 112) system.Migrations                         0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,529 StatusLogger.java (line 112) system.schema_keyspaces                 8,251
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,530 StatusLogger.java (line 112) system.schema_columns               398,24717
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,530 StatusLogger.java (line 112) system.schema_columnfamilies           369,22187
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,530 StatusLogger.java (line 112) system.IndexInfo                          0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,531 StatusLogger.java (line 112) system.range_xfers                        0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,531 StatusLogger.java (line 112) system.peer_events                        0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,532 StatusLogger.java (line 112) system.hints                              0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,532 StatusLogger.java (line 112) system.HintsColumnFamily                  0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,532 StatusLogger.java (line 112) system_traces.sessions                    0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,533 StatusLogger.java (line 112) system_traces.events                      0,0
",16/Sep/13 14:46;jjordan;After CASSANDRA-4292 flushing checks the space reserved by compactions.  So check if you have a bunch of pending compactions with a huge size.,"16/Sep/13 16:33;jjordan;Have seen a lot of instances of people hitting this reported recently.  I think we probably shouldn't fail stuff if C* ""thinks"" it might not have enough free space, because stuff is reserved.  Probably a good idea to use that reserving information as a hint to which drive to pick for the JBOD case, but if no drive will fit the data when looking at reserved, pick the one with the most free space (like we used to).",17/Sep/13 18:55;jbellis;Patchset to avoid prematurely declaring ourselves out of space at https://github.com/jbellis/cassandra/commits/5605,"18/Sep/13 15:05;yukim;+1 to the patch.

Though (maybe in separate ticket?) we still need to add error handler to FlushRunnable so postExecutor does not get blocked.","18/Sep/13 15:17;jbellis;bq. we still need to add error handler to FlushRunnable so postExecutor does not get blocked

I'm not sure we want to unblock it -- if the flush errors out, then we definitely don't want commitlog segments getting cleaned up.  What did you have in mind?","18/Sep/13 15:32;yukim;Ah, right.
Just wondered if we can do something about preventing filling up postExecutor queue.","18/Sep/13 17:45;jbellis;Committed.  If you come up with a good idea there, let's open a new ticket.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQLSH exception handling could leave a session in a bad state,CASSANDRA-5481,12642733,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jpi,jpi,jpi,16/Apr/13 14:18,12/Mar/19 14:04,13/Mar/19 22:29,29/Jul/14 19:01,1.2.19,2.0.10,,,,,0,,,,,"Playing with CTRL+C in a cqlsh session can leave the (Thrift|Native) connection in a bad state.

To reproduce :
1) Run a long running COPY FROM command (COPY test (k, v) FROM '/tmp/test.csv')
2) Interrupt the importer with CTRL+C

Repeat step 1 and 2 until you start seeing weird things in the cql shell (see attached screenshot)

The reason is, I believe, the connection (and the cursor) is not correclty closed and reopened on interruption.

I am working to propose a fix.

Jordan",cqlsh 2.3.0 | Cassandra 1.2.4 | CQL spec 3.0.0 | Thrift protocol 19.35.0,,,,,,,,,,,,,,,,,,,,,,,17/Apr/13 08:17;jpi;5481.diff;https://issues.apache.org/jira/secure/attachment/12579110/5481.diff,16/Apr/13 14:20;jpi;CQLSession.png;https://issues.apache.org/jira/secure/attachment/12578942/CQLSession.png,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-07-29 16:35:21.827,,,no_permission,,,,,,,,,,,,323147,,,Tue Jul 29 22:39:21 UTC 2014,,,,,,0|i1jr4v:,323492,1.2.18,2.0.9,,,,,,brandon.williams,brandon.williams,,,,,,,,,,16/Apr/13 14:20;jpi;Broken CQL shell session,"16/Apr/13 22:46;jpi;Patch attached.

It also removes the need to escape the keyspace name since it is now handled by the driver","29/Jul/14 16:35;mshuler;[~enigmacurry] could we see if this repros on the major branches and if the patch helps? (patch might need to be fuzzed, if there's much change, due to the age of this ticket)","29/Jul/14 18:30;philipthompson;The issue reproduces on 1.2-HEAD and 2.0-HEAD, but not on 2.1. The patch still applies relatively cleanly on 1.2 and 2.0, and fixes the issue on both.",29/Jul/14 19:01;brandon.williams;Committed.,"29/Jul/14 22:39;jpi;Great !
Thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W/O specified columns ASPCSI does not get notified of deletes,CASSANDRA-5614,12651026,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,beobal,bcoverston,bcoverston,05/Jun/13 02:29,12/Mar/19 14:04,13/Mar/19 22:29,06/Aug/13 19:17,2.0.1,,,,,,0,,,,,"I'm working on a secondary index implementation based on the composite index type.

AbstractSimplePerColumnSecondaryIndex.java#delete is not called when CQL delete statements do not specify columns.

When I specify columns it is called. Pretty sure this is a bug.

Setup:
{code}
cqlsh> create KEYSPACE foo WITH replication = {'class': 'SimpleStrategy' , 'replication_factor': 1};
cqlsh> use foo;
cqlsh:foo> CREATE TABLE albums (artist text, album text, rating int, release int, PRIMARY KEY (artist, album));
cqlsh:foo> CREATE INDEX ON albums (rating);
{code}

{code}
cqlsh:foo> insert into albums (artist, album, rating, release) VALUES ('artist', 'album', 1, 2);
{code}

Does not get called here:
{code}
cqlsh:foo> DELETE FROM albums where artist='artist' and album='album';
{code}

{code}
cqlsh:foo> insert into albums (artist, album, rating, release) VALUES ('artist', 'album', 1, 2);
{code}

gets called here:
{code}
cqlsh:foo> DELETE rating FROM albums where artist='artist' and album='album';
{code}
",,,,,,,,,,,,,,,,,,,,,,,,23/Jul/13 18:42;beobal;0001-CASSANDRA-5614-PreCompactedRow-updates-2i-correctly.patch;https://issues.apache.org/jira/secure/attachment/12593757/0001-CASSANDRA-5614-PreCompactedRow-updates-2i-correctly.patch,23/Jul/13 18:42;beobal;0002-CASSANDRA-5614-LazilyCompactedRow-outputs-SSTables-a.patch;https://issues.apache.org/jira/secure/attachment/12593758/0002-CASSANDRA-5614-LazilyCompactedRow-outputs-SSTables-a.patch,23/Jul/13 18:42;beobal;0003-CASSANDRA-5614-Memtable-updates-with-RowTombstone-up.patch;https://issues.apache.org/jira/secure/attachment/12593759/0003-CASSANDRA-5614-Memtable-updates-with-RowTombstone-up.patch,24/Jul/13 08:11;beobal;0004-CASSANDRA-5614-Consider-timestamps-when-checking-col.patch;https://issues.apache.org/jira/secure/attachment/12593884/0004-CASSANDRA-5614-Consider-timestamps-when-checking-col.patch,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-06-05 14:19:09.226,,,no_permission,,,,,,,,,,,,331352,,,Tue Aug 06 19:17:45 UTC 2013,,,,,,0|i1l5xb:,331685,,,,,,,,jbellis,jbellis,,,,,,,,,,"05/Jun/13 14:19;jbellis;We probably need to ""expand"" row deletes into column tombstones for the benefit of the index.","05/Jun/13 15:53;bcoverston;Instead of expanding them, at least for the purposes of what I'm doing, I would be OK with just getting keybytes, and column if column had a special marker for a row tombstone. That way I know the whole row was deleted up front (and I can take one action) rather than getting a notification per column.","05/Jun/13 17:42;jbellis;(Oops, wrong Sam.)","12/Jul/13 14:19;beobal;I'm tempted to say that this is not a problem because it works as expected with both KeysIndex & CompositesIndex. Yes, the presence of range rather than individual column tombstones in the DeletionInfo means that the index isn't updated directly at update time, but the index repair-on-read compensates for that at query time. 

We could take the deletion info from the update & use that in some new deleteAll method on SIM.Updater which iterates through the indexed columns issuing deletes, but AFAICT that would require reads to aquire the existing indexed values.","12/Jul/13 21:44;jbellis;I don't see how this is different from ""pretend the user had deleted the indexed columns by name"" as far as read-before-write goes.
","12/Jul/13 22:56;beobal;To delete from the 2i cf you need the old column value to derive the 2i cf key. When a column is deleted by name and that column is present in the memtable, we have the value and so can do that. When the named column isn't in the memtable, then we don't have the old value so we skip the delete and treat the operation as an insert. In SIM.StandardUpdater this is a no-op when isMarkedForDelete() == true, so even when the column is named if it's not in the memtable ASPCSI.delete is never called.","16/Jul/13 22:07;jbellis;bq. When a column is deleted by name and that column is present in the memtable, we have the value and so can do that. When the named column isn't in the memtable, then we don't have the old value

Right, but we know the old value is in the sstable and will be cleaned out on compaction.

So I still don't see what would break here if we ""expand"" the tombstone to the indexed columns -- the same logic applies, we either get it from the memtable or at compaction time.

(I *think* we apply row tombstones to indexed data correctly in compaction already -- if so, all we need to do is handle the memtable case, and not actually create extra tombstones.)","23/Jul/13 18:43;beobal;First, we aren't doing the right thing in compaction either. Neither PCR or LCR clean up the index in the face of range tombstones, relying on repairs at read time to purge obsolete entries.
Looking into this, it also seems that PCR & LCR actually generate different SSTables. Although they're functionally equivalent, those that come from PCR don't have any columns covered by a RangeTombstone. SSTables generated by LCR do contain these redundant extra columns.

eg: if we have two SSTables:
{code}
[{""key"": ""6e697276616e61"",""columns"": 
	[
		[""bleach:"","""",1374246211348000], 
		[""bleach:rating"",""4"",1374246211348000], 
		[""bleach:release"",""1"",1374246211348000], 
		[""nevermind:"","""",1374246211365000], 
		[""nevermind:rating"",""5"",1374246211365000], 
		[""nevermind:release"",""2"",1374246211365000]
	]
}]

[{""key"": ""6e697276616e61"",""columns"": 
	[
		[""nevermind"",""nevermind:!"",1374246212053000,""t"",1374246212]
	]
}]
{code}

When compacted with PCR we end up with in :
{code}
[{
	""key"": ""6e697276616e61"",
	""columns"": [
		[""bleach:"","""",1374246658577000], 
		[""bleach:rating"",""4"",1374246658577000], 
		[""bleach:release"",""1"",1374246658577000], 
		[""nevermind"",""nevermind:!"",1374246659274000,""t"",1374246659]
	]
}]
{code}

But with LCR we get:
{code}
[{
	""key"": ""6e697276616e61"",
	""columns"": [
		[""bleach:"","""",1374246211348000], 
		[""bleach:rating"",""4"",1374246211348000], 
		[""bleach:release"",""1"",1374246211348000], 
		[""nevermind"",""nevermind:!"",1374246212053000,""t"",1374246212], 
		[""nevermind:"","""",1374246211365000], 
		[""nevermind:rating"",""5"",1374246211365000], 
		[""nevermind:release"",""2"",1374246211365000]
	]
}]
{code}

The first patch fixes PCR to clean up indexes properly.  
The second fixes LCR so that it generates SSTables like PCR & fixes its index cleanup. 
The third adds the index cleanup for memtable updates with RT.",24/Jul/13 02:52;jbellis;Thanks! Do you have a github branch for this?,"24/Jul/13 08:11;beobal;I realised I'd missed the case where the timestamp on a column is greater than the RT's max, so attaching a fourth patch which handles that. 
My github branch is https://github.com/beobal/cassandra/tree/5614
","06/Aug/13 19:17;jbellis;Rebased and committed to 2.0.1.  The most significant change was to include columns in the same mutation as the tombstone in the loop here:

{code}
                for (Column currentColumn : Iterables.concat(current.map.values(), cm))
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ORDER BY desc breaks cqlsh COPY,CASSANDRA-5610,12650728,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,jjordan,jjordan,03/Jun/13 20:40,12/Mar/19 14:04,13/Mar/19 22:29,03/Jun/13 22:00,1.2.6,,,Legacy/Tools,,,0,cqlsh,,,,"If you have a reversed text field, COPY chokes on it because the type is
""'org.apache.cassandra.db.marshal.ReversedType'<text>"" not just 'text' so the strings don't get quoted in the generated CQL.

{noformat}
    def do_import_row(self, columns, nullval, layout, row):
        rowmap = {}
        for name, value in zip(columns, row):
            if value != nullval:
                type = layout.get_column(name).cqltype.cql_parameterized_type()
                if type in ('ascii', 'text', 'timestamp', 'inet'):
                    rowmap[name] = self.cql_protect_value(value)
                else:
                    rowmap[name] = value
            else:
                rowmap[name] = 'null'
        return self.do_import_insert(layout, rowmap)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,03/Jun/13 21:54;iamaleksey;5610.txt;https://issues.apache.org/jira/secure/attachment/12585963/5610.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-03 21:55:59.501,,,no_permission,,,,,,,,,,,,331055,,,Mon Jun 03 22:00:38 UTC 2013,,,,,,0|i1l43b:,331388,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,03/Jun/13 21:55;brandon.williams;+1,"03/Jun/13 22:00;iamaleksey;Thanks, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception swallowing in ..net.MessagingService,CASSANDRA-5644,12653022,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,metskem,metskem,metskem,15/Jun/13 13:29,12/Mar/19 14:04,13/Mar/19 22:29,15/Jun/13 15:44,1.2.6,,,Local/Config,,,0,,,,,"While I was trying to setup internode encryption, I spent too much time finding out that the name of my keystore was wrong.
Main reason was the org/apache/cassandra/net/MessagingService.java swallowing the exception and just spitting out:
{noformat}
ERROR [main] 2013-06-15 12:49:43,758 CassandraDaemon.java (line 358) Fatal configuration error
org.apache.cassandra.exceptions.ConfigurationException: Unable to create ssl socket
	at org.apache.cassandra.net.MessagingService.getServerSocket(MessagingService.java:432)
	at org.apache.cassandra.net.MessagingService.listen(MessagingService.java:412)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:564)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:529)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:428)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:354)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
Unable to create ssl socket
{noformat}

I will attach a minor patch, that shows just a bit more:

{noformat}
ERROR [main] 2013-06-15 12:58:44,979 CassandraDaemon.java (line 358) Fatal configuration error
org.apache.cassandra.exceptions.ConfigurationException: Unable to create ssl socket
	at org.apache.cassandra.net.MessagingService.getServerSocket(MessagingService.java:432)
	at org.apache.cassandra.net.MessagingService.listen(MessagingService.java:412)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:564)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:529)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:428)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:354)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
Caused by: java.io.IOException: Error creating the initializing the SSL Context
	at org.apache.cassandra.security.SSLFactory.createSSLContext(SSLFactory.java:124)
	at org.apache.cassandra.security.SSLFactory.getServerSocket(SSLFactory.java:53)
	at org.apache.cassandra.net.MessagingService.getServerSocket(MessagingService.java:428)
	... 7 more
Caused by: java.io.FileNotFoundException: conf/oeps-wrong-truststore.jks (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:97)
	at org.apache.cassandra.security.SSLFactory.createSSLContext(SSLFactory.java:105)
	... 9 more
Unable to create ssl socket
{noformat}

kind regards,
Harry
","RedHat Linux, OpenJDK 7",,,,,,,,,,,,,,,,,,,,,,,15/Jun/13 13:30;metskem;CASSANDRA-5644.patch;https://issues.apache.org/jira/secure/attachment/12587976/CASSANDRA-5644.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-15 15:44:49.821,,,no_permission,,,,,,,,,,,,333345,,,Sat Jun 15 15:44:49 UTC 2013,,,,,,0|i1li5z:,333673,,,,,,,,dbrosius,dbrosius,,,,,,,,,,"15/Jun/13 15:44;dbrosius;Thanks!

committed to cassandra-1.2 as 082684dd806a0bf0964901e3749992e4b1a1d650",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add get commands to nodetool for things with set,CASSANDRA-5588,12648965,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,michalm,jjordan,jjordan,22/May/13 21:14,12/Mar/19 14:04,13/Mar/19 22:29,28/May/13 18:55,1.2.6,,,Tool/nodetool,,,1,lhf,,,,"Can we add:
nodetool getcompactionthroughput
nodetool getstreamthroughput

To go with the set commands?  You currently have to fire up a JMX client to know what the current values are.
",,,,,,,,,,,,,,,,,,,,,,,,24/May/13 08:30;michalm;5588-extended-info-v1.patch;https://issues.apache.org/jira/secure/attachment/12584660/5588-extended-info-v1.patch,24/May/13 08:30;michalm;5588-new-commands-v1.patch;https://issues.apache.org/jira/secure/attachment/12584661/5588-new-commands-v1.patch,25/May/13 08:09;michalm;5588-new-commands-v2.patch;https://issues.apache.org/jira/secure/attachment/12584826/5588-new-commands-v2.patch,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-05-23 06:58:22.794,,,no_permission,,,,,,,,,,,,329294,,,Tue May 28 18:55:15 UTC 2013,,,,,,0|i1kt9r:,329631,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"23/May/13 06:58;michalm;My first thought - do we really need a separate commands for both of them? Maybe we could include it in nodetool info's output, like it's done for cache? ","23/May/13 16:22;brandon.williams;That's not a bad idea, but we actually already have getcompactionthroughput, so simply adding getstreamthroughput would be most consistent.","23/May/13 16:33;michalm;Yes, but please note that getcompactionthreshold (I guess you meant it) is called with two additional params (KS and CF), so it wouldn't make sense to mix with node-wide ""info"" command. 
Anyway, I'm fine with both options, so I'll do it as stated in description if you prefer so :-)","23/May/13 16:37;jjordan;We don't have getcompactionthroughput, we have getcompactionthreshold.

We do have a call to getCompactionThroughput from the time estimation code, but the getcompactionthroughput command is not exposed to the end user.

Putting them in info works for me, we already have the global cache stuff there. ","24/May/13 08:29;michalm;It wasn't too much work to do both, so I'm attaching two patches - pick whatever you like ;-)

Both patches do one more change for consistency's sake - move getCompactionThreshold() from NodeProbe to NodeCmd and rename it to printCompactionThreshold(), so almost all the printing (does not apply to RepairRunner) is done in NodeCmd.","24/May/13 19:29;brandon.williams;Can you rebase new-commands-v1 against the 1.2 branch?  I think I like that one better at this point, since anytime someone sees a set<something> they're going to be expecting a get<something> and it more closely matches the underlying JMX model.",25/May/13 08:09;michalm;Sure :-) Attaching v2 rebased onto 67e57a974f7fe5f7b950fa370b665ba828f6e5e1.,"28/May/13 18:55;brandon.williams;Committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayIndexOutOfBoundsException in LeveledManifest,CASSANDRA-5589,12649106,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jeromatron,jeromatron,23/May/13 12:36,12/Mar/19 14:04,13/Mar/19 22:29,23/May/13 19:51,1.2.6,,,,,,0,compaction,,,,"The following stack trace was in the system.log:

{quote}
ERROR [CompactionExecutor:2] 2013-05-22 16:19:32,402 CassandraDaemon.java (line 174) Exception in thread Thread[CompactionExecutor:2,1,main]
 java.lang.ArrayIndexOutOfBoundsException: 5
	at org.apache.cassandra.db.compaction.LeveledManifest.skipLevels(LeveledManifest.java:176)
	at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:215)
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:155)
	at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:410)
	at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:223)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:991)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:230)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:188)
{quote}",,,,,,,,,,,,,,,,,,,,,,,,23/May/13 15:33;jbellis;5589.txt;https://issues.apache.org/jira/secure/attachment/12584513/5589.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-23 15:33:55.146,,,no_permission,,,,,,,,,,,,329433,,,Thu Sep 25 15:04:46 UTC 2014,,,,,,0|i1ku3r:,329768,,,,,,,,krummas,krummas,,,,,,,,,,"23/May/13 15:33;jbellis;The manifest is assuming that when the sstable size is increased, existing sstables are magically resized to that, which is not the case.

Fix attached, with an additional warning to not increase sstable size above 1GB (which is what the user has done here).",23/May/13 16:31;krummas;lgtm,23/May/13 19:51;jbellis;committed,"25/Sep/14 15:04;jeffery.griffith;hi [~jbellis], it looks like this problem may not have been the total # of generations allocated because I can still see this in 1.2.19. The stack trace is the same as Jeremy's however the index out of bounds is 9 (log 10 of 1 billion=9 from your fix.) skipLevels does not check newLevel against generations.length however the fix obviously isn't that simple since it needs to return the newLevel... some other logic problem in the loop termination?

    private int skipLevels(int newLevel, Iterable<SSTableReader> added)
    {
        while (maxBytesForLevel(newLevel) < SSTableReader.getTotalBytes(added)
            && generations[(newLevel + 1)].isEmpty())
        {
            newLevel++;
        }
        return newLevel;
    }

Apparently, this seems to have begun when someone set the max sstable size to 400MB. Going back to 300MB it seems to have gone away.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossiper incorrectly drops AppState for an upgrading node,CASSANDRA-5660,12653697,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jasobrown,jasobrown,jasobrown,19/Jun/13 13:07,12/Mar/19 14:04,13/Mar/19 22:29,19/Jun/13 19:24,1.2.6,2.0 beta 1,,,,,0,gossip,upgrade,,,"When doing a major rev upgrade (from 1.1 to 1.2, for example), after a node is upgraded to the new version, the older nodes incorrectly drop ApplicationState about the new nodes when the new ones attempt to connect. The short story is the older node doesn't allow the new node to connect because it contains a higher message protocol version number, and will call Gossiper.addSavedEndpoint(), which overwrites any previous entry in the endpointStateMap. 

Here's a fuller description of the steps:

0) have a stable, multi-datacenter cluster running 1.1.x 
1) upgrade one node to 1.2, and bring it up 
2) 1.2 node starts to gossip 
3) 1.1 node gets request in IncomingTcpConnection. 1.1 checks the messaging version, sees that it is greater than it's own, and calls Gossiper.addSavedEnpoint, which overwrites any previous entry in endpointStateMap. 1.1 closes connection 
4) 1.1 node chooses to gossip with 1.2 (as it kept the InetAddr around), on 1.2's publicIp. As standard part of request header, it sends it's version along 
5) 1.2 node ACKs the gossip request, and sends along the message version of 1.1 in it's header 
6) 1.1 can successfully accept the ACK connection and Gossiper.handleMajorStateChange() is called and the 1.2 node is fully 'alive' to this 1.1 node 
6a) if you are using Ec2MultiRegionSnitch, it's onAlive/onJoin methods are called, and eventually onChange. If 1.2 node is in same region as 1.1, 
1.1 node will close the socket on the publicIP, and attempt to connect on localIp 
7) when 1.2 reads on publicIp socket it will find it has been closed (broken pipe). It then closes its end of socket and calls Gossiper.resetVersion(). 
This throws away the previously recorded messaging version for the 1.1 node (Gossiper.versions). Thus, when 1.2 goes to communicate back with 1.1, 
it sends it's message protocol version (instead of 1.1's), and we're back at step 3 again.

For most use cases, there is an very short term problem of the AppState getting dropped, but that will be quickly fixed when the lower version node connects to the newer node (and passes it's message protocol version). However, everything goes to hell in a handbasket when the Ec2MultiRegionSnitch gets involved (steps 6a/7 above) and the connections keep getting closed.",,,,,,,,,,,,,,,,,,,,,,,,19/Jun/13 13:09;jasobrown;0001-5660.diff;https://issues.apache.org/jira/secure/attachment/12588586/0001-5660.diff,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-19 17:50:49.753,,,no_permission,,,,,,,,,,,,333974,,,Wed Jun 19 19:24:06 UTC 2013,,,,,,0|i1lm1b:,334300,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"19/Jun/13 13:09;jasobrown;Attached patch changes the logic in Gossiper.addSavedEndpoint() to check if there is already an existing entry in the endointStateMap, and if so, just reset it's heartbeat version. This will keep around any previous ApplicationState info.",19/Jun/13 17:50;brandon.williams;+1,19/Jun/13 19:24;jasobrown;committed to 1.2 and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in detecting version on a mixed 1.1/1.2 cluster,CASSANDRA-5692,12654375,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,sbtourist,sbtourist,sbtourist,23/Jun/13 12:57,12/Mar/19 14:04,13/Mar/19 22:29,27/Jun/13 20:40,1.2.7,2.0 beta 1,,,,,0,,,,,"On a mixed 1.1 / 1.2 cluster, starting 1.2 nodes fires sometimes a race condition in version detection, where the 1.2 node wrongly detects version 6 for a 1.1 node.

It works as follows:
1) The just started 1.2 node quickly opens an OutboundTcpConnection toward a 1.1 node before receiving any messages from the latter.
2) Given the version is correctly detected only when the first message is received, the version is momentarily set at 6.
3) This opens an OutboundTcpConnection from 1.2 to 1.1 at version 6, which gets stuck in the connect() method.

Later, the version is correctly fixed, but all outbound connections from 1.2 to 1.1 are stuck at this point.

Evidence from 1.2 logs:
TRACE 13:48:31,133 Assuming current protocol version for /127.0.0.2
DEBUG 13:48:37,837 Setting version 5 for /127.0.0.2",,,,,,,,,,,,,,,,,,,,,,,,24/Jun/13 12:52;sbtourist;5692-0005.patch;https://issues.apache.org/jira/secure/attachment/12589397/5692-0005.patch,25/Jun/13 17:23;sbtourist;5692-0006.patch;https://issues.apache.org/jira/secure/attachment/12589627/5692-0006.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-06-23 22:46:15.977,,,no_permission,,,,,,,,,,,,334652,,,Tue Oct 15 16:53:38 UTC 2013,,,,,,0|i1lq6v:,334978,,,,,,,,jasobrown,jasobrown,,,,,,,,,,"23/Jun/13 13:01;sbtourist;I believe the correct thing to do is to _never_ assume a version when opening an outbound connection: rather, retrying a few times waiting for the version to be detected, and eventually failing fast.",23/Jun/13 13:33;sbtourist;Attaching patch with fail fast behaviour if no version is known yet after 30 seconds.,"23/Jun/13 15:23;sbtourist;Attaching a second patch, cleaner and more correct imho.","23/Jun/13 16:15;sbtourist;Third patch, even better :)","23/Jun/13 16:34;sbtourist;Cleaned up things, left only the two relevant versions.","23/Jun/13 22:46;cdaw;[~sbtourist]
i can apply patch 001 or 004 successfully, but not able to apply 001 + 004.   Do I need both?

{noformat}
# via git apply
$ git reset --hard HEAD
HEAD is now at 0f7c7dc Merge pull request #15 from riptano/CASSANDRA-5476

$ git apply --whitespace=fix 5692-0001.patch 5692-0004.patch
5692-0001.patch:37: trailing whitespace.

5692-0004.patch:89: trailing whitespace.

5692-0004.patch:133: trailing whitespace.

5692-0004.patch:138: trailing whitespace.

5692-0004.patch:143: trailing whitespace.

error: patch failed: src/java/org/apache/cassandra/net/OutboundTcpConnection.java:278
error: src/java/org/apache/cassandra/net/OutboundTcpConnection.java: patch does not apply

# via unix patch
$ git reset --hard HEAD
HEAD is now at 0f7c7dc Merge pull request #15 from riptano/CASSANDRA-5476

$ patch -l -p1 < 5692-0001.patch
patching file src/java/org/apache/cassandra/net/OutboundTcpConnection.java

$ patch -l -p1 < 5692-0004.patch
patching file src/java/org/apache/cassandra/net/MessagingService.java
patching file src/java/org/apache/cassandra/net/OutboundTcpConnection.java
Hunk #1 FAILED at 278.
1 out of 1 hunk FAILED -- saving rejects to file src/java/org/apache/cassandra/net/OutboundTcpConnection.java.rej

$ cat src/java/org/apache/cassandra/net/OutboundTcpConnection.java.rej
***************
*** 278,284 ****
          if (logger.isDebugEnabled())
              logger.debug(""attempting to connect to "" + poolReference.endPoint());

-         targetVersion = MessagingService.instance().getVersion(poolReference.endPoint());

          long start = System.currentTimeMillis();
          while (System.currentTimeMillis() < start + DatabaseDescriptor.getRpcTimeout())
--- 278,284 ----
          if (logger.isDebugEnabled())
              logger.debug(""attempting to connect to "" + poolReference.endPoint());

+         targetVersion = MessagingService.instance().getVersion(poolReference.endPoint(), true);

          long start = System.currentTimeMillis();
          while (System.currentTimeMillis() < start + DatabaseDescriptor.getRpcTimeout())
{noformat} ","23/Jun/13 22:56;sbtourist;[~cdaw], those are two different versions of the same patch, you don't need both: it's either one or the other, up to the reviewer for discussion, I second the latter.","24/Jun/13 12:52;sbtourist;Given the first message which should setup the version is sent along the same connection, this patch doesn't actually work, causing two 1.2 nodes to block each other during bootstrap.

So I'm attaching a different patch (0005), which implements a simple handshake by assuming version 6 and trying to read the actual version on a different thread, so that it can be interrupted (disconnected) and can retry the handshake until one of the following happens:
1) The version is confirmed to be >= 6, and the handshake succeeds.
2) The version is an old one, hence it is expected to be found among the tracked versions when the first gossip message is received.

Sorry for all the different patches, but the implementation details of all the version exchange machinery turned out to be quite subtle.","25/Jun/13 13:49;jasobrown;[~tourist], can you rebase against 1.2? I can't apply the patch to either 1.2 nor trunk. thanks.",25/Jun/13 17:23;sbtourist;Rebased patch on top of cassandra-1.2 branch.,"27/Jun/13 19:47;jasobrown;On the whole, lgtm, but I'm not sure about the InterruptedException part. From my reading of the CDL javadoc, the IE is thrown ""if the current thread is interrupted while waiting"". This is indicates that the waiting thread, the one calling versionLatch.await in your code, is being interrupted, not the Handshake thread. Everywhere else in OTC where we catch IE, we throw an AssertionError - basically stop execution of the method and bail. I'm not sure we would we do something different in handshakeVersion(), as if we got an interrupt I think we'd want to bail out, as well, and try to to finish connecting to the remote node.

Also, as a minor nit, I moved the versionLatch.countDown() into a finally block, as we want to unblock the waiting thread regardless of success or failure to read the version from the socket.","27/Jun/13 20:06;sbtourist;bq. From my reading of the CDL javadoc, the IE is thrown ""if the current thread is interrupted while waiting"". This is indicates that the waiting thread, the one calling versionLatch.await in your code, is being interrupted, not the Handshake thread.

Correct. That was to account for spurious interrupts, but I'm fine with adhering to conventions and throwing AE.

bq. Also, as a minor nit, I moved the versionLatch.countDown() into a finally block, as we want to unblock the waiting thread regardless of success or failure to read the version from the socket.

It would have go unblocked after the timeout, but actually better to fail/unblock as fast as possible :)","27/Jun/13 20:39;jasobrown;Cool, thanks for the confirmation.

Committed to 1.2 and trunk. Thanks!","15/Oct/13 06:26;jjordan;This does not seem to be fixed (or there is another race condition) as of 1.2.10.  Just saw this happen that during an upgrade 10 node cluster, 5 in each DC.  There were 6 nodes, 3 in each DC, seeing 4 nodes, 2 in each DC as the wrong version.  This was causing timeout failures, and describe cluster failures (only from the nodes seen as being on the wrong version).  Restarting the ""wrong version"" nodes didn't fix anything.  We had to restart the 6 nodes to get them to re-detect version, and then things started working.","15/Oct/13 07:30;sbtourist;[~jjordan], do we have thread dumps from the timeout failures (prior the timeout)? If that didn't involve the connect method, we're probably seeing a different race.

Anyways, I'll have a look.","15/Oct/13 14:53;jjordan;[~sbtourist] no thread dumps.  given nodes e1-e5.  Queries to e1, and ""describe cluster"" on e1 showed all nodes.  Queries to e5 would usually timeout, and ""describe cluster;"" on e5 would show e1-e3 as ""UNAVAILABLE"".  netstat -tn | grep <e1 ip> on e5 showed:

{noformat}
e5:7000 <-> e1:<high port>
e5:<high port> <-> e1:7000
e5:<high port 2> <-> e1:7000
{noformat}

Where e5 to a node which responded to the ""describe cluster;"" showed:

{noformat}
e5:7000 <-> e4:<high port>
e5:7000 <-> e4:<high port 2>
e5:<high port> <-> e4:7000
e5:<high port 2> <-> e4:7000
{noformat}

And in the TRACE level logs for IncomingTcpConnection.java when restarting e5 I only see one connection come in from e1, but there are two that come in from e4.  When I enabled TRACE level logs for e1, it was just spitting out something about ""version 5"" over and over really fast.  At that point we restarted e1, and it came up and everything was happy from e1->e5, so then we did a rolling restart on the cluster and went to bed.
","15/Oct/13 16:53;jjordan;Actually, I didn't read this ticket right, this is something different.  The nodes are all on 1.2 already, but being seen as version 5 (1.1).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh should support collections in COPY FROM,CASSANDRA-5698,12654636,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,kisehireoshi,kisehireoshi,25/Jun/13 01:12,12/Mar/19 14:04,13/Mar/19 22:29,25/Jul/13 18:48,1.2.8,2.0 rc1,,Legacy/Tools,,,0,collections,cqlsh,,,"Concrete operation is as follows.
---------*---------*---------*---------*---------*---------*---------*---------*
(1)map type's export/import
<export>
[root@castor bin]# ./cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 3.0.2 | Cassandra 1.2.5 | CQL spec 3.0.0 | Thrift protocol 19.36.0]
Use HELP for help.
cqlsh> create keyspace maptestks with replication  = { 'class' : 'SimpleStrategy', 'replication_factor' : '1' };
cqlsh> use maptestks;
cqlsh:maptestks> create table maptestcf (rowkey varchar PRIMARY KEY, targetmap map<varchar,varchar>);
cqlsh:maptestks> insert into maptestcf (rowkey, targetmap) values ('rowkey',{'mapkey':'mapvalue'});
cqlsh:maptestks> select * from maptestcf;

 rowkey | targetmap
--------+--------------------
 rowkey | {mapkey: mapvalue}
cqlsh:maptestks>  copy maptestcf to 'maptestcf-20130619.txt';
1 rows exported in 0.008 seconds.
cqlsh:maptestks> exit;

[root@castor bin]# cat maptestcf-20130619.txt
rowkey,{mapkey: mapvalue}
　　　　　　　　　　　　　　　　　　　　　　　　　　　<------------------------(a)
<import>
[root@castor bin]# ./cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 3.0.2 | Cassandra 1.2.5 | CQL spec 3.0.0 | Thrift protocol 19.36.0]
Use HELP for help.
cqlsh> create keyspace mapimptestks with replication  = { 'class' : 'SimpleStrategy', 'replication_factor' : '1' };
cqlsh> use mapimptestks;
cqlsh:mapimptestks> create table mapimptestcf (rowkey varchar PRIMARY KEY, targetmap map<varchar,varchar>);

cqlsh:mapimptestks> copy mapimptestcf from ' maptestcf-20130619.txt ';
Bad Request: line 1:83 no viable alternative at input '}'
Aborting import at record #0 (line 1). Previously-inserted values still present.
0 rows imported in 0.025 seconds.
---------*---------*---------*---------*---------*---------*---------*---------*
(2)list type's export/import
<export>
[root@castor bin]#./cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 3.0.2 | Cassandra 1.2.5 | CQL spec 3.0.0 | Thrift protocol 19.36.0]
Use HELP for help.
cqlsh> create keyspace listtestks with replication  = { 'class' : 'SimpleStrategy', 'replication_factor' : '1' };
cqlsh> use listtestks;
cqlsh:listtestks> create table listtestcf (rowkey varchar PRIMARY KEY, value list<varchar>);
cqlsh:listtestks> insert into listtestcf (rowkey,value) values ('rowkey',['value1','value2']);
cqlsh:listtestks> select * from listtestcf;

 rowkey | value
--------+------------------
 rowkey | [value1, value2]

cqlsh:listtestks> copy listtestcf to 'listtestcf-20130619.txt';
1 rows exported in 0.014 seconds.
cqlsh:listtestks> exit;

[root@castor bin]# cat listtestcf-20130619.txt
rowkey,""[value1, value2]""
　　　　　　　　　　　　　　　　　　　　　　　　　　　<------------------------(b)
<export>
[root@castor bin]# ./cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 3.0.2 | Cassandra 1.2.5 | CQL spec 3.0.0 | Thrift protocol 19.36.0]
Use HELP for help.
cqlsh> create keyspace listimptestks with replication  = { 'class' : 'SimpleStrategy', 'replication_factor' : '1' };
cqlsh> use listimptestks;
cqlsh:listimptestks> create table listimptestcf (rowkey varchar PRIMARY KEY, value list<varchar>);
cqlsh:listimptestks> copy listimptestcf from ' listtestcf-20130619.txt ';
Bad Request: line 1:79 no viable alternative at input ']'
Aborting import at record #0 (line 1). Previously-inserted values still present.
0 rows imported in 0.030 seconds.
---------*---------*---------*---------*---------*---------*---------*---------*
Reference: (correct, or error, in another dimension)

Manually, I have rewritten the export file.
[root@castor bin]# cat nlisttestcf-20130619.txt
rowkey,""['value1',' value2']""

....
cqlsh:listimptestks> copy listimptestcf from 'nlisttestcf-20130619.txt';
1 rows imported in 0.035 seconds.

cqlsh:listimptestks> select * from implisttestcf;
 rowkey | value
--------+------------------
 rowkey | [value1, value2]
cqlsh:implisttestks> exit;

[root@castor bin]# cat nmaptestcf-20130619.txt
rowkey,”{'mapkey': 'mapvalue'}”

[root@castor bin]# ./cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 3.0.2 | Cassandra 1.2.5 | CQL spec 3.0.0 | Thrift protocol 19.36.0]
Use HELP for help.
cqlsh> use  mapimptestks;
cqlsh:mapimptestks> copy mapimptestcf from 'nmaptestcf-20130619.txt';
1 rows imported in 0.023 seconds.
cqlsh:mapimptestks> select * from mapimptestcf;

 rowkey | targetmap
--------+--------------------
 rowkey | {mapkey: mapvalue}

(It appears to be as normal processing.)
---------*---------*---------*---------*---------*---------*---------*---------*
Please confirm from the operation described above.
Comparing the above (a) and (b), in the data format of export file, 
only the presence or absence of (""), 
it suggests a lack of consistency of the treatment(?).
","Using the Copy of cqlsh, the data included the “{“ and “[“ (= CollectionType) case,
I think in the export / import process, data integrity is compromised.
",,,,,,,,,,,,,,,,,,,,,,,21/Jul/13 17:49;iamaleksey;5698.txt;https://issues.apache.org/jira/secure/attachment/12593424/5698.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-25 06:47:14.977,,,no_permission,,,,,,,,,,,,334913,,,Thu Jul 25 18:48:13 UTC 2013,,,,,,0|i1lrs7:,335237,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"25/Jun/13 06:47;iamaleksey;Nothing is compromised, but collections import isn't supported by cqlsh, yet, if either keys or values require quoting (ascii, text(varchar), timestamp, and inet types).",25/Jun/13 07:07;jbellis;Should we tag it 2.0.1 instead of closing?  Seems reasonably important for COPY to be feature-complete to me.,25/Jun/13 07:12;iamaleksey;As you wish,25/Jul/13 16:22;brandon.williams;+1,"25/Jul/13 18:48;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh doesn't allow semicolons in BATCH statements,CASSANDRA-5697,12654635,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,rspitzer,rspitzer,25/Jun/13 00:59,12/Mar/19 14:04,13/Mar/19 22:29,08/Jul/13 20:36,1.2.7,2.0 beta 1,,Legacy/Tools,,,0,cqlsh,,,,"The documentation for BATCH statements declares that semicolons are required between update operations. Currently including them results in an error 'expecting K_APPLY'. To match the design specifications, semi-colons should be allowed or optional. 

","Mac OSX, cqlsh 3.0.2",,,,,,,,,,,,,,,,,,,,,,,04/Jul/13 19:27;iamaleksey;5697.txt;https://issues.apache.org/jira/secure/attachment/12590933/5697.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-25 05:46:42.742,,,no_permission,,,,,,,,,,,,334912,,,Mon Jul 08 20:36:07 UTC 2013,,,,,,0|i1lrrz:,335236,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,25/Jun/13 05:46;jbellis;The grammar suggests that it was intended for semicolon to be optional.  I guess some kind of antlr nondeterminism makes it not work as designed?,"25/Jun/13 07:19;iamaleksey;The semicolon is optional. You are getting this error because cqlsh splits its statements internally. Using python-cql directly (or any other client), the semicolons will be accepted.

It's questionable whether or not messing with cqlsh internals just to allow the semicolons in BATCH is worth it, but I'll have a look later today.",08/Jul/13 19:53;brandon.williams;+1,"08/Jul/13 20:36;iamaleksey;Thanks, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
running compact on an index did not compact two index files into one,CASSANDRA-5670,12653833,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jasobrown,cdaw,cdaw,20/Jun/13 01:14,12/Mar/19 14:04,13/Mar/19 22:29,30/Jul/13 20:54,1.2.9,2.0 rc1,,Legacy/Tools,,,0,nodetool,secondary_index,,,"With a data directory containing secondary index files ending in -1 and -2, I expected that when I ran compact against the index that they would compact down to a set of -3 files.  This column family uses SizeTieredCompactionStrategy.

Using our standard CQL example, the compact command used was: 
$ ./nodetool compact test1 test1-playlists.playlists_artist_idx

Please note: reproducing this test on 1.1.12 (using a single primary key), you will see that running compact on the keyspace also does not compact the index file.  There is no option to compact the index, so I could not compare that.

{noformat}
CREATE KEYSPACE test1 WITH replication = {'class':'SimpleStrategy', 'replication_factor':1};

use test1;

CREATE TABLE playlists (
  id uuid,
  song_order int,
  song_id uuid,
  title text,
  album text,
  artist text,
  PRIMARY KEY  (id, song_order ) );

INSERT INTO playlists (id, song_order, song_id, title, artist, album)
  VALUES (62c36092-82a1-3a00-93d1-46196ee77204, 1,
  a3e64f8f-bd44-4f28-b8d9-6938726e34d4, 'La Grange', 'ZZ Top', 'Tres Hombres');

select * from playlists;

=====================================
./nodetool flush test1

$ ls /var/lib/cassandra/data/test1/playlists
test1-playlists-ic-1-CompressionInfo.db				
test1-playlists-ic-1-Data.db	
test1-playlists-ic-1-Filter.db					
test1-playlists-ic-1-Index.db					
test1-playlists-ic-1-Statistics.db				
test1-playlists-ic-1-Summary.db					
test1-playlists-ic-1-TOC.txt					

=====================================

CREATE INDEX ON playlists(artist );
select * from playlists;
select * from playlists where artist = 'ZZ Top';

=====================================
$ ./nodetool flush test1

$ ls /var/lib/cassandra/data/test1/playlists
test1-playlists-ic-1-CompressionInfo.db			
test1-playlists-ic-1-Data.db					
test1-playlists-ic-1-Filter.db					
test1-playlists-ic-1-Index.db					
test1-playlists-ic-1-Statistics.db				
test1-playlists-ic-1-Summary.db					
test1-playlists-ic-1-TOC.txt					
	
test1-playlists.playlists_artist_idx-ic-1-CompressionInfo.db
test1-playlists.playlists_artist_idx-ic-1-Data.db
test1-playlists.playlists_artist_idx-ic-1-Filter.db
test1-playlists.playlists_artist_idx-ic-1-Index.db
test1-playlists.playlists_artist_idx-ic-1-Statistics.db
test1-playlists.playlists_artist_idx-ic-1-Summary.db
test1-playlists.playlists_artist_idx-ic-1-TOC.txt

=====================================

delete artist from playlists where id = 62c36092-82a1-3a00-93d1-46196ee77204 and song_order = 1;
select * from playlists;
select * from playlists where artist = 'ZZ Top';

=====================================
$ ./nodetool flush test1

$ ls /var/lib/cassandra/data/test1/playlists
test1-playlists-ic-1-CompressionInfo.db	
test1-playlists-ic-1-Data.db					
test1-playlists-ic-1-Filter.db					
test1-playlists-ic-1-Index.db					
test1-playlists-ic-1-Statistics.db				
test1-playlists-ic-1-Summary.db					
test1-playlists-ic-1-TOC.txt					
test1-playlists-ic-2-CompressionInfo.db				
test1-playlists-ic-2-Data.db					
test1-playlists-ic-2-Filter.db					
test1-playlists-ic-2-Index.db					
test1-playlists-ic-2-Statistics.db				
test1-playlists-ic-2-Summary.db					
test1-playlists-ic-2-TOC.txt
			
test1-playlists.playlists_artist_idx-ic-1-CompressionInfo.db
test1-playlists.playlists_artist_idx-ic-1-Data.db
test1-playlists.playlists_artist_idx-ic-1-Filter.db
test1-playlists.playlists_artist_idx-ic-1-Index.db
test1-playlists.playlists_artist_idx-ic-1-Statistics.db
test1-playlists.playlists_artist_idx-ic-1-Summary.db
test1-playlists.playlists_artist_idx-ic-1-TOC.txt
test1-playlists.playlists_artist_idx-ic-2-CompressionInfo.db
test1-playlists.playlists_artist_idx-ic-2-Data.db
test1-playlists.playlists_artist_idx-ic-2-Filter.db
test1-playlists.playlists_artist_idx-ic-2-Index.db
test1-playlists.playlists_artist_idx-ic-2-Statistics.db
test1-playlists.playlists_artist_idx-ic-2-Summary.db
test1-playlists.playlists_artist_idx-ic-2-TOC.txt

=====================================

./nodetool compact test1

$ ls /var/lib/cassandra/data/test1/playlists
test1-playlists-ic-3-CompressionInfo.db
test1-playlists-ic-3-Data.db
test1-playlists-ic-3-Filter.db
test1-playlists-ic-3-Index.db
test1-playlists-ic-3-Statistics.db
test1-playlists-ic-3-Summary.db
test1-playlists-ic-3-TOC.txt
test1-playlists.playlists_artist_idx-ic-1-CompressionInfo.db
test1-playlists.playlists_artist_idx-ic-1-Data.db
test1-playlists.playlists_artist_idx-ic-1-Filter.db
test1-playlists.playlists_artist_idx-ic-1-Index.db
test1-playlists.playlists_artist_idx-ic-1-Statistics.db
test1-playlists.playlists_artist_idx-ic-1-Summary.db
test1-playlists.playlists_artist_idx-ic-1-TOC.txt
test1-playlists.playlists_artist_idx-ic-2-CompressionInfo.db
test1-playlists.playlists_artist_idx-ic-2-Data.db
test1-playlists.playlists_artist_idx-ic-2-Filter.db
test1-playlists.playlists_artist_idx-ic-2-Index.db
test1-playlists.playlists_artist_idx-ic-2-Statistics.db
test1-playlists.playlists_artist_idx-ic-2-Summary.db
test1-playlists.playlists_artist_idx-ic-2-TOC.txt

=====================================

$ ./nodetool compact test1 test1-playlists.playlists_artist_idx

$ ls /var/lib/cassandra/data/test1/playlists
test1-playlists-ic-3-CompressionInfo.db
test1-playlists-ic-3-Data.db
test1-playlists-ic-3-Filter.db
test1-playlists-ic-3-Index.db
test1-playlists-ic-3-Statistics.db
test1-playlists-ic-3-Summary.db
test1-playlists-ic-3-TOC.txt
test1-playlists.playlists_artist_idx-ic-1-CompressionInfo.db
test1-playlists.playlists_artist_idx-ic-1-Data.db
test1-playlists.playlists_artist_idx-ic-1-Filter.db
test1-playlists.playlists_artist_idx-ic-1-Index.db
test1-playlists.playlists_artist_idx-ic-1-Statistics.db
test1-playlists.playlists_artist_idx-ic-1-Summary.db
test1-playlists.playlists_artist_idx-ic-1-TOC.txt
test1-playlists.playlists_artist_idx-ic-2-CompressionInfo.db
test1-playlists.playlists_artist_idx-ic-2-Data.db
test1-playlists.playlists_artist_idx-ic-2-Filter.db
test1-playlists.playlists_artist_idx-ic-2-Index.db
test1-playlists.playlists_artist_idx-ic-2-Statistics.db
test1-playlists.playlists_artist_idx-ic-2-Summary.db
test1-playlists.playlists_artist_idx-ic-2-TOC.txt


=====================================
cqlsh:test1> describe keyspace test1;

CREATE KEYSPACE test1 WITH replication = {
  'class': 'SimpleStrategy',
  'replication_factor': '1'
};

USE test1;

CREATE TABLE playlists (
  id uuid,
  song_order int,
  album text,
  artist text,
  song_id uuid,
  title text,
  PRIMARY KEY (id, song_order)
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};

CREATE INDEX playlists_artist_idx ON playlists (artist);

{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,30/Jul/13 19:54;jasobrown;5670-v1.diff;https://issues.apache.org/jira/secure/attachment/12595031/5670-v1.diff,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-22 06:25:35.302,,,no_permission,,,,,,,,,,,,334110,,,Tue Jul 30 20:53:48 UTC 2013,,,,,,0|i1lmvj:,334436,,,,,,,,jbellis,jbellis,,,,,,,,,,"22/Jun/13 06:25;michalm;I think it's impossible to compact secondary index CF using nodetool - I remember that I tried it once with 1.2.1 when having a problem with indexes and as far as I remember I was getting an exception (or just notting happened, I'm not sure now). I had to use JMX to compact it. Anyway, I'll check it.
","22/Jun/13 06:38;michalm;Yes, as I supposed, found in log:

{noformat} WARN 08:09:53,952 Operation not allowed on secondary Index column family (test1-
playlists.playlists_artist_idx){noformat}

So ""it's not a bug, it's a feature"" ;-)

JMX's forceMajorCompaction call works fine.","22/Jun/13 08:18;michalm;BTW. method that decides what to compact accepts two boolean flags that tell (a) if index CFs should be allowed and (b) if index CFs should be automatically included in compaction, so adding a ""--with-indexes"" to ""nodetool compact"" command and/or adding a separate ""nodetool compactindex"" command will not be a problem.","23/Jun/13 05:59;jbellis;bq. it's not a bug, it's a feature

I thought we are allowing it in CASSANDRA-4464.

What is nodetool doing if not calling forceMajorCompaction?","23/Jun/13 07:24;michalm;bq. I thought we are allowing it in CASSANDRA-4464.

My bad, didn't know about the task you mentioned. 
I thought that it maybe was overwritten by other patch, but it wasn't - it seems it was intended (or it's just a mistake):

{noformat}$ git show cef8eb0
(...)
     public void forceTableCompaction(String tableName, String... columnFamilies) throws IOException, ExecutionException, InterruptedExcepti
     {
-        for (ColumnFamilyStore cfStore : getValidColumnFamilies(tableName, columnFamilies))
+        for (ColumnFamilyStore cfStore : getValidColumnFamilies(false, false, tableName, columnFamilies))
         {
             cfStore.forceMajorCompaction();
         }{noformat}

BTW. I can see your comment in that task ( https://issues.apache.org/jira/browse/CASSANDRA-4464?focusedCommentId=13461974&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13461974 ) didn't mention ""compact"" in any of the categories, so I'd guess it was just ""forgotten"".

Anyway, I can't see a good reason to disallow compacting indexes via nodetool.
","30/Jul/13 19:41;jasobrown;(Getting back to this one now). I worked from the list of tasks that Jonathan provided in the comment linked to above - and, yeah, looks like compact was left out or forgotten. Will work on adding compacting 2is today.",30/Jul/13 19:54;jasobrown;v1 simply enables compaction on 2Is (changes a false argument to true),30/Jul/13 20:14;jbellis;+1,30/Jul/13 20:53;jasobrown;commmitted to 1.2 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Connection thrashing in multi-region ec2 during upgrade, due to messaging version",CASSANDRA-5669,12653820,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jasobrown,jasobrown,jasobrown,19/Jun/13 22:51,12/Mar/19 14:04,13/Mar/19 22:29,20/Jun/13 19:24,1.2.6,2.0 beta 1,,,,,0,ec2,ec2multiregionsnitch,gossip,,"While debugging the upgrading scenario described in CASSANDRA-5660, I discovered the ITC.close() will reset the message protocol version of a peer node that disconnects. CASSANDRA-5660 has a full description of the upgrade path, but basically the Ec2MultiRegionSnitch will close connections on the publicIP addr to reconnect on the privateIp, and this causes ITC to drop the message protocol version of previously known nodes. I think we want to hang onto that version so that when the newer node (re-)connects to the lower node version, it passes the correct protocol version rather than the current version (too high for the older node),the connection attempt getting dropped, and going through the dance again.

To clarify, the 'thrashing' is at a rather low volume, from what I observed. Anecdotaly, perhaps one connection per second gets turned over.",,,,,,,,,,,,,,,,,,,,,,,,19/Jun/13 22:53;jasobrown;5669-v1.diff;https://issues.apache.org/jira/secure/attachment/12588719/5669-v1.diff,20/Jun/13 18:22;jasobrown;5669-v2.diff;https://issues.apache.org/jira/secure/attachment/12588882/5669-v2.diff,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-06-19 23:27:46.914,,,no_permission,,,,,,,,,,,,334097,,,Tue Jul 09 17:43:01 UTC 2013,,,,,,0|i1lmsn:,334423,,,,,,,,jbellis,jbellis,,,,,,,,,,19/Jun/13 22:53;jasobrown;Attached patch simply deletes the call to Gossiper.resetVersion() from ITC.close().,"19/Jun/13 23:27;jbellis;The reason that's there is so that if the other node got upgraded while he was disconnected, we'll now negotiate the new version instead of continuing to use the old.","19/Jun/13 23:55;jasobrown;It looks like we'll (re)set the version on any new connection from a given node, so I'm not sure we need to explicitly throw away the version on close(). 

Just to clarify (even if just for my own benefit): The problem I'm trying to solve here is the upgraded node trying to contact the older node, and things getting wonky (data race) when the Ec2MultiRegionSnitch chooses to close the publicIP connection in favor of the localIP. When the older node closes the connection (after we've established the first round of gossip and we've discovered we're in the same DC), the new node triggers ICP.close() and forgets the older's version number (even though it had just negotiated a moments previously). Thus, when new node attempts to connect on localIP, older node sees the newer protocol version, and refuses the connection.","20/Jun/13 16:23;jbellis;bq. It looks like we'll (re)set the version on any new connection from a given node, so I'm not sure we need to explicitly throw away the version on close()

Here's the scenario.  A is 1.2.  B is 1.1.

B is restarted for upgrade.  A reconnects to B before B connects to A -- maybe it had an ""undroppable"" command to retry, or maybe it's just luck of the draw that A gossips or sends a command to B.

If we don't reset the version on close, A will connect to B as 1.1, and then B will think, ""Oh, A is a 1.1 node, I'd better connect to him that way too.""

bq. The problem I'm trying to solve here is the upgraded node trying to contact the older node, and things getting wonky (data race) when the Ec2MultiRegionSnitch chooses to close the publicIP connection in favor of the localIP

So damned if you do, damned if you don't...

What if we add logic to EC2MRS to only reconnect if we're both on the current version?  1.1 -> 1.2 would reconnect then (because 1.2 drops down to 1.1 after initial negotiation) but that's okay since it would reconnect at 1.1 again.  1.2 -> 1.1 would not reconnect, so you'd have extra public traffic until everyone upgrades.  Acceptable?","20/Jun/13 17:29;jasobrown;Ahh, I see your point. Our upgrades are never that short, time-wise, when we bounce a node for upgrade, A would usually mark B as dead and drop any messages.

Yes, I think your proposal will be fine, a little extra public traffic is better than thrashing (all) connections. This will work now that we keep the version with the OTC rather than in each individual message (as we did pre-1.2).",20/Jun/13 18:22;jasobrown;v2 adds additional check in Ec2MRS.reConnect() to make sure peer node is at same MS.current_version before closing connection on publicIP (and reconnecting on privateIP).,"20/Jun/13 19:03;jbellis;+1, just fix your IDE alignment settings on the && clause :)",20/Jun/13 19:13;jasobrown;changed name of ticket to better reflect the problem (and the solution),"20/Jun/13 19:24;jasobrown;committed to 1.2 and trunk, with indentation alignment change. thanks!","07/Jul/13 17:12;vijay2win@yahoo.com;This patch actually breaks the assumption while using EC2MRS, within the DC we always use private IP and public IP communication is only needed for seeds within a AZ (please see CASSANDRA-5432). This assumption was partly because of the older version and Priam.... 

Should we add this info to changes.txt or communicate to users?","08/Jul/13 22:38;jasobrown;How does this patch break the 'use localIP addr when in the same DC (ec2 region)'? Yes, it temporarily bypasses it during upgrades (due to insanity described in CASSANDRA-5660), but otherwise I believe it behaves as before. Is there a bug or subtlety that I'm not seeing?","08/Jul/13 22:45;vijay2win@yahoo.com;Yes because now reconnect to local ip is not happening during upgrades (you will try to connect in a Non SSL port within a DC).... 
Lets say Public IP address is not open between node A and B (which are in the local DC and they are not seeds) then node A cannot talk to B if you dont reconnect using private IP...

which is the case in https://issues.apache.org/jira/browse/CASSANDRA-5432?focusedCommentId=13637454&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13637454","09/Jul/13 12:27;jasobrown;I spent a lot of time thinking about this :), and I think the situation in this ticket is subtly different from what happened in CASSANDRA-5171/CASSANDRA-5432. I commented on that ticket as to why I think it had a problem (short answer: connecting to publicIP on non-SSL port). This ticket does not get us into that situation as we will continue to connect to the publicIP/(SSL) port - we simply bypass reconnecting on the local port if we see the other node has a lower messaging version.

I did test out this upgrade scenario a few weeks ago when we concocted it (and it worked), and will be happy to try it out again. It'll take a few hours (including time for dropping kids of at camp), so I'll update this ticket later in the morning.","09/Jul/13 17:43;vijay2win@yahoo.com;Makes sense, it is hard to conform the theory without testing it :), Sorry to pollute the ticket.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"cqlsh shouldn't display ""null"" for empty values",CASSANDRA-5675,12653906,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,slebresne,slebresne,20/Jun/13 10:36,12/Mar/19 14:04,13/Mar/19 22:29,26/Jun/13 22:29,1.2.7,2.0 beta 1,,,,,0,cqlsh,,,,"For historical reason (and compatibility with thrift), all type support an empty value, even type like int for which it doesn't really make sense (see CASSANDRA-5674 too on that subject).

If you input such an empty value for a type like int, cqlsh will display it as null:
{noformat}
cqlsh:ks> CREATE TABLE test (k text PRIMARY KEY, v int);
cqlsh:ks> INSERT INTO test(k, v) VALUES ('someKey', blobAsInt(0x));
cqlsh:ks> SELECT * FROM test;

 k       | v
---------+------
 someKey | null

{noformat} 

But that's not correct, it suggests {{v}} has no value but that's not true, it has a value, it's just an empty one.

Now, one may argue support empty values for a type like int is broken, and I would agree with that. But thrift allows it so we probably need to preserve that behavior for compatibility sake. And I guess the need to use blobAsInt at least make it clear that it's kind of a hack.

That being said, cqlsh should not display null as this is confusing. Instead I'd suggest either displaying nothing (that's how an empty string is displayed after all), or to just go with some random explicit syntax like say ""[empty value]""",,,,,,,,,,,,,,,,,,,,,,,,22/Jun/13 11:58;iamaleksey;5675.txt;https://issues.apache.org/jira/secure/attachment/12589247/5675.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-26 20:21:55.159,,,no_permission,,,,,,,,,,,,334183,,,Wed Jun 26 22:29:08 UTC 2013,,,,,,0|i1lnbr:,334509,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,26/Jun/13 20:21;brandon.williams;+1,"26/Jun/13 22:29;iamaleksey;Thanks, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DESC TABLE omits some column family settings,CASSANDRA-5749,12657484,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,enigmacurry,enigmacurry,12/Jul/13 16:00,12/Mar/19 14:04,13/Mar/19 22:29,15/Jul/13 10:49,2.0 beta 2,,,,,,0,cqlsh,,,,"In CQL I can create a table with settings introduced in 2.0:

{code}
cqlsh:Keyspace1> CREATE TABLE r1 ( key int PRIMARY KEY, value varchar) WITH speculative_retry='ALWAYS';
{code}

But the settings don't show up when I DESC TABLE:
{code}
cqlsh:Keyspace1> DESC TABLE r1;

CREATE TABLE r1 (
  key int PRIMARY KEY,
  value text
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};
{code}

For comparison, here is the same table viewed from cassandra-cli:

{code}
[default@Keyspace1] describe r1;

WARNING: CQL3 tables are intentionally omitted from 'describe' output.
See https://issues.apache.org/jira/browse/CASSANDRA-4377 for details.

WARNING: Could not connect to the JMX on 127.0.0.1:7199 - some information won't be shown.

    ColumnFamily: r1
      Key Validation Class: org.apache.cassandra.db.marshal.Int32Type
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Cells sorted by: org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type)
      GC grace seconds: 0
      Compaction min/max thresholds: 0/0
      Read repair chance: 0.0
      DC Local Read repair chance: 0.0
      Populate IO Cache on flush: false
      Replicate on write: false
      Caching: keys_only
      Default time to live: 0
      Bloom Filter FP chance: default
      Index interval: default
      Speculative Retry: NONE
      Compaction Strategy: null
{code}

Ideally, all of these values that cli shows would be shown by cqlsh.
",,,,,,,,,,,,,,,,,,,,,,,,14/Jul/13 22:51;iamaleksey;5749.txt;https://issues.apache.org/jira/secure/attachment/12592219/5749.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-14 22:53:14.072,,,no_permission,,,,,,,,,,,,337705,,,Mon Jul 15 10:49:34 UTC 2013,,,,,,0|i1m8zb:,338028,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"12/Jul/13 16:03;enigmacurry;In the example above, the speculative_retry value didn't actually get set. I'll open a separate issue for this.

EDIT: Actually, I think this is just that the CLI doesn't understand cql3 tables. I opened CASSANDRA-5750 to save some else this same confusion.","14/Jul/13 22:53;iamaleksey;The patch adds the missing default_time_to_live, speculative_retry, and memtable_flush_period_in_ms to DESCRIBE output.",15/Jul/13 03:19;brandon.williams;+1,"15/Jul/13 10:49;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apache.Cassandra.Cassandra.get_count will disconnect but not throw InvalidRequestException when column family is not exist.,CASSANDRA-5701,12654890,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lyubent,yuemaoxing,yuemaoxing,26/Jun/13 08:41,12/Mar/19 14:04,13/Mar/19 22:29,20/Dec/13 20:40,2.0.5,,,Legacy/CQL,,,0,,,,,"When I use get_count interface which is defined in Cassandra.thrift, the Cassandra Server(1.2.4) close the connection from Client when column family is not exist in that keyspace but not throw InvalidRequestException. 

It seemed the get_count method in cassandra.thrift.CassandraServer.java did not validate parameters(ThriftValidation.validateColumnFamily) in this method.


system.log:
ERROR [RPC-Thread:3373] 2013-06-26 14:23:09,264 TNonblockingServer.java (line 638) Unexpected exception while invoking!
java.lang.IllegalArgumentException: Unknown table/cf pair (Keyspace1.Standard)
	at org.apache.cassandra.db.Table.getColumnFamilyStore(Table.java:165)
	at org.apache.cassandra.thrift.CassandraServer.get_count(CassandraServer.java:471)
	at org.apache.cassandra.thrift.Cassandra$Processor$get_count.getResult(Cassandra.java:3381)
	at org.apache.cassandra.thrift.Cassandra$Processor$get_count.getResult(Cassandra.java:3369)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.thrift.server.TNonblockingServer$FrameBuffer.invoke(TNonblockingServer.java:632)
	at org.apache.cassandra.thrift.CustomTHsHaServer$Invocation.run(CustomTHsHaServer.java:109)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)


Please check this bug, thanks!","cassandra server : 1.2.4
and cassandra1.2.5 has same bug.
client : C# client",,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,27/Nov/13 15:42;lyubent;5701_cassandra-2.0.patch;https://issues.apache.org/jira/secure/attachment/12616062/5701_cassandra-2.0.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-27 15:41:22.654,,,no_permission,,,,,,,,,,,,335167,,,Fri Dec 20 20:40:06 UTC 2013,,,,,,0|i1ltcf:,335491,,,,,,,,jbellis,jbellis,,,,,,,,,,02/Aug/13 02:39;yuemaoxing;Uh...,14/Aug/13 02:21;yuemaoxing;Is it not important?,27/Nov/13 15:41;lyubent;Added a catch clause to handle IllegalArgumentException and throw a InvalidRequestException which will be picked up by the client.,27/Nov/13 15:44;lyubent;This is also reproducible in 2.0.x,"27/Nov/13 18:18;jbellis;patch is on the doInsert method; still need fix for get_count?.  also, build fails b/c you're mixing thrift and internal IRE classes.",19/Dec/13 23:36;jbellis;Where were we on this?  ISTR having some issues applying the patch.  You were going to post a branch?,"20/Dec/13 11:13;lyubent;Here's the [branch|https://github.com/lyubent/cassandra/tree/5701] I think the problem was that the patch was added to trunk, but this branch is on cassandra-2.0 and should build properly. ","20/Dec/13 20:40;jbellis;LGTM, committed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"add cassandra.unsafesystem property (Truncate flushes to disk again in 1.2, even with durable_writes=false)",CASSANDRA-5704,12654927,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,christianmovi,christianmovi,26/Jun/13 13:39,12/Mar/19 14:04,13/Mar/19 22:29,17/Jul/13 19:05,1.2.7,2.0 beta 2,,,,,0,,,,,"I just upgraded my dev-environment to C* 1.2. Unfortunetaly 1.2 makes my JUnit tests slow again, due to a blocking-flush in saveTruncationRecord().

With Cassandra 1.1 truncate was very fast due to: CASSANDRA-4153


My proposal is to make saveTruncationRecord() only flush when durableWrites are enabled.

My assumption is that if somebody turn off durable writes then he does not mind if truncate is not guaranteed to be durable either.


I successfully tested the following patch with my testsuite. Its as fast as it was with 1.1 (maybe even faster!):
{code}
@@ -186,5 +186,8 @@ public class SystemTable
         String req = ""UPDATE system.%s SET truncated_at = truncated_at + %s WHERE key = '%s'"";
         processInternal(String.format(req, LOCAL_CF, truncationAsMapEntry(cfs, truncatedAt, position), LOCAL_KEY));
-        forceBlockingFlush(LOCAL_CF);
+        
+        KSMetaData ksm = Schema.instance.getKSMetaData(cfs.table.name);
+        if (ksm.durableWrites) // flush only when durable_writes are enabled
+            forceBlockingFlush(LOCAL_CF);
     }
{code}
",,,,,,,,,,,,,,CASSANDRA-4153,,,,,,,,,,17/Jul/13 17:50;jbellis;5704-v2.txt;https://issues.apache.org/jira/secure/attachment/12592809/5704-v2.txt,17/Jul/13 23:19;christianmovi;5704_unsafeSystem.txt;https://issues.apache.org/jira/secure/attachment/12592887/5704_unsafeSystem.txt,26/Jun/13 13:45;christianmovi;CASSANDRA_5704_V1_1_2.patch;https://issues.apache.org/jira/secure/attachment/12589747/CASSANDRA_5704_V1_1_2.patch,26/Jun/13 13:45;christianmovi;CASSANDRA_5704_V1_trunk.patch;https://issues.apache.org/jira/secure/attachment/12589748/CASSANDRA_5704_V1_trunk.patch,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-06-26 14:29:02.154,,,no_permission,,,,,,,,,,,,335204,,,Thu Jul 18 19:00:09 UTC 2013,,,,,,0|i1ltkn:,335528,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,26/Jun/13 13:46;christianmovi;I attached patchfiles for 1.2 and trunk,"26/Jun/13 14:29;jbellis;Hmm.  I'm not sure I'm convinced that we should not save the truncation record when durable writes are disabled.  Durable writes means ""I'm okay if you lose the data I'm about to write in case of power failure,"" but not flushing the truncate means ""Data that was supposed to be gone, could reappear"" which feels semantically different to me.

WDYT [~brandon.williams] [~tjake]?

(I note that for production use, CASSANDRA-4906 made the biggest difference since now we only need to flush the table being truncated instead of everyone.)","26/Jun/13 14:36;brandon.williams;bq. ""Data that was supposed to be gone, could reappear"" which feels semantically different to me.

I tend to agree.  Certainly if the biggest problem here is tests are slow, it's not worth the risk just to speed them up.  We could do something like add a flag if we need this.","26/Jun/13 14:55;tjake;I agree, perhaps add sone test.mode property that skips it for unit tests?","26/Jun/13 15:44;christianmovi;{quote}""Data that was supposed to be gone, could reappear"" which feels semantically different to me.{quote}
I can understand that. My thought was that durable_writes already applies to deletes. So I thought why not also apply that pattern to truncate?


I could live with a new ""test.mode"" property though (in cassandra.yaml, right?). I would even go as far to say that this one should also deactivate all fsyncs in Cassandra :-)

Nevertheless, for practical reasons I like the idea of using the durable_writes flag, because it allows me to use have different behaviour on my dev-laptop for...
- ... one keyspace for my junits. These require constant truncation.
- ... my local installation of our application. Here I like a bit more data safety, because I not want to set up my testdata all the time.
","26/Jun/13 15:47;christianmovi;{quote}I'm not sure I'm convinced that we should not save the truncation record when durable writes are disabled.{quote}
My understanding was that my patch only stops it from flushing. The record is still being written to a memtable, right?
","17/Jul/13 17:49;jbellis;Yes, it's a memtable write ... to the system.local table, with information about the truncated table.

No doubt you would reply, ""then let's skip the flush when durable writes are disabled on the system keyspace,"" but you'll note that we've already special cased this flush even though the memtable writes is commitlogged -- we want it to persist immediately even with periodic commitlog mode.  Truncated data reappearing is very high on the list of Behaviors That Surprise People.

Therefore I propose the attached patch along the lines of what Jake and Brandon suggest, that adds a {{cassandra.unsafetruncate}} property.  (Normally I am not a fan of using a 'negative' property name like 'unsafe' but I don't want to encourage people to use it, who shouldn't, which I think we risk if we use {{quicktruncate}} or similar.)",17/Jul/13 18:08;brandon.williams;+1,17/Jul/13 19:05;jbellis;Committed,"17/Jul/13 23:18;christianmovi;[~jbellis], [~brandon.williams]: I'm fine with a property. Thanks!

Is there any chance we can make it a ""cassandra.unsafesystem""-property and apply the same behaviour to all system table operations? This would then also speed up schema changes, which are are also a pain for tests.

FWIW: I attached a patch which works nicely for me.

Update: Perhaps as a separate 2.0 ticket?
",18/Jul/13 16:45;jbellis;WFM.  Committed w/ light modifications.,"18/Jul/13 19:00;christianmovi;Awesome, thanks!

For documentation purposes: The property is: -Dcassandra.unsafesystem=true",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disallow renaming an indexed column,CASSANDRA-5705,12654945,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,26/Jun/13 15:17,12/Mar/19 14:04,13/Mar/19 22:29,26/Jun/13 18:01,2.0 beta 1,,,,,,0,,,,,"We can now index CQL3 primary key parts (CASSANDRA-5125) but also allow to rename such columns. Unfortunately, since 2ndary index CF names currently depend on the name of the column they index, renaming an indexed column basically ends up dropping the index.

For now, the simplest solution is probably to just forbid renaming when the column is 2ndary indexed. If you really want to rename, you can always drop the index and recreate it after the rename.",,,,,,,,,,,,,,,,,,,,,,,,26/Jun/13 15:18;slebresne;5705.txt;https://issues.apache.org/jira/secure/attachment/12589762/5705.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-26 17:56:14.591,,,no_permission,,,,,,,,,,,,335222,,,Wed Jun 26 18:01:10 UTC 2013,,,,,,0|i1lton:,335546,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,26/Jun/13 15:18;slebresne;Attaching trivial patch.,26/Jun/13 17:56;iamaleksey;+1,"26/Jun/13 18:01;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix system.schema_triggers-related trigger/schema issues,CASSANDRA-5774,12658342,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,iamaleksey,iamaleksey,17/Jul/13 19:34,12/Mar/19 14:04,13/Mar/19 22:29,22/Jul/13 23:03,2.0.0,,,,,,0,triggers,,,,"Among other things, the patch does the following:
- adds missing schema_triggers to MigrationManager.resetLocalSchema()
- adds missing schema_triggers to SystemKeyspace.serializeSchema() - so that triggers would be part of schema version calculation
- adds missing schema_triggers to DefsTables.flushSchemaCFs()
- adds missing triggers to CFMetaData.toSchema(), so that CFs created via thrift with triggers from the beginning would serialize triggers
- removes triggers from CFMetaData.newIndexMetadata(), so that 2i CFs wouldn't inherit the triggers from the parent CF

There are other minor and not so minor changes, but these were the most critical ones. The patch also (unnecessarily) cleans up ColumnDefinition, but that was done to make it consistent with the new TriggerDefinition class.

The bulk of the patch is the updated thrift-gen files.

",,,,,,,,,,,,,,,,,,,,,,,,17/Jul/13 19:35;iamaleksey;5774.txt;https://issues.apache.org/jira/secure/attachment/12592836/5774.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-22 15:40:06.714,,,no_permission,,,,,,,,,,,,338536,,,Mon Jul 22 23:03:28 UTC 2013,,,,,,0|i1me33:,338856,,,,,,,,jbellis,jbellis,,,,,,,,,,"22/Jul/13 15:40;jbellis;Do we need a mutable map in the CFDefinition copies?

Should probably move the to/from/deleteFrom Schema code into SystemTable for consistency.

Would prefer to split the ColumnDefinition cleanup (others?) to a separate commit, ninja is fine.

Otherwise LGTM.","22/Jul/13 20:41;iamaleksey;bq. Do we need a mutable map in the CFDefinition copies?

Right now we do.

bq. Should probably move the to/from/deleteFrom Schema code into SystemTable for consistency.

it's currently in TriggerDefinition for consistency with ColumnDefinition, which follows very similar structure. If we move it to SystemTables, then we should move both. So I'm gonna keep it as is in this commit, and maybe move all of schema read/write code from both ColumnDefinition and TriggerDefinition in that ninja commit.

bq. Would prefer to split the ColumnDefinition cleanup (others?) to a separate commit, ninja is fine.

will do",22/Jul/13 21:25;jbellis;Ship it!,"22/Jul/13 23:03;iamaleksey;Pushed in separate commits, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Providing multiple keys to sstable2json results in invalid json,CASSANDRA-5781,12658853,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,cnlwsu,cnlwsu,cnlwsu,19/Jul/13 23:13,12/Mar/19 14:04,13/Mar/19 22:29,20/Jul/13 19:18,1.2.7,,,Legacy/Tools,,,0,,,,,"if you pass multiple keys via the -k parameter to sstable for json the 2nd row will be appended to the end of the first without a comma.  It would look like so:
{code}
sstable2json foo-Data.db -k key1 -k key2 -k key3 -k key4
{
key1 : [[]...]key2: [[]...],
key3 : [[]...],
key4 : [[]...]
}
{code}


",,,,,,,,,,,,,,,,,,,,,,,,19/Jul/13 23:14;cnlwsu;patch;https://issues.apache.org/jira/secure/attachment/12593294/patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-20 19:18:19.206,,,no_permission,,,,,,,,,,,,339046,,,Sat Jul 20 19:18:19 UTC 2013,,,,,,0|i1mh8f:,339366,,,,,,,,dbrosius,dbrosius,,,,,,,,,,"19/Jul/13 23:15;cnlwsu;patch to print comma before instead of after row
https://github.com/clohfink/cassandra/commit/b7307a618090cd802c42f34482723a78f720f79a",20/Jul/13 19:18;dbrosius;committed to cassandra-1.2 as 089f92b1a04cbcc7d391ea7e61d90df56174ad73,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI can show bad DESCRIBE for CQL3 CF if given the CF explicitly,CASSANDRA-5750,12657488,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,enigmacurry,enigmacurry,12/Jul/13 16:33,12/Mar/19 14:04,13/Mar/19 22:29,21/Nov/13 08:59,1.2.13,,,,,,0,,,,,"The CLI omits CQL3 tables if you do a regular describe command. It also emits a nice warning about it. However, if you do a describe with an explicit CF name, it does something a bit unintuitive:

{code}
[default@ryan] describe r1;

WARNING: CQL3 tables are intentionally omitted from 'describe' output.
See https://issues.apache.org/jira/browse/CASSANDRA-4377 for details.

    ColumnFamily: r1
      Key Validation Class: org.apache.cassandra.db.marshal.Int32Type
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Cells sorted by: org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type)
      GC grace seconds: 0
      Compaction min/max thresholds: 0/0
      Read repair chance: 0.0
      DC Local Read repair chance: 0.0
      Populate IO Cache on flush: false
      Replicate on write: false
      Caching: keys_only
      Bloom Filter FP chance: default
      Built indexes: []
      Compaction Strategy: null
{code}

In this case it emitted the WARNING message, but it still showed the table anyway, and many of the CF settings are incorrect because of this. Better to show nothing than incorrect values.",,,,,,,,,,,,,,,,,,,,,,,,20/Nov/13 15:33;slebresne;5750.txt;https://issues.apache.org/jira/secure/attachment/12614906/5750.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-20 15:33:49.608,,,no_permission,,,,,,,,,,,,337709,,,Thu Nov 21 08:59:14 UTC 2013,,,,,,0|i1m907:,338032,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,20/Nov/13 15:33;slebresne;Attaching a patch that omits CQL3 even if you use their name explicitly with a message that tells you to use cqlsh instead in that case. ,20/Nov/13 21:40;iamaleksey;+1,"21/Nov/13 08:59;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
shuffle clear fails,CASSANDRA-5755,12657558,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,hsn,hsn,13/Jul/13 01:51,12/Mar/19 14:04,13/Mar/19 22:29,13/Jul/13 18:17,2.0.1,,,Legacy/Tools,,,0,,,,,"C:\cassandra2\bin>shuffle.bat clear
Exception in thread ""main"" java.lang.RuntimeException: InvalidRequestException(why:Invalid STRING constant (ee7f3e119820c96d) for token_bytes of type blob)
        at org.apache.cassandra.tools.Shuffle.executeCqlQuery(Shuffle.java:516)
        at org.apache.cassandra.tools.Shuffle.clear(Shuffle.java:439)
        at org.apache.cassandra.tools.Shuffle.main(Shuffle.java:689)
Caused by: InvalidRequestException(why:Invalid STRING constant (ee7f3e119820c96d
) for token_bytes of type blob)
        at org.apache.cassandra.thrift.Cassandra$execute_cql3_query_result$execu
te_cql3_query_resultStandardScheme.read(Cassandra.java:45153)
        at org.apache.cassandra.thrift.Cassandra$execute_cql3_query_result$execu
te_cql3_query_resultStandardScheme.read(Cassandra.java:45130)
        at org.apache.cassandra.thrift.Cassandra$execute_cql3_query_result.read(",,,,,,,,,,,,,,,,,,,,,,,,13/Jul/13 06:53;dbrosius;5755.txt;https://issues.apache.org/jira/secure/attachment/12592128/5755.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-13 06:54:51.298,,,no_permission,,,,,,,,,,,,337779,,,Sat Jul 13 18:17:29 UTC 2013,,,,,,0|i1m9fj:,338101,,,,,,,,jbellis,jbellis,,,,,,,,,,13/Jul/13 06:54;dbrosius;patch switches to new blob format,"13/Jul/13 15:10;jbellis;+1

(Note, changing this in 1.2.7 is fine, but it's not broken until 2.0 where we don't support the string syntax anymore.)","13/Jul/13 18:17;dbrosius;ah, you're right... must have confused myself switching between branches.

committed to trunk as commit 0ed859a5d0ece694f240b64e3212af37e0def1ac",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not all STATUS_CHANGE UP events reported via the native protocol,CASSANDRA-5769,12658251,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,baldrick,baldrick,17/Jul/13 13:06,12/Mar/19 14:04,13/Mar/19 22:29,19/Jul/13 06:54,1.2.7,,,Legacy/CQL,,,0,,,,,"Not all gossip UP events are pushed to native protocol users who have registered for them.  This seems to be a native protocol issue because nodes themselves get the UP event (as seen in their logs).  I can consistently reproduce this issue as follows:

1) connect a client to a cluster node (""node1"") using the native protocol, register for TOPOLOGY_CHANGE and STATUS_CHANGE events.  (Probably you only need to register for STATUS_CHANGE to see this, however my client registers for both).
2) on another node (""node2""), send SIGSTOP to the Cassandra process.
3) after about 30 seconds the client gets pushed a STATUS_CHANGE DOWN event for the stopped node.
4) on node2, send SIGCONT to the the Cassandra process.
5) wait forever to get a STATUS_CHANGE UP event.  This is failure: no event is ever received.

Observe that node1 does know that node2 is back up: in its system log I see for example
  INFO [GossipStage:1] 2013-07-17 14:27:41,238 Gossiper.java (line 771) InetAddress /172.18.34.169 is now UP
shortly after sending SIGCONT to the stopped process.

To eliminate the possibility that my client is at fault, I performed the following sanity check:

2') on node2, stopped Cassandra nicely using: sudo service cassandra stop
4') on node2, restarted Cassandra using: sudo service cassandra start

In this case the client soon after gets a STATUS_CHANGE DOWN event followed by a STATUS_CHANGE UP event for node2.","Uubuntu 12.04, x86, 64 bit",,,,,,,,,,,,,,,,,,,,,,,18/Jul/13 12:27;slebresne;5769.txt;https://issues.apache.org/jira/secure/attachment/12592963/5769.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-17 18:06:04.363,,,no_permission,,,,,,,,,,,,338445,,,Fri Jul 19 06:54:08 UTC 2013,,,,,,0|i1mdiv:,338765,,,,,,,,jasobrown,jasobrown,,,,,,,,,,"17/Jul/13 18:06;jbellis;Tyler, can you reproduce?","17/Jul/13 18:40;thobbs;Yes, I can reliably reproduce the issue (through the python driver) with those steps using ccm.","18/Jul/13 12:27;slebresne;We are currently calling the native protocol notification in SS.handleStateNormal(), but that's only called on major state changes, while in the this case there is no generation change.

I wouldn't claim the gossiper code is always easy to follow but it seems that moving the notification code from SS.handleStateNormal() to SS.onAlive() ensures we'll always notify (without over-notifying) so attaching a patch to do that. I've checked it does fix the notification in the case above.
","18/Jul/13 22:06;jasobrown;patch lgtm, although I might debate your claim about the gossiper code - it's not that bad :)","19/Jul/13 06:54;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If a Seed can't be contacted, a new node comes up as a cluster of 1",CASSANDRA-5768,12658249,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,aecobley,aecobley,17/Jul/13 12:51,12/Mar/19 14:04,13/Mar/19 22:29,18/Jul/13 21:18,1.2.7,2.0 beta 2,,,,,0,,,,,"Setting up a new test cluster using  2.0.0-beta1 and I noticed the following behaviour with vnodes turned on.  

I bring up one node all well and good.  however if I bring up a second node, that can't contact the first (the first being the seed for the second) after a short period of time, the second goes ahead and assumes it's the only node and bootstraps with all tokens.  

NOTE also this email from Robert Coli 
To: user@cassandra.apache.org
Obviously if you have defined a seed and cannot contact it, the node should not start as a cluster of one. I have a to-do list item to file a JIRA on the subject, but if you wanted to file and link us, that'd be super. :)


Startup trace (from the can't contact the seed messages below).

http://aep.appspot.com/display/ABcWltCES1srzPrj5CkS69-GB8o/",,,,,,,,,,,,,,,,,,,,,,,,17/Jul/13 17:13;brandon.williams;5768.txt;https://issues.apache.org/jira/secure/attachment/12592799/5768.txt,18/Jul/13 18:32;aecobley;cassandra.yaml;https://issues.apache.org/jira/secure/attachment/12593026/cassandra.yaml,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-07-17 17:13:58.366,,,no_permission,,,,,,,,,,,,338443,,,Wed Jul 24 22:21:40 UTC 2013,,,,,,0|i1mdif:,338763,,,,,,,,jasobrown,jasobrown,,,,,,,,,,17/Jul/13 17:13;brandon.williams;It seems reasonable to me that we should ensure we've contacted a seed during bootstrap when one has been set. Patch to check for this and refuse if it fails.,"18/Jul/13 05:33;jasobrown;lgtm. One (incredibly minor) optimization in Gossiper.checkSeedContact() would be to check the boolean before checking the map for the seed ep, like this:

{code}protected void checkSeedContact(InetAddress ep)
{
    if (!seedContacted && seeds.contains(ep))
        seedContacted = true;
}
{code}

It could be that the effort of me writing this comment is greater than the grand sum of all processing time saved by this optimization, but what the hell :).

UPDATE: Thought about this some more, but what if you only have one seed in the cluster, and that node is starting up, I'm not sure it will ever set the Gossiper.seedContacted to true as it's only in the GossipDigest*VerbHandlers where we call checkSeedContact(). Thus, the seed won't start up (unless I've missed something).

Perhaps in Gossiper.buildSeedsList() we can do this:

{code}private void buildSeedsList()
{
    List<InetAddress> seeds = DatabaseDescriptor.getSeeds(); 
    for (InetAddress seed : seeds)
    {
        if (seed.equals(FBUtilities.getBroadcastAddress()) && seeds.size == 1)
            seedContacted = true;
        else
            seeds.add(seed);
    }
}{code}

(or something similar to this)

This way if there's only one seed (and this node is it), it can still come up. Otherwise, if there are other seeds besides this one, maybe wait to set the seedContacted? (not sure about that, though).


","18/Jul/13 05:35;jasobrown;Also, is there any harm in adding this to 1.2? Seems rather innocuous, I think.","18/Jul/13 08:33;aecobley;I tried 5768.txt on cassandra-trunk.  It does halt cassandra if the seed can't be connected (with  java.lang.IllegalStateException: Unable to contact any seeds!)

However it also halts with the same exception if seeds is set to 127.0.0.1 (the default in cassandra.yaml):

seed_provider:
    # Addresses of hosts that are deemed contact points.
    # Cassandra nodes use this list of hosts to find each other and learn
    # the topology of the ring.  You must change this if you are running
    # multiple nodes!
    - class_name: org.apache.cassandra.locator.SimpleSeedProvider
      parameters:
          # seeds is actually a comma-delimited list of addresses.
          # Ex: ""<ip1>,<ip2>,<ip3>""
          - seeds: ""127.0.0.1""

So thats not correct I guess.","18/Jul/13 09:03;aecobley;Jason,

You code for build list should read:

{code}

private void buildSeedsList()
    {
        Set<InetAddress> seeds = DatabaseDescriptor.getSeeds();
        for (InetAddress seed : seeds)
            {
                if (seed.equals(FBUtilities.getBroadcastAddress()) && seeds.size() == 1)
                    seedContacted = true;
                else
                    seeds.add(seed);
            }
    }
{code}
Sadly that throws an exception at startup:

{noformat}

ERROR 09:58:35,896 Exception encountered during startup
java.lang.UnsupportedOperationException
	at com.google.common.collect.ImmutableCollection.add(ImmutableCollection.java:92)
	at org.apache.cassandra.gms.Gossiper.buildSeedsList(Gossiper.java:1072)
	at org.apache.cassandra.gms.Gossiper.start(Gossiper.java:1046)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:555)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:527)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:426)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:354)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
java.lang.UnsupportedOperationException
	at com.google.common.collect.ImmutableCollection.add(ImmutableCollection.java:92)
	at org.apache.cassandra.gms.Gossiper.buildSeedsList(Gossiper.java:1072)
	at org.apache.cassandra.gms.Gossiper.start(Gossiper.java:1046)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:555)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:527)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:426)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:354)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
Exception encountered during startup: null
ERROR 09:58:35,933 Exception in thread Thread[StorageServiceShutdownHook,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.stopRPCServer(StorageService.java:310)
	at org.apache.cassandra.service.StorageService.shutdownClientServers(StorageService.java:359)
	at org.apache.cassandra.service.StorageService.access$000(StorageService.java:95)
	at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:492)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.lang.Thread.run(Thread.java:724)
{noformat}","18/Jul/13 09:53;aecobley;Jason,

this code seems to work:

{code}

private void buildSeedsList()
    {
	Set<InetAddress> tempseeds = DatabaseDescriptor.getSeeds();
       
	for (InetAddress seed : tempseeds)
	    {
		
		if (seed.equals(FBUtilities.getBroadcastAddress()) && seeds.size() == 1){

		    seedContacted = true;
		}else{
		    seeds.add(seed);
		}
	    }
    }
{code}

This boots if localhost ip = seed ip
It throws an exception if localhost ip != seed ip and seed ip can not be contacted

However if seed ip is the default 127.0.0.1 the code will throw a ""java.lang.IllegalStateException: Unable to contact any seeds!"" exception. I'm not sure thats correct","18/Jul/13 16:23;brandon.williams;bq. This way if there's only one seed (and this node is it), it can still come up.

A seed node can't bootstrap as it is, so this shouldn't be a problem.

bq. Also, is there any harm in adding this to 1.2? Seems rather innocuous, I think.

When I started I thought it was going to be a bit riskier, but now that it's done I think 1.2 is fine.

[~aecobley] bq. However it also halts with the same exception if seeds is set to 127.0.0.1

I can't reproduce this, and it doesn't make any sense to me (since seeds don't bootstrap)",18/Jul/13 18:32;aecobley;Yaml file at 127.0.0.1,"18/Jul/13 18:34;aecobley;Brandon,
For info:

With the code from 5678.txt, the code above and the attached yaml file and no other nodes in the cluster I'm getting the following trace:


sudo ./cassandra
xss =  -ea -javaagent:./../lib/jamm-0.2.5.jar -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms1024M -Xmx1024M -Xmn256M -XX:+HeapDumpOnOutOfMemoryError
LifeintheAirAge:bin Administrator$ objc[2779]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0.jdk/Contents/Home/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined.
 INFO 10:50:38,698 Logging initialized
 INFO 10:50:38,719 JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.8.0-ea
 INFO 10:50:38,719 Heap size: 1046937600/1046937600
 INFO 10:50:38,720 Classpath: ./../conf:./../build/classes/main:./../build/classes/thrift:./../lib/antlr-3.2.jar:./../lib/commons-cli-1.1.jar:./../lib/commons-codec-1.2.jar:./../lib/commons-lang-2.6.jar:./../lib/compress-lzf-0.8.4.jar:./../lib/concurrentlinkedhashmap-lru-1.3.jar:./../lib/disruptor-3.0.1.jar:./../lib/guava-13.0.1.jar:./../lib/high-scale-lib-1.1.2.jar:./../lib/jackson-core-asl-1.9.2.jar:./../lib/jackson-mapper-asl-1.9.2.jar:./../lib/jamm-0.2.5.jar:./../lib/jbcrypt-0.3m.jar:./../lib/jline-1.0.jar:./../lib/json-simple-1.1.jar:./../lib/libthrift-0.9.0.jar:./../lib/log4j-1.2.16.jar:./../lib/lz4-1.1.0.jar:./../lib/metrics-core-2.0.3.jar:./../lib/netty-3.5.9.Final.jar:./../lib/servlet-api-2.5-20081211.jar:./../lib/slf4j-api-1.7.2.jar:./../lib/slf4j-log4j12-1.7.2.jar:./../lib/snakeyaml-1.11.jar:./../lib/snappy-java-1.0.5.jar:./../lib/snaptree-0.1.jar:./../lib/thrift-server-0.2.jar:./../lib/jamm-0.2.5.jar
 INFO 10:50:38,721 JNA not found. Native methods will be disabled.
 INFO 10:50:38,739 Loading settings from file:/Users/Administrator/Documents/raspberry/cassandra2/test/cassandra-trunk/conf/cassandra.yaml
 INFO 10:50:39,159 Data files directories: [/var/lib/cassandra/data]
 INFO 10:50:39,159 Commit log directory: /var/lib/cassandra/commitlog
 INFO 10:50:39,160 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 10:50:39,160 disk_failure_policy is stop
 INFO 10:50:39,165 Global memtable threshold is enabled at 332MB
 INFO 10:50:39,298 Not using multi-threaded compaction
 INFO 10:50:39,547 Initializing key cache with capacity of 49 MBs.
 INFO 10:50:39,560 Scheduling key cache save to each 14400 seconds (going to save all keys).
 INFO 10:50:39,562 Initializing row cache with capacity of 0 MBs
 INFO 10:50:39,570 Scheduling row cache save to each 0 seconds (going to save all keys).
 INFO 10:50:39,977 Couldn't detect any schema definitions in local storage.
 INFO 10:50:39,977 To create keyspaces and column families, see 'help create keyspace' in cqlsh.
 INFO 10:50:40,064 Enqueuing flush of Memtable-local@1164440413(149/149 serialized/live bytes, 6 ops)
 INFO 10:50:40,066 Writing Memtable-local@1164440413(149/149 serialized/live bytes, 6 ops)
 INFO 10:50:40,109 Completed flushing /var/lib/cassandra/data/system/local/system-local-ja-1-Data.db (178 bytes) for commitlog position ReplayPosition(segmentId=1374141039946, position=417)
 INFO 10:50:40,129 No commitlog files found; skipping replay
 INFO 10:50:40,466 Cassandra version: 2.0.0-beta1-SNAPSHOT
 INFO 10:50:40,467 Thrift API version: 19.37.0
 INFO 10:50:40,473 CQL supported versions: 2.0.0,3.1.0 (default: 3.1.0)
 INFO 10:50:40,509 Loading persisted ring state
 INFO 10:50:40,512 Starting up server gossip
 WARN 10:50:40,534 No host ID found, created 8e0fac5a-b8c6-4a6b-b476-de794fb4b85c (Note: This should happen exactly once per node).
 INFO 10:50:40,539 Enqueuing flush of Memtable-local@366873404(300/300 serialized/live bytes, 11 ops)
 INFO 10:50:40,540 Writing Memtable-local@366873404(300/300 serialized/live bytes, 11 ops)
 INFO 10:50:40,552 Completed flushing /var/lib/cassandra/data/system/local/system-local-ja-2-Data.db (293 bytes) for commitlog position ReplayPosition(segmentId=1374141039946, position=81742)
Broadcast: /134.36.9.8
Size of seeds0
Seed : 127.0.0.1
 INFO 10:50:40,605 Starting Messaging Service on port 7000
 INFO 10:50:40,647 Enqueuing flush of Memtable-local@210156003(86/86 serialized/live bytes, 4 ops)
 INFO 10:50:40,647 Writing Memtable-local@210156003(86/86 serialized/live bytes, 4 ops)
 INFO 10:50:40,657 Completed flushing /var/lib/cassandra/data/system/local/system-local-ja-3-Data.db (118 bytes) for commitlog position ReplayPosition(segmentId=1374141039946, position=82003)
 INFO 10:50:40,659 JOINING: waiting for ring information
 INFO 10:50:41,582 Handshaking version with /127.0.0.1
 INFO 10:50:46,585 Handshaking version with /127.0.0.1
 INFO 10:50:46,585 Cannot handshake version with /127.0.0.1
 INFO 10:50:51,586 Cannot handshake version with /127.0.0.1
 INFO 10:50:51,587 Handshaking version with /127.0.0.1
 INFO 10:50:56,589 Cannot handshake version with /127.0.0.1
 INFO 10:50:56,590 Handshaking version with /127.0.0.1
 INFO 10:51:01,591 Cannot handshake version with /127.0.0.1
 INFO 10:51:01,593 Handshaking version with /127.0.0.1
 INFO 10:51:06,594 Cannot handshake version with /127.0.0.1
 INFO 10:51:06,595 Handshaking version with /127.0.0.1
 INFO 10:51:10,713 JOINING: schema complete, ready to bootstrap
 INFO 10:51:10,716 JOINING: getting bootstrap token
 INFO 10:51:10,766 Enqueuing flush of Memtable-local@1819776360(10114/10114 serialized/live bytes, 257 ops)
 INFO 10:51:10,767 Writing Memtable-local@1819776360(10114/10114 serialized/live bytes, 257 ops)
 INFO 10:51:10,797 Completed flushing /var/lib/cassandra/data/system/local/system-local-ja-4-Data.db (5290 bytes) for commitlog position ReplayPosition(segmentId=1374141039946, position=94018)
 INFO 10:51:10,813 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-ja-2-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-ja-1-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-ja-3-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-ja-4-Data.db')]
 INFO 10:51:10,814 JOINING: sleeping 30000 ms for pending range setup
 INFO 10:51:10,864 Compacted 4 sstables to [/var/lib/cassandra/data/system/local/system-local-ja-5,].  5,879 bytes to 5,718 (~97% of original) in 46ms = 0.118546MB/s.  4 total rows, 1 unique.  Row merge counts were {1:0, 2:0, 3:0, 4:1, }
 INFO 10:51:11,597 Cannot handshake version with /127.0.0.1
 INFO 10:51:11,598 Handshaking version with /127.0.0.1
 INFO 10:51:16,598 Cannot handshake version with /127.0.0.1
 INFO 10:51:16,599 Handshaking version with /127.0.0.1
 INFO 10:51:21,600 Cannot handshake version with /127.0.0.1
 INFO 10:51:21,601 Handshaking version with /127.0.0.1
 INFO 10:51:26,602 Cannot handshake version with /127.0.0.1
 INFO 10:51:26,604 Handshaking version with /127.0.0.1
 INFO 10:51:31,605 Cannot handshake version with /127.0.0.1
 INFO 10:51:31,606 Handshaking version with /127.0.0.1
 INFO 10:51:36,607 Cannot handshake version with /127.0.0.1
 INFO 10:51:36,609 Handshaking version with /127.0.0.1
ERROR 10:51:40,816 Exception encountered during startup
java.lang.IllegalStateException: Unable to contact any seeds!
	at org.apache.cassandra.service.StorageService.bootstrap(StorageService.java:897)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:668)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:527)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:426)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:354)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
java.lang.IllegalStateException: Unable to contact any seeds!
	at org.apache.cassandra.service.StorageService.bootstrap(StorageService.java:897)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:668)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:527)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:426)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:354)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
Exception encountered during startup: Unable to contact any seeds!
ERROR 10:51:40,822 Exception in thread Thread[StorageServiceShutdownHook,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.stopRPCServer(StorageService.java:310)
	at org.apache.cassandra.service.StorageService.shutdownClientServers(StorageService.java:359)
	at org.apache.cassandra.service.StorageService.access$000(StorageService.java:95)
	at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:492)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.lang.Thread.run(Thread.java:724)
","18/Jul/13 18:35;brandon.williams;Your seed is 127.0.0.1, but your listen_address is something else.  So the behavior is actually correct, since it can't contact 127.0.0.1 since you're not listening there.","18/Jul/13 18:41;aecobley;Brandon,  you're right of course !  Apologies
","18/Jul/13 21:08;jasobrown;bq. A seed node can't bootstrap as it is, so this shouldn't be a problem

Ahh, crap, forgot about that. Then on the whole, the original patch lgtm. You might want to consider setting seedContacted to true in buildSeedsList() to optimize the lookup in checkSeedContact(), but that's a minor optimization and shouldn't stop us from moving forward on this.","18/Jul/13 21:18;brandon.williams;Committed with the !seedContacted check, but skipped the buildSeedsList() optimization since I couldn't figure out how if we're a seed we'd bootstrap (and if we do, as you say, it should be minor.)","24/Jul/13 22:21;rcoli;This comment is not particularly substantive, I just want to express my enthusiasm for this behavior being fixed. YAY! This formerly very confusing-to-noobs behavior will now be much less potentially confusing. Thanks to the reporter and to the contributors of the patch! :D",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh DESCRIBE should properly describe CUSTOM secondary indexes,CASSANDRA-5760,12657651,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,iamaleksey,iamaleksey,14/Jul/13 23:19,12/Mar/19 14:04,13/Mar/19 22:29,15/Jul/13 15:39,1.2.7,,,Feature/2i Index,,,0,cqlsh,describe,,,"CASSANDRA-5484 and then CASSANDRA-5639 added CREATE CUSTOM INDEX support to CQL3, but cqlsh hasn't been updated to describe such indexes properly.",,,,,,,,,,,,,,,,,,,,,,,,15/Jul/13 00:12;iamaleksey;5760.txt;https://issues.apache.org/jira/secure/attachment/12592221/5760.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-15 15:28:17.269,,,no_permission,,,,,,,,,,,,337871,,,Mon Jul 15 15:39:13 UTC 2013,,,,,,0|i1m9zz:,338193,,,,,,,,jbellis,jbellis,,,,,,,,,,15/Jul/13 15:28;jbellis;+1,"15/Jul/13 15:39;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Small bug in Range.intersects(Bound),CASSANDRA-5771,12658301,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,17/Jul/13 17:06,12/Mar/19 14:04,13/Mar/19 22:29,18/Jul/13 06:57,1.2.7,,,,,,0,,,,,Range.intersects(Bound) doesn't correctly handle the case where the bound in argument has the same start and end (i.e. is a Bound of 1 value). It basically always return true in that case.,,,,,,,,,,,,,,,,,,,,,,,,17/Jul/13 17:07;slebresne;5771.txt;https://issues.apache.org/jira/secure/attachment/12592798/5771.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-17 18:07:46.924,,,no_permission,,,,,,,,,,,,338495,,,Thu Jul 18 06:57:06 UTC 2013,,,,,,0|i1mdtz:,338815,,,,,,,,jbellis,jbellis,,,,,,,,,,17/Jul/13 17:07;slebresne;Attaching patch to fix.,"17/Jul/13 18:07;jbellis;+1, although I'm not sure what a punition is.","18/Jul/13 06:57;slebresne;Aren't you supposed to speak french !? :)

Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minor bugs in the native protocol v2 on 2.0.0-beta1,CASSANDRA-5770,12658292,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,17/Jul/13 16:26,12/Mar/19 14:04,13/Mar/19 22:29,19/Jul/13 06:50,2.0 beta 2,,,,,,0,,,,,There is a few minor bugs in the new features of the native protocol. Attaching patch to fix those.,,,,,,,,,,,,,,,,,,,,,,,,17/Jul/13 16:27;slebresne;5770.txt;https://issues.apache.org/jira/secure/attachment/12592792/5770.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-18 20:16:36.145,,,no_permission,,,,,,,,,,,,338486,,,Fri Jul 19 06:50:33 UTC 2013,,,,,,0|i1mdrz:,338806,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,18/Jul/13 20:16;iamaleksey;+1,"19/Jul/13 06:50;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OPP seems completely unsupported,CASSANDRA-5793,12659295,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,varakumar,varakumar,varakumar,23/Jul/13 09:42,12/Mar/19 14:04,13/Mar/19 22:29,30/Jul/13 20:40,1.2.9,,,,,,0,,,,,"We were using 0.7.6 version. And upgraded to 1.2.5 today. We were using OPP (OrderPreservingPartitioner).

OPP throws error when any node join the cluster. Cluster can not be brought up due to this error. After digging little deep, We realized that ""peers"" column family is defined with key as type ""inet"". Looks like many other column families in system keyspace has same issue.

Exception trace:
java.lang.RuntimeException: The provided key was not UTF8 encoded.
	at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:172)
	at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
	at org.apache.cassandra.db.Table.apply(Table.java:379)
	at org.apache.cassandra.db.Table.apply(Table.java:353)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:258)
	at org.apache.cassandra.cql3.statements.ModificationStatement.executeInternal(ModificationStatement.java:117)
	at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:172)
	at org.apache.cassandra.db.SystemTable.updatePeerInfo(SystemTable.java:258)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1231)
	at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:1948)
	at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:823)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:901)
	at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:50)


Possibilities:
- Changing partitioner to BOP (or something else) fails while loading schema_keyspaces. So, it does not look like an option.
- One possibility is that getToken of OPP can return hex value if it fails to encode bytes to UTF-8 instead of throwing error. By this system tables seem to be working fine with OPP.
- Or Completely remove OPP from code base & configuration files. Mark clearly that OPP is no longer supported in upgrade instructions.",Cassandra on Ubuntu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-07-23 10:32:48.581,,,no_permission,,,,,,,,,,,,339488,,,Tue Jul 30 20:40:14 UTC 2013,,,,,,0|i1mjyn:,339808,,,,,,,,jbellis,jbellis,,,,,,,,,,"23/Jul/13 10:32;jeromatron;See https://github.com/apache/cassandra/blob/cassandra-1.2.6/NEWS.txt#L184-L186

You can't change partitioner once the data has been written.  Rows of data are ordered in sstables according to the partitioner.  You can upgrade to 1.1.x or stay at 0.7, then using Hadoop or a batch job, you can read from your existing cluster and write to a different cluster running 1.2.6 with your new partitioner.","23/Jul/13 12:59;jeromatron;I suppose I was assuming that you were using the now removed COPP, but since you're using just the OPP, that *should* work.  Sorry for the confusion.","23/Jul/13 13:31;jbellis;bq. One possibility is that getToken of OPP can return hex value if it fails to encode bytes to UTF-8 instead of throwing error.

I can't think of how it would break anything to accept keys we previously rejected.","30/Jul/13 08:21;varakumar;Should we handle in this way (return hex value if OPP fails to encode bytes to UTF-8 instead of throwing error) & mark OPP as unsupported in relevant documentation?
","30/Jul/13 20:40;jbellis;Made the change to OPP in 2ccbe3c6f547511e79454b79cbef682ef8a6973a.

cassandra.yaml has noted that OPP is deprecated in favor of BOP for... years.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NumberFormatException during decommission,CASSANDRA-5857,12662442,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,07/Aug/13 17:36,12/Mar/19 14:04,13/Mar/19 22:29,08/Aug/13 18:05,1.2.9,,,,,,0,,,,,"We half-fixed this in CASSANDRA-5696, but unfortunately StorageService is still looking at the token to get the expiretime in some cases.

{noformat}
java.lang.NumberFormatException: For input string: ""113427455640312821154458202477256070484""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        at java.lang.Long.parseLong(Long.java:444)
        at java.lang.Long.parseLong(Long.java:483)
        at org.apache.cassandra.service.StorageService.extractExpireTime(StorageService.java:1660)
        at org.apache.cassandra.service.StorageService.handleStateLeft(StorageService.java:1515)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1234)
        at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:953)
        at org.apache.cassandra.gms.Gossiper.applyNewStates(Gossiper.java:944)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:902)
        at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:50)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,08/Aug/13 17:30;brandon.williams;5857.txt;https://issues.apache.org/jira/secure/attachment/12596885/5857.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-08 14:25:51.296,,,no_permission,,,,,,,,,,,,342446,,,Fri Aug 23 21:22:18 UTC 2013,,,,,,0|i1n23j:,342751,1.2.8,,,,,,,jasobrown,jasobrown,,,1.2.0,,,,,,,"08/Aug/13 14:25;enigmacurry;-OK, so this is upgrade related? How do I reproduce this? I'm wondering what scenarios we need to add to the upgrade_through_versions dtest to avoid this error again.-

Nevermind, [~jjordan] set me straight that this is purely decomission.",08/Aug/13 17:30;brandon.williams;Patch to have SS look at the correct piece for the expire time.,08/Aug/13 17:56;jasobrown;lgtm,08/Aug/13 18:05;brandon.williams;Committed.,"23/Aug/13 21:22;mheffner;Should this be a higher priority issue for release planning purposes? We ran into this issue when decommissioning a node from a 1.2.8 ring. Actually getting that node to finally leave was quite a task:

1) Ran decommision, failed with above error after streaming all data.
2) Tried to use nodetool removenode, node was in 'UL' state so wouldn't run.
3) Shutdown cassandra to try and move node to a DOWN state, node was still stuck in 'UL' state.
4) Tried a unsafeAssassinateEndpoint operation. That failed with same error as above and node was not removed. It did however move the node from UL -> DL.
5) Reran removenode as node was now in a DOWN state. After restreaming, node was finally removed.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Running sstableupgrade on C* 1.0 data dir, before starting C* 1.2 for the first time breaks stuff",CASSANDRA-5831,12660765,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thobbs,jjordan,jjordan,30/Jul/13 19:55,12/Mar/19 14:04,13/Mar/19 22:29,07/Aug/13 23:05,1.2.9,,,Legacy/Tools,,,0,,,,,"If you try to upgrade from C* 1.0.X to 1.2.X and run offline sstableupgrade to try and migrate the sstables before starting 1.2.X for the first time, it messes up the system folder, because it doesn't migrate it right, and then C* 1.2 can't start.
sstableupgrade should either refuse to run against a C* 1.0 data folder, or migrate stuff the right way.",,,,,,,,,,,,,,,,,,,,,,,,02/Aug/13 00:07;thobbs;0001-Handle-pre-1.1-data-directory-layout.patch;https://issues.apache.org/jira/secure/attachment/12595521/0001-Handle-pre-1.1-data-directory-layout.patch,07/Aug/13 18:03;thobbs;0002-Handle-old-system-data-in-health-check.patch;https://issues.apache.org/jira/secure/attachment/12596668/0002-Handle-old-system-data-in-health-check.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-07-30 20:23:56.466,,,no_permission,,,,,,,,,,,,340954,,,Wed Aug 07 23:05:43 UTC 2013,,,,,,0|i1msz3:,341272,,,,,,,,jjordan,jjordan,,,,,,,,,,"30/Jul/13 20:23;jbellis;I think all we need to do here is ""don't run upgradesstables if the ks/cf/ heirarchy doesn't exist already for the system tables.""

In particular, upgradesstables against a 1.1 install should be fine.","31/Jul/13 18:10;thobbs;bq. I think all we need to do here is ""don't run upgradesstables if the ks/cf/ heirarchy doesn't exist already for the system tables.""

If I'm interpreting you correctly, we'll just want upgradesstables to error out in that case and mention something about starting Cassandra 1.1+ before running upgradesstables again, is that correct?",31/Jul/13 18:12;jbellis;Right.,31/Jul/13 20:45;thobbs;Attached patch 0001 does exactly that.,"31/Jul/13 23:07;jbellis;I think we probably want to decouble sstableNeedsMigration from its caller in CassandraDaemon, it throws a bunch of exceptions that are probably not expected.

However, I also note that StandaloneScrubber calls sNM, so maybe making upgradesstables able to perform the migration automagically isn't as painful as I thought.","01/Aug/13 18:19;thobbs;bq. I think we probably want to decouble sstableNeedsMigration from its caller in CassandraDaemon, it throws a bunch of exceptions that are probably not expected.

Are you just referring to the RuntimeExceptions around the total path length on Windows?  If not, can you clarify?

bq. However, I also note that StandaloneScrubber calls sNM, so maybe making upgradesstables able to perform the migration automagically isn't as painful as I thought.

Yeah, I noticed that with only a few minor changes to {{migrateSSTables()}}, the upgrader should be able to do this pretty easily.  Do you want me to add that in?","01/Aug/13 18:49;jbellis;bq. Are you just referring to the RuntimeExceptions

Yes. 

bq. with only a few minor changes to migrateSSTables(), the upgrader should be able to do this pretty easily

Let's go ahead and do that, then.","01/Aug/13 22:40;thobbs;Hmm, there's one complication: {{sstableupgrade}} requires a keyspace and table to be specified, whereas the data directory migration is normally done all at once.  Changing the directory layout for only a single CF feels strange, but changing the layout for other keyspaces and CFs that you didn't specify also feels wrong.",01/Aug/13 23:14;jbellis;How does scrub deal with this?,"01/Aug/13 23:27;thobbs;bq. How does scrub deal with this?

Good point. It just runs the full migration.  That still seems less-than-perfect, but at least the behavior would be consistent.","01/Aug/13 23:29;jbellis;Seems like if they're running upgrade then they've signaled their intent to be on the new version, so maybe just add a notice that we're moving *everything* to the new directory structure and call it good.","01/Aug/13 23:31;jjordan;Agreed.  Also, if you want to add a ""migrate all keyspaces"" option, I don't think anyone would complain.","02/Aug/13 00:07;thobbs;The new 0001 patch adds a {{\-\-migrate}} option to {{sstableupgrade}} and {{sstablescrub}}.  If that option is not used and a pre-1.1 layout is detected, both tools will error out and mention the {{\-\-migrate}} option.  If the option is used, all keyspaces and column families will be migrated.","07/Aug/13 03:17;jjordan;Something more needs to happen on migration for 1.2.  Both sstableupgrade and sstablescrub create a broken set of system tables.

{noformat}
ERROR 22:11:40,896 Fatal exception during initialization
org.apache.cassandra.exceptions.ConfigurationException: Found system table files, but they couldn't be loaded!
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:440)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:243)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
{noformat}

Works fine if I just run ./cassandra to do the upgrade.","07/Aug/13 18:03;thobbs;Patch 0002 alters the failing health check to also look for a saved cluster name in {{System.LocationInfo}}, which indicates that the system data has not been migrated yet (it happens at the end of the startup process, whereas the health check is early in the startup process).

I also have a [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-5831] that contains the two patches.","07/Aug/13 21:58;jjordan;Still not sure what is really going on here.  ""Directories.migrateSSTables();"" is called before ""SystemTable.checkHealth()"" on a regular C* startup.  All sstablescrub and sstableupgrade do is call Directories.migrateSSTables();, then call DatabaseDescriptor.loadSchemas();, so why does ""SystemTable.checkHealth"" fail after that?  SystemTable.finishStartup(); which does the table upgrade doesn't get called until well after checkHealth.  Is calling DatabaseDescriptor.loadSchemas(); in the sstable* scripts doing something that breaks checkHealth?  I don't know that the answer should be ""change check health"".  Should we call upgrade tables from the migration stuff in sstable* scripts?

Regular C* startup does:
{noformat}
Directories.migrateSSTables();
SystemTable.checkHealth();
DatabaseDescriptor.loadSchemas();
{noformat}

sstable* do:
{noformat}
Directories.migrateSSTables();
DatabaseDescriptor.loadSchemas();
{noformat}

So if you run sstable* then start c* the order is:

{noformat}
Directories.migrateSSTables();
DatabaseDescriptor.loadSchemas();
SystemTable.checkHealth();
DatabaseDescriptor.loadSchemas();
{noformat}
","07/Aug/13 22:38;jjordan;So after going through it some more I think the health check change is fine.  Looks like the issue is that loadSchemas is causing empty new tables to be made, so those get checked for stuff.

Everything looks good now.  +1 from me.","07/Aug/13 23:05;jbellis;Committed (to 1.2 only), thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
json2sstable breaks on data exported from sstable2json.,CASSANDRA-5852,12662160,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lyubent,enigmacurry,enigmacurry,06/Aug/13 16:58,12/Mar/19 14:04,13/Mar/19 22:29,19/Sep/13 10:00,1.2.10,2.0.1,,Legacy/Tools,,,0,,,,,"Attached is a JSON formatted sstable generated by sstable2json.

This file cannot be loaded back into Cassandra via json2sstable; it outputs this error:

{code}
Counting keys to import, please wait... (NOTE: to skip this use -n <num_keys>)
Importing 16 keys...
java.lang.NumberFormatException: Non-hex characters in value6
	at org.apache.cassandra.utils.Hex.hexToBytes(Hex.java:60)
	at org.apache.cassandra.utils.ByteBufferUtil.hexToBytes(ByteBufferUtil.java:503)
	at org.apache.cassandra.tools.SSTableImport.stringAsType(SSTableImport.java:578)
	at org.apache.cassandra.tools.SSTableImport.access$000(SSTableImport.java:59)
	at org.apache.cassandra.tools.SSTableImport$JsonColumn.<init>(SSTableImport.java:154)
	at org.apache.cassandra.tools.SSTableImport.addColumnsToCF(SSTableImport.java:231)
	at org.apache.cassandra.tools.SSTableImport.addToStandardCF(SSTableImport.java:214)
	at org.apache.cassandra.tools.SSTableImport.importSorted(SSTableImport.java:432)
	at org.apache.cassandra.tools.SSTableImport.importJson(SSTableImport.java:319)
	at org.apache.cassandra.tools.SSTableImport.main(SSTableImport.java:543)
ERROR: Non-hex characters in value6
{code}

Steps to reproduce:

{code}
$ ccm create -v git:trunk test-json-import
Fetching Cassandra updates...
Current cluster is now: test-json-import
$ ccm populate -n 1
$ ccm start
$ ccm node1 cqlsh
Connected to test-json-import at 127.0.0.1:9160.
[cqlsh 4.0.0 | Cassandra 2.0.0-rc1-SNAPSHOT | CQL spec 3.1.0 | Thrift protocol 19.37.0]
Use HELP for help.
cqlsh> CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
cqlsh> CREATE TABLE test.test (key varchar PRIMARY KEY, value varchar);
cqlsh> INSERT INTO test.test (key, value) VALUES ('ryan', 'ryan');
cqlsh> 
$ ccm node1 flush
$ ccm stop
$ ~/.ccm/test-json-import/node1/bin/json2sstable -s -K test -c test ~/Downloads/import_error/r.json ~/.ccm/test-json-import/node1/data/test/test/test-test-ja-1-Data.db 
{code}",,,,,,,,,,,,CASSANDRA-5853,,,,,,,,,,,,18/Sep/13 10:27;lyubent;5852_cassandra-1.2.patch;https://issues.apache.org/jira/secure/attachment/12603805/5852_cassandra-1.2.patch,18/Sep/13 15:09;lyubent;5852_cassandra-1.2_v2.patch;https://issues.apache.org/jira/secure/attachment/12603842/5852_cassandra-1.2_v2.patch,18/Sep/13 16:34;lyubent;5852_cassandra-1.2_v3.patch;https://issues.apache.org/jira/secure/attachment/12603858/5852_cassandra-1.2_v3.patch,18/Sep/13 10:45;lyubent;5852_cassandra-2.0.patch;https://issues.apache.org/jira/secure/attachment/12603806/5852_cassandra-2.0.patch,18/Sep/13 14:10;lyubent;5852_cassandra-2.0_v2.patch;https://issues.apache.org/jira/secure/attachment/12603830/5852_cassandra-2.0_v2.patch,06/Aug/13 16:59;enigmacurry;r.json;https://issues.apache.org/jira/secure/attachment/12596369/r.json,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2013-09-01 12:58:41.042,,,no_permission,,,,,,,,,,,,342164,,,Thu Sep 19 10:00:26 UTC 2013,,,,,,0|i1n0d3:,342469,2.1 rc3,,,,,,,slebresne,slebresne,,,,,,,,,,"01/Sep/13 12:58;lyubent;There are two problems here.
* First the json has an empty column that causes empty values to be added to the sstable [gist|https://gist.github.com/lyubent/6404249] 
* Second, the key is stored as hex which is correct, but the values are stored as strings, should we change this in sstable2json (when we export the data) or should the import tool (json2sstable) just allow an option to import from string values and hex values?",01/Sep/13 13:05;jbellis;sstable2json should convert values to be human-readable.  Blob type should be hex but others should not.,"17/Sep/13 21:52;lyubent;Changed the lookup for comparator to use CFMetaData#getColumnDefinition when it isn't null, otherwise the function falls back to the previously used CFMetaData#getValueValidator. Updated AbstractCompositeType#fromString to interpret empty cell names """" as cql3 row markers.",17/Sep/13 22:30;jbellis;I assume this is a problem in 1.2 as well?  If so we should commit there too.,18/Sep/13 10:45;lyubent;+1 Added patch for C* 1.2 ,"18/Sep/13 11:37;slebresne;Looking closer at this, I don't think we can change AbstractCompositeType.fromString() like that. The problem is that ACT.getString() will return an empty string in 2 different cases: if its argument is one component, but an empty one, and if it's argument is empty. Which means ACT.fromString() is basically stuck at picking one interpretation over the other and it currently picks ""completely empty"" currently.

Now, the problem is that there is cases where we do want ACT.fromString("""") to return an empty byte buffer and that's when you do a select query with CQL2. In that case, an empty byte buffer is ok (basically empty cell name are not ok, but empty is allowed as the bound of slices) and probably almost always what you want.

In sstable2json, that's the contrary, you'd want fromString("""") to always return a one empty component buffer, because a stored on disk cell name cannot be empty.

So anyway, just changing ACT.fromString("""") will break CQL2 and I'd rather avoid that. One possible fix would be to change getString() so it generates something different than """" when given a one empty component name and make fromString understand that. But maybe it's just not worth the trouble and we should just special case then handling of """" in SSTableImport directly, before passing it to ACT.fromString.

Other than that, on the patch itself, the corret way to get the type for the value would be to use: meta.getValueValidator(meta.getColumnDefinitionFromColumnName(...))
",18/Sep/13 14:10;lyubent;I agree with special casing being simpler. Switched to using CFMetaData#getValueValidator#getColumnDefinitionFromColumnName for retrieving column types and tweaked CFMetaData#getColumnDefinitionFromColumnName to avoid an [ArrayIndexOutOfBoundsException ex|https://gist.github.com/lyubent/6609670],"18/Sep/13 16:15;slebresne;You're right that getColumnDefinitionFromColumnName doesn't always work, but in fact to be more reliable it shouldn't assume anything on the number of components after split. So I'd changed it to something like:
{noformat}
ByteBuffer[] components = composite.split(columnName);
for (ColumnDefinition def : column_metadata.values())
{
    ByteBuffer toCompare;
    if (def.componentIndex == null)
    {
        toCompare = columnName;
    }
    else
    {
        if (def.componentIndex >= components.length)
            break;

        toCompare = components[def.componentIndex];
    }
    if (def.name.equals(toCompare))
        return def;
}
{noformat}

Other nits:
* the patch adds a println, not sure that was the intent.
* getValueValidator actually knows how to handle null, so we don't need to check the result of getColumnDefinitionFromColumnName first.

As a side note, it's ok to only provide a 1.2 patch (unless the patch for 2.0 has important differences that are related to the ticket, which is not the case here), I'll fix the 2.0 specificities during merge.
",18/Sep/13 16:34;lyubent;Added 5852_cassandra-1.2_v3.patch,"19/Sep/13 10:00;slebresne;+1, committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQLSH Windows: TypeError: argument of type 'NoneType' is not iterable,CASSANDRA-5812,12660109,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,mickdelaney,mickdelaney,26/Jul/13 18:05,12/Mar/19 14:04,13/Mar/19 22:29,27/Jul/13 01:46,1.2.8,2.0 rc1,,Legacy/Tools,,,0,cqlsh,windows,,,"I downloaded Cassandra Beta 2. 
I've tried to use CQLSH against Cassandra. 

My python version is: 
Enthought Canopy Python 2.7.3 | 32-bit | (default, Mar 25 2013, 15:38:39) [MSC v.1500 32 bit (Intel)] on win32

I get the following exception when I run the utility:

c:\Servers\apache-cassandra\bin>python cqlsh 127.0.0.1 9160
Traceback (most recent call last):
  File ""cqlsh"", line 131, in <module>
    if readline is not None and 'libedit' in readline.__doc__:
TypeError: argument of type 'NoneType' is not iterable",Windows 8 64 Bit,,,,,,,,,,,,,,,,,,,,,,,27/Jul/13 01:32;iamaleksey;5812.txt;https://issues.apache.org/jira/secure/attachment/12594499/5812.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-27 01:33:43.532,,,no_permission,,,,,,,,,,,,340301,,,Sat Jul 27 01:46:35 UTC 2013,,,,,,0|i1moyn:,340619,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,27/Jul/13 01:33;brandon.williams;+1,"27/Jul/13 01:46;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE from migration manager,CASSANDRA-5815,12660139,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,vkasar,vkasar,26/Jul/13 21:51,12/Mar/19 14:04,13/Mar/19 22:29,22/Oct/13 20:34,1.2.12,2.0.2,,,,,0,,,,,"In one of our production clusters we see this error often. Looking through the source, Gossiper.instance.getEndpointStateForEndpoint(endpoint) is returning null for some end point. De we need any config change on our end to resolve this? In any case, cassandra should be updated to protect against this NPE.

{noformat}
ERROR [OptionalTasks:1] 2013-07-24 13:40:38,972 AbstractCassandraDaemon.java (line 132) Exception in thread Thread[OptionalTasks:1,5,main] 
java.lang.NullPointerException 
at org.apache.cassandra.service.MigrationManager$1.run(MigrationManager.java:134) 
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441) 
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) 
at java.util.concurrent.FutureTask.run(FutureTask.java:138) 
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98) 
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206) 
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
at java.lang.Thread.run(Thread.java:662)
{noformat}

It turned out that the reason for NPE was we bootstrapped a node with the same token as another node. Cassandra should not throw an NPE here but log a meaningful error message. ",,,,,,,,,,,,,,,,,,,,,,,,18/Oct/13 18:05;brandon.williams;5185.txt;https://issues.apache.org/jira/secure/attachment/12609180/5185.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-07 15:37:46.338,,,no_permission,,,,,,,,,,,,340331,,,Tue Oct 22 20:34:21 UTC 2013,,,,,,0|i1mp53:,340649,,,,,,,,thobbs,thobbs,,,,,,,,,,"07/Oct/13 15:37;cburroughs;I'm seeing an NPE in migration manager in 1.2.9 and what I think is the same spot (line numbers changed slightly since July).  This occurs on at least one node every time (about 10 attempts) I try to bootstrap with a 2 dc production cluster using the GPFS w/ reconnecting.

{noformat}
ERROR [OptionalTasks:1] 2013-10-07 08:06:05,658 CassandraDaemon.java (line 194) Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.service.MigrationManager$1.run(MigrationManager.java:130)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

I added a log message to confirm that Gossiper really really thinks it's not there (off of the 1.2.10 tag if that matters).  I'm suspicious of this being a timing problem the reconnect dance, but I'm not sure how to prove or disprove that.

{noformat}
                    logger.warn(""[csb] Trying to get endpoint state for {} ; exists {}"", new Object[] {endpoint, Gossiper.instance.isKnownEndpoint(endpoint)});

 INFO [GossipTasks:1] 2013-10-07 11:19:10,565 Gossiper.java (line 803) InetAddress /208.49.103.36 is now DOWN
 INFO [GossipTasks:1] 2013-10-07 11:19:13,572 Gossiper.java (line 608) FatClient /208.49.103.36 has been silent for 30000ms, removing from gossip
 INFO [HANDSHAKE-/208.49.103.36] 2013-10-07 11:19:13,863 OutboundTcpConnection.java (line 399) Handshaking version with /208.49.103.36
 INFO [HANDSHAKE-/208.49.103.36] 2013-10-07 11:19:15,275 OutboundTcpConnection.java (line 399) Handshaking version with /208.49.103.36
 WARN [OptionalTasks:1] 2013-10-07 11:19:36,696 MigrationManager.java (line 130) [csb] Trying to get endpoint state for /208.49.103.36 ; exists false
ERROR [OptionalTasks:1] 2013-10-07 11:19:36,696 CassandraDaemon.java (line 193) Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.service.MigrationManager$1.run(MigrationManager.java:131)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}","07/Oct/13 15:41;brandon.williams;It looks the same to me.  The good news is the error is purely cosmetic at this point, there's nothing left to do if the gossiper has removed the node (not to mention it's a fat client)","07/Oct/13 16:28;cburroughs;Whoops, missed the important part for the case I am seeing but might not be part of the original (bootstrapping with the same token would presumably fail anyway).  The situation I am seeing post NPE is:
 * Bootstrapping node expects steams from NPE-node
 * NPE-node says it has no outstanding streams

And thus bootstrap never completes.","07/Oct/13 16:39;brandon.williams;[~cburroughs] I think your problem is something else, since the bootstrapping node has not only been marked down, but it's been down long enough to get removed (which is the race between the gossiper and MM causing this NPE)  I will note for myself though that the fat client removal should also wait until the node has been marked down before beginning the 30s countdown to removal.

If the node has connected but the gossiper doesn't know about it, they haven't gossiped yet, so there's really nothing for MM to do yet anyway.","18/Oct/13 18:02;brandon.williams;Mostly this problem is cosmetic, but the crux of it is that in a couple of places we assume that since we're in a call regarding an endpoint, the gossiper will always know about the endpoint while we're in the call.  This isn't the case with fat clients though, which the gossiper could expire at any time.  Patch to check that the gossiper actually still knows about the endpoint to avoid the NPE.",22/Oct/13 20:27;thobbs;+1,22/Oct/13 20:34;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CqlRecordWriter sends empty composite partition-key components to thrift,CASSANDRA-5949,12666030,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mccraig,mccraigmccraig,mccraigmccraig,28/Aug/13 23:12,12/Mar/19 14:04,13/Mar/19 22:29,28/Aug/13 23:27,1.2.10,,,,,,0,,,,,"when there is a composite partition-key, CqlRecordWriter.getPartitionKey() consumes the content of the key-component ByteBuffers, leaving empty key-components to be written to thrift",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-08-28 23:27:38.758,,,no_permission,,,,,,,,,,,,345969,,,Wed Aug 28 23:27:38 UTC 2013,,,,,,0|i1nnrz:,346270,1.2.8,,,,,,,jbellis,jbellis,,,,,,,,,,28/Aug/13 23:23;mccraigmccraig;patch in https://github.com/apache/cassandra/pull/19 builds the composite partition key with duplicate components,28/Aug/13 23:27;jbellis;Committed.  (To 1.2 only; CompositeType.build in 2.0 already builds in duplicate()).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sampling bug in metrics-core-2.0.3.jar used by Cassandra,CASSANDRA-5947,12665816,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,jblangston@datastax.com,jblangston@datastax.com,27/Aug/13 22:59,12/Mar/19 14:04,13/Mar/19 22:29,19/Sep/13 05:59,1.2.10,2.0.1,,Legacy/Tools,,,0,,,,,"There is a sampling bug in the version of the metrics library we're using in Cassandra. See https://github.com/codahale/metrics/issues/421. ExponentiallyDecayingSample is used by the Timer's histogram that is used in stress tool, and according to [~brandon.williams] it is also in a few other places like the dynamic snitch. The statistical theory involved in this bug goes over my head so i'm not sure if this would bug would meaningfully affect its usage by Cassandra.  One of the comments on the bug mentions that it affects slow sampling rates (10 samples/min was the example given).  We're currently distributing metrics-core-2.0.3.jar and according to the release nodes, this bug is fixed in 2.1.3: http://metrics.codahale.com/about/release-notes/#v2-1-3-aug-06-2012",,,,,,,,,,,,,,,,,,,,,,,,18/Sep/13 20:05;yukim;5947-1.2.txt;https://issues.apache.org/jira/secure/attachment/12603904/5947-1.2.txt,10/Sep/13 21:32;ravilr;Screen Shot 2013-09-10 at 2.23.11 PM.png;https://issues.apache.org/jira/secure/attachment/12602426/Screen+Shot+2013-09-10+at+2.23.11+PM.png,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-08-30 18:58:32.813,,,no_permission,,,,,,,,,,,,345755,,,Thu Sep 19 05:59:46 UTC 2013,,,,,,0|i1nmgf:,346056,,,,,,,,jbellis,jbellis,,,,,,,,,,30/Aug/13 18:58;jbellis;Upgraded metrics-core to 2.2.0.,"05/Sep/13 15:42;cburroughs;Obligatory ""you forget to update build.xml"".

One thing that might be worth noting is that this introduces annoying ""quoting"" in jconsole https://github.com/codahale/metrics/commit/e2336060816ad339676c0e162359c17381ded99f  so it's

{noformat}
""org.apache.cassandra.metrics""
{noformat}

instead of 

{noformat}
org.apache.cassandra.metrics
{noformat}

But unless there is some JMX voodoo I'm missing I think that's just a cosmetic issue.  cc [~yukim]","05/Sep/13 16:05;jbellis;bq. you forget to update build.xml

fixed.","10/Sep/13 21:29;ravilr;Is it just me or anyone else seeing this issue:  all mbean objectNames of org.apache.cassandra.metrics* have double quotes surrounding them after upgrading to metrics-core-2.2.0.jar.  like ""org.apache.cassandra.metrics"":type=""DroppedMessage"",scope=""READ"",name=""Dropped""  ?
",10/Sep/13 21:32;ravilr;Screenshot of jconsole showing double quoted object names only for org.apache.cassandra.metrics yammer metrics in cassandra-1.2.10(latest cassandra-1.2 branch) with metrics-core-2.2.0.jar,"11/Sep/13 00:28;ravilr;Sorry, missed Chris's comment above. If this is going to be the case, can we have the http://wiki.apache.org/cassandra/Metrics and NEWS.txt updated on this change.","12/Sep/13 14:49;jjordan;What jconsole are you running?  The 1.7u21 jconsole doesn't show the quoting.  As [~cburroughs] said earlier, I think it is just cosmetic either way.

Edit: Never mind, I see it now, it sorts the quoted one to the top.","12/Sep/13 15:37;yukim;With this update, I see o.a.c.metrics.ColumnFamily and ThreadPools are unquoted and all others are quoted.
This is because above those metrics build their own JMX object name instead of auto generating from coda metrics name.
We can force JMX name as before, and I think we should to avoid confusion.",18/Sep/13 20:05;yukim;Patch attached to change JMX ObjectName as the same (unquoted) as when we had metrics-core-2.0.3.jar.,18/Sep/13 23:15;jbellis;+1,19/Sep/13 05:59;yukim;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitLogReplayer date time issue,CASSANDRA-5909,12664833,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,artur.kronenberg,artur.kronenberg,21/Aug/13 13:12,12/Mar/19 14:04,13/Mar/19 22:29,15/Sep/13 00:59,1.2.10,,,,,,0,,,,,"Hi,

First off I am sorry if the component is not right for this. 

I am trying to get the point-in-time backup to work. And I ran into the following issues: 

1. The documentation in the commitlog_archiving.properties seems to be out of date, as the example date format is no more valid and can't be parsed. 

2. 

The restore_point_in_time property seems to differ from the actual maxTimeStamp. I added additional logging to the codebase in the class CommitLogReplayer like that: 

protected boolean pointInTimeExceeded(RowMutation frm)
    {
        long restoreTarget = CommitLog.instance.archiver.restorePointInTime;
        logger.info(String.valueOf(restoreTarget));
        for (ColumnFamily families : frm.getColumnFamilies())
        {
            logger.info(String.valueOf(families.maxTimestamp()));
        	if (families.maxTimestamp() > restoreTarget)
                return true;
        }
        return false;
    }

The following output can be seen: 

The restoreTarget timestamp is: 1377015783000
This has been correctly parsed as I added this date to the properties: 
2013:08:20 17:23:03

the value for families.maxTimestamp() is: 1377009021033000
This date corresponds to: Mon 45605-09-05 10:50:33 BST (44 millennia from now)

It seems like the timestamp has 3 additional zeros. This also means that the code can never return false on the call, as the restoreTarget will always be smaller then the maxTimestamp(). Therefore the Replayer can never replay any of my commitlog files. 
The timestamp minus the 3 zeros corresponds to ""Tue 2013-08-20 15:30:21 BST (23 hours ago)"" which makes more sense and would allow for the replay to work. 

My config: 

Cassandra-1.2.4
Java 1.6
Ubuntu 12.04 64bit 

If you need any more information let me know and I'll be happy to suply whatever info I can. 

-- artur 


",,,,,,,,,,,,,,,,,,,,,,,,27/Aug/13 07:35;vijay2win@yahoo.com;0001-CASSANDRA-5909.patch;https://issues.apache.org/jira/secure/attachment/12600113/0001-CASSANDRA-5909.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-27 01:52:53.305,,,no_permission,,,,,,,,,,,,344776,,,Sun Sep 15 00:59:59 UTC 2013,,,,,,0|i1ngf3:,345076,1.2.4,,,,,,,jbellis,jbellis,,,,,,,,,,"27/Aug/13 01:52;vijay2win@yahoo.com;Ahaaa looks like we need a configuration for Milli/Micro second precisions, 
Users should not mix those in a cluster to have a reliable delete and updates, so it should be fine. The other option is to write additional long field while storing the RM in the commit log.  ",27/Aug/13 07:35;vijay2win@yahoo.com;Attached patch and test case as a fix to add precision. Thanks!,"13/Sep/13 08:22;jbellis;Comments:

- Default in the config file is millis, but default in the code is micros.  For 1.2, should both be whatever it was before, for compatibility [think this is millis]; for 2.0 should be micros (add NEWS of change)
- Extra no-op eol semicolon in the test code; extra unused imports as well
- Formatting of ?: in archiver is nonstandard

Otherwise, LGTM.","15/Sep/13 00:59;vijay2win@yahoo.com;Fixed the comments and Committed to 1.2, 2.0 and trunk, Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Leveled compaction may cause overlap in L1 when L0 compaction get behind,CASSANDRA-5907,12664648,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,20/Aug/13 17:27,12/Mar/19 14:04,13/Mar/19 22:29,21/Aug/13 20:37,1.2.9,,,,,,0,lcs,,,,"1.2 makes LCS run parallel, though if L0 compaction get far behind and concurrent compactions at L0 where each compaction holds maximum number of SSTable to compact at L0(32), it will likely cause overlap in L1. There will be ERROR log as follows:

{code}
ERROR [CompactionExecutor:30] 2013-08-19 17:54:29,648 LeveledManifest.java (line 244) At level 1, SSTableReader(path='xxx-Data.db') [DecoratedKey(204853724659241194183955214890519, 30303132343830), DecoratedKey(69227335985728660912035125310473966323, 30393537373332)] overlaps SSTableReader(path='xxx-Data.db') [DecoratedKey(217896711032704014921095870827202, 30333635363932), DecoratedKey(71430242198281555888954138354238066233, 30333035343132)].  This is caused by a bug in Cassandra 1.1.0 .. 1.1.3.  Sending back to L0.  If you have not yet run scrub, you should do so since you may also have rows out-of-order within an sstable
{code}

We should send back compacted SSTables to L0 when compacting max SSTables at L0. Also, the above error message is confusing, at version 1.2, we can reduce to WARNing level without mentioning scrub.

C* 2.0 performs Size-Tiered compaction on L0 when it has max SSTables and sends back compacted SSTable to L0, so I think we don't need to fix this on 2.0.",,,,,,,,,,,,,,,,,,,,,,,,21/Aug/13 16:42;yukim;5907-1.2.txt;https://issues.apache.org/jira/secure/attachment/12599220/5907-1.2.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-20 17:36:07.889,,,no_permission,,,,,,,,,,,,344591,,,Wed Aug 21 20:37:20 UTC 2013,,,,,,0|i1nf9z:,344891,,,,,,,,jbellis,jbellis,,,1.2.0,,,,,,,"20/Aug/13 17:36;jbellis;bq. We should send back compacted SSTables to L0 when compacting max SSTables at L0

I'm not sure if that's the right solution.  Isn't it possible-but-unlikely to generate the same kind of overlap with non-maxed-out compactions if we happen to flush the right data at the right time after the first compaction starts?",20/Aug/13 18:36;jjordan;Is this an issue any time L0 compactions happen in parallel?  Maybe we should not allow that?,"20/Aug/13 19:12;yukim;No.
We check overlap with compacting L0 to prevent L1 overlap if L0 is not maxed out.
(https://github.com/apache/cassandra/blob/cassandra-1.2.8/src/java/org/apache/cassandra/db/compaction/LeveledManifest.java#L534)

So this only likely happen when compaction task with 32 L0 sstables run concurrently and those sstables have overlap.","20/Aug/13 23:07;jbellis;It looks like the problem then is a race when we are running getCandidatesFor before a parallel compaction has marked its sources compacting.  We synchronize getCompactionCandidates (caller of getCandidatesFor) but that is not adequate.

(Edit: which would mean that it can indeed happen with any number of candidates, in theory.)","21/Aug/13 16:42;yukim;I think simply moving check overlap with compacting L0 out to the end of getCandidateFor will fix the problem.
Patch attached.","21/Aug/13 17:00;jjordan;Reading the patch 10 times, I think that logic is right to skip doing parallel L0 compactions if there is any overlap.","21/Aug/13 18:16;jbellis;+1

NB: I think {{!Sets.intersection(candidates, compacting).isEmpty()}} is redundant since markCompacting will fail later.  Take that out in trunk?","21/Aug/13 20:37;yukim;Committed, with nit change in trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Most CQL3 functions should handle null gracefully,CASSANDRA-5910,12664835,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,21/Aug/13 13:21,12/Mar/19 14:04,13/Mar/19 22:29,11/Sep/13 06:32,1.2.10,,,,,,0,,,,,"Currently, we don't allow null parameters for functions. So
{noformat}
UPDATE test SET d=dateOf(null) WHERE k=0
{noformat}
is basically an invalid query. Unfortunately, there's at least one case where we don't validate correctly, namely if we do:
{noformat}
SELECT k, dateOf(t) FROM test
{noformat}
In that case, if for any of the row {{t}} is null, we end up with a server side NPE. But more importantly, throwing an InvalidException in that case would be pretty inconvenient and actually somewhat wrong since the query is not invalid in itself. So, at least in that latter case, we want {{dateOf(t) == null}} when {{t == null}}. And if we do that, I suggest making it always the case (i.e. make the first query valid but assigning {{null}} to {{d}}).
",,,,,,,,,,,,,,,,,,,,,,,,10/Sep/13 13:10;slebresne;5910-v2.txt;https://issues.apache.org/jira/secure/attachment/12602331/5910-v2.txt,21/Aug/13 13:23;slebresne;5910.txt;https://issues.apache.org/jira/secure/attachment/12599187/5910.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-08-21 13:46:51.744,,,no_permission,,,,,,,,,,,,344778,,,Wed Sep 11 06:32:30 UTC 2013,,,,,,0|i1ngfj:,345078,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"21/Aug/13 13:29;slebresne;Attaching simple patch for this. I'll note that because of multi-parameters functions (and because a future function may not want to return null when passed null as argument), I think it should be each function job to handle null the way it makes the most sense. That being said, outside of token() that refuse null competely, the patch makes all our currently existing function return null on null.
","21/Aug/13 13:46;jbellis;I don't think turning RTE into IRE is correct -- if it passes validation, it should execute, and if it doesn't, it's a bug and not an invalid request.

Where does this leave varcharasblob?  (Wish we'd gone with textasblob, FTR.)","21/Aug/13 14:43;slebresne;bq. I don't think turning RTE into IRE is correct

I don't really disagree and those catch all could probably be removed. The motivation was that if the execution of a function triggers an unexpected exception, then we send it back to the user instead of ""crashing"" server side, but as long as we don't have user custom function, an unexpected exception in a function is a bug so it's probably fine to throw server side. And I agree IRE is a bad exception anyway, we just don't have anything better so far.

But the bigger problem is that we have functions (in the selection of a SELECT) that take their input at execution time. So I don't fully agree with ""if it passes validation, it should execute, and if it doesn't, it's a bug"" as that would mean we limit ourselves to functions that can never error out but that's going to be pretty limiting. We already have the token() method for which it's very unclear how to deal with null. We could return null on a null parameter, but if we have a composite partition key (in which case token() is a multi-parameter function), what to do when only one argument is null and not the other? Of course we could still return null in that case, but it feels wrong to silent what is essentially an error. Or to take another example, what do we do about division by 0 if we add a division function tomorrow?

My hunch is that we need a new ExecutionException, though unfortunately it's not that easy to add new exceptions (rather, there is backward compatibility concerns).

To be clear, we can probably find a simple hack for now, making token() return null as soon as any of its parameter is null is, while not perfect, probably good enough for now. But it's probably worth having a longer term plan.

bq. Where does this leave varcharasblob?

varcharasblob (and all the asBlob functions for that matter) execute method just return it's argument without any processing (it's really just a way to make the CQL type system happy, since internally everything is already bytes anyway), so if said argument is null, it will return null.

bq. Wish we'd gone with textasblob

We have it too, we have both. We have one XasBlob and one blobAsX for every CQL3 type. The only reason varchar is a special case in ByteConversionFcts is that the makeToBlobFunction method wouldn't generate it.",26/Aug/13 22:05;jbellis;So how about we drop the RTE->IRE conversion for now and we can look at adding an ExecutionException around when we add more problematic functions/operators and/or UDF?,10/Sep/13 13:10;slebresne;Sounds good to me. Attaching v2 that just makes all existing function handle null correctly (token() will return null as soon as any one of its argument is null). We can see later for what we do when returning null doesn't fly anymore.,10/Sep/13 15:49;iamaleksey;+1,"11/Sep/13 06:32;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Offline scrubs can choke on broken files,CASSANDRA-5930,12665333,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thobbs,jjordan,jjordan,23/Aug/13 21:54,12/Mar/19 14:04,13/Mar/19 22:29,03/Feb/14 20:33,2.0.5,,,,,,1,,,,,"There are cases where offline scrub can hit an exception and die, like:

{noformat}
WARNING: Non-fatal error reading row (stacktrace follows)
Exception in thread ""main"" java.io.IOError: java.io.IOError: java.io.EOFException
	at org.apache.cassandra.db.compaction.Scrubber.scrub(Scrubber.java:242)
	at org.apache.cassandra.tools.StandaloneScrubber.main(StandaloneScrubber.java:121)
Caused by: java.io.IOError: java.io.EOFException
	at org.apache.cassandra.db.compaction.PrecompactedRow.merge(PrecompactedRow.java:116)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:99)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:176)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:182)
	at org.apache.cassandra.db.compaction.Scrubber.scrub(Scrubber.java:171)
	... 1 more
Caused by: java.io.EOFException
	at java.io.RandomAccessFile.readFully(RandomAccessFile.java:399)
	at java.io.RandomAccessFile.readFully(RandomAccessFile.java:377)
	at org.apache.cassandra.utils.BytesReadTracker.readFully(BytesReadTracker.java:95)
	at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:401)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:363)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:120)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:37)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumns(ColumnFamilySerializer.java:144)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:234)
	at org.apache.cassandra.db.compaction.PrecompactedRow.merge(PrecompactedRow.java:112)
	... 5 more
{noformat}

Since the purpose of offline scrub is to fix broken stuff, it should be more resilient to broken stuff...",,,,,,,,,,,,,,,,,,,,,,,,03/Jan/14 22:39;thobbs;5930-v1.patch;https://issues.apache.org/jira/secure/attachment/12621396/5930-v1.patch,07/Jan/14 21:40;thobbs;5930-v2.patch;https://issues.apache.org/jira/secure/attachment/12621862/5930-v2.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-08-23 21:57:29.156,,,no_permission,,,,,,,,,,,,345273,,,Mon Feb 03 20:33:07 UTC 2014,,,,,,0|i1njhj:,345574,1.1.7,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"23/Aug/13 21:57;jbellis;Can you have a look, Jason?","02/Sep/13 16:49;jeffpotter;We're seeing this too -- slightly different stack trace, which I'll include here in case it's of use.


WARNING: Non-fatal error reading row (stacktrace follows)
Exception in thread ""main"" java.io.IOError: java.lang.IllegalArgumentException
at org.apache.cassandra.db.compaction.Scrubber.scrub(Scrubber.java:244)
at org.apache.cassandra.tools.StandaloneScrubber.main(StandaloneScrubber.java:125)
Caused by: java.lang.IllegalArgumentException 
at java.nio.Buffer.limit(Buffer.java:247)
at org.apache.cassandra.db.marshal.AbstractCompositeType.getBytes(AbstractCompositeType.java:51)
at org.apache.cassandra.db.marshal.AbstractCompositeType.getWithShortLength(AbstractCompositeType.java:60)
at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:78) 
at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:31)
at org.apache.cassandra.db.ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:128)
at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:114)
at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:109) 
at org.apache.cassandra.db.ColumnFamily.addAtom(ColumnFamily.java:219)
at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumnsFromSSTable(ColumnFamilySerializer.java:149)
at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:234)
at org.apache.cassandra.db.compaction.PrecompactedRow.merge(PrecompactedRow.java:114) 
at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:98)
at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:160)
at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:166)
at org.apache.cassandra.db.compaction.Scrubber.scrub(Scrubber.java:173) 
... 1 more
","02/Jan/14 18:50;thobbs;[~jeffpotter] what version of Cassandra were you running when you hit the above error?

As far as the original stacktrace for this ticket goes, it's unfortunately necessary for counter CFs.  CASSANDRA-2759 explains the reasoning.  I suppose I could make the error message mention that and point to the ticket.

The scrub code looks reasonably robust in general, so I think it's better to wait for individual bugs to get reported than to try to improve the code without any failure examples.","02/Jan/14 23:02;jpotter;Hi Tyler -- based on my notes, it should have been Cassandra 1.2.6.1 (DSE 3.1), at least, that's what other tickets we have filed at this same time suggest.","03/Jan/14 22:39;thobbs;Thanks, [~jeffpotter].  It looks like you also had a Counter table, in this case.

5930-v1.patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-5930-2.0]) clarifies the error message for counter tables.","03/Jan/14 23:52;jbellis;It would be nice to be able to tell people how to fix it (realistically: what their options are) rather than just ""sorry, scrub can't help you.""  But I'm not sure what those options are. :)  /cc [~slebresne] [~iamaleksey]","04/Jan/14 00:00;iamaleksey;[~jbellis] There are no options. That said, we should probably allow users to override this behavior, if they prefer losing some of the counters history to not scrubbing at all.

Also, with CASSANDRA-6504 in this becomes a non-issue (for the newly written 'global' 2.1 shards, at least - we *can* repair those after the scrub).","07/Jan/14 21:40;thobbs;You're right, it would be nice to give the user an option to skip the corrupted rows anyway.

5930-v2.patch (the [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-5930-2.0] is still good) adds a {{--skip-corrupted}} option and a unit test to exercise it.","03/Feb/14 20:33;iamaleksey;LGTM, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot drop keyspace Keyspace1 after running cassandra-stress,CASSANDRA-5957,12666319,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thobbs,pkolaczk,pkolaczk,30/Aug/13 13:26,12/Mar/19 14:04,13/Mar/19 22:29,14/Oct/13 09:38,1.2.11,2.0.2,,,,,0,,,,,"Steps to reproduce:
# Set MAX_HEAP=""2G"", HEAP_NEWSIZE=""400M""
# Run ./cassandra-stress -n 50000 -c 400 -S 256
# The test should complete despite several warnings about low heap memory.
# Try to drop keyspace:
{noformat}
cqlsh> drop keyspace Keyspace1;
TSocket read 0 bytes
{noformat}

system.log:
{noformat}
 INFO 15:10:46,516 Enqueuing flush of Memtable-schema_columnfamilies@2127258371(0/0 serialized/live bytes, 1 ops)
 INFO 15:10:46,516 Writing Memtable-schema_columnfamilies@2127258371(0/0 serialized/live bytes, 1 ops)
 INFO 15:10:46,690 Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-ic-6-Data.db (38 bytes) for commitlog position ReplayPosition(segmentId=1377867520699, position=19794574)
 INFO 15:10:46,692 Enqueuing flush of Memtable-schema_columns@1997964959(0/0 serialized/live bytes, 1 ops)
 INFO 15:10:46,693 Writing Memtable-schema_columns@1997964959(0/0 serialized/live bytes, 1 ops)
 INFO 15:10:46,857 Completed flushing /var/lib/cassandra/data/system/schema_columns/system-schema_columns-ic-6-Data.db (38 bytes) for commitlog position ReplayPosition(segmentId=1377867520699, position=19794574)
 INFO 15:10:46,897 Enqueuing flush of Memtable-local@1366216652(98/98 serialized/live bytes, 3 ops)
 INFO 15:10:46,898 Writing Memtable-local@1366216652(98/98 serialized/live bytes, 3 ops)
 INFO 15:10:47,064 Completed flushing /var/lib/cassandra/data/system/local/system-local-ic-12-Data.db (139 bytes) for commitlog position ReplayPosition(segmentId=1377867520699, position=19794845)
 INFO 15:10:48,956 Enqueuing flush of Memtable-local@432522279(46/46 serialized/live bytes, 1 ops)
 INFO 15:10:48,957 Writing Memtable-local@432522279(46/46 serialized/live bytes, 1 ops)
 INFO 15:10:49,132 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 400882073/1094043713)bytes
 INFO 15:10:49,175 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 147514075/645675954)bytes
 INFO 15:10:49,185 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 223249644/609072261)bytes
 INFO 15:10:49,202 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 346471085/990388210)bytes
 INFO 15:10:49,215 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 294748503/2092376617)bytes
 INFO 15:10:49,257 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 692722235/739328646)bytes
 INFO 15:10:49,285 Completed flushing /var/lib/cassandra/data/system/local/system-local-ic-13-Data.db (82 bytes) for commitlog position ReplayPosition(segmentId=1377867520699, position=19794974)
 INFO 15:10:49,286 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-ic-10-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-ic-13-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-ic-12-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-ic-11-Data.db')]
ERROR 15:10:49,287 Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-78-Data.db') was already marked compacted
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:378)
	at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:281)
	at org.apache.cassandra.service.MigrationManager.announceKeyspaceDrop(MigrationManager.java:262)
	at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:718)
	at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:775)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1668)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:4048)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:4036)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-78-Data.db') was already marked compacted
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:374)
	... 13 more
Caused by: java.lang.AssertionError: SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-78-Data.db') was already marked compacted
	at org.apache.cassandra.db.DataTracker.removeOldSSTablesSize(DataTracker.java:354)
	at org.apache.cassandra.db.DataTracker.postReplace(DataTracker.java:325)
	at org.apache.cassandra.db.DataTracker.unreferenceSSTables(DataTracker.java:264)
	at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:302)
	at org.apache.cassandra.db.Table.unloadCf(Table.java:314)
	at org.apache.cassandra.db.Table.dropCf(Table.java:296)
	at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:607)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:469)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:355)
	at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:299)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
ERROR 15:10:49,287 Exception in thread Thread[MigrationStage:1,5,main]
java.lang.AssertionError: SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-78-Data.db') was already marked compacted
	at org.apache.cassandra.db.DataTracker.removeOldSSTablesSize(DataTracker.java:354)
	at org.apache.cassandra.db.DataTracker.postReplace(DataTracker.java:325)
	at org.apache.cassandra.db.DataTracker.unreferenceSSTables(DataTracker.java:264)
	at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:302)
	at org.apache.cassandra.db.Table.unloadCf(Table.java:314)
	at org.apache.cassandra.db.Table.dropCf(Table.java:296)
	at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:607)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:469)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:355)
	at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:299)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
 INFO 15:10:49,471 Compacted 4 sstables to [/var/lib/cassandra/data/system/local/system-local-ic-14,].  829 bytes to 501 (~60% of original) in 184ms = 0,002597MB/s.  4 total rows, 1 unique.  Row merge counts were {1:0, 2:0, 3:0, 4:1, }
{noformat}

",Cassandra 1.2.9 freshly built from cassandra-1.2 branch (f5b224cf9aa0f319d51078ef4b78d55e36613963),,,,,,,,,,,,,,,,,,,,,,,09/Oct/13 22:10;thobbs;5957-1.2-v1.patch;https://issues.apache.org/jira/secure/attachment/12607668/5957-1.2-v1.patch,11/Oct/13 20:33;thobbs;5957-1.2-v2.patch;https://issues.apache.org/jira/secure/attachment/12608067/5957-1.2-v2.patch,31/Aug/13 19:23;pkolaczk;system.log;https://issues.apache.org/jira/secure/attachment/12600956/system.log,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-08-30 13:51:22.967,,,no_permission,,,,,,,,,,,,346258,,,Mon Oct 14 09:39:05 UTC 2013,,,,,,0|i1npjr:,346559,,,,,,,,jbellis,jbellis,,,,,,,,,,"30/Aug/13 13:51;aecobley;IS this because stress test is creating the keyspace via thrift rather than CQL ?  try running the test with the --enable-cql option and then droping the keyspace ?
",30/Aug/13 15:32;pkolaczk;No. I checked with --enable-cql (after starting from fresh empty /var/lib/cassandra) and the problem still persists.,"30/Aug/13 17:00;aecobley;Did you try deleting the keyspace with cassandra-cli  before running the test with --enable-cql ?
","31/Aug/13 05:58;pkolaczk;Nope. But I can try it for you. However, my intuition tells me it is unrelated ;)

So far I tried:
- running cassandra-stress without cql and then dropping from cqlsh / cqlsh -3
- running cassandra-stress with --enable-cql and then dropping from cqlsh / cqlsh -3

","31/Aug/13 06:02;pkolaczk;If I let cassandra-stress write only *one* row, deleting keyspace from cqlsh works fine.
I guess the problem might be compaction that is running long after cassandra-stress finished its work. And dropping keyspace during compaction breaks it. ","31/Aug/13 06:29;pkolaczk;Ok, I tried from cassandra-cli and it worked fine. Not sure if it was a one-time luck so I'll try once again.
I tried once again with cqlsh and this time I got a different error:

{noformat}
INFO 08:26:10,314 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 107063147/625780253)bytes
 INFO 08:26:10,326 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 256225944/1056670573)bytes
 INFO 08:26:10,340 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 256445786/533336692)bytes
 INFO 08:26:10,350 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 369114811/593903163)bytes
ERROR 08:26:10,415 Unable to delete /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-34-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR 08:26:10,415 Unable to delete /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-13-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR 08:26:10,415 Unable to delete /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-36-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR 08:26:10,416 Unable to delete /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-23-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR 08:26:10,416 Unable to delete /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-22-Data.db (it will be removed on server restart; we'll also retry after GC)
 INFO 08:26:10,416 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 794619126/1234742593)bytes
 INFO 08:26:10,548 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 186316188/523663644)bytes
 INFO 08:26:10,604 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 495853724/605444868)bytes
ERROR 08:26:10,618 Unable to delete /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-30-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR 08:26:10,627 Unable to delete /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-15-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR 08:26:10,627 Unable to delete /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-14-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR 08:26:10,639 Unable to delete /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-29-Data.db (it will be removed on server restart; we'll also retry after GC)
 INFO 08:26:10,793 Completed flushing /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-92-Data.db (24952067 bytes) for commitlog position ReplayPosition(segmentId=1377929629788, position=10887214)
 INFO 08:26:10,802 Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-57-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-83-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-92-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-55-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-91-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-74-Data.db')]
 INFO 08:26:10,805 Compacted 4 sstables to [/var/lib/cassandra/data/system/schema_keyspaces/system-schema_keyspaces-ic-5,].  529 bytes to 249 (~47% of original) in 872ms = 0,000272MB/s.  6 total rows, 3 unique.  Row merge counts were {1:2, 2:0, 3:0, 4:1, }
 INFO 08:26:10,810 Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-48-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-52-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-62-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-68-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-87-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-65-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-31-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-33-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-81-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-45-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-28-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-66-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-69-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-25-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-19-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-21-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-88-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-58-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-84-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-61-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-78-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-72-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-73-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-75-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-64-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-59-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-24-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-46-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-76-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-60-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-80-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-79-Data.db')]
 INFO 08:26:10,813 Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-40-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-56-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-10-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-8-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-50-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-86-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-67-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-49-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-44-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-85-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-26-Data.db')]
ERROR 08:26:10,815 Exception in thread Thread[CompactionExecutor:4,1,main]
java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-31-Data.db (No such file or directory)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:53)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1212)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:54)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1032)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1044)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:157)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:163)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:117)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:208)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-31-Data.db (No such file or directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:216)
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
	at org.apache.cassandra.io.util.ThrottledReader.<init>(ThrottledReader.java:35)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:49)
	... 18 more
ERROR 08:26:10,815 Exception in thread Thread[CompactionExecutor:5,1,main]
java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-10-Data.db (No such file or directory)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:53)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1212)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:54)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1032)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1044)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:157)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:163)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:117)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:208)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-10-Data.db (No such file or directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:216)
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
	at org.apache.cassandra.io.util.ThrottledReader.<init>(ThrottledReader.java:35)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:49)
	... 18 more
 INFO 08:26:10,821 Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-48-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-52-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-62-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-68-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-87-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-65-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-31-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-33-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-81-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-45-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-28-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-66-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-69-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-25-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-19-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-21-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-88-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-58-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-84-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-61-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-78-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-72-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-73-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-75-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-64-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-59-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-24-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-46-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-76-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-60-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-80-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-79-Data.db')]
ERROR 08:26:10,822 Exception in thread Thread[CompactionExecutor:8,1,main]
java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-31-Data.db (No such file or directory)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:53)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1212)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:54)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1032)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1044)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:157)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:163)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:117)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:208)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-31-Data.db (No such file or directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:216)
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
	at org.apache.cassandra.io.util.ThrottledReader.<init>(ThrottledReader.java:35)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:49)
	... 18 more
ERROR 08:26:10,862 Exception in thread Thread[MigrationStage:1,5,main]
java.lang.RuntimeException: Tried to hard link to file that does not exist /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-28-Index.db
	at org.apache.cassandra.io.util.FileUtils.createHardLink(FileUtils.java:72)
	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:1081)
	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1567)
	at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1612)
	at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:608)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:471)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:356)
	at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:304)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
ERROR 08:26:10,863 Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Tried to hard link to file that does not exist /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-28-Index.db
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:379)
	at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:286)
	at org.apache.cassandra.service.MigrationManager.announceKeyspaceDrop(MigrationManager.java:267)
	at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:718)
	at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:775)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1668)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:4048)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:4036)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Tried to hard link to file that does not exist /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-28-Index.db
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:375)
	... 13 more
Caused by: java.lang.RuntimeException: Tried to hard link to file that does not exist /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-28-Index.db
	at org.apache.cassandra.io.util.FileUtils.createHardLink(FileUtils.java:72)
	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:1081)
	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1567)
	at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1612)
	at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:608)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:471)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:356)
	at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:304)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
 INFO 08:26:19,760 Compacted 6 sstables to [/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-93,].  137 181 408 bytes to 137 181 408 (~100% of original) in 8 958ms = 14,604419MB/s.  1 248 total rows, 1 248 unique.  Row merge counts were {1:1248, 2:0, 3:0, 4:0, 5:0, 6:0, }
 INFO 08:26:19,765 Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-40-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-56-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-10-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-8-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-50-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-86-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-67-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-49-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-44-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-85-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-26-Data.db')]
ERROR 08:26:19,765 Exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-10-Data.db (No such file or directory)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:53)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1212)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:54)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1032)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1044)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:157)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:163)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:117)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:208)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-10-Data.db (No such file or directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:216)
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
	at org.apache.cassandra.io.util.ThrottledReader.<init>(ThrottledReader.java:35)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:49)
	... 18 more
 INFO 08:26:19,766 Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-48-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-52-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-62-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-68-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-87-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-65-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-31-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-33-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-81-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-45-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-28-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-66-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-69-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-25-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-19-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-21-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-88-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-58-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-84-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-61-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-78-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-72-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-73-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-75-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-64-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-59-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-24-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-46-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-76-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-60-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-80-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-79-Data.db')]
 INFO 08:26:19,769 Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-40-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-56-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-10-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-8-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-50-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-86-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-67-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-49-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-44-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-85-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-26-Data.db')]
ERROR 08:26:19,769 Exception in thread Thread[CompactionExecutor:3,1,main]
java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-31-Data.db (No such file or directory)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:53)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1212)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:54)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1032)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1044)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:157)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:163)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:117)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:208)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-31-Data.db (No such file or directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:216)
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
	at org.apache.cassandra.io.util.ThrottledReader.<init>(ThrottledReader.java:35)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:49)
	... 18 more
ERROR 08:26:19,771 Exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-10-Data.db (No such file or directory)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:53)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1212)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:54)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1032)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1044)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:157)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:163)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:117)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:208)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-10-Data.db (No such file or directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:216)
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
	at org.apache.cassandra.io.util.ThrottledReader.<init>(ThrottledReader.java:35)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:49)
	... 18 more

{noformat}","31/Aug/13 06:30;pkolaczk;And on the cqlsh side:

{noformat}
pkolaczk@m4600:~/Projekty/datastax/cassandra$ bin/cqlsh 
Connected to Test Cluster at 127.0.0.1:9160.
[cqlsh 3.1.7 | Cassandra 0.0.0 | CQL spec 2.0.0 | Thrift protocol 19.36.0]
Use HELP for help.
cqlsh> drop keyspace Keyspace1;
TSocket read 0 bytes
cqlsh> drop keyspace Keyspace1;
TSocket read 0 bytes
cqlsh> drop keyspace Keyspace1;
Traceback (most recent call last):
  File ""bin/cqlsh"", line 1038, in perform_statement_untraced
    self.cursor.execute(statement, decoder=decoder)
  File ""bin/../lib/cql-internal-only-1.4.0.zip/cql-1.4.0/cql/cursor.py"", line 80, in execute
    response = self.get_response(prepared_q, cl)
  File ""bin/../lib/cql-internal-only-1.4.0.zip/cql-1.4.0/cql/thrifteries.py"", line 80, in get_response
    return self.handle_cql_execution_errors(doquery, compressed_q, compress)
  File ""bin/../lib/cql-internal-only-1.4.0.zip/cql-1.4.0/cql/thrifteries.py"", line 96, in handle_cql_execution_errors
    return executor(*args, **kwargs)
  File ""bin/../lib/cql-internal-only-1.4.0.zip/cql-1.4.0/cql/cassandra/Cassandra.py"", line 1741, in execute_cql_query
    self.send_execute_cql_query(query, compression)
  File ""bin/../lib/cql-internal-only-1.4.0.zip/cql-1.4.0/cql/cassandra/Cassandra.py"", line 1751, in send_execute_cql_query
    self._oprot.trans.flush()
  File ""bin/../lib/thrift-python-internal-only-0.7.0.zip/thrift/transport/TTransport.py"", line 293, in flush
    self.__trans.write(buf)
  File ""bin/../lib/thrift-python-internal-only-0.7.0.zip/thrift/transport/TSocket.py"", line 117, in write
    plus = self.handle.send(buff)
error: [Errno 32] Broken pipe


{noformat}","31/Aug/13 06:32;pkolaczk;After this I cannot also drop it from cassandra-cli:

{noformat}

pkolaczk@m4600:~/Projekty/datastax/cassandra$ bin/cassandra-cli
Column Family assumptions read from /home/pkolaczk/.cassandra/assumptions.json
Connected to: ""Test Cluster"" on 127.0.0.1/9160
Welcome to Cassandra CLI version 1.2.9-SNAPSHOT

Type 'help;' or '?' for help.
Type 'quit;' or 'exit;' to quit.

[default@unknown] drop keyspace Keyspace1;
null
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_system_drop_keyspace(Cassandra.java:1437)
	at org.apache.cassandra.thrift.Cassandra$Client.system_drop_keyspace(Cassandra.java:1424)
	at org.apache.cassandra.cli.CliClient.executeDelKeySpace(CliClient.java:1364)
	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:249)
	at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:213)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:339)

{noformat}","31/Aug/13 17:01;aecobley;I'm having trouble reproducing this myself.  Can you post your cluster size, OS and java version ?
","31/Aug/13 19:17;pkolaczk;Ubuntu Linux 13.04, single node
{noformat}
java version ""1.6.0_45""
Java(TM) SE Runtime Environment (build 1.6.0_45-b06)
Java HotSpot(TM) 64-Bit Server VM (build 20.45-b01, mixed mode)
{noformat}",31/Aug/13 19:23;pkolaczk;Attaching system.log from the whole last run.,"02/Sep/13 14:20;pkolaczk;Today I got this on C* 1.2.6 dse 3.1.2. Slightly different exception, but maybe will be helpful:

{noformat}
ERROR 16:16:37,702 Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Tried to hard link to file that does not exist /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-64-Statistics.db
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:378)
	at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:281)
	at org.apache.cassandra.service.MigrationManager.announceKeyspaceDrop(MigrationManager.java:262)
	at org.apache.cassandra.cql3.statements.DropKeyspaceStatement.announceMigration(DropKeyspaceStatement.java:60)
	at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:73)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:145)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:162)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1714)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4074)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4062)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:201)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Tried to hard link to file that does not exist /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-64-Statistics.db
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:374)
	... 15 more
{noformat}","09/Oct/13 22:10;thobbs;Quick summary of the problem: when the table is dropped, there are multiple ongoing compactions.  When handling the table drop, DataTracker first builds a view of non-compacting sstables.  Meanwhile, the compaction tasks get cancelled, and as part of the cleanup, it marks the sstables as compacted (because the column family is no longer valid).  Then the DataTracker finally tries to mark the sstables in its view as compacted, hitting the failing assertion.  Additionally, when a compaction task is cancelled, it doesn't unreference the sstables involved, and since the DataTracker ignored compacting sstables, they never get unreferenced (even if the failing assertion is disabled).

5957-1.2-v1.patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-5957]) uses the simplest fix I could think of, which is to block the table drop operation until all compactions on that table have stopped (either normally, because of an error, or because they saw the stop signal).  This way, when the DataTracker cleans up sstables as part of the drop operation, there are no compacting sstables.","10/Oct/13 02:45;jbellis;I think we have a problem still.  We could have a compaction started by another thread after we stop the existing ones.  This requires the schema to be fetched before the purge, then the thread to be suspended until after we purge and stop compactions.  Rare, but possible.

Turns out it was truncate I was thinking of that changed in 2.0.  What we did there was introduce CFS.runWithCompactionsDisabled to avoid this race.

So you might want to backport that (d72e9381 for CASSANDRA-3430 -- I don't remember if this introduced bugs that we fixed in later commits.  Quite possible) but it's a bit involved.

I think there's a similar problem with flushing.  With truncate we don't care (if new writes arrive after the truncate starts, we're totally fine with having them survive; in fact, that's desirable) but we might care here.

All things considered I suspect it will be simpler to fix the cleanup race rather than try to lock down everything for the drop.","11/Oct/13 20:33;thobbs;5957-1.2-v2.patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-5957-v2]) disables the ""was not marked compacted"" assertion when cleaning up sstables for a dropped table and unreferences sstables when compaction tasks finish (or are interrupted) if the table has been dropped.

Regarding flushing, a blocking flush is forced after unregistering the CF but prior to the CFS being invalidated (which is when sstables get unreferenced), so we shouldn't see a similar problem there (and I haven't seen one while testing this).",14/Oct/13 09:39;jbellis;LGTM; committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Unable to find property"" errors from snakeyaml are confusing",CASSANDRA-5958,12666334,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mishail,jblangston@datastax.com,jblangston@datastax.com,30/Aug/13 14:39,12/Mar/19 14:04,13/Mar/19 22:29,03/Sep/13 17:12,2.0.1,,,,,,0,,,,,"When an unexpected property is present in cassandra.yaml (e.g. after upgrading), snakeyaml outputs the following message:

{code}Unable to find property 'some_property' on class: org.apache.cassandra.config.Config{code}

The error message is kind of counterintuitive because at first glance it seems to suggest the property is missing from the yaml file, when in fact the error is caused by the *presence* of an unrecognized property.  I know if you read it carefully it says it can't find the property on the class, but this has confused more than one user.

I think we should catch this exception and wrap it in another exception that says something like this:

{code}Please remove 'some_property' from your cassandra.yaml. It is not recognized by this version of Cassandra.{code}

Also, it might make sense to make this a warning instead of a fatal error, and just ignore the unwanted property.",,,,,,,,,,,,,,,,,,,,,,,,02/Sep/13 00:17;mishail;trunk-5958-skip-missing-properties.patch;https://issues.apache.org/jira/secure/attachment/12601017/trunk-5958-skip-missing-properties.patch,02/Sep/13 18:01;mishail;trunk-5958-v2-print-all-invalid-properties.patch;https://issues.apache.org/jira/secure/attachment/12601081/trunk-5958-v2-print-all-invalid-properties.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-08-30 15:08:28.525,,,no_permission,,,,,,,,,,,,346273,,,Tue Sep 03 17:12:31 UTC 2013,,,,,,0|i1npn3:,346574,2.0 rc2,,,,,,,jbellis,jbellis,,,,,,,,,,30/Aug/13 15:08;cburroughs;Which version is this with?  We upgraded snakyaml in 2.0.x and it might have improved error messages.,30/Aug/13 16:10;jblangston@datastax.com;1.2 and prior,30/Aug/13 17:26;jblangston@datastax.com;I just tested with 2.0.0-rc2 and the message is the same as before.,02/Sep/13 00:17;mishail;Instruct SnakeYaml to skip missing properties,"02/Sep/13 02:22;jbellis;# Skipping missing properties looks like the inverse of the problem here, which is dealing with properties that don't exist in the Config class
# We don't want to skip over errors, just turn the stacktrace into a friendlier message","02/Sep/13 18:01;mishail;A new patch.
Don't skip missing properties, print them all out and terminate.",03/Sep/13 03:57;jbellis;does HashSet.toString actually give us a human-readable error?,"03/Sep/13 04:10;mishail;For example I have the following in my cassandra.yaml

{code:title=cassandra.yaml}
oh: my
bla: bla
{code}

Then the stacktrace will be
{code}
ERROR 04:06:57 Fatal configuration error
org.apache.cassandra.exceptions.ConfigurationException: Invalid yaml. Please remove properties [bla, oh] from your cassandra.yaml
        at org.apache.cassandra.config.YamlConfigurationLoader$MissingPropertiesChecker.check(YamlConfigurationLoader.java:131) ~[main/:na]
        at org.apache.cassandra.config.YamlConfigurationLoader.loadConfig(YamlConfigurationLoader.java:94) ~[main/:na]
        at org.apache.cassandra.config.DatabaseDescriptor.loadConfig(DatabaseDescriptor.java:128) ~[main/:na]
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:104) ~[main/:na]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:153) ~[main/:na]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:391) ~[main/:na]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:434) ~[main/:na]
Invalid yaml. Please remove properties [bla, oh] from your cassandra.yaml
Fatal configuration error; unable to start. See log for stacktrace.
{code}","03/Sep/13 17:12;jbellis;LGTM, committed!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool info throws NPE when connected to a booting instance,CASSANDRA-5968,12666573,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mishail,jalkanen,jalkanen,02/Sep/13 12:21,12/Mar/19 14:04,13/Mar/19 22:29,13/Sep/13 17:20,1.2.10,2.0.1,,,,,0,,,,,"When an instance is newly added to the cluster and it's still streaming stuff, trying to call nodetool info on it throws NPE. Stack trace below.

To replicate: add a new node to the cluster, run nodetool info before bootstrap is complete.

Expected behaviour: is nice and just says RPC server is not running.

{noformat}
$ nodetool info
Token            : (invoke with -T/--tokens to see all 0 tokens)
ID               : cc7bcf48-4a54-48af-97f6-99c82bce76f2
Gossip active    : true
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.isRPCServerRunning(StorageService.java:330)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1464)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
	at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:657)
	at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at sun.rmi.transport.Transport$1.run(Transport.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
{noformat}
","Cassandra 1.2.9, Ubuntu 12.04 LTS, Oracle JVM 7u25",,,,,,,,,,,,,,,,,,,,,,,03/Sep/13 06:29;mishail;cassandra-1.2-5968.patch;https://issues.apache.org/jira/secure/attachment/12601118/cassandra-1.2-5968.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-03 06:29:45.772,,,no_permission,,,,,,,,,,,,346511,,,Fri Sep 13 17:20:22 UTC 2013,,,,,,0|i1nr3r:,346812,1.2.9,,,,,,,brandon.williams,brandon.williams,,,1.2.9,,,,,,,03/Sep/13 06:29;mishail;Patch: thriftServer and nativeServer can be null,"13/Sep/13 17:20;brandon.williams;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh raises a ValueError when connecting to Cassandra running in Eclipse,CASSANDRA-5964,12666433,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,gdeangel,gdeangel,31/Aug/13 00:14,12/Mar/19 14:04,13/Mar/19 22:29,01/Sep/13 05:24,2.0.1,,,,,,0,,,,,"The release_version is set to 'Unknown' in system.local so the version parsing logic fails.

Traceback (most recent call last):
  File ""./cqlsh"", line 2027, in <module>
    main(*read_options(sys.argv[1:], os.environ))
  File ""./cqlsh"", line 2013, in main
    display_float_precision=options.float_precision)
  File ""./cqlsh"", line 486, in __init__
    self.get_connection_versions()
  File ""./cqlsh"", line 580, in get_connection_versions
    self.cass_ver_tuple = tuple(map(int, vers['build'].split('-', 1)[0].split('.')[:3]))
ValueError: invalid literal for int() with base 10: 'Unknown'",,,,,,,,,,,,,,,,,,,,,,,,28/May/14 09:26;odpeer;5964-v2.patch;https://issues.apache.org/jira/secure/attachment/12647090/5964-v2.patch,31/Aug/13 03:57;dbrosius;5964.txt;https://issues.apache.org/jira/secure/attachment/12600920/5964.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-08-31 03:57:51.086,,,no_permission,,,,,,,,,,,,346372,,,Wed May 28 10:48:12 UTC 2014,,,,,,0|i1nq8v:,346673,,,,,,,,jbellis,jbellis,,,,,,,,,,"31/Aug/13 03:57;dbrosius;generate the version.properties file into ${basedir}/src/resources, and allow it to be copied into classes by the regular build process. Doing this allows eclipse/idea to see it as a resource that needs copying during build as well, and so they don't just delete the file on scrub-build. The IDE should just have src/resources as a src directory.  (Against trunk)","01/Sep/13 04:38;jbellis;+1, works on my machine!",01/Sep/13 05:24;dbrosius;committed to 2.0.1 as commit d4884c76c7b6112e3cd2fa6ecf8bc6e0c8ed67f7,"25/Feb/14 14:49;eric@apache.org;[~dbrosius@apache.org] Works also for me (adding the src/resources).

I'm not sure if 5964.txt is a correct fix to this, as i still encountered the issue on my env.

Shouldn't we fix this in the ant targets? For eclipse, this is simply adding a <classpathentry kind=""src"" path=""src/resources""/>
Tell me if you like me to open a new JIRA and provide a patch for eclipse / idea?
","01/May/14 21:33;alf239;That's interesting, the HEAD at the moment of writing ([c2579b92|https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commit;h=c2579b92bf2e721c099720a27b5d9e56be66e49c]) seems to fail exactly as described in the ticket, with the only difference that the line numbering was changed:
{code}
$ ./cqlsh
Traceback (most recent call last):
  File ""./cqlsh"", line 1855, in <module>
    main(*read_options(sys.argv[1:], os.environ))
  File ""./cqlsh"", line 1841, in main
    ssl=options.ssl)
  File ""./cqlsh"", line 490, in __init__
    self.get_connection_versions()
  File ""./cqlsh"", line 578, in get_connection_versions
    self.cass_ver_tuple = tuple(map(int, vers['build'].split('-', 1)[0].split('.')[:3]))
ValueError: invalid literal for int() with base 10: 'Unknown'
{code}

So it does not appear to be fixed in HEAD or, it seems, 2.1.",01/May/14 22:40;mishail;[~alf239] just regenerate {{version.properties}} file and you'll be fine. {{ant generate-eclipse-files}} will do that.,"28/May/14 09:12;odpeer;[~mishail] Alexey Filippov is right. Regenerating {{version.properties}} creates the {{version.properties}} file in {{$&#123;build.src.resources&#125;/org/apache/cassandra/config}}, however, {{$&#123;build.src.resources&#125;}} is not configured as an Eclipse src directory causing {{version.properties}} not to be copied to {{$&#123;build.classes.main&#125;}}.

",28/May/14 09:26;odpeer;This is a proposed fix to the issue - adding the resources directory as an Eclipse source directory,"28/May/14 10:48;dbrosius;thanks (didn't realize we generated an eclipse .project file :)

 committed cassandra-2.0 as commit 0c96f99e481c4dc70dc8bbe326db10fdbc7fd213",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Netty frame length exception when storing data to Cassandra using binary protocol,CASSANDRA-5981,12667327,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,jsweene2,jsweene2,06/Sep/13 13:52,12/Mar/19 14:04,13/Mar/19 22:29,04/Nov/13 15:13,2.0.3,,,,,,0,,,,,"Using Cassandra 1.2.8, I am running into an issue where when I send a large amount of data using the binary protocol, I get the following netty exception in the Cassandra log file:

{quote}
ERROR 09:08:35,845 Unexpected exception during request
org.jboss.netty.handler.codec.frame.TooLongFrameException: Adjusted frame length exceeds 268435456: 292413714 - discarded
        at org.jboss.netty.handler.codec.frame.LengthFieldBasedFrameDecoder.fail(LengthFieldBasedFrameDecoder.java:441)
        at org.jboss.netty.handler.codec.frame.LengthFieldBasedFrameDecoder.failIfNecessary(LengthFieldBasedFrameDecoder.java:412)
        at org.jboss.netty.handler.codec.frame.LengthFieldBasedFrameDecoder.decode(LengthFieldBasedFrameDecoder.java:372)
        at org.apache.cassandra.transport.Frame$Decoder.decode(Frame.java:181)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:422)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:84)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.processSelectedKeys(AbstractNioWorker.java:472)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:333)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:35)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
{quote}

I am using the Datastax driver and using CQL to execute insert queries. The query that is failing is using atomic batching executing a large number of statements (~55).

Looking into the code a bit, I saw that in the org.apache.cassandra.transport.Frame$Decoder class, the MAX_FRAME_LENGTH is hard coded to 256 mb.

Is this something that should be configurable or is this a hard limit that will prevent batch statements of this size from executing for some reason?","Linux, Java 7",,,,,,,,,,,,,,,,,,,,,,,16/Sep/13 14:17;slebresne;0001-Correctly-catch-frame-too-long-exceptions.txt;https://issues.apache.org/jira/secure/attachment/12603333/0001-Correctly-catch-frame-too-long-exceptions.txt,16/Sep/13 14:17;slebresne;0002-Allow-to-configure-the-max-frame-length.txt;https://issues.apache.org/jira/secure/attachment/12603334/0002-Allow-to-configure-the-max-frame-length.txt,02/Oct/13 09:34;slebresne;5981-v2.txt;https://issues.apache.org/jira/secure/attachment/12606272/5981-v2.txt,28/Oct/13 14:12;slebresne;5981-v3.txt;https://issues.apache.org/jira/secure/attachment/12610561/5981-v3.txt,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-09-06 14:46:25.081,,,no_permission,,,,,,,,,,,,347264,,,Mon Nov 04 15:13:45 UTC 2013,,,,,,0|i1nvqf:,347563,,,,,,,,danielnorberg,danielnorberg,,,1.2.8,,,,,,,"06/Sep/13 14:46;jbellis;CQL is not meant to be a bulk load protocol.  Use sstableloader / AbstractSSTableSimpleWriter for that.

That said, we should turn this into an InvalidRequestException instead of erroring out internally.","06/Sep/13 14:58;slebresne;bq. Is this something that should be configurable or is this a hard limit that will prevent batch statements of this size from executing for some reason?

Sending a message of more than 256mb is a fairly bad idea tbh. Cassandra is just not optimized for that kind of huge request and so you will run into problems and get bad performance if you try to do even if we were lifting the hard coded limit. In fact, the only reason for this limit is to protect the server from OOM if the client sends something clearly wrong.

That being said, I'm not totally opposed to making the limit configurable. But more because I think most people may want to make it lower rather than higher.","06/Sep/13 17:42;jsweene2;Thank you for taking a look into this, I agree 256mb is certainly too large, just something I ran into with using atomic batches. I'll be ensuring we aren't sending data this big going forward, but I think making the limit configurable would be useful. It seems to me to be similar to the thrift max message limit, which can be configured in the cassandra.yaml. Regardless, I appreciate the quick response.","16/Sep/13 14:17;slebresne;bq. That said, we should turn this into an InvalidRequestException instead of erroring out internally

I agree and that's why there is a 'catch (TooLongFrameException)' in the Frame decoding code. But it appears that instead of throwing an exception the normal way (like it does for corrupted frames for instance), Netty instead fires the exceptionCaught callback directly so that catch was bypassed. Anyway, attaching patch that fixes that. The initial code was throwing a ProtocolException but I do agree an InvalidRequestException is probably more appropriate: after all a ProtocolException also close the connection which is not necessary here.  However, this required a few minor changes in Frame as we were not using Netty's LengthFieldBasedFrameDecoder.

As for making the max frame length configurable, I'm not necessarily against the idea. But as said above, more so than people can set it lower than the default if they want a more strict protection against badly behaving clients.  Attaching patch for that too.
",17/Sep/13 22:32;jbellis;[~dln] can you review?,"19/Sep/13 16:14;jbellis;Oops, wrong Daniel.  Meant [~danielnorberg].","26/Sep/13 09:05;danielnorberg;When handling the TooLongFrameException and sending an ErrorMessage reply with the InvalidRequestException without closing the connection, where is the stream id set on the ErrorMessage? I assume that without the stream id set and the connection still open, the client will be unable to infer that the request failed.
","26/Sep/13 10:35;slebresne;Right, you're correct, the patch doesn't preserve the stream id correctly.

However, I have to say that I'm not too sure what's the easiest way to make that work correctly with Netty currently. To be able to use the stream id we've need to be able to start decoding the frame header before LengthFieldBasedFrameDecoder triggers the TooLongFrameException, but I don't know how to do that without knowing if LFBFD is in it's ""discardingTooLongFrame"" mode, and that's not exposed currently. Meaning that the only solutions I see so far are:
# push some feature request to netty so that LengthFieldBasedFrameDecoder exposes it's currently private discardingTooLongFrame field. Don't know if they'll be up for it and how quickly that'd get released.
# recode LengthFieldBasedFramedDecoder ourselves instead of using the netty one. Not the end of the world, it's not like it's a lot of code, but still a bit annoying in principle.

[~danielnorberg] Seeing any other simple solution that I would have missed?","26/Sep/13 12:26;danielnorberg;Right, that's annoying.

I'd be tempted to actually close the connection immediately. It doesn't seem very attractive to read and discard that huge frame, potentially using up a lot of bandwidth doing only that. IMO better to prioritize well behaved clients and let the offending client reconnect.

If you still want to keep the connection open and fail the request nicely I'd probably go for implementing a custom frame decoder.

","26/Sep/13 12:51;slebresne;bq. I'd be tempted to actually close the connection immediately.

That was the initial intent, but now I feel closing the connection in that case is too harsh. If we do allows to configure the max frame length (reasonable if only because some may want to lower it from the relatively high default) then client libraries can't valid frame size on their side and this become a end-user error. And closing the connection on a end-user error feels wrong (especially because it potentially cuts other unrelated streams on that connection).

bq. I'd probably go for implementing a custom frame decoder

Agreed, that's probably the simpler. I'll work that out.
","02/Oct/13 09:34;slebresne;Alright, attaching a v2 (that includes making the ""max frame length configurable"" patch) that rewrite the Frame decoder to handle the frame slightly more manually to allow us to do what we want. This mostly mimick the code of Netty LengthFieldBasedFrameDecoder, though a bit simplified since adapted to just what we need. I'll note that this patch is against the 2.0 branch: I've been able to run the java driver tests with that patch so we should be good but this still is not entirely trivial a change so I'm starting to wonder if it's worth pushing it in 1.2, especially given that the current behavior (having the error logged server side) is not really a big deal.",02/Oct/13 11:52;jbellis;Agreed on the 2.0 call.,08/Oct/13 02:39;jbellis;How does v2 look [~danielnorberg]?,"21/Oct/13 15:49;jbellis;Hmm, timeout on that request.  [~norman], could you review v2?","26/Oct/13 17:17;danielnorberg;Looks to me like it might discard too much data if buffer.readableBytes() > MAX_FRAME_LENGTH. Unless I'm mistaken this problem is also present in the original LengthFieldBasedFrameDecoder though. [~norman], what do you say? Admittedly it's a corner case that's unlikely to be encountered in production.

Are there any tests for the dropping of too large requests?

Apart from this it looks good to me.","28/Oct/13 14:12;slebresne;I believe you're right. I suppose there is no reason to ever get into that case if you don't pick and unreasonably low max frame size, but there's no hurt in being careful so attaching v3 that make sure we don't discard too much.

And no, there isn't really a test in Cassandra for dropping large message because well, we don't really have any test for the native protocol so far. That being said, I do have a test for it in the java driver tests (though i'll need to commit it).","03/Nov/13 20:06;danielnorberg;Looks good, thumbs up.","04/Nov/13 15:13;slebresne;Alright, committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OutOfMemoryError when writing text blobs to a very large number of tables,CASSANDRA-5982,12667358,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,enigmacurry,enigmacurry,06/Sep/13 17:22,12/Mar/19 14:04,13/Mar/19 22:29,17/Sep/13 17:16,1.2.10,2.0.1,,,,,0,,,,,"This test goes outside the norm for Cassandra, creating ~2000 column families, and writing large text blobs to them. 

The process goes like this:

Bring up a 6 node m2.2xlarge cluster on EC2. This instance type has enough memory (34.2GB) so that Cassandra will allocate a full 8GB heap without tuning cassandra-env.sh. However, this instance type only has a single drive, so data and commitlog are comingled. (This test has also been run m1.xlarge instances which have four drives (but lower memory) and has exhibited similar results when assigning one to commitlog and 3 to datafile_directories.)

Use the 'memtable_allocator: HeapAllocator' setting from CASSANDRA-5935.

Create 2000 CFs:
{code}
CREATE KEYSPACE cf_stress WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3}
CREATE COLUMNFAMILY cf_stress.tbl_00000 (id timeuuid PRIMARY KEY, val1 text, val2 text, val3 text ) ;
# repeat for tbl_00001, tbl_00002 ... tbl_02000
{code}

This process of creating tables takes a long time, about 5 hours, but for anyone wanting to create that many tables, presumably they only need to do this once, so this may be acceptable.

Write data:

The test dataset consists of writing 100K, 1M, and 10M documents to these tables:

{code}
INSERT INTO {table_name} (id, val1, val2, val3) VALUES (?, ?, ?, ?)
{code}

With 5 threads doing these inserts across the cluster, indefinitely, randomly choosing a table number 1-2000, the cluster eventually topples over with 'OutOfMemoryError: Java heap space'.

A heap dump analysis indicates that it's mostly memtables:

!2000CF_memtable_mem_usage.png!

Best current theory is that this is commitlog bound and that the memtables cannot flush fast enough due to locking issues. But I'll let [~jbellis] comment more on that.
",,,,,,,,,,,,,,,,,,,,,,,,06/Sep/13 17:22;enigmacurry;2000CF_memtable_mem_usage.png;https://issues.apache.org/jira/secure/attachment/12601852/2000CF_memtable_mem_usage.png,06/Sep/13 17:23;enigmacurry;system.log.gz;https://issues.apache.org/jira/secure/attachment/12601853/system.log.gz,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-09-13 22:31:56.245,,,no_permission,,,,,,,,,,,,347295,,,Tue Sep 17 17:16:35 UTC 2013,,,,,,0|i1nvxb:,347594,1.2.10,,,,,,,yukim,yukim,,,,,,,,,,"11/Sep/13 14:26;enigmacurry;This seems to be the solution:

* Use [~jbellis]' [cfs10k patch|https://github.com/jbellis/cassandra/tree/cfs10k]
* set concurrent_write:8 and memtable_total_space_in_mb:1024 in the yaml.
* Requires 8GB heap.

With these additional settings, I no longer see any OOM errors on any EC2 instance I've tested (m1.xlarge, m2.2xlarge, hs1.8xlarge)","13/Sep/13 22:31;jbellis;This workload presents several challenges to Cassandra:

- MemoryMeter is slow, and we [correctly] limit it to one CFS at a time.  If MemoryMeter is busy measuring a large memtable, we can easily write a lot of data to other CFs (or even the same one in a new memtable).  So we need to pick a more realistic liveRatio default to start with until we can get the first measurements.  (This is not a problem for Ryan's workload here; extremely large blobs have a liveRatio close to 1 anyway.  But it could be a problem for a workload with smaller cells.)
- The MemoryMeter queue is unbounded, and can keep memtables on-heap long after they've been flushed.  We should keep a reference to CFS instead and measure the currently active memtable when the task runs.
- forceFlush acquires Table.switchLock.writeLock temporarily, which means that it will block as long as readLock is held.  In particular, this means it will block for CommitLog.add.  This can actually be worse for PCLE than BCLE, since the former will allow up to 1K unwritten entries on its queue.  If the queue is full of large blobs like the ones in this workload, clearing up space in that queue can take a while.  (The queue itself can also be a significant source of memory consumption!)  We should move forceFlush to a separate executor so it can be truely nonblocking (which will allow MeteredFlusher to impose a moratorium on writes to the other CFs that should be flushed that much faster), and also reduce the CL queue size.  There is no reasonable way to adjust the queue size at runtime, so a configuration setting should be introduced.

IMO, all but the new pre-flush executor changes are reasonable to make in 1.2.10; the last I'd rather keep 2.0-only.","13/Sep/13 22:31;jbellis;NB: even with these changes, testing shows that thousands of CFs fragments writes to the point that compaction and flush are effectively performing random i/o.  Rather than introduce a ""global flush log"" that imposes a moratorium on all CFs when the memtable memory budget is exeeded for this corner case, it is recommended to reduce concurrent_writes instead until flushing is able to keep up with the ingest.",13/Sep/13 22:36;jbellis;Pushed branches to https://github.com/jbellis/cassandra/tree/5982-1.2 and https://github.com/jbellis/cassandra/tree/5982-2.0,17/Sep/13 15:32;jjordan;We should add https://github.com/jbellis/cassandra/commit/2e22cf23ec4e18cd99b69bb3d419931c55e3ba93 to tpstats also.,"17/Sep/13 16:00;yukim;5982-1.2 is looking good to me (nit: unused import sun.security.provider.Sun in Memtable).

Approach in 5982-2.0 works, though changes to Future<Future> breaks some of the places like CommitLogReplayer that waits on only the first Future. So I think we need to fix those too.
Also, ColumnFamilyStore#reload needs to acquire write lock, so we have to switch switchMemtable to switchMemtableInternal there.","17/Sep/13 16:25;yukim;In relation to CASSANDRA-5605, it may be better to change preExecutor -> flushwriter -> postExecuter chain with something different that correctly handles errors thrown.","17/Sep/13 16:58;jbellis;bq. We should add https://github.com/jbellis/cassandra/commit/2e22cf23ec4e18cd99b69bb3d419931c55e3ba93 to tpstats also.

I'll turn Memtable.meterExecutor into a JMXEnabledTPE, which is what shows up in tpstats, but there isn't really a good way to shoehorn the commitlog executor in there.","17/Sep/13 17:16;jbellis;bq. 5982-1.2 is looking good to me (nit: unused import sun.security.provider.Sun in Memtable).

Committed w/ nit fixed.","17/Sep/13 17:16;jbellis;bq. it may be better to change preExecutor -> flushwriter -> postExecuter chain with something different that correctly handles errors thrown.

I'll just drop this for now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to prepare statement with batch and delete from collection,CASSANDRA-6607,12690057,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,jan.chochol,jan.chochol,21/Jan/14 16:23,12/Mar/19 14:03,13/Mar/19 22:29,27/Jan/14 17:48,1.2.14,,,,,,1,,,,,"It is not possible to prepare statement with batch containing delete operation on one item of collection, e.g.:
{noformat}
BEGIN BATCH
DELETE colection[?] FROM table WHERE key = ?;
APPLY BATCH
{noformat}
Result of preparing such statement is:
{noformat}
java.lang.ArrayStoreException: org.apache.cassandra.cql3.ColumnSpecification
{noformat}
With stacktrace:
{noformat}
ERROR 16:26:36,816 Unexpected exception during request
java.lang.ArrayStoreException: org.apache.cassandra.cql3.ColumnSpecification
	at org.apache.cassandra.cql3.AbstractMarker.collectMarkerSpecification(AbstractMarker.java:40)
	at org.apache.cassandra.cql3.Operation.collectMarkerSpecification(Operation.java:75)
	at org.apache.cassandra.cql3.statements.DeleteStatement.prepare(DeleteStatement.java:160)
	at org.apache.cassandra.cql3.statements.BatchStatement.prepare(BatchStatement.java:125)
	at org.apache.cassandra.cql3.statements.BatchStatement.prepare(BatchStatement.java:133)
	at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:273)
	at org.apache.cassandra.cql3.QueryProcessor.prepare(QueryProcessor.java:201)
	at org.apache.cassandra.transport.messages.PrepareMessage.execute(PrepareMessage.java:77)
	at org.apache.cassandra.transport.Message$Dispatcher.messageReceived(Message.java:287)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:43)
	at org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:67)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
{noformat}

This fix seems to help:
{noformat}
diff --git a/src/java/org/apache/cassandra/cql3/statements/BatchStatement.java b/src/java/org/apache/cassandra/cql3/statements/BatchStatement.java
index f93eb63..74c0a45 100644
--- a/src/java/org/apache/cassandra/cql3/statements/BatchStatement.java
+++ b/src/java/org/apache/cassandra/cql3/statements/BatchStatement.java
@@ -129,7 +129,7 @@ public class BatchStatement extends ModificationStatement
 
     public ParsedStatement.Prepared prepare() throws InvalidRequestException
     {
-        CFDefinition.Name[] boundNames = new CFDefinition.Name[getBoundsTerms()];
+        ColumnSpecification[] boundNames = new ColumnSpecification[getBoundsTerms()];
         return prepare(boundNames);
     }
{noformat}

It is probably corrected in Cassandra 2.0 by commit {{e431fb722f80d8957a0a7fd2ecf80333e9275c53}} (CASSANDRA-5443).
We  are facing this issue with Cassandra version 1.2.11.
Would it be possible to fix this issue in branch 1.2?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-01-27 17:48:54.798,,,no_permission,,,,,,,,,,,,369014,,,Tue Jan 28 08:11:16 UTC 2014,,,,,,0|i1rlpz:,369319,,,,,,,,,,,,,,,,,,,"27/Jan/14 17:48;slebresne;Oh, right, that's definitively an oversight. Fix committed to 1.2. Thanks for the report.",28/Jan/14 08:11;jan.chochol;Thanks for resolution.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException during nodetool netstats,CASSANDRA-6577,12688424,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,JoshuaMcKenzie,shaochuan,shaochuan,12/Jan/14 03:53,12/Mar/19 14:02,13/Mar/19 22:29,22/Feb/14 14:18,2.0.6,,,Tool/nodetool,,,0,decommission,nodetool,,,"The node is leaving and I wanted to check its netstats, but it raises ConcurrentModificationException.

{code}
[ubuntu@ip-10-4-202-48 :~]# /mnt/cassandra_latest/bin/nodetool netstats
Mode: LEAVING
Exception in thread ""main"" java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)
	at java.util.HashMap$ValueIterator.next(HashMap.java:954)
	at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48)
	at com.google.common.collect.Iterators.addAll(Iterators.java:357)
	at com.google.common.collect.Lists.newArrayList(Lists.java:146)
	at com.google.common.collect.Lists.newArrayList(Lists.java:128)
	at org.apache.cassandra.streaming.management.SessionInfoCompositeData.toArrayOfCompositeData(SessionInfoCompositeData.java:161)
	at org.apache.cassandra.streaming.management.SessionInfoCompositeData.toCompositeData(SessionInfoCompositeData.java:98)
	at org.apache.cassandra.streaming.management.StreamStateCompositeData$1.apply(StreamStateCompositeData.java:82)
	at org.apache.cassandra.streaming.management.StreamStateCompositeData$1.apply(StreamStateCompositeData.java:79)
	at com.google.common.collect.Iterators$8.transform(Iterators.java:794)
	at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48)
	at com.google.common.collect.Iterators.addAll(Iterators.java:357)
	at com.google.common.collect.Lists.newArrayList(Lists.java:146)
	at com.google.common.collect.Lists.newArrayList(Lists.java:128)
	at org.apache.cassandra.streaming.management.StreamStateCompositeData.toCompositeData(StreamStateCompositeData.java:78)
	at org.apache.cassandra.streaming.StreamManager$1.apply(StreamManager.java:87)
	at org.apache.cassandra.streaming.StreamManager$1.apply(StreamManager.java:84)
	at com.google.common.collect.Iterators$8.transform(Iterators.java:794)
	at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48)
	at com.google.common.collect.Iterators.addAll(Iterators.java:357)
	at com.google.common.collect.Sets.newHashSet(Sets.java:238)
	at com.google.common.collect.Sets.newHashSet(Sets.java:218)
	at org.apache.cassandra.streaming.StreamManager.getCurrentStreams(StreamManager.java:83)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
	at sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1464)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
	at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:657)
	at sun.reflect.GeneratedMethodAccessor54.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at sun.rmi.transport.Transport$1.run(Transport.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:556)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:811)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:670)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
{code}

The gossip info is as follows:
{code}
/10.215.114.239
  SCHEMA:e2a4b7b2-df50-3cc4-97f6-2ce7fb77982b
  NET_VERSION:7
  LOAD:6.42655250606E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1305722878370288637
  HOST_ID:d13374d8-e4ae-466d-ad5a-44229a2fa190
  DC:pagedb-frontend
  SEVERITY:0.0
/10.122.218.80
  SCHEMA:548308e6-bd1c-388f-ad1e-5ecf39f994fd
  NET_VERSION:7
  LOAD:2.71405175303E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1796000685025988745
  HOST_ID:7746a875-1e53-4ebe-aef6-ad59fadd6ea7
  DC:pagedb-frontend
  SEVERITY:0.0
/10.178.13.230
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.34997966591E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:LEAVING,-1646882803236172485
  HOST_ID:160d83bf-4cc0-4872-aec6-7908446eccbf
  DC:pagedb-backend
  SEVERITY:1.5584415197372437
/10.71.141.179
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.21492186385E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1277850891915823266
  HOST_ID:baa81888-6417-4e7c-8d7b-2bb79c38ca40
  DC:pagedb-frontend
  SEVERITY:0.0
/10.251.114.31
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.95280061394E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1b
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1866613386904756042
  HOST_ID:a71e7037-c4f8-4ff9-864d-e83b6d3037d7
  DC:pagedb-frontend
  SEVERITY:0.0
/10.95.128.214
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.90628204861E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1e
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1896996972976095280
  HOST_ID:3e5e6e75-56cb-4664-abf4-de8c2801bd5d
  DC:pagedb-backend
  SEVERITY:0.0
/10.228.26.240
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.46469437383E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1359255731430490566
  HOST_ID:8df816c9-7875-4106-aeda-b372b9f1fdc9
  DC:pagedb-frontend
  SEVERITY:1.5037593841552734
/10.185.67.195
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:4.6531989096E10
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1295072457639851651
  HOST_ID:37fe7cc7-3481-48b3-96ff-c92df17a4132
  DC:pagedb-frontend
  SEVERITY:0.0
/10.220.195.198
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:7.8265401144E10
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1514821029440891529
  HOST_ID:0bd7f3ab-d347-4c59-9f1a-1b7104e34a6b
  DC:pagedb-frontend
  SEVERITY:0.0
/10.62.39.130
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.7020827919E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1089815038805941976
  HOST_ID:13b12ee3-36a4-462c-9013-ccc1b83a70ff
  DC:pagedb-frontend
  SEVERITY:0.9411764144897461
/10.137.11.197
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.17329728527E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:LEAVING,-1708596056219631840
  HOST_ID:76d339f4-ac4a-4f95-bada-1ec17f67f4e3
  DC:pagedb-backend
  SEVERITY:0.5555555820465088
/10.87.145.85
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:5.78714900637E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-2223601356751123985
  HOST_ID:bdad1b28-3fb7-4788-8f34-01caace03ca9
  DC:pagedb-frontend
  SEVERITY:0.0
/10.97.135.36
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:9.3081789914E10
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1146411778648768253
  HOST_ID:40fb13d6-0803-45cc-9b4f-45b3ad7194fa
  DC:pagedb-frontend
  SEVERITY:0.0
/10.95.128.6
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:3.56500409995E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1e
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1025262993714352739
  HOST_ID:80b583a5-8f7d-4fee-91db-b948c090d055
  DC:pagedb-backend
  SEVERITY:0.7518796920776367
/10.154.136.39
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.70485777643E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  HOST_ID:0e391fea-e4e9-4a46-b9af-87948459876c
  STATUS:LEAVING,-2690692580263318876
  DC:pagedb-backend
  SEVERITY:0.0
/10.185.9.84
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.87054930947E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:LEAVING,-1468213189211385280
  HOST_ID:080241a9-eadd-48b4-8f94-efbe35bfd6e1
  DC:pagedb-backend
  SEVERITY:0.2557544708251953
/10.251.114.15
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.86231038729E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1b
  RPC_ADDRESS:0.0.0.0
  HOST_ID:99e3e672-8410-4850-8257-edc7b814a776
  STATUS:NORMAL,-1117031336072704821
  DC:pagedb-frontend
  SEVERITY:0.0
/10.97.135.32
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:3.1249908096E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1176341277720804881
  HOST_ID:e9ac8ad9-c3cc-44ff-81e7-af28a4d5f576
  DC:pagedb-backend
  SEVERITY:0.0
/10.99.144.60
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:8.9296442414E10
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1695890443918261320
  HOST_ID:01b323c1-3f0e-40de-8ef7-3e1d33b391c5
  DC:pagedb-frontend
  SEVERITY:0.0
/10.121.13.216
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:3.70269351421E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1081159627867819835
  HOST_ID:05f06b6f-ddb7-42a9-9e56-8efd41cf1545
  DC:pagedb-frontend
  SEVERITY:0.0
/10.99.144.97
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.95197366549E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1188304416367949186
  HOST_ID:7a84853a-2c0a-448c-84e2-cceaa98c7523
  DC:pagedb-backend
  SEVERITY:0.26178011298179626
/10.95.132.5
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:9.8325409989E10
  RELEASE_VERSION:2.0.1
  RACK:us-east-1e
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1168449340133292844
  HOST_ID:6480a3a2-cbbe-44f4-b67b-7310f885b307
  DC:pagedb-frontend
  SEVERITY:0.0
/10.178.2.177
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:3.8547129498E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1337943409437095727
  HOST_ID:aa7fd551-8a47-4e59-869e-ae0afaadf27e
  DC:pagedb-frontend
  SEVERITY:0.0
/10.201.206.166
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.02276920825E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-2302424866842171501
  HOST_ID:f9651587-8f34-4dbc-af5c-64f19bc85ad6
  DC:pagedb-frontend
  SEVERITY:0.0
/10.93.6.87
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:5.55645040614E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1767037199861003446
  HOST_ID:2ca596b1-fa0a-40a8-ace1-bc7b21e82c45
  DC:pagedb-frontend
  SEVERITY:1.0230178833007812
/10.123.74.248
  SCHEMA:b2c3e6ec-3b3f-3ad9-baef-e9e311a24eb9
  NET_VERSION:7
  LOAD:3.09054824961E11
  REMOVAL_COORDINATOR:REMOVER,76d339f4-ac4a-4f95-bada-1ec17f67f4e3
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  STATUS:removed,94daf57d-838d-4285-8048-e60f97629dba,1389650189822
  HOST_ID:94daf57d-838d-4285-8048-e60f97629dba
  DC:pagedb-backend
  SEVERITY:0.2493765652179718
/10.71.141.12
  SCHEMA:26cf5f29-f7f9-321d-b5c4-ac9314d90653
  NET_VERSION:7
  LOAD:1.91108857556E11
  REMOVAL_COORDINATOR:REMOVER,c7a0edfd-629e-47c3-b79c-32a9c2c98801
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:removed,4b9f63e5-8561-4e59-8011-e11fb3e8d627,1389562451886
  HOST_ID:4b9f63e5-8561-4e59-8011-e11fb3e8d627
  DC:pagedb-frontend
  SEVERITY:0.0
/10.185.0.80
  SCHEMA:039db936-c2e7-3f53-9c94-efdf2b861d53
  NET_VERSION:7
  LOAD:2.22049506986E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:removed,9885866e-af4d-4129-ad33-d465c8a7cd58,1389732020477
  HOST_ID:9885866e-af4d-4129-ad33-d465c8a7cd58
  DC:pagedb-backend
  SEVERITY:2.7707808017730713
/10.185.47.50
  SCHEMA:b13a987f-adbe-3c19-afc9-419e595d9002
  NET_VERSION:7
  LOAD:2.19790952307E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  HOST_ID:73c27c92-1f3e-4628-9c91-a1e0a03a9e21
  STATUS:removed,73c27c92-1f3e-4628-9c91-a1e0a03a9e21,1389731586723
  DC:pagedb-backend
  SEVERITY:6.6137566566467285
/10.93.49.164
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.03359005906E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1102475930390387446
  HOST_ID:19f401cb-b418-4ce3-b0e0-ea1530ab2121
  DC:pagedb-frontend
  SEVERITY:0.0
/10.206.97.184
  SCHEMA:ede9498d-befd-3153-87ee-2ea329001466
  NET_VERSION:7
  LOAD:2.34212215287E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  HOST_ID:8d3d9e79-5a72-4561-99f0-0817cd6ad6cc
  STATUS:LEAVING,-1397201050646007911
  DC:pagedb-backend
  SEVERITY:2.236421823501587
/10.71.141.42
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:6.0594679857E10
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1008058669507434673
  HOST_ID:2113b135-8566-4ed9-b74b-5e302d240c42
  DC:pagedb-frontend
  SEVERITY:0.0
/10.154.148.5
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:4.91232885735E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1261154574760184756
  HOST_ID:86642aef-f625-4561-be7b-ab8f58066294
  DC:pagedb-frontend
  SEVERITY:0.7672634720802307
/10.183.153.179
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:3.17986161259E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1e
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1486604450347152783
  HOST_ID:5988d85d-663c-4597-9422-7f5bc195ec5b
  DC:pagedb-frontend
  SEVERITY:0.0
/10.97.131.247
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.43618302855E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1087382214101550882
  HOST_ID:811a2966-b6f0-41fd-ba29-fc980763a69c
  DC:pagedb-backend
  SEVERITY:0.0
/10.99.144.54
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:8.9936707476E10
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-3327961952466683302
  HOST_ID:675e8de7-641f-4914-beeb-fad29942381f
  DC:pagedb-frontend
  SEVERITY:0.0
/10.210.91.83
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:3.19028851441E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1375725999837634215
  HOST_ID:b0726b39-1587-4443-853e-6e668658b1cc
  DC:pagedb-frontend
  SEVERITY:17.39130401611328
/10.71.141.158
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.11308980882E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1306761067597910068
  HOST_ID:31c7690b-30fa-455c-9f69-e98f11d4c235
  DC:pagedb-backend
  SEVERITY:0.0
/10.38.169.32
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.3296808983E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:LEAVING,-137637605179813169
  HOST_ID:67ec4857-c867-46c8-9c62-778fd26360eb
  DC:pagedb-backend
  SEVERITY:1.671309232711792
/10.87.94.230
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:3.52091238152E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  STATUS:LEAVING,-1519665921822534263
  HOST_ID:604b0268-187a-4a5f-a51e-b94608111825
  DC:pagedb-backend
  SEVERITY:0.27397260069847107
/10.99.144.91
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.96583444544E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-3103769032004455685
  HOST_ID:010613b8-9d0b-487d-bdd1-64e80ddc60e6
  DC:pagedb-backend
  SEVERITY:1.0204081535339355
/10.71.141.130
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.45733317443E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1099965132011727160
  HOST_ID:3c30b5fb-bf8c-4169-a60f-6ffad2c28598
  DC:pagedb-frontend
  SEVERITY:0.0
/10.99.146.244
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.6041541835E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1404826323347485057
  HOST_ID:6a96c2e0-6376-461d-bac5-0dcbe3cd1eb5
  DC:pagedb-backend
  SEVERITY:0.0
/10.95.129.124
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.61026008599E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1e
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-2498420976414553971
  HOST_ID:2a987ea1-4037-4b9e-a403-27a6d7995621
  DC:pagedb-frontend
  SEVERITY:0.0
/10.122.50.31
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.18914643786E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  HOST_ID:b9e40521-bf2e-4833-8f6a-a8e3297df042
  STATUS:NORMAL,-1277185879546496035
  DC:pagedb-backend
  SEVERITY:0.0
/10.4.197.53
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:3.15526897917E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:LEAVING,-1388643925813439514
  HOST_ID:9f76061b-43a7-432b-9e6d-612a628534cf
  DC:pagedb-backend
  SEVERITY:0.7537688612937927
/10.44.183.111
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.10331330719E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  STATUS:LEAVING,-283520605333269313
  HOST_ID:1e0e3870-5026-452b-8050-dad7fcd5bd88
  DC:pagedb-backend
  SEVERITY:7.416880130767822
/10.238.138.32
  SCHEMA:b13a987f-adbe-3c19-afc9-419e595d9002
  NET_VERSION:7
  REMOVAL_COORDINATOR:REMOVER,3c30b5fb-bf8c-4169-a60f-6ffad2c28598
  LOAD:1.92772802904E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  HOST_ID:e225c748-ffd8-4905-91bf-4e745b26fa44
  STATUS:removed,e225c748-ffd8-4905-91bf-4e745b26fa44,1389731792774
  DC:pagedb-backend
  SEVERITY:0.0
/10.95.129.103
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.50386165884E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1e
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1389777754931898083
  HOST_ID:18031026-e20c-40eb-8bb3-d771b641fb26
  DC:pagedb-frontend
  SEVERITY:0.0
/10.125.10.14
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:7.2903902818E10
  RELEASE_VERSION:2.0.1
  RACK:us-east-1a
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1822845329295703853
  HOST_ID:5764492f-31f8-4c3f-8bec-d8a2c1af4b4d
  DC:pagedb-frontend
  SEVERITY:0.0
/10.4.202.48
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:3.05065515518E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  HOST_ID:76f98456-2029-4bb6-b3de-e08157220fc9
  STATUS:LEAVING,-2184837513740789816
  DC:pagedb-backend
  SEVERITY:28.76344108581543
/10.95.132.44
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.97019107429E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1e
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-243162178282959980
  HOST_ID:c7a0edfd-629e-47c3-b79c-32a9c2c98801
  DC:pagedb-backend
  SEVERITY:0.5076141953468323
/10.96.31.80
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:2.29408976299E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1c
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1236217677726412210
  HOST_ID:fa3ea99e-f9b6-4f83-9234-21d8059e6eb4
  DC:pagedb-frontend
  SEVERITY:0.0
/10.40.245.193
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:1.27656537169E11
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1237220198988914368
  HOST_ID:d2383010-fd55-4364-8868-5dc8a937778f
  DC:pagedb-frontend
  SEVERITY:0.0
/10.99.144.75
  SCHEMA:f9832569-f987-3545-8c36-196342655a7f
  NET_VERSION:7
  LOAD:8.2445952994E10
  RELEASE_VERSION:2.0.1
  RACK:us-east-1d
  RPC_ADDRESS:0.0.0.0
  STATUS:NORMAL,-1022810197871922800
  HOST_ID:c085c62d-74ea-45e5-9ed6-6a26a72ec3bb
  DC:pagedb-frontend
  SEVERITY:0.0
{code}","AWS EC2 machines, 
java version ""1.7.0_45""
Java(TM) SE Runtime Environment (build 1.7.0_45-b18)
Java HotSpot(TM) 64-Bit Server VM (build 24.45-b08, mixed mode)
Each node has 32 vnodes.",,,,,,,,,,,,,,,,,,,,,,,20/Feb/14 19:59;JoshuaMcKenzie;6577_SessionInfo.patch;https://issues.apache.org/jira/secure/attachment/12630138/6577_SessionInfo.patch,20/Feb/14 21:13;JoshuaMcKenzie;6577_SessionInfo_v2.patch;https://issues.apache.org/jira/secure/attachment/12630152/6577_SessionInfo_v2.patch,20/Feb/14 22:22;JoshuaMcKenzie;6577_SessionInfo_v3.patch;https://issues.apache.org/jira/secure/attachment/12630169/6577_SessionInfo_v3.patch,20/Feb/14 19:59;JoshuaMcKenzie;6577_TokenMetadata.patch;https://issues.apache.org/jira/secure/attachment/12630139/6577_TokenMetadata.patch,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2014-02-19 18:55:09.738,,,no_permission,,,,,,,,,,,,367443,,,Sat Feb 22 14:18:26 UTC 2014,,,,,,0|i1rc3r:,367752,2.0.1,,,,,,,thobbs,thobbs,,,,,,,,,,"12/Jan/14 05:49;shaochuan;Running nodetool removenode force failed.
{code}
ubuntu@ip-10-71-141-158:~$ /mnt/cassandra_latest/bin/nodetool removenode force
RemovalStatus: Removing token (-8408964321996035122). Waiting for replication confirmation from [/10.97.135.32].
Exception in thread ""main"" java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)
	at java.util.HashMap$KeyIterator.next(HashMap.java:960)
	at org.apache.cassandra.service.StorageService.forceRemoveCompletion(StorageService.java:3041)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1487)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:848)
	at sun.reflect.GeneratedMethodAccessor37.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at sun.rmi.transport.Transport$1.run(Transport.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:556)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:811)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:670)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{code}","19/Feb/14 18:55;JoshuaMcKenzie;Regarding the netstats exception:  the SessionInfo objects being operated on inside the creation of CompositeData aren't thread-safe, so the following use presents an opportunity for the Exception you're seeing.

Declaration:
{code:title=SessionInfo.java|borderstyle=solid}
        this.receivingFiles = new HashMap<>();
        this.sendingFiles = new HashMap<>();
{code}

These are iterated across creating the various CompositeData types requested, however updateProgress within SessionInfo would cause invalidation of any open iterators on the collection if any are active.

Edit: ConcurrentHashMaps provide iterator thread-safety with weak guarantee on the contents of the HashMap.  I'll look into the code a bit further and see if using this structure would satisfy the various users of these functions as that should prevent this race condition.

","20/Feb/14 17:50;JoshuaMcKenzie;As for the StorageService.java exception - there's no protection around the HashSet of InetAddress (leavingEndpoints) in TokenMetadata.  TokenMetadata.getLeavingEndpoints() returns an iterator to the underlying collection that could be invalidated by either an addLeavingEndpoint(), removeEndpoint(), or updateNormalTokens() call.  The cleanest way around this looks to be putting the data into a ConcurrentSkipListSet, similar to how we do in Gossiper.java, to provide iterator thread-safety to external users.",20/Feb/14 20:00;JoshuaMcKenzie;Attaching patch for each of the issues since they're logically independent.,"20/Feb/14 20:49;thobbs;[~JoshuaMcKenzie] it looks like the {{TokenMetadata.getLeavingEndpoints()}} was fixed by CASSANDRA-6103.  Access to leavingEndpoints is now locked, and getLeavingEndpoints returns a new immutable copy.

For the SessionInfo patch, your fix looks good.  {{updateProgress()}} was synchronized, but obviously that didn't protect readers.  Now that you're using a ConcurrentHashMap, we should be able to remove that.",20/Feb/14 21:13;JoshuaMcKenzie;Good catch on 6103 - I was working off 2.0.1 where the stacks were produced and didn't think to check that the issue might have been duplicated - that's my mistake.  Attaching revised SessionInfo patch w/synchronized removed.,20/Feb/14 21:34;thobbs;+1 with the exception that your patch accidentally changes the mode on a file in the test/ dir,"20/Feb/14 22:03;JoshuaMcKenzie;Looks like cygwin did that for both the test file and the .java file too - fixed on both.

Edit: Fixed the file name on the patch for consistency.",21/Feb/14 22:32;JoshuaMcKenzie;6577_SessionInfo_v3.patch,22/Feb/14 14:18;jbellis;committed; thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IN on the last clustering columns + ORDER BY DESC yield no results,CASSANDRA-6701,12695036,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,13/Feb/14 16:41,12/Mar/19 14:02,13/Mar/19 22:29,14/Feb/14 12:51,1.2.16,2.0.6,,,,,0,,,,,"That's not a very common mix but well, the following return no results which is obviously bogus:
{noformat}
CREATE TABLE test (k int, c1 int, c2 int, PRIMARY KEY (k, c1, c2));
INSERT INTO test(k, c1, c2) VALUES (0, 0, 0);
INSERT INTO test(k, c1, c2) VALUES (0, 0, 1);
INSERT INTO test(k, c1, c2) VALUES (0, 0, 2);
SELECT * FROM test WHERE k=0 AND c1 = 0 AND c2 IN (2, 0) ORDER BY c1 DESC
{noformat}
Note: it's pretty useless to order on a column which has an equal restriction, and that's probably why nobody ran into this yet, but that's really just due to a minor typo so there is no reason not to fix.
",,,,,,,,,,,,,,,,,,,,,,,,13/Feb/14 16:41;slebresne;6701.txt;https://issues.apache.org/jira/secure/attachment/12628784/6701.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-13 17:59:00.485,,,no_permission,,,,,,,,,,,,373544,,,Fri Feb 14 12:51:07 UTC 2014,,,,,,0|i1sdfj:,373844,,,,,,,,thobbs,thobbs,,,,,,,,,,"13/Feb/14 16:41;slebresne;Attaching trivial patch. I've pushed the example above as a dtest too.

Btw, the patch kind of fix another small issue in that in the example above, results ended-up not ordered following the IN order, which we do it otherwise (since there is no ORDER BY on c1, only on c2).",13/Feb/14 17:59;thobbs;+1 on 6701.txt and your rebased version on CASSANDRA-4911,"14/Feb/14 12:51;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DROP TYPE complains no keyspace is active (when keyspace is active),CASSANDRA-6583,12688836,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,rhatch,rhatch,14/Jan/14 20:28,12/Mar/19 14:02,13/Mar/19 22:29,16/Jan/14 10:20,2.1 beta1,,,Legacy/CQL,,,0,,,,,"ALTER TYPE DROP complains that there is no active keyspace (even when the session has an active keyspace). The drop works when the prefix is provided.

steps to reproduce:
{noformat}
ccm create test_cluster
ccm populate -n 1
ccm start
ccm node1 cqlsh
Connected to test_cluster at 127.0.0.1:9160.
[cqlsh 4.1.1 | Cassandra 2.1-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.39.0]
Use HELP for help.
cqlsh> create keyspace user_type_dropping with replication = {'class':'SimpleStrategy', 'replication_factor':1} ;
cqlsh> use user_type_dropping ;
cqlsh:user_type_dropping>               CREATE TYPE simple_type (
                      ...               user_number int
                      ...               );
cqlsh:user_type_dropping> DROP TYPE simple_type;
Bad Request: You have not set a keyspace for this session
cqlsh:user_type_dropping> DROP TYPE user_type_dropping.simple_type;
{noformat}
","trunk-4910ce8
java version ""1.7.0_45""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-01-16 10:20:29.325,,,no_permission,,,,,,,,,,,,367803,,,Fri Jan 17 19:57:06 UTC 2014,,,,,,0|i1rean:,368110,2.1 rc3,,,,,,,,,,,2.1 rc3,,,,,,,"16/Jan/14 10:20;slebresne;Same problem than for CASSANDRA-6582, the keyspace preparation was missing for DropType. Ninja-committed the trivial fix here too (commit b8070ff). Sorry for not having checked those after CASSANDRA-6438.",17/Jan/14 19:57;rhatch;my dtest is successfully renaming types without the prefix now. Thanks.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException(s) on startup if StorageServiceShutdownHook fails,CASSANDRA-6669,12693764,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,acampeau,acampeau,06/Feb/14 23:02,12/Mar/19 14:02,13/Mar/19 22:29,07/Feb/14 00:48,2.0.4,,,,,,0,,,,,"For example, when starting a newly installed Cassandra node for the first time, if the seed is down (and the current node is not a seed) a RuntimeException gets thrown by the Gossiper with ""Unable to gossip with any seeds"", and Cassandra subsequently shuts down.

The issue is with the StorageServiceShutdownHook thread which, if any exception is thrown as it's executing, will miss the remaining steps in the shutdown process.

Here's a log fragment.

INFO [MemoryMeter:1] 2014-01-29 09:07:04,126 Memtable.java (line 451) CFS(Keyspace='system', ColumnFamily='schema_columnfamilies') liveRatio is 2.8177450396801733 (just-counted was 2.5754099604780003).  calculation took 459ms for 26900 cells
 INFO [ScheduledTasks:1] 2014-01-29 09:07:04,168 GCInspector.java (line 116) GC for ParNew: 293 ms for 1 collections, 318930288 used; max is 6215958528
 INFO [MemoryMeter:1] 2014-01-29 09:07:04,246 Memtable.java (line 451) CFS(Keyspace='system', ColumnFamily='schema_columns') liveRatio is 2.8940509915872097 (just-counted was 2.6458758011978563).  calculation took 119ms for 23968 cells
 INFO [MemoryMeter:1] 2014-01-29 09:07:07,005 Memtable.java (line 451) CFS(Keyspace='system', ColumnFamily='schema_columnfamilies') liveRatio is 2.6841006146942297 (just-counted was 2.550456189708286).  calculation took 297ms for 54325 cells
 INFO [ScheduledTasks:1] 2014-01-29 09:07:07,326 GCInspector.java (line 116) GC for ParNew: 289 ms for 1 collections, 291162792 used; max is 6215958528
 INFO [MemoryMeter:1] 2014-01-29 09:07:07,607 Memtable.java (line 451) CFS(Keyspace='system', ColumnFamily='schema_columns') liveRatio is 2.770798679512837 (just-counted was 2.6475463674384643).  calculation took 601ms for 47908 cells
 INFO [main] 2014-01-29 09:07:09,914 StorageService.java (line 490) Cassandra version: 2.0.4-SNAPSHOT
 INFO [main] 2014-01-29 09:07:09,914 StorageService.java (line 491) Thrift API version: 19.39.0
 INFO [main] 2014-01-29 09:07:09,918 StorageService.java (line 492) CQL supported versions: 2.0.0,3.1.3 (default: 3.1.3)
 INFO [main] 2014-01-29 09:07:09,933 StorageService.java (line 515) Loading persisted ring state
 INFO [main] 2014-01-29 09:07:09,974 MessagingService.java (line 458) Starting Messaging Service on port 7000
ERROR [main] 2014-01-29 09:07:40,993 CassandraDaemon.java (line 473) Exception encountered during startup
java.lang.RuntimeException: Unable to gossip with any seeds
	at org.apache.cassandra.gms.Gossiper.doShadowRound(Gossiper.java:1160)
	at org.apache.cassandra.service.StorageService.checkForEndpointCollision(StorageService.java:426)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:618)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:586)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:485)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:341)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:456)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:499)
ERROR [StorageServiceShutdownHook] 2014-01-29 09:07:41,150 CassandraDaemon.java (line 188) Exception in thread Thread[StorageServiceShutdownHook,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.stopNativeTransport(StorageService.java:349)
	at org.apache.cassandra.service.StorageService.shutdownClientServers(StorageService.java:364)
	at org.apache.cassandra.service.StorageService.access$000(StorageService.java:97)
	at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:551)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.lang.Thread.run(Thread.java:744)
",RHEL 6.5 (likely not hardware specific),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-02-07 00:48:52.875,,,no_permission,,,,,,,,,,,,372273,,,Fri Feb 07 00:48:52 UTC 2014,,,,,,0|i1s5nj:,372577,,,,,,,,,,,,,,,,,,,"07/Feb/14 00:48;brandon.williams;I fixed this, twice, most recently in 723fcd1936ad",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mean cells per sstable is calculated incorrectly,CASSANDRA-6667,12693615,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,06/Feb/14 18:33,12/Mar/19 14:02,13/Mar/19 22:29,07/Feb/14 16:19,1.2.16,2.0.6,,,,,0,,,,,"We currently take the average of the mean for each partition, rather than correctly weighting by cell count.  This affects hint paging as well as index selectivity calculation.",,,,,,,,,,,,,,,,,,,,,,,,06/Feb/14 21:02;jbellis;6667-v2.txt;https://issues.apache.org/jira/secure/attachment/12627468/6667-v2.txt,07/Feb/14 08:06;krummas;6667-v3.txt;https://issues.apache.org/jira/secure/attachment/12627572/6667-v3.txt,06/Feb/14 18:35;jbellis;6667.txt;https://issues.apache.org/jira/secure/attachment/12627390/6667.txt,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-02-07 08:06:09.074,,,no_permission,,,,,,,,,,,,372200,,,Fri Feb 07 16:19:38 UTC 2014,,,,,,0|i1s57z:,372505,,,,,,,,krummas,krummas,,,,,,,,,,06/Feb/14 18:36;jbellis;(Tagged [~krummas] for review.),"06/Feb/14 21:02;jbellis;v2 changes count to long (pointed out by Paulo Gaspar, who brought this to my attention initially)","07/Feb/14 08:06;krummas;looks like we have the same bug in getMeanRowSize()

(seems v2 got committed in trunk with CASSANDRA-6671)","07/Feb/14 14:37;jbellis;Well, they're computing different things, so meanRowSize looks correct to me.

Will fix the CHANGES record.","07/Feb/14 16:19;jbellis;... you're right, I was misreading the CFM code.  Committed (after reverting the accidental trunk commit to keep things clean).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CQL: ""drop table if exists"" throws exception when table does not exist",CASSANDRA-6687,12694380,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mishail,thebrenthaines,thebrenthaines,10/Feb/14 23:01,12/Mar/19 14:02,13/Mar/19 22:29,11/Feb/14 06:53,2.0.6,,,,,,0,2.0.5,,,,"{code}
MacBook-Bro-6:~ brenthaines$ cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 4.1.1 | Cassandra 2.0.5 | CQL spec 3.1.1 | Thrift protocol 19.39.0]
Use HELP for help.
cqlsh> use apps;
cqlsh:apps> describe table brands;

Column family 'brands' not found
cqlsh:apps> drop table if exists brands;
Bad Request: unconfigured columnfamily brands
cqlsh:apps> 
{code}","Mac OSX Mavericks
Cassandra 2.0.5",,,,,,,,,,,,,,,,,,,,,,,11/Feb/14 01:05;mishail;CASSANDRA-2.0-6687.patch;https://issues.apache.org/jira/secure/attachment/12628128/CASSANDRA-2.0-6687.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-11 00:50:29.02,,,no_permission,,,,,,,,,,,,372889,,,Tue Feb 11 06:53:29 UTC 2014,,,,,,0|i1s9fb:,373192,2.0.5,,,,,,,,,,,,,,,,,,11/Feb/14 00:50;mishail;{{.DropTableStatement.checkAccess(ClientState)}} doesn't handle the case if the table doesn't exist,11/Feb/14 01:05;mishail;Attaching the patch to handle exceptions in {{checkAccess}},11/Feb/14 04:50;jbellis;+1,"11/Feb/14 06:53;mishail;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Node hangs when Drop Keyspace / Table is executed,CASSANDRA-6472,12683871,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,amorton,amorton,10/Dec/13 17:04,12/Mar/19 14:02,13/Mar/19 22:29,04/Feb/14 20:17,2.1 beta1,,,,,,1,,,,,"from http://www.mail-archive.com/user@cassandra.apache.org/msg33566.html

CommitLogSegmentManager.flushDataFrom() returns a FutureTask to wait on the flushes, but the task is not started in flushDataFrom(). 

The CLSM manager thread does not use the result and forceRecycleAll (eventually called when making schema mods) does not start it so hangs when calling get().

plan to patch so flushDataFrom() returns a Future. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-01-07 13:00:36.633,,,no_permission,,,,,,,,,,,,362943,,,Tue Feb 04 20:15:19 UTC 2014,,,,,,0|i1qk9r:,363249,2.1 rc3,,,,,,,,,,,2.1 rc3,,,,,,,07/Jan/14 13:00;benedict;This is fixed as a byproduct of the changes in CASSANDRA-5549,"14/Jan/14 20:39;rhatch;This still appears to be happening.

Steps to reproduce:
{noformat}
ccm create test_cluster
ccm populate -n 1
ccm start
ccm node1 cqlsh

Connected to test_cluster at 127.0.0.1:9160.
[cqlsh 4.1.1 | Cassandra 2.1-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.39.0]
Use HELP for help.
cqlsh> create keyspace test_table_dropping with replication = {'class':'SimpleStrategy', 'replication_factor':1} ;
cqlsh> use test_table_dropping ;
cqlsh:test_table_dropping> CREATE TABLE simple_table (
                       ...   id uuid PRIMARY KEY,
                       ...   sometext text);
cqlsh:test_table_dropping> DROP TABLE simple_table;
{noformat}

At this point the cql session hangs. I don't see any exceptions in the log, but this message appears:
{noformat}
INFO  [Thrift:1] 2014-01-14 13:23:40,341 MigrationManager.java:288 - Drop ColumnFamily 'user_type_dropping/simple_table'
{noformat}","14/Jan/14 20:43;rhatch;I just noticed that if I ctrl-C, ctrl-D to kill my cql session, then open a new cql session the table is in fact gone. So the main problem is just cqlsh hanging after the statement.",04/Feb/14 06:04;jbellis;[~mishail] can you shed any light on the cqlsh hang?,"04/Feb/14 19:49;mishail;Hmm, I can't reproduce that locally using the latest trunk. I've tried steps from this issue as well as from CASSANDRA-6519 . [~rhatch] could you please double check?",04/Feb/14 20:15;rhatch;I'm no longer seeing the issue. I can drop keyspace/table now without a problem (also ran a dtest which relies on dropping and it worked fine).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix validator lookup when converting cells to objects (pig),CASSANDRA-6515,12685746,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius,dbrosius,20/Dec/13 04:42,12/Mar/19 14:02,13/Mar/19 22:29,20/Dec/13 11:05,2.1 beta1,,,,,,0,,,,,"due to refactor, lookup using validators map would fail as the col name type was changed from ByteBuffer to String., use the byte buffer instead.",,,,,,,,,,,,,,,,,,,,,,,,20/Dec/13 04:43;dbrosius;6515.txt;https://issues.apache.org/jira/secure/attachment/12619747/6515.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-12-20 07:19:34.825,,,no_permission,,,,,,,,,,,,364821,,,Fri Dec 20 11:05:54 UTC 2013,,,,,,0|i1qvrr:,365121,,,,,,,,slebresne,slebresne,,,,,,,,,,20/Dec/13 07:19;slebresne;+1,20/Dec/13 11:05;dbrosius;committed to trunk as commit 0d695d4d1eac1241fa251dc45103953aeb977ce2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make snapshot/sequential repair the default,CASSANDRA-5950,12666169,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lyubent,jbellis,jbellis,29/Aug/13 17:40,12/Mar/19 14:02,13/Mar/19 22:29,28/Sep/13 00:47,2.0.2,,,Legacy/Tools,,,0,,,,,,,,,,,,,,,,,,,CASSANDRA-6283,,,,,,,,,,19/Sep/13 10:18;lyubent;5950.patch;https://issues.apache.org/jira/secure/attachment/12604020/5950.patch,25/Sep/13 16:28;lyubent;5950_v2.patch;https://issues.apache.org/jira/secure/attachment/12605032/5950_v2.patch,26/Sep/13 17:36;lyubent;5950_v3.patch;https://issues.apache.org/jira/secure/attachment/12605286/5950_v3.patch,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-09-19 10:17:42.314,,,no_permission,,,,,,,,,,,,346108,,,Sat Sep 28 00:47:55 UTC 2013,,,,,,0|i1nomv:,346409,,,,,,,,dbrosius,dbrosius,,,,,,,,,,19/Sep/13 10:17;lyubent;Changed the function of the SNAPSHOT_REPAIR_OPT parameter to represent a parallel repair.,"25/Sep/13 03:31;dbrosius;The patch itself is fine, but the existing setting name isn't very good imo.

the setting is snapshot... true/false? 

to me that would mean create snapshots or not.

not parallel, serial.

given that we are changing the sense of the parameter, i think it would make sense to change the name so that folks aren't confused by the flip. (and to make it more clear)

","25/Sep/13 16:28;lyubent;Changed the param from snapshot to *parallel* The command is one of the below:

{code}
./nodetool repair -parallel
./nodetool repair -par
./nodetool repair
{code}","26/Sep/13 00:39;dbrosius;probably should rename 

boolean snapshot, to boolean sequential

also please fix up NodeToolHelp.yaml",26/Sep/13 17:36;lyubent;Renamed variable to *sequential* and updated NodeToolHelp.yaml,"28/Sep/13 00:47;dbrosius;+1 thanks, committed to cassandra-2.0 as commit 3a7093356ca032d9ce8767b2c47980aebe4bce60",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix logback configuration in scripts and debian packaging for trunk/2.1,CASSANDRA-6530,12686456,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mshuler,mshuler,mshuler,26/Dec/13 19:15,12/Mar/19 14:02,13/Mar/19 22:29,10/Jan/14 15:34,2.1 beta1,,,Legacy/Tools,,,0,qa-resolved,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Dec/13 16:56;mshuler;logback_configurations_final.patch;https://issues.apache.org/jira/secure/attachment/12620621/logback_configurations_final.patch,10/Jan/14 15:39;mshuler;logback_configurations_final2.patch;https://issues.apache.org/jira/secure/attachment/12622398/logback_configurations_final2.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-12-30 19:02:48.119,,,no_permission,,,,,,,,,,,,365446,,,Fri Jan 10 15:39:21 UTC 2014,,,,,,0|i1qzmv:,365747,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,26/Dec/13 19:34;mshuler;There is a log4j config in test/conf/ and a few places that is used - additions to patch to come.,26/Dec/13 19:55;mshuler;Replaced my initial patch with one that includes removal of the test/conf/ log4j configuration and an update of conf/README.txt,"26/Dec/13 21:22;mshuler;log4 has -server and -tools configurations. -tools only adds WARN,stderr info to the output of the various tools and it appears that a complete logback change will likely need a new logback-tools.xml to add the same.

Scratch my patch for the moment, since it produces some ugly tool output.","27/Dec/13 17:17;mshuler;logback_configurations_final.patch looks good!
Fixes debian package build for trunk and running through some of the bin/* scripts works fine for me.  I'm not exactly sure how to generate some stderr output to see if logback spits those out properly from tools like nodetool or cqlsh, so that might need some testing.","30/Dec/13 19:02;brandon.williams;Looks like we need to suppress the HSHA disconnect warnings like we did in log4j:

{noformat}
WARN  19:01:51 Got an IOException in internalRead!
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[na:1.7.0_17]
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[na:1.7.0_17]
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225) ~[na:1.7.0_17]
        at sun.nio.ch.IOUtil.read(IOUtil.java:198) ~[na:1.7.0_17]
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359) ~[na:1.7.0_17]
        at org.apache.thrift.transport.TNonblockingSocket.read(TNonblockingSocket.java:141) ~[libthrift-0.9.1.jar:0.9.1]
        at com.thinkaurelius.thrift.util.mem.Buffer.readFrom(Buffer.java:96) ~[thrift-server-0.3.2.jar:na]
        at com.thinkaurelius.thrift.Message.internalRead(Message.java:338) [thrift-server-0.3.2.jar:na]
        at com.thinkaurelius.thrift.Message.read(Message.java:141) [thrift-server-0.3.2.jar:na]
        at com.thinkaurelius.thrift.TDisruptorServer$SelectorThread.handleRead(TDisruptorServer.java:521) [thrift-server-0.3.2.jar:na]
        at com.thinkaurelius.thrift.TDisruptorServer$SelectorThread.processKey(TDisruptorServer.java:500) [thrift-server-0.3.2.jar:na]
        at com.thinkaurelius.thrift.TDisruptorServer$AbstractSelectorThread.select(TDisruptorServer.java:375) [thrift-server-0.3.2.jar:na]
        at com.thinkaurelius.thrift.TDisruptorServer$AbstractSelectorThread.run(TDisruptorServer.java:339) [thrift-server-0.3.2.jar:na]
{noformat}","01/Jan/14 15:48;mshuler;conf/log4j-server.properties :
{code}
# Adding this to avoid thrift logging disconnect errors.
log4j.logger.org.apache.thrift.server.TNonblockingServer=ERROR
{code}","02/Jan/14 23:25;mshuler;This appears to already be set up in the logback config:
{code}
(c169)mshuler@hana:~/git/cassandra$ grep thrift conf/logback.xml 
  <logger name=""org.apache.thrift.server.TNonblockingServer"" level=""ERROR""/>
{code}

Setting rpc_server_type: hsha and running stress and killing it a dozen times or so did not give the error output as above.","03/Jan/14 16:17;brandon.williams;This line is bumping org.apache.thrift.server up to ERROR, but my error never goes through that, it comes from org.apache.thrift.transport.  Changing that line in logback does suppress the warnings for me.","08/Jan/14 16:09;brandon.williams;I was wrong, this doesn't suppress the warnings, they just don't repro 100% of the time.  It seems that logback is ignoring us here, but as far as I can tell we have it configured correctly.","09/Jan/14 19:40;brandon.williams;Looks like logback is picky about how you specify what to ignore, it won't 'drill down' past the top level, and not even down to the exact class.  If I use 'com.thinkaurelius.thrift' that suppresses all the warnings, but 'com.thinkaurelius.thrift.TDisruptorServer' does not.  We don't use anything else in c.t.t so I think using that is fine.","10/Jan/14 15:34;brandon.williams;Committed, with the change I detailed above.",10/Jan/14 15:39;mshuler;logback_configurations_final2.patch rebased and s/o.a.t.s.TNonblockingServer/c.t.t/ - that works for me as well (could swear I tried that  :) ),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Intermittently, CQL SELECT  with WHERE on secondary indexed field value returns null when there are rows",CASSANDRA-5599,12649952,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,rb732u,rb732u,29/May/13 16:17,12/Mar/19 14:02,13/Mar/19 22:29,20/Nov/13 03:11,,,,Feature/2i Index,,,0,,,,,"Intermittently, CQL SELECT  with WHERE on secondary indexed field value returns null when there are rows.

As it happens intermittently, it is difficult to replicate. To resolve we have had to recreate the index. Also using the nodetool to reindex did not help us either.

We would create a table, create a secondary index for that table on a field, import data then when we try to select rows from that table with where on said field which should return results, we get a null back ...intermittently.  Sometimes works, sometimes not.








",x86_64 / RedHat Enterprise Linux 4.x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-06-06 09:00:50.763,,,no_permission,,,,,,,,,,,,330279,,,Wed Nov 20 03:11:27 UTC 2013,,,,,,0|i1kzbj:,330613,,,,,,,,,,,,,,,,,,,"06/Jun/13 09:00;michalm;To make sure: does it work after recreating index or the problem still happens sometimes?
If it's still failing sometimes:
* Did you try running repair?
* What consistency level do you use for writes and reads?
* How many nodes do you have? 
* Does it happen when you SELECT with CL.CL.ALL?","05/Jul/13 17:56;srrepaka;We have 2 dc and 2 nodes/dc. Heres the keyspace configuration:

CREATE KEYSPACE grd WITH replication = {
  'class': 'NetworkTopologyStrategy',
  'HYWRCA02': '2',
  'CHRLNCUN': '2'
};

All these queries are executed on node1 on dc HYWRCA02. As you can see we only see the results when consistency level is set to ""ALL"". 

After repair on node1 (dc: HYWRCA02), its the same result.
After repair on node2 (dc: HYWRCA02), its the same result.
After repair on node1 (dc: CHRLNCUN), its the same result.	
After repair on node2 (dc: CHRLNCUN), its the same result.

Let me know if you need any more info.

cqlsh> consistency one;
Consistency level set to ONE.
cqlsh> select count(*) from grd.route where serviceidentifier='com.att.scld.GRMServerTestService'
   ... ;

 count
-------
     0

cqlsh> consistency local_quorum;
Consistency level set to LOCAL_QUORUM.
cqlsh> select count(*) from grd.route where serviceidentifier='com.att.scld.GRMServerTestService';

 count
-------
     0
	 
cqlsh> consistency all;
Consistency level set to ALL.
cqlsh> select count(*) from grd.route where serviceidentifier='com.att.scld.GRMServerTestService' ;

 count
-------
   158","05/Jul/13 18:13;michalm;I had similar problem - repairing nodes and rebuilding index didn't help too (and I couldn't drop & recreate  it due to some other problems). I had to use JMX to compact index CF.
So I can confirm that such problem exists, however I didn't manage to reproduce it.",05/Jul/13 18:22;srrepaka;I could not reproduce it as well. But my guess its occurring when we keep deleting the same content and recreate it over a period of time.,"20/Nov/13 03:11;jbellis;bq. my guess its occurring when we keep deleting the same content and recreate it over a period of time

Suspect this has been addressed by one of the index tickets recently (CASSANDRA-5614, CASSANDRA-5732).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
assassinate should continue when the endpoint vanishes,CASSANDRA-6787,12698068,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,01/Mar/14 00:53,12/Mar/19 14:02,13/Mar/19 22:29,03/Apr/14 20:58,1.2.17,2.0.7,,,,,0,,,,,"Assassinate can NPE in various situations, most notably if the endpoint vanishes during the sleep-for-safety check.",,,,,,,,,,,,,,,,,,,,,,,,03/Apr/14 17:36;brandon.williams;6787.txt;https://issues.apache.org/jira/secure/attachment/12638524/6787.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-04-03 17:41:54.115,,,no_permission,,,,,,,,,,,,376539,,,Thu Apr 03 20:58:25 UTC 2014,,,,,,0|i1svuv:,376834,,,,,,,,jjordan,jjordan,,,,,,,,,,"03/Apr/14 17:36;brandon.williams;When I created this, I wanted to protect against the NPE, but thinking more about it and what spurred to me to create this ticket, I think we should actually go a step further, and if the endpoint vanishes, assassinate it anyway.  The reasoning for this is, if a user is trying to assassinate an endpoint and it expires, they want it gone regardless.  No one is going to go around assassinating endpoints on the verge of expiring without a good reason, and they're most likely stuck in the 'bouncing endpoint' situation in gossip, where it expires on node X, but still exists on Y which repopulates it to X, but then expires on Y, and so on.  In this situation all the user can do is keep trying assassinate until they get lucky and it works, or resort to a hack like starting a bootstrap on that IP and then killing it to turn the endpoint into a fat client that will hopefully expire correctly.

Patch to do this.",03/Apr/14 17:41;jjordan;LGTM +1,03/Apr/14 20:58;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MemoryMeter miscalculating memtable live ratio,CASSANDRA-5377,12638529,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,abashir,abashir,22/Mar/13 16:02,12/Mar/19 14:02,13/Mar/19 22:29,19/Nov/13 22:32,1.2.12,,,,,,0,,,,,"I've noticed the following logs in our running cluster:
WARN [MemoryMeter:1] 2013-03-17 23:15:55,876 Memtable.java (line 197) setting live ratio to minimum of 1.0 instead of 0.6378445488771007

It seems odd for the deep size calculation to be smaller than the aggregate sum of serialized columns.  Perhaps it's because we're mutating on a bunch of existing columns, or perhaps there's some miscalculation somewhere

Our column families all have regular columns (no super columns, expiring columns, etc)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-11-19 22:32:05.757,,,no_permission,,,,,,,,,,,,319005,,,Tue Nov 19 22:32:05 UTC 2013,,,,,,0|i1j1k7:,319346,,,,,,,,,,,,,,,,,,,19/Nov/13 22:32;jbellis;This can happen when the cell-name interning (CASSANDRA-1255) is particularly effective.  Changed the logging to {{debug}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTableReader.loadSummary may leave an open file,CASSANDRA-6380,12680110,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mishail,mishail,mishail,20/Nov/13 05:25,12/Mar/19 14:02,13/Mar/19 22:29,20/Nov/13 13:26,2.0.3,,,,,,0,,,,,"When {{SSTableReader.loadSummary}} catches _IOException_ it tries to delete {{summariesFile}}, but the {{iStream}} is still open and the file is locked, so {{FileUtils.deleteWithConfirm}} fails, at least on Windows",,,,,,,,,,,,,,,,,,,,,,,,20/Nov/13 05:27;mishail;CASSANDRA-2.0-6380.patch;https://issues.apache.org/jira/secure/attachment/12614793/CASSANDRA-2.0-6380.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-20 13:24:54.585,,,no_permission,,,,,,,,,,,,359467,,,Wed Nov 20 13:26:51 UTC 2013,,,,,,0|i1pyvr:,359766,2.0.2,,,,,,,jbellis,jbellis,,,,,,,,,,20/Nov/13 05:27;mishail;Attaching the patch to close the stream to unlock the file,"20/Nov/13 13:24;yukim;'finally' block does close iStream, so I think there isn't a problem in current code.","20/Nov/13 13:26;jbellis;LGTM; committed

(Didn't see Yuki's comment.  The problem is that the catch block runs before the finally, so it tries to delete before the close, which works on linux but not windows.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set minTimestamp correctly to be able to drop expired sstables,CASSANDRA-6337,12678868,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,krummas,krummas,krummas,12/Nov/13 19:21,12/Mar/19 14:02,13/Mar/19 22:29,13/Nov/13 19:22,2.0.3,,,,,,0,,,,,"When calculating which sstables we can drop we set minTimestamp to Integer.MAX_VALUE, this is wrong since minTimestamp is a long, and in most cases minTimestamp on sstables is larger than Integer.MAX_VALUE.

We should set it to Long.MAX_VALUE, patch does that.",,,,,,,,,,,,,,,,,,,,,,,,12/Nov/13 19:22;krummas;0001-Set-minTimestamp-correctly.patch;https://issues.apache.org/jira/secure/attachment/12613412/0001-Set-minTimestamp-correctly.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-12 20:50:11.131,,,no_permission,,,,,,,,,,,,358234,,,Wed Nov 13 19:22:10 UTC 2013,,,,,,0|i1pr6v:,358524,,,,,,,,,,,,2.0 beta 1,,,,,,,12/Nov/13 20:50;jbellis;+1,13/Nov/13 19:22;krummas;committed!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
clean out populate_io_cache_on_flush option,CASSANDRA-6785,12697964,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,28/Feb/14 16:25,12/Mar/19 14:02,13/Mar/19 22:29,28/Feb/14 16:53,2.1 beta2,,,Local/Config,,,0,,,,,populate_io_cache_on_flush is a per-table option as of 1.2.2 (CASSANDRA-4694) but the old global option remains in Config where it is ignored.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,376438,,,Fri Feb 28 16:53:58 UTC 2014,,,,,,0|i1sv8n:,376734,,,,,,,,,,,,,,,,,,,28/Feb/14 16:53;jbellis;removed the option from cassandra.yaml in 1.2 and 2.0 but left Config.java alone so that we don't break restart on upgrade.  Removed the option from Config as well in 2.1.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AE in PrecompactedRow.update(PrecompactedRow.java:171),CASSANDRA-6277,12676860,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,vilda,vilda,31/Oct/13 14:15,12/Mar/19 14:02,13/Mar/19 22:29,31/Oct/13 14:24,2.0.3,,,,,,0,repair,,,,"Getting this AE on destination nodes during repair:

ERROR [ValidationExecutor:78] 2013-10-31 04:35:31,243 CassandraDaemon.java (line 187) Exception in thread Thread[ValidationExecutor:78,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.PrecompactedRow.update(PrecompactedRow.java:171)
        at org.apache.cassandra.repair.Validator.rowHash(Validator.java:198)
        at org.apache.cassandra.repair.Validator.add(Validator.java:151)
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:799)
        at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
        at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
","Linux, 12 nodes, 3 AZ EC2
Cassandra version 2.0.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-10-31 14:24:53.918,,,no_permission,,,,,,,,,,,,356236,,,Mon Nov 18 15:31:09 UTC 2013,,,,,,0|i1pevj:,356524,2.0.2,,,,,,,,,,,2.0.0,,,,,,,31/Oct/13 14:24;jbellis;Fixed in cf8fa6e11bf16ffed84a8805c4bc63077d6a7bcf,"14/Nov/13 20:15;rcoli;Does this affect all repair in 2.0.0-2.0.2, or just certain cases? If just certain cases, how common do we estimate they are?",18/Nov/13 15:31;vilda;My experience running 2.0.2: 21 AEs during repair on all 9 nodes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid flushing compaction_history after each operation,CASSANDRA-6287,12677184,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,02/Nov/13 00:46,12/Mar/19 14:02,13/Mar/19 22:29,02/Nov/13 14:11,2.0.3,,,,,,0,compaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Nov/13 00:47;jbellis;6287.txt;https://issues.apache.org/jira/secure/attachment/12611724/6287.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-02 01:12:35.874,,,no_permission,,,,,,,,,,,,356559,,,Sat Nov 02 14:11:00 UTC 2013,,,,,,0|i1pgvb:,356847,,,,,,,,yukim,yukim,,,,,,,,,,"02/Nov/13 01:12;yukim;+1.
It's just stats so we don't need to flush every inserts.",02/Nov/13 14:11;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add new TimestampType to cqlsh,CASSANDRA-5729,12656631,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,slebresne,slebresne,08/Jul/13 16:58,12/Mar/19 14:02,13/Mar/19 22:29,08/Jul/13 20:09,2.0.0,,,,,,0,,,,,"Since cqlsh used CQL-over-thrift currently, we'd need to add support for the new TimestampType introduced in CASSANDRA-5723.",,,,,,,,,,,,,,,,,,,,,,,,08/Jul/13 19:46;iamaleksey;5729.txt;https://issues.apache.org/jira/secure/attachment/12591260/5729.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-08 19:48:12.439,,,no_permission,,,,,,,,,,,,336854,,,Mon Jul 08 20:09:46 UTC 2013,,,,,,0|i1m3qn:,337177,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"08/Jul/13 19:48;iamaleksey;Adding the type locally in cqlsh. Will add to the next cassandra-dbapi2 release, too, but it's not ready yet.",08/Jul/13 19:53;brandon.williams;+1,"08/Jul/13 20:09;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ant test should only show WARN+ on stdout,CASSANDRA-5961,12666387,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,jbellis,jbellis,30/Aug/13 20:37,12/Mar/19 14:02,13/Mar/19 22:29,31/Aug/13 00:41,2.1 beta1,,,Legacy/Testing,,,0,,,,,We had log4j configured to only output WARN or ERROR messages on stdout during ant test.  Would be nice to get the same behavior on trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-08-31 00:41:38.882,,,no_permission,,,,,,,,,,,,346326,,,Sat Aug 31 00:41:38 UTC 2013,,,,,,0|i1npyn:,346627,,,,,,,,,,,,,,,,,,,"31/Aug/13 00:41;dbrosius;add threshold filter on stderr for tests to WARN, committed to trunk as commit 0ec481bd64874e8e03d046c5177dcf0f4a64e500 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible NPE on EACH_QUORUM writes,CASSANDRA-5498,12643583,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jasobrown,jasobrown,jasobrown,19/Apr/13 13:22,12/Mar/19 14:02,13/Mar/19 22:29,26/Jun/13 15:02,1.2.6,,,,,,0,each_quorum,ec2,,,"When upgrading from 1.0 to 1.1, we observed that DatacenterSyncWriteResponseHandler.assureSufficientLiveNodes() can throw an NPE if one of the writeEndpoints has a DC that is not listed in the keyspace while one of the nodes is down. We observed this while running in EC2, and using the Ec2Snitch. The exception typically was was brief, but a certain segment of writes (using EACH_QUORUM) failed during that time.

This ticket will address the NPE in DSWRH, while a followup ticket will be created once we get to the bottom of the incorrect DC being reported from Ec2Snitch.
",,,,,,,,,,,,,,CASSANDRA-5660,,,,,,,,,,19/Apr/13 13:25;jasobrown;5498-v1.patch;https://issues.apache.org/jira/secure/attachment/12579537/5498-v1.patch,19/Apr/13 14:28;jasobrown;5498-v2.patch;https://issues.apache.org/jira/secure/attachment/12579543/5498-v2.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-04-19 13:56:10.814,,,no_permission,,,,,,,,,,,,323950,,,Wed Jun 19 13:15:05 UTC 2013,,,,,,0|i1jw3b:,324295,,,,,,,,,,,,,,,,,,,"19/Apr/13 13:25;jasobrown;This patch simply checks if we got a null value from the keyspace-dc map, and if so, throws UnavailableException.","19/Apr/13 13:56;jbellis;I'm not convinced that we should turn an internal bug into a UAE, although I'd be fine with an assert to make more clear what we expect.  (Pretty sure Thrift will catch it and return an InternalError, so clients shouldn't be just left hanging.)

Separately, I note that TokenMetadata.getWriteEndpoints is ReplicationStrategy-agnostic.  If you have any bootstrap action going on, that could be causing your problem.  (Similar bug in CASSANDRA-5424.)","19/Apr/13 14:14;jasobrown;Good point about not not throwing a UAE and having assert instead. With the NPE, thrift did return an InternalError to clients, so there was reasonable commincation for subclasses of Exception (hopefully the same for AssertionError?). Bootstrap is not going on at the time, but I'll check out that ticket, as well. For my testing, I left one of the nodes down intentionally and was able to reproduce sporadically, but still digging in.",19/Apr/13 14:28;jasobrown;v2 has assert rather than throwing UAE.,"19/Apr/13 14:40;jbellis;bq. With the NPE, thrift did return an InternalError to clients

Now I'm puzzled, because I don't see how this works...  Thrift 0.7 appears to be missing the internal error catch; it was added back in THRIFT-1658 for Thrift 0.8.","23/Apr/13 22:01;jasobrown;bq. Now I'm puzzled, because I don't see how this works...

Are you referring to the NPE or the AssertionError? In my statement above, I mean the NPE was caught and the client app got a TApplicationException. Here the output via an astyanax client:

{code}2013-04-15 18:27:29,618 ERROR com.netflix.cassandra.NetflixConnectionPoolMonitor:428 [http-0.0.0.0-7101-33] [trackError] Unknown operation error
com.netflix.astyanax.connectionpool.exceptions.ThriftStateException: ThriftStateException: [host=ec2-54-234-29-24.compute-1.amazonaws.com(10.29.141.78):7102, latency=2(2), attempts=1]org.apache.thrift.TApplicationException:
 Internal error processing batch_mutate
        at com.netflix.astyanax.thrift.ThriftConverter.ToConnectionPoolException(ThriftConverter.java:177)
        at com.netflix.astyanax.thrift.AbstractOperationImpl.execute(AbstractOperationImpl.java:65)
        at com.netflix.astyanax.thrift.AbstractOperationImpl.execute(AbstractOperationImpl.java:28)
        at com.netflix.astyanax.thrift.ThriftSyncConnectionFactoryImpl$ThriftConnection.execute(ThriftSyncConnectionFactoryImpl
{code}

I'll test again with throwing the AE, but I can see from your thrift bug report that it might, in fact, be an issue. Sorry if my early statement was unclear.
","24/Apr/13 01:23;jbellis;I mean that I don't see how Thrift 0.7 catches *any* internal error as TAE, so my mental picture must be somehow incomplete (and therefore I'm not comfortable changing things until that's repaired).",24/Apr/13 01:24;jbellis;[~tjake] [~bterm] Can you shed any light here?,24/Apr/13 03:16;tjake;The exception is thrown on the client.  It's the default exception in Java impl when none of the declared exceptions are a match.,22/May/13 15:45;jbellis;But I don't see how 0.7 would return an exception at all; it would just close the connection uncleanly.,"03/Jun/13 17:39;jjordan;I don't think an exception is being caught and passed to the client.  I think the connection closes so org.apache.thrift.TApplicationException gets thrown.

[~jasobrown] have you done any more debug on what is causing this?","03/Jun/13 18:52;jasobrown;[~jjordan] working on it now on #cassandra-dev IRC. My suspicion is a problem with Gossiper.addSavedEndopint(), which clears out the endpoint's previous data from the endpointStateMap when a node with a greater messaging version attempts to connect. Which then causes the downstream affect in DSWRH when it requests the DC data from the EC2Snitch, which gets it from Gossiper.endopintStateMap.

Here's the server-side stacktrace:

{code}ERROR [RPC-Thread:150339] 2013-05-08 17:29:55,048 Cassandra.java (line 3462) Internal error processing batch_mutate 
java.lang.NullPointerException 
at org.apache.cassandra.service.DatacenterSyncWriteResponseHandler.assureSufficientLiveNodes(DatacenterSyncWriteResponseHandler.java:109) 
at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:253) 
at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:194) 
at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:639) 
at org.apache.cassandra.thrift.CassandraServer.internal_batch_mutate(CassandraServer.java:590) 
at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:598) 
at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:3454) 
at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889) 
at org.apache.thrift.server.TNonblockingServer$FrameBuffer.invoke(TNonblockingServer.java:631) 
at org.apache.cassandra.thrift.CustomTHsHaServer$Invocation.run(CustomTHsHaServer.java:105) 
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
at java.lang.Thread.run(Thread.java:662){code}",19/Jun/13 13:15;jasobrown;The cause of this NPE is due to the Gossiper issue in the linked ticket,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow empty blob literals in CQL3,CASSANDRA-5452,12641878,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,10/Apr/13 18:07,12/Mar/19 14:02,13/Mar/19 22:29,11/Apr/13 09:22,1.2.5,,,,,,0,,,,,"The current grammar don't allow empty blob literals (so '0x'). The goal here is to allow the following syntax for that:
{noformat}
INSERT INTO test(k, b) VALUES (0, 0x)
{noformat}
I'll admit that '0x' is not the most beautiful syntax ever, but I think that's the only thing that make sense.

I'll note that currently there is 2 workaround to insert empty blobs: you can either use prepared statement (not a bad idea when using blobs anyway) or, because we've deprecated but still support until 2.0 using strings as blob (to allow upgrade from 1.2.0 to 1.2.1), you can use an empty string. I'll note that this latter workaround will trigger a deprecation warning in the log however and will stop working in 2.0.",,,,,,,,,,,,,,,,,,,,,,,,10/Apr/13 18:08;slebresne;5452.txt;https://issues.apache.org/jira/secure/attachment/12578046/5452.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-10 21:09:32.444,,,no_permission,,,,,,,,,,,,322293,,,Thu Apr 11 09:22:22 UTC 2013,,,,,,0|i1jlvb:,322638,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,10/Apr/13 18:08;slebresne;Trivial patch attached.,10/Apr/13 21:09;iamaleksey;+1,"11/Apr/13 09:22;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrub disk footprint needs to be reduced,CASSANDRA-5891,12663745,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lyubent,kohlisankalp,kohlisankalp,14/Aug/13 22:42,12/Mar/19 14:02,13/Mar/19 22:29,20/Aug/13 21:41,1.2.9,,,Legacy/Tools,,,0,,,,,"Currently scrub creates a snapshot at the beginning of the scrub. This causes the disk used to be doubled after the scrub. 

If the disk utilization is more than 50%, scrub wont work. It would be nice to have an overriding option to disable snapshot. Something like --no-snapshot.",,,,,,,,,,,,,,,,,,,,,,,,19/Aug/13 14:09;lyubent;5891.patch;https://issues.apache.org/jira/secure/attachment/12598762/5891.patch,20/Aug/13 19:46;lyubent;5891_cassandra-1.2.patch;https://issues.apache.org/jira/secure/attachment/12599016/5891_cassandra-1.2.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-08-19 14:09:18.365,,,no_permission,,,,,,,,,,,,343746,,,Tue Aug 20 21:41:43 UTC 2013,,,,,,0|i1na3b:,344048,,,,,,,,dbrosius,dbrosius,,,,,,,,,,"19/Aug/13 14:09;lyubent;Added a -no-snapshot parameter to the ""nodetool scrub"" command. When supplied, the parameter tells scrub to skip snapshot creation and saves disk space:

The below is for the size of the var/lib/cassandra directory
./nodetool scrub with -no-snapshot: cassandra dir size before scrub: 4.1G, after scrub 4.1G
./nodetool scrub (without -no-snapshot): cassandra dir size before scrub: 4.1G, after scrub 7.1G","20/Aug/13 04:25;dbrosius;patch seems fine, but does not apply cleanly to the cassandra-1.2 branch, can you please rebase?


nit: it seems awkward that the disableSnapshot parm is between keyspace and columnfamilies, (understood that varargs gets in the way) perhaps it should be first?","20/Aug/13 19:46;lyubent;Rebased to cassandra-1.2 branch, switched order of params for StorageService#scrub so that disableSnapshot is first.","20/Aug/13 21:41;dbrosius;thanks,

committed to cassandra-1.2 as commit dbb55ebd685ca36dc962e07d1a33b3354a1ce433",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Value of JVM_OPTS is partially lost when enabling JEMallocAllocator in cassandra-env.sh,CASSANDRA-6006,12668048,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,ngrigoriev,ngrigoriev,11/Sep/13 13:25,12/Mar/19 14:02,13/Mar/19 22:29,11/Sep/13 14:37,2.0.1,,,Local/Config,,,0,,,,,"In conf/cassandra-env.sh I see this:

{code}
# Configure the following for JEMallocAllocator and if jemalloc is not available in the system
# library path (Example: /usr/local/lib/). Usually ""make install"" will do the right thing.
# export LD_LIBRARY_PATH=<JEMALLOC_HOME>/lib/
# JVM_OPTS=""-Djava.library.path=<JEMALLOC_HOME>/lib/""
{code}


When I have enabled JEMalloc I have noticed that Cassandra complained about JAMM agent not being configured. Then I have realized that a bunch of JVM settings do not get passed to JVM, like heap size etc. This is because here the new argument replaces the previous value of JVM_OPTS instead of being added to it.

Here is the diff:
{code}
*** cassandra-env.sh.orig       2013-08-28 13:07:53.000000000 +0000
--- cassandra-env.sh    2013-09-11 13:25:12.904640141 +0000
***************
*** 227,233 ****
  # Configure the following for JEMallocAllocator and if jemalloc is not available in the system
  # library path (Example: /usr/local/lib/). Usually ""make install"" will do the right thing.
  # export LD_LIBRARY_PATH=<JEMALLOC_HOME>/lib/
! # JVM_OPTS=""-Djava.library.path=<JEMALLOC_HOME>/lib/""

  # uncomment to have Cassandra JVM listen for remote debuggers/profilers on port 1414
  # JVM_OPTS=""$JVM_OPTS -Xdebug -Xnoagent -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=1414""
--- 227,233 ----
  # Configure the following for JEMallocAllocator and if jemalloc is not available in the system
  # library path (Example: /usr/local/lib/). Usually ""make install"" will do the right thing.
  # export LD_LIBRARY_PATH=<JEMALLOC_HOME>/lib/
! # JVM_OPTS=""$JVM_OPTS -Djava.library.path=<JEMALLOC_HOME>/lib/""

  # uncomment to have Cassandra JVM listen for remote debuggers/profilers on port 1414
  # JVM_OPTS=""$JVM_OPTS -Xdebug -Xnoagent -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=1414""
{code}","Linux, cassandra 2.0.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-09-11 14:37:28.305,,,no_permission,,,,,,,,,,,,347983,,,Wed Sep 11 14:37:28 UTC 2013,,,,,,0|i1o05b:,348279,,,,,,,,,,,,,,,,,,,"11/Sep/13 14:37;brandon.williams;Thanks, done in 1ae996d38259ad6d18fef7344b745eba8af56a4d",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CQL identifier regex in documentation is wrong, (or cqlsh doesn't implement correctly)",CASSANDRA-6019,12668344,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,enigmacurry,enigmacurry,12/Sep/13 20:04,12/Mar/19 14:02,13/Mar/19 22:29,01/Apr/15 15:31,,,,Legacy/Documentation and Website,,,0,cql,,,,"The [CQL docs|http://cassandra.apache.org/doc/cql3/CQL.html#identifiers] state that a CQL identifier can be any characters [a-zA-Z0-9_]*. ,but in fact, they cannot start with an '_' or a number.

Empirically, it looks like the regex should be [a-zA-Z][a-zA-Z0-9_].*",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-04-01 15:31:13.684,,,no_permission,,,,,,,,,,,,348278,,,Wed Apr 01 15:31:13 UTC 2015,,,,,,0|i1o1yn:,348574,,,,,,,,,,,,,,,,,,,01/Apr/15 15:31;philipthompson;This has been fixed in the docs file in the code for a while. The html merely needed regenerated.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted Handoff: java.lang.ArithmeticException: / by zero,CASSANDRA-5990,12667683,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,kmueller,kmueller,09/Sep/13 22:09,12/Mar/19 14:02,13/Mar/19 22:29,12/Sep/13 11:22,1.2.10,2.0.1,,,,,0,,,,,"This node was down for a few hours. When bringing it back up, I saw this error in the logs. I'm not sure if it's receiving or sending hinted hand-offs.

 INFO [HintedHandoff:1] 2013-09-09 14:41:04,020 HintedHandOffManager.java (line 292) Started hinted handoff for host: 42bba02f-3088-4be1-8cb2-748a6f15e15d with IP: /10.93.12.14
ERROR [HintedHandoff:1] 2013-09-09 14:41:04,024 CassandraDaemon.java (line 192) Exception in thread Thread[HintedHandoff:1,1,main]
java.lang.ArithmeticException: / by zero
        at org.apache.cassandra.db.HintedHandOffManager.calculatePageSize(HintedHandOffManager.java:441)
        at org.apache.cassandra.db.HintedHandOffManager.doDeliverHintsToEndpoint(HintedHandOffManager.java:299)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:278)
        at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:90)
        at org.apache.cassandra.db.HintedHandOffManager$4.run(HintedHandOffManager.java:497)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)","cassandra 1.2.8
Oracle Java 1.7.0_25-b15
RHEL6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-09-12 11:22:34.299,,,no_permission,,,,,,,,,,,,347620,,,Thu Sep 12 11:22:34 UTC 2013,,,,,,0|i1nxxb:,347919,,,,,,,,,,,,,,,,,,,12/Sep/13 11:22;jbellis;fixed in 8cc28a1477e19545bd0f6dca9180c937f9c85c8d,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong toString() of CQL3Type.Collection,CASSANDRA-6051,12669211,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,alexander_radzin,alexander_radzin,18/Sep/13 08:35,12/Mar/19 14:02,13/Mar/19 22:29,18/Sep/13 12:27,1.2.10,,,,,,0,,,,,"{{Collection}} is an inner class of {{CQL3Type}} that represents column types that can contain several values (lists, sets, maps). Its {{toString()}} is very helpful: it generates a string representation of type that can be used for generating of {{CREATE TABLE}} statement. 

Unfortunately this method works incorrectly for maps. Instead of returning something like {{map<text, int>}} it returns {{set<text, int>}}.

Here is the appropriate code fragment:

{code:title=CQL3Type$Collection.java|borderStyle=solid}
       public String toString()
        {
            switch (type.kind)
            {
                case LIST:
                    return ""list<"" + ((ListType)type).elements.asCQL3Type() + "">"";
                case SET:
                    return ""set<"" + ((SetType)type).elements.asCQL3Type() + "">"";
                case MAP:
                    MapType mt = (MapType)type;
                    return ""set<"" + mt.keys.asCQL3Type() + "", "" + mt.values.asCQL3Type() + "">"";
            }
            throw new AssertionError();
        }
{code}


The obvious bug is here:
{code:java}
                case MAP:
                    MapType mt = (MapType)type;
                    return ""set<"" + mt.keys.asCQL3Type() + "", "" + 
{code}

It should be {{""map<"" + ...}} instead of {{""set<"" + ...}}",,,,300,300,,0%,300,300,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-09-18 12:27:48.837,,,no_permission,,,,,,,,,,,,349143,,,Wed Sep 18 12:27:48 UTC 2013,,,,,,0|i1o7an:,349441,,,,,,,,,,,,,,,,,,,18/Sep/13 12:27;slebresne;Fixed in commit fc9b3a7. Thanks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace the deprecated MapMaker with CacheLoader for compatibility with Guava 15.0+,CASSANDRA-6007,12668081,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,mpenet,mpenet,11/Sep/13 17:05,12/Mar/19 14:02,13/Mar/19 22:29,12/Sep/13 15:47,1.2.10,,,,,,1,,,,,"Attempting to load datastax/java-driver 2.0 beta1 and cassandra-all 2.0 in the same jvm causes some issues mainly because of clashes between guava versions  (15.0 in the driver vs 13.0.1 in c*). This makes automated testing using EmbeddedCassandraService problematic for instance.

Stacktrace from https://github.com/mpenet/alia/tree/2.0 running ""lein test""

Upgrading c* 2.0 to guava 15+ should help fix this issue. 

{code:java}
java.lang.IllegalAccessError: tried to access method com.google.common.collect.MapMaker.makeComputingMap(Lcom/google/common/base/Function;)Ljava/util/concurrent/ConcurrentMap; from class org.apache.cassandra.service.StorageProxy
	at org.apache.cassandra.service.StorageProxy.<clinit>(StorageProxy.java:87)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:190)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:447)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:426)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:344)
	at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:377)
	at org.apache.cassandra.service.EmbeddedCassandraService.start(EmbeddedCassandraService.java:52)
	at qbits.alia.test.embedded$start_service_BANG_.invoke(embedded.clj:20)
	at qbits.alia.test.embedded$eval10911.invoke(embedded.clj:24)
	at clojure.lang.Compiler.eval(Compiler.java:6619)
	at clojure.lang.Compiler.load(Compiler.java:7064)
	at clojure.lang.RT.loadResourceScript(RT.java:370)
	at clojure.lang.RT.loadResourceScript(RT.java:361)
	at clojure.lang.RT.load(RT.java:440)
	at clojure.lang.RT.load(RT.java:411)
	at clojure.core$load$fn__5018.invoke(core.clj:5530)
	at clojure.core$load.doInvoke(core.clj:5529)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at clojure.core$load_one.invoke(core.clj:5336)
	at clojure.core$load_lib$fn__4967.invoke(core.clj:5375)
	at clojure.core$load_lib.doInvoke(core.clj:5374)
	at clojure.lang.RestFn.applyTo(RestFn.java:142)
	at clojure.core$apply.invoke(core.clj:619)
	at clojure.core$load_libs.doInvoke(core.clj:5413)
	at clojure.lang.RestFn.applyTo(RestFn.java:137)
	at clojure.core$apply.invoke(core.clj:621)
	at clojure.core$use.doInvoke(core.clj:5507)
	at clojure.lang.RestFn.invoke(RestFn.java:703)
	at qbits.alia.test.core$eval161$loading__4910__auto____162.invoke(core.clj:1)
	at qbits.alia.test.core$eval161.invoke(core.clj:1)
	at clojure.lang.Compiler.eval(Compiler.java:6619)
	at clojure.lang.Compiler.eval(Compiler.java:6608)
	at clojure.lang.Compiler.load(Compiler.java:7064)
	at clojure.lang.RT.loadResourceScript(RT.java:370)
	at clojure.lang.RT.loadResourceScript(RT.java:361)
	at clojure.lang.RT.load(RT.java:440)
	at clojure.lang.RT.load(RT.java:411)
	at clojure.core$load$fn__5018.invoke(core.clj:5530)
	at clojure.core$load.doInvoke(core.clj:5529)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at clojure.core$load_one.invoke(core.clj:5336)
	at clojure.core$load_lib$fn__4967.invoke(core.clj:5375)
	at clojure.core$load_lib.doInvoke(core.clj:5374)
	at clojure.lang.RestFn.applyTo(RestFn.java:142)
	at clojure.core$apply.invoke(core.clj:619)
	at clojure.core$load_libs.doInvoke(core.clj:5413)
	at clojure.lang.RestFn.applyTo(RestFn.java:137)
	at clojure.core$apply.invoke(core.clj:619)
	at clojure.core$require.doInvoke(core.clj:5496)
	at clojure.lang.RestFn.applyTo(RestFn.java:137)
	at clojure.core$apply.invoke(core.clj:619)
	at user$eval85.invoke(NO_SOURCE_FILE:1)
	at clojure.lang.Compiler.eval(Compiler.java:6619)
	at clojure.lang.Compiler.eval(Compiler.java:6609)
	at clojure.lang.Compiler.eval(Compiler.java:6582)
	at clojure.core$eval.invoke(core.clj:2852)
	at clojure.main$eval_opt.invoke(main.clj:308)
	at clojure.main$initialize.invoke(main.clj:327)
	at clojure.main$null_opt.invoke(main.clj:362)
	at clojure.main$main.doInvoke(main.clj:440)
	at clojure.lang.RestFn.invoke(RestFn.java:421)
	at clojure.lang.Var.invoke(Var.java:419)
	at clojure.lang.AFn.applyToHelper(AFn.java:163)
	at clojure.lang.Var.applyTo(Var.java:532)
	at clojure.main.main(main.java:37)

{code}",,,,,,,,,,,,,,,,,,,,,,,,12/Sep/13 11:30;iamaleksey;6007.txt;https://issues.apache.org/jira/secure/attachment/12602768/6007.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-12 05:54:44.902,,,no_permission,,,,,,,,,,,,348015,,,Thu Sep 12 15:47:41 UTC 2013,,,,,,0|i1o0cf:,348311,,,,,,,,jbellis,jbellis,,,,,,,,,,"12/Sep/13 05:54;ash2k;We use cassandra-maven-plugin 1.2.1, Astyanax 1.56.42 with cassandra jar version 1.2.5 (not possible to update due to [1] and [2]). With Guava 14.0.1 everything works fine, but if I update to Guava 15.0 then this exception happens when building project (integration tests fail):

{code}
[INFO ] 11:36:09.855 [main][] ERROR CassandraDaemon:430 - Exception encountered during startup
[INFO ] java.lang.IllegalAccessError: tried to access method com.google.common.collect.MapMaker.makeComputingMap(Lcom/google/common/base/Function;)Ljava/util/concurrent/ConcurrentMap; from class org.apache.cassandra.service.StorageProxy
[INFO ] 	at org.apache.cassandra.service.StorageProxy.<clinit>(StorageProxy.java:84) ~[cassandra-all-1.2.5.jar:1.2.5]
[INFO ] 	at java.lang.Class.forName0(Native Method) ~[na:1.7.0_25]
[INFO ] 	at java.lang.Class.forName(Class.java:190) ~[na:1.7.0_25]
[INFO ] java.lang.IllegalAccessError: tried to access method com.google.common.collect.MapMaker.makeComputingMap(Lcom/google/common/base/Function;)Ljava/util/concurrent/ConcurrentMap; from class org.apache.cassandra.service.StorageProxy
[INFO ] 	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:466) ~[cassandra-all-1.2.5.jar:1.2.5]
[INFO ] 	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:445) ~[cassandra-all-1.2.5.jar:1.2.5]
[INFO ] 	at org.apache.cassandra.service.StorageProxy.<clinit>(StorageProxy.java:84)
[INFO ] 	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:325) ~[cassandra-all-1.2.5.jar:1.2.5]
[INFO ] 	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:413) ~[cassandra-all-1.2.5.jar:1.2.5]
[INFO ] 	at java.lang.Class.forName0(Native Method)
[INFO ] 	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:456) ~[cassandra-all-1.2.5.jar:1.2.5]
[INFO ] 	at java.lang.Class.forName(Class.java:190)
[INFO ] 	at org.codehaus.mojo.cassandra.CassandraMonitor.main(CassandraMonitor.java:148) ~[cassandra-maven-plugin-1.2.1-1.jar:na]
[INFO ] 	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:466)
[INFO ] 	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:445)
[INFO ] Exception encountered during startup: tried to access method com.google.common.collect.MapMaker.makeComputingMap(Lcom/google/common/base/Function;)Ljava/util/concurrent/ConcurrentMap; from class org.apache.cassandra.service.StorageProxy
[INFO ] 	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:325)
[INFO ] 	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:413)
[INFO ] 	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:456)
[INFO ] 	at org.codehaus.mojo.cassandra.CassandraMonitor.main(CassandraMonitor.java:148)
{code}

MapMaker.makeComputingMap() was deprecated some time ago and removed in 15.0 that's why this exception happens. The code in StorageProxy class can trivially be fixed using Guava's LoadingCache.

[1]: https://github.com/Netflix/astyanax/issues/352
[2]: https://github.com/Netflix/astyanax/issues/391","12/Sep/13 11:34;iamaleksey;Attaching a 1.2 based patch that replaces the deprecated MapMaker usage with CacheLoader (http://code.google.com/p/guava-libraries/wiki/MapMakerMigration)

Will update the jar and build.xml separately, and only for 2.0.1.",12/Sep/13 14:58;jbellis;+1,12/Sep/13 15:47;iamaleksey;Guava updated separately in 53f19c8495e37433d70bcfa19f8575b712d8b763,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow local batchlog writes for CL.ANY,CASSANDRA-5967,12666490,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,01/Sep/13 13:42,12/Mar/19 14:02,13/Mar/19 22:29,01/Sep/13 19:35,1.2.10,2.0.1,,,,,0,,,,,"We tell people that ANY means you can write if even a single node is reachable.  We should apply that to batchlog writes as well.

Note: we already allow local batchlog writes for any single-node datacenter; batchlog writes are synchronous, so cross-dc latency would be slow enough to be unusable.",,,,,,,,,,,,,,,,,,,,,,,,01/Sep/13 13:42;jbellis;5967.txt;https://issues.apache.org/jira/secure/attachment/12600980/5967.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-01 15:15:38.092,,,no_permission,,,,,,,,,,,,346429,,,Sun Sep 01 19:35:04 UTC 2013,,,,,,0|i1nqlj:,346730,,,,,,,,iamaleksey,iamaleksey,,,1.2.0,,,,,,,01/Sep/13 15:15;iamaleksey;+1,01/Sep/13 19:35;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Eternal iteration when using older hadoop version due to next() call and empty key value,CASSANDRA-5504,12643918,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,22/Apr/13 11:50,12/Mar/19 14:02,13/Mar/19 22:29,25/Apr/13 23:02,1.2.5,,,,,,2,,,,,"Currently, when using newer hadoop versions, due to the call to 

next(ByteBuffer key, SortedMap<ByteBuffer, IColumn> value)

within ColumnFamilyRecordReader, because `key.clear();` is called, key is emptied. That causes the StaticRowIterator and WideRowIterator to glitch, namely, when Iterables.getLast(rows).key is called, key is already empty. This will cause Hadoop to request the same range again and again all the time.

Please see the attached patch/diff, it simply adds lastRowKey (ByteBuffer) and saves it for the next iteration along with all the rows, this allows query for the next range to be fully correct.

This patch is branched from 1.2.3 version.

Tested against Cassandra 1.2.3, with Hadoop 1.0.3, 1.0.4 and 0.20.2",,,,,,,,,,,,,,,,,,,,,,,,25/Apr/13 22:47;jbellis;5504-v3.txt;https://issues.apache.org/jira/secure/attachment/12580610/5504-v3.txt,22/Apr/13 11:56;ifesdjeen;patch.diff;https://issues.apache.org/jira/secure/attachment/12579825/patch.diff,22/Apr/13 17:09;ifesdjeen;patch2.diff;https://issues.apache.org/jira/secure/attachment/12579852/patch2.diff,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-04-22 15:47:15.762,,,no_permission,,,,,,,,,,,,324285,,,Sat Jun 29 20:55:00 UTC 2013,,,,,,0|i1jy5r:,324630,,,,,,,,,,,,,,,,,,,"22/Apr/13 15:47;budlight;I believe this issue also affects 1.1.11 and 1.2.4,  I tried both had the same issue as on http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/Thrift-message-length-exceeded-td7587006.html and http://stackoverflow.com/questions/15487540/pig-cassandra-message-length-exceeded

I then tried 1.1.9 (skipped .10) and it worked correctly","22/Apr/13 16:48;budlight;patch doesn't fix my issue still get:

{quote}
java.lang.RuntimeException: org.apache.thrift.TException: Message length exceeded: 21
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.maybeInit(ColumnFamilyRecordReader.java:384)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.computeNext(ColumnFamilyRecordReader.java:390)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.computeNext(ColumnFamilyRecordReader.java:313)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.getProgress(ColumnFamilyRecordReader.java:103)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.getProgress(PigRecordReader.java:158)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.getProgress(MapTask.java:514)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:539)
	at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Unknown Source)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)
Caused by: org.apache.thrift.TException: Message length exceeded: 21
	at org.apache.thrift.protocol.TBinaryProtocol.checkReadLength(TBinaryProtocol.java:393)
	at org.apache.thrift.protocol.TBinaryProtocol.readBinary(TBinaryProtocol.java:363)
	at org.apache.cassandra.thrift.Column.read(Column.java:528)
	at org.apache.cassandra.thrift.ColumnOrSuperColumn.read(ColumnOrSuperColumn.java:507)
	at org.apache.cassandra.thrift.KeySlice.read(KeySlice.java:408)
	at org.apache.cassandra.thrift.Cassandra$get_range_slices_result.read(Cassandra.java:12905)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_get_range_slices(Cassandra.java:734)
	at org.apache.cassandra.thrift.Cassandra$Client.get_range_slices(Cassandra.java:718)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.maybeInit(ColumnFamilyRecordReader.java:346)
{quote}

Maybe these are separate issues","22/Apr/13 16:56;ifesdjeen;Sorry, my bad, I have forgotten to sync changes for static rows, give me a sec.
Faced that one, too.
","22/Apr/13 17:11;ifesdjeen;Ben, I've been putting lastRowKey in a wrong place, now it's after get_range_slices occuring, which should be correct.

I've had same exact stack trace.
Hope that solves issue for you.","22/Apr/13 17:59;budlight;still not working for me.  I can't really tell what is going on due to the magic thrift import line 

bq. import org.apache.cassandra.thrift.*;","22/Apr/13 18:15;ifesdjeen;Hm... That's quite weird. 
Maybe there's something different with Thrift.

I've seen people having trouble because of the message size, too, though. There're two settings, one for framed and one non-framed thrift.",22/Apr/13 18:18;budlight;relevant changes since 1.1.9 https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=blobdiff;f=src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java;h=dfeacc39a6ba49d7eaf6336251875e5b55528bb0;hp=a40e6c56c1fd0b52c482832d04e73387db001699;hb=4feb87d37544b9fde722786555475f2f790059ca;hpb=73d828e4e8023b9f7ca8fafd12becec34eb59211,"22/Apr/13 18:31;budlight;actually the diff between 1.1.11 and 1.1.9 is very simple 

bq. git diff cassandra-1.1.9 cassandra-1.1.11 -- src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java

{code}
-            TTransport transport = ConfigHelper.getInputTransportFactory(conf).openTransport(socket);
-            TBinaryProtocol binaryProtocol = new TBinaryProtocol(transport);
+            TTransport transport = ConfigHelper.getInputTransportFactory(conf).openTransport(socket, conf);
+            TBinaryProtocol binaryProtocol = new TBinaryProtocol(transport, ConfigHelper.getThriftMaxMessageLength(conf));
{code}

that might be the difference","22/Apr/13 18:36;budlight;reverting that change to TBinaryProtocol fixes it, now the question is do I just have something setup wrong since ConfigHelper.getThriftMaxMessageLength obviously returns some really low value","22/Apr/13 18:52;ifesdjeen;You can configure it through the ConfigHelper.setThriftMaxMessageLength(), can you execute java code there? (e.q. not only pig queries)","22/Apr/13 18:56;ifesdjeen;However, unfortunately, when using it with Cascading, I still get eternal iterations :/ so it'd still be good if someone could take a look at the patch :/","22/Apr/13 19:00;budlight;yeah but i'm using pig, which means I shouldn't have to redo the whole hadoop backend to use pig",22/Apr/13 19:10;budlight;I might be seeing the same bug you originally reported now that the job has started. I can't help but wonder if the reason none of this works is because the main codebase is now in DSE and its all modified to work with DSE and not with hadoop.   ,"24/Apr/13 19:48;lannyripple;Instead of reverting the changes to TBinaryProtocol you probably need to use ConfigHelper to set the thrift_framed-transport_size_in_mb and thrift_max_message_length_in_mb to much larger values (if ConfigHelper is exposed for you).  These values, prior to 1.10, were ignored (and a later version fixed a bug with getting them from ConfigHelper as well).  Setting the values to 2047 and 2048 respectively got us working again.

Oleksandr -- patch2 works for us.  Thanks!","25/Apr/13 22:47;jbellis;Thanks for the patch, Oleksandr.

It looks to me like the root of the problem is that {{key.put(this.getCurrentKey())}} destructively modifies currentKey.  Attached is a patch to duplicate the buffer first.

This has the added benefit that we don't have to impose any overhead on the new mapreduce api to solve this problem in the old mapred one.","25/Apr/13 23:00;jbellis;While investigating whether this was also a problem in 1.1, I found that this was fixed for 1.1.7 in CASSANDRA-4834, with the same .duplicate() solution, but not merged forward.  I've applied this fix to the 1.2 branch.","29/Jun/13 20:55;ifesdjeen;Can anyone confirm if 1.2.4 contains the fix, too?
It seems to work, it's just not clear wether fix made it there or it's just a coincidence...

UPDATE: sorry, I've tested against 1.2.5, so nevermind :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setting bloom filter fp chance to 1.0 causes ClassCastExceptions,CASSANDRA-5900,12664366,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jblangston@datastax.com,jblangston@datastax.com,19/Aug/13 14:56,12/Mar/19 14:02,13/Mar/19 22:29,19/Aug/13 23:24,1.2.9,,,Legacy/Tools,,,0,,,,,"In 1.2, we introduced the ability to turn bloom filters off completely by setting fp chance to 1.0.  It looks like there is a bug with this though. When it's set to 1.0 the following errors occur because AlwaysPresentFilter is not present in the switch statement here at https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/utils/FilterFactory.java#L91, and we default to Murmur3BloomFilter for an unknown type.

Exception in thread ""main"" java.lang.ClassCastException: org.apache.cassandra.utils.AlwaysPresentFilter cannot be cast to org.apache.cassandra.utils.Murmur3BloomFilter
at org.apache.cassandra.utils.FilterFactory.serializedSize(FilterFactory.java:91)
at org.apache.cassandra.io.sstable.SSTableReader.getBloomFilterSerializedSize(SSTableReader.java:531)
at org.apache.cassandra.metrics.ColumnFamilyMetrics$15.value(ColumnFamilyMetrics.java:273)
at org.apache.cassandra.metrics.ColumnFamilyMetrics$15.value(ColumnFamilyMetrics.java:268)
at org.apache.cassandra.db.ColumnFamilyStore.getBloomFilterDiskSpaceUsed(ColumnFamilyStore.java:1825)

",,,,,,,,,,,,,,,,,,,,,,,,19/Aug/13 19:18;jbellis;5900.txt;https://issues.apache.org/jira/secure/attachment/12598808/5900.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-19 19:18:03.711,,,no_permission,,,,,,,,,,,,344367,,,Mon Aug 19 23:24:19 UTC 2013,,,,,,0|i1ndwn:,344667,,,,,,,,yukim,yukim,,,,,,,,,,"19/Aug/13 19:18;jbellis;Patch attached.

Note that this only affects ""nodetool cfstats"" as well as calling the affected method directly; no internal code calls this method.",19/Aug/13 20:57;yukim;+1,19/Aug/13 23:24;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect Syntax for Timestamp in Select query,CASSANDRA-5620,12651414,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,tariqrahiman,tariqrahiman,06/Jun/13 18:36,12/Mar/19 14:02,13/Mar/19 22:29,23/Aug/13 17:29,,,,Legacy/Documentation and Website,,,0,,,,,"In the page http://cassandra.apache.org/doc/cql3/CQL.html under the heading Queries --> SELECT --> <where-clause>  We have a table posts followed by a Query which doesnt seem to work against the Cassandra Server. 
Timestamp requires quotes in the WHERE clause.

CREATE TABLE posts (
    userid text,
    blog_title text,
    posted_at timestamp,
    entry_title text,
    content text,
    category int,
    PRIMARY KEY (userid, blog_title, posted_at)
)

Incorrect
SELECT entry_title, content FROM posts WHERE userid='john doe' AND blog_title='John's Blog' AND posted_at >= 2012-01-01 AND posted_at < 2012-01-31

I get the the error mismatched character '<EOF>' expecting '''

If I correc the error in Johns''Blog and proceed I get missing EOF at '-01'

Correct
SELECT entry_title, content FROM posts WHERE userid='john doe' AND blog_title='John''s Blog' AND posted_at >= '2012-01-01' AND posted_at < '2012-01-31'",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-08-23 17:29:06.502,,,no_permission,,,,,,,,,,,,331740,,,Fri Aug 23 17:29:06 UTC 2013,,,,,,0|i1l8b3:,332071,,,,,,,,,,,,,,,,,,,23/Aug/13 17:29;iamaleksey;Fixed on Aug 10 by 2a8379372e6cbcd2fa3008b203c70f57ad29f392,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix AsyncOneResponse,CASSANDRA-5690,12654369,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,ash2k,ash2k,ash2k,23/Jun/13 07:54,12/Mar/19 14:02,13/Mar/19 22:29,22/Jul/13 17:12,2.0.0,,,,,,0,,,,,"Current implementation of AsyncOneResponse suffers from two problems:
1. Spurious wakeup will lead to TimeoutException being thrown because awaiting for condition is not done in loop;
2. condition.signal() is used where .signalAll() should be used - this leads to only one thread blocked on .get() to be unblocked. Other threads will stay blocked forever.",,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5623,,23/Jun/13 07:55;ash2k;trunk-5690.patch;https://issues.apache.org/jira/secure/attachment/12589300/trunk-5690.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-18 18:48:09.1,,,no_permission,,,,,,,,,,,,334646,,,Mon Jul 22 17:12:22 UTC 2013,,,,,,0|i1lq5j:,334972,,,,,,,,jbellis,jbellis,,,,,,,,,,"18/Jul/13 18:48;jbellis;This doesn't work; if one thread is blocked in get(), the synchronization prevents responses from being accepted.","19/Jul/13 02:30;ash2k;No, that is not true.

get() obtains the lock, then do TimeUnit.NANOSECONDS.timedWait() [1] that uses Object.wait() [2] internally.
From it's javadocs:
{quote}
 The current thread must own this object's monitor. The thread releases ownership of this monitor and waits until either of the following two conditions has occurred:

    Another thread notifies threads waiting on this object's monitor to wake up either through a call to the notify method or the notifyAll method.
    The timeout period, specified by timeout milliseconds plus nanos nanoseconds arguments, has elapsed. 

The thread then waits until it can re-obtain ownership of the monitor and resumes execution. 
{quote}

So, any number of threads blocked in get() are not preventing response() from setting the result.

p.s. Current implementation uses the same pattern but just doing it wrong.

[1]: https://docs.oracle.com/javase/6/docs/api/java/util/concurrent/TimeUnit.html#timedWait%28java.lang.Object,%20long%29
[2]: https://docs.oracle.com/javase/6/docs/api/java/lang/Object.html#wait%28long,%20int%29","22/Jul/13 14:22;jbellis;I see.

Why not just use {{this}} instead of a separate Object?  Are you sure that {{done}} doesn't need to be volatile?","22/Jul/13 15:47;ash2k;Pros for using ""this"":
- smaller object's memory footprint;

Cons:
- exposes synchronization i.e. breaks incapsulation of synchronization mechanics. This can possibly result in unexpected problems/interference if some other code tries to use this instance as it's own synchronization object - synchronized (AsyncOneResponse instance). See [1] for a bit more details.

IMO in this case it's better to hide synchronization details for safety reasons. But I can change patch if you think otherwise.

""done"" doesn't need to be volatile because all access to this field is synchronized using lock. So this lock guarantees both mutual exclusion of concurrent response() method calls (prevents race on setting ""result"" and ""done"") and visibility by establishing happens-before between lock release (finish of response() method) and subsequent lock acquire (start of get() method and exit from corresponding Object.wait()). From [2]:

{quote}
Synchronization is built around an internal entity known as the intrinsic lock or monitor lock. (The API specification often refers to this entity simply as a ""monitor."") Intrinsic locks play a role in both aspects of synchronization: enforcing exclusive access to an object's state and establishing happens-before relationships that are essential to visibility.

Every object has an intrinsic lock associated with it. By convention, a thread that needs exclusive and consistent access to an object's fields has to acquire the object's intrinsic lock before accessing them, and then release the intrinsic lock when it's done with them. A thread is said to own the intrinsic lock between the time it has acquired the lock and released the lock. As long as a thread owns an intrinsic lock, no other thread can acquire the same lock. The other thread will block when it attempts to acquire the lock.

When a thread releases an intrinsic lock, a happens-before relationship is established between that action and any subsequent acquistion of the same lock.
{quote}

For a similar code and more details see [3]. You may also want to read the famous great book Java Concurrency in Practice [4].

[1]: http://stackoverflow.com/questions/442564/avoid-synchronizedthis-in-java
[2]: http://docs.oracle.com/javase/tutorial/essential/concurrency/locksync.html
[3]: http://docs.oracle.com/javase/tutorial/essential/concurrency/guardmeth.html
[4]: http://www.amazon.com/exec/obidos/ASIN/0321349601",22/Jul/13 17:12;jbellis;Sounds good.  Committed a synchronized (this) version; we don't need to be super defensive about things since we ourselves are the only use case we care about.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageProxy.truncateBlocking sends truncation messages to fat client hosts; making truncation timeout,CASSANDRA-6088,12670274,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,m0nstermind,m0nstermind,m0nstermind,24/Sep/13 13:41,12/Mar/19 14:02,13/Mar/19 22:29,24/Sep/13 15:14,1.2.11,2.0.2,,,,,0,,,,,"We have fat clients on our cluster. Trying to truncate CF gives timeout. This is because fat client dont respond to Truncation messages.

Fixed by sending Truncation only to storage endpoints.
",,,,,,,,,,,,,,,,,,,,,,,,24/Sep/13 13:42;m0nstermind;TruncateBlocking.txt;https://issues.apache.org/jira/secure/attachment/12604796/TruncateBlocking.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-24 15:04:30.123,,,no_permission,,,,,,,,,,,,350103,,,Tue Sep 24 15:14:46 UTC 2013,,,,,,0|i1od73:,350397,,,,,,,,jbellis,jbellis,,,,,,,,,,"24/Sep/13 15:04;jbellis;Patch does not apply to 1.2 or 2.0 or trunk, can you rebase?",24/Sep/13 15:05;jbellis;May actually be your gratuitous import reformatting that's the problem; suggest matching http://wiki.apache.org/cassandra/CodeStyle,"24/Sep/13 15:14;jbellis;rebased, fixed code style, and committed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't announce schema version until we've loaded the changes locally,CASSANDRA-5904,12664589,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,20/Aug/13 10:21,12/Mar/19 14:02,13/Mar/19 22:29,20/Aug/13 14:49,1.2.9,,,,,,0,,,,,"Currently, we call updateSchemaAndAnnounce (which sets the schema version in the system table and announce it on gossip) in DefsTable.mergeSchema as soon as we've persisted the schema mutation but *before* we've actually loaded new/updated KS/CF locally. This makes it impossible to reliably check for schema agreement in a cluster as even if all nodes have the same version set in the system tables, you could still get an insert on a new table rejected because the table hasn't been loaded yet.",,,,,,,,,,,,,,,,,,,,,,,,20/Aug/13 10:23;slebresne;5904.txt;https://issues.apache.org/jira/secure/attachment/12598929/5904.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-20 14:01:59.024,,,no_permission,,,,,,,,,,,,344532,,,Tue Aug 20 14:49:11 UTC 2013,,,,,,0|i1nex3:,344832,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,20/Aug/13 10:23;slebresne;Attaching trivial patch that moves the call to updateVersionAndAnnounce at the end of mergeSchema.,20/Aug/13 14:01;iamaleksey;+1,"20/Aug/13 14:49;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory leak when using snapshot repairs,CASSANDRA-6047,12669133,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,jblangston@datastax.com,jblangston@datastax.com,17/Sep/13 21:47,12/Mar/19 14:02,13/Mar/19 22:29,18/Sep/13 19:27,1.2.10,2.0.1,,,,,0,,,,,"Running nodetool repair repeatedly with the -snapshot parameter results in a native memory leak. The JVM process will take up more and more physical memory until it is killed by the Linux OOM killer.

The command used was as follows:

nodetool repair keyspace -local -snapshot -pr -st start_token -et end_token

Removing the -snapshot flag prevented the memory leak.  The subrange repair necessitated multiple repairs, so it made the problem noticeable, but I believe the problem would be reproducible even if you ran repair repeatedly without specifying a start and end token.

Notes from [~yukim]:

Probably the cause is too many snapshots. Snapshot sstables are opened during validation, but memories used are freed when releaseReferences called. But since snapshots never get marked compacted, memories never freed.

We only cleanup mmap'd memories when sstable is mark compacted. https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/io/sstable/SSTableReader.java#L974

Validation compaction never marks snapshots compacted.
",,,,,,,,,,,,,,,,,,,,,,,,18/Sep/13 16:05;yukim;6047-1.2.txt;https://issues.apache.org/jira/secure/attachment/12603851/6047-1.2.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-18 16:05:17.966,,,no_permission,,,,,,,,,,,,349065,,,Wed Sep 18 19:27:25 UTC 2013,,,,,,0|i1o6tb:,349363,,,,,,,,jbellis,jbellis,,,,,,,,,,18/Sep/13 16:05;yukim;Patch attached to let SSTableReader implement Closeable and do clean up at #close. Validation compaction against snapshot calls #close at the end of validation.,18/Sep/13 17:52;jbellis;+1,18/Sep/13 19:27;yukim;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CqlStorage loading compact table adds an extraneous field to the pig schema,CASSANDRA-6071,12669696,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,beobal,beobal,beobal,20/Sep/13 17:07,12/Mar/19 14:01,13/Mar/19 22:29,26/Sep/13 18:54,1.2.11,2.0.2,,,,,0,,,,,"{code}
CREATE TABLE t (
  key text,
  field1 int,
  field2 int
  PRIMARY KEY (key, field1)
) WITH COMPACT STORAGE;

INSERT INTO t (key,field1,field2) VALUES ('key1',1,2);
INSERT INTO t (key,field1,field2) VALUES ('key2',1,2);
INSERT INTO t (key,field1,field2) VALUES ('key3',1,2);
{code}

{code}
grunt> t = LOAD 'cql://ks/t' USING CqlStorage();
grunt> describe t;                                 
t: {key: chararray,field1: int,field2: int,value: int}
dump t;
(key1,1,2,)
(key3,1,2,)
(key2,1,2,)
{code}
",,,,,,,,,,,,,,,,,,,,,,,,24/Sep/13 16:45;beobal;6071-2.txt;https://issues.apache.org/jira/secure/attachment/12604830/6071-2.txt,26/Sep/13 18:40;beobal;6071-3.txt;https://issues.apache.org/jira/secure/attachment/12605302/6071-3.txt,20/Sep/13 17:08;beobal;6071.txt;https://issues.apache.org/jira/secure/attachment/12604269/6071.txt,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-09-23 18:26:29.482,,,no_permission,,,,,,,,,,,,349628,,,Thu Sep 26 18:54:56 UTC 2013,,,,,,0|i1oaaf:,349926,,,,,,,,alexliu68,alexliu68,,,1.2.9,,,,,,,"20/Sep/13 17:08;beobal;This is caused by an additional ColumnDef being added in the CfDef used to construct the Pig schema. Where a value_alias is specified for the compact value column, CqlStorage.getKeysMeta adds a ColumnDef for it. The additional one is then added in AbstractCassandaStorage.getColumnMeta, so I've added an additional boolean arg to indicate whether the the value_alias has already been processed. I've also refactored ACS.getColumnMeta a bit to (hopefully) make the logic clearer. 
","23/Sep/13 18:26;alexliu68;+1 except that I notice the patch is not based on current cassandra-1.2 branch, so it needs be updated to cassandra-1.2 branch.","24/Sep/13 16:45;beobal;Sorry about that, updated patch attached",24/Sep/13 16:56;alexliu68;+1,26/Sep/13 16:05;brandon.williams;Can you rebase [~beobal]? We committed a few pig things yesterday and it no longer applies :(,26/Sep/13 18:40;beobal;attached rebased patch,"26/Sep/13 18:54;brandon.williams;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use non-pooling readers with openForBatch,CASSANDRA-6067,12669535,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,19/Sep/13 20:11,12/Mar/19 14:01,13/Mar/19 22:29,19/Sep/13 21:56,2.0.1,,,Legacy/Tools,,,0,,,,,Looks like CASSANDRA-5555 was incorrectly merged forward.,,,,,,,,,,,,,,,,,,,,,,,,19/Sep/13 20:29;jbellis;6067-v2.txt;https://issues.apache.org/jira/secure/attachment/12604114/6067-v2.txt,19/Sep/13 20:12;jbellis;6067.txt;https://issues.apache.org/jira/secure/attachment/12604110/6067.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-09-19 20:22:08.027,,,no_permission,,,,,,,,,,,,349467,,,Thu Sep 19 21:56:39 UTC 2013,,,,,,0|i1o9an:,349765,,,,,,,,yukim,yukim,,,2.0.0,,,,,,,"19/Sep/13 20:22;yukim;BufferedSegmentedFile extends PoolingSegmentedFile. Still seeing the following error when bulk load:

{code}
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:36)
	at org.apache.cassandra.io.util.SegmentedFile$SegmentIterator.next(SegmentedFile.java:161)
	at org.apache.cassandra.io.util.SegmentedFile$SegmentIterator.next(SegmentedFile.java:142)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:904)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:839)
	at org.apache.cassandra.io.sstable.SSTableReader.getPositionsForRanges(SSTableReader.java:751)
	at org.apache.cassandra.io.sstable.SSTableLoader$1.accept(SSTableLoader.java:122)
	at java.io.File.list(File.java:1087)
	at org.apache.cassandra.io.sstable.SSTableLoader.openSSTables(SSTableLoader.java:73)
	at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:155)
	at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:66)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.config.DatabaseDescriptor.getFileCacheSizeInMB(DatabaseDescriptor.java:1145)
	at org.apache.cassandra.service.FileCacheService.<clinit>(FileCacheService.java:41)
	... 11 more
{code}","19/Sep/13 20:29;jbellis;v2 fixes BSF to not extend PSF.

(Checked and this is fine in 1.2.)",19/Sep/13 20:36;yukim;+1,19/Sep/13 21:56;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE during repair,CASSANDRA-5806,12659929,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,hsn,hsn,25/Jul/13 23:33,12/Mar/19 14:01,13/Mar/19 22:29,07/Aug/13 18:18,2.0 rc1,,,,,,0,,,,," INFO 01:06:00,656 Connecting to /10.0.0.3 for streaming
 INFO 01:06:00,656 Connecting to /10.0.0.3 for streaming
ERROR 01:06:05,828 Streaming error occurred
java.lang.NullPointerException
        at org.apache.cassandra.streaming.ConnectionHandler.sendMessage(Connecti
onHandler.java:175)
        at org.apache.cassandra.streaming.StreamSession.maybeCompleted(StreamSes
sion.java:600)
        at org.apache.cassandra.streaming.StreamSession.prepare(StreamSession.ja
va:446)
        at org.apache.cassandra.streaming.StreamSession.messageReceived(StreamSe
ssion.java:357)
        at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandl
er.run(ConnectionHandler.java:294)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-08-07 18:18:18.139,,,no_permission,,,,,,,,,,,,340121,,,Wed Aug 07 18:18:18 UTC 2013,,,,,,0|i1mnun:,340439,,,,,,,,,,,,2.0 beta 2,,,,,,,"07/Aug/13 18:18;yukim;NPE was due to the wrong ordering of messaging and was fixed in commit 931be4803b1f21dc1605612364df128dce74a6ef.
If you see the error again in upcoming RC1 release, feel free to reopen the ticket.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support pre-1.2 release CQL3 tables in CqlPagingRecordReader,CASSANDRA-5800,12659618,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,alexliu68,alexliu68,24/Jul/13 16:54,12/Mar/19 14:01,13/Mar/19 22:29,13/Aug/13 21:30,1.2.9,,,,,,0,,,,,Pre-1.2 release CQL3 table stores the key in system.schema_columnfamilies key_alias column which is different from 1.2 release. We should support it in CqlPagingRecordReader as well.,,,,,,,,,,,,CASSANDRA-5803,,,,,,,,,,,,24/Jul/13 17:06;alexliu68;5800-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12593980/5800-1.2-branch.txt,29/Jul/13 19:29;alexliu68;5800-2-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12594770/5800-2-1.2-branch.txt,12/Aug/13 10:45;iamaleksey;5800-v3-1.2.txt;https://issues.apache.org/jira/secure/attachment/12597448/5800-v3-1.2.txt,12/Aug/13 10:47;iamaleksey;5800-v3-2.0.txt;https://issues.apache.org/jira/secure/attachment/12597449/5800-v3-2.0.txt,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-07-24 23:52:24.904,,,no_permission,,,,,,,,,,,,339811,,,Tue Aug 13 21:30:49 UTC 2013,,,,,,0|i1mlxz:,340130,,,,,,,,thobbs,thobbs,,,,,,,,,,24/Jul/13 17:06;alexliu68;Patch on 1.2 branch is attached,24/Jul/13 23:52;jbellis;I don't understand what the goal is here.  CPRR is only part of 1.2.,"25/Jul/13 00:26;alexliu68;If the user upgrades from pre-1.2 release to 1.2 release, any CQL3 table created from pre-1.2 release doesn't work for CqlPagingRecordReader because it stores the key name in key_alias column instead of key_aliases column. The fix helps the upgrading.
",25/Jul/13 01:14;jbellis;Sounds like the right fix is to propagate the column rename e.g. in SystemTable.upgradeSystemData. Other parts of the system need that b/s the RR.,29/Jul/13 19:29;alexliu68;5800-2-1.2-branch.txt is attached to update the system.schema_columnfamilies settings for pre-1.2 CQL3 tables,"09/Aug/13 20:23;thobbs;The patch looks good except for a minor nit: I would rename {{upgradeCQL3tables()}} to {{migrateKeyAlias()}}, or something along those lines.",09/Aug/13 20:25;jbellis;I'll tweak that on commit.,09/Aug/13 20:57;jbellis;Committed (to 1.2 only; 2.0 requires upgrading from 1.2.9 already),"09/Aug/13 22:22;iamaleksey;Huh, let me bikeshed a little as well:
- catching NPE there is bad, idiomatically we check row.has(String column) first instead
- (key_alias != null && key_aliases == null) is not comprehensive enough. if key_alias is null, we still want to set key_aliases (to '[]'). so it should be if (key_aliases == null) instead.
- with the above in place, all the special-casing for key_aliases being null and all the references to key_alias should be dropped from the 2.0 codebase entirely, assuming that we already mandate 1.2.9 for upgraders","09/Aug/13 22:33;iamaleksey;Also, if you detect key_alias, you should probably ditch it after the transformation (do a DELETE key_alias etc.)",09/Aug/13 23:21;iamaleksey;Will attach a v3.,"12/Aug/13 16:37;thobbs;Except for the cql3handling.py portion of the 2.0 patch (which you're already aware of), +1.","12/Aug/13 17:23;iamaleksey;Thanks, committed.","13/Aug/13 15:28;iamaleksey;The migration, currently, only runs on startup - and that doesn't cover the case of, for example, 1.2.8- -> 1.2.9+ -> 2.0 migration without a 1.2.9+ -> 1.2.9+ restart (you are going to have the 1.2.8- schema rows carried verbatim into 2.0).

We should perform this migration after each schema pull instead.
",13/Aug/13 21:30;iamaleksey;Fixed by https://github.com/apache/cassandra/commit/82081c7b4b2ad2d602374f0817abc4cfc2c86c98,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default sstable output size not respected when running sstablesplit,CASSANDRA-6028,12668519,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,dmeyer,dmeyer,13/Sep/13 17:57,12/Mar/19 14:01,13/Mar/19 22:29,13/Sep/13 18:54,,,,,,,0,,,,,"Run sstablesplit -h:

usage: sstablessplit [options] <filename> [<filename>]*
--
Split the provided sstables files in sstables of maximum provided file
size (see option --size).
--
Options are:
    --debug         display stack traces
 -h,--help          display this help message
    --no-snapshot   don't snapshot the sstables before splitting
 -s,--size <size>   maximum size in MB for the output sstables (default:
                    50)
 -v,--verbose       verbose output

From the help message we expect the default size to be 50 MB.

repro steps:

supply any sstable of any size to sstablesplit but leave out the --size option.
Expected: should split sstable into 50MB sstables
Actual:
Pre-split sstables snapshotted into snapshot pre-split-1379086801768
Error splitting SSTableReader(path='./Keyspace1/Standard1/Keyspace1-Standard1-ic-15-Data.db'): Invalid target size for SSTables, must be > 0 (got: 0)","java version ""1.7.0_40""
Java(TM) SE Runtime Environment (build 1.7.0_40-b43)
Java HotSpot(TM) 64-Bit Server VM (build 24.0-b56, mixed mode)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-09-13 18:54:49.899,,,no_permission,,,,,,,,,,,,348453,,,Fri Sep 13 18:54:49 UTC 2013,,,,,,0|i1o31b:,348750,1.2.9,2.0.0,,,,,,,,,,,,,,,,dmeyer,13/Sep/13 18:54;brandon.williams;Fixed in b281dd1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shuffle fails to write to system.range_xfers,CASSANDRA-5465,12642366,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,dbrosius,dbrosius,13/Apr/13 07:46,12/Mar/19 14:01,13/Mar/19 22:29,13/Apr/13 21:27,2.0 beta 1,,,,,,0,,,,,"On trunk, only, executing

INSERT INTO system.range_xfers (token_bytes, requested_at) VALUES ('5e29cbe30503a9ec', 'now');

fails with

Exception in thread ""main"" java.lang.RuntimeException: InvalidRequestException(why:Invalid STRING constant (5e29cbe30503a9ec) for token_bytes of type blob)
	at org.apache.cassandra.tools.Shuffle.executeCqlQuery(Shuffle.java:516)
	at org.apache.cassandra.tools.Shuffle.shuffle(Shuffle.java:359)
	at org.apache.cassandra.tools.Shuffle.main(Shuffle.java:681)


patch validates blob->string
",,,,,,,,,,,,,,,,,,,,,,,,13/Apr/13 07:47;dbrosius;5465.txt;https://issues.apache.org/jira/secure/attachment/12578588/5465.txt,13/Apr/13 16:54;dbrosius;5465_2.txt;https://issues.apache.org/jira/secure/attachment/12578604/5465_2.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-04-13 13:12:54.76,,,no_permission,,,,,,,,,,,,322780,,,Sat Apr 13 21:27:34 UTC 2013,,,,,,0|i1jovj:,323125,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"13/Apr/13 13:12;iamaleksey;This fix is wrong. BLOB is not there for a reason, and the reason is that in 2.0 'cafe' syntax for blobs is no longer valid. You should fix shuffle instead to use 0xcafe syntax (and do that against 1.2 as well, to avoid the deprecation warning in Cassandra logs).",13/Apr/13 15:07;urandom;So we're introducing a breaking (non-backward compatible) change?,"13/Apr/13 15:14;iamaleksey;It was introduced back in 1.2.2. In 1.2 both versions will continue to coexist, so you are able to upgrade. See CASSANDRA-5198 and Jake's comment - https://issues.apache.org/jira/browse/CASSANDRA-5198?focusedCommentId=13569019&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13569019.",13/Apr/13 15:27;urandom;So we introduced a breaking change.,"13/Apr/13 16:34;dbrosius;So Aleksey, i take it you are saying that Shuffle needs to use prepared statements so that the token can be sent as a ByteBuffer (byte[]) ? ","13/Apr/13 16:42;jbellis;bq. So we introduced a breaking change.

My understanding is that we did not: the old format continues to work.  What we did do was allow a new format that previously would have errored, and deprecated blob-strings for removal in 2.0.","13/Apr/13 16:54;dbrosius;oh i see, remove ' 's, and add 0x... done


patch on 1.2 -- 5465_2.txt","13/Apr/13 17:13;urandom;bq. My understanding is that we did not: the old format continues to work. What we did do was allow a new format that previously would have errored, and deprecated blob-strings for removal in 2.0.

Right, that's what I meant, we introduced a breaking change, 1.2 -> 2.0.  CQL 4, then?",13/Apr/13 18:29;iamaleksey;+1,13/Apr/13 21:27;dbrosius;added commit to cassandra-1.2 as commit a25ac14e0d245f648e97f888ee4b64acc50c76d1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
isRunning flag set prematurely in org.apache.cassandra.transport.Server,CASSANDRA-5467,12642448,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,jsanda,jsanda,14/Apr/13 15:07,12/Mar/19 14:01,13/Mar/19 22:29,26/Apr/13 15:21,1.2.5,,,,,,0,jmx,server,,,"In org.apache.cassandra.transport.Server, the start() method sets the isRunning flag before calling the run() method. In the event of an initialization error like a port conflict an exception will be thrown at line 136 which is,

    Channel channel = bootstrap.bind(socket);

It seems like it might make more sense to set the isRunning flag after binding to the socket. I have a tool that deploys a node and then verifies it is ready to receive CQL requests. I do this via JMX. Unless I use a delay before making that check, the JMX call will return true even though there is a port conflict. 

",,,,,,,,,,,,,,,,,,,,,,,,24/Apr/13 08:30;slebresne;5467.txt;https://issues.apache.org/jira/secure/attachment/12580256/5467.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-24 08:30:20.743,,,no_permission,,,,,,,,,,,,322862,,,Fri Apr 26 15:21:47 UTC 2013,,,,,,0|i1jpdj:,323207,,,,,,,,krummas,krummas,,,,,,,,,,"24/Apr/13 08:30;slebresne;That's definitively not unreasonable. Initially, isRunning was set first so we don't start the server twice if the {{start()}} method was called twice. But tbh it's not really useful a protection so attaching a trivial patch that just move setting isRunning once the server is indeed started.",25/Apr/13 12:50;krummas;lgtm,"26/Apr/13 15:21;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CqlPagingRecordReader should quote table and column identifiers,CASSANDRA-5763,12657767,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,pkolaczk,pkolaczk,pkolaczk,15/Jul/13 16:24,12/Mar/19 14:01,13/Mar/19 22:29,15/Jul/13 22:37,1.2.7,,,,,,0,,,,,"Using CPIF on table with uppercase name or with uppercase column names doesn't work (""unconfigured table"" message).",,,,,,,,,,,,,,,,,,,,,,,,29/Jul/13 21:33;alexliu68;5763-2-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12594797/5763-2-1.2-branch.txt,15/Jul/13 18:27;pkolaczk;DSP-2292.patch;https://issues.apache.org/jira/secure/attachment/12592368/DSP-2292.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-07-15 22:37:58.738,,,no_permission,,,,,,,,,,,,337987,,,Mon Jul 29 21:42:11 UTC 2013,,,,,,0|i1mapr:,338309,,,,,,,,jbellis,jbellis,,,,,,,,,,15/Jul/13 16:40;pkolaczk;Related DSE issue: https://datastax.jira.com/browse/DSP-2292,15/Jul/13 22:37;jbellis;LGTM.  Rebased for you and committed.,29/Jul/13 21:33;alexliu68;5763-2-1.2-branch.txt patch is to fix a small bug at CqlPagingRecordReader and add case sensitive to CqlRecordWriter as well.,"29/Jul/13 21:42;jbellis;Ah, I see.  Go ahead and reopen the other issue for this then, since it will be committed to 1.2.9.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null pointer exception in custom secondary indexes,CASSANDRA-6498,12685142,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mafernandez,adelapena,adelapena,17/Dec/13 10:24,12/Mar/19 14:01,13/Mar/19 22:29,15/Jan/14 18:09,2.1 beta1,,,Feature/2i Index,,,2,2i,secondary_index,secondaryIndex,,"StorageProxy#estimateResultRowsPerRange raises a null pointer exception when using a custom 2i implementation that not uses a column family as underlying storage:
{code}
resultRowsPerRange = highestSelectivityIndex.getIndexCfs().getMeanColumns();
{code}
According to the documentation, the method SecondaryIndex#getIndexCfs should return null when no column family is used.",,,,,,,,,,,,,,,,,,,,,,,,14/Jan/14 09:13;mafernandez;CASSANDRA-6498.patch;https://issues.apache.org/jira/secure/attachment/12622822/CASSANDRA-6498.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-14 09:13:29.681,,,no_permission,,,,,,,,,,,,364219,,,Wed Jan 15 18:09:28 UTC 2014,,,,,,0|i1qs2n:,364519,2.1 rc3,,,,,,,beobal,beobal,,,,,,,,,,"14/Jan/14 09:13;mafernandez;In order to avoid this null pointer exception, we shouldn't assume that highestSelectivityIndex (which is a SecondaryIndex) has a IndexCfs because that depends on the implementation type.
 
Therefore, a nice way to solve this issue would be to include an abstract method in the SecondaryIndex class, add a implementation of the method where we really know there is a IndexCfs and otherwise delegate the implementation of this method to those who are creating a custom 2i.

I submit a patch that implements this solution.",14/Jan/14 17:45;jbellis;WDYT [~beobal]?,"15/Jan/14 14:23;adelapena;I have been assigned to this bug, the provided patch seems to work fine.
Could the reviewer please review (and, hopefully, commit) the patch?

Thanks",15/Jan/14 14:36;aagea;+1,"15/Jan/14 14:45;beobal;+1 patch looks good to me. 

In fact, the same issue affects Solr backed indexes in DSE and our approach is the same.","15/Jan/14 17:35;mafernandez;+1

I've been running some tests with this patch and there were no errors.",15/Jan/14 18:09;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
line 297 of CQL.textile needs updated,CASSANDRA-6642,12692400,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,khahn,khahn,30/Jan/14 22:48,12/Mar/19 14:01,13/Mar/19 22:29,05/Feb/14 15:55,,,,Legacy/Documentation and Website,,,0,,,,," [the doc|https://github.com/apache/cassandra/blob/trunk/doc/cql3/CQL.textile#partition-key-and-clustering] says: 
{quote}
The restriction for table with COMPACT STORAGE is that they support one and only one column outside of the ones part of the PRIMARY KEY.
{quote}
Shouldn't it say, ... outside of the ones that are part of a compound primary key?","[CQL 3.1.4 doc, cassandra-trunk|https://github.com/apache/cassandra/blob/trunk/doc/cql3/CQL.textile#partition-key-and-clustering]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-02-05 15:55:27.182,,,no_permission,,,,,,,,,,,,370995,,,Wed Feb 05 15:55:27 UTC 2014,,,,,,0|i1rxtz:,371300,,,,,,,,,,,,,,,,,,,05/Feb/14 15:55;slebresne;That whole paragraph about COMPACT STORAGE is indeed a tad whack. I've pushed a hopefully better version of that paragraph that fixes the imprecision you mentioned but also extend a bit on the limitations. Feel free to complete if you think it's still not all that clear :),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix periodic flushing when encountering an empty memtable,CASSANDRA-5931,12665335,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,iamaleksey,iamaleksey,23/Aug/13 21:59,12/Mar/19 14:01,13/Mar/19 22:29,23/Aug/13 23:20,2.0.0,,,,,,0,,,,,"CASSANDRA-5241 broke it by making forceFlush() always return a valid future, never a null, and CASSANDRA-4237 was relying on that null check to determine cleanliness.",,,,,,,,,,,,,,,,,,,,,,,,23/Aug/13 22:00;iamaleksey;5931.txt;https://issues.apache.org/jira/secure/attachment/12599721/5931.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-23 22:24:28.857,,,no_permission,,,,,,,,,,,,345275,,,Fri Aug 23 23:20:28 UTC 2013,,,,,,0|i1njhz:,345576,2.0 rc2,,,,,,,jbellis,jbellis,,,,,,,,,,"23/Aug/13 22:24;jbellis;+1

Nit: it's probably time to add javadoc with @returns to forceFlush to help prevent this confusion","23/Aug/13 23:20;iamaleksey;Committed with the nit addressed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YamlFileNetworkTopologySnitch doesn't call reconnectViaPreferredAddress in onChange,CASSANDRA-5603,12650198,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,jjordan,jjordan,30/May/13 19:32,12/Mar/19 14:00,13/Mar/19 22:29,31/May/13 15:32,2.0 beta 1,,,,,,0,,,,,"I was looking into how EC2 Multi Region did its magic with the private/public ips for the local network.  YamlFileNetworkTopologySnitch looks like it tries to do the same thing, but it doesn't do the reconnectViaPreferredAddress in onChange.  Don't know if it is technically needed, but EC2MRS reconnects in onJoin, onChange, and onAlive.  While YamlFileNetworkTopologySnitch only does it in onJoin and onAlive.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-05-31 15:32:31.928,,,no_permission,,,,,,,,,,,,330525,,,Fri May 31 15:32:31 UTC 2013,,,,,,0|i1l0tz:,330859,,,,,,,,,,,,,,,,,,,"31/May/13 15:32;brandon.williams;I can't think of a scenario where this is actually needed, so I think it's EC2MRS that's being gratuitous, but I went ahead and added it to onChange since the cost is nothing if I'm right, and the behavior is correct if I'm wrong.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
init.d script not working under Ubuntu,CASSANDRA-6090,12670355,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,lr,lr,24/Sep/13 20:40,12/Mar/19 14:00,13/Mar/19 22:29,30/Sep/13 16:29,2.0.1,,,Packaging,,,0,,,,,"When installing the Cassandra package on Ubuntu, it starts up automatically without writing the PID file.
It renders the init.d script useless as it can't status or stop cassandra.

I submitted a PR on github to fix this:
https://github.com/apache/cassandra/pull/21",Ubuntu 12.04.2 LTS x64,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-09-30 16:29:52.421,,,no_permission,,,,,,,,,,,,350184,,,Mon Sep 30 16:29:52 UTC 2013,,,,,,0|i1odp3:,350478,2.0.1,,,,,,,,,,,,,,,,,,"30/Sep/13 16:29;urandom;Partially committed, (the fix for directory creation was committed as part of CASSANDRA-6101); Thanks Laurent!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CqlRecordWriter misses clusterClumns and partitionKeyColumns size issue for thrift tables,CASSANDRA-6002,12667934,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,alexliu68,alexliu68,alexliu68,10/Sep/13 22:17,12/Mar/19 14:00,13/Mar/19 22:29,12/Sep/13 18:26,1.2.10,,,,,,0,,,,,"For thrift tables, partitionKeyColumns size need be set to the right value and clusterColumns are missing",,,,,,,,,,,,,,,,,,,,,,,,10/Sep/13 22:18;alexliu68;6002.txt;https://issues.apache.org/jira/secure/attachment/12602437/6002.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-12 18:26:05.836,,,no_permission,,,,,,,,,,,,347870,,,Thu Sep 12 18:26:05 UTC 2013,,,,,,0|i1nzgf:,348167,,,,,,,,jbellis,jbellis,,,,,,,,,,12/Sep/13 18:26;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Debian init script broken,CASSANDRA-6101,12670650,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,awinter,awinter,26/Sep/13 07:12,12/Mar/19 14:00,13/Mar/19 22:29,04/Oct/13 13:45,2.0.2,,,,,,0,,,,,"The debian init script released in 2.0.1 contains 2 issues:

# The pidfile directory is not created if it doesn't already exist.
# Classpath not exported to the start-stop-daemon.

These lead to the init script not picking up jna.jar, or anything from the debian EXTRA_CLASSPATH environment variable, and the init script not being able to stop/restart Cassandra.",,,,,,,,,,,,,CASSANDRA-6240,,,,,,,,,,,30/Sep/13 16:13;urandom;6101-classpath.patch;https://issues.apache.org/jira/secure/attachment/12605920/6101-classpath.patch,26/Sep/13 07:14;awinter;6101.txt;https://issues.apache.org/jira/secure/attachment/12605201/6101.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-09-30 16:18:23.894,,,no_permission,,,,,,,,,,,,350479,,,Fri Oct 04 13:45:39 UTC 2013,,,,,,0|i1ofi7:,350772,,,,,,,,,,,,2.0.1,,,,,,,26/Sep/13 07:14;awinter;Attached patch with fixes.,"30/Sep/13 16:18;urandom;Good catch Anton.

I've applied the fix to ensure that the PID directory is created, but I think we can go a bit further with the classpath.

That function is a throwback to {{jsvc}} and duplicates the classpath construction we do elsewhere.  I've attached [6101-classpath.patch|https://issues.apache.org/jira/secure/attachment/12605920/6101-classpath.patch] which eliminates that function and uses {{EXTRA_CLASSPATH}} instead.

Can you give this a whirl and see if it (still) fixes the issues you were seeing?","03/Oct/13 05:28;awinter;Yes, that works as well.","03/Oct/13 12:23;pieterc;Not really important, but status isn't working with this patch..

root ~ # service cassandra status
 * could not access pidfile for Cassandra","03/Oct/13 14:49;urandom;bq. Yes, that works as well.

Thanks Anton; Committed","03/Oct/13 15:00;urandom;bq. Not really important, but status isn't working with this patch..

I'm not seeing that [~pieterc]; Which patch are you referring to, [6101.txt|https://issues.apache.org/jira/secure/attachment/12605201/6101.txt] or [6101-classpatch.patch|https://issues.apache.org/jira/secure/attachment/12605920/6101-classpath.patch]?

I wouldn't be surprised at this point to find there are more bugs with this, but only [6101.txt|https://issues.apache.org/jira/secure/attachment/12605201/6101.txt] should have had any impact on this, and it is definitely a change for the better.

When you see this error, does a PID file exist at {{/var/run/cassandra/cassandra.pid}}?  If so, what are the contents of the file?  Is Cassandra running, and if so, what is its PID (hint: try {{pgrep -f CassandraDaemon}})?

Could you attach the output of {{sh -x /etc/init.d/cassandra status}}?",04/Oct/13 01:56;awinter;That {{service cassandra status}} problem is resolved by CASSANDRA-6090,"04/Oct/13 13:45;urandom;bq. That service cassandra status problem is resolved by CASSANDRA-6090

I think so too; Closing this issue",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Installation shouldn't fail if /etc/sysctl.d/cassandra is deleted,CASSANDRA-6232,12675165,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,paravoid,paravoid,23/Oct/13 01:03,12/Mar/19 14:00,13/Mar/19 22:29,24/Oct/13 19:48,1.2.12,2.0.3,,Packaging,,,0,,,,,"The Debian package's postinst currently has this snippet code, under the ""configure"" action (i.e. what runs when installing or upgrading):
{code:none}
        if ! sysctl -p /etc/sysctl.d/cassandra.conf; then
            [...]
            rm -v /etc/sysctl.d/cassandra.conf
        fi
{code}

/etc/sysctl.d/cassandra.conf is a conffile and might be removed by the system administrator. The sysadmin might not want this sysctl setting or have an entirely different system of managing the /etc/sysctl.d hierarchy (in our case that would be puppet).

Since this piece of code doesn't check for the existence and doesn't use rm's ""-f"" argument, if the file doesn't exist the rm call fails and the package installation is aborted.

I'd propose checking for the file's existence instead of just ""sysctl -p"", so that you can avoid the nasty warnings too, but adding -f to rm shouldn't hurt either.

Note that this would probably fail on package upgrades under OpenVZ too, which according to the error message should be a supported configuration.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-10-24 19:48:38.185,,,no_permission,,,,,,,,,,,,354785,,,Thu Oct 24 20:35:31 UTC 2013,,,,,,0|i1p5xz:,355074,2.0.1,,,,,,,,,,,,,,,,,,"24/Oct/13 19:48;brandon.williams;I don't see any harm with adding -f, done in 1f4070dbf2","24/Oct/13 20:35;paravoid;-f works and is a good idea nevertheless, but the warnings mentioning OpenVZ are still echo'ed if the file doesn't exist, which can be confusing.

I'd suggest also making the
{code:none}
        if ! sysctl -p /etc/sysctl.d/cassandra.conf; then
{code}

line into
{code:none}
        if [ -r /etc/sysctl.d/cassandra.conf ] && ! sysctl -p /etc/sysctl.d/cassandra.conf; then
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Equals method in PermissionDetails causes StackOverflowException,CASSANDRA-5655,12653462,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,beobal,beobal,beobal,18/Jun/13 14:48,12/Mar/19 14:00,13/Mar/19 22:29,18/Jun/13 15:20,1.2.6,,,,,,0,,,,,"It simply delegates to Guava's Objects.equal, which itself ends up calling back to the original caller's equals after performing some basic checks.
",,,,,,,,,,,,,,,,,,,,,,,,18/Jun/13 14:50;beobal;5655.txt;https://issues.apache.org/jira/secure/attachment/12588375/5655.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-18 15:20:20.589,,,no_permission,,,,,,,,,,,,333740,,,Tue Jun 18 15:20:20 UTC 2013,,,,,,0|i1lklr:,334068,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,18/Jun/13 14:50;beobal;patch against 1.2 branch attached,"18/Jun/13 15:20;iamaleksey;D-oh. It's correct in AuthenticatedUser and DataResource, but somehow slipped in PermissionDetails.
Committed with slight cosmetic modification for consistency with other equals() methods in auth classes.
Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use correct partitioner in AbstractViewSSTableFinder,CASSANDRA-6734,12695984,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,19/Feb/14 15:25,12/Mar/19 14:00,13/Mar/19 22:29,19/Feb/14 16:02,2.0.6,,,,,,0,,,,,"I don't think this breaks anything yet since we don't do range queries against index tables, but fixing it is a prereq for doing so (CASSANDRA-4476).",,,,,,,,,,,,,,,,,,,,,,,,19/Feb/14 15:25;jbellis;6734.txt;https://issues.apache.org/jira/secure/attachment/12629784/6734.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-19 15:51:43.87,,,no_permission,,,,,,,,,,,,374462,,,Wed Feb 19 16:02:11 UTC 2014,,,,,,0|i1sj33:,374762,,,,,,,,yukim,yukim,,,1.2.6,,,,,,,19/Feb/14 15:25;jbellis;Thanks to Berenguer Blasi for spotting this.,19/Feb/14 15:51;yukim;+1,"19/Feb/14 16:02;jbellis;Committed, with some extra refactoring to replace AbstractViewSSTableFinder with a Function.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Switch CFS histograms to biased sampling,CASSANDRA-6164,12672868,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,08/Oct/13 19:16,12/Mar/19 14:00,13/Mar/19 22:29,09/Oct/13 13:29,1.2.11,2.0.2,,Legacy/Tools,,,0,jmx,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Oct/13 19:16;jbellis;6164.txt;https://issues.apache.org/jira/secure/attachment/12607414/6164.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-08 19:23:48.577,,,no_permission,,,,,,,,,,,,352491,,,Tue Oct 08 20:42:23 UTC 2013,,,,,,0|i1orun:,352778,,,,,,,,yukim,yukim,,,,,,,,,,08/Oct/13 19:16;jbellis;Also cleans up the tombstone histograms from CASSANDRA-6057,08/Oct/13 19:23;yukim;I don't think changing MBean API in minor release is good idea.,"08/Oct/13 19:27;jbellis;The method names being changed have not yet been released, so that should be fair game.","08/Oct/13 20:26;yukim;yeah, that's right.
+1 from me.",08/Oct/13 20:42;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getOperationMode can thow NullPointerException,CASSANDRA-6166,12672898,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,Connor Warrington,Connor Warrington,Connor Warrington,08/Oct/13 20:47,12/Mar/19 14:00,13/Mar/19 22:29,09/Oct/13 00:42,2.0.2,,,,,,0,,,,,While the client or server is being initialized calling StorageService.getOperationMode() can throw a NullPointerException,,,,,,,,,,,,,,,,,,,,,,,,08/Oct/13 20:49;Connor Warrington;trunk-6166.txt;https://issues.apache.org/jira/secure/attachment/12607433/trunk-6166.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-09 00:42:09.137,,,no_permission,,,,,,,,,,,,352521,,,Wed Oct 09 00:42:09 UTC 2013,,,,,,0|i1os1b:,352808,,,,,,,,dbrosius,dbrosius,,,,,,,,,,"08/Oct/13 20:50;Connor Warrington;Add a value to Mode.
Set the operationMode to that Mode so we don't throw a NullPointerException","09/Oct/13 00:42;dbrosius;+1, thanks!

committed to cassandra-2.0 as commit c198b76c46e4beae45e2a98910322a8761b73684",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CAS updates should require P.MODIFY AND P.SELECT,CASSANDRA-6247,12675972,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,iamaleksey,iamaleksey,26/Oct/13 23:57,12/Mar/19 14:00,13/Mar/19 22:29,28/Oct/13 12:53,2.0.3,,,,,,0,LWT,,,,"With CAS it is possible to simulate a SELECT query using conditional UPDATE IF. Hence all CAS updates should require P.SELECT permission, and not just P.MODIFY.",,,,,,,,,,,,,,,,,,,,,,,,28/Oct/13 12:00;iamaleksey;6247.txt;https://issues.apache.org/jira/secure/attachment/12610541/6247.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-28 12:44:08.166,,,no_permission,,,,,,,,,,,,355469,,,Mon Oct 28 12:53:35 UTC 2013,,,,,,0|i1pa5r:,355757,,,,,,,,slebresne,slebresne,,,2.0.0,,,,,,,28/Oct/13 11:39;iamaleksey;Attaching a trivial patch.,"28/Oct/13 12:00;iamaleksey;Re-attaching, with CassandraServer.cas() modified as well.",28/Oct/13 12:44;slebresne;+1,"28/Oct/13 12:53;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce index summary memory use for cold sstables,CASSANDRA-5519,12645053,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thobbs,jbellis,jbellis,27/Apr/13 22:03,12/Mar/19 14:00,13/Mar/19 22:29,22/Nov/13 18:13,2.1 beta1,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5515,21/Nov/13 21:32;thobbs;5519-v1.txt;https://issues.apache.org/jira/secure/attachment/12615198/5519-v1.txt,22/Oct/13 22:53;thobbs;downsample.py;https://issues.apache.org/jira/secure/attachment/12609747/downsample.py,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-08-01 19:47:17.736,,,no_permission,,,,,,,,,,,,325415,,,Fri Nov 22 18:13:25 UTC 2013,,,,,,0|i1k547:,325760,,,,,,,,jbellis,jbellis,,,,,,,,,,"01/Aug/13 19:47;thobbs;An initial idea for the implementation:

Based on the recent (last 15m?) read rate (reads/sec), periodically down-sample the summary for SSTables which fall below the mean rate.  The down-sampling rate could use a sliding scale based on the ratio of the mean to that SSTable's rate.  As a example basic implementation, keep X% of the samples, where {{X = max(25, min(100, 100 * (rate / mean_rate)))}}, so the coldest SSTables keep only 25% of the samples in memory.

Presenting a way for the user to tune this (other than a simple on/off) is a little trickier.  Perhaps make the min (default 25%) adjustable?  Or start down-sampling at a configurable point (the default is the mean)?  Those could also be automatically adjusted based on memory pressure.","07/Aug/13 02:41;jbellis;Good start, but it seems a little fragile to me if a bunch of sstables are suddenly warmed up.

What about this?

We could define a fixed-size memory pool, similar to what we do for memtables or cache, and allocate it to the sstables proportional to their hotness.  Every 15 minutes (which seems like a lot, maybe hourly?) we recalculate and rebuild the summaries.  Maybe we only rebuild the ones that are X% off of where they should be to make it lighter-weight.  Or if we're downsampling by more than 2x then we can just resample what we already have in memory instead of rebuilding ""correctly.""","07/Aug/13 19:03;thobbs;bq. We could define a fixed-size memory pool, similar to what we do for memtables or cache, and allocate it to the sstables proportional to their hotness.

It would be hard to describe this in text, so here's my pythonic psuedocode for distributing the fixed-size memory pool:

{noformat}
total_reads_per_sec = sum(sstable.reads_per_sec for sstable in sstables)
sstables_to_downsample = set()
leftover_entries = 0
for sstable in sstables:
    allocated_space = total_space * (sstable.reads_per_sec / total_reads_per_sec)
    num_entries = total_space / (SPACE_PER_ENTRY)  # space per entry = token + position + overhead
    if (num_entries > sstable.max_index_summary_entries):
        sstable.num_index_summary_entries = max_index_summary_entries
        leftover_entries = num_entries - sstable.max_index_summary_entries
    else
        sstable.num_index_summary_entries = num_entries
        sstables_to_downsample.add(sstable)

# distribute leftover_entries among sstables_to_downsample based on read rates
# (this probably ends up looking like a recursive or iterative function)
{noformat}

bq. Maybe we only rebuild the ones that are X% off of where they should be to make it lighter-weight.

That's a good idea. (I was thinking of using a step function.)  Instead of ""X% off of where they should be"", I would more precisely phrase that as ""X% away from their previous proportion"".

bq.  Or if we're downsampling by more than 2x then we can just resample what we already have in memory instead of rebuilding ""correctly.""

If you down-sample with a particular pattern, you can always down-sample using just the in-memory points; only up-samples need to read from disk.

I'm trying to generalize the down-sampling pattern, but the two main points are (assuming 1% granularity):
* For every 1% you down-sample, the number of points to remove from the in-memory summary is equal to 1% of the original (on-disk) count
* Each 1% down-sampling run starts at a different offset to evenly space the down-sampling

For example, to down-sample from 100% to 99%, you would remove every hundredth point, starting from index 0.  To down-sample from 99% to 98%, you would remove every 99th point, starting from index 50.  To down-sample from 98% to 97%, you would remove every 98th point, starting from index 24 or 74, and so on.",08/Aug/13 15:21;jbellis;Sounds reasonable.,"22/Oct/13 22:53;thobbs;The attached downsample.py script demonstrates the downsampling algorithm.  It's a touch complex, but it would be easy to precompute or cache the downsampling patterns if needed.

An example run with an original index summary size of 16 and a ""resolution"" of 8, meaning each minimal downsample run will remove 1/8th of the original points.  The top row is the original index summary and each row below that represents one downsampling run:

{noformat}
~ $ ./downsample.py 16 8
  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15
      1   2   3   4   5   6   7       9  10  11  12  13  14  15
      1   2   3       5   6   7       9  10  11      13  14  15
      1       3       5   6   7       9      11      13  14  15
      1       3       5       7       9      11      13      15
              3       5       7              11      13      15
              3               7              11              15
{noformat}",22/Oct/13 23:20;jbellis;LGTM,31/Oct/13 22:20;thobbs;How do we want to handle the memory pool not being large enough to accommodate all of the index summaries (even after downsampling)?  Just make it a best-effort?,"12/Nov/13 23:34;thobbs;I need to put this through more thorough testing and benchmarking, but I think it's at a good point for a preliminary review: https://github.com/thobbs/cassandra/compare/CASSANDRA-5519

A few comments/questions:
* I went with best-effort for the memory pool (if all summaries don't fit in the allotted space even at the minimum sampling level, there's nothing we can do about it).  The amount of memory used may also temporarily exceed the limit while building new summaries.
* There are two new cassandra.yaml options: one for controlling the memory pool size and one for regulating how frequently summaries are resized.  These can also be set through JMX. We could conceivably also make the down/upsample thresholds and the minimum sampling level configurable.  All of these default values are just guesses.
* I went with a reference counting strategy for free'ing the IndexSummary's Memory.  This makes the API a bit unpleasant (mostly in SSTR), but it should have low overhead.  A ReadWriteLock might also work well instead of this with a cleaner API; let me know if I should benchmark the two for comparison.
* I'm triggering the IndexSummaryManager singleton's initialization in DatabaseDescriptor; this feels wrong, so I'm open to suggestions.","13/Nov/13 23:49;jbellis;What is the relationship between BASE_SAMPLING_LEVEL and MIN_SAMPLING_LEVEL with indexInterval?

How many rows do we get for 5% of a 8GB heap?

Isn't it a minor bug to just ignore compacting sstables?  Suggest reducing memory pool to allocate to the uncompacting ones, by the amount allocated to the compacting ones.

Could we just resample at compaction time instead of dealing with refcounting or locking?  That probably gives up too much of the potential benefits.  But I think we could make it almost as elegant by using the datatracker replace mechanism originally for compaction, to build a new SSTR and swap it in w/o extra concurrency controls.

Is the idea behind touching it in DD to force the mbean to be loaded, or is there a circular dependency that breaks w/o that?","14/Nov/13 17:06;thobbs;bq. What is the relationship between BASE_SAMPLING_LEVEL and MIN_SAMPLING_LEVEL with indexInterval?

{{BASE/MIN_SAMPLING_LEVEL}} are orthogonal to {{indexInterval}}.  {{BASE_SAMPLING_LEVEL}} essentially sets the granularity at which you can down/upsample.  {{MIN_SAMPLING_LEVEL}} sets a limit on how low you can downsample.  (I'll note that we could potentially raise {{indexInterval}} alongside these changes in order to have more summary entries for hot sstables.)

bq. How many rows do we get for 5% of a 8GB heap?

That gives us ~410 MiB to work with.  If we assume the average key length is 8 bytes, each summary entry uses 20 bytes of space, giving us ~21 million summary entries.

At full sampling, that's 21MM * 128 = 2.7 billion rows, assuming no overlap across sstables. At minimum sampling, that's ~11 billion rows.

If the avg key size is 16 bytes, that drops to ~2 and ~8 billion rows.

bq. Isn't it a minor bug to just ignore compacting sstables? Suggest reducing memory pool to allocate to the uncompacting ones, by the amount allocated to the compacting ones.

Good point, I agree.

bq. Could we just resample at compaction time instead of dealing with refcounting or locking? That probably gives up too much of the potential benefits.

Yeah, that would probably be okay for small sstables that are compacted frequently, but the large sstables would be tuned poorly, and those make up the majority of the memory use.

bq. I think we could make it almost as elegant by using the datatracker replace mechanism originally for compaction, to build a new SSTR and swap it in w/o extra concurrency controls.

That's a good idea; I think it would be fairly clean.  I'll give that a shot.

bq. Is the idea behind touching it in DD to force the mbean to be loaded, or is there a circular dependency that breaks w/o that?

Neither the {{IndexSummaryManager}} singleton nor the mbean are loaded without that.  No other classes use the {{IndexSummaryManager}},
so the static fields are never initialized.  (Just importing the classes doesn't seem to trigger the class loader.)","14/Nov/13 23:12;jbellis;Pushed my cleanup to https://github.com/jbellis/cassandra/commits/5519.

(Moved the ISM init to StorageService were we have some existing examples of similar.)","14/Nov/13 23:21;jbellis;Rather than expose MSL directly as a config option, how about changing index_interval to max_index_interval and adding a min_index_interval?  We could compute (as close as possible) MSL from min_index_interval.

(I don't think users will need to tune BSL.  128 lets us be accurate to with in 1% which seems totally reasonable to me.)","14/Nov/13 23:39;thobbs;The cleanup looks good overall, thanks.

bq. Rather than expose MSL directly as a config option, how about changing index_interval to max_index_interval and adding a min_index_interval? We could compute (as close as possible) MSL from min_index_interval.

That sounds good to me.

bq. (I don't think users will need to tune BSL. 128 lets us be accurate to with in 1% which seems totally reasonable to me.)

Agreed.","19/Nov/13 23:00;thobbs;Would it be alright to split the replacement of {{index_interval}} by {{max_index_interval}} and {{min_index_interval}} into another ticket just for sanity's sake?  It looks like a lot of changes need to be done for that, and they're independent of the changes for this ticket.",19/Nov/13 23:39;jbellis;WFM.  What does that leave for this one?,"19/Nov/13 23:44;thobbs;bq. WFM. What does that leave for this one?

I still need to account for spaced used by compacting SSTables, and I'm putting it through some more thorough testing.",19/Nov/13 23:46;thobbs;Created CASSANDRA-6379 for the {{index_interval}} changes.,"21/Nov/13 18:08;thobbs;This should be good for a second round of reviewing.  I opened a pull request against my own repo so that you can comment inline, if you'd like: https://github.com/thobbs/cassandra/pull/1

Changes since the last review:
* The entire {{SSTableReader}} is replaced instead of just the IndexSummary.
* Space used by compacting SSTables is accounted for
* Enough extra space is reserved to cover rebuilding the largest summary
* In order to stay within the memory usage limit on startup, the on-disk Summary is replaced whenever it is resampled.  I increased the threshold for downsampling to make this less frequent.  The alternative would be to always keep the full summary on disk and have a somewhat more complicated startup procedure.  I would appreciate your thoughts on this.","21/Nov/13 18:16;thobbs;I should also mention that if you want to test it out, I suggest setting logging to TRACE for o.a.c.io.sstable.IndexSummary manager, {{index_summary_capacity_in_mb}} to 1, and {{index_summary_resize_interval_in_minutes}} to 1.  That should give you a good picture of what's going on.","21/Nov/13 18:37;jbellis;bq. the on-disk Summary is replaced whenever it is resampled

Good call; startup time is a big pain point for some people and we don't want to make that worse.","21/Nov/13 21:32;thobbs;[~jbellis] Attached patch 5519-v1.txt includes your suggested changes.  (My CASSANDRA-5519 branch is still good, as well.)",21/Nov/13 22:19;jbellis;I'm pretty sure we can get rid of the isReplaced flag now.,"21/Nov/13 22:34;thobbs;bq. I'm pretty sure we can get rid of the isReplaced flag now.

We still need it in order to do the proper cleanup on the replaced SSTR once all references are released, unless I'm missing something.","21/Nov/13 23:36;jbellis;Hmm.  I see two uses of isReplaced:

# releaseReference, which can be reverted back to trunk form since isReplaced == !isCompacted
# close, which is only called by snapshot repair (and releaseReference) which will never do any index summary replacements","22/Nov/13 17:17;thobbs;bq. releaseReference, which can be reverted back to trunk form since isReplaced == !isCompacted

True

bq. close, which is only called by snapshot repair (and releaseReference) which will never do any index summary replacements

We still need to have different behavior for the {{close()}} call by snapshot repair, as it needs to perform the full close even though {{isCompacted}} will be false.  While we could add a parameter to close() or define a {{closeReplacedReader()}} method, it seems clearer and more future-proof to keep the isReplaced flag.",22/Nov/13 18:13;jbellis;WFM.  Committed!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fixes for compacting larger-than-memory rows,CASSANDRA-6274,12676652,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,30/Oct/13 15:12,12/Mar/19 14:00,13/Mar/19 22:29,30/Oct/13 15:20,2.0.3,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,356084,,,Wed Oct 30 15:20:01 UTC 2013,,,,,,0|i1pdy7:,356372,,,,,,,,krummas,krummas,,,,,,,,,,"30/Oct/13 15:12;jbellis;https://github.com/jbellis/cassandra/commits/6142-2.0 already reviewed on CASSANDRA-6142, but splitting out here to avoid confusion from having 6142 in the 2.0 CHANGES.",30/Oct/13 15:20;jbellis;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid unnecessary second pass on name-based queries,CASSANDRA-5577,12648176,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,17/May/13 13:18,12/Mar/19 14:00,13/Mar/19 22:29,17/May/13 14:43,2.0 beta 1,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/May/13 13:34;jbellis;5577.txt;https://issues.apache.org/jira/secure/attachment/12583651/5577.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-17 14:28:09.338,,,no_permission,,,,,,,,,,,,328532,,,Thu Sep 19 22:38:50 UTC 2013,,,,,,0|i1kom7:,328876,,,,,,,,slebresne,slebresne,,,,,,,,,,"17/May/13 14:28;slebresne;The patch lgtm, though going a bit further I think we can always dispense ourselves of the collateOnDiskAtom call. Because returning gcable tombstones during read should never be a problem since we'll skip any tombstone before returning to the client.

Nit: might be worst adding as a comment than skipping collateOnDiskAtom is only ok because we have a NamesQueryFilter, so we don't inadvertently copy the idea when a slice filter is involved.
","17/May/13 14:43;jbellis;I don't think we have any code in read repair to distinguish b/t gc-able tombstones and otherwise, so I'll leave the collate call in for now.

Committed w/ extra comment.",19/Sep/13 22:38;jbellis;(Turns out this is invalid; see CASSANDRA-6025 for an example.  Reverted in 2.0.1.),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent repair among the nodes of different version,CASSANDRA-5523,12645323,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,29/Apr/13 22:21,12/Mar/19 14:00,13/Mar/19 22:29,02/Aug/13 03:28,,,,,,,0,repair,,,,"Since streaming file to the node of different version is not allowed, and in fact it would be the cause of repair hang, there is no point to allow repairing among the nodes of different versions.
",,,,,,,,,,,,,,,,,,,,,,,,29/Apr/13 22:24;yukim;5523-1.2.txt;https://issues.apache.org/jira/secure/attachment/12581059/5523-1.2.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-29 22:35:06.83,,,no_permission,,,,,,,,,,,,325685,,,Fri Aug 02 03:28:39 UTC 2013,,,,,,0|i1k6s7:,326030,,,,,,,,jbellis,jbellis,,,,,,,,,,"29/Apr/13 22:24;yukim;Patch to fail repair when the participants' versions are different.
Note that we don't need to check if the node is pre-1.1 for sequential repair, I removed that check also.","29/Apr/13 22:35;jbellis;I would change the message to ""different protocol versions"" or ""different major releases"" since different minor releases should be okay.

Otherwise +1",30/Apr/13 19:30;yukim;Committed with the message fix. Thanks!,"14/May/13 02:02;yukim;The fix was unreliable because every time connection is closed, protocol version for that connection is also reset (set to null), leading to not able to repair randomly.
Reverted from 1.2 branch and trunk for now.",02/Aug/13 03:28;jbellis;Wontfixing since we should be able to do cross-version repair in 2.0.  Please reopen if I've misunderstood.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: Map can not be created with the same name as a previously dropped list,CASSANDRA-6276,12676851,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,blerer,gryphius,gryphius,31/Oct/13 12:37,12/Mar/19 14:00,13/Mar/19 22:29,11/Aug/14 16:24,2.0.10,,,,,,2,cql,,,,"If create a list, drop it and create a map with the same name, i get ""Bad Request: comparators do not match or are not compatible.""

{quote}
cqlsh:os_test1> create table thetable(id timeuuid primary key, somevalue text);
cqlsh:os_test1> alter table thetable add mycollection list<text>;  
cqlsh:os_test1> alter table thetable drop mycollection;
cqlsh:os_test1> alter table thetable add mycollection map<text,text>;  
Bad Request: comparators do not match or are not compatible.
{quote}


"," Cassandra 2.0.2 | CQL spec 3.1.0
centos 64 bit
Java(TM) SE Runtime Environment (build 1.7.0-b147)
",,,,,,,,,,,,,,,,,,,,,,,11/Aug/14 15:35;slebresne;6272-2.0.txt;https://issues.apache.org/jira/secure/attachment/12660998/6272-2.0.txt,05/Aug/14 20:25;blerer;CASSANDRA-6276-V2.txt;https://issues.apache.org/jira/secure/attachment/12659938/CASSANDRA-6276-V2.txt,12/Jul/14 14:14;blerer;CASSANDRA-6276.txt;https://issues.apache.org/jira/secure/attachment/12655402/CASSANDRA-6276.txt,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-10-31 13:15:04.158,,,no_permission,,,,,,,,,,,,356227,,,Tue Sep 20 01:36:43 UTC 2016,,,,,,0|i1petj:,356515,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,31/Oct/13 13:15;brandon.williams;I think we're looking at CASSANDRA-5202 here.,31/Oct/13 13:39;jbellis;5202 is drop/recreate of an entire table.,"23/Jan/14 10:23;Daniel Smedegaard Buus;This is quite frustrating when developing and debugging with large portions of data that need to be re-inserted every time you have a schema change because you have to drop an entire CF to make minor schema changes like this...

Is there a workaround one could use until this is resolved?

Thanks :)",05/Jun/14 20:52;jbellis;/cc [~slebresne],"31/Jul/14 12:53;iamaleksey;Unfortunately, we can't allow dropping a component from the comparator, including dropping individual collection columns from ColumnToCollectionType.

If we do allow that, and have pre-existing data of that type, C* simply wouldn't know how to compare those:

{code:title=ColumnToCollectionType.java}
    public int compareCollectionMembers(ByteBuffer o1, ByteBuffer o2, ByteBuffer collectionName)
    {
        CollectionType t = defined.get(collectionName);
        if (t == null)
            throw new RuntimeException(ByteBufferUtil.bytesToHex(collectionName) + "" is not defined as a collection"");

        return t.nameComparator().compare(o1, o2);
    }
{code}

A simple algorithm to hit the RTE:
1. create table test (id int primary key, col1 map<int, int>, col2 set<int>);
2. insert into test (id, col1, col2) VALUES ( 0, \{0:0, 1:1\}, \{0,1\});
3. flush
4. update test set col1 = col1 + \{2:2\}, col2 = col2 + \{2\} where id = 0;
5. flush
6. select * from test;

{noformat}
java.lang.RuntimeException: 636f6c31 is not defined as a collection
	at org.apache.cassandra.db.marshal.ColumnToCollectionType.compareCollectionMembers(ColumnToCollectionType.java:79) ~[main/:na]
	at org.apache.cassandra.db.composites.CompoundSparseCellNameType$WithCollection.compare(CompoundSparseCellNameType.java:296) ~[main/:na]
	at org.apache.cassandra.db.composites.AbstractCellNameType$1.compare(AbstractCellNameType.java:61) ~[main/:na]
	at org.apache.cassandra.db.composites.AbstractCellNameType$1.compare(AbstractCellNameType.java:58) ~[main/:na]
	at org.apache.cassandra.utils.MergeIterator$Candidate.compareTo(MergeIterator.java:154) ~[main/:na]
	at org.apache.cassandra.utils.MergeIterator$Candidate.compareTo(MergeIterator.java:131) ~[main/:na]
{noformat}

For this reason alone we can't allow getting rid of a comparator component.

However, even if we did, and allowed to create a different collection with the same name, we'd hit a different issue: the new collection's comparator would be used to compare potentially incompatible types. Now, your unit tests aren't failing b/c most of our comparators assume valid values and don't perform extra validation, then use something like ByteBufferUtil.compareUnsigned() to compare the values, which doesn't fail and will just stop once the shortest BB gets exhausted. One exception is tuples/usertypes - they *do* expect at least length to be there, and will throw an exception.

Example:
1. create table test (id int primary key, col set<boolean>);
2. insert into test (id, col) values (0, \{true,false\});
3. alter table test drop col;
4. create type test (f1 int);
5. alter table test add col set<test>;
6. update test set col = col + \{ \{f1 : 0 \} \} where id = 0;
7. select * from test;

{noformat}
java.nio.BufferUnderflowException: null
	at java.nio.Buffer.nextGetIndex(Buffer.java:498) ~[na:1.7.0_65]
	at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:355) ~[na:1.7.0_65]
	at org.apache.cassandra.db.marshal.TupleType.compare(TupleType.java:80) ~[main/:na]
	at org.apache.cassandra.db.marshal.TupleType.compare(TupleType.java:38) ~[main/:na]
	at org.apache.cassandra.db.marshal.ColumnToCollectionType.compareCollectionMembers(ColumnToCollectionType.java:81) ~[main/:na]
	at org.apache.cassandra.db.composites.CompoundSparseCellNameType$WithCollection.compare(CompoundSparseCellNameType.java:296) ~[main/:na]
	at org.apache.cassandra.db.composites.AbstractCellNameType$1.compare(AbstractCellNameType.java:61) ~[main/:na]
	at org.apache.cassandra.db.composites.AbstractCellNameType$1.compare(AbstractCellNameType.java:58) ~[main/:na]
	at org.apache.cassandra.utils.MergeIterator$Candidate.compareTo(MergeIterator.java:154) ~[main/:na]
{noformat}","31/Jul/14 12:58;iamaleksey;So the best we can do is provide a better exception message, unfortunately.

This also unfortunately complicates CASSANDRA-6717, since we don't store the comparator itself anymore and reconstruct it from the columns we have, and the current dropped_columns map only has names and drop timestamps in it. This means we must store the type of the dropped columns as well, in case some of those are collections, which is annoying.","04/Aug/14 17:16;slebresne;You're right, I'm not sure there is much we can do about this.

bq. So the best we can do is provide a better exception message, unfortunately.

I agree. Let's detect that case and provide a meaningful error message for now.

In the longer run, one way to maybe fix this would be to push dropped_columns down to the sstable reading level. If we were skipping cells as soon as they are deserialized before they ever hit any comparator we would be free to update the comparator. Probably messy/inefficient to do with the current code, but might become feasible with 3.0 storage engine changes, we'll see. ","04/Aug/14 17:21;iamaleksey;bq. In the longer run, one way to maybe fix this would be to push dropped_columns down to the sstable reading level. If we were skipping cells as soon as they are deserialized before they ever hit any comparator we would be free to update the comparator. Probably messy/inefficient to do with the current code, but might become feasible with 3.0 storage engine changes, we'll see. 

True. Would still be tricky somewhat, given that schema updates don't propagate instantly, but we'll see.","04/Aug/14 17:22;benedict;3.0 storage engine won't be universal for a while (maybe never for thrift), but will index directly into columns (i.e. won't touch any not requested), so could trivially avoid retrieving data for dropped columns. The only problem is we'd need to track the range of sstables for which they were previously dropped (and maybe contains stale data), and which we now apply the new comparator too, which would be a bit ugly/annoying.","05/Aug/14 20:25;blerer;This patch check if a collection of a different type and with the same name already existed and if it is the case it will send an error with the following message:
""Cannot add a collection with the name <collectionName> because a collection with the same name and a different type has already been used in the past""
","11/Aug/14 15:35;slebresne;Patch looks good, though adding a method to {{CellNameType}} feels a tad overkill to me, I'd rather keep the validation in {{AlterTableStatement}} (it's arguably a personal preference). The other reason being that we should fix this in 2.0 and we don't have {{CellNameType}} there. So anyway, attaching a slightly simpler alternative for 2.0. If we're good with that, I'll push a simple dtest so 2.0 is covered and I'll include the tests from [~blerer] patch while merging with 2.1 (since CqlTester is not in 2.0).",11/Aug/14 15:37;iamaleksey;2.0 patch LGTM,"11/Aug/14 16:24;slebresne;Alright, committed, thanks","04/Aug/16 13:06;masumsoft;Facing the same problem in cassandra v3.7. Btw, it works when you drop the column, recreate a non collection column such as int with the same name and then drop that again. After that you can add the same name column with a different collection type and cassandra allows the operation. Dirty workaround though!",20/Sep/16 01:36;iamaleksey;Dirty but also unsafe. There is a reason the limitation is there in the first place - try to 'work around it' and risk corruption.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make nodetool ring print an error message and suggest nodetool status when vnodes are enabled,CASSANDRA-5954,12666245,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lyubent,jbellis,jbellis,30/Aug/13 02:53,12/Mar/19 14:00,13/Mar/19 22:29,24/Sep/13 15:31,1.2.11,2.0.2,,Tool/nodetool,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Sep/13 14:15;lyubent;5954.patch;https://issues.apache.org/jira/secure/attachment/12604038/5954.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-30 03:04:21.072,,,no_permission,,,,,,,,,,,,346184,,,Tue Sep 24 15:31:01 UTC 2013,,,,,,0|i1np3r:,346485,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,"30/Aug/13 03:04;brandon.williams;Well, do need *some* way to list vnode tokens.  Maybe add something like a -YES flag when using ring w/vnodes.",30/Aug/13 03:18;jbellis;We can get it from a system table right?,"30/Aug/13 07:56;aecobley;Can we be careful with this change ?  I'm thinking there may be scripts out there that rely on the current behavior that may break if you change it (does opcenter use nodetool for its operations ?).  It seems to me the original suggestion is because nodetool ring is not the best way to monitor vnodes and user education is needed on the role of nodetool status.

With that in mind, would it be better to simply add a message at the bottom of the output of nodetool ring with vnodes suggesting nodetool status ?  That should minimise any script problems as they should just ignore the last line ?","30/Aug/13 12:22;jjordan;I agree, put a message at the bottom of the output which says ""Now that you watched 5000 tokens scroll by, you probably want nodetool status"".","30/Aug/13 19:11;brandon.williams;bq. We can get it from a system table right?

Yes, but then you'd have to calculate ownership yourself.","19/Sep/13 14:16;lyubent;Added a warning message with a suggestion to use ""nodetool status"" next time.",24/Sep/13 15:31;brandon.williams;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress skipKeys option does not appear to do anything,CASSANDRA-5927,12665223,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,benedict,cburroughs,cburroughs,23/Aug/13 13:19,12/Mar/19 14:00,13/Mar/19 22:29,13/Mar/14 22:06,2.1 rc2,,,Legacy/Tools,,,0,,,,,"I don't see it used anywhere in the code.  FWIW I *thought* when it worked the option was ""start at key X"" instead of ""Fraction of keys to skip initially"".

{noformat}
$ grep -rni skipkeys ./tools/stress/src/org/
./tools/stress/src/org/apache/cassandra/stress/Session.java:122:    private float skipKeys       = 0;
./tools/stress/src/org/apache/cassandra/stress/Session.java:199:                skipKeys = Float.parseFloat(cmd.getOptionValue(""N""));
./tools/stress/src/org/apache/cassandra/stress/Session.java:513:    public float getSkipKeys()
./tools/stress/src/org/apache/cassandra/stress/Session.java:515:        return skipKeys;
{noformat}",1.2.8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-03-13 22:01:59.808,,,no_permission,,,,,,,,,,,,345164,,,Thu Mar 13 22:05:55 UTC 2014,,,,,,0|i1nitb:,345465,,,,,,,,,,,,,,,,,,,13/Mar/14 22:01;jbellis;Is this still a thing in stress-ng?,"13/Mar/14 22:05;benedict;No. I'm not sure what it's even referring to (I don't support it in Legacy either, but maybe that's because it wasn't wired up when I was migrating, which is exactly what this ticket refers to I guess). 

In stress-ng you can specify an explicit range and distribution for your keys.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stressd broken by ClientEncriptionOptions,CASSANDRA-5978,12667116,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,benedict,jjordan,jjordan,05/Sep/13 14:28,12/Mar/19 14:00,13/Mar/19 22:29,12/Mar/14 09:52,2.1 rc2,,,Legacy/Tools,,,1,,,,,"The ClientEncryptionOptions object added to org.apache.cassandra.stress.Session is not Serializable.  So if you try to use stress with stressd, the Session can't be serialized to be passed over to stressd:

{noformat}
Exception in thread ""main"" java.io.NotSerializableException: org.apache.cassandra.config.EncryptionOptions$ClientEncryptionOptions
at java.io.ObjectOutputStream.writeObject0(Unknown Source)
at java.io.ObjectOutputStream.defaultWriteFields(Unknown Source)
at java.io.ObjectOutputStream.writeSerialData(Unknown Source)
at java.io.ObjectOutputStream.writeOrdinaryObject(Unknown Source)
at java.io.ObjectOutputStream.writeObject0(Unknown Source)
at java.io.ObjectOutputStream.writeObject(Unknown Source)
at org.apache.cassandra.stress.Stress.main(Unknown Source)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-11-26 17:43:34.044,,,no_permission,,,,,,,,,,,,347053,,,Tue Nov 26 19:54:50 UTC 2013,,,,,,0|i1nufj:,347352,1.2.6,,,,,,,,,,,,,,,,,,"26/Nov/13 17:43;djatnieks;Ran into this today ... my stack has line numbers:

{noformat}
./dse-3.2.1/resources/cassandra/tools/bin/cassandra-stress -K 100 -t 50 -R org.apache.cassandra.locator.NetworkTopologyStrategy  --num-keys=10000000 --columns=50 -D nodelist -O Cassandra:3 --operation=INSERT --send-to 127.0.0.1

Exception in thread ""main"" java.io.NotSerializableException: org.apache.cassandra.config.EncryptionOptions$ClientEncryptionOptions
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1541)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1506)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1429)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1175)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
	at org.apache.cassandra.stress.Stress.main(Unknown Source)
Control-C caught. Canceling running action and shutting down...
{noformat}
","26/Nov/13 18:29;jbellis;Fine with ""use stress-ng"" if that already supports daemon mode.  Otherwise this should be a quick fix.","26/Nov/13 19:54;benedict;Download the latest snapshot from [6199|https://github.com/belliottsmith/cassandra/tree/iss-6199-stress] and run your command as

./dse-3.2.1/resources/cassandra/tools/bin/cassandra-stress *legacy* -K 100 -t 50 -R org.apache.cassandra.locator.NetworkTopologyStrategy  --num-keys=10000000 --columns=50 -D nodelist -O Cassandra:3 --operation=INSERT --send-to 127.0.0.1

I've confirmed this command and stressd launcher etc work with the latest version.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitLogSegment may be duplicated in unlikely race scenario,CASSANDRA-6557,12687677,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,benedict,benedict,benedict,07/Jan/14 16:32,12/Mar/19 14:00,13/Mar/19 22:29,14/Mar/14 17:40,2.1 beta2,,,,,,0,,,,,"In the unlikely event that the thread that switched to a new CLS has not finished executing the cleanup of its switch by the time the CLS has finished being used, it is possible for the same segment to be 'switched' in again. This would be benign except that it is added to the activeSegments queue a second time also, which would permit it to be recycled twice, creating two different CLS objects in memory pointing to the same CLS on disk, after which all bets are off.

The issue is highly unlikely to occur, but highly unlikely means it will probably happen eventually. I've fixed this based on my patch for CASSANDRA-5549, using the NonBlockingQueue I introduce there to simplify the logic and make it more obviously correct.",2.1,,,,,,,,,,,,,,,,,,,,,,,12/Mar/14 11:51;benedict;6557.2.txt;https://issues.apache.org/jira/secure/attachment/12634141/6557.2.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-17 03:14:31.382,,,no_permission,,,,,,,,,,,,366677,,,Fri Mar 14 17:40:32 UTC 2014,,,,,,0|i1r7ef:,366988,,,,,,,,,,,,,,,,,,,07/Jan/14 16:43;benedict;https://github.com/belliottsmith/cassandra/tree/iss-6557,17/Jan/14 03:14;jbellis;Can you give an overview of the fix?,"17/Jan/14 03:20;benedict;Basically I've merged allocateFrom (the ""current"" segment) with availableSegments (the next segments we will use), eliminating the interval between setting the new segment in allocateFrom and removing it from availableSegments. i.e. allocateFrom is now just the head of the queue, and to move to the next segment you just (approximately) call advanceHeadIf(oldSegment)","17/Jan/14 04:52;jbellis;Just to make sure we're talking about the same problem:

Say I have allocatingFrom=X, active = [X, Y], and available=[Z].

Someone calls advanceAllocatingFrom.  We iterate over available, and try to swap the first element into allocatingFrom.

So now we have 
old = X
aF = Z
active = [X, Y]
available = [Z]

Our next step is to remove Z from available and add it to active, but if another thread calls advanceAllocatingFrom before that happens, the initial thread running aAF and the other will both add Z to active, so we'll have [X, Y, Z, Z] which is a violation of our design.

It seems to me that we can fix this much more easily by simply dequeuing the element from available before trying to CAS.  If we fail, we can add it back.  This is a relatively rare operation, and a race even more rare, so it doesn't have to be super optimized.","17/Jan/14 10:23;benedict;I agree it doesn't need to be super-optimised, and we could use the design you suggest, _if_ we were to modify CLS so that we can create it in the aAF method with the correct segment id. As it stands, the segment ids are assigned in the CLSM thread, so they need to be added to active/allocatingFrom in the order they are added to available. I don't think this is a very neat solution, though, as the CLSM thread needs to at least assign the first segment id, and we'd need an intermediate object.

The alternative, of course, is to lock when we want to aAF. As you say, it's a rare operation, and it would be fine to lock for it. I thought it was a bit ugly in the original MT CL patch, but it doesn't absolutely *need* NBQ, so if we want to avoid adding it for now, we can. 

I had planned to move active and available to a single NBQ, with active a NBQV on the available NBQ, at some point in the near future, as it more accurately models what we're doing (it's just a single queue we have, in three parts, and we just treat the parts slightly differently), but that is also not strictly necessary. I think it simplifies the algorithm, though.","17/Jan/14 10:30;benedict;bq. active a NBQV on the available NBQ

Just to clarify how this would work: a NBQ can create views on their data that retain references to any items polled from the original after the view was created. A view created on available at startup would basically have the option of polling/iterating over all items added to available, just on its own schedule. So we would do nothing to maintain allocatingFrom or active, we would just poll/remove items from their queue as necessary, based on their own criteria.","17/Jan/14 12:39;jbellis;bq. The alternative, of course, is to lock when we want to aAF. 

Let's do that for now then.

bq. it doesn't absolutely need NBQ, so if we want to avoid adding it for now, we can

You have correctly divined my motivation. :)","17/Jan/14 12:50;benedict;bq. Let's do that for now then.

Okay. I'll hold off until we decide if the off-heap memtables are going to make it into 2.1, as they really *need* need the NBQ. There are at least two spots where without atomic swapping of head/tail pointers the algorithm will be either unsafe, have unreasonable consequences, or unreasonable performance tradeoffs. Or, at least, we can decide that when we come to it :-)



","12/Mar/14 11:51;benedict;Updated version that uses synchronized block for the swap, instead of CAS","14/Mar/14 17:40;jbellis;LGTM, committed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TimestampType doesn't support pre-epoch long,CASSANDRA-6212,12674282,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mishail,simonhopkin,simonhopkin,17/Oct/13 09:15,12/Mar/19 14:00,13/Mar/19 22:29,21/Oct/13 07:21,2.0.2,,,Legacy/CQL,,,0,,,,,"org.apache.cassandra.db.marshal.TimestampType.dateStringToTimestamp() contains a regular expression that checks to see if the String argument contains a number.  If so it parses it as a long timestamp.  However pre-epoch timestamps are negative and the code doesn't account for this so it tries to parse it as a formatted Date.  A tweak to the regular expression in TimestampType.dateStringToTimestamp() would solve this issue.

I could use formatted date strings instead, but the TimestampType date parser uses ISO8601 patterns which would cause the timestamp to be rounded to the nearest second.

Currently I get the following exception message:

unable to coerce '-86400000' to a  formatted date (long)

",,,,,,,,,,,,,,,,,,,,,,,,21/Oct/13 06:55;mishail;cassandra-2.0-6212.patch;https://issues.apache.org/jira/secure/attachment/12609370/cassandra-2.0-6212.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-21 04:02:11.514,,,no_permission,,,,,,,,,,,,353904,,,Mon Oct 21 12:50:34 UTC 2013,,,,,,0|i1p0jz:,354196,2.0.0,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"21/Oct/13 04:02;jbellis;Can you take a stab at this, Mikhail?","21/Oct/13 06:55;mishail;Changed the regexp to accept a leading ""-""","21/Oct/13 07:21;iamaleksey;Thanks, committed.

Haven't included the unit test, though - this particular one is better suited for the dtests.",21/Oct/13 12:50;simonhopkin;Thanks for the quick turnaround on this issue guys.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException in TokenMetadata.cloneOnlyTokenMap,CASSANDRA-6103,12670748,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mishail,mikeschrag,mikeschrag,26/Sep/13 15:32,12/Mar/19 14:00,13/Mar/19 22:29,30/Sep/13 18:56,1.2.11,2.0.2,,,,,0,,,,,"This isn't reproducible for me, but it happened to one of the servers in our cluster while starting up. It went away on a restart, but I figured it was worth filing anyway:

ERROR [main] 2013-09-26 08:04:02,478 CassandraDaemon.java (line 464) Exception encountered during startup
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
        at java.util.HashMap$EntryIterator.next(HashMap.java:834)
        at java.util.HashMap$EntryIterator.next(HashMap.java:832)
        at com.google.common.collect.AbstractBiMap$EntrySet$1.next(AbstractBiMap.java:294)
        at com.google.common.collect.AbstractBiMap$EntrySet$1.next(AbstractBiMap.java:286)
        at com.google.common.collect.AbstractBiMap.putAll(AbstractBiMap.java:160)
        at com.google.common.collect.HashBiMap.putAll(HashBiMap.java:42)
        at com.google.common.collect.HashBiMap.create(HashBiMap.java:72)
        at org.apache.cassandra.locator.TokenMetadata.cloneOnlyTokenMap(TokenMetadata.java:561)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getAddressRanges(AbstractReplicationStrategy.java:192)
        at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:1711)
        at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:1692)
        at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1461)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1228)
        at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:949)
        at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1116)
        at org.apache.cassandra.service.StorageService.setTokens(StorageService.java:214)
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:802)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:554)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:451)",,,,,,,,,,,,,,,,,,,,,,,,30/Sep/13 03:44;mishail;cassandra-1.2-6103.patch;https://issues.apache.org/jira/secure/attachment/12605853/cassandra-1.2-6103.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-26 16:09:35.326,,,no_permission,,,,,,,,,,,,350577,,,Mon Sep 30 18:56:07 UTC 2013,,,,,,0|i1og3z:,350870,1.2.6,1.2.8,,,,,,jbellis,jbellis,,,1.2.0,,,,,,,"26/Sep/13 16:09;brandon.williams;Hmm, it looks like endpointToHostIdMap is being mutated, but it's not immediately clear how since the gossiper is blocked here and can't notify anything else, not to mention all the locking in TMD.","26/Sep/13 18:14;mikeschrag;Just to provide as much info as I can, the entire cluster was stopped. I then ran a script to start all of them back up at the same time. This particular cluster is 44 nodes. All 43 other nodes started fine. This one node died at startup with this exception. After a couple minutes I restarted the one node, and it came up fine. Priority ""Major"" is maybe a little aggressive for this one :)","26/Sep/13 19:16;mishail;Off the top of my head {{org.apache.cassandra.locator.TokenMetadata.updateHostId(UUID, InetAddress)}} calls {{endpointToHostIdMap.forcePut(endpoint, hostId);}} but doesn't acquire a write lock before that.  ","28/Sep/13 02:08;djatnieks;I've run into the same error a couple times with a slightly different stack trace with DSE 3.1.3 (C* 1.2.6 I believe).

+1 on {{updateHostId}} as a likely source from my non-expert look at the code.

{noformat}
Exception encountered during startup
java.util.ConcurrentModificationException
at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
at java.util.HashMap$EntryIterator.next(HashMap.java:834)
at java.util.HashMap$EntryIterator.next(HashMap.java:832)
at com.google.common.collect.AbstractBiMap$EntrySet$1.next(AbstractBiMap.java:294)
at com.google.common.collect.AbstractBiMap$EntrySet$1.next(AbstractBiMap.java:286)
at com.google.common.collect.AbstractBiMap.putAll(AbstractBiMap.java:160)
at com.google.common.collect.HashBiMap.putAll(HashBiMap.java:42)
at com.google.common.collect.HashBiMap.create(HashBiMap.java:72)
at org.apache.cassandra.locator.TokenMetadata.cloneOnlyTokenMap(TokenMetadata.java:561)
at org.apache.cassandra.locator.TokenMetadata.cloneAfterAllLeft(TokenMetadata.java:582)
at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:1708)
at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:1686)
at org.apache.cassandra.service.StorageService.handleStateBootstrap(StorageService.java:1306)
at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1207)
at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:935)
at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1102)
at org.apache.cassandra.service.StorageService.bootstrap(StorageService.java:911)
at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:699)
at org.apache.cassandra.service.StorageService.initServer(StorageService.java:554)
at org.apache.cassandra.service.StorageService.initServer(StorageService.java:451)
at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:342)
at com.datastax.bdp.server.DseDaemon.setup(DseDaemon.java:137)
at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:441)
at com.datastax.bdp.server.DseDaemon.main(DseDaemon.java:334)
{noformat}
","28/Sep/13 03:07;mishail;I'm curious, each access to {{tokenToEndpointMap}} is guarded by either read or write lock, on other hand {{endpointToHostIdMap}} is accessed without any lock in most cases. Is it by design?","30/Sep/13 03:44;mishail;Patch for 1.2. 
* Use read lock to read endpoints' collections, and return defensive copies  for them. 
* Use write lock in {{updateHostId}}","30/Sep/13 18:56;jbellis;LGTM; committed.  Thanks, Mikhail!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incremental backups race,CASSANDRA-5410,12640117,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,01/Apr/13 14:01,12/Mar/19 14:00,13/Mar/19 22:29,04/Apr/13 21:43,1.2.4,,,,,,0,compaction,,,,"incremental backups does not mark things referenced or compacting, so it could get compacted away before createLinks runs.  Occasionally you can see this happen during ColumnFamilyStoreTest.  (Since it runs on the background tasks stage, it does not fail the test.)

{noformat}
    [junit] java.lang.RuntimeException: Tried to hard link to file that does not exist build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ja-8-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.createHardLink(FileUtils.java:72)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:1066)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.run(DataTracker.java:168)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,03/Apr/13 16:33;jbellis;5410.txt;https://issues.apache.org/jira/secure/attachment/12576804/5410.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-04 21:43:53.069,,,no_permission,,,,,,,,,,,,320580,,,Thu Apr 04 21:43:53 UTC 2013,,,,,,0|i1jba7:,320921,,,,,,,,krummas,krummas,,,,,,,,,,03/Apr/13 16:33;jbellis;Patch to snapshot synchronously before creating a new View.,"04/Apr/13 21:43;krummas;lgtm, committed as 8a422179c0a0a50e4d4b2e9274cbaf7259e90b2a",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remark on cassandra-5273 : Hanging system after OutOfMemory. Server cannot die due to uncaughtException handling,CASSANDRA-5716,12655574,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,ignaced,ignaced,01/Jul/13 10:28,12/Mar/19 14:00,13/Mar/19 22:29,02/Aug/13 03:30,1.2.7,,,,,,0,,,,,"Possible incorrect handling of an OOM as a result of modifications made for issue cassandra-5273.
I could reproduce the OOM, with the patch of Cassandra-5273 applied.
The good news is that, at least in my case, it works fine : the system did die !
 
However, due to multiple uncaughtException handling, multiple threads are calling the exitThread.start() routine, causing an IllegalStateException. There are some other exceptions also, but that seems logical. Also, after calling the start() function, the thread(s) will continue to run, and that could not be wanted.
 
Below I pasted the stack trace.
Just for your information, after all this works, and I could restart the Cassandra server and redo the OOM

[stack trace moved to http://aep.appspot.com/display/mQFNFHUh1VvQJYGcxRK0lQSM2j8/ ]",linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-07-01 16:12:56.988,,,no_permission,,,,,,,,,,,,335849,,,Wed Jul 03 08:29:07 UTC 2013,,,,,,0|i1lxjr:,336173,,,,,,,,,,,,,,,,,,,01/Jul/13 16:12;jbellis;fixed in 40f0bdce069db14e912f28d7351c4b602389c6a5,"02/Jul/13 16:49;ignaced;I did try your latest patch, applied on 1.2.5. Looks good, no more exceptions due to the start() calls. Question/remark : Also that implementation to shutdown allows the uncaught exceptions to finish without problems. Should these be blocked instead?",02/Jul/13 17:00;jbellis;I'm not sure what you mean.,"03/Jul/13 08:29;ignaced;Sorry, I forgot the fact that the thread is dead when the uncaught exception handling is called and thus was thinking that thread would continue to run until the system.exit would stop all the threads.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
restrict max num_tokens to something Gossip can handle,CASSANDRA-6267,12676467,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,29/Oct/13 19:19,12/Mar/19 14:00,13/Mar/19 22:29,30/Oct/13 15:46,1.2.12,2.0.3,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Oct/13 19:19;jbellis;6267.txt;https://issues.apache.org/jira/secure/attachment/12610907/6267.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-29 20:00:12.273,,,no_permission,,,,,,,,,,,,355899,,,Wed Oct 30 15:46:25 UTC 2013,,,,,,0|i1pct3:,356187,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,29/Oct/13 20:00;brandon.williams;+1,29/Oct/13 23:53;qconner;+1,30/Oct/13 15:46;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PasswordAuthenticator is incompatible with various Cassandra clients,CASSANDRA-5423,12640545,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,sdelmas,sdelmas,03/Apr/13 16:55,12/Mar/19 13:59,13/Mar/19 22:29,04/Apr/13 21:18,1.2.4,,,,,,0,security,,,,"Evidently with the old authenticator it was allowed to set keyspace, and then login.  With the org.apache.cassandra.auth.PasswordAuthenticator you have to login and then setkeyspace

For backwards compatibility it would be good to allow setting before login, and perform the actual operation/validation later after the login.

",,,,,,,,,,,,,,,,,,,,,,,,03/Apr/13 20:57;iamaleksey;5423.txt;https://issues.apache.org/jira/secure/attachment/12576850/5423.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-04 16:52:33.216,,,no_permission,,,,,,,,,,,,321005,,,Thu Apr 04 21:18:00 UTC 2013,,,,,,0|i1jdwn:,321346,,,,,,,,jbellis,jbellis,,,,,,,,,,04/Apr/13 16:52;jbellis;Shouldn't we set some flag on clientState so that we can validate it when the user performs a query?,04/Apr/13 16:55;iamaleksey;Why? We validate keyspaces anyway on each select/insert/update/drop/etc.,"04/Apr/13 17:08;jbellis;Where does that happen on e.g. the insert path?  Throwing NPE doesn't count. :)  (ensureHasAccess doesn't work either for AllowAll...  Even for other authenticators, I'm not sure we want to return ""no access"" instead of ""doesn't exist."")","04/Apr/13 17:11;iamaleksey;Well, a keyspace can be dropped after you call set_keyspace(). So I assumed we always validate keyspace existence. If we don't, we should.",04/Apr/13 17:15;jbellis;I don't think we do.  (Related: CASSANDRA-5358),"04/Apr/13 19:18;iamaleksey;We do. Look at 'find usages' of ThriftValidation validateTable and both validateColumnFamily methods (that do in turn call validateTable).

CASSANDRA-5358 is orthogonal (and IMO should just be resolved with 'not a problem').","04/Apr/13 21:01;jbellis;SGTM, +1","04/Apr/13 21:18;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompressedSequentialWriter can write zero-length segments during scrub,CASSANDRA-6791,12698154,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,kvaster,jbellis,jbellis,02/Mar/14 02:18,12/Mar/19 13:59,13/Mar/19 22:29,03/Mar/14 13:31,1.2.16,2.0.6,2.1 beta2,,,,0,,,,,"This results in errors like this:

{noformat}
java.lang.IllegalArgumentException
	at java.nio.Buffer.limit(Buffer.java:267)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.decompressChunk(CompressedRandomAccessReader.java:108)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.reBuffer(CompressedRandomAccessReader.java:87)
	at org.apache.cassandra.io.util.RandomAccessReader.seek(RandomAccessReader.java:280)
{noformat}

(Because a zero-length chunk actually turns into a length of -4 in the {{compressed.limit(chunk.length)}} call, since no checksum is written either.)

I thought this would be from two bad rows in a row, but it's not; the source file that resulted in scrub creating this, did not have any of those.  (But it does have several instances of bad-good-bad, i.e. separated by exactly one row, that is not large enough to force a new compressed chunk.)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-03-03 09:14:47.435,,,no_permission,,,,,,,,,,,,376623,,,Mon Mar 03 13:31:00 UTC 2014,,,,,,0|i1swdj:,376918,2.0.5,,,,,,,krummas,krummas,,,,,,,,,,"02/Mar/14 02:18;jbellis;Brandon has a copy of the data that can reproduce this, via [~kvaster].","02/Mar/14 02:18;jbellis;In the meantime, one workaround is disabling compression on the source CF.  (Remember that if no options are specified, compression is on by default in 1.1+.)",03/Mar/14 09:14;kvaster;Metadata info is truncated in wrong way. Attached patch.,"03/Mar/14 09:16;kvaster;Metadta info is tuncated in wrong way.

Here goes patch:

diff -r 452d640f07c0 src/java/org/apache/cassandra/io/compress/CompressedSequentialWriter.java
--- a/src/java/org/apache/cassandra/io/compress/CompressedSequentialWriter.java Mon Mar 03 12:11:44 2014 +0300
+++ b/src/java/org/apache/cassandra/io/compress/CompressedSequentialWriter.java Mon Mar 03 12:13:18 2014 +0300
@@ -231,7 +231,7 @@

         // truncate data and index file
         truncate(chunkOffset);
-        metadataWriter.resetAndTruncate(realMark.nextChunkIndex);
+        metadataWriter.resetAndTruncate(realMark.nextChunkIndex - 1);
     }

     /**
",03/Mar/14 13:31;krummas;committed with a unit test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in MeteredFlusher.run,CASSANDRA-6820,12699445,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,nff,nff,nff,07/Mar/14 15:50,12/Mar/19 13:59,13/Mar/19 22:29,22/Mar/14 03:01,2.0.7,,,,,,0,,,,,"Hello,

I've been seeing this exception with Cassandra 2.0.5:

{code}
ERROR 15:41:46,754 Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.db.MeteredFlusher.run(MeteredFlusher.java:40)
	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:75)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{code}

Could it be that {{Memtable.activelyMeasuring}} becomes null right after the test?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-03-22 03:01:48.81,,,no_permission,,,,,,,,,,,,377792,,,Sat Mar 22 03:01:48 UTC 2014,,,,,,0|i1t3jr:,378084,,,,,,,,jbellis,jbellis,,,,,,,,,,22/Mar/14 03:01;jbellis;I bet you are right.  Fixed in 535c56fb217c1a12d2fb9a217203c03d26642444,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoSuchMethodError when calling YamlConfigurationLoader.loadConfig(),CASSANDRA-5917,12664887,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,pookieman,pookieman,21/Aug/13 17:59,12/Mar/19 13:59,13/Mar/19 22:29,22/Aug/13 00:48,2.0.0,,,Legacy/CQL,,,0,,,,,"Hi, when this method is called, I see this:

java.lang.NoSuchMethodError: org.yaml.snakeyaml.Yaml.<init>(Lorg/yaml/snakeyaml/constructor/BaseConstructor;)V
	at org.apache.cassandra.config.YamlConfigurationLoader.loadConfig(YamlConfigurationLoader.java:86)
	at org.apache.cassandra.config.DatabaseDescriptor.loadConfig(DatabaseDescriptor.java:125)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:101)
......

I believe it's because of an enhancement made in ticket CASSANDRA-5606. The version of snakeyaml that 2.0.0-rc1 depends on is 1.6, but this constructor doesn't exist in that version but it does in version 1.12.

Coincidentally CASSANDRA-5317 speaks of upgrading the snakeyaml dependency, but I'm not sure what was upgraded. ",Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-08-21 18:06:09.466,,,no_permission,,,,,,,,,,,,344830,,,Thu Aug 22 00:48:19 UTC 2013,,,,,,0|i1ngr3:,345130,,,,,,,,,,,,2.0 rc1,,,,,,,21/Aug/13 18:06;jbellis;We're shipping 1.11 in lib/.  Is this some kind of maven-only problem?,"21/Aug/13 18:11;pookieman;The build.xml:

https://github.com/apache/cassandra/blob/cassandra-2.0.0-rc1/build.xml

seems to point to version 1.6 of snakeyaml",21/Aug/13 18:18;jbellis;Translation: yes.,"21/Aug/13 18:53;dbrosius;The problem is that the committed version of snakeyaml is 1.11 (in lib), but the ant(maven) specification is 1.6, which is why you don't see it in dev. Need to change maven version to 1.11 to match what we've all been using. Why do we have jars committed to git again?
",21/Aug/13 19:40;jbellis;Because Apache requires that we ship a license for every dependency and the only solution we've found is to hand-maintain that.,"21/Aug/13 19:57;dbrosius;i see..  btw, there is no metrics-core license in 1.2",21/Aug/13 20:01;jbellis;We should add one. /cc [~yukim],21/Aug/13 20:44;yukim;[~jbellis] done in 0db3413548552ea044890b7d7ffd0dbed0078be1,22/Aug/13 00:48;dbrosius;fix ninja'd to cassandra-2.0.0 as commit 8551ff93e9de972bc604d1804b86a10ecb2d3382,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disablebinary nodetool command,CASSANDRA-5425,12640639,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,michalm,j.casares,j.casares,03/Apr/13 21:59,12/Mar/19 13:59,13/Mar/19 22:29,05/Apr/13 17:03,1.2.4,,,,,,0,datastax_qa,,,,"The following commands are available via `nodetool`:
{CODE}
  disablehandoff         - Disable the future hints storing on the current node
  disablegossip          - Disable gossip (effectively marking the node dead)
  disablethrift          - Disable thrift server
{CODE}

Is it possible to get disablebinary added to help with the testing of binary client drivers?",,,,,,,,,,,,,,,,,,,,,,,,05/Apr/13 17:00;michalm;5425-statusbinary.txt;https://issues.apache.org/jira/secure/attachment/12577237/5425-statusbinary.txt,04/Apr/13 17:44;michalm;5425-v1.txt;https://issues.apache.org/jira/secure/attachment/12577021/5425-v1.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-04-03 22:40:40.794,,,no_permission,,,,,,,,,,,,321098,,,Fri Apr 05 17:03:12 UTC 2013,,,,,,0|i1jei7:,321443,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,03/Apr/13 22:40;brandon.williams;That seems reasonable.,04/Apr/13 17:44;michalm;Disable/enable native binary transport using nodetool (plus extending one unrelated log message for thrift server).,"05/Apr/13 14:54;brandon.williams;Committed, thanks!","05/Apr/13 15:22;jjordan;Should we add ""statusbinary"" as well?  Like we have statusthrift?","05/Apr/13 16:59;michalm;Good point, I think it's reasonable.",05/Apr/13 17:00;michalm;Added statusbinary command.,05/Apr/13 17:03;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Manifest file not fsynced,CASSANDRA-5535,12645997,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,krummas,cumarana,cumarana,03/May/13 15:54,12/Mar/19 13:59,13/Mar/19 22:29,04/May/13 18:59,1.1.12,1.2.5,,,,,0,,,,,"We had several cases where the the manifest file would get corrupted when doing power reset tests or iLO resets to mimic power failure scenarios, ungraceful resets, kernel panics etc. It wasn't clear at the time where the problem was, but I think the data below shows that Cassandra is missing an fsync call to the manifest file prior to closing it. This particular stack trace from below is on Cassandra 1.2.4.
The trace was captured using strace options:

strace -f -p 2200 -e trace=open,close,write,fsync,fdatasync,rename
[pid 9710] open(""/opt/mp/storage/persistent/cassandra/cassandra-lib/data/MSA/subinfo/subinfo-tmp.json"", O_WRONLY|O_CREAT|O_TRUNC, 0666) = 238
[pid 9710] write(238, ""{\n \""generations\"" : [ {\n \""gen""..., 3996) = 3996
[pid 9710] write(238, ""14, 263161, 263484, 270816, 2593""..., 3996) = 3996
[pid 9710] write(238, ""275136, 275137, 275138, 275139, ""..., 1173) = 1173
[pid 9710] close(238) = 0
[pid 9710] rename(""/opt/mp/storage/persistent/cassandra/cassandra-lib/data/MSA/subinfo/subinfo.json""
, ""/opt/mp/storage/persistent/cassandra/cassandra-lib/data/MSA/subinfo/subinfo-old.json"") = 0
[pid 9710] rename(""/opt/mp/storage/persistent/cassandra/cassandra-lib/data/MSA/subinfo/subinfo-tmp.j
son"", ""/opt/mp/storage/persistent/cassandra/cassandra-lib/data/MSA/subinfo/subinfo.json"") = 0
","RHEL 6.4
java -version
java version ""1.6.0_31""
Java(TM) SE Runtime Environment (build 1.6.0_31-b04)
Java HotSpot(TM) 64-Bit Server VM (build 20.6-b01, mixed mode)
",,,,,,,,,,,,,,,,,,,,,,,03/May/13 18:36;krummas;0001-CASSANDRA-5535-fsync-leveled-manifest.patch;https://issues.apache.org/jira/secure/attachment/12581724/0001-CASSANDRA-5535-fsync-leveled-manifest.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-03 16:01:22.781,,,no_permission,,,,,,,,,,,,326356,,,Sat May 04 18:59:04 UTC 2013,,,,,,0|i1kax3:,326701,,,,,,,,,,,,,,,,,,,"03/May/13 16:01;jbellis;We've already fixed this in 2.0 by moving level information into sstable metadata (CASSANDRA-4872), but it may be reasonable to add an fsync to 1.2.  What do you think [~krummas]?","03/May/13 16:04;krummas;yeah sounds good, ill fix",03/May/13 18:37;krummas;fsyncs the file after flushing,"03/May/13 23:55;carlyeks;Looks good. The patch doesn't apply to the 1.1 branch; I'm sure it will be easy to fix up so that it does, assuming this is target to 1.1 and 1.2.","04/May/13 05:09;jbellis;+1

(setting affects-version to when this was introduced)","04/May/13 18:59;krummas;committed as fe910e6c90d81cc61c16859bcef9f0dcb42cc827, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add tombstone metrics to cfstats or cfhistograms,CASSANDRA-5889,12663700,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mishail,jbellis,jbellis,14/Aug/13 17:45,12/Mar/19 13:59,13/Mar/19 22:29,08/Oct/13 21:28,1.2.11,2.0.2,,Legacy/Tools,,,0,,,,,/cc [~pmcfadin],,,,,,,,,,,,,,,,,,,,,,,,08/Oct/13 19:36;mishail;cassandra-1.2-5889.patch;https://issues.apache.org/jira/secure/attachment/12607416/cassandra-1.2-5889.patch,08/Oct/13 05:32;mishail;cassandra-2.0-5889.patch;https://issues.apache.org/jira/secure/attachment/12607315/cassandra-2.0-5889.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-10-07 03:21:44.195,,,no_permission,,,,,,,,,,,,343701,,,Wed Oct 09 17:04:19 UTC 2013,,,,,,0|i1n9tr:,344005,,,,,,,,jbellis,jbellis,,,,,,,,,,"07/Oct/13 03:09;jbellis;Want to take a stab at this, [~mishail]?",07/Oct/13 03:21;mishail;[~jbellis] will do my best. ,08/Oct/13 05:32;mishail;Add tombstones/live cells count and ratio to nodetool output,"08/Oct/13 18:53;jbellis;Sorry, realized we committed the histogram to 1.2 as well.  Can you post a patch for that branch too?",08/Oct/13 19:36;mishail;Patch for 1.2,08/Oct/13 21:28;mishail;https://github.com/apache/cassandra/commit/f31e399abe01c0e78f3b8289c562c2f279f04c2c,09/Oct/13 17:04;jbellis;Yes. :),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cqlsh shouldn't display empty ""value alias""",CASSANDRA-6139,12672149,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,slebresne,slebresne,03/Oct/13 14:22,12/Mar/19 13:59,13/Mar/19 22:29,21/Oct/13 15:16,2.0.2,,,,,,0,,,,,"When someone creates:
{noformat}
CREATE TABLE foo (
   k int,
   v int,
   PRIMARY KEY (k, v)
) WITH COMPACT STORAGE
{noformat}
then we internally create a ""value alias"" (1.2)/""compact value definition"" (2.0) with an empty name. Seems that cqlsh don't recognize that fact and display that as:
{noformat}
cqlsh:ks> DESC TABLE foo;

CREATE TABLE foo (
  k int,
  v int,
  """" blob,
  PRIMARY KEY (k, v)
) WITH COMPACT STORAGE AND ...
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,21/Oct/13 08:49;iamaleksey;6139.txt;https://issues.apache.org/jira/secure/attachment/12609391/6139.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-10-21 08:50:36.196,,,no_permission,,,,,,,,,,,,351775,,,Mon Oct 21 15:16:09 UTC 2013,,,,,,0|i1ongf:,352063,2.0.1,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,21/Oct/13 08:50;iamaleksey;Trivial patch attached.,21/Oct/13 14:58;brandon.williams;+1,"21/Oct/13 15:16;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
relocatingTokens should be ConcurrentMap,CASSANDRA-5634,12652563,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,13/Jun/13 05:22,12/Mar/19 13:59,13/Mar/19 22:29,19/Jun/13 18:56,1.2.6,,,,,,0,vnodes,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Jun/13 05:23;jbellis;5634.txt;https://issues.apache.org/jira/secure/attachment/12587562/5634.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-19 17:49:39.09,,,no_permission,,,,,,,,,,,,332887,,,Wed Jun 19 18:56:22 UTC 2013,,,,,,0|i1lfc7:,333215,,,,,,,,brandon.williams,brandon.williams,,,,,,,,,,13/Jun/13 05:23;jbellis;Trivial patch.,19/Jun/13 17:49;brandon.williams;+1,19/Jun/13 18:56;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sudden NPE while accessing Cassandra with CQL driver,CASSANDRA-5779,12658773,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,irengrig,irengrig,19/Jul/13 15:43,12/Mar/19 13:58,13/Mar/19 22:29,22/Jul/13 09:25,1.2.7,,,,,,0,,,,,"java.lang.NullPointerException at org.apache.cassandra.db.RowMutation.addOrGet(RowMutation.java:153)
 at org.apache.cassandra.cql3.statements.UpdateStatement.mutationForKey(UpdateStatement.java:216)
 at org.apache.cassandra.cql3.statements.UpdateStatement.getMutations(UpdateStatement.java:133)
 at org.apache.cassandra.cql3.statements.BatchStatement.getMutations(BatchStatement.java:99)
at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:88)
at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:118)
at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:128)
at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:87)
at org.apache.cassandra.transport.Message$Dispatcher.messageReceived(Message.java:287)
at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:75)
at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:565)
at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:793)
at org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:45)
at org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:69)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:724)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-07-19 16:36:37.556,,,no_permission,,,,,,,,,,,,338967,,,Mon Jul 22 09:25:29 UTC 2013,,,,,,0|i1mgqv:,339287,,,,,,,,,,,,,,,,,,,19/Jul/13 16:36;slebresne;Is that possible that you've dropped a table on which you were writing?,"21/Jul/13 13:35;irengrig;I rechecked logs, and see that indeed, we have been dropping keyspace asynchronously (that was in test),
maybe it's our error to not synchronize flushing buffers vs finishing execution...

However, I suppose that NPE is not good anyway, I'd rather prefer to get ""table doesn't exist"" message","22/Jul/13 09:25;slebresne;bq. However, I suppose that NPE is not good anyway, I'd rather prefer to get ""table doesn't exist"" message

I agree with you in theory. Ideally, we'd want to throw a meaningful error message for the query. However, the problem here is that you dropped the keyspace concurrently of inserting in that keyspace. And making sure we catch such race properly in all cases would, while probably possible, be prohibitive performance wise for queries (and distribution makes this even worst). It's just not worse it.

So at the end of the day, concurrently dropping a table/keyspace while requesting it (reads or writes) is just not supported you should synchonize client side to avoid it.

For what it's worth, I've committed a small change (commit 86a077a190a1cec89112b9f1fd991f0fe06d2423) so that this case shouldn't happen anymore. Instead, in that same situation, an assertion error would get triggered that should at least have a more meaningful error message. And so I'll consider this ""fixed"".  But:
# this will still not be an error returned to the client. So really, you should synchronize client side.
# I do not pretend that a race between a keyspace drop and inserts can't yield a NPE anywhere else in the code.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The token function should allow column identifiers in the correct order only,CASSANDRA-6075,12669792,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,blerer,mfiguiere,mfiguiere,21/Sep/13 02:06,12/Mar/19 13:58,13/Mar/19 22:29,24/Sep/14 02:10,2.0.11,2.1.1,,Legacy/CQL,,,0,cql,,,,"Given the following table:
{code}
CREATE TABLE t1 (a int, b text, PRIMARY KEY ((a, b)));
{code}

The following request returns an error in cqlsh as literal arguments order is incorrect:
{code}
SELECT * FROM t1 WHERE token(a, b) > token('s', 1);
Bad Request: Type error: 's' cannot be passed as argument 0 of function token of type int
{code}

But surprisingly if we provide the column identifier arguments in the wrong order no error is returned:
{code}
SELECT * FROM t1 WHERE token(a, b) > token(1, 'a'); // correct order is valid
SELECT * FROM t1 WHERE token(b, a) > token(1, 'a'); // incorrect order is valid as well
{code}",Cassandra 1.2.9,,,,,,,,,,,,,,,,,,,,,,,24/Sep/14 01:51;iamaleksey;6075-fix-v2.txt;https://issues.apache.org/jira/secure/attachment/12670873/6075-fix-v2.txt,23/Sep/14 08:53;blerer;CASSANDRA-2.0-6075-PART2.txt;https://issues.apache.org/jira/secure/attachment/12670668/CASSANDRA-2.0-6075-PART2.txt,23/Sep/14 08:53;blerer;CASSANDRA-2.1-6075-PART2.txt;https://issues.apache.org/jira/secure/attachment/12670669/CASSANDRA-2.1-6075-PART2.txt,18/Sep/14 10:08;blerer;CASSANDRA-2.1-6075.txt;https://issues.apache.org/jira/secure/attachment/12669692/CASSANDRA-2.1-6075.txt,03/Sep/14 11:49;blerer;CASSANDRA-6075.txt;https://issues.apache.org/jira/secure/attachment/12666209/CASSANDRA-6075.txt,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2014-09-03 11:49:30.422,,,no_permission,,,,,,,,,,,,349720,,,Wed Sep 24 02:10:05 UTC 2014,,,,,,0|i1oauv:,350018,,,,,,,,thobbs,thobbs,,,,,,,,,,03/Sep/14 11:49;blerer;The patch add a test at the end of SelectStatement.processPartitionKeyRestrictions (once we are sure that no argument is missing) to check that the token function arguments have been specify in the good order.,10/Sep/14 22:02;thobbs;I assume this is a problem for 2.1 as well.  Can you post a 2.1 patch as well?  The main thing that should change is the unit test (which can extend CQLTester in 2.1).,18/Sep/14 10:08;blerer;This is the patch for the 2.1 branch. It differs from the one of the 2.0 branch because the 2.1 branch is using CFMetaData instead of CFDefinition. The test has also been migrated to CQLTester,"18/Sep/14 19:02;thobbs;If I specify the partition key items out of order, I get a somewhat confusing type error message:

{noformat}
cqlsh:ks1> create table foo (a int, b text, c int, d int, PRIMARY KEY ((a, b, c)));
cqlsh:ks1> select * from foo WHERE token(a, c, b) > token(0, 0, 'a');
Bad Request: Type error: 0 cannot be passed as argument 1 of function token of type text
{noformat}

We need to do the order check prior to the type check or find a way to do the type check properly.",18/Sep/14 20:07;blerer;I am impressed Tyler. I would have never thought about testing such a crappy case.,"19/Sep/14 09:07;blerer;I had a look and it is not easy to solve that problem. 
The problem come from the fact that we build the restrictions at the same time that we do part of our validation. We will try to compute the value of token(0, 0, 'a') before we even know if we have all the columns required within the token function or if their is a mix of restrictions using token function that we do not support. If we want to do those check earlier we will end up duplicating the test that we do on Restrictions on Relations.
This seems to indicate a problem in the code. I believe that the code should first perform the validation checks on Relations then build Restrictions.
I planning to refactor SelectStatement and I will be able to tackle that problem properly at that time but I want to do that refactoring in 3.0 as it is not a trivial stuff to do.
So my proposal is to deliver the current fix in 2.0 and 2.1 (knowing that it is not perfect) and properly fix it for 3.0.
I opened #CASSANDRA-7981 for the SelectStatement refactoring were I keep track of this issue. 
What do you think Tyler?","19/Sep/14 16:26;thobbs;Seems reasonable to me.

+1, committed.","19/Sep/14 20:57;iamaleksey;This breaks pig tests on trunk, at least.

[~blerer] to run pig tests, do 'ant pig-test'. Ignore most the warnings - there are a lot of them.","23/Sep/14 08:53;blerer;Completely forgot the case of the slice with start and end bound(e.g. token(key) >= token(1) and token(key) < token(2))
That is what broke the Pig-Tests. Sorry.
Here are the patchs to fix that problem on the 2.0 and 2.1+ branches.

I also dicover while trying to add more tests than the current approach to handle token functions is broken. With the current approach we cannot reject queries like:
{{SELECT * FROM %s WHERE token(a, b) > token(?, ?) and token(b) < token(?, ?)}} or {{SELECT * FROM %s WHERE token(a) > token(?, ?) and token(b) > token(?, ?)}} 
I will try to find another solution as part of #CASSANDRA-7981","24/Sep/14 02:10;iamaleksey;Fixed in https://github.com/apache/cassandra/commit/b1166c09983b1678cbc4b241f1da860930c571a5

Used Guava's Iterators#cycle() instead of the null/hasnext check, and removed the (if cfDef.partitionKeyCount() > 0) condition, which is always true.

Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update cassandra-stress README,CASSANDRA-6681,12694073,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,mstump,mstump,08/Feb/14 05:33,12/Mar/19 13:58,13/Mar/19 22:29,29/Jul/14 22:56,,,,Legacy/Tools,,,0,lhf,,,,"The cassandra-stress README hasn't been updated to reflect the recent work in the 2.x releases. Additionally, no example is given in the help string that demonstrates how to use various parameters. For example no input format is given for the ""-rate"" parameter.  It could be threads>=N, threads=n, limit=N etc..",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-02-08 15:36:11.259,,,no_permission,,,,,,,,,,,,372582,,,Tue Jul 29 22:56:15 UTC 2014,,,,,,0|i1s7jz:,372886,,,,,,,,,,,,,,,,,,,08/Feb/14 15:36;benedict;cassandra-stress help -rate,"29/Jul/14 22:56;mshuler;-rate and (I think) pretty complete usage help on all options was included in 2.1 stress changes - not sure why this ticket remained open after the comment above.
{noformat}
(cassandra-2.1)mshuler@hana:~/git/cassandra$ cassandra-stress help -rate

Usage: -rate threads=? [limit=?]
 OR 
Usage: -rate [auto] [threads>=?] [threads<=?]

  threads=?                                run this many clients concurrently
  limit=? (default=0/s)                    limit operations per second across all clients
  auto                                     test with increasing number of threadCount until performance plateaus
  threads>=? (default=4)                   run at least this many clients concurrently
  threads<=? (default=1000)                run at most this many clients concurrently
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
allow now() -> uuid compatibility,CASSANDRA-6766,12697050,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thobbs,jbellis,jbellis,25/Feb/14 01:56,12/Mar/19 13:58,13/Mar/19 22:29,20/Jun/14 17:02,2.0.9,2.1 rc2,,Legacy/CQL,,,0,cql,,,,"Bad Request: Type error: cannot assign result of function now (type timeuuid) to id (type uuid)
",,,,,,,,,,,,,,,,,,,,,,,,19/Jun/14 17:26;thobbs;6766-v2.txt;https://issues.apache.org/jira/secure/attachment/12651453/6766-v2.txt,18/Jun/14 18:29;thobbs;6766.txt;https://issues.apache.org/jira/secure/attachment/12651206/6766.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-03-03 11:45:38.9,,,no_permission,,,,,,,,,,,,375524,,,Fri Jun 20 17:02:01 UTC 2014,,,,,,0|i1spm7:,375820,,,,,,,,slebresne,slebresne,,,,,,,,,,"03/Mar/14 11:45;slebresne;I'll remark that because the code for functions is relatively generic, we can't just special this for now() (nor should we really). Meaning that what we need here is to introduce a bit of subtyping in the type system and make timeuuid a subtype of uuid. Which in itself if probably not that hard, but mentioning it because it's still probably a bit more involved than what the ticket title imply.","04/Jun/14 21:29;thobbs;Couldn't we just use {{AbstractType.isValueCompatibleWith()}}?  It looks like some of the types don't properly implement this (e.g. UUIDType does not return true for TimeUUIDType), but that can be fixed here.","05/Jun/14 06:08;slebresne;bq. Couldn't we just use AbstractType.isValueCompatibleWith()?

Absolutely. That method declares a subtyping relation really :)","18/Jun/14 18:29;thobbs;6766.txt (and a [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-6766]) adjusts {{isValueCompatibleWith()}} for a few types and uses it in place of type equality checks for assignment.

ReversedType makes the {{isValueCompatibleWith()}} logic a little hairy.  Besides getting rid of ReversedType or assuming that {{isValueCompatibleWith()}} will always be called on the receiver's type (and not the value to be received), I'm not sure what else we can do.","19/Jun/14 14:07;slebresne;bq. ReversedType makes the isValueCompatibleWith() logic a little hairy.

Why can't we just add a ReversedType.isValueCompatibleWith override that calls isValueCompatibleWith on the base type?","19/Jun/14 16:01;thobbs;Because you have to assume that you're always calling {{someReversedTypeInstance.isValueCompatibleWith(someOtherType)}} (i.e., {{LongType.instance.isValueCompatibleWith(ReversedType.getInstance(LongType.instance))}} would return false).  With the current usage, that would work, but it seems very prone to bugs.","19/Jun/14 16:29;slebresne;Fair enough, Then what about splitting into 2 methods: the real {{isValueCompatibleWith}} in AbstractType would check if the argument is ReversedType and if so, would ""unwrap"" it, and otherwise would call an auxiliary {{isValueCompatibleWithInternal}} which is the one the subclasses would override. Not absolutely sublime, but at least deal with ReversedType in one place versus doing it in very override of isValueCompatibleWith. ",19/Jun/14 16:50;thobbs;Seems reasonable to me.,19/Jun/14 17:26;thobbs;v2 patch unwraps ReversedType arguments and subclasses now implement {{isValueCompatibleWithInternal()}}.  The branch is also updated.,20/Jun/14 13:22;slebresne;+1,"20/Jun/14 17:02;thobbs;Thanks, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-stress should fail if the same option is provided multiple times,CASSANDRA-6834,12700681,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,benedict,benedict,benedict,11/Mar/14 11:50,12/Mar/19 13:58,13/Mar/19 22:29,14/Mar/14 09:34,2.1 beta2,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11/Mar/14 12:28;benedict;6834.txt;https://issues.apache.org/jira/secure/attachment/12633899/6834.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-11 13:11:37.327,,,no_permission,,,,,,,,,,,,379024,,,Tue Mar 11 13:11:37 UTC 2014,,,,,,0|i1tb53:,379316,,,,,,,,,,,,2.1 beta1,,,,,,,"11/Mar/14 12:28;benedict;Attached fix for this, and also a tidy up of some command line help printing (distributions now have an explanation next to them, and a commands supporting multiple writes/reads at once, e.g. readmulti, correctly print the at-once option)",11/Mar/14 13:11;lyubent;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileCacheService overcounting its memoryUsage,CASSANDRA-6838,12700865,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,benedict,benedict,benedict,12/Mar/14 00:52,12/Mar/19 13:58,13/Mar/19 22:29,12/Mar/14 04:20,2.0.7,2.1 beta2,,,,,0,performance,,,,"On investigating why I was seeing dramatically worse performance for counter updates over prepared CQL3 statements compred to unprepared CQL2 statements, I stumbled upon a bug in FileCacheService wherein, on returning a cached reader back to the pool, its memory is counted again towards the total memory usage, but is not matched by a decrement when checked out. So we effectively are probably not caching readers most of the time.
",,,,,,,,,,,,,,,,,,,,,,,,12/Mar/14 00:53;benedict;6838.txt;https://issues.apache.org/jira/secure/attachment/12634064/6838.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-12 04:20:32.537,,,no_permission,,,,,,,,,,,,379208,,,Wed Mar 12 04:20:32 UTC 2014,,,,,,0|i1tc9r:,379500,,,,,,,,jbellis,jbellis,,,,,,,,,,"12/Mar/14 04:20;jbellis;LGTM, committed to 2.0 + 2.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possibly repairing with verbose nodes,CASSANDRA-6808,12699186,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,06/Mar/14 16:32,12/Mar/19 13:58,13/Mar/19 22:29,06/Mar/14 21:26,2.1 beta2,,,,,,0,,,,,"Incremental repair first sends prepare message to replica(endpoint) of all ranges repairing. Following to that, each repair session starts with replica of certain range but it is given replica of all ranges.
",,,,,,,,,,,,,,,,,,,,,,,,06/Mar/14 16:33;yukim;6808-2.1.txt;https://issues.apache.org/jira/secure/attachment/12633169/6808-2.1.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-06 17:34:22.032,,,no_permission,,,,,,,,,,,,377533,,,Thu Mar 06 21:26:19 UTC 2014,,,,,,0|i1t1z3:,377827,,,,,,,,krummas,krummas,,,2.1 beta1,,,,,,,06/Mar/14 16:33;yukim;Attaching simple patch to fix.,06/Mar/14 17:34;krummas;+1,"06/Mar/14 21:26;yukim;Committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool no longer shows node joining,CASSANDRA-6811,12699255,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,brandon.williams,brandon.williams,06/Mar/14 20:14,12/Mar/19 13:58,13/Mar/19 22:29,12/Mar/14 04:18,1.2.16,2.0.7,,,,,0,,,,,"When we added effective ownership output to nodetool ring/status, we accidentally began excluding joining nodes because we iterate the ownership maps instead of the the endpoint to token map when printing the output, and the joining nodes don't have any ownership.  The simplest thing to do is probably iterate the token map instead, and not output any ownership info for joining nodes.",,,,,,,,,,,,,,,,,,,,,,,,11/Mar/14 09:56;vijay2win@yahoo.com;0001-CASSANDRA-6811-v2.patch;https://issues.apache.org/jira/secure/attachment/12633881/0001-CASSANDRA-6811-v2.patch,08/Mar/14 23:48;brandon.williams;ringfix.txt;https://issues.apache.org/jira/secure/attachment/12633570/ringfix.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-03-08 21:36:54.385,,,no_permission,,,,,,,,,,,,377602,,,Wed Mar 12 04:18:35 UTC 2014,,,,,,0|i1t2dz:,377895,,,,,,,,brandon.williams,brandon.williams,,,1.2.0 beta 1,,,,,,,"08/Mar/14 21:36;vijay2win@yahoo.com;Just to clarify we have this information as a part of ""nodetool status"", we might want to display the same information with 0% Owns? or potentially what it will own after the transient state.
","08/Mar/14 23:48;brandon.williams;Here's a patch that illustrates what I mean and fixes 'ring' output.  'status' is a bit more involved, but considering how long we've done this I'm fine with not getting fancy about inferring ownership on joining nodes.",09/Mar/14 00:17;brandon.williams;The easiest way to test this is to put a node into write survey mode {{-Dcassandra.write_survey_mode=true}},"09/Mar/14 20:41;vijay2win@yahoo.com;Thanks, +1 and committed...

Inlined the comparison and replaced the empty string with ""?""

{code}
Note: Ownership information does not include topology; for complete information, specify a keyspace

Datacenter: datacenter1
==========
Address         Rack        Status State   Load            Owns                Token                                       

17.198.227.158  rack1       Up     Joining 9.44 KB         ?                   -7564491331177403445                        
17.198.227.157  rack1       Up     Normal  14.03 KB        100.00%             -2195150324932625854                        

-bash-4.1$ ./apache-cassandra-1.2.15-SNAPSHOT/bin/nodetool ring
Note: Ownership information does not include topology; for complete information, specify a keyspace

Datacenter: datacenter1
==========
Address         Rack        Status State   Load            Owns                Token                                       
                                                                               -2195150324932625854                        
17.198.227.158  rack1       Up     Normal  10.77 KB        70.89%              -7564491331177403445                        
17.198.227.157  rack1       Up     Normal  14.03 KB        29.11%              -2195150324932625854                        

-bash-4.1$ ./apache-cassandra-1.2.15-SNAPSHOT/bin/nodetool ring
Note: Ownership information does not include topology; for complete information, specify a keyspace

Datacenter: datacenter1
==========
Address         Rack        Status State   Load            Owns                Token                                       
                                                                               -2195150324932625854                        
17.198.227.158  rack1       Up     Leaving 10.77 KB        70.89%              -7564491331177403445                        
17.198.227.157  rack1       Up     Normal  14.03 KB        29.11%              -2195150324932625854                       
{code}",10/Mar/14 19:51;brandon.williams;We shouldn't resolve this until we fix 'status' too.,"11/Mar/14 09:56;vijay2win@yahoo.com;Sorry, I misunderstood the comment thinking nodetool status doesn't need a fix...
Please see the attached, actually it fixes the joining status for Multi DC setup. (http://pastebin.com/QUqcPJen)

NOTE: 
I did a little bit of refactoring to reuse the code, hope it is ok.
Effective ownership for nodes might be broken before and after this patch, since every line shows the host ownership not the line items ownership.","11/Mar/14 18:26;brandon.williams;LGTM, and more efficient by not making a jmx call for every node just to get the first token. +1","12/Mar/14 04:18;vijay2win@yahoo.com;Committed, Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove SimpleCondition,CASSANDRA-5691,12654372,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,ash2k,ash2k,ash2k,23/Jun/13 10:20,12/Mar/19 13:58,13/Mar/19 22:29,26/Jun/13 14:56,,,,,,,0,,,,,"Problematic scenario:
1. two threads get blocked in SimpleCondition.await();
2. some thread calls SimpleCondition.signal();
3. one of blocked threads wakes up and runs;
4. spurious wakeup happens in the second thread and it wakes up too and runs even though nobody signaled it.

Thus this is a broken implementation of Condition interface.

Anyway, looking at how code uses it, SimpleCondition can just be replaced with CountDownLatch.",,,,,,,,,,,,,,,,,,,,,,,,25/Jun/13 16:33;ash2k;trunk-5691-v2.patch;https://issues.apache.org/jira/secure/attachment/12589622/trunk-5691-v2.patch,23/Jun/13 10:21;ash2k;trunk-5691.patch;https://issues.apache.org/jira/secure/attachment/12589307/trunk-5691.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-06-24 13:39:17.779,,,no_permission,,,,,,,,,,,,334649,,,Wed Jun 26 14:56:33 UTC 2013,,,,,,0|i1lq67:,334975,,,,,,,,jbellis,jbellis,,,,,,,,,,"24/Jun/13 13:39;jbellis;Yes, it's pretty obvious that signal==signalAll in SimpleCondition.  (Signaling just a single waiter is fairly useless and not something we care about.)

Unless CDL offers a performance benefit I don't see any point in churning this.","25/Jun/13 03:18;ash2k;I see following reasons to make this change:
1. It removes code that have a bug;
2. It removes code that is not tested;
3. Why invent a wheel? CountDownLatch(1) is a common way to do this kind of signaling and is widely used and known. So it's easier to understand code.

If the patch is not applied then at least singnal() invocations should be replaced with signalAll() and signal() should throw UnsupportedOperationException.

p.s. I don't think CDL offers any better or worse performance.","25/Jun/13 05:48;jbellis;bq. If the patch is not applied then at least singnal() invocations should be replaced with signalAll() and signal() should throw UnsupportedOperationException.

Agreed.","25/Jun/13 16:33;ash2k;Attaching v2 patch.

Can you please explain your point of view? Why do you prefer SimpleCondition that is not really a Condition to CountDownLatch?","26/Jun/13 14:56;jbellis;Because ""I like CountdownLatch better"" isn't a very compelling reason to change a core construct?

Committed v2, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ant codecoverage no longer works,CASSANDRA-6800,12698793,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mishail,appodictic,appodictic,05/Mar/14 02:42,12/Mar/19 13:58,13/Mar/19 22:29,14/Mar/14 20:55,2.0.7,2.1 beta2,,Legacy/Testing,,,0,lhf,,,,Code coverage does not run currently due to cobertura jdk incompatibility. Fix is coming. ,,,,,,,,,,,,,,,,,,,CASSANDRA-6877,,,,,14/Mar/14 20:42;mishail;2014-03-14 13-41-02.png;https://issues.apache.org/jira/secure/attachment/12634830/2014-03-14+13-41-02.png,14/Mar/14 05:04;mishail;cassandra-2.1-6800.patch;https://issues.apache.org/jira/secure/attachment/12634648/cassandra-2.1-6800.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-03-05 15:16:44.044,,,no_permission,,,,,,,,,,,,377141,,,Fri Mar 14 20:55:42 UTC 2014,,,,,,0|i1szkn:,377436,,,,,,,,jbellis,jbellis,,,,,,,,,,"05/Mar/14 02:45;appodictic;{code}
diff --git a/build.xml b/build.xml
index 3a5f8bd..41ae7c9 100644
--- a/build.xml
+++ b/build.xml
@@ -93,7 +93,7 @@
     <property name=""test.long.timeout"" value=""600000"" />

     <!-- http://cobertura.sourceforge.net/ -->
-    <property name=""cobertura.version"" value=""1.9.4.1""/>
+    <property name=""cobertura.version"" value=""2.0.2""/>
     <property name=""cobertura.build.dir"" value=""${build.dir}/cobertura""/>
     <property name=""cobertura.report.dir"" value=""${cobertura.build.dir}/report""/>
     <property name=""cobertura.classes.dir"" value=""${cobertura.build.dir}/classes""/>
{code}","05/Mar/14 15:16;jbellis;I'm getting a lot of errors even after realclean.  The first is:

{noformat}
cobertura-instrument:
[cobertura-instrument] Cobertura null - GNU GPL License (NO WARRANTY) - See COPYRIGHT file
[cobertura-instrument] WARN   instrumentClass, Unable to instrument file /Users/jbellis/projects/cassandra/git/build/classes/main/org/apache/cassandra/cli/CliClient.class
[cobertura-instrument] java.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.cassandra.thrift.CounterSuperColumn
[cobertura-instrument] 	at org.objectweb.asm.ClassWriter.getCommonSuperClass(Unknown Source)
[cobertura-instrument] 	at org.objectweb.asm.ClassWriter.a(Unknown Source)
[cobertura-instrument] 	at org.objectweb.asm.Frame.a(Unknown Source)
[cobertura-instrument] 	at org.objectweb.asm.Frame.a(Unknown Source)
[cobertura-instrument] 	at org.objectweb.asm.MethodWriter.visitMaxs(Unknown Source)
[cobertura-instrument] 	at org.objectweb.asm.MethodVisitor.visitMaxs(Unknown Source)
[cobertura-instrument] 	at org.objectweb.asm.util.CheckMethodAdapter.visitMaxs(Unknown Source)
[cobertura-instrument] 	at org.objectweb.asm.MethodVisitor.visitMaxs(Unknown Source)
[cobertura-instrument] 	at org.objectweb.asm.commons.LocalVariablesSorter.visitMaxs(Unknown Source)
[cobertura-instrument] 	at org.objectweb.asm.tree.MethodNode.accept(Unknown Source)
[cobertura-instrument] 	at org.objectweb.asm.util.CheckMethodAdapter$1.visitEnd(Unknown Source)
[cobertura-instrument] 	at org.objectweb.asm.MethodVisitor.visitEnd(Unknown Source)
[cobertura-instrument] 	at org.objectweb.asm.util.CheckMethodAdapter.visitEnd(Unknown Source)
[cobertura-instrument] 	at org.objectweb.asm.ClassReader.b(Unknown Source)
[cobertura-instrument] 	at org.objectweb.asm.ClassReader.accept(Unknown Source)
[cobertura-instrument] 	at org.objectweb.asm.ClassReader.accept(Unknown Source)
[cobertura-instrument] 	at net.sourceforge.cobertura.instrument.CoberturaInstrumenter.instrumentClass(CoberturaInstrumenter.java:204)
[cobertura-instrument] 	at net.sourceforge.cobertura.instrument.CoberturaInstrumenter.instrumentClass(CoberturaInstrumenter.java:121)
[cobertura-instrument] 	at net.sourceforge.cobertura.instrument.CoberturaInstrumenter.addInstrumentationToSingleClass(CoberturaInstrumenter.java:233)
[cobertura-instrument] 	at net.sourceforge.cobertura.instrument.Main.addInstrumentationToSingleClass(Main.java:274)
[cobertura-instrument] 	at net.sourceforge.cobertura.instrument.Main.addInstrumentation(Main.java:283)
[cobertura-instrument] 	at net.sourceforge.cobertura.instrument.Main.parseArguments(Main.java:373)
[cobertura-instrument] 	at net.sourceforge.cobertura.instrument.Main.main(Main.java:395)
{noformat}","05/Mar/14 15:23;appodictic;I noticed that. This is pretty weird. From maven I have used the cobertura plugin, worked great. What a PITA ant is/ Maybe we should switch to maven :)

I made it all the way though the process and it build the cobertura.ser but ran into some problem with the report target. I will keep looking at it for a bit. ","06/Mar/14 15:41;jbellis;I've committed the version change since it's clearly an improvement, but leaving this open until it actually works. :)","14/Mar/14 05:04;mishail;Patch to make Cobertura happy. {{cobertura-instrument}} works at least, but since {{test}} fails at 2.1 branch, I didn't see the report","14/Mar/14 19:15;jbellis;why does doubling up this line help?

{code}
+      <classpath refid=""cassandra.classpath""/>
{code}","14/Mar/14 19:18;mishail;It's not a doubling, the 1st line refers to {{""cobertura.classpath""}}. ","14/Mar/14 19:21;jbellis;Ah, right.

See if it works against 2.0 branch?","14/Mar/14 20:42;mishail;!2014-03-14 13-41-02.png!
Worked for me",14/Mar/14 20:46;jbellis;Ship it!,14/Mar/14 20:55;mishail;Committed https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commit;h=245038fd25a1a852047d0090ec7964839cdc2aea,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix artifacts task to strip out DOS crlf,CASSANDRA-6798,12698655,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,woolfel,woolfel,woolfel,04/Mar/14 16:05,12/Mar/19 13:58,13/Mar/19 22:29,17/Mar/14 16:40,2.1 rc2,,,Packaging,,,0,,,,,"DOS builds end up with ^M in the unix shell scripts. updated the build to strip them so that builds on windows will run fine on linux. My github fork is https://github.com/woolfel/cassandra

I can't figure out how to make a patch with github, so I'm lame. The fix is checked in.
",windows 7,,,,,,,,,,,,,,,,,,,,,,,14/Mar/14 19:49;JoshuaMcKenzie;6798.patch;https://issues.apache.org/jira/secure/attachment/12634810/6798.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-14 19:02:04.941,,,no_permission,,,,,,,,,,,,377013,,,Mon Mar 17 16:40:35 UTC 2014,,,,,,0|i1sys7:,377308,,,,,,,,JoshuaMcKenzie,JoshuaMcKenzie,,,,,,,,,,14/Mar/14 19:02;jbellis;[~JoshuaMcKenzie] to review,14/Mar/14 19:48;JoshuaMcKenzie;Added tools/bin to fixcrlf and attaching patch to ticket.  Tested out and looks good.,17/Mar/14 16:40;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batch with timestamp failed,CASSANDRA-5415,12640301,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,aytereschenko,aytereschenko,02/Apr/13 13:41,12/Mar/19 13:58,13/Mar/19 22:29,08/Apr/13 13:01,1.2.4,,,,,,0,qa-resolved,,,,"When I create a prepared statement with the following CQL3 using Thrift protocol:
{code}
BEGIN BATCH USING TIMESTAMP <number>
<some INSERT INTO or UPDATE statements ....>
APPLY BATCH
{code}

and execute this statement in a loop, I randomly get the error:
*InvalidRequestException(why:Timestamp must be set either on BATCH or individual statements)*

All statements inside the batch have no individual USING TIMESTAMP.",,,,,,,,,,,,,,,,,,,,,,,,07/Apr/13 20:51;iamaleksey;5415.txt;https://issues.apache.org/jira/secure/attachment/12577467/5415.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-07 20:43:27.818,,,no_permission,,,,,,,,,,,,320764,,,Mon Apr 08 13:01:16 UTC 2013,,,,,,0|i1jcf3:,321105,,,,,,,,slebresne,slebresne,,,,,,,,,enigmacurry,"07/Apr/13 20:43;iamaleksey;BatchStatement.getMutations() currently modifies its nested modification statements when you set a batch-level timestamp:

{noformat}
for (ModificationStatement statement : statements)
{
    if (isSetTimestamp())
        statement.setTimestamp(getTimestamp(now));
{noformat}

Because of this, only the first execution of such a prepared statement will succeed. All subsequent attempts will fall because of this check in BatchStatement.validate():

{noformat}
for (ModificationStatement statement : statements)
{
    if (isSetTimestamp() && statement.isSetTimestamp())
        throw new InvalidRequestException(""Timestamp must be set either on BATCH or individual statements"");
{noformat}

The attached patch fixes this.",08/Apr/13 08:17;slebresne;+1,"08/Apr/13 13:01;iamaleksey;Thanks, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Include fatal errors in trace events,CASSANDRA-5447,12641685,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,09/Apr/13 18:50,12/Mar/19 13:58,13/Mar/19 22:29,09/Apr/13 19:10,1.2.5,,,Legacy/Tools,,,0,qa-resolved,,,,This would help tracking down which query is causing errors.,,,,,,,,,,,,,,,,,,,,,,,,09/Apr/13 18:51;jbellis;5447.txt;https://issues.apache.org/jira/secure/attachment/12577853/5447.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-09 18:59:37.677,,,no_permission,,,,,,,,,,,,322101,,,Tue Apr 09 19:10:58 UTC 2013,,,,,,0|i1jkon:,322446,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,enigmacurry,09/Apr/13 18:51;jbellis;Trivial patch attached.,09/Apr/13 18:59;iamaleksey;+1,09/Apr/13 19:10;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
compaction and scrub data directories race on startup,CASSANDRA-6797,12698552,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,JoshuaMcKenzie,mbyrd,mbyrd,04/Mar/14 02:02,12/Mar/19 13:58,13/Mar/19 22:29,04/Mar/14 23:11,2.0.6,2.1 beta2,,Local/Compaction,Local/Startup and Shutdown,,0,compaction,concurrency,starting,," 
Hi,  

On doing a rolling restarting of a 2.0.5 cluster in several environments I'm seeing the following error:
{code}

 INFO [CompactionExecutor:1] 2014-03-03 17:11:07,549 CompactionTask.java (line 115) Compacting [SSTableReader(path='/Users/Matthew/.ccm/compaction_race/node1/data/system/local/system-local-jb-13-Data.db'), SSTableReader(path='/Users/Matthew/.ccm/compactio
n_race/node1/data/system/local/system-local-jb-15-Data.db'), SSTableReader(path='/Users/Matthew/.ccm/compaction_race/node1/data/system/local/system-local-jb-16-Data.db'), SSTableReader(path='/Users/Matthew/.ccm/compaction_race/node1/data/system/local/syst
em-local-jb-14-Data.db')]
 INFO [CompactionExecutor:1] 2014-03-03 17:11:07,557 ColumnFamilyStore.java (line 254) Initializing system_traces.sessions
 INFO [CompactionExecutor:1] 2014-03-03 17:11:07,560 ColumnFamilyStore.java (line 254) Initializing system_traces.events
 WARN [main] 2014-03-03 17:11:07,608 ColumnFamilyStore.java (line 473) Removing orphans for /Users/Matthew/.ccm/compaction_race/node1/data/system/local/system-local-jb-13: [CompressionInfo.db, Filter.db, Index.db, TOC.txt, Summary.db, Data.db, Statistics.
db]
ERROR [main] 2014-03-03 17:11:07,609 CassandraDaemon.java (line 479) Exception encountered during startup
java.lang.AssertionError: attempted to delete non-existing file system-local-jb-13-CompressionInfo.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:111)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:106)
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:476)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:264)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:462)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:552)
 INFO [CompactionExecutor:1] 2014-03-03 17:11:07,612 CompactionTask.java (line 275) Compacted 4 sstables to [/Users/Matthew/.ccm/compaction_race/node1/data/system/local/system-local-jb-17,].  10,963 bytes to 5,572 (~50% of original) in 57ms = 0.093226MB/s.  4 total partitions merged to 1.  Partition merge counts were {4:1, }

{code}
Seems like a potential race, since compactions are occurring whilst the existing data directories are being scrubbed.
Probably an in progress compaction looks like an incomplete one and results in it being attempted to be scrubbed whilst in progress. 
On the attempt to delete in the scrubDataDirectories we discover that it no longer exists, presumably because it has now been compacted away. 
This then causes an assertion error and the node fails to start up. 

Here is a ccm script which just stops and starts a 3 node 2.0.5 cluster repeatedly. 
It seems to fairly reliably reproduce the problem, in less than ten iterations: 

{code}
#!/bin/bash

ccm create compaction_race -v 2.0.5
ccm populate -n 3
ccm start

for i in $(seq 0 1000); do 
    echo $i;
    ccm stop
    ccm start
    grep ERR ~/.ccm/compaction_race/*/logs/system.log;
done

{code}
 
Someone else should probably confirm that this is what is going wrong,  
however if it is, the solution might be as simple as to disable autocompactions slightly earlier in CassandraDaemon.setup. 
 
Or alternatively if there isn't a good reason why we are first scrubbing the system tables and then scrubbing all keyspaces (including the system keyspace), you could perhaps just scrub solely the non system keyspaces on the second scrub.

Please let me know if there is anything else I can provide.
Thanks,
Matt
",macos (and linux),,,,,,,,,,,,,CASSANDRA-6795,,,,,,,,,,04/Mar/14 22:00;JoshuaMcKenzie;trunk-6797.patch;https://issues.apache.org/jira/secure/attachment/12632677/trunk-6797.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-04 21:58:52.952,,,no_permission,,,,,,,,,,,,376910,,,Tue Mar 04 23:14:59 UTC 2014,,,,,,0|i1sy5b:,377205,2.0.5,,,,,,,jbellis,jbellis,,,,,,,,,,"04/Mar/14 02:10;mbyrd;I think CASSANDRA-6795 may be the same or a similar issue, but since the reproduction is more complicated and the environment is windows, I thought I'd file this ticket also.",04/Mar/14 21:58;JoshuaMcKenzie;CASSANDRA-6795 is similar but should only show up on Windows as it's a file-handle race and *nix is far more tolerant of deleting open handles.  For 6797: attaching a simple patch against trunk to skip cleaning system keyspace on 2nd scrub as it's unnecessary and racing.,"04/Mar/14 23:11;jbellis;LGTM, committed (with .equals instead of ==)",04/Mar/14 23:14;JoshuaMcKenzie;I keep trying to beat the C++ out of me - those habits run deep.  Thanks for fixing that.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DC-local CAS,CASSANDRA-5797,12659484,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,jbellis,jbellis,24/Jul/13 03:02,12/Mar/19 13:58,13/Mar/19 22:29,26/Jul/13 15:31,2.0 rc1,,,Legacy/CQL,,,1,LWT,,,,"For two-datacenter deployments where the second DC is strictly for disaster failover, it would be useful to restrict CAS to a single DC to avoid cross-DC round trips.

(This would require manually truncating {{system.paxos}} when failing over.)",,,,,,,,,,,,,,,,,,,,,,,,25/Jul/13 12:51;slebresne;0001-Thrift-generated-files.txt;https://issues.apache.org/jira/secure/attachment/12594174/0001-Thrift-generated-files.txt,25/Jul/13 12:51;slebresne;0002-Add-LOCAL_SERIAL-CL.txt;https://issues.apache.org/jira/secure/attachment/12594175/0002-Add-LOCAL_SERIAL-CL.txt,25/Jul/13 12:51;slebresne;0003-CQL-and-native-protocol-changes.txt;https://issues.apache.org/jira/secure/attachment/12594176/0003-CQL-and-native-protocol-changes.txt,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-07-24 11:49:46.435,,,no_permission,,,,,,,,,,,,339677,,,Fri Jul 26 15:31:27 UTC 2013,,,,,,0|i1ml4n:,339997,,,,,,,,jbellis,jbellis,,,,,,,,,,24/Jul/13 03:03;jbellis;Not sure what CQL syntax for this is.  Is it protocol level the way CL is?,"24/Jul/13 11:49;slebresne;bq. Not sure what CQL syntax for this is. Is it protocol level the way CL is?

That's a good question and I'm not really sure what's the right answer.

I think it may make the most sense to make it protocol level because of reads.  For CAS writes, we do have a CQL syntax for it, so we could extends it with say:
{noformat}
UPDATE foo SET v1 = 2, v2 = 3 WHERE k = 1 IF v1 = 1 AND v2 = 1 IN LOCAL DC
{noformat}
But for reads, we don't have any syntax, the consistency level (SERIAL) is the only thing that makes a read go through paxos, so I'm afraid adding some CQL syntax in that case would be confusing.

But even making it protocol level is not that easy. For thrift, on the read side, the only way I can see us supporting this DC-local CAS would be add a LOCAL_SERIAL consistency level (short of duplicating all read methods for CAS reads that is). But that doesn't really work for writes since the consistency level for writes is really the consistency of the paxos learn/commit phase.

One option (the best I can come up with so far) would be to add the LOCAL_SERIAL consistency level, and then to change CAS write to take 2 CL: the first one would be for the commit (learn) phase (as we have now, but we would refuse CL.SERIAL and CL.LOCAL_SERIAL in that case) and a 2nd CL that would control the ""Paxos consistency"" (and for that one, only CL.SERIAL or CL.LOCAL_SERIAL would be valid). It's not perfect however because the one thing you can't properly express is the ability to do CL.SERIAL for paxos but don't wait on any node for the learn phase. Unless we make CL.ANY for the ""commit consistency"" mean that, but that's a slight stretch.

In any case, we should probably make it sure to shove that in 2.0.0 because I don't want to change the thrift API nor break the native protocol in 2.0.1.

Any better idea?
","24/Jul/13 15:14;jbellis;Sounds reasonable, although I think it would be better to come up w/ a different enum for the Paxos phases than re-use CL, most of whose options are not appropriate.

I actually think CL.ANY on commit is fine.","24/Jul/13 15:23;slebresne;bq. although I think it would be better to come up w/ a different enum for the Paxos phases than re-use CL

The thing is that for reads, we must have SERIAL and LOCAL_SERIAL in CL if we want thrift to support it. So once we have them in CL, is it really worth adding a separate enum for the write case? (honest question, I'm fine doing it, just wonder if it's worth bothering since things will be mixed up for reads anyway).",24/Jul/13 15:27;pmcfadin;I like LOCAL_SERIAL over ANY. It makes a closer match to LOCAL_QUORUM in that it's not meant to cross datacenter boundaries. There is enough confusion about ANY as it is and I think this would simplify things.,"24/Jul/13 16:33;slebresne;bq. I like LOCAL_SERIAL over ANY

I think there is some confusion. The suggestion of CL.ANY was for the commit part of Paxos. That part is basically a standard write (that happens after the paxos algorithm has unfolded but does impact the visibility of the CAS write by non-serial reads). For that, LOCAL_SERIAL don't really make sense imo (it's ""wrong"" even). ANY does is what match the most what happens, because you are guaranteed the write is replicated somewhere (paxos ensures that) but you may not be able to see your write right away with normal reads, even at CL.ALL (which is also something that CL.ANY).  ","24/Jul/13 23:51;jbellis;bq. The thing is that for reads, we must have SERIAL and LOCAL_SERIAL in CL if we want thrift to support it. So once we have them in CL, is it really worth adding a separate enum for the write case?

The problem is that none of {ANY, ONE, TWO, THREE, LOCAL_QUORUM, EACH_QUORUM} are valid on writes, which isn't very clear if we reuse CL for everything.

Then again we ANY is not a valid CL for read, and EACH_QUORUM is not valid for writes.  I dunno.","25/Jul/13 12:51;slebresne;Attaching patch for this. I've currently stayed with the idea of 2 CL for writes. One small reason for which it is somewhat convenient is that when we throw a write timeout exception, we need to ship the consistency level. And when that timeout happens during the paxos prepare/propose phases, returning CL.SERIAL or CL.LOCAL_SERIAL make the most sense, so using CL as argument of the method in the first place is somewhat consistent. Anyway, it's certainly possible to add a new enum for that instead, but I don't think that' too horrible as is.

There's 3 patches: the first one is just the update to the thrift generated files and can be largely ignored. The 2nd one does the main change but does not change CQL. The 3rd patch is the CQL and native protocol change. That latter patch is a tad big because while adding the new ""serial consistency level"", I realized this was a pain with the current code and that in the protocol v2, QUERY and EXECUTE had basically the same parameters but they were set out in different order (in the protocol) which was killing all possibility of code reuse for no good reason. So I decided to go ahead and refactor that more cleanly since there's no point in making the life of client implementors harder for no good reason.

Anyway, moving this issue to 2.0.0 because it changes the native protocol and thrift, so we really should have it in 2.0.
","26/Jul/13 14:14;jbellis;+1 (mostly looked at patch 2)

Nit: rename isSameDCThan to isSameDCAs, or maybe better sameDCPredicateFor (since ""is"" implies it's doing a boolean evaluation)",26/Jul/13 15:30;iamaleksey;QueryOptions.Codec.encode() writes flags before writing the CL - should be reversed. Otherwise LGTM.,"26/Jul/13 15:31;slebresne;Alright, committed (with nit fixed). Thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
