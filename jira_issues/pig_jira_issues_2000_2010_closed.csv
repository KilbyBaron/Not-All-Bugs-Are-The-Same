Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Outward issue link (Cloners),Outward issue link (Duplicate),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (dependent),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (Hadoop Flags),Custom field (Hadoop Flags),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Patch Info),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Severity),Custom field (Severity),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Tags),Custom field (Test and Documentation Plan),Custom field (Testcase included),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
pig wrapper script tends to fail if pig is in the path and PIG_HOME isn't set,PIG-1293,12458875,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aw,aw,aw,12/Mar/10 00:39,14/May/10 06:47,14/Mar/19 03:04,17/Mar/10 23:42,0.6.0,,,,,,0.7.0,,,,,0,,,,"If PIG_HOME isn't set and pig is in the path, the pig wrapper script can't find its home.  Setting PIG_HOME makes it hard to support multiple versions of pig. ",,,,,,,,,,,,,,,,,,,12/Mar/10 00:41;aw;PIG-1293.txt;https://issues.apache.org/jira/secure/attachment/12438558/PIG-1293.txt,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-03-12 12:35:06.501,,,no_permission,,,,,,,,,,,,164796,,,,,Wed Mar 17 23:42:26 UTC 2010,,,,,,,0|i0gu7z:,96326,,,,,,,,,,"12/Mar/10 00:41;aw;Since we require bash, we can cheat a bit.  BASH_SOURCE should contain the full path to the script that we are executing.  This means we can ... piggyback... off of that variable to figure out where our real PIG_HOME is at.","12/Mar/10 12:35;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12438558/PIG-1293.txt
  against trunk revision 922097.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/245/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/245/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/245/console

This message is automatically generated.","12/Mar/10 15:55;aw;Let's try submitting again, since it seems my test results disappeared.  Not surprised at the missing test, but core failing? that's seems unlikely.","12/Mar/10 20:41;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12438558/PIG-1293.txt
  against trunk revision 922169.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/236/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/236/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/236/console

This message is automatically generated.","17/Mar/10 00:12;alangates;Allen,

I'm having trouble reproducing this issue, so I'm not sure how to test your fix.  If I take top of trunk and install it, then do:

{code}
gates> echo $PIG_HOME

gates> PATH=/usr/bin:/usr/local/bin:/bin:./bin which pig
/home/gates/tmp/pig-0.7.0-dev/bin/pig
gates> PATH=/usr/bin:/usr/local/bin:/bin:./bin pig -x local ~/pig/scripts/Checkin_2.local.pig
10/03/16 17:09:24 INFO pig.Main: Logging error messages to: /home/gates/tmp/pig-0.7.0-dev/pig_1268784564902.log
2010-03-16 17:09:25,205 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: file:///
2010-03-16 17:09:26,047 [main] WARN  org.apache.pig.PigServer - Encountered Warning IMPLICIT_CAST_TO_INT 2 time(s).
...
{code}

What am I doing wrong here?","17/Mar/10 00:31;aw;You likely have PIG_HOME configured.

Unset it, then try running bash -x pig and the message about being unable to find pig-env.sh won't be hidden by bash.  BTW, the hadoop equiv jira is HADOOP-6630, as it suffers from the same problem.","17/Mar/10 00:40;aw;Err, not PIG_HOME, PIG_CONF_DIR.","17/Mar/10 20:18;aw;Let's get a bit more explicit:

$ cd /tmp
$ which pig
/export/apps/hadoop/bin/pig
$ bash -x pig
+ cygwin=false
+ case ""`uname`"" in
++ uname
+ debug=false
+ this=pig
+ '[' -h pig ']'
++ dirname pig
+ bin=.
++ basename pig
+ script=pig
++ unset CDPATH
++ cd .
++ pwd
+ bin=/tmp
+ this=/tmp/pig
++ dirname /tmp/pig
+ export PIG_HOME=/tmp/..


This is clearly wrong.  After applying the fix:


+ cygwin=false
+ case ""`uname`"" in
++ uname
+ debug=false
+ this=/export/apps/hadoop/bin/pig
+ '[' -h /export/apps/hadoop/bin/pig ']'
++ ls -ld /export/apps/hadoop/bin/pig
+ ls='lrwxrwxrwx   1 root     root          24 Mar  5 09:05 /export/apps/hadoop/bin/pig -> /export/apps/pig/bin/pig'
++ expr 'lrwxrwxrwx   1 root     root          24 Mar  5 09:05 /export/apps/hadoop/bin/pig -> /export/apps/pig/bin/pig' : '.*-> \(.*\)$'
+ link=/export/apps/pig/bin/pig
+ expr /export/apps/pig/bin/pig : '.*/.*'
+ this=/export/apps/pig/bin/pig
+ '[' -h /export/apps/pig/bin/pig ']'
++ dirname /export/apps/pig/bin/pig
+ bin=/export/apps/pig/bin
++ basename /export/apps/pig/bin/pig
+ script=pig
++ unset CDPATH
++ cd /export/apps/pig/bin
++ pwd
+ bin=/export/apps/pig/bin
+ this=/export/apps/pig/bin/pig
++ dirname /export/apps/pig/bin/pig
+ export PIG_HOME=/export/apps/pig/bin/..

This is correct, even dealing with the symlink in the process, as expected.
","17/Mar/10 23:42;alangates;Thanks, that last comment helped me reproduce the issue.  Patch checked in.  Thanks Allen for the patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Interface Refinements,PIG-1292,12458857,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,11/Mar/10 21:57,14/May/10 06:47,14/Mar/19 03:05,16/Mar/10 17:13,0.7.0,,,,,,0.7.0,,,,,0,,,,"A loader can't implement both OrderedLoadFunc and IndexableLoadFunc, as both are abstract classes instead of being interfaces.",,,,,,,,,,,,,,,,,,,12/Mar/10 20:41;ashutoshc;pig-1292.patch;https://issues.apache.org/jira/secure/attachment/12438638/pig-1292.patch,11/Mar/10 22:04;ashutoshc;pig-interfaces.patch;https://issues.apache.org/jira/secure/attachment/12438546/pig-interfaces.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-03-11 23:54:12.038,,,no_permission,,,,,,,,,,,,164795,,,,,Tue Mar 16 17:13:14 UTC 2010,,,,,,,0|i0gu7j:,96324,,,,,,,,,,"11/Mar/10 21:58;ashutoshc;Currently LoadFunc is an abstract class. OrderedLoadFunc is another abstract class which extends LoadFunc and adds the method which tells Pig in what order to read the splits. Similarly, there is IndexableLoadFunc which also extends LoadFunc and adds the functionality that loader can arbitrarily seek near to specified keys. Its not hard to imagine that there may exist a loader which can do both. Currently there can't be such a loader since both of these are abstract classes. Proposal is to change them to interfaces. 

Further, a loader may also provide a guarantee that all instances of a key appear together in one split. A similar loader is required for map-side groups PIG-984 . Currently, its assumed that underlying loader is providing data in a way its expected. We should formalize this assumption by introducing new interface and checking if loader is implementing it.
",11/Mar/10 22:04;ashutoshc;A preview patch with suggested changes.,"11/Mar/10 23:54;xuefuz;A few comments from my personal perspective:

1. WritableComparable<?> getSplitComparable(PigSplit split) should not pass PigSplit instace. Instead, it should only pass the actual split wrapped in PigSplit. On interface level, the type should be just InputSplit.

2. In CollectableLoadFunc interface, void should be the return type. If a LoadFunc implementation implements the interface, it means it's capable to support what is asking for. There is no point to implement an interface and later to say it's not capably to do what the interface is asking.

3. ensureAllKeyInstancesInSameSplit() method might need a better name. However, comparing to another two issues, this is minor.","12/Mar/10 00:46;ashutoshc;Thanks for review, Xuefu.

1. Thats a valid point. Where possible we want loadfunc implementers to deal with Hadoop concepts and not with Pig concepts.

2. So, lets assume there is a loader which is capable of  implementing this interface but only if underlying data is sorted (information which is available to loader only at run-time). Now this loader will implement this interface, indicating to Pig it is capable of doing it. But just because it is capable of doing it, doesn't necessarily imply it will do it (possibly because of performance reasons). Then, when Pig calls the method of interface, it is communicating to loader that it wants data in particular fashion, no matter what. Inside this method, loader may come to know about some metadata (like data is not sorted, possibly by reading its schema or contacting some metadata repo) and decides that it cant honor the contract because of information which is available to it only at run time. Then, loader may return false for the method. Pig may then choose to rewrite the query and still carry-on the execution. Because of these scenarios, I think having a boolean return value is useful. what do you think?

3. Can't come up with better name. Feel free to suggest :)","12/Mar/10 18:35;xuefuz;If the response to #2 is the assumption, then don't create a new interface with a single method in it. Put this method in any existing interfaces, say, as an abstract method in LoadFunc itself. If a loader is absolutely incapable, then return false always. Otherwise, if the loader's answer varies case-by-case, then return true/false selectively in the implementation. 

In general, I think an API should be simple and clear. I can image that other developers may have the same difficulty understanding what's this interface about.

FYI, Zebra TableLoader now extends one parent class, and implements FIVE interfaces. With this trend, it may grow even more. To me, this seems excessive.","12/Mar/10 18:55;dvryaboy;Agreed with Xuefu's comment regarding the interfaces. This really seems like something we can just have the abstract func default to false.

Method name suggestion: how about hasKeyToSplitAffinity() 
","12/Mar/10 19:31;ashutoshc;One reason for not putting it in LoadFunc is to keep loadfunc simple and not have such highly specific methods in there. We want to move such specialized capabilities away from LoadFunc into their own interfaces. This is also the reason PIG-966 decided to split LoadFunc into separate interfaces like LoadPushDown, LoadCaster, LoadMetaData etc. and not put all of them in LoadFunc. This frees loadfunc implementers from not thinking about them, if they don't want to. And if one wants to have such specific capability in his loader, he has to think about it anyway whether its in loadfunc or in its own interface. 

That said, I agree having boolean return value for the method seems to be confusing, so I agree method return value should be void.","12/Mar/10 19:48;dvryaboy;.. but we have an abstract class that can provide default implementations so that implementers don't have to think about this.

Most of the interfaces introduced in PIG-966 have significant chunks of functionality associated with them. This is just a single method about a particular property of the incoming data.
I can see why you'd be against putting it into LoadFunc, though, as it's very specific. What about ResourceSchema or LoadMetaData?","12/Mar/10 20:41;ashutoshc;Didn't get about LoadMetaData, ResourceSchema. LoadMetaData is one of those interfaces which loaders can choose to implement. ResourceSchema is independent class of its own.

New patch incorporating suggested changes in the above comments. This patch also adds checks in the MRCompiler to enforce loader to implement new CollectableLoader interface if there is a map-side grouping ( PIG-984 ) in the script.","12/Mar/10 20:42;ashutoshc;Hudson is fickle recently. Hopefully, this patch gets lucky and is tested correctly.","12/Mar/10 22:32;xuefuz;Looking at the OrderedLoadFunc interface, public WritableComparable<?> getSplitComparable(InputSplit split, int splitIdx), I am not sure why split index suddenly comes into the picture. Though it was in earlier discussion between Pig and Zebra, we agree that this is very implementation specific, which shouldn't dictate API design. Thus, I don't think that split index should be in the signature even if it helps Zebra implementation. If an implementation needs the split index, it can always store the index in the split it generates. That's what exactly Zebra plan to do.
","15/Mar/10 14:47;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12438638/pig-1292.patch
  against trunk revision 923043.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 531 release audit warnings (more than the trunk's current 530 warnings).

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/237/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/237/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/237/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/237/console

This message is automatically generated.","16/Mar/10 05:37;pkamath;As Xuefu mentioned, we can get rid of the splitIdx argument in public WritableComparable<?> getSplitComparable(InputSplit split, int splitIdx).

Otherwise the changes look good, +1 for commit with the above change.",16/Mar/10 17:13;ashutoshc;Patch checked-in with changes suggested in previous comment. Core test failure reported by hudson was transient. It passed on my machine.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WeightedRangePartitioner should not check if input is empty if quantile file is empty,PIG-1290,12458764,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,11/Mar/10 00:52,14/May/10 06:47,14/Mar/19 03:05,13/Mar/10 20:05,0.6.0,0.7.0,,,,,0.7.0,,,,,0,,,,"Currently WeightedRangePartitioner checks if the input is also empty if the quantile file is empty. For this it tries to read the input (which under the covers will result in creating splits for the input etc). If the input is a directory with many files, this could result in many calls to the namenode from each task - this can be avoided.

If the input is non empty and quantile file is empty, then we would error out anyway (this should be confirmed). Also while fixing this jira we should ensure that pig can still do order by on empty input.",,,,,,,,,,,,,,,,,,,12/Mar/10 00:33;pkamath;PIG-1290.patch;https://issues.apache.org/jira/secure/attachment/12438556/PIG-1290.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-03-12 01:04:07.691,,,no_permission,,,,,,,,,,,,164793,Reviewed,,,,Sat Mar 13 20:05:10 UTC 2010,,,,,,,0|i0gu6n:,96320,,,,,,,,,,12/Mar/10 00:33;pkamath;Attached patch removes the check in WeightedRangePartitioner to check that the input is empty when quantile file is empty. There is already a test -testEmptyStore in TestEvalPipeline2 to test that pig handles order by on empty files fine - so this patch does not include any new tests.,12/Mar/10 01:04;daijy;+1,"12/Mar/10 06:22;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12438556/PIG-1290.patch
  against trunk revision 922097.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/244/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/244/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/244/console

This message is automatically generated.",12/Mar/10 18:58;pkamath;Looks like the unit test failure was due to some other check in which has now got fixed - resubmitting,"13/Mar/10 01:47;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12438556/PIG-1290.patch
  against trunk revision 922169.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/247/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/247/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/247/console

This message is automatically generated.",13/Mar/10 07:08;pkamath;Again there seem to be transient unrelated test failures - am resubmitting one more time - will also kick off a unit test run on my machine.,"13/Mar/10 18:50;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12438556/PIG-1290.patch
  against trunk revision 922169.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/249/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/249/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/249/console

This message is automatically generated.","13/Mar/10 20:05;pkamath;Core tests ran successfully on my machine and looking at the test report the failures seem transient. I haven't included new tests in this patch since an existing test covers the change in this patch.

Patch committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PIG Join fails while doing a filter on joined data,PIG-1289,12458684,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,daijy,karims,karims,10/Mar/10 16:36,20/Jul/10 17:36,14/Mar/19 03:05,19/Mar/10 06:48,0.6.0,,,,,,0.7.0,,,,,0,,,,"PIG Join fails while doing a filter on joined data

Here are the steps to reproduce it:
-bash-3.1$ pig -latest -x local
grunt> a = load 'first.dat' using PigStorage('\u0001') as (f1:int, f2:chararray);
grunt> DUMP a;
(1,A)
(2,B)
(3,C)
(4,D)

grunt> b = load 'second.dat' using PigStorage() as (f3:chararray);
grunt> DUMP b;
(A)
(D)
(E)

grunt> c = join a by f2 LEFT OUTER, b by f3;
grunt> DUMP c;
(1,A,A)
(2,B,)
(3,C,)
(4,D,D)
grunt> describe c;
c: {a::f1: int,a::f2: chararray,b::f3: chararray}
grunt> d = filter c by (f3 is null or f3 =='');

grunt> dump d;
2010-03-03 15:00:37,129 [main] INFO  org.apache.pig.impl.logicalLayer.optimizer.PruneColumns - No column pruned for b
2010-03-03 15:00:37,129 [main] INFO  org.apache.pig.impl.logicalLayer.optimizer.PruneColumns - No map keys pruned for b
2010-03-03 15:00:37,129 [main] INFO  org.apache.pig.impl.logicalLayer.optimizer.PruneColumns - No column pruned for a
2010-03-03 15:00:37,130 [main] INFO  org.apache.pig.impl.logicalLayer.optimizer.PruneColumns - No map keys pruned for a
2010-03-03 15:00:37,130 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1002: Unable to store alias d


This one is failing too:

grunt> d = filter c by (b::f3 is null or b::f3 =='');

or this one not returning results as expected:

grunt> d = foreach c generate f1 as f1, f2 as f2, f3 as f3;
grunt> e = filter d by (f3 is null or f3 =='');
grunt> DUMP e;
(1,A,)
(2,B,)
(3,C,)
(4,D,)

while the expected result is
(2,B,)
(3,C,)

",,,,,,,,,,,,,,,,,,,12/Mar/10 02:38;daijy;PIG-1289-1.patch;https://issues.apache.org/jira/secure/attachment/12438569/PIG-1289-1.patch,17/Mar/10 01:40;daijy;PIG-1289-2.patch;https://issues.apache.org/jira/secure/attachment/12438992/PIG-1289-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-03-10 18:50:23.155,,,no_permission,,,,,,,,,,,,164792,Reviewed,,,,Fri Mar 19 06:48:20 UTC 2010,,,,,,,0|i0gu67:,96318,,,,,,,,,,"10/Mar/10 18:50;daijy;The problem is we push filter in front of the out join. We should not push filter when:
1. filter is pushed to inner branch
2. filter condition is to test nullability","12/Mar/10 17:18;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12438569/PIG-1289-1.patch
  against trunk revision 922169.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/246/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/246/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/246/console

This message is automatically generated.","16/Mar/10 23:44;alangates;I think I'm missing something here.  It seems the criteria here should be that the filter is not pushed up a branch that will be producing nulls.  So, given a script like:

A = load 'foo' as (q, r, s);
B = load 'bar ' as (t, u, v);
C = join A on q outer, B on t;

D = filter C by q > 0; 

is pushable, but 

D = filter C by t > 0;

is not.  I think the test that at most one element of the join is outer and it is the one we are considering meets this criteria.  But I don't understand the need to check for null tests.

","17/Mar/10 00:06;daijy;Yes, it is safe not push filter up a branch that will be producing nulls. I might be wrong but what I did is try to be a little bit more aggressive. Since the only extra value outer join will produce is null, so if filter is not testing null, we can still push it up even if it is on the inner branch. 

Eg:
A = load 'foo' as (q, r, s);
B = load 'bar ' as (t, u, v);
C = join A on q outer, B on t;
D = filter C by t > 0;

The production C consists of two parts:
A + B
A + ""null""

If we do a filter after join, it is a union on this two parts:
filter(A + B) union filter(A + ""null"")

If we are not testing nullability (eg, t > 0), then filter(A + ""null"") will not have any production, so
filter(A + B) union filter(A + ""null"") = filter(A + B)

In this case, outer join is equivalent as a regular join (since all generated null B records are filtered away), so we can still push the filter up.","17/Mar/10 00:21;alangates;In the case of 

D = filter C by t > 0

the filter will evaluate to null when t is null.  By definition filters return only records that evaluate true.  So t > 0 will have the affect of filtering out all outer records of A because t will be null for every one of them.  That is, it turns the join into an inner join.  However, if the filter is pushed above the join, it will remain an outer join, since it will only filter the records from B where t > 0 and not the outer records from A.  Thus this transformation is not output neutral.","17/Mar/10 01:06;daijy;Oh, you are right. I need to further change the join type to regular join to make my idea work. Since changing the join type may broke some other part of the code, so let's be safe here and only push filter in front of a branch not generating null.","17/Mar/10 06:37;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12438992/PIG-1289-2.patch
  against trunk revision 924034.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/253/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/253/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/253/console

This message is automatically generated.",19/Mar/10 06:48;daijy;Unit test failure due to port conflict. Manual test successful. Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EvalFunc returnType is wrong for generic subclasses,PIG-1288,12458609,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,10/Mar/10 03:21,17/Dec/10 22:43,14/Mar/19 03:05,05/Aug/10 20:43,0.7.0,,,,,,0.8.0,,impl,,,0,,,,"From Garrett Buster Kaminaga:

The EvalFunc constructor has code to determine the return type of the function.
This walks up the object hierarchy until it encounters EvalFunc, then calls getActualTypeArguments and extracts type
param 0.

However, if the user class is itself a generic extension of EvalFunc, then the returned object is not the correct type,
but a TypeVariable.

Example:
  class MyAbstractEvalFunc<T> extends EvalFunc<T> ...
  class MyEvalFunc extends MyAbstractEvalFunc<String> ...

when MyEvalFunc() is called, inside EvalFunc constructor the return type is set to a TypeVariable rather than
String.class.

The workaround we've implemented is for the MyAbstractEvalFunc<T> to determine *its* type parameters using code
similar to that in the EvalFunc constructor, and then reset protected data member returnType manually in the
MyAbstractEvalFunc constructor.  (though this has the same drawback of not working if someone then extends
MyAbstractEvalFunc)
",,,,,,,,,,,,,,,,,,,30/Apr/10 17:47;daijy;PIG-1288-1.patch;https://issues.apache.org/jira/secure/attachment/12443302/PIG-1288-1.patch,02/May/10 18:06;daijy;PIG-1288-2.patch;https://issues.apache.org/jira/secure/attachment/12443417/PIG-1288-2.patch,04/May/10 23:43;daijy;PIG-1288-3.patch;https://issues.apache.org/jira/secure/attachment/12443659/PIG-1288-3.patch,26/Jul/10 23:15;daijy;PIG-1288-4.patch;https://issues.apache.org/jira/secure/attachment/12450538/PIG-1288-4.patch,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2010-04-30 22:30:34.521,,,no_permission,,,,,,,,,,,,164791,Reviewed,,,,Thu Aug 05 20:43:34 UTC 2010,,,,,,,0|i0gu5r:,96316,,,,,,,,,,"30/Apr/10 22:30;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12443302/PIG-1288-1.patch
  against trunk revision 939727.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 17 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 531 release audit warnings (more than the trunk's current 528 warnings).

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/306/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/306/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/306/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/306/console

This message is automatically generated.",02/May/10 18:06;daijy;Deal with release audit warnings.,"02/May/10 23:04;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12443417/PIG-1288-2.patch
  against trunk revision 939807.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 13 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/311/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/311/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/311/console

This message is automatically generated.",04/May/10 23:43;daijy;Fix unit test failures,"05/May/10 09:52;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12443659/PIG-1288-3.patch
  against trunk revision 941005.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 17 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/315/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/315/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/315/console

This message is automatically generated.",26/Jul/10 22:17;daijy;Resync with trunk,27/Jul/10 17:30;rding;+1,"28/Jul/10 00:31;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12450538/PIG-1288-4.patch
  against trunk revision 979781.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 17 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/381/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/381/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/381/console

This message is automatically generated.",05/Aug/10 20:43;daijy;Tested manually. Patch committed. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig should give error message when cogroup on tuple keys of different inner type,PIG-1277,12458279,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,daijy,daijy,05/Mar/10 19:02,04/Aug/11 00:35,14/Mar/19 03:05,24/Dec/10 01:32,0.6.0,,,,,,0.9.0,,impl,,,0,,,,"When we cogroup on a tuple, if the inner type of tuple does not match, we treat them as different keys. This is confusing. It is desirable to give error/warnings when it happens.

Here is one example:
UDF:
{code}
public class MapGenerate extends EvalFunc<Map> {
    @Override
    public Map exec(Tuple input) throws IOException {
        // TODO Auto-generated method stub
        Map m = new HashMap();
        m.put(""key"", new Integer(input.size()));
        return m;
    }
    
    @Override
    public Schema outputSchema(Schema input) {
        return new Schema(new Schema.FieldSchema(null, DataType.MAP));
    }
}
{code}

Pig script: 
{code}
a = load '1.txt' as (a0);
b = foreach a generate a0, MapGenerate(*) as m:map[];
c = foreach b generate a0, m#'key' as key;
d = load '2.txt' as (c0, c1);
e = cogroup c by (a0, key), d by (c0, c1);
dump e;
{code}

1.txt
{code}
1
{code}

2.txt
{code}
1 1
{code}

User expected result (which is not right):
{code}
((1,1),{(1,1)},{(1,1)})
{code}

Real result:
{code}
((1,1),{(1,1)},{})
((1,1),{},{(1,1)})
{code}

We shall give user the message that we can not merge the key due to the type mismatch.",,,,,,,,,,,,,,,,,,,17/Dec/10 01:02;daijy;PIG-1277-1.patch;https://issues.apache.org/jira/secure/attachment/12466432/PIG-1277-1.patch,18/Dec/10 00:25;daijy;PIG-1277-2.patch;https://issues.apache.org/jira/secure/attachment/12466517/PIG-1277-2.patch,23/Dec/10 18:51;daijy;PIG-1277-3.patch;https://issues.apache.org/jira/secure/attachment/12466909/PIG-1277-3.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-12-17 19:43:59.672,,,no_permission,,,,,,,,,,,,66105,Reviewed,,,,Fri Dec 24 01:32:26 UTC 2010,,,,,,,0|i0gu1b:,96296,,,,,,,,,,"17/Dec/10 01:02;daijy;The patch also aim to solve PIG-999, PIG-1065","17/Dec/10 19:43;alangates;Comments:

# I'm not sure the null handling in NullableBytesWritable.getValueAsPigType is the same.  Previously it would check specifically if the value had been marked as null.  Now it looks like if there isn't an entry in the first slot of the tuple (which I think would be what would happen if it were null) it will throw an exception.  I think you want to return the isNull check, and make sure the constructors properly set that value.
# I don't understand the change in LOUnion.","18/Dec/10 00:23;daijy;Response to Alan's comments:
1. Yes, you are right. It introduces some subtle differences. When we see a null value, we set mNull flag, and put null into a tuple. We do read the null back; however, in NullableBytesWritable, we rely on mNull flag to do the comparison, which may results wrong result. I will change it.

2. LOUnion is a bug fix. We shall get null schema if union two different schema. It is to fix PIG-1065. Since PIG-1065 is more of the same nature, I don't want to put fix in a separate patch.",18/Dec/10 00:25;daijy;PIG-1277-2.patch address Alan's review comments.,20/Dec/10 18:47;alangates;+1,"23/Dec/10 18:51;daijy;test-patch result:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 9 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 468 release audit warnings (more than the trunk's current 464 warnings).

Release audit warning is because changed NullableBytesWritable construct triggered a new jdiff file.

Unit test:
    all pass

end-to-end test:
    all pass",24/Dec/10 01:32;daijy;Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 [Zebra] Changes requried for Zebra due to PIG-1259 changes,PIG-1276,12458185,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,,xuefuz,xuefuz,04/Mar/10 22:28,14/May/10 06:47,14/Mar/19 03:05,05/Mar/10 21:32,,,,,,,0.7.0,,,,,0,,,,"Pig resource schema interface changed, so Zebra needs to catch exception thrown from the new interfaces.",,,,,,,,,,,,,,,,,,,04/Mar/10 22:40;xuefuz;zebra.0304;https://issues.apache.org/jira/secure/attachment/12437941/zebra.0304,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-03-05 03:19:05.627,,,no_permission,,,,,,,,,,,,164783,,,,,Fri Mar 05 21:32:55 UTC 2010,,,,,,,0|i0gu13:,96295,,,,,,,,,,"05/Mar/10 03:19;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12437941/zebra.0304
  against trunk revision 919202.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/234/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/234/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/234/console

This message is automatically generated.",05/Mar/10 16:39;xuefuz;The patch only simply catches exceptions that weren't thrown but is thrown now by Pig. It's purely for the purpose of fixing the build. No test cases are required. Please commit this ASAP.,05/Mar/10 17:43;yanz;+1,05/Mar/10 21:32;yanz;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
empty bag in PigStorage read as null,PIG-1275,12458026,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,thejas,thejas,03/Mar/10 19:56,14/May/10 06:47,14/Mar/19 03:05,11/Mar/10 18:37,0.7.0,,,,,,0.7.0,,,,,0,,,,"This seems to be introduced after changes in PIG-613 .

grunt> cat /tmp/students.txt                                                               
qwer    F       {(1),(2)}
zxldf   M       {}

grunt> l = load '/tmp/students.txt' as (n : chararray, s : chararray, b: {t : (i : int)} );
grunt> dump l;  
(qwer,F,{(1),(2)})
(zxldf,M,)
grunt> ",,,,,,,,,,,,,,,,,,,03/Mar/10 20:27;daijy;PIG-1275-1.patch;https://issues.apache.org/jira/secure/attachment/12437787/PIG-1275-1.patch,03/Mar/10 21:35;daijy;PIG-1275-2.patch;https://issues.apache.org/jira/secure/attachment/12437793/PIG-1275-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-03-04 07:48:34.445,,,no_permission,,,,,,,,,,,,164782,Reviewed,,,,Thu Mar 11 01:43:03 UTC 2010,,,,,,,0|i0gu0n:,96293,,,,,,,,,,"04/Mar/10 07:48;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12437793/PIG-1275-2.patch
  against trunk revision 917827.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/222/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/222/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/222/console

This message is automatically generated.",11/Mar/10 01:43;rding;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skewed join throws error ,PIG-1273,12457940,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,ankur,ankur,03/Mar/10 05:41,14/May/10 06:47,14/Mar/19 03:05,04/Mar/10 21:35,0.6.0,,,,,,0.7.0,,,,,0,,,,When the sampled relation is too small or empty then skewed join fails.,,,,,,,,,,,,,,,,,,,04/Mar/10 00:44;rding;PIG-1273.patch;https://issues.apache.org/jira/secure/attachment/12437818/PIG-1273.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-03-04 00:44:52.84,,,no_permission,,,,,,,,,,,,164780,,,,,Thu Mar 04 21:21:19 UTC 2010,,,,,,,0|i0gtzr:,96289,,,,,,,,,,"03/Mar/10 05:43;ankur;Here is a simple script to reproduce it

a = load 'test.dat' using PigStorage() as (nums:chararray);
b = load 'join.dat' using PigStorage('\u0001') as (number:chararray,text:chararray);
c = filter a by nums == '7';
d = join c by nums LEFT OUTER, b by number USING ""skewed"";
dump d;

==== test.dat ====
1
2
3
4
5

===== join.dat =====
1^Aone
2^Atwo
3^Athree

where ^A means Control-A charatcer used as a separator.","03/Mar/10 05:44;ankur;Complete stack trace of the error thrown my 3rd M/R job in the pipeline

java.lang.RuntimeException: Error in configuring object
	at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
	at org.apache.hadoop.mapred.MapTask$OldOutputCollector.(MapTask.java:448)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)
	at org.apache.hadoop.mapred.Child.main(Child.java:159)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)
	... 6 more
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: Empty samples file
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.SkewedPartitioner.configure(SkewedPartitioner.java:128)
	... 11 more
Caused by: java.lang.RuntimeException: Empty samples file
	at org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil.loadPartitionFile(MapRedUtil.java:128)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.SkewedPartitioner.configure(SkewedPartitioner.java:125)
	... 11 more
",04/Mar/10 00:44;rding;Modify the code to handle the empty input file being sampled for skewed join.,"04/Mar/10 12:34;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12437818/PIG-1273.patch
  against trunk revision 917827.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/223/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/223/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/223/console

This message is automatically generated.",04/Mar/10 21:21;thejas;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Column pruner causes wrong results,PIG-1272,12457916,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,viraj,viraj,02/Mar/10 22:25,14/May/10 06:47,14/Mar/19 03:05,15/Mar/10 03:24,0.6.0,,,,,,0.7.0,,impl,,,0,,,,"For a simple script the column pruner optimization removes certain columns from the original relation, which results in wrong results.

Input file ""kv"" contains the following columns (tab separated)
{code}
a       1
a       2
a       3
b       4
c       5
c       6
b       7
d       8
{code}

Now running this script in Pig 0.6 produces

{code}
kv = load 'kv' as (k,v);
keys= foreach kv generate k;
keys = distinct keys; 
keys = limit keys 2;
rejoin = join keys by k, kv by k;
dump rejoin;
{code}

(a,a)
(a,a)
(a,a)
(b,b)
(b,b)


Running this in Pig 0.5 version without column pruner results in:
(a,a,1)
(a,a,2)
(a,a,3)
(b,b,4)
(b,b,7)

When we disable the ""ColumnPruner"" optimization it gives right results.

Viraj",,,,,,,,,,,,,,,,,,,03/Mar/10 02:20;daijy;PIG-1272-1.patch;https://issues.apache.org/jira/secure/attachment/12437666/PIG-1272-1.patch,14/Mar/10 08:45;daijy;PIG-1272-2.patch;https://issues.apache.org/jira/secure/attachment/12438743/PIG-1272-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-03-03 06:57:11.368,,,no_permission,,,,,,,,,,,,164779,Reviewed,,,,Mon Mar 15 03:24:39 UTC 2010,,,,,,,0|i0gtzb:,96287,,,,,,,,,,"02/Mar/10 23:45;viraj;Now with Pig 0.7 or trunk we have the following error:

2010-03-02 23:35:09,349 FATAL org.apache.hadoop.mapred.Child: Error running child : java.lang.NoSuchFieldError: sJobConf
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POJoinPackage.getNext(POJoinPackage.java:110)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.processOnePackageOutput(PigMapReduce.java:380)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:363)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:240)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:567)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:409)
	at org.apache.hadoop.mapred.Child.main(Child.java:159)

Viraj","03/Mar/10 06:57;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12437666/PIG-1272-1.patch
  against trunk revision 917827.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/220/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/220/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/220/console

This message is automatically generated.",12/Mar/10 01:02;pkamath;+1,"12/Mar/10 07:56;daijy;Rollback the patch due to ""TestMultiQuery.testMultiQueryJiraPig1157"" fail. Strange hudson does return +1. I checked actually hudson skip TestMultiQuery.testMultiQueryJiraPig1157.","15/Mar/10 00:03;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12438743/PIG-1272-2.patch
  against trunk revision 922664.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/252/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/252/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/252/console

This message is automatically generated.",15/Mar/10 03:24;daijy;Manual unit test pass.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Push limit into loader,PIG-1270,12457898,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,02/Mar/10 18:55,26/Apr/12 20:33,14/Mar/19 03:05,23/Mar/12 01:03,0.7.0,,,,,,0.10.0,,impl,,,1,,,,"We can optimize limit operation by stopping early in PigRecordReader. In general, we need a way to communicate between PigRecordReader and execution pipeline. POLimit could instruct PigRecordReader that we have already had enough records and stop feeding more data.",,,,,,,,,,,,,,,,,,,29/Aug/11 21:43;daijy;PIG-1270-1.patch;https://issues.apache.org/jira/secure/attachment/12492146/PIG-1270-1.patch,01/Sep/11 04:48;daijy;PIG-1270-2.patch;https://issues.apache.org/jira/secure/attachment/12492577/PIG-1270-2.patch,24/Nov/11 08:03;coderplay;PIG-1270-3.patch;https://issues.apache.org/jira/secure/attachment/12504993/PIG-1270-3.patch,23/Mar/12 01:00;daijy;PIG-1270-4.patch;https://issues.apache.org/jira/secure/attachment/12519563/PIG-1270-4.patch,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2011-11-24 07:34:49.037,,,no_permission,,,,,,,,,,,,35767,Reviewed,,,,Fri Mar 23 01:03:10 UTC 2012,,,,,,,0|i09ykn:,56042,,,,,,,,,,"01/Sep/11 04:48;daijy;PIG-1270-2.patch fix all unit tests. However, I didn't see noticeable performance improvement. The script I test is:

a = load 'studenttab20m' as (name, age, gpa);
b = limit a 10;
dump b;

Both in local mode and mapreduce mode. 

Need further investigation to find out why performance not improve.","24/Nov/11 07:34;coderplay;@Daniel

It improves here, but with a bug. I did the test in a 25-nodes cluster which such script
{noformat}
A = load '/tpch/orders' USING PigStorage('\u0001') AS (o_orderkey:int, o_custkey:int, o_orderstatus:chararray, o_totalprice:double, o_orderdate:chararray, o_orderpriority:chararray, o_clerk:chararray, o_shippriority:int, o_comment: chararray);
F = FOREACH A GENERATE o_orderkey;
L = LIMIT F 10;
DUMP L; 
{noformat}

||case||job cost time||HDFS bytes read||Average time taken by Map tasks|Worst performing map task
|w/o optimization| 26 sec|12,976,128|1 sec | 1 sec|
|with optimization| 24 sec|19,347,931,305|3 sec | 5 sec|

Since with your patch, the LimitOptimizer would remove LOLimit from logic plans after set the limit to LOLoad, this would generate a map-only job. Record number of the result would be map_num * 10, this is incorrect. 

I will submit a patch soon.




",24/Nov/11 08:03;coderplay;Here is the patch which would fix the bug.,"24/Nov/11 08:14;coderplay;sorry, some mistakes 

||case||job cost time||HDFS bytes read||Average time taken by Map tasks||Worst performing map task||
|w/o optimization|26 sec|19,347,931,305|3 sec|5 sec|
|w/ optimization|24 sec|12,976,128|1 sec|1 sec|
","28/Nov/11 01:49;daijy;Thanks Min, that's encouraging. Which version of Hadoop are you using?

Also I discussed with Min in IM, it will be better to have a global flag to signal the sufficiency of records, so that we can address more cases for this optimization. ","30/Nov/11 04:46;coderplay;We are using a modified version of 0.19.1. However, that internal version provide new MR API and is compatible with both hadoop clients under the versions of 0.19.x and 0.20.2. Our version doesn't change any logic of map phase from the community version, so this patch should improves the latter as well.


That's a good attempt if we can address more cases like limit optimization on LOFilter.
","10/Feb/12 03:33;viraj;What version is this likely going to be fixed?

Daniel in your original comment the script mentioned is similar to a ""SELECT * .. LIMIT 10"" Hive currently does not run a M/R job for these situations, it just reads the data and streams it to stdout. Can we do such an optimization for the query mentioned?

Additionally can we use some optimizations that Hadoop 23 has such as running in an Uberized task rather than launch M/R jobs? 

Viraj","10/Feb/12 03:45;dvryaboy;Uberized will kick in automatically in 0.23, no need to do anything on the Pig side afaik.","13/Feb/12 08:23;daijy;@Viraj
For the patch itself, I would like to commit it into trunk soon. For the direct hdfs access you mention, it will probably be part of the backend rework we planned, but I am not sure at the moment.","22/Feb/12 02:29;viraj;Hi Daniel,
 Can we target this patch for Pig 0.10.1?
Viraj","14/Mar/12 23:53;daijy;Unit test pass. Plan to check it into 0.10, objection?",16/Mar/12 21:01;thejas;+1. Can you make a minor change before checkin ? - Make the limit variable in PigRecordReader a final.,"20/Mar/12 01:53;thejas;These are the results I got from running tests to check the performance on larger data.

Query - 
{code}
grunt> l = load '/tmp/bigfile2' as (a,b,c);
grunt> lim = limit l 10;
grunt> dump lim;
{code}

Ran on a cluster with 8 map slots. 

With 128MB block size , 499 Maps -
|| || trunk || trunk+patch ||
|avg Run time | 17 min 7 sec| 6 min 44 sec |
|avg run time of map | 12 sec | 4 sec|


With smaller number of splits the numbers are better - 
With 'set pig.maxCombinedSplitSize 1073741824' (ie split size of 1G) and 64 Maps -
|| || trunk || trunk+patch ||
|avg Run time | 15 min 19 sec| 1 min 10 sec |
|avg run time of map | 106 sec | 4 sec|
",23/Mar/12 01:00;daijy;PIG-1270-4.patch resync with trunk and address Thejas's comment.,23/Mar/12 01:03;daijy;Patch committed to 0.10/trunk. Thanks Thejas and Min for testing!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Zebra] Restrict schema definition for collection,PIG-1269,12457894,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,02/Mar/10 17:59,14/May/10 06:47,14/Mar/19 03:05,11/Mar/10 21:07,,,,,,,0.7.0,,,,,0,,,,"Currently Zebra grammar for schema definition for collection field allows many types of definition. To reduce complexity and remove ambiguity, and more importantly, to make the meta data more representative of the actual data instances, the grammar rules need to be changed. Only a record type is allowed and required for collection definition. Thus,  fieldName:collection(record(c1:int, c2:string)) is legal, while fieldName:collection(c1:int, c2:string), fieldName:collection(f:record(c1:int, c2:string)), fieldName:collection(c1:int), or feildName:collection(int) is illegal.

This will have some impact on existing Zebra M/R programs or Pig scripts that use Zebra. Schema acceptable in previous release now may become illegal because of this change. This should be clearly documented.
",,,,,,,,,,,,,,,,,,,11/Mar/10 05:12;xuefuz;zebra.0310;https://issues.apache.org/jira/secure/attachment/12438479/zebra.0310,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-03-03 00:50:41.194,,,no_permission,,,,,,,,,,,,164777,,,,,Thu Mar 11 21:07:20 UTC 2010,,,,,,,0|i0gty7:,96282,,,,,,,,,,"03/Mar/10 00:50;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12437638/zebra.0302
  against trunk revision 917827.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 63 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/219/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/219/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/219/console

This message is automatically generated.","04/Mar/10 18:37;xuefuz;After consolidating with Pig and Owl, it's decided that a ""name"" can be optionally given for a collection schema. Thus, the following is also valid:

c:collection(r:record(f:float, i:int)).

The new patch includes corresponding changes.","04/Mar/10 23:15;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12437911/patch.0304
  against trunk revision 919109.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 60 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/224/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/224/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/224/console

This message is automatically generated.",11/Mar/10 05:14;xuefuz;missed two test cases. Now nightly test passes.,"11/Mar/10 12:31;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12438479/zebra.0310
  against trunk revision 921585.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 66 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/242/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/242/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/242/console

This message is automatically generated.",11/Mar/10 20:04;chaow;Patch looks good +1,11/Mar/10 21:07;yanz;Patch committed to the trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problems with partition filter optimizer,PIG-1267,12457625,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,27/Feb/10 01:37,14/May/10 06:47,14/Mar/19 03:05,04/Mar/10 18:23,0.7.0,,,,,,0.7.0,,,,,0,,,,"There are a couple of problems with the current partition filter optimizer:

1. When a partition filter is removed from the logical plan, the input index of the following join/cogroup operator may change, which in turn changes the ordering of the fields in the schema and results in compile-time errors.

2. At most one partition filter can be removed per plan,  while multiple partition filters can exist in the cases of joins. 

",,,,,,,,,,,,,,,,,,,01/Mar/10 23:42;rding;PIG-1267.patch;https://issues.apache.org/jira/secure/attachment/12437539/PIG-1267.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-03-02 04:37:46.498,,,no_permission,,,,,,,,,,,,164775,,,,,Tue Mar 02 06:41:36 UTC 2010,,,,,,,0|i0gtxb:,96278,,,,,,,,,,"02/Mar/10 04:37;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12437539/PIG-1267.patch
  against trunk revision 916793.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/231/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/231/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/231/console

This message is automatically generated.",02/Mar/10 06:41;daijy;+1 for commit.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Show spill count on the pig console at the end of the job,PIG-1266,12457623,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sriranjan,sriranjan,sriranjan,27/Feb/10 00:58,14/May/10 06:47,14/Mar/19 03:05,18/Mar/10 22:49,,,,,,,0.7.0,,,,,0,,,,Currently the spill count is displayed only on the job tracker log. It should be displayed on the console as well.,,,,,,,,,,,,,,,,,,,27/Feb/10 01:00;sriranjan;PIG_1266.patch;https://issues.apache.org/jira/secure/attachment/12437288/PIG_1266.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-02-27 09:43:15.166,,,no_permission,,,,,,,,,,,,164774,Reviewed,,,,Thu Mar 18 22:39:31 UTC 2010,,,,,,,0|i0gtwv:,96276,,,,,,,,,,27/Feb/10 01:00;sriranjan;The patch does not contain any unit tests. The change is cosmetic and I have manually verified that the spill count is displayed at the end of script execution.,"27/Feb/10 09:43;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12437288/PIG_1266.patch
  against trunk revision 916793.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/229/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/229/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/229/console

This message is automatically generated.",12/Mar/10 19:57;rding;Now the spill count is disabled due to Pig's changing to new Hadoop API  in 0.7. We need to enable the spill count. ,15/Mar/10 22:16;rding;Assign to me to add code enabling the Pig counters.,18/Mar/10 22:39;rding;Pig counter is enabled by PIG-1287. ,18/Mar/10 22:39;rding;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change LoadMetadata and StoreMetadata to use Job instead of Configuraiton and add a cleanupOnFailure method to StoreFuncInterface,PIG-1265,12457610,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pradeepkth,pkamath,pkamath,26/Feb/10 22:43,14/May/10 06:47,14/Mar/19 03:05,02/Mar/10 01:09,0.7.0,,,,,,0.7.0,,,,,0,,,,"Speaking to the hadoop team folks, the direction in hadoop is to use Job instead of Configuration - for example InputFormat/OutputFormat implementations use Job to store input/output location. So pig should also do the same in LoadMetadata and StoreMetadata to be closer to hadoop.

Currently when a job fails, pig assumes the output locations (corresponding to the stores in the job) are hdfs locations and attempts to delete them. Since output locations could be non hdfs locations, this cleanup should be delegated to the StoreFuncInterface implementation - hence a new method - cleanupOnFailure() should be introduced in StoreFuncInterface and a default implementation should be provided in the StoreFunc abstract class which checks if the location exists on hdfs and deletes it if so.",,,,,,,,,,,,,,,,,,,01/Mar/10 20:26;pkamath;PIG-1265-2.patch;https://issues.apache.org/jira/secure/attachment/12437515/PIG-1265-2.patch,27/Feb/10 00:09;pkamath;PIG-1265.patch;https://issues.apache.org/jira/secure/attachment/12437280/PIG-1265.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-02-27 01:01:22.997,,,no_permission,,,,,,,,,,,,164773,Reviewed,,,,Tue Mar 02 01:09:47 UTC 2010,,,,,,,0|i0gtwf:,96274,,,,,,,,,,27/Feb/10 00:09;pkamath;Attached patch with changes per description. I have also removed code which used to store the storefunc funcscpec in string form into the conf - this should no longer be needed by StoreFuncs. It was added for StoreFuncs in the earlier release which had an OutputFormat underneath (example zebra) - now that StoreFunc and OutputFormat are closely integrated this should no longer be needed.,27/Feb/10 01:01;rding;+1,"27/Feb/10 05:04;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12437280/PIG-1265.patch
  against trunk revision 916793.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 8 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/228/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/228/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/228/console

This message is automatically generated.",01/Mar/10 02:17;pkamath;All unit tests succeeded on a local run on my machine.,01/Mar/10 20:26;pkamath;There were some failures in zebra nightly tests which are addressed in the new patch.,"02/Mar/10 00:38;ashutoshc;Looked at the diff of first and second patch. +1 for that part. 

One thing unrelated to patch I want to highlight is: these kind of issues creeps in because our api is too wide, there are multiple ways of getting same thing done in Pig (executing script through Java api in this case), each exercising different code paths. So, first thing we need is to establish what is part of our api  and what is meant for Pig's internal purposes only? Is every single public method part of api  ? Or, only those which have properly documented Javadocs are supported ?  Or only those documented in wiki page ?
First we establish what constitutes our public api, then we should systematically start decreasing the visibility of those methods which are public but not meant as an api, then deprecate them and eventually remove them. ","02/Mar/10 01:07;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12437515/PIG-1265-2.patch
  against trunk revision 916793.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 17 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 520 release audit warnings (more than the trunk's current 519 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/217/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/217/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/217/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/217/console

This message is automatically generated.","02/Mar/10 01:09;pkamath;The release audit warning is due to changes in html files and not due to any issues in files in the patch.
Patch committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skewed join sampler misses out the key with the highest frequency,PIG-1264,12457602,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,sriranjan,sriranjan,26/Feb/10 20:03,14/May/10 06:47,14/Mar/19 03:05,06/Mar/10 00:05,0.7.0,,,,,,0.7.0,,,,,0,,,,"I am noticing two issues with the sampler used in skewed join:
1. It does not allocate multiple reducers to the key with the highest frequency.
2. It seems to be allocating the same number of reducers to every key (8 in this case).

Query:

a = load 'studenttab10k' using PigStorage() as (name, age, gpa);
b = load 'votertab10k' as (name, age, registration, contributions);
e = join a by name right, b by name using ""skewed"" parallel 8;
store e into 'SkewedJoin_9.out';
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-03-05 19:36:58.342,,,no_permission,,,,,,,,,,,,164772,,,,,Fri Mar 05 19:36:58 UTC 2010,,,,,,,0|i0gtvz:,96272,,,,,,,,,,"05/Mar/10 19:36;rding;Somehow during the many merges between the trunk and LSR branch, the random sampler is used in place of Poisson sampler for skewed join. This was corrected with PIG-1273.   ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Script producing varying number of records when COGROUPing value of map data type with and without types,PIG-1263,12457518,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,viraj,viraj,26/Feb/10 02:22,14/May/10 06:47,14/Mar/19 03:05,05/Mar/10 19:08,0.6.0,,,,,,0.7.0,,impl,,,0,,,,"I have a Pig script which I am experimenting upon. [[Albeit this is not optimized and can be done in variety of ways]] I get different record counts by placing load store pairs in the script.

Case 1: Returns 424329 records
Case 2: Returns 5859 records
Case 3: Returns 5859 records
Case 4: Returns 5578 records
I am wondering what the correct result is?

Here are the scripts.
Case 1: 
{code}
register udf.jar

A = LOAD '/user/viraj/data/20100203' USING MapLoader() AS (s, m, l);

B = FOREACH A GENERATE
        s#'key1' as key1,
        s#'key2' as key2;

C = FOREACH B generate key2;

D = filter C by (key2 IS NOT null);

E = distinct D;

store E into 'unique_key_list' using PigStorage('\u0001');

F = Foreach E generate key2, MapGenerate(key2) as m;

G = FILTER F by (m IS NOT null);

H = foreach G generate key2, m#'id1' as id1, m#'id2' as id2, m#'id3' as id3, m#'id4' as id4, m#'id5' as id5, m#'id6' as id6, m#'id7' as id7, m#'id8' as id8, m#'id9' as id9, m#'id10' as id10, m#'id11' as id11, m#'id12' as id12;

I = GROUP H BY (id1, id2, id3, id4, id5, id6, id7, id8, id9, id10, id11, id12);

J = Foreach I generate group.id1 as id1, group.id2 as id2, group.id3 as id3, group.id4 as id4,group.id5 as id5, group.id6 as id6, group.id7 as id7, group.id8 as id8, group.id9 as id9, group.id10 as id10, group.id11 as id11, group.id12 as id12;

--load previous days data
K = LOAD '/user/viraj/data/20100202' USING PigStorage('\u0001') as (id1, id2, id3, id4, id5, id6, id7, id8, id9, id10, id11, id12);

L = COGROUP  K by (id1, id2, id3, id4, id5, id6, id7, id8, id9, id10, id11, id12) OUTER,
             J by (id1, id2, id3, id4, id5, id6, id7, id8, id9, id10, id11, id12) OUTER;

M = filter L by IsEmpty(K);

store M into 'cogroupNoTypes' using PigStorage();
{code}

Case 2:  Storing and loading intermediate results in J 
{code}
register udf.jar

A = LOAD '/user/viraj/data/20100203' USING MapLoader() AS (s, m, l);

B = FOREACH A GENERATE
        s#'key1' as key1,
        s#'key2' as key2;

C = FOREACH B generate key2;

D = filter C by (key2 IS NOT null);

E = distinct D;

store E into 'unique_key_list' using PigStorage('\u0001');

F = Foreach E generate key2, MapGenerate(key2) as m;

G = FILTER F by (m IS NOT null);

H = foreach G generate key2, m#'id1' as id1, m#'id2' as id2, m#'id3' as id3, m#'id4' as id4, m#'id5' as id5, m#'id6' as id6, m#'id7' as id7, m#'id8' as id8, m#'id9' as id9, m#'id10' as id10, m#'id11' as id11, m#'id12' as id12;

I = GROUP H BY (id1, id2, id3, id4, id5, id6, id7, id8, id9, id10, id11, id12);

J = Foreach I generate group.id1 as id1, group.id2 as id2, group.id3 as id3, group.id4 as id4,group.id5 as id5, group.id6 as id6, group.id7 as id7, group.id8 as id8, group.id9 as id9, group.id10 as id10, group.id11 as id11, group.id12 as id12;

--store intermediate data to HDFS and re-read
store J into 'output/20100203/J' using PigStorage('\u0001');

--load previous days data
K = LOAD '/user/viraj/data/20100202' USING PigStorage('\u0001') as (id1, id2, id3, id4, id5, id6, id7, id8, id9, id10, id11, id12);

--read J into K1
K1 = LOAD 'output/20100203/J' using PigStorage('\u0001') as (id1, id2, id3, id4, id5, id6, id7, id8, id9, id10, id11, id12);

L = COGROUP  K by (id1, id2, id3, id4, id5, id6, id7, id8, id9, id10, id11, id12) OUTER,
             K1 by (id1, id2, id3, id4, id5, id6, id7, id8, id9, id10, id11, id12) OUTER;

M = filter L by IsEmpty(K);

store M into 'cogroupNoTypesIntStore' using PigStorage();
{code}


Case 3: Types information specified but no intermediate store of J

{code}
register udf.jar

A = LOAD '/user/viraj/data/20100203' USING MapLoader() AS (s, m, l);

B = FOREACH A GENERATE
        s#'key1' as key1,
        s#'key2' as key2;

C = FOREACH B generate key2;

D = filter C by (key2 IS NOT null);

E = distinct D;

store E into 'unique_key_list' using PigStorage('\u0001');

F = Foreach E generate key2, MapGenerate(key2) as m;

G = FILTER F by (m IS NOT null);

H = foreach G generate key2, (long)m#'id1' as id1, (long)m#'id2' as id2, (long)m#'id3' as id3, (long)m#'id4' as id4, (long)m#'id5' as id5, (long)m#'id6' as id6, (long)m#'id7' as id7, (chararray)m#'id8' as id8, (chararray)m#'id9' as id9, (chararray)m#'id10' as id10, (chararray)m#'id11' as id11, (chararray)m#'id12' as id12;


I = GROUP H BY (id1, id2, id3, id4, id5, id6, id7, id8, id9, id10, id11, id12);

J = Foreach I generate group.id1 as id1, group.id2 as id2, group.id3 as id3, group.id4 as id4,group.id5 as id5, group.id6 as id6, group.id7 as id7, group.id8 as id8, group.id9 as id9, group.id10 as id10, group.id11 as id11, group.id12 as id12;

store J into 'output/20100203/J' using PigStorage('\u0001');

--load previous days data with type information
K = LOAD '/user/viraj/data/20100202' USING PigStorage('\u0001') as  (id1:chararray, id2:long, id3:long, id4:long, id5:long, id6:long, id7:long, id8:long, id9:chararray, id10:chararray, id11:chararray,id12:chararray,id13:chararray);

L = COGROUP  K by (id1, id2, id3, id4, id5, id6, id7, id8, id9, id10, id11, id12) OUTER,
             J by (id1, id2, id3, id4, id5, id6, id7, id8, id9, id10, id11, id12) OUTER;

M = filter L by IsEmpty(K);

store M into 'cogroupTypesStore' using PigStorage();

{code}

Case 4: Split the store of script into 2 parts one which stores alias G and the other which loads G. Both are run separately.
Script 1
{code}
register udf.jar

A = LOAD '/user/viraj/data/20100203' USING MapLoader() AS (s, m, l);

B = FOREACH A GENERATE
        s#'key1' as key1,
        s#'key2' as key2;

C = FOREACH B generate key2;

D = filter C by (key2 IS NOT null);

E = distinct D;

store E into 'unique_key_list' using PigStorage('\u0001');

F = Foreach E generate key2, MapGenerate(key2) as m;

G = FILTER F by (m IS NOT null);

store G into 'output/20100203/G' using PigStorage('\u0001');
{code}

Script 2:
{code}
G = load 'output/20100203/G' using PigStorage('\u0001') as (ip, m:map[]);

H = foreach G generate key2, (long)m#'id1' as id1, (long)m#'id2' as id2, (long)m#'id3' as id3, (long)m#'id4' as id4, (long)m#'id5' as id5, (long)m#'id6' as id6, (long)m#'id7' as id7, (chararray)m#'id8' as id8, (chararray)m#'id9' as id9, (chararray)m#'id10' as id10, (chararray)m#'id11' as id11, (chararray)m#'id12' as id12;

I = GROUP H BY (id1, id2, id3, id4, id5, id6, id7, id8, id9, id10, id11, id12);

J = Foreach I generate group.id1 as id1, group.id2 as id2, group.id3 as id3, group.id4 as id4,group.id5 as id5, group.id6 as id6, group.id7 as id7, group.id8 as id8, group.id9 as id9, group.id10 as id10, group.id11 as id11, group.id12 as id12;

store J into 'output/20100203/J' using PigStorage('\u0001');

K = LOAD '/user/viraj/data/20100202' USING PigStorage('\u0001') as  (id1:chararray, id2:long, id3:long, id4:long, id5:long, id6:long, id7:long, id8:long, id9:chararray, id10:chararray, id11:chararray,id12:chararray,id13:chararray);

L = COGROUP  K by (id1, id2, id3, id4, id5, id6, id7, id8, id9, id10, id11, id12) OUTER,
             J by (id1, id2, id3, id4, id5, id6, id7, id8, id9, id10, id11, id12) OUTER;

M = filter L by IsEmpty(K);

store M into 'cogroupTypesIntStore' using PigStorage();
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-03-05 19:07:39.725,,,no_permission,,,,,,,,,,,,164771,,,,,Fri Mar 05 19:07:39 UTC 2010,,,,,,,0|i0gtvj:,96270,,,,,,,,,,"05/Mar/10 19:07;daijy;Case 2 and Case 3 produce the right result.

In case 1, MapGenerate generate a map which has the map value other than ByteArray, and then try to cogroup with the other relation which is loaded with PigStorage. Since the type does not match, so cogroup do not merge key as user expected. Pig should throw exception or give warning in this case. Open [PIG-1277|https://issues.apache.org/jira/browse/PIG-1227] for it. User can get around by explicit cast ByteArray to match the type in UDF.

In case 4, user try to save intermediate file using PigStorage. There is a bug when reading txt file contains map. It is fixed in 0.7 ([PIG-613|https://issues.apache.org/jira/browse/PIG-613]). As a general rule, we recommend user to use BinStorage to store intermediate result.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Additional findbugs and javac warnings,PIG-1262,12457515,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,26/Feb/10 01:21,14/May/10 06:47,14/Mar/19 03:05,17/Mar/10 16:30,0.7.0,,,,,,0.7.0,,,,,0,,,,"After a while, we have introduced some new findbugs and javacc warnings. Will fix them in this Jira.",,,,,,,,,,,,,,,,,,,26/Feb/10 05:40;daijy;PIG-1262-1.patch;https://issues.apache.org/jira/secure/attachment/12437127/PIG-1262-1.patch,11/Mar/10 07:18;daijy;PIG-1262-2.patch;https://issues.apache.org/jira/secure/attachment/12438487/PIG-1262-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-02-26 08:24:53.78,,,no_permission,,,,,,,,,,,,164770,Reviewed,,,,Wed Mar 17 16:30:21 UTC 2010,,,,,,,0|i0gtv3:,96268,,,,,,,,,,"26/Feb/10 08:24;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12437127/PIG-1262-1.patch
  against trunk revision 916429.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to cause Findbugs to fail.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/226/testReport/
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/226/console

This message is automatically generated.","26/Feb/10 17:29;daijy;Hudson is not working, resubmit.","26/Feb/10 17:36;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12437127/PIG-1262-1.patch
  against trunk revision 916429.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to cause Findbugs to fail.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/227/testReport/
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/227/console

This message is automatically generated.",02/Mar/10 18:12;olgan;+1,"05/Mar/10 15:14;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12437127/PIG-1262-1.patch
  against trunk revision 919320.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to cause Findbugs to fail.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/225/testReport/
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/225/console

This message is automatically generated.","11/Mar/10 05:56;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12437127/PIG-1262-1.patch
  against trunk revision 921585.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to cause Findbugs to fail.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/233/testReport/
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/233/console

This message is automatically generated.",11/Mar/10 07:19;daijy;Reattach patch to address hudson failure.,"11/Mar/10 18:15;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12438487/PIG-1262-2.patch
  against trunk revision 921585.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/243/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/243/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/243/console

This message is automatically generated.",17/Mar/10 16:30;daijy;Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigStorageSchema broke after changes to ResourceSchema,PIG-1261,12457490,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,daijy,daijy,25/Feb/10 22:08,14/May/10 06:47,14/Mar/19 03:05,05/Mar/10 07:19,0.7.0,,,,,,0.7.0,,impl,,,0,,,,"After we add a new method ""getCastString"" into ResourceSchema, TestPigStorageSchema begin to fail. Seems PigStorageSchema try to serialize cast string into the schema. If I change the name of the method from ""getCastString"" to ""genCastString"", then the error message go away. Since Dmitriy is the author of TestPigStorageSchema, I need his help to check if this is the right approach to fix it.",,,,,,,,,,,,,,,,,,,26/Feb/10 09:49;dvryaboy;PIG_1261.diff;https://issues.apache.org/jira/secure/attachment/12437156/PIG_1261.diff,25/Feb/10 22:15;daijy;TestPigStorageSchema.java;https://issues.apache.org/jira/secure/attachment/12437059/TestPigStorageSchema.java,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-02-25 23:02:32.177,,,no_permission,,,,,,,,,,,,41717,Reviewed,,,,Thu Mar 04 09:11:51 UTC 2010,,,,,,,0|i0gtun:,96266,Patch committed. Thanks Dmitriy!,,,,,,,,,"25/Feb/10 22:15;daijy;BTW, TestPigStorageSchema is temporarily removed. I attached original code in the Jira.",25/Feb/10 23:02;dvryaboy;I'll check it out.,"26/Feb/10 09:49;dvryaboy;The json reflection PigStorageSchema is using to auto-serialize and deserialize assumes things about methods that start with set and get. I renamed the methods.

That's not a good solution, but it's a fast one. ","04/Mar/10 09:11;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12437156/PIG_1261.diff
  against trunk revision 917827.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/233/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/233/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/233/console

This message is automatically generated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Param Subsitution results in parser error if there is no EOL after last line in script,PIG-1260,12457488,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,ashutoshc,ashutoshc,25/Feb/10 21:36,14/May/10 06:47,14/Mar/19 03:05,10/Mar/10 18:57,0.7.0,,,,,,0.7.0,,impl,,,0,,,,"{noformat}
A = load '$INPUT' using PigStorage(':');
B = foreach A generate $0 as id;
store B into '$OUTPUT' USING PigStorage();
{noformat}

Invoking above script which contains no EOL in the last line of script as following:

{noformat} 
pig -param INPUT=mydata/input -param OUTPUT=mydata/output myscript.pig
{noformat}

results in parser error:
{noformat}
[main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Lexical error at line 3, column 42.  Encountered: <EOF> after : """"
{noformat}",,,,,,,,,,,,,,,,,,,05/Mar/10 18:32;rding;PIG-1260.patch;https://issues.apache.org/jira/secure/attachment/12438031/PIG-1260.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-03-05 18:32:01.496,,,no_permission,,,,,,,,,,,,164769,Reviewed,,,,Wed Mar 10 18:29:30 UTC 2010,,,,,,,0|i0gtu7:,96264,,,,,,,parser,,,25/Feb/10 21:37;ashutoshc;Work around is to add a EOL after last line in the script-file.,05/Mar/10 18:32;rding;The cause is the  EOL is missing at the end of script. The problem is that this is a regression from 0.6. This patch fixes this regression. ,"05/Mar/10 23:14;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12438031/PIG-1260.patch
  against trunk revision 919320.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/226/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/226/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/226/console

This message is automatically generated.","09/Mar/10 03:22;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12438031/PIG-1260.patch
  against trunk revision 919634.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/228/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/228/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/228/console

This message is automatically generated.",10/Mar/10 18:29;thejas;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResourceFieldSchema.setSchema should not allow a bag field without a Tuple as its only sub field  (the tuple itself can have a schema with > 1 subfields),PIG-1259,12457372,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,25/Feb/10 01:05,14/May/10 06:47,14/Mar/19 03:05,04/Mar/10 22:11,0.7.0,,,,,,0.7.0,,,,,0,,,,Currently Schema.getPigSchema(ResourceSchema) does not allow a bag field in the ResourceSchema with a subschema containing anything other than a tuple. The tuple itself can have a schema with > 1 subfields. This check should also  be enforced in ResourceFieldSchema.setSchema(),,,,,,,,,,,,,,,,,,,25/Feb/10 18:07;pkamath;PIG-1259-2.patch;https://issues.apache.org/jira/secure/attachment/12437025/PIG-1259-2.patch,25/Feb/10 01:33;pkamath;PIG-1259.patch;https://issues.apache.org/jira/secure/attachment/12436939/PIG-1259.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-02-25 01:50:29.059,,,no_permission,,,,,,,,,,,,164768,Incompatible change,Reviewed,,,Thu Mar 04 22:11:15 UTC 2010,,,,,,,0|i0gttr:,96262,,,,,,,,,,25/Feb/10 01:33;pkamath;Attached patch adds validation as suggested in description,"25/Feb/10 01:50;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12436939/PIG-1259.patch
  against trunk revision 916065.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/220/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/220/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/220/console

This message is automatically generated.",25/Feb/10 18:07;pkamath;Patch to address unit test failures - some tests had a missing try-catch block,"25/Feb/10 22:43;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12437025/PIG-1259-2.patch
  against trunk revision 916065.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 9 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/223/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/223/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/223/console

This message is automatically generated.",26/Feb/10 20:12;daijy;+1,04/Mar/10 22:11;pkamath;Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[zebra] Number of sorted input splits is unusually high,PIG-1258,12457367,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,yanz,yanz,25/Feb/10 00:16,14/May/10 06:47,14/Mar/19 03:05,22/Mar/10 06:28,0.6.0,,,,,,0.7.0,,,,,0,,,,"Number of sorted input splits is unusually high if the projections are on multiple column groups, or a union of tables, or column group(s) that hold many small tfiles. In one test, the number is about 100 times bigger that from unsorted input splits on the same input tables.",,,,,,,,,,,,,,,,,,,16/Mar/10 17:57;yanz;PIG-1258.patch;https://issues.apache.org/jira/secure/attachment/12438944/PIG-1258.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-03-19 07:50:11.696,,,no_permission,,,,,,,,,,,,164767,,,,,Mon Mar 22 06:28:16 UTC 2010,,,,,,,0|i0gttb:,96260,,,,,,,,,,"19/Mar/10 07:50;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12438944/PIG-1258.patch
  against trunk revision 925034.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 9 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/244/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/244/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/244/console

This message is automatically generated.",19/Mar/10 16:36;yanz;The test report page having the claimed failures of some core tests is not available on the web. Will resubmit.,19/Mar/10 16:36;yanz;Resumbit so hudson will rerun.,"19/Mar/10 20:29;gauravj;
+1","21/Mar/10 04:12;yanz;Hudson's rerun appears to be hanging. Here is the result from my private run:

     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 9 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.",22/Mar/10 06:28;yanz;Patch committed to the trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigStorage per the new load-store redesign should support splitting of bzip files,PIG-1257,12457242,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,23/Feb/10 23:48,14/May/10 06:47,14/Mar/19 03:05,16/Mar/10 22:45,0.7.0,,,,,,0.7.0,,,,,0,,,,"PigStorage implemented per new load-store-redesign (PIG-966) is based on TextInputFormat for reading data. TextInputFormat has support for reading bzip data but without support for splitting bzip files. In pig 0.6, splitting was enabled for bzip files - we should attempt to enable that feature.",,,,,,,,,,,,,,,,,,,25/Feb/10 23:28;pkamath;PIG-1257-2.patch;https://issues.apache.org/jira/secure/attachment/12437067/PIG-1257-2.patch,16/Mar/10 00:07;pkamath;PIG-1257-3.patch;https://issues.apache.org/jira/secure/attachment/12438880/PIG-1257-3.patch,25/Feb/10 00:39;pkamath;PIG-1257.patch;https://issues.apache.org/jira/secure/attachment/12436937/PIG-1257.patch,16/Mar/10 00:07;pkamath;blockEndingInCR.txt.bz2;https://issues.apache.org/jira/secure/attachment/12438881/blockEndingInCR.txt.bz2,16/Mar/10 00:07;pkamath;blockHeaderEndsAt136500.txt.bz2;https://issues.apache.org/jira/secure/attachment/12438882/blockHeaderEndsAt136500.txt.bz2,16/Mar/10 00:09;pkamath;recordLossblockHeaderEndsAt136500.txt.bz2;https://issues.apache.org/jira/secure/attachment/12438883/recordLossblockHeaderEndsAt136500.txt.bz2,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2010-02-25 09:14:22.255,,,no_permission,,,,,,,,,,,,164766,Reviewed,,,,Tue Mar 16 22:45:25 UTC 2010,,,,,,,0|i0gtsv:,96258,,,,,,,,,,25/Feb/10 00:39;pkamath;Attached patch builds an InputFormat (Bzip2TextInputFormat) on top of the existing CBZip2InputStream.,"25/Feb/10 09:14;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12436937/PIG-1257.patch
  against trunk revision 916065.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 10 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/215/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/215/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/215/console

This message is automatically generated.",25/Feb/10 23:28;pkamath;Attached new patch to address unit test failures,"26/Feb/10 03:51;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12437067/PIG-1257-2.patch
  against trunk revision 916429.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 9 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/224/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/224/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/224/console

This message is automatically generated.","16/Mar/10 00:07;pkamath;Since the last patch, I uncovered some issue with code while testing some boundary conditions. I have fixed those in the new patch PIG-1257-3.patch and included those boundary conditions in testcases in TestBZip",16/Mar/10 00:09;pkamath;The .bz2 files attached to this issue should be put in test/org/apache/pig/test/data for this patch to pass unit tests.,"16/Mar/10 00:42;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12438883/recordLossblockHeaderEndsAt136500.txt.bz2
  against trunk revision 923043.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/239/console

This message is automatically generated.","16/Mar/10 17:47;breed;excellent work pradeep. just one minor thing:  you always append a \n before inputData in your test case, so you never test the case when you end with just \r
","16/Mar/10 18:01;pkamath;In the following case in inputData the record will end with \r won't it? (notice the \r in the middle after 2)
{code}
          ""1\t2\r3\t4"", // '\r' case - this will be split into two tuples
{code}","16/Mar/10 19:11;pkamath;I ran all unit tests on my local machines and also  the ""test-patch"" ant target:
    [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 12 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec] 
     [exec] 
",16/Mar/10 19:24;breed;+1 you are right. thanx pradeep. i think it is ready to commit.,16/Mar/10 22:45;pkamath;Patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Zebra] Bag field should always contain a tuple type as the field schema in ResourceSchema object converted from Zebra Schema,PIG-1256,12457236,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,xuefuz,xuefuz,xuefuz,23/Feb/10 23:04,14/May/10 06:47,14/Mar/19 03:05,24/Feb/10 19:28,0.7.0,,,,,,0.7.0,,impl,,,0,,,,Pig now requires that schema converted from Zebra Schema contains Tuple type as field schema in .7 release. Zebra needs to take care of all cases where Record is not explicitly specified.,,,,,,,,,,,,,,,,,,,23/Feb/10 23:04;xuefuz;patch.0223;https://issues.apache.org/jira/secure/attachment/12436779/patch.0223,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-02-24 13:59:38.824,,,no_permission,,,,,,,,,,,,164765,,,,,Wed Feb 24 19:28:50 UTC 2010,,,,,,,0|i0gtsf:,96256,,,,,,,,,,"24/Feb/10 13:59;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12436779/patch.0223
  against trunk revision 915634.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/214/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/214/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/214/console

This message is automatically generated.",24/Feb/10 16:49;yanz;All failed tests are PIG tests and not zebra related.,"24/Feb/10 18:57;xuefuz;1. All test case failure seemed due to env issues. Manually running the failed cases succeeds.
2. In this patch, schemas for a collection field such as f:collection(f1:int, f2:string) and f:collection(record(f1:int, f2:string)) are treated as the same by both Zebra and by PIG when they are converted to Pig ResourceSchema and later in Pig to Pig schema. Previously when Zebra schema is directly converted to Pig schema, the second case has no second level access in Pig script. Now both the cases has second level access.
",24/Feb/10 19:26;yanz;+1,24/Feb/10 19:28;yanz;Patch committed to the trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tiny code cleanup for serialization code for PigSplit,PIG-1255,12457202,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,23/Feb/10 19:44,14/May/10 06:47,14/Mar/19 03:05,24/Feb/10 19:36,0.6.0,,,,,,0.7.0,,impl,,,0,,,,A bug which close output stream while serialization.,,,,,,,,,,,,,,,,,,,23/Feb/10 19:48;daijy;PIG-1255-1.patch;https://issues.apache.org/jira/secure/attachment/12436750/PIG-1255-1.patch,23/Feb/10 22:09;daijy;PIG-1255-2.patch;https://issues.apache.org/jira/secure/attachment/12436771/PIG-1255-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-02-23 23:02:18.708,,,no_permission,,,,,,,,,,,,164764,Reviewed,,,,Wed Feb 24 19:36:10 UTC 2010,,,,,,,0|i0gtrz:,96254,,,,,,,,,,"23/Feb/10 22:09;daijy;Checked with hadoop guys. For Serializer/Deserializer, we should always open, but never close. There is another place in pig code does not follow this. Attach a new patch address both positions.
",23/Feb/10 23:02;pkamath;+1 to the changes since it is on the recommendation of the hadoop folks - but it would be good to know why is it a requirement that the Serializer/Deserialize should not be closed? It seems counter-intuitive.,"24/Feb/10 00:13;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12436750/PIG-1255-1.patch
  against trunk revision 915470.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/218/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/218/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/218/console

This message is automatically generated.",24/Feb/10 19:36;daijy;No test included since this patch does not include any new feature.  Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Diamond splitter does not generate correct results when using Multi-query optimization,PIG-1252,12457109,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,viraj,viraj,23/Feb/10 02:15,14/May/10 06:47,14/Mar/19 03:05,10/Mar/10 21:01,0.6.0,,,,,,0.7.0,,,,,0,,,,"I have script which uses split but somehow does not use one of the split branch. The skeleton of the script is as follows

{code}

loadData = load '/user/viraj/zebradata' using org.apache.hadoop.zebra.pig.TableLoader('col1,col2, col3, col4, col5, col6, col7');

prjData = FOREACH loadData GENERATE (chararray) col1, (chararray) col2, (chararray) col3, (chararray) ((col4 is not null and col4 != '') ? col4 : ((col5 is not null) ? col5 : '')) as splitcond, (chararray) (col6 == 'c' ? 1 : IS_VALID ('200', '0', '0', 'input.txt')) as validRec;

SPLIT prjData INTO trueDataTmp IF (validRec == '1' AND splitcond != ''), falseDataTmp IF (validRec == '1' AND splitcond == '');

grpData = GROUP trueDataTmp BY splitcond;

finalData = FOREACH grpData {
                               orderedData = ORDER trueDataTmp BY col1,col2;
                               GENERATE FLATTEN ( MYUDF (orderedData, 60, 1800, 'input.txt', 'input.dat','20100222','5', 'debug_on')) as (s,m,l);
                              }

dump finalData;

{code}


You can see that ""falseDataTmp"" is untouched.

When I run this script with no-Multiquery (-M) option I get the right result.  This could be the result of complex BinCond's in the POLoad. We can get rid of this error by using  FILTER instead of SPIT.

Viraj",,,,,,,,,,,,,,,,,,,09/Mar/10 07:45;daijy;PIG-1252-2.patch;https://issues.apache.org/jira/secure/attachment/12438263/PIG-1252-2.patch,05/Mar/10 21:38;rding;PIG-1252.patch;https://issues.apache.org/jira/secure/attachment/12438039/PIG-1252.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-03-03 20:07:27.408,,,no_permission,,,,,,,,,,,,164761,Reviewed,,,,Wed Mar 10 20:57:51 UTC 2010,,,,,,,0|i0gtqn:,96248,,,,,,,,,,"02/Mar/10 21:42;viraj;A modified version of the script works, does this have to do with nested foreach? 

{code}
loadData = load '/user/viraj/zebradata' using org.apache.hadoop.zebra.pig.TableLoader('col1,col2, col3, col4, col5, col6, col7');

prjData = FOREACH loadData GENERATE (chararray) col1, (chararray) col2, (chararray) col3, (chararray) ((col4 is not null and col4 != '') ? col4 : ((col5 is not null) ? col5 : '')) as splitcond, (chararray) (col6 == 'c' ? 1 : IS_VALID ('200', '0', '0', 'input.txt')) as validRec;

SPLIT prjData INTO trueDataTmp IF (validRec == '1' AND splitcond != ''), falseDataTmp IF (validRec == '1' AND splitcond == '');

grpData = GROUP trueDataTmp BY splitcond;

finalData = FOREACH grpData GENERATE FLATTEN ( MYUDF (orderedData, 60, 1800, 'input.txt', 'input.dat','20100222','5', 'debug_on')) as (s,m,l);
                             
dump finalData;
{code}","03/Mar/10 20:07;rding;This is the result of diamond query optimizer merging a job that has secondary key optimization. This patch disallows such merge.

In practice, users should consider the performance trade-off between using multiquery optimization and using secondary key optimization. Right now the secondary key optimizer runs before the multiquery optimizer which now doesn't merge any job that has secondary key optimization.

To disable multiquery optimization, use option -M. To disable secondary key optimization, use option -Dpig.exec.nosecondarykey=true.","03/Mar/10 21:19;dvryaboy;Richard,
Is there any documentation on what the secondary key optimization does, when it kicks in, benchmarks of how much improvement it provides, and hints on what the expected tradeoffs would be?
",03/Mar/10 21:31;rding;The secondary key optimization is documented in PIG-1038.   ,"04/Mar/10 02:35;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12437777/PIG-1252.patch
  against trunk revision 917827.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/232/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/232/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/232/console

This message is automatically generated.","06/Mar/10 02:18;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12438039/PIG-1252.patch
  against trunk revision 919628.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/235/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/235/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/235/console

This message is automatically generated.",09/Mar/10 07:45;daijy;The root cause of this problem should be the wrong plan cloner. Attach a preliminary patch to see if it works.,"10/Mar/10 20:33;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12438263/PIG-1252-2.patch
  against trunk revision 921185.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/232/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/232/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/232/console

This message is automatically generated.",10/Mar/10 20:57;rding;+1 for Daniel's patch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move SortInfo calculation earlier in compilation ,PIG-1251,12457107,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,23/Feb/10 02:03,14/May/10 06:47,14/Mar/19 03:05,26/Feb/10 19:34,,,,,,,0.7.0,,,,,0,,,,"In LSR Pig does Input Output Validation by calling hadoop's checkSpecs() A storefunc might need schema to do such a validation. So, we should call checkSchema() before doing the validation. checkSchema() in turn requires SortInfo which is calculated later in compilation phase. We need to move it earlier in compilation phase. ",,,,,,,,,,,,,,,,,,,23/Feb/10 02:07;ashutoshc;pig-1251.patch;https://issues.apache.org/jira/secure/attachment/12436664/pig-1251.patch,26/Feb/10 19:28;ashutoshc;pig-1251_1.patch;https://issues.apache.org/jira/secure/attachment/12437231/pig-1251_1.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-02-23 12:43:55.461,,,no_permission,,,,,,,,,,,,164760,,,,,Fri Feb 26 19:45:13 UTC 2010,,,,,,,0|i0gtq7:,96246,,,,,,,,,,"23/Feb/10 02:07;ashutoshc;Patch which moves SortInfo calculation from LogToPhyTranslation to SortInfoSetter. This facilitate calling checkSchema() before checkOutputSpecs() during store location validation. 
Result of test-patch
{noformat}
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec] 
     [exec] 
{noformat}

Javadoc warning is unrelated to the patch.","23/Feb/10 12:43;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12436664/pig-1251.patch
  against trunk revision 915079.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/212/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/212/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/212/console

This message is automatically generated.",24/Feb/10 00:00;ashutoshc;Seems some problem with hudson. I ran the unit tests manually. They passed. Patch is ready for review.,"26/Feb/10 18:43;daijy;+1 for the patch. Please resync with trunk, uncomment testLocalModeNegative2 and testMapReduceModeInputNegative2 in TestInputOutputFileValidator, then commit.",26/Feb/10 19:28;ashutoshc;Resynced with trunk and updated to address Daniel's comments. Will be committing it shortly.,26/Feb/10 19:34;ashutoshc;Patch committed. ,"26/Feb/10 19:35;dvryaboy;When we first added SortInfo there was some talk (from me :)) about its functionality overlapping with ResourceSchema, which also knows sort order, and how we should get rid of SortInfo and propagate ResourceSchema with the appropriate sortedness, instead, once LSR goes in.  How do you feel about this now? Should we open a ticket, or are there reasons to keep things the way they are?","26/Feb/10 19:45;ashutoshc;@Dmitriy

I agree. ResourceSchema encapsulates both SortInfo and PigSchema within it. So, SortInfo should not really be exposed. It should be a private member of ResourceSchema and where SortInfo is required, it should be accessed via ResourceSchema.  I thought about doing all those changes in this patch, but changes were more involved then I thought. So, I backed-off to keep changes minimal for this patch. We can track that as a separate ticket. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error Number makes it hard to debug: ERROR 2999: Unexpected internal error. org.apache.pig.backend.datastorage.DataStorageException cannot be cast to java.lang.Error,PIG-1247,12456876,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,viraj,viraj,20/Feb/10 02:33,04/Aug/11 00:35,14/Mar/19 03:05,06/Apr/11 21:22,0.6.0,,,,,,0.9.0,,impl,,,0,,,,"I have a large script in which there are intermediate stores statements, one of them writes to a directory I do not have permission to write to. 

The stack trace I get from Pig is this:

2010-02-20 02:16:32,055 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2999: Unexpected internal error. org.apache.pig.backend.datastorage.DataStorageException cannot be cast to java.lang.Error

Details at logfile: /home/viraj/pig_1266632145355.log

Pig Stack Trace
---------------

ERROR 2999: Unexpected internal error. org.apache.pig.backend.datastorage.DataStorageException cannot be cast to java.lang.Error
java.lang.ClassCastException: org.apache.pig.backend.datastorage.DataStorageException cannot be cast to java.lang.Error
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.StoreClause(QueryParser.java:3583)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:1407)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:949)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:762)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:63)
        at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1036)
        at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:986)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:386)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:720)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:324)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:168)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:144)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:89)
        at org.apache.pig.Main.main(Main.java:386)
================================================================================

The only way to find the error was to look at the javacc generated QueryParser.java code and do a System.out.println()


Here is a script to reproduce the problem:

{code}
A = load '/user/viraj/three.txt' using PigStorage();
B = foreach A generate ['a'#'12'] as b:map[] ;
store B into '/user/secure/pigtest' using PigStorage();
{code}

""three.txt"" has 3 lines which contain nothing but the number ""1"".

{code}
$ hadoop fs -ls /user/secure/

ls: could not get get listing for 'hdfs://mynamenode/user/secure' : org.apache.hadoop.security.AccessControlException: Permission denied: user=viraj, access=READ_EXECUTE, inode=""secure"":secure:users:rwx------

{code}


Viraj",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-02-20 02:48:05.774,,,no_permission,,,,,,,,,,,,66132,,,,,Wed Apr 06 21:22:47 UTC 2011,,,,,,,0|i0gtof:,96238,,,,,,,,,,20/Feb/10 02:48;daijy;This error handling code is hard coded by javacc. Seems we do not have a way to get around currently.,25/Feb/10 23:14;olgan;This depends on parser changes which we can't change in Pig 0.7.0,"17/Mar/11 21:12;olgan;Viraj,

I could not actually reproduce this behavior with no permissions to the output location.

This is the error I got:

2011-03-17 20:56:07,278 [main] ERROR org.apache.pig.tools.grunt.Grunt - You don't have permission to perform the operation. Error from the server: Output Location Validation Failed for: '/user/tejas/foo More info to follow:
org.apache.hadoop.security.AccessControlException: Permission denied: user=olgan, access=EXECUTE, inode=""tejas"":tejas:users:rwx------
2011-03-17 20:56:07,279 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 6000: Output Location Validation Failed for: '/user/tejas/foo More info to follow:
org.apache.hadoop.security.AccessControlException: Permission denied: user=olgan, access=EXECUTE, inode=""tejas"":tejas:users:rwx------
","06/Apr/11 21:22;olgan;I could not reproduce the issue with the latest code. Please, re-open if this is still a problem",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Passing Complex map types to and from streaming causes a problem,PIG-1243,12456728,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,viraj,viraj,18/Feb/10 23:26,02/May/13 02:29,14/Mar/19 03:05,24/Feb/10 20:01,0.6.0,,,,,,0.7.0,,,,,0,,,,"I have a program which generates different types of Maps fields and stores it into PigStorage.
{code}
A = load '/user/viraj/three.txt' using PigStorage();

B = foreach A generate ['a'#'12'] as b:map[], ['b'#['c'#'12']] as c, ['c'#{(['d'#'15']),(['e'#'16'])}] as d;

store B into '/user/viraj/pigtest' using PigStorage();
{code}

Now I test the previous output in the below script to make sure I have the right results. I also pass this data to a Perl script and I observe that the complex Map types I have generated, are lost when I get the result back.

{code}
DEFINE CMD `simple.pl` SHIP('simple.pl');

A = load '/user/viraj/pigtest' using PigStorage() as (simpleFields, mapFields, mapListFields);

B = foreach A generate $0, $1, $2;

dump B;

C = foreach A generate  (chararray)simpleFields#'a' as value, $0,$1,$2;

D = stream C through CMD as (a0:map[], a1:map[], a2:map[]);

dump D;
{code}


dumping B results in:

([a#12],[b#[c#12]],[c#{([d#15]),([e#16])}])
([a#12],[b#[c#12]],[c#{([d#15]),([e#16])}])
([a#12],[b#[c#12]],[c#{([d#15]),([e#16])}])

dumping D results in:

([a#12],,)
([a#12],,)
([a#12],,)

The Perl script used here is:
{code}
#!/usr/local/bin/perl

use warnings;

use strict;

while(<>) {

    my($bc,$s,$m,$l)=split/\t/;

    print(""$s\t$m\t$l"");

}
{code}

Is there an issue with handling of complex Map fields within streaming? How can I fix this to obtain the right result?

Viraj",,,,,,,,,,,,,,,,,PIG-613,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-02-19 01:18:25.154,,,no_permission,,,,,,,,,,,,164756,,,,,Wed Feb 24 20:01:13 UTC 2010,,,,,,,0|i0gtn3:,96232,,,,,,,,,,"19/Feb/10 01:18;olgan;Richard, could you please take a look at what the issue is and whether there are workaround","19/Feb/10 23:35;rding;This is a known issue on casting bytearrays  to maps that will be fixed by PIG-613.

  ",24/Feb/10 20:01;rding;This is fixed by way of PIG-613.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Accumulator is turned on when a map is used with a non-accumulative UDF,PIG-1241,12456490,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yinghe,yinghe,yinghe,16/Feb/10 23:46,14/May/10 06:46,14/Mar/19 03:05,20/Feb/10 02:26,0.6.0,,,,,,0.7.0,,,,,0,,,,"Exception is thrown for a script like the following:

register /homes/yinghe/owl/string.jar;
a = load 'a.txt' as (id, url);
b = group  a by (id, url);
c = foreach b generate  COUNT(a), (CHARARRAY) string.URLPARSE(group.url)#'url';
dump c;

In this query, URLPARSE() is not accumulative, and it returns a map. 

The accumulator optimizer failed to check UDF in this case, and tries to run the job in accumulative mode. ClassCastException is thrown when trying to cast UDF into Accumulator interface.",,,,,,,,,,,,,,,,,,,16/Feb/10 23:47;yinghe;accum.patch;https://issues.apache.org/jira/secure/attachment/12436056/accum.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-02-17 04:15:54.665,,,no_permission,,,,,,,,,,,,164754,,,,,Sat Feb 20 02:26:32 UTC 2010,,,,,,,0|i0gtmf:,96229,,,,,,,,,,16/Feb/10 23:47;yinghe;patch to check UDF when it's with map operation,"17/Feb/10 04:15;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12436056/accum.patch
  against trunk revision 909921.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/212/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/212/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/212/console

This message is automatically generated.","17/Feb/10 19:24;olgan;Ying, I thought that accumlate is on by default but it looks like you are setting it on only if the property is present. Is this true?","17/Feb/10 19:39;yinghe;no, by default it is on.

boolean isAccum = ""true"".equalsIgnoreCase(pc.getProperties().getProperty(""opt.accumulator"",""true""));

means if ""opt.accumulator"" is not present, the default value is ""true""","20/Feb/10 02:26;olgan;patch committed to the trunk. Thanks, Ying",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigContext.connect() should not create a jobClient and jobClient should be created on demand when needed,PIG-1239,12456465,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,16/Feb/10 18:46,24/Mar/10 22:16,14/Mar/19 03:05,16/Feb/10 20:37,0.6.0,,,,,,0.6.0,0.7.0,,,,0,,,,PigContext.connect() currently connects to the jobtracker and creates a JobClient - this causes issue in POMergeJoin/POFRJoin wherein these connections to the jobtracker are made from each map task. The creation of the JobClient is not necessary in PigContext.connect() and a JobClient should be created on demand where it is needed instead.,,,,,,,,,,,,,,,,,,,16/Feb/10 19:37;pkamath;PIG-1239-branch-0.6.patch;https://issues.apache.org/jira/secure/attachment/12436015/PIG-1239-branch-0.6.patch,16/Feb/10 19:37;pkamath;PIG-1239-load-store-redesign-branch.patch;https://issues.apache.org/jira/secure/attachment/12436016/PIG-1239-load-store-redesign-branch.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-02-16 19:41:59.486,,,no_permission,,,,,,,,,,,,164752,Reviewed,,,,Tue Feb 16 20:37:54 UTC 2010,,,,,,,0|i0gtlr:,96226,,,,,,,,,,"16/Feb/10 19:37;pkamath;Attached patches for branch-0.6 and load-store-redesign branch.

Changes are:
 * PigContext.connect() does not create a JobClient - instead it creates and holds a JobConf object - callers have been changed to use the JobConf and create a JobClient 
 * On the load-store-redesign branch, POMergeJoin no longer does a pc.connect since it is no longer needed",16/Feb/10 19:41;olgan;+1 on both patches,"16/Feb/10 20:32;pkamath;* No unit tests are included in both patches since this is difficult to capture in a unit test - manual tests were done to ensure that connections to JobTracker no longer happens from a script using replicated join.
 * Release audit warning are due to diffs in html docs
 * The extra javac warnings are due to use of JobConf which is deprecated - I have added suppressWarning tags which don't seem to help. We need to use JobConf here and there is no way around the warning.

Results from running test-patch ant target for branch-0.6
   [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     -1 release audit.  The applied patch generated 391 release audit warnings (more than the trunk's current 389 warnings).
     [exec]
     [exec]
     [exec]
     [exec]
     [exec] ======================================================================
     [exec] ======================================================================
     [exec]     Finished build.
     [exec] ======================================================================
     [exec] ======================================================================

Results from running test-patch ant target for load-store-redesign branch:

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     -1 javac.  The applied patch generated 105 javac compiler warnings (more than the trunk's current 103 warnings).
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
     [exec] 
     [exec] 

",16/Feb/10 20:37;pkamath;Patch committed to branch-0.6 and load-store-redesign branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dump does not respect the schema,PIG-1238,12456415,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,ankur,ankur,16/Feb/10 10:36,14/May/10 06:46,14/Mar/19 03:05,22/Mar/10 18:14,0.6.0,,,,,,0.7.0,,,,,0,,,,For complex data type and certain sequence of operations dump produces results with non-existent field in the relation.,,,,,,,,,,,,PIG-1278,,,,,,,09/Mar/10 01:46;rding;PIG-1238.patch;https://issues.apache.org/jira/secure/attachment/12438246/PIG-1238.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-02-16 20:02:14.356,,,no_permission,,,,,,,,,,,,164751,Reviewed,,,,Mon Mar 22 18:14:40 UTC 2010,,,,,,,0|i0gtlb:,96224,,,,,,,,,,"16/Feb/10 10:45;ankur;Here is a script to reproduce the issue:-

A = LOAD 'two.txt' USING PigStorage();
B = FOREACH A GENERATE ['a'#'12'] as b:map[], ['b'#['c'#'12']] as mapFields;
C = FOREACH B GENERATE(CHARARRAY) mapFields#'b'#'c' AS f1, RANDOM() AS f2;
D = ORDER C BY f2 PARALLEL 10;
E = LIMIT D 20;
F = FOREACH E GENERATE f1;
describe F;
dump F;

With the above script here is a snippet of the logs that might be useful
...
...
2010-02-16 10:42:44,814 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 90% complete
2010-02-16 10:42:55,966 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete
2010-02-16 10:42:55,981 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Successfully stored result in: ""hdfs://foo/tmp/temp-1870551954/tmp-470213889""
2010-02-16 10:42:55,991 [main] WARN  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Encountered Warning ACCESSING_NON_EXISTENT_FIELD 1 time(s).
2010-02-16 10:42:55,991 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Records written : 1
2010-02-16 10:42:55,991 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Bytes written : 14
2010-02-16 10:42:55,991 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!
(12,)

Note:- If we remove ""PARALLEL 10"" from Order by correct results are produced and NO warning is thrown.","16/Feb/10 20:02;daijy;Hi, Ankur, I encounter syntax error in ""B = FOREACH A GENERATE 'a'#'12' as b:map[], ['b'#'c'#'12'] as mapFields;"". Can you verity the script?",17/Feb/10 04:50;ankur;Daniel the correct syntax is - ['b'#['c'#'12']] as mapFields.,"17/Feb/10 04:51;ankur;Seems like inner [] are making parts of it appear underlined. Correct syntax is
['b'# ['c'#'12'] ] as mapFields","17/Feb/10 04:54;ankur;Sigh....
Enclose 'c'#'12' in a square bracket and then enclose 'b'# ... in another square bracket","18/Feb/10 01:34;daijy;Do an explain, the last limit job is :

MapReduce node 1-99
Map Plan
Local Rearrange[tuple]{double}(false) - 1-103
|   |
|   Project[double][1] - 1-102
|
|---Limit - 1-101
    |
    |---Load(file:/tmp/temp-513510662/tmp1311900615:org.apache.pig.builtin.BinStorage) - 1-100--------
Reduce Plan
Store(fakefile:org.apache.pig.builtin.PigStorage) - 1-109
|
|---Limit - 1-108
    |
    |---New For Each(true)[bag] - 1-107
        |   |
        |   Project[tuple][1] - 1-106
        |
        |---Package[tuple]{double} - 1-105--------
Global sort: false

The project in the map plan is wrong.","09/Mar/10 01:46;rding;Pig inserts a new Limit (or Top-K) job with one reducer after a Limit (or Top-K) job with multiple reducers to ensure the output has the right number of records. 

In the case of Top-K, the new job must also preserve the ordering of the original job. Therefore the sorting key must be passed to the new job. This patch moves the last ForEach statement in above script from the original job to the new job so that the sorting keys are not removed by the first job. ",09/Mar/10 22:25;daijy;+1. Please commit once hudson comes back.,"10/Mar/10 02:15;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12438246/PIG-1238.patch
  against trunk revision 920956.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/230/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/230/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/230/console

This message is automatically generated.",19/Mar/10 12:53;ankur;This does not work for me when i take a fresh checkout from the trunk. I still get the same error,"19/Mar/10 17:57;rding;Hi Ankur,

I run following script

{code}
A = LOAD '1.txt' USING PigStorage();
B = FOREACH A GENERATE ['a'#'12'] as b:map[], ['b'#['c'#'12']] as mapFields;
C = FOREACH B GENERATE(CHARARRAY) mapFields#'b'#'c' AS f1, RANDOM() AS f2;
D = ORDER C BY f2 PARALLEL 10;
E = LIMIT D 20;
F = FOREACH E GENERATE f1;
dump F;
{code}

and it gets the correct result. Can you sync again with the trunk and let me know if the problem still exists?","22/Mar/10 18:14;olgan;Closing since we have not seen a reproducible case. Please, reopen when/if you have one",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to create input slice for har:// files,PIG-1234,12455933,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,szetszwo,szetszwo,10/Feb/10 19:59,14/May/10 06:46,14/Mar/19 03:05,01/Mar/10 23:43,,,,,,,0.7.0,,,,,0,,,,"Tried to load har:// files
{noformat}
grunt> a = LOAD 'har://hdfs-namenode/user/tsz/t20.har/t20' USING PigStorage('\n') AS (line);
grunt> dump 
{noformat}
but pig says
{noformat}
2010-02-10 18:42:20,750 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2118:
 Unable to create input slice for: har://hdfs-namenode/user/tsz/t20.har/t20
{noformat}
",,,,,,,,,,,,,,,PIG-1378,,,,12/Feb/10 22:50;pkamath;PIG-1234.patch;https://issues.apache.org/jira/secure/attachment/12435740/PIG-1234.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-02-10 21:18:21.541,,,no_permission,,,,,,,,,,,,164747,,,,,Mon Mar 01 23:42:49 UTC 2010,,,,,,,0|i0gtjr:,96217,,,,,,,,,,"10/Feb/10 20:03;szetszwo;More error messages:
{noformat}
Backend error message during job submission
-------------------------------------------
org.apache.pig.backend.executionengine.ExecException: ERROR 2118: Unable to create input slice for: har://hdfs-namenode/user/tsz/t20.har/t20
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:269)
	at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:810)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:781)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:730)
	at org.apache.hadoop.mapred.jobcontrol.Job.submit(Job.java:378)
	at org.apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs(JobControl.java:247)
	at org.apache.hadoop.mapred.jobcontrol.JobControl.run(JobControl.java:279)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.IllegalArgumentException: Wrong FS: har://hdfs-namenode/user/tsz/t20.har/t20, expected: hdfs://namenode
	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:310)
	at org.apache.hadoop.hdfs.DistributedFileSystem.checkPath(DistributedFileSystem.java:99)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:155)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:453)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:648)
	at org.apache.pig.backend.hadoop.datastorage.HDataStorage.isContainer(HDataStorage.java:203)
	at org.apache.pig.backend.hadoop.datastorage.HDataStorage.asElement(HDataStorage.java:131)
	at org.apache.pig.impl.io.FileLocalizer.fileExists(FileLocalizer.java:553)
	at org.apache.pig.backend.executionengine.PigSlicer.validate(PigSlicer.java:123)
	at org.apache.pig.impl.io.ValidatingInputFileSpec.validate(ValidatingInputFileSpec.java:59)
	at org.apache.pig.impl.io.ValidatingInputFileSpec.<init>(ValidatingInputFileSpec.java:44)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:240)
	... 7 more

Pig Stack Trace
---------------
ERROR 2118: Unable to create input slice for: har://hdfs-namenode/user/tsz/t20.har/t20

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias a
	at org.apache.pig.PigServer.openIterator(PigServer.java:482)
	at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:539)
	at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:241)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:168)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:144)
	at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:75)
	at org.apache.pig.Main.main(Main.java:352)
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 6015: During execution, encountered a Hadoop error.
	at .apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:269)
	at .apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:810)
	at .apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:781)
	at .apache.hadoop.mapred.JobClient.submitJob(JobClient.java:730)
	at .apache.hadoop.mapred.jobcontrol.Job.submit(Job.java:378)
	at .apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs(JobControl.java:247)
	at .apache.hadoop.mapred.jobcontrol.JobControl.run(JobControl.java:279)
	at .lang.Thread.run(Thread.java:619)
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 2118: Unable to create input slice for: har://hdfs-namenode/user/tsz/t20.har/t20
	... 8 more
Caused by: java.lang.IllegalArgumentException: Wrong FS: har://hdfs-namenode/user/tsz/t20.har/t20, expected: hdfs://namenode
	at .apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:310)
	at .apache.hadoop.hdfs.DistributedFileSystem.checkPath(DistributedFileSystem.java:99)
	at .apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:155)
	at .apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:453)
	at .apache.hadoop.fs.FileSystem.exists(FileSystem.java:648)
	at .apache.pig.backend.hadoop.datastorage.HDataStorage.isContainer(HDataStorage.java:203)
	at .apache.pig.backend.hadoop.datastorage.HDataStorage.asElement(HDataStorage.java:131)
	at .apache.pig.impl.io.FileLocalizer.fileExists(FileLocalizer.java:553)
	at .apache.pig.backend.executionengine.PigSlicer.validate(PigSlicer.java:123)
	at .apache.pig.impl.io.ValidatingInputFileSpec.validate(ValidatingInputFileSpec.java:59)
	at .apache.pig.impl.io.ValidatingInputFileSpec.<init>(ValidatingInputFileSpec.java:44)
	at .apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:240)
================================================================================
{noformat}","10/Feb/10 21:18;pkamath;can you try using a pig.jar compiled from the load store redesign branch - http://svn.apache.org/repos/asf/hadoop/pig/branches/load-store-redesign?

","10/Feb/10 22:22;szetszwo;> can you try using a pig.jar compiled from the load store redesign branch ...
Sure, I will try it later.


Got the same problem if PigStorage is replaced with TextLoader.  My pig version is

Apache Pig version 0.6.0.0.20.1.1001221613 (r902141) 
compiled Jan 22 2010, 16:13:46

","11/Feb/10 19:09;szetszwo;Tried branches/load-store-redesign.  Got an different exception:
{noformat}
Pig Stack Trace
---------------
ERROR 2998: Unhandled internal error. org.apache.pig.impl.logicalLayer.FrontendException: ERROR 0: Incompatible file URI scheme: har : hdfs

java.lang.Error: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 0: Incompatible file URI scheme: har : hdfs
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.LoadClause(QueryParser.java:1483)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:1245)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:911)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:700)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:63)
        at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1035)
        at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:985)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:385)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:712)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:324)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:160)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:136)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:75)
        at org.apache.pig.Main.main(Main.java:356)
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 0: Incompatible file URI scheme: har : hdfs
        at org.apache.pig.LoadFunc.getAbsolutePath(LoadFunc.java:246)
        at org.apache.pig.LoadFunc.relativeToAbsolutePath(LoadFunc.java:62)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.LoadClause(QueryParser.java:1472)
        ... 13 more
================================================================================
{noformat}",12/Feb/10 22:50;pkamath;Patch against load-store-redesign branch which fixes this - the code currently was trying to validate the schema supplied in the load vs. schema of the current directory path (which is always hdfs). The patch makes the change to not do this check if the local is a valid url with authority.,"12/Feb/10 22:52;pkamath;Results from running test-patch ant target:
  [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

I am currently running unit tests against this patch on load-store-redesign branch.","12/Feb/10 23:04;pkamath;A couple of observations about the load statement in the script posted above:
1) '\n' is not a valid argument for PigStorage - the argument is meant to be the field delimiter and '\n' cannot be used for a field delimiter (since it is considered to be the record delimiter by PigStorage)
2) The patch here only fixes the incorrect checking of the scheme in the url - whether ""har://.."" resources can be read by PigStorage or not will depend on whether TextInputFormat can read ""har://.."" resources. PigStorage simply passes the location onto to TextInputFormat which does the real reading.",12/Feb/10 23:11;rding;+1 for commit.,"16/Feb/10 18:51;szetszwo;> 1) '\n' is not a valid argument for PigStorage ...

I wrote a pig wordcount program as show below.
{code}
-- ExtractWord below is an UDF
REGISTER ./tutorial.jar;

a = LOAD 'inputDir' USING PigStorage('\n') AS (line);
b = FOREACH a GENERATE flatten(org.apache.pig.tutorial.ExtractWord(line)) as word;
c = GROUP b BY word;
d = FOREACH c GENERATE group, COUNT(b);
STORE d INTO 'outputDir' USING PigStorage('\t');
{code}
If inputDir is a hdfs:// dir, the program works fine but if it is replaced by a har:// dir, it fails as shown previously.  It actually don't need any field delimiter.  So I put '\n'.

BTW, do you think it is good to add a pig wordcount example/tutorial?  The existing tutorials are quite lengthy.  They may be too hard for beginners.

> 2) The patch here only fixes the incorrect checking of the scheme ...
I will test it.  Thanks, Pradeep.","16/Feb/10 21:33;szetszwo;The patch worked fine: The wordcount program succeeded and took 53 mins.
{noformat}
grunt> STORE d INTO 't100000_har_pig_wc' USING PigStorage('\t');
2010-02-16 19:35:28,501 [main] WARN  org.apache.pig.PigServer - Encountered Warning IMPLICIT_CAST_TO_CHARARRAY 1 time(s).
...
2010-02-16 19:57:14,018 [Thread-10] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 100000
...
2010-02-16 20:20:23,936 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher
 - Submitting job: job_201002042035_43197 to execution engine.
2010-02-16 20:20:23,936 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher
 - More information at: http://jobtracker:50030/jobdetails.jsp?jobid=job_201002042035_43197
2010-02-16 20:20:23,936 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher
 - To kill this job, use: kill job_201002042035_43197
2010-02-16 20:20:24,447 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete
2010-02-16 20:21:44,201 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 12% complete
2010-02-16 20:21:49,587 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 13% complete
2010-02-16 20:22:03,904 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 17% complete
2010-02-16 20:22:27,098 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 24% complete
2010-02-16 20:23:11,759 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 32% complete
2010-02-16 20:23:33,678 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 39% complete
2010-02-16 20:23:42,900 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 42% complete
2010-02-16 20:23:51,739 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 45% complete
2010-02-16 20:24:00,521 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 47% complete
2010-02-16 20:24:10,577 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 50% complete
2010-02-16 20:24:18,561 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 53% complete
2010-02-16 20:24:27,555 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 56% complete
2010-02-16 20:24:34,101 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 58% complete
2010-02-16 20:24:43,539 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 62% complete
2010-02-16 20:24:58,398 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 66% complete
2010-02-16 20:26:31,290 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 66% complete
2010-02-16 20:27:22,225 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 66% complete
2010-02-16 20:28:17,773 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete
2010-02-16 20:28:17,773 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher
 - Successfully stored result in: ""hdfs://namenode/user/tsz/t100000_har_pig_wc""
2010-02-16 20:28:32,253 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Records written : 132
2010-02-16 20:28:32,253 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Bytes written : 1862
2010-02-16 20:28:32,253 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!
{noformat}","16/Feb/10 21:45;szetszwo;Pig is amazing!  The pig wordcount program with har:// ran faster than the mapreduce wordcount example, which took 57 mins [in my previous test|https://issues.apache.org/jira/browse/HADOOP-6467?focusedCommentId=12830401&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12830401].",01/Mar/10 23:42;pkamath;Patch was committed on Feb 16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in AVG ,PIG-1233,12455870,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ankur,ankur,ankur,10/Feb/10 07:09,14/May/10 06:46,14/Mar/19 03:05,20/Feb/10 01:31,0.6.0,,,,,,0.7.0,,,,,0,,,,The overridden method - getValue() in AVG throws null pointer exception in case accumulate() is not called leaving variable 'intermediateCount'  initialized to null. This causes java to throw exception when it tries to 'unbox' the value for numeric comparison.,,,,,,,,,,,,,,,,,,,15/Feb/10 09:35;ankur;jira-1233.patch;https://issues.apache.org/jira/secure/attachment/12435854/jira-1233.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-02-10 12:02:16.624,,,no_permission,,,,,,,,,,,,164746,,,,,Sat Feb 20 01:31:43 UTC 2010,,,,,,,0|i0gtj3:,96214,,,,,,,,,,10/Feb/10 07:12;ankur;Attached is a very simple patch that adds the required null checks. This is a very simple code change so I don't think any new test cases are needed. ,"10/Feb/10 12:02;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12435420/jira-1233.patch
  against trunk revision 908324.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/198/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/198/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/198/console

This message is automatically generated.","10/Feb/10 16:52;alangates;Shouldn't a test be added that checks that this change fixes the observed bug?  Unless it is very complex to add the test, it should be added.",15/Feb/10 09:35;ankur;Added test case,15/Feb/10 09:35;ankur;Retrying hudson after adding the suggested test case,"15/Feb/10 15:22;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12435854/jira-1233.patch
  against trunk revision 909921.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/205/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/205/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/205/console

This message is automatically generated.","16/Feb/10 04:19;ankur;The test report URLs don't work. Is this the correct one ?
http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/205/testReport/

Looks alright to me.","16/Feb/10 18:50;olgan;Hi Ankur,

We see this kind of issue with tests from time to time. The best thing to do is just resubmit the patch. Also, what are the queries that are impacted by this issue?",16/Feb/10 18:51;olgan;This does not seem like a candidate for 0.6.0 unless the problem is wide spread,"17/Feb/10 04:59;ankur;Olga,
       All queries that use AVG(),  have null values for certain keys and have accumulator turned on for them are affected by this. Please see the test case for a sample query. The current workaround is to filter the nulls before averaging.",17/Feb/10 05:00;ankur;Retrying as suggested by Olga,"17/Feb/10 09:19;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12435854/jira-1233.patch
  against trunk revision 909921.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/208/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/208/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/208/console

This message is automatically generated.",17/Feb/10 19:13;olgan;the patch looks reasonable. One question I have is what would happen if intermediateSum is null. Seems like we should test for that as well.,18/Feb/10 06:56;ankur;In the current code path we cannot have a situation where intermediateCount in NOT null but intermediateSum is null. So just checking the former if sufficient.,"20/Feb/10 01:31;olgan;patch committed to the trunk. Thanks, Ankur!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default DataBagIterator.hasNext() should be idempotent in all cases,PIG-1231,12455675,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,08/Feb/10 19:50,24/Mar/10 22:16,14/Mar/19 03:05,12/Feb/10 20:03,0.6.0,,,,,,0.6.0,,impl,,,0,,,,"DefaultDataBagIterator.hasNext() is not repeatable when the below conditions met:
1. There is no more tuple in the last spill file
2. There is no tuples in memory (all contents are spilled to files)

This is not acceptable cuz the name hasNext() implies that it is idempotent. In BagFormat, we do misuse DataBagIterator.hasNext() because of the assumption that hasNext() is always idempotent, which leads to some mysterious errors. 

Condition 2 seems to be very restrictive, but when the databag is really big, the memory can hold less than a couple of tuples, the chance to hit 2. is high enough.

Here is one error we saw:

Caused by: java.io.IOException: Stream closed
        at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:145)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:189)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
        at java.io.DataInputStream.readByte(DataInputStream.java:248)
        at org.apache.pig.data.DefaultTuple.readFields(DefaultTuple.java:278)
        at org.apache.pig.data.DefaultDataBag$DefaultDataBagIterator.readFromFile(DefaultDataBag.java:237)
        ... 20 more

This happens because: we call hasNext(), which reach EOF and we close the file. Then we call hasNext() again in the assumption that it is idempotent. However, the stream is closed so we get this error message.",,,,,,,,,,,,,,,,,,,09/Feb/10 00:58;daijy;PIG-1231-1.patch;https://issues.apache.org/jira/secure/attachment/12435230/PIG-1231-1.patch,12/Feb/10 01:41;daijy;PIG-1231-2.patch;https://issues.apache.org/jira/secure/attachment/12435645/PIG-1231-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-02-09 10:31:13.854,,,no_permission,,,,,,,,,,,,164744,Reviewed,,,,Fri Feb 12 20:03:15 UTC 2010,,,,,,,0|i0gtif:,96211,,,,,,,,,,09/Feb/10 00:58;daijy;DefaultDataBagIterator is the only DataBag has this problem. Other databag handles this through different mechanisms. ,"09/Feb/10 10:31;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12435230/PIG-1231-1.patch
  against trunk revision 907760.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/206/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/206/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/206/console

This message is automatically generated.",09/Feb/10 17:50;daijy;testCompressed1: java.lang.IllegalArgumentException: port out of range:-1. Not a real problem. Manual test passes.,09/Feb/10 18:20;alangates;+1 Changes look good.  ,09/Feb/10 19:46;daijy;Patch committed to both trunk and 0.6 branch.,12/Feb/10 01:41;daijy;There is unit case failure in 0.6 branch.,"12/Feb/10 09:05;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12435645/PIG-1231-2.patch
  against trunk revision 909210.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/202/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/202/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/202/console

This message is automatically generated.",12/Feb/10 18:16;alangates;+1 for patch 2.,12/Feb/10 19:51;daijy;Seems hudson is not running the testing process at all. Manual test success in both trunk and 0.6 branch. Did not include new testcase since it is a fix to existing testcase.,12/Feb/10 20:03;daijy;Patch committed to both trunk and 0.6 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming input in POJoinPackage should use nonspillable bag to collect tuples,PIG-1230,12455672,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,08/Feb/10 19:32,14/May/10 06:46,14/Mar/19 03:05,10/Feb/10 02:31,0.6.0,,,,,,0.7.0,,impl,,,0,,,,"Last table of join statement is streamed through instead of collecting all its tuple in a bag. As a further optimization of that, tuples of that relation are collected in chunks in a bag. Since we don't want to spill the tuples from this bag, NonSpillableBag should be used to hold tuples for this relation. Initially, DefaultDataBag was used, which was later changed to InternalCachedBag as a part of PIG-1209.",,,,,,,,,,,,,,,,,,,08/Feb/10 20:22;ashutoshc;pig-1230.patch;https://issues.apache.org/jira/secure/attachment/12435203/pig-1230.patch,09/Feb/10 01:51;ashutoshc;pig-1230_1.patch;https://issues.apache.org/jira/secure/attachment/12435238/pig-1230_1.patch,10/Feb/10 02:27;ashutoshc;pig-1230_2.patch;https://issues.apache.org/jira/secure/attachment/12435398/pig-1230_2.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-02-09 00:48:03.648,,,no_permission,,,,,,,,,,,,164743,,,,,Wed Feb 10 02:31:33 UTC 2010,,,,,,,0|i0gthz:,96209,,,,,,,,,,08/Feb/10 20:22;ashutoshc;Patch as per description.,"09/Feb/10 00:48;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12435203/pig-1230.patch
  against trunk revision 907760.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 2 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/204/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/204/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/204/console

This message is automatically generated.","09/Feb/10 01:51;ashutoshc;Fixed findbugs warnings. Result of test-patch:
{code}
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

{code}

Hard to write unit test case. Patch ready for review.",09/Feb/10 18:50;ashutoshc;This patch switches POJoinPackage to use NonSpillableDataBag for last bag instead of currently used InternalCachedBag. Both of these bag implementations are already covered by existing unit tests and thus this patch needs no new tests. ,"09/Feb/10 18:59;olgan;The patch looks good. One comment: when iterating through bags,  we should say numInputs -1 rather than lastBagIndex (which happens to have the right value.) to make the code more readable and intent more clear. After the change is made, the patch can be committed","10/Feb/10 02:27;ashutoshc;As per comment changed lastBagIndex to numInputs - 1, no other changes.",10/Feb/10 02:31;ashutoshc;Patch checked-in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[zebra] Missing column group meta file should not be allowed at query time,PIG-1227,12455516,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,yanz,yanz,yanz,06/Feb/10 01:35,24/Mar/10 22:16,14/Mar/19 03:05,08/Feb/10 22:04,0.6.0,,,,,,0.6.0,,,,,0,,,,"A missing column group meta file may indicate an unfinished or interrupted data publish process and should not be allowed at query time. Currently for sorted tables, an exception is thrown; but query on an unsorted table goes through.",,,,,,,,,,,,,,,,,,,06/Feb/10 23:40;yanz;PIG-1227.patch;https://issues.apache.org/jira/secure/attachment/12435094/PIG-1227.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-02-08 17:23:22.786,,,no_permission,,,,,,,,,,,,164740,,,,,Thu Feb 18 17:10:18 UTC 2010,,,,,,,0|i0gtgv:,96204,,,,,,,,,,"08/Feb/10 17:23;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12435094/PIG-1227.patch
  against trunk revision 907463.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 36 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/202/console

This message is automatically generated.","08/Feb/10 17:28;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12435094/PIG-1227.patch
  against trunk revision 907463.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 36 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/203/console

This message is automatically generated.","08/Feb/10 20:07;yanz;The patch is only applicable to the Apache 0.6 branch only, not on the trunk.

A separate Hudson run on the branch succeeds.",08/Feb/10 21:58;chaow;Patch looks good +1.,08/Feb/10 22:04;yanz;Patch committed to the 0.6 branch.,18/Feb/10 17:10;yanz;Patch committed to the load-store-redesign branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to be able to register jars on the command line,PIG-1226,12455502,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,alangates,alangates,05/Feb/10 22:10,08/Jul/10 00:35,14/Mar/19 03:05,17/Feb/10 19:22,,,,,,,0.7.0,,,,,0,,,,"Currently 'register' can only be done inside a Pig Latin script.  Users often run their scripts in different environments, so jar locations or versions may change.  But they don't want to edit their script to fit each environment.  Instead they could register on the command line, something like:

pig -Dpig.additional.jars=my.jar:your.jar script.pig

These would not override registers in the Pig Latin script itself.
",,,,,,,,,,,,,,,,,,,12/Feb/10 04:02;thejas;PIG-1126.patch;https://issues.apache.org/jira/secure/attachment/12435658/PIG-1126.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-02-12 08:32:48.852,,,no_permission,,,,,,,,,,,,164739,,,,,Wed Feb 17 19:22:27 UTC 2010,,,,,,,0|i0gtgf:,96202,,,,,,,,,,"12/Feb/10 08:32;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12435658/PIG-1126.patch
  against trunk revision 909210.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/210/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/210/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/210/console

This message is automatically generated.","16/Feb/10 21:42;olgan;+1, changes look good.

Thejas, can you make sure that the patch applies to LSR branch since we are getting ready to merge it back to the trunk.","17/Feb/10 19:22;olgan;patch committed to trunk. Thanks, Thejas",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filter equality does not work for tuples,PIG-1221,12455287,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,zjffdu,neilblue-bw,neilblue-bw,04/Feb/10 09:20,17/Dec/10 22:43,14/Mar/19 03:05,21/Jun/10 02:14,0.8.0,,,,,,0.8.0,,impl,,,0,equality,filter,tuple,"From the documentation I understand that it should be possible to  filter a relation based on the equality of tuples. http://wiki.apache.org/pig/PigTypesFunctionalSpec , http://hadoop.apache.org/pig/docs/r0.5.0/piglatin_reference.html#deref:

 However with this data file

-- indext.txt:
(1,one) (1,ONE)
(2,two) (22, twentytwo)
(3,three)       (3,three)

I run this pig script:
A = LOAD 'indext.txt' AS (t1:(a:int, b:chararray), t2:(a:int, b:chararray)); B = FILTER A BY t1==t2; DUMP B;
Expecting the output:
((3,three),(3,three))

However there is an error:
2010-02-03 09:05:20,523 [main] ERROR org.apache.pig.tools.grunt.Grunt 
- ERROR 2067: EqualToExpr does not know how to handle type: tuple
> Pig Stack Trace
> ---------------
> ERROR 2067: EqualToExpr does not know how to handle type: tuple
> org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066:  
> Unable to
> open iterator for alias B
>        at org.apache.pig.PigServer.openIterator(PigServer.java:475)
>        at
> org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java: 
> 532)
>        at
> org
> .apache
> .pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.
> java:190)
>        at
> org
> .apache
> .pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:166
> )
>        at
> org
> .apache
> .pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:142
> )
>        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:89)
>        at org.apache.pig.Main.main(Main.java:397)
> Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR
> 1002:
> Unable to store alias B
>        at org.apache.pig.PigServer.store(PigServer.java:530)
>        at org.apache.pig.PigServer.openIterator(PigServer.java:458)
>        ... 6 more
> Caused by: org.apache.pig.backend.executionengine.ExecException:  
> ERROR 2067:
> EqualToExpr does not know how to handle type: tuple
>        at
> org
> .apache
> .pig.backend.hadoop.executionengine.physicalLayer.expressionOperat
> ors.EqualToExpr.getNext(EqualToExpr.java:108)
>        at
> org
> .apache
> .pig.backend.hadoop.executionengine.physicalLayer.relationalOperat
> ors.POFilter.getNext(POFilter.java:148)
>        at
> org
> .apache
> .pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator
> .processInput(PhysicalOperator.java:231)
>        at
> org
> .apache
> .pig.backend.local.executionengine.physicalLayer.counters.POCounte
> r.getNext(POCounter.java:71)
>        at
> org
> .apache
> .pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator
> .processInput(PhysicalOperator.java:231)
>        at
> org
> .apache
> .pig.backend.hadoop.executionengine.physicalLayer.relationalOperat
> ors.POStore.getNext(POStore.java:117)
>        at
> org
> .apache
> .pig.backend.local.executionengine.LocalPigLauncher.runPipeline(Lo
> calPigLauncher.java:146)
>        at
> org
> .apache
> .pig.backend.local.executionengine.LocalPigLauncher.launchPig(Loca
> lPigLauncher.java:109)
>        at
> org
> .apache
> .pig.backend.local.executionengine.LocalExecutionEngine.execute(Lo
> calExecutionEngine.java:165)

Thanks
Neil
",Windows and Linux. Java 1.6 hadoop 0.20.1,,,,,,,,,,,,,,,,,,17/Jun/10 06:46;zjffdu;PIG_1221.patch;https://issues.apache.org/jira/secure/attachment/12447317/PIG_1221.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-02-04 15:14:19.3,,,no_permission,,,,,,,,,,,,164735,,,,,Mon Jun 21 02:11:27 UTC 2010,,,,,,,0|i0gte7:,96192,,,,,,,tuple filter equality,,,"04/Feb/10 15:14;ashutoshc;Looking at code it seems we don't support equality on maps either, while specification tells us we should. ","17/Jun/10 06:46;zjffdu;Attach the patch for make Tuple and Map supported in filter statement
 (both equal to and not equal to). I do not make tuple and map supported in  GreaterThanExpr and LessThanExpr. Because I think the comparison meaning in less than and greater than is not clear for tuple and map.","17/Jun/10 12:40;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12447317/PIG_1221.patch
  against trunk revision 955028.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/340/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/340/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/340/console

This message is automatically generated.","18/Jun/10 01:38;zjffdu;I checked the test log, the errors from contrib tests do not related with this patch.
",18/Jun/10 18:09;alangates;+1,21/Jun/10 02:11;zjffdu;patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document unknown keywords as missing or to do in future,PIG-1220,12455256,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,viraj,viraj,04/Feb/10 01:02,14/May/10 06:46,14/Mar/19 03:05,04/Mar/10 00:10,0.6.0,,,,,,0.7.0,,documentation,,,0,Documentation,,,"To get help at the grunt shell I do the following:

grunt>touchz

010-02-04 00:59:28,714 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Encountered "" <IDENTIFIER> ""touchz """" at line 1, column 1.
Was expecting one of:
    <EOF> 
    ""cat"" ...
    ""fs"" ...
    ""cd"" ...
    ""cp"" ...
    ""copyFromLocal"" ...
    ""copyToLocal"" ...
    ""dump"" ...
    ""describe"" ...
    ""aliases"" ...
    ""explain"" ...
    ""help"" ...
    ""kill"" ...
    ""ls"" ...
    ""mv"" ...
    ""mkdir"" ...
    ""pwd"" ...
    ""quit"" ...
    ""register"" ...
    ""rm"" ...
    ""rmf"" ...
    ""set"" ...
    ""illustrate"" ...
    ""run"" ...
    ""exec"" ...
    ""scriptDone"" ...
    """" ...
    <EOL> ...
    "";"" ...

I looked at the code and found that we do nothing at:

""scriptDone"": Is there some future value of that command.

Viraj",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-03-04 00:10:45.834,,,no_permission,,,,,,,,,,,,164734,,,,,Thu Mar 04 00:10:45 UTC 2010,,,,,,,0|i0gtdr:,96190,,,,,,,,,,04/Mar/10 00:10;olgan;There are some internal keywords. They can't be removed from the list - the message come from javacc. We don't want to document them.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[piggybank] evaluation.util.Top is broken,PIG-1217,12455186,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,dvryaboy,dvryaboy,dvryaboy,03/Feb/10 16:40,14/May/10 06:46,14/Mar/19 03:05,11/Feb/10 19:35,0.3.0,0.4.0,0.5.0,0.6.0,0.7.0,site,0.7.0,,,,,0,,,,"The Top udf has been broken for a while, due to an incorrect implementation of getArgToFuncMapping.",,,,,,,,,,,,,,,,,,,10/Feb/10 06:26;dvryaboy;fix_top_udf.diff;https://issues.apache.org/jira/secure/attachment/12435416/fix_top_udf.diff,04/Feb/10 17:36;dvryaboy;fix_top_udf.diff;https://issues.apache.org/jira/secure/attachment/12434839/fix_top_udf.diff,04/Feb/10 00:45;dvryaboy;fix_top_udf.diff;https://issues.apache.org/jira/secure/attachment/12434746/fix_top_udf.diff,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-02-03 21:12:50.589,,,no_permission,,,,,,,,,,,,41720,,,,,Thu Feb 11 19:35:51 UTC 2010,,,,,,,0|i0gtc7:,96183,,,,,,,,,,"03/Feb/10 16:44;dvryaboy;Attached patch fixes the issue.

While I was in there, I also made it algebraic, and reduced the amount of reflection this function required (it used to call instanceof twice for every comparison operation, several of which may be needed for every tuple).

This patch should apply to 0.6 as well.",03/Feb/10 17:08;dvryaboy;Attaching a version for pig 0.5,"03/Feb/10 21:12;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12434689/fix_top_udf.diff
  against trunk revision 905377.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/189/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/189/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/189/console

This message is automatically generated.",04/Feb/10 00:45;dvryaboy;a patch that actually works :-),"04/Feb/10 09:52;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12434746/fix_top_udf.diff
  against trunk revision 906326.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/198/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/198/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/198/console

This message is automatically generated.",04/Feb/10 11:50;dvryaboy;The test failures appear to be unrelated to this change. Please review.,"04/Feb/10 17:36;dvryaboy;Huh. I wonder what Hudson tested -- I accidentally attached an old version of the unit test, which doesn't even compile with the new Top.  But Hudson passed contrib tests, and managed to fail on core tests.","04/Feb/10 18:40;alangates;In general, looks good.  A comment on Top.Initial.  If you do something like

B = group A ...
C = foreach B generate myudf(A);

and myudf is algebraic, you are guaranteed to only get one record at a time in the Initial function because Pig doesn't do any collecting of the keys.  That is, even if ten records in a row have the same key Pig won't detect that and collate them into the bag before calling Initial.  We take advantage of that in a number of the built in functions (eg COUNT) to make the processing of Initial easier.  You may want to do the same here.

As far as getting it into 0.6 release, I think Olga was trying to roll the package today or tomorrow, so we may be out of time.","04/Feb/10 18:56;dvryaboy;I see, thanks for the tip. How does this work with tuple reuse -- can I just return the input tuple, or do I need to copy the contents to a new tuple in Top.Initial() ?

No worries about 0.6, I'd rather it finally go out than try to get something like this in at the last moment.","04/Feb/10 22:00;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12434839/fix_top_udf.diff
  against trunk revision 906326.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/200/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/200/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/200/console

This message is automatically generated.","10/Feb/10 06:26;dvryaboy;Simplified Initial per Alan's comments (just returning the tuple doesn't work, btw).
Also made it a bit safer around nulls.","10/Feb/10 10:54;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12435416/fix_top_udf.diff
  against trunk revision 908324.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/207/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/207/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/207/console

This message is automatically generated.","11/Feb/10 01:01;alangates;When I ran the piggybank tests with this patch I got:

{code}
Testsuite: org.apache.pig.piggybank.test.evaluation.util.TestTop
Tests run: 2, Failures: 1, Errors: 0, Time elapsed: 0.044 sec

Testcase: testTopExec took 0.017 sec
Testcase: testTopAlgebraic took 0.006 sec
    FAILED
Value 0 exceeded the expected limit
junit.framework.AssertionFailedError: Value 0 exceeded the expected limit
    at org.apache.pig.piggybank.test.evaluation.util.TestTop.checkItemsGT(Unknown Source)
    at org.apache.pig.piggybank.test.evaluation.util.TestTop.testTopAlgebraic(Unknown Source)
{code}","11/Feb/10 01:14;dvryaboy;Oops, right, that check is actually no longer valid based on what you said about how the data gets fed into Initial.
Can you delete line 72 of TestTop and rerun? 
Or I can repost a patch.

","11/Feb/10 19:35;alangates;Removed line 72 of the test per Dmitriy request.  With that change, tests went fine.  Patch checked in.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New load store design does not allow Pig to validate inputs and outputs up front,PIG-1216,12455131,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ashutoshc,alangates,alangates,03/Feb/10 01:14,14/May/10 06:46,14/Mar/19 03:05,26/Feb/10 19:35,0.7.0,,,,,,0.7.0,,,,,0,,,,"In Pig 0.6 and before, Pig attempts to verify existence of inputs and non-existence of outputs during parsing to avoid run time failures when inputs don't exist or outputs can't be overwritten.  The downside to this was that Pig assumed all inputs and outputs were HDFS files, which made implementation harder for non-HDFS based load and store functions.  In the load store redesign (PIG-966) this was delegated to InputFormats and OutputFormats to avoid this problem and to make use of the checks already being done in those implementations.  Unfortunately, for Pig Latin scripts that run more then one MR job, this does not work well.  MR does not do input/output verification on all the jobs at once.  It does them one at a time.  So if a Pig Latin script results in 10 MR jobs and the file to store to at the end already exists, the first 9 jobs will be run before the 10th job discovers that the whole thing was doomed from the beginning.  

To avoid this a validate call needs to be added to the new LoadFunc and StoreFunc interfaces.  Pig needs to pass this method enough information that the load function implementer can delegate to InputFormat.getSplits() and the store function implementer to OutputFormat.checkOutputSpecs() if s/he decides to.  Since 90% of all load and store functions use HDFS and PigStorage will also need to, the Pig team should implement a default file existence check on HDFS and make it available as a static method to other Load/Store function implementers.  ",,,,,,,,,,PIG-966,,,,,,,,,13/Feb/10 02:21;ashutoshc;pig-1216.patch;https://issues.apache.org/jira/secure/attachment/12435759/pig-1216.patch,17/Feb/10 00:40;ashutoshc;pig-1216_1.patch;https://issues.apache.org/jira/secure/attachment/12436067/pig-1216_1.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-02-16 19:46:03.478,,,no_permission,,,,,,,,,,,,164731,Reviewed,,,,Fri Feb 19 16:55:22 UTC 2010,,,,,,,0|i0gtbr:,96181,,,,,,,,,,"16/Feb/10 19:46;pkamath;Review comments:
 * Is it ok to call outputSpecs multiple times (since we will now be calling it in the visitor and Hadoop will be calling it later when the job is launched) - hope that does not break the contract per Hadoop's OutputFormat interface
 * The test case for validation failure should ensure that PlanValidationException is indeed thrown (through some boolean flag?) - currently the code has :
{code}
} catch (PlanValidationException pve){
+           // We expect this to happen.
+        }
{code}
 * import org.omg.PortableInterceptor.SUCCESSFUL; in TestStore.java seems accidental - if you will be submitting a new patch for above comment, you can remove this import also.

Otherwise looks good.","17/Feb/10 00:40;ashutoshc;bq. Is it ok to call outputSpecs multiple times [...]
Talked with Arun regarding this. In a user supplied OutputFormat, implementation of checkOutputSpecs() will also be provided by user. So, user needs to make sure this call is idempotent. PigStorage uses TextOutputFormat for which checkOutputSpecs() is idempotent. We need to document this fact in user manual.

bq. the test case for validation failure [...]
Done.

bq. import [...]
Done.

Result of test-patch.sh on the patch:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.","17/Feb/10 02:22;ashutoshc;Thinking more about this. We don't do validation on input side because the input location (or files) may get created over the course of execution of pig script, rendering such validation for input not only useless but incorrect. But similar situation may exist for output validation as well. Assume simple case of HDFS as storage and  the output location exists in HDFS. Now user may have rmf statements within the script, so output location is actually deleted before that job is executed, but if we do upfront validation Pig will fail and refuse to run script saying outputformat.checkspecs() is asserting output location exists at compile time. 
In a more general case, invariants which are true at the compile time of Pig script may no longer hold at runtime, resulting in doing such kind of validation at compile time dangerous. ","17/Feb/10 04:19;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12436067/pig-1216_1.patch
  against trunk revision 909921.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/207/console

This message is automatically generated.","17/Feb/10 18:24;pkamath;Patch committed to load-store-redesign branch - Thanks Ashutosh!

Note that only outputs will be validated up front (in line with Pig 0.6.0) - inputs will not be validated up front since for the following case validating inputs is not easy:
{code}
...
store into 'foo'...
load 'foo'...
...
{code}","18/Feb/10 22:49;ashutoshc;Referring to point # 1 of Pradeep's review comment:

So, for Zebra It is not safe to call check outputSpecs() multiple times because they create indices in this function call. So, this approach doesn't work. Proposal is to introduce validate() in storefunc api which Storer can implement in whatever way they want, thus getting rid of this restriction. In PigStorage's validate() we will call outputSpecs() since it is safe to do so there.",19/Feb/10 16:55;ashutoshc;Reopening as the assumption made for the patch doesn't hold.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schema serialization is broken,PIG-1213,12447049,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,29/Jan/10 20:32,24/Mar/10 22:16,14/Mar/19 03:05,16/Feb/10 18:56,0.6.0,,,,,,0.6.0,,,,,0,,,,"Consider a udf which needs to know the schema of its input in the backend while executing. To achieve this, the udf needs to store the schema into the UDFContext. Internally the UDFContext will serialize the schema into the jobconf. However this currently is broken and gives a Serialization exception",,,,,,,,,,,,,,,,,,,29/Jan/10 21:59;pkamath;PIG-1213.patch;https://issues.apache.org/jira/secure/attachment/12431826/PIG-1213.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-01-29 22:41:49.478,,,no_permission,,,,,,,,,,,,164728,Reviewed,,,,Tue Feb 16 18:56:10 UTC 2010,,,,,,,0|i0gtan:,96176,,,,,,,,,,29/Jan/10 21:59;pkamath;Attached patch addresses the issue - a couple of data structures in Schema are only used by the front-end query planning code to figure out lineage to handle casts of bytearrays. These data structures don't need to be serialized to the backend. Hence in the patch they are marked transient which fixes the issue.,29/Jan/10 22:41;daijy;+1. Please commit once hudson reviewed.,"30/Jan/10 03:15;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12431826/PIG-1213.patch
  against trunk revision 904241.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/193/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/193/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/193/console

This message is automatically generated.",16/Feb/10 18:56;pkamath;Patch was committed to trunk and branch-0.6 on 01 Feb 2010,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogicalPlan.replaceAndAddSucessors produce wrong result when successors are null,PIG-1212,12446991,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,29/Jan/10 07:48,14/May/10 06:46,14/Mar/19 03:05,30/Jan/10 02:25,0.6.0,,,,,,0.7.0,,impl,,,0,,,,"The following script throw a NPE:

a = load '1.txt' as (a0:chararray);
b = load '2.txt' as (b0:chararray);
c = join a by a0, b by b0;
d = filter c by a0 == 'a';
explain d;",,,,,,,,,,,,,,,,,,,29/Jan/10 08:26;daijy;PIG-1212-1.patch;https://issues.apache.org/jira/secure/attachment/12431758/PIG-1212-1.patch,29/Jan/10 20:18;daijy;PIG-1212-2.patch;https://issues.apache.org/jira/secure/attachment/12431811/PIG-1212-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-01-29 12:54:47.908,,,no_permission,,,,,,,,,,,,164727,Reviewed,,,,Sat Jan 30 02:25:27 UTC 2010,,,,,,,0|i0gt9z:,96173,,,,,,,,,,"29/Jan/10 12:54;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12431758/PIG-1212-1.patch
  against trunk revision 904241.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/192/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/192/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/192/console

This message is automatically generated.","29/Jan/10 19:46;rding;Looks good. The only comment I have is about coding convention: ""if statements always use braces {}.""  Otherwise +1.",29/Jan/10 20:18;daijy;Address Richard's comment.,30/Jan/10 02:25;daijy;Patch committed. Thanks Richard!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fieldsToRead send the same fields more than once in some cases,PIG-1210,12446973,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,29/Jan/10 01:29,24/Mar/10 22:16,14/Mar/19 03:05,30/Jan/10 02:23,0.6.0,,,,,,0.6.0,,,,,0,,,,"This bug will happen if the following condition meet:
1. LoadFunc is susceptible to duplicated fields in fieldsToRead. The only LoadFunc we notice now is Zebra.
2. The first item in FOREACH statement contains reference to the same input more than once.

For example, the following script will be affected:
a = load '11' using org.apache.hadoop.zebra.pig.TableLoader('a0');
b = foreach a generate a0+a0;
",,,,,,,,,,,,,,,,,,,29/Jan/10 02:52;daijy;PIG-1210-1.patch;https://issues.apache.org/jira/secure/attachment/12431740/PIG-1210-1.patch,29/Jan/10 18:23;daijy;PIG-1210-2.patch;https://issues.apache.org/jira/secure/attachment/12431790/PIG-1210-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-01-29 11:12:47.084,,,no_permission,,,,,,,,,,,,164725,Reviewed,,,,Sat Jan 30 02:23:10 UTC 2010,,,,,,,0|i0gt9b:,96170,,,,,,,,,,"29/Jan/10 11:12;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12431740/PIG-1210-1.patch
  against trunk revision 904241.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/183/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/183/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/183/console

This message is automatically generated.",29/Jan/10 18:23;daijy;Attach patch with test case.,"29/Jan/10 23:10;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12431790/PIG-1210-2.patch
  against trunk revision 904241.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/184/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/184/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/184/console

This message is automatically generated.","29/Jan/10 23:19;daijy;Test failure is due to ""java.lang.IllegalArgumentException: port out of range:-1"". Should be an temporal one.","30/Jan/10 02:06;olgan;+1. The changes looks good.

I am wondering why we are using listed instead of set in this case but that's a bigger question outside of this patch.","30/Jan/10 02:15;daijy;List is the data structure needed for the construct of RequiredFields. Yes, we could Set, but we need to check if any of our code assume the order within the list, since if we use Set, we lose the order. We can think about that in the new logical plan.",30/Jan/10 02:23;daijy;Patch committed to both trunk and 0.6 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Port POJoinPackage to proactively spill,PIG-1209,12446970,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ashutoshc,sriranjan,sriranjan,29/Jan/10 01:01,14/May/10 06:46,14/Mar/19 03:05,04/Feb/10 21:29,,,,,,,0.7.0,,,,,0,,,,POPackage proactively spills the bag whereas POJoinPackage still uses the SpillableMemoryManager. We should port this to use InternalCacheBag which proactively spills.,,,,,,,,,,,,,,,,,,,02/Feb/10 02:16;ashutoshc;pig-1209.patch;https://issues.apache.org/jira/secure/attachment/12434483/pig-1209.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-01-29 21:25:38.983,,,no_permission,,,,,,,,,,,,164724,,,,,Tue Feb 09 18:32:51 UTC 2010,,,,,,,0|i0gt8v:,96168,,,,,,,,,,"29/Jan/10 21:25;olgan;The change would require stitching from old style spillable bags to the new one implemented by Ying. Testing would be a bit tricky. I think Pradeep had some ideas

","29/Jan/10 21:51;pkamath;For testing, the data for the first input of the join can be a large number of records (so that size in memory > 500 MB or so) with the same join key. This will hopefully spill and fail with old code and not fail with new code.",02/Feb/10 02:16;ashutoshc;Attached patch which makes POJoinPackage to use InternalCachedBag instead of DefaultBag. Will be testing it as Pradeep suggested. Running through hudson to make sure it doesn't fail existing test cases. ,"02/Feb/10 08:53;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12434483/pig-1209.patch
  against trunk revision 905377.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/196/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/196/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/196/console

This message is automatically generated.","04/Feb/10 03:24;ashutoshc;Did manual testing on this. With large enough dataset some reducers fail with ""Error: GC overhead limit exceeded"". After applying this patch, those failures didn't happen. This patch is ready for review. ",04/Feb/10 18:45;olgan;+1. Changes look good,04/Feb/10 21:29;ashutoshc;Patch checked-in.,"09/Feb/10 18:32;olgan;The current unit tests adequately cover the testing of this internal change. Additionally, Ashutosh ran several e2e tests and also verified that this change fixed user problem. User script no longer ran out of memory",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig hangs when joining two streaming relations in local mode,PIG-1204,12446753,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,26/Jan/10 23:55,14/May/10 06:46,14/Mar/19 03:05,28/Jan/10 18:27,0.6.0,,,,,,0.7.0,,,,,0,,,,"
The following script hangs running in local mode  when inpuf files contains many lines (e.g. 10K). The same script works when runing in MR mode.

{code}
A = load 'input1' as (a0, a1, a2);
B = stream A through `head -1` as (a0, a1, a2);
C = load 'input2' as (a0, a1, a2);
D = stream C through `head -1` as (a0, a1, a2);
E = join B by a0, D by a0;
dump E
{code}  

Here is one stack trace:

""Thread-13"" prio=10 tid=0x09938400 nid=0x1232 in Object.wait() [0x8fffe000..0x8ffff030]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x9b8e0a40> (a org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStream)
        at java.lang.Object.wait(Object.java:485)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStream.getNextHelper(POStream.java:291)
        - locked <0x9b8e0a40> (a org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStream)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStream.getNext(POStream.java:214)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:272)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNext(POLocalRearrange.java:256)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POUnion.getNext(POUnion.java:162)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:232)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:227)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:52)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:583)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:176)",,,,,,,,,,,,,,,,,,,27/Jan/10 01:17;rding;PIG-1204.patch;https://issues.apache.org/jira/secure/attachment/12431490/PIG-1204.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-01-27 05:52:00.558,,,no_permission,,,,,,,,,,,,164720,,,,,Thu Jan 28 18:27:55 UTC 2010,,,,,,,0|i0gt6n:,96158,,,,,,,,,,"27/Jan/10 01:17;rding;The cause was a final class variable was modified by another class, and,  in local mode, all the mappers are running in the same JVM that resulted in the dead lock.   

This patch provides a fix.","27/Jan/10 05:52;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12431490/PIG-1204.patch
  against trunk revision 903030.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/190/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/190/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/190/console

This message is automatically generated.","27/Jan/10 22:05;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12431490/PIG-1204.patch
  against trunk revision 903030.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/180/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/180/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/180/console

This message is automatically generated.",28/Jan/10 18:12;pkamath;+1 for commit,28/Jan/10 18:27;rding;Committed patch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[zebra] HDFS meta queries are issued by all mappers; Pig Loader serialize all JobConf contents including those unused by zebra,PIG-1201,12446376,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,yanz,yanz,yanz,22/Jan/10 21:54,24/Mar/10 22:16,14/Mar/19 03:05,03/Feb/10 01:20,0.6.0,,,,,,0.6.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,28/Jan/10 01:43;yanz;PIG-1201.patch;https://issues.apache.org/jira/secure/attachment/12431631/PIG-1201.patch,27/Jan/10 00:32;yanz;PIG-1201.patch;https://issues.apache.org/jira/secure/attachment/12431488/PIG-1201.patch,23/Jan/10 05:33;yanz;PIG-1201.patch;https://issues.apache.org/jira/secure/attachment/12431202/PIG-1201.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-01-23 09:59:17.559,,,no_permission,,,,,,,,,,,,164717,,,,,Thu Feb 18 17:09:12 UTC 2010,,,,,,,0|i0gt5b:,96152,,,,,,,,,,"23/Jan/10 09:59;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12431202/PIG-1201.patch
  against trunk revision 902253.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/188/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/188/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/188/console

This message is automatically generated.","25/Jan/10 08:27;vinodkv;I am not at all familiar with zebra or pig code, but just checked the patch to verify the JobConf issue.
{code}
-			Configuration localConf = new Configuration();
+			Configuration localConf = ConfigurationUtil.toConfiguration(store.getConfiguration());
{code}
{{ConfigurationUtil.toConfiguration()}} still uses a {{new Configuration()}} which will initialize a new object with *all* the default properties from the default resource(*-default.xml)s also populated. So the size of the config object will still be huge, or atleast not small. Instead, I think we should use {{new Configuration(false)}}.

Am I making any sense or are my observations completely invalid?","25/Jan/10 17:18;yanz;The point is to minimize the extra serialized size on disk.  In other words, the ""configMap"" serialized to disk in ""writeObject"" method is the one that caused the problem. localConf is never serialized to disk.

We have tested this change and saw the serialized size on disk drops significantly from around 140K to 1.2K, proving the correctness of the current fix.",25/Jan/10 17:28;yanz;This is a performance/resource fix. Existing tests are good enough for correctness and sanity coverage and no new tests are required.,"26/Jan/10 20:51;yanz;HDFS listStatus calls by every mapper to the name node is costly, particularly if the target has huge number of disk entries, i.e., files and directories. Zebra has the problem in a couple of ways:

1) for unsorted tables,  the index is not built on disk. The input split which is a tfile row split has file index that needs to be mapped to the file name using the index, which contains file names in order and their sizes, by each and every mapper. Building the index makes the listStatus call as it needs info of all files. And if the number of files are huge, this caused name node resource cramps. Instead, the file index can be well replaced with the file name so that the mapping, and consequently the index,  is not needed at all for the routine ops like queries against the tables. For other informational requests like dumpInfo where a comprehensive picture is required, the index could be built as needed. The on-disk index is still preferred as it will save one listStatus call by the front end. But it would require more changes to support backward compatibility and the meta file that holds the index does not support versioning. Consequently, this work is deferred to a future release, although the on-disk index will be built for future convinience;

2) Each BasicTable.Reader, at construction, will check and mark all deleted CGs in the SchemaFile.setCGDeletedFlags method, which makes the listStatus call. This may not be as bad as the one in 1), but for the tables with lots of CGs, it could present a problem. Instead, the check can only be made by a front end and passed to mappers the info.

The huge JobConf serialization size in Pig loader implementation will be fixed by only serializing the few configuration variables that Zebra need. ","27/Jan/10 05:54;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12431488/PIG-1201.patch
  against trunk revision 903030.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/178/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/178/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/178/console

This message is automatically generated.","28/Jan/10 06:37;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12431631/PIG-1201.patch
  against trunk revision 903030.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/181/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/181/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/181/console

This message is automatically generated.",02/Feb/10 23:06;chaow;Patch looks good +1,03/Feb/10 01:20;yanz;Patch committed to 0.6 branch,18/Feb/10 17:09;yanz;Patch committed to the load-store-redesign branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
help includes obsolete options,PIG-1199,12446185,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,21/Jan/10 02:55,17/Dec/10 22:43,14/Mar/19 03:05,05/Aug/10 20:06,0.6.0,,,,,,0.8.0,,,,,0,,,,This is confusing to users,,,,,,,,,,,,,,,,,,,04/Aug/10 00:52;olgan;PIG-1199.patch;https://issues.apache.org/jira/secure/attachment/12451182/PIG-1199.patch,05/Aug/10 20:04;olgan;PIG-1199_2.patch;https://issues.apache.org/jira/secure/attachment/12451361/PIG-1199_2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-01-21 19:43:54.821,,,no_permission,,,,,,,,,,,,164715,,,,,Thu Aug 05 20:06:30 UTC 2010,,,,,,,0|i0gt4f:,96148,"Help now takes properties keyword to show all java properties supported by Pig:

The following properties are supported:
    Logging:
        verbose=true|false; default is false. This property is the same as -v switch
        brief=true|false; default is false. This property is the same as -b switch
        debug=OFF|ERROR|WARN|INFO|DEBUG; default is INFO. This property is the same as -d switch
.......",,,,,,,,,21/Jan/10 19:43;alangates;I think both -cluster/-c and -jar/-j don't work anymore.,"04/Aug/10 00:54;olgan;I have done the following:

- removed dead options
- clarified some other options such as what optimizations can be turned off. Daniel, this information will need to be updated when we switch to the new optimizer.
- added information about properties that we want users to be able to adjust. This information is not displayed by default. To see properties info, you need to type -help properties.","04/Aug/10 22:50;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12451182/PIG-1199.patch
  against trunk revision 981984.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 406 release audit warnings (more than the trunk's current 405 warnings).

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/374/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/374/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/374/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/374/console

This message is automatically generated.","04/Aug/10 22:55;olgan;The patch just changes help message which I tested manually - hence no new tests. Release audit warning is in html file. The tests are failing for unrelated reasons. I ran test commit and I think it should be sufficient for this patch since it is not touching any real code.

","05/Aug/10 00:01;thejas;+1 .
We should change the statement about pig.cachedbag.memusage - ""Note that this memory is shared across all large bags used by the application."" .
 InternalDistinctBag and InternalSortedBag are not aware of the actual number of bags that it needs to share the space with. The constructor argument is passed 3 as the number of bags in all cases ( distinct udf, PODistinct, POSort).
","05/Aug/10 00:11;olgan;Thanks, Thejas. I am going to leave the statement as is until we actually figure out what it should. This is also what our documentation states - so we can update both places at once as needed.

I will commit the patch once I get review from Corinne.","05/Aug/10 20:04;olgan;wording cleanup, thanks Corinne!",05/Aug/10 20:06;olgan;patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TextLoader should be updated to match changes to PigStorage,PIG-1197,12445978,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,alangates,alangates,alangates,19/Jan/10 23:05,24/Mar/10 22:16,14/Mar/19 03:05,25/Jan/10 19:29,0.6.0,,,,,,0.6.0,,impl,,,0,,,,In 0.6 PigStorage was changed to use LineRecordReader to parse lines out of its stream instead of doing the parsing itself.  This resulted in about a 30% speed up in parsing time.  TextLoader should be changed to use LineRecordReader in the same way to benefit from the same speed up.,,,,,,,,,,,,,,,,,,,19/Jan/10 23:16;alangates;PIG-1197.patch;https://issues.apache.org/jira/secure/attachment/12430812/PIG-1197.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-01-20 00:21:31.732,,,no_permission,,,,,,,,,,,,164713,,,,,Mon Jan 25 19:29:55 UTC 2010,,,,,,,0|i0gt3j:,96144,,,,,,,,,,19/Jan/10 23:16;alangates;Patch that changes TextLoader to use LineRecordReader.  No unit tests are included because there are already unit tests for TextLoader in TestBuiltin.,19/Jan/10 23:19;alangates;Initial quick performance tests showed a 25% improvement in performance from the patch.,"20/Jan/10 00:21;dvryaboy;+1 looks good.

Does it need to be changed separately for the load-store redesign branch?","20/Jan/10 09:04;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12430812/PIG-1197.patch
  against trunk revision 901021.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/185/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/185/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/185/console

This message is automatically generated.","20/Jan/10 18:21;alangates;It's already been rewritten for that branch.  I'll check with Pradeep on whether he wants to check this patch in (which will make his merges harder) or just leave it here as a patch for anyone who wants to use it, since hopefully by 0.7 we'll have PIG-966 checked in and this isn't going into 0.6.",20/Jan/10 18:30;pkamath;Alan is right - TextLoader on the load-store-redesign branch already uses TextInputFormat (and hence LineReader) - do committers feel this patch is important enough that it should be committed to trunk? Otherwise I would vote in favor of just keeping it a patch as Alan suggested for people to use since TextLoader probably is not a frequently used Loader (am guessing).,"20/Jan/10 19:08;dvryaboy;I know you guys feel strongly about not adding anything but bug-fixes into 0.6 at this point, but I would love for this to make it in. It's a huge performance boost, and people use TextLoader a lot.

Agreed that it doesn't really need to go into 0.7 if we are hoping to get 966 completed for that release. ","21/Jan/10 00:29;alangates;I'm ok with putting it in 0.6, as it is very localized and it is a significant performance boost.  If I don't hear any complaints over the next couple of days I'll check it in.",25/Jan/10 19:29;alangates;Patch checked into 0.6 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
POSort should take care of sort order,PIG-1195,12445714,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,16/Jan/10 02:12,24/Mar/10 22:16,14/Mar/19 03:05,20/Jan/10 06:23,0.6.0,,,,,,0.6.0,,impl,,,0,,,,"POSort always use ascending order. We shall obey the sort order as specified in the script.

For example, the following script does not do the right thing if we turn off secondary sort (which means, we will rely on POSort to sort):
{code}
A = load 'input' as (a0:int);
B = group A ALL;
C = foreach B {
    D = order A by a0 desc;
    generate D;
};
dump C;
{code}

If we run it using the command line ""java -Xmx512m -Dpig.exec.nosecondarykey=true -jar pig.jar 1.pig"".

The sort order for D is ascending.",,,,,,,,,,,,,,,,,,,16/Jan/10 02:13;daijy;PIG-1195-1.patch;https://issues.apache.org/jira/secure/attachment/12430478/PIG-1195-1.patch,17/Jan/10 03:45;daijy;PIG-1195-2.patch;https://issues.apache.org/jira/secure/attachment/12430532/PIG-1195-2.patch,19/Jan/10 19:42;daijy;PIG-1195-3.patch;https://issues.apache.org/jira/secure/attachment/12430787/PIG-1195-3.patch,20/Jan/10 00:24;daijy;PIG-1195-4.patch;https://issues.apache.org/jira/secure/attachment/12430822/PIG-1195-4.patch,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2010-01-16 13:17:04.507,,,no_permission,,,,,,,,,,,,164711,Reviewed,,,,Wed Jan 20 04:32:38 UTC 2010,,,,,,,0|i0gt2n:,96140,Patch committed to both trunk and 0.6 branch.,,,,,,,,,"16/Jan/10 13:17;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12430478/PIG-1195-1.patch
  against trunk revision 899502.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/179/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/179/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/179/console

This message is automatically generated.",17/Jan/10 03:45;daijy;Change the comparison logic a little bit.,"17/Jan/10 08:08;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12430532/PIG-1195-2.patch
  against trunk revision 899502.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/182/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/182/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/182/console

This message is automatically generated.","19/Jan/10 18:24;alangates;The sorting algorithm in DefaultComparator does not match the sorting algorithm in DefaultTuple.compare.

The algorithm used here first compares the values of each column, and only considers the overall size of the tuples once one tuple has run out of fields.  The algorithm used in DefaultTuple.compare first compares tuple size, then individual column values.  So in this algorithm (5, 3) > (4, 3, 1), but in DefaultTuple's algorithm (5, 3) < (4, 3, 1).  We should use the same algorithm in both places.","19/Jan/10 19:01;daijy;Thanks Alan. Actually after talking with Ying, I realize originally POSort use mComparator to do the sorting. This code is broken in the current code, we should fix this rather than introduce something new.",19/Jan/10 19:52;yinghe;+1,"19/Jan/10 23:59;pkamath;One comment:
- In unit test I would recommend you use Util.createInputFile() method which take the minicluster as an input arg and creates a input file on the cluster. Also delete the file in a finally using Util.deleteFile() - the Util.generateURI() is something which will create problems while merging to load-store-redesign branch.
Otherwise +1",20/Jan/10 00:24;daijy;Use createInputFile instead of generateURI in test as per Pradeep's comment.,20/Jan/10 00:36;pkamath;+1 for commit,"20/Jan/10 04:32;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12430787/PIG-1195-3.patch
  against trunk revision 900926.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/184/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/184/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/184/console

This message is automatically generated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ERROR 2055: Received Error while processing the map plan,PIG-1194,12445698,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,viraj,viraj,15/Jan/10 22:59,14/May/10 06:46,14/Mar/19 03:05,17/Feb/10 18:57,0.5.0,0.6.0,,,,,0.7.0,,impl,,,0,,,,"I have a simple Pig script which takes 3 columns out of which one is null. 
{code}

input = load 'inputdata.txt' using PigStorage() as (col1, col2, col3);
a = GROUP input BY (((double) col3)/((double) col2) > .001 OR col1 < 11 ? col1 : -1);
b = FOREACH a GENERATE group as col1, SUM(input.col2) as col2, SUM(input.col3) as  col3;
store b into 'finalresult';

{code}


When I run this script I get the following error:

ERROR 2055: Received Error while processing the map plan.

org.apache.pig.backend.executionengine.ExecException: ERROR 2055: Received Error while processing the map plan.

        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:277)

        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:240)

        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:93)

        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)

        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)

        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)

================================================================================

A more useful error message for the purpose of debugging would be helpful.

Viraj",,,,,,,,,,,,,,,,,,,22/Jan/10 18:20;rding;PIG-1194.patch;https://issues.apache.org/jira/secure/attachment/12431128/PIG-1194.patch,11/Feb/10 01:24;rding;PIG-1294_1.patch;https://issues.apache.org/jira/secure/attachment/12435531/PIG-1294_1.patch,15/Jan/10 23:00;viraj;inputdata.txt;https://issues.apache.org/jira/secure/attachment/12430454/inputdata.txt,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-01-16 00:55:04.288,,,no_permission,,,,,,,,,,,,164710,,,,,Wed Feb 17 18:57:23 UTC 2010,,,,,,,0|i0gt27:,96138,,,,,,,,,,15/Jan/10 23:00;viraj;Testdata to run with this script,"16/Jan/10 00:55;rding;The problem here is the conditional operator above doesn't handle NULL.
 
For the conditional operator

{code}
condition ?  operand1 :  operand2
{code}
 
What is the correct answer if the _condition_ is NULL? 

The current implementation returns an error.

The proposal is to return a valid result with value NULL.
",20/Jan/10 23:57;rding;Change is made to POLocalRearrange class so it can handle nulls returned by conditional operator (POBinCond).,"22/Jan/10 08:49;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12430952/PIG-1194.patch
  against trunk revision 901900.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/186/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/186/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/186/console

This message is automatically generated.","22/Jan/10 23:13;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12431128/PIG-1194.patch
  against trunk revision 902027.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/187/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/187/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/187/console

This message is automatically generated.","23/Jan/10 08:20;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12431128/PIG-1194.patch
  against trunk revision 902253.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/174/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/174/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/174/console

This message is automatically generated.","28/Jan/10 02:30;ashutoshc;This is related to PIG-1188 . In the patch missing column is assumed to be null. That is if number of elements in tuple in data dont match with schema provided, tuple is padded with null. I am not sure if this is the semantics  that we want to have for situation where data doesnt match with the schema provided. As discussed in PIG-1188 other alternatives could be to drop such tuples altogether or to assume nulls in all the fields of tuple. I think we need to agree on semantics before we go ahead with this patch. ","28/Jan/10 19:22;rding;
This is different from PIG-1188 in that there is no missing field in the input data (only some fields are nulls). The exception was caused by the incorrect handling of null return values of the conditional operator.","28/Jan/10 19:51;ashutoshc;Thanks for the clarification, Richard. +1 for the changes.
Will commit it shortly.","28/Jan/10 20:00;ashutoshc;Patch Committed. Thanks, Richard !","10/Feb/10 20:02;viraj;Hi Richard,
 I ran the script attached on the ticket and found out that the map tasks fails with the following error:

org.apache.pig.backend.executionengine.ExecException: ERROR 2055: Received Error while processing the map plan. at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:281) at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:244) at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:94) at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307) at org.apache.hadoop.mapred.Child.main(Child.java:159) 

I am using the latest pig.jar without hadoop.
Viraj","10/Feb/10 21:23;rding;Hi Viraj,

You're right. I need also consider the case where combiner is used.

Thanks,
-Richard",11/Feb/10 01:24;rding;The new patch deals with the case where combiner is invoked.,"11/Feb/10 07:09;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12435531/PIG-1294_1.patch
  against trunk revision 908324.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/209/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/209/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/209/console

This message is automatically generated.",17/Feb/10 18:25;ashutoshc;+1 for the commit.,17/Feb/10 18:57;rding;The second patch is committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Secondary sort issue on nested desc sort,PIG-1193,12445696,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,15/Jan/10 22:56,24/Mar/10 22:16,14/Mar/19 03:05,20/Jan/10 00:56,0.6.0,,,,,,0.6.0,,impl,,,0,,,,"Secondary sort doing nested desc sort order incorrectly if the following conditions meet:

1. We have sort and UDF in nested plan
2. This UDF will use the same input tuples more than once
3. The input tuples are sorted in desc order

Here is a test case:
{code}
register sequence.jar;
A = load 'input' as (a0:int);
B = group A ALL;
C = foreach B {
    D = order A by a0 desc;
    generate sequence.CUMULATIVE(D,D);
};
dump C;
{code}

input file:
{code}
3
4
{code}

The input for the UDF is:
{code}
({(4),(3)},{(3),(4)})
{code}

The first bag is sorted desc, but the second is not.",,,,,,,,,,,,,,,,,,,15/Jan/10 23:18;daijy;PIG-1193-1.patch;https://issues.apache.org/jira/secure/attachment/12430462/PIG-1193-1.patch,16/Jan/10 07:52;daijy;PIG-1193-2.patch;https://issues.apache.org/jira/secure/attachment/12430494/PIG-1193-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-01-16 04:19:17.492,,,no_permission,,,,,,,,,,,,164709,Reviewed,,,,Wed Jan 20 00:56:20 UTC 2010,,,,,,,0|i0gt1r:,96136,,,,,,,,,,"15/Jan/10 23:01;daijy;Diagnosis for this issue:
{code}
Reduce plan:
Store(fakefile:org.apache.pig.builtin.PigStorage) - 1-56
|
|---New For Each(false)[bag] - 1-55
    |   |
    |   POUserFunc(sequence.CUMULATIVE)[bag] - 1-54
    |   |
    |   |---RelationToExpressionProject[bag][*] - 1-49
    |   |   |
    |   |   |---RelationToExpressionProject[bag][*] - 1-58
    |   |       |
    |   |       |---Project[tuple][1] - 1-46
    |   |
    |   |---RelationToExpressionProject[bag][*] - 1-53
    |       |
    |       |---POSort[bag]() - 1-52
    |           |   |
    |           |   Project[int][0] - 1-51
    |           |
    |           |---Project[tuple][1] - 1-50
    |
    |---Package[tuple]{chararray} - 1-43--------
{code}

We take the first input's reverse POSort and make it a secondary sort key. However, we did not remove the second input's POSort. So the second input for the UDF is reverse reverse sorted.","16/Jan/10 04:19;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12430462/PIG-1193-1.patch
  against trunk revision 899502.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/177/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/177/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/177/console

This message is automatically generated.",16/Jan/10 07:52;daijy;Address findbug warnings,"16/Jan/10 18:27;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12430494/PIG-1193-2.patch
  against trunk revision 899502.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/180/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/180/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/180/console

This message is automatically generated.","19/Jan/10 18:40;alangates;+1, Patch looks good to me.  But Ying should review it before it is committed, since she wrote the original code for this.","19/Jan/10 21:47;daijy;Actually I wrote this part, it is independent with Ying's Accumulator code. ",20/Jan/10 00:47;pkamath;Reviewed the changes +1 for commit,20/Jan/10 00:56;daijy;Patch committed to trunk and 0.6 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"POCast throws exception for certain sequences of LOAD, FILTER, FORACH",PIG-1191,12445605,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,pkamath,ankur,ankur,15/Jan/10 07:46,24/Mar/10 22:16,14/Mar/19 03:05,19/Jan/10 19:41,0.6.0,,,,,,0.6.0,,,,,0,,,,"When using a custom load/store function, one that returns complex data (map of maps, list of maps), for certain sequences  of LOAD, FILTER, FOREACH pig script throws an exception of the form -
 
org.apache.pig.backend.executionengine.ExecException: ERROR 1075: Received a bytearray from the UDF. Cannot determine how to convert the bytearray to <actual-type>
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(POCast.java:639)
...
Looking through the code of POCast, apparently the operator was unable to find the right load function for doing the conversion and consequently bailed out with the exception failing the entire pig script.
",,,,,,,,,,,,,,,,,,,15/Jan/10 07:55;daijy;PIG-1191-1.patch;https://issues.apache.org/jira/secure/attachment/12430376/PIG-1191-1.patch,16/Jan/10 08:25;pkamath;PIG-1191-2.patch;https://issues.apache.org/jira/secure/attachment/12430495/PIG-1191-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-01-15 07:55:08.322,,,no_permission,,,,,,,,,,,,164707,,,,,Wed Jan 20 18:54:46 UTC 2010,,,,,,,0|i0gt13:,96133,,,,,,,,,,"15/Jan/10 07:55;daijy;Hi, Ankur,
Can you check if this patch works?","15/Jan/10 07:56;ankur;Listed below are the identified cases. 

CASE 1: LOAD -> FILTER -> FOREACH -> LIMIT -> STORE
===================================================

SCRIPT
-----------
sds = LOAD '/my/data/location'
      USING my.org.MyMapLoader()
      AS (simpleFields:map[], mapFields:map[], listMapFields:map[]);
queries = FILTER sds BY mapFields#'page_params'#'query' is NOT NULL;
queries_rand = FOREACH queries
               GENERATE (CHARARRAY) (mapFields#'page_params'#'query') AS query_string;
queries_limit = LIMIT queries_rand 100;
STORE queries_limit INTO 'out'; 

RESULT 
------------
FAILS in reduce stage with the following exception

org.apache.pig.backend.executionengine.ExecException: ERROR 1075: Received a bytearray from the UDF. Cannot determine
how to convert the bytearray to string.
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(POCast.java:639)
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:364)
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:288)
        at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.runPipeline(PigMapReduce.java:423)
        at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.processOnePackageOutput(PigMapReduce.java:391)
        at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:371)


CASE 2: LOAD -> FOREACH -> FILTER -> LIMIT -> STORE
===================================================
Note that FILTER and FOREACH order is reversed

SCRIPT
-----------
sds = LOAD '/my/data/location'
      USING my.org.MyMapLoader()
      AS (simpleFields:map[], mapFields:map[], listMapFields:map[]);
queries_rand = FOREACH sds
               GENERATE (CHARARRAY) (mapFields#'page_params'#'query') AS query_string;
queries = FILTER queries_rand BY query_string IS NOT null;
queries_limit = LIMIT queries 100; 
STORE queries_limit INTO 'out';

RESULT
-----------
SUCCESS - Results are correctly stored. So if a projection is done before FILTER it recieves the LoadFunc in the POCast
operator and everything is cool.


CASE 3: LOAD -> FOREACH -> FOREACH -> FILTER -> LIMIT -> STORE
==============================================================

SCRIPT
-----------
ds = LOAD '/my/data/location'
      USING my.org.MyMapLoader()
      AS (simpleFields:map[], mapFields:map[], listMapFields:map[]);
params = FOREACH sds GENERATE 
          (map[]) (mapFields#'page_params') AS params;
queries = FOREACH params
          GENERATE (CHARARRAY) (params#'query') AS query_string;
queries_filtered = FILTER queries
                   BY query_string IS NOT null;
queries_limit = LIMIT queries_filtered 100;
STORE queries_limit INTO 'out';

RESULT
-----------
FAILS in Map stage. Looks like the 2nd FOREACH did not get the loadFunc and bailed out with following stack trace

org.apache.pig.backend.executionengine.ExecException: ERROR 1075: Received a bytearray from the UDF. Cannot determine
how to convert the bytearray to string. at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(POCast.java:639) at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:364)
at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:288)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:260)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter.getNext(POFilter.java:95)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:260)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLimit.getNext(POLimit.java:85) at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:260) at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNext(POLocalRearrange.java:256)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:253) at

CASE 4: LOAD -> FOREACH -> FOREACH -> LIMIT -> STORE
====================================================

SCRIPT
-----------
sds = LOAD '/my/data/location'
      USING my.org.MyMapLoader()
      AS (simpleFields:map[], mapFields:map[], listMapFields:map[]);
params = FOREACH sds GENERATE
          (map[]) (mapFields#'page_params') AS params;
queries = FOREACH params
          GENERATE (CHARARRAY) (params#'query') AS query_string;
queries_limit = LIMIT queries 100;
STORE queries_limit INTO 'out';

RESULT
-----------
SUCCESS. The two FOREACH seem to be getting the loadFunc. 

CASE 5: LOAD -> FOREACH -> FOREACH -> FOREACH -> LIMIT -> STORE
================================================================

SCRIPT
-----------
ds = LOAD '/my/data/location'
      USING my.org.MyMapLoader()
      AS (simpleFields:map[], mapFields:map[], listMapFields:map[]);
params = FOREACH sds GENERATE
          (map[]) (mapFields#'page_params') AS params;
queries = FOREACH params
          GENERATE (CHARARRAY) (params#'query') AS query_string;
rand_queries = FOREACH queries GENERATE query_string as query;
queries_limit = LIMIT rand_queries 100;
STORE rand_queries INTO 'out';

RESULT
-----------
FAILS in map stage. Again the poor second FOREACH seems to be bailing out with stack trace

org.apache.pig.backend.executionengine.ExecException: ERROR 1075: Received a bytearray from the UDF. Cannot determine
how to convert the bytearray to string. at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(POCast.java:639) at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:364)
at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:288)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:260)
at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:237)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:253) 
 ",15/Jan/10 07:58;ankur;I'll check and update the ticket,"15/Jan/10 09:39;ankur;Case 1, 2: Succeeds
Case 3 : Fails
Case 4,5: Empty results. Both of them are using consecutive projection of complex fields.

I'll add 1 more test case","15/Jan/10 10:25;ankur;CASE 6:  In CASE 1 replace LIMIT with a GROUP BY followed by FOREACH 
============================================================

Succeeds with the given patch.
","15/Jan/10 10:48;ankur;Small correct in comment dated - 15/Jan/10 09:39 AM

Case 5: Still FAILS","15/Jan/10 12:18;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12430376/PIG-1191-1.patch
  against trunk revision 899502.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/176/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/176/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/176/console

This message is automatically generated.","16/Jan/10 08:25;pkamath;Attached patch to fix the issues in the script reported. The cause for the issue is that in the parser, getFieldSchema is called on Cast's expression which results in lineage information (canonical map) being updated in the inner map look up operator in the case of two level map look ups. However later the inner plans of foreach get cloned invalidating the previously stored lineage information. In the TypeCheckingVisitor , while visiting LOForeach, the schemas of all operators in the inner plans is supposed to be reset using SchemaRemover so that schemas get re-calculated. However SchemaRemover did not have many of the visit() methods implemented - this patch implements the required methods and fixes the issues reported here.","16/Jan/10 22:44;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12430495/PIG-1191-2.patch
  against trunk revision 899502.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 479 release audit warnings (more than the trunk's current 478 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/181/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/181/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/181/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/181/console

This message is automatically generated.","18/Jan/10 05:53;ankur;Verified that PIG-1191-2.patch successfully passes all 6 test cases with expected results.
So barring the increased number of release audit  warnings,
+1 for commit","19/Jan/10 18:47;alangates;+1, looks good.","19/Jan/10 19:41;alangates;Checked into trunk for Pradeep, since he's out.

As this is significant bug, I intend to apply this to the 0.6 branch as well.",20/Jan/10 18:54;alangates;Checked into 0.6 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handling of quoted strings in pig-latin/grunt commands,PIG-1190,12445557,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ashutoshc,thejas,thejas,14/Jan/10 21:12,14/May/10 06:46,14/Mar/19 03:05,12/Feb/10 02:37,,,,,,,0.7.0,,,,,0,,,,"There is some inconsistency in the way quoted strings are used/handled in pig-latin .
In load/store and define-ship commands, files are specified in quoted strings , and the file name is the content within the quotes.  But in case of register, set, and file system commands , if string is specified in quotes, the quotes are also included as part of the string. This is not only inconsistent , it is also unintuitive. 
This is also inconsistent with the way hdfs commandline (or bash shell) interpret file names.

For example, currently with the command - 
set job.name 'job123'
The job name set set to 'job123' (including the quotes) not job123 .

This needs to be fixed, and above command should be considered equivalent to - set job.name job123. 
",,,,,,,,,,,,,,,,,,,10/Feb/10 23:51;ashutoshc;correct-testcase.patch;https://issues.apache.org/jira/secure/attachment/12435519/correct-testcase.patch,02/Feb/10 00:08;ashutoshc;pig-1190.patch;https://issues.apache.org/jira/secure/attachment/12434468/pig-1190.patch,04/Feb/10 02:00;ashutoshc;pig-1190_1.patch;https://issues.apache.org/jira/secure/attachment/12434772/pig-1190_1.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-01-28 02:17:35.973,,,no_permission,,,,,,,,,,,,164706,,,,,Fri Feb 12 02:37:46 UTC 2010,,,,,,,0|i0gt0n:,96131,,,,,,,,,,"14/Jan/10 21:14;thejas;This breaks backward compatibility, but I don't think thing the use of file names or job names that actually have quotes is likely to be common . For the long run, I think this is the right thing to do.


",28/Jan/10 02:17;olgan;we don't need to fix file system commands because they are obsolete,"02/Feb/10 00:08;ashutoshc;this patch fixes the issue of quotes with register and set commands. Consider following two cases:

set job.priority 'high'
set job.priority high

In both the case job priority will now get set to high. 
Similarly for register command.",02/Feb/10 00:09;ashutoshc;Test cases are included. ,"02/Feb/10 04:31;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12434468/pig-1190.patch
  against trunk revision 905377.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/195/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/195/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/195/console

This message is automatically generated.","03/Feb/10 23:45;olgan;+1. The patch looks good.

One minor comment:

src/org/apache/pig/tools/grunt/GruntParser.java near line 422, we no longer need to compare to quoted and unquoted versions of on/off. It would be nice to clean that up.

Once you update and re-attach the patch, you can go ahead and commit the changes (after running ant test-commit). There is no need to re-reun test-patch since the change is mostly cosmetic.",04/Feb/10 02:00;ashutoshc;Updated patch incorporating Olga's comments. Ran ant test-commit with the patch and build was successful.,04/Feb/10 02:37;ashutoshc;Patch checked-in.,10/Feb/10 23:49;ashutoshc;If we do svn co; ant test; instead of svn co; ant jar; ant test; test introduced in the patch fails because test checks for pig.jar which is built only in jar target not in test target. ,10/Feb/10 23:51;ashutoshc;Changing test cases to use pig-withouthadoop.jar which will always be present while running unit tests. ,12/Feb/10 02:31;daijy;+1 for the new change.,12/Feb/10 02:37;ashutoshc;Patch checked-in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"StoreFunc UDF should ship to the backend automatically without ""register""",PIG-1189,12445549,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,14/Jan/10 19:55,14/May/10 06:46,14/Mar/19 03:05,26/Jan/10 00:34,0.6.0,,,,,,0.7.0,,impl,,,0,,,,"Pig should ship store UDF to backend even if user do not use ""register"". The prerequisite is that UDF should be in classpath on frontend. We make that work for load UDF in (PIG-881|https://issues.apache.org/jira/browse/PIG-881), we shall do the same thing for store UDF.",,,,,,,,,,,,,,,,,,,22/Jan/10 18:07;daijy;PIG-1189-1.patch;https://issues.apache.org/jira/secure/attachment/12431127/PIG-1189-1.patch,25/Jan/10 02:40;daijy;PIG-1189-2.patch;https://issues.apache.org/jira/secure/attachment/12431270/PIG-1189-2.patch,25/Jan/10 07:30;daijy;PIG-1189-3.patch;https://issues.apache.org/jira/secure/attachment/12431289/PIG-1189-3.patch,21/Jan/10 00:12;daijy;multimapstore.pig;https://issues.apache.org/jira/secure/attachment/12430963/multimapstore.pig,21/Jan/10 00:12;daijy;multireducestore.pig;https://issues.apache.org/jira/secure/attachment/12430964/multireducestore.pig,21/Jan/10 00:03;daijy;singlemapstore.pig;https://issues.apache.org/jira/secure/attachment/12430959/singlemapstore.pig,21/Jan/10 00:03;daijy;singlereducestore.pig;https://issues.apache.org/jira/secure/attachment/12430962/singlereducestore.pig,,,,,,,7.0,,,,,,,,,,,,,,,,,,,2010-01-21 20:08:01.586,,,no_permission,,,,,,,,,,,,164705,Reviewed,,,,Tue Jan 26 00:34:39 UTC 2010,,,,,,,0|i0gt07:,96129,,,,,,,,,,"21/Jan/10 00:02;daijy;Unable to write a unit test for that since this issue only happens in real cluster, attach testing script I am using for manual verify. Notice there is no ""register"" command in the testing scripts. To run the script, include the jar in the class path:

java -Xmx512m -cp $HADOOP_CONF_DIR:pig.jar:zebra.jar org.apache.pig.Main xxx.pig","21/Jan/10 00:03;daijy;Unable to write a unit test for that since this issue only happens in real cluster, attach testing script I am using for manual verify. Notice there is no ""register"" command in the testing scripts. To run the script, include the jar in the class path:

java -Xmx512m -cp $HADOOP_CONF_DIR:pig.jar:zebra.jar org.apache.pig.Main xxx.pig","21/Jan/10 00:03;daijy;Unable to write a unit test for that since this issue only happens in real cluster, attach testing script I am using for manual verify. Notice there is no ""register"" command in the testing scripts. To run the script, include the jar in the class path:

java -Xmx512m -cp $HADOOP_CONF_DIR:pig.jar:zebra.jar org.apache.pig.Main xxx.pig",21/Jan/10 20:08;olgan;+ 1 on the code assuming that test-patch does not generate any warnings,"22/Jan/10 09:10;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12430964/multireducestore.pig
  against trunk revision 902027.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/172/console

This message is automatically generated.",22/Jan/10 18:07;daijy;Hudson apply *.pig as the patch. Reattach to wake up hudson with the right patch file.,"22/Jan/10 23:16;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12431127/PIG-1189-1.patch
  against trunk revision 902027.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 215 javac compiler warnings (more than the trunk's current 212 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/173/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/173/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/173/console

This message is automatically generated.","25/Jan/10 02:40;daijy;Attach patch to address unit failures. These failures are because we add storeFunc udf into udf array in MapReduceOper, so the return value for MapReduceOper.name() changes. Instead of fixing Golden file, I change the way we generate POStore in TestMRCompiler because it is inconsistent with the way we generate POLoad. Also I change MapReduceOper.udf from a List to a set which I feel more proper.","25/Jan/10 07:09;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12431270/PIG-1189-2.patch
  against trunk revision 902253.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 9 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 215 javac compiler warnings (more than the trunk's current 212 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 484 release audit warnings (more than the trunk's current 483 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/175/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/175/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/175/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/175/console

This message is automatically generated.",25/Jan/10 07:30;daijy;Address javac warnings.,"25/Jan/10 11:55;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12431289/PIG-1189-3.patch
  against trunk revision 902253.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 9 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 485 release audit warnings (more than the trunk's current 484 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/177/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/177/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/177/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/177/console

This message is automatically generated.",26/Jan/10 00:34;daijy;Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Padding nulls to the input tuple according to input schema,PIG-1188,12445547,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,14/Jan/10 19:32,22/May/12 02:04,14/Mar/19 03:05,09/Mar/11 22:58,0.6.0,,,,,,0.9.0,,impl,,,0,,,,"Currently, the number of fields in the input tuple is determined by the data. When we have schema, we should generate input data according to the schema, and padding nulls if necessary. Here is one example:

Pig script:
{code}
a = load '1.txt' as (a0, a1);
dump a;
{code}
Input file:
{code}
1       2
1       2       3
1
{code}
Current result:
{code}
(1,2)
(1,2,3)
(1)
{code}

Desired result:
{code}
(1,2)
(1,2)
(1, null)
{code}",,,,,,,,,,,,,,,PIG-2661,PIG-1131,,,08/Mar/11 00:02;daijy;PIG-1188-1.patch;https://issues.apache.org/jira/secure/attachment/12472881/PIG-1188-1.patch,09/Mar/11 18:15;daijy;PIG-1188-2.patch;https://issues.apache.org/jira/secure/attachment/12473167/PIG-1188-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-01-14 20:22:15.119,,,no_permission,,,,,,,,,,,,66240,Reviewed,,,,Wed Mar 09 22:58:11 UTC 2011,,,,,,,0|i0gszr:,96127,"If load statement specify schema, Pig will truncate/padding null to make sure the loaded data has exactly the same number of fields specified in load statement.",,,,,,,,,"14/Jan/10 20:22;alangates;I don't think padding is a good idea.  We don't know which field in the record is missing.  We're just guessing that the last field is missing, when in fact it might be the first.  Then we've made the situation worse by inserting invalid data in the all the fields.

I think the loader should either throw the record out, or make all fields in the record null.  This guarantees that we are not further propagating the error.  Then a warning can be issued that the record was invalid (I'm assuming even in the above proposal the loader would issue a warning.) ",14/Jan/10 20:39;daijy;I am fine with throwing this record away and put a warning to the user. The key issue is not to introduce a tuple with less items in it. The follow-up operation depends on the consistency of the tuple size otherwise we will see strange errors which is very hard to diagnose. ,"03/Feb/10 02:06;alangates;After further thought I want to change my position on this.

There are two cases to consider, when schema is present and when it isn't.  The problem is by the time Pig is trying to access the missing field (in the backend), it has no idea whether the schema exists or not.  So at runtime, Pig should just return a null if it gets ArrayOutOfBoundsException.

How to pad missing data should be left up to the load function.  Perhaps certain load functions do know how to pad missing data, or are ok with the pad at the end scheme proposed here.  If the load function does not check, then Pig would effectively pad at the end, given the proposal above.  If the load function implementer does not what this to happen, s/he can check each tuple being read from the input to assure it matches the schema, and then decide to pad the tuple with nulls, reject the tuple, or return a tuple full of nulls.

In the case of PigStorage, checking each tuple for a match against the schema is too expensive.  Ideally I would like it to, because I think that when the user gives a schema it's an error if the data doesn't match.  But I don't want to pay the performance penalty in this case.  ","09/Feb/10 23:18;ashutoshc;I have a different take on this. Referring to original description of Jira, I would expect Pig's behavior should be one given in ""Current result"" and not as given in ""Desired result"". Pig should not try to do anything behind the scenes with data which ""Desired result"" is proposing to do. In cases where columns are not consistent, there are two scenarios with or without schema. If user did supply the schema, then I would consider that user is telling to Pig that data is consistent with the schema he is providing and if thats not the case, its perfectly fine to throw exception at runtime. Tricky case is when schema is not provided and user tries to access a non-existent field. I think even in such cases its valid to throw exception at runtime, instead of returning null. First, if user is trying to access a non-existent field thats an error condition in any case. Second, it can't be assumed that user wants those non-existent field to be treated as null. If he wants it that way, he should implement LoadFunc interface which treats them that way. Third, doing further operations on these columns down the pipeline may result in non-predictable results in other operators. Fourth, returning null will obscure the bugs in Pig where Pig (instead of user himself) tries to access non-existent fields to construct new tuples at run time to do e.g. joins (see PIG-1131). 

In short, I am suggesting that Pig should continue to have a behavior it has today. That is it can load variable number of columns in a tuple. But, if user access a non-existent field throw the exception and let user deal with  such scenario himself by implementing his own LoadFunc interface. 

Thoughts ?","10/Feb/10 00:25;alangates;A few thoughts:

In a job that is going to process a billion rows and run for 3 hours 1 bad row should not cause the whole job to fail.

This invalid access should certainly cause a warning.  Users can look at the warnings at the end of the query and decide they do not want to keep the output because of the warnings.  But failure should not be the default case (see previous point).  Perhaps we should have a warnings = error option like compilers do so users who are very worried about the warnings can make sure they fail.  But that's a different proposal for a different JIRA.

bq. Third, doing further operations on these columns down the pipeline may result in non-predictable results in other operators.

I don't follow.  Nulls in the pipeline shouldn't cause a problem.  UDFs and operators need to be able to handle null values whether they come from processing or from the data itself.

bq. Second, it can't be assumed that user wants those non-existent field to be treated as null. If he wants it that way, he should implement LoadFunc interface which treats them that way.

One could argue that it can't be assumed the user wants his query to fail when a field is missing.  We have to assume one way or another.  Null is a better assumption than failure, since it is possible for a user who doesn't want that behavior to detect it and deal with it.  As it is now, the user has to modify his data or write a new load function to deal with padding his data.

I agree with you that in the schema case, it would be ideal if not having a field was an error.  However, given the architecture this is difficult.  And stipulating that load functions test every record to assure it matches the schema is too much of a performance penalty.  But for the non-schema case I don't agree.  Pig's philsophy of ""Pigs eat anything"" doesn't mean much if Pig gags as soon as it gets a record that doesn't match it's expectation.


","13/Feb/10 01:08;rding;I suggest we don't change the current behavior of Pig regarding the non-confirming input data. Pig already handles invalid access (projection) of non-exist field and return a null as a substitute. Pig does this optimistically, not checking every tuple up front. 

With PIG-1131,  the runtime exception user encountered is also fixed.

","16/Feb/10 18:25;rding;Actually, Pig is already padding nulls to the input tuple according to input schema (with data types):

For example, given Pig script:

{code}
a = load '1.txt' as (a0:int, a1:int);
dump a;
{code}

and input file:

{code}
1       2
1       2       3
1
{code}

The result is

{code}
(1,2)
(1,2)
(1, null)
{code}
","19/Feb/10 20:44;rding;To summarize where we are:

Right now Pig project operator pads null if the value to be projected doesn't exist. As a consequence, the desired result is achieved if  PigStorage is used and a schema with data types is specified, since in this case Pig inserts a project+cast operator for each field in the schema.

In the case where no schema is specified in the load statement, Pig is doing a good job adhering to the Pig's philosophy and  let the program run without throwing runtime exception.

Now leave the case where a schema is specified without data types. There are several options:

   * Pig automatically insert a project operator for each field in the schema to ensure the input data matches the schema. The trade-off for this is the performance penalty. Is it worthwhile if most user data is well-behaved?

   * Users can explicitly add a foreach statement after the load statement which projects all the fields in the schema. This is similar to the practice by the users to run a map job first to cleanup the data.  

   * Pig can also delegate the padding work to the loaders. The problem is that now  the schema isn't passed to the loaders. 



",19/Feb/10 22:29;olgan;Looks like most common cases are already working. Unlinking from 0.7.0 release.,"02/Mar/11 21:11;olgan;The remaining work for Pig 0.9 on this issue is to make sure that the same behavior is implemented for the case where schema is provided during load regardless of whether type information is provided.

We want to implement the current behavior of the typed data for the untyped case.

I believe the way we agreed to do this is by adding foreach regardless of whether type information is available",09/Mar/11 18:15;daijy;PIG-1188-2.patch fix unit test failures.,09/Mar/11 22:10;rding;+1,09/Mar/11 22:58;daijy;Patch committed to trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UTF-8 (international code) breaks with loader when load with schema is specified,PIG-1187,12445472,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ashutoshc,viraj,viraj,14/Jan/10 02:41,14/May/10 06:46,14/Mar/19 03:05,10/Mar/10 03:05,0.6.0,,,,,,0.7.0,,,,,0,,,,"I have a set of Pig statements which dump an international dataset.
{code}
INPUT_OBJECT = load 'internationalcode';
describe INPUT_OBJECT;
dump INPUT_OBJECT;
{code}

Sample output

(756a6196-ebcd-4789-ad2f-175e5df65d55,{(labelAaÂâÀ),(labelあいうえお1),(labelஜார்க2),(labeladfadf)})

It works and dumps results but when I use a schema for loading it fails.

{code}
INPUT_OBJECT = load 'internationalcode' AS (object_id:chararray, labels: bag {T: tuple(label:chararray)});
describe INPUT_OBJECT;
{code}

The error message is as follows:2010-01-14 02:23:27,320 FATAL org.apache.hadoop.mapred.Child: Error running child : org.apache.pig.data.parser.TokenMgrError: Error: Bailing out of infinite loop caused by repeated empty string matches at line 1, column 21.
	at org.apache.pig.data.parser.TextDataParserTokenManager.TokenLexicalActions(TextDataParserTokenManager.java:620)
	at org.apache.pig.data.parser.TextDataParserTokenManager.getNextToken(TextDataParserTokenManager.java:569)
	at org.apache.pig.data.parser.TextDataParser.jj_ntk(TextDataParser.java:651)
	at org.apache.pig.data.parser.TextDataParser.Tuple(TextDataParser.java:152)
	at org.apache.pig.data.parser.TextDataParser.Bag(TextDataParser.java:100)
	at org.apache.pig.data.parser.TextDataParser.Datum(TextDataParser.java:382)
	at org.apache.pig.data.parser.TextDataParser.Parse(TextDataParser.java:42)
	at org.apache.pig.builtin.Utf8StorageConverter.parseFromBytes(Utf8StorageConverter.java:68)
	at org.apache.pig.builtin.Utf8StorageConverter.bytesToBag(Utf8StorageConverter.java:76)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(POCast.java:845)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:250)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:204)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:249)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:240)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.map(PigMapOnly.java:65)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)
	at org.apache.hadoop.mapred.Child.main(Child.java:159)

Viraj",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-01-14 03:27:24.351,,,no_permission,,,,,,,,,,,,164704,,,,,Wed Mar 10 03:05:54 UTC 2010,,,,,,,0|i0gszb:,96125,,,,,,,,,,"14/Jan/10 03:27;zjffdu;Hi Viraj, 
I run your script, and it works OK on my machine.  Maybe it relates with encoding of OS, I install Asia character on my machine.","14/Jan/10 19:28;viraj;Hi Jeff,
 This is specific to the data we are using and it looks like parser failed when it is trying to interpret some characters. As such we have tested this with Chinese characters and it works.
Viraj",25/Jan/10 02:19;olgan;0.6.0 is frozen.,"28/Jan/10 02:13;olgan;one thing that might help here is to set UTF encoding in src/org/apache/pig/data/parser/TextDataParser.jjt. 

Example is in src/org/apache/pig/impl/logicalLayer/parser/QueryParser.jjt 

options {
  // Generate non-static functions
  STATIC = false;
  // Case is ignored in keywords
  IGNORE_CASE = true;
  JAVA_UNICODE_ESCAPE = true;
}
",29/Jan/10 21:12;olgan;Since a simple change did not help - unlinking from 0.7.0 release. This will be addressed as part of parser change,10/Mar/10 03:05;daijy;This is fixed as part of [PIG-613|https://issues.apache.org/jira/browse/PIG-613].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Pig do not take values in ""pig-cluster-hadoop-site.xml""",PIG-1186,12445455,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,14/Jan/10 00:02,24/Mar/10 22:16,14/Mar/19 03:05,15/Jan/10 00:10,0.6.0,,,,,,0.6.0,,impl,,,0,,,,,,,,,,,,,,,,,,,,,,,14/Jan/10 00:29;daijy;PIG-1186-1.patch;https://issues.apache.org/jira/secure/attachment/12430194/PIG-1186-1.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-01-14 05:54:46.802,,,no_permission,,,,,,,,,,,,164703,Reviewed,,,,Fri Jan 15 00:10:37 UTC 2010,,,,,,,0|i0gsyv:,96123,,,,,,,,,,"14/Jan/10 05:54;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12430194/PIG-1186-1.patch
  against trunk revision 898497.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/173/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/173/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/173/console

This message is automatically generated.",14/Jan/10 18:19;daijy;I didn't include unit test because it is very hard to write a unit test for this. I tested it manually and it works.,14/Jan/10 23:58;olgan;+1,15/Jan/10 00:10;daijy;Patch committed to both trunk and 0.6 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Data bags do not close spill files after using iterator to read tuples,PIG-1185,12445364,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yinghe,yinghe,yinghe,13/Jan/10 00:43,24/Mar/10 22:16,14/Mar/19 03:05,14/Jan/10 23:42,,,,,,,0.6.0,,,,,0,,,,"spill files are not closed after reading the tuples from iterator. When large number of spill files exists, this can exceed specified max number of open files on the system and therefore, cause application failure.",,,,,,,,,,,,,,,,,,,13/Jan/10 00:46;yinghe;PIG_1185.patch;https://issues.apache.org/jira/secure/attachment/12430078/PIG_1185.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-01-13 07:31:34.155,,,no_permission,,,,,,,,,,,,164702,Reviewed,,,,Thu Jan 14 23:42:04 UTC 2010,,,,,,,0|i0gsyf:,96121,,,,,,,,,,13/Jan/10 00:46;yinghe;close files are spill file is read out.,"13/Jan/10 00:48;yinghe;this patch doesn't contain any junit test, because I can't verify the files are still used by an application from java, and the name of spill files are not available. I've manually checked the files are not used by any process after iteration is done.

","13/Jan/10 07:31;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12430078/PIG_1185.patch
  against trunk revision 898497.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/172/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/172/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/172/console

This message is automatically generated.",14/Jan/10 00:31;daijy;+1,14/Jan/10 23:42;daijy;Patch committed to both trunk and 0.6 branch. No unit test included because it is a fix to existing features. It is very hard to make a unit test for it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PruneColumns optimization does not handle the case of foreach flatten correctly if flattened bag is not used later,PIG-1184,12445362,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,pkamath,pkamath,12/Jan/10 23:31,14/May/10 06:46,14/Mar/19 03:05,26/Jan/10 00:32,0.6.0,,,,,,0.7.0,,,,,0,,,,"The following script :
{noformat}
-e ""a = load 'input.txt' as (f1:chararray, f2:chararray, f3:bag{t:tuple(id:chararray)}, f4:bag{t:tuple(loc:chararray)}); b = foreach a generate f1, f2, flatten(f3), flatten(f4), 10; b = foreach b generate f1, f2, \$4; dump b;""
{noformat}
gives the following result:
(oiue,M,10)

{noformat}
cat input.txt:
oiue    M       {(3),(4)}       {(toronto),(montreal)}
{noformat}

If PruneColumns optimizations is disabled, we get the right result:
(oiue,M,10)
(oiue,M,10)
(oiue,M,10)
(oiue,M,10)

The flatten results in 4 records - so the output should contain 4 records.",,,,,,,,,,,,,,,,,,,20/Jan/10 19:50;daijy;PIG-1184-1.patch;https://issues.apache.org/jira/secure/attachment/12430919/PIG-1184-1.patch,25/Jan/10 04:34;daijy;PIG-1184-2.patch;https://issues.apache.org/jira/secure/attachment/12431277/PIG-1184-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-01-22 08:00:55.958,,,no_permission,,,,,,,,,,,,164701,Reviewed,,,,Tue Jan 26 00:32:00 UTC 2010,,,,,,,0|i0gsxz:,96119,,,,,,,,,,21/Jan/10 19:07;pkamath;+1,22/Jan/10 08:00;daijy;Patch committed.,"22/Jan/10 08:41;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12430919/PIG-1184-1.patch
  against trunk revision 901900.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/171/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/171/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/171/console

This message is automatically generated.",22/Jan/10 20:24;daijy;Still need to address core test failures.,25/Jan/10 04:34;daijy;Fix unit test failures,"25/Jan/10 08:59;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12431277/PIG-1184-2.patch
  against trunk revision 902253.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/189/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/189/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/189/console

This message is automatically generated.",26/Jan/10 00:32;daijy;Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Piggybank should compile even if we only have ""pig-withouthadoop.jar"" but no ""pig.jar"" in the pig home directory",PIG-1180,12444815,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,06/Jan/10 18:21,24/Mar/10 22:16,14/Mar/19 03:05,06/Jan/10 18:39,0.6.0,,,,,,0.6.0,,impl,,,0,,,,"Piggybank depends on pig.jar to compile. If we build pig using the option ""ant jar-withouthadoop"", we only get pig-withouthadoop.jar. In this case, piggybank should look for pig-withouthadoop.jar. ",,,,,,,,,,,,,,,,,,,06/Jan/10 18:23;daijy;PIG-1180-1.patch;https://issues.apache.org/jira/secure/attachment/12429557/PIG-1180-1.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-01-06 18:38:01.121,,,no_permission,,,,,,,,,,,,164697,Reviewed,,,,Wed Jan 06 18:39:10 UTC 2010,,,,,,,0|i0gswn:,96113,,,,,,,,,,"06/Jan/10 18:23;daijy;pig.jar have higher priority than pig-withouthadoop.jar. If pig.jar exists, use pig.jar first, if not, then look for pig-withouthadoop.jar.",06/Jan/10 18:38;olgan;+1,06/Jan/10 18:39;daijy;This patch is applied to build.xml only. No need for hudson process. Patch committed to trunk and 0.6 branch. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Column Pruner issues in union of loader with and without schema,PIG-1176,12444652,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,05/Jan/10 02:36,14/May/10 06:46,14/Mar/19 03:05,07/Jan/10 18:19,0.6.0,,,,,,0.7.0,,impl,,,0,,,,"Column pruner for union could fail if one source of union have the schema and the other does not have schema. For example, the following script fail:

{code}
a = load '1.txt' as (a0, a1, a2);
b = foreach a generate a0;
c = load '2.txt';
d = foreach c generate $0;
e = union b, d;
dump e;
{code}

However, this issue is in trunk only and is not applicable to 0.6 branch.",,,,,,,,,,,,,,,,,,,05/Jan/10 02:37;daijy;PIG-1176-1.patch;https://issues.apache.org/jira/secure/attachment/12429411/PIG-1176-1.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-01-05 06:58:03.271,,,no_permission,,,,,,,,,,,,164693,Reviewed,,,,Thu Jan 07 18:19:17 UTC 2010,,,,,,,0|i0gsvb:,96107,,,,,,,,,,"05/Jan/10 06:58;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12429411/PIG-1176-1.patch
  against trunk revision 895753.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/165/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/165/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/165/console

This message is automatically generated.","05/Jan/10 11:37;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12429411/PIG-1176-1.patch
  against trunk revision 895874.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/168/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/168/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/168/console

This message is automatically generated.","05/Jan/10 22:48;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12429411/PIG-1176-1.patch
  against trunk revision 896156.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/166/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/166/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/166/console

This message is automatically generated.","06/Jan/10 17:24;daijy;Hudson keeps complaining, however, manual test is successful.",07/Jan/10 00:09;pkamath;+1,07/Jan/10 18:19;daijy;Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig cannot be built without an internet connection,PIG-1173,12444188,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,jmhodges,jmhodges,jmhodges,25/Dec/09 06:09,14/May/10 06:46,14/Mar/19 03:05,05/Jan/10 18:08,,,,,,,0.7.0,,,,,0,,,,"Pig's build.xml does not allow for offline building even when it's been built before. This is because the ivy-download target has not conditional associated with it to turn it off. The Hadoop seems to be adding an unless=""offline"" to the ivy-download target.",,,,,,,,,,,,,,,,,,,28/Dec/09 21:38;jmhodges;offlinebuild-v2.patch;https://issues.apache.org/jira/secure/attachment/12429035/offlinebuild-v2.patch,25/Dec/09 06:10;jmhodges;offlinebuild.patch;https://issues.apache.org/jira/secure/attachment/12428943/offlinebuild.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-12-25 07:03:21.823,,,no_permission,,,,,,,,,,,,164690,Reviewed,,,,Tue Jan 05 18:08:28 UTC 2010,,,,,,,0|i0gstz:,96101,,,,,,,,,,25/Dec/09 06:10;jmhodges;Patch to allow for offline building.,"25/Dec/09 07:03;daijy;Hi, Jeff,
I tried your patch, but I still get this error when I offline, any clue?

[get] Error getting http://repo2.maven.org/maven2/org/apache/ivy/ivy/2.0.0-rc2/ivy-2.0.0-rc2.jar to /home/jianyong/pig/ivy/ivy-2.0.0-rc2.jar

BUILD FAILED
java.net.UnknownHostException: repo2.maven.org","25/Dec/09 10:46;jmhodges;Apologies. I left out the implicit knowledge that ant must have ""-Doffline=true"" passed to it. This also assumes that you have the dependencies already downloaded.","25/Dec/09 11:33;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428943/offlinebuild.patch
  against trunk revision 893827.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/164/console

This message is automatically generated.",28/Dec/09 21:38;jmhodges;Fixed paths in patch. Should now apply with patch -p0 instead of p-level 1.,"05/Jan/10 07:01;daijy;+1, will commit patch shortly.",05/Jan/10 18:08;daijy;Patch committed. Thanks Jeff!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PushDownForeachFlatten shall not push ForEach below Join if the flattened fields is used in Join,PIG-1172,12444178,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,24/Dec/09 22:13,14/Apr/10 00:38,14/Mar/19 03:05,05/Jan/10 03:53,0.6.0,,,,,,0.6.0,,impl,,,0,,,,"Currently the following script will push B below D. But we will use fattened column in the join, we cannot push that.

A = load '1.txt' as (bg:bag{t:tuple(a0,a1)});
B = FOREACH A generate flatten($0);
C = load '3.txt' AS (c0, c1);
D = JOIN B by a1, C by c1;
E = limit D 10;
explain E;
",,,,,,,,,,,,,,,,,,,25/Dec/09 00:07;daijy;PIG-1172-1.patch;https://issues.apache.org/jira/secure/attachment/12428938/PIG-1172-1.patch,04/Jan/10 22:40;daijy;PIG-1172-2.patch;https://issues.apache.org/jira/secure/attachment/12429398/PIG-1172-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-12-25 10:47:19.907,,,no_permission,,,,,,,,,,,,164689,Reviewed,,,,Tue Jan 05 03:53:15 UTC 2010,,,,,,,0|i0gstj:,96099,,,,,,,,,,"25/Dec/09 10:47;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428938/PIG-1172-1.patch
  against trunk revision 893827.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/163/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/163/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/163/console

This message is automatically generated.","04/Jan/10 22:31;alangates;Changes look good, +1.

The patch lists a new hadoop20.jar.  Is this intentional?",04/Jan/10 22:40;daijy;hadoop20.jar should not be in patch. I reattched the patch. Thanks.,05/Jan/10 03:53;daijy;Patch committed to both trunk and 0.6 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Top-N queries produce incorrect results when followed by a cross statement,PIG-1171,12444173,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,24/Dec/09 18:09,14/May/10 06:46,14/Mar/19 03:05,05/Jan/10 19:38,0.6.0,,,,,,0.7.0,,,,,0,,,,"??I am not sure if this is a bug, or something more subtle, but here is the problem that I am having.??

??When I LOAD a dataset, change it with an ORDER, LIMIT it, then CROSS it with itself, the results are not correct. I expect to see the cross of the limited, ordered dataset, but instead I see the cross of the limited dataset. Effectively, its like the LIMIT is being excluded.??

??Example code follows:??

{code}
A = load 'foo' as (f1:int, f2:int, f3:int); B = load 'foo' as (f1:int, f2:int, f3:int);
a = ORDER A BY f1 DESC;
b = ORDER B BY f1 DESC;
aa = LIMIT a 1;
bb = LIMIT b 1;
C = CROSS aa, bb;
DUMP C;
{code}

",,,,,,,,,,,,,,,,,,,24/Dec/09 18:28;rding;PIG-1171.patch;https://issues.apache.org/jira/secure/attachment/12428922/PIG-1171.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-12-25 00:49:37.05,,,no_permission,,,,,,,,,,,,164688,,,,,Tue Jan 05 19:38:17 UTC 2010,,,,,,,0|i0gst3:,96097,,,,,,,,,,24/Dec/09 18:28;rding;This patch provides a fix.,"25/Dec/09 00:49;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428922/PIG-1171.patch
  against trunk revision 893785.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/161/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/161/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/161/console

This message is automatically generated.",05/Jan/10 19:17;olgan;+1; patch looks good. Will be committing it to the trunk shortly,"05/Jan/10 19:38;olgan;patch committed. Thanks, Richard!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Top-N queries produce incorrect results when a store statement is added between order by and limit statement,PIG-1169,12444037,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,22/Dec/09 18:33,14/May/10 06:46,14/Mar/19 03:05,17/Feb/10 23:17,0.7.0,,,,,,0.7.0,,impl,,,0,,,,"??We tried to get top N results after a groupby and sort, and got different results with or without storing the full sorted results. Here is a skeleton of our pig script.??

{code}
raw_data = Load '<input_files>' AS (f1, f2, ..., fn);
grouped = group raw_data by (f1, f2);
data = foreach grouped generate FLATTEN(group). SUM(raw_data.fk) as value;
ordered = order data by value DESC parallel 10;
topn = limit ordered 10;
store ordered into 'outputdir/full';
store topn into 'outputdir/topn';
{code}

??With the statement 'store ordered ...', top N results are incorrect, but without the statement, results are correct. Has anyone seen this before? I know a similar bug has been fixed in the multi-query release. We are on pig .4 and hadoop .20.1.??

",,,,,,,,,,,,,,,,,,,16/Feb/10 23:45;rding;PIG-1169.patch;https://issues.apache.org/jira/secure/attachment/12436055/PIG-1169.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-02-17 04:13:09.5,,,no_permission,,,,,,,,,,,,164686,,,,,Wed Feb 17 23:17:54 UTC 2010,,,,,,,0|i0gss7:,96093,,,,,,,,,,"24/Dec/09 18:07;rding;In this case, a workaroud is to disable the multi-query optimization (use command line option -M).","12/Feb/10 22:31;rding;The cause of this bug is that, because the output of orderby statement is consumed by both store (to 'full') and limit statements, the limit operation ends up in a subsequent MR job. Since data cross MR job boundary isn't order-preserving, the script produces the wrong TopN results.

The proposed solution is to replace the limit operator with an orderby-with-limit operator, i.e. the orderby operation runs twice. This ensures the correctness of the TopN results.

","17/Feb/10 04:13;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12436055/PIG-1169.patch
  against trunk revision 909921.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/206/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/206/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/206/console

This message is automatically generated.",17/Feb/10 19:57;daijy;+1,17/Feb/10 23:17;rding;patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Signature of loader does not set correctly for order by,PIG-1165,12443827,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,19/Dec/09 01:26,24/Mar/10 22:16,14/Mar/19 03:05,22/Dec/09 01:35,0.6.0,,,,,,0.6.0,,impl,,,0,,,,"In pig, we need to set signature for each LoadFunc. Currently, we use alias of the LOAD statement in Pig script of the signature of the LoadFunc. One use case we have is in LoadFunc, we use signature to retrieve pruned columns of each specific loader. However, in ""order by"" statement, we do not set signature for the loader correctly. In this case, we do not prune the loader correctly. 

For example, the following script produce wrong result:

{code}
a = load '1.txt' as (a0, a1);
b = order a by a1;
c = order b by a1;
d = foreach c generate a1;
dump d;
{code}

1.txt:
{code}
1       a
2       b
3       c
6       d
5       e
{code}

expected result:
a
b
c
d
e

current result:
1
2
3
5
6",,,,,,,,,,,,,,,,,,,21/Dec/09 18:35;daijy;PIG-1165-1.patch;https://issues.apache.org/jira/secure/attachment/12428644/PIG-1165-1.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-12-21 23:59:13.821,,,no_permission,,,,,,,,,,,,164682,Reviewed,,,,Tue Dec 22 01:35:14 UTC 2009,,,,,,,0|i0gsqn:,96086,,,,,,,,,,"21/Dec/09 23:59;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428644/PIG-1165-1.patch
  against trunk revision 892939.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/149/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/149/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/149/console

This message is automatically generated.",22/Dec/09 00:26;olgan;+1; changes look good!,22/Dec/09 01:35;daijy;Patch committed to both trunk and 0.6 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
merge join right side table does not support comma seperated paths,PIG-1159,12443603,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,jing1234,jing1234,17/Dec/09 02:03,14/May/10 06:46,14/Mar/19 03:05,21/Dec/09 20:42,0.6.0,,,,,,0.7.0,,,,,0,,,,"For example this is my script:(join_jira1.pig)

register /grid/0/dev/hadoopqa/jars/zebra.jar;

--a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
--a2 = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);

--sort1 = order a1 by a parallel 6;
--sort2 = order a2 by a parallel 5;

--store sort1 into 'asort1' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c,d]');
--store sort2 into 'asort2' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c,d]');
--store sort1 into 'asort3' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c,d]');
--store sort2 into 'asort4' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c,d]');

joinl = LOAD 'asort1,asort2' USING org.apache.hadoop.zebra.pig.TableLoader('a,b,c,d', 'sorted');

joinr = LOAD 'asort3,asort4' USING org.apache.hadoop.zebra.pig.TableLoader('a,b,c,d', 'sorted');


joina = join joinl by a, joinr by a using ""merge"" ;
dump joina;


======
here is the log:
Backend error message
---------------------
java.lang.IllegalArgumentException: Pathname /user/hadoopqa/asort3,hdfs:/gsbl90380.blue.ygrid.yahoo.com/user/hadoopqa/asort4 from hdfs://gsbl90380.blue.ygrid.yahoo.com/user/hadoopqa/asort3,hdfs:/gsbl90380.blue.ygrid.yahoo.com/user/hadoopqa/asort4 is not a valid DFS filename.
        at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:158)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:453)
        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:648)
        at org.apache.pig.backend.hadoop.datastorage.HDataStorage.isContainer(HDataStorage.java:203)
        at org.apache.pig.backend.hadoop.datastorage.HDataStorage.asElement(HDataStorage.java:131)
        at org.apache.pig.backend.hadoop.datastorage.HDataStorage.asElement(HDataStorage.java:147)
        at org.apache.pig.impl.io.FileLocalizer.fullPath(FileLocalizer.java:534)
        at org.apache.pig.impl.io.FileLocalizer.open(FileLocalizer.java:338)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.seekInRightStream(POMergeJoin.java:398)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.getNext(POMergeJoin.java:184)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:253)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:244)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.map(PigMapOnly.java:65)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)
        at org.apache.hadoop.mapred.Child.main(Child.java:159)

Pig Stack Trace
---------------
ERROR 6015: During execution, encountered a Hadoop error.

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias joina
        at org.apache.pig.PigServer.openIterator(PigServer.java:482)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:539)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:241)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:168)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:144)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:89)
        at org.apache.pig.Main.main(Main.java:386)
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 6015: During execution, encountered a Hadoop error.
        at .apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:158)
        at .apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:453)
        at .apache.hadoop.fs.FileSystem.exists(FileSystem.java:648)        at .apache.pig.backend.hadoop.datastorage.HDataStorage.isContainer(HDataStorage.java:203)
        at .apache.pig.backend.hadoop.datastorage.HDataStorage.asElement(HDataStorage.java:131)
        at .apache.pig.backend.hadoop.datastorage.HDataStorage.asElement(HDataStorage.java:147)
        at .apache.pig.impl.io.FileLocalizer.fullPath(FileLocalizer.java:534)
        at .apache.pig.impl.io.FileLocalizer.open(FileLocalizer.java:338)
        at .apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.seekInRightStream(POMergeJoin.java:398)
        at .apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.getNext(POMergeJoin.java:184)
        at .apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:253)
        at .apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:244)
        at .apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.map(PigMapOnly.java:65)
        at .apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
        at .apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)
        at .apache.hadoop.mapred.MapTask.run(MapTask.java:307)
Caused by: java.lang.IllegalArgumentException: Pathname /user/hadoopqa/asort3,hdfs:/gsbl90380.blue.ygrid.yahoo.com/user/hadoopqa/asort4 from hdfs://gsbl90380.blue.ygrid.yahoo.com/user/hadoopqa/asort3,hdfs:/gsbl90380.blue.ygrid.yahoo.com/user/hadoopqa/asort4 is not a valid DFS filename.
        ... 16 more
================================================================================
                                                                                                                                              
",,,,,,,,,,,,,,,,,,,19/Dec/09 00:01;rding;PIG-1159.patch;https://issues.apache.org/jira/secure/attachment/12428500/PIG-1159.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-12-17 02:26:42.91,,,no_permission,,,,,,,,,,,,164677,,,,,Mon Dec 21 20:42:48 UTC 2009,,,,,,,0|i0gsnz:,96074,,,,,,,,,,"17/Dec/09 02:26;daijy;Some initial investigation:
In POMergeJoin.seekInRightStream, we will open a side file using FileLocalizer.open, In the test case, it takes the comma separated file name as the input, and try to open that file. However, FileLocalizer does not handle comma separated files, and it bails out. One solution for that is changing the FileLocalizer.open() to handle comma separated file names, return the input stream which actually iterate through all items in the list.","19/Dec/09 00:01;rding;With this patch, Pig runtime no longer passes an InputStream to IndexableLoader through the bindTo method. An IndexableLoader is resposible to create its own InputStream for reading data. 

This actually isn't a new requirement:  currently all existing IndexableLoaders create their own InputStreams. And, in the future, with the load-store redesign, Pig runtime will no longer create InputStreams for the loaders.    ","19/Dec/09 09:41;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428500/PIG-1159.patch
  against trunk revision 892416.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/145/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/145/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/145/console

This message is automatically generated.","21/Dec/09 20:42;olgan;patch committed. Thanks, Richard",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig command line -M option doesn't support table union correctly (comma seperated paths),PIG-1158,12443601,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,jing1234,jing1234,17/Dec/09 01:55,14/May/10 06:46,14/Mar/19 03:05,21/Dec/09 20:16,0.6.0,,,,,,0.7.0,,,,,0,,,,"for example, load (1.txt,2.txt) USING org.apache.hadoop.zebra.pig.TableLoader()
i see this errror from stand out:
[main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2100: hdfs://gsbl90380.blue.ygrid.yahoo.com/user/hadoopqa/1.txt,2.txt does not exist.
",,,,,,,,,,,,,,,,,,,18/Dec/09 21:09;rding;PIG-1158.patch;https://issues.apache.org/jira/secure/attachment/12428482/PIG-1158.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-12-18 19:13:30.809,,,no_permission,,,,,,,,,,,,164676,,,,,Mon Dec 21 20:16:31 UTC 2009,,,,,,,0|i0gsnj:,96072,,,,,,,,,,"18/Dec/09 19:13;rding;Without -M option Pig converts paths to their absolute locations before passing them to the loaders/storers. With -M option, Pig passes the paths ""as is"" to the loaders/storers. 

This distinction seems to be obsolete. The fix will be to convert paths to their absolute locations in both cases.

","19/Dec/09 05:25;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428482/PIG-1158.patch
  against trunk revision 892408.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/143/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/143/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/143/console

This message is automatically generated.","21/Dec/09 20:16;olgan;patch committed to the trunk. Thanks, Richard",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sucessive replicated joins do not generate Map Reduce plan and fails due to OOM,PIG-1157,12443566,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,viraj,viraj,16/Dec/09 19:27,14/May/10 06:46,14/Mar/19 03:05,18/Dec/09 22:26,0.6.0,,,,,,0.7.0,,impl,,,0,,,,"Hi all,
 I have a script which does 2 replicated joins in succession. Please note that the inputs do not exist on the HDFS.

{code}
A = LOAD '/tmp/abc' USING PigStorage('\u0001') AS (a:long, b, c);
A1 = FOREACH A GENERATE a;
B = GROUP A1 BY a;
C = LOAD '/tmp/xyz' USING PigStorage('\u0001') AS (x:long, y);
D = JOIN C BY x, B BY group USING ""replicated"";
E = JOIN A BY a, D by x USING ""replicated"";
dump E;
{code}

2009-12-16 19:12:00,253 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 4
2009-12-16 19:12:00,254 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - Merged 1 map-only splittees.
2009-12-16 19:12:00,254 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - Merged 1 map-reduce splittees.
2009-12-16 19:12:00,254 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - Merged 2 out of total 2 splittees.
2009-12-16 19:12:00,254 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 2
2009-12-16 19:12:00,713 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2998: Unhandled internal error. unable to create new native thread
Details at logfile: pig_1260990666148.log

Looking at the log file:

Pig Stack Trace
---------------
ERROR 2998: Unhandled internal error. unable to create new native thread

java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:597)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:131)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:265)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:773)
        at org.apache.pig.PigServer.store(PigServer.java:522)
        at org.apache.pig.PigServer.openIterator(PigServer.java:458)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:532)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:190)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:166)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:142)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:89)
        at org.apache.pig.Main.main(Main.java:397)
================================================================================

If we want to look at the explain output, we find that there is no Map Reduce plan that is generated. 

 Why is the M/R plan not generated?


Attaching the script and explain output.
Viraj",,,,,,,,,,,,,,,,,,,18/Dec/09 16:20;rding;PIG-1157.patch;https://issues.apache.org/jira/secure/attachment/12428448/PIG-1157.patch,17/Dec/09 22:34;rding;PIG-1157.patch;https://issues.apache.org/jira/secure/attachment/12428359/PIG-1157.patch,16/Dec/09 20:01;viraj;oomreplicatedjoin.pig;https://issues.apache.org/jira/secure/attachment/12428215/oomreplicatedjoin.pig,16/Dec/09 20:01;viraj;replicatedjoinexplain.log;https://issues.apache.org/jira/secure/attachment/12428214/replicatedjoinexplain.log,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2009-12-17 01:43:36.451,,,no_permission,,,,,,,,,,,,164675,,,,,Fri Dec 18 22:26:21 UTC 2009,,,,,,,0|i0gsnb:,96071,,,,,,,,,,16/Dec/09 20:01;viraj;Explain output and Pig script.,"17/Dec/09 01:43;rding;Two quick observations:

1. The script works if multi-query optimization is disabled (-M).
2. The script also works if using regular join instead of replicated join.

I'll look into it further.","17/Dec/09 18:42;viraj;Hi Richard,

 Thanks for your suggestion, it works.  Additionally we could also use the ""exec"" statement before the alias E to prevent the implicit dependency.

How hard/easy is it for Pig to find out if there is an implicit dependency or not. Pig anyway has a copy of the logical plan in memory, where it knows that alias E requires output from D which is generated in the previous step.

Can we not warn the user about this implicit dependency? 

Viraj



","17/Dec/09 22:34;rding;The problem is that, by merging a MR splittee with a FR join, the MultiQuery optimizer may introduce a direct cycle to the graph of the MR plan. This patch fixed this problem by not merging FR splitees.

This is actually stronger than necessary. A better solution would be to check if merging a MR splittee would form a directed cycle in the original DAG before merging it, and if not, allow the merge to go ahead.","18/Dec/09 13:16;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428359/PIG-1157.patch
  against trunk revision 892125.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/140/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/140/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/140/console

This message is automatically generated.",18/Dec/09 19:17;olgan;+1. Patch looks good. Will commit once the tests pass.,"18/Dec/09 20:42;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428448/PIG-1157.patch
  against trunk revision 892125.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/141/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/141/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/141/console

This message is automatically generated.","18/Dec/09 22:26;olgan;patch committed, thanks Richard",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Need to make sure existing loaders work ""as is""",PIG-1155,12443460,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,olgan,olgan,15/Dec/09 22:48,24/Mar/10 22:16,14/Mar/19 03:05,17/Dec/09 00:24,0.6.0,,,,,,0.6.0,,,,,0,,,,"Need to make sure that existing loaders work ""as is"" without requiring recompile for new fieldsRequired signature. Currently, they fail.",,,,,,,,,,,,,,,,,,,16/Dec/09 01:41;daijy;PIG-1155-1.patch;https://issues.apache.org/jira/secure/attachment/12428115/PIG-1155-1.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-12-16 01:41:18.272,,,no_permission,,,,,,,,,,,,164674,Reviewed,,,,Thu Dec 17 00:24:03 UTC 2009,,,,,,,0|i0gsmf:,96067,,,,,,,,,,16/Dec/09 01:41;daijy;Attach a patch. Don't have way to generate the obsolete class in the code. So skip unit test and do manual test.,16/Dec/09 01:44;olgan;+1 on the code change.,"16/Dec/09 14:54;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428115/PIG-1155-1.patch
  against trunk revision 890596.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/129/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/129/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/129/console

This message is automatically generated.",17/Dec/09 00:24;daijy;Patch committed to both trunk and 0.6 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
local mode fails when hadoop config directory is specified in classpath,PIG-1154,12443433,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ankit.modi,thejas,thejas,15/Dec/09 18:51,14/May/10 06:46,14/Mar/19 03:05,04/Feb/10 00:46,0.7.0,,,,,,0.7.0,,,,,0,,,,"In local mode, the hadoop configuration should not be taken from the classpath . 
",,,,,,,,,,,,,,,,,,,02/Feb/10 23:26;ankit.modi;pig_1154.patch;https://issues.apache.org/jira/secure/attachment/12434613/pig_1154.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-02-02 23:05:21.61,,,no_permission,,,,,,,,,,,,164673,,,,,Thu Feb 04 00:46:39 UTC 2010,,,,,,,0|i0gslz:,96065,,,,,,,,,,"15/Dec/09 18:52;thejas;It is difficult to identify the problem from the exception thrown -

Backend error message during job submission
-------------------------------------------
org.apache.hadoop.util.Shell$ExitCodeException: chmod: cannot access `/mapredsystem/hadoop/mapredsystem/job_local_0001': No such file or directory

        at org.apache.hadoop.util.Shell.runCommand(Shell.java:245)
        at org.apache.hadoop.util.Shell.run(Shell.java:172)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:365)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:451)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:434)
        at org.apache.hadoop.fs.RawLocalFileSystem.execCommand(RawLocalFileSystem.java:481)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:473)
        at org.apache.hadoop.fs.FilterFileSystem.setPermission(FilterFileSystem.java:280)
        at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:261)
        at org.apache.hadoop.mapred.JobClient.configureCommandLineOptions(JobClient.java:573)
        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:761)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:730)
        at org.apache.hadoop.mapred.jobcontrol.Job.submit(Job.java:378)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs(JobControl.java:247)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.run(JobControl.java:279)
        at java.lang.Thread.run(Thread.java:619)

Error before Pig is launched
----------------------------
ERROR 1951: Caught exception

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1951: Caught exception
        at org.apache.pig.PigServer.genStorePlanAndExecute(PigServer.java:594)
        at org.apache.pig.PigServer.openInputStream(PigServer.java:511)
        at org.apache.pig.impl.logicalLayer.SQLParser.QueryProcessor.executeAndWriteToSTDOUT(QueryProcessor.java:251)
        at org.apache.pig.impl.logicalLayer.SQLParser.QueryProcessor.execute(QueryProcessor.java:203)
        at org.apache.pig.SQLDriver.execute(SQLDriver.java:56)
        at org.apache.pig.Main.processSQL(Main.java:465)
        at org.apache.pig.Main.main(Main.java:400)
Caused by: java.io.IOException: Job terminated with anomalous status FAILED
        at org.apache.pig.PigServer.genStorePlanAndExecute(PigServer.java:584)
        ... 6 more
================================================================================","02/Feb/10 23:05;ankit.modi;It looks like the problem is caused by overwritten value of mapred.system.dir from mapred-default.xml and the path mentioned above ""/mapredsystem/hadoop/mapredsystem/"" may not exist.

This cannot be solved in local mode as it is not possible to change classpath at runtime.

I'll provide a patch which would
   * Provide a warning whenever classpath contains mapred-site.xml or hdfs-site.xml.
   * It'll exit pig with an error message if above case is encountered.",02/Feb/10 23:16;alangates;I'm assuming you mean it will provide a warning and error out when the user is in local mode and mapred-site.xml or hdfs-site.xml are found in the classpath?,"02/Feb/10 23:22;ankit.modi;It will provide warning whenever the files are encountered in Local Mode.

On top of that it will exit with error if mapred.system.dir is different from the default one and it does not exist.",02/Feb/10 23:26;ankit.modi;Patch according to comments mentioned above.,02/Feb/10 23:28;ankit.modi;This patch only affects only Local Mode in pig.,"03/Feb/10 05:59;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12434613/pig_1154.patch
  against trunk revision 905377.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/188/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/188/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/188/console

This message is automatically generated.",04/Feb/10 00:46;alangates;Patch checked in.  Thanks Ankit.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[zebra] spliting columns at different levels in a complex record column into different column groups throws exception,PIG-1153,12443429,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yanz,xuefuz,xuefuz,15/Dec/09 18:07,14/May/10 06:46,14/Mar/19 03:05,23/Dec/09 01:22,0.7.0,,,,,,0.7.0,,impl,,,0,,,,"The following code sample:

      String strSch = ""r1:record(f1:int, f2:int), r2:record(f5:int, r3:record(f3:float, f4))"";
      String strStorage = ""[r1.f1, r2.r3.f3, r2.f5]; [r1.f2, r2.r3.f4]"";
      Partition p = new Partition(schema.toString(), strStorage, null);

gives the following exception:

    org.apache.hadoop.zebra.parser.ParseException: Different Split Types Set on the same field: r2.f5",,,,,,,,,,,,,,,,,,,22/Dec/09 01:37;yanz;PIG-1153.patch;https://issues.apache.org/jira/secure/attachment/12428683/PIG-1153.patch,19/Dec/09 01:28;yanz;PIG-1153.patch;https://issues.apache.org/jira/secure/attachment/12428513/PIG-1153.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-12-19 18:30:01.784,,,no_permission,,,,,,,,,,,,164672,,,,,Wed Dec 23 01:22:17 UTC 2009,,,,,,,0|i0gslj:,96063,,,,,,,,,,"19/Dec/09 18:30;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428513/PIG-1153.patch
  against trunk revision 892416.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/147/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/147/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/147/console

This message is automatically generated.",20/Dec/09 04:54;yanz;the single test failure appearts to be a test env issue; resubmitting now.,"20/Dec/09 09:08;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428513/PIG-1153.patch
  against trunk revision 892416.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/148/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/148/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/148/console

This message is automatically generated.","22/Dec/09 06:04;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428683/PIG-1153.patch
  against trunk revision 893053.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 5 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/150/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/150/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/150/console

This message is automatically generated.",22/Dec/09 21:40;chaow;Patch reviewed. +1,23/Dec/09 01:22;yanz;Committed to Apache trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bincond operator throws parser error,PIG-1152,12443336,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,ankur,ankur,15/Dec/09 06:11,04/Aug/11 00:34,14/Mar/19 03:05,15/Mar/11 16:19,0.6.0,,,,,,0.9.0,,,,,0,,,,"Bincond operator throws parser error when true condition contains a constant bag with 1 tuple containing a single field of int type with -ve value. 

Here is the script to reproduce the issue

A = load 'A' as (s: chararray, x: int, y: int);
B = group A by s;
C = foreach B generate group, flatten(((COUNT(A) < 1L) ? {(-1)} : A.x));
dump C;
",,,,,,,,,,,,,,,,,PIG-1618,,14/Mar/11 22:29;xuefuz;PIG-1152-2.patch;https://issues.apache.org/jira/secure/attachment/12473622/PIG-1152-2.patch,14/Mar/11 23:23;xuefuz;PIG-1152-3.patch;https://issues.apache.org/jira/secure/attachment/12473635/PIG-1152-3.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-03-10 00:43:03.859,,,no_permission,,,,,,,,,,,,66254,,,,,Tue Mar 15 16:18:54 UTC 2011,,,,,,,0|i0gsl3:,96061,,,,,,,,,,"10/Mar/11 00:43;xuefuz;Actually the problem is not caused by either bincond operator or parser, but by pig's grammar limitation. Data fields in a literal bag can only have literals. By Pig's grammar, -1 is not a literal, but an expression.

Thus, parser is happy with C = foreach B generate group, flatten(((COUNT(A) < 1L) ? {(1)} : A.x));.

To solve the problem, Pig grammar needs to be extended.","14/Mar/11 19:01;xuefuz;Test patch run:

     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 4 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
",14/Mar/11 22:29;xuefuz;Add support for negative numerical constants.,14/Mar/11 23:23;xuefuz;Added a more test case.,"14/Mar/11 23:25;xuefuz;Test-patch run result (new javacc warning is caused by generated code) :

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     -1 javac.  The applied patch generated 876 javac compiler warnings (more than the trunk's current 869 warnings).
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
",15/Mar/11 00:15;thejas;+1 for PIG-1152.3.patch,15/Mar/11 16:12;xuefuz;unit test passed also.,"15/Mar/11 16:18;thejas;Patch committed to trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow instantiation of SampleLoaders with parametrized LoadFuncs,PIG-1149,12443300,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,dvryaboy,dvryaboy,dvryaboy,14/Dec/09 20:51,14/May/10 06:46,14/Mar/19 03:05,21/Dec/09 20:06,,,,,,,0.7.0,,,,,0,,,,"Currently, it is not possible to instantiate a SampleLoader with something like PigStorage(':').  We should allow passing parameters to the loaders being sampled.",,,,,,,,,,,,,,,,,,,15/Dec/09 05:15;dvryaboy;pig_1149.patch;https://issues.apache.org/jira/secure/attachment/12428005/pig_1149.patch,19/Dec/09 05:01;dvryaboy;pig_1149_lsr-branch.patch;https://issues.apache.org/jira/secure/attachment/12428526/pig_1149_lsr-branch.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-12-16 05:53:38.18,,,no_permission,,,,,,,,,,,,41725,Reviewed,,,,Mon Dec 21 20:06:08 UTC 2009,,,,,,,0|i0gsjz:,96056,,,,,,,,,,"15/Dec/09 05:18;dvryaboy;Due to the string being parsed a few times along the way, three backslashes need to precede the escaped quote in PigLatin. Which means six backslashes when expressing PigLatin as a string in Java. ","16/Dec/09 05:53;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428005/pig_1149.patch
  against trunk revision 890596.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/126/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/126/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/126/console

This message is automatically generated.","18/Dec/09 18:42;alangates;Changes look fine.

Pradeep, will this apply as is to the load-store redesign branch or will we need a separate patch for that?","18/Dec/09 18:56;pkamath;I tried applying it on the branch and it failed:
{noformat}
:/tmp/load-store-redesign]patch -p0 < /homes/pradeepk/dev/pig-apache/pig/trunk/pig_1149.patch 
patching file src/org/apache/pig/impl/builtin/SampleLoader.java
Hunk #1 succeeded at 31 with fuzz 2 (offset -4 lines).
Hunk #2 FAILED at 46.
1 out of 2 hunks FAILED -- saving rejects to file src/org/apache/pig/impl/builtin/SampleLoader.java.rej
patching file test/org/apache/pig/test/TestPoissonSampleLoader.java
[pradeepk@chargesize:/tmp/load-store-redesign]
{noformat}

Since Thejas worked on PIG-1062, he might be in a better position to check whether this patch needs changes.","18/Dec/09 20:25;thejas;I notice that this patch is using org.mortbay.log instead of org.apache.commons.logging. That is not used anywhere else in pig code. Should we replace that with org.apache.commons.logging ?

A small change is required to get the patch working with load-store branch. It no longer requires the load func to implement SampleLoader interface, and that interface has been removed. I can submit the modified patch. 
{code}
+        loader = (SamplableLoader)PigContext.instantiateFuncFromSpec(funcSpec);
{code}
changes to 
+        loader = (LoadFunc)PigContext.instantiateFuncFromSpec(funcSpec);
{code}
","18/Dec/09 20:50;dvryaboy;The log reference and the log.info call can be completely removed (I can post a new patch, or you can just remove the change when integrating the patch).  That's Eclipse being ""helpful""...

I already made a patch for LSR branch, but TestPoissonSampleLoader fails on both tests. The not sure why the first one fails, but in the second one (the one I added), the problem is that when I fetch the first result, I get an unexpected number of fields -- the tuple looks like this:

(100,apple1,aaa0,䥖㠸_pig_inTeRnal-spEcial_roW_num_tuple3kt579CFLehkblah,300L)

I don't recall -- does the special stuff get inserted into every row, or just the last one? What should I ""properly"" expect?","18/Dec/09 22:57;thejas;The first test case failure is known, I will be fixing that with a patch in PIG-1094. 

The special string gets added to the last row only. But that looks unnecessary. I will be removing that with a new patch in PIG-1062. 

You can submit your patch for LSR branch patch, by checking for 5 columns in your test case. I will change your new test case as well when I submit  new PIG-1062 patch (to check for 4 columns).
","18/Dec/09 23:43;thejas;I spoke too soon about the special string being unnecessary. GetMemNumRows uses it. I will add some comments to document that in PoissonSampleLoader .
In previous comment,  ""special string gets added to the last row only""  should be ""special string gets added to the last *sample* row only"".
","19/Dec/09 05:01;dvryaboy;Attaching patch for lsr branch.
I also retabbed the involved files to replace tabs with spaces, and got rid of some unused imports.

Note the FIXME in the test case, as discussed.","21/Dec/09 17:20;thejas;+1 to the lsr branch version.
But the FIXME comment in the test case is not correct. There does not have to be > 1 samples sampled for every map, if the number of rows are very small. Though this behavior is different from earlier version of the trunk version of poisson sampler, it satisfies the requirements as per http://wiki.apache.org/pig/PigSampler and PIG-1062.
I can remove the FIXME comment as part of the patch I am going to submit to fix the other test case.
",21/Dec/09 19:53;olgan;patch pig_1149.patch is committed to the trunk.,"21/Dec/09 20:06;pkamath;Patch for lsr branch also committed, thanks Dmitriy!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent column pruning in LOUnion,PIG-1146,12443047,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,10/Dec/09 23:21,14/May/10 06:46,14/Mar/19 03:05,25/Dec/09 00:28,0.6.0,,,,,,0.7.0,,impl,,,0,,,,"This happens when we do a union on two relations, if one column comes from a loader, the other matching column comes from a constant, and this column get pruned. We prune for the one from loader and did not prune the constant. Thus leaves union an inconsistent state. Here is a script:

{code}
a = load '1.txt' as (a0, a1:chararray, a2);
b = load '2.txt' as (b0, b2);
c = foreach b generate b0, 'hello', b2;
d = union a, c;
e = foreach d generate $0, $2;
dump e;
{code}

1.txt: 
{code}
ulysses thompson        64      1.90
katie carson    25      3.65
{code}

2.txt:
{code}
luke king       0.73
holly davidson  2.43
{code}

expected output:
(ulysses thompson,1.90)
(katie carson,3.65)
(luke king,0.73)
(holly davidson,2.43)

real output:
(ulysses thompson,)
(katie carson,)
(luke king,0.73)
(holly davidson,2.43)",,,,,,,,,,,,,,,,,,,19/Dec/09 01:11;daijy;PIG-1146-1.patch;https://issues.apache.org/jira/secure/attachment/12428510/PIG-1146-1.patch,23/Dec/09 19:51;daijy;PIG-1146-2.patch;https://issues.apache.org/jira/secure/attachment/12428868/PIG-1146-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-12-19 13:49:01.1,,,no_permission,,,,,,,,,,,,164668,Reviewed,,,,Fri Dec 25 00:28:26 UTC 2009,,,,,,,0|i0gsin:,96050,,,,,,,,,,"19/Dec/09 13:49;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428510/PIG-1146-1.patch
  against trunk revision 892416.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/146/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/146/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/146/console

This message is automatically generated.","23/Dec/09 19:51;daijy;Address couple of suggestions from Pradeep in ColumnPruner:
1. Clear some code for LOUnion handling
2. Remove the code to merge cached ""pruned columns"" structure for each logical operator
3. Simplify the logic which require all relevant fields to be pruned before pruning",23/Dec/09 21:49;pkamath;+1,"24/Dec/09 11:10;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428868/PIG-1146-2.patch
  against trunk revision 893660.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/158/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/158/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/158/console

This message is automatically generated.",25/Dec/09 00:28;daijy;Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[zebra] merge join on large table ( 100,000.000 rows zebra table) failed",PIG-1145,12442942,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yanz,jing1234,jing1234,10/Dec/09 01:35,24/Mar/10 22:16,14/Mar/19 03:05,11/Dec/09 18:29,0.6.0,0.7.0,,,,,0.6.0,0.7.0,,,,0,,,,"Pig script :
register $zebraJar;
--fs -rmr $outputDir


a1 = LOAD '$inputDir/unsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2');
a2 = LOAD '$inputDir/unsorted2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2');

sort1 = order a1 by str2;
sort2 = order a2 by str2;

--store sort1 into '$outputDir/sorted11' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2]');
--store sort2 into '$outputDir/sorted21' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2]');

rec1 = load '$outputDir/sorted11' using org.apache.hadoop.zebra.pig.TableLoader();
rec2 = load '$outputDir/sorted21' using org.apache.hadoop.zebra.pig.TableLoader();

joina = join rec1 by str2, rec2 by str2 using ""merge"" ;

--E = foreach joina  generate $0 as count,  $1 as seed,  $2 as int1,  $3 as str2;


store joina into '$outputDir/join1' using org.apache.hadoop.zebra.pig.TableStorer('');
~                                                                                                                                                               
~                                                                                                                                                               
~                                                  
======
stacktrace:
org.apache.pig.backend.executionengine.ExecException: ERROR 2176: Error processing right input during merge join at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.throwProcessingException(POMergeJoin.java:453) at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.getNextRightInp(POMergeJoin.java:443) at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.getNext(POMergeJoin.java:337) at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:253) at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.close(PigMapBase.java:107) at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307) at org.apache.hadoop.mapred.Child.main(Child.java:159) Caused by: java.io.EOFException: No key-value to read at org.apache.hadoop.zebra.tfile.TFile$Reader$Scanner.checkKey(TFile.java:1590) at org.apache.hadoop.zebra.tfile.TFile$Reader$Scanner.entry(TFile.java:1611) at org.apache.hadoop.zebra.io.ColumnGroup$Reader$TFileScanner.getKey(ColumnGroup.java:854) at org.apache.hadoop.zebra.io.ColumnGroup$Reader$CGScanner.getCGKey(ColumnGroup.java:1035) at org.apache.hadoop.zebra.io.BasicTable$Reader$BTScanner.getKey(BasicTable.java:1082) at org.apache.hadoop.zebra.mapred.TableRecordReader.next(TableRecordReader.java:105) at org.apache.hadoop.zebra.pig.TableLoader.getNext(TableLoader.java:414) at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.getNextRightInp(POMergeJoin.java:415) ... 7 more 

",,,,,,,,,,,,,,,,,,,11/Dec/09 09:59;yanz;PIG-1145.patch;https://issues.apache.org/jira/secure/attachment/12427696/PIG-1145.patch,11/Dec/09 04:37;yanz;PIG-1145.patch;https://issues.apache.org/jira/secure/attachment/12427674/PIG-1145.patch,10/Dec/09 02:37;yanz;PIG-1145.patch;https://issues.apache.org/jira/secure/attachment/12427556/PIG-1145.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-12-10 04:41:39.434,,,no_permission,,,,,,,,,,,,164667,,,,,Fri Dec 11 18:29:30 UTC 2009,,,,,,,0|i0gsi7:,96048,,,,,,,,,,10/Dec/09 04:41;chaow;Patch reviewed +1.,"10/Dec/09 05:43;yanz;The problem is that if the seek call on the index table is on a key that is past the last key of a data file, the scanner is positioned past the EOF of that data file. Instead it should be positioned to the beginning of the next data file. As result, since the CGScanner.atEnd method only checks if the current file index is within the valid range and leaves the responsibility of setting the proper file index to the position movers such as the scanner's advance and seekTo methods, positioning the scanner past the EOF of any data file will cause an EOF to be thrown.

The fix is to add a check in the scanner's seekTo method so that if after seek the position is past the end of a data file, it will be positioned to the start of the next data file, just as the advace method already does.","10/Dec/09 09:06;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427556/PIG-1145.patch
  against trunk revision 888852.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/111/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/111/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/111/console

This message is automatically generated.","10/Dec/09 21:48;jing1234;I verified fix. 
It works.","11/Dec/09 09:21;jing1234;found another failure on merge join
This merge join script failed:
register $zebraJar;
--fs -rmr $outputDir


--a1 = LOAD '$inputDir/unsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,byte2');
--a2 = LOAD '$inputDir/unsorted2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,byte2');

--sort1 = order a1 by byte2;
--sort2 = order a2 by byte2;

--store sort1 into '$outputDir/100Msortedbyte21' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2];[byte2]');
--store sort2 into '$outputDir/100Msortedbyte22' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2];[byte2]');

rec1 = load '$outputDir/100Msortedbyte21' using org.apache.hadoop.zebra.pig.TableLoader('','sorted');
rec2 = load '$outputDir/100Msortedbyte22' using org.apache.hadoop.zebra.pig.TableLoader('','sorted');

joina = join rec1 by byte2, rec2 by byte2 using ""merge"" ;

E = foreach joina  generate $0 as count,  $1 as seed,  $2 as int1,  $3 as str2, $4 as byte2;

store E into '$outputDir/bad1' using org.apache.hadoop.zebra.pig.TableStorer('');
=========
instead, this similiar script works with the previous patch:
register $zebraJar;
--fs -rmr $outputDir


a1 = LOAD '$inputDir/unsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,byte2');
a2 = LOAD '$inputDir/unsorted2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,byte2');

sort1 = order a1 by byte2;
sort2 = order a2 by byte2;

store sort1 into '$outputDir/100Msortedbyte21' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,byte2]');
store sort2 into '$outputDir/100Msortedbyte22' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,byte2]');

rec1 = load '$outputDir/100Msortedbyte21' using org.apache.hadoop.zebra.pig.TableLoader('','sorted');
rec2 = load '$outputDir/100Msortedbyte22' using org.apache.hadoop.zebra.pig.TableLoader('','sorted');

joina = join rec1 by byte2, rec2 by byte2 using ""merge"" ;

E = foreach joina  generate $0 as count,  $1 as seed,  $2 as int1,  $3 as str2, $4 as byte2;

store E into '$outputDir/join3' using org.apache.hadoop.zebra.pig.TableStorer('');
~         
================
Here is stack trace:
Backend error message
---------------------
org.apache.pig.backend.executionengine.ExecException: ERROR 2176: Error processing right input during merge join
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.throwProcessingException(POMergeJoin.java:453)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.getNextRightInp(POMergeJoin.java:443)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.getNext(POMergeJoin.java:337)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:260)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:237)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:253)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.close(PigMapBase.java:107)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)
        at org.apache.hadoop.mapred.Child.main(Child.java:159)
Caused by: java.io.EOFException: No key-value to read
        at org.apache.hadoop.zebra.tfile.TFile$Reader$Scanner.checkKey(TFile.java:1590)
        at org.apache.hadoop.zebra.tfile.TFile$Reader$Scanner.entry(TFile.java:1611)
        at org.apache.hadoop.zebra.io.ColumnGroup$Reader$TFileScanner.getKey(ColumnGroup.java:854)
        at org.apache.hadoop.zebra.io.ColumnGroup$Reader$CGScanner.getCGKey(ColumnGroup.java:1035)
        at org.apache.hadoop.zebra.io.BasicTable$Reader$BTScanner.getKey(BasicTable.java:1083)
        at org.apache.hadoop.zebra.mapred.TableRecordReader.next(TableRecordReader.java:105)
        at org.apache.hadoop.zebra.pig.TableLoader.getNext(TableLoader.java:414)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.getNextRightInp(POMergeJoin.java:415)
        ... 9 more
=============
This is how I run it (i disabled pruning to simply the possible problem)
java -cp /grid/0/dev/hadoopqa/jing1234/conf:/grid/0/dev/hadoopqa/jars/pig.jar:/grid/0/dev/hadoopqa/jars/tfile.jar:/grid/0/dev/hadoopqa/jars/zebra.jar org.apache.pig.Main -m config -M -t PruneColumns bad_join.pig 

","11/Dec/09 09:48;yanz;Actually with pruning enabled the exception stack is:

Backend error message
---------------------
org.apache.pig.backend.executionengine.ExecException: ERROR 2176: Error processing right input during merge join
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.throwProcessingException(POMergeJoin.java:453)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.getNext(POMergeJoin.java:186)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:260)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:237)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:253)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:244)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.map(PigMapOnly.java:65)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)
        at org.apache.hadoop.mapred.Child.main(Child.java:159)
Caused by: java.io.IOException: seekTo() failed: Column Groups are not evenly positioned.
        at org.apache.hadoop.zebra.io.BasicTable$Reader$BTScanner.seekTo(BasicTable.java:1148)
        at org.apache.hadoop.zebra.mapred.TableRecordReader.seekTo(TableRecordReader.java:120)
        at org.apache.hadoop.zebra.pig.TableLoader.seekNear(TableLoader.java:190)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.seekInRightStream(POMergeJoin.java:406)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.getNext(POMergeJoin.java:184)
        ... 9 more
","11/Dec/09 16:35;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427696/PIG-1145.patch
  against trunk revision 889346.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/116/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/116/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/116/console

This message is automatically generated.","11/Dec/09 17:33;yanz;All failed test cases are PIG tests and look like enviromental. I reran the first failed test, TestJoin, in my local cluser, and it passes cleanly.",11/Dec/09 18:06;chaow;The patch looks good +1.,11/Dec/09 18:29;yanz;Committed to Apache trunk and 0.6 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
set default_parallelism construct does not set the number of reducers correctly,PIG-1144,12442936,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,viraj,viraj,10/Dec/09 00:59,24/Mar/10 22:16,14/Mar/19 03:05,18/Dec/09 04:08,0.6.0,,,,,,0.6.0,,impl,,,0,,,,"Hi all,
 I have a Pig script where I set the parallelism using the following set construct: ""set default_parallel 100"" . I modified the ""MRPrinter.java"" to printout the parallelism
{code}
...
public void visitMROp(MapReduceOper mr)
mStream.println(""MapReduce node "" + mr.getOperatorKey().toString() + "" Parallelism "" + mr.getRequestedParallelism());
...
{code}

When I run an explain on the script, I see that the last job which does the actual sort, runs as a single reducer job. This can be corrected, by adding the PARALLEL keyword in front of the ORDER BY.

Attaching the script and the explain output

Viraj",Hadoop 20 cluster with multi-node installation,,,,,,,,,,,,,,,,,,10/Dec/09 03:00;daijy;PIG-1144-1.patch;https://issues.apache.org/jira/secure/attachment/12427558/PIG-1144-1.patch,11/Dec/09 02:06;daijy;PIG-1144-2.patch;https://issues.apache.org/jira/secure/attachment/12427670/PIG-1144-2.patch,11/Dec/09 23:21;daijy;PIG-1144-3.patch;https://issues.apache.org/jira/secure/attachment/12427792/PIG-1144-3.patch,17/Dec/09 18:50;daijy;PIG-1144-4.patch;https://issues.apache.org/jira/secure/attachment/12428331/PIG-1144-4.patch,10/Dec/09 01:15;viraj;brokenparallel.out;https://issues.apache.org/jira/secure/attachment/12427550/brokenparallel.out,10/Dec/09 01:15;viraj;genericscript_broken_parallel.pig;https://issues.apache.org/jira/secure/attachment/12427549/genericscript_broken_parallel.pig,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2009-12-10 01:20:53.179,,,no_permission,,,,,,,,,,,,164666,Reviewed,,,,Fri Dec 18 04:08:08 UTC 2009,,,,,,,0|i0gshr:,96046,,,,,,,,,,10/Dec/09 01:15;viraj;Script and explain output,"10/Dec/09 01:20;daijy;Hi, Viraj,
default parallelism is set in JobControlCompiler, which is after MRCompiler. Also if you just do explain, this code will not be invoked. Did you see in the real cluster, it actually use 1 reducer?","10/Dec/09 01:25;viraj;This happens on the real cluster, where the sorting job did not complete because of a single reducer. ","10/Dec/09 01:32;viraj;Hi Daniel,
One more thing to note is that the Last Sort M/R job has a parallelism of 1. Should it not be -1?
Viraj","10/Dec/09 01:54;daijy;I find the root cause of the problem. For every sort job, we hard code parallelism as 1 if user do not use PARALLEL keyword. We shall leave parallelism as -1 in this case, and then the later code will find it and use default_parallel value instead.","10/Dec/09 03:00;daijy;Leaving parallelism to -1 is not a solution here. In setting up the sampling job, we should know how many reducers we are going to use. So we need use default_parallel way before JobControlCompiler.","10/Dec/09 03:43;viraj;Hi Daniel,
 Thanks again for your input. This is more of a performance issue, where users do not detect, till they see that 1 reducer job has failed in the sort phase. They safely assume that the default_parallel keyword will do the trick.
Viraj","10/Dec/09 13:01;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427558/PIG-1144-1.patch
  against trunk revision 888852.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/112/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/112/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/112/console

This message is automatically generated.","10/Dec/09 23:28;alangates;The previous code was trying to read the default parallelism from the JobClient rather than setting it to -1, as the current code does.  This seems strange, but was there a reason for that?","11/Dec/09 02:06;daijy;I think the reason is quantile job need to know how many reducers we are going to use in order to decide tuples to write into quantilesFile. The number of reducers is a constant field of the plan. We cannot say -1 and let hadoop to decide the parallelism later. The fix actually take default_parallel as the constant if user do not use PARALLEL key word. It applies to both order by and skew join. Merge join and FRJoin are map only and regular join has been taken care of in the original code. Attach the patch again, nothing change except for including a new test case for skew join.","11/Dec/09 06:18;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427670/PIG-1144-2.patch
  against trunk revision 889346.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/114/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/114/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/114/console

This message is automatically generated.","11/Dec/09 23:21;daijy;Change the patch to take ""mapred.reduce.tasks"" into account. The hierarchy for determining the parallelism is:
1. PARALLEL keywords
2. default_parallel
3. mapred.reduce.tasks system property
4. default value: 1","12/Dec/09 12:47;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427792/PIG-1144-3.patch
  against trunk revision 889870.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/120/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/120/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/120/console

This message is automatically generated.",15/Dec/09 01:29;olgan;resubmitting to rerun the tests,15/Dec/09 22:46;olgan;this has performance consequences so need to get it into 0.6.0 release,"16/Dec/09 01:29;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427792/PIG-1144-3.patch
  against trunk revision 890596.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/125/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/125/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/125/console

This message is automatically generated.",16/Dec/09 01:57;olgan;I think the code that always set parallelism to 1 in local mode has been unintentionally removed.,"17/Dec/09 18:50;daijy;Include logic for local mode. However, this is only for user use ""-x local"". If user do not use -x option, and run into local mode because of no hadoop configuration file in CLASSPATH, Pig do not have a way to detect that, and order by job will fail.","18/Dec/09 00:16;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428331/PIG-1144-4.patch
  against trunk revision 891499.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/137/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/137/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/137/console

This message is automatically generated.",18/Dec/09 01:52;olgan;+1; patch looks good!,18/Dec/09 04:08;daijy;Patch committed to both trunk and 0.6 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Poisson Sample Loader should compute the number of samples required only once,PIG-1143,12442931,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sriranjan,sriranjan,sriranjan,10/Dec/09 00:10,24/Mar/10 22:16,14/Mar/19 03:05,22/Dec/09 18:24,,,,,,,0.6.0,,,,,0,,,,The current poisson sampler forces each of the maps to compute the sample number. This is redundant and causes issues when a large directory is specified in the join. The sampler should be changed to calculate the sample count only once and this information should be shared with the remaining mappers.,,,,,,,,,,,,,,,,,,,15/Dec/09 00:17;sriranjan;PIG_1143.patch;https://issues.apache.org/jira/secure/attachment/12427980/PIG_1143.patch,17/Dec/09 05:58;sriranjan;PIG_1143.patch.1;https://issues.apache.org/jira/secure/attachment/12428266/PIG_1143.patch.1,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-12-10 22:59:34.813,,,no_permission,,,,,,,,,,,,164665,,,,,Tue Dec 22 18:24:03 UTC 2009,,,,,,,0|i0gshb:,96044,,,,,,,,,,"10/Dec/09 22:24;sriranjan;To describe the problem in more detail, the current implementation does not handle a glob efficiently. When the sample loader encounters a directory (or combinations thereof), it gets the element descriptors of all the files inside the directory to compute the file sizes.
For ex: A = load ""{view, click}"" will result in computing file sizes of all the files underneath both ""view"" and ""click"" directories. If we have a large number of mappers, this will result in a ton of hdfs system calls, clogging the name node.

I intend to modify Poisson Sample Loader as follows. The algorithm for computing the total number of samples remains the same. However, it will not be computed by every mapper. I will be using the UDFContext object to share this information across mappers. Since mapper/ reducers can only read the information from UDFContext, the slicer will store this information. The slicer will compute the sampler count for the first map. As before, PigSlice will call computeSamples() for the first map. It will then store this value as a property in the UDFContext object. The Slicer will check UDFContext to see if this value is set and if it is, it will use it instead of computing it again. I intend to use ""pig.input.0.sampleCount"" as the key.

This solution will reduce the fileSize() invocations to a minimum and should reduce the burden on the name node.",10/Dec/09 22:59;olgan;Sounds like a good approach. We need to figure out how this will translate into Load-Store redesign and make sure to port it there once the patch is available.,"10/Dec/09 23:21;thejas;The PoissonSampleLoader implementation in Load-store redesign does not check the file size and has a different approach for the following reason (as mentioned in PIG-1062) -

With new interfaces in load-store redesign, pig can compute the file size by adding up size of each split (from InputSplit.getLenght()) . But the documentation of the function does not make it clear if this is size on disk , compressed/uncompressed etc. Looks like it just needs to be some number proportional to size of the file. Assuming it is size on disk (uncompressed), using this to estimate the total memory it will require is tricky, one has to make assumptions about the compression ratio and the serialization method.
Using Tuple.getMemorySize() while sampling will give more accurate numbers for reducer memory that it will consume. 
","10/Dec/09 23:23;thejas;To summarize my above comment, the approach in load-store redesign of not using the file-size at all is better .","10/Dec/09 23:30;sriranjan;The file size in the documentation refers to the size on disk. In order to account for compression, encoding etc. a configurable parameter - pig.inputfile.conversionfactor is provided. I agree that this cannot be set to a good value for compressed data. It is just a guidance. The implications of setting it to a bad value are minimal - we will end up sampling little more than the required number of samples (unless you set it to a fraction).","11/Dec/09 00:14;thejas;Pig input does not have to be a file, the LoadFunc could be reading from HBase or some other source. So the use of FileLocalizer.getSize(fname,pcProps) will not work in all cases.
 InputSplits.getLength() can be used instead, but as per the documentation, the purpose of  InputSplits.getLength() is ""so that the input splits can be sorted by size"". So implementations might just give a number that is proportional to the size if they don't have access to actual size. 

Even if the actual file size on disk is available through  InputSplits.getLength(), in case of columnar storage the compression can be very high (eg run-length encoding of column that is sort key with only few unique values), and we might end up sampling very little.","11/Dec/09 01:15;sriranjan;I am OK with using InputSplits.getLength() as long as these provide you a good estimate of the file size. Without the population size, poisson samplers do now work well.

Samplers expect the data to be in BinStorage. If not, the first job reads it and stores it into BinStorage. The only exception being if the join follows a load/store only MR job.
",11/Dec/09 01:19;sriranjan;*now* should have been *not*!,"11/Dec/09 01:38;thejas;If the data is going to be in BinStorage, my comments regarding the approach for this patch are not applicable. But the patch does not need to be ported to load-store redesign branch.
","15/Dec/09 05:24;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427980/PIG_1143.patch
  against trunk revision 890553.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/123/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/123/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/123/console

This message is automatically generated.",15/Dec/09 19:21;olgan;I am reviewing this patch,"15/Dec/09 22:11;olgan;I think this needs to be tested with multiple skew joins both in a case of single store and multiquery. Please, add unit tests",17/Dec/09 05:58;sriranjan;I have added the successive join and multiquery unit tests.,"17/Dec/09 10:51;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428266/PIG_1143.patch.1
  against trunk revision 891499.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 9 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/135/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/135/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/135/console

This message is automatically generated.",21/Dec/09 18:45;olgan;+1 on the code changes. There is a extra debug trace in the code that I will remove as part of the commit,21/Dec/09 19:53;olgan;patch committed to the trunk. Will commit to 0.6 branch tomorrow.,"22/Dec/09 18:24;olgan;patch committed to 0.6.0 branch. Thanks, Sri!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Got NullPointerException merge join with pruning,PIG-1142,12442929,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,jing1234,jing1234,10/Dec/09 00:03,24/Mar/10 22:16,14/Mar/19 03:05,11/Dec/09 18:43,0.6.0,,,,,,0.6.0,,,,,0,,,,"Here is my pig script:
register $zebraJar;
--fs -rmr $outputDir


a1 = LOAD '$inputDir/small1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2');
a2 = LOAD '$inputDir/small2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2');

sort1 = order a1 by str2;
sort2 = order a2 by str2;

--store sort1 into '$outputDir/smallsorted11' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2]');
--store sort2 into '$outputDir/smallsorted21' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2]');

rec1 = load '$outputDir/smallsorted11' using org.apache.hadoop.zebra.pig.TableLoader();
rec2 = load '$outputDir/smallsorted21' using org.apache.hadoop.zebra.pig.TableLoader();

joina = join rec1 by str2, rec2 by str2 using ""merge"" ;

E = foreach joina  generate $0 as count,  $1 as seed,  $2 as int1,  $3 as str2;

--limitedVals = LIMIT E 5;
--dump limitedVals;

store E into '$outputDir/smalljoin2' using org.apache.hadoop.zebra.pig.TableStorer('');



============
Here is the stacktrace:
java.lang.NullPointerException at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNext(POLocalRearrange.java:312) at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.extractKeysFromTuple(POMergeJoin.java:464) at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.getNext(POMergeJoin.java:341) at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:260) at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:237) at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:253) at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.close(PigMapBase.java:107) at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307) at org.apache.hadoop.mapred.Child.main(Child.java:159) 
",,,,,,,,,,,,,,,,,,,10/Dec/09 00:27;daijy;PIG-1142-1.patch;https://issues.apache.org/jira/secure/attachment/12427539/PIG-1142-1.patch,11/Dec/09 02:56;daijy;PIG-1142-2.patch;https://issues.apache.org/jira/secure/attachment/12427671/PIG-1142-2.patch,11/Dec/09 18:28;daijy;PIG-1142-3.patch;https://issues.apache.org/jira/secure/attachment/12427747/PIG-1142-3.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-12-10 00:26:07.839,,,no_permission,,,,,,,,,,,,164664,Reviewed,,,,Fri Dec 11 18:43:00 UTC 2009,,,,,,,0|i0gsgv:,96042,,,,,,,,,,"10/Dec/09 00:26;daijy;I can reproduce it in regular join as well. Here is the script:
a = LOAD '1.txt' as (a0, a1, a2);
b = LOAD '2.txt' as (b0, b1, b2);
c = join a by a2, b by b2;
d = foreach c generate $0,  $1,  $2;
dump d;

The logical plan is wrong:
{code}
.........
    |---LOJoin 1-17 Schema: {a::a0: bytearray,a::a1: bytearray,a::a2: bytearray,b::b2: bytearray} Type: bag
        |   |
        |   Project 1-15 Projections: [2] Overloaded: false FieldSchema: a2: bytearray Type: bytearray
        |   Input: Load 1-13
        |   |
        |   Project 1-16 Projections: [1] Overloaded: false FieldSchema: Caught Exception: Attempt to fetch field 1 from schema of size 1 Type: Unknown
        |   Input: Load 1-14
        |
        |---Load 1-13 Schema: {a0: bytearray,a1: bytearray,a2: bytearray} Type: bag
        |
        |---Load 1-14 Schema: {b2: bytearray} Type: bag
{code}

The second project of LOJoin should project column 0","10/Dec/09 03:02;daijy;To make it clear, this issue happens when we prune more than one columns in front of join key.","10/Dec/09 05:01;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427539/PIG-1142-1.patch
  against trunk revision 888852.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/110/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/110/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/110/console

This message is automatically generated.",10/Dec/09 05:14;olgan;+1. Changes look good.,10/Dec/09 17:51;daijy;Patch committed to both trunk and 0.6 branch.,"10/Dec/09 21:46;jing1234;Verified fix on pig 0.6.0 branch. 
Fix works.",11/Dec/09 02:55;daijy;Find several lines of code share the same nature as the initial one. Should fix them all. ,"11/Dec/09 10:21;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427671/PIG-1142-2.patch
  against trunk revision 889346.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/115/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/115/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/115/console

This message is automatically generated.","11/Dec/09 17:03;olgan;+1, Daniel, code changes look good but do we need to add more unit tests to cover them?","11/Dec/09 18:28;daijy;No code change in PIG-1142-3.patch, except for more test cases.",11/Dec/09 18:31;alangates;Additional test cases look good.  +1,11/Dec/09 18:43;daijy;New patch committed to both trunk and 0.6 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[zebra] Map Split of Storage info do not allow for leading underscore char '_',PIG-1136,12442877,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,,yanz,yanz,09/Dec/09 18:05,14/May/10 06:46,14/Mar/19 03:05,24/Dec/09 00:38,0.7.0,,,,,,0.7.0,,,,,0,,,,"There is some user need to support that type of map keys. Pig's column does not allow for leading underscore, but apparently no restriction is placed on the map key.",,,,,,,,,,,,,,,,,,,23/Dec/09 19:38;xuefuz;pig-1136-xuefu-new.patch;https://issues.apache.org/jira/secure/attachment/12428866/pig-1136-xuefu-new.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-12-17 18:02:58.906,,,no_permission,,,,,,,,,,,,164658,,,,,Thu Dec 24 00:38:50 UTC 2009,,,,,,,0|i0gse7:,96030,,,,,,,,,,17/Dec/09 18:02;xuefuz;This is the patch for bug.,23/Dec/09 07:03;xuefuz;My fix with test cases. please ignore previous submission.,23/Dec/09 18:41;yanz;Patch reviewed +1,"24/Dec/09 00:20;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428866/pig-1136-xuefu-new.patch
  against trunk revision 893373.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/156/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/156/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/156/console

This message is automatically generated.",24/Dec/09 00:38;yanz;Patch committed to Apache trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skewed Join sampling job overwhelms the name node,PIG-1134,12442811,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sriranjan,sriranjan,sriranjan,09/Dec/09 01:33,24/Mar/10 22:16,14/Mar/19 03:05,09/Dec/09 16:25,,,,,,,0.6.0,,,,,0,,,,The map tasks of the sampling job estimate the file size. For a large directory and a large number of maps the file system calls over whelm the name node. ,,,,,,,,,,,,,,,,,,,09/Dec/09 01:35;sriranjan;PIG-1134.patch;https://issues.apache.org/jira/secure/attachment/12427404/PIG-1134.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-12-09 01:52:22.32,,,no_permission,,,,,,,,,,,,164656,,,,,Wed Dec 09 16:25:47 UTC 2009,,,,,,,0|i0gsdb:,96026,,,,,,,,,,"09/Dec/09 01:35;sriranjan;As a stop gap, I have replaced PoissonSampleLoader with RandomSampleLoader. This does not obtain the input size. Instead it obtains 100 samples per block. ","09/Dec/09 01:52;olgan;+1 on the code change.

Please, open a separate JIRA to resolve the problem with PoissomSampleLoader","09/Dec/09 05:31;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427404/PIG-1134.patch
  against trunk revision 888601.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/106/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/106/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/106/console

This message is automatically generated.",09/Dec/09 05:52;olgan;resubmitting because of the test failures,09/Dec/09 05:54;olgan;this patch does not need new tests - existing skewed join tests cover the functionality,"09/Dec/09 15:06;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427404/PIG-1134.patch
  against trunk revision 888704.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/108/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/108/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/108/console

This message is automatically generated.","09/Dec/09 16:25;olgan;patch committed to both the trunk and branch 0.6. Thanks, Sri.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDFContext should be made available to LoadFunc.bindTo,PIG-1133,12442692,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,08/Dec/09 02:19,24/Mar/10 22:16,14/Mar/19 03:05,08/Dec/09 22:48,0.4.0,,,,,,0.6.0,,impl,,,0,,,,"Currently UDFContext is available at frontend, map and reduce. However, LoadFunc.bindTo will be called on InputFormat.getSplits, which is on a different context then the ones mentioned before. UDFContext should be available in that context as well.",,,,,,,,,,,,,,,,,,,08/Dec/09 02:21;daijy;PIG-1133-1.patch;https://issues.apache.org/jira/secure/attachment/12427280/PIG-1133-1.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-12-08 11:04:25.413,,,no_permission,,,,,,,,,,,,164655,Reviewed,,,,Tue Dec 08 22:48:14 UTC 2009,,,,,,,0|i0gscv:,96024,,,,,,,,,,08/Dec/09 02:21;daijy;This issue is not reproducible in either hadoop local mode and MiniCluster. So it is meaningless to include a unit test. I test it manually on a real cluster.,"08/Dec/09 11:04;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427280/PIG-1133-1.patch
  against trunk revision 888186.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/103/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/103/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/103/console

This message is automatically generated.",08/Dec/09 19:00;alangates;Changes look good.  It might be good to change the unit tests in TestUDFContext to test that the conf is available in the bindTo.,"08/Dec/09 19:09;daijy;The interesting thing is UDFContext is always available to SliceWrapper in MiniCluster and local mode. Only in real cluster, we cannot get correct UDFContext setup. I guess MiniCluster and real cluster use different context when invoking SliceWrapper. That's why I do not put unit test because none unit test can simulate the real cluster situation.","08/Dec/09 19:54;alangates;Yeah, I missed your first comment before I asked that question.  Ok, no test seems fine then.",08/Dec/09 22:48;daijy;Patch committed to both trunk and 0.6 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Column Pruner issues in dealing with unprunable loader,PIG-1132,12442688,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,08/Dec/09 02:09,24/Mar/10 22:16,14/Mar/19 03:05,08/Dec/09 23:29,0.4.0,,,,,,0.6.0,,,,,0,,,,"The following script fail:

{code}
a = load '1.txt' using BinStorage() as (a0, a1, a2);
b = foreach a generate a2, a0, a1;
c = foreach b generate a0, a2;
explain c;
{code}

Error message:
ERROR 2163: Error during fixing projections. Could not locate replacement column for column: 1 in the new predecessor        at org.apache.pig.impl.logicalLayer.ProjectFixerUpper.visit(ProjectFixerUpper.java:262)
        at org.apache.pig.impl.logicalLayer.LOProject.visit(LOProject.java:404)
        at org.apache.pig.impl.logicalLayer.LOProject.visit(LOProject.java:58)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:67)
        at org.apache.pig.impl.plan.DepthFirstWalker.walk(DepthFirstWalker.java:50)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.impl.logicalLayer.LOForEach.rewire(LOForEach.java:770)
        ... 19 more

Also even we fix the exception, the result is wrong.

Thanks Thejas for finding a reproducible test case for PigLatin",,,,,,,,,,,,,,,,,,,08/Dec/09 02:16;daijy;PIG-1132-1.patch;https://issues.apache.org/jira/secure/attachment/12427278/PIG-1132-1.patch,08/Dec/09 06:45;daijy;PIG-1132-2.patch;https://issues.apache.org/jira/secure/attachment/12427303/PIG-1132-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-12-08 06:34:45.722,,,no_permission,,,,,,,,,,,,164654,Reviewed,,,,Tue Dec 08 23:29:33 UTC 2009,,,,,,,0|i0gscf:,96022,,,,,,,,,,"08/Dec/09 06:34;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427278/PIG-1132-1.patch
  against trunk revision 888186.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/102/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/102/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/102/console

This message is automatically generated.",08/Dec/09 06:45;daijy;Ignore the first patch. ,"08/Dec/09 15:40;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427303/PIG-1132-2.patch
  against trunk revision 888186.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/104/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/104/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/104/console

This message is automatically generated.","08/Dec/09 19:46;alangates;Is this bug specific to a script with two foreachs in a row, or will it manifest with any script with more than one foreach?  So would:

A = load
B = foreach ..
C = filter
D = group
E = foreach

trigger this bug as well?","08/Dec/09 20:18;daijy;Yes, it could happen for any logical operators after Load. The key issue for this problem is OperatorPlan.insertBetween assume logical plan in a consistent state. What I want to do here is inserting a LOForEach after LOLoad if the loader do not support pruning. I will need to achieve this in two steps:

1. insert a LOForEach after LoLoad
2. prune columns the logical operator below

I have to leave the logical plan in an inconsistent state in the middle. If I insert LOForEach first, then the underlying logical operator has not been pruned yet; If I prune underlying logical operator first, then there is a schema gap between underlying logical operator and LOLoad. Both situations are not allowed for insertBetween. I have to use a trick to get around this problem.","08/Dec/09 22:00;alangates;Thanks, that clears up several questions I had.  

+1, patch looks good.",08/Dec/09 23:29;daijy;Core test failures are due to port conflict. Test manually and it is successful. Patch committed to both trunk and 0.6 branch. Thanks Alan!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig simple join does not work when it contains empty lines,PIG-1131,12442680,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ashutoshc,viraj,viraj,08/Dec/09 00:22,14/May/10 06:46,14/Mar/19 03:05,13/Feb/10 21:47,0.7.0,,,,,,0.7.0,,impl,,,0,,,,"I have a simple script, which does a JOIN.

{code}
input1 = load '/user/viraj/junk1.txt' using PigStorage(' ');
describe input1;

input2 = load '/user/viraj/junk2.txt' using PigStorage('\u0001');
describe input2;

joineddata = JOIN input1 by $0, input2 by $0;

describe joineddata;

store joineddata into 'result';
{code}

The input data contains empty lines.  

The join fails in the Map phase with the following error in the PRLocalRearrange.java

java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.pig.data.DefaultTuple.get(DefaultTuple.java:143)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.constructLROutput(POLocalRearrange.java:464)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNext(POLocalRearrange.java:360)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POUnion.getNext(POUnion.java:162)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:253)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:244)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:94)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)
	at org.apache.hadoop.mapred.Child.main(Child.java:159)

I am surprised that the test cases did not detect this error. Could we add this data which contains empty lines to the testcases?

Viraj",,,,,,,,,,,,,,,,,,,08/Dec/09 00:27;viraj;junk1.txt;https://issues.apache.org/jira/secure/attachment/12427257/junk1.txt,08/Dec/09 00:27;viraj;junk2.txt;https://issues.apache.org/jira/secure/attachment/12427258/junk2.txt,10/Feb/10 03:03;ashutoshc;pig-1131.patch;https://issues.apache.org/jira/secure/attachment/12435402/pig-1131.patch,10/Feb/10 01:37;ashutoshc;pig-1131.patch;https://issues.apache.org/jira/secure/attachment/12435394/pig-1131.patch,08/Dec/09 00:27;viraj;simplejoinscript.pig;https://issues.apache.org/jira/secure/attachment/12427259/simplejoinscript.pig,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2009-12-08 01:24:10.723,,,no_permission,,,,,,,,,,,,164653,,,,,Sat Feb 13 21:47:46 UTC 2010,,,,,,,0|i0gsc7:,96021,,,,,,,,,,08/Dec/09 00:27;viraj;Dummy datasets and pig script,"08/Dec/09 01:24;pkamath;Not sure, but looking through the code, the issue might be present even if first input tuple has more fields than subsequent input tuples for either of the inputs in the join. The issue is because in the optimization to not send parts of the value present in the key, in POLocalRearrange, we further try to optimize by remembering which parts of the value were present in the key for the first value - so if the next value has different number of fields, we hit the exception seen in the description.","09/Dec/09 23:34;viraj;Hi Pradeep,
 So the workaround for this is for the user to specify the schema for the largest size tuple or which contains the maximum number of fields/columns?
Viraj",02/Feb/10 18:19;olgan;Not sure why this issue was marked critical,"04/Feb/10 19:47;ashutoshc;Can't reproduce this on trunk. PIG-1194 touched upon the same piece of code and was recently checked in. That one might have fixed this one too. Viraj, can you please confirm if you can reproduce it or some variant of it ?","09/Feb/10 01:51;viraj;Olga I marked it as critical since we mention that Pig can eat any type of data, and the example script shows that we need data with fixed schema's and to perform a simple join.

Viraj","09/Feb/10 01:54;viraj;Ashutosh I was able to recreate a similar problem using the trunk. 

java -cp pig-withouthadoop.jar org.apache.pig.Main -version


Apache Pig version 0.7.0-dev (r907874) 

compiled Feb 08 2010, 17:35:04

Viraj","09/Feb/10 23:12;ashutoshc;Thanks Viraj. I am able to reproduce it. Root cause of it is a optimization in POLocalRearrange which assumes that number of elements in a tuple is constant across whole relation, which is not a valid assumption as shown in the case here. I will be posting a patch for it soon.",09/Feb/10 23:36;olgan;This is duplicate of PIG-1188,09/Feb/10 23:57;ashutoshc;Reopening since related to 1188 not a duplicate of it.,10/Feb/10 01:37;ashutoshc;In POLocalRearrange number of elements in tuple not present in key (and thus put in value) is computed first time and then cached as an optimization. This patch removes this caching because of the problem illustrated in the bug. Test case included which reproduces the bug. ,"10/Feb/10 02:06;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12435394/pig-1131.patch
  against trunk revision 908177.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/196/console

This message is automatically generated.",10/Feb/10 03:03;ashutoshc;Previous patch was stale. Merged with trunk and regenerated the patch.,"10/Feb/10 07:42;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12435402/pig-1131.patch
  against trunk revision 908324.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/197/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/197/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/197/console

This message is automatically generated.",12/Feb/10 23:51;rding;+1 for commit.,13/Feb/10 21:47;ashutoshc;Patch committed. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
column pruning causing failure when foreach has user-specified schema,PIG-1128,12442511,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,thejas,thejas,04/Dec/09 21:49,24/Mar/10 22:16,14/Mar/19 03:05,07/Dec/09 22:02,0.6.0,0.7.0,,,,,0.6.0,,,,,0,,,,"Issue is seen in 0.6.0 and trunk.

grunt> l = load 'dummy.txt' as ( c1 : chararray,  c2 : int);                                   
grunt> f1 = foreach l generate c1 as c1 : chararray, c2 as c2 : int, 'CA' as state : chararray;
grunt> f2 = foreach f1 generate c1 as c1 : chararray;                                          
grunt> explain f2;
2009-12-04 13:11:19,010 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1022: Type mismatch merging schema prefix. Field Schema: chararray. Other Field Schema: c2: int


( it does not matter if the new schema has new/different column name - )
grunt>l = load 'dummy.txt' as ( c1 : chararray,  c2 : int);
grunt>f1 = foreach l generate c1 as c11 : chararray, c2 as c22 : int, 'CA' as state : chararray;
grunt>f2 = foreach f1 generate c11 as c111 : chararray;
grunt> explain f2;
2009-12-04 13:13:01,462 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1022: Type mismatch merging schema prefix. Field Schema: chararray. Other Field Schema: c22: int
",,,,,,,,,,,,,,,,,,,05/Dec/09 02:17;daijy;PIG-1128-1.patch;https://issues.apache.org/jira/secure/attachment/12427028/PIG-1128-1.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-12-06 03:15:41.769,,,no_permission,,,,,,,,,,,,164650,Reviewed,,,,Mon Dec 07 22:02:50 UTC 2009,,,,,,,0|i0gsav:,96015,,,,,,,,,,"06/Dec/09 03:15;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427028/PIG-1128-1.patch
  against trunk revision 887401.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/99/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/99/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/99/console

This message is automatically generated.",07/Dec/09 21:39;pkamath;reviewed - +1,07/Dec/09 21:52;daijy;Core tests failures are due to port conflict. Run unit tests manually and all passed.,07/Dec/09 22:02;daijy;Committed to both trunk and branch 0.6.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Logical operator should contains individual copy of schema object,PIG-1127,12442491,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,04/Dec/09 18:56,24/Mar/10 22:16,14/Mar/19 03:05,07/Dec/09 23:33,0.4.0,,,,,,0.6.0,,,,,0,,,,"Currently some logical operator only contains a schema reference to the predecessor's schema object. These logical operators include: LOSplitOutput, LOLimit, LOSplit, LOFilter, LOSort, LODistinct, LOUnion. It is ok in the before because we do not change schema object once it is set. Now with the column pruner (PIG-922), we need to change individual schema object so it is no longer acceptable. For example, the following script fail:

{code}
a = load '1.txt' as (a0, a1:map[], a2);
b = foreach a generate a1;
c = limit b 10;
dump c;
{code}

We need to fix it.",,,,,,,,,,,,,,,,,,,04/Dec/09 21:09;daijy;PIG-1127-1.patch;https://issues.apache.org/jira/secure/attachment/12426964/PIG-1127-1.patch,05/Dec/09 04:55;daijy;PIG-1127-2.patch;https://issues.apache.org/jira/secure/attachment/12427035/PIG-1127-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-12-05 06:36:08.692,,,no_permission,,,,,,,,,,,,164649,Reviewed,,,,Mon Dec 07 23:33:17 UTC 2009,,,,,,,0|i0gsaf:,96013,,,,,,,,,,"05/Dec/09 06:36;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426964/PIG-1127-1.patch
  against trunk revision 887401.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/94/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/94/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/94/console

This message is automatically generated.","06/Dec/09 07:50;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427035/PIG-1127-2.patch
  against trunk revision 887401.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/100/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/100/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/100/console

This message is automatically generated.",07/Dec/09 21:29;pkamath;Changes look good - I am wondering though if we can change the copy constructor in FieldSchema to make copies of canonicalMap and ReverseCanonicalMap and then the code to setParent can be removed from the patch.,"07/Dec/09 23:15;daijy;It is ideal we put the setParent logic into copy constructor of FieldSchema. However, canonicalMap of FieldSchema points to the grand parents of the logical operator we address. We cannot simply copy it from the input. Inside Schema/FieldSchema, we cannot access the logical plan in order to get the parent of the logical operator. It has to be done outside Schema/FieldSchema. So I have to keep setParent method call as it is until we have a better idea.",07/Dec/09 23:33;daijy;Committed to both trunk and 0.6 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
piggybank loaders need to update fieldsToRead function,PIG-1126,12442469,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,04/Dec/09 16:43,24/Mar/10 22:16,14/Mar/19 03:05,04/Dec/09 18:32,,,,,,,0.6.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,04/Dec/09 16:47;olgan;PIG-1126.patch;https://issues.apache.org/jira/secure/attachment/12426914/PIG-1126.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-12-04 17:29:28.081,,,no_permission,,,,,,,,,,,,164648,,,,,Fri Dec 04 18:32:06 UTC 2009,,,,,,,0|i0gs9z:,96011,,,,,,,,,,"04/Dec/09 16:47;olgan;All unit tests passed. Please, review. (This is for both the trunk and 0.6 branch)",04/Dec/09 17:29;daijy;+1,04/Dec/09 18:32;olgan;patch committed to trank and 0.6 branch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to set Custom Job Name using the -Dmapred.job.name parameter,PIG-1124,12442373,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,ashutoshc,viraj,viraj,03/Dec/09 19:53,14/May/10 06:46,14/Mar/19 03:05,01/Feb/10 19:21,0.6.0,,,,,,0.7.0,,impl,,,0,,,,"As a Hadoop user I want to control the Job name for my analysis via the command line using the following construct::

java -cp pig.jar:$HADOOP_HOME/conf -Dmapred.job.name=hadoop_junkie org.apache.pig.Main broken.pig

-Dmapred.job.name should normally set my Hadoop Job name, but somehow during the formation of the job.xml in Pig this information is lost and the job name turns out to be:

""PigLatin:broken.pig""

The current workaround seems to be wiring it in the script itself, using the following ( or using parameter substitution).

set job.name 'my job'

Viraj",,,,,,,,,,,,,,,,,,,29/Jan/10 00:55;ashutoshc;pig-1124.patch;https://issues.apache.org/jira/secure/attachment/12431730/pig-1124.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-01-27 18:45:22.733,,,no_permission,,,,,,,,,,,,164646,,,,,Mon Feb 01 19:21:18 UTC 2010,,,,,,,0|i0gs93:,96007,,,,,,,,,,27/Jan/10 18:45;ashutoshc;This seems to be related to PIG-1186 which is now fixed. Viraj can you try adding hadoop properties you wish to override in pig-cluster-hadoop-site.xml and then add that xml in the classpath. That should set hadoop properties of the job via Pig.,"29/Jan/10 00:55;ashutoshc;If the job is submitted through script-file, Pig sets filename as name of job. If script is submitted using -e Pig sets name of job as ""PigLatin:DefaultJobName"". In both the cases ignoring -D switch. This patch fixes that. Pig now sets default job names only if user doesnt provide the job name using -D switch. Hard to write unit test. Tested manually.","29/Jan/10 07:14;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12431730/pig-1124.patch
  against trunk revision 904241.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/191/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/191/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/191/console

This message is automatically generated.","29/Jan/10 21:24;ashutoshc;As per previous comment, did manual testing on the patch, no easy way to write unit test. Patch is ready for review. ","29/Jan/10 22:24;dvryaboy;Looks fine.

Can we do something like list all properties Pig overrides, and do a loop over them checking if they are set by the user? It seems like there are a lot of these one-off tickets lately.","01/Feb/10 04:30;ashutoshc;Thanks for the review, Dmitriy. 
AFAIK, there aint any other property that Pig overrides such that user can't set it himself using -D switch. Correct me if I am wrong. job.name was special-cased so that if user doesnt provide job name, pig sets the name itself to make it easy to track jobs submitted by pig on the cluster.  Bug in this ticket is Pig was always setting the name(instead of setting only when user wasnt supplying any), thus completely ignoring user supplied name through -D switch.",01/Feb/10 19:21;ashutoshc;Patch checked-in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[zebra] Zebra build.xml still uses 0.6 version,PIG-1122,12442295,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yanz,yanz,yanz,03/Dec/09 00:55,14/May/10 06:46,14/Mar/19 03:05,03/Dec/09 23:27,0.7.0,,,,,,0.7.0,,,,,0,,,, Zebra still uses pig-0.6.0-dev-core.jar in build-contrib.xml. It should be changed to pig-0.7.0-dev-core.jar on APACHE trunk only.,,,,,,,,,,,,,,,,,,,03/Dec/09 01:24;yanz;PIG-1122.patch;https://issues.apache.org/jira/secure/attachment/12426734/PIG-1122.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-12-03 01:42:36.396,,,no_permission,,,,,,,,,,,,164644,,,,,Thu Dec 03 23:27:41 UTC 2009,,,,,,,0|i0gs87:,96003,,,,,,,,,,"03/Dec/09 01:26;yanz;Note that the patch should be applied to trunk. 

Also note that there is no test case for this trivial versioning change so any Hundson grievance in that regard should be ignored.","03/Dec/09 01:33;yanz;This has not caused any problems since the CLASSPATH also contains the path pointing to the dir holding the PIG classes directly. But still, it's not perfect and could cause nasty headaches if someone has a leftover 0.6 Pig jar when build zebra.",03/Dec/09 01:42;chaow;+1,"03/Dec/09 08:07;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426734/PIG-1122.patch
  against trunk revision 886650.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/80/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/80/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/80/console

This message is automatically generated.",03/Dec/09 23:27;yanz;Patch checked in. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[zebra] ""group"" is a Pig preserved word, zebra needs to use other string for table's group information",PIG-1119,12442159,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,jing1234,jing1234,02/Dec/09 00:02,24/Mar/10 22:16,14/Mar/19 03:05,07/Dec/09 19:45,0.6.0,,,,,,0.6.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,04/Dec/09 09:12;gauravj;PIG-1119.patch;https://issues.apache.org/jira/secure/attachment/12426881/PIG-1119.patch,03/Dec/09 08:29;gauravj;PIG-1119.patch;https://issues.apache.org/jira/secure/attachment/12426761/PIG-1119.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-12-03 08:29:46.278,,,no_permission,,,,,,,,,,,,164641,,,,,Mon Dec 07 19:45:30 UTC 2009,,,,,,,0|i0gs73:,95998,,,,,,,,,,"03/Dec/09 08:29;gauravj;
Please review the patch","03/Dec/09 08:30;gauravj;
Please review the patch","03/Dec/09 17:52;yanz;The fix treats ""gid"" instead of ""group"" as a zebra reserved word. To be consistent, the keyword ""user"" should better be changed to ""uid"". In addition, one error message still complains about ""Group defined more than once"" which should read as ""Gid defined more than once"" now. 

There is no new test case added. There should be at least one test case that shows the PIG problem without this fix but runs ok with this fix.

Also note that this security feature as supported by the ""secure by"" clause in storage info during STORE is new in this release, and only experimental and subject to changes in the future with possible loss of backward compatibility support.","03/Dec/09 21:45;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426761/PIG-1119.patch
  against trunk revision 886650.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 30 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/83/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/83/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/83/console

This message is automatically generated.","04/Dec/09 00:18;jing1234;Here is the pig script for the test:(group_wrong.pig)

register /grid/0/dev/hadoopqa/jars/zebra.jar;
A = load 'filter.txt' as (name:chararray, age:int);
B = group A by name;
C = foreach B generate group, COUNT(A.name) as cnt;
Store C into 'group1' using org.apache.hadoop.zebra.pig.TableStorer('[group];[cnt]');

===================
Here is the error message before the fix:
ackend error message during job submission
-------------------------------------------
java.io.IOException: ColumnGroup.Writer constructor failed : Partition constructor failed :Encountered "" ""group"" ""group """" at line 1, column 2.
Was expecting one of:
    <IDENTIFIER> ...
    ""]"" ...

        at org.apache.hadoop.zebra.io.BasicTable$Writer.<init>(BasicTable.java:1259)
        at org.apache.hadoop.zebra.pig.TableOutputFormat.checkOutputSpecs(TableStorer.java:135)
        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:772)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:730)
        at org.apache.hadoop.mapred.jobcontrol.Job.submit(Job.java:378)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs(JobControl.java:247)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.run(JobControl.java:279)
        at java.lang.Thread.run(Thread.java:619)

Pig Stack Trace
---------------
ERROR 2997: Unable to recreate exception from backend error: java.io.IOException: ColumnGroup.Writer constructor failed : Partition constructor failed :Encountered "" ""group"" ""group """" at line 1, column 2.

org.apache.pig.backend.executionengine.ExecException: ERROR 2997: Unable to recreate exception from backend error: java.io.IOException: ColumnGroup.Writer constructor failed : Partition constructor failed :Encountered "" ""group"" ""group """" at line 1, column 2.
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher.getStats(Launcher.java:176)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:253)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:249)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:780)
        at org.apache.pig.PigServer.execute(PigServer.java:773)
        at org.apache.pig.PigServer.access$100(PigServer.java:89)
        at org.apache.pig.PigServer$Graph.execute(PigServer.java:951)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:248)
        at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:115)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:172)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:144)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:89)
        at org.apache.pig.Main.main(Main.java:386)
================================================================================

Now with the patch we can successfully create the table with the same script.
$HADOOP_HOME/bin/hadoop fs -cat group1/.btschema
[group];[cnt]cnt:long
 group:strincnt:long ","04/Dec/09 09:11;gauravj;
Providing an updated version","04/Dec/09 09:12;gauravj;
Changes incorporated as part for code review feedback",04/Dec/09 14:07;yanz;+1,"04/Dec/09 17:51;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426881/PIG-1119.patch
  against trunk revision 887049.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 39 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/90/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/90/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/90/console

This message is automatically generated.",07/Dec/09 19:45;yanz;patched committed to 0.6 branch and Apache trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"expression with aggregate functions returning null, with accumulate interface",PIG-1118,12442152,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yinghe,thejas,thejas,01/Dec/09 21:56,24/Mar/10 22:16,14/Mar/19 03:05,04/Dec/09 19:01,,,,,,,0.6.0,,,,,0,,,,"The problem is in trunk . It works fine in 0.6 branch.

l = load '/tmp/students.txt' as (a : chararray,b : chararray,c : int);
grunt> g = group l by 1;
grunt> dump g;
(1,{(asdfxc,M,23),(qwer,F,21),(uhsdf,M,34),(zxldf,M,21),(qwer,F,23),(oiue,M,54)})
grunt> f = foreach g generate SUM(l.c), 1 + SUM(l.c) + SUM(l.c);
grunt> dump f;
(176L,)
",,,,,,,,,,,,,,,,,,,02/Dec/09 19:37;yinghe;PIG_1118.patch;https://issues.apache.org/jira/secure/attachment/12426698/PIG_1118.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-12-02 19:37:30.152,,,no_permission,,,,,,,,,,,,164640,,,,,Fri Dec 04 19:01:08 UTC 2009,,,,,,,0|i0gs6n:,95996,,,,,,,,,,"01/Dec/09 21:57;thejas;Note that result of ""1 + SUM(l.c) + SUM(l.c)"" above is null . It should have been 353L .",02/Dec/09 19:37;yinghe;bug fix.,"02/Dec/09 22:00;olgan;Ying, the change looks good. Please, add a unit test for this bug.","02/Dec/09 22:55;yinghe;Olga, thank for review.  A unit test is in the patch, TestAccumulator. ","02/Dec/09 23:40;olgan;Thanks, I am not sure how I missed it :)","03/Dec/09 03:49;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426698/PIG_1118.patch
  against trunk revision 886015.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/79/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/79/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/79/console

This message is automatically generated.",03/Dec/09 17:08;olgan;looks like the failures due to problems with framework no the patch itself. Will resubmit the patch.,"04/Dec/09 06:51;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426698/PIG_1118.patch
  against trunk revision 887017.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/86/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/86/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/86/console

This message is automatically generated.",04/Dec/09 19:01;olgan;patch committed on trunk and branch 0.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove redundant map-reduce job for merge join,PIG-1116,12442016,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,daijy,daijy,01/Dec/09 01:11,24/Mar/10 22:16,14/Mar/19 03:05,03/Dec/09 18:21,,,,,,,0.6.0,,,,,0,,,,"In merge join, when we convert right hand side file into a side file, we didn't remove it from the map-reduce plan, we only disconnect it from the plan. When we run the query, the redundant load will load the data but doing nothing. This operation should be removed entirely. 

Eg: 
a = load '/user/pig/tests/data/zebra/singlefile/studentsortedtab10k' using org.apache.hadoop.zebra.pig.TableLoader('', 'sorted') as (name, age, gpa);
b = load '/user/pig/tests/data/zebra/singlefile/votersortedtab10k' using org.apache.hadoop.zebra.pig.TableLoader('', 'sorted') as (name, age, registration, contributions);
c = join a by name, b by name using ""merge"";
explain c;

{code}
#--------------------------------------------------
# Map Reduce Plan                                  
#--------------------------------------------------
MapReduce node 1-21
Map Plan
Load(hdfs://wilbur20.labs.corp.sp1.yahoo.com:9020/user/pig/tests/data/zebra/singlefile/votersortedtab10k:org.apache.hadoop.zebra.pig.TableLoader('','sorted')) - 1-13--------
Global sort: false
----------------

MapReduce node 1-20
Map Plan
Store(fakefile:org.apache.pig.builtin.PigStorage) - 1-19
|
|---MergeJoin[tuple] - 1-16
    |
    |---Load(hdfs://wilbur20.labs.corp.sp1.yahoo.com:9020/user/pig/tests/data/zebra/singlefile/studentsortedtab10k:org.apache.hadoop.zebra.pig.TableLoader('','sorted')) - 1-12--------
Global sort: false
----------------
{code}

1-21 should be removed.",,,,,,,,,,,,,,,,,,,02/Dec/09 06:55;pkamath;PIG-1116.patch;https://issues.apache.org/jira/secure/attachment/12426637/PIG-1116.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-12-02 06:55:37.866,,,no_permission,,,,,,,,,,,,164638,Reviewed,,,,Thu Dec 03 18:21:48 UTC 2009,,,,,,,0|i0gs5r:,95992,,,,,,,,,,02/Dec/09 06:55;pkamath;Attached patch to fix this issue in MRCompiler - the extra job is now removed out of the MROperPlan.,"02/Dec/09 11:02;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426637/PIG-1116.patch
  against trunk revision 886015.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/74/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/74/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/74/console

This message is automatically generated.",03/Dec/09 01:53;olgan;+1,03/Dec/09 18:21;pkamath;Patch checked in into trunk and 0.6 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[zebra] temp files are not cleaned.,PIG-1115,12441981,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,gauravj,hong.tang,hong.tang,30/Nov/09 18:33,14/May/10 06:46,14/Mar/19 03:05,17/Feb/10 00:33,0.7.0,,,,,,0.7.0,,,,,0,,,,"Temp files created by zebra during table creation are not cleaned where there is any task failure, which results in waste of disk space.",,,,,,,,,,,,,,,,,,,16/Feb/10 18:27;gauravj;PIG-1115.patch;https://issues.apache.org/jira/secure/attachment/12436006/PIG-1115.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-02-06 00:00:54.231,,,no_permission,,,,,,,,,,,,164637,,,,,Wed Feb 17 00:33:06 UTC 2010,,,,,,,0|i0gs5b:,95990,,,,,,,,,,"06/Feb/10 00:00;gauravj;Proposed Solution:

-- Zebra will implement ZebraOutputCommitter

-- Zebra FrontEnd will create all the final directories and schema files 

                    $basicTable/.btschema
                    $basicTable/CG0/.schema
                    $basicTable/CG1/.schema


-- Zebra will create a temporary directory per BasicTable and write all data there during RecordWrite.write() under

                     $basicTable/_temporary/CG0/part-0000
                     $basicTable/_temporary/CG1/part-0000

-- _temporary directory will always be created under $basicTable

-- In BackEnd, Zebra created RecordWrites which in turn creates CGInserter. CGInserter works on directory, which we call 'workOutputPath' , 
                                  $basicTable/_temporary/$CG/
             But It needs .schema file which is located 2 levels up. So it reads schema file from
                                  $basicTable/$workOutputPath.getName()

-- In CGInserter.close(), 
                     $basicTable/_temporary/CG0/part-0000       ----------->              $basicTable/CG0/part-0000
-- In ZebraOutputCommitter.cleanupJob(), BasicTableOutputFormat.close() will be called.
-- In BasicTableOutPutFormat.close()
                      remove (                $basicTable/_temporary/               )




","16/Feb/10 18:27;gauravj;
Patch for the fix.

We rely on application to call BTOF.close() for successful jobs as in Hadoop 0.21 OutputCommitter we can not differetiate b/w failed and successful jobs. Hadoop patch for this issue is available in Hadoop 0.22

For the same reasons, we rely on applications to clean any unwanted files/dirs for FAILED JOBS as they are doing currently.

Once the Hadoop patch/release is available, we can port the above inside zebra libraries.",16/Feb/10 18:46;hong.tang;Why not requesting the patch to be back ported to Hadoop 0.21 (btw do you mean Hadoop 0.21 or 0.20)?,"16/Feb/10 18:52;gauravj;
We discussed the backport with M/R team ( patch MAPREDUCE-947), earliest it can be done is in the next release of Hadoop.

I meant Hadoop 0.20/0.21 ( any release other than trunk )",16/Feb/10 19:42;yanz;patch reviewed +1,"16/Feb/10 20:34;yanz;Hudson results on the load-store-redesign branch:

+1 overall.

+1 @author.  The patch does not contain any @author tags.

+1 tests included.  The patch appears to include 7 new or modified tests.

+1 javadoc.  The javadoc tool did not generate any warning messages.

+1 javac.  The applied patch does not increase the total number of javac compiler warnings.

 +1 findbugs.  The patch does not introduce any new Findbugs warnings.

 +1 release audit.  The applied patch does not increase the total number of release audit warnings.",17/Feb/10 00:33;yanz;Patch committed to the load-store-redesign branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MultiQuery optimization throws error when merging 2 level splits,PIG-1114,12441922,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,rding,ankur,ankur,30/Nov/09 06:42,24/Mar/10 22:15,14/Mar/19 03:05,03/Dec/09 18:07,,,,,,,0.6.0,,,,,0,,,,"Multi-query optimization throws an error when merging 2 level splits. Following is the script to reproduce the error

data = LOAD 'data' USING PigStorage() AS (id:int, name:chararray);

ids = FOREACH data GENERATE id;
allId = GROUP ids all;
allIdCount = FOREACH allId GENERATE group as allId, COUNT(ids) as total;
idGroup = GROUP ids by id;
idGroupCount = FOREACH idGroup GENERATE group as id, COUNT(ids) as count;
countTotal = cross idGroupCount, allIdCount;
idCountTotal = foreach countTotal generate
        id,
        count,
        total,
        (double)count / (double)total as proportion;
orderedCounts = order idCountTotal by count desc;
STORE orderedCounts INTO 'mq_problem/ids';

names = FOREACH data GENERATE name;
allNames = GROUP names all;
allNamesCount = FOREACH allNames GENERATE group as namesAll, COUNT(names) as total;
nameGroup = GROUP names by name;
nameGroupCount = FOREACH nameGroup GENERATE group as name, COUNT(names) as count;
namesCrossed = cross nameGroupCount, allNamesCount;
nameCountTotal = foreach namesCrossed generate
        name,
        count,
        total,
        (double)count / (double)total as proportion;
nameCountsOrdered = order nameCountTotal by count desc;
STORE nameCountsOrdered INTO 'mq_problem/names';


",,,,,,,,,,,,,,,,,,,01/Dec/09 20:24;rding;PIG-1114.patch;https://issues.apache.org/jira/secure/attachment/12426576/PIG-1114.patch,01/Dec/09 05:44;ankur;Pig_1114_Client.log;https://issues.apache.org/jira/secure/attachment/12426503/Pig_1114_Client.log,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-12-01 01:35:15.253,,,no_permission,,,,,,,,,,,,164636,,,,,Thu Dec 03 18:07:28 UTC 2009,,,,,,,0|i0gs4v:,95988,,,,,,,,,,"30/Nov/09 06:45;ankur;The error thrown is 

java.io.IOException: Type mismatch in key from map: expected org.apache.pig.impl.io.NullableTuple, recieved org.apache.pig.impl.io.NullableText
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:807)
	at org.apache.hadoop.mapred.MapTask$OldOutputCollector.collect(MapTask.java:466)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:108)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:249)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:238)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:93)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)
	at org.apache.hadoop.mapred.Child.main(Child.java:159)

","30/Nov/09 06:49;ankur;The same script works with -M (multi-query disabled) option, BUT surprisingly the run indicates that now multi-query optimization being applied separately to the first STORE and the second STORE. This is just a workaround but it also indicates that in cases like this, disabling multi-query actually DOES NOT disable it completely instead just makes it run on parts of the script.",01/Dec/09 01:35;rding;The reason we got this exception is that the MultiQuery optimizer doesn't recursively set data type in local rearrange operators (it only sets on the first level). This is required in the case where merged jobs don't have the same map key types.,"01/Dec/09 01:47;rding;Ankur,

Can you look into the log file to make sure that the MQ is not disableed when you use -M option? MultiQuery optimizer always log in info level when it's applied. ","01/Dec/09 05:43;ankur;Richard,
             I ran the above script again with -M  option to confirm that Multiquery was not disabled, instead it worked on 2 separated parts of the script. I am attaching the pig client logs from the run for your reference.","01/Dec/09 20:24;rding;Ankur, I just checked and the latest pig 0.6 jar doesn't disable MQ optimization completely. The problem was fixed as part of PIG-1060 and the fixed has been checked in to both trunk and 0.6 branch.  ",01/Dec/09 20:24;rding;This patch fixed the problem.,"02/Dec/09 03:04;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426576/PIG-1114.patch
  against trunk revision 885953.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/72/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/72/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/72/console

This message is automatically generated.",03/Dec/09 01:24;olgan;+1 on the changes. will be committing now to trunk and 0.6 branch.,"03/Dec/09 18:07;olgan;patch committed to trunk and 0.6 branch. Thanks, Richard!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Diamond query optimization throws error in JOIN,PIG-1113,12441772,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,ankur,ankur,26/Nov/09 12:31,24/Mar/10 22:15,14/Mar/19 03:05,03/Dec/09 18:47,,,,,,,0.6.0,,,,,0,,,,"The following script results in 1 M/R job as a result of diamond query optimization but the script fails.

set1 = LOAD 'set1' USING PigStorage as (a:chararray, b:chararray, c:chararray);
set2 = LOAD 'set2' USING PigStorage as (a: chararray, b:chararray, c:bag{});

set2_1 = FOREACH set2 GENERATE a as f1, b as f2, (chararray) 0 as f3;
set2_2 = FOREACH set2 GENERATE a as f1, FLATTEN((IsEmpty(c) ? null : c)) as f2, (chararray) 1 as f3;

all_set2 = UNION set2_1, set2_2;

joined_sets = JOIN set1 BY (a,b), all_set2 BY (f2,f3);
dump joined_sets;

And here is the error

org.apache.pig.backend.executionengine.ExecException: ERROR 1071: Cannot convert a bag to a String
	at org.apache.pig.data.DataType.toString(DataType.java:739)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(POCast.java:625)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:364)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:288)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:260)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNext(POLocalRearrange.java:256)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POUnion.getNext(POUnion.java:162)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:247)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:238)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:93)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)
	at org.apache.hadoop.mapred.Child.main(Child.java:159)

",,,,,,,,,,,,,,,,,,,01/Dec/09 19:23;rding;PIG-1113.patch;https://issues.apache.org/jira/secure/attachment/12426566/PIG-1113.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-12-01 01:27:47.492,,,no_permission,,,,,,,,,,,,164635,Reviewed,,,,Thu Dec 03 18:47:39 UTC 2009,,,,,,,0|i0gs4f:,95986,,,,,,,,,,"26/Nov/09 12:37;ankur;The script fails even if correct schema is specified for the c:bag{}. So the following change does not alleviate the problem

set2 = LOAD 'set2' USING PigStorage as (a: chararray, b:chararray, c:bag{T:tuple(l:chararray)});","01/Dec/09 01:27;rding;The problem here is that the diamond query optimization didn't take into account that the diamond ""tail"" may also load files other than the file stored by the diamond ""head"". The diamond query optimization should check the file specs (make sure the load file of the diamond ""tail"" is the same as the store file of the diamon ""head"") before removing store/load combination.",01/Dec/09 19:23;rding;This patch fixed the problem.,"01/Dec/09 23:17;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426566/PIG-1113.patch
  against trunk revision 885858.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/71/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/71/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/71/console

This message is automatically generated.",03/Dec/09 18:17;olgan;+1. Will be committing the patch onto trunk and branch 0.6,03/Dec/09 18:47;olgan;patch committed to trunk and 0.6 branch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLATTEN eliminates the alias,PIG-1112,12441747,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,ankur,ankur,26/Nov/09 08:00,04/Aug/11 00:34,14/Mar/19 03:05,10/Jan/11 22:29,,,,,,,0.9.0,,,,,0,,,,"If schema for a field of type 'bag' is partially defined then FLATTEN() incorrectly eliminates the field and throws an error. 
Consider the following example:-

A = LOAD 'sample' using PigStorage() as (first:chararray, second:chararray, ladder:bag{});              
B = FOREACH A GENERATE first,FLATTEN(ladder) as third,second;                                   
C = GROUP B by (first,third);

This throws the error
 ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Invalid alias: third in {first: chararray,second: chararray}
",,,,,,,,,,,,,,,PIG-998,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-12-01 18:38:24.338,,,no_permission,,,,,,,,,,,,66196,,,,,Mon Jan 10 22:29:43 UTC 2011,,,,,,,0|i0gs3z:,95984,,,,,,,,,,"01/Dec/09 18:38;pkamath;Pig doesn't handle partial schemas well - the fix for this issue will depend on how we want to treat unknown schemas. I did verify that this works when the schema specified is complete:

{code}
A = LOAD 'sample' using PigStorage() as (first:chararray, second:chararray, ladder:bag{t:tuple(x:int)});
B = FOREACH A GENERATE first,FLATTEN(ladder) as third,second;
C = GROUP B by (first,third);
describe C;
{code}

Here's the output:
C: {group: (first: chararray,third: int),B: {first: chararray,third: int,second: chararray}}",01/Dec/09 19:22;olgan;Moving out of 0.6.0 release. The right way to run this query is to specify the complete schema for the bag. We are not sure how we should be dealing with partial schemas and need to figure out the overall strategy before fixing individual issues.,"20/Sep/10 18:23;alangates;In the example above, the user specified that he expects two fields to come out of the flatten of ladder.  This seems equivalent to saying A = load 'ladder' as (third, second).  So I propose that when users give field names (and possibly types) in an AS that is attached to a flatten Pig takes that to be the schema of the flattened data.","05/Nov/10 21:50;daijy;In current trunk, the schema for B becomes:
B: {first: chararray,third: bytearray,second: chararray}

The alias for FLATTEN(ladder) is right, but we need to decide whether to mandate the type for ""third"" as bytearray, or the entire schema for B is unknown.","06/Nov/10 00:06;alangates;Daniel, I don't understand the choice here.  I think we agreed that if the user specifies (third, second) as the schema then we take that to mean there are two bytearray fields and we project them to guarantee this.  So

{code}
B = FOREACH A GENERATE first,FLATTEN(ladder) as third,second;
{code}

will now be equivalent to
{code}
Bprime = FOREACH A GENERATE first,FLATTEN(ladder);
B = FOREACH Bprime GENERATE first, $1 as third, $2 as second;
{code}","24/Dec/10 02:29;daijy;In current trunk, 
{code}
Bprime = FOREACH A GENERATE first,FLATTEN(ladder);
B = FOREACH Bprime GENERATE first, $1 as third, $2 as second;
{code}
is equivalent to
{code}
B = FOREACH A GENERATE first,FLATTEN(ladder) as (third,second);
{code}
Which I think is right.",10/Jan/11 22:29;daijy;It is fixed on the current trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect map output key type in MultiQuery optimization,PIG-1108,12441642,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,ankur,ankur,25/Nov/09 05:52,24/Mar/10 22:15,14/Mar/19 03:05,01/Dec/09 20:22,0.6.0,,,,,,0.6.0,,,,,0,,,,"When trying to merge 2 split plans, one of which never progresses along an M/R boundary, PIG sets the map-output key type incorrectly resulting in the following error:-

java.io.IOException: Type mismatch in key from map: expected org.apache.pig.impl.io.NullableText, recieved org.apache.pig.impl.io.NullableTuple
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:807)
	at org.apache.hadoop.mapred.MapTask$OldOutputCollector.collect(MapTask.java:466)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:108)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:249)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:238)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:93)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)
	at org.apache.hadoop.mapred.Child.main(Child.java:159)

Here is a small script to be used a reproducible test case

rmf plan1
rmf plan2
A = LOAD 'data' USING PigStorage() as (a: int, b: chararray);
SPLIT A into plan1 IF (a>5), plan2 IF (a<5);
B = GROUP plan1 BY b;
C = FOREACH B {
              tmp = ORDER plan1 BY a desc;
              GENERATE FLATTEN(group) as b, tmp;
              };
D = FILTER C BY b is not null;
STORE D into 'plan1';
STORE plan2 into 'plan2';
",,,,,,,,,,,,,,,,,,,30/Nov/09 18:20;rding;PIG-1108.patch;https://issues.apache.org/jira/secure/attachment/12426434/PIG-1108.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-11-25 19:37:38.89,,,no_permission,,,,,,,,,,,,164631,,,,,Tue Dec 01 20:22:19 UTC 2009,,,,,,,0|i0gs27:,95976,,,,,,,,,,"25/Nov/09 19:37;rding;The problme is that the secondary key optimization doesn't work well with multiquery optimization where one splittee has secondary key and the other splittee has no secondary key.

The work around for now is to disable either multiquery optimization (with -M) or secondary key optimization (with -Dpig.exec.nosecondarykey=true).

To fix this, we can modify the multiquery optimizer (since now secondary key optimizer runs before the multiquery optimizer) to take into account the exsitence of secondary key optimization.

 ","25/Nov/09 21:46;olgan;How much work would it be to fix this?

Also, were you able to verify that disabling MQ works on 0.6.0 branch?","25/Nov/09 22:09;rding;Either disableing MQ optimization or disabling second key optimization works on 0.6.0 branch.

In the short term, the proposed solution is for MQ optimizer to not merge any MR job that is annotated with ""UseSecondaryKey"".  We need further investigation to see if there's any performance advantages for MQ optimizer to merge MR jobs with ""UseSecondaryKey"" annotation.   ","26/Nov/09 07:40;ankur;In my test run on 0.6.0 branch, disabling MQ did not work. Pig client logs showed that MQ was still kicking in and the mappers failed with the same error message as in description. It will be good if we can add few points about ""SecondaryKey"" here - http://wiki.apache.org/pig/PigMultiQueryPerformanceSpecification","30/Nov/09 18:20;rding;With this patch, the multiquery optimizer doesn't merge MR jobs that use secondary key. ","30/Nov/09 22:11;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426434/PIG-1108.patch
  against trunk revision 885465.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 4 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/67/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/67/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/67/console

This message is automatically generated.",01/Dec/09 19:52;olgan;+1. I will be committing this patch now to both trunk and branch,"01/Dec/09 20:22;olgan;patch committed to trunk and branch 0.6.0. Thanks, Richard!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigLineRecordReader bails out on an empty line for compressed data,PIG-1107,12441637,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ankit.modi,ankit.modi,ankit.modi,25/Nov/09 03:18,24/Mar/10 22:15,14/Mar/19 03:05,30/Nov/09 16:10,0.6.0,,,,,,0.6.0,,,,,0,,,,"PigLineRecordReader bails out with an exception when it encounters an empty line in a compressed file

java.lang.ArrayIndexOutOfBoundsException: -1
       at org.apache.pig.impl.io.PigLineRecordReader$LineReader.getNext(PigLineRecordReader.java:136)
        at org.apache.pig.impl.io.PigLineRecordReader.next(PigLineRecordReader.java:57)
        at org.apache.pig.builtin.PigStorage.getNext(PigStorage.java:121)
        at org.apache.pig.backend.executionengine.PigSlice.next(PigSlice.java:139)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.SliceWrapper$1.next(SliceWrapper.java:164)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.SliceWrapper$1.next(SliceWrapper.java:140)
        at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:192)
        at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:176)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)


",,,,,,,,,,,,,,,,,,,26/Nov/09 01:14;ankit.modi;pig_piglinerecordreader_bug.patch;https://issues.apache.org/jira/secure/attachment/12426168/pig_piglinerecordreader_bug.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-11-26 09:19:57.505,,,no_permission,,,,,,,,,,,,164630,,,,,Mon Nov 30 16:10:47 UTC 2009,,,,,,,0|i0gs1r:,95974,,,,,,,,,,26/Nov/09 01:14;ankit.modi;Submitting a small patch. It has 2 new unit tests for the patch applied.,"26/Nov/09 09:19;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426168/pig_piglinerecordreader_bug.patch
  against trunk revision 884235.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/57/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/57/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/57/console

This message is automatically generated.","30/Nov/09 14:43;olgan;Looks like this fix is specific to windows; however, we are seeing this issue on Linux. There must be more to this issue.",30/Nov/09 16:10;olgan;I was incorrect in my previous comment. The offending code is executed regardless of the OS. Committed the patch to the trunk and 0.6 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FR join should not spill,PIG-1106,12441632,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ankit.modi,olgan,olgan,25/Nov/09 01:26,14/May/10 06:46,14/Mar/19 03:05,14/Dec/09 23:33,,,,,,,0.7.0,,,,,0,,,,"Currently, the values for the replicated side of the data are placed in a spillable bag (POFRJoin near line 275). This does not make sense because the whole point of the optimization is that the data on one side fits into memory. We already have a non-spillable bag implemented (NonSpillableDataBag.java) and we need to change FRJoin code to use it. And of course need to do lots of testing to make sure that we don't spill but die instead when we run out of memory",,,,,,,,,,,,,,,,,,,11/Dec/09 12:52;ankit.modi;frjoin-nonspill.patch;https://issues.apache.org/jira/secure/attachment/12427716/frjoin-nonspill.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-12-11 12:52:08.389,,,no_permission,,,,,,,,,,,,164629,,,,,Mon Dec 14 23:33:44 UTC 2009,,,,,,,0|i0gs1b:,95972,,,,,,,,,,"11/Dec/09 12:52;ankit.modi;This patch does not have any tests. Creating a test would be creating a big file about 250 MB and testing it.

I have ran some tests in similar fashion.
","11/Dec/09 13:13;ankit.modi;Tests I ran were using two files

file format
f1: random chararray(100)
f2: random int

leftside file contained 100 tuples and right side file contain 3million tuples.

Code
{noformat}
A = load 'leftsidefrjoin.txt' as ( key, value);
B = load 'rightsidefrjoin.txt' as (key, value);
C = join A by key left, B by key using ""repl"";
--- Fragmented input and replicated input
store C into 'output';
{noformat}

This generated following error
{noformat}
FATAL org.apache.hadoop.mapred.TaskTracker: Error running child : java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.util.ArrayList.<init>(ArrayList.java:112)
	at org.apache.pig.data.DefaultTuple.<init>(DefaultTuple.java:63)
	at org.apache.pig.data.DefaultTupleFactory.newTuple(DefaultTupleFactory.java:35)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.constructLROutput(POLocalRearrange.java:369)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNext(POLocalRearrange.java:288)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin.setUpHashMap(POFRJoin.java:351)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin.getNext(POFRJoin.java:211)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:250)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:241)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.map(PigMapOnly.java:65)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)
	at org.apache.hadoop.mapred.Child.main(Child.java:170)
{noformat}

I ran the same job with same records on left hand side and 100K records on right hand side. The job completed successfully.",11/Dec/09 13:14;ankit.modi;This patch does not have any unit tests.,"12/Dec/09 00:36;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427716/frjoin-nonspill.patch
  against trunk revision 889346.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/118/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/118/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/118/console

This message is automatically generated.","12/Dec/09 00:47;olgan;Test failures are not due to this patch. Also, I don't believe it is easy to test with an automatic test but I believe Ankit tested it manually.

I will review the code and run test-commit + FRJoin tests before committing the patch.","14/Dec/09 23:33;olgan;patch committed. Thanks, Ankit!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
COUNT_STAR accumulate interface implementation cases failure,PIG-1105,12441629,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sriranjan,thejas,thejas,25/Nov/09 01:00,24/Mar/10 22:15,14/Mar/19 03:05,06/Dec/09 23:47,,,,,,,0.6.0,,impl,,,0,,,,"COUNT_STAR.accumulate is calling sum() which is supposed to be used by intermediate and final parts of algebraic interface.
",,,,,,,,,,,,,,,,,,,25/Nov/09 01:05;thejas;PIG-1105.1.patch;https://issues.apache.org/jira/secure/attachment/12426039/PIG-1105.1.patch,05/Dec/09 01:27;sriranjan;PIG-1105.2.patch;https://issues.apache.org/jira/secure/attachment/12427014/PIG-1105.2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-12-04 17:55:16.021,,,no_permission,,,,,,,,,,,,164628,,,,,Sun Dec 06 23:47:18 UTC 2009,,,,,,,0|i0gs0v:,95970,,,,,,,,,,"25/Nov/09 01:01;thejas;Stack trace from failed job -

org.apache.pig.backend.executionengine.ExecException: ERROR 2106: Error while computing min in COUNT_STAR
        at org.apache.pig.builtin.COUNT_STAR.accumulate(COUNT_STAR.java:145)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:204)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:293)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:358)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:277)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:260)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:237)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.runPipeline(PigMapReduce.java:423)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.processOnePackageOutput(PigMapReduce.java:391)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:371)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:239)
        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:463)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:411)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:215)
Caused by: java.lang.RuntimeException: AccumulativeBag does not support size() operation
        at org.apache.pig.data.AccumulativeBag.size(AccumulativeBag.java:71)
        at org.apache.pig.builtin.COUNT_STAR.accumulate(COUNT_STAR.java:139)
        ... 13 more
",25/Nov/09 01:05;thejas;Test case needs to be added to this patch. ,"04/Dec/09 17:55;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426887/PIG-1105.patch
  against trunk revision 887290.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/91/console

This message is automatically generated.",05/Dec/09 01:25;sriranjan;Cancelling since the patch does not have all the changes.,"05/Dec/09 15:09;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427014/PIG-1105.2.patch
  against trunk revision 887401.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/96/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/96/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/96/console

This message is automatically generated.","06/Dec/09 23:47;olgan;committed patch to trunk an 0.6 branch. Thanks, Thejas and Sri.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig parser does not recognize its own data type in LIMIT statement,PIG-1101,12441357,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,ashutoshc,viraj,viraj,21/Nov/09 02:28,14/May/10 06:46,14/Mar/19 03:05,24/Nov/09 21:07,0.6.0,,,,,,0.7.0,,impl,,,0,,,,"I have a Pig script in which I specify the number of records to limit as a long type. 

{code}
A = LOAD '/user/viraj/echo.txt' AS (txt:chararray);

B = LIMIT A 10L;

DUMP B;
{code}

I get a parser error:

2009-11-21 02:25:51,100 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Encountered "" <LONGINTEGER> ""10L """" at line 3, column 13.
Was expecting:
    <INTEGER> ...
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.generateParseException(QueryParser.java:8963)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.jj_consume_token(QueryParser.java:8839)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.LimitClause(QueryParser.java:1656)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:1280)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:893)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:682)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:63)
        at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1017)

In fact 10L seems to work in the foreach generate construct.

Viraj    

",,,,,,,,,,,,,,,,,,,21/Nov/09 03:38;ashutoshc;pig-1101.patch;https://issues.apache.org/jira/secure/attachment/12425702/pig-1101.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-11-21 03:38:03.663,,,no_permission,,,,,,,,,,,,49759,,,,,Tue Nov 24 21:07:59 UTC 2009,,,,,,,0|i0grzb:,95963,,,,,,,,,,21/Nov/09 03:38;ashutoshc;Simple fix to accept either long or int in limit clause. Test case included.,"21/Nov/09 07:24;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12425702/pig-1101.patch
  against trunk revision 882818.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/164/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/164/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/164/console

This message is automatically generated.",21/Nov/09 15:18;ashutoshc;Test failure seems to be hudson's quirks. Same test passes on my local machine. This patch is ready for review.,23/Nov/09 22:28;alangates;I'll review this patch.,24/Nov/09 21:07;alangates;Patch checked in.  Thanks Ashutosh for fixing this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[zebra] version on APACHE trunk should be 0.7.0 to be in pace with PIG,PIG-1099,12441019,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,yanz,yanz,yanz,18/Nov/09 06:13,14/May/10 06:45,14/Mar/19 03:05,19/Nov/09 22:37,,,,,,,0.7.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,19/Nov/09 02:04;yanz;PIG_1099.patch;https://issues.apache.org/jira/secure/attachment/12425431/PIG_1099.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-11-19 06:19:33.082,,,no_permission,,,,,,,,,,,,164623,,,,,Thu Nov 19 22:37:37 UTC 2009,,,,,,,0|i0gryf:,95959,,,,,,,,,,"19/Nov/09 06:19;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12425431/PIG_1099.patch
  against trunk revision 881937.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/161/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/161/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/161/console

This message is automatically generated.",19/Nov/09 15:54;yanz;This is a trivial zebra jar version number change intended for APACHE trunk. No test is needed.,19/Nov/09 17:25;chaow;Patch reviewed +1,19/Nov/09 22:37;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[zebra] Schema support of anonymous fields in COLECTION fails,PIG-1095,12440902,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,yanz,yanz,yanz,17/Nov/09 05:12,24/Mar/10 22:15,14/Mar/19 03:05,25/Nov/09 19:38,,,,,,,0.6.0,0.7.0,,,,0,,,,The schema parser fails on schemas of COLLECTION columns like c:collection(int).,,,,,,,,,,,,,,,,,,,23/Nov/09 22:46;yanz;PIG-1095.patch;https://issues.apache.org/jira/secure/attachment/12425897/PIG-1095.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-11-24 05:30:22.647,,,no_permission,,,,,,,,,,,,164620,,,,,Thu Nov 26 01:09:19 UTC 2009,,,,,,,0|i0grwv:,95952,,,,,,,,,,20/Nov/09 20:03;yanz;This pacth is also targeted for the 0.6 release so it needs to be on the 0.6 branch too. ,"24/Nov/09 05:30;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12425897/PIG-1095.patch
  against trunk revision 883515.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 5 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/53/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/53/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/53/console

This message is automatically generated.","25/Nov/09 01:20;gauravj;
+1

Ideally, specific references/implementations should not be made in generic concepts. In this case I did not like the idea of handling projection specially  in Schema class.

However, given the dev explanation, I am giving a +1

","25/Nov/09 18:46;yanz;There is no generic concepts or implementations in terms of schema. All projection needs is just a different way to construct a  schema. After construction, the schema behaves exactly the same with no sepcial methods for projection.
The difference in construction is because that projection allows for duplicate column names, and null names. In other words, the restriction is a bit relaxed at construction.",25/Nov/09 18:51;yanz;Note that this patch does not change any class design at all on schema as used by storage or projection.,25/Nov/09 19:38;alangates;Patch checked in.,26/Nov/09 01:09;alangates;Patch checked into 0.6 branch as well.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig.properties file is missing from distributions,PIG-1093,12440646,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,13/Nov/09 18:27,14/May/10 06:45,14/Mar/19 03:05,05/Mar/10 01:34,0.5.0,0.6.0,,,,,0.7.0,,build,,,0,,,,pig.properties (in fact the entire conf directory) is not included in the jars distributed as part of the 0.5 release.,,,,,,,,,,,,,,,,,,,18/Feb/10 19:06;alangates;PIG-1093.patch;https://issues.apache.org/jira/secure/attachment/12436235/PIG-1093.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-02-18 23:33:55.667,,,no_permission,,,,,,,,,,,,164618,,,,,Fri Mar 05 01:34:46 UTC 2010,,,,,,,0|i0grw7:,95949,,,,,,,,,,"13/Nov/09 18:28;alangates;This also affects the 0.6 release, and should be repaired before that release.",18/Feb/10 19:08;alangates;Attached a patch with pig.properties file.  Also removed a few old Yahoo specific entries that were left over in pig.properties.  This patch doesn't include any new tests because this doesn't by default get included in the test runs anyway.,"18/Feb/10 23:33;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12436235/PIG-1093.patch
  against trunk revision 911219.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 493 release audit warnings (more than the trunk's current 492 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/209/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/209/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/209/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/209/console

This message is automatically generated.",18/Feb/10 23:43;olgan;+1,18/Feb/10 23:56;dvryaboy;Can we add a commented out pig.logfile property to the default pig.properties file? ,"05/Mar/10 01:34;alangates;Patch checked in, with Dmitriy's requested addition.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[zebra] Exception when load with projection of map keys on a map column that is not map split ,PIG-1091,12440637,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,yanz,yanz,yanz,13/Nov/09 17:49,24/Mar/10 22:15,14/Mar/19 03:05,21/Nov/09 01:30,,,,,,,0.6.0,0.7.0,,,,0,,,,"With schema of ""f1:string, f2:map"", storage info of ""[f1]; [f2]"", a projection of ""f2#{a}"" will see exception.",,,,,,,,,,,,,,,,,,,19/Nov/09 22:51;yanz;PIG-1091.patch;https://issues.apache.org/jira/secure/attachment/12425542/PIG-1091.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-11-20 01:53:17.622,,,no_permission,,,,,,,,,,,,164617,,,,,Mon Nov 23 19:11:15 UTC 2009,,,,,,,0|i0grvb:,95945,,,,,,,,,,20/Nov/09 01:53;chaow;Patch reviewed. +1,"20/Nov/09 06:03;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12425542/PIG-1091.patch
  against trunk revision 882340.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/162/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/162/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/162/console

This message is automatically generated.",20/Nov/09 20:00;yanz;This pacth is also targeted for the 0.6 release so it needs to be on the 0.6 branch too. ,21/Nov/09 01:30;alangates;Patch checked in.,23/Nov/09 19:11;alangates;Patch applied to 0.6  branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nested sort by * throw exception,PIG-1086,12440456,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,daijy,daijy,11/Nov/09 21:56,14/May/10 06:45,14/Mar/19 03:05,11/Dec/09 23:32,0.5.0,,,,,,0.7.0,,,,,0,,,,"The following script fail:
A = load '1.txt' as (a0, a1, a2);
B = group A by a0;
C = foreach B { D = order A by *; generate group, D;};
explain C;

Here is the stack:
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.ArrayList.get(ArrayList.java:324)
        at org.apache.pig.impl.logicalLayer.schema.Schema.getField(Schema.java:752)
        at org.apache.pig.impl.logicalLayer.LOSort.getSortInfo(LOSort.java:332)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.LogToPhyTranslationVisitor.visit(LogToPhyTranslationVisitor.java:1365)
        at org.apache.pig.impl.logicalLayer.LOSort.visit(LOSort.java:176)
        at org.apache.pig.impl.logicalLayer.LOSort.visit(LOSort.java:43)
        at org.apache.pig.impl.plan.DependencyOrderWalkerWOSeenChk.walk(DependencyOrderWalkerWOSeenChk.java:69)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.LogToPhyTranslationVisitor.visit(LogToPhyTranslationVisitor.java:1274)
        at org.apache.pig.impl.logicalLayer.LOForEach.visit(LOForEach.java:130)
        at org.apache.pig.impl.logicalLayer.LOForEach.visit(LOForEach.java:45)
        at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:69)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:234)
        at org.apache.pig.PigServer.compilePp(PigServer.java:864)
        at org.apache.pig.PigServer.explain(PigServer.java:583)
        ... 8 more",,,,,,,,,,,,,,,,,,,04/Dec/09 18:48;rding;PIG-1086.patch;https://issues.apache.org/jira/secure/attachment/12426934/PIG-1086.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-12-02 20:41:22.957,,,no_permission,,,,,,,,,,,,164612,Reviewed,,,,Fri Dec 11 23:32:26 UTC 2009,,,,,,,0|i0grt3:,95935,,,,,,,,,,"02/Dec/09 20:41;rding;The script works if the input has no schema:

{code}
A = load '1.txt';
B = group A by a0;
C = foreach B { D = order A by *; generate group, D;};
explain C;
{code}","03/Dec/09 16:37;rding;The above script has a typo. It should be

{code}
A = load '1.txt';
B = group A by $0;
C = foreach B { D = order A by *; generate group, D;};
explain C;
{code}",04/Dec/09 18:48;rding;The cause of this bug is that the schema of the project (*) operator isn't set correctly before applying ProjectStarTranslator and therefore project (*) operator isn't  translated to a list of projection operators. This causes the inconsistency (and the exception). This patch provides a fix.,"05/Dec/09 01:46;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426934/PIG-1086.patch
  against trunk revision 887318.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/93/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/93/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/93/console

This message is automatically generated.",10/Dec/09 03:10;daijy;+1,11/Dec/09 23:32;daijy;Patch committed. Thanks Richard!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Pig CookBook documentation ""Take Advantage of Join Optimization"" additions:Merge and Skewed Join",PIG-1084,12440363,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,viraj,viraj,11/Nov/09 00:44,24/Mar/10 22:15,14/Mar/19 03:05,04/Dec/09 00:52,0.6.0,,,,,,0.6.0,,documentation,,,0,,,,"Hi all,
 We have a host of Join optimizations that have been implemented recently in Pig to improve performance. These include:

http://hadoop.apache.org/pig/docs/r0.5.0/piglatin_reference.html#JOIN

1) Merge Join
2) Skewed Join

It would be nice to mention the Merge Join and Skewed join in the following section on the PigCookBook

http://hadoop.apache.org/pig/docs/r0.5.0/cookbook.html#Take+Advantage+of+Join+Optimization

Can we update this release 0.6??

Thanks
Viraj",,,,,,,,,,,,,,,,,,,30/Nov/09 22:02;chandec;cookbook.patch;https://issues.apache.org/jira/secure/attachment/12426453/cookbook.patch,03/Dec/09 23:22;chandec;pig-6.patch;https://issues.apache.org/jira/secure/attachment/12426836/pig-6.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-11-30 22:01:25.962,,,no_permission,,,,,,,,,,,,164610,,,,,Fri Dec 04 00:52:20 UTC 2009,,,,,,,0|i0grs7:,95931,,,,,,,,,,"30/Nov/09 22:01;chandec;Patch file.
Updated Cookbook: Take Advantageof Join Optimizations","30/Nov/09 22:02;chandec;Patch file.

Updated Cookbook: Take Advantage of Join Optimizations","30/Nov/09 22:04;chandec;Apply this patch to the trunk: http://svn.apache.org/repos/asf/hadoop/pig/trunk


No new test code required; changes to documentation only.","01/Dec/09 01:54;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426453/cookbook.patch
  against trunk revision 885465.

    +1 @author.  The patch does not contain any @author tags.

    +0 tests included.  The patch appears to be a documentation patch that doesn't require tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/68/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/68/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/68/console

This message is automatically generated.","01/Dec/09 20:45;olgan;Corinne, 

We also need a reference to Accumulator interface. SImilarly how we say about the Algebraic interface.","01/Dec/09 21:59;chandec;
Here's what we have:

(1)  Cookbook: Make your UDFs Algebraic

(2) UDF: Accumulator Interface


Which manual/section does the link belong in? (where do I add the link)

Which manual/section does the link point to? (the UDF manual, Accumulator Interface?)","03/Dec/09 17:32;olgan;Both Accumulator and Alebraic UDFs are described in the UDF manual. They are both intended to improve performance and as such both need to be mentioned in the cookbook and then refered to the manual. Something like:

If your UDF can't be made Algebraic but is able to deal with getting input in chunks rather than all at once, consider implementing Aggregator interface [link to UDF manual here] to reduce the amount of memory used by your script. Even if your function is Algebraic but can be used on conjunction with Accumulator functions, it needs to implement the Accumulator interface.","03/Dec/09 23:22;chandec;This patch includes fixes for 3 JIRAs and misc changes.

(1) PIG-1084 (this JIRA)

• Cookbook -  add new section ""Implement the Aggregator Interface""; updated 
""Make Your UDFs Algebraic"" section (changed link to point to Aggregate Functions (where algebraic functions are discussed), other minor edits)

• UDF Guide  - updated ""Accumulator Interface"" section (minor edits)

(2) PIG-1123

• Pig Latin Reference Guide - Updated set command (added default_parallel)

• Cookbook - Updated ""Use the PARALLEL Keyword"" section (added link to set command)


(3) PIG-1081

• Cookbook - Updated the ""Use the PARALLEL Keyword"" section (updated description, checked and then added links to operators that use PARALLEL)

(4) Misc changes

• Pig Latin Users Guide - removed the Zebra section (moved to the Zebra docs)

• Pig Latin Reference Guide - minor edits, removed out-dated materials

------------
Apply this patch to the Pig trunk: http://svn.apache.org/repos/asf/hadoop/pig/trunk

-----------
Note: No new test code required; changes to documentation only.



","04/Dec/09 00:52;olgan;patch committed to the trunk and 0.6 branch. Thanks, Corinne!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigStorage may miss records when loading a file,PIG-1080,12440337,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,rding,rding,10/Nov/09 21:34,24/Mar/10 22:15,14/Mar/19 03:05,11/Nov/09 18:29,0.6.0,,,,,,0.6.0,,,,,0,,,,"When a file is assigned to multiple mappers (one block per mapper), the blocks may not end at the exact record boundary. Special care is taken to ensure that all records are loaded by mappers (and exactly once), even for records that cross the block boundary. 

The PigStorage, however, doesn't correctly handle the case where a block ends at exactly record boundary and results in missing records.

 ",,,,,,,,,,,,,,,,,,,10/Nov/09 23:25;rding;PIG-1080.patch;https://issues.apache.org/jira/secure/attachment/12424531/PIG-1080.patch,10/Nov/09 22:14;rding;PIG-1080.patch;https://issues.apache.org/jira/secure/attachment/12424515/PIG-1080.patch,10/Nov/09 21:35;rding;PIG-1080.patch;https://issues.apache.org/jira/secure/attachment/12424510/PIG-1080.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-11-10 22:13:40.081,,,no_permission,,,,,,,,,,,,164606,,,,,Wed Nov 11 18:29:03 UTC 2009,,,,,,,0|i0grq7:,95922,,,,,,,,,,10/Nov/09 21:35;rding;This patch fixes the problem.,"10/Nov/09 22:13;alangates;To be clear, this bug affects only trunk code, not any released version of Pig.  It is a result of the switch to using LineRecordReader, (PIG-960).",10/Nov/09 23:25;rding;This patch excludes the bzip and gzip files from the change.,10/Nov/09 23:27;olgan;+1. Last patch looks good. Will commit once the automated test comes back,"11/Nov/09 00:56;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12424510/PIG-1080.patch
  against trunk revision 834285.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/146/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/146/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/146/console

This message is automatically generated.","11/Nov/09 01:35;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12424515/PIG-1080.patch
  against trunk revision 834285.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/42/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/42/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/42/console

This message is automatically generated.","11/Nov/09 04:23;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12424531/PIG-1080.patch
  against trunk revision 834285.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/147/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/147/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/147/console

This message is automatically generated.","11/Nov/09 18:29;olgan;patch committed, thanks Richard!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Modify merge join to use distributed cache to maintain the index,PIG-1079,12440262,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,sriranjan,sriranjan,10/Nov/09 08:16,14/May/10 06:45,14/Mar/19 03:05,24/Feb/10 18:15,,,,,,,0.7.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,23/Feb/10 18:28;rding;PIG-1079.patch;https://issues.apache.org/jira/secure/attachment/12436736/PIG-1079.patch,22/Feb/10 23:28;rding;PIG-1079.patch;https://issues.apache.org/jira/secure/attachment/12436645/PIG-1079.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-11-10 19:55:44.791,,,no_permission,,,,,,,,,,,,164605,,,,,Wed Feb 24 18:15:10 UTC 2010,,,,,,,0|i0grpr:,95920,,,,,,,,,,10/Nov/09 19:55;ashutoshc;+1 for the direction as it will help reducing the contention for index file. Another option could be to write out the index file with high replication factor. ,22/Feb/10 23:28;rding;Attached patch uses Hadoop DistributedCache for distribution of merge join index file.,"23/Feb/10 06:37;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12436645/PIG-1079.patch
  against trunk revision 915079.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/215/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/215/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/215/console

This message is automatically generated.",23/Feb/10 18:28;rding;New patch to remove the javadoc warning.,23/Feb/10 19:36;pkamath;+1,"24/Feb/10 04:15;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12436736/PIG-1079.patch
  against trunk revision 915079.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/213/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/213/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/213/console

This message is automatically generated.","24/Feb/10 18:15;rding;Patch committed (No new unit test is added, just use the existing merge join tests).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[zebra] merge join with empty table failed,PIG-1078,12440231,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yanz,jing1234,jing1234,10/Nov/09 00:52,24/Mar/10 22:15,14/Mar/19 03:05,24/Nov/09 17:55,,,,,,,0.6.0,0.7.0,,,,0,,,,"Got indexOutOfBound exception. 

Here is the pig script:
register /grid/0/dev/hadoopqa/jars/zebra.jar;
--a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);

--a2 = load 'empty.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
--dump a1;

--a1order = order a1 by a;
--a2order = order a2 by a;


--store a1order into 'a1' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
--store a2order into 'empty' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');

rec1 = load 'a1' using org.apache.hadoop.zebra.pig.TableLoader();
rec2 = load 'empty' using org.apache.hadoop.zebra.pig.TableLoader();
joina = join rec1 by a, rec2 by a using ""merge"" ;
dump joina;

======
please note that table ""a1"" and ""empty"" are created correctly. 

Here is the stack trace:
Backend error message
---------------------
java.lang.ArrayIndexOutOfBoundsException: 0
        at org.apache.hadoop.zebra.mapred.TableInputFormat.getTableRecordReader(TableInputFormat.java:478)
        at org.apache.hadoop.zebra.pig.TableLoader.bindTo(TableLoader.java:166)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.seekInRightStream(POMergeJoin.java:400)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.getNext(POMergeJoin.java:181)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:247)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:238)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.map(PigMapOnly.java:65)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)
        at org.apache.hadoop.mapred.Child.main(Child.java:159)

Pig Stack Trace
---------------
ERROR 6015: During execution, encountered a Hadoop error.

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias joina
        at org.apache.pig.PigServer.openIterator(PigServer.java:481)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:539)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:241)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:168)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:144)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:89)
        at org.apache.pig.Main.main(Main.java:386)
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 6015: During execution, encountered a Hadoop error.
        at .apache.hadoop.zebra.mapred.TableInputFormat.getTableRecordReader(TableInputFormat.java:478)
        at .apache.hadoop.zebra.pig.TableLoader.bindTo(TableLoader.java:166)
        at .apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.seekInRightStream(POMergeJoin.java:400)
        at .apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.getNext(POMergeJoin.java:181)
        at .apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:247)
        at .apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:238)
        at .apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.map(PigMapOnly.java:65)
        at .apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
        at .apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)
        at .apache.hadoop.mapred.MapTask.run(MapTask.java:307)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
        ... 10 more
================================================================================
                                     

",,,,,,,,,,,,,,,,,,,20/Nov/09 17:15;yanz;PIG-1078.patch;https://issues.apache.org/jira/secure/attachment/12425632/PIG-1078.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-11-10 07:56:27.93,,,no_permission,,,,,,,,,,,,164604,,,,,Tue Nov 24 23:33:25 UTC 2009,,,,,,,0|i0grpb:,95918,,,,,,,,,,"10/Nov/09 07:56;ashutoshc;This seems to be related to Zebra. Jing, do you think it has to do with merge join implementation of Pig ?",20/Nov/09 20:00;yanz;This pacth is also targeted for the 0.6 release so it needs to be on the 0.6 branch too. ,"20/Nov/09 21:02;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12425632/PIG-1078.patch
  against trunk revision 882340.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/163/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/163/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/163/console

This message is automatically generated.",23/Nov/09 22:10;chaow;Patch reviewed. +1,24/Nov/09 17:55;alangates;Patch checked in.,24/Nov/09 23:33;alangates;Patch checked into 0.6 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error in Cogroup when key fields types don't match,PIG-1075,12440000,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,ankur,ankur,06/Nov/09 07:01,14/May/10 06:45,14/Mar/19 03:05,17/Dec/09 00:50,0.5.0,,,,,,0.7.0,,,,,0,,,,"When Cogrouping 2 relations on multiple key fields, pig throws an error if the corresponding types don't match. 
Consider the following script:-
A = LOAD 'data' USING PigStorage() as (a:chararray, b:int, c:int);
B = LOAD 'data' USING PigStorage() as (a:chararray, b:chararray, c:int);
C = CoGROUP A BY (a,b,c), B BY (a,b,c);
D = FOREACH C GENERATE FLATTEN(A), FLATTEN(B);
describe D;
dump D;

The complete stack trace of the error thrown is

Pig Stack Trace
---------------
ERROR 1051: Cannot cast to Unknown

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1001: Unable to describe schema for alias D
        at org.apache.pig.PigServer.dumpSchema(PigServer.java:436)
        at org.apache.pig.tools.grunt.GruntParser.processDescribe(GruntParser.java:233)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:253)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:168)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:144)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:89)
        at org.apache.pig.Main.main(Main.java:397)
Caused by: org.apache.pig.impl.plan.PlanValidationException: ERROR 0: An unexpected exception caused the validation to stop
        at org.apache.pig.impl.plan.PlanValidator.validateSkipCollectException(PlanValidator.java:104)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingValidator.validate(TypeCheckingValidator.java:40)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingValidator.validate(TypeCheckingValidator.java:30)
        at org.apache.pig.impl.logicalLayer.validators.LogicalPlanValidationExecutor.validate(LogicalPlanValidationExecutor.java:83)
        at org.apache.pig.PigServer.compileLp(PigServer.java:821)
        at org.apache.pig.PigServer.dumpSchema(PigServer.java:428)
        ... 6 more
Caused by: org.apache.pig.impl.logicalLayer.validators.TypeCheckerException: ERROR 1060: Cannot resolve COGroup output schema
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.visit(TypeCheckingVisitor.java:2463)
        at org.apache.pig.impl.logicalLayer.LOCogroup.visit(LOCogroup.java:372)
        at org.apache.pig.impl.logicalLayer.LOCogroup.visit(LOCogroup.java:45)
        at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:69)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.impl.plan.PlanValidator.validateSkipCollectException(PlanValidator.java:101)
        ... 11 more
Caused by: org.apache.pig.impl.logicalLayer.validators.TypeCheckerException: ERROR 1051: Cannot cast to Unknown
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.insertAtomicCastForCOGroupInnerPlan(TypeCheckingVisitor.java:2552)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.visit(TypeCheckingVisitor.java:2451)
        ... 16 more

The error message does not help the user in identifying the issue clearly especially if the pig script is large and complex.
",,,,,,,,,,,,,,,,,,,15/Dec/09 01:24;rding;PIG-1075.patch;https://issues.apache.org/jira/secure/attachment/12427991/PIG-1075.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-12-15 01:24:13.994,,,no_permission,,,,,,,,,,,,164601,,,,,Thu Dec 17 00:50:30 UTC 2009,,,,,,,0|i0grnz:,95912,,,,,,,,,,06/Nov/09 07:03;ankur;Pig should throw an error message that better identifies the cause of the problem.,"15/Dec/09 01:24;rding;This patch moves the error up to the parser and gives a better error message for cogroup statement with incompatible group types:

Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1110: Cogroup column 1 has incompatible types: chararray versus int
        at org.apache.pig.impl.logicalLayer.LOCogroup.getTupleGroupBySchema(LOCogroup.java:499)
        at org.apache.pig.impl.logicalLayer.LOCogroup.getSchema(LOCogroup.java:325)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:779)","15/Dec/09 21:09;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427991/PIG-1075.patch
  against trunk revision 890596.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/124/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/124/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/124/console

This message is automatically generated.",17/Dec/09 00:03;olgan;+1 on the change. will be committing it shortly.,17/Dec/09 00:50;olgan;patch committed. thanks Richard!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Zebra store function should allow '::' in column names in output schema,PIG-1074,12439999,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yanz,pkamath,pkamath,06/Nov/09 04:56,24/Mar/10 22:15,14/Mar/19 03:05,01/Dec/09 14:38,,,,,,,0.6.0,0.7.0,,,,0,,,,"the following script fails: 

 {noformat}

            a = load '/zebra/singlefile/studenttab10k' using org.apache.hadoop.zebra.pig.TableLoader() as (name, age, gpa);

                        b = load '/zebra/singlefile/votertab10k' using org.apache.hadoop.zebra.pig.TableLoader() as (name, age, registration, contributions);

                        c = filter a by age < 20;

                        d = filter b by age < 20;

                        store c into '/user/pig/out//ZebraMultiQuery_30.out.1' using org.apache.hadoop.zebra.pig.TableStorer('');

                        store d into '/user/pig/out//ZebraMultiQuery_30.out.2' using org.apache.hadoop.zebra.pig.TableStorer('');

                        e = cogroup c by name, d by name;

                        f = foreach e generate flatten(c), flatten(d);

                        store f into '/user/pig//ZebraMultiQuery_30.out.3' using org.apache.hadoop.zebra.pig.TableStorer('');

{noformat}
Here the schema of f has names like c::name and it looks like zebra storefunc does not allow '::' in column name 

The stack trace is

 

ERROR 2997: Unable to recreate exception from backend error: java.io.IOException: ColumnGroup.Writer constructor failed : Partition constructor failed :Encountered "" "":"" "": """" at line 1, column 3.

 

",,,,,,,,,,,,,,,,,,,26/Nov/09 02:05;yanz;PIG-1074.patch;https://issues.apache.org/jira/secure/attachment/12426177/PIG-1074.patch,26/Nov/09 01:01;yanz;PIG-1074.patch;https://issues.apache.org/jira/secure/attachment/12426166/PIG-1074.patch,25/Nov/09 02:00;yanz;PIG-1074.patch;https://issues.apache.org/jira/secure/attachment/12426051/PIG-1074.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-11-20 20:03:42.888,,,no_permission,,,,,,,,,,,,164600,,,,,Tue Dec 01 18:33:25 UTC 2009,,,,,,,0|i0grnj:,95910,,,,,,,,,,20/Nov/09 20:03;yanz;This pacth is also targeted for the 0.6 release so it needs to be on the 0.6 branch too. ,"25/Nov/09 06:19;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426051/PIG-1074.patch
  against trunk revision 883903.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 7 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/54/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/54/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/54/console

This message is automatically generated.","26/Nov/09 00:42;gauravj;
Looks incorrect

<IDENTIFIER: ( <LETTER> )+ ( <DIGIT> | <LETTER> | <SPECIALCHAR> )* ( <SCOPEOP> )? ( <LETTER> )* ( <DIGIT> | <LETTER> | <SPECIALCHAR> )* >

Should be

<IDENTIFIER: ( <LETTER> )+ ( <DIGIT> | <LETTER> | <SPECIALCHAR> )* ( <SCOPEOP> )? ( <LETTER> )+ ( <DIGIT> | <LETTER> | <SPECIALCHAR> )* >

or

<IDENTIFIER: ( <LETTER> )+ ( <DIGIT> | <LETTER> | <SPECIALCHAR> )* ( <SCOPEOP> )? ( <DIGIT> | <LETTER> | <SPECIALCHAR> )+ >
",26/Nov/09 01:10;yanz;The syntax for an IDENTIFIER is now  ( <LETTER> )+ ( <DIGIT> | <LETTER> | <SPECIALCHAR> )* ( <SCOPEOP>  ( <LETTER> )+ ( <DIGIT> | <LETTER> | <SPECIALCHAR> )*)? ,26/Nov/09 01:12;gauravj;+1,"26/Nov/09 02:05;yanz;To allow for the the column names with ""::"" on disk to be further used in another COGROUP call before stored in Zebra, multiple ""::"" should be supported instead of a single one.
Therefore, the identifier syntax reads as ( <LETTER> )+ ( <DIGIT> | <LETTER> | <SPECIALCHAR> )* ( <SCOPEOP>  ( <LETTER> )+ ( <DIGIT> | <LETTER> | <SPECIALCHAR> )*)*","26/Nov/09 02:06;gauravj;
Question for Pig:


From the example in descrption, looks like column name can be 

c::name while storing in zebra table.

Can it be 

c::c::c::c::name?","26/Nov/09 02:10;gauravj;+1

With the latest patch,

it allow

c::c::c::c::d

but it does not allow

a::::a::::b::c

If that is needed by PIG,  then we need to revisit",26/Nov/09 02:11;yanz;A typo in my last comment: the identifier syntax reads as ( <LETTER> )+ ( <DIGIT> | <LETTER> | <SPECIALCHAR> )* ( <SCOPEOP>  ( <LETTER> )+ ( <DIGIT> | <LETTER> | <SPECIALCHAR> )*)*,"26/Nov/09 05:33;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426166/PIG-1074.patch
  against trunk revision 884235.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 7 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/56/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/56/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/56/console

This message is automatically generated.","26/Nov/09 06:07;yanz;Actually there was no typo. But somehow jira takes a ""* ) *"" as a highlighted ')'. It should read as A typo in my last comment: the identifier syntax reads as ( <LETTER> )+ ( <DIGIT> | <LETTER> | <SPECIALCHAR> )* ( <SCOPEOP> ( <LETTER> )+ ( <DIGIT> | <LETTER> | <SPECIALCHAR> ) *  ) *","26/Nov/09 06:19;yanz;MY last comment is still messed up.  the identifier syntax should reads as 
     ( <LETTER> )+ ( <DIGIT> | <LETTER> | <SPECIALCHAR> )  *  ( <SCOPEOP> ( <LETTER> )+ ( <DIGIT> | <LETTER> | <SPECIALCHAR> ) *  )  * 

","26/Nov/09 12:59;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426177/PIG-1074.patch
  against trunk revision 884235.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 7 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/58/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/58/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/58/console

This message is automatically generated.",01/Dec/09 14:38;alangates;Patch checked in.,01/Dec/09 18:33;alangates;Checked into 0.6 branch as well.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"COGROUP fails with 'Type mismatch in key from map: expected org.apache.pig.impl.io.NullableText, recieved org.apache.pig.impl.io.NullableTuple'",PIG-1068,12439573,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,voberoi,voberoi,30/Oct/09 20:47,24/Mar/10 22:15,14/Mar/19 03:05,03/Dec/09 22:57,0.4.0,,,,,,0.6.0,,,,,0,,,,"The COGROUP in the following script fails in its map:

{code}
logs = LOAD '$LOGS' USING PigStorage() AS (ts:int, id:chararray, command:chararray, comments:chararray);                                                                                                       
                                                                                                                                                                                                               
SPLIT logs INTO logins IF command == 'login', all_quits IF command == 'quit';                                                                                                                                  
                                                                                                                                                                                                               
-- Project login clients and count them by ID.                                                                                                                                                                 
login_info = FOREACH logins {                                                                                                                                                                                  
    GENERATE id as id,                                                                                                                                                                                         
    comments AS client;                                                                                                                                                                                        
};                                                                                                                                                                                                             
                                                                                                                                                                                                               
logins_grouped = GROUP login_info BY (id, client);                                                                                                                                                             
                                                                                                                                                                                                               
count_logins_by_client = FOREACH logins_grouped {                                                                                                                                                              
    generate group.id AS id, group.client AS client, COUNT($1) AS count;                                                                                                                                       
}                                                                                                                                                                                                              
                                                                                                                                                                                                               
-- Get the first quit.                                                                                                                                                                                         
all_quits_grouped = GROUP all_quits BY id;                                                                                                                                                                     
                                                                                                                                                                                                               
quits = FOREACH all_quits_grouped {                                                                                                                                                                            
    ordered = ORDER all_quits BY ts ASC;                                                                                                                                                                       
    last_quit = LIMIT ordered 1;                                                                                                                                                                               
    GENERATE FLATTEN(last_quit);                                                                                                                                                                               
}                                                                                                                                                                                                              
                                                                                                                                                                                                               
-- Now, group all the info together.                                                                                                                                                                           
joined_session_info = COGROUP quits BY id, count_logins_by_client BY id;                                                                                                                                       
                                                                                                                                                                                                               
DUMP joined_session_info;
{code}

Here's the stack trace:

{code}
java.io.IOException: Type mismatch in key from map: expected org.apache.pig.impl.io.NullableText, recieved org.apache.pig.impl.io.NullableTuple
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:415)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:108)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:229)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:93)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:227)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:157)
{code}",,,,,,,,,,,,,,,,,,,02/Dec/09 18:21;rding;PIG-1068.patch;https://issues.apache.org/jira/secure/attachment/12426691/PIG-1068.patch,30/Oct/09 20:49;voberoi;cogroup-bug.pig;https://issues.apache.org/jira/secure/attachment/12423719/cogroup-bug.pig,30/Oct/09 20:49;voberoi;log;https://issues.apache.org/jira/secure/attachment/12423718/log,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-12-01 01:17:32.961,,,no_permission,,,,,,,,,,,,164594,,,,,Thu Dec 03 22:57:09 UTC 2009,,,,,,,0|i0grl3:,95899,,,,,,,,,,30/Oct/09 20:49;voberoi;Attached the script and some sample data.,"01/Dec/09 01:17;rding;The cause of this bug is this: On the one hand, the value (as in key/value pairs) received by a reducer may not be the complete ""value"", It may have portions in the key. In this case, the real ""value"" is stitched together by the packager. On the other hand, MultiQuery optimizer merges the jobs with different map key types by wrapping the keys in tuples (so that the resulting job has tuple as common map key type). Unfortunately, the unwrapping the key happens in the demuxer (after packager) and the ""stitched up"" value isn't the expected value.  

The solution will be to move the Multiquery unwrapping logic from demuxer to packager.",02/Dec/09 18:21;rding;This patch fixed the problem by moving the unwrapping logic from demuxer to packager.,"02/Dec/09 22:27;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426691/PIG-1068.patch
  against trunk revision 886015.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/76/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/76/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/76/console

This message is automatically generated.",03/Dec/09 22:57;alangates;Patch checked in.  Thanks Richard.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
In-determinate behaviour of Union when there are 2 non-matching schema's,PIG-1065,12439479,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,viraj,viraj,30/Oct/09 03:05,04/Aug/11 00:34,14/Mar/19 03:05,24/Dec/10 01:33,0.6.0,,,,,,0.9.0,,,,,0,,,,"I have a script which first does a union of these schemas and then does a ORDER BY of this result.

{code}
f1 = LOAD '1.txt' as (key:chararray, v:chararray);
f2 = LOAD '2.txt' as (key:chararray);
u0 = UNION f1, f2;
describe u0;
dump u0;

u1 = ORDER u0 BY $0;
dump u1;
{code}

When I run in Map Reduce mode I get the following result:
$java -cp pig.jar:$HADOOP_HOME/conf org.apache.pig.Main broken.pig
====================
Schema for u0 unknown.
====================
(1,2)
(2,3)
(1)
(2)
====================
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias u1
        at org.apache.pig.PigServer.openIterator(PigServer.java:475)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:532)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:190)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:166)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:142)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:89)
        at org.apache.pig.Main.main(Main.java:397)
====================
Caused by: java.io.IOException: Type mismatch in key from map: expected org.apache.pig.impl.io.NullableBytesWritable, recieved org.apache.pig.impl.io.NullableText
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:415)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:108)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:251)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:240)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:93)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:227)
====================

When I run the same script in local mode I get a different result, as we know that local mode does not use any Hadoop Classes.
$java -cp pig.jar org.apache.pig.Main -x local broken.pig
====================
Schema for u0 unknown
====================
(1,2)
(1)
(2,3)
(2)
====================
(1,2)
(1)
(2,3)
(2)
====================

Here are some questions
1) Why do we allow union if the schemas do not match
2) Should we not print an error message/warning so that the user knows that this is not allowed or he can get unexpected results?

Viraj",,,,,,,,,,PIG-998,,PIG-1277,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-11-05 20:05:18.263,,,no_permission,,,,,,,,,,,,66339,,,,,Fri Dec 24 01:33:34 UTC 2010,,,,,,,0|i0grjr:,95893,,,,,,,,,,"05/Nov/09 20:05;pkamath;This is an instance of the problem of representing unknown schema with a null schema. If the schema of a relational operator is null, pig assumes the fields are of type bytearray which is incorrect. An unknown schema really means we don't know the types of the fields. In the above case, once pig determines that the two schemas have different sizes, it sets the schema of LOUnion to null (to represent unknown schema). Hence the order by expects the fields coming out of the union to be byte arrays but in reality the first field (which is the sort key above) is a chararray - this results in a runtime exception.

I propose that when either of the two inputs to a union have a schema we should error out if the two are incompatible and not continue. If the two inputs don't have a schema then we can proceed with null schema - thoughts?","05/Nov/09 20:33;thejas;Can this be allowed (in case of incompatible schemas as in description) - u0 = UNION f1, f2 as (key:chararray, v:chararray); ?
","05/Nov/09 21:36;pkamath;Currently pig parser does not allow specifying a schema for the union. If we do want to allow it there are a few details which arise:
1) From what I know, currently pig allowing specifying row schemas only in load statements. Is this restriction by design? ForEach allows only to give different schemas at individual field level and even there the type has to match if I recollect right.
2)  If the input schemas ae unequal in size, should the specified union schema size strictly be MAX of the input schema sizes with nulls being projected for missing columns in the input with the smaller schema? So a specified schema of size < MAX(size of input schemas) will not be allowed?
3) Can the specified union schema have different types (castable) than the result of merging the two input schemas - for example if after merging the two input schemas if the first input has int and if specified schema has long? What about demotions like the merged schema being long and specified one being int - would those be disallowed? - I suppose we just allow whatever is allowed in casts","06/Nov/09 00:25;sms;Answer to Question 1: Pig 1.0 had that syntax and it was retained for backward compatibility. Paolo suggested that for uniformity, the 'AS' clause for the load statements should be extended to all relational operators. Gradually, the column aliasing in the foreach should be removed from the documentation and eventually removed from the language.","10/Nov/09 17:51;alangates;As originally defined UNION does allow two inputs to be of different schema, the result of which should have no schema.  So the error here is really that the runtime system doesn't adapt to the unexpected type.  I'm not sure how useful this functionality is, but it is in keeping with the original spirit of Pig's no schema required, so I'm not inclined to fix it.  At some point in the future we should consider how general we want to be in these cases, as there is significant cost for it.  But let's decide it as a whole for the language rather than piecemeal.

I would support the addition of a conforming union, (I have no idea what to call it) that requires that each input have either the same schema or that the two schemas be somehow compatible.  It would then handle type promotion and adding nulls for missing columns.

Finally, in response to Santhosh's comment on AS, I think whether we extend AS to statements beyond LOAD depends on choices we make above about schemas or lack thereof.  But removing it from the foreach isn't easy.  Consider the following:

{code}
B = foreach A generate $0, flatten($1), $2, flatten($3) AS (a:int, b:int, c:float, d:long, e:chararray);
{code}

If we don't have a schema for $1 and $3, we don't know whether $2 should end up being c:float or d:long.
",10/Nov/09 18:07;sms;The schema will then correspond to the prefix as it is implemented today. For example if the AS statement is define for the flatten($1) and if $1 flattens to 10 columns and if the AS clause has 3 columns then the prefix is used and the remaining are left undefined.,"10/Nov/09 18:12;dvryaboy;Aliasing inside foreach is hugely useful for readability. Are you suggesting removing the ability to assign aliases inside a forearch, or just to change/assign schemas?","10/Nov/09 18:19;pkamath;bq. As originally defined UNION does allow two inputs to be of different schema, the result of which should have no schema. 

I think leaving the above definition of UNION as status quo will always lead to an error situation. Is there a use case for the above definition?  In my opinion with the addition of types and schema into the language, we should always be strict when at least one input has a schema and require that all inputs have a schema (this would be consistent for example with https://issues.apache.org/jira/browse/PIG-1064?focusedCommentId=12775976&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12775976). In almost all cases other than an immediate store, allowing a mix of schema and no schema for inputs will result in completely unrelated error messages in the back end. Am wondering if it is better we fix these issues on a case by case basis than delay it for an overall language fix since we may lose track of the many such cases and cause more bug reports while we wait. Thoughts?","10/Nov/09 21:59;sms;bq. Aliasing inside foreach is hugely useful for readability. Are you suggesting removing the ability to assign aliases inside a forearch, or just to change/assign schemas?

For consistency, all relational operators should support the AS clause. Gradually, the aliasing on a per column basis in foreach should be removed from the documentation, deprecated and eventually removed. This is a long term recommendation.",10/Nov/09 22:18;olgan;I don't  think we should add yet another Union especially if later we decide that the union as it is define now have no real value.,"10/Nov/09 22:23;pkamath;To elaborate a little on the errors that arise, consider the script in the description of this issue. We have incompatible schemas and hence we set the schema of the union to be null (unknown). However the first field actually is a chararray as specified by the schemas. Hence there is a type mismatch in the backend (unknown schema implies all fields are bytearrays).

Another example is something like:
{noformat}
A = load 'foo' as (c:chararray);
B = load 'bar';
C = Union A, B;
D = group C by $0;
{noformat}

In the above script also, currently we assign unknown schema to C. However the key of the group by will be bytearray when it comes from B and chararray when it comes from A.

If we have to honor schema of any of the inputs of the union, we can only do so if we can have a legitimate schema for the union - currently in all incompatible schema situations, we essentially disregard input schemas and assume all fields are bytearrays.",28/Jan/10 00:30;olgan;Delaying this fix till we address PIG-998,24/Dec/10 01:33;daijy;Fixed along with PIG-1277.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Behvaiour of COGROUP with and without schema when using ""*"" operator",PIG-1064,12439475,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,viraj,viraj,30/Oct/09 00:48,24/Mar/10 22:15,14/Mar/19 03:05,19/Nov/09 17:56,0.6.0,,,,,,0.6.0,,impl,,,0,,,,"I have 2 tab separated files, ""1.txt"" and ""2.txt""

$ cat 1.txt 
====================
1       2

2       3

====================
$ cat 2.txt 

1       2

2       3

I use COGROUP feature of Pig in the following way:

$java -cp pig.jar:$HADOOP_HOME org.apache.pig.Main

{code}
grunt> A = load '1.txt';            
grunt> B = load '2.txt' as (b0, b1);
grunt> C = cogroup A by *, B by *;  
{code}

2009-10-29 12:46:04,150 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1012: Each COGroup input has to have the same number of inner plans
Details at logfile: pig_1256845224752.log
==========================================================

If I reverse, the order of the schema's
{code}
grunt> A = load '1.txt' as (a0, a1);
grunt> B = load '2.txt';            
grunt> C = cogroup A by *, B by *;  
{code}
2009-10-29 12:49:27,869 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1013: Grouping attributes can either be star (*) or a list of expressions, but not both.
Details at logfile: pig_1256845224752.log

==========================================================
Now running without schema??
{code}
grunt> A = load '1.txt';            
grunt> B = load '2.txt';            
grunt> C = cogroup A by *, B by *;
grunt> dump C; 
{code}

2009-10-29 12:55:37,202 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Successfully stored result in: ""file:/tmp/temp-319926700/tmp-1990275961""
2009-10-29 12:55:37,202 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Records written : 2
2009-10-29 12:55:37,202 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Bytes written : 154
2009-10-29 12:55:37,202 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - 100% complete!
2009-10-29 12:55:37,202 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Success!!

((1,2),{(1,2)},{(1,2)})
((2,3),{(2,3)},{(2,3)})
==========================================================

Is this a bug or a feature?

Viraj",,,,,,,,,,,,,,,,,,,12/Nov/09 19:40;pkamath;PIG-1064-2.patch;https://issues.apache.org/jira/secure/attachment/12424755/PIG-1064-2.patch,12/Nov/09 23:45;pkamath;PIG-1064-3.patch;https://issues.apache.org/jira/secure/attachment/12424792/PIG-1064-3.patch,13/Nov/09 18:52;daijy;PIG-1064-4.patch;https://issues.apache.org/jira/secure/attachment/12424878/PIG-1064-4.patch,18/Nov/09 18:34;pkamath;PIG-1064-5.patch;https://issues.apache.org/jira/secure/attachment/12425360/PIG-1064-5.patch,12/Nov/09 00:07;pkamath;PIG-1064.patch;https://issues.apache.org/jira/secure/attachment/12424676/PIG-1064.patch,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2009-11-10 18:15:20.643,,,no_permission,,,,,,,,,,,,164592,Reviewed,,,,Thu Nov 19 17:56:19 UTC 2009,,,,,,,0|i0grjb:,95891,,,,,,,,,,"10/Nov/09 18:15;pkamath;A proposal to fix this is to catch the situation wherein the user specifies '*' as the cogrouping key and does not have a schema for the corresponding input to the cogroup. In these situations we would issue an error message - ""Cogroup by * is only allowed if the input has a schema"" and error out.","11/Nov/09 17:19;alangates;Why is cogrouping on * without a schema causing trouble?  Because we can't guarantee that inputs have the same number of fields?

Why would anyone ever want to cogroup on *?  Do we need to spend any effort fixing this?","11/Nov/09 17:28;pkamath;Cogroup needs the same arity for the grouping key from both inputs. If there is a cogroup by *, the '*' needs to be expanded so we know the arity. This is done in ProjectStarTranslator - the current code leaves the '*' as is when there is no schema. This causes problems in the backend - hence the proposed fix to catch this and error out.

If we feel that users should not cogroup on '*' we should prevent it in the parser. The proposed fix is easy enough that I don't think we need to restrict the use of '*'.","11/Nov/09 17:29;pkamath;The last paragraph in my previous comment should read:
If we feel that users should not cogroup on star we should prevent it in the parser. The proposed fix is easy enough that I don't think we need to restrict the use of star.","12/Nov/09 00:07;pkamath;The patch implements the proposal to catch the situation wherein the user specifies '*' as the cogrouping key and does not have a schema for the corresponding input to the cogroup. In these situations we would issue an error message - ""Cogroup by * is only allowed if the input has a schema"" and error out.","12/Nov/09 04:31;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12424676/PIG-1064.patch
  against trunk revision 835005.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/149/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/149/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/149/console

This message is automatically generated.","12/Nov/09 19:40;pkamath;Attached patch address unit test failures - the failures were in other tests wherein cogroup * without schema would be valid in the front end. With the changes in the patch, this is no longer the case. I have removed these testcases and in one case retained it since it tests with different loadfuncs.","12/Nov/09 23:28;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12424755/PIG-1064-2.patch
  against trunk revision 835499.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 12 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/153/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/153/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/153/console

This message is automatically generated.",12/Nov/09 23:45;pkamath;There were a couple of new tests added by a recent patch (PIG-1038) which had group by star and broke the tests with this patch - attached patch with fix in the tests.,"13/Nov/09 06:41;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12424792/PIG-1064-3.patch
  against trunk revision 835499.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 15 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/154/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/154/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/154/console

This message is automatically generated.",13/Nov/09 18:52;daijy;Attach a patch to fix TestSecondarySort unit failure.,"13/Nov/09 22:32;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12424878/PIG-1064-4.patch
  against trunk revision 835499.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 15 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/155/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/155/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/155/console

This message is automatically generated.",13/Nov/09 23:49;pkamath;Can't make out what is wrong with the unit tests from the report above - am running them all on my local box - will update with the results,"16/Nov/09 18:10;daijy;With this patch, ""group by *"" without schema does not work anymore. I think there could be some valid use case on that, eg, people may want to use this to do a count for each distinctive values using statement ""group by *; foreach generate group, COUNT(*);"". It is much safe to allow ""group by *"" work, and only disallow ""cogroup by *"".",18/Nov/09 18:34;pkamath;Attached patch to ensure group by star with out schema still works.,"18/Nov/09 22:20;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12425360/PIG-1064-5.patch
  against trunk revision 881008.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/160/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/160/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/160/console

This message is automatically generated.",19/Nov/09 01:55;daijy;+1,19/Nov/09 17:56;pkamath;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig does not call checkOutSpecs() on OutputFormat provided by StoreFunc in the multistore case,PIG-1063,12439462,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,29/Oct/09 21:54,24/Mar/10 22:15,14/Mar/19 03:05,30/Oct/09 20:41,0.4.0,,,,,,0.6.0,,,,,0,,,,"A StoreFunc implementation can inform pig of an OutputFormat it uses through the getStoragePreparationClass() method. In a query with multiple stores which gets optimized into a single mapred job, Pig does not call the checkOutputSpecs() method on the outputformat. An example of such a script is:
{noformat}
a = load 'input.txt';
b = filter a by $0 < 10;
store b into 'output1' using StoreWithOutputFormat();
c = group a by $0;
d = foreach c generate group, COUNT(a.$0);
store d into 'output2' using StoreWithOutputFormat();
{noformat}",,,,,,,,,,,,,,,,,,,29/Oct/09 23:01;pkamath;PIG-1063.patch;https://issues.apache.org/jira/secure/attachment/12423638/PIG-1063.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-30 06:47:50.738,,,no_permission,,,,,,,,,,,,164591,Reviewed,,,,Fri Oct 30 20:41:07 UTC 2009,,,,,,,0|i0griv:,95889,,,,,,,,,,"29/Oct/09 23:01;pkamath;Attached patch fixes the issue. The fix is in PigOutputFormat.checkOutputSpecs(), we now check if there are any stores in the map or reduce plan which provide a non null OutputFormat as return value of  getStorePreparationClass(). In this case, the checkOutputSpecs() method on the OutputFormat of the store is called.",29/Oct/09 23:03;pkamath;The patch will give one extra javac deprecation warning due to a hadoop deprecated class which cannot be suppressed in code due to a javac [bug|http://bugs.sun.com/view_bug.do?bug_id=6594914] - this would need to be resolved when we move to new hadoop api.,"30/Oct/09 06:47;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12423638/PIG-1063.patch
  against trunk revision 831169.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 199 javac compiler warnings (more than the trunk's current 198 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/131/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/131/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/131/console

This message is automatically generated.",30/Oct/09 16:29;pkamath;The 1 javac warning is due to the deprecation warning explained in my previous comment. The unit test failure seems unrelated and looks like a temporary env. issue - resubmitting to check if the tests pass now.,"30/Oct/09 20:18;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12423638/PIG-1063.patch
  against trunk revision 831169.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 199 javac compiler warnings (more than the trunk's current 198 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/133/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/133/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/133/console

This message is automatically generated.",30/Oct/09 20:35;olgan;+1; changes look good,30/Oct/09 20:41;pkamath;Patch committed to trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MultiQuery optimization throws error for multi-level splits,PIG-1060,12439392,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,ankur,ankur,29/Oct/09 11:24,24/Mar/10 22:15,14/Mar/19 03:05,18/Nov/09 19:02,0.5.0,,,,,,0.6.0,,,,,0,,,,"Consider the following scenario :-
1. Multi-level splits in the map plan.
2. Each split branch further progressing across a local-global rearrange.
3. Output of each of these finally merged via a UNION.

MultiQuery optimizer throws the following error in such a case:
""ERROR 2146: Internal Error. Inconsistency in key index found during optimization.""
",,,,,,,,,,,,,,,,,,,05/Nov/09 18:02;rding;PIG-1060.patch;https://issues.apache.org/jira/secure/attachment/12424143/PIG-1060.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-29 18:51:18.595,,,no_permission,,,,,,,,,,,,164588,Reviewed,,,,Wed Nov 18 19:02:00 UTC 2009,,,,,,,0|i0grhr:,95884,,,,,,,,,,"29/Oct/09 11:33;ankur;Here's a sample script to illustrate the issue. Note that sample data isn't very important here since the optimization and execution fail. 
=== test.pig ====

data = LOAD 'dummy' as (name:chararray, freq:int);

filter1 = FILTER data BY freq < 5;
group1 = GROUP filter1 BY name;
proj1 = FOREACH group1 GENERATE FLATTEN(group), 'string1', SUM(filter1.freq);

filter2 = FILTER data by freq > 5;
group2 = GROUP filter2 BY name;
proj2 = FOREACH group2 GENERATE FLATTEN(group), 'string2', SUM(filter2.freq);

filter3 = FILTER filter2 by freq < 10;
group3 = GROUP filter3 By name;
proj3 = FOREACH group3 GENERATE FLATTEN(group), 'string3', SUM(filter3.freq);

filter4 = FILTER filter3 by freq > 7;
group4 = GROUP filter4 By name;
proj4 = FOREACH group4 GENERATE FLATTEN(group), 'string4', SUM(filter4.freq);

M1 = LIMIT proj1 10;
M2 = LIMIT proj2 10;
M3 = LIMIT proj3 10;
M4 = LIMIT proj4 10;

U = UNION M1, M2, M3, M4;

STORE U INTO 'res' USING PigStorage();

The dot output can dumped via command - ""explain -dot -script test.pig;"" to visualize the scenario.
A surprising observation is that despite turning MultiQuery off using -M, it seems that the MultiQuery optimizer is still runs and fails the script.


","29/Oct/09 18:51;rding;Hi Ankur, 

I can't reproduce the bug with the latest code in trunk. Can you please attach the log output in the bug?

Thanks,
-- Richard","29/Oct/09 21:38;rding;Never mind, there is a typo in my script. Here is the stack trace:

Caused by: org.apache.pig.impl.plan.optimizer.OptimizerException: ERROR 2146: Internal Error. Inconsistency in key index found during optimization.
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer.addShiftedKeyInfoIndex(MultiQueryOptimizer.java:679)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer.addShiftedKeyInfoIndex(MultiQueryOptimizer.java:686)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer.mergeOneReducePlanWithIndex(MultiQueryOptimizer.java:584)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer.mergeAllMapReduceSplittees(MultiQueryOptimizer.java:903)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer.mergeMapReduceSplittees(MultiQueryOptimizer.java:371)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer.visitMROp(MultiQueryOptimizer.java:175)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper.visit(MapReduceOper.java:209)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper.visit(MapReduceOper.java:44)
        at org.apache.pig.impl.plan.ReverseDependencyOrderWalker.walk(ReverseDependencyOrderWalker.java:69)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer.visit(MultiQueryOptimizer.java:90)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.compile(MapReduceLauncher.java:393)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:103)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:265)
","05/Nov/09 02:00;viraj;Hi Ankur and Richard,
 I have a script which demonstrates a similar problem, but can be solved by using the -M option. This script can reproduce the problem even without the UNION operator , but it has  properties 1 and 2 of the original problem description.

Try commenting out the F alias. It works fine.

{code}

ORGINALDATA = load '/user/viraj/somedata.txt' using PigStorage() as (col1, col2, col3, col4, col5, col6, col7, col8);



--Check data

A = foreach ORGINALDATA generate col1, col2, col3, col4, col5, col6;

B = group A all;

C = foreach B generate COUNT(A);

store C into '/user/viraj/result1';



D = filter A by (col1 == col2) or (col1 == col3);

E = group D all;

F = foreach E generate COUNT(D);

--try commenting F
store F into '/user/viraj/result2';



G = filter D by (col4 == col5) ;

H = group G all;

I = foreach H generate COUNT(G);

store I into '/user/viraj/result3';



J = filter G by (((col6 == 'm') or (col6 == 'M')) and (col6 == 1)) or (((col6 == 'f') or (col6 == 'F')) and (col6 == 0)) or ((col6 == '') and (col6 == -1));

K = group J all;

L = foreach K generate COUNT(J);

store L into '/user/viraj/result4';

{code}

",05/Nov/09 18:02;rding;This patch fixes the bug.,"05/Nov/09 18:32;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12424143/PIG-1060.patch
  against trunk revision 833102.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 319 release audit warnings (more than the trunk's current 318 warnings).

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/40/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/40/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/40/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/40/console

This message is automatically generated.","05/Nov/09 22:35;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12424143/PIG-1060.patch
  against trunk revision 833126.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 319 release audit warnings (more than the trunk's current 318 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/41/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/41/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/41/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/41/console

This message is automatically generated.","12/Nov/09 18:24;rding;The release audit warnings are all from html files.
",18/Nov/09 18:57;daijy;Patch looks good. Will commit to both trunk and 0.6 branch as it is.,18/Nov/09 19:02;daijy;Patch committed. Thanks Richard!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Zebra] Zebra does not support concurrent deletions of column groups now.,PIG-1057,12439306,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chaow,chaow,chaow,28/Oct/09 19:07,24/Mar/10 22:15,14/Mar/19 03:05,30/Oct/09 21:45,0.4.0,,,,,,0.6.0,,,,,0,,,,"Zebra does not support concurrent deletions of column groups now.  As a result, the TestDropColumnGroup testcase can sometimes fail due to this.
In this testcase, multiple threads will be launched together, with each one deleting one particular column group.  The following exception can be thrown (with callstack):


/*************************************************************************************************************************/
... 
java.io.FileNotFoundException: File /.../pig-trunk/build/contrib/zebra/test/data/DropCGTest/CG02 does not exist.
  at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:361)
  at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:290)
  at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:716)
  at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:741)
  at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:465)
  at org.apache.hadoop.zebra.io.BasicTable$SchemaFile.setCGDeletedFlags(BasicTable.java:1610)
  at org.apache.hadoop.zebra.io.BasicTable$SchemaFile.readSchemaFile(BasicTable.java:1593)
  at org.apache.hadoop.zebra.io.BasicTable$SchemaFile.<init>(BasicTable.java:1416)
  at org.apache.hadoop.zebra.io.BasicTable.dropColumnGroup(BasicTable.java:133)
  at org.apache.hadoop.zebra.io.TestDropColumnGroup$DropThread.run(TestDropColumnGroup.java:772)
...
/*************************************************************************************************************************/

We plan to fix this in Zebra to support concurrent deletions of column groups. The root cause is that a thread or process reads in some stale file system information (e.g., it sees /CG0 first) and then can fail later on (it tries to access /CG0, however /CG0 may be deleted by another thread or process).  Therefore, we plan to adopt a retry logic to resolve this issue. More detailed, we allow a dropping column group thread to retry n times when doing its deleting job - n is the total number of column groups. 

Note that here we do NOT try to resolve the more general concurrent column group deletions + reads issue. If a process is reading some data that could be deleted by another process, it can fail as we expect.
Here we only try to resolve the concurrent column group deletions issue. If you have multiple threads or processes to delete column groups, they should succeed.


",,,,,,,,,,,,,,,,,,,29/Oct/09 20:07;chaow;patch_1057;https://issues.apache.org/jira/secure/attachment/12423611/patch_1057,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-29 23:04:09.081,,,no_permission,,,,,,,,,,,,164585,,,,,Fri Oct 30 21:45:28 UTC 2009,,,,,,,0|i0grgf:,95878,,,,,,,,,,"29/Oct/09 23:04;yanz;patch reviewed +1.

This patch will address the concern Raghu had in Pig-993.","30/Oct/09 00:20;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12423611/patch_1057
  against trunk revision 831051.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/130/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/130/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/130/console

This message is automatically generated.",30/Oct/09 21:45;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inner join using 'skewed' produces multiple rows for keys with single row in both input relations,PIG-1048,12438875,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sriranjan,thejas,thejas,23/Oct/09 01:12,24/Mar/10 22:15,14/Mar/19 03:05,30/Oct/09 22:49,,,,,,,0.6.0,,,,,0,,,,"${code}
grunt> cat students.txt                           
asdfxc  M       23      12.44
qwer    F       21      14.44
uhsdf   M       34      12.11
zxldf   M       21      12.56
qwer    F       23      145.5
oiue    M       54      23.33

 l1 = load 'students.txt';            
l2 = load 'students.txt';                  
j = join l1 by $0, l2 by $0 ; 
store j into 'tmp.txt'             

grunt> cat tmp.txt
oiue    M       54      23.33   oiue    M       54      23.33
oiue    M       54      23.33   oiue    M       54      23.33
qwer    F       21      14.44   qwer    F       21      14.44
qwer    F       21      14.44   qwer    F       23      145.5
qwer    F       23      145.5   qwer    F       21      14.44
qwer    F       23      145.5   qwer    F       23      145.5
uhsdf   M       34      12.11   uhsdf   M       34      12.11
uhsdf   M       34      12.11   uhsdf   M       34      12.11
zxldf   M       21      12.56   zxldf   M       21      12.56
zxldf   M       21      12.56   zxldf   M       21      12.56
asdfxc  M       23      12.44   asdfxc  M       23      12.44
asdfxc  M       23      12.44   asdfxc  M       23      12.44$


${code}",,,,,,,,,,,,,,,,,,,30/Oct/09 04:24;sriranjan;pig_1048.patch;https://issues.apache.org/jira/secure/attachment/12423658/pig_1048.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-28 03:53:14.295,,,no_permission,,,,,,,,,,,,164576,,,,,Wed Nov 04 01:36:29 UTC 2009,,,,,,,0|i0grcn:,95861,,,,,,,,,,28/Oct/09 03:53;sriranjan;The patch solves this issue.,28/Oct/09 23:43;alangates;Could you describe briefly the cause of the problem and how the one line change fixes it?,"29/Oct/09 01:35;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12423389/pig_1048.patch
  against trunk revision 830757.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/124/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/124/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/124/console

This message is automatically generated.","29/Oct/09 19:55;sriranjan;The reason this issue was happening was because for a key with 1 value, we were allocating 2 reducers. The culprit being:

                        // number of reducers
-                       Integer cnt = 0;
-                       if (minIndex < maxIndex) {
-                               cnt = maxIndex - minIndex;
-                       } else {
-                               cnt = totalReducers[0] + maxIndex - minIndex;
-                       }
-

If maxIndex = minIndex  = 0 and totalReducers was 1, cnt would get a value of 1 instead of 0!

cnt was based on 0-index whereas, totalReducers was based on 1-index. This resulted in POParitionRearrange distributing the tuple to 1 more than the required amount of reducers.

The fix is to always set the value of cnt to ""maxIndex - minIndex"". The code guarantees that maxIndex is always greater than minIndex.",29/Oct/09 23:31;sriranjan;I have also modified a skewed join test case to check if atleast one key is present in more than 1 partition instead of checking for all the keys being present in multiple partitions. Since the dataset was too small sampler with the RLR change did not detect these small keys causing the unit test to fail.,30/Oct/09 04:20;sriranjan;Re-uploaded the same patch. Attaching a new one.,"30/Oct/09 05:30;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12423641/pig_1048.patch
  against trunk revision 831169.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/34/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/34/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/34/console

This message is automatically generated.","30/Oct/09 08:42;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12423658/pig_1048.patch
  against trunk revision 831169.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/35/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/35/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/35/console

This message is automatically generated.",30/Oct/09 22:49;alangates;Patch checked in.  Thanks Sri for fixing this.,"04/Nov/09 01:36;alangates;When attempting to apply this patch to the 0.5 branch, I got the following error:

Testcase: testSkewedJoinOneValue took 145.739 sec
    Caused an ERROR
Unable to open iterator for alias E
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias E
    at org.apache.pig.PigServer.openIterator(PigServer.java:475)
    at org.apache.pig.test.TestSkewedJoin.testSkewedJoinOneValue(TestSkewedJoin.java:340)
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 2997: Unable to recreate exception from backed error: java.lang.RuntimeException: Error in configuring object
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher.getErrorMessages(Launcher.java:237)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher.getStats(Launcher.java:181)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:209)
    at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:265)
    at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:773)
    at org.apache.pig.PigServer.store(PigServer.java:522)
    at org.apache.pig.PigServer.openIterator(PigServer.java:458)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
join algorithm specification is within double quotes,PIG-1046,12438864,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ashutoshc,thejas,thejas,22/Oct/09 22:55,14/May/10 06:45,14/Mar/19 03:05,07/Feb/10 18:53,,,,,,,0.7.0,,,,,0,,,,"This fails -
j = join l1 by $0, l2 by $0 using 'skewed';
This works -
j = join l1 by $0, l2 by $0 using ""skewed"";

String constants are single-quoted in pig-latin. If the algorithm specification is supposed to be a string, specifying it within single quotes should be supported.
Alternatively, we should be using identifiers here, since these are pre-defined in pig users will not be specifying arbitrary values that might not be valid identifier. ",,,,,,,,,,,,,,,,,,,30/Jan/10 01:47;ashutoshc;pig-1046.patch;https://issues.apache.org/jira/secure/attachment/12431848/pig-1046.patch,01/Feb/10 22:10;ashutoshc;pig-1046_1.patch;https://issues.apache.org/jira/secure/attachment/12434440/pig-1046_1.patch,02/Feb/10 19:43;ashutoshc;pig-1046_2.patch;https://issues.apache.org/jira/secure/attachment/12434583/pig-1046_2.patch,04/Feb/10 03:16;ashutoshc;pig-1046_3.patch;https://issues.apache.org/jira/secure/attachment/12434775/pig-1046_3.patch,04/Feb/10 23:05;ashutoshc;pig-1046_4.patch;https://issues.apache.org/jira/secure/attachment/12434884/pig-1046_4.patch,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2010-01-30 01:21:08.227,,,no_permission,,,,,,,,,,,,164574,,,,,Sun Feb 07 18:53:37 UTC 2010,,,,,,,0|i0grc7:,95859,,,,,,,,,,"30/Jan/10 01:21;ashutoshc;Since algorithm specification is not arbitrary we should be using pre-defined identifier. This is also in line with how we specify [ left | right | full | outer] join. So, algorithm specification should look like:
j = join l1 by $0, l2 by $0 using SKEWED;
To remain backward compatible we will continue to support double quoted strings as well.  ",30/Jan/10 01:47;ashutoshc;Patch as per comment above. Test cases included.,"30/Jan/10 07:44;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12431848/pig-1046.patch
  against trunk revision 904713.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/194/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/194/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/194/console

This message is automatically generated.","01/Feb/10 15:51;alangates;-1.  The original intent of the JIRA was to move from double quotes to single, to match the rest of string usage in the language.  Further discussion is needed if we want to move these to identifiers instead.",01/Feb/10 21:46;ashutoshc;Alan suggested use of identifier or keywords should be taken up as a whole for the language (instead of one clause like join at a time). Doing that is out of scope of this jira as well as for 0.7 release. It will be taken up some point later in time. For now we will support both single and double quoted strings for join algorithm specification and print deprecation warning when user uses double-quoted strings.,"01/Feb/10 22:10;ashutoshc;Patch as per previous comment. With this patch both of following will work:
j = join l1 by $0, l2 by $0 using 'skewed';
j = join l1 by $0, l2 by $0 using ""skewed"";","02/Feb/10 06:48;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12434440/pig-1046_1.patch
  against trunk revision 905377.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/186/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/186/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/186/console

This message is automatically generated.",02/Feb/10 19:41;ashutoshc;Collected hint for Group needs similar fix. Canceling patch as a result. Will be uploading new patch with that included.  ,02/Feb/10 19:43;ashutoshc;Patch which includes same fix for collected hint for Group by. Test cases included.,"03/Feb/10 00:07;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12434583/pig-1046_2.patch
  against trunk revision 905377.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/187/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/187/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/187/console

This message is automatically generated.",04/Feb/10 02:26;olgan;The changes look good. One thing that was missed for both group and join is that the string after using is one of the strings that we recognize and give an error otherwise.,04/Feb/10 03:16;ashutoshc;Updated patch incorporating Olga's comments.,"04/Feb/10 14:16;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12434775/pig-1046_3.patch
  against trunk revision 906326.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/199/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/199/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/199/console

This message is automatically generated.","04/Feb/10 19:43;olgan;(1) I think the error message should be made a little more clear on invalid cogroup modifier. Something like:

Only COLLECTED or REGULAR are valid GROUP modifiers.

(2) There seems to be some code duplication to support doubequotes. It would be better if you just had warning for deprication but then had the rest of the code in one place.

(3) Similar comments for the join part.",04/Feb/10 23:05;ashutoshc;Updated patch incorporating Olga's comments.,"05/Feb/10 04:39;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12434884/pig-1046_4.patch
  against trunk revision 906657.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/201/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/201/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/201/console

This message is automatically generated.","07/Feb/10 03:45;olgan;+1;  please, commit",07/Feb/10 18:53;ashutoshc;Patch checked-in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 javac warnings: unchecked,PIG-1042,12438823,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,22/Oct/09 17:11,24/Mar/10 22:15,14/Mar/19 03:05,23/Oct/09 20:42,0.4.0,,,,,,0.6.0,,impl,,,0,,,,"Pig have 164 javac warnings when you build it with the option ""-Dall.warnings=1"" which fall into category ""unchecked"". We need to suppress all of them",,,,,,,,,,,,,,,,,,,23/Oct/09 17:49;daijy;PIG-1042-1.patch;https://issues.apache.org/jira/secure/attachment/12423037/PIG-1042-1.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-23 19:44:54.005,,,no_permission,,,,,,,,,,,,164570,Reviewed,,,,Fri Oct 23 20:42:58 UTC 2009,,,,,,,0|i0graf:,95851,,,,,,,,,,"22/Oct/09 20:57;daijy;We can sub-divide unchecked warnings into several categories:
1. Logical plan related type conversion checking. Logical layer use generics extensively and there are places we do not use them consistently. We shall address these issues in our logical plan rework. Currently, I simply suppress them and put a note on that. We will look back later
2. Backend related type conversion checking. We skip type conversion checking intentionally for performance reasons. For these, I will simply suppress them
3. javacc generated warnings. We shall find a way to address them
4. Some valid warnings we shall fix them","23/Oct/09 01:37;daijy;javacc related warnings are still there, we need to find a way to suppress that.",23/Oct/09 19:44;olgan;+1,23/Oct/09 20:42;daijy;Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"javac warnings: cast, fallthrough, serial",PIG-1041,12438821,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,22/Oct/09 17:09,24/Mar/10 22:15,14/Mar/19 03:05,23/Oct/09 20:42,0.4.0,,,,,,0.6.0,,,,,0,,,,"Pig have javac warnings when you build it with the option ""-Dall.warnings=1"". We need to suppress all of them. This issue is to track the javac warnings in the following categories:

cast (49)
fallthrough (1)
serial (19)

The number in the parenthesis is the times of occurrence of particular javac warning.",,,,,,,,,,,,,,,,,,,23/Oct/09 17:43;daijy;PIG-1041-1.patch;https://issues.apache.org/jira/secure/attachment/12423036/PIG-1041-1.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-23 02:14:28.235,,,no_permission,,,,,,,,,,,,164569,,,,,Fri Oct 23 20:42:19 UTC 2009,,,,,,,0|i0gr9z:,95849,,,,,,,,,,"22/Oct/09 17:35;daijy;The plan is: 
1. Suppressing warnings in the code level as much as possible
2. Some ""cast"" warnings are from the generated code of javacc, which is out of our control. As a result, we will suppress all ""cast"" warnings in build.xml.","22/Oct/09 18:00;daijy;Attached patch address target javac warnings in the code level. Still couple of warnings from javacc which is out of our control, we will suppress it in build.xml. The change of build.xml will be included in PIG-1033. ","23/Oct/09 02:14;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422919/PIG-1041-1.patch
  against trunk revision 828773.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/110/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/110/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/110/console

This message is automatically generated.","23/Oct/09 19:47;olgan;+1 on the patch. In general, I believe there is a way to supress warnings at the package level.",23/Oct/09 20:42;daijy;Patch committed. Move javacc related warnings to PIG-1049.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig does not support ORDER ... BY group alias,PIG-1034,12438726,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,zjffdu,ciemo,ciemo,21/Oct/09 16:54,17/Dec/10 22:43,14/Mar/19 03:05,27/Jul/10 17:40,0.8.0,,,,,,0.8.0,,,,,0,,,,"GROUP ... ALL and GROUP ... BY produce an alias ""group"".

Pig produces a syntax error if you attempt to ORDER ... BY group.

This does seem like a perfectly reasonable thing to do.

The workaround is to create an alias for group using an AS clause.  But I think this workaround should be unnecessary.

Here's sample code which elicits the syntax error:

{code}
A = load 'one.txt' using PigStorage as (one: int);

B = group A all;

C = foreach B generate
	group,
	COUNT(A) as count;

D = order C by group parallel 1; -- group is one of the aliases in C, why does this throw a syntax error?

dump D;
{code}",,,,,,,,,,,,,,,,,,,21/Jun/10 07:19;zjffdu;PIG_1034.patch;https://issues.apache.org/jira/secure/attachment/12447586/PIG_1034.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-06-21 07:19:36.285,,,no_permission,,,,,,,,,,,,164562,,,,,Tue Jul 27 17:40:59 UTC 2010,,,,,,,0|i0gr6v:,95835,,,,,,,,,,21/Jun/10 07:19;zjffdu;Attach the patch,"21/Jun/10 12:54;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12447586/PIG_1034.patch
  against trunk revision 956440.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/344/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/344/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/344/console

This message is automatically generated.","23/Jun/10 07:35;zjffdu;Check the testReport, the failed testcase does not relate to this patch

",24/Jul/10 00:02;thejas;I am reviewing this patch.,"27/Jul/10 15:25;thejas;+1 please commit.
I also ran the test-patch and unit tests on my machine, they pass.
","27/Jul/10 17:40;thejas;I have committed the patch. Jeff, thanks for the contribution.
Also, marking the jira as fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
javac warnings: deprecated hadoop APIs,PIG-1033,12438649,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,21/Oct/09 01:27,24/Mar/10 22:15,14/Mar/19 03:05,23/Oct/09 20:42,0.4.0,,,,,,0.6.0,,impl,,,0,,,,Suppress javac warnings related to deprecated hadoop APIs.,,,,,,,,,,,,,,,,,,,23/Oct/09 17:55;daijy;PIG-1033-1.patch;https://issues.apache.org/jira/secure/attachment/12423039/PIG-1033-1.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-22 16:23:06.529,,,no_permission,,,,,,,,,,,,164561,Reviewed,,,,Mon Oct 26 17:17:08 UTC 2009,,,,,,,0|i0gr6f:,95833,,,,,,,,,,"22/Oct/09 00:11;daijy;We have options:
1. Change build.xml to disable deprecation warnings. BTW, hadoop does not check for deprecation when compiling
2. Using @SuppressWarnings(""deprecation"") to suppress specific blocks. The problem is we cannot suppress import block, all import warnings still there
3. Using @SuppressWarnings(""deprecation"") and do not import deprecate class. Use full qualified class name for deprecate class when using it. The downside is patch is relatively lengthy.",22/Oct/09 16:23;olgan;I think it is ok to go with the option 1 for now with the understanding that we clean things up as part of the transition to Hadoop 21,23/Oct/09 18:11;olgan;+1,23/Oct/09 20:42;daijy;Patch committed.,"26/Oct/09 17:17;daijy;Not sure if this blanket suppression of deprecation javac warnings is good. See the same discussion on MAPREDUCE side, link to it for reference.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
explain and dump not working with two UDFs inside inner plan of foreach,PIG-1030,12438611,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,yinghe,yinghe,20/Oct/09 19:33,24/Mar/10 22:15,14/Mar/19 03:05,02/Nov/09 20:10,,,,,,,0.6.0,,,,,0,,,,"this scprit does not work

register /homes/yinghe/owl/string.jar;
a = load '/user/yinghe/a.txt' as (id, color);
b = group a all;
c = foreach b {
    d = distinct a.color;
    generate group, string.BagCount2(d), string.ColumnLen2(d, 0);
}

the udfs are regular, not algebraic.

then if I call  ""dump c;"" or ""explain c"", I would get  this error message.
ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2019: Expected to find plan with single leaf. Found 2 leaves.

The error only occurs for the first time, after getting this error, if I call ""dump c"" or ""explain c"" again, it would succeed.


",,,,,,,,,,,,,,,,,,,30/Oct/09 18:50;rding;PIG-1030.patch;https://issues.apache.org/jira/secure/attachment/12423708/PIG-1030.patch,28/Oct/09 18:19;rding;PIG-1030.patch;https://issues.apache.org/jira/secure/attachment/12423476/PIG-1030.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-10-28 18:19:17.262,,,no_permission,,,,,,,,,,,,164558,Reviewed,,,,Mon Nov 02 20:10:54 UTC 2009,,,,,,,0|i0gr5b:,95828,,,,,,,,,,"28/Oct/09 18:19;rding;The problem is that the code assums to combine only in the case where there is only  one PODistinct which is the only input to an aggregate function. This patch disables the combiner if an aggregate function in foreach statements have multiple inputs and one of them is PODistinct.
  ","29/Oct/09 11:06;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12423476/PIG-1030.patch
  against trunk revision 830757.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/126/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/126/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/126/console

This message is automatically generated.","29/Oct/09 23:24;pkamath;In the following code :
{code}
750                 List<PhysicalOperator> preds = mPlan.getPredecessors(leaf);     
  751                 if (preds.size() > 1) {                                                                        
  752                     sawNonAlgebraic = true;                                                           
  753                 }     
{code}

Shouldn't there be a return after setting sawNonAlgebraic since we shouldn't proceed to make more checks?",30/Oct/09 18:50;rding;Add returns as suggested.,"30/Oct/09 22:17;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12423708/PIG-1030.patch
  against trunk revision 831402.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/36/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/36/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/36/console

This message is automatically generated.","02/Nov/09 20:10;pkamath;+1, patch committed, Thanks Richard!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FINDBUGS: DM_NUMBER_CTOR: Method invokes inefficient Number constructor; use static valueOf instead,PIG-1028,12438507,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,19/Oct/09 22:48,17/Oct/11 20:41,14/Mar/19 03:05,22/Oct/09 18:51,,,,,,,0.6.0,,,,,0,,,,"Bx 	Method org.apache.pig.backend.hadoop.datastorage.HDataStorage.getStatistics() invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.backend.hadoop.datastorage.HDataStorage.init() invokes inefficient new Short(short) constructor; use Short.valueOf(short) instead
Bx 	Method org.apache.pig.backend.hadoop.datastorage.HPath.getConfiguration() invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.backend.hadoop.datastorage.HPath.getConfiguration() invokes inefficient new Short(short) constructor; use Short.valueOf(short) instead
Bx 	Method org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer.addShiftedKeyInfoIndex(int, POPackage) invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead
Bx 	Method org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.POPackageAnnotator$LoRearrangeDiscoverer.visitLocalRearrange(POLocalRearrange) invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead
Bx 	Method org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Add.getNext(Integer) invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead
Bx 	Method org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Add.getNext(Long) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Divide.getNext(Integer) invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead
Bx 	Method org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Divide.getNext(Long) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Mod.getNext(Integer) invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead
Bx 	Method org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Mod.getNext(Long) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Multiply.getNext(Integer) invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead
Bx 	Method org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Multiply.getNext(Long) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(Integer) invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead
Bx 	Method org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(Long) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Subtract.getNext(Integer) invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead
Bx 	Method org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Subtract.getNext(Long) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.setIndex(int, boolean) invokes inefficient new Byte(byte) constructor; use Byte.valueOf(byte) instead
Bx 	Method org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrangeForIllustrate.constructLROutput(List, Tuple) invokes inefficient new Byte(byte) constructor; use Byte.valueOf(byte) instead
Bx 	Method org.apache.pig.builtin.ARITY.exec(Tuple) invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead
Bx 	Method org.apache.pig.builtin.AVG.combine(DataBag) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.builtin.BagSize.exec(Tuple) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.builtin.BinStorage.bytesToInteger(byte[]) invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead
Bx 	Method org.apache.pig.builtin.BinStorage.bytesToLong(byte[]) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.builtin.ConstantSize.exec(Tuple) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.builtin.COUNT$Initial.exec(Tuple) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.builtin.DoubleAvg.combine(DataBag) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.builtin.FloatAvg.combine(DataBag) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.builtin.IntAvg.combine(DataBag) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.builtin.IntAvg.sum(Tuple) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.builtin.IntAvg$Initial.exec(Tuple) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.builtin.IntMax.max(Tuple) invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead
Bx 	Method org.apache.pig.builtin.IntMin.min(Tuple) invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead
Bx 	Method org.apache.pig.builtin.IntSum.sum(Tuple) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.builtin.IntSum.sumLongs(Tuple) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.builtin.IntSum$Initial.exec(Tuple) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.builtin.LongAvg.combine(DataBag) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.builtin.LongAvg.sum(Tuple) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.builtin.LongMax.max(Tuple) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.builtin.LongMin.min(Tuple) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.builtin.LongSum.sum(Tuple) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.builtin.MapSize.exec(Tuple) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.builtin.SIZE.exec(Tuple) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.builtin.StringSize.exec(Tuple) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.builtin.TupleSize.exec(Tuple) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method new org.apache.pig.builtin.Utf8StorageConverter() invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead
Bx 	Method new org.apache.pig.builtin.Utf8StorageConverter() invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.builtin.Utf8StorageConverter.bytesToInteger(byte[]) invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead
Bx 	Method org.apache.pig.builtin.Utf8StorageConverter.bytesToLong(byte[]) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.data.DataReaderWriter.readDatum(DataInput, byte) invokes inefficient new Byte(byte) constructor; use Byte.valueOf(byte) instead
Bx 	Method org.apache.pig.data.DataReaderWriter.readDatum(DataInput, byte) invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead
Bx 	Method org.apache.pig.data.DataReaderWriter.readDatum(DataInput, byte) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.data.DataType.toInteger(Object, byte) invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead
Bx 	Method org.apache.pig.data.DataType.toLong(Object, byte) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Primitive value is boxed then unboxed to perform primative coercion in org.apache.pig.impl.builtin.FindQuantiles.exec(Tuple)
Bx 	Method org.apache.pig.impl.builtin.FindQuantiles.exec(Tuple) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.impl.builtin.GFReplicate.exec(Tuple) invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead
Bx 	Method org.apache.pig.impl.logicalLayer.LOProject.clone() invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead
Bx 	Method org.apache.pig.Main.main(String[]) invokes inefficient new Character(char) constructor; use Character.valueOf(char) instead
Bx 	Method org.apache.pig.pen.AugmentBaseDataVisitor.GetLargerValue(Object) invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead
Bx 	Method org.apache.pig.pen.AugmentBaseDataVisitor.GetLargerValue(Object) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.pen.AugmentBaseDataVisitor.GetSmallerValue(Object) invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead
At AugmentBaseDataVisitor.java:[line 847]
Bx 	Method org.apache.pig.pen.AugmentBaseDataVisitor.GetSmallerValue(Object) invokes inefficient new Long(long) constructor; use Long.valueOf(long) instead
Bx 	Method org.apache.pig.shock.SSHSocketImpl.getOption(int) invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead
Bx 	Method org.apache.pig.tools.cmdline.CmdLineParser.getNextOpt() invokes inefficient new Character(char) constructor; use Character.valueOf(char) instead
Bx 	Method org.apache.pig.tools.cmdline.CmdLineParser.getNextOpt() invokes inefficient new Integer(int) constructor; use Integer.valueOf(int) instead
Another occurrence at CmdLineParser.java:[line 126]
Bx 	Method org.apache.pig.tools.cmdline.CmdLineParser.registerOpt(char, String, CmdLineParser$ValueExpected) invokes inefficient new Character(char) constructor; use Character.valueOf(char) instead",,,,,,,,,,,,,,,,,,,21/Oct/09 00:51;olgan;PIG-1028.patch;https://issues.apache.org/jira/secure/attachment/12422755/PIG-1028.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-21 13:29:45.914,,,no_permission,,,,,,,,,,,,88683,,,,,Thu Oct 22 18:51:26 UTC 2009,,,,,,,0|i0gr47:,95823,,,,,,,,,,"21/Oct/09 13:29;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422755/PIG-1028.patch
  against trunk revision 827829.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/105/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/105/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/105/console

This message is automatically generated.","21/Oct/09 20:37;daijy;+1, target findbugs warnings suppressed.",22/Oct/09 18:51;olgan;patch was coommitted yesterday,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Number of bytes written are always zero in local mode,PIG-1027,12438461,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,zjffdu,ashutoshc,ashutoshc,19/Oct/09 16:07,24/Mar/10 22:15,14/Mar/19 03:05,23/Oct/09 17:02,0.6.0,,,,,,0.6.0,,impl,,,0,,,,"Consider this very simple script containing few records

{code}
a = load 'foo';
store a into 'out';
{code}

Following message gets printed on grunt shell:

[main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Records written : 39
[main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Bytes written : 0

File has 39 records which is correctly reported. But number of bytes is always reported as zero, no matter what.  I am observing this on latest trunk, not sure if this existed on previous/current releases.",,,,,,,,,,,,,,,,,,,20/Oct/09 15:32;zjffdu;Pig_1027.Patch;https://issues.apache.org/jira/secure/attachment/12422692/Pig_1027.Patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-20 15:30:42.817,,,no_permission,,,,,,,,,,,,164556,,,,,Fri Oct 23 17:02:11 UTC 2009,,,,,,,0|i0gr3r:,95821,,,,,,,local mode,,,"20/Oct/09 15:30;zjffdu;The cause of this bug is because of the path problem.  the file name in FileSpec has the schema.
When we create a new file, we should remove the schema.","20/Oct/09 22:30;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422692/Pig_1027.Patch
  against trunk revision 826927.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 4 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/103/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/103/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/103/console

This message is automatically generated.","21/Oct/09 13:37;zjffdu;I look the test report, It seems the failed test case is not related the patch.

Could anyone help look into it ?

","21/Oct/09 13:44;alangates;There's something wrong with the test runs on Hudson.  We are looking into it.  We'll get to running your patch through the tests manually, but it will take a bit as there's several in the queue and we have to run all the unit tests (about 3 hours) on each.",23/Oct/09 17:02;alangates;Fix checked in.  Thanks Jeff for fixing this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[zebra] map split returns null,PIG-1026,12438340,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yanz,jing1234,jing1234,16/Oct/09 21:23,24/Mar/10 22:15,14/Mar/19 03:05,06/Nov/09 20:24,0.6.0,,,,,,0.6.0,,,,,0,,,,"Here is the test scenario:
 final static String STR_SCHEMA = ""m1:map(string),m2:map(map(int))"";
  //final static String STR_STORAGE = ""[m1#{a}];[m2#{x|y}]; [m1#{b}, m2#{z}];[m1]"";
 final static String STR_STORAGE = ""[m1#{a}, m2#{x}];[m2#{x|y}]; [m1#{b}, m2#{z}];[m1,m2]"";

projection: String projection2 = new String(""m1#{b}, m2#{x|z}"");
User got null pointer exception on reading m1#{b}.

Yan, please refer to the test class:
TestNonDefaultWholeMapSplit.java ",,,,,,,,,,,,,,,,,,,31/Oct/09 00:35;yanz;PIG_1026.patch;https://issues.apache.org/jira/secure/attachment/12423738/PIG_1026.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-19 19:30:56.773,,,no_permission,,,,,,,,,,,,164555,Reviewed,,,,Fri Nov 06 20:24:43 UTC 2009,,,,,,,0|i0gr3b:,95819,,,,,,,,,,"19/Oct/09 19:30;yanz;The STR_STORAGE actually has a problem : m2#{x} is actually specified twice. So a syntax exception should have been thrown. I'm going to attach a patch that throws the exception in a proper manner.

After I correct the syntax error, the tests pass, particularly with ""[m1#{a|b}, m2#{x|y}]; [m1]; [m2]"" as the storage specification.",19/Oct/09 19:32;yanz;This patch is not applicable to SVN yet since it's not generated based upon the current SVN image.,"31/Oct/09 00:47;yanz;Another problem is that during STORE, when  handleMapSplit method is called to set up the CG schema mapping with the Table schema, an incremented index was used as the column indices to the CG schemas. This works ok if the columns in the CG schemas are in the same order as in the table schema, but will be wrong if the orders are different, causing no values to be stored in some MAP columns in any CG and hence, during LOAD, getValue() returns null.",02/Nov/09 23:24;chaow;Patch reviewed.  +1,"06/Nov/09 05:12;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12423738/PIG_1026.patch
  against trunk revision 833266.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 10 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/141/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/141/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/141/console

This message is automatically generated.",06/Nov/09 20:24;pkamath;All the nightly tests pass. Patch checked in.  Thanks Yan!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Script contains nested limit fail due to ""LOLimit does not support multiple outputs""",PIG-1024,12438136,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,14/Oct/09 22:43,24/Mar/10 22:15,14/Mar/19 03:05,15/Oct/09 18:21,0.4.0,,,,,,0.6.0,,impl,,,0,,,,"The following script fail: 

a = load '1.txt' as (a0:int, a1:int, a2:int);
b = group a by a0;
c = foreach b { c1 = limit a 10;
c2 = (c1.a0/c1.a1);
c3 = (c1.a0/c1.a2);
generate c2, c3;}

Error message:

ERROR org.apache.pig.impl.plan.OperatorPlan - Attempt to give operator of type
org.apache.pig.impl.logicalLayer.LOLimit multiple outputs.  This operator does not support multiple outputs.",,,,,,,,,,,,,,,,,,,14/Oct/09 22:45;daijy;PIG-1024-1.patch;https://issues.apache.org/jira/secure/attachment/12422154/PIG-1024-1.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-15 01:42:39.039,,,no_permission,,,,,,,,,,,,164554,Reviewed,,,,Thu Oct 15 18:21:43 UTC 2009,,,,,,,0|i0gr2f:,95815,,,,,,,,,,14/Oct/09 22:45;daijy;Patch included. Thanks Pradeep's diagnosis.,"15/Oct/09 01:42;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422154/PIG-1024-1.patch
  against trunk revision 825308.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/26/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/26/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/26/console

This message is automatically generated.","15/Oct/09 17:19;olgan;Need a comment to explain how the test tests the fix since it has no asserts. Otherwise, +1",15/Oct/09 18:21;daijy;Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FINDBUGS: exclude CN_IDIOM_NO_SUPER_CALL,PIG-1023,12438117,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,14/Oct/09 19:54,25/Mar/10 00:12,14/Mar/19 03:05,14/Oct/09 21:57,,,,,,,0.6.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,14/Oct/09 20:05;olgan;PIG-1023.patch;https://issues.apache.org/jira/secure/attachment/12422130/PIG-1023.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-14 20:58:31.943,,,no_permission,,,,,,,,,,,,164553,,,,,Wed Oct 14 21:57:11 UTC 2009,,,,,,,0|i0gr1z:,95813,,,,,,,,,,14/Oct/09 20:06;olgan;This does not have to go through patch test process. Could one of the committers please review,14/Oct/09 20:58;daijy;+1. Target findbug warnings suppressed. Findbugs generate 37 less warnings.,14/Oct/09 21:57;olgan;patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
optimizer pushes filter before the foreach that generates column used by filter,PIG-1022,12438102,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,thejas,thejas,14/Oct/09 16:25,14/May/10 06:45,14/Mar/19 03:05,01/Dec/09 18:34,0.4.0,,,,,,0.7.0,,impl,,,0,,,,"grunt> l = load 'students.txt' using PigStorage() as (name:chararray, gender:chararray, age:chararray, score:chararray);
grunt> f = foreach l generate name, gender, age,score, '200'  as gid:chararray;
grunt> g = group f by (name, gid);
grunt> f2 = foreach g generate group.name as name: chararray, group.gid as gid: chararray;
grunt> filt = filter f2 by gid == '200';
grunt> explain filt;

In the plan generated filt is pushed up after the load and before the first foreach, even though the filter is on gid which is generated in first foreach.",,,,,,,,,,,,,,,,,,,21/Oct/09 00:46;daijy;PIG-1022-1.patch;https://issues.apache.org/jira/secure/attachment/12422754/PIG-1022-1.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-14 18:35:51.637,,,no_permission,,,,,,,,,,,,164552,,,,,Wed Dec 02 01:47:59 UTC 2009,,,,,,,0|i0gr1j:,95811,,,,,,,,,,"14/Oct/09 16:27;thejas;${code}
grunt> explain filt;
#-----------------------------------------------
# Logical Plan:
#-----------------------------------------------

Store 1-1162 Schema: {name: chararray,gid: chararray} Type: Unknown
|
|---ForEach 1-1148 Schema: {name: chararray,gid: chararray} Type: bag
    |   |
    |   Project 1-1144 Projections: [0] Overloaded: false FieldSchema: name: chararray Type: chararray
    |   Input: Project 1-1145 Projections: [0] Overloaded: false|
    |   |---Project 1-1145 Projections: [0] Overloaded: false FieldSchema: group: tuple({name: chararray,gid: chararray}) Type: tuple
    |       Input: CoGroup 1-1138
    |   |
    |   Project 1-1146 Projections: [1] Overloaded: false FieldSchema: gid: chararray Type: chararray
    |   Input: Project 1-1147 Projections: [0] Overloaded: false|
    |   |---Project 1-1147 Projections: [0] Overloaded: false FieldSchema: group: tuple({name: chararray,gid: chararray}) Type: tuple
    |       Input: CoGroup 1-1138
    |
    |---CoGroup 1-1138 Schema: {group: (name: chararray,gid: chararray),f: {name: chararray,gender: chararray,age: chararray,score: chararray,gid: chararray}} Type: bag
        |   |
        |   Project 1-1136 Projections: [0] Overloaded: false FieldSchema: name: chararray Type: chararray
        |   Input: ForEach 1-1135
        |   |
        |   Project 1-1137 Projections: [4] Overloaded: false FieldSchema: gid: chararray Type: chararray
        |   Input: ForEach 1-1135
        |
        |---ForEach 1-1135 Schema: {name: chararray,gender: chararray,age: chararray,score: chararray,gid: chararray} Type: bag
            |   |
            |   Project 1-1130 Projections: [0] Overloaded: false FieldSchema: name: chararray Type: chararray
            |   Input: Filter 1-1152
            |   |
            |   Project 1-1131 Projections: [1] Overloaded: false FieldSchema: gender: chararray Type: chararray
            |   Input: Filter 1-1152
            |   |
            |   Project 1-1132 Projections: [2] Overloaded: false FieldSchema: age: chararray Type: chararray
            |   Input: Filter 1-1152
            |   |
            |   Project 1-1133 Projections: [3] Overloaded: false FieldSchema: score: chararray Type: chararray
            |   Input: Filter 1-1152
            |   |
            |   Const 1-1134( 200 ) FieldSchema: chararray Type: chararray
            |
            |---Filter 1-1152 Schema: {name: chararray,gender: chararray,age: chararray,score: chararray} Type: bag
                |   |
                |   Equal 1-1151 FieldSchema: boolean Type: boolean
                |   |
                |   |---Project 1-1149 Projections: [0] Overloaded: false FieldSchema: name: chararray Type: chararray
                |   |   Input: ForEach 1-1161
                |   |
                |   |---Const 1-1150( 200 ) FieldSchema: chararray Type: chararray
                |
                |---ForEach 1-1161 Schema: {name: chararray,gender: chararray,age: chararray,score: chararray} Type: bag
                    |   |
                    |   Cast 1-1154 FieldSchema: name: chararray Type: chararray
                    |   |
                    |   |---Project 1-1153 Projections: [0] Overloaded: false FieldSchema: name: bytearray Type: bytearray
                    |       Input: Load 1-1123
                    |   |
                    |   Cast 1-1156 FieldSchema: gender: chararray Type: chararray
                    |   |
                    |   |---Project 1-1155 Projections: [1] Overloaded: false FieldSchema: gender: bytearray Type: bytearray
                    |       Input: Load 1-1123
                    |   |
                    |   Cast 1-1158 FieldSchema: age: chararray Type: chararray
                    |   |
                    |   |---Project 1-1157 Projections: [2] Overloaded: false FieldSchema: age: bytearray Type: bytearray
                    |       Input: Load 1-1123
                    |   |
                    |   Cast 1-1160 FieldSchema: score: chararray Type: chararray
                    |   |
                    |   |---Project 1-1159 Projections: [3] Overloaded: false FieldSchema: score: bytearray Type: bytearray
                    |       Input: Load 1-1123
                    |
                    |---Load 1-1123 Schema: {name: bytearray,gender: bytearray,age: bytearray,score: bytearray} Type: bag

${code}","14/Oct/09 18:35;daijy;Actually we cannot push the filter even before f2. Since we do not keep track of the source of data inside tuple, so gid should be treated as a generated field of f2. However, projection map of f2 give us the wrong result that gid is a directly mapped field of group (which is a tuple (name, gid)), and this triggers all the subsequences. The fix for this problem is to modify the projection map generation logic for the mapped field. 

Santhosh, do you have any comment?","15/Oct/09 22:29;daijy;Seems ""project fixer up"" require nested field to be counted as mapped fields. And for pushupfilter, nested field should not be counted as a mapped fields. We need to clarify the definition of projectMap.mappedFields first. ",21/Oct/09 00:46;daijy;Attach the patch. Thanks Santhosh for helping analyze the problem.,"21/Oct/09 07:27;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422754/PIG-1022-1.patch
  against trunk revision 827829.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/104/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/104/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/104/console

This message is automatically generated.",21/Oct/09 16:24;daijy;core tests pass manually,"28/Oct/09 08:38;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422754/PIG-1022-1.patch
  against trunk revision 830335.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/119/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/119/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/119/console

This message is automatically generated.","28/Oct/09 22:20;daijy;The core test failure is temporal due to ""port conflict""","01/Dec/09 16:22;alangates;+1, patch looks good.  I also reran the unit tests since it had been a while since this patch was posted, and they all still pass.",01/Dec/09 18:34;alangates;Checked that patch in for Daniel since I'd already applied it and tested it.,02/Dec/09 01:47;alangates;Patch checked into 0.6 branch as well.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FINDBUGS: add exclude file,PIG-1019,12437923,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,13/Oct/09 01:15,25/Mar/10 00:12,14/Mar/19 03:05,13/Oct/09 16:41,,,,,,,0.6.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,13/Oct/09 01:29;olgan;PIG-1019.patch;https://issues.apache.org/jira/secure/attachment/12421933/PIG-1019.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-13 04:46:12.502,,,no_permission,,,,,,,,,,,,164549,,,,,Tue Oct 13 16:14:36 UTC 2009,,,,,,,0|i0gr0n:,95807,,,,,,,,,,"13/Oct/09 04:46;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421933/PIG-1019.patch
  against trunk revision 824446.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 292 release audit warnings (more than the trunk's current 291 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/20/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/20/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/20/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/20/console

This message is automatically generated.","13/Oct/09 15:21;olgan;-1 on tests is ok since this is not a code related patch
-1 on release audit is also ok - it is due to exclude file not having a header

can one of the committers review the patch, please.",13/Oct/09 16:14;alangates;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FINDBUGS: NM_FIELD_NAMING_CONVENTION: Field names should start with a lower case letter,PIG-1018,12437919,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,13/Oct/09 01:00,25/Mar/10 00:12,14/Mar/19 03:05,15/Oct/09 21:05,,,,,,,0.6.0,,,,,0,,,,"Nm 	The field name org.apache.pig.backend.hadoop.executionengine.physicalLayer.LogToPhyTranslationVisitor.LogToPhyMap doesn't start with a lower case letter
Nm 	The method name org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.CreateTuple(Object[]) doesn't start with a lower case letter
Nm 	The class name org.apache.pig.backend.hadoop.executionengine.physicalLayer.util.operatorHelper doesn't start with an upper case letter
Nm 	Class org.apache.pig.impl.util.WrappedIOException is not derived from an Exception, even though it is named as such
Nm 	The method name org.apache.pig.pen.EquivalenceClasses.GetEquivalenceClasses(LogicalOperator, Map) doesn't start with a lower case letter
Nm 	The field name org.apache.pig.pen.util.DisplayExamples.Result doesn't start with a lower case letter
Nm 	The method name org.apache.pig.pen.util.DisplayExamples.PrintSimple(LogicalOperator, Map) doesn't start with a lower case letter
Nm 	The method name org.apache.pig.pen.util.DisplayExamples.PrintTabular(LogicalPlan, Map) doesn't start with a lower case letter
Nm 	The method name org.apache.pig.tools.parameters.TokenMgrError.LexicalError(boolean, int, int, int, String, char) doesn't start with a lower case letter",,,,,,,,,,,,,,,,,,,14/Oct/09 22:32;olgan;PIG-1018.patch;https://issues.apache.org/jira/secure/attachment/12422153/PIG-1018.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-15 04:33:00.559,,,no_permission,,,,,,,,,,,,164548,,,,,Thu Oct 15 21:05:18 UTC 2009,,,,,,,0|i0gr07:,95805,,,,,,,,,,"15/Oct/09 04:33;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422153/PIG-1018.patch
  against trunk revision 825308.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 315 release audit warnings (more than the trunk's current 309 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/80/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/80/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/80/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/80/console

This message is automatically generated.",15/Oct/09 17:42;daijy;+1,15/Oct/09 21:05;olgan;patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[piggybank] DateExtractor should take into account timezones,PIG-1015,12437824,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,dvryaboy,dvryaboy,12/Oct/09 03:02,24/Mar/10 22:15,14/Mar/19 03:05,12/Oct/09 18:22,,,,,,,0.6.0,,,,,0,,,,"The current implementation defaults to the local timezone when parsing strings, thereby providing inconsistent results depending on the settings of the computer the program is executing on (this is causing unit test failures). We should set the timezone to a consistent default, and allow users to override this default.",,,,,,,,,,,,,,,,,,,12/Oct/09 03:19;dvryaboy;date_extractor.patch;https://issues.apache.org/jira/secure/attachment/12421836/date_extractor.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-12 06:25:08.81,,,no_permission,,,,,,,,,,,,41726,,,,,Mon Oct 12 18:22:33 UTC 2009,,,,,,,0|i0gqz3:,95800,,,,,,,piggybank,,,"12/Oct/09 03:19;dvryaboy;Note that this changes the contract slightly, as the DateExtractor extracts dates in GMT by default, whereas before it extracted them in system's local time. ","12/Oct/09 06:25;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421836/date_extractor.patch
  against trunk revision 823693.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/71/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/71/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/71/console

This message is automatically generated.","12/Oct/09 18:22;olgan;patch committed. Thanks, Dmitry!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FINDBUGS: DMI_INVOKING_TOSTRING_ON_ARRAY: Invocation of toString on an array,PIG-1013,12437774,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,10/Oct/09 00:41,25/Mar/10 00:12,14/Mar/19 03:05,19/Oct/09 20:52,,,,,,,0.6.0,,,,,0,,,,"DMI 	Invocation of toString on stackTraceLines in org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher.getExceptionFromStrings(String[], int)
DMI 	Invocation of toString on b in org.apache.pig.builtin.Utf8StorageConverter.bytesToBag(byte[])
DMI 	Invocation of toString on b in org.apache.pig.builtin.Utf8StorageConverter.bytesToDouble(byte[])
DMI 	Invocation of toString on b in org.apache.pig.builtin.Utf8StorageConverter.bytesToFloat(byte[])
DMI 	Invocation of toString on b in org.apache.pig.builtin.Utf8StorageConverter.bytesToInteger(byte[])
DMI 	Invocation of toString on b in org.apache.pig.builtin.Utf8StorageConverter.bytesToLong(byte[])
DMI 	Invocation of toString on b in org.apache.pig.builtin.Utf8StorageConverter.bytesToMap(byte[])
DMI 	Invocation of toString on b in org.apache.pig.builtin.Utf8StorageConverter.bytesToTuple(byte[])
DMI 	Invocation of toString on args in org.apache.pig.impl.PigContext.instantiateFuncFromSpec(FuncSpec)",,,,,,,,,,,,,,,,,,,16/Oct/09 19:48;olgan;PIG-1013.patch;https://issues.apache.org/jira/secure/attachment/12422395/PIG-1013.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-18 01:52:35.003,,,no_permission,,,,,,,,,,,,164546,,,,,Mon Oct 19 20:52:23 UTC 2009,,,,,,,0|i0gqyn:,95798,,,,,,,,,,"18/Oct/09 01:52;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422395/PIG-1013.patch
  against trunk revision 826110.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/95/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/95/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/95/console

This message is automatically generated.","19/Oct/09 19:45;daijy;+1, target findbugs warnings suppressed",19/Oct/09 20:52;olgan;patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FINDBUGS: SE_BAD_FIELD: Non-transient non-serializable instance field in serializable class,PIG-1012,12437770,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,09/Oct/09 23:49,24/Mar/10 22:15,14/Mar/19 03:05,21/Oct/09 20:59,,,,,,,0.6.0,,,,,0,,,,"Se 	Class org.apache.pig.backend.executionengine.PigSlice defines non-transient non-serializable instance field is
Se 	Class org.apache.pig.backend.executionengine.PigSlice defines non-transient non-serializable instance field loader
Se 	java.util.zip.GZIPInputStream stored into non-transient field PigSlice.is
Se 	org.apache.pig.backend.datastorage.SeekableInputStream stored into non-transient field PigSlice.is
Se 	org.apache.tools.bzip2r.CBZip2InputStream stored into non-transient field PigSlice.is
Se 	org.apache.pig.builtin.PigStorage stored into non-transient field PigSlice.loader
Se 	org.apache.pig.backend.hadoop.DoubleWritable$Comparator implements Comparator but not Serializable
Se 	org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler$PigBagWritableComparator implements Comparator but not Serializable
Se 	org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler$PigCharArrayWritableComparator implements Comparator but not Serializable
Se 	org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler$PigDBAWritableComparator implements Comparator but not Serializable
Se 	org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler$PigDoubleWritableComparator implements Comparator but not Serializable
Se 	org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler$PigFloatWritableComparator implements Comparator but not Serializable
Se 	org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler$PigIntWritableComparator implements Comparator but not Serializable
Se 	org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler$PigLongWritableComparator implements Comparator but not Serializable
Se 	org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler$PigTupleWritableComparator implements Comparator but not Serializable
Se 	org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler$PigWritableComparator implements Comparator but not Serializable
Se 	Class org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper defines non-transient non-serializable instance field nig
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.EqualToExpr defines non-transient non-serializable instance field log
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.GreaterThanExpr defines non-transient non-serializable instance field log
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.GTOrEqualToExpr defines non-transient non-serializable instance field log
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.LessThanExpr defines non-transient non-serializable instance field log
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.LTOrEqualToExpr defines non-transient non-serializable instance field log
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.NotEqualToExpr defines non-transient non-serializable instance field log
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast defines non-transient non-serializable instance field log
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject defines non-transient non-serializable instance field bagIterator
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserComparisonFunc defines non-transient non-serializable instance field log
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc defines non-transient non-serializable instance field log
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCombinerPackage defines non-transient non-serializable instance field log
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODemux defines non-transient non-serializable instance field log
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODistinct defines non-transient non-serializable instance field log
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach defines non-transient non-serializable instance field its
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach defines non-transient non-serializable instance field log
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin defines non-transient non-serializable instance field log
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLimit defines non-transient non-serializable instance field log
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad defines non-transient non-serializable instance field is
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad defines non-transient non-serializable instance field loader
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad defines non-transient non-serializable instance field log
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange defines non-transient non-serializable instance field log
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMultiQueryPackage defines non-transient non-serializable instance field log
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMultiQueryPackage defines non-transient non-serializable instance field myKey
	
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage defines non-transient non-serializable instance field log
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage defines non-transient non-serializable instance field tupIter
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSort defines non-transient non-serializable instance field log
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSort defines non-transient non-serializable instance field mComparator
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit defines non-transient non-serializable instance field log
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore defines non-transient non-serializable instance field impl
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore defines non-transient non-serializable instance field log
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore defines non-transient non-serializable instance field storer
Se 	Class org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStream defines non-transient non-serializable instance field executableManager
Se 	Class org.apache.pig.backend.local.executionengine.physicalLayer.relationalOperators.POCogroup defines non-transient non-serializable instance field its
Se 	org.apache.pig.backend.local.executionengine.physicalLayer.relationalOperators.POCogroup$groupComparator implements Comparator but not Serializable
Se 	Class org.apache.pig.backend.local.executionengine.physicalLayer.relationalOperators.POCross defines non-transient non-serializable instance field its
Se 	Class org.apache.pig.backend.local.executionengine.physicalLayer.relationalOperators.POSplitOutput defines non-transient non-serializable instance field it
Se 	Class org.apache.pig.data.InternalCachedBag defines non-transient non-serializable instance field factory
Se 	Class org.apache.pig.data.InternalCachedBag defines non-transient non-serializable instance field out
Se 	Class org.apache.pig.data.SortedDataBag defines non-transient non-serializable instance field mComp
Se 	org.apache.pig.data.SortedDataBag$DefaultComparator implements Comparator but not Serializable
Se 	The field org.apache.pig.impl.logicalLayer.LOLoad.mLoadFunc is transient but isn't set by deserialization
Se 	The field org.apache.pig.impl.logicalLayer.LOStore.mStoreFunc is transient but isn't set by deserialization
Se 	Class org.apache.pig.impl.logicalLayer.LOStream defines non-transient non-serializable instance field executableManager
Se 	The field org.apache.pig.impl.PigContext.extraJars is transient but isn't set by deserialization
Se 	The field org.apache.pig.impl.PigContext.skipJars is transient but isn't set by deserialization
Se 	The field org.apache.pig.impl.PigContext.log is transient but isn't set by deserialization",,,,,,,,,,,,,,,,,,,21/Oct/09 18:39;daijy;PIG-1012-2.patch;https://issues.apache.org/jira/secure/attachment/12422833/PIG-1012-2.patch,21/Oct/09 20:40;daijy;PIG-1012-3.patch;https://issues.apache.org/jira/secure/attachment/12422846/PIG-1012-3.patch,19/Oct/09 21:12;olgan;PIG-1012.patch;https://issues.apache.org/jira/secure/attachment/12422612/PIG-1012.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-10-20 04:40:59.84,,,no_permission,,,,,,,,,,,,164545,Reviewed,,,,Wed Oct 21 20:59:12 UTC 2009,,,,,,,0|i0gqy7:,95796,,,,,,,,,,"20/Oct/09 04:40;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422612/PIG-1012.patch
  against trunk revision 826821.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/102/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/102/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/102/console

This message is automatically generated.","20/Oct/09 22:40;daijy;+1, target findbugs warnings suppressed. ",20/Oct/09 22:55;olgan;patch committed,"21/Oct/09 17:59;ashutoshc;Marking ""log"" in POFRJoin transient causes FRJoin to fail. Because at the backend it can't be deserialized and log.debug is used while building hashtables resulting in NPE. Either it shouldn't be marked transient or it should be instantiated in ""readObject()"" method.  

Stack Trace:

Pig Stack Trace
---------------
ERROR 2999: Unexpected internal error. null

java.lang.NullPointerException
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin.setUpHashMap(POFRJoin.java:293)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin.getNext(POFRJoin.java:197)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:249)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:240)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.map(PigMapOnly.java:65)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)

Thanks to Tejal to pointing this out.","21/Oct/09 18:00;ashutoshc;We just looked at POFRJoin, this might be happening at other places as well.",21/Oct/09 18:45;olgan;+1; I don't think we need to rerun tests for this change. Thanks. Daniel for figuring this out!,"21/Oct/09 18:49;sms;I just looked at the first patch. It was setting generate to true in TestMRCompiler.java It should be set to false in order to run the test case correctly.

+++ test/org/apache/pig/test/TestMRCompiler.java

-    private boolean generate = false;
+    private boolean generate = true;","21/Oct/09 20:33;olgan;good catch! Daniel, could you swap the value in your patch",21/Oct/09 20:59;daijy;Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FINDBUGS: SE_NO_SERIALVERSIONID: Class is Serializable, but doesn't define serialVersionUID",PIG-1011,12437767,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,09/Oct/09 23:34,25/Mar/10 00:12,14/Mar/19 03:05,19/Oct/09 20:49,,,,,,,0.6.0,,,,,0,,,,SnVI  org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODistinct is Serializable; consider declaring a SnVI  org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PORead is Serializable; consider declaring a serialVersionUID,,,,,,,,,,,,,,,,,,,16/Oct/09 18:29;olgan;PIG-1011.patch;https://issues.apache.org/jira/secure/attachment/12422381/PIG-1011.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-17 19:48:13.727,,,no_permission,,,,,,,,,,,,164544,,,,,Mon Oct 19 20:49:11 UTC 2009,,,,,,,0|i0gqxr:,95794,,,,,,,,,,"17/Oct/09 19:48;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422381/PIG-1011.patch
  against trunk revision 826110.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 6 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/93/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/93/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/93/console

This message is automatically generated.","19/Oct/09 20:00;daijy;+1, target findbugs warnings are suppressed.",19/Oct/09 20:49;olgan;patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FINDBUGS: RV_RETURN_VALUE_IGNORED_BAD_PRACTICE,PIG-1010,12437763,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,09/Oct/09 22:58,25/Mar/10 00:12,14/Mar/19 03:05,30/Oct/09 23:19,,,,,,,0.6.0,,,,,0,,,,"RV 	org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.deleteLocalDir(File) ignores exceptional return value of java.io.File.delete()
RV 	org.apache.pig.backend.local.datastorage.LocalPath.delete() ignores exceptional return value of java.io.File.delete()
RV 	org.apache.pig.data.DefaultAbstractBag.clear() ignores exceptional return value of java.io.File.delete()
RV 	org.apache.pig.data.DefaultAbstractBag.finalize() ignores exceptional return value of java.io.File.delete()
RV 	org.apache.pig.impl.io.FileLocalizer.create(String, boolean, PigContext) ignores exceptional return value of java.io.File.mkdirs()",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,164543,,,,,Fri Oct 30 23:19:36 UTC 2009,,,,,,,0|i0gqxb:,95792,,,,,,,,,,30/Oct/09 23:19;olgan;already resolved,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FINDBUGS: OS_OPEN_STREAM: Method may fail to close stream,PIG-1009,12437762,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,09/Oct/09 22:55,25/Mar/10 00:12,14/Mar/19 03:05,16/Oct/09 00:41,,,,,,,0.6.0,,,,,0,,,,"OS 	org.apache.pig.impl.io.FileLocalizer.parseCygPath(String, int) may fail to close stream
OS 	org.apache.pig.impl.logicalLayer.parser.QueryParser.which(String) may fail to close stream
OS 	org.apache.pig.impl.util.PropertiesUtil.loadPropertiesFromFile(Properties) may fail to close stream
OS 	org.apache.pig.Main.configureLog4J(Properties, PigContext) may fail to close stream
OS 	org.apache.pig.tools.parameters.PreprocessorContext.executeShellCommand(String) may fail to close stream
OS 	org.apache.pig.tools.parameters.PreprocessorContext.executeShellCommand(String) may fail to close stream",,,,,,,,,,,,,,,,,,,15/Oct/09 00:04;olgan;PIG-1009.patch;https://issues.apache.org/jira/secure/attachment/12422166/PIG-1009.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-15 06:36:55.812,,,no_permission,,,,,,,,,,,,164542,,,,,Thu Oct 15 23:02:59 UTC 2009,,,,,,,0|i0gqwv:,95790,,,,,,,,,,"15/Oct/09 06:36;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422166/PIG-1009.patch
  against trunk revision 825375.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/81/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/81/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/81/console

This message is automatically generated.","15/Oct/09 17:56;daijy;+1, target findbugs warnings suppressed. ","15/Oct/09 22:47;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422166/PIG-1009.patch
  against trunk revision 825601.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/83/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/83/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/83/console

This message is automatically generated.","15/Oct/09 23:02;daijy;+1, target findbugs warnings suppressed. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FINDBUGS: NP_TOSTRING_COULD_RETURN_NULL,PIG-1008,12437758,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,09/Oct/09 21:59,25/Mar/10 00:12,14/Mar/19 03:05,15/Oct/09 21:08,,,,,,,0.6.0,,,,,0,,,,"NP 	org.apache.pig.data.DataByteArray.toString() may return null
NP 	org.apache.pig.impl.streaming.StreamingCommand$HandleSpec.equals(Object) does not check for null argument",,,,,,,,,,,,,,,,,,,14/Oct/09 22:59;olgan;PIG-1008.patch;https://issues.apache.org/jira/secure/attachment/12422155/PIG-1008.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-15 04:46:24.563,,,no_permission,,,,,,,,,,,,164541,,,,,Thu Oct 15 18:08:46 UTC 2009,,,,,,,0|i0gqwf:,95788,,,,,,,,,,"15/Oct/09 04:46;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422155/PIG-1008.patch
  against trunk revision 825308.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/27/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/27/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/27/console

This message is automatically generated.",15/Oct/09 18:08;daijy;+1. Two target findbugs warnings suppressed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FINDBUGS: HE_EQUALS_USE_HASHCODE,PIG-1007,12437755,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,09/Oct/09 21:28,25/Mar/10 00:12,14/Mar/19 03:05,30/Oct/09 23:19,,,,,,,0.6.0,,,,,0,,,,"HE                org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan defines equals and uses Object.hashCode()
HE	org.apache.pig.backend.local.executionengine.physicalLayer.relationalOperators.POCogroup$groupComparator defines equals and uses Object.hashCode()
HE 	org.apache.pig.builtin.BinaryStorage defines equals and uses Object.hashCode()
HE 	org.apache.pig.builtin.BinStorage defines equals and uses Object.hashCode()
HE 	org.apache.pig.builtin.PigStorage defines equals and uses Object.hashCode()
HE 	org.apache.pig.data.NonSpillableDataBag defines equals and uses Object.hashCode()
HE 	org.apache.pig.data.SortedDataBag$DefaultComparator defines equals and uses Object.hashCode()
HE 	org.apache.pig.impl.streaming.StreamingCommand$HandleSpec defines equals and uses Object.hashCode()",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,164540,,,,,Fri Oct 30 23:19:16 UTC 2009,,,,,,,0|i0gqvz:,95786,,,,,,,,,,30/Oct/09 23:19;olgan;already resolved,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FINDBUGS: EQ_COMPARETO_USE_OBJECT_EQUALS in bags and tuples,PIG-1006,12437745,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,09/Oct/09 20:22,25/Mar/10 00:12,14/Mar/19 03:05,30/Oct/09 23:18,,,,,,,0.6.0,,,,,0,,,,"Eq 	org.apache.pig.data.DistinctDataBag$DistinctDataBagIterator$TContainer defines compareTo(DistinctDataBag$DistinctDataBagIterator$TContainer) and uses Object.equals()
Eq 	org.apache.pig.data.SingleTupleBag defines compareTo(Object) and uses Object.equals()
Eq 	org.apache.pig.data.SortedDataBag$SortedDataBagIterator$PQContainer defines compareTo(SortedDataBag$SortedDataBagIterator$PQContainer) and uses Object.equals()
Eq 	org.apache.pig.data.TargetedTuple defines compareTo(Object) and uses Object.equals()
Eq 	org.apache.pig.pen.util.ExampleTuple defines compareTo(Object) and uses Object.equals()",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,164539,,,,,Fri Oct 30 23:18:56 UTC 2009,,,,,,,0|i0gqvj:,95784,,,,,,,,,,30/Oct/09 23:18;olgan;already resolved,30/Oct/09 23:18;olgan;already resolved,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FINDBUGS: BC: Equals method should not assume anything about the type of its argument ,PIG-1002,12437740,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,09/Oct/09 19:36,25/Mar/10 00:12,14/Mar/19 03:05,03/Nov/09 22:18,,,,,,,0.6.0,,,,,0,,,,"BC 	Equals method for org.apache.pig.builtin.PigStorage assumes the argument is of type PigStorage
BC 	Equals method for org.apache.pig.impl.streaming.StreamingCommand$HandleSpec assumes the argument is of type StreamingCommand$HandleSpec",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,164535,,,,,Tue Nov 03 22:18:05 UTC 2009,,,,,,,0|i0gqtz:,95777,,,,,,,,,,03/Nov/09 22:18;olgan;this has been addressed in other JIRAs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generate more meaningful error message when one input file does not exist,PIG-1001,12437665,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,09/Oct/09 00:12,24/Mar/10 22:15,14/Mar/19 03:05,06/Nov/09 00:47,0.4.0,,,,,,0.6.0,,,,,0,,,,"In the following query, if 1.txt does not exist, 

a = load '1.txt';
b = group a by $0;
c = group b all;
dump c;

Pig throws error message ""ERROR 2100: file:/tmp/temp155054664/tmp1144108421 does not exist."", Pig should deal with it with the error message ""Input file 1.txt not exist"" instead of those confusing messages.",,,,,,,,,,,,,,,,,,,13/Oct/09 06:53;daijy;PIG-1001-1.patch;https://issues.apache.org/jira/secure/attachment/12421956/PIG-1001-1.patch,28/Oct/09 23:36;daijy;PIG-1001-2.patch;https://issues.apache.org/jira/secure/attachment/12423510/PIG-1001-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-10-13 09:51:30.647,,,no_permission,,,,,,,,,,,,164534,Reviewed,,,,Wed Nov 11 18:18:00 UTC 2009,,,,,,,0|i0gqtr:,95776,,,,,,,,,,"13/Oct/09 07:01;daijy;Change the error handling for map-reduce jobs. Changes are:
1. If one job complete fail (If we have only one store and that job fail), then we do not start all dependent map-reduce jobs
2. Before, we do not report error message of MR job with temporary store. It is possible that we miss the root cause of the entire script. So we instead print error message of all MR jobs we kick off. With the change of not starting dependent map-reduce jobs, hopefully only the error message of the MR job causing the problem printed out.
3. Change the way we report error message when ""-F"" is on. Before, we only report ""fail to produce xxxx"", now we print out the error message that cause the problem.","13/Oct/09 09:51;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421956/PIG-1001-1.patch
  against trunk revision 824446.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/21/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/21/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/21/console

This message is automatically generated.","28/Oct/09 19:20;alangates;I have a question on this code in JobControlCompiler.updateMROpPlan:

{code}
        for (Job job : completeFailedJobs)  // remove all subsequent jobs
        {
            List<MapReduceOper> fifo = new ArrayList<MapReduceOper>();
            fifo.add(jobMroMap.get(job));
            while (!fifo.isEmpty())
            {
                MapReduceOper mro = fifo.remove(0);
                List<MapReduceOper> succs = plan.getSuccessors(mro);
                if (succs != null)
                    fifo.addAll(succs);
                plan.remove(mro);
                numRemoved++;
            }
        }
{code}

If we have a dependency graph like:  A->B->C and A fails, won't the above code
only remove B and not C?  OperatorPlan.getSuccessors() only gets immediate
successors, not all successors.  I think you want OperatorPlan.trimBelow()
instead.
","28/Oct/09 22:15;daijy;Hi, Alan,
The code do remove B and C. It uses fifo to achieve that to avoid recursion. But you reminds me that we have trimBlow(). We should use trimBlow() instead of implementing a duplicate logic. ",28/Oct/09 23:36;daijy;Reattach the patch per Alan's comment.,"29/Oct/09 17:55;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12423510/PIG-1001-2.patch
  against trunk revision 830757.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/31/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/31/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/31/console

This message is automatically generated.","05/Nov/09 17:50;alangates;Changes look good, +1.",06/Nov/09 00:47;daijy;Patch committed.,"11/Nov/09 18:05;nidaley;Please ensure the issue is ""Assigned"" to the patch author when committing.

Also, please provide a justification for why there is no unit test, then describe how you DID test it before you uploaded the patch.","11/Nov/09 18:18;daijy;Hi, Nigel,
Since this patch is all about error message and user experience, so it is hard to write a unit test case for that. I tested it manually with the following situations:
1. All jobs are successful
2. One complete failed job, and we stop launching dependent jobs
3. Two independent job, one fail, with and without -F option

All of those give us desired error messages.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
revisit frontend logic and pig-latin semantics,PIG-998,12437538,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,thejas,thejas,07/Oct/09 19:36,04/Aug/11 00:34,14/Mar/19 03:05,15/Mar/11 01:50,,,,,,,0.9.0,,,,,0,,,,"This jira has been created to keep track of issues with current frontend logic and pig-latin semantics.
One example is handling of type information of map-values. At time of  query plan generation pig does not know the type for map-values and assumes it is bytearray. This leads to problems when the loader returns map-value of other types.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-15 01:50:32.02,,,no_permission,,,,,,,,,,,,66318,,,,,Tue Mar 15 01:50:32 UTC 2011,,,,,,,0|i0gqs7:,95769,,,,,,,,,,15/Mar/11 01:50;daijy;All depending Jira are fixed. Close umbrella Jira.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[zebra] Zebra build script does not have findbugs and clover targets.,PIG-996,12437444,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chaow,chaow,chaow,06/Oct/09 23:52,24/Mar/10 22:15,14/Mar/19 03:05,23/Oct/09 20:01,0.4.0,,,,,,0.6.0,,build,,,0,,,,"Zebra build script does not have findbugs and clover targets, leading hudson build process to fail on Zebra.

This jira is to fix this by adding these two targets.",,,,,,,,,,,,,,,,,,,19/Oct/09 22:11;chaow;patch_build;https://issues.apache.org/jira/secure/attachment/12422619/patch_build,07/Oct/09 20:29;chaow;patch_build;https://issues.apache.org/jira/secure/attachment/12421577/patch_build,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-10-07 23:44:00.467,,,no_permission,,,,,,,,,,,,164531,,,,,Fri Oct 23 20:01:38 UTC 2009,,,,,,,0|i0gqrb:,95765,,,,,,,,,,07/Oct/09 21:22;chaow;Also added a dummy checkstyle target. ,"07/Oct/09 23:44;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421577/patch_build
  against trunk revision 822382.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 5 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/14/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/14/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/14/console

This message is automatically generated.","08/Oct/09 00:57;jing1234;+1 
Patch reviewed.",12/Oct/09 22:59;chaow;This patch should be applied only after pig-944 patch has been applied - cancelling for now - will re-submit again.,"20/Oct/09 18:45;jing1234;+1
New patch reviewed.","22/Oct/09 03:07;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422619/patch_build
  against trunk revision 828213.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 4 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/108/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/108/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/108/console

This message is automatically generated.","22/Oct/09 05:14;gkesavan;How do we want this findbugs and checkstsyle to be run?
with this patch if someone wants to execute findbugs they cant run it from the top level pig project..
 
Can we not use subant task to run finbugs on zebra as well when findbugs at the root level(pig tunk) is called?




",23/Oct/09 20:01;alangates;Checked in the patch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Limit Optimizer throw exception ""ERROR 2156: Error while fixing projections""",PIG-995,12437306,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,05/Oct/09 18:53,24/Mar/10 22:15,14/Mar/19 03:05,09/Oct/09 21:14,0.3.0,,,,,,0.6.0,,impl,,,0,,,,"The following script fail:

A = load '1.txt' AS (a0, a1, a2);
B = order A by a1;
C = limit B 10;
D = foreach C generate $0;
dump D;

Error log:
Caused by: org.apache.pig.impl.plan.VisitorException: ERROR 2156: Error while fixing projections. Projection map of node to be replaced is null.
        at org.apache.pig.impl.logicalLayer.ProjectFixerUpper.visit(ProjectFixerUpper.java:138)
        at org.apache.pig.impl.logicalLayer.LOProject.visit(LOProject.java:408)
        at org.apache.pig.impl.logicalLayer.LOProject.visit(LOProject.java:58)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:65)
        at org.apache.pig.impl.plan.DepthFirstWalker.walk(DepthFirstWalker.java:50)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.impl.logicalLayer.LOForEach.rewire(LOForEach.java:761)",,,,,,,,,,,,,,,,,,,07/Oct/09 21:49;daijy;PIG-995-1.patch;https://issues.apache.org/jira/secure/attachment/12421587/PIG-995-1.patch,08/Oct/09 22:01;daijy;PIG-995-2.patch;https://issues.apache.org/jira/secure/attachment/12421671/PIG-995-2.patch,09/Oct/09 17:26;daijy;PIG-995-3.patch;https://issues.apache.org/jira/secure/attachment/12421753/PIG-995-3.patch,09/Oct/09 18:01;daijy;PIG-995-4.patch;https://issues.apache.org/jira/secure/attachment/12421755/PIG-995-4.patch,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2009-10-08 00:51:22.606,,,no_permission,,,,,,,,,,,,164530,Reviewed,,,,Fri Oct 09 21:14:47 UTC 2009,,,,,,,0|i0gqqv:,95763,,,,,,,,,,"08/Oct/09 00:51;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421587/PIG-995-1.patch
  against trunk revision 822382.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/65/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/65/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/65/console

This message is automatically generated.",08/Oct/09 20:51;pkamath;+1,"08/Oct/09 22:01;daijy;After discussion with Santhosh, I get a better patch. The problem is we do not generate projection map before applying optimization rules. If the optimization rules change the structure of the logical plan and then generate the projection map, we will end up using a wrong projection map. In the new patch, we regenerate projection map before applying each optimization rule.","09/Oct/09 03:48;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421671/PIG-995-2.patch
  against trunk revision 823257.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/67/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/67/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/67/console

This message is automatically generated.","09/Oct/09 17:26;daijy;Change the patch a little to make it more efficient. We don't have to do regenerate projection map every time before applying the rule and then appying agan after the rule, it is a duplication. We only need to regenerate projection map the first time.","09/Oct/09 17:51;sms;Review comments:

The initialization code is fine. However, the try catch block is shared between the rebuildSchemas() and rebuildProjectionMaps() method invocation. This could lead to misleading error message. Specifically, if the rebuildSchemas() throws an exception then the error message will indicate that rebuilding projection maps failed.",09/Oct/09 18:01;daijy;Change the error message as per Santhosh's review.,"09/Oct/09 20:26;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421753/PIG-995-3.patch
  against trunk revision 823412.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/68/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/68/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/68/console

This message is automatically generated.",09/Oct/09 21:14;daijy;Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[zebra] Abitlity to drop a column group in a table,PIG-993,12437166,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rangadi,rangadi,rangadi,02/Oct/09 22:37,02/May/13 02:29,14/Mar/19 03:05,16/Oct/09 22:16,,,,,,,0.6.0,,,,,0,,,,"A Zebra table is stored as multiple sub tables each containing a set of columns called column group (CG). The user specifies how these columns are grouped while creating a table through the _storage hint_.

For some of the large tables, it might be necessary for users to remove a set of columns and retain the rest. This jira provides a way for users to delete an entire column group. 

The following comments will have more details on API and the semantics. ",,,,,,,,,,,,,,,,,PIG-986,PIG-992,02/Oct/09 23:00;rangadi;DropColumnGroupExample.java;https://issues.apache.org/jira/secure/attachment/12421170/DropColumnGroupExample.java,16/Oct/09 17:17;alangates;TEST-org.apache.hadoop.zebra.io.TestCheckin.txt;https://issues.apache.org/jira/secure/attachment/12422374/TEST-org.apache.hadoop.zebra.io.TestCheckin.txt,12/Oct/09 17:05;yanz;zebra-drop-cg.patch;https://issues.apache.org/jira/secure/attachment/12421881/zebra-drop-cg.patch,03/Oct/09 01:15;yanz;zebra-drop-cg.patch;https://issues.apache.org/jira/secure/attachment/12421184/zebra-drop-cg.patch,02/Oct/09 23:00;rangadi;zebra-drop-cg.patch;https://issues.apache.org/jira/secure/attachment/12421171/zebra-drop-cg.patch,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2009-10-03 01:12:47.766,,,no_permission,,,,,,,,,,,,164528,,,,,Thu Oct 29 23:11:09 UTC 2009,,,,,,,0|i0gqpz:,95759,,,,,,,,,,"02/Oct/09 22:49;rangadi;
API  is pretty simple : {code}
class org.apache.hadoop.zebra.BasicTable {
 /** see the patch for JavaDoc and attached example for usage */

public static void dropColumnGroup(Path path,
                                   Configuration conf,   String cgName)
                               throws IOException { ... }
}
{code}

  * Table schema is not modified.  
  * this API takes a name for a column group. PIG-986 adds explicit names for CGs.
  * Once a CGs is deleted, NULL is returned for the fields that were stored in the CG. 
     ** This is the main difference between just manually deleting  a directory on filesystem and 'properly' deleting a CG.
     ** Many changes made in other parts of zebra are related to handling the missing CGs.
","02/Oct/09 22:52;rangadi;Deletion procedure : 

   # Check if a column group with the given name exists and throw an error if there is no such group.
   # If the column group is already deleted return normally.
      ** If a column group is already marked deleted and the corresponding physical directory still 
        exists, try to remove the the column group data again. An earlier attempt might not have
        removed the directory.
   # Create a an empty file "".deleted-CGNAME"" in the top level directory. 
   # If the creation fails, check if the file already exists. This can happen when two users concurrently
      try to delete the same column group. If CG is marked deleted after this, return success. Exception is 
      thrown for any other error.
   # Delete the column group directory. 
   # An exception is thrown if deletion fails. Note that, column group is already marked deleted even though 
      the deletion of a directory failed. A subsequent deletion of such a column group will again try to to delete the directory.","02/Oct/09 23:00;rangadi;Attachments ; 

  DropColumnGropuExample.java : a simple example to illustrate the functionality.

  zebra-drop-cg.patch : This patch would apply only after a patch for PIG-896.
    
  Some of the tests included there are written by Jing Huang. Jing also helped with testing the patchon real clusters with various errors. Yan Zhou helped with correctly handling missing column groups.

","02/Oct/09 23:03;rangadi;> zebra-drop-cg.patch : This patch would apply only after a patch for PIG-896.
I meant say PIG-986.
",03/Oct/09 01:12;yanz;this patch should be applied after the patch for Jira992 has been applied.,03/Oct/09 01:15;yanz;This patych should be applied after the patch for Jira992 is applied.,06/Oct/09 17:52;yanz;The patch attached by me (Yan Zhou) was based upon Raghu's patch minus some unrelated changes.,06/Oct/09 18:37;yanz;Patch Reviewed +1,12/Oct/09 05:03;rangadi;This patch depends on PIG-992. It is not a functional dependency and can be removed if required.,"12/Oct/09 16:57;yanz;This new version  of patch is needed due to the upstream changes in Pig-992. Although this Jira is functionally independent of Pig-992, but the application sequence of the patches must be honored.","16/Oct/09 16:16;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421881/zebra-drop-cg.patch
  against trunk revision 825712.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 33 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/88/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/88/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/88/console

This message is automatically generated.","16/Oct/09 17:17;alangates;When I run the zebra unit tests for this patch, I get a failure from org.apache.hadoop.zebra.io.TestCheckin.txt.  Output of that test attached.",16/Oct/09 22:12;alangates;We looked over the failure info and couldn't understand why it was failing.  I've rerun the unit tests multiple times since and seen no issue.  We've run it on several different machines and not seen an issue.  So I'm going to declare the test failure a fluke and commit the patch.,16/Oct/09 22:16;alangates;Patch checked in.,"17/Oct/09 06:40;rangadi;
I think the test needs to be fixed.  It deletes 6 column groups from 6 different threads. The spec explicitly states read accesses and parallel deletions expected to fail. But the table is always left in consistent state. The rationale for this is that in practice these tables are accessed from different machines and it would add unnecessary complication to support coordinate all the readers and the writers (especially with no locking support on HDFS). Zebra tables have no state outside the directory. This applies to writing as well.

One options I see is to make each thread make multiple attempts in case of errors. 
  ","29/Oct/09 23:11;chaow;Raghu's comment has been addressed in Jira 1057 :

""[Zebra] Zebra does not support concurrent deletions of column groups now.""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[zebra] A few minor bugs as described in the Description section,PIG-991,12437142,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,yanz,yanz,yanz,02/Oct/09 17:45,02/May/13 02:29,14/Mar/19 03:05,09/Oct/09 05:15,0.4.0,,,,,,0.6.0,,,,,0,,,,"1) ""lzo2"" was used as the compressor name for the LZO compression algorithm; it should be ""lzo"" instead;
2) the default compression is changed from ""lzo"" to ""gz"" for gzip;
3) In JAVACC file SchemaParser.jjt, the package name was wrong using the old ""package org.apache.pig.table.types"";
4) in build.xml, two new javacc targets are added to generate TableSchemaParser and TableStorageParser java codes;
5) Support of column group security ( https://issues.apache.org/jira/browse/PIG-987 ) lacked support of the dumpinfo method: the groups and permissions were not displayed. Note that as a consequence, the patch herein must be applied after that of JIRA987.
6) and 7) a couple of issues reported in Jira917.",,,,,,,,,,,,,PIG-917,,,,PIG-987,,09/Oct/09 05:12;rangadi;Bugs-2.patch;https://issues.apache.org/jira/secure/attachment/12421705/Bugs-2.patch,02/Oct/09 18:43;yanz;Bugs.patch;https://issues.apache.org/jira/secure/attachment/12421138/Bugs.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-10-02 18:55:22.887,,,no_permission,,,,,,,,,,,,164526,,,,,Fri Oct 09 05:15:40 UTC 2009,,,,,,,0|i0gqp3:,95755,,,,,,,,,,"02/Oct/09 18:55;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421138/Bugs.patch
  against trunk revision 821101.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 18 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/57/console

This message is automatically generated.","06/Oct/09 18:42;gauravj;

Patch Reviewed 
+1","06/Oct/09 23:30;rangadi;bq. Patch should be applied after that of Jira987.

[moved above comment from 'Release Notes' to this comment].","09/Oct/09 05:12;rangadi;I am committing a slightly modified patch. I removed the following lines that modified build.xml at the top level. Please ask one of the PIG committers to commit that change.

The part that is removed :
{noformat}
@@ -940,4 +942,13 @@

      <target name=""published"" depends=""ivy-publish-local, maven-artifacts""/>

+    <target name=""pig-test"">
+    <jar
+      jarfile=""${build.dir}/pig-test-${version}.jar""
+      basedir=""${build.dir}/test/classes""
+      excludes=""**/Test*.class""
+    >
+    </jar>
+    </target>
+
 </project>
{noformat}",09/Oct/09 05:15;rangadi;I just committed this. Thanks Yan.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide a way to pin LogicalOperator Options,PIG-990,12437094,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,dvryaboy,dvryaboy,dvryaboy,02/Oct/09 02:58,14/May/10 06:45,14/Mar/19 03:05,01/Dec/09 21:05,,,,,,,0.7.0,,impl,,,0,,,,"This is a proactive patch, setting up the groundwork for adding an optimizer.

Some of the LogicalOperators have options. For example, LOJoin has a variety of join types (regular, fr, skewed, merge), which can be set by the user or chosen by a hypothetical optimizer.  If a user selects a join type, pig philoophy guides us to always respect the user's choice and not explore alternatives.  Therefore, we need a way to ""pin"" options.  ",,,,,,,,,,,,,,,,,,,02/Oct/09 03:05;dvryaboy;pinned_options.patch;https://issues.apache.org/jira/secure/attachment/12421095/pinned_options.patch,22/Oct/09 23:47;dvryaboy;pinned_options_2.patch;https://issues.apache.org/jira/secure/attachment/12422971/pinned_options_2.patch,25/Nov/09 23:53;dvryaboy;pinned_options_3.patch;https://issues.apache.org/jira/secure/attachment/12426160/pinned_options_3.patch,26/Nov/09 02:10;dvryaboy;pinned_options_4.patch;https://issues.apache.org/jira/secure/attachment/12426178/pinned_options_4.patch,26/Nov/09 21:57;dvryaboy;pinned_options_5.patch;https://issues.apache.org/jira/secure/attachment/12426246/pinned_options_5.patch,27/Nov/09 21:15;dvryaboy;pinned_options_6.patch;https://issues.apache.org/jira/secure/attachment/12426301/pinned_options_6.patch,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2009-10-12 21:35:32.153,,,no_permission,,,,,,,,,,,,41727,,,,,Tue Dec 01 21:05:33 UTC 2009,,,,,,,0|i0gqon:,95753,,,,,,,,,,"02/Oct/09 03:05;dvryaboy;This patch does a few things:

1) Introduce a HashSet of pinnedOptions to LogicalOperator.

2) Add a class constant to LOJoin to represent the ""type of join"" option

3) Modify the parser to pin the OPTION_JOIN when it is explicitly specified in the pig script

4) Introduce the syntax 'using ""regular""' to pin the default (regular) join type. Currently that's redundant, but it's required if we start choosing join types for the user, and the user wants to tell us that this particular join must be done in the regular way.

5) minor cleanup in this area of the parser to use just one LogicalOperator variable instead of three.

No rush to commit this as it's not being used by anything yet, but posting it for review now so as to let people review the cbo-oriented changes one at a time instead of a giant code dump.  

Note that if/when we add a group type as per PIG-984, that should also be an option that can be pinned by the user.","12/Oct/09 21:35;alangates;Looks good.

One comment, rather than referring to the default join as ""regular"" I think we should refer to it as ""hash"" or ""symmetric hash"", since these accurately describe how it works.  That way users can specify the type of join they want, and if for whatever reason we switch the default they'll still get what they want.","12/Oct/09 21:48;dvryaboy;Thanks for reviewing.

I only called it ""regular"" because that's what it was called in the enum that already existed inside LOJoin.

Maybe a better option would be to call the currently-default join 'hash', and also provide a 'default' key that, for now, will translate to hash, but can translate to something else (and stay pinned) if the defaults change. I'll add that to the next iteration of the patch (which will also contain whatever keyword is decided on for the map-side groups).","22/Oct/09 23:47;dvryaboy;- ""using regular"" will now translate into the default join method (hash join at the moment).
- ""using hash"" is an option in case we change what ""regular"" is
- renamed REGULAR to HASH in the Join code
- added the new group type option to CoGROUP, pinned where appropriate.",25/Nov/09 21:54;alangates;Is this ready to be run through the tests and possibly committed?  It looks ready to go but it's never been marked patch available.,"25/Nov/09 22:01;dvryaboy;I need to update it to apply to current trunk (I have the code but it's mixed in with some of my CBO work).
I'll do that today.",25/Nov/09 23:53;dvryaboy;updated to work with latest trunk (r884299),"26/Nov/09 00:13;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426160/pinned_options_3.patch
  against trunk revision 884235.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to cause Findbugs to fail.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/55/testReport/
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/55/console

This message is automatically generated.","26/Nov/09 00:26;ashutoshc;We need to make a distinction between ""user-pinned"" and ""system-pinned"" options.  I guess that needs to be added.","26/Nov/09 00:31;dvryaboy;This patch lost a parenthesis somewhere in my merge attempt.  I am running the fixed version through test-commit and findbugs.

Ashutosh: pinOptions is userPinned. If we want to add systemPinned, we can do that as a separate patch (we don't even need that for the cbo as it stands, right?). 

",26/Nov/09 01:47;ashutoshc;Right. This jira is about user pinned so this is good to go. We do need system pinned for cbo. Lets discuss that offline.,"26/Nov/09 17:15;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426178/pinned_options_4.patch
  against trunk revision 884235.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/59/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/59/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/59/console

This message is automatically generated.","26/Nov/09 21:22;dvryaboy;added a test for pinning group and join options.
","27/Nov/09 05:56;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426246/pinned_options_5.patch
  against trunk revision 884235.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 363 release audit warnings (more than the trunk's current 362 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/63/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/63/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/63/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/63/console

This message is automatically generated.",27/Nov/09 21:15;dvryaboy;added Apache License header to the TestPinOptions.java,"28/Nov/09 05:45;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426301/pinned_options_6.patch
  against trunk revision 884235.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/65/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/65/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/65/console

This message is automatically generated.",01/Dec/09 21:05;alangates;Patch checked in.  Thanks Dmitry.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ERROR 2100 (hdfs://localhost/tmp/temp175740929/tmp-1126214010 does not exist) and ERROR 2999: (Unexpected internal error. null) when using Multi-Query optimization,PIG-978,12436654,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,viraj,viraj,25/Sep/09 23:04,24/Mar/10 22:15,14/Mar/19 03:05,01/Dec/09 21:49,0.6.0,,,,,,0.6.0,,documentation,,,0,,,,"I have  Pig script of this form.. which I execute using Multi-query optimization.

{code}
A = load '/user/viraj/firstinput' using PigStorage();
B = group ....
C = ..agrregation function
store C into '/user/viraj/firstinputtempresult/days1';
..
Atab = load '/user/viraj/secondinput' using PigStorage();
Btab = group ....
Ctab = ..agrregation function
store Ctab into '/user/viraj/secondinputtempresult/days1';
..
E = load '/user/viraj/firstinputtempresult/' using PigStorage();
F = group 
G = aggregation function
store G into '/user/viraj/finalresult1';

Etab = load '/user/viraj/secondinputtempresult/' using PigStorage();
Ftab = group 
Gtab = aggregation function
store Gtab into '/user/viraj/finalresult2';
{code}


2009-07-20 22:05:44,507 [main] ERROR org.apache.pig.tools.grunt.GruntParser - ERROR 2100: hdfs://localhost/tmp/temp175740929/tmp-1126214010 does not exist. Details at logfile: /homes/viraj/pigscripts/pig_1248127173601.log)  

is due to the mismatch of store/load commands. The script first stores files into the 'days1' directory (store C into '/user/viraj/firstinputtempresult/days1' using PigStorage();), but it later loads from the top level directory (E = load '/user/viraj/firstinputtempresult/' using PigStorage()) instead of the original directory (/user/viraj/firstinputtempresult/days1).

The current multi-query optimizer can't solve the dependency between these two commands--they have different load file paths. So the jobs will run concurrently and result in the errors.

The solution is to add 'exec' or 'run' command after the first two stores . This will force the first two store commands to run before the rest commands.

It would be nice to see this fixed as a part of an enhancement to the Multi-query. We either disable the Multi-query or throw a warning/error message, so that the user can correct his load/store statements.

Viraj",,,,,,,,,,,,,,,,,,,01/Dec/09 21:27;chandec;pig-latin-users-guide-2.patch;https://issues.apache.org/jira/secure/attachment/12426581/pig-latin-users-guide-2.patch,30/Nov/09 22:07;chandec;pig-latin-users-guide.patch;https://issues.apache.org/jira/secure/attachment/12426454/pig-latin-users-guide.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-10-05 20:06:47.982,,,no_permission,,,,,,,,,,,,164515,,,,,Tue Dec 01 21:49:48 UTC 2009,,,,,,,0|i0gqjr:,95731,,,,,,,,,,"05/Oct/09 20:06;olgan;Richard, please, provide information necessary to document this issue.","05/Oct/09 20:07;olgan;Viraj,

We have no plan to address this dependency. This will need to be documented as one of the implicit dependencies that need to be resolved by the user.","26/Oct/09 21:55;rding;In Pig Latin Manual, this is called ""Implicit Dependencies in Multi-Query Execution"": 

{quote}
*Implicit Dependencies*

If a script has dependencies on the execution order outside of what Pig knows about, execution may fail. For instance, in this script MYUDF might try to read from out1, a file that A was just stored into. However, Pig does not know that MYUDF depends on the out1 file and might submit the jobs producing the out2 and out1 files at the same time. To make the script work (to ensure that the right execution order is enforced) add the exec statement. The exec statement will trigger the execution of the statements that produce the out1 file.
{quote}

The Pig script in this Jira shows another form of those ""implicit dependencies"" in multi-query scripts. Namely, the store/load operators have different file paths, but the load operator actually depends the store operator. An exec statement should be inserted between the store and load statements to ensure the right execution order is enforced.","30/Nov/09 22:07;chandec;Patch file.

Upated Pig Latin Users Guide: Implicit Dependencies","30/Nov/09 22:09;chandec;Apply patch to trunk: http://svn.apache.org/repos/asf/hadoop/pig/trunk

Note: No new test code required; changes to documentation only.","01/Dec/09 06:58;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12426454/pig-latin-users-guide.patch
  against trunk revision 885465.

    +1 @author.  The patch does not contain any @author tags.

    +0 tests included.  The patch appears to be a documentation patch that doesn't require tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/69/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/69/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/69/console

This message is automatically generated.","01/Dec/09 20:52;olgan;Corinne,

The comment from Richard is that exec or run should go after the second store but in your example, it is after the third one.","01/Dec/09 21:27;chandec;Patch #2 (moved EXEC statement).

Opps. I actually walked over asked Richard where the EXEC should go, but I guess things got mixed up.","01/Dec/09 21:49;olgan;Patch committed to both the trunk and 0.6 branch. Thanks, Corinne!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
exit status does not account for JOB_STATUS.TERMINATED,PIG-977,12436636,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ashutoshc,thejas,thejas,25/Sep/09 20:03,14/May/10 06:45,14/Mar/19 03:05,30/Jan/10 00:55,,,,,,,0.7.0,,,,,0,,,,"For determining the exit status of pig query, only JOB_STATUS.FAILED is being used and status TERMINATED is ignored.
I think the reason for this is that in  ExecJob.JOB_STATUS only FAILED and COMPLETED are being used anywhere. Rest are unused. I think we should comment out the unused parts for now to indicate that, or fix the code  for determining success/failure in GruntParser. executeBatch 

{code}
    public enum JOB_STATUS {
        QUEUED,
        RUNNING,
        SUSPENDED,
        TERMINATED,
        FAILED,
        COMPLETED,
    }
{code}
{code}
    private void executeBatch() throws IOException {
        if (mPigServer.isBatchOn()) {
            if (mExplain != null) {
                explainCurrentBatch();
            }

            if (!mLoadOnly) {
                List<ExecJob> jobs = mPigServer.executeBatch();
                for(ExecJob job: jobs) {
== ====>      if (job.getStatus() == ExecJob.JOB_STATUS.FAILED) {
                        mNumFailedJobs++;
                        if (job.getException() != null) {
                            LogUtils.writeLog(
                              job.getException(), 
                              mPigServer.getPigContext().getProperties().getProperty(""pig.logfile""), 
                              log, 
                              ""true"".equalsIgnoreCase(mPigServer.getPigContext().getProperties().getProperty(""verbose"")),
                              ""Pig Stack Trace"");
                        }
                    }
                    else {
                        mNumSucceededJobs++;
                    }
                }
            }
        }
    }

{code}

Any opinions ?",,,,,,,,,,,,,,,,,,,28/Jan/10 21:56;ashutoshc;pig-977.patch;https://issues.apache.org/jira/secure/attachment/12431709/pig-977.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-09-25 20:53:28.53,,,no_permission,,,,,,,,,,,,164514,,,,,Sat Jan 30 00:55:15 UTC 2010,,,,,,,0|i0gqjb:,95729,,,,,,,,,,25/Sep/09 20:53;pkamath;It does look like we only us COMPLETED and FAILED - +1 to remove other unused states - we can add them when the need arises.,28/Jan/10 21:56;ashutoshc;Removed unused job statuses + typo fix,"29/Jan/10 02:27;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12431709/pig-977.patch
  against trunk revision 904241.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 486 release audit warnings (more than the trunk's current 485 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/182/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/182/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/182/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/182/console

This message is automatically generated.","29/Jan/10 21:26;ashutoshc;This is just a code cleanup. No new functionality added, removed or modified. Passing of existing testcases should suffice. Audit warning related to html file could be ignored. Patch is ready for review.",29/Jan/10 23:15;rding;+1 for commit.,30/Jan/10 00:55;ashutoshc;Patch Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multi-query optimization throws ClassCastException,PIG-976,12436584,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,ankur,ankur,25/Sep/09 10:44,24/Mar/10 22:15,14/Mar/19 03:05,21/Oct/09 20:58,0.4.0,,,,,,0.6.0,,impl,,,1,,,,"Multi-query optimization fails to merge 2 branches when 1 is a result of Group By ALL and another is a result of Group By field1 where field 1 is of type long. Here is the script that fails with multi-query on.

data = LOAD 'test' USING PigStorage('\t') AS (a:long, b:double, c:double); 
A = GROUP data ALL;
B = FOREACH A GENERATE SUM(data.b) AS sum1, SUM(data.c) AS sum2;
C = FOREACH B GENERATE (sum1/sum2) AS rate; 
STORE C INTO 'result1';

D = GROUP data BY a; 
E = FOREACH D GENERATE group AS a, SUM(data.b), SUM(data.c);
STORE E into 'result2';
 
Here is the exception from the logs

java.lang.ClassCastException: org.apache.pig.data.DefaultTuple cannot be cast to org.apache.pig.data.DataBag
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.processInputBag(POProject.java:399)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:180)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.processInput(POUserFunc.java:145)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:197)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:235)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:254)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:204)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:231)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNext(POLocalRearrange.java:240)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODemux.runPipeline(PODemux.java:264)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODemux.getNext(PODemux.java:254)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.processOnePackageOutput(PigCombiner.java:196)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.reduce(PigCombiner.java:174)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.reduce(PigCombiner.java:63)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.combineAndSpill(MapTask.java:906)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:786)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:698)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:228)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2206)
",,,,,,,,,,,,,,,,,,,15/Oct/09 18:08;rding;PIG-976.patch;https://issues.apache.org/jira/secure/attachment/12422254/PIG-976.patch,13/Oct/09 17:14;rding;PIG-976.patch;https://issues.apache.org/jira/secure/attachment/12422000/PIG-976.patch,12/Oct/09 22:42;rding;PIG-976.patch;https://issues.apache.org/jira/secure/attachment/12421921/PIG-976.patch,08/Oct/09 16:24;rding;PIG-976.patch;https://issues.apache.org/jira/secure/attachment/12421633/PIG-976.patch,06/Oct/09 19:15;rding;PIG-976.patch;https://issues.apache.org/jira/secure/attachment/12421451/PIG-976.patch,21/Oct/09 18:25;rding;PIG-976_1.patch;https://issues.apache.org/jira/secure/attachment/12422832/PIG-976_1.patch,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2009-10-06 19:15:22.409,,,no_permission,,,,,,,,,,,,164513,Reviewed,,,,Wed Oct 21 20:58:43 UTC 2009,,,,,,,0|i0gqj3:,95728,,,,,,,,,,"06/Oct/09 19:15;rding;The cause of this bug is the wrong assumption made by the multi-query optimizer. Namely, it assumed that the tuples emitted from a package object always had the key ('group') at the tuple's first field. When the key ('group') wasn't part of the output of the following foreach clause (as in the script above), the first field of the tuple (from a package object) was actually a bag (value), not the key.

This patch fixed this problem.","06/Oct/09 22:19;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421451/PIG-976.patch
  against trunk revision 822382.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 7 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/61/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/61/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/61/console

This message is automatically generated.","08/Oct/09 16:24;rding;It turns out that there is another case that needs to consider.

The following two scripts represent these different cases:

{code}
A = LOAD 'input';
B = GROUP A ALL;
C = FOREACH B GENERATE COUNT(A);
D = STORE C INTO 'output';
{code}

and 

{code}
A = LOAD 'input';
B = GROUP A BY $0;
C = FOREACH B GENERATE COUNT(A),  group;
D = STORE C INTO 'output';
{code}

When running these scripts, the key ('group') value is not the first field of the output tuple of the combiner package. In the second case, the key ('group') is part of the final output.

In this patch, multi-query optimizer considers both cases when merging combiners and reducers.  ","08/Oct/09 19:11;pkamath;Reviewed the new patch - one comment is on POMultiQueryPackage:
{code}
203         Object obj = tuple.get(0);                                                                                                                                                                                                   
  204         if (obj instanceof PigNullableWritable) {                                                                                                                                                                                    
  205             ((PigNullableWritable)obj).setIndex(origIndex);                                                                                                                                                                          
  206         }                                                                                                                                                                                                                            
  207         else {                                                                                                                                                                                                                       
  208             PigNullableWritable myObj = HDataType.getWritableComparableTypes(obj, (byte)0);                                                                                                                                          
  209             myObj.setIndex(origIndex);                                                                                                                                                                                               
  210             tuple.set(0, myObj);                                                                                                                                                                                                     
  211         }                             
{code}

If obj is null then the above code in the else would give an exception - I think the code should check for obj == null and if so create a NullWritable object where NullWritable is a subclass of PigNullableWritable representing a null. Since only the getValueAsPigType() method is used in PODemux, that would always return null for this use case.","08/Oct/09 20:06;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421633/PIG-976.patch
  against trunk revision 822382.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 8 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 302 release audit warnings (more than the trunk's current 299 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/66/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/66/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/66/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/66/console

This message is automatically generated.","12/Oct/09 22:42;rding;Another corner case:

{code}
A = LOAD 'input';
B = GROUP A BY $0;
C = FOREACH B GENERATE COUNT(A), group, group;
{code}

The multi-query optimizer needs to track the positions of the group key in the output tuple of above foreach statement.

This patch takes care of this case as well.
","13/Oct/09 03:27;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421921/PIG-976.patch
  against trunk revision 824446.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 8 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to cause Findbugs to fail.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/74/testReport/
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/74/console

This message is automatically generated.",13/Oct/09 17:14;rding;This patch fixed the above errors.,"13/Oct/09 20:18;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422000/PIG-976.patch
  against trunk revision 824838.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 8 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 3 new Findbugs warnings.

    -1 release audit.  The applied patch generated 295 release audit warnings (more than the trunk's current 292 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/22/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/22/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/22/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/22/console

This message is automatically generated.",14/Oct/09 02:16;pkamath;+1 changes look good - please address the findbugs and release audit warnings if appropriate.,15/Oct/09 18:08;rding;This patch fixed the findbugs issues.,"16/Oct/09 04:34;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422254/PIG-976.patch
  against trunk revision 825712.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 8 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 308 release audit warnings (more than the trunk's current 305 warnings).

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/84/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/84/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/84/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/84/console

This message is automatically generated.","17/Oct/09 12:26;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422254/PIG-976.patch
  against trunk revision 826110.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 8 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 308 release audit warnings (more than the trunk's current 305 warnings).

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/92/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/92/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/92/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/92/console

This message is automatically generated.",20/Oct/09 20:43;pkamath;Patch committed to trunk - Thanks Richard!,21/Oct/09 18:25;rding;This patch fixed a unit test error.,"21/Oct/09 20:55;daijy;+1, fix is good.",21/Oct/09 20:58;daijy;Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
type resolution inconsistency,PIG-973,12436367,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,olgan,olgan,23/Sep/09 00:24,14/May/10 06:45,14/Mar/19 03:05,15/Dec/09 01:26,,,,,,,0.7.0,,,,,0,,,,"This script works:

A = load 'test' using PigStorage(':') as (name: chararray, age: int, gpa: float);
B = group A by age;
C = foreach B {
   D = filter A by gpa > 2.5;
   E = order A by name;
   F = A.age;
   describe F;
   G = distinct F;
   generate group, COUNT(D), MAX (E.name), MIN(G.$0);}
dump C;

This one produces an error:

A = load 'test' using PigStorage(':') as (name: chararray, age: int, gpa: float);
B = group A by age;
C = foreach B {
   D = filter A by gpa > 2.5;
   E = order A by name;
   F = A.age;
   G = distinct F;
   generate group, COUNT(D), MAX (E.name), MIN(G);}
dump C;

Notice the difference in how MIN is passed the data.",,,,,,,,,,,,,,,,,,,14/Dec/09 17:58;rding;PIG-973.patch;https://issues.apache.org/jira/secure/attachment/12427943/PIG-973.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-12-09 22:43:04.989,,,no_permission,,,,,,,,,,,,164510,,,,,Tue Dec 15 01:26:34 UTC 2009,,,,,,,0|i0gqhr:,95722,,,,,,,,,,"09/Dec/09 22:43;rding;
This following script also produces an error:

{code}
A = load 'input' as (id:int, g:bag{t:tuple(u:int)});;
B = foreach A generate id, SUM(g); ;
dump B;
{code}

The problem appears to be with the patch of PIG-315, which only pushed the exception from compile-time to runtime for the script below:

{code}
a = load 'studenttab10k' as (name:chararray, age:int, gpa:double);
b = foreach a generate (long)age as age, (int)gpa as gpa;
c = foreach b generate SUM(age), SUM(gpa);
dump c; 
{code}

Actually, one can argue that this is an invalid script -- the input type of the eval function SUM is a bag of numbers, not a number. So it should be caught at compile-time (by the parser if posible).","14/Dec/09 22:13;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12427943/PIG-973.patch
  against trunk revision 889870.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 7 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/122/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/122/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/122/console

This message is automatically generated.",15/Dec/09 00:46;olgan;+1 on the changes. I will be committing the patch shortly,"15/Dec/09 01:26;olgan;patch committed. thanks, Richard",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Default constructor of UDF gets called for UDF with parameterised constructor , if the udf has a getArgToFuncMapping function defined",PIG-969,12436116,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,18/Sep/09 23:45,04/Aug/11 00:35,14/Mar/19 03:05,14/Feb/11 23:54,,,,,,,0.9.0,,impl,,,0,,,,"This issue is discussed in  http://www.mail-archive.com/pig-user@hadoop.apache.org/msg00524.html . I am able to reproduce the issue. While it is easy to fix the udf, it can take a lot of time to figure out the problem (until they find this email conversation!).

The root cause is that when getArgToFuncMapping is defined in the udf , the FuncSpec returned by the method replaces one set by define statement . The constructor arguments get lost.  We can handle this in following ways -

1. Preserve the constructor arguments, and use it with the class name of the matching FuncSpec from getArgToFuncMapping . 
2. Give an error if constructor paramerters are given for a udf which has FuncSpecs returned from getArgToFuncMapping .

The problem with  approach 1 is that we are letting the user define the FuncSpec , so user could have defined a FuncSpec with constructor (though they don't have a valid reason to do so.). It is also possible the the constructor of the different class that matched might not support same constructor parameters. The use of this function outside builtin udfs are also probably not common.

With option 2, we are telling the user that this is not a supported use case, and user can easily change the udf to fix the issue, or use the udf which would have matched given parameters (which unlikely to have the getArgToFuncMapping method defined).

I am proposing that we go with option 2 . 
",,,,,,,,,,,,PIG-1303,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-02-14 19:40:38.236,,,no_permission,,,,,,,,,,,,66121,,,,,Mon Feb 14 23:53:20 UTC 2011,,,,,,,0|i0gqg7:,95715,,,,,,,,,,"14/Feb/11 19:40;dvryaboy;Why do you say that there isn't a valid reason to provide arguments in the DEFINEd constructor?

Seems needlessly limiting to disallow constructor arguments when getArgToFuncMapping is defined. getArgToFuncMapping is not static, so it can pass on the parameters the constructor was called with to any FuncSpecs it returns, giving us option 3 that just requires documentation changes. Unless I am missing something?

","14/Feb/11 22:31;thejas;I can't think of why I originally suggested that it is better to disable the constructor argument feature earlier! Both option 1 and 3 seems reasonable to me. 

But i tried a quick test with 0.8 and it seems to be trying to instantiate the constructor of class returned by argsToFuncMapping() with the constructor arguments in define. ie, it is using option 1. But I am not sure if this is the same behavior in earlier versions of pig. If not, going with option 1 might not be backward compatible.

I think it is a matter of deciding between these two options, adding some unit tests and documenting it.

I will not be able to work on this in next few days. So if anybody would like to drive this, please feel free to reassign the jira.
",14/Feb/11 23:24;dvryaboy;Hm I might have added that in a different ticket. ,"14/Feb/11 23:38;dvryaboy;PIG-1303 did it for 0.7+, because Algebraic was quite odd otherwise.
I think we should just close this as duplicate and move on. Do you agree, Thejas?","14/Feb/11 23:53;thejas;bq. I think we should just close this as duplicate and move on. Do you agree, Thejas?
Agreed. So it is option 1 that we already have now. Marking as duplicate of PIG-1303.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
findContainingJar fails when there's a + in the path,PIG-968,12436113,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,tlipcon,tlipcon,tlipcon,18/Sep/09 22:54,25/Mar/10 00:12,14/Mar/19 03:05,14/Oct/09 00:10,0.4.0,0.5.0,,,,,0.6.0,,impl,,,0,,,,This is the same bug as in MAPREDUCE-714. Please see discussion there.,,,,,,,,,,,,,,,,,,,18/Sep/09 22:55;tlipcon;pig-968.txt;https://issues.apache.org/jira/secure/attachment/12420098/pig-968.txt,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-09-19 01:51:16.927,,,no_permission,,,,,,,,,,,,164507,,,,,Wed Oct 14 00:10:36 UTC 2009,,,Patch Available,,,,0|i0gqfr:,95713,,,,,,,,,,"19/Sep/09 01:51;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12420098/pig-968.txt
  against trunk revision 816723.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/39/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/39/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/39/console

This message is automatically generated.","21/Sep/09 20:16;alangates;You need to add a unit test that checks that this works when there is a + in the path.

Also, a more general question:  I'm guessing that '+' isn't the only mishandled character.  Are there others that should be checked?","22/Sep/09 19:50;tlipcon;bq. You need to add a unit test that checks that this works when there is a + in the path. 

This is very difficult to test - we'd need to add several more ant rules to build a jar into the test directory with a + in it and get that on the classpath for tests. findContainingJar itself is not currently tested.

bq. Also, a more general question: I'm guessing that '+' isn't the only mishandled character. Are there others that should be checked?

It's well-known that URLDecoder actually decodes x-www-form-urlencoded rather than true URL encoding. The spec for that encoding is here: http://www.w3.org/MarkUp/html-spec/html-spec_8.html#SEC8.2.1

As far as I've been able to find, '+' is the only difference, except for perhaps newlines which can't occur in pathnames afaik.

More info here: http://en.wikipedia.org/wiki/Percent-encoding#The_application.2Fx-www-form-urlencoded_type","24/Sep/09 17:29;alangates;Ok, if it's hard to test in an automated way that's fine.  To test it manually, is it sufficient to create a jar with a + in the path, register it in a Pig Latin script, and then use a UDF from that jar in the script?","07/Oct/09 21:16;tlipcon;Yes, I think that would be a good manual test.",14/Oct/09 00:10;alangates;Patch checked in.  Thanks Todd.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handling null  in skewed join,PIG-964,12435916,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sriranjan,sriranjan,sriranjan,17/Sep/09 07:27,25/Mar/10 00:12,14/Mar/19 03:05,20/Sep/09 01:26,,,,,,,0.4.0,,,,,0,,,,"For null tuples, the tuple size is calculated incorrectly and thus  skewed join ends up expecting a large number of reducers. Further, skewed join should not bail out after the second job if the number of reducers specified by the user is low. It should print a warning message and continue execution.",,,,,,,,,,,,,,,,,,,17/Sep/09 22:18;sriranjan;skewedjoinnull.patch;https://issues.apache.org/jira/secure/attachment/12419938/skewedjoinnull.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-09-17 10:15:01.636,,,no_permission,,,,,,,,,,,,164503,,,,,Sun Sep 20 01:26:01 UTC 2009,,,,,,,0|i0gqdz:,95705,,,,,,,,,,17/Sep/09 07:28;sriranjan;Attached patch solves both the issues.,"17/Sep/09 10:15;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419855/skjoin2b.patch
  against trunk revision 816012.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/36/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/36/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/36/console

This message is automatically generated.",17/Sep/09 18:57;olgan;The patch needs unit tests,17/Sep/09 22:18;sriranjan;Cleared end-end tests and added a new unit test to check for nulls in the dataset.,"18/Sep/09 01:09;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419938/skewedjoinnull.patch
  against trunk revision 816339.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/37/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/37/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/37/console

This message is automatically generated.",18/Sep/09 04:31;olgan;+1 on the code,18/Sep/09 04:56;olgan;patch committed to branch-0.5,18/Sep/09 16:21;olgan;patch committed to the trunk,20/Sep/09 01:26;olgan;patch committed to branch-0.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Join in local mode matches null keys,PIG-963,12435780,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,16/Sep/09 01:34,24/Mar/10 22:14,14/Mar/19 03:05,17/Sep/09 00:01,0.4.0,,,,,,0.5.0,,,,,0,,,,Semantics of join and cogroup dictate that null values for the keys from different inputs are NOT supposed to match. This is true in map reduce mode but local mode incorrectly matches these records.,,,,,,,,,,,,,,,,,,,16/Sep/09 18:38;pkamath;PIG-963-2.patch;https://issues.apache.org/jira/secure/attachment/12419802/PIG-963-2.patch,16/Sep/09 02:06;pkamath;PIG-963.patch;https://issues.apache.org/jira/secure/attachment/12419719/PIG-963.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-09-16 04:08:37.985,,,no_permission,,,,,,,,,,,,164502,Reviewed,,,,Thu Sep 17 00:58:12 UTC 2009,,,,,,,0|i0gqdj:,95703,,,,,,,,,,16/Sep/09 02:06;pkamath;Attached patch which addresses the issue - the fix is in POCogroup to make the comparator of group keys treat two keys with null value as different if they are from different inputs,"16/Sep/09 04:08;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419719/PIG-963.patch
  against trunk revision 815571.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/30/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/30/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/30/console

This message is automatically generated.","16/Sep/09 18:38;pkamath;Attached new patch with the only diff to address the findbugs warning - findbugs does not like that t1 is null and then we are using it in the next comparison - so switching the conditions:
{noformat}
[pradeepk@afterside:~/dev/pig-commit/PIG-963/trunk]diff /homes/pradeepk/dev/pig-apache/pig/trunk/PIG-963.patch /homes/pradeepk/dev/pig-apache/pig/trunk/PIG-963-2.patch 
651c651
< +        if(t1 == null && t1 == t2) {
---
> +        if(t1 == t2 && t1 == null) {

{noformat}",16/Sep/09 18:48;daijy;+1,"16/Sep/09 18:49;pkamath;This patch does not address the case where the join key is more than one column and hence represented as a tuple. In this case if one of the key's value is null, join will still use Tuple.compare() which will treat two null fields as equals. This is a known issue in map reduce mode also and should be fixed for both map reduce and local mode through https://issues.apache.org/jira/browse/PIG-927","16/Sep/09 19:50;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419719/PIG-963.patch
  against trunk revision 815571.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/33/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/33/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/33/console

This message is automatically generated.","16/Sep/09 22:23;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419802/PIG-963-2.patch
  against trunk revision 815934.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/34/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/34/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/34/console

This message is automatically generated.","17/Sep/09 00:01;pkamath;Patch committed to trunk, branch-0.4, branch-0.5","17/Sep/09 00:58;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419802/PIG-963-2.patch
  against trunk revision 815934.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/35/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/35/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/35/console

This message is automatically generated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skewed join creates 3 map reduce jobs,PIG-962,12435759,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sriranjan,sriranjan,sriranjan,15/Sep/09 20:44,24/Mar/10 22:14,14/Mar/19 03:05,15/Sep/09 23:37,0.4.0,,,,,,0.5.0,,,,,0,,,,The first job is a load / store job which loads the data from PigStorage and stores it in BinStorage. This hampers performance. The desired behavior is for the sampler to read from PigStorage instead of relying on the first load/store job. Skewed join should thus be 2 M/R jobs and not 3.,,,,,,,,,,,,,,,,,,,15/Sep/09 20:53;sriranjan;skewedjoin2job.patch;https://issues.apache.org/jira/secure/attachment/12419679/skewedjoin2job.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-09-15 22:51:43.187,,,no_permission,,,,,,,,,,,,164501,,,,,Tue Sep 15 23:37:12 UTC 2009,,,,,,,0|i0gqdb:,95702,,,,,,,,,,15/Sep/09 20:53;sriranjan;The attached patch resolves this issue.,"15/Sep/09 22:51;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419679/skewedjoin2job.patch
  against trunk revision 815283.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/7/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/7/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/7/console

This message is automatically generated.","15/Sep/09 23:30;olgan;+1, the patch looks good. I will be committing it shortly","15/Sep/09 23:37;olgan;Patch committed. Thanks, Sri!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Splitting output data on key field,PIG-958,12435600,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ankur,ankur,ankur,14/Sep/09 13:56,24/Mar/10 22:15,14/Mar/19 03:05,05/Nov/09 21:45,0.4.0,,,,,,0.6.0,,,,,0,,,,"Pig users often face the need to split the output records into a bunch of files and directories depending on the type of record. Pig's SPLIT operator is useful when record types are few and known in advance. In cases where type is not directly known but is derived dynamically from values of a key field in the output tuple, a custom store function is a better solution.",,,,,,,,,,,,,,,,,,,09/Oct/09 10:30;ankur;958.v3.patch;https://issues.apache.org/jira/secure/attachment/12421723/958.v3.patch,26/Oct/09 14:29;ankur;958.v4.patch;https://issues.apache.org/jira/secure/attachment/12423200/958.v4.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-09-16 06:18:45.497,,,no_permission,,,,,,,,,,,,164497,Reviewed,,,,Thu Nov 05 21:45:01 UTC 2009,,,Patch Available,,,,0|i0gqc7:,95697,,,,,,,,,,14/Sep/09 13:58;ankur;Attached is an implementation of a custom store function that splits the data dynamically based on the values of user specified key field in the output tuple,"16/Sep/09 06:18;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419527/958.v1.patch
  against trunk revision 815571.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to cause Findbugs to fail.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/31/testReport/
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/31/console

This message is automatically generated.","16/Sep/09 08:05;ankur;Hudson seems to be failing during compilation as my test case defined in package org.apache.pig.piggybank.test.storage  is reusing certain classes from org.apache.pig.test, namely 'Util' and MiniCluster.


",22/Sep/09 08:42;ankur;Fixed wrong src path of the classes,"22/Sep/09 11:29;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12420264/958.v2.patch
  against trunk revision 817319.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 280 release audit warnings (more than the trunk's current 278 warnings).

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/43/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/43/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/43/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/43/console

This message is automatically generated.","24/Sep/09 00:48;pkamath;In general the patch looks good - do address the hudson QA issues. Some code review comments:

1) I don't think pig.properties changes should be part of this patch - was this accidental? Likewise for Main.java.

2)
{noformat}
 72   /**                                                                                                                                                                                                                                  
 73    *  storeUID is needed to get the around the case of multi-query optimisation where                                                                                                                                                  
 74    *  multiple instances of MultiStorage could be running in a single mapper/reducer                                                                                                                                                   
 75    *  and run the risk of overwriting each other's output.                                                                                                                                                                             
 76    */      
{noformat}
Wouldn't the multiple instances be writing to different locations in which case there should be no race condition right?

3)
{noformat}
172   private void initJobSpecificParams() throws IOException {                                                                                                                                                                            
173     if (partition == null || outputPath == null) {   
{noformat}
Wouldn't the outputPath never be null since it is initialized in the constructor?

4) Consider removing the log.debug() since these are all in the inner loop and would possibly impact performance.

5)
{noformat}
291         String fieldValueBasedPathStr = fieldValueBasedPath.toUri().getPath(); 
{noformat}
This variable is only really used in a log.debug(), I think removing this and using fieldValueBasedPath in all the fs.create() will make the code shorter and cleaner.

6) Some comments on why the move of the output dirs is needed and in the moveResults() and removePart() methods would be helpful. Additional comments on the code flow would also help
","25/Sep/09 20:36;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12420264/958.v2.patch
  against trunk revision 818929.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 281 release audit warnings (more than the trunk's current 279 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/46/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/46/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/46/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/46/console

This message is automatically generated.",25/Sep/09 20:58;pkamath;The release audit warning I think is related to  missing Apache header comment - can you add Apache header comment by pasting it from some other source file in svn - every file needs to have the apache header as a comment at the beginning of the file - you will need to add it to the beginning of source and test file. Also if you agree with any of the review comments you can incorporate those changes when you submit the next version of the patch.,"09/Oct/09 10:30;ankur;Pradeep,
                 Thanks for your review comments. I have incorporated the suggestions provided in the code review. The code is vastly simplified, cleaner and more readable :-). 

Unit test now pass in local mode but fail in cluster mode after taking an update of Pig code base. The error I see is :-
hdfs://localhost.localdomain:40352/user/gankur/output/_temporary/_attempt_20091009030519686_0001_m_000000_0/output, expected: file:///

Looks like a config issue with org.apache.pig.test.MiniCluster in the latest pig code. I didn't get time to debug this as I am going on a vacation. Regardless, I have attached the new patch for your review. Please suggest what needs to be done to pass the unit test in cluster mode.

-Ankur","12/Oct/09 18:11;pkamath;+1 - changes looks good!
For the test, I observed you were using the mapreduce mode pigserver object even in local mode - I made some changes but was unable to run the tests due to some config issue in setting up the test run - did not explore more - nevertheless here is what I changed:
{noformat}
127   private void testMultiStorage(PigServer pigServer, Mode mode,                                                                                                                                                                        
128       String... queries) throws IOException {                                                                                                                                                                                          
129     PigServer ps = (mode == Mode.cluster) ? pigServer: pigServerLocal;                                                                                                                                                                 
130     ps.setBatchOn();                                                                                                                                                                                                                   
131     for (String query : queries) {                                                                                                                                                                                                     
132       ps.registerQuery(query);                                                                                                                                                                                                         
133     }                                                                                                                                                                                                                                  
134     ps.executeBatch();                                                                                                                                                                                                                 
135     verifyResults(mode);                                                                                                                                                                                                               
136   }                           
{nofrmat}

Check if making the above changes solves the issue you are seeing.","26/Oct/09 14:29;ankur;1. When run in cluster mode, static variable PigMapReduce.sJobConf is null when checked in the UDF constructor but NOT null when UDF is actually invoked. This  causes incorrect initialization of FileSystem object 'fs' to local filesystem, causing the test to fail. Moved to 'fs' initialization to intijobSpecificParams() method.

2. Deleting the temporary directory manually in finish(), causes the job to fail. Removed the manual deletion. As a side effect, user specified PARENT output directory in the UDF will have empty part-* files. These should be deleted manually by the user.

Verfied that UDF works correctly and that unit test pass","26/Oct/09 14:33;ankur;Just back from vacation. Have updated the code with required changes. It should be good to go now. Pradeep can you or any other committer review it ?

",03/Nov/09 07:43;ankur;Can we have an update on this please ?,"03/Nov/09 20:34;pkamath;bq. 2. Deleting the temporary directory manually in finish(), causes the job to fail. Removed the manual deletion. As a side effect, user specified PARENT output directory in the UDF will have empty part-* files. These should be deleted manually by the user.

Can you explain this a little more - been long since I last looked at the code - there seems to be some mv and this deletion happening - if you can explain that part too it would be helpful

Otherwise looks good.","03/Nov/09 20:42;pkamath;I saw compile errors while trying to run unit test:

{noformat}
[..contrib/piggybank/java]ant test
..

    [javac] /homes/pradeepk/dev/pig-commit/PIG-958.v4/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestMultiStorage.java:44: cannot find symbol
    [javac] symbol  : variable MiniCluster
    [javac] location: class org.apache.pig.piggybank.test.storage.TestMultiStorage
    [javac]   private MiniCluster cluster = MiniCluster.buildCluster();
    [javac]                                 ^
    [javac] /homes/pradeepk/dev/pig-commit/PIG-958.v4/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestMultiStorage.java:73: cannot find symbol
    [javac] symbol  : variable Util
    [javac] location: class org.apache.pig.piggybank.test.storage.TestMultiStorage
    [javac]     Util.deleteFile(cluster, INPUT_FILE);
    [javac]     ^
    [javac] /homes/pradeepk/dev/pig-commit/PIG-958.v4/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestMultiStorage.java:74: cannot find symbol
    [javac] symbol  : variable Util
    [javac] location: class org.apache.pig.piggybank.test.storage.TestMultiStorage
    [javac]     Util.copyFromLocalToCluster(cluster, INPUT_FILE, INPUT_FILE);
    [javac]     ^
    [javac] /homes/pradeepk/dev/pig-commit/PIG-958.v4/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestMultiStorage.java:96: cannot find symbol
    [javac] symbol  : variable Util
    [javac] location: class org.apache.pig.piggybank.test.storage.TestMultiStorage
    [javac]     Util.deleteFile(cluster, INPUT_FILE);
    [javac]     ^
..
{noformat}","04/Nov/09 06:30;ankur;> Can you explain this a little bit more - ......
In the earlier patch (958.v3.patch), After moving the results from the tasks current working directory, I was manually deleting the directory. This is to ensure that empty part files don't get moved to the final output directory. But doing so causes hadoop to complain that it can no longer write to task's output dir and the task fails.

> I saw compile errors while trying to run unit test: ...
Did you compile the pig.jar  and ran core test before. This creates the necessary classes and jar file son the local machine required by contrib tests.

On my local machine
gankur@grainflydivide-dr:pig_trunk$ ant 
...
buildJar:
     [echo] svnString 830456
      [jar] Building jar: /home/gankur/eclipse/workspace/pig_trunk/build/pig-0.6.0-dev-core.jar
      [jar] Building jar: /home/gankur/eclipse/workspace/pig_trunk/build/pig-0.6.0-dev.jar
     [copy] Copying 1 file to /home/gankur/eclipse/workspace/pig_trunk

gankur@grainflydivide-dr:pig_trunk$ ant test
...
test-core:
   [delete] Deleting directory /home/gankur/eclipse/workspace/pig_trunk/build/test/logs
    [mkdir] Created dir: /home/gankur/eclipse/workspace/pig_trunk/build/test/logs
    [junit] Running org.apache.pig.test.TestAdd
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.024 sec
    [junit] Running org.apache.pig.test.TestAlgebraicEval
...
gankur@grainflydivide-dr:pig_trunk$ cd contrib/piggybank/java/
gankur@grainflydivide-dr:java$ ant test
...
test:
     [echo]  *** Running UDF tests ***
   [delete] Deleting directory /home/gankur/eclipse/workspace/pig_trunk/contrib/piggybank/java/build/test/logs
    [mkdir] Created dir: /home/gankur/eclipse/workspace/pig_trunk/contrib/piggybank/java/build/test/logs
    [junit] Running org.apache.pig.piggybank.test.evaluation.TestEvalString
    [junit] Tests run: 6, Failures: 0, Errors: 0, Time elapsed: 0.15 sec
    [junit] Running org.apache.pig.piggybank.test.evaluation.TestMathUDF
    [junit] Tests run: 35, Failures: 0, Errors: 0, Time elapsed: 0.123 sec
    [junit] Running org.apache.pig.piggybank.test.evaluation.TestStat
    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0.114 sec
    [junit] Running org.apache.pig.piggybank.test.evaluation.datetime.TestDiffDate
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.105 sec
    [junit] Running org.apache.pig.piggybank.test.evaluation.decode.TestDecode
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 0.089 sec
    [junit] Running org.apache.pig.piggybank.test.evaluation.string.TestHashFNV
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.094 sec
    [junit] Running org.apache.pig.piggybank.test.evaluation.string.TestLookupInFiles
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 17.163 sec
    [junit] Running org.apache.pig.piggybank.test.evaluation.string.TestRegex
    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0.092 sec
    [junit] Running org.apache.pig.piggybank.test.evaluation.util.TestSearchQuery
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.093 sec
    [junit] Running org.apache.pig.piggybank.test.evaluation.util.TestTop
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.099 sec
    [junit] Running org.apache.pig.piggybank.test.evaluation.util.apachelogparser.TestDateExtractor
    [junit] Tests run: 8, Failures: 0, Errors: 0, Time elapsed: 0.087 sec
    [junit] Running org.apache.pig.piggybank.test.evaluation.util.apachelogparser.TestHostExtractor
    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0.083 sec
    [junit] Running org.apache.pig.piggybank.test.evaluation.util.apachelogparser.TestSearchEngineExtractor
    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0.091 sec
    [junit] Running org.apache.pig.piggybank.test.evaluation.util.apachelogparser.TestSearchTermExtractor
    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0.1 sec
    [junit] Running org.apache.pig.piggybank.test.storage.TestCombinedLogLoader
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 0.535 sec
    [junit] Running org.apache.pig.piggybank.test.storage.TestCommonLogLoader
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 0.54 sec
    [junit] Running org.apache.pig.piggybank.test.storage.TestHelper
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.014 sec
    [junit] Running org.apache.pig.piggybank.test.storage.TestMultiStorage
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 16.964 sec
    [junit] Running org.apache.pig.piggybank.test.storage.TestMyRegExLoader
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.452 sec
    [junit] Running org.apache.pig.piggybank.test.storage.TestRegExLoader
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.302 sec
    [junit] Running org.apache.pig.piggybank.test.storage.TestSequenceFileLoader
    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0.883 sec

BUILD SUCCESSFUL
Total time: 58 seconds

","05/Nov/09 21:45;pkamath;Patch committed, thanks for the contribution Ankur!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tutorial is broken with 0.4 branch and trunk,PIG-957,12435492,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,olgan,olgan,12/Sep/09 00:24,24/Mar/10 22:13,14/Mar/19 03:05,14/Sep/09 19:45,0.3.0,,,,,,0.4.0,,,,,0,,,,"As I was testing the Pig Tutorial in preparation for the release, I found that we broke the second script both in local mode and in MR mode. The issue has to do with schema and naming fields.  

Here is what I see:

 

java -cp pig.jar org.apache.pig.Main -x local script2-local.pig


2009-09-11 12:52:46,961 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Invalid alias: hour00::group::ngram in {group::ngram: chararray,group::hour: chararray,hour00::count: long,ngram: chararray,hour: chararray,hour12::count: long}

09/09/11 12:52:46 ERROR grunt.Grunt: ERROR 1000: Error during parsing. Invalid alias: hour00::group::ngram in {group::ngram: chararray,group::hour: chararray,hour00::count: long,ngram: chararray,hour: chararray,hour12::count: long}
",,,,,,,,,,,,,,,,,,,14/Sep/09 17:33;pkamath;PIG-957-2.patch;https://issues.apache.org/jira/secure/attachment/12419544/PIG-957-2.patch,12/Sep/09 01:15;pkamath;PIG-957.patch;https://issues.apache.org/jira/secure/attachment/12419363/PIG-957.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-09-12 01:15:15.254,,,no_permission,,,,,,,,,,,,164496,Reviewed,,,,Mon Sep 14 19:45:21 UTC 2009,,,,,,,0|i0gqbz:,95696,,,,,,,,,,12/Sep/09 01:15;pkamath;Attached patch to address the issue. LOJoin's getSchema() now keeps both the disambiguated (outeralias::inneralis)  alias and the simple inner alias for non duplicate columns coming out of the LOJoin.,12/Sep/09 04:00;daijy;+1,"12/Sep/09 04:45;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419363/PIG-957.patch
  against trunk revision 814075.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/6/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/6/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/6/console

This message is automatically generated.","14/Sep/09 17:33;pkamath;There were two unit test failures in the last patch 
1) TestPigServer had a failure which was because join's describe now prefixes the outer relation alias for each field - corrected the test case to update the expected result.
2) TestSkewedJoin had a timeout - this ran fine on my local box.

Resubmitting with just the change in 1) above.",14/Sep/09 17:36;daijy;+1,"14/Sep/09 17:39;olgan;Pradeep, please, commit. The change is trivial enough not to wait for another automated test run.","14/Sep/09 19:24;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419544/PIG-957-2.patch
  against trunk revision 814075.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/27/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/27/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/27/console

This message is automatically generated.",14/Sep/09 19:45;pkamath;Patch committed to both trunk and branch-0.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reset parallelism to 1 for indexing job in MergeJoin,PIG-951,12435273,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,10/Sep/09 03:43,24/Mar/10 22:15,14/Mar/19 03:05,18/Sep/09 17:42,,,,,,,0.6.0,,impl,,,0,,,,"After sampling one tuple from every block, one reducer is used to sort the index entries in reduce phase to produce sorted index to be used in actual join job. Thus, parallelism of index job should be explictly set to 1. Currently, its not.

Currently, this is a non-issue, since we don't allow any blocking operators in pipeline before merge-join. However, later when we do allow blocking operators, then parallelism of indexing job will be that of preceding blocking operator. Even then, job will complete successfully because all tuple will go to only one reducer, because we are grouping on only one key ""all"". However, it will waste cluster resources by starting all the extra reducers which get no data and thus do nothing.",,,,,,,,,,,,,,,,,,,10/Sep/09 03:46;ashutoshc;pig-951.patch;https://issues.apache.org/jira/secure/attachment/12419132/pig-951.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-09-11 01:33:42.669,,,no_permission,,,,,,,,,,,,164490,,,,,Fri Sep 18 17:42:01 UTC 2009,,,,,,,0|i0gq9r:,95686,,,,,,,,,,"10/Sep/09 03:46;ashutoshc;One line patch which fixes this. Also, added test case to catch regression on this.","11/Sep/09 01:33;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419132/pig-951.patch
  against trunk revision 813601.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/23/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/23/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/23/console

This message is automatically generated.",17/Sep/09 17:42;alangates;I'll be reviewing this patch.,18/Sep/09 17:42;alangates;Patch checked in.  Thanks Ashutosh.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Zebra Bug: splitting map into multiple column group using storage hint causes unexpected behaviour,PIG-949,12435261,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yanz,aloknsingh,aloknsingh,09/Sep/09 22:41,24/Mar/10 22:14,14/Mar/19 03:05,25/Sep/09 22:30,0.4.0,,,,,,0.5.0,,,,,0,,,,"Hi 

 The storage hint
specification plays a important part whether the output table is readable or not

say if we have have the map 'map'.

One can split the map into a column group using [map#{k1}, map#{k2}...] 
however the remaining map field will automatically be added to the default group.

if user try to create a new column group for the remaining fields as follows

[map#{k1}, map#{k2}, ..][map] i.e create a seperate column group

the table writer will create the table.

however, if one tries to load the created table via pig or via map reduce using TableInputFormat
 
then the reader  have problem reading the map

We get the following stack trace

09/09/09 00:09:45 INFO mapred.JobClient: Task Id : attempt_200908191538_33939_m_000021_2, Status : FAILED
java.io.IOException: getValue() failed: null
        at org.apache.hadoop.zebra.io.BasicTable$Reader$BTScanner.getValue(BasicTable.java:775)
        at org.apache.hadoop.zebra.mapred.TableRecordReader.next(TableInputFormat.java:717)
        at org.apache.hadoop.zebra.mapred.TableRecordReader.next(TableInputFormat.java:651)
        at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:191)
        at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:175)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:356)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
        at org.apache.hadoop.mapred.Child.main(Child.java:170)



Alok
",linux,,,,,,,,,,,,,,,,,,22/Sep/09 21:32;yanz;Pig_949.patch;https://issues.apache.org/jira/secure/attachment/12420313/Pig_949.patch,22/Sep/09 19:16;yanz;Pig_949.patch;https://issues.apache.org/jira/secure/attachment/12420304/Pig_949.patch,21/Sep/09 16:14;yanz;Pig_949.patch;https://issues.apache.org/jira/secure/attachment/12420202/Pig_949.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-09-11 21:15:21.88,,,no_permission,,,,,,,,,,,,164488,,,,,Fri Sep 25 22:30:33 UTC 2009,,,,,,,0|i0gq93:,95683,,,,,,,,,,"11/Sep/09 21:15;jing1234;Thanks Alok. 
I am able to reproduce the problem. 
I was only using i/o layer (not pig loader) to test map split. 
This is what I did:
  final static String STR_SCHEMA = ""m1:map(string),m2:map(map(int))"";
  final static String STR_STORAGE = ""[m1#{a}];[m2#{x|y}]; [m1#{b}, m2#{z}];[m1]"";
.......create table and insert data ......

load:  String projection = new String(""m1#{a}"");

I only got null returned. 

============

Without storage hint [m1], everything works fine. , i.e. 
 final static String STR_STORAGE = ""[m1#{a}];[m2#{x|y}]; [m1#{b}, m2#{z}]"";
 .......create table and insert data ......
load:  String projection = new String(""m1#{a}"");
I am able to get value m1#{a}. 

Zebra team is working on the fix.

","14/Sep/09 17:34;yanz;The problem is caused by not adding ""ColumnMappingEntry""s from the key-split specs in storage info to an  explicitly specified MAP item in storage info, thus causing missing CGs as needed by the key-split specs. Everything falls apart thereafter. Will create a patch for R1 patch release soon.","21/Sep/09 19:07;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12420202/Pig_949.patch
  against trunk revision 816832.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/40/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/40/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/40/console

This message is automatically generated.","22/Sep/09 00:17;yanz;The test case is 

contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestJira949.java","22/Sep/09 18:08;rangadi;Yan, please include the test case in the patch. 

Also I would suggest a regular name for the test case file something like 'TestMapAcrossMultipleCGs.java' or something shorter. Inside the file you could mention JIRA number in the comment.

Raghu.","22/Sep/09 19:33;yanz;Test case added.

Thanks,

Yan

",22/Sep/09 21:32;yanz;change the unit test case to TestNonDefaultWholeMapSplit,"23/Sep/09 01:45;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12420313/Pig_949.patch
  against trunk revision 817739.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/9/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/9/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/9/console

This message is automatically generated.",25/Sep/09 21:59;chaow;Already viewed the patch +1,25/Sep/09 22:30;rangadi;I just committed this. Thanks Yan for the fix and Jing for the test!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Zebra schema is taken from Pig through TableStorer's construct,PIG-944,12434791,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yanz,yanz,yanz,03/Sep/09 22:56,24/Mar/10 22:15,14/Mar/19 03:05,21/Oct/09 14:18,0.4.0,,,,,,0.6.0,,,,,0,,,,It should be from StoreConfig in TableOutputFormat.checkOutputSpecs method because the information is dynamic in Pig's execution engine and should not be taking a static argument to the constructor.,,,,,,,,,,,,,,,,,,,12/Oct/09 17:07;yanz;SchemaConversion.patch;https://issues.apache.org/jira/secure/attachment/12421882/SchemaConversion.patch,03/Oct/09 05:33;yanz;SchemaConversion.patch;https://issues.apache.org/jira/secure/attachment/12421187/SchemaConversion.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-09-05 01:49:56.169,,,no_permission,,,,,,,,,,,,164484,,,,,Wed Oct 21 14:18:11 UTC 2009,,,,,,,0|i0gq73:,95674,,,,,,,,,,"05/Sep/09 00:41;yanz; ""pig to zebra"" schema conversion support is added to fix the problem",05/Sep/09 00:42;yanz;patch file to fix the problem,"05/Sep/09 01:49;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12418687/zebra_pig_interface.patch
  against trunk revision 811203.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 15 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/1/console

This message is automatically generated.","10/Sep/09 00:15;yanz;The previously attached patch is based upon some new features under development and consequently might not be applicable to the trunk. I'm going to attahc another patch shortly based upon version 1 branch.

In addition to the problem in TableOutputFormat.checkOutputSpecs, the SchemaConverter.toPigSchema  did not convert the Zebra schema to Pig's properly if nested types are involved: the low level column schemas were simply missing.
Also, the conversion from Pig to Zebra schema is just missing beyond a hack to work on specially prefixed column names.",03/Oct/09 05:33;yanz;This patch must be applied after the patch for Jira PIG-933 has been applied.,07/Oct/09 19:05;chaow;Patch reviewed. +1,12/Oct/09 17:07;yanz;The new version of patch is needed because of its dependence upon an upstream Jira's change in Pig-992.,12/Oct/09 22:13;yanz;cancel tha patch,"14/Oct/09 21:42;yanz;A typo in one of my earlier comments at 02/Oct/09 10:33 PM. Instead of 

This patch must be applied after the patch for Jira PIG-933 has been applied. 

it should have read as

This patch must be applied after the patch for Jira PIG-993 has been applied. ","19/Oct/09 05:30;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421882/SchemaConversion.patch
  against trunk revision 826110.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 15 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/99/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/99/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/99/console

This message is automatically generated.","19/Oct/09 18:00;yanz;I just run the test case of TestBestFitCast,  which saw 4 failures in the Hudson run, using both the SVN image and with addition of this patch. In both scenarios, the tests pass cleanly.  For one the falire tests are not related to Zebra at all so that by all accounts should have not been affected by this patch. I can't help but wondering if the Hudson was running in a clean and healthy env ?",19/Oct/09 20:27;alangates;Hudson is messed up at the moment.  In the meantime I'll take a look at this patch and run it manually.  But it will take a bit longer.,21/Oct/09 14:18;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig crash when it cannot get counter from hadoop,PIG-943,12434780,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,03/Sep/09 21:19,24/Mar/10 22:13,14/Mar/19 03:05,08/Sep/09 18:02,0.3.0,,,,,,0.4.0,,impl,,,0,,,,"We see following call stacks in Pig:
Case 1:
Caused by: java.lang.NullPointerException
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.computeWarningAggregate(MapReduceLauncher.java:390)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:238)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:265)

Case 2:
Caused by: java.lang.NullPointerException
        at org.apache.pig.tools.pigstats.PigStats.accumulateMRStats(PigStats.java:150)
        at org.apache.pig.tools.pigstats.PigStats.accumulateStats(PigStats.java:91)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:192)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:265)

In both cases, hadoop jobs finishes without error. The cause of both problems is RunningJob.getCounters() returns a null, and Pig do not currently check for that. ",,,,,,,,,,,,,,,,,,,03/Sep/09 21:59;daijy;PIG-943-1.patch;https://issues.apache.org/jira/secure/attachment/12418545/PIG-943-1.patch,04/Sep/09 22:19;daijy;PIG-943-2.patch;https://issues.apache.org/jira/secure/attachment/12418677/PIG-943-2.patch,05/Sep/09 01:08;daijy;PIG-943-3.patch;https://issues.apache.org/jira/secure/attachment/12418690/PIG-943-3.patch,07/Sep/09 04:59;daijy;PIG-943-4.patch;https://issues.apache.org/jira/secure/attachment/12418774/PIG-943-4.patch,08/Sep/09 17:38;daijy;PIG-943-5.patch;https://issues.apache.org/jira/secure/attachment/12418941/PIG-943-5.patch,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2009-09-03 22:35:01.92,,,no_permission,,,,,,,,,,,,164483,,,,,Tue Sep 08 18:43:40 UTC 2009,,,,,,,0|i0gq6f:,95671,,,,,,,,,,03/Sep/09 21:59;daijy;No unit test is included. The script reproducing the exception requires a large input data which is not possible to put it here.,"03/Sep/09 22:35;dvryaboy;Daniel,
-1 is a bit of a hack. Maybe cleaner to insert and check for nulls? Also, perhaps using stringToLong from CastUtils is warranted as it's a bit safer than parseLong().

-D","03/Sep/09 23:01;daijy;Hi, Dmitriy,
All this patch does is checking for nulls. The extra work in the patch is when counters object is null, we treat all counter as unknown, so we do not report confusing results to user. Which part do you think ""hacky""? Also parseLong is there for a long time and currently the only problem we observe is RunningJob.getCounters is null. If this is your concern I can change it to use stringToLong.","04/Sep/09 18:25;pkamath;I am wondering if we should report that we were unable to determine the number of records/bytes written - 
 {code}
           if (stats.getRecordsWritten()!=-1)                                                                                                                                                                                             
                log.info(""Records written : "" + stats.getRecordsWritten());                                                                                                                                                                
            if (stats.getBytesWritten()!=-1)                                                                                                                                                                                               
                log.info(""Bytes written : "" + stats.getBytesWritten());                                                                                                                                                                    
        }                              
 {code}
could change to
{code}
	if(stats.getRecordsWritten()==-1) {
        log.info(""Records written : Unable to determine number of records written"");
    } else {
        log.info(""Records written : stats.getRecordsWritten()"");
    }
	if(stats.getBytesWritten()==-1) {
        log.info(""Bytes written : Unable to determine number of bytes written"");
    } else {
        log.info(""Bytes written : stats.getBytesWritten()"");
    }
{code}

If we are using -1 to indicate we did not get counter information but the warning has occured some number of times (maybe in one of the
jobs where we did get counter information),  the following function's output message should change
:
{code}
public static void logAggregate(Map<Enum, Long> aggMap, MessageType messageType, Log log) {
    	for(Enum e: aggMap.keySet()) {
    		Long count = aggMap.get(e);
    		if(count != null && count > 0) {
    			String message = ""Encountered "" + messageType + "" "" + e.toString() + "" "" + count + "" time(s)."";
    			logMessage(message, messageType, log);
    		}
    	}	
    }
{code}
to
{code}
public static void logAggregate(Map<Enum, Long> aggMap, MessageType messageType, Log log) {
    	for(Enum e: aggMap.keySet()) {
    		Long count = aggMap.get(e);
    		if(count != null && (count == -1 || count > 0)) {
    			String message = ""Encountered "" + messageType + "" "" + e.toString() + "" "" + (count == -1 ? ""unknown number of "": count.toString()) + "" time(s)."";
    			logMessage(message, messageType, log);
    		}
    	}	
    }
{code}

This way we still warn the user about the warning message which got aggregated but just not report the number of occurences.

To enable the above change the code in MapReduceLauncher.java should also change to ensure we report 3 types of counts:
0 -> the warning did not occur
-1 ->the warning occured in some of the jobs but in one or more jobs we get a null counter and hence we cannot report an accurate count
>0 -> we can accurately report the count of occurences of the warning.
","04/Sep/09 19:13;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12418545/PIG-943-1.patch
  against trunk revision 811203.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/13/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/13/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/13/console

This message is automatically generated.","04/Sep/09 21:51;dvryaboy;Hi Daniel,
My apologies, I worded my comment poorly. I wasn't minus-oneing the patch, I was saying that the use of -1 as a magic value is a bit hacky.
I think inserting Long.NaN or null and checking for it on the other end, instead of checking for -1, is cleaner.
","04/Sep/09 22:19;daijy;This patch include Pradeep's comment. Now if we encounter any null counters from hadoop, we ignore it when doing the calculation. Then in the report, we put a note ""We cannot retrieve hadoop counter for x jobs, the number following warnings may not be correct"".","04/Sep/09 22:43;daijy;Hi, Dmitriy,
Thank you for your clarification. I look up the code, if we change -1 to a Long.NaN or null, then we need to change more code. Besides, I don't see much difference if we use null instead of -1. In both cases, we use a special value to indicate something special.","04/Sep/09 23:10;pkamath;Reviewed PIG-943-2.patch. Changes look good +1

One very minor comment, the following message:
""We cannot retrieve hadoop counter for  .."" should change to  ""Unable to retrieve hadoop counter for  .."" ","05/Sep/09 00:44;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12418677/PIG-943-2.patch
  against trunk revision 811203.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/14/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/14/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/14/console

This message is automatically generated.",07/Sep/09 04:59;daijy;Fix one bug in calculation of the number of occurrence of null counters,"08/Sep/09 17:38;daijy;Minor change. If no warnings, shall not put the message ""the number following warnings may not be correct""",08/Sep/09 18:02;daijy;Patch committed,"08/Sep/09 18:43;daijy;Since there is no unit test included, I manually tested with two failed script, both pass without error message.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Maps are not implicitly casted,PIG-942,12434695,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,sriranjan,sriranjan,03/Sep/09 01:22,24/Mar/10 22:15,14/Mar/19 03:05,25/Sep/09 17:48,,,,,,,0.6.0,,,,,0,,,,"A = load 'foo' as (m) throws the following exception when foo has maps.

java.lang.ClassCastException: org.apache.pig.data.DataByteArray cannot be cast to java.util.Map
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POMapLookUp.getNext(POMapLookUp.java:98)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POMapLookUp.getNext(POMapLookUp.java:115)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(POCast.java:612)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:278)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:204)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:231)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNext(POLocalRearrange.java:240)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:249)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:240)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:93)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:227)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2198)

The same works if I explicitly cast m to a map: A = load 'foo' as (m:[])",,,,,,,,,,,,,,,,,,,23/Sep/09 17:55;pkamath;PIG-942-2.patch;https://issues.apache.org/jira/secure/attachment/12420393/PIG-942-2.patch,18/Sep/09 17:21;pkamath;PIG-942.patch;https://issues.apache.org/jira/secure/attachment/12420050/PIG-942.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-09-18 17:21:43.369,,,no_permission,,,,,,,,,,,,164482,Reviewed,,,,Fri Sep 25 17:48:08 UTC 2009,,,,,,,0|i0gq5z:,95669,,,,,,,,,,"03/Sep/09 02:59;sriranjan;Here's the complete script:

A = load 'map.txt' as (e);
B = load 'map.txt' as (f);
C = join A by (chararray)e#'100', B by (chararray)f#'100';
dump C;",18/Sep/09 17:21;pkamath;Attached patch which addresses the issue by introducing implicit cast to Map if the input to Map lookup is not a map. As a part of this patch also changed LOCast.getFieldSchema() to set lineage information (set its parent to the expression on which it is operating). The latter change was needed to fix unit tests which were failing due to the first change.,"18/Sep/09 20:23;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12420050/PIG-942.patch
  against trunk revision 816699.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/38/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/38/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/38/console

This message is automatically generated.",19/Sep/09 00:43;daijy;+1,19/Sep/09 00:53;pkamath;Patch committed to trunk.,"23/Sep/09 17:54;pkamath;In LOCast.getFieldSchema the following code needs to change:
{code}
 mFieldSchema.setParent(getExpression().mFieldSchema.canonicalName, getExpression()); 
{code}

if the ExpressionOperator returned from getExpression() has not calcuated its field schema (mFieldSchema) this can result in a NPE",23/Sep/09 17:55;pkamath;Attached patch which calls getExpression().getFieldSchema() to get the fieldschema and further checks for null before using the canonical name. Unit tests were already present in the original patch - no tests have been added for this additional patch,24/Sep/09 06:44;daijy;+1,25/Sep/09 06:36;gkesavan;resubmiting patch to hudson ,"25/Sep/09 12:29;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12420393/PIG-942-2.patch
  against trunk revision 818175.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/45/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/45/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/45/console

This message is automatically generated.","25/Sep/09 17:48;pkamath;Unit test was present in the original patch. 

Patch committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cross site HDFS access using the default.fs.name not possible in Pig,PIG-940,12434470,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,viraj,viraj,01/Sep/09 00:50,14/May/10 06:45,14/Mar/19 03:05,28/Jan/10 00:19,0.5.0,,,,,,0.7.0,,impl,,,0,,,,"I have a script which does the following.. access data from a remote HDFS location (via a HDFS installed at:hdfs://remotemachine1.company.com/ ) [[as I do not want to copy this huge amount of data between HDFS locations]].

However I want my Pigscript  to write data to the HDFS running on localmachine.company.com.

Currently Pig does not support that behavior and complains that: ""hdfs://localmachine.company.com/user/viraj/A1.txt does not exist""

{code}
A = LOAD 'hdfs://remotemachine1.company.com/user/viraj/A1.txt' as (a, b); 
B = LOAD 'hdfs://remotemachine1.company.com/user/viraj/B1.txt' as (c, d); 
C = JOIN A by a, B by c; 
store C into 'output' using PigStorage();  
{code}
=======================================================================================================================================
2009-09-01 00:37:24,032 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://localmachine.company.com:8020
2009-09-01 00:37:24,277 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to map-reduce job tracker at: localmachine.company.com:50300
2009-09-01 00:37:24,567 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler$LastInputStreamingOptimizer - Rewrite: POPackage->POForEach to POJoinPackage
2009-09-01 00:37:24,573 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 1
2009-09-01 00:37:24,573 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 1
2009-09-01 00:37:26,197 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job
2009-09-01 00:37:26,249 [Thread-9] WARN  org.apache.hadoop.mapred.JobClient - Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
2009-09-01 00:37:26,746 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete
2009-09-01 00:37:26,746 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete
2009-09-01 00:37:26,747 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map reduce job(s) failed!
2009-09-01 00:37:26,756 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Failed to produce result in: ""hdfs:/localmachine.company.com/tmp/temp-1470407685/tmp-510854480""
2009-09-01 00:37:26,756 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Failed!
2009-09-01 00:37:26,758 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2100: hdfs://localmachine.company.com/user/viraj/A1.txt does not exist.
Details at logfile: /home/viraj/pigscripts/pig_1251765443851.log
=======================================================================================================================================

The error file in Pig contains:
=======================================================================================================================================
ERROR 2998: Unhandled internal error. org.apache.pig.backend.executionengine.ExecException: ERROR 2100: hdfs://localmachine.company.com/user/viraj/A1.txt does not exist.
        at org.apache.pig.backend.executionengine.PigSlicer.validate(PigSlicer.java:126)
        at org.apache.pig.impl.io.ValidatingInputFileSpec.validate(ValidatingInputFileSpec.java:59)
        at org.apache.pig.impl.io.ValidatingInputFileSpec.<init>(ValidatingInputFileSpec.java:44)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:228)
        at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:810)
        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:781)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:730)
        at org.apache.hadoop.mapred.jobcontrol.Job.submit(Job.java:378)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs(JobControl.java:247)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.run(JobControl.java:279)
        at java.lang.Thread.run(Thread.java:619)

java.lang.Exception: org.apache.pig.backend.executionengine.ExecException: ERROR 2100: hdfs://localmachine.company.com/user/viraj/A1.txt does not exist.
        at org.apache.pig.backend.executionengine.PigSlicer.validate(PigSlicer.java:126)
        at org.apache.pig.impl.io.ValidatingInputFileSpec.validate(ValidatingInputFileSpec.java:59)
        at org.apache.pig.impl.io.ValidatingInputFileSpec.<init>(ValidatingInputFileSpec.java:44)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:228)
        at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:810)
        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:781)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:730)
        at org.apache.hadoop.mapred.jobcontrol.Job.submit(Job.java:378)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs(JobControl.java:247)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.run(JobControl.java:279)
        at java.lang.Thread.run(Thread.java:619)
=======================================================================================================================================

",Hadoop 20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-09-01 13:17:29.26,,,no_permission,,,,,,,,,,,,164480,,,,,Thu Jan 28 00:19:26 UTC 2010,,,,,,,0|i0gq5b:,95666,,,,,,,,,,"01/Sep/09 01:04;viraj;One important point to add:
{code}
localmachine.company.com prompt> hadoop fs -ls hdfs://remotemachine1.company.com/user/viraj//*.txt
-rw-r--r--   3 viraj users         13 2009-08-13 23:42 /user/viraj/A1.txt
-rw-r--r--   3 viraj users          8 2009-08-29 00:51 /user/viraj/B1.txt
{code}","01/Sep/09 13:17;mridul;Is this supported in hadoop ? As in, can you specify the input to be on a different hdfs and get a mapred job to work ? IIRC no, but I could be missing something.

If it is no, then not sure if pig can support it without an intermediate distcp ...","01/Sep/09 18:52;knoguchi;bq. Is this supported in hadoop ? 
Sure.",28/Jan/10 00:19;olgan;This is fixed as part of LSR and will be released as part of Pig 0.7.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkstyle pulls in junit3.7 which causes the build of test code to fail.,PIG-939,12434458,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,gkesavan,ltucker,ltucker,31/Aug/09 21:15,14/Apr/10 22:55,14/Mar/19 03:05,15/Sep/09 11:47,0.3.0,,,,,,0.4.0,,build,,,0,,,,"Pig fails to compile if you execute: 

    ant -D<associated flags for various components> clean findbugs checkstyle test 

It gets the error:

    [javac] Compiling 153 source files to /export/crawlspace/kryptonite/hadoopqa/workspace/workspace/CCDI-Pig-2.3/pig-2.3.0.0.20.0.2967040009/build/test/classes
    [javac] /export/crawlspace/kryptonite/hadoopqa/workspace/workspace/CCDI-Pig-2.3/pig-2.3.0.0.20.0.2967040009/test/org/apache/pig/test/PigExecTestCase.java:31: cannot find symbol
    [javac] symbol  : constructor TestCase()
    [javac] location: class junit.framework.TestCase
    [javac] public abstract class PigExecTestCase extends TestCase {
    [javac]                 ^


Once that's done, there's a copy of junit 3.7 cached from ivy that will continue to cause the build to fail.  It will succeed, if you remove it, and then do:

    ant -D<associated flags for various components> clean findbugs test

This proves it's running checkstyle that pulls in junit 3.7
",,,,,,,,,,,,,,,,,,,01/Sep/09 10:55;gkesavan;pig-939.patch;https://issues.apache.org/jira/secure/attachment/12418232/pig-939.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-09-01 10:55:05.645,,,no_permission,,,,,,,,,,,,164479,,,,,Wed Apr 14 22:55:56 UTC 2010,,,,,,,0|i0gq4v:,95664,,,,,,,,,,01/Sep/09 10:55;gkesavan;this patch should fix this issue of downloading junit-3.7,"01/Sep/09 14:20;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12418232/pig-939.patch
  against trunk revision 806668.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/6/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/6/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/6/console

This message is automatically generated.","08/Sep/09 04:51;gkesavan;Can someone review this?
-Giri",08/Sep/09 17:21;ltucker;+1 change has fixed it so junit 3.7 no longer downloads,08/Sep/09 17:23;alangates;Why is antlr being added as a dependency?  I don't think Pig uses antlr.,"08/Sep/09 18:18;gkesavan;Its not pig which depends on antlr but checkstyle depends on antlr and thats the reason antlr is defined under conf=""checkstyle->master""
http://mvnrepository.com/artifact/checkstyle/checkstyle/4.2 - has the details of checkstyle's transient dependency.",09/Sep/09 17:04;alangates;+1,14/Apr/10 22:48;hadoopqa;To test jira cli,14/Apr/10 22:55;hadoopqa;To test jira cli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task get stuck in BasicTable's BTScaner's atEnd() method,PIG-937,12434340,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,he yongqiang,he yongqiang,29/Aug/09 05:31,14/May/10 06:45,14/Mar/19 03:05,28/Dec/09 22:03,,,,,,,0.7.0,,,,,0,,,,"It seems is caused by the infinite loop in the code:
BasicTable, Line 698
{noformat}
        while (true)
        {
          int index = random.nextInt(cgScanners.length - 1) + 1;
          if (cgScanners[index] != null) {
            if (cgScanners[index].atEnd() != ret) {
              throw new IOException(
                  ""atEnd() failed: Column Groups are not evenly positioned."");
            }
            break;
          }
        }
{noformat}

I think it's fine to just use a for loop here, like:
{noformat}
        for (int index = 0; index < cgScanners.length; index++) {
          if (cgScanners[index] != null) {
            if (cgScanners[index].atEnd() != ret) {
              throw new IOException(
                  ""atEnd() failed: Column Groups are not evenly positioned."");
            }
            break;
          }
        }
{noformat}",,,,,,,,,,,,PIG-918,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-08-30 22:11:13.825,,,no_permission,,,,,,,,,,,,164477,,,,,Mon Dec 28 22:03:25 UTC 2009,,,,,,,0|i0gq3z:,95660,,,,,,,,,,30/Aug/09 03:46;he yongqiang;The same loop also occurs at  BasicTable->Reader->BTScanner->getKey. I do not check if other places also have this kind of infinite loop. ,30/Aug/09 22:11;yanz;This bug has been fixed in a patch (see https://issues.apache.org/jira/browse/PIG-918) pretty much along the same line as suggested.,28/Dec/09 22:03;he yongqiang;close this as it had already been fixed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
making dump and PigDump independent from Tuple.toString,PIG-936,12434210,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,zjffdu,olgan,olgan,27/Aug/09 18:48,24/Mar/10 22:13,14/Mar/19 03:05,04/Sep/09 02:27,0.3.0,,,,,,0.4.0,,,,,0,,,,"Since Tuple is an interface, a toString implementation can change from one tuple implementation to the next. This means that format of dump and PigDump will be different depending on the tuples processed. This could be quite confusing to the users.",,,,,,,,,,,,,,,,,,,30/Aug/09 14:21;zjffdu;Pig_936.Patch;https://issues.apache.org/jira/secure/attachment/12418090/Pig_936.Patch,02/Sep/09 18:39;daijy;Pig_936_2.Patch;https://issues.apache.org/jira/secure/attachment/12418420/Pig_936_2.Patch,02/Sep/09 18:59;daijy;Pig_936_3.Patch;https://issues.apache.org/jira/secure/attachment/12418422/Pig_936_3.Patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-08-30 12:54:52.701,,,no_permission,,,,,,,,,,,,164476,,,,,Fri Sep 04 02:27:48 UTC 2009,,,,,,,0|i0gq3j:,95658,,,,,,,,,,"30/Aug/09 12:54;zjffdu;Extract the output format of Dump and PigDump into class TupleFormat

","30/Aug/09 13:06;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12418087/Pig_936.Patch
  against trunk revision 806668.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/1/console

This message is automatically generated.","30/Aug/09 13:32;dvryaboy;Patch makes sense.
pig.data doesn't seem like the right package for this class -- perhaps pig.tools ?

Also please make sure to format your code in accordance with the style guidelines (http://java.sun.com/docs/codeconv/ ), and use 4 spaces -- not tabs -- for indentation.  ","30/Aug/09 14:21;zjffdu;Dmitriy, 
Thank you for your review and suggestion. 
1. I add one class BagFormat, because Tuple maybe have DataBag as its field, And like Tuple, DataBag also has different implementation,so call toString() on DataBag is not enough.

2. And I put the BagFormat and TupleFormat in package org.apache.pig.impl.util.  I am not sure whether this is the right package I should put.","30/Aug/09 14:58;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12418090/Pig_936.Patch
  against trunk revision 806668.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/2/console

This message is automatically generated.","30/Aug/09 15:01;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12418090/Pig_936.Patch
  against trunk revision 806668.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/3/console

This message is automatically generated.","02/Sep/09 07:18;zjffdu;Does anyone know why my patch failed ?
The error message in build log is : (Stripping trailing CRs from patch.)
I do not quite understand it.  I developed this patch on windows, does it necessary for me to code in linux platform ?

","02/Sep/09 18:39;daijy;Hi, Jeff,
It is fine to make patch on Windows. ""Stripping trailing CRs from patch"" does fail the patch. The problem is PigDump.java, which is now surprisingly in Windows format (You can see the tailing ^M if you open in vi). If you convert PigDump.java into Unix format (by using dos2unix), then your patch can be applied. I attached the patch again. The only change is that it will convert PigDump.java into Unix as well, so it can be applied to trunk.

I also reviewed the patch. It looks good to me. I am fine with the putting TupleFormat and BagFormat into package ""org.apache.pig.impl.util"". I will commit it shortly if no other comments. Thanks!","02/Sep/09 18:46;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12418420/Pig_936_2.Patch
  against trunk revision 810327.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to cause Findbugs to fail.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/8/testReport/
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/8/console

This message is automatically generated.",02/Sep/09 18:59;daijy;Missing several files in the last patch. Resubmitting,"02/Sep/09 20:42;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12418422/Pig_936_3.Patch
  against trunk revision 810327.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/9/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/9/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/9/console

This message is automatically generated.",03/Sep/09 00:47;daijy;Unit test failure is not related to this patch.,"03/Sep/09 01:18;zjffdu;Daniel,

Thank you for your help.

",04/Sep/09 02:27;daijy;Patch committed. Thanks Jeff for contributing!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skewed join throws an exception when used with map keys,PIG-935,12434079,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sriranjan,sriranjan,sriranjan,26/Aug/09 18:36,25/Mar/10 00:12,14/Mar/19 03:05,02/Sep/09 19:32,,,,,,,0.4.0,,,,,0,,,,"Skewed join throws a runtime exception for the following query:

A = load 'map.txt' as (e);
B = load 'map.txt' as (f);
C = join A by (chararray)e#'a', B by (chararray)f#'a' using ""skewed"";
explain C;

Exception:

Caused by: java.lang.ClassCastException: org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast cannot be cast to org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PO
Project
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.getSortCols(MRCompiler.java:1492)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.getSamplingJob(MRCompiler.java:1894)
        ... 27 more

",,,,,,,,,,,,,,,,,,,02/Sep/09 01:45;sriranjan;skmapbug.patch;https://issues.apache.org/jira/secure/attachment/12418325/skmapbug.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-08-26 23:20:35.714,,,no_permission,,,,,,,,,,,,38739,Reviewed,,,,Wed Sep 02 19:32:04 UTC 2009,,,Patch Available,,,,0|i0gq33:,95656,,,,,,,,,,26/Aug/09 18:37;sriranjan;The attached patch solves this issue.,"26/Aug/09 23:20;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12417766/skjoinmapbug.patch
  against trunk revision 806668.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/179/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/179/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/179/console

This message is automatically generated.",30/Aug/09 01:20;sriranjan;Fixed the issue with unit tests,"30/Aug/09 04:00;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12418081/skjoinmapbug.patch
  against trunk revision 806668.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/181/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/181/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/181/console

This message is automatically generated.",31/Aug/09 21:36;sriranjan;The timeout is unrelated to the patch. It passes on my local box.,"31/Aug/09 22:41;pkamath;Review Comment:
The fix looks good - the only change that would be good to have is in MRCompiler.getSamplingJob() to explicity check if any field position obtained from the getSortCols() method is -1 and in those cases throw an ""Internal Error setting up sampling job"" exception (you may need to introduce a new error code and follow the error handling guidelines to do this.

",02/Sep/09 01:45;sriranjan;Added code to explicitly check for -1 in orderby,"02/Sep/09 03:31;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12418325/skmapbug.patch
  against trunk revision 810327.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/7/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/7/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/7/console

This message is automatically generated.",02/Sep/09 06:57;sriranjan;The unit tests are unrelated to my patch.,"02/Sep/09 19:32;pkamath;Reviewed and checked that the latest patch does not cause unit test failures on my local patch.

Patch committed. Thanks Sriranjan!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Merge join implementation currently does not seek to right point on the right side input based on the offset provided by the index,PIG-934,12434075,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ashutoshc,pkamath,pkamath,26/Aug/09 18:26,25/Mar/10 00:12,14/Mar/19 03:05,02/Sep/09 01:14,0.4.0,,,,,,0.4.0,,,,,0,,,,"We use POLoad to seek into right file which has the following code: 
{noformat}
   public void setUp() throws IOException{

        String filename = lFile.getFileName();

        loader = (LoadFunc)PigContext.instantiateFuncFromSpec(lFile.getFuncSpec());        

        is = FileLocalizer.open(filename, pc);

        loader.bindTo(filename , new BufferedPositionedInputStream(is), this.offset, Long.MAX_VALUE);

    }
{noformat}

Between opening the stream and bindTo we do not seek to the right offset. bindTo itself does not perform any seek.
",,,,,,,,,,,,,,,,,,,01/Sep/09 05:28;ashutoshc;pig-934_2.patch;https://issues.apache.org/jira/secure/attachment/12418219/pig-934_2.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-08-28 23:45:50.919,,,no_permission,,,,,,,,,,,,164475,Reviewed,,,,Wed Sep 02 01:14:14 UTC 2009,,,,,,,0|i0gq2n:,95654,,,,,,,,,,"26/Aug/09 18:30;pkamath;To get an idea of how this seeking in case of regular loads in map tasks, I looked at PigSlice.java, the seek happens in the init() code before bindTo():
{code}
public void init(DataStorage base) throws IOException {
        ..

        fsis = base.asElement(base.getActiveContainer(), file).sopen();

        fsis.seek(start, FLAGS.SEEK_CUR);

 
        end = start + getLength();


        if (file.endsWith("".bz"") || file.endsWith("".bz2"")) {

            is = new CBZip2InputStream(fsis, 9);

        } else if (file.endsWith("".gz"")) {

            is = new GZIPInputStream(fsis);

            // We can't tell how much of the underlying stream GZIPInputStream

            // has actually consumed

            end = Long.MAX_VALUE;

        } else {

            is = fsis;

        }

        loader.bindTo(file.toString(), new BufferedPositionedInputStream(is,

                start), start, end);

    }
{code}
    
I think we need a FileLocalizer.sOpenSingleFile() method which can return a SeekableInputStream and we can use that in setup() in POLoad.
Something along the lines of :
{code}
static public InputStream open(String fileSpec, PigContext pigContext) throws IOException {
        fileSpec = checkDefaultPrefix(pigContext.getExecType(), fileSpec);
        if (!fileSpec.startsWith(LOCAL_PREFIX)) {
            init(pigContext);
            ElementDescriptor elem = pigContext.getDfs().asElement(fullPath(fileSpec, pigContext));
            return elem.sopen();
        }
        else {
            fileSpec = fileSpec.substring(LOCAL_PREFIX.length());
            //buffering because we only want buffered streams to be passed to load functions.
            /*return new BufferedInputStream(new FileInputStream(fileSpec));*/
            init(pigContext);
            ElementDescriptor elem = pigContext.getLfs().asElement(fullPath(fileSpec, pigContext));
            return elem.sopen;
        }
    }

{code}
 The above code would only work with single files and not dirs which should be ok for merge join. We should probably set this up with a new constructor in POLoad which also indicates that a single file is being processed.

","28/Aug/09 23:45;ashutoshc;I think instead of creating a new method, better is to overload FileLocalizer.open() to have a consistent api. 
Currently, there is 
{code}
InputStream open(String fileSpec, PigContext pigContext)
{code}

Add an Overload method like this and within that fallback to previous method, if there is no need to seek.
{code}
InputStream open(String fileSpec, long offset, PigContext pigContext){
      if (offset == 0)
            return open(String fileSpec, PigContext pigContext);
   else
      // first open, then seek and then return

}
{code}

This ensures minimum impact to clients of this api. 

Attaching a patch which does this.",28/Aug/09 23:56;ashutoshc;Also this doesnt warrant a new constructor in POLoad for seeking.,"29/Aug/09 00:21;pkamath;The reason I thought a separate function with a ""singleFile"" in the name was needed was because the current FileLocalizer.open() can handle directories and hence returns a DataStorageInputStreamIterator which internally iterates over the underlying multiple streams of the files in the directory. Keeping the same name may give the impression that the same capability is present even for the version which seeks to an offset. Seeking to an offset would only work for a single file - hence maybe have a separate function where the name implies this restriction might be cleaner.","29/Aug/09 21:39;ashutoshc;>> Seeking to an offset would only work for a single file - hence maybe have a separate function...

Since open() returns an input stream it is not hard to conceive of usecase when one would want to seek into that stream even when filespec points to a directory or a glob. We have to define the semantics here. What does seeking in a directory/glob means? One reasonable answer is to view all the files in directory/glob as one big logical file and offset as an offset in this logical file and then seek into this file. Something along the lines of :
{code}
iterator = DataStreamIterator
bytesSeen = 0;
while(itertor.hasNext()){
  open current file pointed by iterator
  bytesSeen += current file length
  if (bytesSeen > offset)
    bind to adjusted offset in current file and return
 else
    continue; 
}
{code} 

But since there is no requirement for such currently, we can catch the situation when seeking is asked for directory/glob and throw an exception (as is done in this patch).  Later on, if we decide to support it instead of throwing exception, we can implement whatever semantics we decide on. If we create a new function with separate name it will be confusing to do these changes later on. Moreover, if there is a different function, user of the api needs to ""know"" about it and deal with it (e.g., need of special constructor in POLoad). Presence/absence of offset parameter in argument list I think is a sufficient indicator to tell which version of overloaded open() to call if there is a need for seek. 
Thoughts?","29/Aug/09 22:41;dvryaboy;Throwing an exception when a seek is past the file boundary seems acceptable to me (and preferable to adding new functions and changing upstream code that shouldn't care about this detail). Especially since if there is a way to get a consistent ordering among files in a directory, it's trivial to later update this code to seek past file boundaries and into the next file.","31/Aug/09 23:04;pkamath;Agree with both the above comments. I was wondering if instead of returning an InputStream, if the code could return a SeekableInputStream it would be usable in other scenarios (like creating a CBZip2InputStream out of it - this would be needed for http://issues.apache.org/jira/browse/PIG-930 for example). Callers only needing an InputStream would still be able to use the method.",01/Sep/09 05:28;ashutoshc;Explictly returning SeekableInputStream instead of InputStream in this case is a good idea. Apart from helping PIG-930 it also reinforces the idea that when version of open() with offset parameter is called seek is requested and thus will be performed.  Updated the patch reflecting the same.,"01/Sep/09 07:46;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12418219/pig-934_2.patch
  against trunk revision 806668.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/4/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/4/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/4/console

This message is automatically generated.",01/Sep/09 09:18;gkesavan;I resubmitted the patch to hudson as the core tests failed for not finding javac.,"01/Sep/09 10:59;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12418219/pig-934_2.patch
  against trunk revision 806668.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/5/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/5/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/5/console

This message is automatically generated.",01/Sep/09 13:05;ashutoshc;All tests passed on my local box. Not sure why they failed on hudson. ,"02/Sep/09 01:14;pkamath;Checked that the unit tests works locally on my machine too.

Patch committed - Thanks Ashutosh!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
broken link in pig-latin reference manual to hadoop file glob pattern documentation,PIG-933,12434059,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,olgan,thejas,thejas,26/Aug/09 15:45,14/May/10 06:45,14/Mar/19 03:05,27/Jan/10 20:22,,,,,,,0.7.0,,documentation,,,0,,,,"http://wiki.apache.org/pig-data/attachments/FrontPage/attachments/plrm.htm#_LOAD has a link to 
http://lucene.apache.org/hadoop/api/org/apache/hadoop/fs/FileSystem.html#globPaths(org.apache.hadoop.fs.Path)the , 
it should be - http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/fs/FileSystem.html#globStatus(org.apache.hadoop.fs.Path)


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-08-29 21:38:50.488,,,no_permission,,,,,,,,,,,,164474,,,,,Mon Feb 01 16:37:01 UTC 2010,,,,,,,0|i0gq27:,95652,,,,,,,,,,29/Aug/09 21:38;chandec;Link fixed in 0.4.0 docs (jira PIG-938).,"27/Jan/10 20:22;thejas;The links are correct in 0.4.0 , 0.5.0 docs","01/Feb/10 16:37;chandec;Please note:

(1) I do not maintain the Apache Wiki pages

(2) plrm.htm is no longer in use - any reference to this file should be deleted

Thanks/C",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
null should be handled consistently in Join,PIG-927,12433374,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,pkamath,pkamath,18/Aug/09 20:33,24/Mar/10 22:15,14/Mar/19 03:05,23/Oct/09 18:10,0.4.0,,,,,,0.6.0,,,,,0,,,,"Currenlty Pig mostly follows SQL semantics for handling null. However there are certain cases where pig may need to handle nulls correctly. One example is the join - joins on single keys results in null keys not matching to produce an output. However if the join is on >1 keys, in the key tuple, if one of the values is null, it still matches with another key tuple which has a null for that value. We need to decide the right semantics here. ",,,,,,,,,,,,,,,,,,,15/Oct/09 16:28;daijy;PIG-927-1.patch;https://issues.apache.org/jira/secure/attachment/12422249/PIG-927-1.patch,15/Oct/09 20:10;daijy;PIG-927-2.patch;https://issues.apache.org/jira/secure/attachment/12422271/PIG-927-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-09-09 20:35:37.145,,,no_permission,,,,,,,,,,,,164469,Reviewed,,,,Fri Oct 23 18:10:48 UTC 2009,,,,,,,0|i0gpzb:,95639,,,,,,,,,,"09/Sep/09 20:35;alangates;It seems that the right semantic would be to follow SQL consistently, as that is what we say we do.",16/Sep/09 18:50;pkamath;This is a known issue in both map reduce and local mode.,"16/Sep/09 18:51;pkamath;I meant ""This is a issue in both map reduce and local mode""","15/Oct/09 16:28;daijy;In the patch, we follow SQL behavior. When we join on more than one key (it is a tuple key in Pig), as long as one of keys is null, we do not merge them. Eg: we do not merge below tuple pair:

(1, 2, null) vs (1, 2, null)","15/Oct/09 19:36;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422249/PIG-927-1.patch
  against trunk revision 825393.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/82/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/82/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/82/console

This message is automatically generated.","15/Oct/09 20:10;daijy;Address unit test failure of TestAlgebraicEval. The other unit test failure is due to port conflict of Minicluster, hope it is temporal.","16/Oct/09 10:34;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422271/PIG-927-2.patch
  against trunk revision 825712.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/87/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/87/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/87/console

This message is automatically generated.","20/Oct/09 23:35;alangates;The new test doesn't seem to test this case.  Other than that the code looks good.  Nice comments too, made it easier to understand what was going on.","21/Oct/09 00:17;daijy;Hi, Alan,
Thank you for your comment. For the test case, if we do not have this patch, ""1\t"" for input1 will merge with ""1\t"" for input2, thus ""join"" will produce 2 output records. With this patch, we can only see 1 output record.","22/Oct/09 19:24;alangates;Sorry, I missed the \t at the end of the line.  Test looks good.  +1",23/Oct/09 18:10;daijy;Patch committed. Thanks Alan!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix join in local mode,PIG-925,12433137,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,15/Aug/09 00:44,24/Mar/10 22:13,14/Mar/19 03:05,19/Aug/09 07:02,0.3.0,,,,,,0.4.0,,impl,,,0,,,,"Join is broken after LOJoin patch (Optimizer_Phase5.patch of [PIG-697|https://issues.apache.org/jira/browse/PIG-697). Even the simplest join script is not working under local mode:

eg:
a = load '1.txt';
b = load '2.txt';
c = join a by $0, b by $0;
dump c;

Caused by: java.lang.NullPointerException
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage.getNext(POPackage.java:206)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:231)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:191)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:231)
        at org.apache.pig.backend.local.executionengine.physicalLayer.counters.POCounter.getNext(POCounter.java:71)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:231)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore.getNext(POStore.java:117)
        at org.apache.pig.backend.local.executionengine.LocalPigLauncher.runPipeline(LocalPigLauncher.java:146)
        at org.apache.pig.backend.local.executionengine.LocalPigLauncher.launchPig(LocalPigLauncher.java:109)
        at org.apache.pig.backend.local.executionengine.LocalExecutionEngine.execute(LocalExecutionEngine.java:165)",,,,,,,,,,,,,,,,,,,17/Aug/09 23:05;daijy;PIG-925-1.patch;https://issues.apache.org/jira/secure/attachment/12416812/PIG-925-1.patch,18/Aug/09 05:13;daijy;PIG-925-2.patch;https://issues.apache.org/jira/secure/attachment/12416839/PIG-925-2.patch,18/Aug/09 18:05;daijy;PIG-925-3.patch;https://issues.apache.org/jira/secure/attachment/12416899/PIG-925-3.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-08-18 01:02:06.253,,,no_permission,,,,,,,,,,,,164467,,,,,Wed Aug 19 11:58:47 UTC 2009,,,,,,,0|i0gpyf:,95635,,,,,,,,,,"17/Aug/09 23:05;daijy;We do not handle LOJoin at all in local mode. Previously, LOJoin is converted to CoGroup + ForEach in the parser. Include patch to address this issue.","18/Aug/09 01:02;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12416812/PIG-925-1.patch
  against trunk revision 804406.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 251 javac compiler warnings (more than the trunk's current 250 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/167/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/167/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/167/console

This message is automatically generated.",18/Aug/09 05:13;daijy;Address the javac warning,"18/Aug/09 07:26;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12416839/PIG-925-2.patch
  against trunk revision 804406.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 251 javac compiler warnings (more than the trunk's current 250 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/169/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/169/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/169/console

This message is automatically generated.",18/Aug/09 18:05;daijy;Fix the javac warning,"18/Aug/09 18:19;pkamath;The new unit test has the following comment:
{code}
// Regression test for Pig-800
{code}

Shouldn't the jira number be PIG-925?

Changes look good - it would be good to have some 
comments at different parts of the code explaining
how we are translating LOJoin into POCogroup and foreach with flattens on the bag

I am assuming if the javac issue is fix-able, there will be a new patch with the fix.

+1 otherwise.","19/Aug/09 05:39;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12416899/PIG-925-3.patch
  against trunk revision 804406.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/172/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/172/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/172/console

This message is automatically generated.",19/Aug/09 07:02;daijy;Patch committed. Thanks Pradeep.,"19/Aug/09 11:58;hudson;Integrated in Pig-trunk #527 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/527/])
    : Fix join in local mode
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Pig work with multiple versions of Hadoop,PIG-924,12433124,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,dvryaboy,dvryaboy,14/Aug/09 20:48,25/Mar/10 00:12,14/Mar/19 03:05,21/Aug/09 18:44,,,,,,,0.4.0,,,,,0,,,,"The current Pig build scripts package hadoop and other dependencies into the pig.jar file.
This means that if users upgrade Hadoop, they also need to upgrade Pig.

Pig has relatively few dependencies on Hadoop interfaces that changed between 18, 19, and 20.  It is possibly to write a dynamic shim that allows Pig to use the correct calls for any of the above versions of Hadoop. Unfortunately, the building process precludes us from the ability to do this at runtime, and forces an unnecessary Pig rebuild even if dynamic shims are created.

",,,,,,,,,,,,,PIG-660,,,,,,18/Aug/09 17:38;dvryaboy;pig_924.2.patch;https://issues.apache.org/jira/secure/attachment/12416895/pig_924.2.patch,19/Aug/09 01:39;dvryaboy;pig_924.3.patch;https://issues.apache.org/jira/secure/attachment/12416945/pig_924.3.patch,14/Aug/09 20:50;dvryaboy;pig_924.patch;https://issues.apache.org/jira/secure/attachment/12416610/pig_924.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-08-17 21:22:55.648,,,no_permission,,,,,,,,,,,,41728,,,,,Fri Aug 21 18:44:41 UTC 2009,,,,,,,0|i0gpxz:,95633,,,,,,,,,,14/Aug/09 20:50;dvryaboy;The attached patch includes dynamic shims that could be used with Pig if it didn't bundle its hadoop classes.,"17/Aug/09 21:22;tlipcon;Hey guys,

Any word on this? From the packaging perspective it's pretty important that a single build of Hive will work with both Hadoop 18 and Hadoop 20. Obviously packaging isn't the Yahoo team's highest priority, but I think it is very important for community adoption, etc. If we require separate builds for 18 and 20 it's one more thing that can cause confusion for new users.

As I understand it from Dmitriy, for this to work we just need to stop packing the Hadoop JAR into the pig JAR. Instead, the wrapper script just needs to specify the hadoop JAR on the classpath. Is there some barrier to doing this that I'm unaware of?","17/Aug/09 21:39;tlipcon;Oops, apparently it is Monday and my brain is scrambled. Above should read ""pretty important that a single build of *Pig* will work..."", of course.",17/Aug/09 23:14;daijy;I am reviewing the patch.,"18/Aug/09 00:30;tlipcon;Couple notes on the patch:

- you've turned javac.deprecation from ""on"" to ""off"" - seems unwise. perhaps you should just do this for the one javac task where you want that behavior
- src.shims.dir.com in the build.xml has a ""REMOVE"" mark on it - is this still needed? it looks like it is, but perhaps is better named .common instead of .com
- you've moved junit.hadoop.conf into basedir instead of ${user.home} - this seems reasonable but is orthogonal to this patch. should be a separate JIRA
- why are we now excluding HBase storage test?
- some spurious whitespace changes (eg TypeCheckingVisitor.java)
- in MRCompiler, a factor of 0.9 seems to have disappeared. the commented-out line should be removed
- some tab characters seem to be introduced
- in MiniCluster, also some commented-out code which should be cleaned up
","18/Aug/09 00:46;dvryaboy;Thanks for looking, Todd -- most of those changes, like the factor of 0.9, deprecation, excluding HBase test, etc, are consistent with the 0.20 patch posted to PIG-660 . 
Moving junit.hadoop.conf is critical -- there are comments about this in 660 -- without it, resetting hadoop.version doesn't actually work, as some of the information from a previous build sticks around.

I'll fix the whitespace; this wasn't a final patch, more of a proof of concept.  Point being this could work, but it can't, because Hadoop is bundled in the jar. I am looking for comments from the core developer team regarding the possibility of un-bundling.","18/Aug/09 00:48;tlipcon;Gotcha, thanks for explaining. Aside from the nits, patch looks good to me.","18/Aug/09 06:00;daijy;Hi, Dmitriy, 
Generally the patch is good. Just like Todd said, we don't want to change anything else besides the shims layer. In addition to Todd's comment, Main.java contains the change for ""pig.logfile"", which you address in Pig-923. Would you please clear things up and resubmit?

Thanks","18/Aug/09 17:38;dvryaboy;This patch addresses the reviewer comments.
I put the factor of 0.9 into the 18 shim to restore old behavior (not sure what the motivation was for changing this for 20.. 

I set the default hadoop version to 18, so that we can verify correctness by running automated tests.

The existing unit tests are sufficient verification of this patch (at least as far as 18 is concerned).","18/Aug/09 17:54;tlipcon;Few more comments I missed on the first pass through:

- A few of the shim methods appear unused:
  - fileSystemDeleteOnExit
  - inputFormatValidateInput
  - setTmpFiles

- Is the inner MiniDFSCluster class used? I think this is replaced by the MiniDFSClusterShim if I understand it correctly.

- Still seems to be some unrelated changes to build.xml - the javac.deprecation for example
- If we are now excluding TestHBaseStorage on all platforms, we should get rid of the two lines above it that exclude it only on windows - it's redundant and confusing.

Thanks
-Todd","19/Aug/09 01:39;dvryaboy;Removed unused methods from the shims.

I don't know what reasoning led to turning off deprecation in 20, or to turning off the hbase test -- so just following suit on this one.","19/Aug/09 08:13;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12416945/pig_924.3.patch
  against trunk revision 804406.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 8 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/173/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/173/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/173/console

This message is automatically generated.","19/Aug/09 16:29;dvryaboy;Regarding deprecation -- I tried setting it back to off, and adding @SuppressWarnings(""deprecation"") to the shims for 20, but and complained about deprecation nonetheless. Not sure what its deal is.

Adding something like this to the main build.xml works. Does this seem like a reasonable solution?

{code}
    <!-- set deprecation off if hadoop version greater or equals 20 -->
    <target name=""set_deprecation"">
      <condition property=""hadoop_is20"">
	<equals arg1=""${hadoop.version}"" arg2=""20""/>
      </condition>
      <antcall target=""if_hadoop_is20""/>
      <antcall target=""if_hadoop_not20""/>
    </target>
    <target name=""if_hadoop_is20"" if=""hadoop_is20"">
      <property name=""javac.deprecation"" value=""off"" />
    </target>
    <target name=""if_hadoop_not20"" unless=""hadoop_is20"">
      <property name=""javac.deprecation"" value=""on"" />
    </target>


    <target name=""init"" depends=""set_deprecation"">
  [....]
{code}","19/Aug/09 18:32;daijy;From your latest patch, shims works this way
1. The version of shims Pig compiles is controlled by ""hadoop.version"" property in build.xml
2. The version of shims Pig uses is determined dynamically by hacking the string returned by VersionInfo.getVersion

As in your code comment, version string hack is not safe. My thinking is that pig only use bundled hadoop unless override:
1. Pig compile all version of shims, There is no conflict between different version of shims, why not compile them all? So user do not need to recompile the code if he want to use different external hadoop.
2. Pig bundles a default hadoop, which is specified by hadoop.version in build.xml. Pig use this version of shims by default
3. If user want to use an external hadoop, he/she need to override the default hadoop version explicitly, eg, ""-Dhadoop_version"" in command line. ","19/Aug/09 18:56;dvryaboy;Daniel, you've hit the nail on the head.

This patch is specifically written to enable us to compile against all the versions of hadoop, and let the user pick which one he wants at runtime (by virtue of including the right hadoop on the path -- no flags needed).  In fact the default ant task in the shims directory compiles all the shims at once.

The version string hack is safe, as long as hadoop is built correctly (the zebra version is not, as it returns ""Unknown"", hence the last-resort hack of defaulting to 20).
If hadoop came from its own jar I could use reflection to get the jar name, and use that as a fallback for an Unknown version -- but in pig, hadoop comes from the pig.jar !

Ideally, Pig would compile all the versions of shims into its jars, and the pig jar woud not include hadoop. Then the user would include the right hadoop on the path (or bin/pig would do it for him), and everything would happen automagically.  

By bundling hadoop into the jar, however, switching hadoop versions on the fly is next to impossible (or at least I don't know how) -- we have multiple jars on the classpath, and the classloader will use whatever is the latest (or is it earliest?). Finding the right resource becomes fraught with peril.

If existing deployments need a single pig.jar without a hadoop dependency, it might be possible to create a new target (pig-all) that would create a statically bundled jar; but I think the default behavior should be to not bundle, build all the shims, and use whatever hadoop is on the path.

The current patch is written as is so that it can be applied to trunk, enabling people to compile statically, and only require a change to the ant build files to switch to a dynamic compile later on (after 0.4, probably)","19/Aug/09 19:01;tlipcon;bq. If existing deployments need a single pig.jar without a hadoop dependency, it might be possible to create a new target (pig-all) that would create a statically bundled jar; but I think the default behavior should be to not bundle, build all the shims, and use whatever hadoop is on the path.

+1 for making the default to *not* bundle hadoop inside pig.jar, and adding another non-default target for those people who might want it.

bq. The current patch is written as is so that it can be applied to trunk, enabling people to compile statically, and only require a change to the ant build files to switch to a dynamic compile later on (after 0.4, probably)

From the packager's perspective, I'd love if this change could get in for 0.4. If it doesn't, we'll end up applying the patch ourselves for packaging purposes - we need to have the hadoop dependency be on the user's installed hadoop, not on whatever happened to get bundled into pig.jar.",19/Aug/09 21:53;dvryaboy;linking this with Pig for Hadoop 20 issue.,"20/Aug/09 17:23;owen.omalley;-1

I think this is a bad idea and is totally unmaintainable. In particular, the HadoopShim interface is very specific to the changes in those particular versions. We are trying to stabilize the FileSystem and Map/Reduce interfaces to avoid these problems and that is a much better solution.",20/Aug/09 17:29;daijy;Wrapping hadoop functionality add extra maintenance cost to adopting new features of hadoop. We still need to figure out the balance point between usability and maintenance cost. I don't think this issue is a blocker for 0.4.,"20/Aug/09 17:33;tlipcon;bq. I think this is a bad idea and is totally unmaintainable. In particular, the HadoopShim interface is very specific to the changes in those particular versions. We are trying to stabilize the FileSystem and Map/Reduce interfaces to avoid these problems and that is a much better solution.

Agreed that this is not a long term solution. Like you said, the long term solution is stabilized cross-version APIs so this isn't necessary. The fact is, though, that there are a significant number of people running 0.18.x who would like to use Pig 0.4.0, and supporting them out of the box seems worth it. This patch is pretty small and easily verifiable both by eye and by tests. Given that the API is still changing for 0.21, and Pig hasn't adopted the ""new"" MR APIs yet, it seems like it's premature to leave 18 in the cold.

Do you have an objection to committing this only on the 0.4.0 branch and *not* planning to maintain it in trunk/0.5?","20/Aug/09 17:36;dvryaboy;Owen -- I may not have made the intent clear; the idea is that when Pig is rewritten to use the future-proofed APIs, the shims will go away (presumably for 0.5).   Right now, pig is not using the new APIs, even the 20 patch posted by Olga uses the deprecated mapred calls. 

This is only to make life easier in the transitional period while Pig is using the old, mutating APIs.

Check out the pig user list archives for motivation of why these shims are needed.","20/Aug/09 17:37;acmurthy;I agree with Owen.

One conceivable option is for the Pig project to maintain separate branches (per Pig release) to support the various Hadoop versions... several projects are run this way. Clearly it adds to the cost for pushing out a release for the Pig committers and it is their call.","20/Aug/09 18:13;acmurthy;bq. The fact is, though, that there are a significant number of people running 0.18.x who would like to use Pig 0.4.0, and supporting them out of the box seems worth it. Given that the API is still changing for 0.21, and Pig hasn't adopted the ""new"" MR APIs yet, it seems like it's premature to leave 18 in the cold.

I believe the plan is for 0.4.0 to work with hadoop-0.18.* anyway... wouldn't that suffice?","20/Aug/09 18:18;olgan;Todd and Dmitry, I understand your intention. I am wondering if in the current situation, the following might not be the best course of action:

(1) Release Pig 0.4.0. I think we resolved all the blockers and can start the process
(2) Wait till Hadoop 20.1 is released and release Pig 0.5.0.

Owen promised that Hadoop 20.1 will go out for a vote next week. This means that Pig 0.4.0 and 0.5.0 will be just a couple of weeks apart which should not be a big issue for users. Meanwhile they can apply PIG-660 to the code bundled with Pig 0.4.0 or the trunk. I am currently working with the release engineering to get an official hadoop20.jar that Pig can  be build with. I expect to have it in the next couple of days.

The concern with applying the patch is the code complexity it introduces. Also, if there are patches that are version specific, they will not be easy to apply. Multiple branches is something we understand and know how to work with better. We also don't want to set a precedent of supporting pig releases on multiple versions on Hadoop because it is not clear that this is something we will be able to maintain going forward.","20/Aug/09 18:23;dvryaboy;Arun -- it wouldn't suffice for those who want to use pig-0.4 with hadoop 0.19* or 0.20*

Pig 0.5 isn't due out for 4 to 6 months which is behind the curve for adoption of 20.  Putting in this patch will make compatibility an issue of a compile-time flag.  Putting in this patch and restructuring the ant tasks somewhat will make this completely transparent.

Waiting until 0.5 means that users wind up with instructions like this: http://behemoth.strlen.net/~alex/hadoop20-pig-howto.txt for half a year.","20/Aug/09 18:32;tlipcon;Hey guys,

As we understood it, Pig 0.5 wasn't due for quite some time. If it's the case that 0.5 is a small release on top of 0.4 and it should be out in a few weeks, this seems a lot more reasonable.

Most likely we'll end up applying this patch to the 0.4 release for our distribution, even if there are multiple branches made in SVN. That's fine, though - we've got a process developed for this and are happy to support users on both versions for the next several months as people transition to 0.20 and the new APIs.

Feel free to resolve as wontfix
-Todd","20/Aug/09 18:50;sms;Hadoop has promised ""APIs in stone"" forever and has not delivered on that promise yet. Higher layers in the stack have to learn how to cope with a ever changing lower layer. How this change is managed is a matter of convenience to the owners of the higher layer. I really like Shims approach which avoids the cost of branching out Pig every time we make a compatible release. The cost of creating a branch for each version of hadoop seems to be too high compared to the cost of the Shims approach.

Of course, there are pros and cons to each approach. The question here is when will Hadoop set its APIs in stone and how many more releases will we have before this happens. If the answer to the question is 12 months and 2 more releases, then we should go with the Shims approach. If the answer is 3-6 months and one more release then we should stick with our current approach and pay the small penalty of patches supplied to work with the specific release of Hadoop.

Summary: Use the shims patch if APIs are not set in stone within a quarter or two and if there is more than one release of Hadoop.",21/Aug/09 18:44;olgan;Closing since we have a plan of action. I just aploaded needed code to PIG-660 to work with Hadoop 20 till we have an official release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow setting logfile location in pig.properties,PIG-923,12433109,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,dvryaboy,dvryaboy,14/Aug/09 18:52,24/Mar/10 22:13,14/Mar/19 03:05,20/Aug/09 22:18,0.3.0,,,,,,0.4.0,,,,,0,,,,"Local log file location can be specified through the -l flag, but it cannot be set in pig.properties.

This JIRA proposes a change to Main.java that allows it to read the ""pig.logfile"" property from the configuration.",,,,,,,,,,,,,,,,,,,14/Aug/09 18:56;dvryaboy;pig_923.patch;https://issues.apache.org/jira/secure/attachment/12416593/pig_923.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-08-17 20:53:43.987,,,no_permission,,,,,,,,,,,,41729,,,,,Fri Aug 21 11:59:32 UTC 2009,,,,,,,0|i0gpxj:,95631,,,,,,,,,,"14/Aug/09 18:56;dvryaboy;One-line change to allow Main.java to default to the value specified in pig.logfile.
-l still overrides.
Not specifying pig.logfile in pig.properties results in the same behavior as before.

No unit tests; checked manually.","17/Aug/09 20:53;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12416593/pig_923.patch
  against trunk revision 804406.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/166/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/166/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/166/console

This message is automatically generated.","17/Aug/09 20:56;dvryaboy;As mentioned above, this patch does not require unit tests.",18/Aug/09 21:51;daijy;+1,20/Aug/09 18:32;dvryaboy;Any chance this can go into 0.4?,20/Aug/09 22:18;daijy;Patch committed. It will goes to 0.4. Thank Dmitriy for contributing.,"21/Aug/09 11:59;hudson;Integrated in Pig-trunk #529 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/529/])
    : Allow setting logfile location in pig.properties
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Strange use case for Join which produces different results in local and map reduce mode,PIG-921,12433038,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,viraj,viraj,14/Aug/09 00:23,24/Mar/10 22:15,14/Mar/19 03:05,15/Oct/09 02:17,0.4.0,,,,,,0.6.0,,impl,,,0,,,,"I have script in this manner, loads from 2 files A.txt and B.txt
{code}
A = LOAD 'A.txt' as (a:tuple(a1:int, a2:chararray));
B = LOAD 'B.txt' as (b:tuple(b1:int, b2:chararray));
C = JOIN A by a.a1, B by b.b1;
DESCRIBE C;
DUMP C;
{code}

A.txt contains the following lines:
{code}
(1,a)
(2,aa)
{code}


B.txt contains the following lines:
{code}
(1,b)
(2,bb)
{code}

Now running the above script in local and map reduce mode on Hadoop 18 & Hadoop 20, produces the following:

Hadoop 18
=====================================================================
(1,1)
(2,2)
=====================================================================
Hadoop 20
=====================================================================
(1,1)
(2,2)
=====================================================================
Local Mode: Pig with Hadoop 18 jar release 
=====================================================================
2009-08-13 17:15:13,473 [main] INFO  org.apache.pig.Main - Logging error messages to: /homes/viraj/pig-svn/trunk/pigscripts/pig_1250208913472.log
09/08/13 17:15:13 INFO pig.Main: Logging error messages to: /homes/viraj/pig-svn/trunk/pigscripts/pig_1250208913472.log
C: {a: (a1: int,a2: chararray),b: (b1: int,b2: chararray)}
2009-08-13 17:15:13,932 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1002: Unable to store alias C
09/08/13 17:15:13 ERROR grunt.Grunt: ERROR 1002: Unable to store alias C
Details at logfile: /homes/viraj/pig-svn/trunk/pigscripts/pig_1250208913472.log
=====================================================================
Caused by: java.lang.NullPointerException
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage.getNext(POPackage.java:206)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:231)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:191)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:231)
        at org.apache.pig.backend.local.executionengine.physicalLayer.counters.POCounter.getNext(POCounter.java:71)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:231)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore.getNext(POStore.java:117)
        at org.apache.pig.backend.local.executionengine.LocalPigLauncher.runPipeline(LocalPigLauncher.java:146)
        at org.apache.pig.backend.local.executionengine.LocalPigLauncher.launchPig(LocalPigLauncher.java:109)
        at org.apache.pig.backend.local.executionengine.LocalExecutionEngine.execute(LocalExecutionEngine.java:165)
        ... 9 more
=====================================================================
Local Mode: Pig with Hadoop 20 jar release
=====================================================================
((1,a),(1,b))
((2,aa),(2,bb)
=====================================================================",Hadoop 18 and Hadoop 20,,,,,,,,,,,,,,,,,,14/Aug/09 00:30;viraj;A.txt;https://issues.apache.org/jira/secure/attachment/12416517/A.txt,14/Aug/09 00:30;viraj;B.txt;https://issues.apache.org/jira/secure/attachment/12416518/B.txt,13/Oct/09 22:10;daijy;PIG-921-1.patch;https://issues.apache.org/jira/secure/attachment/12422030/PIG-921-1.patch,14/Aug/09 00:30;viraj;joinusecase.pig;https://issues.apache.org/jira/secure/attachment/12416519/joinusecase.pig,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2009-10-13 22:08:41.918,,,no_permission,,,,,,,,,,,,164465,Reviewed,,,,Thu Oct 15 02:17:59 UTC 2009,,,,,,,0|i0gpwv:,95628,,,,,,,,,,14/Aug/09 00:30;viraj;Script with test data.,"13/Oct/09 22:08;daijy;The result should be ((1,a),(1,b)), ((2,aa),(2,bb). Map-reduce mode produces wrong result.","13/Oct/09 22:10;daijy;The problem is in POLocalReArragement, we skip the entire tuple in the value if we use one field of the tuple as join key.","14/Oct/09 03:56;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422030/PIG-921-1.patch
  against trunk revision 824980.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/78/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/78/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/78/console

This message is automatically generated.","15/Oct/09 02:08;pkamath;+1 - minor comment, we can probably remove preds==null || preds.get(0)==null from the if() since the project should always have a predecessor and if it does not the execution would fail somewhere else .",15/Oct/09 02:17;daijy;Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Type mismatch in key from map: expected org.apache.pig.impl.io.NullableBytesWritable, recieved org.apache.pig.impl.io.NullableText when doing simple group",PIG-919,12432929,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,viraj,viraj,viraj,13/Aug/09 00:45,08/Feb/11 07:09,14/Mar/19 03:05,27/Aug/09 23:35,0.3.0,,,,,,0.3.0,,impl,,,0,,,,"I have a Pig script, which takes in a student file and generates a bag of maps.  I later want to group on the value of the key ""name0"" which corresponds to the first name of the student.
{code}
register mymapudf.jar;



data = LOAD '/user/viraj/studenttab10k' AS (somename:chararray,age:long,marks:float);



genmap = foreach data generate flatten(mymapudf.GenHashList(somename,' ')) as bp:map[], age, marks;



getfirstnames = foreach genmap generate bp#'name0' as firstname, age, marks;



filternonnullfirstnames = filter getfirstnames by firstname is not null;




groupgenmap = group filternonnullfirstnames by firstname;



dump groupgenmap;
{code}

When I execute this code, I get an error in the Map Phase:
===========================================================================================================
java.io.IOException: Type mismatch in key from map: expected org.apache.pig.impl.io.NullableBytesWritable, recieved org.apache.pig.impl.io.NullableText
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:415)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:108)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:253)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:242)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:93)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:227)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2209)
===========================================================================================================",,,,,,,,,,,,,,,,,,,13/Aug/09 00:47;viraj;GenHashList.java;https://issues.apache.org/jira/secure/attachment/12416382/GenHashList.java,13/Aug/09 00:47;viraj;mapscript.pig;https://issues.apache.org/jira/secure/attachment/12416384/mapscript.pig,13/Aug/09 00:47;viraj;mymapudf.jar;https://issues.apache.org/jira/secure/attachment/12416383/mymapudf.jar,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-08-13 07:18:18.227,,,no_permission,,,,,,,,,,,,164463,,,,,Tue Feb 08 07:09:30 UTC 2011,,,,,,,0|i0gpvz:,95624,Jeff is right - the issue is in how maps are interpreted by Pig. I agree we need to figure out how best to work with maps but it is a bigger issue than the scope of this bug and I would like to deal with it separately. (In fact it is on the roadmap to address.),,,,,,,,,"13/Aug/09 00:47;viraj;Attaching script, Java UDF and jar file for testing","13/Aug/09 00:51;viraj;This problem can be solved simply by casting the firstname to chararray!! Why??
{code}
groupgenmap = group filternonnullfirstnames by (chararray)firstname;

dump groupgenmap;
{code}

Is there a problem with the UDF??","13/Aug/09 07:18;ankur;I have seem this issue in other places when the value coming out of a map[] is used in a group/cogroup/join. Pig throws a the same error. And Viraj is right, explicit casting to chararray alleviates the issue. But this is confusing for users. Pig should be converting ""NullableText"" to ""NullableBytesWritable"" automatically. Here is another sample script that throws an error. Exlicit casting to chararray resolves the issue

data = LOAD 'mydata' USING CustomLoader()  AS (f1:double, f2: map[])

dataProjected =  FOREACH data GENERATE f2#'Url' as url, f1 as rank

data2 = LOAD 'urlList' AS (url:bytearray);

grouped = COGROUP data BY url, data2 url Parallel 10;

STORE grouped INTO 'results'
","13/Aug/09 14:49;zjffdu;Viraj,
The error is because Pig Latin do not support declaring the type of map's key and value type.
In your UDF, the type of map's value is String.
But regarding your script, Pig can not guess what is the type of map's value. So default is bytearray which is do not consistent with the actual real type.

If you change you UDF to :              
{code}
HashMap<String, DataByteArray> pairs = new HashMap<String, DataByteArray>();
pairs.put(key, new DataByteArray(names[i]));
{code}
then it will be OK.


BTW, it really a shortage of Pig that Pig Latin can not support declaring the type of map's key and value type.
Hope, in the future it can support this.



","10/Dec/09 11:14;gmavromatis;The problem documented in this ticket has NOT been resolved.

I ran into it today.

If the problem is more general than described in this ticket, then what is the ticket capturing it?

If there is no such ticket, please reopen this ticket or file a new one.","10/Dec/09 23:24;alangates;This was closed Fixed when it should have been closed Won't Fix or Later.  No patch was checked in for it.  Where are you seeing this error?  In general Pig Latin does not allow users to declare the type of map values, so you have to explicitly cast map values if you want to group by them so that Pig knows how to set the key in the MR job.","21/Dec/09 08:43;gmavromatis;> This was closed Fixed when it should have been closed Won't Fix or Later.

Can we then resolve it with the correct resolution (Won't Fix or Later)?

> Where are you seeing this error?

I am seeing it in product code that I cannot refer to here. It has occurred twice, one instance of which was referred to this ticket and closed. I will send you more information offline.",07/Feb/11 21:06;alexmc;What is the progress on this? It is still an issue for me in Pig 0.8.0 !!!,08/Feb/11 00:49;olgan;The bug is marked as fixed so you are likely having a similar but different problem. I would suggest openning a new JIRA and pasting your failing query there.,"08/Feb/11 03:53;dvryaboy;This bug is marked as fixed incorrectly. It's actually ""not a bug"" or ""won't fix"" (see Alan's comments above).
Jeff's comment earlier in this ticket explains why this isn't working -- pig doesn't support anything but bytearrays in maps.
You have to cast explicitly. 

Allowing typed values in maps would be cool, but it's not a ""bug"", it's more of a limitation of the type system.",08/Feb/11 07:09;daijy;Actually this issue is addressed by PIG-1277 and typed map in 0.9.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[zebra] LOAD call will hang if only the first column group is queried,PIG-918,12432925,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yanz,yanz,yanz,12/Aug/09 23:43,24/Mar/10 22:13,14/Mar/19 03:05,03/Sep/09 23:35,0.4.0,,,,,,0.4.0,,,,,0,,,,"Zebra's LOAD call with projections that only nclude column(s) in the first column group will hang because an improper range of random numbers for index to the array of column groups always skips the first element so that if all other column groups are not used, the looping keeps running without a chance to break.  ",,,,,,,,,,,,,,,,,,,01/Sep/09 18:59;rangadi;pig-zebra.patch;https://issues.apache.org/jira/secure/attachment/12418281/pig-zebra.patch,12/Aug/09 23:47;yanz;pig-zebra.patch;https://issues.apache.org/jira/secure/attachment/12416377/pig-zebra.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-08-13 19:02:36.211,,,no_permission,,,,,,,,,,,,164462,,,,,Tue Sep 01 19:11:48 UTC 2009,,,Patch Available,,,,0|i0gpvj:,95622,,,,,,,,,,12/Aug/09 23:47;yanz;Fix patch.,13/Aug/09 19:02;chaow;Already reviewed the fix.,31/Aug/09 06:16;he yongqiang;why not just use a for loop?,31/Aug/09 06:45;yanz;To give all column groups in use equal chances to be checked.,31/Aug/09 18:53;chaow;+1 for the fix.,"01/Sep/09 18:59;rangadi;When you generate a patch with 'git diff' please use 'git diff --no-prefix' so that patch applies with 'patch -p0' command. I am updating the attached patch with this change.
",01/Sep/09 19:11;rangadi;I just committed this. Thanks Yan.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error in Pig script when grouping on chararray column,PIG-913,12432465,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,daijy,viraj,viraj,07/Aug/09 01:50,24/Mar/10 22:13,14/Mar/19 03:05,14/Aug/09 16:29,0.4.0,,,,,,0.4.0,,,,,0,,,,"I have a very simple script which fails at parsetime due to the schema I specified in the loader.
{code}
data = LOAD '/user/viraj/studenttab10k' AS (s:chararray);

dataSmall = limit data 100;

bb = GROUP dataSmall by $0;

dump bb;
{code}

=====================================================================================================================
2009-08-06 18:47:56,297 [main] INFO  org.apache.pig.Main - Logging error messages to: /homes/viraj/pig-svn/trunk/pig_1249609676296.log
09/08/06 18:47:56 INFO pig.Main: Logging error messages to: /homes/viraj/pig-svn/trunk/pig_1249609676296.log
2009-08-06 18:47:56,459 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://localhost:9000
09/08/06 18:47:56 INFO executionengine.HExecutionEngine: Connecting to hadoop file system at: hdfs://localhost:9000
2009-08-06 18:47:56,694 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to map-reduce job tracker at: localhost:9001
09/08/06 18:47:56 INFO executionengine.HExecutionEngine: Connecting to map-reduce job tracker at: localhost:9001
2009-08-06 18:47:57,008 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1002: Unable to store alias bb
09/08/06 18:47:57 ERROR grunt.Grunt: ERROR 1002: Unable to store alias bb
Details at logfile: /homes/viraj/pig-svn/trunk/pig_1249609676296.log
=====================================================================================================================
=====================================================================================================================

Pig Stack Trace
---------------
ERROR 1002: Unable to store alias bb

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias bb
        at org.apache.pig.PigServer.openIterator(PigServer.java:481)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:531)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:190)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:165)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:141)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:89)
        at org.apache.pig.Main.main(Main.java:397)
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1002: Unable to store alias bb
        at org.apache.pig.PigServer.store(PigServer.java:536)
        at org.apache.pig.PigServer.openIterator(PigServer.java:464)
        ... 6 more
Caused by: java.lang.NullPointerException
        at org.apache.pig.impl.logicalLayer.LOCogroup.unsetSchema(LOCogroup.java:359)
        at org.apache.pig.impl.logicalLayer.optimizer.SchemaRemover.visit(SchemaRemover.java:64)
        at org.apache.pig.impl.logicalLayer.LOCogroup.visit(LOCogroup.java:335)
        at org.apache.pig.impl.logicalLayer.LOCogroup.visit(LOCogroup.java:46)
        at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:68)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.impl.logicalLayer.optimizer.LogicalTransformer.rebuildSchemas(LogicalTransformer.java:67)
        at org.apache.pig.impl.logicalLayer.optimizer.LogicalOptimizer.optimize(LogicalOptimizer.java:187)
        at org.apache.pig.PigServer.compileLp(PigServer.java:854)
        at org.apache.pig.PigServer.compileLp(PigServer.java:791)
        at org.apache.pig.PigServer.store(PigServer.java:509)
        ... 7 more

=====================================================================================================================",,,,,,,,,,,,,,,,,,,13/Aug/09 20:57;daijy;PIG-913-2.patch;https://issues.apache.org/jira/secure/attachment/12416483/PIG-913-2.patch,11/Aug/09 23:30;daijy;PIG-913.patch;https://issues.apache.org/jira/secure/attachment/12416263/PIG-913.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-08-11 23:33:04.261,,,no_permission,,,,,,,,,,,,164458,,,,,Fri Aug 14 18:16:25 UTC 2009,,,,,,,0|i0gptj:,95613,,,,,,,,,,"07/Aug/09 01:52;viraj;The following works though..
{code}
data = LOAD '/user/viraj/studenttab10k' AS (s:bytearray);
--data =  LOAD '/user/viraj/studenttab10k' AS (s);

dataSmall = limit data 100;

bb = GROUP dataSmall by $0;

dump bb;
{code}

or ","11/Aug/09 23:33;daijy;The problem is caused by OpLimitOptimizer, which should use a correct way to rewire operators.",11/Aug/09 23:40;dvryaboy;Daniel -- throw in a test to check for optimizer regressions in the future?,"12/Aug/09 00:00;daijy;Thanks, Dmitriy, 
I will put unit test. I submit it first to see if it broke any existing unit test first.","12/Aug/09 00:46;sms;+1 for the fix. As Dmitriy indicates, we need new unit test cases after Hudson verifies the patch.","12/Aug/09 01:19;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12416263/PIG-913.patch
  against trunk revision 803312.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/158/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/158/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/158/console

This message is automatically generated.",13/Aug/09 20:57;daijy;New patch include test case,"13/Aug/09 23:11;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12416483/PIG-913-2.patch
  against trunk revision 803377.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 7 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 162 release audit warnings (more than the trunk's current 161 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/161/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/161/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/161/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/161/console

This message is automatically generated.",14/Aug/09 16:16;daijy;This release audit warning is caused by a new golden file. We cannot add release audit notes to golden files.,14/Aug/09 16:29;daijy;Patch committed.,"14/Aug/09 18:16;hudson;Integrated in Pig-trunk #522 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/522/])
    : Error in Pig script when grouping on chararray column
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TOKENIZE throws exception on null data,PIG-905,12432172,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,olgan,olgan,04/Aug/09 15:30,24/Mar/10 22:13,14/Mar/19 03:05,07/Aug/09 02:27,0.3.0,,,,,,0.4.0,,,,,0,,,,it should just return null,,,,,,,,,,,,,,,,,,,05/Aug/09 06:59;daijy;PIG-905-1.patch;https://issues.apache.org/jira/secure/attachment/12415579/PIG-905-1.patch,05/Aug/09 18:07;daijy;PIG-905-2.patch;https://issues.apache.org/jira/secure/attachment/12415639/PIG-905-2.patch,06/Aug/09 17:40;daijy;PIG-905-3.patch;https://issues.apache.org/jira/secure/attachment/12415765/PIG-905-3.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-08-05 02:56:47.07,,,no_permission,,,,,,,,,,,,164452,,,,,Fri Aug 07 12:12:08 UTC 2009,,,,,,,0|i0gpqv:,95601,,,,,,,,,,"05/Aug/09 02:56;zjffdu;I find that TOKENIZE can not handle DataByteArray.  It can only handle string. 
I believe it should be better to handle both DataByteArray and String. In my opinion, whenever an UDF support one of them, it should support both of them.
Because they are almost the same except that DataByteArray is Comparable and Serializable.

","05/Aug/09 06:59;daijy;Include null handling suggested by Olga. For Jeff's comment on DataByteArray, it is because TOKENIZE do not declare getArgToFuncMapping. If UDF declare getArgToFuncMapping, Pig can handle type cast correctly. I also include getArgToFuncMapping in the patch.",05/Aug/09 14:33;olgan;I think we should also check for null or empty input tuple and return null in those cases as well. Other than that the patch looks good.,05/Aug/09 18:07;daijy;Address Olga's comments and add unit test,"06/Aug/09 16:33;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12415639/PIG-905-2.patch
  against trunk revision 801460.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/151/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/151/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/151/console

This message is automatically generated.",06/Aug/09 17:40;daijy;Fix unit test,"07/Aug/09 01:35;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12415765/PIG-905-3.patch
  against trunk revision 801460.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/154/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/154/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/154/console

This message is automatically generated.",07/Aug/09 02:27;daijy;Patch committed.,"07/Aug/09 12:12;hudson;Integrated in Pig-trunk #515 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/515/])
    : TOKENIZE throws exception on null data
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InputSplit (SliceWrapper) created by Pig is big in size due to serialized PigContext,PIG-901,12432073,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,03/Aug/09 18:40,24/Mar/10 22:13,14/Mar/19 03:05,05/Aug/09 23:19,0.4.0,,,,,,0.4.0,,,,,0,,,,InputSplit (SliceWrapper) created by Pig is big in size due to serialized PigContext. SliceWrapper only needs ExecType - so the entire PigContext should not be serialized and only the ExecType should be serialized.,,,,,,,,,,,,,,,,,,,03/Aug/09 23:14;daijy;PIG-901-1.patch;https://issues.apache.org/jira/secure/attachment/12415422/PIG-901-1.patch,04/Aug/09 01:13;pkamath;PIG-901-branch-0.3.patch;https://issues.apache.org/jira/secure/attachment/12415430/PIG-901-branch-0.3.patch,04/Aug/09 17:12;pkamath;PIG-901-trunk.patch;https://issues.apache.org/jira/secure/attachment/12415495/PIG-901-trunk.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-08-03 18:55:19.537,,,no_permission,,,,,,,,,,,,164451,Reviewed,,,,Fri Aug 07 12:12:09 UTC 2009,,,,,,,0|i0gppb:,95594,,,,,,,,,,03/Aug/09 18:55;daijy;PigContext.packageImportList needs to be serialized as well. Otherwise InputSplit cannot instantiate Loader function.,03/Aug/09 23:14;daijy;Add a unit test to make sure this change will not affect udf.import.list,04/Aug/09 01:13;pkamath;Patch for 0.3 branch,"04/Aug/09 01:25;olgan;+1 on the patch for the 0.3 branch. Please, commit",04/Aug/09 01:30;acmurthy;It would be nice to add a test case which (for now) checks to ensure that the size of a serialized 'slice' is less than 500KB or so...,"04/Aug/09 17:18;pkamath;PIG-901-trunk.patch is for the trunk. The change is in SliceWrapper to serialize ExecType only instead of PigContext since only the ExecType from the PigContext is used on deserialization. The package import list which Daniel referred to is a static member of PigContext which is explicitly set in SliceWrapper.makeRecordReader() and hence is taken care of.

It is a good suggestion to include a test case to check that even with a sizeable PigContext, we actually create small input splits. However to do this in the current Pig code layout means opening up PigServer and JobControlCompiler so that we can compile a pig script upto job creation and then instead of submitting the job to hadoop, instatiate PigInputFormat with the jobConf and get the Input Splits. This may require some design changes which we should address at some point for these kinds of tests. For now there is regression test in the patch to ensure the package import list is correctly handled and we have manually tested to ensure the split size is small (order of KBs).","04/Aug/09 17:25;acmurthy;bq. This may require some design changes which we should address at some point for these kinds of tests.

Could you please track this with a new jira? Thanks!",04/Aug/09 17:36;pkamath;https://issues.apache.org/jira/browse/PIG-906 has been created to track changes to enable unit testing these types of hadoop integration scenarios.,05/Aug/09 13:50;gkesavan;resubmitting to hudson patch queue ,"05/Aug/09 16:10;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12415495/PIG-901-trunk.patch
  against trunk revision 800494.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/147/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/147/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/147/console

This message is automatically generated.",05/Aug/09 18:45;olgan;+1 on the patch to the trunk,05/Aug/09 23:19;pkamath;Patch committed to trunk and branch-0.3,"07/Aug/09 12:12;hudson;Integrated in Pig-trunk #515 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/515/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
order-by fails when input is empty,PIG-894,12431060,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,thejas,thejas,21/Jul/09 18:55,24/Mar/10 22:15,14/Mar/19 03:05,09/Oct/09 18:08,0.4.0,,,,,,0.6.0,,,,,1,,,,"grunt> l = load 'students.txt' ;
grunt> f = filter l by 1 == 2;
grunt> o = order f by $0 ;
grunt> dump o;

This results in 3 MR jobs . The 2nd (sampling) MR creates empty sample file, and 3rd MR (order-by) fails with following error in Map job -

java.lang.RuntimeException: java.lang.RuntimeException: Empty samples file
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.configure(WeightedRangePartitioner.java:104)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:58)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.(MapTask.java:348)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:193)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2207)
Caused by: java.lang.RuntimeException: Empty samples file
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.configure(WeightedRangePartitioner.java:89)
	... 5 more
",,,,,,,,,,,,,,,,,,,08/Oct/09 23:04;daijy;PIG-894-1.patch;https://issues.apache.org/jira/secure/attachment/12421681/PIG-894-1.patch,09/Oct/09 01:53;daijy;PIG-894-2.patch;https://issues.apache.org/jira/secure/attachment/12421694/PIG-894-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-09-14 09:04:23.528,,,no_permission,,,,,,,,,,,,164445,Reviewed,,,,Fri Oct 09 18:08:02 UTC 2009,,,,,,,0|i0gplr:,95578,,,,,,,,,,14/Sep/09 09:04;ankur;Is empty inputs referring to relation - l ('students.txt')  or f (filter l by 1 == 2). I am seeing a similar issue where the sampler produces an empty file when the number of records in the relation being sorted in too low ( < 4 ). ,"09/Oct/09 01:42;pkamath;The patch uses pig.inputs property from jobconf which does not directly have the input file name - it actually has a serialized arrayList<Pair<FileSpec, Boolean>> in string form containing the filespec and the issplittable flag for each input for the job - this serialized string will need to be deserialized using ObjectSerializer.deserialize and then from the filespec, the filename will need to be retrieved.",09/Oct/09 01:53;daijy;Fix the issue Pradeep find.,"09/Oct/09 02:09;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421681/PIG-894-1.patch
  against trunk revision 823257.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/16/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/16/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/16/console

This message is automatically generated.","09/Oct/09 05:07;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421694/PIG-894-2.patch
  against trunk revision 823257.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/17/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/17/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/17/console

This message is automatically generated.",09/Oct/09 17:55;pkamath;+1 to new patch,09/Oct/09 18:08;daijy;Patch committed. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixing dfs statement for Pig,PIG-891,12430834,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,zjffdu,daijy,daijy,18/Jul/09 00:19,24/Mar/10 22:15,14/Mar/19 03:05,21/Sep/09 16:56,0.4.0,,,,,,0.6.0,,,,,0,,,,"Several hadoop dfs commands are not support or restrictive on current Pig. We need to fix that. These include:
1. Several commands do not supported: lsr, dus, count, rmr, expunge, put, moveFromLocal, get, getmerge, text, moveToLocal, mkdir, touchz, test, stat, tail, chmod, chown, chgrp. A reference for these command can be found in http://hadoop.apache.org/common/docs/current/hdfs_shell.html
2. All existing dfs commands do not support globing.
3. Pig should provide a programmatic way to perform dfs commands. Several of them exist in PigServer, but not all of them.",,,,,,,,,,,,,,,,,,,12/Sep/09 10:00;zjffdu;Pig_891.patch;https://issues.apache.org/jira/secure/attachment/12419372/Pig_891.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-08-03 05:13:42.915,,,no_permission,,,,,,,,,,,,164442,,,,,Mon Sep 21 16:55:59 UTC 2009,,,,,,,0|i0gpk7:,95571,,,,,,,,,,"03/Aug/09 05:13;zjffdu;Daniel,  are you working on this right now ?
If not, I can help resolve this issue.

","03/Aug/09 05:20;daijy;Hi Jeff, I am not working on it. We appreciate and need your help on this.",04/Aug/09 18:12;pkamath;There have been some discussions around whether hadoop commands should be supported through hadoop fsShell - see https://issues.apache.org/jira/browse/PIG-79. I think that option should be explored before expanding pig's grunt shell - it would cleaner to not have that code in pig since truely hadoop commands should be handled by hadoop's utility code (fsShell). ,05/Aug/09 18:08;alangates;+1 to Pradeep's suggestion.  I think it would be better for Pig dump its shell command implementations and have grunt dispatch shell commands to Hadoop.  This avoids Pig needing to implement these commands.  It also means Pig will automatically be in sync with Hadoop on how shell commands work.,"10/Sep/09 14:38;zjffdu;Attach the patch, delegate the dfs command to FsShell.  
dfs command syntax is the same as hive, 
e.g.  dfs -ls .   


 ",11/Sep/09 01:16;daijy;I am reviewing the patch. ,"11/Sep/09 01:42;daijy;+1. Patch is concise and clear. 

For the syntax, I need opinion from other developers / users:
1. After this patch, shall we remove existing hadoop file system commands since they are duplicate with the new one
2. Shall we allow both fs/dfs to indicate a dfs command, just like hadoop does? ","11/Sep/09 17:30;daijy;Tested in local mode also, the patch even works well in local mode. We have discussed issues in my previous comment, the suggestions are:

1. We can keep existing file system commands for now
2. We shall use ""fs"" instead of ""dfs"" to indicate a file system command as latest hadoop does

Jeff, can you make this little change (""dfs""->""fs"") and submit again? Thanks!","12/Sep/09 10:00;zjffdu;Update the patch, change ""dfs"" to ""fs""","12/Sep/09 11:53;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419372/Pig_891.patch
  against trunk revision 814075.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 4 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 278 release audit warnings (more than the trunk's current 276 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/26/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/26/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/26/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/26/console

This message is automatically generated.","12/Sep/09 12:23;zjffdu;Daniel,

Do you know what causes the release audit failed ?
I do not understand the message in [http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/26/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt]",14/Sep/09 17:12;daijy;Not quite sure about it now. But I will figure out and let you know. Thanks.,"15/Sep/09 16:59;daijy;Hi, Jeff,
I get response from the Giri who implement this release audit checking. We can ignore the release audit warnings in this case. I will commit the patch shortly.",21/Sep/09 16:55;daijy;Patch committed. Thanks Jeff for contributing!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig do not pass udf to the backend in some situation,PIG-888,12430618,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,16/Jul/09 01:34,24/Mar/10 22:13,14/Mar/19 03:05,17/Jul/09 16:22,0.3.0,,,,,,0.4.0,,impl,,,0,,,,"If we use udf and do not use register, in some situation backend will complain that it cannot resolve class. For example, the following script do not work.

A = load '1.txt' using udf1();
B = load '2.txt';
C = join A by $0, B by $0;",,,,,,,,,,,,,,,,,,,16/Jul/09 02:12;daijy;PIG-888-1.patch;https://issues.apache.org/jira/secure/attachment/12413622/PIG-888-1.patch,16/Jul/09 03:58;daijy;PIG-888-2.patch;https://issues.apache.org/jira/secure/attachment/12413624/PIG-888-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-07-16 05:39:31.08,,,no_permission,,,,,,,,,,,,164439,,,,,Fri Jul 17 12:10:45 UTC 2009,,,,,,,0|i0gpj3:,95566,,,,,,,,,,16/Jul/09 02:12;daijy;The problem is that we didn't copy udf properly when we merge mrplan of different inputs in visitGlobalRearrange. Another related fix is that we should also deserialize import_list in PigInputFormat (as we did in https://issues.apache.org/jira/browse/PIG-883). I put these two fixes together in the patch.,16/Jul/09 03:58;daijy;New patch include unit test.,"16/Jul/09 05:39;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12413622/PIG-888-1.patch
  against trunk revision 793750.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 255 javac compiler warnings (more than the trunk's current 254 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/128/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/128/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/128/console

This message is automatically generated.",16/Jul/09 17:07;daijy;Resubmit for Hudson pick up the new patch.,"16/Jul/09 20:53;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12413624/PIG-888-2.patch
  against trunk revision 793750.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 255 javac compiler warnings (more than the trunk's current 254 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/130/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/130/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/130/console

This message is automatically generated.","16/Jul/09 22:52;daijy;javac warning is caused by unchecked cast to a template class. By convention, we do not suppress this warning in Pig. ",17/Jul/09 01:15;olgan;+1,17/Jul/09 01:29;daijy;Patch committed.,"17/Jul/09 12:10;hudson;Integrated in Pig-trunk #506 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/506/])
    : Pig do not pass udf to the backend in some situation
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
udf import list does not send to the backend,PIG-883,12430154,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,11/Jul/09 23:17,24/Mar/10 22:13,14/Mar/19 03:05,13/Jul/09 18:58,0.3.0,,,,,,0.4.0,,impl,,,0,,,,UDF import list is static variable in PigContext. It does not serialized and send to the backend along with PigContext. We need to serialize it separately.,,,,,,,,,,,,,,,,,,,11/Jul/09 23:18;daijy;PIG-883-1.patch;https://issues.apache.org/jira/secure/attachment/12413215/PIG-883-1.patch,12/Jul/09 01:44;daijy;PIG-883-2.patch;https://issues.apache.org/jira/secure/attachment/12413217/PIG-883-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-07-12 01:03:06.729,,,no_permission,,,,,,,,,,,,164434,,,,,Tue Jul 14 12:23:45 UTC 2009,,,,,,,0|i0gpgv:,95556,,,,,,,,,,"12/Jul/09 01:03;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12413215/PIG-883-1.patch
  against trunk revision 793147.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 254 javac compiler warnings (more than the trunk's current 250 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/124/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/124/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/124/console

This message is automatically generated.",12/Jul/09 01:44;daijy;Fix the unit test and submit again.,"12/Jul/09 03:33;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12413217/PIG-883-2.patch
  against trunk revision 793147.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 254 javac compiler warnings (more than the trunk's current 250 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/125/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/125/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/125/console

This message is automatically generated.","13/Jul/09 16:21;daijy;javac warning is caused by unchecked cast to a template class. By convention, we do not suppress this warning in Pig.","13/Jul/09 18:10;olgan;+1, please, commit",13/Jul/09 18:58;daijy;Patch committed.,"14/Jul/09 12:23;hudson;Integrated in Pig-trunk #503 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/503/])
    : udf import list does not send to the backend
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
log level not propogated to loggers ,PIG-882,12430096,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,thejas,thejas,10/Jul/09 20:09,24/Mar/10 22:13,14/Mar/19 03:05,11/Sep/09 20:35,0.3.0,,,,,,0.4.0,,impl,,,1,,,,"Pig accepts log level as a parameter. But the log level it captures is not set appropriately, so that loggers in different classes log at the specified level.",,,,,,,,,,,,,,,,,,,26/Jul/09 23:49;daijy;PIG-882-1.patch;https://issues.apache.org/jira/secure/attachment/12414572/PIG-882-1.patch,28/Jul/09 22:30;daijy;PIG-882-2.patch;https://issues.apache.org/jira/secure/attachment/12414814/PIG-882-2.patch,28/Jul/09 23:14;daijy;PIG-882-3.patch;https://issues.apache.org/jira/secure/attachment/12414817/PIG-882-3.patch,29/Jul/09 18:05;daijy;PIG-882-4.patch;https://issues.apache.org/jira/secure/attachment/12414928/PIG-882-4.patch,30/Jul/09 05:55;daijy;PIG-882-5.patch;https://issues.apache.org/jira/secure/attachment/12414991/PIG-882-5.patch,11/Sep/09 00:44;daijy;duplicate_message.patch;https://issues.apache.org/jira/secure/attachment/12419249/duplicate_message.patch,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2009-07-28 22:30:09.967,,,no_permission,,,,,,,,,,,,164433,,,,,Fri Sep 11 20:35:59 UTC 2009,,,,,,,0|i0gpgf:,95554,,,,,,,,,,"28/Jul/09 22:30;daijy;New patch propagates pig log level to backend. I do not find a good way to write a unit test especially for the backend part so no unit test was added,","28/Jul/09 23:11;sms;Minor comment:

Index: src/org/apache/pig/Main.java
===================================================================

Instead of printing the warning message to stdout, it should be printed to stderr.

{code}
+        catch (IOException e)
+        {
+            System.out.println(""Warn: Cannot open log4j properties file, use default"");
+        }
{code}


The rest of the patch looks fine.",28/Jul/09 23:14;daijy;Modify patch according to Santhosh's suggestion.,"28/Jul/09 23:27;daijy;Some comments about how to use debug level:
1. conf/log4j.properties is not used by default, if you want to use it, you need explicitly say: -4 conf/log4j.properties
2. -d DEBUG and -b works as desired
3. Only log level will propogate to backend, all logs appear in hadoop syslog, no matter what you put into log4j.properties. Hadoop have its own mechanism for all backend logging, Pig only override log level for Pig logs.",29/Jul/09 18:05;daijy;Sync with latest trunk,"30/Jul/09 05:50;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12414928/PIG-882-4.patch
  against trunk revision 799141.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to cause Findbugs to fail.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/145/testReport/
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/145/console

This message is automatically generated.","30/Jul/09 20:30;daijy;Pick me up, Hudson!","03/Aug/09 18:59;daijy;All unit tests passed. Hudson do not respond, committed patch anyway.","04/Aug/09 13:27;hudson;Integrated in Pig-trunk #512 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/512/])
    : log level not propogated to loggers
","11/Sep/09 00:44;daijy;Previous patch introduce a duplicate error message problem. The cause of the problem is that we set the org.apache.pig to use appender PIGCONSOLE, but do not change the root logger. Log4j think that root logger is using a default appender and org.apache.pig is using PIGCONSOLE. So the message is printed out twice to the default appender and PIGCONSOLE. ","11/Sep/09 02:56;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419249/duplicate_message.patch
  against trunk revision 813601.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/3/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/3/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/3/console

This message is automatically generated.","11/Sep/09 06:04;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419249/duplicate_message.patch
  against trunk revision 813601.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/4/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/4/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/4/console

This message is automatically generated.",11/Sep/09 17:09;olgan;+1,11/Sep/09 20:35;daijy;I don't have a unit test case for the same reason of the first patch. See my first comment.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig should ship load udfs to the backend,PIG-881,12430089,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,10/Jul/09 18:37,24/Mar/10 22:13,14/Mar/19 03:05,11/Jul/09 23:28,0.3.0,,,,,,0.4.0,,impl,,,0,,,,"Currently, when we use load udfs, we have to use ""register"" statement. It is ideal that if user put udf jars in classpath, we can omit register statement, Pig can pick the udf from classpath automatically.

However, Pig do not ship load udfs currently, the classpath approach does not work. ""register"" works because Pig ship that entire jar. Pig do ship eval udfs and storage udfs, we should ship load udfs as well.",,,,,,,,,,,,,,,,,,,10/Jul/09 19:05;daijy;PIG-881-1.patch;https://issues.apache.org/jira/secure/attachment/12413147/PIG-881-1.patch,10/Jul/09 21:25;daijy;PIG-881-2.patch;https://issues.apache.org/jira/secure/attachment/12413156/PIG-881-2.patch,10/Jul/09 23:22;daijy;PIG-881-3.patch;https://issues.apache.org/jira/secure/attachment/12413171/PIG-881-3.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-07-10 19:23:59.54,,,no_permission,,,,,,,,,,,,164432,,,,,Sat Jul 11 23:28:16 UTC 2009,,,,,,,0|i0gpfz:,95552,,,,,,,,,,"10/Jul/09 18:45;daijy;In the patch, I also modify JarManager. One statement in JarManager prevent all classes which is load from classpath to be included. I currently remove this but I am not sure if I am doing it right. ","10/Jul/09 18:53;daijy;Find some problem, I will deliver patch again shortly.",10/Jul/09 19:23;olgan;+1; the patch looks good!,"10/Jul/09 21:25;daijy;One unit test fail, resubmit the patch.",10/Jul/09 23:22;daijy;Get all unit test pass.,"10/Jul/09 23:28;olgan;+1 on the patch. Patch process seems to ve stuck again. We ran the tests manually and they passed, so please, commit the patch.","10/Jul/09 23:48;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12413156/PIG-881-2.patch
  against trunk revision 792663.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/122/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/122/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/122/console

This message is automatically generated.","11/Jul/09 00:03;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12413171/PIG-881-3.patch
  against trunk revision 793147.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/123/console

This message is automatically generated.","11/Jul/09 23:28;daijy;Patch committed. We did a manual unit test. All test pass. For the findbug warning, it is due to a dead variable. The line reference to the variable is commented out in the patch. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Order by is borken with complex fields,PIG-880,12430084,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,olgan,olgan,10/Jul/09 17:51,24/Mar/10 22:13,14/Mar/19 03:05,31/Jul/09 21:16,0.3.0,,,,,,0.4.0,,,,,0,,,,"Pig script:

a = load 'studentcomplextab10k' as (smap:map[],c2,c3);
f = foreach a generate smap#'name, smap#'age', smap#'gpa' ;            
s = order f by $0;           
store s into 'sc.out'         

Stack:

Caused by: java.lang.ArrayStoreException
        at java.lang.System.arraycopy(Native Method)
        at java.util.Arrays.copyOf(Arrays.java:2763)
        at java.util.ArrayList.toArray(ArrayList.java:305)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.convertToArray(WeightedRangePartitioner.java:154)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.configure(WeightedRangePartitioner.java:96)
        ... 5 more

        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher.getErrorMessages(Launcher.java:230)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher.getStats(Launcher.java:179)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:204)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:265)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:769)
        at org.apache.pig.PigServer.execute(PigServer.java:762)
        at org.apache.pig.PigServer.access$100(PigServer.java:91)
        at org.apache.pig.PigServer$Graph.execute(PigServer.java:933)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:245)
        at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:112)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:168)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:140)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:88)
        at org.apache.pig.Main.main(Main.java:389)

",,,,,,,,,,,,,,,,,,,16/Jul/09 20:37;pkamath;PIG-880-bytearray-mapvalue-code-without-tests.patch;https://issues.apache.org/jira/secure/attachment/12413733/PIG-880-bytearray-mapvalue-code-without-tests.patch,31/Jul/09 00:13;sms;PIG-880_1.patch;https://issues.apache.org/jira/secure/attachment/12415077/PIG-880_1.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-07-10 23:20:30.496,,,no_permission,,,,,,,,,,,,164431,Incompatible change,Reviewed,,,Fri Jul 31 21:16:01 UTC 2009,,,,,,,0|i0gpfj:,95550,,,,,,,MapValues Bytearray,,,"10/Jul/09 23:20;pkamath;The root cause of this issue is that in interpreting map data, PigStorage returns values in the map to be of the type that it deduces based on the data. So string data for values are returned as String, integer values are returned as Integer. However the logical layer in Pig assumes the type of the values in the map to be ByteArray since it cannot assume any type. If one of the sampled values forming the quantile list is a null, it is assumed to be of type of the reduce key of the final order by job. In this case, since the order by key is smap#'name', it is thought to be of type ByteArray. However the values resulting from the map lookup are actually of type String.  This mismatch results in the above exception - if nulls are filtered out, map.collect() fails because hadoop thinks the map key type is bytearray but it gets a Text (string).

A proposal to fix this is to Change TextDataParser which is used by PigStorage for reading map data to return ByteArray type for the values in the map.

Thoughts?

",10/Jul/09 23:28;pkamath;The root cause here is the same issue discussed in PIG-724,"16/Jul/09 20:37;pkamath;Attached code only patch which changes the way PigStorage would return values for maps (change is to return bytearray type). This is based on the proposal above. If this proposal is fine, this patch can be augmented with unit tests.",17/Jul/09 01:52;olgan;+1. I think this is reasonable fix until we figure out a better way of dealing with maps,"19/Jul/09 15:53;sms;Review Comment:

PigStorage should read map values as strings instead of interpreting the types. This way, integers that are too long to fit into Integer, etc. will still be interpreted as bytearray.","30/Jul/09 05:02;sms;Attached patch creates maps with value type set to DataByteArray (i.e., bytearray) for text data parsed by PigStorage. This change is consistent with the language semantics of treating value type as bytearray. New test cases have been added.",30/Jul/09 21:46;pkamath;+1 - looks good.,31/Jul/09 00:13;sms;Attaching a new patch that fixes a couple of unit tests.,31/Jul/09 00:36;pkamath;+1 to the new changes.,31/Jul/09 21:16;sms;Patch has been committed. This fix breaks backward compatibility where PigStorage reads maps. The type of the map values will now be bytearray instead of the actual type.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig should provide a way for input location string in load statement to be passed as-is to the Loader,PIG-879,12430083,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,pkamath,pkamath,10/Jul/09 17:38,14/May/10 06:45,14/Mar/19 03:05,19/Nov/09 23:39,0.3.0,,,,,,0.7.0,,,,,0,,,," Due to multiquery optimization, Pig always converts the filenames to absolute URIs (see http://wiki.apache.org/pig/PigMultiQueryPerformanceSpecification - section about Incompatible Changes - Path Names and Schemes). This is necessary since the script may have ""cd .."" statements between load or store statements and if the load statements have relative paths, we would need to convert to absolute paths to know where to load/store from. To do this QueryParser.massageFilename() has the code below[1] which basically gives the fully qualified hdfs path
 
However the issue with this approach is that if the filename string is something like ""hdfs://localhost.localdomain:39125/user/bla/1,hdfs://localhost.localdomain:39125/user/bla/2"", the code below[1] actually translates this to hdfs://localhost.localdomain:38264/user/bla/1,hdfs://localhost.localdomain:38264/user/bla/2 and throws an exception that it is an incorrect path.
 
Some loaders may want to interpret the filenames (the input location string in the load statement) in any way they wish and may want Pig to not make absolute paths out of them.
 
There are a few options to address this:
1)    A command line switch to indicate to Pig that pathnames in the script are all absolute and hence Pig should not alter them and pass them as-is to Loaders and Storers. 
2)    A keyword in the load and store statements to indicate the same intent to pig
3)    A property which users can supply on cmdline or in pig.properties to indicate the same intent.
4)    A method in LoadFunc - relativeToAbsolutePath(String filename, String curDir) which does the conversion to absolute - this way Loader can chose to implement it as a noop.

Thoughts?
 
",,,,,,,,,,PIG-966,,,,,,,,,19/Nov/09 22:24;rding;PIG-879.patch;https://issues.apache.org/jira/secure/attachment/12425534/PIG-879.patch,18/Nov/09 20:25;rding;PIG-879.patch;https://issues.apache.org/jira/secure/attachment/12425377/PIG-879.patch,17/Nov/09 23:12;rding;PIG-879.patch;https://issues.apache.org/jira/secure/attachment/12425274/PIG-879.patch,17/Nov/09 19:39;rding;PIG-879.patch;https://issues.apache.org/jira/secure/attachment/12425252/PIG-879.patch,17/Nov/09 19:23;rding;PIG-879.patch;https://issues.apache.org/jira/secure/attachment/12425250/PIG-879.patch,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2009-07-10 18:00:28.249,,,no_permission,,,,,,,,,,,,164430,Reviewed,,,,Thu Nov 19 23:39:21 UTC 2009,,,,,,,0|i0gpf3:,95548,,,,,,,,,,"10/Jul/09 18:00;hong.tang;1) and 3) are kind of equivalent to user, and are preferred for customized loaders that do not wish pig to do the escaping at all. 
","10/Jul/09 18:11;dvryaboy;Having this be a global flag through properties wouldn't work for scripts that require both behaviors in different load statements.

Maybe a boolean ""performPathConversion"" flag which is true by default, and can be overridden via the load statement?
Custom Loaders could change what their default is.
I think a boolean flag is more straightforward than a method you have to override with a no-op.","10/Jul/09 18:19;thejas;The problem with 1 & 3 is that the setting is universal to the grunt shell or script.
In cases where user wants to read from read from multiple sources with different loaders, it will be inconvenient to be forced to use absolute uri's for all of them.
","10/Jul/09 18:55;hong.tang;Both are valid arguments. The problem of 2) and 4) are that they require change to the load statement syntax or load-func api and would take longer to get there. 

I guess we could structure the fix in two phases: Phase One: supporting 1) and 3), so that we can have the minimum to move along without having to disable multi-query optimization completely. User should be able to modify the script to change all relative paths to absolute ones (the chance of such usage should be rare that most people should not be impacted). Phase Two: support either 2) or 4) (but I do not think we need both). And personally I think 4) would be better because loader should be the one that interprets the location string syntax.","10/Jul/09 22:03;milindb;I see some long term issues with all the approaches/options.

First, not all loaders require a path. (e.g. DBLoader) Some paths (e.g. hftp:// or hsftp://) do not have a notion of relative or absolute. Indeed, the right way to fix this is to change the syntax of load and store statements, so that the loader itself deals with the path handling, and not pig. Second, take out copyToLocal, cp, mv, and all the dfs shell functionality from pig. These are side effects and impose a barrier for optimization. In the current form, they do not belong in a dataflow language. Grunt could still support it.","10/Jul/09 22:37;dvryaboy;Milind -- as far as hftp:// and hsftp:// are concerned, the multiquery spec says ""Arguments used in a load statement that have a scheme other than ""hdfs"" or ""file"" will not be expanded and passed to the LoadFunc/Slicer unchanged. """,22/Oct/09 18:56;pkamath;We may want to address this as part of implementing http://wiki.apache.org/pig/LoadStoreRedesignProposal. Option 4 seems the most extensible since loadfuncs get flexibility of dealing with the location string. At the same time we could provide some utility method to LoadFunc writers which they can use or refer to and this can be provided by pig as a static utility method (this can be the implementation that Pig internally uses in its builtin loaders),"13/Nov/09 01:25;rding;As a related issue, there is a feature in Pig right now that allows user to specify a local file in a load statement even if Pig is running in MapReduce mode. Namely, 

{code}
A = load 'file:test/org/apache/pig/test/data/passwd' using PigStorage(':');
{code}

is a valid Pig statement. Internally Pig moves the file from the local file system to the HDFS file system and gives the corresponding HDFS URI to the loader: 

{code}
hdfs://localhost.localdomain:37575/tmp/temp957104276/tmp124591329
{code}

As we move to the new Load/Store API, there are two options:

1. Stop supporting this feature. The above load statement can be replaced by the following statements:

{code}
copyFromLocal ./test/org/apache/pig/test/data/passw ./passw
A = load './passw' using PigStorage(':');
{code}

2. The default implementation of _relativeToAbsolutePath(String location, Path curDir)_ method will move the local file (specified by location) to the HDFS and return the corresponding HDFS URI.

Any comments on these options? Especially does anyone have strong opinion against choosing option 1? 
",17/Nov/09 19:23;rding;This patch implements the option 1 of the above comment and is for load-store-redesign branch.,"17/Nov/09 21:03;rding;The test-patch run results:
      
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 23 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 333 release audit warnings (more than the trunk's current 331 warnings).",17/Nov/09 23:12;rding;This patch added apache header to the new unit test file. The remaining audit warning is html related. ,"18/Nov/09 20:25;rding;This patch is generated after applying the patch for PIG-1094. Here is the test-patch results:

     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 41 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 332 release audit warnings (more than the trunk's current 331 warnings).

The only additional audit warning is about html:

<      [java]  !????? /homes/rding/apache-pig/load-store-redesign/build/pig-0.6.0-dev/docs/jdiff/changes/org.apache.pig.ReversibleLoadStoreFunc.html","19/Nov/09 18:59;pkamath;In TestLoad.java it looks like a couple of tests have been commented since they should run in local mode.
Since TestLoad is currently working in MAPREDUCE mode and we no longer support file:<path> in MAPREDUCE mode, we
should probably change TestLoad to run each test in both local and map-red mode so we test relative to absolute
path conversion in both modes.

I think there is a jira opened against hadoop to make the method to parse file globs public - currently
the same code is in the patch - we should put a comment in the code that we should use Hadoop's method when
the jira-XXXX is fixed 

While creating a LOStore in the QueryParser, we should be calling StoreFunc's relToAbsoluteStoreLocation() method.
We should add a comment in StoreFunc interface that implementations can just use LoadFunc's static methid to implement
this method.
","19/Nov/09 22:24;rding;This patch addressed the above comments.

     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 53 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 339 release audit warnings (more than the trunk's current 338 warnings).
","19/Nov/09 23:39;pkamath;+1, patch committed on load-store-redesign branch with minor change in TestLoad to correctly set up file on MiniCluster.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig is returning too many blocks in the InputSplit,PIG-878,12430063,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,alangates,alangates,alangates,10/Jul/09 15:06,24/Mar/10 22:13,14/Mar/19 03:05,20/Jul/09 17:41,0.3.0,,,,,,0.4.0,,,,,0,,,,"When SlicerWrapper builds a slice, it currently returns the 3 locations for every block in the file it is slicing, instead of the 3 locations for the block covered by that slice.  This means Pig's odds of having its maps placed on nodes local to the data goes way down.",,,,,,,,,,,,,,,,,,,17/Jul/09 14:15;alangates;PIG-878.patch;https://issues.apache.org/jira/secure/attachment/12413820/PIG-878.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-07-17 14:58:52.593,,,no_permission,,,,,,,,,,,,164429,,,,,Fri Jul 31 18:50:31 UTC 2009,,,,,,,0|i0gpef:,95545,,,,,,,,,,17/Jul/09 14:15;alangates;Patch written collaboratively with Arun Murthy,17/Jul/09 14:58;olgan;+1,"17/Jul/09 16:44;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12413820/PIG-878.patch
  against trunk revision 794937.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/132/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/132/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/132/console

This message is automatically generated.",20/Jul/09 17:41;alangates;Checked in patch.  Thanks Arun.,"20/Jul/09 17:42;alangates;Should note also that I didn't add any tests because this was a fix for existing functionality, and frankly I'm not exactly sure how to test it.

Also should have noted thanks to Milind for first brining this to our attention.","20/Jul/09 18:31;acmurthy;bq. Should note also that I didn't add any tests because this was a fix for existing functionality, and frankly I'm not exactly sure how to test it. 

We could check the #splits returned by the slicer to ensure it's equal to the replication factor of the input files?",31/Jul/09 18:50;alangates;Port fix to 0.3 branch as well.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Push up filter does not account for added columns in foreach,PIG-877,12430003,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,sms,sms,10/Jul/09 00:37,24/Mar/10 22:13,14/Mar/19 03:05,14/Jul/09 17:42,0.4.0,,,,,,0.4.0,,,,,0,,,,"If a filter follows a foreach that produces an added column then push up filter fails with a null pointer exception.

{code}
...
x = foreach w generate $0, COUNT($1);
y = filter x by $1 > 10;
{code}

In the above example, the column in the filter's expression is an added column. As a result, the optimizer rule is not able to map it back to the input resulting in a null value. The subsequent for loop is failing due to NPE.",,,,,,,,,,,,,,,,,,,10/Jul/09 00:40;sms;PIG-877.patch;https://issues.apache.org/jira/secure/attachment/12413068/PIG-877.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-07-10 02:31:02.138,,,no_permission,,,,,,,,,,,,164428,Reviewed,,,,Tue Jul 14 17:42:25 UTC 2009,,,,,,,0|i0gpdz:,95543,,,,,,,,,,10/Jul/09 00:40;sms;Attached patch fixes the NPE.,"10/Jul/09 02:31;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12413068/PIG-877.patch
  against trunk revision 792663.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/120/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/120/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/120/console

This message is automatically generated.",10/Jul/09 17:22;sms;The unit test failures are unrelated to the fix.,13/Jul/09 21:26;alangates;Is the NPE at run time or optimization time?,13/Jul/09 21:31;sms;Its at Optimization time.,"13/Jul/09 21:35;alangates;+1, patch looks good.",13/Jul/09 23:37;sms;Patch has been committed.,"14/Jul/09 12:23;hudson;Integrated in Pig-trunk #503 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/503/])
    : Push up filter does not account for added columns in foreach
",14/Jul/09 17:42;sms;Issue has been fixed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
limit changes order of order-by to ascending,PIG-876,12429853,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,thejas,thejas,08/Jul/09 20:45,24/Mar/10 22:13,14/Mar/19 03:05,09/Jul/09 20:19,0.3.0,,,,,,0.4.0,,,,,0,,,,"grunt> a = load 'students.txt' as (c1,c2,c3,c4);
grunt> s = order a by c1 desc;
grunt> dump s;
(zxldf,M,21,12.56)
(uhsdf,M,34,12.11)
(qwer,F,21,14.44)
(qwer,F,23,145.5)
(oiue,M,54,23.33)
(asdfxc,M,23,12.44)

grunt> l = limit s 3;
grunt> dump l;
(asdfxc,M,23,12.44)
(oiue,M,54,23.33)
(qwer,F,21,14.44)
",,,,,,,,,,,,,,,,,,,09/Jul/09 04:04;daijy;PIG-876-1.patch;https://issues.apache.org/jira/secure/attachment/12412958/PIG-876-1.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-07-09 04:04:49.609,,,no_permission,,,,,,,,,,,,164427,,,,,Sun Jul 12 11:46:52 UTC 2009,,,,,,,0|i0gpdj:,95541,,,,,,,,,,09/Jul/09 04:04;daijy;Need to change sort order of limitAfterSort operator the same as the sort operator.,"09/Jul/09 06:14;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12412958/PIG-876-1.patch
  against trunk revision 792368.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/117/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/117/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/117/console

This message is automatically generated.","09/Jul/09 17:18;olgan;+ 1 on the patch.

I looked at the unit test failures and they are unrelated to the patch. We have seen this before and it was related to the test automation setup. I will ask our release engineer to take a look",09/Jul/09 20:19;daijy;Patch committed,"12/Jul/09 11:46;hudson;Integrated in Pig-trunk #501 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/501/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
POJoinPackage lose tuple in large dataset,PIG-861,12428636,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,23/Jun/09 07:46,24/Mar/10 22:13,14/Mar/19 03:05,04/Jul/09 01:39,0.3.0,,,,,,0.4.0,,impl,,,0,,,,"Some script using POJoinPackage loses records when processing large amount of input data. We do not see this problem in smaller input. We can reproduce this problem, however, the dataset for the test case is too big to be included here. We suspect that POJoinPackage causes the problem.",,,,,,,,,,,,,,,,,,,01/Jul/09 00:10;daijy;PIG-861-1.patch;https://issues.apache.org/jira/secure/attachment/12412218/PIG-861-1.patch,03/Jul/09 23:47;daijy;PIG-861-2.patch;https://issues.apache.org/jira/secure/attachment/12412527/PIG-861-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-07-01 01:03:24.4,,,no_permission,,,,,,,,,,,,164412,,,,,Sat Jul 04 11:44:17 UTC 2009,,,,,,,0|i0gp7b:,95513,,,,,,,,,,01/Jul/09 00:10;daijy;The problem is caused by a bug in BinStorage.java which erroneously interprets character \255 in the binary stream as EOF. Tested on the original queries and the patch fix the problem. No unit test is included since this patch does not introduce any new feature.,"01/Jul/09 01:03;olgan;+1, changes look good. Great catch! 

Need to make sure all tests pass before committing",02/Jul/09 18:30;daijy;Submit again for Hudson to pick up.,"03/Jul/09 16:14;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12412218/PIG-861-1.patch
  against trunk revision 790735.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/110/console

This message is automatically generated.",03/Jul/09 23:47;daijy;Resync the patch to the latest trunk.,"04/Jul/09 01:26;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12412527/PIG-861-2.patch
  against trunk revision 790735.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/113/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/113/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/113/console

This message is automatically generated.",04/Jul/09 01:39;daijy;Patch committed,"04/Jul/09 11:44;hudson;Integrated in Pig-trunk #494 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/494/])
    : POJoinPackage lose tuple in large dataset
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimizer throw error on self-joins,PIG-859,12428586,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,ashutoshc,ashutoshc,22/Jun/09 16:38,04/Aug/11 00:34,14/Mar/19 03:05,11/Feb/11 21:27,0.3.0,,,,,,0.9.0,,impl,,,0,,,,"Doing self-join results in exception thrown by Optimizer. Consider the following query
{code}
grunt> A = load 'a';
grunt> B = Join A by $0, A by $0;
grunt> explain B;

2009-06-20 15:51:38,303 [main] ERROR org.apache.pig.tools.grunt.Grunt -
ERROR 1094: Attempt to insert between two nodes that were not connected.
Details at logfile: pig_1245538027026.log
{code}

Relevant stack-trace from log-file:
{code}

Caused by: org.apache.pig.impl.plan.optimizer.OptimizerException: ERROR
2047: Internal error. Unable to introduce split operators.
        at
org.apache.pig.impl.logicalLayer.optimizer.ImplicitSplitInserter.transform(ImplicitSplitInserter.java:163)
        at
org.apache.pig.impl.logicalLayer.optimizer.LogicalOptimizer.optimize(LogicalOptimizer.java:163)
        at org.apache.pig.PigServer.compileLp(PigServer.java:844)
        at org.apache.pig.PigServer.compileLp(PigServer.java:781)
        at org.apache.pig.PigServer.getStorePlan(PigServer.java:723)
        at org.apache.pig.PigServer.explain(PigServer.java:566)
        ... 8 more
Caused by: org.apache.pig.impl.plan.PlanException: ERROR 1094: Attempt
to insert between two nodes that were not connected.
        at
org.apache.pig.impl.plan.OperatorPlan.doInsertBetween(OperatorPlan.java:500)
        at
org.apache.pig.impl.plan.OperatorPlan.insertBetween(OperatorPlan.java:480)
        at
org.apache.pig.impl.logicalLayer.optimizer.ImplicitSplitInserter.transform(ImplicitSplitInserter.java:139)
        ... 13 more
{code}


A possible workaround is:
{code}

grunt> A = load 'a';
grunt> B = load 'a';
grunt> C = join A by $0, B by $0;
grunt> explain C;
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-10-29 00:29:37.576,,,no_permission,,,,,,,,,,,,66235,,,,,Fri Feb 11 21:27:10 UTC 2011,,,,,,,0|i0gp6n:,95510,,,,,,,,,,"29/Oct/09 00:29;jing1234;if we do 
joina = join rec1 by (a), rec1 by (a) using ""merge"" ;

New error message is thrown by parser, 

Error message is :
Caused by: org.apache.pig.impl.plan.PlanValidationException: ERROR 1108: Duplicate schema alias: rec1::a in ""joina""
        at org.apache.pig.impl.logicalLayer.validators.SchemaAliasVisitor.validate(SchemaAliasVisitor.java:69)
        at org.apache.pig.impl.logicalLayer.validators.SchemaAliasVisitor.visit(SchemaAliasVisitor.java:115)
        at org.apache.pig.impl.logicalLayer.LOJoin.visit(LOJoin.java:203)
        at org.apache.pig.impl.logicalLayer.LOJoin.visit(LOJoin.java:45)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:67)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:69)
        at org.apache.pig.impl.plan.DepthFirstWalker.walk(DepthFirstWalker.java:50)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.impl.plan.PlanValidator.validateSkipCollectException(PlanValidator.java:101)
        ... 13 more
================================================================================
","29/Oct/09 00:49;daijy;I was wondering whether Pig Latin should allow self-join. In the script:

A = load 'a' as (a0, a1);
B = Join A by $0, A by $0;

The output schema for B is (A.a0, A.a1, A.a0, A.a1). It is doom to cause a schema alias conflict. ","04/Jul/10 10:11;mrflip;There are many use cases for a self join -- graph explorations especially. Would it work to say something like

{code:sql|title=bfs.pig}
-- Enumerate paths of length two
Edges = LOAD 'a' AS (src, dest);
E2Paths = Join Edges AS InLinks BY dest, Edges AS Outlinks BY src;
{code}

","04/Oct/10 21:49;hcbusy;Another work-around:

{code}
grunt> A = load 'a';
grunt> B = group A all;
grunt> C = foreach B generate FLATTEN(B.($0,$3)) as (key1, value1), FLATTEN(B.($0,$3)) as (key2,value2);
grunt> D = filter C by key1==key2;
grunt> E = foreach D generate key1 as key, value1 as left, value2 as right;
{code}",08/Nov/10 23:38;olgan;The right thing already happenning since the self join as-is would produce two columns with the same name. Second load is needed for self-join to work,"09/Nov/10 01:50;viraj;Hi Olga,
 According to the use case of dfs.pig, we need to support this syntax. It would help the user to avoid having to write 2 load statements, which is non-intuitive.
 If you believe that this is not required we need to document this behavior that the self-join requires 2 load statements.
Regards
Viraj","09/Nov/10 01:52;ciemo;An alternative workaround is:

{code}
A = load ...;
A1 = foreach A generate *;
AA1 = join A by ..., A1 by ...;
{code}

If Pig supported re-aliasing, then we could do:

{code}
A = load ...;
A1 = A;
AA1 = join A by ..., A1 by ...;
{code}","09/Nov/10 02:30;olgan;Viraj, as I pointed out, we can't support this case because it is ambiguous. We are planning to support the re-aliasing as suggested by Ciemo to avoid name conflict. I will re-assign this to Corinne for documentation purpose","09/Nov/10 17:17;alangates;We should also be considering this from a performance angle.  Since we can detect that this is a self join we should only be reading the table once, not twice.  This should be true whether it is done via A1 = A or a second load.",11/Feb/11 21:27;chandec;Docs updated. Self-Joins section added to JOIN(inner). Patch will be submitted under Pig-1772.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Order By followed by ""replicated"" join fails while compiling MR-plan from physical plan",PIG-858,12428390,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ashutoshc,ashutoshc,ashutoshc,19/Jun/09 18:20,24/Mar/10 22:15,14/Mar/19 03:05,16/Oct/09 19:21,0.4.0,,,,,,0.6.0,,impl,,,0,,,,"Consider the query:
{code}
A = load 'a';
B = order A by $0;
C = join A by $0, B by $0;
explain C;
{code}
works. But if replicated join is used instead
{code}
A = load 'a';
B = order A by $0;
C = join A by $0, B by $0 using ""replicated"";
explain C;
{code}
this fails with ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2034: Error compiling operator POFRJoin
relevant stacktrace:
{code}
Caused by: java.lang.RuntimeException: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompilerException: ERROR 2034: Error compiling operator POFRJoin
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.explain(HExecutionEngine.java:306)
        at org.apache.pig.PigServer.explain(PigServer.java:574)
        ... 8 more
Caused by: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompilerException: ERROR 2034: Error compiling operator POFRJoin
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.visitFRJoin(MRCompiler.java:942)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin.visit(POFRJoin.java:173)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.compile(MRCompiler.java:342)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.compile(MRCompiler.java:327)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.compile(MRCompiler.java:233)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.compile(MapReduceLauncher.java:301)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.explain(MapReduceLauncher.java:278)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.explain(HExecutionEngine.java:303)
        ... 9 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.visitFRJoin(MRCompiler.java:901)
        ... 16 more
{code}",,,,,,,,,,,,,,,,,,,15/Sep/09 00:33;ashutoshc;pig-858.patch;https://issues.apache.org/jira/secure/attachment/12419592/pig-858.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-09-28 03:04:17.325,,,no_permission,,,,,,,,,,,,164410,,,,,Fri Oct 16 19:21:57 UTC 2009,,,,,,,0|i0gp6f:,95509,,,,,,,,,,"19/Jul/09 19:51;ashutoshc;While POFRJoin is getting compiled in MRCompiler, it needs to identify for each of its 
predecessor in physical plan of which compiled MROperator they are part of. Currently, it is
assumed to be one of the compiledInputs(an array of MRoper which are immediate predecessor of current MROper in MROper DAG). 
Mostly this is true, but in cases where one physical operator results in two or more MR operator, this may not be true, as is the
case here. When there is an order-by before FRJoin; one of the inputs of POFRJoin will be
POSort, but POSort operator will be in the first MROper of the two generated MROperator
and thus will not be found in compiledInputs (which contains second MROper). Thus,
current way of identifying corresponding MRoper of a physical operator is unreliable.
This bug also affects the implementation of merge-sort join 
https://issues.apache.org/jira/browse/PIG-845 . Since POMergeJoin needs to know which MROper
corresponds to its left input and which one corresponds to its right. It can do so by looking
into compiledInputs as long as there is no order-by (or similiar PO which results in
multiple MROper) as its predecessors. Doing order-by before using merge
join is however a natural use-case there.

Proposal is to introduce a new private member variable in MRCompiler phyToMROperMap 
(similiar to logToPhyMap) using which leaf MROper for a given
physical operator can be identified. Thoughts?
  ","15/Sep/09 00:33;ashutoshc;Patch as discussed in previous comment. Also included are test cases, where blocking operator (order-by, distinct) occurs before FRjoin.","28/Sep/09 03:04;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419592/pig-858.patch
  against trunk revision 819057.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/47/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/47/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/47/console

This message is automatically generated.",14/Oct/09 00:26;alangates;I'm reviewing this patch.,14/Oct/09 15:56;alangates;Mostly looks straight forward and passes all the tests.  You made a number of changes in MRCompiler.visitUnion.  I don't understand what exactly you were changing there.  Could you give a brief overview of those changes?,"14/Oct/09 20:08;ashutoshc;visitUnion has same changes as others visit functions, that is it adds MR Operator corresponding to POUnion in phyToMROpMap map. Real changes are in visitFRJoin. Earlier in visitFRJoin, it used to look in compiledInputs array of MROper one by one trying to match MROPer leaf PO with POFRJoin using operator key. Now, it doesn't need to do that it can simply lookup in the phyToMROpMap.","14/Oct/09 20:29;ashutoshc;Its been a while since I did that patch. So, bit more clarification: We are interested in finding PO which corresponds to ""fragment"" PO input of POFRJoin. This PO is already compiled and is in one the MROper. Earlier we  will iterate through compiledInputs array trying to match this PO  with PO contained in each MROperator. This fails as discussed in previous comments. With this change, since we keep track of MR operator with each physical operator it need not to do that but can simply look up for MROper corresponding to ""fragment"" PO in the phyToMROpMap.",16/Oct/09 19:21;alangates;Fix checked in.  Thanks Ashutosh for the patch and explanation.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig -version or pig -help returns exit code of 1,PIG-852,12427963,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,milindb,milindb,milindb,16/Jun/09 04:24,24/Mar/10 22:13,14/Mar/19 03:05,16/Jun/09 20:24,0.3.0,,,,,,0.4.0,,grunt,,,0,,,,"{code}
java -jar pig.jar -x local [-version|-help]
{code}

returns an exit code of 1 to the shell.",All,,,,,,,,,,,,,,,,,,16/Jun/09 04:24;milindb;rc.patch;https://issues.apache.org/jira/secure/attachment/12410752/rc.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-06-16 06:02:32.585,,,no_permission,,,,,,,,,,,,164404,,,,,Wed Jun 17 05:56:50 UTC 2009,,,,,,,0|i0gp3z:,95498,,,,,,,,,,16/Jun/09 04:26;milindb;Making patch available. Manual testing done.,"16/Jun/09 06:02;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12410752/rc.patch
  against trunk revision 784333.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/83/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/83/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/83/console

This message is automatically generated.","16/Jun/09 06:21;milindb;Note that JUnit tests that test the return code of a completely different JVM are kludgy at best. Therefore writing a test case of checking the System.exit() return value is insane. Hadoop folks have *mostly* fixed this issue by having a static public run() method that can be invoked directly in the tests, and can have it's return value (which is what main() uses as an exit code) checked.

So, committers, please ignore the ""no-tests"" warning.","16/Jun/09 20:24;olgan;Patch committed. Thanks, Milind for contributing.","17/Jun/09 05:56;hudson;Integrated in Pig-trunk #476 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/476/])
    : pig -version or pig -help returns exit code of 1 (milindb via
    olgan)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Map type used as return type in UDFs not recognized at all times,PIG-851,12427956,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,zjffdu,sms,sms,16/Jun/09 01:15,24/Mar/10 22:13,14/Mar/19 03:05,14/Jul/09 03:15,0.3.0,,,,,,0.4.0,,impl,,,1,,,,"When an UDF returns a map and the outputSchema method is not overridden, Pig does not figure out the data type. As a result, the type is set to unknown resulting in run time failure. An example script and UDF follow

{code}
public class mapUDF extends EvalFunc<Map<Object, Object>> {

    @Override
    public Map<Object, Object> exec(Tuple input) throws IOException {
            return new HashMap<Object, Object>();
    }

//Note that the outputSchema method is commented out

/*
    @Override
    public Schema outputSchema(Schema input) {
        try {
            return new Schema(new Schema.FieldSchema(null, null, DataType.MAP));
        } catch (FrontendException e) {
            return null;
        }
    }
*/
{code}

{code}
grunt> a = load 'student_tab.data';           
grunt> b = foreach a generate EXPLODE(1);
grunt> describe b;

b: {Unknown}

grunt> dump b;

2009-06-15 17:59:01,776 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Failed!

2009-06-15 17:59:01,781 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2080: Foreach currently does not handle type Unknown

{code}",,,,,,,,,,,,,,,,,,,24/Jun/09 15:16;zjffdu;Pig_851_patch.txt;https://issues.apache.org/jira/secure/attachment/12411655/Pig_851_patch.txt,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-06-16 14:13:23.773,,,no_permission,,,,,,,,,,,,164403,,,,,Sun Jul 12 11:46:52 UTC 2009,,,Patch Available,,,,0|i0gp3j:,95496,,,,,,,,,,16/Jun/09 01:16;sms;A workaround for this issue is to override the outputSchema method and return the appropriate schema.,"16/Jun/09 16:48;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12410810/Pig_815_patch.txt
  against trunk revision 784333.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 7 new or modified tests.

    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.

    -1 javac.  The applied patch generated 227 javac compiler warnings (more than the trunk's current 224 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 162 release audit warnings (more than the trunk's current 160 warnings).

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/85/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/85/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/85/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/85/console

This message is automatically generated.","16/Jun/09 18:46;sms;Review comments:

1. The new sources test/org/apache/pig/test/utils/MyUDFReturnMap.java and test/org/apache/pig/test/TestUDFReturnMap.java need to include the Apache license headers
2. The use of package sun.reflect.generics.reflectiveObjects.ParameterizedTypeImpl is resulting in 3 compiler warnings and 1 javadoc warning. Can we use a different package?
3. The test case in TestUDFReturnMap runs the test in local mode (i.e., ExecType.LOCAL). Another test for map reduce mode, ExecType.MAPREDUCE, should be added.",22/Jun/09 18:18;sms;I assigned the patch to myself in order to submit the patch for automated testing.,22/Jun/09 18:28;sms;Assigning the jira to Jeff Zhang,"24/Jun/09 09:15;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12411309/patch_815.txt
  against trunk revision 787908.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 7 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/98/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/98/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/98/console

This message is automatically generated.","24/Jun/09 15:16;zjffdu;Update the patch.
1. Should delete tmp files in MiniCluster mode, invoke this method : FileLocalizer.deleteTempFiles();
2. Change the TestCase to include the Jira number, so it's easy for developers to track the issue
","02/Jul/09 15:42;zjffdu;The status of this issue is patch available, how can I submit the patch to hudson ?

","03/Jul/09 11:37;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12411655/Pig_851_patch.txt
  against trunk revision 790735.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 7 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/108/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/108/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/108/console

This message is automatically generated.",09/Jul/09 01:14;sms;Patch has been committed. Thanks for fixing this issue Jeff.,"12/Jul/09 11:46;hudson;Integrated in Pig-trunk #501 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/501/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Dump produce wrong result while ""store into"" is ok",PIG-850,12427937,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,15/Jun/09 21:20,24/Mar/10 22:13,14/Mar/19 03:05,17/Jun/09 01:06,0.2.0,,,,,,0.4.0,,impl,,,0,,,,"The following script will wrongly produce 20 output, however, if we change dump to ""store into"", the result is correct. Not sure if the problem is only for limited sort case.

A = load '/user/pig/tests/data/singlefile/studenttab10k' as (name, age, gpa);
B = order A by gpa parallel 2;
C = limit B 10;
dump C;
",,,,,,,,,,,,,,,,,,,16/Jun/09 20:54;daijy;PIG-850.patch;https://issues.apache.org/jira/secure/attachment/12410851/PIG-850.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-06-16 22:52:32.139,,,no_permission,,,,,,,,,,,,164402,,,,,Wed Jun 17 01:06:35 UTC 2009,,,,,,,0|i0gp33:,95494,,,,,,,,,,"16/Jun/09 20:54;daijy;When we add extra limit map-reduce operator (see [PIG-364|http://issues.apache.org/jira/browse/PIG-364]), we should mark the output file in the original map-reduce as temporary; Otherwise, dump will pick the wrong output file.","16/Jun/09 22:52;olgan;+1; looks good, please, commit once out qa is done","16/Jun/09 23:41;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12410851/PIG-850.patch
  against trunk revision 785378.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/88/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/88/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/88/console

This message is automatically generated.",17/Jun/09 01:06;daijy;Patch submitted,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Local engine loses records in splits,PIG-849,12427858,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,hagleitn,hagleitn,hagleitn,15/Jun/09 07:43,24/Mar/10 22:13,14/Mar/19 03:05,16/Jun/09 20:29,0.2.0,,,,,,0.4.0,,,,,0,,,,"When there is a split in the physical plan records can be dropped in certain circumstances.

The local split operator puts all records in a databag and turns over iterators to the POSplitOutput operators. The problem is that the local split also adds STATUS_NULL records to the bag. That will cause the databag's iterator to prematurely return false on the hasNext call (so a STATUS_NULL becomes a STATUS_EOP in the split output operators).
",,,,,,,,,,,,,,,,,,,16/Jun/09 07:51;hagleitn;local_engine.patch;https://issues.apache.org/jira/secure/attachment/12410762/local_engine.patch,15/Jun/09 07:44;hagleitn;local_engine.patch;https://issues.apache.org/jira/secure/attachment/12410627/local_engine.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-06-15 16:05:04.385,,,no_permission,,,,,,,,,,,,164401,,,,,Wed Jun 17 05:56:50 UTC 2009,,,,,,,0|i0gp2n:,95492,,,,,,,,,,"15/Jun/09 16:05;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12410627/local_engine.patch
  against trunk revision 784333.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/81/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/81/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/81/console

This message is automatically generated.",15/Jun/09 21:27;olgan;I rerun the tests manually and they all passed. Looks like an issue with automated patch testing as we so another case of it this morning.,16/Jun/09 00:53;olgan;new patch will be submitted with unit tests,16/Jun/09 07:53;hagleitn;the new patch has a unit test - otherwise it's the same,"16/Jun/09 09:40;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12410762/local_engine.patch
  against trunk revision 784333.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/84/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/84/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/84/console

This message is automatically generated.",16/Jun/09 19:24;hagleitn;Same errors as before. Ran manually and passed. The issue with the automated patch testing seems to be still there.,16/Jun/09 20:29;olgan;patch committed. Thanks Gunther for contributing.,"17/Jun/09 05:56;hudson;Integrated in Pig-trunk #476 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/476/])
    : Local engine loses records in splits (hagleitn via olgan)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MultiQuery optimization in some cases has an issue when there is a split in the map plan ,PIG-846,12427789,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,12/Jun/09 20:53,24/Mar/10 22:11,14/Mar/19 03:05,13/Jun/09 05:29,0.3.0,,,,,,0.3.0,,,,,0,,,,"The following script produces the error that follows:
{noformat}

A = LOAD 'input.txt' as (f0, f1, f2, f3, f4, f5, f6, f7, f8); 
B = FOREACH A GENERATE f0, f1, f2, f3, f4;
B1 = foreach B generate f0, f1, f2;
C = GROUP B1 BY (f1, f2);
STORE C into 'foo1';

B2 = FOREACH B GENERATE f0, f3, f4;
E = GROUP B2 BY (f3, f4);
STORE E into 'foo2';

F = FOREACH A GENERATE f0, f5, f6, f7, f8;
F1 = FOREACH F GENERATE f0, f5,f6;
G = GROUP F1 BY (f5, f6);
STORE G into 'foo3';

F2  = FOREACH F GENERATE f0, f7, f8;
I = GROUP F2 BY (f7, f8);
STORE I into 'foo4';
{noformat}

Exception encountered during execution:
{noformat}
java.lang.NullPointerException
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage.getValueTuple(POPackage.java:262)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage.getNext(POPackage.java:209)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMultiQueryPackage.getNext(POMultiQueryPackage.java:186)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMultiQueryPackage.getNext(POMultiQueryPackage.java:186)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.processOnePackageOutput(PigMapReduce.java:277)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:268)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:142)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:318)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2207)

{noformat}",,,,,,,,,,,,,,,,,,,13/Jun/09 01:35;pkamath;PIG-846-v2.patch;https://issues.apache.org/jira/secure/attachment/12410528/PIG-846-v2.patch,12/Jun/09 22:12;pkamath;PIG-846.patch;https://issues.apache.org/jira/secure/attachment/12410519/PIG-846.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-06-13 01:47:48.698,,,no_permission,,,,,,,,,,,,164399,Reviewed,,,,Sat Jun 13 05:29:28 UTC 2009,,,,,,,0|i0gp13:,95485,,,,,,,,,,"12/Jun/09 22:22;pkamath;The root cause is that MultiQueryOptimizer as part of merging MROpers updates the keyInfo Hashmap present in POPackage to reflect the new ""index"" (originally the index is 0 since multiquery optimization only happens with single input MROpers). MultiQueryOptimization uses indices in the ranges 0x80 to 0x8f by ORing the index with a multiQuery bitmask (0x80). These indices are assigned by flattenning out all POLocalRearranges across all merged map plans (including those nested in any POSplit opreator in a map plan). Now the corresponding POPackages need to have their keyInfo HashMaps updated to reflect these new indexes. MultiQueryOptimizer does this update for POPackage operators that occur in the top level POMultiQueryPackage. However if merging of MROpers happens recursively, we could have POMultiQueryPackage operators in the list of packages maintained by the topLevel POMultiQueryPackage. These were not getting updated.

The fix updates (recursively) the packages in any POMultiQueryPackage present in the top level POMultiQueryPackage.",12/Jun/09 22:52;pkamath;Will be resubmitting a new patch - just realized that a few unit tests are broken,"13/Jun/09 01:35;pkamath;New patch - the only change is to not add extra information in POLocalRearrange.name() - was in the earlier patch only to add more information in explain outputs but this breaks some unit tests.

TestHBaseStorage unit test still fails for me but the failure is not related to the changes in the patch - am assuming that is an environment issue on my machine.",13/Jun/09 01:47;olgan;Walked through the changes with Pradeep. The patch looks good. +1.,"13/Jun/09 04:11;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12410528/PIG-846-v2.patch
  against trunk revision 783955.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 18 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/79/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/79/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/79/console

This message is automatically generated.",13/Jun/09 05:29;pkamath;Patch committed to trunk and branch-0.3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incorrect return codes on failure when using -f or -e flags,PIG-839,12427314,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,hagleitn,hagleitn,hagleitn,08/Jun/09 03:41,24/Mar/10 22:11,14/Mar/19 03:05,09/Jun/09 00:34,,,,,,,0.3.0,,,,,0,,,,"To repro: pig -e ""a = load '<some file>' ; b = stream a through \`false\` ; store b into '<some file>';""

Both the -e and -f flags do not return the right code upon exit. Running the script w/o using -f works fine.",,,,,,,,,,,,,,,,,,,08/Jun/09 03:42;hagleitn;fix_return_code.patch;https://issues.apache.org/jira/secure/attachment/12410099/fix_return_code.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-06-08 15:25:50.635,,,no_permission,,,,,,,,,,,,164392,Reviewed,,,,Tue Jun 09 11:48:49 UTC 2009,,,,,,,0|i0goxz:,95471,,,,,,,,,,08/Jun/09 15:25;sms;+1 for the review.,08/Jun/09 16:28;gkesavan;resubmitting the patch,"08/Jun/09 19:35;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12410099/fix_return_code.patch
  against trunk revision 782703.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/74/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/74/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/74/console

This message is automatically generated.",08/Jun/09 21:28;sms;Patch has been committed to trunk. Will port it to the 0.3 branch shortly. Thanks for fixing the issue Gunther.,"08/Jun/09 21:54;sms;There are no unit test cases as the testing was performed manually. Pasting a test run below.

{code}

$ cat errcode.pig 
a = load '/user/sms/data/student_tab.data' ;
b = stream a through `false` ;
store b into '/user/sms/data/errcode.out'; 

#Before fix
$ java -cp pig.jar:/home/y/conf/pig/piglet/released org.apache.pig.Main -f errcode.pig 
2009-06-08 14:40:51,917 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Failed!
2009-06-08 14:40:51,926 [main] ERROR org.apache.pig.tools.grunt.GruntParser - ERROR 2055: Received Error while processing the map plan: 'false ' failed with exit status: 1
Details at logfile: pig_1244497222536.log
afterside 14:40:53 ~/src_pig/pig/trunk_optimizer_phase3_part2 $ echo $?
0

#After fix
$ java -cp pig.jar:/home/y/conf/pig/piglet/released org.apache.pig.Main -f errcode.pig 
2009-06-08 14:42:20,422 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Failed!
2009-06-08 14:42:20,434 [main] ERROR org.apache.pig.tools.grunt.GruntParser - ERROR 2055: Received Error while processing the map plan: 'false ' failed with exit status: 1
Details at logfile: /homes/sms/src_commit/pig/trunk/pig_1244497306578.log
afterside 14:42:21 ~/src_commit/pig/trunk $ echo $?
2

{code}",09/Jun/09 00:34;sms;Patch has been applied and committed to branch-0.3,09/Jun/09 00:34;sms;Issue has been fixed.,"09/Jun/09 11:48;hudson;Integrated in Pig-trunk #468 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/468/])
    : incorrect return codes on failure when using -f or -e flags (hagletin via sms)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
docs ant target is broken ,PIG-837,12427174,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,gkesavan,gkesavan,05/Jun/09 05:38,25/Mar/10 00:12,14/Mar/19 03:05,05/Jun/09 18:21,,,,,,,0.3.0,,,,,0,,,,"docs ant target is broken , this would fail the trunk builds..

 [exec] Java Result: 1
     [exec] 
     [exec]   Copying broken links file to site root.
     [exec]       
     [exec] Copying 1 file to /home/hudson/hudson-slave/workspace/Pig-Patch-minerva.apache.org/trunk/src/docs/build/site
     [exec] 
     [exec] BUILD FAILED
     [exec] /home/nigel/tools/forrest/latest/main/targets/site.xml:180: Error building site.
     [exec]         
     [exec] There appears to be a problem with your site build.
     [exec] 
     [exec] Read the output above:
     [exec] * Cocoon will report the status of each document:
     [exec]     - in column 1: *=okay X=brokenLink ^=pageSkipped (see FAQ).
     [exec] * Even if only one link is broken, you will still get ""failed"".
     [exec] * Your site would still be generated, but some pages would be broken.
     [exec]   - See /home/hudson/hudson-slave/workspace/Pig-Patch-minerva.apache.org/trunk/src/docs/build/site/broken-links.xml
     [exec] 
     [exec] Total time: 28 seconds
BUILD FAILED
/home/hudson/hudson-slave/workspace/Pig-Patch-minerva.apache.org/trunk/build.xml:326: exec returned: 1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-06-05 18:21:54.377,,,no_permission,,,,,,,,,,,,164390,,,,,Fri Jun 05 18:21:54 UTC 2009,,,,,,,0|i0gox3:,95467,,,,,,,,,,05/Jun/09 18:21;olgan;committed missing file,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiquery optimization does not handle the case where the map keys in the split plans have different key types (tuple and non tuple key type),PIG-835,12427165,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,05/Jun/09 00:08,24/Mar/10 22:11,14/Mar/19 03:05,09/Jun/09 22:17,0.3.0,,,,,,0.3.0,,,,,1,,,,"A query like the following results in an exception on execution:
{noformat}
a = load 'mult.input' as (name, age, gpa);
b = group a ALL;
c = foreach b generate group, COUNT(a);
store c into 'foo';
d = group a by (name, gpa);
e = foreach d generate flatten(group), MIN(a.age);
store e into 'bar';
{noformat}

Exception on execution:
09/06/04 16:56:11 INFO mapred.TaskInProgress: Error from attempt_200906041655_0001_r_000000_3: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.pig.data.Tuple
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:312)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:254)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:204)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:231)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore.getNext(POStore.java:117)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODemux.runPipeline(PODemux.java:248)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODemux.getNext(PODemux.java:238)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.runPipeline(PigMapReduce.java:320)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.processOnePackageOutput(PigMapReduce.java:288)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:268)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:142)
    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:318)
    at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2207)

",,,,,,,,,,,,,,,,,,,08/Jun/09 22:40;pkamath;PIG-835-v2.patch;https://issues.apache.org/jira/secure/attachment/12410186/PIG-835-v2.patch,05/Jun/09 18:08;pkamath;PIG-835.patch;https://issues.apache.org/jira/secure/attachment/12410010/PIG-835.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-06-08 16:29:58.919,,,no_permission,,,,,,,,,,,,164388,Reviewed,,,,Wed Jun 10 11:49:18 UTC 2009,,,,,,,0|i0gow7:,95463,,,,,,,,,,"05/Jun/09 18:08;pkamath;The root cause of the issue is that the current multiQueryOptimizer checks if the map key is of the same type for different map plans it merges. If they are of different types, it ensures that the type is made tuple for all map plans - this implies keys which are not tuples will be wrapped in an extra tuple and keys which are already of Tuple type will be left alone (this is ensured in POLocalRearrange). However the Demux operator which passes the key and bag of values to the merged reduce plan currently always unwraps the tuple whenever the map keys are different. This results in unwrapping of keys which were originally tuples and should not be unwrapped. 

The attached patch fixes this by storing an array of boolean flags in the Demux operator to indicates which map keys are wrapped and which are not so that unwrapping occurs only in cases where the original map key was not already a tuple and was wrapped.",08/Jun/09 16:29;gkesavan;resubmitting the patch,"08/Jun/09 22:00;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12410010/PIG-835.patch
  against trunk revision 782703.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 2 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/75/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/75/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/75/console

This message is automatically generated.",08/Jun/09 22:40;pkamath;New patch with findbugs warnings addressed - essentially findbugs wanted the public static members in PigNUllableWritable to be marked final.,"09/Jun/09 01:20;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12410186/PIG-835-v2.patch
  against trunk revision 782790.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/77/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/77/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/77/console

This message is automatically generated.","09/Jun/09 20:59;olgan;+1, the patch looks good.",09/Jun/09 22:17;pkamath;Patch commited to both trunk and branch-0.3,"10/Jun/09 11:49;hudson;Integrated in Pig-trunk #469 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/469/])
    added entry in CHANGES.txt into 0.3 branch section for  since  has been committed to the 0.3 branch also
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incorrect plan when algebraic functions are nested,PIG-834,12427152,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ashutoshc,thejas,thejas,04/Jun/09 21:39,14/May/10 06:45,14/Mar/19 03:05,11/Feb/10 20:17,,,,,,,0.7.0,,impl,,,0,,,,"a = load 'students.txt' as (c1,c2,c3,c4); 
c = group a by c2;  
f = foreach c generate COUNT(org.apache.pig.builtin.Distinct($1.$2));

Notice that Distinct udf is missing in Combiner and reduce stage. As a result distinct does not function, and incorrect results are produced.
Distinct should have been evaluated in the 3 stages and output of Distinct should be given to COUNT in reduce stage.

{code}
# Map Reduce Plan                                  
#--------------------------------------------------
MapReduce node 1-122
Map Plan
Local Rearrange[tuple]{bytearray}(false) - 1-139
|   |
|   Project[bytearray][1] - 1-140
|
|---New For Each(false,false)[bag] - 1-127
    |   |
    |   POUserFunc(org.apache.pig.builtin.COUNT$Initial)[tuple] - 1-125
    |   |
    |   |---POUserFunc(org.apache.pig.builtin.Distinct)[bag] - 1-126
    |       |
    |       |---Project[bag][2] - 1-123
    |           |
    |           |---Project[bag][1] - 1-124
    |   |
    |   Project[bytearray][0] - 1-133
    |
    |---Pre Combiner Local Rearrange[tuple]{Unknown} - 1-141
        |
        |---Load(hdfs://wilbur11.labs.corp.sp1.yahoo.com/user/tejas/students.txt:org.apache.pig.builtin.PigStorage) - 1-111--------
Combine Plan
Local Rearrange[tuple]{bytearray}(false) - 1-143
|   |
|   Project[bytearray][1] - 1-144
|
|---New For Each(false,false)[bag] - 1-132
    |   |
    |   POUserFunc(org.apache.pig.builtin.COUNT$Intermediate)[tuple] - 1-130
    |   |
    |   |---Project[bag][0] - 1-135
    |   |
    |   Project[bytearray][1] - 1-134
    |
    |---POCombinerPackage[tuple]{bytearray} - 1-137--------
Reduce Plan
Store(fakefile:org.apache.pig.builtin.PigStorage) - 1-121
|
|---New For Each(false)[bag] - 1-120
    |   |
    |   POUserFunc(org.apache.pig.builtin.COUNT$Final)[long] - 1-119
    |   |
    |   |---Project[bag][0] - 1-136
    |
    |---POCombinerPackage[tuple]{bytearray} - 1-145--------
Global sort: false
{code}",,,,,,,,,,,,,,,,,,,05/Feb/10 03:06;ashutoshc;pig-834.patch;https://issues.apache.org/jira/secure/attachment/12434920/pig-834.patch,05/Feb/10 22:59;ashutoshc;pig-834_2.patch;https://issues.apache.org/jira/secure/attachment/12435027/pig-834_2.patch,10/Feb/10 21:33;ashutoshc;pig-834_3.patch;https://issues.apache.org/jira/secure/attachment/12435493/pig-834_3.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-01-28 00:17:50.634,,,no_permission,,,,,,,,,,,,164387,,,,,Thu Feb 11 20:17:53 UTC 2010,,,,,,,0|i0govr:,95461,,,,,,,,,,28/Jan/10 00:17;olgan;The short term solution will be to catch this case and not enable combiner,"05/Feb/10 03:06;ashutoshc;In this patch, I look for a pattern of POUserFunc followed by another POUserFunc in the inner plan of ForEach and if thats found I flag the combiner optimizer to not fire. This disables the combiner for this particular query (test case included). Wondering if this fix is sufficient for this bug ?","05/Feb/10 22:59;ashutoshc;Correct approach is following: If leaf of inner plan of ForEach is not combinable then we dont put combiner in any case. If it is, there should not be any other combinable POUserFunc in the ForEach's inner plan. First check already exists in trunk. This patch checks for this second conditon and makes sure not to fire combiner if there is any other combinable POUserFunc in the ForEach inner plan apart from leaf POUserFunc.",09/Feb/10 07:17;ashutoshc;Trying to get hudson going on this.,"09/Feb/10 11:36;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12435027/pig-834_2.patch
  against trunk revision 907760.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/195/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/195/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/195/console

This message is automatically generated.",09/Feb/10 16:45;ashutoshc;Another hudson quirk : ( Failed test passes successfully on local machine. Patch is ready for review.,"10/Feb/10 21:33;ashutoshc;Instead of having recursive function walking on plan, better to have a visitor doing that. So, this patch replaces that function with a visitor.","11/Feb/10 02:00;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12435493/pig-834_3.patch
  against trunk revision 908324.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/208/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/208/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/208/console

This message is automatically generated.",11/Feb/10 18:26;rding;+1 for commit.,11/Feb/10 20:17;ashutoshc;Patch checked-in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Records and bytes written reported by pig are wrong in a multi-store program,PIG-831,12426966,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,alangates,alangates,alangates,02/Jun/09 20:20,24/Mar/10 22:13,14/Mar/19 03:05,15/Sep/09 16:55,0.3.0,,,,,,0.4.0,,impl,,,0,,,,The stats features checked in as part of PIG-626 (reporting the number of records and bytes written at the end of the query) print wrong values (often but not always 0) when the pig script being run contains more than 1 store.,,,,,,,,,,,,,,,,,,,03/Jun/09 14:56;alangates;PIG-831.patch;https://issues.apache.org/jira/secure/attachment/12409777/PIG-831.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-06-04 20:33:12.813,,,no_permission,,,,,,,,,,,,164384,,,,,Tue Sep 15 16:55:37 UTC 2009,,,,,,,0|i0gouf:,95455,,,,,,,,,,"02/Jun/09 20:22;alangates;There are a couple of issues going on here.

One, PigStats looks through the plan until it finds the first root and then stops.  So for multi-store scripts that have multiple roots in their plans, this does not work.

Two, Hadoop does not return accurate numbers for records written in many cases.  I do not know if this is a bug in hadoop or a bug in the output format pig uses when doing multiple stores in one job.",03/Jun/09 14:56;alangates;This patch addresses the two problems listed above.  It changes the stats patch to collect all root MR jobs instead of just the first it encounters.  The second issue (that MR returns bogus results for multi-store scripts) is addressed by having pig not report records written in this case.,"04/Jun/09 20:33;olgan;+1 on the patch. please, keep the bug open since we should at some point correctly report numbers for multiquery
","06/Jun/09 11:40;hudson;Integrated in Pig-trunk #465 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/465/])
    : Turned off reporting of records and bytes written for mutli-store
queries as the returned results are confusing and wrong.
",15/Sep/09 16:55;alangates;Fix checked in 6 June 2009,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"DECLARE statement stop processing after special characters such as dot ""."" , ""+"" ""%"" etc..",PIG-829,12426869,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,viraj,viraj,02/Jun/09 01:31,14/May/10 06:45,14/Mar/19 03:05,15/Apr/10 00:28,0.3.0,,,,,,0.7.0,,grunt,,,0,,,,"The below Pig script does not work well, when special characters are used in the DECLARE statement.
{code}
%DECLARE OUT foo.bar

x = LOAD 'something' as (a:chararray, b:chararray);

y = FILTER x BY ( a MATCHES '^.*yahoo.*$' );

STORE y INTO '$OUT';
{code}

When the above script is run in the dry run mode; the substituted file does not contain the special character.

{code}
java -cp pig.jar:/homes/viraj/hadoop-0.18.0-dev/conf -Dhod.server='' org.apache.pig.Main -r declaresp.pig
{code}

Resulting file: ""declaresp.pig.substituted""
{code}
x = LOAD 'something' as (a:chararray, b:chararray);

y = FILTER x BY ( a MATCHES '^.*yahoo.*$' );

STORE y INTO 'foo';
{code}",,,,,,,,,,PIG-564,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,164383,,,,,Thu Apr 15 00:28:29 UTC 2010,,,,,,,0|i0gotj:,95451,,,,,,,,,,02/Jun/09 01:34;viraj;Linking all parameter substitution bugs to one Jira,"15/Apr/10 00:28;viraj;Pig 0.7 yields the correct result.
{code}
x = LOAD 'something' as (a:chararray, b:chararray);
y = FILTER x BY ( a MATCHES '^.*yahoo.*$' );
STORE y INTO 'foo.bar';
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PIG_HADOOP_VERSION should be 18,PIG-825,12426712,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,dvryaboy,dvryaboy,dvryaboy,29/May/09 20:49,24/Mar/10 22:11,14/Mar/19 03:05,01/Jun/09 16:51,,,,,,,0.3.0,,grunt,,,0,,,,"PIG_HADOOP_VERSION should be set to 18, not 17, as Hadoop 0.18 is now considered default.
Patch coming.",,,,,,,,,,,,,,,,,,,29/May/09 22:17;dvryaboy;pig-825.patch;https://issues.apache.org/jira/secure/attachment/12409427/pig-825.patch,29/May/09 20:54;dvryaboy;pig-825.patch;https://issues.apache.org/jira/secure/attachment/12409418/pig-825.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-05-29 22:02:09.973,,,no_permission,,,,,,,,,,,,41734,,,,,Tue Jun 02 04:40:47 UTC 2009,,,,,,,0|i0gorr:,95443,,,,,,,,,,"29/May/09 20:54;dvryaboy;Attached trivial patch, please review.",29/May/09 22:02;alangates;I'll take a look at this patch.,29/May/09 22:17;dvryaboy;Minor update to minor patch --fixed a typo in the bug number in CHANGES.txt,01/Jun/09 16:51;alangates;Patch checked in.  Thanks Dmitriy.,"02/Jun/09 04:40;hudson;Integrated in Pig-trunk #460 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/460/])
    : PIG_HADOOP_VERSION should be set to 18.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flatten semantics are unknown,PIG-822,12426642,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,breed,gmavromatis,gmavromatis,29/May/09 02:05,25/Mar/10 00:12,14/Mar/19 03:05,23/Sep/09 00:21,,,,,,,0.4.0,,documentation,,,0,,,,"There is no formal specification of the flatten keyword in http://hadoop.apache.org/pig/docs/r0.2.0/piglatin.html 
There are only some examples.

I have found flatten to be very fragile and unpredictable with the data types it reads and creates. 

Please document:
Flatten to be explained formally in its own dedicated section: What are the valid input types, the output types it creates, what transformation it does from input to output and how the resulting data are named.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-09-23 00:21:51.695,,,no_permission,,,,,,,,,,,,164378,,,,,Wed Sep 23 00:21:51 UTC 2009,,,,,,,0|i0goqn:,95438,,,,,,,,,,23/Sep/09 00:21;olgan;ben updated the docs and they have been committed. Will be coming out as part of Pig 0.4.0 release,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
run -param -param; is a valid grunt command,PIG-819,12426397,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,milindb,milindb,milindb,26/May/09 22:02,24/Mar/10 22:11,14/Mar/19 03:05,28/May/09 22:24,0.3.0,,,,,,0.3.0,,grunt,,,0,,,,"By mistake, I typed 

{code}
run -param -param;
{code}

in grunt. And was surprised to find it to be  a valid grunt command.",all,,,,,,,,,,,,,,,,,,26/May/09 22:42;milindb;invalidparam.patch;https://issues.apache.org/jira/secure/attachment/12409094/invalidparam.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-05-27 00:45:28.502,,,no_permission,,,,,,,,,,,,164375,,,,,Fri May 29 11:40:16 UTC 2009,,,,,,,0|i0gopj:,95433,,,,,,,,,,"26/May/09 22:04;milindb;This patch makes the ""-arguments"" to pig commands actually contain an argument when needed after them, rather than accepting commands such as ""run -param -param"".","26/May/09 22:06;milindb;cancelling patch, so as to fix the testcase.",26/May/09 22:33;milindb;Attaching fixed patch with testcase.,26/May/09 22:36;milindb;somehow uploaded the old patch again :-(,26/May/09 22:42;milindb;Hoping to upload the right patch this time :-),"26/May/09 22:42;milindb;yes, it is the right patch :-)","27/May/09 00:45;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12409094/invalidparam.patch
  against trunk revision 778872.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/61/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/61/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/61/console

This message is automatically generated.","28/May/09 22:24;olgan;Patch reviewed and committed. Thanks, Milind!","29/May/09 11:40;hudson;Integrated in Pig-trunk #457 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/457/])
    : run -param -param; is a valid grunt command (milindb via olgan)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Explain doesn't handle PODemux properly,PIG-818,12426331,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,hagleitn,hagleitn,hagleitn,26/May/09 08:25,24/Mar/10 22:11,14/Mar/19 03:05,28/May/09 22:41,,,,,,,0.3.0,,,,,0,,,,"The PODemux operator has nested plans but they are not expanded in the -dot version of explain.

Also, both split and demux are displayed as clusters of nodes, but it really makes more sense to just show them as multi output operators.",,,,,,,,,,,,,,,,,,,26/May/09 08:25;hagleitn;explain.patch;https://issues.apache.org/jira/secure/attachment/12409009/explain.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-05-26 10:02:44.902,,,no_permission,,,,,,,,,,,,164374,,,,,Fri May 29 11:40:16 UTC 2009,,,,,,,0|i0gop3:,95431,,,,,,,,,,"26/May/09 10:02;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12409009/explain.patch
  against trunk revision 777708.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/58/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/58/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/58/console

This message is automatically generated.","28/May/09 22:41;olgan;patch reviewed and committed. Thanks, Gunther!","29/May/09 11:40;hudson;Integrated in Pig-trunk #457 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/457/])
    : Explain doesn't handle PODemux properly (hagleitn via olgan)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigStorage() does not accept Unicode characters in its contructor ,PIG-816,12426166,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,pkamath,viraj,viraj,23/May/09 01:46,24/Mar/10 22:11,14/Mar/19 03:05,29/May/09 21:27,0.3.0,,,,,,0.3.0,,impl,,,0,,,,"Simple Pig script which uses Unicode characters in the PigStorage() constructor fails with the following error:

{code}
studenttab = LOAD '/user/viraj/studenttab10k' AS (name:chararray, age:int,gpa:float);
X2 = GROUP studenttab by age;
Y2 = FOREACH X2 GENERATE group, COUNT(studenttab);
store Y2 into '/user/viraj/y2' using PigStorage('\u0001');
{code}

========================================================================================
ERROR org.apache.pig.tools.grunt.GruntParser - ERROR 2997: Unable to recreate exception from backend error: org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.RuntimeException: org.xml.sax.SAXParseException: Character reference ""&#1"" is an invalid XML character.
========================================================================================
Attaching log file.
",,,,,,,,,,,,,,,,,,,29/May/09 17:31;pkamath;PIG-816.patch;https://issues.apache.org/jira/secure/attachment/12409405/PIG-816.patch,23/May/09 01:54;viraj;pig_1243043613713.log;https://issues.apache.org/jira/secure/attachment/12408862/pig_1243043613713.log,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-05-29 17:31:16.828,,,no_permission,,,,,,,,,,,,164372,Reviewed,,,,Sat May 30 12:11:15 UTC 2009,,,,,,,0|i0goo7:,95427,,,,,,,,,,23/May/09 01:54;viraj;Log file for detailed error message,"29/May/09 17:31;pkamath;Attached patch to address the issue. The change is to serialize the store func spec using ObjectSerializer before storing it in the jobconf. ObjectSerializer.serialize() uses default java serialization
 and then further encodes the output so that control characters get encoded as regular characters. Otherwise any control characters in the store funcspec would break the job.xml which is created by hadoop from the jobconf.","29/May/09 17:38;olgan;+1, the fix looks good","29/May/09 19:24;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12409405/PIG-816.patch
  against trunk revision 779788.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/63/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/63/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/63/console

This message is automatically generated.",29/May/09 21:27;pkamath;Patch committed.,"30/May/09 12:11;hudson;Integrated in Pig-trunk #458 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/458/])
    : PigStorage() does not accept Unicode characters in its contructor (pradeepkth)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Binstorage more robust when data contains record markers,PIG-814,12426072,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,21/May/09 23:11,24/Mar/10 22:11,14/Mar/19 03:05,22/May/09 20:30,0.3.0,,,,,,0.3.0,,,,,0,,,,"When the inputstream for BinStorage is at a position where the data has the record marker sequence, the code incorrectly assumes that it is at the beginning of a record (tuple) and calls DataReaderWriter.readDatum() trying to read the tuple. The problem is more likely when RandomSampleLoader (used in order by implementation) skips the input stream for sampling and calls Binstorage.getNext(). The code should be more robust in such cases",,,,,,,,,,,,,,,,,,,22/May/09 18:22;pkamath;PIG-814.patch;https://issues.apache.org/jira/secure/attachment/12408830/PIG-814.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-05-22 18:38:46.108,,,no_permission,,,,,,,,,,,,164371,Reviewed,,,,Sat May 23 11:40:36 UTC 2009,,,,,,,0|i0gon3:,95422,,,,,,,,,,"22/May/09 18:28;pkamath;The basic issue is that we use ctrl-A,ctrl-B,ctrl-C sequence to identify beginning of a record in binstorage format. We
keep parsing the inputstream till we see this sequence. After seeing this sequence, we send the input stream to another
function to read the tuple which represents the record. The tuple itself is stored in Binstorage by first having a byte
representing the tuple type(tuple marker), followed by the tuple size which is stored as an integer (in java
serialization format) and then the actual tuple fields each stored in java serialization format with a type marker
prefix.

An exception is thrown when the data itself has ctrl-A,ctrl-B,ctrl-C (maybe in the serialized form of a
field in some tuple). This can happen when the RandomSampleLoader (used in ordre by ) tries to uniformly sample 100 tuples and lands in some
part of the data which has this sequence but is not a RECORD begin sequence put in by BinStorage.

The fix will be to look for ctrl-A,ctrl-B,ctrl-c and additionally TUPLEMARKER before trying to read the tuple. This
decreases the probability of finding all these four markers in the data as well ( and it also fixes the error for this
particular query).
","22/May/09 18:35;pkamath;The patch also contains a simple fix for enable split by 'file' in the load statement - in this case, pig should not try to split the input file by block size, but process the entire file in a map.","22/May/09 18:38;olgan;+1, please commit","22/May/09 20:10;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12408830/PIG-814.patch
  against trunk revision 777334.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/54/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/54/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/54/console

This message is automatically generated.","22/May/09 20:30;pkamath;This patch fixes existing code which is tested in other tests, hence no tests were included.

Patch committed.","23/May/09 11:40;hudson;Integrated in Pig-trunk #451 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/451/])
    :Make Binstorage more robust when data contains record markers (pradeepkth)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
COUNT(*) does not work ,PIG-812,12425873,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,breed,viraj,viraj,20/May/09 01:45,24/Mar/10 22:04,14/Mar/19 03:05,21/Aug/09 18:35,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"Pig script to count the number of rows in a studenttab10k file which contains 10k records.
{code}
studenttab = LOAD 'studenttab10k' AS (name:chararray, age:int,gpa:float);

X2 = GROUP studenttab ALL;

describe X2;

Y2 = FOREACH X2 GENERATE COUNT(*);

explain Y2;

DUMP Y2;

{code}

returns the following error
================================================================
ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1066: Unable to open iterator for alias Y2
Details at logfile: /homes/viraj/pig-svn/trunk/pig_1242783700970.log
================================================================

If you look at the log file:
================================================================
Caused by: java.lang.ClassCastException
        at org.apache.pig.builtin.COUNT$Initial.exec(COUNT.java:76)
        at org.apache.pig.builtin.COUNT$Initial.exec(COUNT.java:68)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:201)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:235)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:254)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:204)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:231)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNext(POLocalRearrange.java:223)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:245)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:236)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:88)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:227)
================================================================",,,,,,,,,,,,,PIG-813,PIG-822,,,,,19/Aug/09 03:52;breed;PIG-812.patch;https://issues.apache.org/jira/secure/attachment/12416961/PIG-812.patch,10/Jul/09 05:18;breed;PIG-812.patch;https://issues.apache.org/jira/secure/attachment/12413078/PIG-812.patch,10/Jul/09 00:35;breed;PIG-812.pdf;https://issues.apache.org/jira/secure/attachment/12413067/PIG-812.pdf,20/May/09 01:46;viraj;studenttab10k;https://issues.apache.org/jira/secure/attachment/12408544/studenttab10k,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2009-05-21 01:29:58.406,,,no_permission,,,,,,,,,,,,164369,,,,,Mon Aug 24 00:41:33 UTC 2009,,,,,,,0|i0gom7:,95418,,,,,,,,,,20/May/09 01:46;viraj;Input file,"20/May/09 02:04;viraj;The workaround is to use:
{code}
Y2 = FOREACH X2 GENERATE COUNT(studenttab);
{code}

Please update the documentation at http://hadoop.apache.org/pig/docs/r0.2.0/piglatin.html  if COUNT(*) is actually supported in the latest release.  This is definitely a regression from the previous releases.

Thanks
Viraj","21/May/09 01:29;olgan;The fact that this worked in earlier code was a bug. Now pig has a consistent implementation of *.

From the beginning, Pig chose a different semantics for * than SQL. (it is unfortunate that we chose to use ""*"" for this but is something we need to leave in for consistency and backward compatibility.)

""*"" in SQL means a relation while in pig it means a tuple passed to an operator. So in Pig you can order on the entire row by saying

B = order A by *;

You can also pass the entire row to a UDF by doing myUDF(*).

In this context COUNT(*) makes no sense.","21/May/09 20:32;gmavromatis;Olga's clarification makes sens but it is missing from the pig latin spec. Even Viraj did not know it. How can users know it?

In order for this to be resolved in the right manner the following must added in the http://hadoop.apache.org/pig/docs/r0.2.0/piglatin.html

1) The semantics of * as explained by Olga.
2) An example of GROUP ALL

Otherwise people will waste their time doing the same (documentation-caused) mistakes again.",09/Jul/09 03:39;breed;updating the docs to reflect this semantics.,09/Jul/09 06:01;breed;i've modified the doc to better explain the expressions that can be used in the functions and how flatten works. for convenience for reviewing i've attached a pdf of the changed pages.,"09/Jul/09 09:10;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12412966/PIG-812.pdf
  against trunk revision 792368.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/118/console

This message is automatically generated.","10/Jul/09 00:42;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12413067/PIG-812.pdf
  against trunk revision 792663.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/119/console

This message is automatically generated.",10/Jul/09 05:18;breed;Reattaching the patch so that it gets applied rather than the PDF.,"10/Jul/09 07:07;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12413078/PIG-812.patch
  against trunk revision 792663.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/121/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/121/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/121/console

This message is automatically generated.","23/Jul/09 18:42;olgan;Ben, thanks for updating the docs. A couple of comments/suggestions:

(1) In Star expression section, I think it would be helpful to explain the difference between *  in Pig and SQL in more details.
(2) Boolean, tuple, field, and general expression sections seems a little brief and I am not sure they add much to the user's understanding of the language. Perhaps examples would be helpful?
(3) Description of map dereferencing has key while the Symbol column says 'key'. I think that's confusing. 
(4) The flatten description for a bag is not very clear and I also think has a typo: ({(b,c),(d,e)}) - I think the parenthesis are wrong - I think you meant  to have a bag with a tuple that contains other tuples, right?
(5) Group vs. Cogroup - I think we should put all the information under COUGROUP because we always sold that as the general case and GROUP as ""alias"" for 1 relation case. 

",19/Aug/09 03:52;breed;small changes suggested by Corinne.,"19/Aug/09 04:04;breed;@olga
1) can you suggest some text, i'm not a sql guru
2) this patch is focused on documenting * and flatten. i didn't really change the documentation for boolean, tuple, field, and general expression
3) you are saying that i should have 'key' in the description correct?
4) i don't think that is a typo. it is a tuple with one field that is a bag of tuples, when you flatten the bag field the individual tuples will pop out. can you suggest how to make it more clear?
5) it's easy enough to switch around, but it seemed that group was the more general term (if such a term could be used since they are aliases of each other :) where cogroup implies more than one. don't you think?","19/Aug/09 18:26;gmavromatis;First, I have to say that the updated documentation specifies adequately further undocumented but important details of pig syntax and behavior (e.g. dereference) in addition to the ones mentioned in this ticket. Very good job! This will improve usability of pig significantly. Thank you.

Regarding the particular points in this ticket:

1) Description is adequate for pig. Contrast with sql is nice to have, not crucial.
2) These parts were probably edited by Corinne. Examples may help, but again the spec is important to have and that was addressed.
3) 'key' or key should be consistent in both ""symbol"" and ""notes"" sections.
4) I think the parenthesis pair is correct. This specification of flatten is quite clear!
5) No opinion.
","20/Aug/09 19:20;olgan;+1. There seems to be a consensus that this patch is an improvement. Ben, please, commit. We can make incremental improvements later",21/Aug/09 18:35;breed;Committed revision 806668.,"24/Aug/09 00:41;hudson;Integrated in Pig-trunk #530 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/530/])
    . COUNT(*) does not work
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Globs with ""?"" in the pattern are broken in local mode",PIG-811,12425764,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,hagleitn,olgan,olgan,18/May/09 21:48,24/Mar/10 22:11,14/Mar/19 03:05,22/May/09 00:29,0.3.0,,,,,,0.3.0,,,,,0,,,,"Script:
a = load 'studenttab10?';
dump a;

Actual file name: studenttab10k

Stack trace:

ERROR 2081: Unable to setup the load function.
org.apache.pig.backend.executionengine.ExecException: ERROR 2081: Unable to setup the load function.
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad.getNext(POLoad.java:128)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:231)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter.getNext(POFilter.java:95)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:231)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore.getNext(POStore.java:117)
        at org.apache.pig.backend.local.executionengine.LocalPigLauncher.runPipeline(LocalPigLauncher.java:129)
        at org.apache.pig.backend.local.executionengine.LocalPigLauncher.launchPig(LocalPigLauncher.java:102)
        at org.apache.pig.backend.local.executionengine.LocalExecutionEngine.execute(LocalExecutionEngine.java:163)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:763)
        at org.apache.pig.PigServer.execute(PigServer.java:756)
        at org.apache.pig.PigServer.access$100(PigServer.java:88)
        at org.apache.pig.PigServer$Graph.execute(PigServer.java:923)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:242)
        at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:110)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:151)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:123)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:88)
        at org.apache.pig.Main.main(Main.java:372)
Caused by: java.io.IOException: file:/home/y/share/pigtest/local/data/singlefile/studenttab10 does not exist
        at org.apache.pig.impl.io.FileLocalizer.openDFSFile(FileLocalizer.java:188)
        at org.apache.pig.impl.io.FileLocalizer.openLFSFile(FileLocalizer.java:244)
        at org.apache.pig.impl.io.FileLocalizer.open(FileLocalizer.java:299)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad.setUp(POLoad.java:96)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad.getNext(POLoad.java:124)",,,,,,,,,,,,,,,,,,,21/May/09 08:26;hagleitn;local_engine_glob.patch;https://issues.apache.org/jira/secure/attachment/12408664/local_engine_glob.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-05-21 08:26:21.329,,,no_permission,,,,,,,,,,,,164368,,,,,Fri May 22 11:39:27 UTC 2009,,,,,,,0|i0golz:,95417,,,,,,,,,,"21/May/09 08:26;hagleitn;This patch should fix the problem. Globs are working again in local engine mode.
","21/May/09 10:02;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12408664/local_engine_glob.patch
  against trunk revision 776106.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 7 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/52/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/52/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/52/console

This message is automatically generated.","22/May/09 00:23;olgan;+1, patch looks good. will be committing shortly","22/May/09 00:29;olgan;patch committed; thanks, gunther!","22/May/09 11:39;hudson;Integrated in Pig-trunk #450 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/450/])
    : Globs with ? in the pattern are broken in local mode
(hagleitn via olgan)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scripts failing with NPE,PIG-810,12425487,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,14/May/09 17:47,24/Mar/10 22:11,14/Mar/19 03:05,15/May/09 02:44,0.3.0,,,,,,0.3.0,,,,,0,,,,"Scripts such as:

{code}
a = load 'nosuchfile';
b = store a into 'bla';
{code}

are failing with

{code}
ERROR 2043: Unexpected error during execution.
org.apache.pig.backend.executionengine.ExecException: ERROR 2043: Unexpected error during execution.
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:275)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:757)
        at org.apache.pig.PigServer.execute(PigServer.java:750)
        at org.apache.pig.PigServer.access$100(PigServer.java:88)
        at org.apache.pig.PigServer$Graph.execute(PigServer.java:917)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:242)
        at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:110)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:151)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:123)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:88)
        at org.apache.pig.Main.main(Main.java:372)
Caused by: java.lang.NullPointerException
        at org.apache.pig.tools.pigstats.PigStats.accumulateMRStats(PigStats.java:175)
        at org.apache.pig.tools.pigstats.PigStats.accumulateStats(PigStats.java:94)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:148)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:262)
        ... 10 more
{code}",,,,,,,,,,,,,,,,,,,14/May/09 20:43;alangates;PIG-810.patch;https://issues.apache.org/jira/secure/attachment/12408183/PIG-810.patch,14/May/09 21:38;hagleitn;null_pointer.patch;https://issues.apache.org/jira/secure/attachment/12408191/null_pointer.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-05-14 21:38:13.787,,,no_permission,,,,,,,,,,,,164367,,,,,Fri May 15 17:08:51 UTC 2009,,,,,,,0|i0golj:,95415,,,,,,,,,,"14/May/09 21:38;hagleitn;Ran into the same issue. I have a similar fix, but I also added a unit test, in case you're interested.",15/May/09 02:44;alangates;Patch checked in.,15/May/09 17:08;olgan;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
to remove author tags in the pig source code,PIG-806,12425208,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,gkesavan,gkesavan,12/May/09 12:57,24/Mar/10 22:11,14/Mar/19 03:05,12/May/09 18:01,0.3.0,,,,,,0.3.0,,,,,0,,,,"Following java source files has author tags in them ; which need to to be cleaned. 

src/org/apache/pig/Algebraic.java
src/org/apache/pig/backend/local/executionengine/physicalLayer/relationalOperators/POCross.java
src/org/apache/pig/backend/local/executionengine/physicalLayer/relationalOperators/POCogroup.java
src/org/apache/pig/impl/io/FileSpec.java
src/org/apache/pig/impl/streaming/StreamingCommand.java
src/org/apache/pig/StoreFunc.java
src/org/apache/pig/tools/cmdline/CmdLineParser.java
src/org/apache/pig/tools/timer/PerformanceTimer.java
src/org/apache/pig/tools/timer/PerformanceTimerFactory.java

Thanks,",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-05-12 15:22:28.704,,,no_permission,,,,,,,,,,,,164363,,,,,Tue May 12 18:01:21 UTC 2009,,,,,,,0|i0gojz:,95408,,,,,,,,,,"12/May/09 12:59;gkesavan;This issue blocks : 
https://issues.apache.org/jira/browse/PIG-765","12/May/09 15:22;olgan;what does ""author tags"" mean? Are you talking about control characters?","12/May/09 15:28;thejas;Example of author tag in a java file -

{code}

/*
* @author xyz
*/
{code}","12/May/09 15:39;olgan;Thanks, Tejas. What is wrong with author tag? The error on PIG-765 says that Pig community agreed to disallow that but I don't remember that.",12/May/09 16:56;alangates;http://wiki.apache.org/pig/HowToContribute see section on Making Changes.,12/May/09 18:01;sms;Committed the changes. Except for StreamingCommand.java all the other files noted in the bug report were modified to remove the @author tag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
problem with lineage with double map redirection,PIG-804,12424878,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,olgan,olgan,07/May/09 23:40,24/Mar/10 22:11,14/Mar/19 03:05,26/May/09 17:53,0.3.0,,,,,,0.3.0,,,,,0,,,,"v1       = load     'data' as (s,m,l);
v2       = foreach  v1  GENERATE
                        s#'src_spaceid' AS vspaceid ;
v3       = foreach  v2  GENERATE
                        (chararray)vspaceid#'foo';
explain v3;

The last cast does not have a loader associated with it and as the result the script fails on the backend with the following error: ""Received a bytearray from the UDF. Cannot determine how to convert the bytearray to string.""
",,,,,,,,,,,,,,,,,,,13/May/09 17:51;pkamath;PIG-804.patch;https://issues.apache.org/jira/secure/attachment/12408020/PIG-804.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-05-13 17:51:14.407,,,no_permission,,,,,,,,,,,,164361,Reviewed,,,,Tue May 26 17:53:20 UTC 2009,,,,,,,0|i0gojb:,95405,,,,,,,,,,"13/May/09 17:51;pkamath;The root cause was in the parsers, in CastExp(), a getFieldSchema() was being called on the target operand of the cast to get the alias. This had a side effect of setting up lineage information (i.e. the canonical map in the operand). Apparently this place in the code is early for setting up lineage information since operators may be added/removed later on due to optimizations. This should be done at a later safe point (this change will be tracked in PIG-808). For a fix now, unsetFieldSchema() is called to unset the lineage information.",13/May/09 18:11;sms;+1 for the patch.,"13/May/09 19:28;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12408020/PIG-804.patch
  against trunk revision 774167.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/38/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/38/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/38/console

This message is automatically generated.",26/May/09 17:53;pkamath;Patch was commited on May 13 2009.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig Latin Reference Manual - discussion of Pig streaming is incomplete,PIG-803,12424861,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,ciemo,ciemo,07/May/09 20:58,14/May/10 06:45,14/Mar/19 03:05,22/Apr/10 19:31,,,,,,,0.7.0,,documentation,,,0,,,,"The Pig Latin Reference Manual section on STREAM is missing broad swaths of information such as a discussion of the ship() clause.

http://wiki.apache.org/pig-data/attachments/FrontPage/attachments/plrm.htm#_STREAM_

A more complete definition seems to be here:

http://wiki.apache.org/pig/PigStreamingFunctionalSpec

However, it discusses auto shipping of scripts which doesn't seem to be working.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-02-01 16:49:57.985,,,no_permission,,,,,,,,,,,,164360,,,,,Thu Apr 22 19:31:11 UTC 2010,,,,,,,0|i0goiv:,95403,,,,,,,,,,"01/Feb/10 16:49;chandec;Please note:

(1) I do not maintain the Apache Wiki pages.

(2) plrm.htm is no longer in use - the file and all references to it should be delted.

Thanks/C","12/Apr/10 20:46;olgan;Looks like streaming discussion in http://hadoop.apache.org/pig/docs/r0.6.0/piglatin_ref2.html#STREAM is not very clear on how define is used with streaming. Corinne could we make the connection more clear, thanks.",22/Apr/10 19:31;olgan;Resolved with the latest path in PIG-1320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
script1-hadoop.pig in pig tutorial hangs when run in local mode,PIG-800,12424772,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,06/May/09 23:06,24/Mar/10 22:11,14/Mar/19 03:05,07/May/09 19:24,0.2.0,,,,,,0.3.0,,,,,0,,,,"Any script of the form

{code}
B = foreach A generate flatten(X); -- X is a bag
C = distinct B;
{code}

where X is sometimes an empty bag will hang in local mode.  If distinct is replaced by order by it will also hang.

The issue is that the flatten in foreach returns STATUS_NULL whenever the bag X is empty.  PODistinct and POSort handle this incorrectly and go into an infinite loop.",,,,,,,,,,,,,,,,,,,07/May/09 16:24;alangates;PIG-800.patch;https://issues.apache.org/jira/secure/attachment/12407547/PIG-800.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-05-07 18:28:53.768,,,no_permission,,,,,,,,,,,,164357,,,,,Thu May 07 19:24:03 UTC 2009,,,,,,,0|i0gohj:,95397,,,,,,,,,,07/May/09 16:25;alangates;Changed POSort and PODistinct to swallow POStatus.STATUS_NULL instead of going into an infinite loop when they see it.,"07/May/09 18:28;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12407547/PIG-800.patch
  against trunk revision 771844.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/31/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/31/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/31/console

This message is automatically generated.",07/May/09 18:57;olgan;+1. ,07/May/09 19:24;alangates;Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit tests on windows are failing after multiquery commit,PIG-799,12424568,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,olgan,olgan,04/May/09 23:38,24/Mar/10 22:11,14/Mar/19 03:05,13/May/09 02:24,,,,,,,0.3.0,,,,,0,,,,Daniel could you take a look. It should be reproducible with the latest trunk. Thanks,,,,,,,,,,,,,,,,,,,11/May/09 19:53;daijy;PIG-799.patch;https://issues.apache.org/jira/secure/attachment/12407805/PIG-799.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-05-11 19:53:28.573,,,no_permission,,,,,,,,,,,,164356,,,,,Tue May 12 15:42:24 UTC 2009,,,,,,,0|i0gohb:,95396,,,,,,,,,,11/May/09 19:53;daijy;The failure is caused by changed logic of QueryParser.massageFilename in multi-query patch. I attached patch and please review.,"12/May/09 15:42;olgan;Daniel, thanks for the patch!

Looks like the automated patch testing is not working. If the tests pass in both windows and unix, please, commit the patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit with ORDER BY producing wrong results,PIG-797,12424420,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,olgan,olgan,01/May/09 20:12,24/Mar/10 22:13,14/Mar/19 03:05,19/Jun/09 18:04,0.2.0,,,,,,0.4.0,,,,,0,,,,"Query:

A = load 'studenttab10k' as (name, age, gpa);
B = group A by name;
C = foreach B generate group, SUM(A.gpa) as rev;
D = order C by rev;
E = limit D 10;
dump E;

Output:

(alice king,31.7)
(alice laertes,26.450000000000003)
(alice thompson,25.869999999999997)
(alice van buren,23.59)
(bob allen,19.900000000000002)
(bob ichabod,29.0)
(bob king,28.459999999999994)
(bob miller,10.28)
(bob underhill,28.139999999999997)
(bob van buren,25.990000000000002)",,,,,,,,,,,,,,,,,,,16/Jun/09 19:15;daijy;PIG-797-2.patch;https://issues.apache.org/jira/secure/attachment/12410843/PIG-797-2.patch,17/Jun/09 19:00;daijy;PIG-797-3.patch;https://issues.apache.org/jira/secure/attachment/12410970/PIG-797-3.patch,15/Jun/09 21:11;daijy;PIG-797.patch;https://issues.apache.org/jira/secure/attachment/12410713/PIG-797.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-06-15 21:11:52.853,,,no_permission,,,,,,,,,,,,164354,,,,,Sat Jun 20 11:41:32 UTC 2009,,,,,,,0|i0gogn:,95393,,,,,,,,,,"15/Jun/09 21:11;daijy;For the limited sort case, the extra limit map-reduce operator introduced in [PIG-364|http://issues.apache.org/jira/browse/PIG-364] should use the same output key as the previous sort map-reduce operator. ","15/Jun/09 22:51;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12410713/PIG-797.patch
  against trunk revision 784333.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 2 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/82/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/82/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/82/console

This message is automatically generated.",16/Jun/09 19:15;daijy;New patch solve the findbug issues and add testcase.,"16/Jun/09 21:57;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12410843/PIG-797-2.patch
  against trunk revision 785371.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/87/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/87/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/87/console

This message is automatically generated.","17/Jun/09 17:46;sms;Review Comments:

Index: src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPackage.java

This else part of the code needs to be clone.inner = null. If super.clone is taking care of it then the else part is not required at all.

{code}
+        if (inner!=null)
+        {
+            clone.inner = new boolean[inner.length];
+            for (int i = 0; i < inner.length; i++) {
+                clone.inner[i] = inner[i];
+            }
         }
+        else
+            inner = null;{code}

The rest of the code looks fine.","17/Jun/09 18:11;daijy;The reason I add this is because POPackageLite which inherits POPackage. When we invoke POPackageLite.clone, it will invoke POPackage.clone first. In POPackageLite, inner=null. If I do not put these lines, invoking POPackageLite.clone will throw exception.",17/Jun/09 19:00;daijy;This patch include Santhosh's comment. It is obviously a problem. Thanks!,"17/Jun/09 21:59;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12410970/PIG-797-3.patch
  against trunk revision 785450.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/91/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/91/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/91/console

This message is automatically generated.",19/Jun/09 18:04;daijy;Patch submitted,"20/Jun/09 11:41;hudson;Integrated in Pig-trunk #480 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/480/])
    : Limit with ORDER BY producing wrong results
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error message should indicate in which line number in the Pig script the error occured (debugging BinCond),PIG-790,12424120,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,daijy,viraj,viraj,29/Apr/09 02:39,24/Mar/10 22:15,14/Mar/19 03:05,28/Oct/09 20:47,0.4.0,,,,,,0.6.0,,,,,0,,,,"I have a simple Pig script which loads integer data and does a Bincond, where it compares, (col1 eq ''). There is an error message that is generated in this case, but it does not specify the line number in the script. 
{code}
MYDATA = load '/user/viraj/myerrordata.txt' using PigStorage() as (col1:int, col2:int);

MYDATA_PROJECT = FOREACH MYDATA GENERATE ((col1 eq '') ? 1 : 0) as newcol1,
                                         ((col1 neq '') ? col1 - col2 : 160000)
                                                        as time_diff;

dump MYDATA_PROJECT;
{code}

======================================
2009-04-29 02:33:07,182 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://localhost:9000
2009-04-29 02:33:08,584 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to map-reduce job tracker at: localhost:9001
2009-04-29 02:33:08,836 [main] INFO  org.apache.pig.PigServer - Create a new graph.
2009-04-29 02:33:10,040 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1039: Incompatible types in EqualTo Operator left hand side:int right hand side:chararray
Details at logfile: /home/viraj/pig-svn/trunk/pig_1240972386081.log
==========================
It would be good if the error message has a line number and a copy of the line in the script which is causing the problem.

Attaching data, script and log file. ",,,,,,,,,,,,,,,,,,,15/Oct/09 23:01;daijy;PIG-790-1.patch;https://issues.apache.org/jira/secure/attachment/12422298/PIG-790-1.patch,23/Oct/09 22:14;daijy;PIG-790-2.patch;https://issues.apache.org/jira/secure/attachment/12423074/PIG-790-2.patch,29/Apr/09 02:42;viraj;error_rerport.pig;https://issues.apache.org/jira/secure/attachment/12406732/error_rerport.pig,29/Apr/09 02:42;viraj;myerrordata.txt;https://issues.apache.org/jira/secure/attachment/12406733/myerrordata.txt,29/Apr/09 02:42;viraj;pig_1240972895275.log;https://issues.apache.org/jira/secure/attachment/12406734/pig_1240972895275.log,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2009-10-15 20:08:00.708,,,no_permission,,,,,,,,,,,,164349,Reviewed,,,,Wed Oct 28 20:47:21 UTC 2009,,,,,,,0|i0godj:,95379,,,,,,,,,,"29/Apr/09 02:42;viraj;Pig script, data and log file",15/Oct/09 20:08;daijy;The exception is raised when we doing the type checker. It is after the parsing and currently we do not annotate logical plan with line number. There is no short term plan to put line number there. One thing we can do now is to print out alias in the error message. ,15/Oct/09 23:01;daijy;Attach the patch to add alias to the error message,"16/Oct/09 19:10;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422298/PIG-790-1.patch
  against trunk revision 825712.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/89/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/89/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/89/console

This message is automatically generated.",16/Oct/09 19:47;daijy;Looks like a temporal unit test error. Submit again.,"18/Oct/09 10:10;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422298/PIG-790-1.patch
  against trunk revision 826110.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/96/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/96/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/96/console

This message is automatically generated.",20/Oct/09 23:31;alangates;+1,"20/Oct/09 23:54;dvryaboy;This bit of code is repeated almost a dozen times:

<code>
            String alias = currentAlias;
            if (binOp.getAlias()!=null)
                alias = binOp.getAlias();
            String msg = ""In alias "" + alias + "", "";
</code>

This class is already clocking in at over 2500 lines..

Make it a helper function, shrink the class a bit?",21/Oct/09 00:19;daijy;Definite I can create a helper function for that if necessary. ,21/Oct/09 16:25;daijy;core tests pass manually,23/Oct/09 22:14;daijy;Reattach the patch to address Dmitriy's concern. Write a utility function to reduce the number of lines. No semantics change.,"28/Oct/09 14:54;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12423074/PIG-790-2.patch
  against trunk revision 830335.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/121/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/121/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/121/console

This message is automatically generated.",28/Oct/09 20:47;daijy;Patch committed. No test included in patch since it is only about error message and there is no good way to test automatically. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
coupling load and store in script no longer works,PIG-789,12424114,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,hagleitn,alangates,alangates,29/Apr/09 01:28,24/Mar/10 22:11,14/Mar/19 03:05,04/May/09 23:24,0.3.0,,,,,,0.3.0,,impl,,,0,,,,"Many user's pig script do something like this:

a = load '/user/pig/tests/data/singlefile/studenttab10k' as (name, age, gpa);
c = filter a by age > 500;
e = group c by (name, age);
f = foreach e generate group, COUNT($1);
store f into 'bla';
f1 = load 'bla';
g = order f1 by $1;
dump g;

With the inclusion of the multi-query phase2 patch this appears to no longer work.  You get an error:

2009-04-28 18:24:50,776 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2100: hdfs://wilbur11.labs.corp.sp1.yahoo.com/user/gates/bla does not exist.

We shouldn't be checking for bla's existence here because it will be created eventually by the script.",,,,,,,,,,,,,,,,,,,30/Apr/09 09:01;hagleitn;dump_bug.patch;https://issues.apache.org/jira/secure/attachment/12406888/dump_bug.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-04-30 09:01:24.427,,,no_permission,,,,,,,,,,,,164348,,,,,Mon May 04 23:24:04 UTC 2009,,,,,,,0|i0god3:,95377,,,,,,,,,,30/Apr/09 09:01;hagleitn;Both dump (openIterator) and illustrate (getExamples) show this problem. dump_bug.patch contains a fix; The patch is for the trunk.,04/May/09 23:24;alangates;Patch checked in.  Thanks Gunther.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
javadoc throws warnings - this would break hudson patch test process.,PIG-782,12423684,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,gkesavan,gkesavan,24/Apr/09 09:42,25/Mar/10 00:12,14/Mar/19 03:05,24/Apr/09 17:08,,,,,,,0.3.0,,,,,0,,,,"  [javadoc] /home/gkesavan/pig-trunk/src/org/apache/pig/impl/logicalLayer/schema/SchemaUtil.java:233: warning - @return tag has no arguments.
  [javadoc] /home/gkesavan/pig-trunk/src/org/apache/pig/impl/logicalLayer/schema/SchemaUtil.java:205: warning - @return tag has no arguments.
  [javadoc] /home/gkesavan/pig-trunk/src/org/apache/pig/impl/logicalLayer/schema/SchemaUtil.java:185: warning - @return tag has no arguments.
  [javadoc] /home/gkesavan/pig-trunk/src/org/apache/pig/impl/logicalLayer/schema/SchemaUtil.java:220: warning - @return tag has no arguments.
  [javadoc] /home/gkesavan/pig-trunk/src/org/apache/pig/impl/logicalLayer/schema/SchemaUtil.java:158: warning - @return tag has no arguments.
  [javadoc] /home/gkesavan/pig-trunk/src/org/apache/pig/impl/logicalLayer/schema/SchemaUtil.java:134: warning - @return tag has no arguments.
  [javadoc] /home/gkesavan/pig-trunk/src/org/apache/pig/impl/logicalLayer/schema/SchemaUtil.java:105: warning - @return tag has no arguments.
  [javadoc] /home/gkesavan/pig-trunk/src/org/apache/pig/impl/logicalLayer/schema/SchemaUtil.java:92: warning - @return tag has no arguments.
  [javadoc] /home/gkesavan/pig-trunk/src/org/apache/pig/impl/logicalLayer/schema/SchemaUtil.java:120: warning - @return tag has no arguments.
  [javadoc] /home/gkesavan/pig-trunk/src/org/apache/pig/impl/logicalLayer/schema/SchemaUtil.java:48: warning - @return tag has no arguments.
  [javadoc] /home/gkesavan/pig-trunk/src/org/apache/pig/impl/logicalLayer/schema/SchemaUtil.java:77: warning - @return tag has no arguments.
  [javadoc] /home/gkesavan/pig-trunk/src/org/apache/pig/impl/logicalLayer/schema/SchemaUtil.java:92: warning - @param argument ""names"" is not a parameter name.



",javadoc throws warnings - this would break the hudson patch test process.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-04-24 17:08:12.889,,,no_permission,,,,,,,,,,,,164343,,,,,Fri Apr 24 17:08:12 UTC 2009,,,,,,,0|i0goa7:,95364,,,,,,,,,,24/Apr/09 17:08;sms;I changed the javadoc to remove the warnings.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PORelationToExprProject should create a NonSpillableDataBag to create empty bags,PIG-775,12423620,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,pkamath,pkamath,pkamath,23/Apr/09 19:29,24/Mar/10 22:10,14/Mar/19 03:05,24/Apr/09 16:54,0.2.0,,,,,,0.3.0,,,,,0,,,,"PORelationToExprProject currently uses ""BagFactory.newDefaultBag()"" to create an empty bag in cases where it has to send an empty bag on EOP - each such empty bag created will be registered with the SpillableMemoryManager as a spillable bag. Since it is an empty bag, it really should not be registered as a spillable bag. For this, NonSpillableDataBag can be used.",,,,,,,,,,,,,,,,,,,24/Apr/09 00:33;pkamath;PIG-775.patch;https://issues.apache.org/jira/secure/attachment/12406302/PIG-775.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-04-24 00:51:10.551,,,no_permission,,,,,,,,,,,,164336,,,,,Fri Apr 24 16:54:10 UTC 2009,,,,,,,0|i0go7b:,95351,,,,,,,,,,24/Apr/09 00:33;pkamath;Attached patch with changes suggested in the description of the issue. The patch also has unit test cases. ,24/Apr/09 00:51;alangates;+1,24/Apr/09 16:54;pkamath;Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig does not handle Chinese characters (in both the parameter subsitution using -param_file or embedded in the Pig script) correctly,PIG-774,12423432,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,daijy,viraj,viraj,22/Apr/09 02:43,24/Mar/10 22:10,14/Mar/19 03:05,18/May/09 19:09,0.0.0,,,,,,0.3.0,,grunt,impl,,0,,,,"I created a very small test case in which I did the following.

1) Created a UTF-8 file which contained a query string in Chinese and wrote it to HDFS. I used this dfs file as an input for the tests.
2) Created a parameter file which also contained the same query string as in Step 1.
3) Created a Pig script which takes in the parametrized query string and hard coded Chinese character.
================================================================
Pig script: chinese_data.pig
================================================================
{code}
rmf chineseoutput;
I = load '/user/viraj/chinese.txt' using PigStorage('\u0001');

J = filter I by $0 == '$querystring';
--J = filter I by $0 == ' 歌手    香港情牽女人心演唱會';

store J into 'chineseoutput';
dump J;
{code}
=================================================================

Parameter file: nextgen_paramfile
=================================================================
queryid=20090311
querystring='   歌手    香港情牽女人心演唱會'
=================================================================

Input file: /user/viraj/chinese.txt
=================================================================
shell$ hadoop fs -cat /user/viraj/chinese.txt
        歌手    香港情牽女人心演唱會
=================================================================

I ran the above set of inputs in the following ways:

Run 1:
=================================================================
{code}
java -cp pig.jar:/home/viraj/hadoop-0.18.0-dev/conf/ -Dhod.server='' org.apache.pig.Main -param_file nextgen_paramfile chinese_data.pig
{code}
=================================================================
2009-04-22 01:31:35,703 [Thread-7] WARN  org.apache.hadoop.mapred.JobClient - Use GenericOptionsParser for parsing the
arguments. Applications should implement Tool for the same.
2009-04-22 01:31:40,700 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher -
0% complete
2009-04-22 01:31:50,720 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher -
100% complete
2009-04-22 01:31:50,720 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher -
Success!
=================================================================

Run 2: removed the parameter substitution in the Pig script instead used the following statement.
=================================================================
{code}
J = filter I by $0 == ' 歌手    香港情牽女人心演唱會';
{code}
=================================================================
java -cp pig.jar:/home/viraj/hadoop-0.18.0-dev/conf/ -Dhod.server='' org.apache.pig.Main chinese_data_withoutparam.pig
=================================================================
2009-04-22 01:35:22,402 [Thread-7] WARN  org.apache.hadoop.mapred.JobClient - Use GenericOptionsParser for parsing the
arguments. Applications should implement Tool for the same.
2009-04-22 01:35:27,399 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher -
0% complete
2009-04-22 01:35:32,415 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher -
100% complete
2009-04-22 01:35:32,415 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher -
Success!
=================================================================

In both cases:
=================================================================
{code}
shell $ hadoop fs -ls /user/viraj/chineseoutput
Found 2 items
drwxr-xr-x   - viraj supergroup          0 2009-04-22 01:37 /user/viraj/chineseoutput/_logs
-rw-r--r--   3 viraj supergroup          0 2009-04-22 01:37 /user/viraj/chineseoutput/part-00000
{code}
=================================================================

Additionally tried the dry-run option to figure out if the parameter substitution was occurring properly.
=================================================================
{code}
java -cp pig.jar:/home/viraj/hadoop-0.18.0-dev/conf/ -Dhod.server='' org.apache.pig.Main -param_file nextgen_paramfile -r chinese_data.pig
{code}
=================================================================
{code}
shell$ file chinese_data.pig.substituted 
chinese_data.pig.substituted: ASCII text
shell$ cat chinese_data.pig.substituted 
{code}

{code}
rmf chineseoutput;
I = load '/user/viraj/chinese.txt' using PigStorage('\u0001');

J = filter I by $0 == ' ??????  ??????????????????????????????';

store J into 'chineseoutput';
{code}
=================================================================
This issue has to do with the parser not handling UTF-8 data. ",,,,,,,,,,,,,,,PIG-1187,,,,22/Apr/09 02:51;viraj;chinese.txt;https://issues.apache.org/jira/secure/attachment/12406087/chinese.txt,22/Apr/09 02:51;viraj;chinese_data.pig;https://issues.apache.org/jira/secure/attachment/12406088/chinese_data.pig,22/Apr/09 02:51;viraj;nextgen_paramfile;https://issues.apache.org/jira/secure/attachment/12406089/nextgen_paramfile,29/Apr/09 01:19;viraj;pig_1240967860835.log;https://issues.apache.org/jira/secure/attachment/12406726/pig_1240967860835.log,04/May/09 05:35;daijy;utf8.patch;https://issues.apache.org/jira/secure/attachment/12407131/utf8.patch,25/Apr/09 06:16;daijy;utf8_parser-1.patch;https://issues.apache.org/jira/secure/attachment/12406423/utf8_parser-1.patch,29/Apr/09 03:31;daijy;utf8_parser-2.patch;https://issues.apache.org/jira/secure/attachment/12406736/utf8_parser-2.patch,,,,,,,7.0,,,,,,,,,,,,,,,,,,,2009-04-24 17:05:54.729,,,no_permission,,,,,,,,,,,,164335,,,,,Tue May 19 06:14:10 UTC 2009,,,,,,,0|i0go6v:,95349,,,,,,,,,,"22/Apr/09 02:51;viraj;Input file, Pig Script and Parameter File","24/Apr/09 17:05;alangates;Two lines of change are needed to fix this:
1. In QueryParser.jjt, introduce a new option for handling unicode
2. In the LogicalPlanBuilder, use the getBytes method with the UTF-8
charset

These changes also need to be propagated to the remaining
JavaCC parsers.  Then testing will need to be done.  Estimate 3-5 days of work.

Reference:
http://www.xrce.xerox.com/competencies/content-analysis/tools/publis/javacc_unicode.pdf
","24/Apr/09 20:31;ciemo;A somewhat related bug is JIRA PIG-755 - the difficulty of debugging issues related to passed parameters.

If Pig produced an output file of the code with parameter substitutions made, we could have more rapidly isolated the problem.","24/Apr/09 20:40;sms;In order to obtain the substituted pig script use the -dryrun switch. By default, the substituted pig script is not stored on disk.","24/Apr/09 23:31;viraj;One workaround for this issue is using the FilterFunc, which reads its filter list from a file written on HDFS.

Care has to be taken in the FilterFunc UDF, to invoke the BufferedReader to read UTF8 data.
{code}

public class FILTERFROMFILE extends FilterFunc{
   private String FilterListFileName = """";
   
   private void init() throws IOException {

     Properties props = ConfigurationUtil.toProperties(PigInputFormat.sJob);
     InputStream is = FileLocalizer.openDFSFile(FilterListFileName, props);
     BufferedReader reader = new BufferedReader(new InputStreamReader(is,""UTF8""));
   
   }

   public Boolean exec(Tuple input) throws IOException {
        init();
        //do the matching here
        
   }
}
{code}

Pig code to instantiate the filter function UDF
{code}
register pigudf/myfilterfunc.jar;

define MATCHQUERY FILTERFROMFILE('/user/viraj/chinesedata');

rmf chineseoutput;

I = load '/user/viraj/testchinese' using PigStorage('\u0001') as (teststring:chararray);

J = filter I by MATCHQUERY(teststring);

store J into 'chineseoutput';
{code}
","24/Apr/09 23:33;viraj;Ciemo, as stated in the original problem description there is a  '-r' switch for achieving the same.
{code}
java -cp pig.jar:/home/viraj/hadoop-0.18.0-dev/conf/ -Dhod.server='' org.apache.pig.Main -param_file nextgen_paramfile -r chinese_data.pig
{code}","25/Apr/09 06:16;daijy;As Alan said, adding option to QueryParser.jjt and ParamLoader.jj will do the trick. Probably we do not need to hardcode ""UTF8"" into getBytes. If the OS encoding is UTF8 (LANG=UTF-8), getBytes generates byte array using OS encoding, which is UTF8. If the OS is native encoding (LANG=GB2312), getBytes generate byte array of native encoding, then JavaCharStream will interpret the input stream as native encoding also, so everything goes fine.

One thing I want to point out. On UTF8 OS, everything is perfect. However, on legacy system with native encoding, PigStorage treats all input/output file UTF8, which is reasonable because all data files come from or go to hadoop backend for which UTF8 is highly desired. However, these input/output files cannot be read by ""vi"" on OS with native encoding. Most applications (eg: vi, cat) interpret input file using OS encoding. In addition, if we do a Pig dump on such OS, we will see UTF8 output stream which is messy. Script files and parameter files are local and most users will use vi to edit. We shall interpret script files and parameter files as OS encoding. 

utf8_parser-1.patch is a preliminary patch. Viraj, can you give a try?

Also we need to fix jline. It does not deal with multibyte characters well now.","27/Apr/09 07:04;daijy;Currently Jline does not handle backspace correctly for multibyte characters. When we hit backspace in a UTF8 encoding OS, only partial character will be deleted. If the OS encoding is native, the situation is even worse, Jline will throw an exception for multibyte character entered. This problem is inherent in Jline and all applications utilize JLine share this problem. I will try to fix it in Jline, however, fixing this problem is out of the scope of Pig. So currently, we will have to live with these problem:

# Multibyte character inputing is not supportted if OS encoding is native
# Backspace handling is incorrect if line contains multibyte characters and OS encoding is UTF8

Interstingly, under Cygwin, Jline works fine. The above problem are only for Unix.","29/Apr/09 01:17;viraj;Daniel, 
 Thanks again for your patch, I worked with Pradeep and changed the parser code to invoke that behavior you suggested and then filed Jira PIG-774. 
Here is one problem that I faced..
Suppose I have a script like this, known as chinese_data.pig
{code}
rmf chineseoutput;
%default querystring 'myquery';
I = load '/user/viraj/chinese.txt' using PigStorage('\u0001');

--dump I;

J = filter I by $0 == '$querystring';
--J = filter I by $0 == '       歌手    香港情牽女人心演唱會';

--store J into 'chineseoutput';
dump J;
{code}

I have a parameter file known as ""nextgen_paramfile"" which contains the $querystring variable..

{code}
querystring=""   歌手    香港情牽女人心演唱會""
{code}

I run the above script and parameter file as:
{code}
java -cp pig.jar:/home/viraj/hadoop-0.18.0-dev/conf/ -Dhod.server='' org.apache.pig.Main -param_file nextgen_paramfile chinese_data.pig
{code}

I get the following error:
================================================================================
2009-04-29 01:05:14,979 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://localhost:9000
2009-04-29 01:05:16,328 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to map-reduce job tracker at: localhost:9001
2009-04-29 01:05:16,907 [main] INFO  org.apache.pig.PigServer - Create a new graph.
2009-04-29 01:05:17,794 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Lexical error at line 7, column 33.  Encountered: ""\u6b4c"" (27468), after : ""
================================================================================
I realized that it was something to do with the commented line in the pig script. 
{code}
--J = filter I by $0 == '       歌手    香港情牽女人心演唱會';
{code}
Why is that so, I am attaching the pig_*log on this Jira.

Additionally I found that the parameter substitution is happening correctly when I run the script as:
{code}
java -cp pig.jar:/home/viraj/hadoop-0.18.0-dev/conf/ -Dhod.server='' org.apache.pig.Main -param_file nextgen_paramfile -r chinese_data.pig
{code}
The substituted file, chinese_data.pig.substituted is correct.

Viraj",29/Apr/09 01:19;viraj;Pig Error Log,"29/Apr/09 03:31;daijy;Hi, Viraj,
I see the problem. I should add javacc option to PigScriptParser.jj as well. I attach utf8_parser-2.patch, can you try again? In my previous run I removed the commented line in your script so I didn't see this problem.","29/Apr/09 03:41;viraj;I modified the file PigScriptParser.jj,  and it works.","29/Apr/09 22:53;olgan;Daniel, could you add unit tests for this? Thanks","04/May/09 05:35;daijy;utf8.patch include:
1. utf8 fix for QueryParser.jjt, ParamLoader.jj, PigScriptParser.jj
2. unit test
3. change java source code encoding from ISO-8859-1 to utf8 (in build.xml)
4. change DataByteArray, always use utf8 as string encoding in DataByteArray (even if OS encoding is native, we assume data file is alway utf8 encoding)

Unit test pass on both utf8 system and native encoding system.","04/May/09 21:11;olgan;Daniel,

I don't think we can assume that bytearray data is encouded with UTF8. we can be processing images or other binary data.","04/May/09 22:33;daijy;Hi, Olga, 
I actually assume only textural data inside bytearray as UTF8. The functions I change are DataByteArray.DataByteArray(String s) and DataByteArray.toString(), which are the functions to convert String to/from byte array. If image or other binary data, we will not need to convert them to/from a String, so they will be fine.","04/May/09 23:13;olgan;sorry, Daniel, I misunderstood you comment. The patch looks good, please commit.","18/May/09 15:45;olgan;Daniel, was this change committed? Can the JIRA be closed?","18/May/09 19:09;daijy;Yes, the patch is committed. Thanks","19/May/09 05:53;viraj;Hi Daniel,
 For this patch to work, is it important  to set:

LESSCHARSET to utf-8

LANG to en_US.utf8

I am observing that the dry run using pig -r does not yield the right parameter substitution, if we do not have these variables set. 

They are not set by default on the RH-EL 5.0

You have mentioned this in your earlier comments!!

Thanks Viraj","19/May/09 06:14;daijy;You get the point, Viraj. 

Actually we can have two different configurations:
1. LANG=UTF8, all data files, script files, parameter files are UTF8
2. LANG=GB2321, data files are UTF8; script files, parameter files are GB2321
However, for RH-EL default settings, LANG=POSIX, which does not work well for Chinese characters. 

So for simplicity, we can have everything UTF8 (case 1). This is the default setting for Ubuntu. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Empty complex constants (empty bag, empty tuple and empty map) should be supported",PIG-773,12423391,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,ashutoshc,pkamath,pkamath,21/Apr/09 17:40,24/Mar/10 22:13,14/Mar/19 03:05,24/Jul/09 01:00,0.3.0,,,,,,0.4.0,,,,,1,,,,"We should be able to create empty bag constant using {}, empty tuple constant using (), empty map constant using [] within a pig script",,,,,,,,,,,,,,,,,,,08/Jun/09 20:03;ashutoshc;pig-773.patch;https://issues.apache.org/jira/secure/attachment/12410155/pig-773.patch,21/Jun/09 22:51;ashutoshc;pig-773_v2.patch;https://issues.apache.org/jira/secure/attachment/12411357/pig-773_v2.patch,28/Jun/09 22:30;ashutoshc;pig-773_v3.patch;https://issues.apache.org/jira/secure/attachment/12412030/pig-773_v3.patch,02/Jul/09 16:34;ashutoshc;pig-773_v4.patch;https://issues.apache.org/jira/secure/attachment/12412406/pig-773_v4.patch,19/Jul/09 02:18;ashutoshc;pig-773_v5.patch;https://issues.apache.org/jira/secure/attachment/12413938/pig-773_v5.patch,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2009-06-08 20:03:35.668,,,no_permission,,,,,,,,,,,,164334,Reviewed,,,,Fri Jul 24 01:00:11 UTC 2009,,,,,,,0|i0go67:,95346,,,,,,,,,,"08/Jun/09 20:03;ashutoshc;Currently Parser assumes non-empty complex constants, so it fails when it encounters an empty complex constants. Patch is included which fixes this. A new regex production is added to process empty complex constants,",08/Jun/09 20:04;ashutoshc;Submitting patch,"08/Jun/09 23:48;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12410155/pig-773.patch
  against trunk revision 782790.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/76/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/76/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/76/console

This message is automatically generated.","09/Jun/09 00:57;ashutoshc;FindBugs is complaining for starting function name with uppercase letter. All the method names in QueryParser.jjt starts with uppercase letter. So, following that convention I am leaving that function name as it is. 

FindBug warning: 
The method name org.apache.pig.impl.logicalLayer.parser.QueryParser.UnionClause(LogicalPlan) doesn't start with a lower case letter
Bug type NM_METHOD_NAMING_CONVENTION
In class org.apache.pig.impl.logicalLayer.parser.QueryParser
In method org.apache.pig.impl.logicalLayer.parser.QueryParser.UnionClause(LogicalPlan)
At QueryParser.java:[lines 2662-2713]  ",09/Jun/09 01:00;sms;You can ignore that.,"12/Jun/09 06:02;sms;Review comments:

I am not sure if iterating over the contents of the tuple and adding them to the bag is correct.

{code}

-{
- (""("" t = Tuple() {bag.add(t);} "")"" ("","" ""("" t = Tuple() {bag.add(t);} "")"" )* )
  {
+//  At this point bag is not empty (i.e., its not{}). So, it must be containing one or more tuples.
+// Since Tuple again makes a recursive call it results in all tuples of this bag getting  retrieved wrapped
+// in a single tuple. So, we iterate over all the tuples inside the outer tuple and then add them in a bag.
+    (
+        t = Tuple() {
+                        for( Object tuple: t.getAll() )
+                            bag.add( (Tuple)tuple );
+                    }
+    ) 
+{
{code}",21/Jun/09 22:51;ashutoshc;This patch doesn't have extra EmptyConstant production but rather matches for empty content of bag / tuple / map in their respective productions. As a result it avoids the unintuitive logic as Santhosh pointed above.,21/Jun/09 23:09;ashutoshc;trying to make hudson pick the patch,"22/Jun/09 17:35;sms;Comments:

1. Minor comment - the comment on the empty productions all have the same text For tuple and bag, it should be changed to tuple and bag respectively

{code}
+	|{ } // Match the empty content in map.
{code}

2. I am not sure about the test case testEmptyBagConstRecursive. Here the bag contains an empty tuple. As a result, the field schema for the bag should contain the schema of the empty tuple. The test case will probably fail.

{code}
+    @Test
+    public void testEmptyBagConstRecursive() throws FrontendException{
+       
+        LogicalPlan lp = buildPlan(""a = foreach (load 'b') generate {()};"");
+        LOForEach foreach = (LOForEach) lp.getLeaves().get(0);
+        
+        Schema.FieldSchema bagFs = new Schema.FieldSchema(null,null,DataType.BAG);
+        Schema expectedSchema = new Schema(bagFs);
+       
+        assertTrue(Schema.equals(foreach.getSchema(), expectedSchema, false, true));
+    }

{code}

3. There are no tests that check if the empty constants are actually created, i.e., there are no checks for expected empty constants. The test below checks if the parser can parse the new syntax for empty constants. In addition, the values generated by the parser have to checked against expected values for these constants.

{code}
+    @Test
+    public void testRandomEmptyConst(){
+        // Various random scripts to test recursive nature of parser with empty constants.
+       
+        buildPlan(""a = foreach (load 'b') generate {({})};"");
+        buildPlan(""a = foreach (load 'b') generate ({()});"");
+        buildPlan(""a = foreach (load 'b') generate {(),()};"");
+        buildPlan(""a = foreach (load 'b') generate ({},{});"");
+        buildPlan(""a = foreach (load 'b') generate ((),());"");
+        buildPlan(""a = foreach (load 'b') generate ([],[]);"");
+        buildPlan(""a = foreach (load 'b') generate {({},{})};"");
+        buildPlan(""a = foreach (load 'b') generate {([],[])};"");
+        buildPlan(""a = foreach (load 'b') generate (({},{}));"");
+        buildPlan(""a = foreach (load 'b') generate (([],[]));"");
+    }
{code}","23/Jun/09 17:22;ashutoshc;Santhosh, thanks for the review.

1. Will be fixing it in new patch.
2. Test passes while it should fail. Seems like there is an issue how Bag handles its schema. Will be investigating it further.
3. Will include test cases which check for existence of constants in the plan.
","24/Jun/09 04:42;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12411357/pig-773_v2.patch
  against trunk revision 787878.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/97/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/97/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/97/console

This message is automatically generated.","28/Jun/09 22:30;ashutoshc;It turned out that there was a bug in DataType.java where schema for a Bag is computed. The patch fixes the bug. Test cases are modified 
to match the expected behavior. Also the values generated by the parser are checked against expected values for the parsed constants.","29/Jun/09 00:17;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12412030/pig-773_v3.patch
  against trunk revision 788174.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/106/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/106/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/106/console

This message is automatically generated.","29/Jun/09 17:53;sms;Review comments:

1. In addition to checking the type of the constant, the value should also be checked. The checks on the data type is good. A check on the actual contents of the empty bag, empty tuple and empty map will complete the testing.

{code}
+        LOConst loConst = (LOConst)logOp;
+        assertTrue(loConst.getType() == DataType.TUPLE);
+        assertTrue(loConst.getValue() instanceof Tuple);
{code}

2. When you have a bag like {(), (1)}, the schema of this bag is returned as a bag that contains a tuple that has no schema. This might be the right approach for now, i.e., if a bag contains a tuple with no schema then the schema of the bag will contain a tuple with no schema irrespective of the contents of the remaining tuple. This approach/idea falls into the bigger question of how to handle unknown schemas in Pig. Since Alan is looking at this question for all of Pig, it will be good if he can review this part.","01/Jul/09 16:23;alangates;In response to point 2 of Santhosh's previous comment, I agree it is fine for now.  We need to answer this question definitively, but until we do I think this is a reasonable answer.",02/Jul/09 16:34;ashutoshc;Updated the patch incorporating Santhosh's suggestion about test cases.,"03/Jul/09 15:11;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12412406/pig-773_v4.patch
  against trunk revision 790735.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/109/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/109/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/109/console

This message is automatically generated.","06/Jul/09 18:25;sms;A comment on the latest patch (pig-773_v4.patch):

Index: src/org/apache/pig/data/DataType.java
===================================================================

Since the bag schema contains the tuple schema, bag schema should set the two level access to true.

{code}
@@ -998,8 +999,8 @@
                     schema = schemas.get(0);
                     if(null == schema) {
                         Schema.FieldSchema tupleFs = new Schema.FieldSchema(null, null, TUPLE);
-                        Schema bagSchema = new Schema(tupleFs);
-                        return new Schema.FieldSchema(null, null, BAG);
+                        bagSchema = new Schema(tupleFs);
+                        return new Schema.FieldSchema(null, bagSchema, BAG);
{code}",19/Jul/09 02:18;ashutoshc;Updated patch.,"19/Jul/09 04:59;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12413938/pig-773_v5.patch
  against trunk revision 794937.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/135/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/135/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/135/console

This message is automatically generated.",23/Jul/09 21:57;sms;+ 1 for the changes.,24/Jul/09 01:00;sms;Patch has been committed. Thanks for the fix Ashutosh.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schema reported from DESCRIBE and actual schema of inner bags are different.,PIG-767,12422976,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,gmavromatis,gmavromatis,16/Apr/09 04:43,04/Aug/11 00:35,14/Mar/19 03:05,24/Jan/11 18:28,,,,,,,0.9.0,,,,,0,,,,"The following script:

urlContents = LOAD 'inputdir' USING BinStorage() AS (url:bytearray, pg:bytearray);
-- describe and dump are in-sync
DESCRIBE urlContents;
DUMP urlContents;

urlContentsG = GROUP urlContents BY url;
DESCRIBE urlContentsG;

urlContentsF = FOREACH urlContentsG GENERATE group,urlContents.pg;

DESCRIBE urlContentsF;
DUMP urlContentsF;


Prints for the DESCRIBE commands:

urlContents: {url: chararray,pg: chararray}
urlContentsG: {group: chararray,urlContents: {url: chararray,pg: chararray}}
urlContentsF: {group: chararray,pg: {pg: chararray}}

The reported schemas for urlContentsG and urlContentsF are wrong. They are also against the section ""Schemas for Complex Data Types"" in http://wiki.apache.org/pig-data/attachments/FrontPage/attachments/plrm.htm#_Schemas.

As expected, actual data observed from DUMP urlContentsG and DUMP urlContentsF do contain the tuple inside the inner bags.

The correct schema for urlContentsG is:  {group: chararray,urlContents: {t1:(url: chararray,pg: chararray)}}

This may sound like a technicality, but it isn't. For instance, a UDF that assumes an inner bag of {chararray} will not work with {(chararray)}. 



",,,,,,,,,,,,,,,,,,,10/Jan/11 20:06;daijy;PIG-767-1.patch;https://issues.apache.org/jira/secure/attachment/12467916/PIG-767-1.patch,12/Jan/11 22:36;daijy;PIG-767-2.patch;https://issues.apache.org/jira/secure/attachment/12468179/PIG-767-2.patch,21/Jan/11 00:44;daijy;PIG-767-3.patch;https://issues.apache.org/jira/secure/attachment/12468932/PIG-767-3.patch,24/Jan/11 03:56;daijy;PIG-767-4.patch;https://issues.apache.org/jira/secure/attachment/12469118/PIG-767-4.patch,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2009-04-16 19:07:31.553,,,no_permission,,,,,,,,,,,,657,Reviewed,,,,Mon Jan 24 18:28:57 UTC 2011,,,,,,,0|i0go3z:,95336,,,,,,,,,,"16/Apr/09 19:07;sms;Firstly, the describe output is broken for bags in some cases. You will not see the inner tuple (t1 in your example). This can be fixed. It will not cause any problems for the runtime execution.

Bags are containers of tuples. There are no bags that do not contain tuples unless the bags are empty. As a result, UDFs that assume an inner bag of chararray will always get a bag with chararray.

I am pasting the output of similar queries and you should see the inner tuples in the output. Notice that you see the tuples in the bags. Also notice that the bags in the describe output do not have the inner tuples.

{code}

grunt> a = load '/user/sms/data/student_tab.data' as (name: chararray, age:int, gpa: float);
grunt> b = group a by age; 

grunt> describe b;
b: {group: int,a: {name: chararray,age: int,gpa: float}}

grunt> dump b;
(19,{(John,19,3.8F),(Jack,19,3.1F)})
(20,{(Joe,20,3.5F),(Harry,20,3.2F),(Govinda,20,4.0F)})

grunt> c = foreach b generate group, a.gpa;

grunt> describe c;
c: {group: int,gpa: {gpa: float}}

grunt> dump c;
(19,{(3.8F),(3.1F)})
(20,{(3.5F),(3.2F),(4.0F)})
{code}","16/Apr/09 20:05;gmavromatis;Santosh: You are saying the exact same thing I said, i.e. that inner bags contain tuples and that describe does not report the tuples in the schema it prints. That's precisely the bug.

Our examples are in fact similar too! I hope that my description shows that I understand that inner bags contain tuples (""As expected, actual data observed from DUMP... contain the tuple inside the inner bags"").","16/Apr/09 20:25;sms;As a reference, please look at PIG-449

The tuple inside a bag cannot be accessed by name or position. There are no semantics that support accessing a tuple inside a bag. However, the contents of the tuple inside a bag are accessible. As such, the presence or absence of a tuple inside a bag (as part of the schema) in the describe output does not matter.

E.g.: urlContentsG: {group: chararray,urlContents: {t1:(url: chararray,pg: chararray)}} 
In the above schema, you can access urlContents.url. You will not be able to access urlContents.t1

An example to illustrate this point follows:

{code}
grunt> a = load 'input' as (bagColumn: bag{t: tuple(i: int, f: float)});
grunt> b = foreach a generate bagColumn.t;
2009-04-16 13:23:43,324 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1028: Access to the tuple (t) of the bag is disallowed. Only access to the elements of the tuple in the bag is allowed.

Details at logfile: /homes/sms/src_pig/pig/trunk_optimizer_phase1/pig_1239908562989.log

grunt> c = foreach a generate bagColumn.i;

{code}","10/Jan/11 20:06;daijy;The only place we generate bag schema without tuple inside is LOGroup. Attach PIG-767-1.patch to change LOGroup schema generation in new logical plan. Once describe migrate to new logical plan, this issue is fixed.",12/Jan/11 22:36;daijy;PIG-767-2.patch fix all unit test failures.,"18/Jan/11 22:38;rding;
Together with PIG-1786, the output of above describe commands is

{code}
urlContents: {url: bytearray,pg: bytearray}
urlContentsG: {group: bytearray,urlContents: {(url: bytearray,pg: bytearray)}}
urlContentsF: {group: bytearray,{pg: bytearray}}
{code}

should the output for urlContentsF be 

{code}
urlContentsF: {group: bytearray,{(pg: bytearray)}}
{code}
","19/Jan/11 02:30;daijy;It is depend on PIG-1786 to move describe to new logical plan. Once PIG-1786 commit, we will see the right schema.",21/Jan/11 00:44;daijy;Richard is right. Miss the change in Dereference in original patch. Attach PIG-767-3.patch.,24/Jan/11 03:56;daijy;PIG-767-4.patch fix unit test failure of PIG-767-3.patch,24/Jan/11 18:28;daijy;Review notes: https://reviews.apache.org/r/278/,24/Jan/11 18:28;daijy;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ERROR 2086 on simple JOIN,PIG-761,12422428,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,vzaliva,vzaliva,09/Apr/09 00:53,24/Mar/10 22:15,14/Mar/19 03:05,25/Dec/09 00:20,0.2.0,,,,,,0.6.0,,,,,0,,,,"ERROR 2086: Unexpected problem during optimization. Could not find all LocalRearrange operators.org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1002: Unable to store alias 109

doing pretty straightforward join in one of my pig scripts. I am able to 'dump' both relationship involved in this join. when I try to join them I am getting this error.

Here is a full log:


ERROR 2086: Unexpected problem during optimization. Could not find all
LocalRearrange operators.
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1002: Unable
to store alias 109
       at org.apache.pig.PigServer.registerQuery(PigServer.java:296)
       at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:529)
       at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:280)
       at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:99)
       at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:75)
       at org.apache.pig.Main.main(Main.java:319)
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR
2043: Unexpected error during execution.
       at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:274)
       at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:700)
       at org.apache.pig.PigServer.execute(PigServer.java:691)
       at org.apache.pig.PigServer.registerQuery(PigServer.java:292)
       ... 5 more
Caused by: org.apache.pig.impl.plan.optimizer.OptimizerException:
ERROR 2086: Unexpected problem during optimization. Could not find all
LocalRearrange operators.
       at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.POPackageAnnotator.handlePackage(POPackageAnnotator.java:116)
       at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.POPackageAnnotator.visitMROp(POPackageAnnotator.java:88)
       at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper.visit(MapReduceOper.java:194)
       at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper.visit(MapReduceOper.java:43)
       at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:65)
       at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:67)
       at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:67)
       at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:67)
       at org.apache.pig.impl.plan.DepthFirstWalker.walk(DepthFirstWalker.java:50)
       at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
       at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.
MapReduceLauncher.compile(MapReduceLauncher.java:198)
       at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:80)
       at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:261)
       ... 8 more
ERROR 1002: Unable to store alias 398
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1002: Unable
to store alias 398
       at org.apache.pig.PigServer.registerQuery(PigServer.java:296)
       at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:529)
       at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:280)
       at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:99)
       at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:75)
       at org.apache.pig.Main.main(Main.java:319)
Caused by: java.lang.NullPointerException
       at org.apache.pig.backend.hadoop.executionengine.physicalLayer.LogToPhyTranslationVisitor.visit(LogToPhyTranslationVisitor.java:669)
       at org.apache.pig.impl.logicalLayer.LOCogroup.visit(LOCogroup.java:330)
       at org.apache.pig.impl.logicalLayer.LOCogroup.visit(LOCogroup.java:41)
       at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:68)
       at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
       at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:246)
       at org.apache.pig.PigServer.compilePp(PigServer.java:771)
       at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:697)
       at org.apache.pig.PigServer.execute(PigServer.java:691)
       at org.apache.pig.PigServer.registerQuery(PigServer.java:292)
       ... 5 more
",mapreduce mode,,,,,,,,,,,,,,,,,,23/Dec/09 19:13;daijy;PIG-761-1.patch;https://issues.apache.org/jira/secure/attachment/12428861/PIG-761-1.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-04-09 16:23:07.82,,,no_permission,,,,,,,,,,,,164324,Reviewed,,,,Fri Dec 25 00:20:35 UTC 2009,,,,,,,0|i0go1b:,95324,,,,,,,,,,"09/Apr/09 16:23;olgan;Please, provide a query that is causing this problem. Without reproducible case, this will be hard to diagnose. We don't see this problem in our tests",16/Oct/09 18:26;timmsc;I ran into this bug as well.  I am using Pig 0.4.0.  I did a LIMIT on one of the data sets to be joined too.  I worked around the problem by approximating the LIMIT with a FILTER.  I'll see if I can distill it down to a small reproducible test case.  I won't get time to do that for a week or two though.,"23/Dec/09 10:34;ankur;Here is a very simple script to reproduce the issue:-

----- Start ---------
data1 = LOAD 'data1' as (a:int, b:int, c:chararray);
proj1 = LIMIT data1 5;

data2 = LOAD 'data2' as (x:int, y:chararray, z:chararray);
proj2 = FOREACH data2 GENERATE x, y;

cogrouped = COGROUP proj1 BY a, proj2 BY x INNER PARALLEL 2;
joined = FOREACH cogrouped GENERATE FLATTEN(proj1), FLATTEN(proj2);

store joined into 'results';
----- End ------------

The problem seems to be with the LIMIT operator for one of the relations participating in the join.  Seems like this causes the mismatch between expected and found local re-arrange operators","23/Dec/09 19:13;daijy;The problem lies in the complexity between limit and one of the optimization. More specific, optimization POPackageAnnotator search for matching POLocalRearrange in the map plan, if not, search in the predecessor's reduce plan. However, if we have a limit, limit will introduce a map-reduce job between the original map-reduce job and its predecessor. POPackageAnnotator cannot find the POLocalRearrange then. To fix it, we mark the map reduce job introduced by limit, and in POPackageAnnotator, if we saw a limit map reduce job, we will search POLocalRearrange in limit job's parent.","24/Dec/09 06:26;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428861/PIG-761-1.patch
  against trunk revision 893660.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/157/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/157/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/157/console

This message is automatically generated.","24/Dec/09 22:51;pkamath;+1, Changes look good. I had a question on the following lines:
{code}
108                 if (mrOper.isLimitOnly())
  109                     mrOper = this.mPlan.getPredecessors(mrOper).get(0); 
{code}

Will there be a case when the POLocalRearrange may be present in the limit only MROper?
Will there be a case where the limit only MROper may have > 1 predecessors?

If the answer to both of these is no, then the patch is good to commit otherwise, these condition may need to be handled.
","24/Dec/09 22:56;daijy;The answer to both is no. There is only one case to insert limit only MROper, It always have the same structure and only have one predecessor, which is the real limit MROper job.",25/Dec/09 00:20;daijy;Patch committed to both trunk and 0.6 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Converting load/store locations into fully qualified absolute paths,PIG-758,12422398,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,hagleitn,hagleitn,08/Apr/09 19:56,14/May/10 06:45,14/Mar/19 03:05,28/Jan/10 00:14,,,,,,,0.7.0,,,,,0,,,,"As part of the multiquery optimization work there is a need to use absolute paths for load and store operations (because the current directory changes during the execution of the script). In order to do so, we are suggesting a change to the semantics of the location/filename string used in LoadFunc and Slicer/Slice.

The proposed change is:

   * Load locations without a scheme part are expected to be hdfs (mapreduce mode) or local (local mode) paths
   * Any hdfs or local path will be translated to a fully qualified absolute path before it is handed to either a LoadFunc or Slicer
   * Any scheme other than ""file"" or ""hdfs"" will result in the load path to be passed through to the LoadFunc or Slicer without any modification.

Example:

If you have a LoadFunc that reads from a database, in the current system the following could be used:

{noformat}
a = load 'table' using DBLoader();
{noformat}

With the proposed changes table would be translated into an hdfs path though (""hdfs://..../table""). Probably not what the DBLoader would want to see. In order to make it work one could use:

{noformat}
a = load 'sql://table' using DBLoader();
{noformat}

Now the DBLoader would see the unchanged string ""sql://table"".

This is an incompatible change, but hopefully not affecting many existing Loaders/Slicers. Since this is needed with the multiquery feature, the behavior can be reverted back by using the ""no_multiquery"" pig flag.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-01-28 00:14:17.439,,,no_permission,,,,,,,,,,,,164322,Incompatible change,,,,Thu Jan 28 00:14:17 UTC 2010,,,,,,,0|i0gnzz:,95318,,,,,,,,,,28/Jan/10 00:14;olgan;Fixed as part of LSR.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDFs should have API for transparently opening and reading files from HDFS or from local file system with only relative path,PIG-756,12422371,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,ciemo,ciemo,08/Apr/09 14:40,14/May/10 06:45,14/Mar/19 03:05,08/Apr/10 01:41,,,,,,,0.7.0,,,,,1,,,,"I have a utility function util.INSETFROMFILE() that I pass a file name during initialization.

{code}
define inQuerySet util.INSETFROMFILE(analysis/queries);
A = load 'logs' using PigStorage() as ( date int, query chararray );
B = filter A by inQuerySet(query);
{code}

This provides a computationally inexpensive way to effect map-side joins for small sets plus functions of this style provide the ability to encapsulate more complex matching rules.

For rapid development and debugging purposes, I want this code to run without modification on both my local file system when I do pig -exectype local and on HDFS.

Pig needs to provide an API for UDFs which allow them to either:

1) ""know""  when they are in local or HDFS mode and let them open and read from files as appropriate
2) just provide a file name and read statements and have pig transparently manage local or HDFS opens and reads for the UDF

UDFs need to read configuration information off the filesystem and it simplifies the process if one can just flip the switch of -exectype local.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-04-08 01:41:32.957,,,no_permission,,,,,,,,,,,,164320,,,,,Thu Apr 08 01:41:55 UTC 2010,,,,,,,0|i0gnz3:,95314,,,,,,,,,,"08/Apr/09 14:42;ciemo;BTW, there used to be a mechanism to do this in early versions of Pig that was last in the transition to the new execution system.","08/Apr/10 01:41;viraj;In Pig 0.7 we have moved local mode of Pig to local mode of Hadoop.
https://issues.apache.org/jira/browse/PIG-1053

Closing issue",08/Apr/10 01:41;viraj;https://issues.apache.org/jira/browse/PIG-1053 fixes this issue.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Inability to debug pig scripts due to ERROR 2999: ""FrontendException cannot be cast to java.lang.Error"" cast error",PIG-751,12421937,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,viraj,viraj,03/Apr/09 01:04,25/Mar/10 00:12,14/Mar/19 03:05,14/Jul/09 16:00,0.3.0,,,,,,0.4.0,,grunt,,,0,,,,"I have an input file which is being loaded by BinStorage()
{code}
myinput = LOAD 'partfile' USING BinStorage() AS (eid:long, url:chararray, title:chararray, index:int, num_candidates:int, num_inlinks:int, inlinks);

lim_myinput = limit myinput 100;

dump lim_myinput;

{code}

I get the following error: 
=========================================================================================================================================
2009-04-03 00:50:57,490 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2999: Unexpected internal error. org.apache.pig.impl.logicalLayer.FrontendException cannot be cast to java.lang.Error
=========================================================================================================================================
ERROR 2999: Unexpected internal error. org.apache.pig.impl.logicalLayer.FrontendException cannot be cast to java.lang.Error
java.lang.ClassCastException: org.apache.pig.impl.logicalLayer.FrontendException cannot be cast to java.lang.Error
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:1098)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:804)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:595)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:60)
        at org.apache.pig.PigServer.parseQuery(PigServer.java:311)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:277)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:529)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:280)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:99)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:88)
        at org.apache.pig.Main.main(Main.java:352)
=========================================================================================================================================
It seems that the error is caused during the schema creation, in BinStorage(), however this error is not reported!! If I use PigStorage() the error goes away but the results are incorrect.
{code}
myinput = LOAD 'partfile' USING PigStorage() AS (eid:long, url:chararray, title:chararray, index:int, num_candidates:int, num_inlinks:int, inlinks);
{code}

Currently it is not possible to find out where the error really occurred but to manually inspect each keyword.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-04-04 08:56:18.452,,,no_permission,,,,,,,,,,,,164315,,,,,Sat Apr 04 08:56:18 UTC 2009,,,,,,,0|i0gnwv:,95304,"please, reopen if there is a reproducible case",,,,,,,,,"03/Apr/09 06:54;viraj;The intention of filing this Jira is to show illegal casting which is happening in pig. It is not to find what is wrong with this script/dataset - but the fact that any schema inferring errors/mismatch with specified schema results in exceptions which make it impossible to debug why it is happening.

The presented exception is a trivial enough case where we can find out what is wrong by manual inspection, for much larger scripts with multiple load/store plans, it is not feasible to always break it down into 25 odd individual scripts just to find out where things fail.

","04/Apr/09 08:56;zjffdu;Could you attach your source data, I run this script, and it works.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No attempt to check if 'flatten(group) as' has the same cardinality as 'group alias by',PIG-749,12421926,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,alangates,viraj,viraj,02/Apr/09 22:40,04/Aug/11 00:34,14/Mar/19 03:05,24/Dec/10 02:33,0.3.0,,,,,,0.9.0,,grunt,,,0,,,,"Pig script which does grouping for 3 columns and flattens as 4 columns works when in principle it should not and maybe fail as a front-end error.
{code}
A = load 'groupcardinalitycheck.txt' using PigStorage() as (col1:chararray, col2:chararray, col3:int, col4:chararray);

B = group A by (col1, col2, col3);

C = foreach B generate
           flatten(group) as (col1, col2, col3, col4),
           SIZE(A) as frequency;

dump C;

{code}
==========================================================================================
Data
==========================================================================================
hello   CC      1       there
hello   YSO     2       out
ouch    CC      2       hey
==========================================================================================
Result of the preceding script
==========================================================================================
(ouch,CC,2,1L)
(hello,CC,1,1L)
(hello,YSO,2,1L)
==========================================================================================
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-11-05 21:54:20.987,,,no_permission,,,,,,,,,,,,66309,,,,,Fri Dec 24 02:33:42 UTC 2010,,,,,,,0|i0gnw7:,95301,,,,,,,,,,"05/Nov/10 21:54;daijy;In the current code, it fails, but error message need to improve.","24/Dec/10 02:33;daijy;Error message is enhanced by PIG-1737. Now the error message becomes:
ERROR 1117: Cannot merge (group::col1:chararray,group::col2:chararray,group::col3:int) with user defined schema (col1:NULL,col2:NULL,col3:NULL,col4:NULL)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Works in --exectype local, fails on grid - ERROR 2113: SingleTupleBag should never be serialized",PIG-746,12421837,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rding,ciemo,ciemo,02/Apr/09 06:01,24/Mar/10 22:15,14/Mar/19 03:05,29/Oct/09 23:37,,,,,,,0.6.0,,,,,1,,,,"
The script below works on Pig 2.0 local mode but fails when I run the same program on the grid.

I was attempting to create a workaround for PIG-710.

Here's the error:
{code}
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 2113: SingleTupleBag should never be serialized
or serialized.
        at org.apache.pig.data.SingleTupleBag.write(SingleTupleBag.java:129)
        at org.apache.pig.data.DataReaderWriter.writeDatum(DataReaderWriter.java:147)
        at org.apache.pig.data.DefaultTuple.write(DefaultTuple.java:291)
        at org.apache.pig.impl.io.PigNullableWritable.write(PigNullableWritable.java:83)
        at
org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:90)
        at
org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:77)
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:439)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:101)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:219)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:208)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:86)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:227)
{code}

Here's the program:
{code}
A = load 'filterbug.data' using PigStorage() as ( id, str );

A = foreach A generate
        id,
        str,
        (
        str matches 'hello' or
        str matches 'hello'
        ? 1 : 0
        )                       as matched;
describe A;

B = group A by ( id );
describe B;

D = foreach B generate
        group,
        SUM(A.matched)  as matchedcount,
        A;
describe D;

E = filter D by matchedcount > 0;
describe E;

F = foreach E generate
        FLATTEN(A);
describe F;
dump F;
{code}

Here's the data filterbug.data
{code}
a       hello
a       goodbye
b       goodbye
c       hello
c       hello
c       hello
e       what
{code}


		

",,,,,,,,,,,,,,,,,,,28/Oct/09 19:17;rding;PIG-746.patch;https://issues.apache.org/jira/secure/attachment/12423482/PIG-746.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-04-02 17:32:39.749,,,no_permission,,,,,,,,,,,,164313,Reviewed,,,,Thu Oct 29 23:37:21 UTC 2009,,,,,,,0|i0gnuv:,95295,,,,,,,,,,"02/Apr/09 17:32;pkamath;The issue is that the combiner code does not handle projection of bags (because currently in the pig implementation, projection of bags is not combineable). The combiner optimizer code should identify projection of bags in the foreach following the group and in such cases decide not to use the combiner. So the quick fix is to disable the combiner for this (till projection of bags is made combineable).  THis can be done by supplying -Dpig.exec.nocombiner=true on the pig command line.","03/Apr/09 00:46;ciemo;I'd still like to use the combiner in other instances in my combined Pig scripts (I concatentate several pig scripts together to create compound Pig scripts).

It would be nice if Pig had a per statement option to turn off or force on the combiner.

In the mean time, I discovered a ""feature"" (flaw?) in Pig that turns off the combiner - perform a scalar operation (such as +0L) on the Algebraic aggregation function.

D = foreach B generate
        group,
        SUM(A.matched) + 0L  as matchedcount, -- +0L :flaw"" turns off combiner
        A;
describe D;

I have tried this workaround and it works, at least in the current version of Pig.  Until someone figures out how to permit use of the combiner for combined Algebraic and scalar  operations.",09/Apr/09 01:42;viraj;I think this issue is very similar to the one reported earlier,"28/Oct/09 19:17;rding;As Pradeep suggested, the combiner optimizer code in this patch identifies projection of bags in the foreach following the group and in such cases decides not to use the combiner. ","29/Oct/09 14:11;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12423482/PIG-746.patch
  against trunk revision 830757.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/127/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/127/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/127/console

This message is automatically generated.","29/Oct/09 23:37;pkamath;+1, patch committed - Thanks Richard!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect line number is generated when a string  with double quotes is used instead of single quotes and is passed to UDF,PIG-740,12421609,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,pradeepkth,viraj,viraj,31/Mar/09 01:38,17/Dec/10 22:43,14/Mar/19 03:05,03/May/10 16:22,0.3.0,,,,,,0.8.0,,grunt,,,0,,,,"Consider the Pig script with the error that a String with double quotes {code}""www\\.""{code} is used instead of a single quote {code}'www\\.'{code} in the UDF string.REPLACEALL()

{code}
register string-2.0.jar;
A = load 'inputdata' using PigStorage() as ( curr_searchQuery );

B = foreach A {
        domain = string.REPLACEALL(curr_searchQuery,""^www\\."",'');
        generate
        domain;
        };

dump B;
{code}

I get the following error message where ""Line 11"" points to the end of file. The error message should point to ""Line 5"".
===================================================================================================================================
2009-03-31 01:33:38,403 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://localhost:9000
2009-03-31 01:33:39,168 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to map-reduce job tracker at: localhost:9001
2009-03-31 01:33:39,589 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Lexical error at line 11, column 0.  Encountered: <EOF> after : """"
Details at logfile: /home/viraj/pig-svn/trunk/pig_1238463218046.log
===================================================================================================================================


The log file contains the following contents
===================================================================================================================================
ERROR 1000: Error during parsing. Lexical error at line 11, column 0.  Encountered: <EOF> after : """"
org.apache.pig.tools.pigscript.parser.TokenMgrError: Lexical error at line 11, column 0.  Encountered: <EOF> after : """"
        at org.apache.pig.tools.pigscript.parser.PigScriptParserTokenManager.getNextToken(PigScriptParserTokenManager.java:2739)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.jj_ntk(PigScriptParser.java:778)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:89)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:99)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:88)
        at org.apache.pig.Main.main(Main.java:352)
===================================================================================================================================",,,,,,,,,,,,,,,,,,,29/Apr/10 21:25;pradeepkth;PIG-740.patch;https://issues.apache.org/jira/secure/attachment/12443236/PIG-740.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-04-29 21:25:05.095,,,no_permission,,,,,,,,,,,,164307,Reviewed,,,,Mon May 03 16:22:30 UTC 2010,,,,,,,0|i0gnsf:,95284,,,,,,,,,,29/Apr/10 21:25;pradeepkth;GruntParser was not handling double quotes within foreach blocks correctly by incorrectly treating them the same way as single quote for the starting double quote and not handling the end double quote - the patch addresses the bug by treating double quotes correctly.,30/Apr/10 18:47;daijy;+1,"30/Apr/10 22:16;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12443236/PIG-740.patch
  against trunk revision 939727.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 139 javac compiler warnings (more than the trunk's current 138 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/310/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/310/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/310/console

This message is automatically generated.",03/May/10 16:22;pradeepkth;The javac warning is due to generated javacc code and cannot be avoided. I ran all unit tests on my local machine and they passed - patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regexp passed from pigscript fails in UDF  ,PIG-738,12421472,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,viraj,viraj,28/Mar/09 09:12,24/Mar/10 22:15,14/Mar/19 03:05,22/Sep/09 17:33,0.3.0,,,,,,0.6.0,,grunt,,,0,,,,"Consider a pig script which parses and counts regular expressions from a text file. 
The regular expression supplied in the Pig script needs to escape the "".""  (dot) character.

{code}
register myregexp.jar;

-- pattern not picked up

define minelogs ci_pig_udfs.RegexGroupCount('www\\.yahoo\\.com/sports');

A = load '/user/viraj/regexpinput.txt'  using PigStorage() as (source : chararray);

B = foreach A generate minelogs(source) as sportslogs;

dump B;

{code}

Snippet of UDF RegexGroupCount.java

{code}

public class RegexGroupCount extends EvalFunc<Integer> {

    private final Pattern pattern_;

    public RegexGroupCount(String patternStr) {

       System.out.println(""My pattern supplied is ""+patternStr);

       System.out.println(""Equality test ""+patternStr.equals(""www\\.yahoo\\.com/sports""));

       pattern_ = Pattern.compile(patternStr, Pattern.DOTALL|Pattern.CASE_INSENSITIVE);

   }
  public Integer exec(Tuple input)  throws IOException {
   }
}
{code}
Running the above script on the following dataset :
====================================================================================================
dshfdskfwww.yahoo.com/sportsjoadfjdslpdshfdskfwww.yahoo.com/sportsjoadfjdsl
kas;dka;sd
jsjsjwww.yahoo.com/sports
jsdLSJDcom/sports
wwwJyahooMcom/sports
====================================================================================================

Results in the following:

My pattern supplied is www\\.yahoo\\.com/sports
Equality test false
My pattern supplied is www\\.yahoo\\.com/sports
Equality test false
My pattern supplied is www\\.yahoo\\.com/sports
Equality test false
My pattern supplied is www\\.yahoo\\.com/sports
Equality test false
My pattern supplied is www\\.yahoo\\.com/sports
Equality test false
My pattern supplied is www\\.yahoo\\.com/sports
Equality test false
Userfunc: (Name: UserFunc viraj-Sat Mar 28 02:06:31 PDT 2009-14 function: ci_pig_udfs.RegexGroupCount('www\\.yahoo\\.com/sports') Operator Key: viraj-Sat Mar 28 02:06:31 PDT 2009-14)
Userfunc fs: int
My pattern supplied is www\\.yahoo\\.com/sports
Equality test false
My pattern supplied is www\\.yahoo\\.com/sports
Equality test false
My pattern supplied is www\\.yahoo\\.com/sports
Equality test false

2009-03-28 02:06:43,923 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete
2009-03-28 02:06:43,923 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!
(0)
(0)
(0)
(0)
(0)
====================================================================================================

In essence there seems to be no way of passing this type of constructor argument through the Pig script. The only workaround seems to be hard coding the values in the UDF!!
",,,,,,,,,,,,,,,,,,,22/Sep/09 01:29;pkamath;PIG-738.patch;https://issues.apache.org/jira/secure/attachment/12420246/PIG-738.patch,28/Mar/09 09:15;viraj;RegexGroupCount.java;https://issues.apache.org/jira/secure/attachment/12404048/RegexGroupCount.java,28/Mar/09 09:15;viraj;myregexp.jar;https://issues.apache.org/jira/secure/attachment/12404049/myregexp.jar,28/Mar/09 09:16;viraj;regexp.pig;https://issues.apache.org/jira/secure/attachment/12404051/regexp.pig,28/Mar/09 09:15;viraj;regexpinput.txt;https://issues.apache.org/jira/secure/attachment/12404050/regexpinput.txt,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2009-03-30 13:14:11.18,,,no_permission,,,,,,,,,,,,164305,Reviewed,,,,Tue Sep 22 17:33:59 UTC 2009,,,,,,,0|i0gnrj:,95280,,,,,,,,,,"28/Mar/09 09:15;viraj;Java,Jar for UDF & Input file",28/Mar/09 09:16;viraj;Pig script,"30/Mar/09 13:14;mridul;
As a workaround, you can use the hex code for '.' within your script.
Like : 'www\u002eyahoo\u002ecom/sports' instead of trying to escape '.'.

But yes, the basic problem seems to exist.","31/Mar/09 01:44;viraj;This works, as the Pig parser ignores single front slash (\) followed by u ""\u"" ","26/May/09 23:00;milindb;This is not an issue in Pig parsing. The Definition of a quoted string in the pig parser is:

{code}
TOKEN : { <QUOTEDSTRING :  ""'""
(   (~[""'"",""\\"",""\n"",""\r""])
  | (""\\""
      ( [""n"",""t"",""b"",""r"",""f"",""\\"",""'""] )
    )
  | (""\\u""
        [""0""-""9"",""A""-""F"",""a""-""f""]
        [""0""-""9"",""A""-""F"",""a""-""f""]
        [""0""-""9"",""A""-""F"",""a""-""f""]
        [""0""-""9"",""A""-""F"",""a""-""f""]
    )
)*
""'""> }
{code}

It matches: 
{code}
'www\\.yahoo\\.com/sports'
{code}.

So, its the stages after parsing that the regexp is getting messed up. (My suspicion is FuncSpec serialization.)",22/Sep/09 01:29;pkamath;Attached patch which will allow define statements to treat strings the same way as strings are treated elsewhere in a pig script. So www\\..xyz\\.com will be treated as a java string and will be sent a regex pattern where the '.' is escaped by a backslash to ensure it is not treated as the match-all character class. The extra backslash is to escape the backslash itself.,"22/Sep/09 05:14;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12420246/PIG-738.patch
  against trunk revision 817319.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/42/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/42/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/42/console

This message is automatically generated.","22/Sep/09 15:31;olgan;+1, please, commit",22/Sep/09 17:33;pkamath;Patch committed to trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Non-string keys in maps,PIG-734,12419419,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,alangates,alangates,alangates,25/Mar/09 20:02,24/Mar/10 22:13,14/Mar/19 03:05,24/Jun/09 00:48,0.2.0,,,,,,0.4.0,,,,,0,,,,"With the addition of types to pig, maps were changed to allow any atomic type to be a key.  However, in practice we do not see people using keys other than strings.  And allowing multiple types is causing us issues in serializing data (we have to check what every key type is) and in the design for non-java UDFs (since many scripting languages include associative arrays such as Perl's hash).

So I propose we scope back maps to only have string keys.  This would be a non-compatible change.  But I am not aware of anyone using non-string keys, so hopefully it would have little or no impact.",,,,,,,,,,,,,,,,,,,07/May/09 23:06;alangates;PIG-734.patch;https://issues.apache.org/jira/secure/attachment/12407585/PIG-734.patch,18/Jun/09 20:32;alangates;PIG-734_2.patch;https://issues.apache.org/jira/secure/attachment/12411133/PIG-734_2.patch,18/Jun/09 23:20;alangates;PIG-734_3.patch;https://issues.apache.org/jira/secure/attachment/12411160/PIG-734_3.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-05-07 21:09:11.432,,,no_permission,,,,,,,,,,,,164301,,,,,Wed Jun 24 13:06:28 UTC 2009,,,,,,,0|i0gnpr:,95272,,,,,,,,,,"07/May/09 21:09;yhan;I don't get the serializing part. I would expect the type-checking just happen once, would that be a performance problem?

Actually we are thinking if we should switch to integer key to save space.

I wouldn't post strong against to this rollback, but I don't see a significant reason for dong that.","07/May/09 23:09;alangates;For serialization, a type discovery has to happen on every key, because there's no guarantee that every key is of the same type.  By forcing all keys to strings we're eliminating this step, thus speeding the serialization significantly.",07/May/09 23:15;yhan;Then why not just to restrict all the keys to be of the same type? I don't see the point that different records should have different key types. But I do see the point that people may want to use non-string type of keys.,"08/May/09 00:38;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12407585/PIG-734.patch
  against trunk revision 772750.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 63 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 228 javac compiler warnings (more than the trunk's current 225 warnings).

    -1 findbugs.  The patch appears to introduce 7 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/32/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/32/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/32/console

This message is automatically generated.","08/May/09 00:44;alangates;Changing maps to allow the user to specify a type would be significantly more work.  Also, it would be a bigger disruption in the interface, since users would now have to declare a key type.  We could say the type default to string if the user doesn't declare a type.

Nothing prevents us from eventually supporting other declared types for the key, with string as the default.  However, as noted in the original description, it will make types translations in non-java UDFs much more painful, so it's not clear to me that we would want to do it.","08/May/09 15:30;ciemo;Alan, if I don't think this is going to be that problematic.

Even if I try to pass in a map dereference with an integer such as mymap#1, would pig automagically convert the 1 to a string equivalent to mymap#'1'.  If so, I think this would be quite acceptable.","08/May/09 15:44;alangates;I wasn't planning on making mymap#1 translate to mymap#'1'.  The issue I see with that is if that works, why doesn't mymap#intcol work?  I'm concerned that sayings keys need to be strings but then cheating in this one case will make the semantics confusing.","18/Jun/09 20:32;alangates;New version of the patch, brought up to date with current trunk.","18/Jun/09 22:16;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12411133/PIG-734_2.patch
  against trunk revision 785450.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 63 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 2 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/92/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/92/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/92/console

This message is automatically generated.","18/Jun/09 23:20;alangates;Attaching a version of the file that fixes some of the introduced compiler warnings.  The findbugs warnings have to do with naming convention.  All of the function names in QueryParser start with upper case, so I am only following the convention there.","19/Jun/09 01:06;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12411160/PIG-734_3.patch
  against trunk revision 785450.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 63 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 2 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/93/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/93/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/93/console

This message is automatically generated.",23/Jun/09 23:11;daijy;Patch looks good to me.,24/Jun/09 00:48;alangates;Patch checked in.,"24/Jun/09 13:06;hudson;Integrated in Pig-trunk #484 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/484/])
    :  Changed maps to only take strings as keys.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Order by sampling dumps entire sample to hdfs which causes dfs ""FileSystem closed"" error on large input",PIG-733,12419415,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,25/Mar/09 19:04,24/Mar/10 22:10,14/Mar/19 03:05,10/Apr/09 02:36,0.2.0,,,,,,0.3.0,,,,,0,,,,"Order by has a sampling job which samples the input and creates a sorted list of sample items. CUrrently the number of items sampled is 100 per map task. So if the input is large resulting in many maps (say 50,000) the sample is big. This sorted sample is stored on dfs. The WeightedRangePartitioner computes quantile boundaries and weighted probabilities for repeating values in each map by reading the samples file from DFS. In queries with many maps (in the order of 50,000) the dfs read of the sample file fails with ""FileSystem closed"" error. This seems to point to a dfs issue wherein a big dfs file being read simultaneously by many dfs clients (in this case all maps) causes the clients to be closed. However on the pig side, loading the sample from each map in the final map reduce job and computing the quantile boundaries and weighted probabilities is inefficient. We should do this computation through a FindQuantiles udf in the same map reduce job which produces the sorted samples. This way lesser data is written to dfs and in the final map reduce job, the weightedRangePartitioner needs to just load the computed information.",,,,,,,,,,,,,,,,,,,06/Apr/09 20:58;pkamath;PIG-733-v2.patch;https://issues.apache.org/jira/secure/attachment/12404769/PIG-733-v2.patch,01/Apr/09 23:33;pkamath;PIG-733.patch;https://issues.apache.org/jira/secure/attachment/12404402/PIG-733.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-04-02 01:12:34.365,,,no_permission,,,,,,,,,,,,164300,,,,,Fri Apr 10 02:36:14 UTC 2009,,,,,,,0|i0gnpb:,95270,,,,,,,,,,01/Apr/09 23:32;pkamath;Attached patch which implements the fix described in the description of the issue,"02/Apr/09 01:12;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12404402/PIG-733.patch
  against trunk revision 759376.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 207 javac compiler warnings (more than the trunk's current 200 warnings).

    -1 findbugs.  The patch appears to introduce 5 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/18/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/18/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/18/console

This message is automatically generated.","06/Apr/09 20:53;pkamath;Tests are not included in this patch since there are existing tests for order by.

All core unit tests did pass and finbugs gave the same number of warnings with and without the patch (output below). The excess warnings produced by the patch have been addressed in the new version of the patch (PIG-733-v2.patch).

{noformat}
=== CORE UNIT TESTS OUTPUT WITH PATCH====
[pradeepk@afterside:/tmp/PIG-733/trunk]


test-core:
    [mkdir] Created dir: /tmp/PIG-733/trunk/build/test/logs
    [junit] Running org.apache.pig.test.TestAdd
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.056 sec
...
    [junit] Running org.apache.pig.test.TestTypeCheckingValidatorNoSchema
    [junit] Tests run: 13, Failures: 0, Errors: 0, Time elapsed: 0.629 sec
    [junit] Running org.apache.pig.test.TestUnion
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 49.94 sec

test-contrib:

BUILD SUCCESSFUL
Total time: 77 minutes 47 seconds

=== FINDBUGS OUTPUT WITH PATCH====
[pradeepk@afterside:/tmp/PIG-733/trunk]

[pradeepk@chargesize:/tmp/PIG-733/trunk]ant -Dfindbugs.home=/homes/pradeepk/findbugs-1.3.8 findbugs
Buildfile: build.xml
...
findbugs:
    [mkdir] Created dir: /tmp/PIG-733/trunk/build/test/findbugs
 [findbugs] Executing findbugs from ant task
 [findbugs] Running FindBugs...
 [findbugs] Warnings generated: 665
 [findbugs] Calculating exit code...
 [findbugs] Setting 'bugs found' flag (1)
 [findbugs] Exit code set to: 1
 [findbugs] Java Result: 1
 [findbugs] Output saved to /tmp/PIG-733/trunk/build/test/findbugs/pig-findbugs-report.xml
     [xslt] Processing /tmp/PIG-733/trunk/build/test/findbugs/pig-findbugs-report.xml to /tmp/PIG-733/trunk/build/test/findbugs/pig-findbugs-report.html
     [xslt] Loading stylesheet /homes/pradeepk/findbugs-1.3.8/src/xsl/default.xsl

=== FINDBUGS OUTPUT WITHOUT PATCH====
[pradeepk@chargesize:/tmp/svncheckout/trunk]ant -Dfindbugs.home=/homes/pradeepk/findbugs-1.3.8 findbugs
Buildfile: build.xml

check-for-findbugs:

...
findbugs:
    [mkdir] Created dir: /tmp/svncheckout/trunk/build/test/findbugs
 [findbugs] Executing findbugs from ant task
 [findbugs] Running FindBugs...
 [findbugs] Warnings generated: 665
 [findbugs] Calculating exit code...
 [findbugs] Setting 'bugs found' flag (1)
 [findbugs] Exit code set to: 1
 [findbugs] Java Result: 1
 [findbugs] Output saved to /tmp/svncheckout/trunk/build/test/findbugs/pig-findbugs-report.xml
     [xslt] Processing /tmp/svncheckout/trunk/build/test/findbugs/pig-findbugs-report.xml to /tmp/svncheckout/trunk/build/test/findbugs/pig-findbugs-report.html
     [xslt] Loading stylesheet /homes/pradeepk/findbugs-1.3.8/src/xsl/default.xsl



{noformat}",07/Apr/09 06:11;gkesavan;I'm going to resubmit the patch to hudson .. ,"07/Apr/09 09:03;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12404769/PIG-733-v2.patch
  against trunk revision 759376.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 5 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/21/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/21/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/21/console

This message is automatically generated.","07/Apr/09 15:42;olgan;The patch process still seems broken - Giri could you please investigate.

Pradeep, since the patch has been sufficiently tested and passes outside of hudson, please, go ahead and commit it.",10/Apr/09 02:36;pkamath;patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"problem combining schema from a union of several LOAD expressions, with a nested bag inside the schema.",PIG-730,12419342,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,olston,olston,24/Mar/09 19:36,04/Aug/11 00:34,14/Mar/19 03:05,24/Jan/11 18:41,,,,,,,0.9.0,,,,,1,,,,"grunt> a = load 'foo' using BinStorage as (url:chararray,outlinks:{t:(target:chararray,text:chararray)});
grunt> b = union (load 'foo' using BinStorage as (url:chararray,outlinks:{t:(target:chararray,text:chararray)})), (load 'bar' using BinStorage as (url:chararray,outlinks:{t:(target:chararray,text:chararray)}));
grunt> c = foreach a generate flatten(outlinks.target);
grunt> d = foreach b generate flatten(outlinks.target);

---> Would expect both C and D to work, but only C works. D gives the error shown below.
---> Turns out using outlinks.t.target (instead of outlinks.target) works for D but not for C.
---> I don't care which one, but the same syntax should work for both!

2009-03-24 13:15:05,376 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Invalid alias: target in {t: (target: chararray,text: chararray)}
Details at logfile: /echo/olston/data/pig_1237925683748.log
grunt> quit

$ cat pig_1237925683748.log 
ERROR 1000: Error during parsing. Invalid alias: target in {t: (target: chararray,text: chararray)}
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1000: Error during parsing. Invalid alias: target in {t: (target: chararray,text: chararray)}
        at org.apache.pig.PigServer.parseQuery(PigServer.java:317)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:276)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:529)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:280)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:99)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:69)
        at org.apache.pig.Main.main(Main.java:321)
Caused by: org.apache.pig.impl.logicalLayer.parser.ParseException: Invalid alias: target in {t: (target: chararray,text: chararray)}
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.AliasFieldOrSpec(QueryParser.java:6042)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.ColOrSpec(QueryParser.java:5898)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BracketedSimpleProj(QueryParser.java:5423)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseEvalSpec(QueryParser.java:4100)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.UnaryExpr(QueryParser.java:3967)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.CastExpr(QueryParser.java:3920)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.MultiplicativeExpr(QueryParser.java:3829)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.AdditiveExpr(QueryParser.java:3755)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.InfixExpr(QueryParser.java:3721)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItem(QueryParser.java:3617)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItemList(QueryParser.java:3557)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.GenerateStatement(QueryParser.java:3514)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.NestedBlock(QueryParser.java:2985)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.ForEachClause(QueryParser.java:2395)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:1028)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:804)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:595)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:60)
        at org.apache.pig.PigServer.parseQuery(PigServer.java:310)
        ... 6 more
",pig local mode,,,,,,,,,,,,,,,,,,10/Jan/11 22:07;daijy;PIG-730-1.patch;https://issues.apache.org/jira/secure/attachment/12467927/PIG-730-1.patch,24/Jan/11 18:38;daijy;PIG-730-2.patch;https://issues.apache.org/jira/secure/attachment/12469183/PIG-730-2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-03-27 22:19:08.38,,,no_permission,,,,,,,,,,,,66211,Reviewed,,,,Mon Jan 24 18:41:02 UTC 2011,,,,,,,0|i0gnnz:,95264,,,,,,,,,,27/Mar/09 22:19;sms;This issue looks like a duplicate of PIG-694,"10/Jan/11 21:53;daijy;This is because after mergeSchema(union will call mergeSchema), we lose twoLevelAccess flag. PIG-730-1.patch brings it back.",18/Jan/11 20:05;rding;+1,24/Jan/11 18:38;daijy;PIG-730-2.patch resync with current trunk.,"24/Jan/11 18:41;daijy;Review notes:
https://reviews.apache.org/r/273/

Patch committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
All backend error messages must be logged to preserve the original error messages,PIG-728,12417333,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,sms,sms,sms,20/Mar/09 00:12,24/Mar/10 22:13,14/Mar/19 03:05,17/Jul/09 01:03,0.3.0,,,,,,0.4.0,,,,,0,,,,"The current error handling framework logs backend error messages only when Pig is not able to parse the error message. Instead, Pig should log the backend error message irrespective of Pig's ability to parse backend error messages. On a side note, the use of instantiateFuncFromSpec in Launcher.java is not consistent and should avoid the use of class_name + ""("" + string_constructor_args + "")"".",,,,,,,,,,PIG-736,,,,,,,,,16/Jul/09 17:43;sms;PIG-728_1.patch;https://issues.apache.org/jira/secure/attachment/12413711/PIG-728_1.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-07-16 01:13:29.835,,,no_permission,,,,,,,,,,,,164298,Reviewed,,,,Fri Jul 17 12:10:45 UTC 2009,,,,,,,0|i0gnn3:,95260,,,,,,,,,,"15/Jun/09 16:13;sms;In addition, when the framework is not able to parse the error message, the message should be annotated as such. Extraneous details like ""Unable to recreate exception"", ""Cannot create exception from empty string"", etc should not be communicated to the user. These messages reflect internal workings of the error handling framework and do not add value to the user.","15/Jul/09 22:47;sms;The attached patch logs all backend error messages before Pig tries to parse the messages. In addition, the log format has been cleaned up to be more user friendly. No new test cases have been added.","16/Jul/09 01:13;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12413606/PIG-728.patch
  against trunk revision 793750.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/127/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/127/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/127/console

This message is automatically generated.",16/Jul/09 17:43;sms;Attaching a new patch that fixes the findbugs issue.,"16/Jul/09 22:34;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12413711/PIG-728_1.patch
  against trunk revision 793750.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/131/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/131/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/131/console

This message is automatically generated.",16/Jul/09 23:42;alangates;+1,17/Jul/09 01:03;sms;Patch has been committed.,17/Jul/09 01:03;sms;Issue has been resolved.,"17/Jul/09 12:10;hudson;Integrated in Pig-trunk #506 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/506/])
    : All backend error messages must be logged to preserve the original error messages
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Treating map values in PigStorage,PIG-724,12417225,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,sms,sms,18/Mar/09 22:25,24/Mar/10 22:10,14/Mar/19 03:05,31/Jul/09 21:16,0.2.0,,,,,,0.3.0,,impl,,,0,,,,"Currently, PigStorage cannot treats the materialized string 123 as an integer with the value 123. If the user intended this to be the string 123, PigStorage cannot deal with it. This reasoning also applies to doubles. Due to this issue, maps that contain values which are of the same type but manifest the issue discussed at beginning of the paragraph, Pig throws its hands up at runtime.  An example to illustrate the problem will help.

In the example below a sample row in the data (map.txt) contains the following:

[key01#35,key02#value01]

When Pig tries to convert the stream to a map, it creates a Map<Object, Object> where the key is a string and the value is an integer. Running the script shown below, results in a run-time error.

{code}
grunt> a = load 'map.txt' as (themap: map[]);                    
grunt> b = filter a by (chararray)(themap#'key01') == 'hello';                  
grunt> dump b;

2009-03-18 15:19:03,773 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete
2009-03-18 15:19:28,797 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Map reduce job failed
2009-03-18 15:19:28,817 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1081: Cannot cast to chararray. Expected bytearray but received: int
{code} 

There are two ways to resolve this issue:

1. Change the conversion routine for bytesToMap to return a map where the value is a bytearray and not the actual type. This change breaks backward compatibility
2. Introduce checks in POCast where conversions that are legal in the type checking world are allowed, i.e., run time checks will be made to check for compatible casts. In the above example, an int can be converted to a chararray and the cast will be made. If on the other hand, it was a chararray to int conversion then an exception will be thrown.",,,,,,,,,,,,,PIG-880,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-04-04 12:04:49.47,,,no_permission,,,,,,,,,,,,164294,Incompatible change,,,,Fri Jul 31 21:16:36 UTC 2009,,,,,,,0|i0gnlb:,95252,,,,,,,,,,"04/Apr/09 12:04;zjffdu;In my opinion, it would be better if we can explicit declare the type of key and value of map.

like this statement:

A = LOAD 'file:data/c.txt' USING PigStorage() AS (themap: map[key:chararray#value:chararray]);

","08/Apr/09 15:16;alangates;Currently Pig doesn't require that all keys and values in a map share the same type.  There is a proposal to change it so that key types can only be chararray (see PIG-734), as we don't see anyone using anything but chararray and the generality is causing us some other issues.  But we still wouldn't require that all values in a given map be of the same type.  Are you proposing allowing users to put a constraint on a given map so that all values in that particular map must be of that type?",31/Jul/09 21:16;sms;Issue fixed as part of PIG-880,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
puttin in overview.html file to a more appropriate locaiton,PIG-720,12416997,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,gkesavan,gkesavan,gkesavan,16/Mar/09 20:05,25/Mar/10 00:12,14/Mar/19 03:05,17/Mar/09 01:21,,,,,,,0.3.0,,build,,,0,,,,overview.html file will now get into src/overview.html as in hadoop.,,,,,,,,,,,,,,,,,,,16/Mar/09 20:55;gkesavan;PIG-720.patch;https://issues.apache.org/jira/secure/attachment/12402315/PIG-720.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-03-16 20:21:50.503,,,no_permission,,,,,,,,,,,,164291,,,,,Tue Mar 17 01:21:52 UTC 2009,,,,,,,0|i0gnjr:,95245,,,,,,,,,,16/Mar/09 20:21;olgan;please provide the file that you want to be in that location.,"16/Mar/09 20:55;gkesavan;here is the patch.. 

after applying the patch please do an 
svn rm docs

and then we can commit the code as we do it
","16/Mar/09 23:48;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12402315/PIG-720.patch
  against trunk revision 754716.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.

    -1 javac.  The applied patch generated 5 javac compiler warnings (more than the trunk's current 206 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 82 release audit warnings (more than the trunk's current 46 warnings).

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/7/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/7/artifact/trunk/current/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/7/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/7/console

This message is automatically generated.","17/Mar/09 00:02;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12402315/PIG-720.patch
  against trunk revision 754716.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 82 release audit warnings (more than the trunk's current 46 warnings).

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/8/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/8/artifact/trunk/current/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/8/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/8/console

This message is automatically generated.","17/Mar/09 01:21;olgan;patch committed. thanks, Giri!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"store <expr> into 'filename'; should be valid syntax, but does not work",PIG-719,12416852,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,xuefuz,olston,olston,13/Mar/09 17:11,04/Aug/11 00:34,14/Mar/19 03:05,14/Mar/11 17:12,,,,,,,0.9.0,,,,,0,,,,"This pig script should work:

STORE (LOAD 'inputfile') INTO 'outputfile';

but it does not.
","pig local model (although I think it's a parsing problem, not an execution problem)",,,,,,,,,,,,,,,,PIG-1618,,11/Mar/11 00:52;xuefuz;pig-719.patch;https://issues.apache.org/jira/secure/attachment/12473355/pig-719.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-11 00:52:32.162,,,no_permission,,,,,,,,,,,,66269,,,,,Mon Mar 14 17:12:28 UTC 2011,,,,,,,0|i0gnjb:,95243,,,,,,,,,,"11/Mar/11 00:52;xuefuz;1. provide fix for the issue
2. Minor error message reporting change.","11/Mar/11 17:49;xuefuz;Unit test passed. Test-patch run:

     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
",11/Mar/11 17:54;thejas;+1,14/Mar/11 17:12;thejas;Patch committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove 2 doc files: hello.pdf and overview.html,PIG-715,12416780,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,chandec,chandec,chandec,12/Mar/09 19:31,25/Mar/10 00:12,14/Mar/19 03:05,13/Mar/09 20:23,,,,,,,0.2.0,,documentation,,,0,,,,"Please remove these 2 doc files. They don't belong with the Pig 2.0 documnetation and will cause confusion.

(1) hello.pdf ... located in:

trunk/src/docs/src/documentaiton/content/xdocs

(2) overview.html ... located in:

trunk/docs",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-03-13 20:23:20.321,,,no_permission,,,,,,,,,,,,164288,,,,,Mon Mar 16 20:03:25 UTC 2009,,,,,,,0|i0gnhr:,95236,,,,,,,,,,13/Mar/09 20:23;olgan;changes applied,"13/Mar/09 23:40;gkesavan;this overview.html is used by javadocs target to generate the api docs and it looks like javadoc target show error output.

[exec]   [javadoc] /home/hudson/hudson-slave/workspace/Pig-Patch-minerva.apache.org/trunk/src/org/apache/pig/impl/logicalLayer/schema/Schema.java:1578: warning - @param argument ""schema"" is not a parameter name.
[exec]   [javadoc] /home/hudson/hudson-slave/workspace/Pig-Patch-minerva.apache.org/trunk/src/org/apache/pig/impl/plan/OperatorPlan.java:189: warning - Tag @link: reference not found: insertBetween
[exec]   [javadoc] Building index for all the packages and classes...
[exec]   [javadoc] Building index for all classes...
[exec]   [javadoc] javadoc: error - Error while reading file /home/hudson/hudson-slave/workspace/Pig-Patch-minerva.apache.org/trunk/docs/overview.html
[exec]   [javadoc] Generating /home/hudson/hudson-slave/workspace/Pig-Patch-minerva.apache.org/trunk/build/docs/api/stylesheet.css...
[exec]   [javadoc] 1 error",14/Mar/09 00:19;olgan;do you need me to put it back in?,"16/Mar/09 20:03;gkesavan;I'm going to file a different jira and put this overview.html file in location exctly as hadoop does.
tnx
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig should display a better error message when backend error messages cannot be parsed,PIG-705,12416359,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,sms,sms,07/Mar/09 05:23,24/Mar/10 22:04,14/Mar/19 03:05,09/Mar/09 22:22,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"If the backend error message cannot be parsed correctly, Pig displays an error message indicating that there was an internal error. In addition, the original error message is lost. Pig should display a better error message and display the relevant part of the backend error message.",,,,,,,,,,,,,,,,,,,09/Mar/09 15:15;sms;PIG-705.patch;https://issues.apache.org/jira/secure/attachment/12401756/PIG-705.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-03-09 17:31:12.574,,,no_permission,,,,,,,,,,,,38481,Reviewed,,,,Mon Mar 09 22:22:58 UTC 2009,,,Patch Available,,,,0|i0gndr:,95218,,,,,,,,,,09/Mar/09 15:15;sms;Attached patch fixes the issue of capturing the first line of the backend error message when the backend message cannot be parsed by PIG. There are no unit test cases. Testing was performed manually.,"09/Mar/09 17:31;olgan;+1; please, commit",09/Mar/09 22:22;sms;Patch has been committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simple join fails on records not loaded with schema,PIG-698,12416256,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,ciccolo,ciccolo,05/Mar/09 22:29,13/Sep/10 22:05,14/Mar/19 03:05,13/Sep/10 22:02,,,,,,,0.7.0,,impl,,,2,,,,"Joins can fail with an out-of-bounds access to fields that are not referenced in the script when records without schema (including all variable-length records) are involved.
Example by Ben Reed:
i1:
1       c       D       E
1       a       B

i2:
0
0       Q
1       x       z
1       a       b       c


i1 = load 'i1';                                                                                                                                                                        
i2 = load 'i2';                                                                                                                                                                        
j = join i1 by $0, i2 by $0;                                                                                                                                                           
dump j",Yahoo! clusters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-09-13 22:02:59.624,,,no_permission,,,,,,,,,,,,164272,,,,,Mon Sep 13 22:02:59 UTC 2010,,,,,,,0|i0gnav:,95205,,,,,,,,,,13/Sep/10 22:02;alangates;Tested this against version 0.7 and it works fine.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Proposed improvements to pig's optimizer,PIG-697,12416048,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,alangates,alangates,04/Mar/09 01:07,25/Mar/10 00:12,14/Mar/19 03:05,07/Aug/09 17:20,,,,,,,0.4.0,,impl,,,0,,,,"I propose the following changes to pig optimizer, plan, and operator functionality to support more robust optimization:

1) Remove the required array from Rule.  This will change rules so that they only match exact patterns instead of allowing missing elements in the pattern.
This has the downside that if a given rule applies to two patterns (say Load->Filter->Group, Load->Group) you have to write two rules.  But it has the upside that
the resulting rules know exactly what they are getting.  The original intent of this was to reduce the number of rules that needed to be written.  But the
resulting rules have do a lot of work to understand the operators they are working with.  With exact matches only, each rule will know exactly the operators it
is working on and can apply the logic of shifting the operators around.  All four of the existing rules set all entries of required to true, so removing this
will have no effect on them.

2) Change PlanOptimizer.optimize to iterate over the rules until there are no conversions or a certain number of iterations has been reached.  Currently the
function is:

{code}
    public final void optimize() throws OptimizerException {
        RuleMatcher matcher = new RuleMatcher();
        for (Rule rule : mRules) {
            if (matcher.match(rule)) {
                // It matches the pattern.  Now check if the transformer
                // approves as well.
                List<List<O>> matches = matcher.getAllMatches();
                for (List<O> match:matches)
                {
	                if (rule.transformer.check(match)) {
	                    // The transformer approves.
	                    rule.transformer.transform(match);
	                }
                }
            }
        }
    }
{code}

It would change to be:
{code}
    public final void optimize() throws OptimizerException {
        RuleMatcher matcher = new RuleMatcher();
        boolean sawMatch;
        int iterators = 0;
        do {
            sawMatch = false;
            for (Rule rule : mRules) {
                List<List<O>> matches = matcher.getAllMatches();
                for (List<O> match:matches) {
                    // It matches the pattern.  Now check if the transformer
                    // approves as well.
                    if (rule.transformer.check(match)) {
                        // The transformer approves.
                        sawMatch = true;
                        rule.transformer.transform(match);
                    }
                }
            }
            // Not sure if 1000 is the right number of iterations, maybe it
            // should be configurable so that large scripts don't stop too 
            // early.
        } while (sawMatch && numIterations++ < 1000);
    }
{code}

The reason for limiting the number of iterations is to avoid infinite loops.  The reason for iterating over the rules is so that each rule can be applied multiple
times as necessary.  This allows us to write simple rules, mostly swaps between neighboring operators, without worrying that we get the plan right in one pass.
For example, we might have a plan that looks like:  Load->Join->Filter->Foreach, and we want to optimize it to Load->Foreach->Filter->Join.  With two simple
rules (swap filter and join and swap foreach and filter), applied iteratively, we can get from the initial to final plan, without needing to understanding the
big picture of the entire plan.

3) Add three calls to OperatorPlan:

{code}
/**
 * Swap two operators in a plan.  Both of the operators must have single
 * inputs and single outputs.
 * @param first operator
 * @param second operator
 * @throws PlanException if either operator is not single input and output.
 */
public void swap(E first, E second) throws PlanException {
    ...
}

/**
 * Push one operator in front of another.  This function is for use when
 * the first operator has multiple inputs.  The caller can specify
 * which input of the first operator the second operator should be pushed to.
 * @param first operator, assumed to have multiple inputs.
 * @param second operator, will be pushed in front of first
 * @param inputNum, indicates which input of the first operator the second
 * operator will be pushed onto.  Numbered from 0.
 * @throws PlanException if inputNum does not exist for first operator
 */
public void pushBefore(E first, E second, int inputNum) throws PlanException {
    ...
}

/**
 * Push one operator after another.  This function is for use when the second
 * operator has multiple outputs.  The caller can specify which output of the
 * second operator the first operator should be pushed to.
 * @param first operator, will be pushed after the second operator
 * @param second operator, assumed to have multiple outputs
 * @param outputNum indicates which output of the second operator the first 
 * operator will be pushed onto.  Numbered from 0.
 * @throws PlanException if outputNum does not exist for second operator
 */
public void pushAfter(E first, E second, int outputNum) throws PlanException {
    ...
}
{code}

The rules in the optimizer can use these three functions, along with the existing insertBetween(), replace(), and removeAndReconnect() calls to operate on the
plan.

4) Add a new call to Operator:

{code}
/**
 * Make any necessary changes to a node based on a change of position in the
 * plan.  This allows operators to rewire their projections, etc. when they
 * are relocated in a plan.
 * @param oldPred Operator that was previously the predecessor.
 * @param newPred Operator thwas will now be the predecessor.
 * @throws PlanException
 */
public abstract void rewire(Operator oldPred, Operator newPred) throws PlanException;
{code}

This method will be called by the swap, pushBefore, pushAfter, insertBetween, replace, and removeAndReconnect in OperatorPlan whenever an operator is moved
around so that the operator has a chance to make any necessary changes.  

5) Add new calls to LogicalOperator and PhysicalOperator

{code}
/**
 * A struct detailing how a projection is altered by an operator.
 */
public class ProjectionMap {
    /**
     * Quick way for an operator to note that its input and output are the
     * same.
     */
    public boolean noChange;

    /**
     * Map of field changes, with keys being the output fields of the 
     * operator and values being the input fields.  Fields are numbered from
     * 0.  So for a foreach operator derived from
     * 'B = foreach A generate $0, $2, $3, udf($1)' 
     * would produce a mapping of 0->0, 1->2, 2->3
     */
    public Map<Integer, Integer> mappedFields;

    /**
     * List of fields removed from the input.  This includes fields that were
     * transformed, and thus are no longer the same fields.  Using the
     * example foreach given under mappedFields, this list would contain '1'.
     */
    public List<Integer> removedFields;

    /**
     * List of fields in the output of this operator that were created by this
     * operator.  Using the example foreach given under mappedFields, this list
     * would contain '3'.
     */
    public List<Integer> addedFields;
}

/**
 * Produce a map describing how this operator modifies its projection.
 * @returns ProjectionMap null indicates it does not know how the projection
 * changes, for example a join of two inputs where one input does not have
 * a schema.
 */
public abstract ProjectionMap getProjectionMap();

/**
 * Get a list of fields that this operator requires.  This is not necessarily
 * equivalent to the list of fields the operator projects.  For example,
 * a filter will project anything passed to it, but requires only the fields
 * explicitly referenced in its filter expression.
 * @return list of fields, numbered from 0.
 */
public abstract List<Integer> getRequiredFields();
{code}

These calls will be called by optimizer rules to determine whether or not a swap can be done (for example, you can't swap two operators if the second one uses a
field added by the first), and once the swap is done they will be used by rewire to understand how to map projections in the operators.

6)  It's not clear that the RuleMatcher, in its current form, will work with rules that are not linear.  That is, it matches rules that look like:
Operators {Foreach, Filter}
Edges {0->1}

But I don't know if it will match rules that look like:
Operators {Scan, Scan, Join}
Edges {0->2, 1->2}

For the optimizer to be able to determine join types and operations with splits, it will have to be able to do that.

Examples of types of rules that is optimizer could support:

1) Pushing filters in front of joins.
2) Pushing foreachs with flattens (which thus greathly expand the data) down the tree past filters, joins, etc.
3) Pushing type casting used for schemas in loads down to the point where the field is actually used.
4) Deciding when to do fragment/replicate join or sort/merge join instead of the standard hash join.
5) The current optimizations:  pushing limit up the tree, making implicit splits explicit, merge load and stream where possible, using the combiner.
6) Merge filters or foreachs where possible

In particular the combiner optimizer hopefully can be completely rewritten to use the optimizer framework to make decisions about how to rework physical plans
to push work into the combiner.


",,,,,,,,,,,,,PIG-922,,,,,,10/Apr/09 17:53;sms;OptimizerPhase1.patch;https://issues.apache.org/jira/secure/attachment/12405171/OptimizerPhase1.patch,15/Apr/09 17:14;sms;OptimizerPhase1_part2.patch;https://issues.apache.org/jira/secure/attachment/12405547/OptimizerPhase1_part2.patch,14/May/09 18:35;sms;OptimizerPhase2.patch;https://issues.apache.org/jira/secure/attachment/12408159/OptimizerPhase2.patch,21/May/09 23:46;sms;OptimizerPhase3_parrt1-1.patch;https://issues.apache.org/jira/secure/attachment/12408756/OptimizerPhase3_parrt1-1.patch,20/May/09 23:25;sms;OptimizerPhase3_parrt1.patch;https://issues.apache.org/jira/secure/attachment/12408636/OptimizerPhase3_parrt1.patch,17/Jun/09 16:54;sms;OptimizerPhase3_part2_3.patch;https://issues.apache.org/jira/secure/attachment/12410960/OptimizerPhase3_part2_3.patch,26/Jun/09 17:39;sms;OptimizerPhase4_part1-1.patch;https://issues.apache.org/jira/secure/attachment/12411941/OptimizerPhase4_part1-1.patch,01/Jul/09 17:51;sms;OptimizerPhase4_part2.patch;https://issues.apache.org/jira/secure/attachment/12412291/OptimizerPhase4_part2.patch,31/Jul/09 18:18;sms;Optimizer_Phase5.patch;https://issues.apache.org/jira/secure/attachment/12415138/Optimizer_Phase5.patch,,,,,9.0,,,,,,,,,,,,,,,,,,,2009-03-31 00:50:35.961,,,no_permission,,,,,,,,,,,,164271,Reviewed,,,,Fri Aug 07 17:20:00 UTC 2009,,,,,,,0|i0gnaf:,95203,,,,,,,,,,"31/Mar/09 00:50;sms;Problem: Find a sub-graph within a directed acyclic graph (DAG) aka pattern matching
==============================================================

For optimization, a common process is to find patterns in a graph and rewire the graph to have an optimized version of the pattern. The problem of finding a sub-graph within a graph is the well known problem, sub-graph isomorphism, a NP complete problem. Within the context of PIG, the problem is recast as finding a sub DAG within a DAG.

The problem is divided into multiple sub-problems of representing the pattern (sub DAG) and finding the pattern within the DAG.

Representing the pattern.
-------------------------------------

The objective is to use the same optimizer framework to implement rule specification across the board, i.e., logical plans, physical plans and map reduce plans. In order to facilitate this, a new plan called RulePlan will be designed. The RulePlan will subclass the OperatorPlan and will be typed on RuleNode.

A RuleNode will extend the Operator class and will be annoated with the following member variables:

   1. mNodeClass: A private member variable that is of type Class to denote the class of the node. E.g.: mNodeClass could be LOFilter, LOLoad, POFilter, etc.
   2. mNodeType: An enum that idenitifes a node to be a simple node, a multi node or a common node. A brief descriptions of the three kinds of nodes follow.
      * Simple node: A normal rule node in the rule plan
      * Multi node: A node that appears more than once in the rule plan
      * Common node: A node that is common to more than one path, i.e., has multiple incoming edges or multiple outgoing edges. 

Finding the pattern.
----------------------------

The existing RuleMatcher class uses dependency order or depth first order to traverse the graph. This ensures that a given node is not seen more than once. Currently, the RuleMatcher relies on the list of nodes (provided as input) and picks the first node in the list as the root of the pattern. With the rule plan approach, the algorithm will be modified to look for all the roots of the RulePlan.

For each node in the matched path, the number of edges per node should match the number of edges for the corresponding node in the RulePlan. In addition, instead of looking for one edge from each node, the RuleMatcher will look for all edges from a given node in the RulePlan. The implication of this change, is that each RulePlan should be self contained, i.e., there cannot be any dangling edges out of the roots and into the intermediate and leaf nodes in the RulePlan.

When a set of matches is found, the following algorithm will compute the matches:

{code}
if there is a common node then
	for all matches m
		for all remaining matches r
			if(common_nodes(m) == common_nodes(r)) then
				put list of roots of each match into final_match list
			end if
		end for
	end for
else
	all matches have been found
end if
{code}","08/Apr/09 18:42;ciemo;Some thoughts on optimization problems and patterns from SQL and coding Pig and my desire for a higher level version of Pig than we have today.

I know this may come off as ""distraction"" but hopefully you'll have some time to hear me out.

* after a conversation with Santhosh about the SQL to Pig translation work 
* multiple issues I have countered with nested foreach statements including redundant function execution 
* nested FOREACH statement ""assignment"" computation bugs 
* hand coding chains of foreach statements so I can get the Algebraic combiner to kick 
* hand coding chains of foreach statements and grouping statements rather than using a single statement

I think I might have stumbled on a potentially improved model for Pig to Pig execution plan generation:

{code}
            High Level Pig to Low Level Pig translation
{code}

I think this would potentially benefit the SQL to Pig efforts and provide for programmer coding efficiency in Pig as well.

This will be a bit protracted, but I hope you have some time to consider it.

Take the following SQL idiom that the SQL to Pig translator will need to support:

{code}
            select
                        EXP(AVG(LN(time+0.1))) as geomean_time
            from
                        events
            where
                        time is not null and
                        time >= 0;
{code}

In ""high level pig"", I have wanted to code this as""
 
{code}
            A = load 'events' using PigStorage() as ( time: int );
            B = filter A by time is not null and time >= 0;
            C = group B all;
            D = foreach C generate EXP(AVG(LN(B.time+0.1))) as geomean_time;
{code}

In fact, this would seem to provide a nice translation path from SQL to ""low level pig"" via ""high level pig"".

Unfortunately, this won't work.  We developers must write Pig scripts at a lower level and break all of this apart into various steps.

An additional issue is that, because of some, um, workarounds, in the execution plan optimizations, the combiner won't kick in if we don't do further steps.

So the most ""performant"" version of the desired pig script is the following really ""low level pig"" where D is broken into 3 steps, merging one with B and the remaining 2 steps as separate D steps:

 
{code}
            A = load 'events' using PigStorage() as ( time: int );
            B = filter A by time is not null and time >= 0;
            B = foreach A generate LOG(time+0.1) as log_time;
            C = group B all;
            D = foreach C generate group, AVG(B.log_time) as mean_log_time;
                                    -- note that group alias is required for Algebraic combiner to kick in
            D = foreach D generate EXP(mean_log_time) as geomean_time;
{code}

If we can figure out how to translate SQL into this last ""low-level"" set of statements, why couldn't we or shouldn't we have ""high level pig"" as well and permit more efficient code writing and optimization?


Next example

I do a bunch of nested intermediate computations in a nested FOREACH statement:

{code}
C = foreach C {
        curr_mean_log_timetonextevent = curr_sum_log_timetonextevent / (double)count;
        curr_meansq_log_timetonextevent = curr_sumsq_log_timetonextevent / (double)count;
        curr_var_log_timetonextevent = curr_meansq_log_timetonextevent - 
                        (curr_mean_log_timetonextevent * curr_mean_log_timetonextevent);
        curr_sterr_log_timetonextevent = math.SQRT(curr_var_log_timetonextevent / (double)count);
 

        curr_geomean_timetonextevent = math.EXP(curr_mean_log_timetonextevent);
        curr_geosterr_timetonextevent = math.EXP(curr_sterr_log_timetonextevent);
        curr_mean_timetonextevent = curr_sum_log_timetonextevent / (double)count;
        curr_meansq_timetonextevent = curr_sumsq_log_timetonextevent / (double)count;
        curr_var_timetonextevent = curr_meansq_timetonextevent - 
                        (curr_mean_timetonextevent * curr_mean_timetonextevent);

        curr_sterr_timetonextevent = math.SQRT(curr_var_timetonextevent / count);

        generate
            ...
{code}

The code for nested statements in Pig has been particularly problematic and buggy including problems such as:

* redundant execution of functions such as SUM, AVG
* nested function problems
* mathematical operator problems (illustrated in this bug)
* no type propagation
* the need to use AS clauses to name nested alias assignments projected in the GENERATE clauses

What if instead of trying to do all of these operations in some specialized execution code, what if this was treated as ""high level"" pig that translated all of these intermediate statements into two or more ""low level"" foreach expansions.

This isn't as wild as it seems because 9 times out of 10, the ""workaround"" that I have had to do is exactly that: I had to stop using nested foreach and instead break the code into two separate foreach statements chained together.

In other words I went from the above nested foreach statement that generated errors and didn't work to two hand coded foreach statements (or more) that did:

{code}
C = foreach C generate
            *,
            curr_sum_log_timetonextevent / (double)count as curr_mean_log_timetonextevent,
            curr_sumsq_log_timetonextevent / (double)count as curr_meansq_log_timetonextevent;

C = foreach C generate=
            *,
            curr_meansq_log_timetonextevent - 
                        (curr_mean_log_timetonextevent * curr_mean_log_timetonextevent)
                                                                                    as curr_var_log_timetonextevent;

C = foreach C generate
            *,
            math.SQRT(curr_var_log_timetonextevent / (double)count) as curr_sterr_log_timetonextevent;
{code}

This was the only way I could avoid the redundant computations and get the code actually work. Well, actually if I added casts at appropriate places, it also worked, but what a pain.

This would also have the advantage that alias names used in the nested ""assignments"" would actually propagate without an ""as"" clause in the subsequent generate statement.

I know this is a ""brain fart"" but it does have a time honored tradition in languages like C, C++, Lisp of using the language to ""bootstrap"" the language by translating from more ""high level"" idioms to less feature rich ""low level"" idioms.

It just seemed like a plausible way of speeding up both development of a SQL to Pig translator as well as allowing a more rapid transition of Pig to higher level idioms while correcting whole swaths of execution bugs and performance optimization issues as well.


","10/Apr/09 00:48;sms;Attached patch has changed the design and implementation of the sub-graph pattern matching. Now, a subgraph pattern can be specified instead of a list of nodes and some edges. The existing rule specification was changed to use the new framework. Additional test cases have been added to validate and verify the new framework.

In addition,  PlanPrinter a generic plan printing class has been added. In the future, existing plan printers for the various types of plans (Logical, Physical, MR, RulePlan) should be changed to extend the PlanPrinter and override required methods.

All unit test cases pass.",10/Apr/09 17:53;sms;Attaching a new patch that fixes a javadoc warning.,"13/Apr/09 18:34;alangates;Patch looks good.  A few comments on comments.

It looks like some of the comments in the code haven't been updated to reflect the changes.  They still talk about expressing rules as a list of nodes and edges, about only matching linear subsections of the graph, etc.  

Also, and more importantly, since the optimizer is someone complicated now I think it would be good to put a large comment in the package header for org.apache.pig.impl.plan.optimizer.  This comment should contain a basic outline of the optimizer design, including stuff like how graph of OperatorRule and RulePlan are used to match plans, the primitives used in graph transformations, etc.

I don't think either of these are big enough issues to prevent committing this patch.  They can both be included in the next patch.",15/Apr/09 07:58;sms;Patch has been committed.,"15/Apr/09 17:14;sms;Part2 of the Phase 1 patch. This patch adds the graph operators swap, pushBefore and pushAfter. In addition unit test cases have been added to cover the use of these new operators.

All unit tests pass.",20/Apr/09 21:47;alangates;+1 on Part2 of Phase 1 patch.,20/Apr/09 23:05;sms;Committed Part2 patch of Phase 1.,14/May/09 17:54;sms;Phase 2 of the optimizer introduces projection maps for the relational operators.,"14/May/09 18:21;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12408150/OptimizerPhase2.patch
  against trunk revision 774582.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to cause Findbugs to fail.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/40/testReport/
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/40/console

This message is automatically generated.",14/May/09 18:35;sms;Attaching a new patch for Optimizer Phase 2. The previous patch did not include a newly added file.,"17/May/09 07:19;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12408159/OptimizerPhase2.patch
  against trunk revision 775340.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/45/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/45/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/45/console

This message is automatically generated.",18/May/09 17:17;sms;Re-submitting the patch as the test cases as reported by HadoopQA pass on the developer's box.,"18/May/09 19:59;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12408159/OptimizerPhase2.patch
  against trunk revision 775340.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/46/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/46/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/46/console

This message is automatically generated.",18/May/09 20:14;alangates;+1 for OptimizerPhase2.patch,19/May/09 00:12;sms;OptimizerPhase2 committed.,20/May/09 00:32;sms;Part 1 of the Phase3 patch. It implements the requiredFields feature in all the relational operators. New unit tests have been added.,"20/May/09 02:14;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12408536/OptimizerPhase3_parrt1.patch
  against trunk revision 776106.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/49/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/49/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/49/console

This message is automatically generated.",20/May/09 15:59;sms;Attaching new patch that fixes the findbugs warning.,"20/May/09 23:25;sms;New patch that adds projection map and required fields to operators that were left out in the previous patch (limit, split, split output and streaming).","21/May/09 05:59;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12408636/OptimizerPhase3_parrt1.patch
  against trunk revision 776106.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/51/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/51/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/51/console

This message is automatically generated.","21/May/09 23:30;alangates;Comments on OptimizerPhase3_parrt1.patch

Why does LOSplit say it requires no fields?  If the split has filter conditions then it seems like it would need those fields.

Shouldn't LOStream require all fields rather than none?  It seems like users will have written their scripts assuming that their stream executable gets all of the fields coming out of the previous operator.

","21/May/09 23:41;sms;LOSplit is a no-op operator. LOSplitOutput is modeled after filter.

Fair comment about LOStream.  I will make this change and resubmit the patch.",21/May/09 23:46;sms;Attaching patch incorporating the review comments.,"22/May/09 01:25;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12408756/OptimizerPhase3_parrt1-1.patch
  against trunk revision 776106.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/53/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/53/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/53/console

This message is automatically generated.",22/May/09 16:36;alangates;+1 for latest rev of part 3.,22/May/09 21:01;sms;Patch OptimizerPhase3_part-1.patch has been committed.,"23/May/09 11:40;hudson;Integrated in Pig-trunk #451 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/451/])
    : Proposed improvements to pig's optimizer
","01/Jun/09 16:28;sms;The graph operation pushAfter was added as a complementary operation to pushBefore. Currently, on the logical side, there are no concrete use cases for pushAfter. The only operator that truly supports multiple outputs is split. Our current model for split is to have an no-op split operator that has multiple successors, split outputs, each of which is the equivalent of a filter. The split output has inner plans which could have projection operators that hold references to the split's predecessor. 

When an operator is pushed after split, the operator will be placed between the split and split output. As a result, when rewire on split is called, the call is dispatched to the split output. The references in the split output after the rewire will now point to split's predecessor instead of pointing to the operator that was pushed after.

The intention of the pushAfter in the case of a split is to push it after the split output. However, the generic pushAfter operation does not distinguish between split and split output. A possible way out is to override this method in the logical plan and duplicate most of the code in the OperatorPlan and add new code to handle split.

As of now, the pushAfter will not be used in the logical layer.
","16/Jun/09 17:48;sms;Attached patch includes the following:

1. Implementation of rewire with a modified API
2. Changes to projection map to facilitate the use of rewire
3. Turns of the store/load optimization in multi-query execution
4. Unit tests for rewire and modifications to existing unit tests","16/Jun/09 20:14;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12410837/OptimizerPhase3_part2_1.patch
  against trunk revision 784333.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 18 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 259 javac compiler warnings (more than the trunk's current 224 warnings).

    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/86/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/86/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/86/console

This message is automatically generated.","16/Jun/09 22:10;sms;Attached patch fixes the findbug warning, and cleans up the sources by removing commented out code. The additional 35 compiler warning messages are related to type inference. At this point these messages are harmless.","17/Jun/09 01:51;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12410859/OptimizerPhase3_part2_2.patch
  against trunk revision 785450.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 18 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 259 javac compiler warnings (more than the trunk's current 224 warnings).

    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/89/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/89/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/89/console

This message is automatically generated.",17/Jun/09 16:54;sms;Fixed the new findbugs warning.,"17/Jun/09 19:33;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12410960/OptimizerPhase3_part2_3.patch
  against trunk revision 785450.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 18 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 259 javac compiler warnings (more than the trunk's current 224 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/90/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/90/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/90/console

This message is automatically generated.","19/Jun/09 20:00;alangates;Why is it that some Logical operators (LOCross, LOStream) don't have rewire implemented?

Near the end of ProjectFixerUpper.vist(POProject), you have a TODO about the walking.  We should figure out whether that is necessary or not, as doing visiting by the visit function and by the walker can result in double visiting.

Is there a need to add a clear concept to LogicalTransformer in order to clear state between calls to check, since each transformer will potentially be called multiple times now?","19/Jun/09 20:11;sms;1. Some operators do not have any internal state that requires rewiring. Examples of such operators include LOStream, LOCross, etc.

2. I think that the additional walking should be removed. I added a TODO as I was not sure why it was added in the first place.

3. Yes, it will be added as part of the next patch.","19/Jun/09 20:15;alangates;+1, looks good.",19/Jun/09 22:43;sms;OptimizerPhase3_part2_3.patch has been committed.,"20/Jun/09 11:41;hudson;Integrated in Pig-trunk #480 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/480/])
    : Proposed improvements to pig's optimizer (sms)
","24/Jun/09 05:50;sms;Attached patch, implements the optimization rule for pushing filters up.","25/Jun/09 08:15;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12411605/OptimizerPhase4_part1.patch
  against trunk revision 788174.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 12 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/100/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/100/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/100/console

This message is automatically generated.",26/Jun/09 17:39;sms;Fixed the findbugs issue. This problem was prevalent in other parts of existing code. Fixed that too. Attaching new patch with these changes.,"26/Jun/09 20:37;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12411941/OptimizerPhase4_part1-1.patch
  against trunk revision 788174.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 12 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 2 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/103/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/103/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/103/console

This message is automatically generated.","26/Jun/09 20:47;sms;The find bug warnings are harmless, there are explicit checks for null to print null as opposed to the contents of the object.",26/Jun/09 21:52;alangates;+1 on the phase 4 part 1 patch.,29/Jun/09 17:24;sms;Phase 4 part 1 patch has been committed.,"30/Jun/09 12:43;hudson;Integrated in Pig-trunk #490 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/490/])
    : Proposed improvements to pig's optimizer (sms)
","01/Jul/09 17:51;sms;Attached patch introduces a new rule to push foreach with flatten down the tree, i.e., delay for each with faltten execution to reduce the number of records. A caveat here: This rule optimizes only one foreach with flatten input per cross/fragment replicate join. A new rule is required for binary, ternary, etc foreach with flatten input per cross/fragment replicate join. The cause for this limitation is the local view of the optimizer as opposed to a global view.

","02/Jul/09 16:16;alangates;A couple of questions and a comment on patch4-part2

I don't understand what the following code does:
{code}
            List<Integer> foreachAddedFields = foreachProjectionMap.getAddedFields();
            if(foreachAddedFields != null) {
                Set<Integer> foreachAddedFieldsSet = new HashSet<Integer>(foreachAddedFields);
                flattenedColumnSet.removeAll(foreachAddedFieldsSet);
            }
{code}

Why are you removing added fields from the flattened set?  Won't all flattened fields appear as added in the projection map?

I think it would be very helpful to insert some comments on why this rule only applies if the successor is an Order, Cross, or Join.  

Why was the code dealing with flattening a bag with an unknown schema removed from LOForeach?
","02/Jul/09 17:27;sms;1. Removing added fields from the flattened set.

The flattened set is the set of all flattened columns. It can contain mapped and added fields. In order to remove the added fields from this set, the removeAll method is used.

2. Comments on why the rule applies only to Order, Cross and Join

Will add these comments.

3. Removing code in LOForEach for flattening a bag with unknown schema

The code that I removed was redundant and also had a bug. The check for a field getting mapped was neglected in one case. After I added the check, the code for the if and the else was identical. I removed the redundant code and made it simpler.","02/Jul/09 17:56;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12412291/OptimizerPhase4_part2.patch
  against trunk revision 790635.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 15 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 250 javac compiler warnings (more than the trunk's current 248 warnings).

    -1 findbugs.  The patch appears to introduce 2 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/107/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/107/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/107/console

This message is automatically generated.","02/Jul/09 18:27;sms;-1 javac. The applied patch generated 250 javac compiler warnings (more than the trunk's current 248 warnings).

The additional 2 compiler warning messages are related to type inference. At this point these messages are harmless. 

-1 javac. The applied patch generated 250 javac compiler warnings (more than the trunk's current 248 warnings).

Dodgy warning:
The find bug warnings are harmless, there is an  explicit check for null to print null as opposed to the contents of the object.  

Correctness warning:
There are checks in place to ensure that the variable can never be null.
",02/Jul/09 21:14;sms;Phase 4 part 2 patch has been committed,"04/Jul/09 11:44;hudson;Integrated in Pig-trunk #494 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/494/])
    ",31/Jul/09 18:18;sms;Attached patch removes references to LOFRJoin and replaces it with LOJoin. All the optimization rules and test cases now use LOJoin.,"07/Aug/09 02:13;daijy;Two comments for Optimizer_Phase5.patch:
1. We can remove LOFRJoin.java, it is no longer in use
2. Remove comment ""// For skewed join, add a local rearrange operator to the plan"" in LogToPhyTranslationVisitor.java, both skewed join and regular join will do that, this comment is misleading.

Other part of the patch is good. ",07/Aug/09 02:19;daijy;Phase5 patch committed. Thanks Santhosh!,"07/Aug/09 12:12;hudson;Integrated in Pig-trunk #515 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/515/])
    : Proposed improvements to pig's optimizer, Phase5
",07/Aug/09 17:20;sms;All optimizer related patches have been committed.
Fatal error produced when malformed scalar types within complex type is converted to given type,PIG-696,12416039,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,thejas,thejas,03/Mar/09 22:29,04/Aug/11 00:34,14/Mar/19 03:05,05/Nov/10 18:53,,,,,,,0.9.0,,,,,0,,,,"Instead of fatal error, the failed conversions should result in null values.
Example -

grunt > cat cbag3.dat
{(asdf)}
{(2344)}
{(2344}
{(323423423423434)}
{(323423423423434L)}
{(asdff)}

grunt> A = load 'cbag3.dat' as (f1:bag{t:tuple(i:int)});  B = foreach A generate flatten(f1);  C = foreach B generate $0 + 1; dump C;
2009-03-03 14:25:19,604 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete
2009-03-03 14:25:44,628 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Map reduce job failed
2009-03-03 14:25:44,642 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2043: Unexpected error during execution.
Details at logfile: /d1/tejas/pig_1236118410343.log

tail  /d1/tejas/pig_1236118410343.log
  Caused by: java.lang.ClassCastException
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Add.getNext(Add.java:110)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:260)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:198)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:217)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:208)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.map(PigMapOnly.java:65)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:227)


The 'conversion' of scalar types in complex types is happening in the physicaloperators, and not in the loaders. The expressions (such as Add in example) attempts to cast input to given type, and ClassCastException is thrown when conversion fails.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-11-05 18:53:59.432,,,no_permission,,,,,,,,,,,,66205,,,,,Fri Nov 05 18:53:59 UTC 2010,,,,,,,0|i0gn9z:,95201,,,,,,,,,,05/Nov/10 18:53;daijy;Verified already fixed in trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig should not fail when error logs cannot be created,PIG-695,12416037,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,sms,sms,03/Mar/09 22:25,24/Mar/10 22:13,14/Mar/19 03:05,21/Jul/09 17:03,0.2.0,,,,,,0.4.0,,impl,,,0,,,,"Currently, PIG validates the log file location and fails/exits when the log file cannot be created. Instead, it should print a warning and continue.",,,,,,,,,,,,,,,,,,,17/Jul/09 17:03;sms;PIG-695.patch;https://issues.apache.org/jira/secure/attachment/12413830/PIG-695.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-07-17 23:13:16.772,,,no_permission,,,,,,,,,,,,164270,Reviewed,,,,Tue Jul 21 17:03:37 UTC 2009,,,,,,,0|i0gn9j:,95199,,,,,,,,,,17/Jul/09 17:03;sms;Attached patch ensures that Pig does not error out when the error log file is not writable. ,"17/Jul/09 23:13;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12413830/PIG-695.patch
  against trunk revision 794937.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/134/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/134/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/134/console

This message is automatically generated.",21/Jul/09 00:22;pkamath;+1,21/Jul/09 00:24;sms;There are no unit tests added for this fix as this is part of the testing Main. Currently there are no unit tests for Main.,21/Jul/09 17:03;sms;Patch has been committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schema merge should take into account bags with tuples and bags with schemas,PIG-694,12416036,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,sms,sms,03/Mar/09 22:22,02/May/13 02:29,14/Mar/19 03:05,10/Jan/11 22:39,0.2.0,,,,,,0.9.0,,impl,,,1,,,,The merge method in Schema does not treat bags with schemas and bags with tuples as equivalent. This will bring closure to PIG-448 and PIG-577.,,,,,,,,,,PIG-730,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-01-10 22:39:56.599,,,no_permission,,,,,,,,,,,,66275,,,,,Mon Jan 10 22:39:56 UTC 2011,,,,,,,0|i0gn93:,95197,,,,,,,,,,"10/Jan/11 22:39;daijy;After PIG-767, bag schema always contains tuple. This is no longer an issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parameter to UDF which is an alias returned in another UDF in nested foreach causes incorrect results,PIG-693,12415911,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,viraj,viraj,02/Mar/09 09:21,25/Mar/10 00:12,14/Mar/19 03:05,21/Mar/09 01:30,0.2.0,,,,,,0.3.0,,impl,,,0,,,,"Consider the following Pig Script
{code}
register myudf.jar;

A = load 'one.txt' using PigStorage() as ( one: int ); --use this dummy file to start execution

B = foreach A {
        dec = myudf.URLDECODE('hello');
        str1 = myudf.REPLACEALL(dec, '[\\u0000-\\u0020]', ' '); -- ERROR
        str2 = myudf.REPLACEALL('hello', '[\\u0000-\\u0020]', ' ');
        generate
                dec,
                str1,
                str2;
        };
describe B;

dump B;

{code}

where one.txt is a file which contains number one (1) for starting execution of the Pig script!!
{code}
describe B; 
{code}
 returns the following:

B: {urldecode_9: chararray,replaceall_urldecode_10_11: chararray,replaceall_12: chararray}

{code}
dump B;
{code}

returns 

(hello,[\u0000-\u0020],hello)

The result should be:



There is a workaround for the same, 

{code}
register myudf.jar;

A = load 'one.txt' using PigStorage() as ( one: int );

B = foreach A {
        dec = myudf.URLDECODE('hello');
        generate
                dec as dec,
                myudf.REPLACEALL(dec, '[\\u0000-\\u0020]', ' ') as str1,
                myudf.REPLACEALL('hello', '[\\u0000-\\u0020]', ' ') as str2;
        };
describe B;

dump B;

{code}

where 

{code}
dump B;
{code}

returns (hello,hello,hello)
",,,,,,,,,,,,,,,,,,,20/Mar/09 17:07;thejas;693.2.patch;https://issues.apache.org/jira/secure/attachment/12402676/693.2.patch,12/Mar/09 18:34;thejas;693.patch;https://issues.apache.org/jira/secure/attachment/12402073/693.patch,12/Mar/09 22:52;thejas;693.utest.patch;https://issues.apache.org/jira/secure/attachment/12402095/693.utest.patch,02/Mar/09 09:35;viraj;REPLACEALL.java;https://issues.apache.org/jira/secure/attachment/12401227/REPLACEALL.java,02/Mar/09 09:33;viraj;URLDECODE.java;https://issues.apache.org/jira/secure/attachment/12401226/URLDECODE.java,02/Mar/09 09:35;viraj;one.txt;https://issues.apache.org/jira/secure/attachment/12401228/one.txt,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2009-03-12 18:34:30.601,,,no_permission,,,,,,,,,,,,164269,Reviewed,,,,Sat Mar 21 01:30:40 UTC 2009,,,Patch Available,,,,0|i0gn8n:,95195,,,,,,,,,,02/Mar/09 09:33;viraj;Eval UDF,02/Mar/09 09:35;viraj;EVAL UDF,02/Mar/09 09:35;viraj;Test input file to start execution,"12/Mar/09 18:34;thejas;- All subclasses of LogicalOperator(LOP) (except LOConst,LOProject) no longer maintain a local reference of their inputs. getOperand()/getLhs()/getArguments() etc will access the  predecessor in the graph each time. This will avoid problems caused by predecessor and input arguments reference going out of sync.
- In above classes, the functions for setting the input arguments have been removed. The constructor also no longer takes arguments as input.
- LogToPhyTranslationVisitor - Checking if getPredecessor() is null before calling getOperand().getType(). (Fixes a negative test case failure).
- TypeCheckingVisitor - Insertion of casts in LOP s uses the  OperatorPlan.insertBetween() function. Common logic used to insert cast in LOPs have been moved to new insertCast(..) function.
- LOConst does not have above change because its input is contained within the object, not on the graph. LOProject does not change because input  need not be predecessor in graph if input is relational operator.
","12/Mar/09 22:52;thejas;Unit test for this bug.
This patch is in addition to the patch file attached earlier.",13/Mar/09 18:34;sms;I will be reviewing this patch.,"13/Mar/09 22:14;sms;Review comments on 693.patch

Overall comment:

Instead of removing the private member variables and the associated constructors, its better to mark the constructor and the other methods as deprecated. If there are developers who are using these sources then compiler warning messages will help them move the code instead of forcing them to make immediate changes.

Specific comments:

Index: src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/LogToPhyTranslationVisitor.java
=====================================================================================

Is it required to have the complete package name?

{code}
+        List<org.apache.pig.impl.logicalLayer.ExpressionOperator> fromList = func.getArguments();
{code}

Index: src/org/apache/pig/impl/logicalLayer/RemoveRedundantOperators.java
=============================================================

1. Can you add an error code and an appropriate message to the VisitorException ?
{code}
+                            throw new VisitorException(pe.getMessage(),pe);
{code}


Index: src/org/apache/pig/impl/logicalLayer/validators/TypeCheckingVisitor.java
===================================================================
1. Instead of node use node.getClass().getSimpleName(). Otherwise, users will see a huge string with contains the operator key and scope. This is useful for developers but not for users.
{code}
+            String msg = ""Problem with inserting cast operator for "" + node + "" in plan."";
{code}

Index: src/org/apache/pig/impl/plan/OperatorPlan.java
===========================================
1.  Use Integer instead of string. Integer, Long, etc. can handle null and toString will ensure that null is printed as ""null"".

{code}
+            String size = "" null "";
+            if(preds != null)
+                size = ((Integer)preds.size()).toString();
{code}

2. I see that you are following the convention of the source for throwing exceptions. It will be good if you can include an error code, not log the error and just throw the exception.

{code}
+            PlanException pe = new PlanException(""Attempt to remove "" +
+                    "" and reconnect for node with  "" + size +
+            "" predecessors."");
+            log.error(pe.getMessage());
+            throw pe;
{code}

{code}
+        //checking pre-requisite conditions
+        if (C == null || C.size() == 0) {
+            PlanException pe = new PlanException(""Attempt to remove "" +
+            "" and reconnect for node with no successors."");
+            log.error(pe.getMessage());
+            throw pe;
+        }   

{code}

3. OperatorPlan is a generic class and should not hold any references to LogicalOperator, ExpressionOperator or LOProject. In addition, the last section of removeAndReconnectMultiSucc checks for LOProject. This should instead go into the patchInputReference method in RemoveRedundantOperators.java.

{code}
+import org.apache.pig.impl.logicalLayer.ExpressionOperator;
+import org.apache.pig.impl.logicalLayer.LOProject;
+import org.apache.pig.impl.logicalLayer.LogicalOperator;
{code}

{code}
+            //special handling of LOProject because its getExpression() does
+            // need not be same as getPredecessors(LOProject)
+            if(c instanceof LOProject){
+                LOProject lop = (LOProject)c;
+                if(B == lop.getExpression()){
+                    lop.setExpression((LogicalOperator)B);
+                }
+            }
{code}

4. The removeAndReconnectMultiSucc does not take into account successors of A. If A has other successors apart from B then the following code will remove the edge from A to the other successors.

{code}
+        // Do A.succesors = B.successors(ie C)
+        mFromEdges.removeKey(A);
+        mFromEdges.put(A,C);
{code}

5. Minor comment, the use of variables A, B, C should be changed to nodeA, nodeB, nodeC to distinguish between types (like E) and instances (like A, B, C). Its a little bit confusing.","13/Mar/09 23:56;sms;Review comments for 693.utest.patch

Index: test/org/apache/pig/test/TestEvalPipeline2.java
==========================================

Use org.apache.pig.test.utils.Idenity instead of creating another IdenityUDF.

{code}
+    static public class IdentityUdf 
+        extends EvalFunc<Tuple> {
+        
+        // Expects 3 chararrays in input tuple and returns the 3 chararrays in 
+        // output tuple
+        @Override
+        public Tuple exec(Tuple input) throws IOException {
+            if (input == null || input.size() == 0)
+                return null;
+            return input ;
+            
+        }
+       
+    }
{code}","14/Mar/09 00:39;thejas;Re: Review comments on 693.patch 
 Re: Overall comment:  The private member variables are not part of the interface, so removing them should be fine.  I can add deprecated versions of the constructor and set methods. But is it OK to keep even deprecated versions of them around if we not using the input parameters passed , or in case of set methods they are no-op ? 

Re Complete package name used in Index: src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/LogToPhyTranslationVisitor.java ,
There are two ExpressionOperator classes used in there, from - org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.ExpressionOperator; and org.apache.pig.impl.logicalLayer.ExpressionOperator. Either of those have to be used with full package name. The one logicalLayer one is only used once, so I am using full name for it.


Re: Index: src/org/apache/pig/impl/logicalLayer/validators/TypeCheckingVisitor.java , 
If i use node.getClass().getSimpleName(), it will print names like LOOr, LOBinCond which are also not very user friendly. I think we should change the name() function in these classes to not print the scope.

Re: Index: src/org/apache/pig/impl/plan/OperatorPlan.java 
1. Not sure if i understood what you meant there . The following code will not work - 
        Integer i = null;
        System.out.println(i.toString());

( I am reviewing rest of the suggestions.)","16/Mar/09 15:55;sms;1. If you deprecate the constructors then you need to re-instate the private member variables. You can ignore this comment considering the fact that all uses of this class are inside Pig for now.

2. Ok. Its fine for now.

3. Changing name() will have implications on existing test cases. Its better to use node.getClass().geSimpletName(). Its more user friendly than using the node.toString() which provides details which cannot be understood by users.

4. Do not use i.toString(). Just use the following:

{code}
Integer i = null;
System.out.println(i);
{code}","16/Mar/09 16:01;thejas;Re: Index: src/org/apache/pig/impl/plan/OperatorPlan.java
I will make the changes you suggested in comments 2,3,4,5 in a new patch.

Thanks for reviewing the code and for the comments.
","19/Mar/09 18:46;thejas;Re:
bq.  Changing name() will have implications on existing test cases. Its better to use node.getClass().geSimpletName(). Its more user friendly than using the node.toString() which provides details which cannot be understood by users.

I have created a separate JIRA - PIG-726 to make node.toString() more user friendly. After than change, it will be more user friendly than printing class name. I will address that JIRA soon.
",19/Mar/09 22:35;thejas;Created PIG-727 to address exception creation in OperatorPlan.java ,"20/Mar/09 17:07;thejas;Updated patch incorporating changes suggested in code review . 
OperatorPlan.java 
- using Integer for size instead of string
- removed imports of LogicalOperator and subclasses
- moved special handling of LOProject to patchInputReference method in RemoveRedundantOperators.java
- removeAndReconnectMultiSucc()  takes into account other successors of A.

TestEvalPipeline2.java
- Using org.apache.pig.test.utils.Idenity instead of  IdenityUDF.
",20/Mar/09 17:10;thejas;Patch file 693.2.patch replaces all earlier patch files.,21/Mar/09 01:30;sms;Patch has been committed. Thanks for the fix Thejas. Good work on cleaning up the type checker.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BinStorage skips tuples when ^A is present in data,PIG-691,12415821,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,olgan,olgan,27/Feb/09 20:51,24/Mar/10 22:04,14/Mar/19 03:05,03/Mar/09 04:26,0.2.0,,,,,,0.2.0,,,,,0,,,,Pradeep found a problem with BinStorage.getNext function that causes data loss. He is working on the fix,,,,,,,,,,,,,,,,,,,02/Mar/09 23:14;pkamath;PIG-691.patch;https://issues.apache.org/jira/secure/attachment/12401268/PIG-691.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-03-02 23:13:53.575,,,no_permission,,,,,,,,,,,,164267,Reviewed,,,,Tue Mar 03 04:26:09 UTC 2009,,,,,,,0|i0gn7j:,95190,,,,,,,,,,"02/Mar/09 23:13;pkamath;Binstorage uses RECORD_1, RECORD_2 and RECORD_3 byte markers (the bytes 0x01, 0x02, 0x03) as the beginning of a new record. The current bug in BinStorage is that in getNext(), the code looks for RECORD_1 and if it finds RECORD_1, it looks for RECORD_2. If it fails to find RECORD_2, it goes back to look for entire sequence starting with looking for RECORD_1. However this failes when we have the following sequence:RECORD_1-RECORD_1-RECORD_2-RECORD_3. After reading the second RECORD_1 in the above sequence, we should not look for RECORD_1 again but start by looking for RECORD_2. This is an issue only when a record in binstorage spans two blocks and the part in the head of the second block has the above sequence. This can happen when the last field in the record is null (null is represented by the byte 0x01 which is RECORD_1). The attached patch fixes this issue.",02/Mar/09 23:56;sms;I will be reviewing this patch.,03/Mar/09 04:26;sms;Patch has been comiited. Thanks for the fix Pradeep.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UNION doesn't work in the latest code,PIG-690,12415779,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,amirhyoussefi,amirhyoussefi,27/Feb/09 11:58,24/Mar/10 22:04,14/Mar/19 03:05,04/Mar/09 01:45,0.2.0,,,,,,0.2.0,,,,,0,,,,"grunt> a = load 'tmp/f1' using BinStorage();
grunt> b = load 'tmp/f2' using BinStorage();
grunt> describe a;
a: {int,chararray,int,{(int,chararray,chararray)}}
grunt> describe b;
b: {int,chararray,int,{(int,chararray,chararray)}}
grunt> c = union a,b;
grunt> describe c;
2009-02-27 11:51:46,012 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1052: Cannot cast bag with schema bag({(int,chararray,chararray)}) to tuple with schema tuple
Details at logfile: /homes/amiry/pig_1235735380348.log

dump a and dump b work fine.

Sample data provided to dev team in an e-mail.","mapred mode. local mode.has the same problem under linux.
code is taken from trunk",,,,,,,,,,,,,,,,,,03/Mar/09 02:32;pkamath;PIG-690.patch;https://issues.apache.org/jira/secure/attachment/12401280/PIG-690.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-03-03 02:31:51.986,,,no_permission,,,,,,,,,,,,164266,Reviewed,,,,Wed Mar 04 01:45:19 UTC 2009,,,,,,,0|i0gn73:,95188,,,,,,,,,,"03/Mar/09 02:31;pkamath;The root cause of the issue is while merging schemas, the code recursively merges subschemas if a field is a tuple or a bag. At that point, it does not properly attribute the type to be bag if that was the case. It always marks the type as tuple whenever the field schema is of type bag or tuple. This is fixed in the patch and a unit test case has been added which tries to union two relations which have a bag field. 
",04/Mar/09 00:01;sms;I am reviewing this patch.,04/Mar/09 01:45;sms;Patch has been committed. Thanks for fixing this issue Pradeep.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error Handling - improve message for hadoop18.3 quotas feature,PIG-689,12415719,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,,araceli,araceli,27/Feb/09 00:37,25/Mar/10 00:12,14/Mar/19 03:05,14/Jul/09 16:02,,,,,,,0.4.0,,,,,0,,,,"TEST: Set disk quota and attempt to create directory or load a file such that the quota is exceeded
RESULT: Throws an error 2998 that indicates an unhandled exception, but the hadoop message is correct.
EXPECTED: a message to the effect that the ""disk quota was exceeed"" 
The error message from hadoop is correct and adequate, but pig is throwing a ""2998"" error which is an ""unhandled internal error"". It should throw ""Quota exceed error"".

Log shows:

ERROR 2998: Unhandled internal error. org.apache.hadoop.dfs.QuotaExceededException: The quota of /user/hadoopqa/foo is exceeded: quota=2 count=3
org.apache.hadoop.dfs.QuotaExceededException: org.apache.hadoop.dfs.QuotaExceededException: The quota of /user/hadoopqa/foo is exceeded: quota=2 count=3
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:90)
        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:52)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:2311)
        at org.apache.hadoop.dfs.DFSClient.create(DFSClient.java:477)
        at org.apache.hadoop.dfs.DistributedFileSystem.create(DistributedFileSystem.java:178)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:503)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:484)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:391)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:213)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:150)
        at org.apache.pig.backend.hadoop.datastorage.HPath.copy(HPath.java:86)
        at org.apache.pig.backend.hadoop.datastorage.HPath.copy(HPath.java:165)
        at org.apache.pig.tools.grunt.GruntParser.processCopyFromLocal(GruntParser.java:499)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:178)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:98)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:72)
        at org.apache.pig.Main.main(Main.java:296)
Caused by: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.dfs.QuotaExceededException: The quota of /user/hadoopqa/foo is exceeded: quota=2 count=3
        at org.apache.hadoop.dfs.INodeDirectoryWithQuota.verifyQuota(INode.java:782)
        at org.apache.hadoop.dfs.INodeDirectoryWithQuota.updateNumItemsInTree(INode.java:761)
        at org.apache.hadoop.dfs.FSDirectory.updateCount(FSDirectory.java:767)
        at org.apache.hadoop.dfs.FSDirectory.addChild(FSDirectory.java:896)
        at org.apache.hadoop.dfs.FSDirectory.addNode(FSDirectory.java:886)
        at org.apache.hadoop.dfs.FSDirectory.addFile(FSDirectory.java:151)
        at org.apache.hadoop.dfs.FSNamesystem.startFileInternal(FSNamesystem.java:1014)
        at org.apache.hadoop.dfs.FSNamesystem.startFile(FSNamesystem.java:909)
        at org.apache.hadoop.dfs.NameNode.create(NameNode.java:284)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:481)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:890)
        at org.apache.hadoop.ipc.Client.call(Client.java:716)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:216)
        at org.apache.hadoop.dfs.$Proxy0.create(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at org.apache.hadoop.dfs.$Proxy0.create(Unknown Source)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:2308)
        ... 14 

TEST CASES

General setup:
hadoop --config /grid/2/pig_harness/tmp/hod dfs -mkdir  /user/hadoopqa/foo
/hadoop --config /grid/2/pig_harness/tmp/hod dfsadmin -setQuota 2 /user/hadoopqa/foo
From the pig shell test:

1) creation of directory
2) try loading a file

STEPS TO REPRODUCE: 
 Allocate Hod
2) Create directories and set quota using hadoop
/homes/hadoopqa> hadoop --config /grid/2/pig_harness/tmp/hod dfs -mkdir  /user/hadoopqa/foo
/homes/hadoopqa> hadoop --config /grid/2/pig_harness/tmp/hod dfsadmin -setQuota 2 /user/hadoopqa/foo
/homes/hadoopqa> hadoop --config /grid/2/pig_harness/tmp/hod dfs -mkdir  /user/hadoopqa/foo/foo1
/homes/hadoopqa> hadoop --config /grid/2/pig_harness/tmp/hod dfs -mkdir  /user/hadoopqa/foo/foo2

mkdir: org.apache.hadoop.dfs.QuotaExceededException: The quota of /user/hadoopqa/foo is exceeded: quota=2 count=3/homes/hadoopqa> hadoop --config /grid/2/pig_harness/tmp/hod dfs -copyFromLocal /grid/2/pig/in/studentcolon10k /user/hadoopqa
/homes/hadoopqa> hadoop --config /grid/2/pig_harness/tmp/hod dfs -copyFromLocal /grid/2/pig/in/studentcolon10k /user/hadoopqa/foo

 
3) open grunt shell and load file into directory that does not have quota set, then into directory that does have quota set.

grunt> copyFromLocal /grid/2/pig/in/studentcolon10k /user/hadoopqa/studentcolon10K.p1

grunt> ls /user/hadoopqa/
hdfs://xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx/user/hadoopqa/foo       <dir>
hdfs://xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx/user/hadoopqa/mapredsystem      <dir>
hdfs://xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx/user/hadoopqa/studentcolon10K.p1<r 3>   218620
hdfs://xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx/user/hadoopqa/studentcolon10k<r 3>      218620
grunt> copyFromLocal /grid/2/pig/in/studentcolon10k /user/hadoopqa/foo/studentcolon10K.p1

2009-02-26 22:54:42,858 main ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2998: Unhandled internal error. org.apache.hadoop.dfs.QuotaExceededException: The quota of /user/hadoopqa/foo is exceeded: quota=2 count=3
Details at logfile: /homes/hadoopqa/pig_1235688818725.log
","linux
hadoop18.3",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-07-14 16:02:24.009,,,no_permission,,,,,,,,,,,,164265,,,,,Tue Jul 14 16:02:24 UTC 2009,,,,,,,0|i0gn6n:,95186,,,,,,,,,,14/Jul/09 16:02;olgan;The error message looks adequate to me since it does state that the problem is with the quota. We can't special case every error we encounter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
outputSchema method in TOKENIZE is broken,PIG-684,12415166,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,sms,sms,19/Feb/09 19:48,24/Mar/10 22:04,14/Mar/19 03:05,03/Mar/09 04:27,0.2.0,,,,,,0.2.0,,impl,,,0,,,,The outputSchema method in TOKENIZE is broken. It should return a bag with a tuple that contains a string and not just a string.,,,,,,,,,,,,,,,,,,,02/Mar/09 19:21;thejas;PIG-684.txt;https://issues.apache.org/jira/secure/attachment/12401256/PIG-684.txt,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-02-26 17:22:09.259,,,no_permission,,,,,,,,,,,,38601,Reviewed,,,,Tue Mar 03 04:27:03 UTC 2009,,,Patch Available,,,,0|i0gn4f:,95176,,,,,,,,,,"26/Feb/09 17:22;thejas;TOKENIZE creates correct schema. Added unit test case.
",28/Feb/09 00:14;olgan;submitting on behalf ot Tejas,"02/Mar/09 19:21;thejas;Updated patch.
(Patch updated to add bagSchema.setTwoLevelAccessRequired(true); in TOKENIZE.java to handle the use of flatten(TOKENIZE(..)) )",02/Mar/09 22:56;sms;I am reviewing this patch,"02/Mar/09 23:01;sms;Review Comment:

Index: src/org/apache/pig/builtin/TOKENIZE.java
======================================

The RuntimeException should have a reasonable error message. Otherwise it make it hard for users and developers to make sense of the exception. There are two possible options:

1. Include an appropriate message (preferred)
OR
2. return a NULL schema.

{code}
+            // throwing RTE because
+            //above schema creation is not expected to throw an exception
+            // and also because superclass does not throw exception
+            throw new RuntimeException();
{code}",03/Mar/09 04:27;sms;Patch has been committed. Thanks for the fix Tejas.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve errors in Pig parser,PIG-674,12414702,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,xuefuz,araceli,araceli,12/Feb/09 21:25,04/Aug/11 00:35,14/Mar/19 03:05,14/Apr/11 17:21,,,,,,,0.9.0,,,,,0,,,,"These tests are for Aggregate Functions


################################################################
   Recomend msg -  SHould indicate that this is an invalid cast.
   ERROR - MAX with int with invalid cast
   TEST:  106,
   PIG SCRIPT:  A =LOAD ':INPATH:/types/DataAll' USING PigStorage() AS ( Fint:int, Flong:long, Fdouble:double, Ffloat:float, Fchar:chararray, Fchararray:chararray, Fbytearray:bytearray, Fmap:map[], Fbag:BAG{ t:tuple( name, age, avg ) }, Ftuple:( name:chararray, age:int, avg:float) );B =GROUP A ALL; X =FOREACH B GENERATE A.Fint, MAX( (invalid) A.Fint ); STORE X INTO ':OUTPATH:' USING PigStorage();\,
  CURRENT ERROR MESSAGE: ""ERROR 1000:.*Invalid alias: MAX"",

################################################################
   Recomend msg -  SHould indicate that this is an invalid cast.
   ERROR - MAX with int with invalid cast
   TEST:  106,
   PIG SCRIPT:  A =LOAD ':INPATH:/types/DataAll' USING PigStorage() AS ( Fint:int, Flong:long, Fdouble:double, Ffloat:float, Fchar:chararray, Fchararray:chararray, Fbytearray:bytearray, Fmap:map[], Fbag:BAG{ t:tuple( name, age, avg ) }, Ftuple:( name:chararray, age:int, avg:float) );B =GROUP A ALL; X =FOREACH B GENERATE A.Fint, MAX( (invalid) A.Fint ); STORE X INTO ':OUTPATH:' USING PigStorage();\,
  CURRENT ERROR MESSAGE: ""ERROR 1000:.*Invalid alias: MAX"",

################################################################
   Recomend msg -
   ERROR: invalid use of foreach with multiple functions and positional parameters
   TEST:  107,
   PIG SCRIPT:  A =LOAD ':INPATH:/types/DataAll' USING PigStorage() AS ( Fint:int, Flong:long, Fdouble:double, Ffloat:float, Fchar:chararray, Fchararray:chararray, Fbytearray:bytearray, Fmap:map[], Fbag:BAG{ t:tuple( name, age, avg ) }, Ftuple:( name:chararray, age:int, avg:float) );B =GROUP A ALL; X =FOREACH A GENERATE  SUM( A.$0), AVG( A.$0), COUNT( A.$0), MAX(A.$0), MIN( A.$0); STORE X INTO ':OUTPATH:' USING PigStorage();\,
  CURRENT ERROR MESSAGE: ""FIX: improve msg"",

################################################################
   Recomend msg - ERROR 1052: Cannot cast bag with schema.*: bag
   ERROR: invalid use of MIN with int with valid cast
   TEST:  108,
   PIG SCRIPT:  A =LOAD ':INPATH:/types/DataAll' USING PigStorage() AS ( Fint:int, Flong:long, Fdouble:double, Ffloat:float, Fchar:chararray, Fchararray:chararray, Fbytearray:bytearray, Fmap:map[], Fbag:BAG{ t:tuple( name, age, avg ) }, Ftuple:( name:chararray, age:int, avg:float) );B =GROUP A ALL; X =FOREACH B GENERATE A.Fint, MIN( (double) A.Fint ); STORE X INTO ':OUTPATH:' USING PigStorage();\,
  CURRENT ERROR MESSAGE: ""ERROR 1052: Cannot cast.*"",

################################################################
   Recomend msg -
   ERROR - AVG needs bag
   TEST:  113,
   PIG SCRIPT:  A =LOAD ':INPATH:/types/DataAll' USING PigStorage() AS ( Fint:int, Flong:long, Fdouble:double, Ffloat:float, Fchar:chararray, Fchararray:chararray, Fbytearray:bytearray, Fmap:map[], Fbag:BAG{ t:tuple( name, age, avg ) }, Ftuple:( name:chararray, age:int, avg:float) ); B = GROUP A ALL; X =FOREACH B GENERATE  AVG( A.Fint); STORE X INTO ':OUTPATH:' USING PigStorage();\,
  CURRENT ERROR MESSAGE: ""ERROR 1052: Cannot cast bag with schema.*bag"",

################################################################
   Recomend msg -  this should indicate there was an invalid Cast
   ERROR - AVG with int with invalid cast
   TEST:  115,
   PIG SCRIPT:  A =LOAD ':INPATH:/types/DataAll' USING PigStorage() AS ( Fint:int, Flong:long, Fdouble:double, Ffloat:float, Fchar:chararray, Fchararray:chararray, Fbytearray:bytearray, Fmap:map[], Fbag:BAG{ t:tuple( name, age, avg ) }, Ftuple:( name:chararray, age:int, avg:float) );B =GROUP A ALL; X =FOREACH B GENERATE A.Fint, AVG( (invalid) A.Fint ); STORE X INTO ':OUTPATH:' USING PigStorage();\,
  CURRENT ERROR MESSAGE: ""ERROR 1000:.*Invalid alias: AVG"",

################################################################
   Recomend msg -  this should indicate that COUNT expects a bag for an argument
   ERROR - COUNT needs bag
   TEST:  118,
   PIG SCRIPT:  A =LOAD ':INPATH:/types/DataAll' USING PigStorage() AS ( Fint:int, Flong:long, Fdouble:double, Ffloat:float, Fchar:chararray, Fchararray:chararray, Fbytearray:bytearray, Fmap:map[], Fbag:BAG{ t:tuple( name, age, avg ) }, Ftuple:( name:chararray, age:int, avg:float) ); B = GROUP A ALL; X =FOREACH B GENERATE  COUNT( Fint); STORE X INTO ':OUTPATH:' USING PigStorage();\,
  CURRENT ERROR MESSAGE: ""ERROR 1000:.*Invalid alias: COUNT"",

################################################################
   Recomend msg - missing parenthesis while parsing MAX."",
   ERROR - MAX without parenthesis
   TEST:  133,
   PIG SCRIPT:  A =LOAD ':INPATH:/types/DataAll' USING PigStorage() AS ( Fint:int, Flong:long, Fdouble:double, Ffloat:float, Fchar:chararray, Fchararray:chararray, Fbytearray:bytearray, Fmap:map[], Fbag:BAG{ t:tuple( name, age, avg ) }, Ftuple:( name:chararray, age:int, avg:float) );B =GROUP A ALL; X =FOREACH B GENERATE  MAX A.$0; STORE X INTO ':OUTPATH:' USING PigStorage();\,
  CURRENT ERROR MESSAGE: ""ERROR 1000:.*MAX"",

################################################################
   ERROR - SUM with missing argument
   TEST:  161,
   PIG SCRIPT:  A =LOAD ':INPATH:/types/DataAll' USING PigStorage() AS ( Fint:int, Flong:long, Fdouble:double, Ffloat:float, Fchar:chararray, Fchararray:chararray, Fbytearray:bytearray, Fmap:map[], Fbag:BAG{ t:tuple( name, age, avg ) }, Ftuple:( name:chararray, age:int, avg:float) );B =GROUP A ALL; X =FOREACH B GENERATE  SUM(); STORE X INTO ':OUTPATH:' USING PigStorage();\,
  CURRENT ERROR MESSAGE: ""ERROR 2064: Unsupported root type in LOForEach: LOUserFunc"",

################################################################
   Recomend msg -
   ERROR - SUM with invalid number of parameters
   TEST:  162,
   PIG SCRIPT:  A =LOAD ':INPATH:/types/DataAll' USING PigStorage() AS ( Fint:int, Flong:long, Fdouble:double, Ffloat:float, Fchar:chararray, Fchararray:chararray, Fbytearray:bytearray, Fmap:map[], Fbag:BAG{ t:tuple( name, age, avg ) }, Ftuple:( name:chararray, age:int, avg:float) );B =GROUP A ALL; X =FOREACH B GENERATE SUM ( A.$0, A.$0 ); STORE X INTO ':OUTPATH:' USING PigStorage();\,
  CURRENT ERROR MESSAGE: ""ERROR 1045:.*SUM"",

################################################################
   Recomend msg -
   ERROR - SUM with incompatible datatype
   TEST:  163,
   PIG SCRIPT:  A =LOAD ':INPATH:/types/DataAll' USING PigStorage() AS ( Fint:int, Flong:long, Fdouble:double, Ffloat:float, Fchar:chararray, Fchararray:chararray, Fbytearray:bytearray, Fmap:map[], Fbag:BAG{ t:tuple( name, age, avg ) }, Ftuple:( name:chararray, age:int, avg:float) );B =GROUP A ALL; X =FOREACH B GENERATE  SUM ( A.Fbag ); STORE X INTO ':OUTPATH:' USING PigStorage();\,
  CURRENT ERROR MESSAGE: ""FIX: Improve error message. Currently ERROR: 1045"",

################################################################
   Recomend msg - invalid use of SUM
   ERROR: invalid use of foreach with multiple functions and positional parameters
   TEST:  107,
   PIG SCRIPT:  A =LOAD ':INPATH:/types/DataAll' USING PigStorage() AS ( Fint:int, Flong:long, Fdouble:double, Ffloat:float, Fchar:chararray, Fchararray:chararray, Fbytearray:bytearray, Fmap:map[], Fbag:BAG{ t:tuple( name, age, avg ) }, Ftuple:( name:chararray, age:int, avg:float) );B =GROUP A ALL; X =FOREACH A GENERATE  SUM( A.$0), AVG( A.$0), COUNT( A.$0), MAX(A.$0), MIN( A.$0); STORE X INTO ':OUTPATH:' USING PigStorage();\,
  CURRENT ERROR MESSAGE: "" ERROR 1000: Error during parsing. Invalid alias: A"",

################################################################
   Recomend msg - Incompatible type in argument to MAX
   ERROR - MAX with incompatible type
   TEST:  143,
   PIG SCRIPT:  A =LOAD ':INPATH:/types/DataAll' USING PigStorage() AS ( Fint:int, Flong:long, Fdouble:double, Ffloat:float, Fchar:chararray, Fchararray:chararray, Fbytearray:bytearray, Fmap:map[], Fbag:BAG{ t:tuple( name, age, avg ) }, Ftuple:( name:chararray, age:int, avg:float) ); B = GROUP A ALL; X= FOREACH B GENERATE MAX( A.Ftuple ); STORE X INTO ':OUTPATH:' USING PigStorage();\,
  CURRENT ERROR MESSAGE: "" ERROR 1045: Could not infer the matching function"",

################################################################
   Recomend msg - Missing parenthesis in MIN
   ERROR - MIN without parenthesis
   TEST:  144,
   PIG SCRIPT:  A =LOAD ':INPATH:/types/DataAll' USING PigStorage() AS ( Fint:int, Flong:long, Fdouble:double, Ffloat:float, Fchar:chararray, Fchararray:chararray, Fbytearray:bytearray, Fmap:map[], Fbag:BAG{ t:tuple( name, age, avg ) }, Ftuple:( name:chararray, age:int, avg:float) );B =GROUP A ALL; X =FOREACH B GENERATE  MIN A.$0; STORE X INTO ':OUTPATH:' USING PigStorage();\,
  CURRENT ERROR MESSAGE: ""ERROR 1000:.*Invalid alias: MIN"",

################################################################
   Recomend msg -
   ERROR - SUM with incompatible cast
   TEST:  164,
   PIG SCRIPT:  A =LOAD ':INPATH:/types/DataAll' USING PigStorage() AS ( Fint:int, Flong:long, Fdouble:double, Ffloat:float, Fchar:chararray, Fchararray:chararray, Fbytearray:bytearray, Fmap:map[], Fbag:BAG{ t:tuple( name, age, avg ) }, Ftuple:( name:chararray, age:int, avg:float) );B =GROUP A ALL; X =FOREACH B  GENERATE SUM ( (chararray) Fint, Fint ); STORE X INTO ':OUTPATH:' USING PigStorage();\,
  CURRENT ERROR MESSAGE: ""ERROR 1000: Error during parsing. Invalid alias: Fint"",


################################################################
   Recomend msg -
   ERROR - AVG with incompatible cast
   TEST:  175,
   PIG SCRIPT:  A =LOAD ':INPATH:/types/DataAll' USING PigStorage() AS ( Fint:int, Flong:long, Fdouble:double, Ffloat:float, Fchar:chararray, Fchararray:chararray, Fbytearray:bytearray, Fmap:map[], Fbag:BAG{ t:tuple( name, age, avg ) }, Ftuple:( name:chararray, age:int, avg:float) );B =GROUP A ALL; X =FOREACH B  GENERATE AVG ( (chararray) Fint ); STORE X INTO ':OUTPATH:' USING PigStorage();\,
  CURRENT ERROR MESSAGE: ""ERROR 1045:.*AVG"",

################################################################
   Recomend msg -  Incompatible type in argument for AVG
   ERROR - AVG with incompatible type
   TEST:  177,
   PIG SCRIPT:  A =LOAD ':INPATH:/types/DataAll' USING PigStorage() AS ( Fint:int, Flong:long, Fdouble:double, Ffloat:float, Fchar:chararray, Fchararray:chararray, Fbytearray:bytearray, Fmap:map[], Fbag:BAG{ t:tuple( name, age, avg ) }, Ftuple:( name:chararray, age:int, avg:float) ); B = GROUP A ALL; X = FOREACH B GENERATE AVG( A.Ftuple ); STORE X INTO ':OUTPATH:' USING PigStorage();\,
   CURRENT ERROR MESSAGE: "" ERROR 1045:.*AVG"",
",,,,,,,,,,,,,,,,,PIG-1618,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-04-14 17:21:09.119,,,no_permission,,,,,,,,,,,,66148,,,,,Thu Apr 14 17:21:09 UTC 2011,,,,,,,0|i0gn07:,95157,,,,,,,,,,"14/Apr/11 17:21;xuefuz;tried all the cases above, Pig now is giving much better error messages. For example, for the first test case above, this is what Pig emits:

grunt> X =FOREACH B GENERATE A.Fint, MAX( (invalid) A.Fint );
2011-04-14 10:18:57,513 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1200: <line 6, column 33>  Syntax error, unexpected symbol at or near '('

For the last test case:

grunt> X = FOREACH B GENERATE AVG( A.Ftuple );
2011-04-14 10:09:30,633 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1045: <line 5, column 23> Could not infer the matching function for org.apache.pig.builtin.AVG as multiple or none of them fit. Please use an explicit cast.

Thus, case is closed.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
typechecker does not throw an error when multiple arguments are passed to COUNT,PIG-671,12414690,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,deepujain,araceli,araceli,12/Feb/09 19:38,04/Aug/11 00:34,14/Mar/19 03:05,23/Mar/11 15:44,0.9.0,,,,,,0.9.0,,,,,0,,,,"In this example, the agggregate function COUNT is passed multiple arguments and does not throw an error.



TEST: Aggregate_184

 A =LOAD '/user/pig/tests/data/types/DataAll' USING PigStorage() AS ( Fint:int, Flong:long, Fdouble:double, Ffloat:float, Fchar:chararray, Fchararray:chararray, Fbytearray:bytearray, Fmap:map[], Fbag:BAG{ t:tuple( name, age, avg ) }, Ftuple:( name:chararray, age:int, avg:float) );
B =GROUP A ALL; 
X =FOREACH B GENERATE COUNT ( A.$0, A.$0 ); 
STORE X INTO '/user/pig/tests/results/araceli.1234381533/AggregateFunc_184.out' USING PigStorage();
","i686 i386 GNU/Linux
",,,,,,,,,,,,,,,,,,11/Mar/11 17:40;deepujain;PIG-671.txt;https://issues.apache.org/jira/secure/attachment/12473412/PIG-671.txt,08/Mar/11 18:11;deepujain;PIG-671.txt;https://issues.apache.org/jira/secure/attachment/12473019/PIG-671.txt,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-03-08 06:11:00.984,,,no_permission,,,,,,,,,,,,66286,,,,,Wed Mar 23 15:44:15 UTC 2011,,,,,,,0|i0gmyv:,95151,"The fix is tested against 0.9 Pig source.
Unit test cases included.",,,,,,,,,"08/Mar/11 06:11;deepujain;COUNT.exec() always retrieves the first item from input tuple which it assumes is a bag and counts the numbers of items in the bag. 
Even if we pass multiple arguments to COUNT(), it will always pick the first argument.

There are few ways we go through this
a) Leave as is cause it returns correct result for counting the number of items in the first argument.
OR
b) Make a check for the size of the input tuple in COUNT.exec() and if it is not 1 then throw ExecException()  or IllegalArgumentException {might be correct}
which will cause the Map job to fail.

Let me know how to we go about it.
","08/Mar/11 10:08;deepujain;Here is the final approach:
Include public List<FuncSpec> getArgToFuncMapping() throws FrontendException in Count.java which will take care of number of arguments , their data types for COUNT.

I have the patch ready and have tested it. If anyone is not working on it, can this bug be assigned to me so that i can submit the patch for review.","08/Mar/11 15:06;thejas;Deepak,
I can't find your name in the drop down to assign it to you. Can you assign it to yourself ?
","08/Mar/11 16:06;deepujain;I could not locate the option to re-assign the defect to me. Can you point me.


UserName: deepujain
Name: Deepak Kumar V
if that helps in assigning the issue.","08/Mar/11 17:45;alangates;Any changes we make have to still work in two scenarios:

{code}
B = group A by x;
C = foreach B generate group, COUNT(A);
{code}

{code}
B = group A by x;
C = foreach B generate group, COUNT(A.$0);
{code}

I think what you're proposing to do with getArgToFuncMapping meets this, but we want to be sure to test that.
","08/Mar/11 17:58;deepujain;The following scenarios were tested with the patch.
input.txt
1 2 3 4
a b c d
r s t u

Script #1:
grunt> a = load 'test.txt' ;              
grunt> b = group a all;                   
grunt> x = foreach b generate COUNT(a);
grunt> dump x;         
Output(s):
Successfully stored records in: ""file:/tmp/temp451361479/tmp-1035885131""
(3)

Script #2:
grunt> a = load 'test.txt' ;              
grunt> b = group a all;                   
grunt> x = foreach b generate COUNT(a.$0);
grunt> dump x;         
Output(s):
Successfully stored records in: ""file:/tmp/temp451361479/tmp1005936870""
(3)

Script #3:
grunt> a = load 'test.txt' ;              
grunt> b = group a all;                   
grunt> x = foreach b generate COUNT(a.$0,a.$0);
grunt> dump x;
2011-03-08 23:26:54,554 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1045: Could not infer the matching function for org.apache.pig.builtin.COUNT as multiple or none of them fit. Please use an explicit cast.

Script #4:
grunt> a = load 'test.txt' ;              
grunt> b = group a all;                   
grunt> x = foreach b generate COUNT(a.$0,a);   
grunt> dump x;                              
2011-03-08 23:26:59,397 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1045: Could not infer the matching function for org.apache.pig.builtin.COUNT as multiple or none of them fit. Please use an explicit cast.
grunt> 
",11/Mar/11 17:40;deepujain;Patch with fix and test case.,11/Mar/11 19:08;deepujain;Please review the patch submitted on 11/Mar.,15/Mar/11 15:30;alangates;Reviewing the patch.,17/Mar/11 18:02;alangates;Patch looks good.  I'm running the tests and test-patch.,"17/Mar/11 22:40;alangates;Results of test-patch:

     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     -1 release audit.  The applied patch generated 539 release audit warnings (more than the trunk's current 538 warnings).
     [exec]
     [exec]
     [exec]
     [exec]

As far as I can tell you can ignore this release audit warning.","18/Mar/11 03:45;deepujain;Hi Allan,
What do you mean by release audit warning ?","21/Mar/11 19:43;alangates;test-patch uses a tool to audit the files and make sure they are ok to release (have appropriate headers, etc.)  But it gives a lot of bogus warnings, of which this seems to be one.

The unit tests pass, so I'll check this in.",21/Mar/11 20:11;alangates;Patch checked in.  Thanks Deepak.,"23/Mar/11 05:01;deepujain;Yee-Haw
2nd Commit.",23/Mar/11 05:02;deepujain;Should i move the issue to resolved state? (Resolve Issue),"23/Mar/11 15:44;alangates;Actually, I'm supposed to do that when I check in the patch.  Thanks for catching that.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DIFF contains an invalid expression - possible parser error,PIG-670,12414687,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,xuefuz,araceli,araceli,12/Feb/09 19:30,04/Aug/11 00:34,14/Mar/19 03:05,10/Mar/11 18:24,,,,,,,0.9.0,,,,,0,,,,"Requires further investigation.

This test takes in an invalid expression as the first argument in the DIFF function and results in the following error:

ERROR 1000: Error during parsing. Invalid alias: DIFF

Why is the parser interpreting DIFF as an alias? 


TEST: AggregateFunc_131

A =LOAD '/user/pig/tests/data/types/DataAll' USING PigStorage() AS ( Fint:int, Flong:long, Fdouble:double, Ffloat:float, Fchar:chararray, Fchararray:chararray, Fbytearray:bytearray, Fmap:map[], Fbag:BAG{ t:tuple( name, age, avg ) }, Ftuple:( name:chararray, age:int, avg:float) );
B =GROUP A ALL; 
X =FOREACH B GENERATE DIFF( A.Fint + A.Fint + ); 
STORE X INTO '/user/pig/tests/results/araceli.1234381533/AggregateFunc_131.out' USING PigStorage();"," i686 i386 GNU/Linux
",,,,,,,,,,,,,,,,PIG-1618,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-10 18:24:48.904,,,no_permission,,,,,,,,,,,,66289,,,,,Thu Mar 10 18:24:48 UTC 2011,,,,,,,0|i0gmyn:,95150,,,,,,,,,,"10/Mar/11 18:24;xuefuz;Better error message is given now:

runt> X =FOREACH B GENERATE DIFF( A.Fint + A.Fint + );
2011-03-10 10:23:47,137 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 0: line 4:46 no viable alternative at input [@163,390:390=')',<105>,4:46]
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Map key type not correctly set (for use when key is null) when map plan does not have localrearrange,PIG-665,12414608,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,11/Feb/09 20:06,24/Mar/10 22:04,14/Mar/19 03:05,12/Feb/09 22:54,0.2.0,,,,,,0.2.0,,,,,0,,,,"KeyTypeDiscoveryVisitor visits the map plan to figure out the datatype of the map key. This is required so that when the map key is null, we can still construct a valid NullableXXXWritable object to pass on to hadoop in the collect() call (hadoop needs a valid object even for null objects). Currently the KeyTypeDiscoveryVisitor only looks at POPackage and POLocalRearrange to figure out the key type. In a pig script which results in multiple Map reduce jobs, one of the jobs could have a map plan with only POLoads in it. In such a case, the map key type is not discovered and this results in a null being returned from HDataType.getWritableComparableTypes() method. This in turn will result in a NullPointerException in the collect().

Here is a script which can prompt this behavior:
{code}
a = load 'a.txt' as (x:int, y:int, z:int);
b = load 'b.txt' as (x:int, y:int);
b_group = group b by x;
b_sum = foreach b_group generate flatten(group) as x, SUM(b.y) as clicks;
a_group = group a by (x, y);
a_aggs = foreach a_group {
            generate 
                flatten(group) as (x, y),
                SUM(a.z) as zs;
                };
join_a_b = join b_sum by x, a_aggs by x; --> the map plan for this join will only have two POLoads which will result in the NullPointerException at runtime in collect()
dump join_a_b;

{code} 

Contents of a.txt (columns are tab separated):
The first column of the first two rows is null (represented by an empty column)
{noformat}
        7       8
        8       9
1       20      30
1       20      40
{noformat}

Contents of b.txt (columns are tab separated):
{noformat}
7       2
1       5
1       10
{noformat}",,,,,,,,,,,,,,,,,,,12/Feb/09 20:19;pkamath;PIG-665.patch;https://issues.apache.org/jira/secure/attachment/12400127/PIG-665.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-02-12 22:40:43.925,,,no_permission,,,,,,,,,,,,164247,Reviewed,,,,Thu Feb 12 22:54:39 UTC 2009,,,,,,,0|i0gmwf:,95140,,,,,,,,,,"12/Feb/09 20:19;pkamath;The issue as described in the description was that when the map plan has only POLoad, KeyTypeDiscoveryVisitor fails to find the map Key type (since it is only visiting POPackage and POLocalRearrange). The fix has the following changes to address the issue:
1) The visitor no longer visits POPackage since map key type information should only come from a POLocalRearrange
2) First the visitor tries to visit POLocalRearranges in the map plan - if it does not find the key type, it looks visits the reduce plans of its predecessor MapReduceOpers. If there are no predecessors, then this could be a simple load-store script, so the visitor normally terminates. If it discovers the same key types from its predecessors, it succeeds else it aborts since it should be getting identical key type from all its predecessors (which should be having the corresponding POLocalRearranges). If the visitor is unable to discover the key type in the map plan or in the predecessors, then a check is made if there is reduce phase to curent mapReduceOper. Only if there is a reduce phase and we are unable to discover the key type, the visitor aborts with a failure.

I have added a unit test case for the script in this issue. I have also added a visit method in POCombinerPackage as part of this patch since it was missing one.  ","12/Feb/09 22:40;olgan;+1; please, commit",12/Feb/09 22:54;pkamath;Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Semantics of * is not consistent,PIG-664,12414509,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,sms,sms,10/Feb/09 19:16,24/Mar/10 22:04,14/Mar/19 03:05,26/Feb/09 05:18,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"The semantics of * is not consistent in PIG. The use of * with generate results in the all the columns of the record being flattened. However, the use of * as an input to a UDF results in a tuple (wrapped in another tuple). For consistency, * should always result in all the columns of the record (i.e., flattened). The use of * occurs in:

1. Foreach generate: E.g.: foreach input generate *;
2. Input to UDFs: E.g. foreach input generate myUDF(*);
3. Order by: E.g.: order input by *;
4. (Co)Group: E.g.: group a by *; cogroup a by *, b by *;

In terms of implementation, this involves rolling back the fix introduced in PIG-597 and fixing the following builtin UDFs:

1. ARITY - Should return the size of the input tuple instead of extracting the first column of the input tuple
2. SIZE - Should return the size of the input tuple instead of extracting the first column of the input tuple",,,,,,,,,,,,,,,,,,,24/Feb/09 05:18;sms;PIG-664.patch;https://issues.apache.org/jira/secure/attachment/12400833/PIG-664.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-02-10 19:21:42.182,,,no_permission,,,,,,,,,,,,37998,Reviewed,,,,Thu Feb 26 05:18:40 UTC 2009,,,Patch Available,,,,0|i0gmw7:,95139,,,,,,,,,,"10/Feb/09 19:21;yhan;I would second Santhosh. In PIG 1.x, * in UDF parameter list does expend as flattened list of columns. While converting into PIG 2.0, this create a lot of inconvenience. * should always generate flattened columns.",24/Feb/09 05:18;sms;Attached patch fixes the issue. All unit test cases pass.,25/Feb/09 21:24;olgan;I am reviewing this changes. The patch looks good. Running the tests now. Will commit once they complete,26/Feb/09 05:18;sms;Patch has been committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integration with Hadoop 0.20,PIG-660,12414403,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,sms,sms,09/Feb/09 17:20,24/Mar/10 22:14,14/Mar/19 03:05,29/Sep/09 00:31,0.4.0,,,,,,0.5.0,,impl,,,0,,,,"With Hadoop 0.20, it will be possible to query the status of each map and reduce in a map reduce job. This will allow better error reporting. Some of the other items that could be on Hadoop's feature requests/bugs are documented here for tracking.

1. Hadoop should return objects instead of strings when exceptions are thrown
2. The JobControl should handle all exceptions and report them appropriately. For example, when the JobControl fails to launch jobs, it should handle exceptions appropriately and should support APIs that query this state, i.e., failure to launch jobs.",Hadoop 0.20,,,,,,,,,,,,,,,,,,03/Aug/09 21:50;pkamath;PIG-660-for-branch-0.3.patch;https://issues.apache.org/jira/secure/attachment/12415414/PIG-660-for-branch-0.3.patch,04/Mar/09 23:50;sms;PIG-660.patch;https://issues.apache.org/jira/secure/attachment/12401470/PIG-660.patch,05/Mar/09 03:29;sms;PIG-660_1.patch;https://issues.apache.org/jira/secure/attachment/12401481/PIG-660_1.patch,06/Mar/09 23:03;sms;PIG-660_2.patch;https://issues.apache.org/jira/secure/attachment/12401653/PIG-660_2.patch,11/Mar/09 01:04;sms;PIG-660_3.patch;https://issues.apache.org/jira/secure/attachment/12401885/PIG-660_3.patch,02/Jul/09 20:58;olgan;PIG-660_4.patch;https://issues.apache.org/jira/secure/attachment/12412421/PIG-660_4.patch,21/Jul/09 00:19;dvryaboy;PIG-660_5.patch;https://issues.apache.org/jira/secure/attachment/12414043/PIG-660_5.patch,21/Aug/09 18:42;olgan;PIG-660_trunk.patch;https://issues.apache.org/jira/secure/attachment/12417297/PIG-660_trunk.patch,08/Sep/09 18:12;daijy;PIG-660_trunk_2.patch;https://issues.apache.org/jira/secure/attachment/12418947/PIG-660_trunk_2.patch,21/Aug/09 18:40;olgan;hadoop20.jar.gz;https://issues.apache.org/jira/secure/attachment/12417296/hadoop20.jar.gz,05/Aug/09 02:42;dvryaboy;pig_660_shims.patch;https://issues.apache.org/jira/secure/attachment/12415567/pig_660_shims.patch,05/Aug/09 15:31;dvryaboy;pig_660_shims_2.patch;https://issues.apache.org/jira/secure/attachment/12415618/pig_660_shims_2.patch,06/Aug/09 22:58;dvryaboy;pig_660_shims_3.patch;https://issues.apache.org/jira/secure/attachment/12415790/pig_660_shims_3.patch,13.0,,,,,,,,,,,,,,,,,,,2009-07-02 20:58:26.57,,,no_permission,,,,,,,,,,,,164243,,,,,Tue Sep 29 00:31:22 UTC 2009,,,,,,,0|i0gmu7:,95130,,,,,,,,,,"09/Feb/09 17:29;sms;JIRAs in Hadoop corresponding to items 1 and 2.

1. https://issues.apache.org/jira/browse/HADOOP-5201
2. https://issues.apache.org/jira/browse/HADOOP-5202",04/Mar/09 23:50;sms;Patch to integrate PIG with Hadoop 20. This patch switches off the deprecation warnings and fixes a NPE.,05/Mar/09 03:29;sms;New patch with TestHBaseStorage.java excluded from unit testing.,06/Mar/09 21:45;sms;New patch that ensures that pig specific properties are picked up and reasonable error messages are returned when backend errors (that cannot be parsed) occur.,06/Mar/09 23:03;sms;Updated patch in synchrony with the latest sources.,11/Mar/09 01:04;sms;Latest patch in synchrony with PIG trunk. Also has a fix for the number of reducers for order by when parallel is not used in the script.,"02/Jul/09 20:58;olgan;Updated patch:

(1) Applies without warnings to the current trunk.
(2) Resolves TestCounter failures. Thanks, Arun, for help with this.",21/Jul/09 00:19;dvryaboy;Updating the patch to set PIG_HADOOP_VERSION to 20 by default.,"28/Jul/09 20:20;rangadi;Currently, hadoop jar for 0.18 under lib/ is called hadoop18.jar. Should we change build.xml to use hadoop20.jar instead of hadoop18.jar?

I can file a jira to commit hadoop20.jar. This might be replaced by updated jar when this jira is committed.",28/Jul/09 20:50;sms;The build.xml in the patch(es) have the reference to hadoop20.jar. The missing part is the hadoop20.jar that Pig can use to build its sources. Pig cannot use the hadoop20.jar coming from the Hadoop release.,"28/Jul/09 20:55;olgan;Raghu, please, add the hadoop20.jar that Zebra is using. We can commit it with the understanding that we will overwrite once we commit hadoop 20 support into PIg","28/Jul/09 21:11;rangadi;Thanks Olga and Santosh.

build.xml change is already in the patch. Thanks.

I will attach hadoop20.jar that works with PIG. This is useful for anyone to tryout the patch. This will also be used by zebra (PIG-833). Please commit the jar file to PIG trunk. It could be updated with a later version of hadoop-0.20 branch.",28/Jul/09 21:28;rangadi;Updated patch fixes two minor conflicts with the current pig trunk.,"28/Jul/09 21:28;dvryaboy;Santosh and Olga -- could you document the differences between a version of 20 Pig can use and that in the Hadoop release? Links to necessary patches, etc?","28/Jul/09 21:34;olgan;removed the latest attachment - I think there is a bit of confusion. We don't need a new patch, just a separate hadoop jar that works with the official hadoop 20 release.","03/Aug/09 21:50;pkamath;Attached a patch for ""branch-0.3"" based on PIG-660_5.patch. The only difference is that a couple of files (HConfiguration.java and HDataStorage.java) need ctrl-M end of lines for the patch to apply correctly to branch-0.3","05/Aug/09 02:42;dvryaboy;Attached patch, pig_660_shims.patch, introduces an compatibility layer similar to that in https://issues.apache.org/jira/browse/HIVE-487 . HadoopShims.java contains wrappers that hide interface differences between Hadoop 18 and 20; when an interface change affects Pig, a shim is added into this class, and used by Pig.

Separate versions of the shims are maintained for different Hadoop versions.

This way, Pig users can compile against either Hadoop 18 or Hadoop 20 by simply changing an ant property, either via the -D flag, or build.properties, instead of having to go through the process of patching.

There has been discussion of officially moving Pig to 0.20; this way, we sidestep the whole question, and only need to worry about version compatibility when using specific Hadoop APIs.

I propose that we use this mechanism until Pig is moved to use the new, future-proofed API.  

Pig compiled against 18 won't be able to use some of the newest features, such as Zebra storage. Ant can be configured not to build ant if Hadoop version is < 20.
","05/Aug/09 05:52;daijy;Hi, Dmitriy, 
I like your idea. One comment, in src/20/java/org/apache/pig/shims/HadoopShims.java, the package line is ""org.apache.hadoop.hive.shims"", I guess it is a typo right?",05/Aug/09 15:31;dvryaboy;Sure is.. uploading a patch with the fixed package name. ,"06/Aug/09 20:39;dvryaboy;The shim patch posted above doesn't work as cleanly as desired; the current build.xml has junit.hadoop.conf points to a directory in ${user.home}

This has an undesired effect -- a hadoop config file gets created the first time you run ant, which among other things sets what class implements the FileSytem interface. When ant gets re-run with a different hadoop version, 'ant clean' does not clean out this file -- so an incorrect fs class name gets used.  Deleting the directory created by junit.hadoop.conf before rerunning fixes the problem; so does putting the value of junit.hadoop.conf relative to ${build.dir} instead of ${user.home}.  

As I am not sure how the Y! developers use their pigconf directories this thing references, I do not know the appropriate way to proceed. Comments?","06/Aug/09 22:58;dvryaboy;The attached patch fixes the mentioned issue with junit.hadoop.conf by setting it to $build.dir/conf
This can be overridden by build.properties if individual contributors want to revert to the old behavior.

Also added a compatibility shim for hadoop19 (from PIG-573)","07/Aug/09 00:50;jashmenn;By applying this patch to r801032 and changing the hadoop.version = 20, I'm still getting problems when trying to use pig against hadoop 20.

Error and trace:

2009-08-07 00:45:48,549 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://x.x.x.x:54310
2009-08-07 00:45:48,834 [main] ERROR org.apache.pig.Main - ERROR 2999: Unexpected internal error. Failed to create DataStorage
2009-08-07 00:45:48,834 [main] ERROR org.apache.pig.Main - java.lang.RuntimeException: Failed to create DataStorage
        at org.apache.pig.backend.hadoop.datastorage.HDataStorage.init(HDataStorage.java:75)
        at org.apache.pig.backend.hadoop.datastorage.HDataStorage.<init>(HDataStorage.java:58)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.init(HExecutionEngine.java:198)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.init(HExecutionEngine.java:137)
        at org.apache.pig.impl.PigContext.connect(PigContext.java:180)
        at org.apache.pig.PigServer.<init>(PigServer.java:169)
        at org.apache.pig.PigServer.<init>(PigServer.java:158)
        at org.apache.pig.tools.grunt.Grunt.<init>(Grunt.java:54)
        at org.apache.pig.Main.main(Main.java:347)
Caused by: java.io.IOException: Call failed on local exception
        at org.apache.hadoop.ipc.Client.call(Client.java:718)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:216)
        at org.apache.hadoop.dfs.$Proxy0.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:319)
        at org.apache.hadoop.dfs.DFSClient.createRPCNamenode(DFSClient.java:103)
        at org.apache.hadoop.dfs.DFSClient.<init>(DFSClient.java:173)
        at org.apache.hadoop.dfs.DistributedFileSystem.initialize(DistributedFileSystem.java:67)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1339)
        at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:56)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1351)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:213)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:118)
        at org.apache.pig.backend.hadoop.datastorage.HDataStorage.init(HDataStorage.java:72)
        ... 8 more
Caused by: java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:375)
        at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:499)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:441","07/Aug/09 01:01;dvryaboy;Nate,
Your stacktrace shows hadoop.dfs calls (as opposed to hdfs) which tells me it's looking for -- and finding -- hadoop 18 classes.

Can you do this:

export PIG_HADOOP_VERSION=20
ant clean; ant -Dhadoop.version=20

any try again?

Just to be sure, try moving hadoop1* out of the lib directory (so that it for sure fails if it's trying to look for 18).","07/Aug/09 18:20;jashmenn;Dmitriy, thanks for the feedback. I did an ant clean and ant -Dhadoop.version=20 as you suggested. That alone did not work, however, when I deleted hadoop17.jar and hadoop18.jar *then* it worked perfectly.

Thanks again,

Nate","21/Aug/09 18:40;olgan;Compressed to fit JIRA size limit. Please, uncompress before placing in lib directory","21/Aug/09 18:43;olgan;Submitted 

(1) Patch to apply to the latest trunk to work with the official Hadoop 20 release
(2) Compressed version of hadoop20.jar. Need to be uncompressed and placed in the lib dir.

This should allow running Pig against Hadoop 20 cluster",08/Sep/09 18:12;daijy;Resync with trunk.,29/Sep/09 00:31;olgan;patch was committed a while back,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Data type long : When 'L' or 'l'  is included with data (123L or 123l) load produces null value.,PIG-658,12414259,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,chandec,chandec,06/Feb/09 20:23,24/Mar/10 22:04,14/Mar/19 03:05,23/Feb/09 21:16,0.2.0,,,,,,0.2.0,,data,,,0,,,,"In data file, you should be able to include L or l for long (same as F or f for floats).

EXAMPLE

grunt> cat long.txt
4829090493980522200L

grunt> a = load 'long.txt' as f1:long;

grunt> dump a;

2009-02-06 20:18:04,373 [main] WARN  org.apache.pig.builtin.PigStorage - Unable to interpret value [B@1d2fc36 in field being converted to long, caught NumberFormatException <For input string: ""4829090493980522200L""> field discarded
2009-02-06 20:18:04,375 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - 100% complete!
2009-02-06 20:18:04,375 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Success!!
()


",,,,,,,,,,,,,,,,,,,23/Feb/09 18:04;thejas;PIG-658.txt;https://issues.apache.org/jira/secure/attachment/12400776/PIG-658.txt,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-02-23 18:04:55.654,,,no_permission,,,,,,,,,,,,38497,Reviewed,,,,Mon Feb 23 21:16:52 UTC 2009,,,Patch Available,,,,0|i0gmtb:,95126,,,,,,,,,,"23/Feb/09 18:04;thejas;Handles long/float values with l/L/f/F at the end in Utf8StorageConverter.java bytesToLong, bytesToFloat .
Added unit tests to TestConversions.java . ",23/Feb/09 21:16;sms;Changes look fine. Patch has been committed. Thanks for the fix Thejas.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use of eval or any other keyword in the package hierarchy of a UDF causes parse exception,PIG-656,12414067,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,milindb,viraj,viraj,05/Feb/09 01:33,26/Jul/10 16:35,14/Mar/19 03:05,26/May/09 20:25,0.3.0,,,,,,0.3.0,,documentation,grunt,,0,,,,"Consider a Pig script which does something similar to a word count. It uses the built-in TOKENIZE function, but packages it inside a class hierarchy such as ""mypackage.eval""
{code}
register TOKENIZE.jar
my_src  = LOAD '/user/viraj/mywordcount.txt' USING PigStorage('\t')  AS (mlist: chararray);
modules = FOREACH my_src GENERATE FLATTEN(mypackage.eval.TOKENIZE(mlist));
describe modules;
grouped = GROUP modules BY $0;
describe grouped;
counts  = FOREACH grouped GENERATE COUNT(modules), group;
ordered = ORDER counts BY $0;
dump ordered;
{code}

The parser complains:
===========================================================================================================================
2009-02-05 01:17:29,231 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Invalid alias: mypackage in {mlist: chararray}
===========================================================================================================================

I looked at the following source code at (src/org/apache/pig/impl/logicalLayer/parser/QueryParser.jjt) and it seems that : EVAL is a keyword in Pig. Here are some clarifications:

1) Is there documentation on what the EVAL keyword actually is?
2) Is EVAL keyword actually implemented?

Viraj",,,,,,,,,,,,,,,,,,,05/Feb/09 01:36;viraj;TOKENIZE.jar;https://issues.apache.org/jira/secure/attachment/12399504/TOKENIZE.jar,05/Feb/09 01:38;viraj;mywordcount.txt;https://issues.apache.org/jira/secure/attachment/12399505/mywordcount.txt,23/Jul/10 17:23;aniket486;pigusergroup656.patch;https://issues.apache.org/jira/secure/attachment/12450328/pigusergroup656.patch,23/May/09 19:28;milindb;reserved.patch;https://issues.apache.org/jira/secure/attachment/12408885/reserved.patch,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2009-02-13 22:14:07.953,,,no_permission,,,,,,,,,,,,164240,Reviewed,,,,Mon Jul 26 16:35:54 UTC 2010,,,,,,,0|i0gmsn:,95123,,,,,,,,,,05/Feb/09 01:36;viraj;TOKENIZE.jar with java source file included,05/Feb/09 01:38;viraj;Test data,13/Feb/09 22:14;olgan;eval is a reserved word in pig.,"19/May/09 20:23;viraj;Another pig parse issue when a udf was defined within a package which had ""matches"" keywords in its path.

So something like :
define DISTANCE_SCORE mypackage.pig.udf.matches.LevensteinMatchUDF();

gives a parse error

""
ERROR 1000: Error during parsing. Encountered "" ""matches"" ""matches """" at 
line 11, column 42.
Was expecting:
     <IDENTIFIER> ...
""

It is possible to have keywords from pig within package names or even  udf - shouldn't pig not be robust to simple grammar disambiguation of  this sort ?","19/May/09 20:32;viraj;Olga the problem with ""eval"" keyword is that it is not found in the manuals, http://hadoop.apache.org/pig/docs/r0.2.0/piglatin.html. 

The only way the Pig user can get a hold of this keyword is opening the file,  QueryParser.jjt and looking for the following line TOKEN : { <EVAL : ""eval""> }

The Pig Manual should list all valid keywords in Pig (implemented or not).

Please let me know.

Viraj",19/May/09 20:33;viraj;Documentation should be updated on the eval keyword and what it actually does otherwise the user can be lost trying to find out the error.,"22/May/09 23:59;milindb;This patch allows the use of reserved words in function names. To avoid parsing ambiguity, the first part of the fully qualified function name (i.e. test before the first ""."") cannot be a reserved word. But the rest of the parts in fully qualified function names can be any identifier, including a reserved word.

So, for example, with this patch, the statement:

{code}
define X com.yahoo.load();
{code}

or

{code}
modules = FOREACH my_src GENERATE FLATTEN(mypackage.eval.TOKENIZE(mlist));
{code}

Now compiles and runs perfectly well.","23/May/09 01:38;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12408857/reserved.patch
  against trunk revision 777708.

    +1 @author.  The patch does not contain any @author tags.

    +0 tests included.  The patch appears to be a documentation patch that doesn't require tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/55/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/55/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/55/console

This message is automatically generated.",23/May/09 19:26;milindb;modifying patch to include test case.,23/May/09 19:28;milindb;Uploading a modified patch that now includes a test case. The findbugs warning is not new to this patch.,"23/May/09 22:28;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12408885/reserved.patch
  against trunk revision 777708.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/56/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/56/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/56/console

This message is automatically generated.",26/May/09 17:57;sms;+1 for the patch.,"26/May/09 20:25;sms;Patch has been committed. Milind, thanks for fixing this issue.","27/May/09 11:43;hudson;Integrated in Pig-trunk #455 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/455/])
    : Use of eval or any other keyword in the package hierarchy of a UDF causes parse exception (milindb via sms)
","26/Jul/10 16:35;aniket486;""eq"",""gt"",""lt"",""gte"",""lte"",""neq"" were missed as part of this fix. Opened jira at https://issues.apache.org/jira/browse/PIG-1517 to track further changes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Comparison of schemas of bincond operands is flawed,PIG-655,12414057,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,sms,sms,04/Feb/09 23:40,24/Mar/10 22:04,14/Mar/19 03:05,03/Mar/09 22:07,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"The comparison of schemas of bincond is flawed. Instead of comparing the field schemas, the type checker is comparing the schemas.",,,,,,,,,,,,,,,,,,,20/Feb/09 23:43;sms;PIG-655.patch;https://issues.apache.org/jira/secure/attachment/12400640/PIG-655.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-03-03 02:36:26.857,,,no_permission,,,,,,,,,,,,38493,Reviewed,,,,Tue Mar 03 22:07:51 UTC 2009,,,Patch Available,,,,0|i0gmsf:,95122,,,,,,,,,,20/Feb/09 23:43;sms;Attached patch fixes the issue of comparing fieldschemas in bincond during type checking. All unit test cases pass.,03/Mar/09 02:36;pkamath;I will be reviewing this patch,"03/Mar/09 02:54;pkamath;+1, Patch committed. Thanks for the contribution Santhosh.",03/Mar/09 22:07;sms;Marking the issue as fixed. Patch has been committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig should look for and use the pig specific 'pig-cluster-hadoop-site.xml' in the non HOD case just like it does in the HOD case,PIG-650,12413666,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,pkamath,pkamath,30/Jan/09 23:12,25/Mar/10 00:12,14/Mar/19 03:05,10/Mar/09 23:16,0.2.0,,,,,,0.2.0,,,,,0,,,,Currently users can create a pig-cluster-hadoop-site.xml with pig specific overrides for hadoop properties for use on the cluster. This file is searched for in the classpath and used in the HOD case but not in the non HOD case. We should also do the same in the non HOD case.,,,,,,,,,,,,,,,,,,,07/Mar/09 05:10;sms;PIG-650.patch;https://issues.apache.org/jira/secure/attachment/12401674/PIG-650.patch,09/Mar/09 20:15;sms;PIG-650_1.patch;https://issues.apache.org/jira/secure/attachment/12401773/PIG-650_1.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-03-07 05:10:24.79,,,no_permission,,,,,,,,,,,,164236,Reviewed,,,,Tue Mar 10 23:16:44 UTC 2009,,,Patch Available,,,,0|i0gmqf:,95113,,,,,,,,,,07/Mar/09 05:10;sms;Attached patch addresses the issue of picking up pig-hadoop_site.xml in the non-HOD case. There are no unit test cases as this involves checking if the PIG values are picked up. The testing was performed manually.,"09/Mar/09 17:35;olgan;What is the order in which different properties are applied?

I think the right order should be:

(1) hadoop-site.xml for the cluster (from hod, or the one defined for static clusters)
(2) hadoop-site.xml that pig sets for the cluster
(3) user specified properties via -D switch.

Can you, please, confirm that this is what happens? Thanks.","09/Mar/09 17:45;sms;The order in which the properties are applied are:

1. hadoop-site.xml from a static cluster or from hod
2. hadopp-site.xml from pig
3. user specified properties on the command line

In terms of one overriding the other, its in the reverse order

1. user specified properties on the command line overrides
2. hadopp-site.xml from pig which overrides
3. hadoop-site.xml from a static cluster or from hod

I noticed that I have a change in my patch which is not required, I will resubmit a patch with this change removed.

{code}
-            jobClient = new JobClient(new JobConf(configuration));
+            JobConf jobConf = new JobConf(configuration);
+            jobConf.addResource(""pig-cluster-hadoop-site.xml"");
+            jobClient = new JobClient(jobConf);
{code}",09/Mar/09 20:15;sms;Attaching a new patch that undoes a previous change.,"10/Mar/09 21:09;olgan;+1, please, commit",10/Mar/09 23:16;sms;Patch has been committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RandomSampleLoader does not handle skipping correctly in getNext(),PIG-649,12413661,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,30/Jan/09 22:15,24/Mar/10 22:04,14/Mar/19 03:05,31/Jan/09 02:05,0.2.0,,,,,,0.2.0,,,,,0,,,,Currently RandomSampleLoader calls skip() on the underlying input stream (BufferedPositionedInputStream) in its getNext(). The input stream may not actually skip over the amount the RandomSampleLoader needs in one call. RandomSampleLoader should check the return value from the skip() call and ensure that skip() is called repeatedly (if necessary) till the needed number of bytes are skipped.,,,,,,,,,,,,,,,,,,,31/Jan/09 01:44;pkamath;PIG-649.patch;https://issues.apache.org/jira/secure/attachment/12399193/PIG-649.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-01-31 01:57:57.467,,,no_permission,,,,,,,,,,,,164235,Reviewed,,,,Sat Jan 31 02:05:21 UTC 2009,,,,,,,0|i0gmpz:,95111,,,,,,,,,,"31/Jan/09 01:43;pkamath;The attached patch fixes the issue by keep track of the return value of the underlying input stream's skip(). If enough bytes are not skipped on the initial call, multiple calls are made till enough bytes are skipped","31/Jan/09 01:57;olgan;+1, please, commit",31/Jan/09 02:05;pkamath;Patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BinStorage fails when it finds markers unexpectedly in the data,PIG-648,12413659,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,30/Jan/09 21:51,24/Mar/10 22:04,14/Mar/19 03:05,31/Jan/09 02:06,0.2.0,,,,,,0.2.0,,,,,0,,,,"The current record begin marker used in BinStorage is the consecutive sequence- 0x21,0x31,0x41 - these byte correspond to the ascii characters ""!1A"". This sequence is not very strong as a marker - this results in failures when the sequence occurs in the data - the markers should be control characters which have a high probability of not occurring in the data.",,,,,,,,,,,,,,,,,,,31/Jan/09 01:47;pkamath;PIG-648.patch;https://issues.apache.org/jira/secure/attachment/12399194/PIG-648.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-01-31 01:56:37.697,,,no_permission,,,,,,,,,,,,164234,Incompatible change,Reviewed,,,Sat Jan 31 02:06:26 UTC 2009,,,,,,,0|i0gmpj:,95109,,,,,,,,,,"31/Jan/09 01:47;pkamath;Attached patch which now uses ctrl-A, Ctrl-B, Ctrl-C (0x01,0x02,0x03) as teh rcord markers.","31/Jan/09 01:56;olgan;+1, please commit","31/Jan/09 02:06;pkamath;The change will invalidate existing files written out in BinStorage. These will now need to be re-created using current BinStorage.

Patch committed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
memory sized passed on pig command line does not get propagated to JobConf,PIG-647,12413509,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,olgan,olgan,29/Jan/09 02:18,25/Mar/10 00:12,14/Mar/19 03:05,30/Jan/09 05:06,,,,,,,0.2.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,30/Jan/09 04:20;sms;PIG-647.patch;https://issues.apache.org/jira/secure/attachment/12399095/PIG-647.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-01-29 08:29:48.308,,,no_permission,,,,,,,,,,,,164233,Reviewed,,,,Fri Jan 30 05:06:33 UTC 2009,,,,,,,0|i0gmp3:,95107,,,,,,,,,,"29/Jan/09 02:19;olgan;if we pass this on command line, we still see -Dmapred.child.java.opts=""-Xmx1024m""' , we don't see this reflected in JobConf passed to hadoop.
",29/Jan/09 08:29;acmurthy;Can we consider using org.apache.hadoop.uti.Tool (http://hadoop.apache.org/core/docs/r0.18.2/mapred_tutorial.html#Tool) ? You'd get all this for free...,30/Jan/09 04:20;sms;Attached patch fixes the issue with not passing the command line parameters to job configuration. The root cause of the issue was not overriding HOD's parameters with the command line parameters. There are no unit test cases as HOD testing is not covered in unit tests. Manual tests were performed to verify the fix.,30/Jan/09 04:26;alangates;+1,30/Jan/09 05:03;sms;All unit tests passed.,30/Jan/09 05:06;sms;Patch has been committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Distinct UDF should report progress,PIG-646,12413508,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,olgan,olgan,29/Jan/09 02:17,25/Mar/10 00:12,14/Mar/19 03:05,30/Jan/09 03:59,,,,,,,0.2.0,,,,,0,,,,We have seen the case where if a bag passed to distinct UDF is too large to fit into main memory the job fails with a time out because no progress is reported.,,,,,,,,,,,,,,,,,,,30/Jan/09 03:36;sms;PIG-646.patch;https://issues.apache.org/jira/secure/attachment/12399094/PIG-646.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-01-30 03:36:10.422,,,no_permission,,,,,,,,,,,,38605,Reviewed,,,,Fri Jan 30 03:59:34 UTC 2009,,,Patch Available,,,,0|i0gmon:,95105,,,,,,,,,,"30/Jan/09 03:36;sms;Attached patch fixes the issue with reporting progress in Distinct.java and also sets up the reporter correctly in PigCombiner.java and PigMapReduce.ReduceWithComparator. In addition there are unit test cases in TestBuiltin.java and TestEvalPipeline.java

All unit tests pass.","30/Jan/09 03:47;olgan;+1, please commit",30/Jan/09 03:59;sms;Patch has been committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming is broken with the latest trunk,PIG-645,12413506,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,olgan,olgan,29/Jan/09 02:07,24/Mar/10 22:04,14/Mar/19 03:05,30/Jan/09 02:14,0.2.0,,,,,,0.2.0,,,,,0,,,,Several tests we run are failing now,,,,,,,,,,,,,,,,,,,30/Jan/09 01:47;pkamath;PIG-645.patch;https://issues.apache.org/jira/secure/attachment/12399084/PIG-645.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-01-30 01:47:05.687,,,no_permission,,,,,,,,,,,,164232,,,,,Fri Jan 30 02:12:36 UTC 2009,,,,,,,0|i0gmo7:,95103,,,,,,,,,,"30/Jan/09 01:47;pkamath;Attached patch to fix the issues with streaming. The root cause of the issue was the changes introduced by PIG-629 (PERFORMANCE: Eliminate use of TargetedTuple for each input tuple in the map()) caused a race condition on the input tuple in the map() between recordReader.next(Tuple value) and the streaming binary.

BEFORE PIG-629, the flow of a tuple from record reader to map was as follows:
The recordReader instance gets the *same* TargetedTuple object reference in every next(TargetedTuple value) call (this is because Hadoop reuses the value object for each recordReader.next(value) call). The recordReader.next(value) call inturn calls PigSlice.next(Tuple value) which has the following implementation:
{code}
public boolean next(Tuple value) throws IOException {
        Tuple t = loader.getNext();
        if (t == null) {
            return false;
        }
        value.reference(t);
        return true;
    }
{code}
Here value.reference(t) calls the TargetedTuple.reference(Tuple) method which simply stores the supplied the tuple in its member Tuple variable ""t"". 

In PigMapBase.map(), the toTuple() method on the input TargetedTuple is called which returns the above store tuple reference ""t"". This reference is then attached to the roots of the map plan. 

The point to note is this final tuple reference which is used by the operators in the map plan is the reference to the tuple returned from the loader and not the reference to the TargetedTuple which we get from the recordReader and which is supplied as an argument to the map() call. The loader creates a new tuple reference on each getNext(). This guarantees that the operators in the map plan always work with a differnt tuple reference on each map() call though the TargetedTuple reference supplied in the map() is the same and reused by Hadoop.

AFTER PIG-629, the flow changed as follows:
TargetedTuple was removed and Tuple was used instead. The PigSlice.next(Tuple value) code remained intact. However DefaultTuple.reference(Tuple) call in it assigns the internal mFields arraylist to the arraylist of the supplied tuple. Note that here the internal member arraylist of the DefaultTuple is changed to ""refer"" to the internal arraylist of the Tuple the loader gives. 
In map(), the tuple which is supplied as input argument to the map() call is attached directly to the roots. So in the case of streaming, this tuple is finally supplied to the binary by using a storage function (PigStorage by default). However this tuple refernce is the same as the one which gets reused by hadoop in the next recordReader.next(value) call. So while the storage function is in the process of writing the current Tuple's contents (the mFields arraylist), it can get changed underneath due to recordReader.next(value) call. So unless the storage functions writes to the binary's stdin BEFORE the next recordReader.next(value) call, the input sent to the Binary will be garbled.

The fix is the following one line change:
{noformat}
         for (PhysicalOperator root : roots) {
-            root.attachInput(inpTuple);
+            root.attachInput(tf.newTupleNoCopy(inpTuple.getAll()));
         }

{noformat}

In map(), instead of attaching the inpTuple directly to the roots of the plan, a new Tuple is created which refers to the same mFields arrayList as in inpTuple. With this change, all operators in the map plan, now work on a different Tuple reference from the one which is supplied in the map() argument (and which is reused by Hadoop). This reference will refer to the mFields of the Tuple returned from the loader which is guaranteed to be a new arraylist for each input tuple since the loader creates a new Tuple each time. ","30/Jan/09 01:55;olgan;+1, please, commit",30/Jan/09 02:12;pkamath;Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate column names in foreach do not throw parser error,PIG-644,12413497,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,viraj,viraj,28/Jan/09 23:21,02/May/13 02:29,14/Mar/19 03:05,19/Oct/09 21:18,0.4.0,,,,,,0.6.0,,impl,,,0,,,,"Consider the following Pig script where we generate column names b and b in the FOREACH
{code}
DATA = LOAD 'blah.txt' as (a:long, b:long);
RESULT = FOREACH DATA GENERATE a, b, (b>20?b:0) as b;
DESCRIBE RESULT;
dump RESULT;
{code}

Pig runs the script successfully and does not complain of the duplicate column names.  I do not know if the new error handling framework will handle these kinds of cases. ",,,,,,,,,,,,,,,,,PIG-485,PIG-584,16/Oct/09 06:09;daijy;PIG-644-1.patch;https://issues.apache.org/jira/secure/attachment/12422324/PIG-644-1.patch,28/Jan/09 23:23;viraj;blah.txt;https://issues.apache.org/jira/secure/attachment/12398944/blah.txt,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-10-16 06:09:55.293,,,no_permission,,,,,,,,,,,,164231,Reviewed,,,,Mon Oct 19 21:24:25 UTC 2009,,,,,,,0|i0gmnr:,95101,,,,,,,,,,28/Jan/09 23:23;viraj;Sample input,28/Jan/09 23:28;viraj;Error Handling in Pig 2.0,"29/Jan/09 01:38;viraj;Santhosh had reported this error previously, which I have linked to the error handling ",16/Oct/09 06:09;daijy;Add a SchemaAliasValidator to do this check.,"17/Oct/09 06:27;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422324/PIG-644-1.patch
  against trunk revision 826110.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 307 release audit warnings (more than the trunk's current 305 warnings).

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/91/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/91/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/91/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/91/console

This message is automatically generated.","19/Oct/09 20:37;pkamath;+1
",19/Oct/09 21:18;daijy;Patch committed,19/Oct/09 21:24;daijy;Also note this change may broke some existing scripts if they do contain duplicated schema alias.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit after FRJ causes problems,PIG-642,12413410,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,olgan,olgan,27/Jan/09 20:39,24/Mar/10 22:04,14/Mar/19 03:05,01/Feb/09 20:39,,,,,,,0.2.0,,,,,0,,,,"Script:

a = load '/user/pig/tests/data/singlefile/studenttab10k' as (name, age,gpa);
b = load '/user/pig/tests/data/singlefile/studenttab10k' as (name, age,gpa);
c = join a by name, b by name using ""replicated"";
d = limit c 10;
dump d;

Error: ERROR 2013: Moving LOLimit in front of LOFRJoin is not implemented

It is fine not to move limit and apply it after the join.
",,,,,,,,,,,,,,,,,,,27/Jan/09 22:59;daijy;PIG-642.patch;https://issues.apache.org/jira/secure/attachment/12398852/PIG-642.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-01-27 22:59:44.016,,,no_permission,,,,,,,,,,,,164229,,,,,Tue Jan 27 23:36:27 UTC 2009,,,,,,,0|i0gmmv:,95097,,,,,,,,,,27/Jan/09 22:59;daijy;Here is the patch for reference.,27/Jan/09 23:36;olgan;I am reviewing this patch now,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fragment replicate join does not work in local mode,PIG-641,12413408,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,shubhamc,olgan,olgan,27/Jan/09 20:36,24/Mar/10 22:13,14/Mar/19 03:05,11/Sep/09 22:30,,,,,,,0.4.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,10/Feb/09 16:39;shubhamc;641.patch;https://issues.apache.org/jira/secure/attachment/12399929/641.patch,07/Feb/09 07:55;shubhamc;641.patch;https://issues.apache.org/jira/secure/attachment/12399715/641.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-02-07 07:55:25.272,,,no_permission,,,,,,,,,,,,164228,,,,,Fri Sep 11 22:30:53 UTC 2009,,,,,,,0|i0gmmf:,95095,,,,,,,,,,"07/Feb/09 07:55;shubhamc;Added a small modification in query parser to treat FRJoin as regular joins since FRJoin - the algorithm, is redundant in the local mode. Also added test cases similar to the ones for FRJoin for the local mode as well.",09/Feb/09 21:52;alangates;The general approach here of rewriting fr join into a standard join in local mode is fine.  But the parser is the wrong place to do this.  We don't want to be doing backend specific operations during parsing.  This should be moved to the local logical to physical translator.,10/Feb/09 16:39;shubhamc;I have added a local POFRJoinLocal operator that extends POFRJoin and uses the hashmap based implementation of joins similar to POFRJoin.,"10/Feb/09 16:52;alangates;Why do we need a new POFRJoinLocal operator?  Can't the solution be that a logical plan of:

{code}
X   Y
|   |
\  /
  |
FRJoin
{code}

gets translated to a physical plan of:

{code}
X   Y
|   |
\  /
  |
Join
{code}

(that is, just a regular join) in local mode?  Is there any reason to have a special operator?
","10/Feb/09 18:15;shubhamc;There is no single join operator in local mode. Like the MR mode, it depended on the rewriting of joins to cogroup + foreach done in the parser. But FRJoinLocal can be the single join operator now and can work as an alternative if anymore join improvements are added to the MR mode.
",10/Feb/09 18:29;alangates;So why can't FRJoin be written to Cogroup + Foreach as well?  I don't see the advantage of adding a new operator that will do exactly the same thing as two existing operators put together.,"05/Mar/09 21:38;olgan;Shubham, could you, please, reply to Alan's question, thanks.","11/Sep/09 22:30;pkamath;This issue is fixed in current trunk since LocalLogToPhyTranslationVisitor always translates LOJoin into POCogroup followed by foreach flatten regardless of join type.

Here is a script I tried to validate:
[pradeepk@chargesize:~/dev/pig-apache/pig/trunk]cat a.txt 
1       2       3
2       3       4
3       4       5
[pradeepk@chargesize:~/dev/pig-apache/pig/trunk]cat b.txt 
3       a
1       x
4       b
[pradeepk@chargesize:~/dev/pig-apache/pig/trunk]cat c.txt 
1       20      30
[pradeepk@chargesize:~/dev/pig-apache/pig/trunk]java -cp /tmp/svncheckout/trunk/pig.jar org.apache.pig.Main -x local -e ""a = load 'a.txt'; b = load 'b.txt'; c = load 'c.txt'; d = join a by \$0, b by \$0 using \""replicated\"";  dump d; e = join a by \$0, c by \$0 using \""replicated\""; dump  e;""
2009-09-11 15:27:54,852 [main] INFO  org.apache.pig.Main - Logging error messages to: /homes/pradeepk/dev/pig-apache/pig/trunk/pig_1252708074851.log
2009-09-11 15:27:55,217 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Successfully stored result in: ""file:/tmp/temp-1388892738/tmp1991974517""
2009-09-11 15:27:55,218 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Records written : 2
2009-09-11 15:27:55,218 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Bytes written : 0
2009-09-11 15:27:55,218 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - 100% complete!
2009-09-11 15:27:55,218 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Success!!
(1,2,3,1,x)
(3,4,5,3,a)
2009-09-11 15:27:55,253 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Successfully stored result in: ""file:/tmp/temp-1388892738/tmp84396309""
2009-09-11 15:27:55,253 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Records written : 1
2009-09-11 15:27:55,253 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Bytes written : 0
2009-09-11 15:27:55,254 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - 100% complete!
2009-09-11 15:27:55,254 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Success!!
(1,2,3,1,20,30)
[pradeepk@chargesize:~/dev/pig-apache/pig/trunk]
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
limit with order by is broken in local mode,PIG-637,12413339,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,shubhamc,olgan,olgan,27/Jan/09 02:07,25/Mar/10 00:12,14/Mar/19 03:05,06/Feb/09 22:35,,,,,,,0.2.0,,,,,0,,,,"Shubham, could you take a look.

The following script when ran in local mode just ignores the limit and outputs the entire data set:

a = load 'studenttab10k' as (name, age,gpa);
b = order a by name;
c = limit b 10;
dump c;

The same script works fine in MR mode",,,,,,,,,,,,,,,,,,,06/Feb/09 18:38;shubhamc;637.patch;https://issues.apache.org/jira/secure/attachment/12399667/637.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-02-06 18:38:30.924,,,no_permission,,,,,,,,,,,,164224,,,,,Fri Feb 06 22:35:30 UTC 2009,,,,,,,0|i0gmkn:,95087,,,,,,,,,,"06/Feb/09 18:38;shubhamc;This happens because the optimizer eliminates the limit after a sort and puts an attribute in POSort/LOSort instead. This attribute is not used in the local mode sorting as this would adversely affect the MR sorting of the samples.

I have modified the code to avoid that optimization happening when executing in the local mode. I have also added a couple of test cases that verify the plans in both local and MR mode.",06/Feb/09 21:26;olgan;I am reviewing this patch.,"06/Feb/09 22:35;olgan;patch committed, thanks, Shubham!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When POUnion is one of the roots of a map plan, POUnion.getNext() gives a null pointer exception",PIG-634,12413145,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,24/Jan/09 01:16,24/Mar/10 22:04,14/Mar/19 03:05,26/Jan/09 21:29,0.2.0,,,,,,0.2.0,,,,,0,,,,"POUnion.getnext() gives a null pointer exception in the following scenario (pasted from a code comment explaining the fix for this issue). If a script results in a plan like the one below, currently POUnion.getNext() gives a null pointer exception
{noformat}
                
                // POUnion
                // |
                // |--POLocalRearrange
                // |    |
                // |    |-POUnion (root 2)--> This union's getNext() can lead the code here
                // |
                // |--POLocalRearrange (root 1)
                
                // The inner POUnion above is a root in the plan which has 2 roots.
                // So these 2 roots would have input coming from different input
                // sources (dfs files). So certain maps would be working on input only
                // meant for ""root 1"" above and some maps would work on input
                // meant only for ""root 2"". In the former case, ""root 2"" would
                // neither get input attached to it nor does it have predecessors
{noformat}

A script which can cause a plan like above is:
{code}
a = load 'xyz'; 
b = load 'abc'; 
c = union a,b; 
d = load 'def'; 
e = cogroup c by $0 inner , d by $0 inner;
dump e;
{code}",,,,,,,,,,,,,,,,,,,26/Jan/09 18:13;pkamath;PIG-634.patch;https://issues.apache.org/jira/secure/attachment/12398750/PIG-634.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-01-26 18:20:20.428,,,no_permission,,,,,,,,,,,,164221,,,,,Mon Jan 26 21:29:43 UTC 2009,,,,,,,0|i0gmjb:,95081,,,,,,,,,,"26/Jan/09 18:12;pkamath;Attached patch which fixed POUnion.getNext() by adding the following condition:
{code}
public Result getNext(Tuple t) throws ExecException {

        if (nextReturnEOP) {
            nextReturnEOP = false ;
            return eopResult ;
        }

        // Case 1 : Normal connected plan
        if (!isInputAttached()) {
            
            if (inputs == null || inputs.size()==0) {
                // Neither does this Union have predecessors nor
                // was any input attached! This can happen when we have
                // a plan like below
                // POUnion
                // |
                // |--POLocalRearrange
                // |    |
                // |    |-POUnion (root 2)--> This union's getNext() can lead the code here
                // |
                // |--POLocalRearrange (root 1)
                
                // The inner POUnion above is a root in the plan which has 2 roots.
                // So these 2 roots would have input coming from different input
                // sources (dfs files). So certain maps would be working on input only
                // meant for ""root 1"" above and some maps would work on input
                // meant only for ""root 2"". In the former case, ""root 2"" would
                // neither get input attached to it nor does it have predecessors
                // which is the case which can lead us here.
                return eopResult;
            }
            ... rest of getNext
{code}

The check to see if inputs is null or inputs.size() is 0 is the new condition added in getNext(). This ensures that when POUnion is one of the roots of the map plan and when it receives no input (i.e. no input is attached), it will send EOP to successor.",26/Jan/09 18:20;olgan;+1 on the patch. Great catch!,26/Jan/09 21:29;pkamath;Patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improved error message for binary operators,PIG-632,12413140,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,olgan,olgan,23/Jan/09 23:16,24/Mar/10 22:04,14/Mar/19 03:05,27/Jan/09 19:37,0.2.0,,,,,,0.2.0,,,,,0,,,,"Current message: ""Incompatible types in Add Operator LHS:chararray RHS:int"". LHS and RHS might not be meaningful to users. Lets try to stick to non-abrivated version of English :).",,,,,,,,,,,,,,,,,,,27/Jan/09 02:24;sms;PIG-632.patch;https://issues.apache.org/jira/secure/attachment/12398793/PIG-632.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-01-27 02:24:59.83,,,no_permission,,,,,,,,,,,,38444,Reviewed,,,,Tue Jan 27 19:37:29 UTC 2009,,,Patch Available,,,,0|i0gmif:,95077,,,,,,,,,,"27/Jan/09 02:24;sms;Attached patch fixes the message for LHS and RHS.  In addition, some of the constructors in PigException.java were cleaned up and AssertionErrors in TypeCheckingVisitor.java were replaced with TypeCheckerExceptions. All unit test cases passed.",27/Jan/09 19:31;olgan;+1,27/Jan/09 19:37;sms;Patch has been committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4 Unit test failures on Windows ,PIG-631,12413138,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,ltucker,ltucker,23/Jan/09 22:28,04/Aug/11 00:34,14/Mar/19 03:05,09/Jan/11 00:58,0.2.0,,,,,,0.9.0,,,,,0,,,,"4 Windows unit test failures.  All timeouts.  Errors occur at tip of branch and have for several days.

    [junit] Running org.apache.pig.test.TestAlgebraicEval
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.pig.test.TestAlgebraicEval FAILED (timeout)

    [junit] Running org.apache.pig.test.TestEvalPipeline
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.pig.test.TestEvalPipeline FAILED (timeout)

    [junit] Running org.apache.pig.test.TestHBaseStorage
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.pig.test.TestHBaseStorage FAILED (timeout)

    [junit] Running org.apache.pig.test.TestMapReduce
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.pig.test.TestMapReduce FAILED (timeout)
",Windows,,,,,,,,,,,,,,,,,,28/Jan/09 02:09;daijy;PIG-631.temp.patch;https://issues.apache.org/jira/secure/attachment/12398872/PIG-631.temp.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-01-26 05:49:28.388,,,no_permission,,,,,,,,,,,,66303,,,,,Sun Jan 09 00:58:29 UTC 2011,,,,,,,0|i0gmi7:,95076,,,,,,,,,,"26/Jan/09 05:49;daijy;TestAlgebraicEval, TestEvalPipeline and TestMapReduce is fine. The problem is it takes longer time. The default timeout for unit test is 15min. Try to modify the property ""test.timeout"" in build.xml to a bigger value. 

TestHBaseStorage is blocked on line 94. I will check it out.",26/Jan/09 18:24;olgan;I am not sure why those tests should be running this long. 15 min per unit test sound like whole lot of time as it is.,"27/Jan/09 23:04;olgan;Daniels, increasing time out worked fine for Lee. Can we change the HBase unit test to not run on windows for now",27/Jan/09 23:15;daijy;I am not sure why TestHBaseStorage is blocked under cygwin. Seems HBase should work for cygwin as they claimed. I think we can exclude it for now and I will look into it later.,28/Jan/09 02:09;daijy;Temporary patch to disable TestHBaseStorage under Windows,"29/Jan/09 19:29;olgan;+1

Daniel please commit if all unit tests are now passing under linux and windows, thanks!",29/Jan/09 22:21;daijy;Patch committed. Please leave the case open until I figured out the problem with TestHBaseStorage. Thanks!,"01/Feb/09 19:39;daijy;The problem is caused by [HBASE-1063|http://issues.apache.org/jira/browse/HBASE-1063]. It is fixed in hbase 0.19.0. However, hbase 0.19.0 works with hadoop 0.19.0 only, so we cannot upgrade hbase right now. I think we can disable TestHBaseStorage under Windows for now. Once we upgraded to hadoop 19, we can also upgrade hbase to 19. This issue is fixed automatically once we use hbase 19 (in my test, one additional action is needed, which is removing /tmp/hbase manually to let hbase recreate its file system). So once we move to hadoop 19 and upgrade to hbase 19, reverse the temporary patch, and close this issue.","08/Jan/11 20:56;dvryaboy;Could someone with a windows setup test if the HBase test still times out in Windows? 
We've been on hadoop 20 and hbase 20 for a while now, might make sense to revert this patch.","09/Jan/11 00:58;daijy;Yes, it is fixed in Windows. Reverted patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
provide indication that pig script only partially succeeded,PIG-630,12412957,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,22/Jan/09 01:48,17/Dec/10 22:43,14/Mar/19 03:05,13/Sep/10 22:32,,,,,,,0.8.0,,,,,0,,,,"Currently, if you have multiple queries (stores/dumps) within the same pig script, the script return the result of the last one which does not provide sufficient information to the users. We need to provide to the user the following information:

- return code that indicates the script only partioally succeeded
- indication which parts have succeeded",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-09-13 22:32:08.293,,,no_permission,,,,,,,,,,,,164219,,,,,Mon Sep 13 22:32:08 UTC 2010,,,,,,,0|i0gmhz:,95075,,,,,,,,,,13/Sep/10 22:32;rding;This jira has been fixed with MultiQuery optimization and Pig Stats.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Include pig executable in distribution,PIG-622,12412590,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,tomwhite,tomwhite,tomwhite,16/Jan/09 13:50,25/Mar/10 00:12,14/Mar/19 03:05,17/Jan/09 02:00,,,,,,,0.2.0,,,,,0,,,,"Running ""ant tar"" does not generate the bin directory with the pig executable in it.",,,,,,,,,,,,,,,,,,,16/Jan/09 13:52;tomwhite;pig-622.patch;https://issues.apache.org/jira/secure/attachment/12398064/pig-622.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-01-17 02:00:46.53,,,no_permission,,,,,,,,,,,,164212,Reviewed,,,,Sat Jan 17 02:00:46 UTC 2009,,,,,,,0|i0gmf3:,95062,,,,,,,,,,"16/Jan/09 13:52;tomwhite;Patch to fix the problem. It also adds a directory prefix to files in the tar so they are unpacked in a ""pig-<version>"" directory.",17/Jan/09 02:00;sms;Patch committed. Thanks for your contribution Tom.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Casts swallow exceptions when there are issues with conversion of bytes to Pig types,PIG-621,12412553,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,sms,sms,16/Jan/09 01:43,04/Aug/11 00:34,14/Mar/19 03:05,13/Jan/11 00:49,0.2.0,,,,,,0.9.0,,,,,0,,,,"In the current implementation of casts, exceptions thrown while converting bytes to Pig types are swallowed. Pig needs to either return NULL or rethrow the exception.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-07-23 22:09:42.631,,,no_permission,,,,,,,,,,,,66209,,,,,Thu Jan 13 00:49:18 UTC 2011,,,,,,,0|i0gmen:,95060,,,,,,,,,,23/Jul/10 22:09;olgan;0.9 is all about improved error handling,"13/Jan/11 00:49;daijy;The behavior of current code is: if we cannot cast bytearray to real type, log a warning, and increase the counter. Seems we are doing the right thing. Close the Jira.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Dumping empty results produces ""Unable to get results for /tmp/temp-1964806069/tmp256878619  org.apache.pig.builtin.BinStorage"" message",PIG-619,12412329,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,viraj,viraj,13/Jan/09 22:30,24/Mar/10 22:10,14/Mar/19 03:05,28/May/09 17:17,0.2.0,,,,,,0.3.0,,impl,,,0,,,,"Following pig script stores empty filter results into  'emptyfilteredlogs' HDFS dir. It later reloads this data from an empty HDFS dir for additional grouping and counting. It has been observed that this script, succeeds on a single node hadoop installation with the following message as the alias COUNT_EMPTYFILTERED_LOGS contains empty data.
==============================================================================================================
2009-01-13 21:47:08,988 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!
==============================================================================================================
But on a multi-node Hadoop installation, the script fails with the following error:
==============================================================================================================
2009-01-13 13:48:34,602 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!
java.io.IOException: Unable to open iterator for alias: COUNT_EMPTYFILTERED_LOGS [Unable to get results for /tmp/temp-1964806069/tmp256878619:org.apache.pig.builtin.BinStorage]
        at org.apache.pig.backend.hadoop.executionengine.HJob.getResults(HJob.java:74)
        at org.apache.pig.PigServer.openIterator(PigServer.java:408)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:269)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:178)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:84)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:64)
        at org.apache.pig.Main.main(Main.java:306)
Caused by: org.apache.pig.backend.executionengine.ExecException: Unable to get results for /tmp/temp-1964806069/tmp256878619:org.apache.pig.builtin.BinStorage
        ... 7 more
Caused by: java.io.IOException: /tmp/temp-1964806069/tmp256878619 does not exist
        at org.apache.pig.impl.io.FileLocalizer.openDFSFile(FileLocalizer.java:188)
        at org.apache.pig.impl.io.FileLocalizer.open(FileLocalizer.java:291)
        at org.apache.pig.backend.hadoop.executionengine.HJob.getResults(HJob.java:69)
        ... 6 more
==============================================================================================================
{code}
RAW_LOGS = load 'mydata.txt' as (url:chararray, numvisits:int);
RAW_LOGS = limit RAW_LOGS 2;
FILTERED_LOGS = filter RAW_LOGS by numvisits < 0;
store FILTERED_LOGS into 'emptyfilteredlogs' using PigStorage();
EMPTY_FILTERED_LOGS = load 'emptyfilteredlogs' as (url:chararray, numvisits:int);
GROUP_EMPTYFILTERED_LOGS = group EMPTY_FILTERED_LOGS by numvisits;
COUNT_EMPTYFILTERED_LOGS = foreach GROUP_EMPTYFILTERED_LOGS generate
                             group, COUNT(EMPTY_FILTERED_LOGS);
explain COUNT_EMPTYFILTERED_LOGS;
dump COUNT_EMPTYFILTERED_LOGS;
{code}","Hadoop 18, Multi-node hadoop installation",,,,,,,,,,,,,,,,,,15/May/09 02:46;alangates;PIG-619.patch;https://issues.apache.org/jira/secure/attachment/12408213/PIG-619.patch,13/Jan/09 22:33;viraj;mydata.txt;https://issues.apache.org/jira/secure/attachment/12397829/mydata.txt,13/Jan/09 22:31;viraj;tmpfileload.pig;https://issues.apache.org/jira/secure/attachment/12397828/tmpfileload.pig,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-04-29 01:23:40.302,,,no_permission,,,,,,,,,,,,164210,,,,,Thu May 28 17:17:36 UTC 2009,,,,,,,0|i0gmdj:,95055,,,,,,,,,,13/Jan/09 22:31;viraj;Dumping empty results script,13/Jan/09 22:33;viraj;Test data,"29/Apr/09 01:23;alangates;Does fixing this still make sense?  IIRC the main reason for doing the store/load thing in the middle was to deal with the fact that Pig couldn't do multiple stores in one script without re-running the entire script.  But since that is in the process of being changed (see PIG-627), this should no longer be necessary.","29/Apr/09 01:30;viraj;So when does the Multi-Store query optimization get committed/merged  into the main branch, (where this is default way the multi-store happens). 
Viraj","29/Apr/09 01:45;alangates;Phase 2 (merging jobs just in the map phase), is already in.  Phase 3 (merging jobs across map/reduce boundaries) should be in by end of this week.","15/May/09 02:50;alangates;In order to see this behavior, you need three map reduce jobs, something like:

A = load
B = filter everything out
C = group
D = foreach
E = distinct
F = group
G = foreach
store G

In this case the first job (A-D) will run and produce 0 length part files.  The second job (E) will run, but no maps will be started because the files are zero length.  As a result Hadoop now seems to create no output files for this second job.  The third job (F-G) then fails complaining that the input files don't exist.  The patch changes pig's slicer to return at least one input split per part file even when the file is zero length.",15/May/09 16:18;olgan;+1 on the patch. It would be good to add a comment that explains why we are doing this. ,"17/May/09 05:35;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12408213/PIG-619.patch
  against trunk revision 775340.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/44/console

This message is automatically generated.",28/May/09 17:17;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad error message when period rather than comma appears as separator in UDF parameter list ,PIG-618,12412238,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,viraj,viraj,12/Jan/09 23:02,02/May/13 02:29,14/Mar/19 03:05,10/Mar/11 19:14,0.2.0,,,,,,0.9.0,,impl,,,0,,,,"Pig script generates the following compile-time error as it contains a period between 0.8 and 0.9 in the MYUDF parameter list. The ""Invalid alias MYUDF"" message should be changed to something that is more meaningful for the user to trace.
{code}
register 'MYUDF.jar';
A = load 'mydata.txt' using PigStorage() as (
        col1:   int,
        col2:   chararray,
        col3:   long,
        col4:   int
        );

B =  group A by (
        col1,
        col2
        );

C = foreach B generate
        group,
        MYUDF(A.col3, 0.0, 0.8. 0.9) as stat: (min, max);
describe C;
{code}
========================================================================================================
java.io.IOException: Invalid alias: MYUDF in {group: (col1: int,col2: chararray),A: {col1: int,col2: chararray,col
3: long,col4: int}}
        at org.apache.pig.PigServer.parseQuery(PigServer.java:301)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:266)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:439)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:249)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:84)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:64)
        at org.apache.pig.Main.main(Main.java:306)
Caused by: org.apache.pig.impl.logicalLayer.parser.ParseException: Invalid alias: MYUDF in {group: (col1: int,col2
: chararray),A: {col1: int,col2: chararray,col3: long,col4: int}}
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.AliasFieldOrSpec(QueryParser.java:6005)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.ColOrSpec(QueryParser.java:5863)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseEvalSpec(QueryParser.java:4049)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.UnaryExpr(QueryParser.java:3946)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.CastExpr(QueryParser.java:3900)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.MultiplicativeExpr(QueryParser.java:3809)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.AdditiveExpr(QueryParser.java:3735)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.InfixExpr(QueryParser.java:3701)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItem(QueryParser.java:3627)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItemList(QueryParser.java:3550)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.GenerateStatement(QueryParser.java:3494)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.NestedBlock(QueryParser.java:2969)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.ForEachClause(QueryParser.java:2384)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:1019)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:795)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:590)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:60)
        at org.apache.pig.PigServer.parseQuery(PigServer.java:298)
========================================================================================================
",,,,,,,,,,,,,,,,,PIG-1618,PIG-584,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-10 19:14:38.883,,,no_permission,,,,,,,,,,,,66199,,,,,Thu Mar 10 19:14:38 UTC 2011,,,,,,,0|i0gmdb:,95054,,,,,,,,,,12/Jan/09 23:06;viraj;Part of the Error Handling Feature of Pig,"10/Mar/11 19:14;xuefuz;Better error message is now given:

grunt> C = foreach B generate
>>         group,
>>         MYUDF(A.col3, 0.0, 0.8. 0.9) as stat: (min, max);
2011-03-10 11:13:47,638 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 0: line 25:30 mismatched input [@389,737:737='.',<79>,25:30] expecting RIGHT_PAREN

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using SUM with basic type fails,PIG-617,12412235,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,sms,sms,12/Jan/09 22:18,14/May/10 06:45,14/Mar/19 03:05,10/May/10 20:23,0.2.0,,,,,,0.7.0,,impl,,,0,,,,"SUM is an aggregate function that expects a bag as an argument. When basic types are used as arguments to SUM, Pig fails during run time. The typechecker should catch this error and fail earlier.

An example is given below:

{code}
grunt> a = load 'one' as (i: int);
grunt> b = foreach a generate SUM(i);
grunt> dump b;

2009-01-12 14:11:47,595 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete
2009-01-12 14:12:12,617 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Map reduce job failed
2009-01-12 14:12:12,618 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Job failed!
2009-01-12 14:12:12,623 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher - Error message from task (map) task_200812151518_9683_m_000000java.lang.ClassCastException: java.lang.Integer cannot be cast to org.apache.pig.data.DataBag

2009-01-12 14:12:12,623 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher - Error message from task (map) task_200812151518_9683_m_000000java.lang.ClassCastException: java.lang.Integer cannot be cast to org.apache.pig.data.DataBag
        at org.apache.pig.builtin.IntSum.sum(IntSum.java:141)
        at org.apache.pig.builtin.IntSum.exec(IntSum.java:41)
        at org.apache.pig.builtin.IntSum.exec(IntSum.java:36)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:185)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:247)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:265)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:197)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:187)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:175)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.map(PigMapOnly.java:65)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:227)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2207)
...

2009-01-12 14:12:12,629 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1066: Unable to open iterator for alias b
2009-01-12 14:12:12,629 [main] ERROR org.apache.pig.tools.grunt.Grunt - org.apache.pig.impl.logicalLayer.FrontendException: Unable to open iterator for alias b
        at org.apache.pig.PigServer.openIterator(PigServer.java:425)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:271)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:178)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:84)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:72)
        at org.apache.pig.Main.main(Main.java:302)
Caused by: java.io.IOException: Job terminated with anomalous status FAILED
        at org.apache.pig.PigServer.openIterator(PigServer.java:419)
        ... 5 more

{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-05-08 12:36:13.593,,,no_permission,,,,,,,,,,,,164209,,,,,Mon May 10 20:23:04 UTC 2010,,,,,,,0|i0gmcn:,95051,,,,,,,,,,"08/May/10 12:36;azaroth;This issue has been solved in trunk


grunt> a = load 'data' as (i:int); 
grunt> b = foreach a generate SUM(i);
grunt> dump b;
2010-05-08 14:35:11,348 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1045: Could not infer the matching function for org.apache.pig.builtin.SUM as multiple or none of them fit. Please use an explicit cast.
","10/May/10 20:23;olgan;Closing. Thanks, Gianmarco for testing",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong number of jobs with limit,PIG-615,12412215,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,shravanmn,olgan,olgan,12/Jan/09 18:02,25/Mar/10 00:12,14/Mar/19 03:05,24/Jan/09 06:36,,,,,,,0.2.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,20/Jan/09 10:57;shravanmn;615.patch;https://issues.apache.org/jira/secure/attachment/12398299/615.patch,24/Jan/09 00:56;sms;615_1.patch;https://issues.apache.org/jira/secure/attachment/12398620/615_1.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-01-19 09:32:11.149,,,no_permission,,,,,,,,,,,,164207,Reviewed,,,,Sat Jan 24 06:36:09 UTC 2009,,,,,,,0|i0gmbr:,95047,,,,,,,,,,"19/Jan/09 09:32;shravanmn;Should I submit the changes I suggested in our discussion as a patch?

A summary of the discussion follows:
{quote}
As per the current logic, the generation of the 4th MR Job in case of limit depends on the use of *""parallel""* keyword.
Though the logic is not directly dependent on cluster configuration, some cluster configs require this 4th MR job and some don't.
For ex., if the cluster is configured to set number of reducers to one if parallelism is -1 or unspecified then our current logic will work as the 4th MR Job is redundant. 
However, if the cluster is configured to set number of reducers to some other number, like 0.9 times the number of reduce slots if parallelism is unspecified then the 4th MRJob is necessary.

That is, we are making an implicit assumption in the code that if parallel is not explicitly mentioned, then the number of reducers is equal to 1. So the logic needs to be changed to include the 4th MRJob whenever the parallelism is not explicitly set to 1 so that we will produce correct results though in some cases its use might be redundant.
{quote}",20/Jan/09 10:57;shravanmn;Here is the change I suggested as a patch. Hope this is what was expected,23/Jan/09 19:03;sms;I will be reviewing this patch.,24/Jan/09 00:56;sms;Attached patch includes Shravan's fix along with test cases that I added. Running unit test cases now.,24/Jan/09 06:36;sms;All unit test cases passed. Patch has been committed. Thanks for the fix Shravan.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Casting complex type(tuple/bag/map) does not take effect,PIG-613,12411989,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,viraj,viraj,09/Jan/09 00:15,02/May/13 02:29,14/Mar/19 03:05,24/Feb/10 19:34,0.2.0,,,,,,0.7.0,,impl,,,0,,,,"Consider the following Pig script which casts return values of the SQUARE UDF which are  tuples of doubles to long. The describe output of B shows it is long, however the result is still double.
{code}
register statistics.jar;
A = load 'myfloatdata.txt' using PigStorage() as (doublecol:double);
B = foreach A generate (tuple(long))statistics.SQUARE(doublecol) as squares:(loadtimesq);
describe B;
explain B;
dump B;
{code}
===========================================
Describe output of B:
B: {squares: (loadtimesq: long)}
===========================================================
Sample output of B:
((7885.44))
((792098.2200010001))
((1497360.9268889998))
((50023.7956))
((0.972196))
((0.30980356))
((9.9760144E-7))
===========================================================
Cause: The cast for Tuples has not been implemented in POCast.java

",,,,,,,,,,,,,,,,,,,23/Feb/10 06:42;daijy;PIG-613-1.patch;https://issues.apache.org/jira/secure/attachment/12436680/PIG-613-1.patch,24/Feb/10 00:00;daijy;PIG-613-2.patch;https://issues.apache.org/jira/secure/attachment/12436788/PIG-613-2.patch,09/Jan/09 00:17;viraj;SQUARE.java;https://issues.apache.org/jira/secure/attachment/12397468/SQUARE.java,09/Jan/09 00:16;viraj;myfloatdata.txt;https://issues.apache.org/jira/secure/attachment/12397467/myfloatdata.txt,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2009-10-14 05:27:16.689,,,no_permission,,,,,,,,,,,,164205,Reviewed,,,,Wed Feb 24 19:36:40 UTC 2010,,,,,,,0|i0gmb3:,95044,,,,,,,,,,09/Jan/09 00:16;viraj;Test input file,09/Jan/09 00:17;viraj;SQUARE UDF,"14/Oct/09 05:27;daijy;Here is another example:

1.txt:
(0.2,0.3)

a = load '1.txt';
b = foreach a generate (tuple(int, int))$0;

describe b;
b: {(int,int)}

dump b;
((0.2,0.3))

The expect result is ((0, 0))","23/Feb/10 15:31;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12436680/PIG-613-1.patch
  against trunk revision 915079.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 19 new or modified tests.

    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/217/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/217/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/217/console

This message is automatically generated.",23/Feb/10 19:25;rding;Initial comment: the patch also needs to remove references to  package 'org.apache.pig.data.parser' from junit tests.,"23/Feb/10 22:08;daijy;Checked with hadoop guys. For Serializer/Deserializer, we should always open, but never close. There is another place in pig code does not follow this. Attach a new patch address both positions.",23/Feb/10 22:09;daijy;Ignore last comment. It means to be for another Jira.,"23/Feb/10 22:23;rding;The patch looks good. A few comments:

   * File TestTextDataParser.java should be removed.

   * In LoadCaster, javadoc is needed for the new parameters.  

   * In Utf8StorageConverter's bytesToTuple method, the cast to Tuple can be removed. Same for the cast in bytesToBag and bytesToMap. In consumeBag method, use consumeTuple instead of consumeComplexType to be more precise. In bytesToBoolean method, use Boolean.valueof to be consistent with other existing methods.  Also should the new helper methods consume{Bag, Tuple, Map} be private (not public)? 

   * The new methods added in impl.util.Utils class should be added to a test.util class since they are only used in the unit tests.

I maybe too picky here, but I would like to see more consistent coding style across files. For example, 

   * Use 4 spaces (not tab) for indentation: tabs are used in ResourceSchema. POCast, and TypeCheckingVisitor.



 



   ",24/Feb/10 00:00;daijy;Attach patch to address Richard's comments and hudson feedback.,"24/Feb/10 00:31;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12436788/PIG-613-2.patch
  against trunk revision 915578.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 22 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/219/console

This message is automatically generated.",24/Feb/10 19:34;daijy;Patch committed.,"24/Feb/10 19:36;daijy;Hudson does not work well, patch has been manually tested.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PIG does not return the correct error code,PIG-609,12411899,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,,yhan,yhan,07/Jan/09 21:17,25/Mar/10 00:12,14/Mar/19 03:05,15/Jan/09 18:35,,,,,,,0.2.0,,,,,0,,,,"Pig still does not always return a correct error code. When the hadoop job fails, sometimes pig still return 0.  ",,,,,,,,,,,,PIG-571,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-01-15 18:35:58.729,,,no_permission,,,,,,,,,,,,164201,,,,,Thu Jan 15 18:35:58 UTC 2009,,,,,,,0|i0gm9r:,95038,,,,,,,,,,15/Jan/09 18:35;olgan;This is duplicate of PIG-571 and PIG-264,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PiggyBank compilation instructions don't work,PIG-600,12411868,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,ciemo,ciemo,07/Jan/09 17:03,25/Mar/10 00:12,14/Mar/19 03:05,22/Jun/09 18:00,0.2.0,,,,,,0.4.0,,impl,,,0,,,,"I know that PiggyBank is ""as-is"" but the instructions are incomplete and should be complete enough (all steps) required to compile PiggyBank.

http://wiki.apache.org/pig/PiggyBank

I checked out the types branch version of PiggyBank by modifying the instructions to check out:

svn co http://svn.apache.org/repos/asf/hadoop/pig/branches/types/contrib/piggybank/

At step 2 it says:

To build a jar file that contains all available user defined functions (UDFs), please follow the steps:

1. Checkout UDF code: svn co http://svn.apache.org/repos/asf/hadoop/pig/trunk/contrib/piggybank
2. Build the jar file: from trunk/contrib/piggybank/java directory run ant. This will generate piggybank.jar in the same directory.


So I went into the piggybank/java directory and and ran ant and got the following errors:

{code}
-bash-3.00$ ant
Buildfile: build.xml

init:

compile:
     [echo]  *** Compiling Pig UDFs ***
    [javac] Compiling 70 source files to /homes/ciemo/piggybank/java/build/classes
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/ABS.java:25: cannot find symbol
    [javac] symbol  : class EvalFunc
    [javac] location: package org.apache.pig
    [javac] import org.apache.pig.EvalFunc;
    [javac]                      ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/ABS.java:26: cannot find symbol
    [javac] symbol  : class FuncSpec
    [javac] location: package org.apache.pig
    [javac] import org.apache.pig.FuncSpec;
    [javac]                      ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/ABS.java:27: package org.apache.pig.data does not exist
    [javac] import org.apache.pig.data.Tuple;
    [javac]                           ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/ABS.java:28: package org.apache.pig.impl.logicalLayer.schema does not exist
    [javac] import org.apache.pig.impl.logicalLayer.schema.Schema;
    [javac]                                               ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/ABS.java:29: package org.apache.pig.data does not exist
    [javac] import org.apache.pig.data.DataType;
    [javac]                           ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/ABS.java:30: package org.apache.pig.impl.logicalLayer does not exist
    [javac] import org.apache.pig.impl.logicalLayer.FrontendException;
    [javac]                                        ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/ABS.java:31: package org.apache.pig.impl.util does not exist
    [javac] import org.apache.pig.impl.util.WrappedIOException;
    [javac]                                ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/ABS.java:61: cannot find symbol
    [javac] symbol: class EvalFunc
    [javac] public class ABS extends EvalFunc<Double>{
    [javac]                          ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/ABS.java:67: cannot find symbol
    [javac] symbol  : class Tuple
    [javac] location: class org.apache.pig.piggybank.evaluation.math.ABS
    [javac]     public Double exec(Tuple input) throws IOException {
    [javac]                        ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/ABS.java:85: cannot find symbol
    [javac] symbol  : class Schema
    [javac] location: class org.apache.pig.piggybank.evaluation.math.ABS
    [javac]     public Schema outputSchema(Schema input) {
    [javac]                                ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/ABS.java:85: cannot find symbol
    [javac] symbol  : class Schema
    [javac] location: class org.apache.pig.piggybank.evaluation.math.ABS
    [javac]     public Schema outputSchema(Schema input) {
    [javac]            ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/ABS.java:93: cannot find symbol
    [javac] symbol  : class FuncSpec
    [javac] location: class org.apache.pig.piggybank.evaluation.math.ABS
    [javac]     public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
    [javac]                 ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/ABS.java:93: cannot find symbol
    [javac] symbol  : class FrontendException
    [javac] location: class org.apache.pig.piggybank.evaluation.math.ABS
    [javac]     public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
    [javac]                                                        ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleBase.java:25: package org.apache.pig.impl.logicalLayer does not exist
    [javac] import org.apache.pig.impl.logicalLayer.FrontendException;
    [javac]                                        ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleBase.java:26: cannot find symbol
    [javac] symbol  : class FuncSpec
    [javac] location: package org.apache.pig
    [javac] import org.apache.pig.FuncSpec;
    [javac]                      ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleBase.java:27: package org.apache.pig.data does not exist
    [javac] import org.apache.pig.data.Tuple;
    [javac]                           ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleBase.java:28: package org.apache.pig.impl.util does not exist
    [javac] import org.apache.pig.impl.util.WrappedIOException;
    [javac]                                ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleBase.java:29: package org.apache.pig.impl.logicalLayer.schema does not exist
    [javac] import org.apache.pig.impl.logicalLayer.schema.Schema;
    [javac]                                               ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleBase.java:30: package org.apache.pig.data does not exist
    [javac] import org.apache.pig.data.DataType;
    [javac]                           ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/Base.java:21: cannot find symbol
    [javac] symbol  : class EvalFunc
    [javac] location: package org.apache.pig
    [javac] import org.apache.pig.EvalFunc;
    [javac]                      ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/Base.java:22: package org.apache.pig.impl.logicalLayer.schema does not exist
    [javac] import org.apache.pig.impl.logicalLayer.schema.Schema;
    [javac]                                               ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/Base.java:23: package org.apache.pig.data does not exist
    [javac] import org.apache.pig.data.DataType;
    [javac]                           ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/Base.java:27: cannot find symbol
    [javac] symbol: class EvalFunc
    [javac] public abstract class Base extends EvalFunc<Double>{
    [javac]                                    ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleBase.java:44: cannot find symbol
    [javac] symbol  : class Tuple
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleBase
    [javac]     public Double exec(Tuple input) throws IOException {
    [javac]                        ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleBase.java:60: cannot find symbol
    [javac] symbol  : class FuncSpec
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleBase
    [javac]     public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
    [javac]                 ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleBase.java:60: cannot find symbol
    [javac] symbol  : class FrontendException
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleBase
    [javac]     public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
    [javac]                                                        ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/Base.java:29: cannot find symbol
    [javac] symbol  : class Schema
    [javac] location: class org.apache.pig.piggybank.evaluation.math.Base
    [javac]     public Schema outputSchema(Schema input) {
    [javac]                                ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/Base.java:29: cannot find symbol
    [javac] symbol  : class Schema
    [javac] location: class org.apache.pig.piggybank.evaluation.math.Base
    [javac]     public Schema outputSchema(Schema input) {
    [javac]            ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleDoubleBase.java:25: package org.apache.pig.impl.logicalLayer does not exist
    [javac] import org.apache.pig.impl.logicalLayer.FrontendException;
    [javac]                                        ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleDoubleBase.java:26: cannot find symbol
    [javac] symbol  : class FuncSpec
    [javac] location: package org.apache.pig
    [javac] import org.apache.pig.FuncSpec;
    [javac]                      ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleDoubleBase.java:27: package org.apache.pig.data does not exist
    [javac] import org.apache.pig.data.Tuple;
    [javac]                           ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleDoubleBase.java:28: package org.apache.pig.impl.util does not exist
    [javac] import org.apache.pig.impl.util.WrappedIOException;
    [javac]                                ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleDoubleBase.java:29: package org.apache.pig.impl.logicalLayer.schema does not exist
    [javac] import org.apache.pig.impl.logicalLayer.schema.Schema;
    [javac]                                               ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleDoubleBase.java:30: package org.apache.pig.data does not exist
    [javac] import org.apache.pig.data.DataType;
    [javac]                           ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleDoubleBase.java:44: cannot find symbol
    [javac] symbol  : class Tuple
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleDoubleBase
    [javac]     public Double exec(Tuple input) throws IOException {
    [javac]                        ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleDoubleBase.java:61: cannot find symbol
    [javac] symbol  : class FuncSpec
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleDoubleBase
    [javac]     public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
    [javac]                 ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleDoubleBase.java:61: cannot find symbol
    [javac] symbol  : class FrontendException
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleDoubleBase
    [javac]     public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
    [javac]                                                        ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleAbs.java:23: cannot find symbol
    [javac] symbol  : class EvalFunc
    [javac] location: package org.apache.pig
    [javac] import org.apache.pig.EvalFunc;
    [javac]                      ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleAbs.java:24: package org.apache.pig.data does not exist
    [javac] import org.apache.pig.data.Tuple;
    [javac]                           ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleAbs.java:25: package org.apache.pig.impl.logicalLayer.schema does not exist
    [javac] import org.apache.pig.impl.logicalLayer.schema.Schema;
    [javac]                                               ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleAbs.java:26: package org.apache.pig.data does not exist
    [javac] import org.apache.pig.data.DataType;
    [javac]                           ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleAbs.java:27: package org.apache.pig.impl.util does not exist
    [javac] import org.apache.pig.impl.util.WrappedIOException;
    [javac]                                ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleAbs.java:58: cannot find symbol
    [javac] symbol: class EvalFunc
    [javac] public class DoubleAbs extends EvalFunc<Double>{
    [javac]                                ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleAbs.java:64: cannot find symbol
    [javac] symbol  : class Tuple
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleAbs
    [javac]     public Double exec(Tuple input) throws IOException {
    [javac]                        ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleAbs.java:79: cannot find symbol
    [javac] symbol  : class Schema
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleAbs
    [javac]     public Schema outputSchema(Schema input) {
    [javac]                                ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleAbs.java:79: cannot find symbol
    [javac] symbol  : class Schema
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleAbs
    [javac]     public Schema outputSchema(Schema input) {
    [javac]            ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleCopySign.java:23: cannot find symbol
    [javac] symbol  : class EvalFunc
    [javac] location: package org.apache.pig
    [javac] import org.apache.pig.EvalFunc;
    [javac]                      ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleCopySign.java:24: package org.apache.pig.data does not exist
    [javac] import org.apache.pig.data.Tuple;
    [javac]                           ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleCopySign.java:25: package org.apache.pig.impl.logicalLayer.schema does not exist
    [javac] import org.apache.pig.impl.logicalLayer.schema.Schema;
    [javac]                                               ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleCopySign.java:26: package org.apache.pig.data does not exist
    [javac] import org.apache.pig.data.DataType;
    [javac]                           ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleCopySign.java:27: package org.apache.pig.impl.logicalLayer does not exist
    [javac] import org.apache.pig.impl.logicalLayer.FrontendException;
    [javac]                                        ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleCopySign.java:28: package org.apache.pig.impl.util does not exist
    [javac] import org.apache.pig.impl.util.WrappedIOException;
    [javac]                                ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleCopySign.java:59: cannot find symbol
    [javac] symbol: class EvalFunc
    [javac] public class DoubleCopySign extends EvalFunc<Double>{
    [javac]                                     ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleCopySign.java:68: cannot find symbol
    [javac] symbol  : class Tuple
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleCopySign
    [javac]     public Double exec(Tuple input) throws IOException {
    [javac]                        ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleCopySign.java:81: cannot find symbol
    [javac] symbol  : class Schema
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleCopySign
    [javac]     public Schema outputSchema(Schema input) {
    [javac]                                ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleCopySign.java:81: cannot find symbol
    [javac] symbol  : class Schema
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleCopySign
    [javac]     public Schema outputSchema(Schema input) {
    [javac]            ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleGetExponent.java:23: cannot find symbol
    [javac] symbol  : class EvalFunc
    [javac] location: package org.apache.pig
    [javac] import org.apache.pig.EvalFunc;
    [javac]                      ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleGetExponent.java:24: package org.apache.pig.data does not exist
    [javac] import org.apache.pig.data.Tuple;
    [javac]                           ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleGetExponent.java:25: package org.apache.pig.impl.logicalLayer.schema does not exist
    [javac] import org.apache.pig.impl.logicalLayer.schema.Schema;
    [javac]                                               ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleGetExponent.java:26: package org.apache.pig.data does not exist
    [javac] import org.apache.pig.data.DataType;
    [javac]                           ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleGetExponent.java:27: package org.apache.pig.impl.util does not exist
    [javac] import org.apache.pig.impl.util.WrappedIOException;
    [javac]                                ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleGetExponent.java:58: cannot find symbol
    [javac] symbol: class EvalFunc
    [javac] public class DoubleGetExponent extends EvalFunc<Integer>{
    [javac]                                        ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleGetExponent.java:65: cannot find symbol
    [javac] symbol  : class Tuple
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleGetExponent
    [javac]     public Integer exec(Tuple input) throws IOException {
    [javac]                         ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleGetExponent.java:78: cannot find symbol
    [javac] symbol  : class Schema
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleGetExponent
    [javac]     public Schema outputSchema(Schema input) {
    [javac]                                ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleGetExponent.java:78: cannot find symbol
    [javac] symbol  : class Schema
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleGetExponent
    [javac]     public Schema outputSchema(Schema input) {
    [javac]            ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleMax.java:23: package org.apache.pig.impl.util does not exist
    [javac] import org.apache.pig.impl.util.WrappedIOException;
    [javac]                                ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleMax.java:24: cannot find symbol
    [javac] symbol  : class EvalFunc
    [javac] location: package org.apache.pig
    [javac] import org.apache.pig.EvalFunc;
    [javac]                      ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleMax.java:25: package org.apache.pig.data does not exist
    [javac] import org.apache.pig.data.Tuple;
    [javac]                           ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleMax.java:26: package org.apache.pig.impl.logicalLayer.schema does not exist
    [javac] import org.apache.pig.impl.logicalLayer.schema.Schema;
    [javac]                                               ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleMax.java:27: package org.apache.pig.data does not exist
    [javac] import org.apache.pig.data.DataType;
    [javac]                           ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleMax.java:57: cannot find symbol
    [javac] symbol: class EvalFunc
    [javac] public class DoubleMax extends EvalFunc<Double>{
    [javac]                                ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleMax.java:63: cannot find symbol
    [javac] symbol  : class Tuple
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleMax
    [javac]     public Double exec(Tuple input) throws IOException {
    [javac]                        ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleMax.java:77: cannot find symbol
    [javac] symbol  : class Schema
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleMax
    [javac]     public Schema outputSchema(Schema input) {
    [javac]                                ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleMax.java:77: cannot find symbol
    [javac] symbol  : class Schema
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleMax
    [javac]     public Schema outputSchema(Schema input) {
    [javac]            ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleMin.java:23: package org.apache.pig.impl.util does not exist
    [javac] import org.apache.pig.impl.util.WrappedIOException;
    [javac]                                ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleMin.java:24: cannot find symbol
    [javac] symbol  : class EvalFunc
    [javac] location: package org.apache.pig
    [javac] import org.apache.pig.EvalFunc;
    [javac]                      ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleMin.java:25: package org.apache.pig.data does not exist
    [javac] import org.apache.pig.data.Tuple;
    [javac]                           ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleMin.java:26: package org.apache.pig.impl.logicalLayer.schema does not exist
    [javac] import org.apache.pig.impl.logicalLayer.schema.Schema;
    [javac]                                               ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleMin.java:27: package org.apache.pig.data does not exist
    [javac] import org.apache.pig.data.DataType;
    [javac]                           ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleMin.java:57: cannot find symbol
    [javac] symbol: class EvalFunc
    [javac] public class DoubleMin extends EvalFunc<Double>{
    [javac]                                ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleMin.java:63: cannot find symbol
    [javac] symbol  : class Tuple
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleMin
    [javac]     public Double exec(Tuple input) throws IOException {
    [javac]                        ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleMin.java:77: cannot find symbol
    [javac] symbol  : class Schema
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleMin
    [javac]     public Schema outputSchema(Schema input) {
    [javac]                                ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleMin.java:77: cannot find symbol
    [javac] symbol  : class Schema
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleMin
    [javac]     public Schema outputSchema(Schema input) {
    [javac]            ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleNextAfter.java:25: cannot find symbol
    [javac] symbol  : class EvalFunc
    [javac] location: package org.apache.pig
    [javac] import org.apache.pig.EvalFunc;
    [javac]                      ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleNextAfter.java:26: cannot find symbol
    [javac] symbol  : class FuncSpec
    [javac] location: package org.apache.pig
    [javac] import org.apache.pig.FuncSpec;
    [javac]                      ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleNextAfter.java:27: package org.apache.pig.data does not exist
    [javac] import org.apache.pig.data.Tuple;
    [javac]                           ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleNextAfter.java:28: package org.apache.pig.impl.logicalLayer.schema does not exist
    [javac] import org.apache.pig.impl.logicalLayer.schema.Schema;
    [javac]                                               ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleNextAfter.java:29: package org.apache.pig.data does not exist
    [javac] import org.apache.pig.data.DataType;
    [javac]                           ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleNextAfter.java:30: package org.apache.pig.impl.logicalLayer does not exist
    [javac] import org.apache.pig.impl.logicalLayer.FrontendException;
    [javac]                                        ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleNextAfter.java:31: package org.apache.pig.impl.util does not exist
    [javac] import org.apache.pig.impl.util.WrappedIOException;
    [javac]                                ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleNextAfter.java:63: cannot find symbol
    [javac] symbol: class EvalFunc
    [javac] public class DoubleNextAfter extends EvalFunc<Double>{
    [javac]                                      ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleNextAfter.java:73: cannot find symbol
    [javac] symbol  : class Tuple
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleNextAfter
    [javac]     public Double exec(Tuple input) throws IOException {
    [javac]                        ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleNextAfter.java:84: cannot find symbol
    [javac] symbol  : class Schema
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleNextAfter
    [javac]     public Schema outputSchema(Schema input) {
    [javac]                                ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleNextAfter.java:84: cannot find symbol
    [javac] symbol  : class Schema
    [javac] location: class org.apache.pig.piggybank.evaluation.math.DoubleNextAfter
    [javac]     public Schema outputSchema(Schema input) {
    [javac]            ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleNextup.java:23: cannot find symbol
    [javac] symbol  : class EvalFunc
    [javac] location: package org.apache.pig
    [javac] import org.apache.pig.EvalFunc;
    [javac]                      ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleNextup.java:24: package org.apache.pig.data does not exist
    [javac] import org.apache.pig.data.Tuple;
    [javac]                           ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleNextup.java:25: package org.apache.pig.impl.logicalLayer.schema does not exist
    [javac] import org.apache.pig.impl.logicalLayer.schema.Schema;
    [javac]                                               ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleNextup.java:26: package org.apache.pig.data does not exist
    [javac] import org.apache.pig.data.DataType;
    [javac]                           ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleNextup.java:27: package org.apache.pig.impl.util does not exist
    [javac] import org.apache.pig.impl.util.WrappedIOException;
    [javac]                                ^
    [javac] /homes/ciemo/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/DoubleNextup.java:58: cannot find symbol
    [javac] symbol: class EvalFunc
    [javac] public class DoubleNextup extends EvalFunc<Double>{
    [javac]                                   ^
    [javac] 100 errors

BUILD FAILED
/homes/ciemo/piggybank/java/build.xml:74: Compile failed; see the compiler error output for details.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-06-22 17:17:09.894,,,no_permission,,,,,,,,,,,,164193,,,,,Mon Jun 22 18:00:25 UTC 2009,,,,,,,0|i0gm73:,95026,,,,,,,,,,"07/Jan/09 17:15;ciemo;I think the problem is this ... the build.xml file is looking for pig.jar several directories up:

    <property name=""pigjar"" value=""../../../pig.jar"" />

The thing is, I'm relying on another build of pig and not the whole pig directory.

If you take the instructions for PiggyBank literally (as I did) you will not get a successful build.","22/Jun/09 17:17;dvryaboy;Setting the CLASSPATH environment variable to include pig.jar is sufficient for building the piggybank jar (and avoids the problem of having the whole pig directory checked out).
I've updated the Wiki.
This can be closed.","22/Jun/09 18:00;olgan;Thanks, Dmitry for your help with this issue!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BufferedPositionedInputStream isn't buffered,PIG-599,12411733,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,06/Jan/09 00:47,24/Mar/10 22:04,14/Mar/19 03:05,17/Feb/09 16:57,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"org.apache.pig.impl.io.BufferedPositionedInputStream is not actually buffered.  This is because it sits atop a FSDataInputStream (somewhere down the stack), which is buffered.  So to avoid double buffering, which can be bad, BufferedPositionedInputStream was written without buffering.  But the FSDataInputStream is far enough down the stack that it is still quite costly to call read() individually for each byte.  A run through a profiler shows that a fair amount of time is being spent in BufferedPositionedInputStream.read().",,,,,,,,,,,,,,,,,,,09/Jan/09 01:42;alangates;loadperf-2.patch;https://issues.apache.org/jira/secure/attachment/12397476/loadperf-2.patch,06/Jan/09 01:11;alangates;loadperf.patch;https://issues.apache.org/jira/secure/attachment/12397165/loadperf.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-01-07 00:03:58.758,,,no_permission,,,,,,,,,,,,164192,,,,,Tue Feb 17 16:57:06 UTC 2009,,,,,,,0|i0gm6n:,95024,,,,,,,,,,06/Jan/09 01:11;alangates;This patch changes BufferedPositionedInputStream to wrap a BufferedInputStream around the provided InputStream.  It also adds a new constructor for DefaultTuple (and new calls in TupleFactory) that take an ArrayList<Object> and use that directly to construct the DefaultTuple instead of copying the list (as was done previously).  In a run of the pig mix queries these changes made most queries about 25-40% faster.,"07/Jan/09 00:03;breed;There is a problem with using a buffered stream and compression. we have to do some really subtle things get the mapping of compression blocks into positions so that the load functions work out properly. if read ahead happens underneath things break. (it would be excellent if someone had a better way of doing it.)  in absence of a better idea, i think we should check that the stream we are buffering and skip the buffering if it as a compressed stream. ",09/Jan/09 01:42;alangates;A second version of the patch that addresses Ben's comments.,09/Jan/09 13:14;breed;+1 looks good.,"13/Feb/09 22:16;olgan;Alan, Has this patch been committed?",17/Feb/09 16:57;alangates;Patch checked in quite some time ago.  Closing bug.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parameter substitution ($PARAMETER) should not be performed in comments,PIG-598,12411729,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,ciemo,ciemo,06/Jan/09 00:23,14/May/10 06:45,14/Mar/19 03:05,24/Nov/09 17:11,0.2.0,,,,,,0.7.0,,impl,,,1,,,,"Compiling the following code example will generate an error that $NOT_A_PARAMETER is an Undefined Parameter.

This is problematic as sometimes you want to comment out parts of your code, including parameters so that you don't have to define them.

This I think it would be really good if parameter substitution was not performed in comments.

{code}
-- $NOT_A_PARAMETER
{code}

{code}
-bash-3.00$ pig -exectype local -latest comment.pig
USING: /grid/0/gs/pig/current
java.lang.RuntimeException: Undefined parameter : NOT_A_PARAMETER
        at org.apache.pig.tools.parameters.PreprocessorContext.substitute(PreprocessorContext.java:221)
        at org.apache.pig.tools.parameters.ParameterSubstitutionPreprocessor.parsePigFile(ParameterSubstitutionPreprocessor.java:106)
        at org.apache.pig.tools.parameters.ParameterSubstitutionPreprocessor.genSubstitutedFile(ParameterSubstitutionPreprocessor.java:86)
        at org.apache.pig.Main.runParamPreprocessor(Main.java:394)
        at org.apache.pig.Main.main(Main.java:296)
{code}",,,,,,,,,,,,,,,,,,,23/Nov/09 18:29;thejas;PIG-598.1.patch;https://issues.apache.org/jira/secure/attachment/12425862/PIG-598.1.patch,23/Oct/09 17:29;thejas;PIG-598.patch;https://issues.apache.org/jira/secure/attachment/12423034/PIG-598.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-04-24 21:33:48.962,,,no_permission,,,,,,,,,,,,164191,,,,,Tue Nov 24 17:11:21 UTC 2009,,,,,,,0|i0gm67:,95022,,,,,,,,,,"24/Apr/09 21:33;gmavromatis;This has been flagged as minor, I think that's a rather serious and elementary one. It takes out the ability to comment/uncomment parameters and creates very bizarre error messages to people who are not aware of it. Can we increase priority?","22/Oct/09 18:28;thejas;With this patch 
- Parameters in comments are no longer substituted
- Line numbers don't change after parameter substitution, as long as declare/default don't span multiple lines (no multi-line literals etc). This will help in giving accurate line numbers in error messages after parameter substitution.

Code changes:
Instead of the parser processing the pig script a line at a time, the whole script is processed at once.
There are changes in test cases, because original line numbers are now preserved.

","23/Oct/09 02:35;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422928/PIG-598.patch
  against trunk revision 828891.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 48 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/111/console

This message is automatically generated.",23/Oct/09 17:29;thejas;New patch generated after updating svn.,27/Oct/09 21:07;thejas;Re-submitting patch for hudson.,"29/Oct/09 08:00;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12423034/PIG-598.patch
  against trunk revision 830757.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 48 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 199 javac compiler warnings (more than the trunk's current 197 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 318 release audit warnings (more than the trunk's current 313 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/125/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/125/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/125/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/125/console

This message is automatically generated.","29/Oct/09 14:04;thejas;bq.  -1 javac. The applied patch generated 199 javac compiler warnings (more than the trunk's current 197 warnings).
The additional warnings are from code generated by javacc, which cannot be fixed in the .jj files.

bq. -1 release audit. The applied patch generated 318 release audit warnings (more than the trunk's current 313 warnings).
The release audit warnings are from new test input and benchmark files, because they don't have the apache license header.","12/Nov/09 23:30;olgan;The code looks reasonable. 

When I tried to run tests with comments (both kind) in the middle of the line - they are not recognized","13/Nov/09 01:30;ashutoshc;One issue I faced while working on PIG-928 was when trying to name variables in ruby bound to java variables. Ruby variable  names (in BSF) needs to be prepended with $ as shown below. 
{code}
define myudf org.apache.pig.scriptedUDFs.GenericEvalFunc('wordtokens','return $input.split();');
{code}
Now this $ appearing in the script should not be substituted. But parser tries to substitute it and fails (with error undefined parameter) since param is not specified while invoking Pig. My hack was to 'escape' this $ and then provide param-substiution while invoking the script as -p input=input 
{code}
define myudf org.apache.pig.scriptedUDFs.GenericEvalFunc('wordtokens','return $$input.split();');
{code}
Note extra $. Obviously, if we are thinking of adding pig-928 and accept udfs written in ruby, this will come up as an issue. One possibility is Pig should not try to do substitution if param is not specified on cmd line. I think way it currently is parser scans the script for $ and then tries to do substiution. If substitution was not defined it error outs. I think if it is not specified on cmd line, then parser should ignore it and continue instead of failing.   ","23/Nov/09 18:15;thejas;bq. One issue I faced while working on PIG-928 was when trying to name variables in ruby bound to java variables.
Ashutosh,
You can use ""\"" to escape parameter substitution . Use 'return \$input.split();' instead of 'return $input.split();' . After parameter substitution, it becomes 'return $input.split();' .

","23/Nov/09 18:29;thejas;Additional changes in this patch-
* Fixed parsing in  PigFileParser.jj 
* Modified test input file for testCommentWithParam() - inputComment.pig, to include comments within and at end of statements","23/Nov/09 19:08;ashutoshc;I guess my question is what should be the behavior when $ is specified in the script and no substitution for it is provided. There are two options: a) If Pig encounters a $ and doesn't find a substitution for it, it fails right there. 
b) Pig logs a warning message and continue assuming user wants literal $ and not the substitution.

Advantage for b) is there will not be a need of escaping. Disadvantage is when no substitution was unintentional, Pig will fail later, possibly with a different error message.
Disadvantage of a) is it mandates user to escape $, where its possible not to have such requirement. Advantage is a clear error message can be thrown if no substitution was unintentional.

What do you think which option shall we choose? 
","23/Nov/09 19:26;thejas;I prefer option a. It is just a matter of putting a ""\"" before the $ in the scripts :) 
I think compared to the cost of time spending debugging a weird error or unexpected output results, the cost of a for the user is trivial.

Ideally, I think we should support an option where user can change from default behavior (a) to (b) using a commandline switch or a statement in the script.","23/Nov/09 20:26;ashutoshc;
bq. compared to the cost of time spending debugging a weird error or unexpected output results, the cost of a for the user is trivial.

I agree. Catching errors early and providing clear explanation for it is more important then the convenience of not requiring escaping. Thus a) is better.

bq. we should support an option where user can change from default behavior (a) to (b) using a commandline switch or a statement in the script.

Adding yet another hook may not be worth the complexity/time. Lets keep this on hold unless someone specifically asks for this.","24/Nov/09 00:05;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12425862/PIG-598.1.patch
  against trunk revision 882818.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 48 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 213 javac compiler warnings (more than the trunk's current 211 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 361 release audit warnings (more than the trunk's current 356 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/52/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/52/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/52/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/52/console

This message is automatically generated.","24/Nov/09 00:19;thejas;bq. -1 javac. The applied patch generated 213 javac compiler warnings (more than the trunk's current 211 warnings).
The additional warnings are from code generated by javacc, which cannot be fixed in the .jj files.

bq. -1 release audit. The applied patch generated 361 release audit warnings (more than the trunk's current 356 warnings).
The release audit warnings are from new test input and benchmark files, because they don't have the apache license header.
","24/Nov/09 17:11;olgan;patch committed. Thanks, Thejas for contributing - this change has been long overdue!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Pig does not handdle correctly the case where ""*"" is passed to UDF",PIG-597,12411727,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,shravanmn,olgan,olgan,05/Jan/09 23:27,25/Mar/10 00:12,14/Mar/19 03:05,17/Jan/09 00:30,,,,,,,0.2.0,,,,,0,,,,"Script:
======
A = LOAD 'foo' USING PigStorage('\t');
B = FILTER A BY ARITY(*) < 5;
DUMP B;

Error:
=====

2009-01-05 21:46:56,355 [main] ERROR
org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc
- Caught error from UDF
org.apache.pig.builtin.ARITY[org.apache.pig.data.DataByteArray cannot be cast to org.apache.pig.data.Tuple [org.apache.pig.data.DataByteArray cannot be cast to org.apache.pig.data.Tuple]

Problem:
=======

Santhosh tracked this to the following code in POUserFunc.java:

if(op instanceof POProject &&
                        op.getResultType() == DataType.TUPLE){
                    POProject projOp = (POProject)op;
                    if(projOp.isStar()){
                        Tuple trslt = (Tuple) temp.result;
                        Tuple rslt = (Tuple) res.result;
                        for(int i=0;i<trslt.size();i++)
                            rslt.append(trslt.get(i));
                        continue;
                    }
                }

It seems to be unwrapping the tuple before passing it to the function. There is no comments so we are not sure why it is there; will need to run tests to see if removing it would solve this issue and not create others.",,,,,,,,,,,,,,,,,,,16/Jan/09 14:50;shravanmn;597.patch;https://issues.apache.org/jira/secure/attachment/12398076/597.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-01-13 11:18:02.461,,,no_permission,,,,,,,,,,,,164190,,,,,Sat Jan 17 00:30:58 UTC 2009,,,,,,,0|i0gm5r:,95020,,,,,,,,,,12/Jan/09 17:09;olgan;Commenting this code breaks ORDER BY. I believe that this change is still correct and the sampling code need to be changed to work with it.,"13/Jan/09 11:18;shravanmn;The exception is being thrown from ARITY where it is trying to convert the first field of the tuple into a tuple. However, since we have a star, the tuple is not wrapped inside another tuple and hence the exception.

This was done in order to model the trunk behavior which is that there is an implicit flatten in front of a *. If we want to retain this behavior, then we need to change ARITY & other functions which were written with the assumption that POUserFunc will wrap anything inside a tuple though most of these functions will be useless when we have a UDF which outputs a tuple. To give an example, say we have a function which returns a tuple and we want to find its arity, ARITY(TupleRetUDF(*)) will always return one since POUserFunc will wrap the output of TupleRetUDF into another tuple and ARITY is changed to return just the size of the input tuple and not the size of the first field.

However, if we comment this code, then we need to modify FindQuantiles to consider the fact that everything will be wrapped inside a tuple & the behavior is not conditional upon the use of a star. I think this is better and Olga seems to agree as per her previous comment. Any other thoughts? Retain trunk behavior or change it?","15/Jan/09 00:34;olgan;Shravan, thanks for providing the background.

I still believe that we need to make ""*"" work like any other tuple. One reason is that a UDF that works with""*"" would not be able to work if any other tuple is passed to it which seems wrong. I don't know of any UDFs other than ARITY and SIZE that are specifically written for * so I think the impact to the users would be minimal.",16/Jan/09 14:47;shravanmn;Fixed by commenting the POStar code in POUserFunc and made minor changes to FindQuantiles & TestFRJoin. Changes TestBuiltin to include the commented assert statement for arity.,"17/Jan/09 00:30;olgan;patch committed. Thanks, SHravan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Anonymous tuples in bags create ParseExceptions,PIG-596,12411548,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,ciemo,ciemo,01/Jan/09 01:50,04/Aug/11 00:34,14/Mar/19 03:05,10/Mar/11 19:16,0.2.0,,,,,,0.9.0,,,,,0,,,,"{code}
One = load 'one.txt' using PigStorage() as ( one: int );

LabelledTupleInBag = foreach One generate { ( 1, 2 ) } as mybag { tuplelabel: tuple ( a, b ) };

AnonymousTupleInBag = foreach One generate { ( 2, 3 ) } as mybag { tuple ( a, b ) }; -- Anonymous tuple creates bug

Tuples = union LabelledTupleInBag, AnonymousTupleInBag;

dump Tuples;
{code}

java.io.IOException: Encountered ""{ tuple"" at line 6, column 66.
Was expecting one of:
    ""parallel"" ...
    "";"" ...
    "","" ...
    "":"" ...
    ""("" ...
    ""{"" <IDENTIFIER> ...
    ""{"" ""}"" ...
    ""["" ...
    
        at org.apache.pig.PigServer.parseQuery(PigServer.java:298)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:263)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:439)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:249)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:84)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:64)
        at org.apache.pig.Main.main(Main.java:306)
Caused by: org.apache.pig.impl.logicalLayer.parser.ParseException: Encountered ""{ tuple"" at line 6, column 66.

Why can't there be an anonymous tuple at the top level of a bag?
",,,,,,,,,,,,,,,,,PIG-1618,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-01-02 17:21:36.849,,,no_permission,,,,,,,,,,,,66193,,,,,Thu Mar 10 19:16:49 UTC 2011,,,,,,,0|i0gm53:,95017,,,,,,,,,,"01/Jan/09 17:05;ciemo;Note that specifying the tuple without the tuple designator doesn't work either.

{code}
One = load 'one.txt' using PigStorage() as ( one: int );

LabelledTupleInBag = foreach One generate { ( 1, 2 ) } as mybag { tuplelabel: tuple ( a, b ) };

AnonymousTupleInBag = foreach One generate { ( 2, 3 ) } as mybag { ( a, b ) };

Tuples = union LabelledTupleInBag, AnonymousTupleInBag;

dump Tuples;
{code}","01/Jan/09 17:09;ciemo;The reason I think it is important to be able to create anonymous tuples is because the tuples are anonymous in the LOAD statements.  Because, if you FLATTEN a bag such as mybag, any intermediate tuple label is immediately lost and the results of the flatten are mybag::a, mybag::b.  They are not mybag::tuplelabel::a, mybag::tuplelabel::b;","02/Jan/09 17:21;alangates;Flattening a bag gets rid of two layers of containment, both the bag and the tuple.  So the result of FLATTEN(bag(tuple(x, y, z)) is x, y, z not tuple(x, y, z).

At this point I believe tuples must be named in the LOAD statement as well as in foreach.  I'm not necessarily voting against anonymous tuples.  But I do believe Pig Latin is consistent in requiring names for tuples at the moment.","10/Mar/11 01:03;xuefuz;The old parser requires a name for the tuple type in a bag type. This requirement is dropped in the new parser. Thus, the following query works:

One = load 'one.txt' using PigStorage() as ( one: int );
LabelledTupleInBag = foreach One generate { ( 1, 2 ) } as mybag : { tuplelabel: tuple ( a, b ) };
AnonymousTupleInBag = foreach One generate { ( 2, 3 ) } as mybag : { ( a, b ) };

However, you do need separate field alias and its type with a "":"", as shown above.","10/Mar/11 19:16;xuefuz;Closed per comments:

The old parser requires a name for the tuple type in a bag type. This requirement is dropped in the new parser. Thus, the following query works:

One = load 'one.txt' using PigStorage() as ( one: int );
LabelledTupleInBag = foreach One generate { ( 1, 2 ) } as mybag : { tuplelabel: tuple ( a, b ) };
AnonymousTupleInBag = foreach One generate { ( 2, 3 ) } as mybag : { ( a, b ) };

However, you do need to separate field alias and its type with a "":"", as shown above.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use of Combiner causes java.lang.ClassCastException in ForEach,PIG-595,12411546,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,viraj,viraj,viraj,31/Dec/08 23:15,25/Mar/10 00:12,14/Mar/19 03:05,20/Oct/09 20:01,0.2.0,,,,,,0.6.0,,impl,,,0,,,,"The following Pig script causes a ClassCastException when QueryPairs is used in the ForEach statement. This is due to the use of the combiner.
{code}
QueryPairs = load 'querypairs.txt' using PigStorage()  as ( q1: chararray, q2: chararray );
describe QueryPairs;
QueryPairsGrouped = group QueryPairs by ( q1 );
describe QueryPairsGrouped;
QueryGroups = foreach QueryPairsGrouped generate
        group                                 as q1,
        COUNT(QueryPairs)       as paircount,
        QueryPairs                        as QueryPairs;
describe QueryGroups;
dump QueryGroups;
{code}
=================================================================================================================
2008-12-31 15:01:48,713 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher - Error message from task (map) task_200812151518_4922_m_000000java.lang.ClassCastException: org.apache.pig.data.DefaultDataBag cannot be cast to org.apache.pig.data.Tuple
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCombinerPackage.getNext(POCombinerPackage.java:122)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.processOnePackageOutput(PigCombiner.java:152)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.reduce(PigCombiner.java:143)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.reduce(PigCombiner.java:57)
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.combineAndSpill(MapTask.java:904)
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:785)
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:698)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:228)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2207)
=================================================================================================================",,,,,,,,,,,,,,,PIG-746,,,,31/Dec/08 23:16;viraj;querypairs.txt;https://issues.apache.org/jira/secure/attachment/12396986/querypairs.txt,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-20 20:01:34.804,,,no_permission,,,,,,,,,,,,164189,,,,,Tue Oct 20 20:01:34 UTC 2009,,,,,,,0|i0gm4n:,95015,,,,,,,,,,31/Dec/08 23:16;viraj;Input file,20/Oct/09 20:01;olgan;This bug is duplicate of https://issues.apache.org/jira/browse/PIG-746,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
schema inferred incorrectly,PIG-592,12411489,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,olston,olston,30/Dec/08 19:40,24/Mar/10 22:15,14/Mar/19 03:05,02/Oct/09 17:10,0.4.0,,,,,,0.6.0,,,,,0,,,,"A simple pig script, that never introduces any schema information:

A = load 'foo';
B = foreach (group A by $8) generate group, COUNT($1);
C = load 'bar';       // ('bar' has two columns)
D = join B by $0, C by $0;
E = foreach D generate $0, $1, $3;

Fails, complaining that $3 does not exist:

java.io.IOException: Out of bound access. Trying to access non-existent column: 3. Schema {B::group: bytearray,long,bytearray} has 3 column(s).

Apparently Pig gets confused, and thinks it knows the schema for C (a single bytearray column).
",,,,,,,,,,,,,,,,,,,14/Sep/09 23:09;daijy;PIG-592-1.patch;https://issues.apache.org/jira/secure/attachment/12419579/PIG-592-1.patch,01/Oct/09 22:25;daijy;PIG-592-2.patch;https://issues.apache.org/jira/secure/attachment/12421075/PIG-592-2.patch,02/Oct/09 01:50;daijy;PIG-592-3.patch;https://issues.apache.org/jira/secure/attachment/12421093/PIG-592-3.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-08-03 18:17:14.359,,,no_permission,,,,,,,,,,,,164186,,,,,Fri Oct 02 17:10:09 UTC 2009,,,,,,,0|i0gm3j:,95010,,,,,,,,,,"03/Aug/09 18:17;daijy;Also the following script produce the wrong schema:

a = load 'a';
b = load 'b';
c = join a by $0, b by $0;
describe c;

c: {bytearray,bytearray}

The correct behavior should be: If any of the input schema is unkown, the output schema is also unkown. ","15/Sep/09 01:01;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12419579/PIG-592-1.patch
  against trunk revision 814800.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/28/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/28/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/28/console

This message is automatically generated.","18/Sep/09 18:05;alangates;+1, patch looks good.  Let's get this in, as it's an annoying bug.",01/Oct/09 22:25;daijy;Fix unit test issues. Pass end-to-end tests.,"02/Oct/09 01:17;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421075/PIG-592-2.patch
  against trunk revision 820394.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 12 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/55/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/55/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/55/console

This message is automatically generated.",02/Oct/09 01:50;daijy;Address the failed unit test,"02/Oct/09 04:57;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421093/PIG-592-3.patch
  against trunk revision 820394.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 12 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/56/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/56/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/56/console

This message is automatically generated.",02/Oct/09 17:10;daijy;Patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
outer join query looses name information,PIG-577,12411279,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,olgan,olgan,24/Dec/08 01:03,24/Mar/10 22:04,14/Mar/19 03:05,03/Mar/09 23:51,0.2.0,,,,,,0.2.0,,,,,0,,,,"The following query:

A = LOAD 'student_data' AS (name: chararray, age: int, gpa: float);
B = LOAD 'voter_data' AS (name: chararray, age: int, registration: chararray, contributions: float);
C = COGROUP A BY name, B BY name;
D = FOREACH C GENERATE group, flatten((IsEmpty(A) ? null : A)), flatten((IsEmpty(B) ? null : B));
describe D;
E = FOREACH D GENERATE A::gpa, B::contributions;

Give the following error: (Even though describe shows correct information: D: {group: chararray,A::name: chararray,A::age: int,A::gpa: float,B::name: chararray,B::age: int,B::registration: chararray,B::contributions: float}

java.io.IOException: Invalid alias: A::gpa in {group: chararray,bytearray,bytearray}
        at org.apache.pig.PigServer.parseQuery(PigServer.java:298)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:263)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:439)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:249)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:84)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:64)
        at org.apache.pig.Main.main(Main.java:306)
Caused by: org.apache.pig.impl.logicalLayer.parser.ParseException: Invalid alias: A::gpa in {group: chararray,bytearray,bytearray}
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.AliasFieldOrSpec(QueryParser.java:5930)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.ColOrSpec(QueryParser.java:5788)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseEvalSpec(QueryParser.java:3974)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.UnaryExpr(QueryParser.java:3871)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.CastExpr(QueryParser.java:3825)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.MultiplicativeExpr(QueryParser.java:3734)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.AdditiveExpr(QueryParser.java:3660)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.InfixExpr(QueryParser.java:3626)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItem(QueryParser.java:3552)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItemList(QueryParser.java:3462)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.GenerateStatement(QueryParser.java:3419)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.NestedBlock(QueryParser.java:2894)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.ForEachClause(QueryParser.java:2309)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:966)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:742)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:537)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:60)
        at org.apache.pig.PigServer.parseQuery(PigServer.java:295)
        ... 6 more",,,,,,,,,,,,,,,,,,,03/Mar/09 18:41;sms;PIG-577.patch;https://issues.apache.org/jira/secure/attachment/12401328/PIG-577.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-12-24 01:44:34.247,,,no_permission,,,,,,,,,,,,164173,Reviewed,,,,Tue Mar 03 23:52:12 UTC 2009,,,Patch Available,,,,0|i0glx3:,94981,,,,,,,,,,"24/Dec/08 01:44;sms;The workaround is to invert the condition of the bincond and swap the two elements of the bincond. 

{code}
D = FOREACH C GENERATE group, flatten((not IsEmpty(A) ? A: null ), flatten((not IsEmpty(B) ? B: null ));
{code}

The root cause for this issue is the schema computation in LOBinCond. The assumption is that the schemas of the LHS and RHS of the bincond match all the time. The type checker ensures that this assumption is true. However, after each statement is parsed we do not run the type checker. The type checker is run only when describe, explain, dump or store is encountered.

As a result for the script reported in the bug, the type of the null constant is seen as bytearray and not as the schema of the RHS which is a bag.

The type checking logic should be invoked early by invoking the type checker after each statement or the type checking logic for bincond should be invoked by the getFieldSchema method to ensure the equivalence of the LHS and RHS schemas.","24/Dec/08 17:11;ciemo;Note that while this workaround does solve the schema problem, the resulting tuples in D will not have sufficient null elements if either A or B is null.  Especially if A is null.

Semantically, ït seems that the correct D statement should be something like:

D = FOREACH C GENERATE group, flatten((not IsEmpty(A) ? A: (null,null,null) ), flatten((not IsEmpty(B) ? B: (null,null,null) ));

However, this generates all sorts of parse errors.","24/Dec/08 18:53;sms;The correct statement to make mimic the outer join semantics will be:

{code}
D = FOREACH C GENERATE group, flatten((not IsEmpty(A) ? A : (bag{tuple(chararray, int, float)}){(null, null, null)})), flatten((not IsEmpty(B) ? B : (bag{tuple(chararray, int, chararray, float)}){(null,null,null, null)}));
{code}

However, this exposed a bag in the type checker where the schemas of the LHS and RHS do not match. The bag with the null constants has a tuple and relation A(or B) has a schema without the tuple. This issue was resolved in PIG-449. The solution proposed in PIG-449 has to be extended to schema comparisons that involve bags.

{code}
2008-12-24 10:31:58,529 [main] ERROR org.apache.pig.tools.grunt.Grunt - Two inputs of BinCond must have compatible schemas

2008-12-24 10:31:58,529 [main] ERROR org.apache.pig.tools.grunt.Grunt - org.apache.pig.impl.logicalLayer.FrontendException: Unable to describe schema for alias D
        at org.apache.pig.PigServer.dumpSchema(PigServer.java:367)
        at org.apache.pig.tools.grunt.GruntParser.processDescribe(GruntParser.java:153)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:188)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:84)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:71)
        at org.apache.pig.Main.main(Main.java:302)
Caused by: org.apache.pig.impl.plan.PlanValidationException: An unexpected exception caused the validation to stop
        at org.apache.pig.impl.plan.PlanValidator.validateSkipCollectException(PlanValidator.java:104)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingValidator.validate(TypeCheckingValidator.java:40)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingValidator.validate(TypeCheckingValidator.java:30)
        at org.apache.pig.impl.logicalLayer.validators.LogicalPlanValidationExecutor.validate(LogicalPlanValidationExecutor.java:79)
        at org.apache.pig.PigServer.compileLp(PigServer.java:687)
        at org.apache.pig.PigServer.dumpSchema(PigServer.java:360)
        ... 5 more
Caused by: org.apache.pig.impl.logicalLayer.validators.TypeCheckerException: Cannot resolve ForEach output schema.
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.visit(TypeCheckingVisitor.java:2731)
        at org.apache.pig.impl.logicalLayer.LOForEach.visit(LOForEach.java:122)
        at org.apache.pig.impl.logicalLayer.LOForEach.visit(LOForEach.java:41)
        at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:68)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.impl.plan.PlanValidator.validateSkipCollectException(PlanValidator.java:101)
        ... 10 more
Caused by: org.apache.pig.impl.logicalLayer.validators.TypeCheckerException: Problem during evaluaton of BinCond output type
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.visit(TypeCheckingVisitor.java:1913)
        at org.apache.pig.impl.logicalLayer.LOBinCond.visit(LOBinCond.java:88)
        at org.apache.pig.impl.logicalLayer.LOBinCond.visit(LOBinCond.java:27)
        at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:68)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.checkInnerPlan(TypeCheckingVisitor.java:2812)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.visit(TypeCheckingVisitor.java:2720)
        ... 15 more
Caused by: org.apache.pig.impl.logicalLayer.validators.TypeCheckerException: Two inputs of BinCond must have compatible schemas
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.visit(TypeCheckingVisitor.java:1903)
        ... 21 more

{code}","24/Dec/08 19:59;sms;The use of the null constant in the bincond in the context of a flatten should handle the following cases:

Assumption: one of the columns in the bincond is a null constant.

1. If the other column is a simple type or a map then cast the null to the other type.
2. If the other column is a complex type other than a map then remove the null constant and supplant it with 
a bag or a tuple or a map constant with the appropriate elements, i.e., if the other column
is a bag with a tuple that contains three columns (say int, float, chararray) then replace the
null constant with a bag that contains a tuple with three null constants. The same reasoning
applies to a tuple column.

Upon flattening the complex types will give out the appropriate number of columns.

Handling null constants for complex type has implications when the constant is materialized either
via dump or store. If the null constant is replaced with an appropriate bag/tuple/map then the materialized
constant will look like {(,,)} or (,,) or []. This conflicts with our existing view of nulls being empty when
materialized.",03/Mar/09 18:41;sms;Attached patch fixes the issue of schema comparisons that fail during an outer join specification. In addition a minor issue regarding the creation of the log file for PIG errors has been resolved. All unit tests pass.,"03/Mar/09 23:52;pkamath;+1, Patch committed - thanks for the fix Santhosh.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Large BZip files  Seem to loose data in Pig,PIG-570,12410972,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,breed,posix4e,posix4e,18/Dec/08 13:31,24/Mar/10 22:04,14/Mar/19 03:05,07/Jan/09 01:39,0.0.0,0.1.0,0.2.0,site,,,0.2.0,,,,,0,,,,"So I don't believe  bzip2 input to pig is working, at least not with large files. It seems as though map files are getting cut off. The maps complete way too quickly and the actual row of data that pig tries to process often randomly gets cut, and becomes incomplete. Here are my symptoms:

- Maps seem to be completing in a unbelievably fast rate

With uncompressed data
Status: Succeeded
Started at: Wed Dec 17 21:31:10 EST 2008
Finished at: Wed Dec 17 22:42:09 EST 2008
Finished in: 1hrs, 10mins, 59sec
map	100.00%
4670	0	0	4670	0	0 / 21
reduce	57.72%
13	0	0	13	0	0 / 4


With bzip compressed data

Started at: Wed Dec 17 21:17:28 EST 2008
Failed at: Wed Dec 17 21:17:52 EST 2008
Failed in: 24sec
Black-listed TaskTrackers: 2
Kind	% Complete	Num Tasks	Pending	Running	Complete	Killed	Failed/Killed
Task Attempts
map	100.00%
183	0	0	15	168	54 / 22
reduce	100.00%
13	0	0	0	13	0 / 0

The errors we get:
ava.lang.IndexOutOfBoundsException: Requested index 11 from tuple (rec	A, 0HAW, CHIX, )
	at org.apache.pig.data.Tuple.getField(Tuple.java:176)
	at org.apache.pig.impl.eval.ProjectSpec.eval(ProjectSpec.java:84)
	at org.apache.pig.impl.eval.SimpleEvalSpec$1.add(SimpleEvalSpec.java:38)
	at org.apache.pig.impl.eval.EvalSpec.simpleEval(EvalSpec.java:223)
	at org.apache.pig.impl.eval.cond.CompCond.eval(CompCond.java:58)
	at org.apache.pig.impl.eval.FilterSpec$1.add(FilterSpec.java:60)
	at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.run(PigMapReduce.java:117)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:227)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2207)
Last 4KB
attempt_200812161759_0045_m_000007_0	task_200812161759_0045_m_000007	tsdhb06.factset.com	FAILED	
java.lang.IndexOutOfBoundsException: Requested index 11 from tuple (rec	A, CSGN, VTX, VTX, 0, 20080303, 90919, 380, 1543, 206002)
	at org.apache.pig.data.Tuple.getField(Tuple.java:176)
	at org.apache.pig.impl.eval.ProjectSpec.eval(ProjectSpec.java:84)
	at org.apache.pig.impl.eval.SimpleEvalSpec$1.add(SimpleEvalSpec.java:38)
	at org.apache.pig.impl.eval.EvalSpec.simpleEval(EvalSpec.java:223)
	at org.apache.pig.impl.eval.cond.CompCond.eval(CompCond.java:58)
	at org.apache.pig.impl.eval.FilterSpec$1.add(FilterSpec.java:60)
	at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.run(PigMapReduce.java:117)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:227)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2207)
",Pig 0.1.1/Linux / 8 Nodes hadoop 0.18.2,,,,,,,,,,,,,,,,,,06/Jan/09 23:31;breed;PIG-570.patch;https://issues.apache.org/jira/secure/attachment/12397242/PIG-570.patch,06/Jan/09 23:31;breed;bzipTest.bz2;https://issues.apache.org/jira/secure/attachment/12397241/bzipTest.bz2,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-12-30 08:22:03.865,,,no_permission,,,,,,,,,,,,164166,,,,,Wed Jan 07 01:39:21 UTC 2009,,,,,,,0|i0gltz:,94967,,,,,,,,,,"30/Dec/08 08:22;breed;I believe the problem is due to bad position tracking. In the current version of the code, we chop up the input into blocks, but unfortunately when using bzip there are bzip block boundaries, HDFS block boundaries, and record boundaries. if the bzip block boundaries line up too closely, a record could get skipped or possibly corrupted.

i was able to reproduce a problem, hopefully it is the same as your problem in the attached test case.

the root cause turn out to be improper tracking of ""position"". if we blindly use the position of the underlying stream and a bzip block and HDFS block line up we may think that we have read the first record of the next slice when in fact we have only read the bzip block header.

the attached patch fixes the problem by defining the position of the stream as the position of the start of the current block header in the underlying stream.",30/Dec/08 08:29;breed;this is the test data for the bzip unit test. it should go under test/org/apache/pig/test/data/bzipTest.bz2,"31/Dec/08 00:42;olgan;Ben, thanks. This is great!

I tried to apply your patch but it failed. Can you make sure that you patch is relative to latest code in types branch. Also, is it possible to have smaller test data or is this the smallest data that shows the problem?","04/Jan/09 00:18;breed;Ah sorry, i did the patch with respect to trunk. i'll regen. it is probably possible to create a smaller test case, but it will take awhile. i did simple brute force trial an error to get the test case. (i thought it was pretty good that i was able to keep it to under 2M :) are you concerned about the size or the run time?","05/Jan/09 18:15;olgan;Ok, lets keep the size and just make the patch against types branch, thanks.",06/Jan/09 06:44;breed;Regenerated patch against types branch,06/Jan/09 16:17;breed;Tightened up the test case and increased the number of bits used for the signature to the full 48-bits. (Since I now use the start of the block boundary as the offset we can use the whole thing.),06/Jan/09 23:31;breed;Fixed the bzip for the test cases to have carefully crafted bad corner cases.,"07/Jan/09 01:39;olgan;patch committed; thanks, Ben!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dump and store outputs do not match for PigStorage,PIG-566,12410823,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,azaroth,sms,sms,16/Dec/08 19:17,17/Dec/10 22:43,14/Mar/19 03:05,17/May/10 17:27,0.7.0,0.8.0,,,,,0.8.0,,,,,0,,,,"The dump and store formats for PigStorage do not match for longs and floats.

{code}
grunt> y = foreach x generate {(2985671202194220139L)};
grunt> describe y;
y: {{(long)}}

grunt> dump y;
({(2985671202194220139L)})

grunt> store y into 'y';
grunt> cat y
{(2985671202194220139)}

{code}",,,,,,,,,,,,,,,,,,,13/May/10 17:44;azaroth;PIG-566.patch;https://issues.apache.org/jira/secure/attachment/12444409/PIG-566.patch,12/May/10 15:27;azaroth;PIG-566.patch;https://issues.apache.org/jira/secure/attachment/12444306/PIG-566.patch,10/May/10 23:05;azaroth;PIG-566.patch;https://issues.apache.org/jira/secure/attachment/12444157/PIG-566.patch,08/May/10 14:35;azaroth;PIG-566.patch;https://issues.apache.org/jira/secure/attachment/12444036/PIG-566.patch,07/May/10 13:08;azaroth;PIG-566.patch;https://issues.apache.org/jira/secure/attachment/12443961/PIG-566.patch,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2010-05-05 14:13:58.678,,,no_permission,,,,,,,,,,,,164162,Reviewed,,,,Mon May 17 17:27:28 UTC 2010,,,,,,,0|i0gls7:,94959,,,,,,,,,,"05/May/10 14:13;azaroth;What should the default format be? With or without L/F at the end?

The loader function already checks for the presence of a letter at the end, so we can accept both.

I think that without is better anyway, it complies to normal Java behaviour. The L/F is used only in source code.","05/May/10 22:47;daijy;Agree, I vote for without L/F.","07/May/10 13:08;azaroth;Attached a patch that modifies TupleFormat.

The online documentation needs to be changed accordignly.
For example, in Pig Latin Reference 2, the Data Types section. But probably in other places as well.","07/May/10 19:18;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12443961/PIG-566.patch
  against trunk revision 941976.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/309/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/309/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/309/console

This message is automatically generated.","08/May/10 11:54;azaroth;TestConversions fails reasons unrelated to the patch (sent an email about this), addressing other test problems.",08/May/10 14:35;azaroth;Modified unit tests to comply to the new format.,"08/May/10 20:04;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12444036/PIG-566.patch
  against trunk revision 941976.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 12 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/320/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/320/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/320/console

This message is automatically generated.","09/May/10 10:37;azaroth;TestConversions is the only test that fails, but it is unrelated to the patch.
It is ready for review.",10/May/10 20:21;olgan;what happens if you re-run the failing tests manually? (We can't commit code if there are failures in tests),"10/May/10 23:04;azaroth;It still fails also when manually run because there is a bug in UTF8StorageConverter.
This patch just made it pop out.

The problem is that bytesToInteger relied on the final L to recognize longs and avoid converting them.

More specifically, using Double d = Double.valueOf(s);
The method checks if the integer is convertible to a double and the double is actually a int by hand.
The check is performed only on the upper bound, so it would fail if we passed a double that is less than Integer.MIN_VALUE

I actually also do not understand why the check was done in this way:
d.doubleValue() > mMaxInt.doubleValue() + 1.0

I refactored it into :
Double.compare(d.doubleValue(), mMaxInt.doubleValue()) > 0

I added some unit tests for these edge cases.

I adapted also bytesToLong for consistency, even though the bug is not as evident because of rounding (MAX_LONG as a double is smaller than as a long).

The new patch passes all tests locally.","11/May/10 04:31;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12444157/PIG-566.patch
  against trunk revision 941976.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 15 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/312/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/312/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/312/console

This message is automatically generated.","11/May/10 07:18;daijy;Hi, Gianmarco
You are right there is a bug in TestConversions.bytesToInt. The problem is it cannot deal with negative large number. Positive large number is fine. mMaxInt.doubleValue() + 1.0 is trying to deal with the case of mMaxInt plus a fraction, which will truncate into mMaxInt. So I think we should keep this and deal with the negative case.","11/May/10 12:38;azaroth;TestTextDataParser.testLong had a problem I introduced with the last revision, the others are simply hudson problems.

I still do not understand the ""mMaxInt.doubleValue() + 1.0"" rationale. If we are converting a fraction to an int, we should probably return null, as a fraction is not a valid int.","11/May/10 17:22;daijy;Usually we convert a double into an integer by truncating. Returning null in this case break backward compatibility unnecessarily. Ideally the behavior of bytesToInteger is:
* Integer.MIN_INTEGER <= d < Integer.MAX_INTEGER+1: return ceil(d)
* Return null otherwise","12/May/10 15:27;azaroth;Ok, I understand.
Anyway this works only for integers, because their MAX/MIN_VALUE can be represented exactly as a double. The same cannot be done for longs.
You can try yourself:
Integer.MIN_VALUE:  -2147483648
As a double:               -2.147483648E9
Integer.MAX_VALUE:  2147483647
As a double:                2.147483647E9

Long.MIN_VALUE:     -9223372036854775808
As a double:               -9.223372036854776E18
Long.MAX_VALUE:     9223372036854775807
As a double:                9.223372036854776E18

The double values for longs are rounded up. Adding or subtracting 1 to MAX/MIN does not make sense for longs.
For example Long.valueOf(Long.MAX_VALUE).doubleValue() + 1 gets casted without errors.
Have a look at the test I added, TestConversions.testOverflow.
To make the test pass I had to add/subtract 10000 to the MAX/MIN in order to have an invalid long.

So, I restored the +/-1 in Utf8StorageConverter for Integers. I added a >= / <= to avoid converting Integer.MAX_VALUE + 1 and Integer.MIN_VALUE -1.
For long, according to me, it does not make sense, because +/-1 is not enough to make a different double.","12/May/10 18:28;daijy;Yes, you are right. This trick does not always work due to the rounding. But the original code seems no harm to try to do that. The bug in the original code is the negative case. As long as we address the negative case we fix the test failure. I don't think we should go beyond that in this patch. Also for the L/l, F/f suffix part, user can generate input data externally (not through TupleFormat), in this case, we still want to recognize these suffix. ","12/May/10 20:46;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12444306/PIG-566.patch
  against trunk revision 943522.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 18 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/327/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/327/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/327/console

This message is automatically generated.",13/May/10 17:44;azaroth;Addressed the issues highlighted in Daniel's comment,14/May/10 03:22;daijy;+1 once hudson test pass.,"15/May/10 14:55;azaroth;I submitted to hudson 2 days ago, but still no result.
Is there some problem? Should I resubmit it?",15/May/10 18:01;daijy;Seems hudson is down. I will manually run the tests.,"17/May/10 10:02;mridul;
Just to point out an error in the comment above :

For the case of 
    *  Integer.MIN_INTEGER <= d < Integer.MAX_INTEGER+1:

pig should return floor(d), not ceil(d)","17/May/10 17:27;daijy;Yes, my mistake. Thanks Mridul. Fortunately Gianmarco doesn't listen to me :) I manually test the patch, all tests pass. Committed to trunk. Thanks Gianmarco!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Several builting functions no longer support bytearray,PIG-565,12410750,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chandec,olgan,olgan,16/Dec/08 00:35,17/Dec/10 22:43,14/Mar/19 03:05,03/Sep/10 22:45,0.2.0,,,,,,0.8.0,,,,,0,,,,"ARITY
DIFF
TOKENIZE

All we need to do is to add lookup tables.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-09-03 22:45:55.74,,,no_permission,,,,,,,,,,,,164161,,,,,Fri Sep 03 22:45:55 UTC 2010,,,,,,,0|i0glrr:,94957,,,,,,,,,,"05/Aug/10 21:46;olgan;ARITY has been depricated for a while and the code looks completely wrong so I am not gointg to fix that. SIZE that replaced it does the right thing.
","05/Aug/10 21:57;olgan;DIFF handles all the types as expected. However, the documentation in 0.7.0 release is slightly of. 

Current docs: ""The DIFF function compares two fields in a tuple. If the field values match, null is returned. If the field values do not match, the non-matching elements are returned.""

Should say something like: 

""DIFF takes two bags as arguments and compares them.   Any tuples that are in one bag but not the other are returned in a bag. If the bags match an empty bag is returned.  If the fields are not bags then they will be wrapped in tuples and returned in a bag if they do not match, or an empty bag will be returned if the two records match. The implementation assumes that both bags being passed to this function will fit entirely into memory simultaneously.  If that is not the case the UDF will still function, but it will be <strong>very</strong> slow.""

I will reassign this bug to Corinne once I am done with it.


",05/Aug/10 22:15;olgan;I also verified that TOKENIZE works as expected with bytearrays.,"05/Aug/10 22:15;olgan;Corinne, please, update DIFF description, thanks",03/Sep/10 22:45;chandec;DIFF function updated in Pig-Latin-Ref-Manual-2 via Pig-1600.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Parameter Substitution using -param option does not seem to work when parameters contain special characters such as +,=,-,?,' """,PIG-564,12410731,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,viraj,viraj,15/Dec/08 20:04,02/May/13 02:29,14/Mar/19 03:05,04/Jun/09 17:21,0.2.0,,,,,,0.3.0,,impl,,,2,,,,"Consider the following Pig script which uses parameter substitution
{code}
%default qual '/user/viraj'
%default mydir 'mydir_myextraqual'
VISIT_LOGS = load '$qual/$mydir' as (a,b,c);
dump VISIT_LOGS;
{code}

If you run the script as:
==================================================================================================================
java -cp pig.jar:${HADOOP_HOME}/conf/ -Dhod.server='' org.apache.pig.Main -param mydir=mydir-myextraqual mypigparamsub.pig
==================================================================================================================
You get the following error:
==================================================================================================================
2008-12-15 19:49:43,964 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - java.io.IOException: /user/viraj/mydir does not exist
        at org.apache.pig.backend.executionengine.PigSlicer.validate(PigSlicer.java:109)
        at org.apache.pig.impl.io.ValidatingInputFileSpec.validate(ValidatingInputFileSpec.java:59)
        at org.apache.pig.impl.io.ValidatingInputFileSpec.<init>(ValidatingInputFileSpec.java:44)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:200)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:742)
        at org.apache.hadoop.mapred.jobcontrol.Job.submit(Job.java:370)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs(JobControl.java:247)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.run(JobControl.java:279)
        at java.lang.Thread.run(Thread.java:619)

java.io.IOException: Unable to open iterator for alias: VISIT_LOGS [Job terminated with anomalous status FAILED]
        at org.apache.pig.PigServer.openIterator(PigServer.java:389)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:269)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:178)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:84)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:64)
        at org.apache.pig.Main.main(Main.java:306)
Caused by: java.io.IOException: Job terminated with anomalous status FAILED
        ... 6 more
==================================================================================================================
Also tried using:  -param mydir='mydir\-myextraqual'
This behavior occurs if the parameter value contains characters such as +,=, ?. 

A workaround for this behavior is using a param_file which contains <param_name>=<param_value> on each line, with the <param_value> enclosed by quotes. For example:
mydir='mydir-myextraqual' and then running the pig script as:
java -cp pig.jar:${HADOOP_HOME}/conf/ -Dhod.server='' org.apache.pig.Main -param_file myparamfile mypigparamsub.pig

The following issues need to be fixed:
1) In -param option if parameter value contains special characters, it is truncated
2) In param_file, if  param_value contains a special characters, it should be enclosed in quotes
3) If 2 is a known issue then it should be documented in http://wiki.apache.org/pig/ParameterSubstitution",,,,,,,,,,,,,,,,,,,02/Jun/09 21:36;olgan;PIG-564.patch;https://issues.apache.org/jira/secure/attachment/12409702/PIG-564.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-01-28 10:28:19.444,,,no_permission,,,,,,,,,,,,164160,,,,,Thu Jun 04 17:21:23 UTC 2009,,,,,,,0|i0glrb:,94955,,,,,,,,,,"28/Jan/09 10:28;romainr;Another possible workaround seems to wrap the param between two ' or "" and escape its beginning with a  \ 

-param url='http://www.yahoo.com' --> http 

-param url='\http://www.yahoo.com' --> http://www.yahoo.com",28/Jan/09 22:04;viraj;There are certain cases where the param contains ' single quotes and '' double quotes. ,"06/Apr/09 21:44;ciemo;Period (.) is also a special character that seems to cause problems.

See related JIRA PIG-754","30/Apr/09 03:03;viraj;Another special character ""/"" is not handled correctly and is particularly useful when passing nested output directories.
It is truncated when it is passed to Pig.

Example case:
{code}
a = load '/user/viraj/test1' using PigStorage(',') as (id1:int, char1:chararray);

b = foreach a generate id1;

store b into '/user/viraj/$outputfolder' using PigStorage();
{code}

Run this script as:
{code}
shell>hadoop fs -mkdir /user/viraj/paramtest

shell>java -cp pig.jar:/home/viraj/hadoop-0.18.0-dev/conf/ -Dhod.server='' org.apache.pig.Main -param outputfolder=""paramtest/moretest"" -r paramtest.pig

2009-04-30 03:00:21,234 [main] INFO  org.apache.pig.Main - Dry run completed. Substituted pig script is at paramtest.pig.substituted

{code}

Now if we open the substituted Pig script (paramtest.pig.substituted).
{code}
a = load '/user/viraj/test1' using PigStorage(',') as (id1:int, char1:chararray);

b = foreach a generate id1;

store b into '/user/viraj/paramtest' using PigStorage();
{code}

Viraj","02/Jun/09 23:14;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12409702/PIG-564.patch
  against trunk revision 780722.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 24 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 release audit.  The applied patch generated 159 release audit warnings (more than the trunk's current 156 warnings).

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/68/testReport/
Release audit warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/68/artifact/trunk/current/releaseAuditDiffWarnings.txt
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/68/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/68/console

This message is automatically generated.","03/Jun/09 00:41;alangates;Questions/comments on the patch.  

1) Why did output1.pig change to look exactly like the new input5.pig?  It seems like output1.pig shouldn't have changed.

2) A comment in the javacc files on how OTHER and IDENTIFIER interact in the pattern matching might be helpful, as it isn't immediately obvious (at least to me :) ).

As long as 1 is ok, then +1.","03/Jun/09 00:47;olgan;Alan, thanks for review. 

(1) output1.pig is a generated file. I think it was checked in initially by mistake. Its content is irrelevant.
(2) I might have to resubmit a patch anyway if I figure out the extra warnings (the link is broken at the moment). If I have to do that, I will also add comments.","03/Jun/09 04:03;gkesavan;Use this link for releaseaudit warnings:
http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/68/artifact/trunk/patchprocess/releaseAuditDiffWarnings.txt

I 've fixed the test-patch scripts for the broken link.","03/Jun/09 16:45;olgan;Thanks, Giri. I looked at warnings anf they are result of adding new test files for parameter substitution that don't have apache license because they don't support comments. So nothing to fix there.

I will be committing the patch later today.","04/Jun/09 11:41;hudson;Integrated in Pig-trunk #463 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/463/])
    : problem with parameter substitution and special charachters (olgan)
",04/Jun/09 17:21;olgan;patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UTFDataFormatException (encoded string too long) is thrown when storing strings > 65536 bytes (in UTF8 form) using BinStorage(),PIG-560,12410514,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,pkamath,pkamath,11/Dec/08 19:17,24/Mar/10 22:04,14/Mar/19 03:05,31/Jan/09 04:39,0.2.0,,,,,,0.2.0,,,,,0,,,,"BinStorage() uses DataOutput.writeUTF() and DataInput.readUTF() Java API to write out Strings as UTF-8 bytes and to read them back. From the Javadoc - ""First, the total number of bytes needed to represent all the characters of s is calculated. If this number is larger than 65535, then a UTFDataFormatException  is thrown. "" (because the writeUTF() API uses 2 bytes to represent the number of bytes). A way to get around this would be to not use writeUTF()/ReadUTF() and instead hand convert the string to the corresponding UTF-8 byte[]  (using String.getBytes(""UTF-8"") and then write the length of the byte array as an int - this will allow a size of upto 2^32 (2 raised to 32).",,,,,,,,,,,,,,,,,,,31/Jan/09 00:13;sms;PIG-560.patch;https://issues.apache.org/jira/secure/attachment/12399185/PIG-560.patch,31/Jan/09 01:01;sms;PIG-560_1.patch;https://issues.apache.org/jira/secure/attachment/12399192/PIG-560_1.patch,28/Jan/09 02:31;laukik;utf-limit-patch.diff;https://issues.apache.org/jira/secure/attachment/12398874/utf-limit-patch.diff,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-01-28 02:31:05.503,,,no_permission,,,,,,,,,,,,164156,Reviewed,,,,Sat Jan 31 04:39:42 UTC 2009,,,,,,,0|i0glpj:,94947,,,,,,,,,,"28/Jan/09 02:31;laukik;The patch uses the String object's getBytes(charsetname) method to convert the string to UTF bytes, instead of the writeUTF() function. Now, an int can be used for storing the length instead of the 2 bytes used by the writeUTF(). Also includes the corresponding change while reading in a CHARARRAY.","30/Jan/09 00:41;alangates;I'm concerned here that we're adding 2 bytes to every string we store for a case which should be quite rare (how often to people have strings longer than 64K?)  Would it be better to have bin storage define a long string type that uses 4 bytes to encode it's length, and then test a string's length before writing it out and leave things as they are now for most strings and use the new long string for anything over 64K?","30/Jan/09 01:06;laukik;The writeUTF() method was adding 2 bytes per string; we would actually be adding an int (32 bits) with this solution.

The new long string would then be required to be a new DataType, right? To make it transparent to the user, this DataType can just be used internally. Also, to keep things efficient, may be we can insert the string as this datatype only on getting the encoded-string-too-long  UTFDataFormatException.

By the way, though it looks quite probable that the average length of a string used would be far less than 64k, do we have any statistic on the average length of (UTF converted) CHARARRAYs? This would also help us in determining how big an overhead the additional 16 bits actually is. ","30/Jan/09 02:03;olgan;We know that since we put this changes into the production, only one other person complained so we are pretty certain it is a very rare case. I agree with Alan that we should only pay the penalty on long strings",30/Jan/09 22:22;sms;Attached patch (PIG-560.patch) addresses the issue of storing strings larger than 65535 bytes in length using BinStorage. A new BIGCHARRAY type has been added to PIG. This type is used internally for storing and loading Strings that are bigger than 64K bytes. A new unit test case that tests this code path has been added and an existing test case has been modified.,30/Jan/09 22:27;sms;Changing the patch by deleting a commented out line.,30/Jan/09 22:33;olgan;I think the unit test was added for BinaryStorage not BinStorage.,31/Jan/09 00:09;sms;New patch (PIG-560.patch) adds the test case in TestEvalPipeline.java. Fixes a bug in the previous patch.,31/Jan/09 00:13;sms;Final patch!,"31/Jan/09 00:26;laukik;In the current patch, when the length is <65536, the string to UTF8 conversion is happening twice -- once with String::getBytes() and once with DataOutput::writeUTF()

To avoid that, instead of writeUTF(), how about using writeShort() followed by writeBytes() since we would already have the length and the UTF8 bytes? 
",31/Jan/09 00:26;olgan;+1,31/Jan/09 00:59;sms;Incorporated comments from Laukik. Submitting a new patch and modified test case. Running the tests now.,31/Jan/09 01:01;sms;Uploading the right patch.,31/Jan/09 04:39;sms;Patch has been committed. Thanks Laukik for the initial patch and the review comments.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ARITY and SIZE function produce wrong value for tuples,PIG-559,12410498,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,11/Dec/08 17:24,25/Mar/10 00:12,14/Mar/19 03:05,12/Dec/08 02:19,,,,,,,0.2.0,,,,,0,,,,"As Santhosh pointed out the problem is that instead of taking the first element of the input tuple and applying size to it, the code just applices the size to the input tuple which is always 1.",,,,,,,,,,,,,,,,,,,11/Dec/08 20:41;olgan;PIG-559.patch;https://issues.apache.org/jira/secure/attachment/12395872/PIG-559.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-12-12 00:47:56.433,,,no_permission,,,,,,,,,,,,164155,,,,,Fri Dec 12 02:19:08 UTC 2008,,,,,,,0|i0glp3:,94945,,,,,,,,,,"12/Dec/08 00:47;pkamath;+1 - the only comment is whether the following check is required :
{code}
if (input == null || input.size() == 0)
            return null;
{code}

In POUserFunc if the input is not directly attached to POUserFunc, it create a new Tuple and fills it with the getNext() result of its input. To my knowledge currently the POUserFunc will not be the root of Foreach inner plan and hence input should not be directly attached. So could these checks be avoided? ",12/Dec/08 02:19;olgan;patch committed with modifications suggested by pradeep,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Distinct followed by a Join results in Invalid size 0 for a tuple error,PIG-558,12410128,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,viraj,viraj,06/Dec/08 02:28,24/Mar/10 22:04,14/Mar/19 03:05,06/Jan/09 01:24,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"The following Pig script does a right outer join after the DISTINCT.
{code}
nonuniqtable1 = LOAD 'table1' AS (f1:chararray);
table1 = DISTINCT nonuniqtable1;
table2 = LOAD 'table2' AS (f1:chararray, f2:int);
temp = COGROUP table1 BY f1 INNER, table2 BY f1;
DESCRIBE temp;
explain temp;
dump temp;
{code}
========================================================================================================
It results in the following error. This is true for other join types as well.
========================================================================================================
java.io.IOException: Invalid size 0 for a tuple
	at org.apache.pig.data.DataReaderWriter.readDatum(DataReaderWriter.java:57)
	at org.apache.pig.data.DataReaderWriter.readDatum(DataReaderWriter.java:62)
	at org.apache.pig.builtin.BinStorage.getNext(BinStorage.java:90)
	at org.apache.pig.backend.executionengine.PigSlice.next(PigSlice.java:103)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.SliceWrapper$1.next(SliceWrapper.java:157)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.SliceWrapper$1.next(SliceWrapper.java:133)
	at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:165)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:45)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:227)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2209)
========================================================================================================",,,,,,,,,,,,,,,,,,,03/Jan/09 00:00;pkamath;PIG-558-additional.patch;https://issues.apache.org/jira/secure/attachment/12397046/PIG-558-additional.patch,03/Jan/09 00:00;pkamath;PIG-558.patch;https://issues.apache.org/jira/secure/attachment/12397045/PIG-558.patch,06/Dec/08 02:30;viraj;table1;https://issues.apache.org/jira/secure/attachment/12395468/table1,06/Dec/08 02:31;viraj;table2;https://issues.apache.org/jira/secure/attachment/12395469/table2,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2009-01-02 23:59:40.187,,,no_permission,,,,,,,,,,,,164154,,,,,Tue Jan 06 01:24:21 UTC 2009,,,,,,,0|i0glon:,94943,,,,,,,,,,06/Dec/08 02:30;viraj;Table1 for test,06/Dec/08 02:31;viraj;Table2 for test,"02/Jan/09 23:59;pkamath;The issue was that table 1 has only one column which is also the join key. Due to a recent optimization wherein parts of the value which are in the key would be omitted, this results in an empty tuple being sent as the value from POLocalRearrange.  The POPackage following the POLocalRearrange would look at metadata stored in itself to figure out how to construct the value out of the key if necessary. However when the POLocalRearrange is in a reduce and the POPackage is in the next map, the POLocalRearrange output gets written to DFS in BinStorage format resulting in a tuple of size 0 being written out. BinStorage while reading considers a tuple of size 0 to be a fatal error.

Fix:
The patch fixes BinStorage to consider a tuple of size 0 to be a valid tuple which is reconstructed as such. The POPackage then builds up the correct value from the key. The patch also has a unit test to test this.

The unit test depends on certain functions introduced in MiniCluster and test/org/apache/pig/test/Util.java as of the patch in PIG-580. If PIG-580 is not committed before this patch, then the ""additional"" patch (""PIG-558-additional.patch"") attached here should also be applied.","06/Jan/09 01:24;olgan;patch committed; thanks, pradeep",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FindQuantiles function does not report progress,PIG-556,12409988,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,04/Dec/08 18:11,24/Mar/10 22:04,14/Mar/19 03:05,12/Dec/08 06:48,0.2.0,,,,,,0.2.0,,,,,0,,,,"FindQuantiles does not call progress.  Most of the time this isn't an issue as it finishes quickly.  But for very large (multi-terabyte) order bys, it does not finish in the required five minutes.  This causes the whole job to fail.",,,,,,,,,,,,,,,,,,,05/Dec/08 01:09;alangates;PIG-556.patch;https://issues.apache.org/jira/secure/attachment/12395352/PIG-556.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-12-05 01:16:55.618,,,no_permission,,,,,,,,,,,,164152,,,,,Fri Dec 12 06:48:35 UTC 2008,,,,,,,0|i0glnz:,94940,,,,,,,,,,"05/Dec/08 01:09;alangates;The attached patch addresses several issues.  It addresses the lack of progress() calls in FindQuantiles.  It also fixes two other problems:

# On order bys the sampling job was running the default number of reduces even though all data was sent to one reduce.  This was fixed to only run one reduce for that job.
# The progress reporter being passed to EvalFuncs is null.  I believe this is because it is not getting properly serialized and sent across from front end to back end.  I changed it so that the progress reporter is set each time the EvalFunc is called.  I also set set EvalFunc.setReporter to be final so that these set calls can hopefully be inlined by the java optimizer.",05/Dec/08 01:16;olgan;So the patch also addresses https://issues.apache.org/jira/browse/PIG-540?,"05/Dec/08 01:18;olgan;The patch loos good, +1","05/Dec/08 01:26;alangates;https://issues.apache.org/jira/browse/PIG-556?focusedCommentId=12653577#action_12653577

yes.",12/Dec/08 06:48;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EvalFunc.finish() not getting called,PIG-553,12409610,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,shravanmn,olston,olston,02/Dec/08 02:29,25/Mar/10 00:12,14/Mar/19 03:05,07/Feb/09 06:50,0.2.0,,,,,,0.2.0,,,,,0,,,,My EvalFunc's finish() method doesn't seem to get invoked.,"""local"" mode",,,,,,,,,,,,,,,,,,28/Jan/09 12:17;shravanmn;553.patch;https://issues.apache.org/jira/secure/attachment/12398897/553.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-01-15 18:52:26.995,,,no_permission,,,,,,,,,,,,164149,Reviewed,,,,Sat Feb 07 06:50:46 UTC 2009,,,,,,,0|i0glmn:,94934,,,,,,,,,,"15/Jan/09 18:52;olgan;Shravan, could you take a look.",19/Jan/09 09:20;shravanmn;I think the finish method in EvalFunc is just a remnant of our earlier code(prev trunk). I do not think we planned to include it because I don't see any infrastructure to call it. Though Chris's concern is only in local mode and it can be fixed temporarily by just calling a visitor that visits all the operators and if a POUserFunc is found calls its finish method. But what do we do for the MapRed case? Should we implement a similar behaviour in the close method? Do we need to support this kind of an operation for all operators? If so then should we also give a place holder for open()? ,20/Jan/09 10:51;shravanmn;Fixed the local mode by writing a visitor that calls the EvalFunc.finish() method. Waiting for comments from others on my earlier query,"20/Jan/09 17:10;alangates;We don't want a different interface for local and MR mode.  This should work in both modes.

Users aren't currently asking for an open method, and it's possible to work around no open by keeping a state variable to see if you've been called before.","20/Jan/09 19:05;olgan;Shravan,

I agree with Alan we should have the same code for local and MR mode. We actually have users who are interested in this functionality in MR mode. They want to generate footer information.",21/Jan/09 12:11;shravanmn;Ok. Will create a modified patch.,"23/Jan/09 09:23;shravanmn;Added a Visitor(UDFFuncVisitor that calls the EvalFunc.finish() method)
Made modifications to LocalPigLauncher. After finishing plan execution, the visitor is called to call EvalFunc.finish() on all UDFs present in the plan
Made similar modifications to PigMapBase.close(), PigMapReduce.Reduce.close(). No change to PigCombiner.Combine.close() as only intermediate versions of the Algebraic functions are called and only in the reduce do they finish. So the PigMapReduce.Reduce.close() takes care of that.

Another thing, is that this being a visitor call might be expensive but considering that its done in the close method and executes once per task it should be ok I guess.",23/Jan/09 19:02;sms;I will be reviewing this patch.,"23/Jan/09 20:04;sms;Review comments:

1. The code looks fine.
2. There are no unit test cases. We need unit test cases to ensure that the code path is exercised in all cases (preferably at least map reduce case)
3. In algebraic functions, since intermediate is called only in PigCombiner and the UDF visitor is never called in the PigCombiner, users should be aware that finish() is never called for intermediate methods. The UDF documentation has to be updated to reflect this caveat.","28/Jan/09 12:17;shravanmn;Created TestCase that tests calling of finish in both MR & local mode and in each mode both scenarios: one where udf is in map phase & another where udf is in reduce phase.

Note that I have not used the minicluster because the MR job will run in another VM and its an overkill to use IPC to figure out whether finish() was called.",07/Feb/09 06:50;sms;Patch has been committed. Thanks for the fix Shravan.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FilterFunc calls empty constructor when it should be calling parameterized constructor,PIG-546,12409365,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,viraj,viraj,27/Nov/08 02:41,24/Mar/10 22:04,14/Mar/19 03:05,08/Dec/08 19:54,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"The following piece of Pig Script uses a custom UDF known as FILTERFROMFILE which extends the FilterFunc. It contains two constructors, an empty constructor which is mandatory and the parameterized constructor. The parameterized constructor  passes the HDFS filename, which the exec function uses to construct a HashMap. The HashMap is later used for filtering records based on the match criteria in the HDFS file.
{code}
register util.jar;
--util.jar contains the FILTERFROMFILE class

define FILTER_CRITERION util.FILTERFROMFILE('/user/viraj/insetfilterfile');

RAW_LOGS = load 'mydata.txt' as (url:chararray, numvisits:int);

FILTERED_LOGS = filter RAW_LOGS by FILTER_CRITERION(numvisits);

dump FILTERED_LOGS;
{code}

When you execute the above script,  it results in a single Map only job with 1 Map. It seems that the empty constructor is called 5 times, and ultimately results in failure of the job.
===========================================
parameterized constructor: /user/viraj/insetfilterfile
parameterized constructor: /user/viraj/insetfilterfile
empty constructor
empty constructor
empty constructor
empty constructor
empty constructor
===========================================
Error in the Hadoop backend
===========================================
java.lang.IllegalArgumentException: Can not create a Path from an empty string
	at org.apache.hadoop.fs.Path.checkPathArg(Path.java:82)
	at org.apache.hadoop.fs.Path.(Path.java:90)
	at org.apache.pig.backend.hadoop.datastorage.HDataStorage.isContainer(HDataStorage.java:199)
	at org.apache.pig.backend.hadoop.datastorage.HDataStorage.asElement(HDataStorage.java:130)
	at org.apache.pig.impl.io.FileLocalizer.openDFSFile(FileLocalizer.java:164)
	at util.FILTERFROMFILE.init(FILTERFROMFILE.java:70)
	at util.FILTERFROMFILE.exec(FILTERFROMFILE.java:89)
	at util.FILTERFROMFILE.exec(FILTERFROMFILE.java:52)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:179)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:217)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter.getNext(POFilter.java:148)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:170)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:158)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.map(PigMapOnly.java:65)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:227)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2209)
===========================================
Attaching the sample data and the filter function UDF.",,,,,,,,,,,,,,,,,,,27/Nov/08 02:45;viraj;FILTERFROMFILE.java;https://issues.apache.org/jira/secure/attachment/12394803/FILTERFROMFILE.java,08/Dec/08 17:47;sms;PIG-546.patch;https://issues.apache.org/jira/secure/attachment/12395578/PIG-546.patch,03/Dec/08 01:49;viraj;insetfilterfile;https://issues.apache.org/jira/secure/attachment/12395155/insetfilterfile,27/Nov/08 02:46;viraj;mydata.txt;https://issues.apache.org/jira/secure/attachment/12394804/mydata.txt,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2008-12-08 17:47:49.938,,,no_permission,,,,,,,,,,,,38084,,,,,Mon Dec 08 19:54:51 UTC 2008,,,Patch Available,,,,0|i0gljj:,94920,,,,,,,,,,27/Nov/08 02:45;viraj;FilterFunc UDF,27/Nov/08 02:46;viraj;Sample data used in the PigScript,03/Dec/08 01:49;viraj;Test Filter,"08/Dec/08 17:47;sms;The patch (PIG-546.patch)  addresses the following issue(s):

1. Fixes the use of an alias declared via the define statement and the subsequent use in
   i. Filter functions
  ii. Load functions
  iii. Store functions
  iv. Order by functions
  v. Streaming specifications (input and output)

2. New unit test cases for the parser, end-to-end test cases for streaming and filter udf have been added.

Note: There are no end-to-end test cases for order by using a UDF.

All unit test cases pass.","08/Dec/08 19:54;olgan;patch committed. Thanks, Santhosh.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PERFORMANCE: Sampler for order bys does not produce a good distribution,PIG-545,12409232,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,alangates,alangates,26/Nov/08 00:52,24/Mar/10 22:04,14/Mar/19 03:05,18/Feb/09 19:22,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"In running tests on actual data, I've noticed that the final reduce of an order by has skewed partitions.  Some reduces finish in a few seconds while some run for 20 minutes.  Getting a better distribution should lead to much better performance for order by.",,,,,,,,,,,,,,,,,,,14/Feb/09 02:11;pkamath;PIG-545-v3.patch;https://issues.apache.org/jira/secure/attachment/12400222/PIG-545-v3.patch,18/Feb/09 00:19;pkamath;PIG-545-v4.patch;https://issues.apache.org/jira/secure/attachment/12400366/PIG-545-v4.patch,05/Feb/09 12:35;shravanmn;WRP.patch;https://issues.apache.org/jira/secure/attachment/12399562/WRP.patch,09/Feb/09 12:34;shravanmn;WRP1.patch;https://issues.apache.org/jira/secure/attachment/12399813/WRP1.patch,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2008-11-26 01:11:11.548,,,no_permission,,,,,,,,,,,,164143,Reviewed,,,,Wed Feb 18 19:22:44 UTC 2009,,,,,,,0|i0glj3:,94918,,,,,,,,,,"26/Nov/08 01:11;sms;The current sampler uses random sampling, assuming uniform distribution of sort keys. Using Poisson distribution will enable the sampler to figure out the expected value of the distribution without knowing the actual distribution. This will ensure (more) even distribution of data for the reducers.","21/Jan/09 00:34;alangates;I ran the pigmix queries L9 (order by a single column) and L10 (order by multiple columns) and found some interesting results.

For L9, the total ordering job (job 3), took 587 seconds.  Min and max times for individual reducers were 92 and 589 seconds (I'm not sure how 1 reducer ran 2 sec longer than total job time, but all these numbers come from the hadoop web ui).  Seven of the 40 reducers (including the 92 second one) received no records to sort.  The long running 589 second job received one key, which had 2M values.

For L10, the total ordering job took 238 seconds.  Min and max times for individual reducers were 99 seconds (3 keys, 32K records) and 232 seconds (413K keys, 496K records).

From this I draw a couple of conclusions:  

One, our order by partitioner could be better built.  There is no reason a reducer should ever receive 0 records.  And in a job with 3 uncorrelated keys we still see a > 10x disparity in data distribution.  The partitioner needs to do a better job of producing even distributions of the keys to reducers.

Two, just getting better sampling won't resolve the issue for order by queries that have one or a few keys with a very high number of values, such as in a zipf distribution.  Unfortunately for us, zipf is a very common data distribution.  In this case our partitioner may need to be able to detect and split large keys by round robining them to a group of reducers.","22/Jan/09 18:15;sms;<quote>
Two, just getting better sampling won't resolve the issue for order by queries that have one or a few keys with a very high number of values, such as in a zipf distribution. Unfortunately for us, zipf is a very common data distribution. In this case our partitioner may need to be able to detect and split large keys by round robining them to a group of reducers.
</quote>

Better sampling will not resolve the issue for order by. It will help in having more evenly sized partitions for the reducers. Since its sampling and not brute force approach of checking out the cardinality of each key, there will always be a non-zero probability of one reducer getting more data than the other reducers. The better sampling approach will minimize such occurrences.

Secondly, post sampling, we can ensure that reducers get the right partitions by using Hadoop's ability to pick reducers based on partition functions. I am not quite sure how Pig can propose a generic partition function to achieve this.","05/Feb/09 12:35;shravanmn;This patch implements the Weighted Range Partitioner as detailed in the Dewitt et. al. paper on Practical Skew Handling in Parallel Joins. The JobControlCompiler has been modified to use the new partitioner for order by. So the old unit tests should be valid.

One caveat is that we need to mention the number of reducers via the parallel keyword when doing order by. Currently, if you don't specify it by default there will just be one partition and it messes up the distribution. We need to do something about this. Another thing is when the Partitioner gets configured it reads the entire sample file from HDFS but it currently doesn't do any reporting as I could not think of a way to do it right now","05/Feb/09 23:02;alangates;I ran the pigmix L9 (order by of single field) and L10 (order by of multiple fields).  L9 went from 14 minutes to 8, so this patch holds huge promise.  But L10 went from 8 minutes to 11, so it doesn't seem to be working well in the multiple field case.  (It could also be related to the fact that L10 uses descending on one of the columns, I don't know if the new partitioner can handle that or not.)  I also ran our end to end order by tests on it, and all passed, except bigdata_1, which fails with an IndexOutOfBounds exception in the new WeightedRangePartitioner class.  

As for the caveat that it needs to know the number of reducers up front, I believe in cases where the user doesn't say parallel, that we can determine the parallelism of the reduces using JobClient.getDefaultReduces().  We need to double check that this will give us the right information in both the hod and non-hod cases.","06/Feb/09 16:05;shravanmn;Thanks for running the patch Alan. I figured out the IndexOutOfBounds exception & fixed it. That should not happen.

I was also working on the L10 issue. I tried it outside of Pig by sending it tuples(int,string) with ordering required as (desc,asc). It works fine. So I don't think there is any problem with the partitioner there. Most of the things like asc, desc & user comparator should be handled as I use the comparator passed to me through the jobConf.  So I checked the samples file that was generated. Its not sorted at all. The main assumption is invalid and the partitioner will definitely get messed up.

I finally figured that the way we are doing the compilation of order by in MRCompiler is wrong. When we do the nested sort using the input POSort, we are converting it into ""order by *"" instead it should be ""order by $0, $1, $2 ...""

I have started L10 with the changes. WIll update with the results.",06/Feb/09 18:00;shravanmn;It worked with the changes to sort. It produced an even distribution and took about 3 mins lesser. There is still a slight tuning to be done as the first partition is not getting enough data. I will try to tweak it a bit and check if its better than the current one.,"09/Feb/09 12:34;shravanmn;Ran some tests and this quantiles scheme seems to have the least deviation from perfect distribution. Also, the time took for L10 has reduced. It took 8 mins vs 7 mins for the old code. But it produces a good distribution as shown below: The patch also modifies MRCompiler to fix sort on multiple fields with different order for each column.
New algorithm:
{noformat}
/part-00000<r 3>	396866140
/part-00001<r 3>	388565356
/part-00002<r 3>	412419093
/part-00003<r 3>	404673062
/part-00004<r 3>	407805613
/part-00005<r 3>	399685590
/part-00006<r 3>	374470156
/part-00007<r 3>	407210410
/part-00008<r 3>	392022575
/part-00009<r 3>	403592598
/part-00010<r 3>	407005509
/part-00011<r 3>	392739807
/part-00012<r 3>	407132246
/part-00013<r 3>	393974442
/part-00014<r 3>	394310422
/part-00015<r 3>	397676923
/part-00016<r 3>	408960794
/part-00017<r 3>	407120924
/part-00018<r 3>	398555578
/part-00019<r 3>	398831802
/part-00020<r 3>	381319493
/part-00021<r 3>	397961816
/part-00022<r 3>	408716378
/part-00023<r 3>	401850651
/part-00024<r 3>	394624621
/part-00025<r 3>	411533286
/part-00026<r 3>	397598333
/part-00027<r 3>	402013011
/part-00028<r 3>	412664722
/part-00029<r 3>	390615865
/part-00030<r 3>	402257701
/part-00031<r 3>	404278892
/part-00032<r 3>	408376085
/part-00033<r 3>	403230193
/part-00034<r 3>	396062725
/part-00035<r 3>	403166437
/part-00036<r 3>	396123295
/part-00037<r 3>	400208557
/part-00038<r 3>	396028297
/part-00039<r 3>	428541846
{noformat}
Old Algorithm:
{noformat}
/part-00000<r 3>	39703
/part-00001<r 3>	396917259
/part-00002<r 3>	388958263
/part-00003<r 3>	412109839
/part-00004<r 3>	405626251
/part-00005<r 3>	411808194
/part-00006<r 3>	385084639
/part-00007<r 3>	618796205
/part-00008<r 3>	59754649
/part-00009<r 3>	506719655
/part-00010<r 3>	403039137
/part-00011<r 3>	406540458
/part-00012<r 3>	395629722
/part-00013<r 3>	404795418
/part-00014<r 3>	394881722
/part-00015<r 3>	393959841
/part-00016<r 3>	398194260
/part-00017<r 3>	408370148
/part-00018<r 3>	334248039
/part-00019<r 3>	260118680
/part-00020<r 3>	642453106
/part-00021<r 3>	383168594
/part-00022<r 3>	364791108
/part-00023<r 3>	408601454
/part-00024<r 3>	404588449
/part-00025<r 3>	392940424
/part-00026<r 3>	413354408
/part-00027<r 3>	412538285
/part-00028<r 3>	385894942
/part-00029<r 3>	412674723
/part-00030<r 3>	392572446
/part-00031<r 3>	403012671
/part-00032<r 3>	398679596
/part-00033<r 3>	410864380
/part-00034<r 3>	405389743
/part-00035<r 3>	397248129
/part-00036<r 3>	401438264
/part-00037<r 3>	396456821
/part-00038<r 3>	402122621
/part-00039<r 3>	816408998
{noformat}","14/Feb/09 02:11;pkamath;Attached a revised version of the last patch with the following changes:
1) When parallel is not specified the code now consults jobClient to get defaultReduces() and uses 0.9 times the value as the number of reducers (and hence the number of quantiles)
2) There was a bug in the way order by * was handled in MRCompiler  which is now fixed
3) In WeightedRangePartitioner the basic idea is to first set up the quantiles array as the last element of a quantile (partition). Then the code iterates over all the sample items and if it finds an item which equals the quantile element for the partition, then there is a good chance this item may repeat in the next quantile. The occurences of such sample items in each partition are recorded to use when deciding which partition such an item in the real data should go to. The occurences in each partition over the total occurences of such an element gives the probability that such an element should go to the given partition. In the earlier version of the patch, to set this up, the code was comparing a sample item with the quantile element of the next partition instead of the quantile element of the partition in which the sample element falls (since the quantile element is the last element of the partition, it should be used in the comparison to decide if this element is likely to crossover to the next partition). This has been fixed.
4) The earlier patch was not handling the case where number of samples < quantiles - this is handled now.","17/Feb/09 23:30;olgan;The patch looks good. A couple of comments/questions:

(1) One comment that I have is that MalFormedProbVecException does not follow new error handling practices. It is not derived from PigException and it does not have a way to pass error code to it.

(2) My understanding is that weighted partitioner would actually add overhead to well randomly distributed data. I am fine saying that this case is not very common or that the overhead is not great enough to worry about but I wanted to check what other committers think.

(3) WeightedRangePartitioner.java throws RuntimeException - this is not consistent with our new error handling.","18/Feb/09 00:19;pkamath;Attached patch which addresses observation (1) in the previous comment.

Regarding (2), the differences from the current SortPartitioner are:
 - In the configure of the WeightedRangePartitioner the entire sample file is loaded and a hash is built for those sample elements which repeat across partition boundaries - this is more expensive than previously but one time set up
- IN the getPartition() call, if the input element is present in the hash, then probabilistically a partition is assigned based on the information in the hash else (as previously), a binary search of the quantiles array is done to figure out the partition - overall this should be nearly the same as previously performance wise.

Regarding (3), getPartition() is a hadoop interface method and hence we cannot throw any other exception besides RuntimeException",18/Feb/09 19:22;pkamath;Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Utf8StorageConverter.java does not always produce NULLs when data is malformed,PIG-544,12409228,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,olgan,olgan,25/Nov/08 23:53,24/Mar/10 22:04,14/Mar/19 03:05,06/Mar/09 22:53,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"It does so for scalar types but not for complext types and not for the fields inside of the complext types.

This is because it uses different code to parse scalar types by themselves and scalar types inside of a complex type. It should really use the same (its own code to do so.)

The code it is currently uses, is inside of TextDataParser.jjt and is also used to parse constants so we need to be careful if we want to make changes to it.",,,,,,,,,,,,,,,,,,,06/Mar/09 21:26;thejas;544.2.patch;https://issues.apache.org/jira/secure/attachment/12401646/544.2.patch,05/Mar/09 22:01;thejas;544.patch;https://issues.apache.org/jira/secure/attachment/12401553/544.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-11-26 00:09:15.001,,,no_permission,,,,,,,,,,,,38465,Reviewed,,,,Fri Mar 06 22:53:57 UTC 2009,,,Patch Available,,,,0|i0glin:,94916,,,,,,,,,,"26/Nov/08 00:09;sms;Another use case where scalars also generate errors:

{code}

grunt> a = load 'student_tab.data';
grunt> store a into 'student_tab.bin' using BinStorage();
grunt> a = load 'student_tab.bin' using BinStorage() as (name: int, age: int, gpa: float);
grunt> dump a;

2008-11-25 16:02:40,986 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher - Error message from task (map) task_200809241441_24635_m_000000java.lang.RuntimeException : Unexpected data type 74 found in stream.
         at org.apache.pig.data.DataReaderWriter.readDatum(DataReaderWriter.java:115)
         at org.apache.pig.builtin.BinStorage.bytesToInteger(BinStorage.java:169)
         at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(POCast.java:143) 
....
{code}","20/Feb/09 23:58;thejas;Handles long/float values with l/L/f/F at the end in Utf8StorageConverter.java bytesToLong, bytesToFloat .
Added unit tests to TestConversions.java .",23/Feb/09 18:02;thejas;Deleting the patch attached that actually belonged to different JIRA.,"03/Mar/09 23:05;thejas;Patch.
- DataReaderWriter bytestoX logic for complex datatypes  re-factored into separate functions in same class and used by BinStorage.
- Utf8StorageConverter logs warning and returns null if type conversions fail.
- Moved logging utility functions from org.apache.pig.tools.grunt.Utils to org.apache.pig.impl.util.LogUtils. Updated files that used grunt.Utils .
- Added unit test cases for type conversion failures in TestEvalPipeline2 .
","04/Mar/09 23:33;sms;Review comments:

Index: test/org/apache/pig/test/TestEvalPipeline2.java
=======================================================

1. Its not clear how the constant 1634952294 is arrived at. A comment alluding to this fact will help. The same comment applies to other constants that are used in this test case.
{code}
+    @Test
+    public void testBinStorageByteArrayCastsSimple() throws IOException {
+        assertTrue( (Integer)tup.get(0) == 1634952294);
{code}

Index: src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POProject.java
========================================================================================

1. Do we need mLog? POProject extends ExpressionOperator which in turn extends PhysicalOperator. PhysicalOperator has a private member variable called log. Its better to use log instead of creating another member variable.
{code}
+    protected final Log mLog = LogFactory.getLog(getClass());
{code}

2. I am not sure if processInputBag calls parseFromBytes(). Can you clarify?

{code}
+        Result res;
+        try{
+            res = processInputBag();
+        }
+        catch (Exception e){
+            Result r = new Result();
+            r.returnStatus = POStatus.STATUS_OK;
+            // can happen if parseFromBytes identifies it as being of different type
{code}


Index: src/org/apache/pig/builtin/BinStorage.java
===================================================================

1. Can we make the log private static ?
{code}
+    protected final Log mLog = LogFactory.getLog(getClass());
{code}

2. Minor comment. Use bytearray instead of byres array to be consistent with the bytearray type. Also, be consistent with use of upper case for types, I see bag and then Long, Map, etc.
{code}
+            LogUtils.warn(this, ""Unable to convert bytes array to bag, "" +
{code}

3. In bytesToTuple, the message should be tuple and not bag

{code}
+    public Tuple bytesToTuple(byte[] b) {
+            LogUtils.warn(this, ""Unable to convert bytes array to bag, "" +
{code}

4. Documentation. Now that error code 2110 has been removed,  http://wiki.apache.org/pig/PigErrorHandlingFunctionalSpecification has to be updated to reflect the removal of this error code.

{code}
-            int errCode = 2110;
-            String msg = ""Could not convert bytearray to map."";
-            throw new ExecException(msg, errCode, PigException.BUG);
{code}

5. The bytes to (Integer/Long/Float/Double) methods are interpreting the first 4 to 8 bytes (depending on the type) and returning the value as the value for the converted type. This is a semantics question. Should we return null or should we do a best effort conversion of bytes to basic types? For example in the bytesToCharArray routine, the first couple of bytes are expected to be representative of the length of the string. The same argument holds good for the tuple too. However for the numeric types, a fixed number of bytes is interpreted as the value.","05/Mar/09 22:01;thejas;Santhosh, Thanks for reviewing the changes and the suggestions.
The suggestions have been incorporated in this new patch, which replaces the old one.
Re: Index: src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POProject.java,  I have removed the changes in that as those are not necessary for fixing this JIRA. (I had accidentally included it in the patch.). It will be useful to fix similar problem described in PIG-696 .

Re: The semantics of bytearray to numeric types, a best effort conversion will be done. ie if there are minimum number of bytes required for the conversion, it will convert the bytearray to the specified number type.


","06/Mar/09 21:26;thejas;Attaching a new patch, because there were problems in applying the diffs generated by ""svn mv"" in the old one (Utils.java moved to LogUtils.java).
This patch does not have LogUtils.java . The changes made in LogUtils.java over Utils.java are under the Utils.java diff in patch.
After the patch is applied, the Utils.java should be moved to LogUtils.java  using 'svn mv'.
",06/Mar/09 22:53;sms;Patch has been committed. Good work on the test cases. Thanks Thejas!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bincond can't work with flatten bags,PIG-538,12408787,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,olgan,olgan,19/Nov/08 00:09,24/Mar/10 22:04,14/Mar/19 03:05,03/Dec/08 23:15,0.2.0,,,,,,0.2.0,,,,,0,,,,"The following script is user with trunk code to simulated outer join not directly supported by pig:

A = load '/studenttab10k' as (name: chararray, age: int, gpa: float);
B = load 'votertab10k' as (name: chararray, age: int, registration: chararray, donation: float);
C = cogroup A by name, B by name;
D = foreach C generate group, (IsEmpty(A) ? '' : flatten(A)), (IsEmpty(B) ? 'null' : flatten(B));

On types branch this gives syntax error and even beyond that not supported since bincond requires that both expressions be of the same type. Santhosh suggested to have  special NULL expression that matches any type. This seems to make sense.",,,,,,,,,,,,,,,,,,,03/Dec/08 20:00;pkamath;PIG-538.patch;https://issues.apache.org/jira/secure/attachment/12395206/PIG-538.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-12-03 19:59:10.5,,,no_permission,,,,,,,,,,,,164137,,,,,Wed Dec 03 23:15:29 UTC 2008,,,,,,,0|i0glfz:,94904,,,,,,,,,,"03/Dec/08 19:59;pkamath;The patch has the following changes:
   * support a null constant through the ""null"" keyword
   * allow for picking right overloaded UDF function when input schema has bytearrays and there are many candidate UDFs to pick from
   * fix for a bug in the matching function for picking right overloaded function when the UDF supports input schema with complex types which have null inner schemas

1) Support for null constants - For this there are changes in QueryParser.jjt and DataType.java to represent null as a Constant of type bytearray. Besides that, there are changes in TypeCheckingVisitor to handle implicit casting of null constants (which are bytearrays) to the appropriate type in Binconds, AND, OR, ==, !=. Arithmetic operators already cast byearrays to doubles.

2) The algorithm for picking the right overloaded UDF function is explained in a comment in TypeCheckingVisitor.java pasted here for reference. The changes follow the comment:
{noformat}
       /**
         * Here is an explanation of the way the matching UDF funcspec will be chosen
         * based on actual types in the input schema.
         * First an ""exact"" match is tried for each of the fields in the input schema
         * with the corresponding fields in the candidate funcspecs' schemas. 
         * 
         * If exact match fails, then first a check if made if the input schema has any
         * bytearrays in it. 
         * 
         * If there are NO bytearrays in the input schema, then a best fit match is attempted
         * for the different fields. Essential a permissible cast from one type to another
         * is given a ""score"" based on its position in the ""castLookup"" table. A final
         * score for a candidate funcspec is deduced as  
         *               SUM(score_of_particular_cast*noOfCastsSoFar). 
         * If no permissible casts are possible, the score for the candidate is -1. Among 
         * the non -1 score candidates, the candidate with the lowest score is chosen. 
         * 
         * If there are bytearrays in the input schema, a modified exact match is tried. In this
         * matching, bytearrays in the input schema are not considered. As a result of
         * ignoring the bytearrays, we could get multiple candidate funcspecs which match
         * ""exactly"" for the other columns - if this is the case, we notify the user of
         * the ambiguity and error out. Else if all other (non byte array) fields 
         * matched exactly, then we can cast bytearray(s) to the corresponding type(s)
         * in the matched udf schema. If this modified exact match fails, the above best fit 
         * algorithm is attempted by initially coming up with scores and candidate funcSpecs 
         * (with bytearray(s) being ignored in the scoring process). Then a check is 
         * made to ensure that the positions which have bytearrays in the input schema
         * have the same type (for a given position) in the corresponding positions in
         * all the candidate funcSpecs. If this is not the case, it indicates a conflict
         * and the user is notified of the error (because we have more than
         * one choice for the destination type of the cast for the bytearray). If this is the case,
         * the candidate with the lowest score is chosen. 
         */
{noformat}


3) To allow the matching function to pick a UDF which supports a schema with a complex type which has null inner schema, the schema equality for matching purposes is relaxed for inner schemas of complex types.","03/Dec/08 23:15;olgan;patch tested and committed. thanks, pradeep",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failure in Hadoop map collect stage due to type mismatch in the keys used in cogroup,PIG-537,12408707,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,pkamath,viraj,viraj,18/Nov/08 04:22,24/Mar/10 22:04,14/Mar/19 03:05,20/Nov/08 20:05,0.2.0,,,,,,0.2.0,,,,,0,,,,"Consider the following pig query, which demonstrates various problems during the Logical Plan creation and the subsequent execution of the M/R job. In this query we do two cogroups, one between A and B to generate an alias ABtemptable. Then we again cogroup A with ABtemptable based on marks which was read in as an int. 
==================================================================================
{code}
A = load 'mymarks.txt' as (marks:int, username:chararray);
B = load 'mygrades.txt' as (username:chararray,grade:chararray);
ABtemp = cogroup A by username, B  by username;
ABtemptable = foreach ABtemp generate
           group as username,
           flatten(A.marks) as newmarks;
--describe ABtemptable;
C = cogroup A by marks, ABtemptable by newmarks;
--describe C;
explain C;
dump C;
{code}
==================================================================================
The schema for C and ABtemptable which pig reports:
==================================================================================
{code}describe ABtemptable;{code} ABtemptable: {username: chararray,newmarks: int}
{code}describe C;{code} C: {group: int,A: {username: chararray,marks: int},ABtemptable: {username: chararray,newmarks: int}}
==================================================================================
If you run the above query you get the following error:
==================================================================================
2008-11-18 03:57:14,372 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher - Error message from task (map) task_200810152105_0156_m_000000java.io.IOException: Type mismatch in key from map: expected org.apache.pig.impl.io.NullableText, recieved org.apache.pig.impl.io.NullableIntWritable
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:415)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:97)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:172)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:158)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:82)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:227)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2209)
==================================================================================
Looking at the {code}explain C;{code} output, you see that newmarks has become a chararray (surprising!!)
==================================================================================
---CoGroup viraj-Tue Nov 18 03:49:42 UTC 2008-25 Schema: {group: Unknown,{username: bytearray,marks: int},ABtemptable: {username: chararray,newmarks: chararray}} Type: bag
      Project viraj-Tue Nov 18 03:49:42 UTC 2008-23 Projections: [1] Overloaded: false FieldSchema: marks: int Type: int
      Input: SplitOutput[null] viraj-Tue Nov 18 03:49:42 UTC 2008-29
      Project viraj-Tue Nov 18 03:49:42 UTC 2008-24 Projections: [1] Overloaded: false FieldSchema: newmarks: chararray Type: chararray
       Input: ForEach viraj-Tue Nov 18 03:49:42 UTC 2008-22
    ---ForEach viraj-Tue Nov 18 03:49:42 UTC 2008-22 Schema: {username: chararray,newmarks: chararray} Type: bag
==================================================================================
In Summary this script demonstrates the following problems:
1) Logical Plan creation
2) When cogrouping with fields of different types which results in group unknown is not caught during compile phase.
Additionally I am enclosing the explain output of alias C and testfiles to run the script which is on this jira!!
Viraj",,,,,,,,,,,,,,,,,,,20/Nov/08 17:29;pkamath;PIG-537.patch;https://issues.apache.org/jira/secure/attachment/12394354/PIG-537.patch,18/Nov/08 04:26;viraj;explain_aliasC.log;https://issues.apache.org/jira/secure/attachment/12394132/explain_aliasC.log,18/Nov/08 04:29;viraj;mygrades.txt;https://issues.apache.org/jira/secure/attachment/12394133/mygrades.txt,18/Nov/08 04:30;viraj;mymarks.txt;https://issues.apache.org/jira/secure/attachment/12394134/mymarks.txt,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2008-11-20 17:27:24.237,,,no_permission,,,,,,,,,,,,164136,,,,,Thu Nov 20 20:05:27 UTC 2008,,,,,,,0|i0glfj:,94902,,,,,,,,,,18/Nov/08 04:26;viraj;Explain output for alias C,18/Nov/08 04:29;viraj;Test file mygrades.txt,18/Nov/08 04:30;viraj;Test file mymarks.txt,"20/Nov/08 17:27;pkamath;The issue was in Implicit Split inserter. In this query, the same load provides input to two cogroups. Hence an implicit split needs to be introduced. However the ImplicitSplitInserter was changing the order of the inputs to the first cogroup as it was rewiring the plan with the new Split and SplitOutput operators. The patch changes the algorithm for introducing these new operators so that the order of the inputs for the successors of the load is maintained.","20/Nov/08 20:05;olgan;patch committed; thanks, pradeep",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"the shell script 'pig' does not work if PIG_HOME has the word 'hadoop' in it's directory, and pig script is missing in the types branch",PIG-536,12408633,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,miguno,iholsman,iholsman,17/Nov/08 10:18,04/Aug/11 00:34,14/Mar/19 03:05,15/Jun/11 21:50,0.9.0,,,,,,0.9.0,,grunt,,,2,,,,"ran into this one today when running from user 'hadoop' whoose home directory is /home/hadoop/

also for some reason this script isn't in the pig-types branch..


Index: bin/pig
===================================================================
--- bin/pig	(revision 711801)
+++ bin/pig	(working copy)
@@ -124,7 +124,8 @@
 # libraries in the lib dir, so don't blindly add them all.    Only add the one
 # that matche PIG_HADOOP_VERSION.
 for f in $PIG_HOME/lib/*.jar; do
-    IS_HADOOP=`echo $f | grep hadoop`
+    FILENAME=`basename $f`
+    IS_HADOOP=`echo $FILENAME | grep hadoop`
     if [ ""${IS_HADOOP}x"" == ""x"" ]; then
         CLASSPATH=${CLASSPATH}:$f;
     else ",,,,,,,,,,,,,,,,,,,18/Mar/11 09:59;miguno;PIG-536.patch;https://issues.apache.org/jira/secure/attachment/12473983/PIG-536.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-01-20 09:01:34.683,,,no_permission,,,,,,,,,,,,66305,,,,,Wed Jun 15 21:50:02 UTC 2011,,,Patch Available,,,,0|i0glf3:,94900,Fixes a bug in bin/pig where jar files in lib/ are not properly added to CLASSPATH when PIG_HOME contains the word 'hadoop'.,,,,,,,,,"20/Jan/11 09:01;mr_luk;It seems, this is still an issue in pig 0.8...","17/Mar/11 15:22;miguno;This bug is also still in the current [trunk version of {{bin/pig}}|http://svn.apache.org/viewvc/pig/trunk/bin/pig?view=markup].

FWIW, we have fixed our {{bin/pig}} scripts with the same 1-liner (using {{basename}}) as orginally reported by Ian.","17/Mar/11 16:58;olgan;please, submit the patch that and we can make the fix for Pig 0.9","18/Mar/11 09:59;miguno;Patch for the current trunk version of bin/pig. Same fix should work for 0.7 and 0.8 branches, too.","15/Jun/11 21:50;olgan;patch committed to 0.9 branch and trunk. Michael, thanks for contributing",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Illustrate can't handle Map's or NULLs,PIG-534,12408613,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yanz,iholsman,iholsman,17/Nov/08 00:46,04/Aug/11 00:35,14/Mar/19 03:05,12/Feb/11 00:49,,,,,,,0.9.0,,grunt,,,0,,,,"when I 'illustrate' a record that contains a map, or has a NULL it crashes with a NPE.",,,,,,,,,,,,,,,,,,,17/Nov/08 00:47;iholsman;Illustrate.patch;https://issues.apache.org/jira/secure/attachment/12394029/Illustrate.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-02-12 00:49:24.095,,,no_permission,,,,,,,,,,,,66107,,,,,Sat Feb 12 00:49:24 UTC 2011,,,Patch Available,,,,0|i0gle7:,94896,,,,,,,,,,"17/Nov/08 00:47;iholsman;I'm not sure if Maps can only contain simple objects, hence the check for a 'tuple'.","12/Feb/11 00:49;olgan;I re-tested this with the latest Pig code and it is working fine:

A = load 'studentab10k' as (name, age, gpa);
B = foreach A generate TOMAP('name', name, 'age', age, 'gpa', gpa);
illustrate B;

| A     | name:bytearray   | age:bytearray   | gpa:bytearray   |
----------------------------------------------------------------
|       | sarah carson     | 28              | 0.37            |
----------------------------------------------------------------
-------------------------------------------------
| B     | null:map                              |
-------------------------------------------------
|       | {age=28, name=sarah carson, gpa=0.37} |
-------------------------------------------------
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Casting a field removes its alias.,PIG-532,12408548,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,thejas,alangates,alangates,15/Nov/08 00:32,24/Mar/10 22:04,14/Mar/19 03:05,07/Mar/09 02:02,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"Given a script like:

{code}
a = loader 'myfile' as (x, y);
b = foreach a generate (int)x, (double)y;
c = group a by x;
{code}

you will get an error that x is an unknown alias.  The cast operator is not carrying through the alias.  It should.",,,,,,,,,,,,,,,,,,,05/Mar/09 16:00;thejas;532.patch;https://issues.apache.org/jira/secure/attachment/12401527/532.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-02-28 01:55:14.422,,,no_permission,,,,,,,,,,,,38572,Reviewed,,,,Sat Mar 07 02:02:32 UTC 2009,,,Patch Available,,,,0|i0gldb:,94892,,,,,,,,,,"28/Feb/09 01:55;sms;The change is in QueryParser.jjt line number 2500

Add the following line

fs.alias = exprOp.alias;

before 

cast.setFieldSchema(fs);
","05/Mar/09 16:00;thejas;Patch
QueryParser.jjt - Passing alias information to LOCast
TestLogicalPlanBuilder.java - Added test cases for this patch. Corrected other test cases that were passing multiple statements at a time to buildPlan().
",06/Mar/09 21:51;sms;I am reviewing this patch,07/Mar/09 02:02;sms;Patch has been committed. Thanks for the fix Thejas.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schema returned in UDF is not used by Pig,PIG-528,12408466,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,sms,sms,14/Nov/08 00:30,24/Mar/10 22:04,14/Mar/19 03:05,15/Nov/08 00:56,0.2.0,,,,,,0.2.0,,,,,0,,,,"Using an identity UDF that returns the input schema as the output schema leads to schema truncation in Pig.

{code}

grunt> a = load '/tudent_tab.data' as (name, age, gpa);
grunt> b = foreach a generate IdentityFunc(name, age);

grunt> describe b;
b: {name: bytearray}
--It should have been b:{(name: bytearray, age: bytearray)}
{code}

The outputSchema method in IdentityFunc is given below:

{code}
    @Override
    public Schema outputSchema(Schema input) {
        return input;  
    }
{code}",,,,,,,,,,,,,,,,,,,14/Nov/08 21:43;sms;PIG-528.patch;https://issues.apache.org/jira/secure/attachment/12393959/PIG-528.patch,14/Nov/08 23:18;sms;PIG-528_1.patch;https://issues.apache.org/jira/secure/attachment/12393968/PIG-528_1.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-11-15 00:56:54.557,,,no_permission,,,,,,,,,,,,38600,,,,,Sat Nov 15 00:56:54 UTC 2008,,,Patch Available,,,,0|i0glbj:,94884,,,,,,,,,,"14/Nov/08 21:43;sms;Attached patch (PIG-528.patch) contains the following:

1. Fix for handling schemas returned by UDFs
2. Unit test cases for the fix

All unit test cases passed.",14/Nov/08 23:18;sms;Updated patch removing merge conflicts due to the earlier patch.,"15/Nov/08 00:56;olgan;patch committed; thanks , Santhosh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig does not support storing nested data using default storage,PIG-527,12408465,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,sms,sms,14/Nov/08 00:26,24/Mar/10 22:04,14/Mar/19 03:05,18/Nov/08 17:13,0.2.0,,,,,,0.2.0,,,,,0,,,,"Pig does not allow storing nested data using the default storage function (PigStorage)

{code}

grunt> a = load 'student_tab.data' as (name, age, gpa);
grunt> b = group a by age;
grunt> store b into '/user/sms/data/complex.data';

2008-11-13 16:21:17,711 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 50% complete

2008-11-13 16:21:52,747 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Map reduce job failed

2008-11-13 16:21:52,747 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Job failed!

2008-11-13 16:21:52,764 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher - Error message from task (reduce) task_200809241441_21188_r_000000java.io.IOException: Cannot store a non-flat tuple using PigStorage

        at org.apache.pig.builtin.PigStorage.putNext(PigStorage.java:196)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:116)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:90)
        at org.apache.hadoop.mapred.ReduceTask$3.collect(ReduceTask.java:300)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.processOnePackageOutput(PigMapReduce.java:238)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:224)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:136)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:318)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2207)

2008-11-13 16:21:52,764 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher - Error message from task (reduce) task_200809241441_21188_r_000000java.io.IOException: Cannot store a non-flat tuple using PigStorage

{code}",,,,,,,,,,,,,,,,,,,18/Nov/08 03:12;sms;PIG-527.patch;https://issues.apache.org/jira/secure/attachment/12394125/PIG-527.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-11-18 17:13:54.778,,,no_permission,,,,,,,,,,,,164130,,,,,Tue Nov 18 17:13:54 UTC 2008,,,Patch Available,,,,0|i0glb3:,94882,,,,,,,,,,"18/Nov/08 03:12;sms;Attached patch (PIG-527.patch) addresses the following:

1. PigStorage allows storage of nested data
2. Unit tests to test the same

All unit tests pass","18/Nov/08 17:13;olgan;patch committed; thanks, santhosh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problem in using negative (-a),PIG-522,12408207,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,sms,sms,11/Nov/08 00:39,24/Mar/10 22:04,14/Mar/19 03:05,18/Dec/08 23:59,0.2.0,,,,,,0.2.0,,,,,0,,,,"Using negative, i.e., -a leads to exceptions. 

{code}
grunt> a = load 'myfile' as  (name:chararray, age:int, gpa:double);
grunt> b = foreach a generate -gpa;
grunt> dump b;

2008-11-10 16:38:12,517 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete
2008-11-10 16:38:37,539 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Map reduce job failed
2008-11-10 16:38:37,540 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Job failed!
2008-11-10 16:38:37,542 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher - Error message from task (map) task_200809241441_19426_m_000000java.io.IOException: Received Error while processing the map plan.
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:197)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:158)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.map(PigMapOnly.java:65)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:227)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2207)

{code}",,,,,,,,,,,,,,,,,,,18/Dec/08 19:58;pkamath;PIG-522.patch;https://issues.apache.org/jira/secure/attachment/12396420/PIG-522.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-12-18 19:58:09.392,,,no_permission,,,,,,,,,,,,164125,,,,,Thu Dec 18 23:59:02 UTC 2008,,,,,,,0|i0gl93:,94873,,,,,,,,,,"18/Dec/08 19:58;pkamath;Currently PIG models negative numeric constants as a PONegative which has the corresponding positive numeric constant as its operand. The root cause of the issue reported is that in the LogToPhyTranslator, a PONegative PhysicalOperator is created from the LONegative LogicalOperator. By default a PhysicalOperator's resultType is Tuple. This causes the error reported in the bug since because the resultType is set to Tuple, PONegative.getNext(Tuple) is incorrectly called instead of PONegative.getNext(Integer). The LogToPhyTranslator should change the resultType for PONegative to be the same as the type of LONegative. 

The patch makes the needed code change in PONegative and also adds unit test to test it.","18/Dec/08 23:59;olgan;patch committed; thanks, pradeep",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LOBinCond  exception in LogicalPlanValidationExecutor when providing default values for bag,PIG-518,12407828,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,viraj,viraj,05/Nov/08 02:34,14/May/10 06:45,14/Mar/19 03:05,15/Apr/10 00:16,0.2.0,,,,,,0.7.0,,impl,,,0,,,,"The following piece of Pig script, which provides default values for bags {('','')}  when the COUNT returns 0 fails with the following error. (Note: Files used in this script are enclosed on this Jira.)
================================================================================================
a = load 'sports_views.txt' as (col1, col2, col3);
b = load 'queries.txt' as (colb1,colb2,colb3);
mycogroup = cogroup a by col1 inner, b by colb1;
mynewalias = foreach mycogroup generate flatten(a), flatten((COUNT(b) > 0L ? b.(colb2,colb3) : {('','')}));
dump mynewalias;
================================================================================================
java.io.IOException: Unable to open iterator for alias: mynewalias [Unable to store for alias: mynewalias [Can't overwrite cause]]
     at java.lang.Throwable.initCause(Throwable.java:320)
     at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.visit(TypeCheckingVisitor.java:1494)
     at org.apache.pig.impl.logicalLayer.LOBinCond.visit(LOBinCond.java:85)
     at org.apache.pig.impl.logicalLayer.LOBinCond.visit(LOBinCond.java:28)
     at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:68)
     at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
     at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.checkInnerPlan(TypeCheckingVisitor.java:2345)
     at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.visit(TypeCheckingVisitor.java:2252)
     at org.apache.pig.impl.logicalLayer.LOForEach.visit(LOForEach.java:121)
     at org.apache.pig.impl.logicalLayer.LOForEach.visit(LOForEach.java:40)
     at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:68)
     at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
     at org.apache.pig.impl.plan.PlanValidator.validateSkipCollectException(PlanValidator.java:101)
     at org.apache.pig.impl.logicalLayer.validators.TypeCheckingValidator.validate(TypeCheckingValidator.java:40)
     at org.apache.pig.impl.logicalLayer.validators.TypeCheckingValidator.validate(TypeCheckingValidator.java:30)
     at org.apache.pig.impl.logicalLayer.validators.LogicalPlanValidationExecutor.validate(LogicalPlanValidationExecutor.java:
79)
     at org.apache.pig.PigServer.compileLp(PigServer.java:684)
     at org.apache.pig.PigServer.compileLp(PigServer.java:655)
     at org.apache.pig.PigServer.store(PigServer.java:433)
     at org.apache.pig.PigServer.store(PigServer.java:421)
     at org.apache.pig.PigServer.openIterator(PigServer.java:384)
     at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:269)
     at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:178)
     at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:84)
     at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:64)
     at org.apache.pig.Main.main(Main.java:306)
Caused by: java.io.IOException: Unable to store for alias: mynewalias [Can't overwrite cause]
     ... 26 more
Caused by: java.lang.IllegalStateException: Can't overwrite cause
     ... 26 more
================================================================================================",,,,,,,,,,,,,,,,,,,05/Nov/08 02:48;viraj;queries.txt;https://issues.apache.org/jira/secure/attachment/12393347/queries.txt,05/Nov/08 02:48;viraj;sports_views.txt;https://issues.apache.org/jira/secure/attachment/12393346/sports_views.txt,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-11-05 03:10:55.371,,,no_permission,,,,,,,,,,,,164121,,,,,Thu Apr 15 00:15:38 UTC 2010,,,,,,,0|i0gl7b:,94865,,,,,,,,,,05/Nov/08 02:48;viraj;sports_views test file,05/Nov/08 02:48;viraj;queries test file,05/Nov/08 03:10;sms;This issue looks like a duplicate of PIG-449.,"15/Apr/10 00:15;viraj;The above script generates the following error in Pig 0.7

2010-04-14 17:10:49,807 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1048: Two inputs of BinCond must have compatible schemas. left hand side: b: bag({colb2: bytearray,colb3: bytearray}) right hand side: bag({(chararray,chararray)})


A type cast to the right type solves the problem.

{code}
a = load 'sports_views.txt' as (col1:chararray, col2:chararray, col3:chararray); 
b = load 'queries.txt' as (colb1:chararray,colb2:chararray,colb3:chararray); 
mycogroup = cogroup a by col1 inner, b by colb1; 
mynewalias = foreach mycogroup generate flatten(a), flatten((COUNT(b) > 0L ? b.(colb2,colb3) : {('','')}));
dump mynewalias; 
{code}

(alice,lakers,3,ipod,3)
(alice,warriors,7,ipod,3)
(peter,sun,7,sun,4)
(peter,nets,7,sun,4)

Closing bug as Pig yields the correct error message which the user can use to recode his script

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Custom Loader Function which takes in a constructor argument fails during typecast,PIG-517,12407807,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,viraj,viraj,04/Nov/08 19:26,24/Mar/10 22:04,14/Mar/19 03:05,10/Nov/08 20:28,0.2.0,,,,,,0.2.0,,,,,0,,,,"I have a custom loader function,  known as RegexLoader that parses a line of input into fields using regex and then sets the fields. This RegexLoader extends Utf8StorageConverter and implements the LoadFunc. It takes in a constructor argument a regex string supplied by the user.
The following piece of code, works when the loaded fields are not typecasted.
{code}
REGISTER pigudf2.0/java/build/loader.jar
fullfile = load 'phonenumber.txt'
               using loader.RegexLoader('4*8')
               as   (a,z,n) ;
-- project required fields
phonerecords = foreach fullfile {
          generate
           a                as area,
           z               as zone,
           n               as number;
        }
dump phonerecords;
{code}

But when the alias a is cast to int, the piece of script fails with the error java.io.IOException: Unable to open iterator for alias: phonerecords [Unable to store for alias: phonerecords [could not instantiate 'loader.RegexLoader' with arguments 'null']]
{code}
REGISTER pigudf2.0/java/build/loader.jar
fullfile = load 'phonenumber.txt'
             using loader.RegexLoader('4*8')
        as   (a,z,n) ;
-- project required fields
phonerecords = foreach fullfile {
          generate
           (int)a          as area,
           z               as zone,
           n               as number;
        }
dump phonerecords;
{code}
Full stack trace of the error:
==================================================================================================================
java.io.IOException: Unable to open iterator for alias: phonerecords [Unable to store for alias: phonerecords [could not instantiate 'loader.RegexLoader' with arguments 'null']]
     at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:448)
     at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:454)
     at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.instantiateFunc(POCast.java:67)
     at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.setLoadFSpec(POCast.java:73)
     at org.apache.pig.backend.hadoop.executionengine.physicalLayer.LogToPhyTranslationVisitor.visit(LogToPhyTranslationVisitor.java:1157)
     at org.apache.pig.impl.logicalLayer.LOCast.visit(LOCast.java:60)
     at org.apache.pig.impl.logicalLayer.LOCast.visit(LOCast.java:28)
     at org.apache.pig.impl.plan.DependencyOrderWalkerWOSeenChk.walk(DependencyOrderWalkerWOSeenChk.java:68)
     at org.apache.pig.backend.hadoop.executionengine.physicalLayer.LogToPhyTranslationVisitor.visit(LogToPhyTranslationVisitor.java:805)
     at org.apache.pig.impl.logicalLayer.LOForEach.visit(LOForEach.java:121)
     at org.apache.pig.impl.logicalLayer.LOForEach.visit(LOForEach.java:40)
     at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:68)
     at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
     at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:232)
     at org.apache.pig.PigServer.compilePp(PigServer.java:731)
     at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:644)
     at org.apache.pig.PigServer.store(PigServer.java:452)
     at org.apache.pig.PigServer.store(PigServer.java:421)
     at org.apache.pig.PigServer.openIterator(PigServer.java:384)
     at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:269)
     at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:178)
     at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:84)
     at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:64)
     at org.apache.pig.Main.main(Main.java:306)
Caused by: java.io.IOException: Unable to store for alias: phonerecords [could not instantiate 'loader.RegexLoader' with arguments 'null']
     ... 24 more
Caused by: java.lang.RuntimeException: could not instantiate 'loader.RegexLoader' with arguments 'null'
     ... 24 more
Caused by: java.lang.InstantiationException: loader.RegexLoader
     at java.lang.Class.newInstance0(Class.java:340)
     at java.lang.Class.newInstance(Class.java:308)
     at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:418)
     ... 23 more
==================================================================================================================
Attaching the custom RegexLoader with this Jira",,,,,,,,,,,,,,,,,,,04/Nov/08 23:54;pkamath;PIG-517.patch;https://issues.apache.org/jira/secure/attachment/12393341/PIG-517.patch,04/Nov/08 19:28;viraj;RegexLoader.java;https://issues.apache.org/jira/secure/attachment/12393323/RegexLoader.java,04/Nov/08 19:29;viraj;phonenumber.txt;https://issues.apache.org/jira/secure/attachment/12393324/phonenumber.txt,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2008-11-04 23:54:28.238,,,no_permission,,,,,,,,,,,,164120,,,,,Mon Nov 10 20:28:36 UTC 2008,,,,,,,0|i0gl6n:,94862,,,,,,,,,,04/Nov/08 19:28;viraj;Custom loader function: RegexLoader,04/Nov/08 19:29;viraj;Sample dataset,"04/Nov/08 23:54;pkamath;Attached patch - details on the issue:
In case of a ""cast"" from bytearray to any other type, TypeCheckingVisitor tries to match the cast to the corresponding load function for the source of the data so that the load function can do the cast correctly ( since there is no generic way to convert from bytearray to other types - only the load function which is the source of the bytearray knows how to interpret the bytes present in the bytearray). The TypeCheckingVisitor in the current code is setting a ""LoadFunc"" reference in LOCast for this purpose. The LoadFunc reference does not have information of the arguments needed to create the Loader object at runtime. This manifests as a problem in the LogToPhyTranslator when it tries to create a POCast from the LOCast. 

The patch now uses FuncSpec to represent the loader function rather than a LoadFunc reference. The FuncSpec has both the classname and the arguments to the constructor of the loader.","07/Nov/08 00:39;ckunz;+1 on the patch.
Works now for me.

It should be noted, that before the patch load functions with a default constructor, but invoked with non-default argument, did not throw an exception but got called with the default constructor in LogToPhyTranslationVisitor, potentially resulting in unexpected results","10/Nov/08 20:28;olgan;patch committed; thanks, pradeep",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
COUNT returns no results as a result of two filter statements in FOREACH,PIG-514,12407570,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,viraj,viraj,31/Oct/08 03:39,02/May/13 02:29,14/Mar/19 03:05,22/Apr/09 17:11,0.2.0,,,,,,0.3.0,,impl,,,6,,,,"For the following piece of sample code in FOREACH which counts the filtered student records based on record_type == 1 and scores and also on record_type == 0 does not seem to return any results.

{code}
mydata = LOAD 'mystudentfile.txt' AS  (record_type,name,age,scores,gpa);
--keep only what we need
mydata_filtered = FOREACH  mydata GENERATE   record_type,  name,  age,  scores ;
--group
mydata_grouped = GROUP mydata_filtered BY  (record_type,age);

myfinaldata = FOREACH mydata_grouped {
     myfilter1 = FILTER mydata_filtered BY record_type == 1 AND age == scores;
     myfilter2 = FILTER mydata_filtered BY record_type == 0;
     GENERATE FLATTEN(group),
-- Only this count causes the problem ??
      COUNT(myfilter1) as col2,
      SUM(myfilter2.scores) as col3,
      COUNT(myfilter2) as col4;  };

--these set of statements confirm that the count on the  filters returns 1
--mycountdata = FOREACH mydata_grouped
--{
--      myfilter1 = FILTER mydata_filtered BY record_type == 1 AND age == scores;
--      GENERATE
--      COUNT(myfilter1) as colcount;
--};
--dump mycountdata;

dump myfinaldata;
{code}

But if you uncomment the  {code} COUNT(myfilter1) as col2, {code}, it seems to work with the following results..
(0,22,45.0,2L)
(0,24,133.0,6L)
(0,25,22.0,1L)

Also I have tried to verify if this is a issue with the {code} COUNT(myfilter1) as col2, {code} returning zero. It does not seem to be the case.
If {code}  dump mycountdata; {code} is uncommented it returns:
(1L)
(1L)

I am attaching the tab separated 'mystudentfile.txt' file used in this Pig script. Is this an issue with 2 filters in the FOREACH followed by a COUNT on these filters??",,,,,,,,,,,,,,,,,,,21/Apr/09 21:40;pkamath;PIG-514.patch;https://issues.apache.org/jira/secure/attachment/12406062/PIG-514.patch,31/Oct/08 03:41;viraj;mystudentfile.txt;https://issues.apache.org/jira/secure/attachment/12393123/mystudentfile.txt,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-11-01 03:51:16.561,,,no_permission,,,,,,,,,,,,164118,Reviewed,,,,Wed Apr 22 17:11:36 UTC 2009,,,,,,,0|i0gl5b:,94856,,,,,,,,,,31/Oct/08 03:41;viraj;Tab separated student file for testing the issue..,"01/Nov/08 03:51;pkamath;The issue is that for each group in the input data, one of the filters always filters out all data and the POFilter returns an POStatus.STATUS_EOP. The POUserFunc sees this EOP and does not call the actual UDF (COUNT() or SUM()) and just sends the EOP to POForeach. The POForeach sees this EOP and just finishes processing that group without outputting any results.
Ideally for COUNT() and SUM() POUserFunc should send an empty bag as input so that COUNT() can be 0 and SUM can be null. However this issue is also present in the following code:
{code}
a = load 'bla';
b = filter a by 2 == 1; -- this is just an illustration of an aggressive filter which filters every tuple
c = foreach b generate myudf($0);
{code}

In the above case also myudf() is never called - is it ok to not call the udf when there is no input to give it (EOP case)? This causes queries like the one in the description to not give the correct COUNT of 0 and SUM of null in cases where the input to them is empty - we need to decide how we should handle this general case (both for aggregate functions like COUNTs and non aggregate functions like myudf())

One other case of the COUNT problem is:
{code}
a = load 'emptyfile'; -- load an empty file
-- neither of the statements below actually ever get executed
b = group a all;
c = foreach b generate COUNT(a);
{code}
When the input data is empty, neither map() nor reduce() gets executed and hence COUNT() never gets called.
","27/Mar/09 01:08;viraj;Another test case: consider the following input file:

1       1       3
1       2       3
2       1       3
2       1       3
 
The pig program is like this:
{code} 
test   = load 'test.txt' as (col1: int, col2: int, col3: int);
test2 = group test by col1;
test3 = foreach test2
{
        filter_one    = filter test by (col2==1);
        filter_notone = filter test by (col2!=1);
        generate group as col1, COUNT(filter_one) as cnt_one, COUNT(filter_notone) as cnt_notone;
};
{code}
 
The output consists of a single line:
(1,1L,1L)
 
But I would expect
(1,1L,1L)
(2,2L,0L)
","31/Mar/09 03:34;pkamath;A proposal for fixing this is the following:

Semantics:
Relational operators (like filter) inside a foreach  which produce no result tuples will provide an empty bag as output. If this empty bag is input to a udf , the udf will receive an empty bag as its input argument.

Implementation:
To achieve the above semantics a change will be needed in POProject (this change could be factored into a new subclass of POProject if that is cleaner). Currently when the leaf ExpressionOperator of an inner plan in POForeach has a relational operator as its input, a POProject is introduced in between which takes tuples from the relational operator into a bag and provides the bag as input to the leaf ExpressionOperator. 
For example a COUNT() with filter as input in a foreach would look like this in the MR plan:
{noformat}
New For Each(false)[bag] - pradeepk-Mon Mar 30 20:21:50 PDT 2009-53
    |   |
    |   POUserFunc(org.apache.pig.builtin.COUNT)[long] - pradeepk-Mon Mar 30 20:21:50 PDT 2009-52
    |   |
    |   |---Project[bag][*] - pradeepk-Mon Mar 30 20:21:50 PDT 2009-51
    |       |
    |       |---Filter[bag] - pradeepk-Mon Mar 30 20:21:50 PDT 2009-46
{noformat}


This POProject will need to maintain internal state to check if it receives no other input and only receives an EOP from its predecessor relational operator (say a POFilter). In such a case, it will need to send an empty bag as its output.

Expected results for aggregates in such a case (say where a filter in the foreach filters away all records ) will be:
COUNT - 0
SUM, MIN, MAX, AVG - null

Comments/Thoughts?","16/Apr/09 18:39;pkamath;I am currently working on implementing the above proposal since I have not seen any objections. After making the core changes to implement the above proposal, I validated that it fixed the issue reported here and also in PIG-739 and PIG-710. I need to add a few more changes to make the patch complete - will supply a patch once done.","21/Apr/09 21:40;pkamath;Attached patch which implements the proposed design. The changes are spread across the following areas:
- Parser - in QueryParser.jjt, the condition wherein a relational operator is followed by a Project(*) now marks the Project to be a special Project which would send empty bags to the predecessor on EOP
- In the LogToPhyTranslationVisitor, based on the presence/absence of the above flag either a PORelationToExprProject is created or a regular POProject is created.
- PORelationToExprProject is extended from POProject and only overrides the getNext(DataBag) method to send an empty bag on first encountering an EOP and sets state to send an EOP down the next time it is called. However if the POForEach in which this project is present, starts a new set of inputs, this flag is reset in the reset() method
- PhysicalOperator now has a reset() method for use in the PORelationToExprProject and in limit/sort/distinct operators when limit is present to reset state when new input for the foreach starts.
- The builtins (SUM/COUNT/MIN/MAX/AVG) now handle empty bags - COUNT gives 0 and the others give null as output (this change includes the type specific implementations of these aggs like IntSum, LongSum etc
- Test cases to test the empty bag case
","21/Apr/09 23:43;alangates;In the following code:

{code}
    /**
     * @param opsToBeReset the opsToBeReset to set
     */
    public void setOpsToBeReset(List<PhysicalOperator> opsToBeReset) {
        opsToBeReset = opsToBeReset;
    }
{code}

I think you want the last line to be this.opstoBeReset = opsToBeReset;

Other than that, +1.

",22/Apr/09 17:11;pkamath;Patch committed with the change in previous comment. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PERFORMANCE: optimize some of the code in DefaultTuple,PIG-513,12407565,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,31/Oct/08 00:31,24/Mar/10 22:15,14/Mar/19 03:05,23/Sep/09 17:51,0.2.0,,,,,,0.6.0,,,,,0,,,,"The following areas in DefaultTuple.java can be changed:

The member methods get(), set(), getType() and isNull() all call checkBounds() which is redundant call since all these 4 functions throw ExecException. Instead of doing a bounds check, we can catch the IndexOutOfBounds exception in a try-catch and throw it as an ExecException

The write() method has the following unused object (d in the code below):
{code}
for (int i = 0; i < sz; i++) {
                try {
                    Object d = get(i);
                } catch (ExecException ee) {
                    throw new RuntimeException(ee);
                }
                DataReaderWriter.writeDatum(out, mFields.get(i));
            }
{code}
{noformat}
The get(i) call in the try should be replaced by the writeDatum call directly since d is never used and there is an unncessary call to get()
{noformat}",,,,,,,,,,,,,,,,,,,22/Sep/09 20:42;pkamath;PIG-513-3.patch;https://issues.apache.org/jira/secure/attachment/12420307/PIG-513-3.patch,07/Nov/08 01:02;pkamath;PIG-513.patch;https://issues.apache.org/jira/secure/attachment/12393476/PIG-513.patch,29/Jul/09 07:07;ashutoshc;pig-513_2.patch;https://issues.apache.org/jira/secure/attachment/12414853/pig-513_2.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2008-11-10 20:35:56.085,,,no_permission,,,,,,,,,,,,164117,Reviewed,,,,Wed Sep 23 17:51:05 UTC 2009,,,,,,,0|i0gl4v:,94854,,,,,,,,,,07/Nov/08 01:01;pkamath;Attached patch with changes as per issue description,10/Nov/08 20:35;olgan;Did we run any performance tests?,"29/Jul/09 07:07;ashutoshc;I encountered the same issue of  wasted work in checkBounds() while profiling the Merge Join. Since java in any case performs bound checks before accessing elements in ArrayList, this method call results in duplication of work. In this particular case, 6% of total time of query is spent in this method call. Attaching the patch generated against current trunk.","30/Jul/09 07:33;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12414853/pig-513_2.patch
  against trunk revision 799141.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/146/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/146/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-minerva.apache.org/146/console

This message is automatically generated.",19/Sep/09 00:05;alangates;Patch checked in.  Thanks Ashutosh.,"22/Sep/09 20:40;pkamath;Pig is supposed to provide nulls for columns not present in the data. For example if a file has 3 columns name, age gpa then the following statement should still work with the column 'extra' getting nulls:
{noformat}
a = load 'input' as (name, age, gpa, extra);
{noformat}

This is broken with the current code.",22/Sep/09 20:42;pkamath;Attached patch to fix the issue. The fix involves changing POProject to catch IndexOutOfBoundsException and set up nulls for non existent fields. Similarly POUserFunc has also been changed to catch IndexOutOfBoundsException so that a more meaningful message can be provided to the user. Unit test has been added to the patch.,"23/Sep/09 00:27;hadoopqa;+1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12420307/PIG-513-3.patch
  against trunk revision 817739.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/44/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/44/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h7.grid.sp2.yahoo.net/44/console

This message is automatically generated.",23/Sep/09 17:20;daijy;+1,23/Sep/09 17:51;pkamath;Patch committed to trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expressions in foreach lead to errors,PIG-512,12407559,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,sms,sms,30/Oct/08 22:41,24/Mar/10 22:04,14/Mar/19 03:05,14/Nov/08 22:56,0.2.0,,,,,,0.2.0,,,,,0,,,,"Use of expressions that use the same sub-expressions in foreach lead to translation errors. This issue is caused due to sharing operators across nested plans. To remedy this issue, logical operators should be cloned and not shared across plans.

{code}
grunt> a = load 'a' as (x, y, z);
grunt> b = foreach a {
>> exp1 = x + y;
>> exp2 = exp1 + x;
>> generate exp1, exp2;
>> }
grunt> explain b;
2008-10-30 15:38:40,257 [main] WARN  org.apache.pig.PigServer - bytearray is implicitly casted to double under LOAdd Operator
2008-10-30 15:38:40,258 [main] WARN  org.apache.pig.PigServer - bytearray is implicitly casted to double under LOAdd Operator
2008-10-30 15:38:40,258 [main] WARN  org.apache.pig.PigServer - bytearray is implicitly casted to double under LOAdd Operator
Logical Plan:
Store sms-Thu Oct 30 11:27:27 PDT 2008-2609 Schema: {double,double} Type: Unknown
|
|---ForEach sms-Thu Oct 30 11:27:27 PDT 2008-2605 Schema: {double,double} Type: bag
    |   |
    |   Add sms-Thu Oct 30 11:27:27 PDT 2008-2600 FieldSchema: double Type: double
    |   |
    |   |---Cast sms-Thu Oct 30 11:27:27 PDT 2008-2606 FieldSchema: double Type: double
    |   |   |
    |   |   |---Project sms-Thu Oct 30 11:27:27 PDT 2008-2598 Projections: [0] Overloaded: false FieldSchema: x: bytearray Type: bytearray
    |   |       Input: Load sms-Thu Oct 30 11:27:27 PDT 2008-2597
    |   |
    |   |---Cast sms-Thu Oct 30 11:27:27 PDT 2008-2607 FieldSchema: double Type: double
    |       |
    |       |---Project sms-Thu Oct 30 11:27:27 PDT 2008-2599 Projections: [1] Overloaded: false FieldSchema: y: bytearray Type: bytearray
    |           Input: Load sms-Thu Oct 30 11:27:27 PDT 2008-2597
    |   |
    |   Add sms-Thu Oct 30 11:27:27 PDT 2008-2603 FieldSchema: double Type: double
    |   |
    |   |---Project sms-Thu Oct 30 11:27:27 PDT 2008-2601 Projections:  [*]  Overloaded: false FieldSchema: double Type: double
    |   |   Input: Add sms-Thu Oct 30 11:27:27 PDT 2008-2600|
    |   |   |---Add sms-Thu Oct 30 11:27:27 PDT 2008-2600 FieldSchema: double Type: double
    |   |       |
    |   |       |---Project sms-Thu Oct 30 11:27:27 PDT 2008-2598 Projections: [0] Overloaded: false FieldSchema: x: bytearray Type: bytearray
    |   |       |   Input: Load sms-Thu Oct 30 11:27:27 PDT 2008-2597
    |   |       |
    |   |       |---Project sms-Thu Oct 30 11:27:27 PDT 2008-2599 Projections: [1] Overloaded: false FieldSchema: y: bytearray Type: bytearray
    |   |           Input: Load sms-Thu Oct 30 11:27:27 PDT 2008-2597
    |   |
    |   |---Cast sms-Thu Oct 30 11:27:27 PDT 2008-2608 FieldSchema: double Type: double
    |       |
    |       |---Project sms-Thu Oct 30 11:27:27 PDT 2008-2602 Projections: [0] Overloaded: false FieldSchema: x: bytearray Type: bytearray
    |           Input: Load sms-Thu Oct 30 11:27:27 PDT 2008-2597
    |
    |---Load sms-Thu Oct 30 11:27:27 PDT 2008-2597 Schema: {x: bytearray,y: bytearray,z: bytearray} Type: bag

2008-10-30 15:38:40,272 [main] ERROR org.apache.pig.impl.plan.OperatorPlan - Attempt to give operator of type org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject multiple outputs.  This operator does not support multiple outputs.
2008-10-30 15:38:40,272 [main] ERROR org.apache.pig.backend.hadoop.executionengine.physicalLayer.LogToPhyTranslationVisitor - Invalid physical operators in the physical planAttempt to give operator of type org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject multiple outputs.  This operator does not support multiple outputs.
2008-10-30 15:38:40,273 [main] ERROR org.apache.pig.tools.grunt.GruntParser - java.io.IOException: Unable to explain alias b [org.apache.pig.impl.plan.VisitorException]
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:235)
        at org.apache.pig.PigServer.compilePp(PigServer.java:731)
        at org.apache.pig.PigServer.explain(PigServer.java:495)
        at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:155)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:193)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:94)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:58)
        at org.apache.pig.Main.main(Main.java:282)
Caused by: org.apache.pig.backend.executionengine.ExecException: org.apache.pig.impl.plan.VisitorException
        ... 8 more
Caused by: org.apache.pig.impl.plan.VisitorException
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.LogToPhyTranslationVisitor.visit(LogToPhyTranslationVisitor.java:324)
        at org.apache.pig.impl.logicalLayer.LOAdd.visit(LOAdd.java:69)
        at org.apache.pig.impl.logicalLayer.LOAdd.visit(LOAdd.java:29)
        at org.apache.pig.impl.plan.DependencyOrderWalkerWOSeenChk.walk(DependencyOrderWalkerWOSeenChk.java:68)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.LogToPhyTranslationVisitor.visit(LogToPhyTranslationVisitor.java:805)
        at org.apache.pig.impl.logicalLayer.LOForEach.visit(LOForEach.java:121)
        at org.apache.pig.impl.logicalLayer.LOForEach.visit(LOForEach.java:40)
        at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:68)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:232)
        ... 7 more
Caused by: org.apache.pig.impl.plan.PlanException: Attempt to give operator of type org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject multiple outputs.  This operator does not support multiple outputs.
        at org.apache.pig.impl.plan.OperatorPlan.connect(OperatorPlan.java:158)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan.connect(PhysicalPlan.java:89)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.LogToPhyTranslationVisitor.visit(LogToPhyTranslationVisitor.java:320)
        ... 16 more

2008-10-30 15:38:40,274 [main] ERROR org.apache.pig.tools.grunt.GruntParser - Unable to explain alias b [org.apache.pig.impl.plan.VisitorException]
2008-10-30 15:38:40,274 [main] ERROR org.apache.pig.tools.grunt.GruntParser - java.io.IOException: Unable to explain alias b [org.apache.pig.impl.plan.VisitorException]

{code}",,,,,,,,,,,,,,,,,,,13/Nov/08 19:45;sms;PIG-512.patch;https://issues.apache.org/jira/secure/attachment/12393885/PIG-512.patch,13/Nov/08 22:06;sms;PIG-512_1.patch;https://issues.apache.org/jira/secure/attachment/12393897/PIG-512_1.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-11-13 22:12:11.412,,,no_permission,,,,,,,,,,,,38063,,,,,Fri Nov 14 22:56:54 UTC 2008,,,Patch Available,,,,0|i0gl4f:,94852,,,,,,,,,,"13/Nov/08 19:45;sms;Atached patch (PIG-512.patch) includes the following:

1. Logical Plan cloning which in turn includes logical operator cloning. Caveat: Only logical plan cloning is allowed and LogicalPlanCloner is the supported mechanism for cloning logical plans. The following operators do not support cloning:
   i. LOLoad
   ii. LOStore
   iii. LOStream

2. A visitor to remove redundant project( * ) operators that occur between two relational operators or between two expression operators.

3. Unit tests for 1 and 2

All unit tests pass.",13/Nov/08 22:06;sms;Updated patch with SVN changes since last patch. All unit tests pass.,"13/Nov/08 22:12;olgan;Alan, could you, please, review it.","14/Nov/08 19:00;alangates;In LogicalPlanCloneHelper, why do you need this:

{code}
    protected void visit(LOCross cs) throws VisitorException {
        super.visit(cs);
    }
{code}

Won't java do that for you?

What is the significance of the changes in TypeCheckingVisitor?

Neither of these issues are big enough to require a new patch.  The current one looks good (and big :) ).","14/Nov/08 21:41;sms;Yes, the visit(LOCross cs)  can be removed from LogicalPlanCloneHelper.java. Its a placeholder if we change LOCross 
to have additional member variables. For now, its redundant.

The change in the type checker is not related to the cloning. Its a bug that I uncovered while I was testing unary expressions
as part of cloning. The insertCastForUniOp method in the typeChecker had a bug where the newly created cast operator was 
not added  to the plan before inserting the cast between the unary expression and the unary expression's input. I fixed it by adding
the cast operator to the plan and patching the reference in the unary expression to point to the cast.

I would like to thank Pradeep Kamath who had done the ground work in an earlier attempt at cloning logical plans.","14/Nov/08 22:56;olgan;patch committed; thanks, santhosh.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DIFF does not work in types branch,PIG-511,12407521,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,civascu,civascu,30/Oct/08 13:42,24/Mar/10 22:04,14/Mar/19 03:05,30/Oct/08 18:34,0.2.0,,,,,,0.2.0,,data,,,0,,,,"using DIFF(bag1, bag2) always returns an empty bag

Reason: in the compute_diff, the input bags are discarded, and the actual operations are done against two newly created, empty bags

fix: make sure the compute_diff(bag1, bag2, output) does its work on bag 1 and bag2, instead of d1 and d2.

Currently:
       DataBag d1 = mBagFactory.newDistinctBag();
        DataBag d2 = mBagFactory.newDistinctBag();
        Iterator<Tuple> i1 = d1.iterator();
        Iterator<Tuple> i2 = d2.iterator();
        while (i1.hasNext()) d1.add(i1.next());
        while (i2.hasNext()) d2.add(i2.next());

Should be:
       DataBag d1 = mBagFactory.newDistinctBag();
        DataBag d2 = mBagFactory.newDistinctBag();
        Iterator<Tuple> i1 = bag1.iterator();
        Iterator<Tuple> i2 = bag2.iterator();
        while (i1.hasNext()) d1.add(i1.next());
        while (i2.hasNext()) d2.add(i2.next());","CentOS 5, hadoop 0.18.0, pig built from types branch",,,,,,,,,,,,,,,,,,30/Oct/08 18:02;alangates;PIG-511.patch;https://issues.apache.org/jira/secure/attachment/12393083/PIG-511.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-10-30 18:02:34.001,,,no_permission,,,,,,,,,,,,164116,,,,,Thu Oct 30 18:34:08 UTC 2008,,,,,,,0|i0gl3z:,94850,,,,,,,,,,30/Oct/08 18:02;alangates;Made the change suggested.  I also added some unit tests that revealed that the algorithm for computing the diff between the bags was flawed.  This patch uses two hash tables instead of trying to sort the two and walk them in unison.,30/Oct/08 18:11;olgan;+1; patch looks good,30/Oct/08 18:34;alangates;Checked in patch.  Thanks Crisitan for finding the issue and pointing it out.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"After revision 707158, pig no longer compiles under java 1.5",PIG-510,12407301,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,27/Oct/08 19:24,24/Mar/10 22:04,14/Mar/19 03:05,30/Oct/08 17:09,0.2.0,,,,,,0.2.0,,,,,0,,,,In this revision Arrays.copyOf is used in org.apache.pig.builtin.PigStorage.  This function was added in java 1.6.  Pig still uses 1.5 as the official version of java.,,,,,,,,,,,,,,,,,,,27/Oct/08 19:49;alangates;PIG-510.patch;https://issues.apache.org/jira/secure/attachment/12392887/PIG-510.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-10-28 06:13:39.273,,,no_permission,,,,,,,,,,,,164115,,,,,Thu Oct 30 17:09:33 UTC 2008,,,,,,,0|i0gl3j:,94848,,,,,,,,,,"27/Oct/08 19:49;alangates;The above patch removes the use of Arrays.copyOf().  The code is in a windows specific section.  Daniel, could you run the unit tests with this patch make sure it passes?  Thanks.","28/Oct/08 06:13;daijy;This patch is good. There are several newly introduced unit test errors under cygwin now, however, they are not related to this patch and I will address in the followup of [PIG-501|https://issues.apache.org/jira/browse/PIG-501]. Thanks Alan!",30/Oct/08 17:09;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Query with a cogroup have one of its inputs coming from a group fails,PIG-508,12407125,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,23/Oct/08 22:25,24/Mar/10 22:04,14/Mar/19 03:05,24/Oct/08 22:55,0.2.0,,,,,,0.2.0,,,,,0,,,,"Script which fails:
{code}
a = load '/user/pig/tests/data/singlefile/studenttab10k';
b = group a by $0;
c = load '/user/pig/tests/data/singlefile/studenttab10k';
d = cogroup b by $0, c by $0;
e = foreach d generate group, c.$1, SUM(c.$1), COUNT(c);
dump e;
{code}

Error message produced:
{noformat}
08/10/23 15:23:54 ERROR mapReduceLayer.MapReduceLauncher: Job failed! 
08/10/23 15:23:54 ERROR mapReduceLayer.Launcher: Error message from task (reduce) task_200810231521_0007_r_000000java.lang.NullPointerException
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage.getNext(POPackage.java:218)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:208)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:134)
    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:318)
    at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2207)
{noformat}

",,,,,,,,,,,,,,,,,,,24/Oct/08 19:23;pkamath;PIG-508.patch;https://issues.apache.org/jira/secure/attachment/12392800/PIG-508.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-10-24 22:55:43.584,,,no_permission,,,,,,,,,,,,164113,,,,,Fri Oct 24 22:55:43 UTC 2008,,,,,,,0|i0gl2n:,94844,,,,,,,,,,"24/Oct/08 19:23;pkamath;Attached patch - details on the fix:
POPackageAnnotator is a visitor which looks for ""POPackage"" and annotates it with ""keyInfo"" from each of the LocalRearranges which provide input to the POPackage. The keyinfo essentially has information about what part of the ""value"" for a given input is present in the ""key"" and hence ommitted from the ""value"". The visitor was incorrectly assuming that if a local rearrange corresponding to the package is found in the given MROper's map plan, then the annotation is done. This breaks in the case of the script in this issue - the POPackage has one of its Local rearranges in the map plan of the same MROper as the POPackage and the other local rearrange in the reduce plan of the predecessor MROper. Hence the visitor was changed to ensure that POPackage is annotated with information from *all* Local rearranges.",24/Oct/08 22:55;olgan;patch committed; thanks Pradeep,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig does not error out while trying to use a input directory to which the user does not have access permissions,PIG-507,12407049,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,23/Oct/08 00:57,24/Mar/10 22:04,14/Mar/19 03:05,23/Oct/08 18:46,0.2.0,,,,,,0.2.0,,,,,0,,,,"Session illustrating the issue.

{noformat}

bash-3.00$ hadoop fs -ls /data/unaccesible_dir 
ls: org.apache.hadoop.fs.permission.AccessControlException: Permission denied: user=<username>, access=READ_EXECUTE, inode=""<inode>""<permissions>-
bash-3.00$ pig -latest 
2008-10-16 23:31:25,134 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to HOD...
...
2008-10-16 23:34:45,810 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: local
grunt> a = load '/data/unaccesible_dir';      
grunt> dump a;
2008-10-16 23:39:05,624 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete
2008-10-16 23:39:05,624 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!
grunt>

{noformat}",,,,,,,,,,,,,,,,,,,23/Oct/08 01:04;pkamath;PIG-507.patch;https://issues.apache.org/jira/secure/attachment/12392690/PIG-507.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-10-23 18:46:47.343,,,no_permission,,,,,,,,,,,,164112,,,,,Thu Oct 23 18:46:47 UTC 2008,,,,,,,0|i0gl27:,94842,,,,,,,,,,"23/Oct/08 01:04;pkamath;Patch to fix the issue:
- the earlier code was not catching the exception which hadoop throws when it tries to access a unauthorized directory. The fix catches this exception and bubbles it up as an IOException

All unit tests passed with the patch","23/Oct/08 18:46;olgan;patch committed; thanks, pradeep",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lineage for UDFs that do not return bytearray,PIG-505,12406958,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,sms,sms,21/Oct/08 22:57,24/Mar/10 22:04,14/Mar/19 03:05,31/Oct/08 20:25,0.2.0,,,,,,0.2.0,,,,,1,,,,"In Pig-335, the lineage design states that UDFs that return bytearrays could cause problems in tracing the lineage. For UDFs that do not return bytearray, the lineage design should pickup the right load function to use as long as there is no ambiguity.  In the current implementation, we could have issues with scripts like:

{code}
a = load 'input' as (field1);
b = foreach a generate myudf_to_double(field1);
c =  foreach b generate $0 + 2.0;
{code}

When $0 has to be cast to a double, the lineage code will complain that it hit a UDF and hence cannot determine the right load function to use.",,,,,,,,,,,,,,,,,,,30/Oct/08 22:36;sms;PIG-505.patch;https://issues.apache.org/jira/secure/attachment/12393112/PIG-505.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-10-23 17:24:10.57,,,no_permission,,,,,,,,,,,,164110,,,,,Fri Oct 31 20:25:28 UTC 2008,,,Patch Available,,,,0|i0gl1b:,94838,,,,,,,,,,"22/Oct/08 18:58;sms;In the existing design, Pig treats unknown types as bytearrays. As a result, if UDFs return complex types (map, bag, tuple) and if the outputSchema method does not specify the schema of the complex type,  Pig will treat the contents of the complex type as bytearray. The exception to this rule is map. The contents of map are always treated as bytearray. In the future, there are plans for treating unknown types as unknown types.

Impact of UDFs creating bytearrays
------------------------------------------------

If a UDF creates bytearrays, and if the bytearray is used in comparisons or in arithmetic operations or cast explicitly to a Pig type, the bytearray has to be cast to the appropriate type. The knowledge of converting the bytearray to the appropriate Pig type is best known to the UDF. In all other cases, we are making best effort guesses at picking the appropriate (load) function to convert bytearrays to Pig types.

While the above paragraph captures the problem of UDFs creating bytearrays and their subsequent use, the view of treating unkown types as bytearrays leads to another problem. The UDF could be returning the correct Pig type and not a bytearray. In such cases, the treatment of the correct type as bytearray leads to an execution time situation. Pig expects DataByteArray but gets the correct type (i.e., Integer, Double, ...) leading to a ClassCastException. This problem has been addressed by capturing the exception, examining the type of the result and returning the right type in POCast. In the situation where a bytearray is returned by the UDF, Pig will never be able to convert the bytearray to the appropriate type resulting in a valid run time exception.

With this background, the impact of treating unknown types as unknown types and the impact of treating unknown types as bytearrays is listed below:

Impact of using unkown type as unknown type:
====================================

Pros:
-------
1. UDFs that return bytearrays are affected at runtime, iff they use bytearrays in contexts where a cast is required, i.e., arithmetic operations, comparisons and explicit casts.

2. Lineage code is not impacted

3. Aligns with the strategy of handling Unknown types in the future

Cons:
---------
1. Finding the right match for UDFs does not handle unknown types

2. Impact of the change is huge. The getNext(Unkown) required of every operator as the output of a UDF could be used anywhere an expression is allowed in Pig

3. Cost of figuring out the right type at runtime is the cost of instanceof in Java

Impact of using unknown type as bytearray:
=================================

Pros:
-------

1. UDFs that return bytearrays are affected at runtime, iff they use bytearrays in contexts where a cast is required, i.e., arithmetic operations, comparisons and explicit casts.

2. Unknowns are treated as bytearrays which is consistent with what the treatment of unknown types in the current design

3. Treatment of unknowns as bytearrays in POCast is already in place

Cons:
--------

1. Lineage code is impacted. Lineage for UDFs will have to trace the correct load function. If there is a single load function then the choice is obvious. If there are more than one load functions to pick then randomly pick a load function. Expect the failure to occur at run time if a bytearray is returned by the UDF. This is a hack as the UDF could create bytearrays that are not recognized by the load functions.

2. Adding to the current view of treating unknown types as bytearrays.

3. Cost of figuring out the right type is the cost of handling an exception in Java

We need to pick an approach based on the short term versus long term view nature of the solution. Comments/questions/thoughts are welcome.","23/Oct/08 17:24;alangates;A couple of comments:

You say that the long term plan is to have a true unknown type, and then pose the problem as do we want to start the switch now or later.  (In fairness, I'm pretty sure you're quoting something I said here, so I'm about to question my own statement.)  I don't know if that's true or not.  In the original design for types we had an unknown type.  We ended up dropping it in implementation because it turned out to be so similar to the bytearray.  While there is some cost to combining byte arrays with unknown types (as you lay out) I'm not sure that that means we should should separate the two.  The long term cost of maintainability may be greater.

I'm a confused by the first con of continuing to use byte arrays as unknowns.  Are you saying that if we do this, in the case where there is only one load function in the script, after a UDF returns what is really a byte array, we'll use the cast from that load function?  I'm not certain what the right course is here.  From a correctness viewpoint, we can argue that pig doesn't know whether that byte array is from the load function or from the UDF.  However, this is a little burdensome to the user because it means any byte arrays inside complex types have to be dealt with before going to a UDF.  The con of using the load function where possible is if the byte array really is from the UDF and not the load function, we may error out or worse silently produce wrong data.  Since silently producing wrong data is a mortal sin in data processing I'd come down on the side of not using the load function's cast here.

A question, if we allow unknown in this one case, do we truly have to change code everywhere?  Instead of adding a getNext(unknown) to all operators, could we instead add a CastFromUnknown operator?  The entry point would still be getNext(ByteArray), so from all outside code's viewpoint the current type system should remain untouched.  And this operator would be written to introspect the type of object it got and either pass it on as is if it's the right type or cast it to the right type if it can.  It would never use a load function's cast (assuming we choose as indicated above), and it wouldn't incur the cost of throwing and catching an exception on the cast, it could use instanceof instead (which should be much faster).","23/Oct/08 18:35;sms;Responses with paragraph numbers:

Paragraph 2: 

The current lineage code barfs if the load function is null for converting bytearrays to Pig type. As a result, we have to pick a load function to use resulting in run time errors or erroneous results. Based on your comment, it seems appropriate to relax the rule that load functions cannot null for bytearray to Pig type conversions and then throw an appropriate error message at run time (assuming no bugs in the lineage code)

Paragraph 3:

The inputs to cast expression can serve as inputs to any operators that expects expressions. As a result, setting the return type of expression operator to unknown will have across the board impact. In order to mitigate this impact, we could introduce a new visitor that changes the type of all expressions that are not inputs to cast to bytearray, However, this introduces a problem. When do we use this visitor? Before the type checker or after the type checker? If we use the visitor before the type checker the we will lose unknown types for casts introduced by the type checker. If we use the visitor after the type checker, the type checker will barf if unknown types occur in the graph. As a result, we will have to either migrate some of the functionality of the type checker into the visitor. This approach is complicated and not worth the benefit.

Based on the discussions and given the cost implications of code complexity, maintenance and performance, the solution is probably the following:

1. Relax the rule of load function not being null in the lineage code.
2. If a null pointer exception occurs in the back end (POCast, specifically) then we assume that it was due to a bytearray created by a UDF and report an appropriate error message.

The only constraint to this solution is the assumption that the lineage code is not buggy. If the lineage code is buggy and we end up with a null load function for the right bytearray to Pig type conversion, it will require investigation.","23/Oct/08 19:30;ciemo;Are we overthinking this problem at this time?

At this time, the only source of ""undefined"" values in user defined functions that I know of are those that return maps.  (I could be wrong).

Why don't we just make the following simplifying assumptions (or conventions) for right now?

1) UDFs that return maps must return the individual values as bytearray type.  Period.
2) When casting using the lineage code, the code assumes that these are bytearray for conversion purposes.
3) Tell me how to code my UDFs to follow these guidelines and conventions.

The other option is to introduce some cast convention that allows me to define whether the map will adhere to a bytearray convention or a chararray convention to reduce the chance of redundant conversions.

For example -- (map<bytearray>) or (map<chararray>).  Or maybe this is handled intrinsically in the function definition.","23/Oct/08 20:07;sms;Undefined values could occur in other complex types - tuples and bags. If the outputSchema method in the UDF is not implemented and the tuple or bag is flattened, the Pig has to deal with unknowns.

1. Pig will prefer that you return the correct type instead of bytearray unless your type is a bytearray.

2. Pig is making that assumption in the current implementation and we are continuing to adhere to that assumption

3. Unless you need bytearrays, return the appropriate type in your Map, i.e, Pig expects a Map<Object, Object>, for the values, please use Integer, Long, Float, Double, String, DataByteArray, Map, Tupe and Bag depending on your use case. Use DataByteArray only if you will use it as a bytearray in Pig

In the future, we might let users specify the value type and even the key type and value types of the map like strongly typed languages.","23/Oct/08 20:35;ciemo;The problem is this - I think we've not done a good a job of communicating this assumption of full typing for Maps.  In fact, it was the Pig 2.0 development team that ported this user defined function from Pig 1.4.

The function is string.URLPARSE and It uses a generic Map<Object,Object> return type rather than, say, Map<String,String> which would seem more appropriate.  Also, this function DOES have a a defined outputSchema of DataType.MAP.

Maybe one of the solutions to working around this problem is making another pass through the example user defined functions and making sure they are FULLY 2.0 compliant for types.

Here's the example code for string.URLPARSE.  Please advise on what you think the correct defintion for the return type and the outputSchema should be.

Thanks.

package string;

import java.io.IOException;
import java.util.Map;
import java.util.HashMap;

import java.util.regex.Pattern;
import java.util.regex.Matcher;

import org.apache.pig.EvalFunc;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;

/**
 * string.URLPARSE parses a URL into the parts as described in RFC 1738 and RFC 1808.
 *
 * According to the RFCs, a URL may be described as:
 *
 * scheme://authority/path;params?query#fragment
 * scheme://userinfo@host:port/path;params?query#fragment
 * 
 * ULRPARSE also supports parsing non-conformant ULRs that have scheme:// stripped.
 * 
 * <dl>
 * <dt><b>Parameters:</b></dt>
 * <dd><code>url</code> - <code>String</code> url string to parse.</dd>
 * 
 * <dt><b>Return Value:</b></dt>
 * <dd><code>Map</code> parsed url</dd>
 * 
 * <dt><b>Return Schema:</b></dt>
 * <dd>urlparse</dd>
 * <dd>#'scheme' String</dd>
 * <dd>#'authority' String</dd>
 * <dd>#'userinfo' String</dd>
 * <dd>#'host' String</dd>
 * <dd>#'port' String</dd>
 * <dd>#'path' String</dd>
 * <dd>#'params' String</dd>
 * <dd>#'paramsFields' Map</dd>
 * <dd>#'query' String</dd>
 * <dd>#'queryFields' Map</dd>
 * <dd>#'fragment' String</dd>
 * 
 * <p>Note that none of the returned values are decoded so the values may contain
 * escaped characters such as %2F and +</p>
 * 
 * <dt><b>Example:</b></dt>
 * <dd><code>
 * register string.jar;<br/>
 * A = load 'mydata' using PigStorage() as ( url );<br/>
 * B = foreach A generate url, string.URLPARSE(url) as parsedurl;
 * C = foreach B generate url, parsedurl#'host' as host;
 * </code></dd>
 * </dl>
 * 
 * @author David (Ciemo) Ciemiewicz
 */
public class URLPARSE extends EvalFunc<Map<Object, Object> > {
        // @Override
        static private String strOrNullStr(String str) {
                return (str == null) ? """" : str;
        };

        static private Pattern scheme_rest = Pattern.compile(""^(([A-Za-z][A-Za-z0-9.+-]*):)?(.*)$"");
        static private Pattern authority_rest = Pattern.compile(""^((//)?((([^@:/]+)@)?([^:/]+)(:([0-9]+))?))(.*)$"");
        static private Pattern path_params_query_fragment = Pattern.compile(""^(/?[^;?#]*)?(;([^?#]*))?(\\?([^#]*))?(#(.*))?$"");
        static private Pattern field_value_pairs = Pattern.compile(""(([^=&]+)=([^=&]*))&*"");

        public Map<Object, Object> exec(Tuple input) throws IOException {
                if (input == null || input.size() == 0)
                        return null;

                String url;

                try{
                        url = (String)input.get(0);
                } catch(Exception e){
                        System.out.println(""Can't convert field to a string; error = "" + e.getMessage());
                        return null;
                }

                //Pattern p;
                HashMap<Object, Object> output = new HashMap<Object, Object>();
                Matcher m;
                String scheme = """";
                String authority = """";
                String userinfo = """";
                String host = """";
                String port = """";
                String path = """";
                String params = """";
                String query = """";
                String fragment = """";
                String rest = """";

                if(url == null) {
                        return null;
                }
                m = scheme_rest.matcher(url);
                if (m.find()) {
                        scheme = strOrNullStr(m.group(2));
                        rest = strOrNullStr(m.group(3));

                        m = authority_rest.matcher(rest);
                        if (m.find()) {
                                //String doubleslash = strOrNullStr(m.group(2));
                                authority = strOrNullStr(m.group(3));
                                userinfo = strOrNullStr(m.group(5));
                                host = strOrNullStr(m.group(6));
                                port = strOrNullStr(m.group(8));

                                rest = strOrNullStr(m.group(9));

                                m = path_params_query_fragment.matcher(rest);
                                if (m.find()) {
                                        path = strOrNullStr(m.group(1));
                                        params = strOrNullStr(m.group(3));
                                        query = strOrNullStr(m.group(5));
                                        fragment = strOrNullStr(m.group(7));
                                }
                        }
                }

                HashMap<Object, Object> paramsFieldsMap = new HashMap<Object, Object>();
                m = field_value_pairs.matcher(params);
                while (m.find()) {
                        String field = strOrNullStr(m.group(2));
                        String value = strOrNullStr(m.group(3));

                        paramsFieldsMap.put(field, value);
                }

                HashMap<Object, Object> queryFieldsMap = new HashMap<Object, Object>();
                m = field_value_pairs.matcher(query);
                while (m.find()) {
                        String field = strOrNullStr(m.group(2));
                        String value = strOrNullStr(m.group(3));

                        queryFieldsMap.put(field, value);
                }

                output.put(""url"",       url);
                output.put(""scheme"",    scheme);
                output.put(""authority"", authority);
                output.put(""userinfo"",  userinfo);
                output.put(""host"",      host);
                output.put(""port"",      port);
                output.put(""path"",      path);
                output.put(""params"",    params);
                output.put(""paramsFields"", paramsFieldsMap);
                output.put(""query"", query);
                output.put(""queryFields"", queryFieldsMap);
                output.put(""fragment"", fragment);

                return output;
        }

        @Override
        public Schema outputSchema(Schema input) {
                return new Schema(new Schema.FieldSchema(getSchemaName(""urlparse"", input), DataType.MAP));
        }
}
","23/Oct/08 22:36;pi_song;Unknown is unknown. It cannot be bytearray.

In case,
Table1 as (x:Unknown, y:Unknown)

we can't even tell the result of (x == y) because we don't know their types and unknown from x can be a different type (not the same as y's unknown)

Haven't looked at the code yet so haven't taken implementation into thinking.","23/Oct/08 22:41;ciemo;Just tried naively changing Map<Object,Object> to Map<String,String> for string.URLPARSE and realized that this won't work.

The thing is that some of the mapped values are strings and some are maps such as paramsFields and queryFields.

When you look at the code, it will make sense.

So the type must be ""unknown"" and not ""bytearray"" because it could be String or Map.

So, it would seem that there is need for ""unknown"" but, in this case, the cast operation is not a bytearray to whatever conversion but is rather Object (unknown) to Type (Map) with an opaque, no conversion cast of the data.

urlparse
#'scheme' String
#'authority' String
#'userinfo' String
#'host' String
#'port' String
#'path' String
#'params' String
#'paramsFields' Map
#'query' String
#'queryFields' Map
#'fragment' String","25/Oct/08 00:33;sms;Response to David's comments:

The map type in Pig was designed to hold any atomic key type (i.e., string, int, float, long, double) and any value type. As a result, the natural representation is a Map<Object, Object>.  The UDF has the right outputSchema implementation. UDFs that return maps should return Map<Object, Object>.

With the proposal in comment 3 (https://issues.apache.org/jira/browse/PIG-505?focusedCommentId=12642223#action_12642223), the UDF will work as long as there are no DataByteArray values in the Map that require a cast.

Response to Pi's comments:

Treating unknowns as bytearrays will lead to run time errors which will not go away if we treat unknowns as unknowns. The trade-off is better error handling. Specifically, in your example, comparing 2 unknowns can be caught during type checking whereas making them bytearrays might result in a run time error iff the two types do not match.

Summary: Treating unknowns as bytearray will result in coarser error messages. On the other hand treating unknown as unknown will require significant changes without eliminating the possibility of run time errors.","25/Oct/08 16:57;ciemo;Santhosh,

This sounds fine.  I don't quite follow comment #3 but if the result is the similar to doing a cast of a Java Object to a String, Float, Double, Integer, Long, etc, then this sounds cool.

It would seem that if some Data was a DataByteArray (bytearray) in the map and this required conversion, would it be possible to do a double cast such as:

(chararray) (bytearray) mymap#'abytearrayfield' as bytearray_now_chararray

Presumably one might need to add some parentheses:

(chararray) ((bytearray) (mymap#'abytearrayfield')) as bytearray_now_chararray


Maybe it should be okay to have runtime errors in the case of unknown.  Ideally, I'd like compile time errors for unknown which force us to use casts -- we can never have an unknown detected at compile time without resulting in the need for a cast.

There is a question though of what load functions that use maps will do that is different than URLPARSE and other UDFs that return maps.","28/Oct/08 22:05;sms;If the map contains a DataByteArray (bytearray) then we will not be able to convert it to any of the Pig types. For DataByteArray, Pig does not have a mechanism to interpret the bytes. Only non DataByteArray types can be converted to Pig types as long they can be converted, i.e., int to float, int to long, etc.

Double cast expressions require the parenthesis as you pointed out and can be used for non-bytearray conversions for data coming out of a UDF.","29/Oct/08 00:15;ciemo;I don't understand this statement:

If the map contains a DataByteArray (bytearray) then we will not be able to convert it to any of the Pig types. For DataByteArray, Pig does not have a mechanism to interpret the bytes. Only non DataByteArray types can be converted to Pig types as long they can be converted, i.e., int to float, int to long, etc. 

The Pig 1.4 to 2.0 Transition document (1.3 Cast) says:
from / to	bag	tuple	map	int	long	float	double	chararray	bytearray
bytearray	 yes	 yes	 yes	 yes	 yes	 yes	 yes	 yes	  

","29/Oct/08 01:00;sms;With our current design, we do not support conversion of bytearray from UDF to Pig types. Its an issue that needs a design/architectural solution.","30/Oct/08 22:36;sms;The attached patch (PIG-505.patch) addresses the following issues:

1. The lineage code does not error out when bytearrays are seen from a UDF. Instead the error message is pushed to runtime if a bytearray originating from a UDF is cast to a Pig type

2. Moved the SchemaUitils class out of the parser and into the Schema class

3. Added unit test cases for the lineage change

All unit test cases pass.","31/Oct/08 20:25;olgan;patch committed; thanks, santhosh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Illustrate and Dump do not seem to work correctly for files containing utf8,PIG-504,12406885,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,shubhamc,viraj,viraj,21/Oct/08 01:22,24/Mar/10 22:04,14/Mar/19 03:05,31/Oct/08 19:12,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"For the snippet of code which runs on the latest types branch. (utf8.txt attached)
{code}
A = load 'utf8.txt' using PigStorage() as (t1: chararray);
illustrate A;
{code}

results in this output being produced
-------------------------------
| A     | t1: bytearray cn: 1 | 
-------------------------------
|       | gabriella??         | 
-------------------------------

Three observations:
1) text should be chararray, not bytearray.
2) cn: 1 should be removed from the display
3) Value for text is ""username??"" is not displayed properly

Now replacing illustrate with dump
{code}
A = load 'utf8.txt' using PigStorage() as (t1: chararray);
dump A;
{code}

(david?)
(rachel?)
(jessica?)
(sarah?)
(katie?)
(wendy?)
(david?)
(priscilla?)
(oscar?)
(xavier?)
..some more. 

The utf8 characters after username are not displayed correctly but instead substituted by ?.",Hadoop 18,,,,,,,,,,,,,,,,,,21/Oct/08 09:19;shubhamc;504.patch;https://issues.apache.org/jira/secure/attachment/12392559/504.patch,21/Oct/08 02:17;viraj;utf8.txt;https://issues.apache.org/jira/secure/attachment/12392542/utf8.txt,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-10-21 09:19:02.677,,,no_permission,,,,,,,,,,,,164109,,,,,Fri Oct 31 19:12:13 UTC 2008,,,,,,,0|i0gl0v:,94836,,,,,,,,,,21/Oct/08 02:17;viraj;utf8.txt for verifying the problem,"21/Oct/08 09:19;shubhamc;Comments inline
1) text should be chararray, not bytearray.
-> Pig casts the data only when it is used. PigStorage loads data as bytearrays. If the data is used, PlanOptimizer inserts a foreach after load that casts data to particular data-types. So, for a script like the following
{{
a = load 'utf8.txt' as (x:chararray);
b = foreach a generate x;
illustrate b;

------------------------
| a     | x: bytearray | 
------------------------
|       | quinnφ      | 
------------------------
------------------------
| b     | x: chararray | 
------------------------
|       | quinn?       | 
------------------------

}}

2) cn: 1 should be removed from the display
-> I had used the toString method of schemas. I have modified the toString method in the attached patch. I would request Santhosh to have a look at it. 

3) Value for text is ""username??"" is not displayed properly
-> The datatypes use toString method of the object. The default charset used might be machine dependent. I am not sure why was the decision taken to go with the default charset instead of utf-8 or utf-16. I would request Alan to comment on it.",21/Oct/08 15:56;olgan;PIG-497 deals with dump issue. This jira should be just about illustrate,"21/Oct/08 16:03;olgan;Shubham: regarding (1) illustrate should be doing the same thing as describe. If you look at describe, you will see that it would say chararray",22/Oct/08 18:39;olgan;patch committed. keeping the issue opened till the type is fixed.,"31/Oct/08 19:12;olgan;dump part is addressed by PIG-497.

The illustrate just requires environment var change:

export LANG=""en_US.UTF-8""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make branches/types work under cygwin,PIG-501,12406654,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,17/Oct/08 02:34,24/Mar/10 22:04,14/Mar/19 03:05,12/Dec/08 00:40,0.2.0,,,,,,0.2.0,,impl,,,0,,,,We've already make all unit tests pass under cygwin for trunk (See [PIG-243|https://issues.apache.org/jira/browse/PIG-243]). We need to do the same for branches/types. ,cygwin,,,,,,,,,,,,,,PIG-243,,,,28/Oct/08 06:40;daijy;PIG-501-new1.patch;https://issues.apache.org/jira/secure/attachment/12392918/PIG-501-new1.patch,17/Oct/08 02:36;daijy;PIG_cygwin.Patch;https://issues.apache.org/jira/secure/attachment/12392326/PIG_cygwin.Patch,18/Oct/08 03:57;daijy;PIG_cygwin2.Patch;https://issues.apache.org/jira/secure/attachment/12392403/PIG_cygwin2.Patch,21/Oct/08 00:50;daijy;PIG_cygwin_original_javacc.Patch;https://issues.apache.org/jira/secure/attachment/12392540/PIG_cygwin_original_javacc.Patch,11/Dec/08 04:51;daijy;cygwin.javacc42.patch;https://issues.apache.org/jira/secure/attachment/12395799/cygwin.javacc42.patch,11/Dec/08 04:51;daijy;javacc.jar;https://issues.apache.org/jira/secure/attachment/12395800/javacc.jar,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2008-10-17 20:58:20.455,,,no_permission,,,,,,,,,,,,38594,,,,,Fri Dec 12 00:40:40 UTC 2008,,,Patch Available,,,,0|i0gkzj:,94830,,,,,,,,,,"17/Oct/08 02:36;daijy;We've already make all unit tests pass under cygwin for trunk (See [PIG-243|https://issues.apache.org/jira/browse/PIG-243]). We need to do the same for branches/types. Here is the problem I noticed and fixed for branches/types under cygwin.

1. We can not compile under cygwin. javacc fails in branches/types, error message ""Invalid Escape character"". We need to update javacc to the latest snapshot. Javacc 4.1 (latest) release do not solve the problem. This issue is described in https://javacc.dev.java.net/issues/show_bug.cgi?id=135. We do not have it before because only in branches/types we will let javacc generate output to directory ""xxx\util"", which contains ""\u"", triggering the problem. Currently I do not notice any problem with this latest javacc snapshot. One difference I notice is it sometimes generates a slightly different message upon error, and to me this message become more informative.


2. The tailing '\r' in input txt file cause some problem as before. I modify PigStorage.java and TextLoader.java to deal with it. However in branches/types, PigStorage is no longer line based and this make the situation much harder. In order to deal with it, we need to be able to read ahead, if we see a '\n' ahead, we can discard current '\r'. But in reality we do not look ahead and changes will be relatively big. So here I simply remove the tailing '\r' for each field under cygwin. There is a semantic diference but I think the chance that tailing '\r' carrying meaningful information is low. Correct me if I am wrong.

3. Most testing errors in cygwin is caused by wrong path name just like before. cygwin only takes the input/output file name in windows form, such as c:\ \ xxxx, and if we need to use a complete scheme, use ""file:/c:\ \ xxxx"", notice there is an extra ""/"" between ""file:"" and file path, which do not exists in unix. Many test cases do not notice that and fail in cygwin. Also we need to use ""\ \"" not ""\"" as seperator as described in [PIG-243|https://issues.apache.org/jira/browse/PIG-243].

","17/Oct/08 20:58;olgan;Hi Daniel,

Thanks,  for fixing the tests and figuring out the issue with javacc. 

Our policy is not to take code that has not been officially released. The quality is a suspect and also there is no support. Could you keep an eye on when the new version is officially released and resubmit it. Thanks.",17/Oct/08 20:59;olgan;I will commit your test changes as soon as my unit tests complete,"17/Oct/08 21:09;daijy;Thanks Olga. Yes, released javacc is certainly desired. However, the problem is we do not have control over javacc release plan, and pig can not be compiled with any javacc release under cygwin. One approach is get javacc release source code, apply specific patch to it. Another solution is, change the package ""org.apache.pig.test.util"" to another name such as ""org.apache.pig.test.tools"", as long as no component starts with ""u"". What's your opinion?",17/Oct/08 21:28;olgan;Reviewed the page. My concer is that changes to the load functions might slow them down. Can you run some numbers? Also maybe reordering if statement conditions can be helpful.,17/Oct/08 21:30;olgan;I think we will have to live with the fact that things don't work under sigwin till this issue is resolved by javacc. Users who need it can pick it up from this patch and use for now. we can advertise this on the dev list.,"18/Oct/08 03:57;daijy;I changed the patch to improve the performance of PigStorage a little bit. I did a performance test on PigStorage on two files with '\r' and without '\r', each file consists of 1m lines, every line has two fields, average field size is 10. Here is the result:

Unix:
|| ||before patch||after patch||
|contain '\r'|36.554|36.812|
|not contain '\r'|35.497|35.241|

Windows:
|| ||before patch||after patch||
|contain '\r'|36.637|37.001|
|not contain '\r'|35.311|35.318|

The overhead seems to be ok.

For javacc issue I can keep track of javacc release plan. And currently we can point pig user to here if they do need cygwin.","20/Oct/08 22:05;daijy;I've got reply from javacc developer.

> 
> Unless anybody has any objections, I was thinking of a release towards
> the end of this month, or early November.
> 
> Cheers,
> Paul

","20/Oct/08 22:18;olgan;Thanks for checking, Daniel!

I will get your patch in today or tomorrow if all tests pass.

","21/Oct/08 00:31;olgan;under unix, I see one failure in TestParamSubPreproc:

Testcase: testInvalidLineinConfigfile took 0.005 sec
    FAILED
null
junit.framework.AssertionFailedError: null
    at org.apache.pig.test.TestParamSubPreproc.testInvalidLineinConfigfile(TestParamSubPreproc.java:677)
","21/Oct/08 00:50;daijy;It is because the javacc.jar. My patch assume the new version of javacc.jar.

Originally the assert statement is:
assertTrue(e.getMessage().startsWith(""Encountered \""is\"" at line 2, column 6.""));

Since new version of javacc change the error message, I change the assert statement to 
assertTrue(e.getMessage().startsWith(""Encountered \"" <IDENTIFIER> \""is \""\"" at line 2, column 6.""));

Now we still use the original javacc.jar, so we should not change this line. I attached the patch without this change.",22/Oct/08 00:48;olgan;I committed the changes. Thanks Daniel. Keeping the issue open till we get new version of javacc,"28/Oct/08 06:40;daijy;Several new discovered bug under cygwin:
1. TestPigServer.java, several testcases forget to close input stream 
2. BinStorage.determineSchema, forget to close input stream

While it does not cause problem under Linux, it does under cygwin. 

We can either patch it now, or wait for javacc release and patch everything all together.",30/Nov/08 16:44;daijy;javacc 4.2 is just released. I will deliver the patch next week.,"11/Dec/08 04:51;daijy;Make Pig work under cygwin with javacc 4.2 release. Unit test pass in both cygwin and Linux. We will need to:
1. Apply cygwin.javacc42.patch
2. Change javacc.jar with the one in javacc 4.2 release. I also attached new javacc.jar.",12/Dec/08 00:40;olgan;patch committed; thanks daniel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Load Func for POCast is not being set in some cases,PIG-500,12406647,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,pkamath,pkamath,17/Oct/08 00:10,24/Mar/10 22:04,14/Mar/19 03:05,17/Oct/08 20:32,0.2.0,,,,,,0.2.0,,,,,0,,,,"The POCasts resulting from the following scripts have a null load Function reference which leads to run time null pointer exceptions:

Script 1:
{code}
a = load 'a' as (s);
b = foreach a generate s#'a' as x;
c = foreach b generate (int) x#'a' as intx;
{code}

Script 2:
{code}
a = load 'bla' as (s, m, l);
b = foreach a generate s#'x' as f1, s#'y' as f2, s#'z' as f3;
c = group b by f1;
d = foreach c {
    fil = filter b by f2 == 1;
    generate flatten(group), SUM(fil.f3);
    }
{code}
",,,,,,,,,,,,,,,,,,,17/Oct/08 15:40;sms;PIG-500.patch;https://issues.apache.org/jira/secure/attachment/12392349/PIG-500.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-10-17 15:40:54.924,,,no_permission,,,,,,,,,,,,38496,,,,,Fri Oct 17 20:32:32 UTC 2008,,,Patch Available,,,,0|i0gkz3:,94828,,,,,,,,,,"17/Oct/08 15:40;sms;Patch to fix issue reported in the bug. I also added a check for null load function when casting from bytearray to a Pig type.

All uni test cases passed.",17/Oct/08 20:32;olgan;patch committed; thanks santhosh for fixing and pradeep for investigation!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Alias which contains substring ""as"" causes Parser to throw a syntax error",PIG-499,12406646,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,viraj,viraj,17/Oct/08 00:02,24/Mar/10 22:04,14/Mar/19 03:05,24/Oct/08 21:41,0.2.0,,,,,,0.2.0,,,,,0,,,,"---------------------------------------------------------------------------------------
{code}
MYDATA = load 'testfile.txt' as (f1, f2, f3, f4, f5);
MYDATA_PROJECTION = foreach MYDATA generate f3, f5;
CAST = group MYDATA_PROJECTION by f3;
RESULT = foreach CAST {
          MYSORTED = ORDER MYDATA_PROJECTION by f5;
          generate flatten(MYSORTED);
}
dump RESULT;
{code}
---------------------------------------------------------------------------------------
The above query throws a syntax error 
java.io.IOException: Encountered ""dump"" at line 8, column 1.
Was expecting one of:
    ""parallel"" ...
    "";"" ...
    
        at org.apache.pig.PigServer.parseQuery(PigServer.java:298)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:263)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:439)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:249)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:84)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:64)
        at org.apache.pig.Main.main(Main.java:306)
Caused by: org.apache.pig.impl.logicalLayer.parser.ParseException: Encountered ""dump"" at line 8, column 1.
Was expecting one of:
    ""parallel"" ...
    "";"" ...
    
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.generateParseException(QueryParser.java:7763)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.jj_consume_token(QueryParser.java:7640)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:551)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:60)
        at org.apache.pig.PigServer.parseQuery(PigServer.java:295)
        ... 6 more
---------------------------------------------------------------------------------------
But now changing this query to:(changing CAST to CST) makes the query execute successfully.  
---------------------------------------------------------------------------------------
{code}
MYDATA = load 'testfile.txt' as (f1, f2, f3, f4, f5);
MYDATA_PROJECTION = foreach MYDATA generate f3, f5;
CST = group MYDATA_PROJECTION by f3;
RESULT = foreach CST {
          MYSORTED = ORDER MYDATA_PROJECTION by f5;
          generate flatten(MYSORTED);
}
dump RESULT;
{code}
---------------------------------------------------------------------------------------
I believe this might have been caused due to the patch which resolved issue: https://issues.apache.org/jira/browse/PIG-437",Hadoop 18,,,,,,,,,,,,,,,,,,18/Oct/08 01:06;sms;PIG-499.patch;https://issues.apache.org/jira/secure/attachment/12392399/PIG-499.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-10-17 03:13:14.804,,,no_permission,,,,,,,,,,,,164107,,,,,Fri Oct 24 21:41:21 UTC 2008,,,Patch Available,,,,0|i0gkyn:,94826,,,,,,,,,,"17/Oct/08 03:13;daijy;Do we need a semicolon after each statement? When I put a "";"" after 4th statement, pig stop complaining.","17/Oct/08 03:28;viraj;Daniel I observed the same thing, but this creates an inconsistent syntax for {code} foreach alias { } {code} when alias contains ""as"" as opposed to when it does not contain...","17/Oct/08 03:56;daijy;Hi, Viraj,
I understand your concern. How about always adding semicolon? I think this should be the right way. When the grammar is wrong, javacc sometimes complains sometimes not. Let's just stick to the right way.",18/Oct/08 01:06;sms;Attached patch fixes the issue reported in the bug. All unit tests except one test in TestStreaming passed. TestStreaming worked when run on a standalone basis.,"24/Oct/08 21:41;olgan;the patch has been committed; thanks, snathosh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig does not error out while trying to use a input file to which the user does not have access permissions,PIG-498,12406645,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nrai,pkamath,pkamath,16/Oct/08 23:45,17/Dec/10 22:43,14/Mar/19 03:05,27/Jul/10 17:24,0.2.0,,,,,,0.8.0,,,,,0,,,,"Session illustrating the issue.
{code}
bash-3.00$ hadoop fs -ls /data/statistics.txt
ls: org.apache.hadoop.fs.permission.AccessControlException: Permission denied: user=<username>, access=READ_EXECUTE, inode=""<inode>""<permissions>-
bash-3.00$ pig -latest 
2008-10-16 23:31:25,134 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to HOD...
...
2008-10-16 23:34:45,810 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: local
grunt> a = load '/data/statistics.txt';      
grunt> dump a;
2008-10-16 23:39:05,624 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete
2008-10-16 23:39:05,624 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!
grunt> 
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-07-23 21:04:25.349,,,no_permission,,,,,,,,,,,,164106,,,,,Tue Jul 27 01:17:54 UTC 2010,,,,,,,0|i0gky7:,94824,,,,,,,,,,"23/Jul/10 21:04;olgan;I am guessing this issue might have gone away with Pig 0.7.0. Niraj, could you verify and if it is gone, please, close","27/Jul/10 01:17;nrai;Hi Olga,
              Yes, this has been fixed. 
Thanks
Niraj",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dump does not deal with non-ascii data,PIG-497,12406638,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,olgan,olgan,16/Oct/08 21:15,24/Mar/10 22:04,14/Mar/19 03:05,31/Oct/08 19:10,0.2.0,,,,,,0.2.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,30/Oct/08 23:24;pkamath;PIG-497-2.patch;https://issues.apache.org/jira/secure/attachment/12393116/PIG-497-2.patch,30/Oct/08 01:08;pkamath;PIG-497.patch;https://issues.apache.org/jira/secure/attachment/12393021/PIG-497.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-10-30 01:08:19.599,,,no_permission,,,,,,,,,,,,164105,,,,,Fri Oct 31 19:10:32 UTC 2008,,,,,,,0|i0gkxr:,94822,,,,,,,,,,"30/Oct/08 01:08;pkamath;Patch attached.
There were three issues which were resolved:
- DataReaderWriter was using DataOutput.writeBytes(String) instead of DataOutput.writeUTF(String). Likewise it was using DataInput.readFully(bytes[]) instead of DataInput.readUTF(). The earlier calls get only lower 8bits out of each character in the string which would mess up multi byte UTF8 data
- illustrate and dump eventually use System.out.println to output results and System.out.println() writes bytes in platform default encoding which is typically UTF-16. This was changed to System.write(String.getBytes(""UTF-8"")","30/Oct/08 23:24;pkamath;New version of patch attached with following changes:
- Rolled back the changes to GruntParser.java and ExampleGenerator.java. In the earlier patch System.out.println(String) was replaced by System.out.write(String.getBytes(""UTF-8""). This would force the output to always be in UTF-8 for ""illustrate"" and ""dump"" commands. This has been reverted back to System.out.println() so that the output is in the VM's default charset (which can be controlled by the LANG environment variable in UNIX). This is to allow users to choose their charset for output.
- Changed Util.createInputFile (helper function used by the unit test) to write the input file in UTF-8 encoding so that the unit test introduced in this patch can run without the need to have ""LANG"" environment variable set up.","31/Oct/08 19:10;olgan;patch committed, thanks pradeep",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
project of bags from complex data causes failures,PIG-496,12406456,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,daijy,olgan,olgan,15/Oct/08 00:37,04/Aug/11 00:34,14/Mar/19 03:05,24/Jan/11 18:42,,,,,,,0.9.0,,,,,0,,,,"A = load 'complex data' as (x: bag{});
B = foreach A generate x.($1, $2);

produces stack trace:

2008-10-14 15:11:07,639 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher - Error message from task (reduce) task_200809241441_9923_r_000000java.lang.NullPointerException
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:183)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:215)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:166)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.runPipeline(PigMapReduce.java:252)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:222)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:134)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:318)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2207)

Pradeep suspects that the problem is in src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POProject.java; line 374",,,,,,,,,,,,,,,,,,,11/Jan/11 02:27;daijy;PIG-496-1.patch;https://issues.apache.org/jira/secure/attachment/12467953/PIG-496-1.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-10-19 18:49:17.174,,,no_permission,,,,,,,,,,,,66210,Reviewed,,,,Mon Jan 24 18:42:54 UTC 2011,,,,,,,0|i0gkxb:,94820,,,,,,,,,,"19/Oct/10 18:49;alangates;If you run a script like the above now (on version 0.7) it does not fail, but instead gives an error message ""ERROR 1026: Attempt to fetch field 0 from schema of size 0""  This is at least a decent error message.  The problem now is that we allow positional notation to work in cases where the schema is undefined, which it is when you say bag{}.  So $0 should work.","11/Jan/11 01:43;daijy;We need to decide how to load empty bag, eg.
{code}
A = load 'data.txt' as (x: bag{});
{code}
Currently, we load x as bag, inside x we don't do any interpretation. So what we load is a bag of bytearrays.

This however cause problem when we do further processing for this bag. Assume in data.txt, the bag actually contains three item tuples:
{code}
B = foreach A generate x.($1, $2); 
{code}
We expect it will project 2nd, 3th field of the tuple. But in current code, x is a bag of one field bytearray, this results an error
{code}
B = foreach A generate flatten x;
{code}
We expect it will flatten x into 3 fields. But in current code, we cannot even flatten x, since x does not contain tuple.

The problem stems in two sources:
1. Currently bag requires tuple in some cases, but not require tuple in other cases. This is inconsistent. We should make it a rule. So when we load a bag, actually means load a bag of tuples

2. When we load a tuple with unknown number of fields (tuple inner schema is unknown), we assume it contains only one bytearray field. However, it is not possible to cast one byte field to multiple fields later. Recall when we load a file with unknown schema:
{code}
A = load 'data.txt';
{code}
We actually load multiple fields seperated by delimit, each field is of type bytearray. When we load empty bag, we can mimic this behavior. 

So I propose two changes:
1. Load a bag implies loading a bag of tuples, even when bag inner schema is empty.
2. When we convert bytearray to tuple with no inner schema, we no longer assume one field. We will take comma as delimit (in the case of UTF8StorageConverter) and produce a tuple of multiple bytearray fields.

Assume data.txt is:
{(1,2,3),(4,5,6)}
After this change, 
A = load 'data.txt' as (x: bag{});
describe A:
We get: bag{}
dump A:
We get: {(1,2,3),(4,5,6)}, which is not a bag of byteArrays, but a bag of three item tuples.",11/Jan/11 01:58;olgan;This looks good with on modification - the fields don't have to be bytearrays - they can be of any type,11/Jan/11 02:27;daijy;PIG-496-1.patch is depended on PIG-730. Otherwise there will be frontend exception.,18/Jan/11 20:05;rding;+1,"24/Jan/11 18:42;daijy;Review notes:
https://reviews.apache.org/r/272/

Patch committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
generate of a bag with multiple columns produces only the first column,PIG-495,12406448,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,14/Oct/08 22:24,24/Mar/10 22:04,14/Mar/19 03:05,15/Oct/08 01:02,0.2.0,,,,,,0.2.0,,,,,0,,,,"The following query produces the incorrect results 

A = load 'data' as (x, y, z);
B = group A by x;
C = foreach B generate A.(y.z);

The output only contains 1 column 

(55)
(33)
....",,,,,,,,,,,,,,,,,,,15/Oct/08 00:04;olgan;PIG-495;https://issues.apache.org/jira/secure/attachment/12392149/PIG-495,15/Oct/08 00:40;olgan;PIG-495.patch;https://issues.apache.org/jira/secure/attachment/12392151/PIG-495.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-10-15 00:45:45.352,,,no_permission,,,,,,,,,,,,164104,,,,,Wed Oct 15 01:02:47 UTC 2008,,,,,,,0|i0gkwv:,94818,,,,,,,,,,15/Oct/08 00:45;pkamath;Looks good - +1,15/Oct/08 01:02;olgan;patch committed; thanks pradeep for help and review,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Utf8StorageConverter.bytesToCharArray does not properly do utf8 conversions,PIG-494,12406406,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,alangates,alangates,14/Oct/08 16:55,24/Mar/10 22:04,14/Mar/19 03:05,14/Oct/08 23:50,0.2.0,,,,,,0.2.0,,impl,,,0,,,,This function just does new String(bytes[]).  It needs instead to use a CharsetDecoder (see BufferedPositionedInputStream.readLine in pig 1.x).  This causes non-ascii characters to be incorrectly translated from byte arrays to strings.,,,,,,,,,,,,,,,,,,,14/Oct/08 22:29;pkamath;PIG-494.patch;https://issues.apache.org/jira/secure/attachment/12392143/PIG-494.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-10-14 22:29:11.515,,,no_permission,,,,,,,,,,,,164103,,,,,Tue Oct 14 23:50:57 UTC 2008,,,,,,,0|i0gkwf:,94816,,,,,,,,,,"14/Oct/08 22:29;pkamath;Attached Patch - I used
{code}
String(byte[] bytes, String charsetName)
          Constructs a new String by decoding the specified array of bytes using the specified charset.
{code}

instead of using CharsetDecoder.

I had to likewise make a change in PigStorage to use 
{code}
getBytes(String charsetName)
          Encodes this String into a sequence of bytes using the named charset, storing the result into a new byte array.
{code}

In both the above calls I use ""UTF-8"" as charset name.

With these changes, users of PigStorage will have to be aware that PigStorage assumes input data to it is in UTF-8 and output from it is in UTF-8 for chararray fields.",14/Oct/08 23:50;olgan;patch committed; thanks pradeep!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Combiner not used when group elements referred to in tuple notation instead of flatten.,PIG-490,12406203,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,alangates,alangates,10/Oct/08 21:09,04/Aug/11 00:34,14/Mar/19 03:05,19/Dec/10 04:00,0.2.0,,,,,,0.9.0,,,,,1,,,,"Given a query like:

{code}
A = load 'myfile';
B = group A by ($0, $1);
C = foreach B generate group.$0, group.$1, COUNT(A);
{code}

The combiner will not be invoked.  But if the last line is changed to:

{code}
C = foreach B generate flatten(group), COUNT(A);
{code}

it will be.  The reason for the discrepancy is because the CombinerOptimizer checks that all of the projections are simple.  If not, it does not use the combiner.  group.$0 is not a simple projection, so this is failed.  However, this is a common enough case that the CombinerOptimizer should detect it and still use the combiner. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-04-22 06:38:37.46,,,no_permission,,,,,,,,,,,,66192,,,,,Sun Dec 19 04:00:11 UTC 2010,,,,,,,0|i0gkun:,94808,,,,,,,,,,"22/Apr/10 06:38;scott_carey;Ugh, so that is what is happening and why I almost never see combiners.  I almost always use the 'generate group.field1, group.field2' notation instead of flatten because it saves me a line projecting out ""group::field1 as field1 ... "" in the next line.  If I don't do the latter it tends to fail to parse when the alias output from the FOREACH is used later.    

That is to say I avoid FLATTEN(group) because it works around other peculiarities and ends up being harder to use than listing all the fields in the group.

This is with Pig 0.5.

",30/Apr/10 19:17;olgan;We have seen users run into this. How much work would this be?,"03/May/10 17:38;alangates;In the current system, a fair amount, just because the code in the combiner optimizer is so byzantine.  I was hoping we could instead reuse the code from the logical optimizer rewrite and thus make it much easier to make this change.",03/May/10 17:48;olgan;Perhaps a good plan of action would be to try this out with the new optimizer framework. This would give us a chance to experiment with using it at MR layer without moving existing optimizations there. (That would be something we can do in 0.9.0 if the new frameworks proves to be flexible and stable enough by that time.),"28/Jul/10 00:54;thejas;Moving fix version to 0.9. I am not likely to be able to fix this in time for 0.8 .
","19/Dec/10 04:00;thejas;Fixed as part of changes in PIG-750 .
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Translate project( * ) in operators that have a nested plan,PIG-489,12406201,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,sms,sms,10/Oct/08 20:47,24/Mar/10 22:04,14/Mar/19 03:05,10/Oct/08 23:07,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"Currently, the project( * ) operator results in the generation of a tuple schema even when the schema of the project's input is known. This is confusing to the user and should be remedied. In addition this will address a bug in Pig-335 which prevents tracking lineage accurately.",,,,,,,,,,,,,,,,,,,10/Oct/08 21:33;sms;PIG-489.patch;https://issues.apache.org/jira/secure/attachment/12391907/PIG-489.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-10-10 22:15:09.228,,,no_permission,,,,,,,,,,,,164099,,,,,Fri Oct 10 23:07:42 UTC 2008,,,Patch Available,,,,0|i0gku7:,94806,,,,,,,,,,"10/Oct/08 21:33;sms;Attached patch, adds a new visitor that translates project( * ) inside nested plans of foreach, cogroup and sort if the project's input has a schema and if the return type of the project is not a bag.

All unit test cases except TestStreaming pass. TestStreaming passes when run in isolation.",10/Oct/08 22:15;alangates;+1,"10/Oct/08 23:07;olgan;patch committed. thanks, santhosh for contributing and alan for reviewing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""task failed to report status"" error",PIG-475,12405975,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,shravanmn,olgan,olgan,07/Oct/08 23:44,24/Mar/10 22:04,14/Mar/19 03:05,11/Oct/08 00:25,0.2.0,,,,,,0.2.0,,,,,0,,,,"When running a very large query a user got the following error after 90 minutes:

org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher
>> - Error message from task (reduce) tip_200810072143_0004_r_000033Task
>> task_200810072143_0004_r_000033_0 failed to report status for 600 
>> seconds. Killing!

Looks like we missed reporting progress in a few places.",,,,,,,,,,,,,,,,,,,10/Oct/08 22:34;shravanmn;475.patch;https://issues.apache.org/jira/secure/attachment/12391910/475.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-10-08 17:53:28.828,,,no_permission,,,,,,,,,,,,164086,,,,,Tue Mar 17 18:17:36 UTC 2009,,,,,,,0|i0gkov:,94782,,,,,,,,,,08/Oct/08 17:53;shravanmn;The query that cause the error will be helpful than generally looking around where we missed it and it might be that there is a udf which doesn't do reporting. Could you pls attach the script?,10/Oct/08 22:33;shravanmn;The PhysicalOperator.reporter initialization in PigMapReduce.Reduce.reduce was missing,11/Oct/08 00:25;olgan;patch committed; thanks shravan!,"17/Mar/09 18:17;vzaliva;I am using SVN trunk version (checked out March 16, 2009) and getting these errors on one of my pig scripts.

Task attempt_200903131720_0047_m_000270_0 failed to report status for 627 seconds. Killing!
...

I could not successfully complete this task even once!

This task is working with data in bz2 format. My other tasks are working with non-bz2 data. Maybe this is a reason?

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Verbose (unrelated) exception stack is printed when input file in the Load is not present,PIG-471,12405863,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,06/Oct/08 22:04,24/Mar/10 22:04,14/Mar/19 03:05,07/Oct/08 00:44,0.2.0,,,,,,0.2.0,,,,,0,,,,"When input file in load is absent the following is printed on the screen:
{noformat}
2008-10-06 10:24:28,048 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - java.io.IOException: bla does not exist
        at org.apache.pig.backend.executionengine.PigSlicer.validate(PigSlicer.java:105)
        at org.apache.pig.impl.io.ValidatingInputFileSpec.validate(ValidatingInputFileSpec.java:59)
        at org.apache.pig.impl.io.ValidatingInputFileSpec.<init>(ValidatingInputFileSpec.java:44)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:200)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:742)
        at org.apache.hadoop.mapred.jobcontrol.Job.submit(Job.java:370)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs(JobControl.java:247)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.run(JobControl.java:279)
        at java.lang.Thread.run(Thread.java:595)

2008-10-06 10:24:28,061 [main] ERROR org.apache.pig.tools.grunt.GruntParser - java.io.IOException: Unable to store for alias: 1 [java.io.IOException: java.lang.NullPointerException
        at java.util.TreeMap.getEntry(TreeMap.java:324)
        at java.util.TreeMap.get(TreeMap.java:255)
        at org.apache.hadoop.mapred.JobTracker.getMapTaskReports(JobTracker.java:1894)
        at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:888)
]
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:255)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:647)
        at org.apache.pig.PigServer.execute(PigServer.java:638)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:278)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:439)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:249)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:94)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:58)
        at org.apache.pig.Main.main(Main.java:282)
Caused by: org.apache.pig.backend.executionengine.ExecException: java.io.IOException: java.lang.NullPointerException
        at java.util.TreeMap.getEntry(TreeMap.java:324)
        at java.util.TreeMap.get(TreeMap.java:255)
        at org.apache.hadoop.mapred.JobTracker.getMapTaskReports(JobTracker.java:1894)
        at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:888)

        ... 9 more
Caused by: org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.NullPointerException
        at java.util.TreeMap.getEntry(TreeMap.java:324)
        at java.util.TreeMap.get(TreeMap.java:255)
        at org.apache.hadoop.mapred.JobTracker.getMapTaskReports(JobTracker.java:1894)
        at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:888)

        at org.apache.hadoop.ipc.Client.call(Client.java:715)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:216)
        at org.apache.hadoop.mapred.$Proxy1.getMapTaskReports(Unknown Source)
        at org.apache.hadoop.mapred.JobClient.getMapTaskReports(JobClient.java:935)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher.getStats(Launcher.java:90)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:92)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:245)
        ... 8 more

2008-10-06 10:24:28,061 [main] ERROR org.apache.pig.tools.grunt.GruntParser - Unable to store for alias: 1 [java.io.IOException: java.lang.NullPointerException
        at java.util.TreeMap.getEntry(TreeMap.java:324)
        at java.util.TreeMap.get(TreeMap.java:255)

{noformat}
Of this only the first part which actually tells that the input file is absent is useful, the rest is misleading and confusing",,,,,,,,,,,,,,,,,,,06/Oct/08 23:24;pkamath;PIG-471.patch;https://issues.apache.org/jira/secure/attachment/12391589/PIG-471.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-10-07 00:44:57.697,,,no_permission,,,,,,,,,,,,164082,,,,,Tue Oct 07 00:44:57 UTC 2008,,,,,,,0|i0gkn3:,94774,,,,,,,,,,"06/Oct/08 23:24;pkamath;Attached patch for the issue. The problem was that when the file is missing, the job submission fails in getSplit() even BEFORE map or reduce tasks are ever run. In MapReduceLauncher after we come to know the job has failed, we try to get mapTaskReport and reduceTaskReport (in Launcher.getStats()) from Hadoop which results in an IOexception. The fix catches this and gives a warning to the user IF the job had succeeded",07/Oct/08 00:44;olgan;patch committed; thanks pradeep,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TextLoader returns string instead of a bytearray,PIG-470,12405587,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,sms,sms,01/Oct/08 20:48,24/Mar/10 22:04,14/Mar/19 03:05,01/Oct/08 22:59,0.2.0,,,,,,0.2.0,,,,,0,,,,"The TextLoader load function returns a string instead of bytearrays. By default, load functions should return bytearrays and Pig will then convert the bytearray into the appropriate Pig Types.",,,,,,,,,,,,,,,,,,,01/Oct/08 21:36;sms;PIG-470.patch;https://issues.apache.org/jira/secure/attachment/12391319/PIG-470.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-10-01 22:59:36.291,,,no_permission,,,,,,,,,,,,37870,,,,,Wed Oct 01 22:59:36 UTC 2008,,,Patch Available,,,,0|i0gkmn:,94772,,,,,,,,,,"01/Oct/08 21:36;sms;Attached patch, fixes the TextLoader to return data bytearray instead of string. All unit test cases pass.","01/Oct/08 22:59;olgan;patch committed. thank, santhosh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"describe call int type as ""integer""",PIG-469,12405504,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,sms,olgan,olgan,30/Sep/08 23:54,24/Mar/10 22:04,14/Mar/19 03:05,22/Oct/08 18:39,0.2.0,,,,,,0.2.0,,,,,0,,,,"grunt> a = load 'data' as (name:chararray, age:int, gpa:double);
grunt> describe a;
a: {name: chararray,age: integer,gpa: double}",,,,,,,,,,,,,,,,,,,22/Oct/08 03:44;sms;PIG-469.patch;https://issues.apache.org/jira/secure/attachment/12392626/PIG-469.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-10-22 03:44:04.49,,,no_permission,,,,,,,,,,,,667,,,,,Wed Oct 22 18:39:01 UTC 2008,,,Patch Available,,,,0|i0gkm7:,94770,,,,,,,,,,22/Oct/08 03:44;sms;Attached patch fixes the issue reported in the bug. All unit test cases passed.,22/Oct/08 03:44;sms;Re-attaching as I had not granted license for ASF,22/Oct/08 18:39;olgan;patch committed; thanks Santhosh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make BinStorage declare its schema,PIG-468,12405501,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,olgan,olgan,30/Sep/08 23:19,24/Mar/10 22:04,14/Mar/19 03:05,14/Oct/08 22:27,0.2.0,,,,,,0.2.0,,,,,0,,,,"Currently, BinStorage breaks the rule that unless it tells Pig what types it is producing it should produce bytearrays. This causes runtime problems as the frontend assumes the data to be of one type while in fact it is of different type.

Loader interface has a way to specify schema via determineSchema API. BinStorage need to implement this. Also, since this interface has not been used before, the pluming might also need to be adjusted.",,,,,,,,,,,,,,,,,,,14/Oct/08 21:25;pkamath;PIG-468.patch;https://issues.apache.org/jira/secure/attachment/12392133/PIG-468.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-10-14 21:23:33.808,,,no_permission,,,,,,,,,,,,164081,,,,,Tue Oct 14 22:27:11 UTC 2008,,,,,,,0|i0gklr:,94768,,,,,,,,,,"14/Oct/08 21:23;pkamath;Attached patch, some notes on the patch:
- determineSchema() was never being called from LOLoad  since the schemaFile was always passed as null from the parser. I have changed the signature of this method so that implementations of this method can open the input file if they need to, to determine the schema. Here is the new API:
{code}
/**
     * Find the schema from the loader.  This function will be called at parse time
     * (not run time) to see if the loader can provide a schema for the data.  The
     * loader may be able to do this if the data is self describing (e.g. JSON).  If
     * the loader cannot determine the schema, it can return a null.
     * LoadFunc implementations which need to open the input ""fileName"", can use 
     * FileLocalizer.open(String fileName, ExecType execType, DataStorage storage) to get
     * an InputStream which they can use to initialize their loader implementation. They
     * can then use this to read the input data to discover the schema. Note: this will
     * work only when the fileName represents a file on Local File System or Hadoop file 
     * system
     * @param fileName Name of the file to be read.(this will be the same as the filename 
     * in the ""load statement of the script)
     * @param execType - execution mode of the pig script - one of ExecType.LOCAL or ExecType.MAPREDUCE
     * @param storage - the DataStorage object corresponding to the execType
     * @return a Schema describing the data if possible, or null otherwise.
     * @throws IOException.
     */
    public Schema determineSchema(String fileName, ExecType execType, DataStorage storage) throws IOException;
{code}

As noted in the comments above, I have also added a static helper method in FileLocalizer.  LoadFunc implementations which need to open the input ""fileName"", can use  FileLocalizer.open(String fileName, ExecType execType, DataStorage storage) to get an InputStream which they can use to initialize their loader implementation. There are some related changes in TypeCastInserter and Schema to handle schema specification in the Load statement. (which would be providing an additional schema in addition to the one determined by determineSchema())

 - Reviewers, please also look at  https://issues.apache.org/jira/browse/PIG-492 to make sure that, that are no blockers for that issue - it seems like for that issue we would need to serialize each of the loader 
in the query and send it to the backend which may not be trivial","14/Oct/08 22:27;olgan;patch committed; thanks, pradeep",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Loading data with a schema that has a bag as a field results in error,PIG-464,12405407,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,29/Sep/08 18:20,24/Mar/10 22:04,14/Mar/19 03:05,03/Oct/08 02:33,0.2.0,,,,,,0.2.0,,,,,0,,,,"Script:
{code}
a = load '/user/pig/tests/data/singlefile/studenttab10k' using PigStorage() as (name:chararray, age:long, gpa: float);
b = group a by  name;
c = foreach b generate a, (1,2,3), ['key1'#'value1','key2'#'value2'];
-- store the bag, tuple and map
store c into '/tmp/intermediate' using BinStorage();
d = load '/tmp/intermediate' using BinStorage() as (b:bag{t:tuple(x,y,z)}, t2:tuple(a,b,c), m:map[]);
e = foreach d generate COUNT(b), t2.a, t2.b, t2.c, m#'key1', m#'key2';
dump e;
{code}

Error:
{noformat}
java.io.IOException: Unable to open iterator for alias: e [Unable to store for alias: e [Unable to insert type casts into plan]]
        at org.apache.pig.impl.logicalLayer.optimizer.TypeCastInserter.transform(TypeCastInserter.java:150)
        at org.apache.pig.impl.plan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:65)
        at org.apache.pig.PigServer.compileLp(PigServer.java:723)
        at org.apache.pig.PigServer.compileLp(PigServer.java:655)
        at org.apache.pig.PigServer.store(PigServer.java:433)
        at org.apache.pig.PigServer.store(PigServer.java:421)
        at org.apache.pig.PigServer.openIterator(PigServer.java:384)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:269)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:178)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:84)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:64)
        at org.apache.pig.Main.main(Main.java:306)
Caused by: java.io.IOException: Unable to store for alias: e [Unable to insert type casts into plan]
        ... 12 more
Caused by: org.apache.pig.impl.plan.optimizer.OptimizerException: Unable to insert type casts into plan
        ... 12 more
Caused by: java.lang.NullPointerException
        at java.util.ArrayList.<init>(ArrayList.java:133)
        at org.apache.pig.impl.util.MultiMap.put(MultiMap.java:82)
        at org.apache.pig.impl.logicalLayer.schema.Schema.clone(Schema.java:725)
        at org.apache.pig.impl.logicalLayer.schema.Schema$FieldSchema.clone(Schema.java:348)
        at org.apache.pig.impl.logicalLayer.optimizer.TypeCastInserter.transform(TypeCastInserter.java:131)

{noformat}",,,,,,,,,,,,,,,,,,,03/Oct/08 00:11;pkamath;PIG-464.patch;https://issues.apache.org/jira/secure/attachment/12391390/PIG-464.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-10-03 02:33:44.654,,,no_permission,,,,,,,,,,,,164077,,,,,Fri Oct 03 02:33:44 UTC 2008,,,,,,,0|i0gkjz:,94760,,,,,,,,,,03/Oct/08 00:10;pkamath;Attached patch with unit test. The change was to use canonical name of FieldSchema as the key in the mFieldSchemas multimap. Earlier the FieldSchema object was the key and this caused problems when the the FieldSchema object was changed (say the schema or some other member was changed) while in the multimap. This caused the hashcode of the FieldSchema object to change and hence corrupt the multimap.,03/Oct/08 02:33;olgan;patch comitted; thanks pradeep,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Strings (or other typed data) stored in maps cause problems with implicit casts in operations,PIG-463,12405279,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,26/Sep/08 20:57,24/Mar/10 22:04,14/Mar/19 03:05,27/Sep/08 19:44,0.2.0,,,,,,0.2.0,,,,,0,,,,"If a map in pig (say returned from a UDF) has a key with the value being a string, then a lookup of that key being used in a context which expects a string will cause an implicit cast to a string. This is because the Pig frontented (logical layer) thinks of all map ""values"" as bytearrays and hence introduces a Cast to convert the bytearray to string. The POCast class then assumes its input is a DataByteArray and first tries to cast its input to DataByteArray. This fails because the object in the input to the case which is the value in the map is in fact already a string. So in these cases, we should not try to cast but just return the input object in the POCast.",,,,,,,,,,,,,,,,,,,27/Sep/08 05:53;pkamath;PIG-463.patch;https://issues.apache.org/jira/secure/attachment/12391077/PIG-463.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-27 19:44:06.474,,,no_permission,,,,,,,,,,,,164076,,,,,Sat Sep 27 19:44:06 UTC 2008,,,,,,,0|i0gkjj:,94758,,,,,,,,,,"27/Sep/08 05:53;pkamath;Attached patch - augments TestPOCast to test for the fix. 
TestJobSubmission and TestLocalJobSubmission unit tests fail but were failing even before the patch on the current code in svn and are not due to the patch","27/Sep/08 19:44;olgan;patch committed; thanks, pradeep",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LIMIT N should create one output file with N rows,PIG-462,12405176,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,shravanmn,amirhyoussefi,amirhyoussefi,25/Sep/08 20:53,24/Mar/10 22:04,14/Mar/19 03:05,26/Sep/08 19:26,0.2.0,,,,,,0.2.0,,,,,0,,,,"LIMIT N should create one output file with N rows. Currently it produces multiple files with N rows each. 

As described in PIG-171 last MR job needs to have 1 reducer:

http://issues.apache.org/jira/browse/PIG-171?focusedCommentId=12623540#action_12623540",,,,,,,,,,,,,,,,,,,26/Sep/08 16:11;shravanmn;limitrq.patch;https://issues.apache.org/jira/secure/attachment/12391032/limitrq.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-26 06:27:57.166,,,no_permission,,,,,,,,,,,,164075,,,,,Fri Sep 26 19:26:15 UTC 2008,,,,,,,0|i0gkj3:,94756,,,,,,,,,,"26/Sep/08 06:27;daijy;Hi, Amir,
[PIG-364|https://issues.apache.org/jira/browse/PIG-364] address the issue you mention. If you still see multiple output files in the latest branches/types, probably it is a bug. Please attach your script, I will take a look.",26/Sep/08 16:10;shravanmn;Just changed the default parallelism to be specified to 1 instead of the unspecified -1.,"26/Sep/08 19:26;olgan;patch committed; thanks, shravan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit produces results in the wrong order,PIG-461,12405172,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,olgan,olgan,25/Sep/08 19:01,24/Mar/10 22:04,14/Mar/19 03:05,26/Sep/08 15:44,0.2.0,,,,,,0.2.0,,,,,0,,,,"Script:

A = load 'studenttab200m' as (name, age, gpa);
B = filter A by age > 20;
C = group B by name;
D = foreach C generate group, COUNT(B) PARALLEL 16;
E = order D by $0 PARALLEL 16;
F = limit E 10;
--explain F;
dump F;

Output:

comes out not sorted on the name",,,,,,,,,,,,,,,,,,,26/Sep/08 00:53;alangates;PIG-461.patch;https://issues.apache.org/jira/secure/attachment/12390983/PIG-461.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-26 00:53:27.236,,,no_permission,,,,,,,,,,,,164074,,,,,Fri Sep 26 15:44:19 UTC 2008,,,,,,,0|i0gkin:,94754,,,,,,,,,,26/Sep/08 00:53;alangates;Fixed MRCompiler and JobControlCompiler so that limit that is added to the end to get a single reduce will use the same sort partitioner as the order by above it.,26/Sep/08 04:14;olgan;+1,"26/Sep/08 06:54;daijy;Looks good to me, thanks Alan",26/Sep/08 15:44;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
increase sleep interval for checking job completion,PIG-459,12405110,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,25/Sep/08 00:06,24/Mar/10 22:04,14/Mar/19 03:05,25/Sep/08 16:21,0.2.0,,,,,,0.2.0,,,,,0,,,,we seem to see more RPC timeout with Pig using JobControl. Looks like we are generating more traffic to JobTracker. I am going to increase sleep time from 500 ms to 5 sec.,,,,,,,,,,,,,,,,,,,25/Sep/08 00:41;olgan;PIG-459.patch;https://issues.apache.org/jira/secure/attachment/12390894/PIG-459.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,164072,,,,,Thu Sep 25 16:21:54 UTC 2008,,,,,,,0|i0gkhz:,94751,,,,,,,,,,25/Sep/08 16:21;olgan;patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig produces errors after a job is said to be 100% done,PIG-457,12405092,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,shravanmn,olgan,olgan,24/Sep/08 18:14,24/Mar/10 22:04,14/Mar/19 03:05,06/Oct/08 18:05,0.2.0,,,,,,0.2.0,,,,,0,,,,It is possible that we get errors for all tasks even the ones we retried. Need to look at the code that handles detecting end of processing and producing errors.,,,,,,,,,,,,,,,,,,,26/Sep/08 10:31;shravanmn;457-2.patch;https://issues.apache.org/jira/secure/attachment/12391009/457-2.patch,26/Sep/08 19:03;alangates;457-3.patch;https://issues.apache.org/jira/secure/attachment/12391043/457-3.patch,26/Sep/08 22:54;alangates;457-4.patch;https://issues.apache.org/jira/secure/attachment/12391068/457-4.patch,30/Sep/08 17:27;shravanmn;457-part2.patch;https://issues.apache.org/jira/secure/attachment/12391222/457-part2.patch,06/Oct/08 06:13;shravanmn;457-prog.patch;https://issues.apache.org/jira/secure/attachment/12391516/457-prog.patch,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2008-09-26 10:30:59.61,,,no_permission,,,,,,,,,,,,164070,,,,,Mon Oct 06 18:05:46 UTC 2008,,,,,,,0|i0gkgv:,94746,,,,,,,,,,"26/Sep/08 10:30;shravanmn;There are two issues that this patch tries to address:
1) Exceptions and traces even after a successful completion:
Currently, we have the same code path for both the success case & failure case for getting & printing error messages. So this fix breaks the code path to use debug for failures in a successful completion which are solved by retries & to use error for failures in an unsuccessful run.

2) Shows 100% even if there are failures
This is a direct result of what hadoop does. It marks the map and reduce tasks as 100 % complete irrespective of their success or failure. In some sense these are unrelated dimensions. Since its better to relate these two, we need to make sure that we don't report 100% complete in case of a failed execution. This is a hack where I check if the progress has become 100% and postpone its display till I am sure that the job has completed successfully.

There are some other fixes to the completion percentage display logic which displays the percentage completion. In the code as we are chasing a moving target and when we assume that the job is in a particular state & try to do some processing based on that assumption, we might get spurious results. One example is we get the list of running jobs and try to get the progress for each job. While doing this, the state of this job might change from running to something else and its not easy to construct all the possible scenarios into the code. Thus when we try to fetch the progress of a previously running job which has changed state, we will get spurious results. To mitigate this, we make a simple assumption that the job can't regress and if we see such a condition, we ignore it as we know its temporary.

Another thing that has been introduced into the logic is an exponential delay scheme which will be useful when we are in a job which is not progressing may be due to bag spilling or some udf running. In this case each progress reported is the same for some time. During this time, we can either implement something where we hard limit saying if we don't see progress we don't report it or we can just report the same progress. There are cons with both approaches: for 1) it might seem like the job is stuck or there is processing happening if we don't display anything. for 2)its surely going to fill the screen with something that is not adding any more information. So we try to introduce delays between each batch of same progress display which increase exponentially with each batch completing. Currently the batch size is half the number of retries which is 6 since sleep time is 5 sec now; like trying to have a progress reported every 30 sec but delaying future displays of the same progress using an exponential delay scheme.",26/Sep/08 19:03;alangates;I propose we split this patch into two.  The first part I think we all agree on and we want to get it soon.  The second part I think we want to review further.  I've tried to split out the stuff for the first part and attached it in this 457-3.patch.,26/Sep/08 19:08;olgan;+1,"26/Sep/08 19:56;shravanmn;The patch also removes the fix which resolves 2 where it shows hundred percent even after failures. Also, pls call jc.stop before returning false from the failure case.  Also, the LocalJobRunner.compile has the StreamVisitor thingy missing which I think causes the stream tests in local mode to fail. If thats actually the case, i would keep that in too.

Optionally we might want to keep the check that discards progress which is decreasing because it causes the condition we have there of prog<lastProg to fail. The progress is like x and lastProg is also x. This goes on for a while and suddenly, probably during failure, the prog becomes zero which causes the condition to hold in the next iteration and makes us print the same progress again and again.

The exponential backoff needs some review I agree","26/Sep/08 22:54;alangates;Here's another whack at splitting the patch.  I addressed Shravan's request to add in the MRStreaming stuff into LocalLauncher, and put in the jc.stop() calls after failed jobs in both Local and MapReduceLauncher.",26/Sep/08 23:33;olgan;+1,"27/Sep/08 00:02;alangates;PIG-457-4 has been checked in.  I'm not closing the bug because this doesn't address the issue of reporting 100% progress and then reporting failures, which is also part of the bug.",30/Sep/08 17:27;shravanmn;Separated the 2nd part,"01/Oct/08 22:54;olgan;I reviewed the current code and the proposed changes.

The current code looks fine and should be doing the right thing. So it seems like the patch is adding quite a bit of complexity to hide some sort of issues with hadoop progress.

I winder if the easiest change for now would be to check the progress and if it adds to 100% not to report it until we check that the job successfully finished.","02/Oct/08 01:03;shravanmn;Actually, the final progress check is exactly that. It doesn't report 100% until it is also sure that the job completed successfully. However, there are 2 other changes: 1) to mask some issue with hadoop progress 2) When the issue (1) is masked we would not see any kind of progress if the job is not showing progress after a certain point of time. So instead appearing frozen, there is some logic which displays the same percentage with an exponentially increasing delay between each report of the same progress just to say that we are alive. Once we get a different progress all counters are reset and it resumes normal progress reporting until the progress is blocked again. (Not exactly right but just to simplify understanding. It actually adds exponential delay between a bunch of same progress reports).","02/Oct/08 01:14;olgan;Yes, I realize that but I question whether we need to add this complexity. My preference is just extract the part of the patch that delays 100% reporting",03/Oct/08 18:49;shravanmn;just delaying the progress part.,"03/Oct/08 18:59;olgan;shravan, we do want to display 100% once the job successfully finishes. I don't the latest version of the patch does that.","06/Oct/08 06:13;shravanmn;Sorry, missed that one. The new patch with the changes for the progress","06/Oct/08 18:05;olgan;patch committed; thanks, shravan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""group"" alias is lost after a flatten(group)",PIG-455,12405029,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,24/Sep/08 01:04,24/Mar/10 22:04,14/Mar/19 03:05,24/Sep/08 22:37,0.2.0,,,,,,0.2.0,,,,,0,,,,"script:
{code}
a = load 'st10k' as (name, age, gpa);
b = group a by name;
c = foreach b generate flatten(group), COUNT(a) as cnt;
d = foreach c generate group;
dump d;
{code}

Error:
{noformat}
java.io.IOException: Invalid alias: group in {bytearray,cnt: long}
        at org.apache.pig.PigServer.parseQuery(PigServer.java:293)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:258)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:432)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:242)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:83)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:64)
        at org.apache.pig.Main.main(Main.java:306)
Caused by: org.apache.pig.impl.logicalLayer.parser.ParseException: Invalid alias: group in {bytearray,cnt: long}
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.AliasFieldOrSpec(QueryParser.java:5851)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.ColOrSpec(QueryParser.java:5709)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseEvalSpec(QueryParser.java:4012)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.UnaryExpr(QueryParser.java:3909)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.CastExpr(QueryParser.java:3863)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.MultiplicativeExpr(QueryParser.java:3772)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.AdditiveExpr(QueryParser.java:3698)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.InfixExpr(QueryParser.java:3664)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItem(QueryParser.java:3590)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItemList(QueryParser.java:3500)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.GenerateStatement(QueryParser.java:3457)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.NestedBlock(QueryParser.java:2933)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.ForEachClause(QueryParser.java:2336)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:973)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:748)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:549)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:60)
        at org.apache.pig.PigServer.parseQuery(PigServer.java:290)
        ... 6 more

{noformat}",,,,,,,,,,,,,,,,,,,24/Sep/08 21:50;pkamath;PIG-455.patch;https://issues.apache.org/jira/secure/attachment/12390872/PIG-455.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-24 22:37:33.467,,,no_permission,,,,,,,,,,,,164068,,,,,Wed Sep 24 22:37:33 UTC 2008,,,Patch Available,,,,0|i0gkfz:,94742,,,,,,,,,,24/Sep/08 21:50;pkamath;Submitted patch,24/Sep/08 22:37;olgan;patch committed; thanks pradeep,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
group by followed by group ALL causes error in reduce,PIG-454,12405028,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,pkamath,pkamath,24/Sep/08 01:00,24/Mar/10 22:04,14/Mar/19 03:05,25/Sep/08 16:21,0.2.0,,,,,,0.2.0,,,,,0,,,,"Script:
{code}
a = load 'st10k' as (name, age, gpa);
b = group a by name;
c = foreach b generate flatten(group), COUNT(a) as cnt;
d = group c all;
e = foreach d generate AVG(c.cnt);
dump e;
{code}

Error:
{noformat}
2008-09-23 17:58:12,002 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Job failed!
2008-09-23 17:58:12,004 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher - Error message from task (map) tip_200809051428_0117_m_000000java.io.IOException: wrong key class: org.apache.pig.impl.io.NullableTuple is not class org.apache.pig.impl.io.NullableText
        at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:995)
        at org.apache.hadoop.mapred.MapTask$CombineOutputCollector.collect(MapTask.java:1079)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.reduce(PigCombiner.java:155)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.reduce(PigCombiner.java:56)
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.combineAndSpill(MapTask.java:872)
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:779)
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:691)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:220)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2124)

...
{noformat}",,,,,,,,,,,,,,,,,,,25/Sep/08 00:19;alangates;PIG-454.patch;https://issues.apache.org/jira/secure/attachment/12390892/PIG-454.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-25 00:19:05.103,,,no_permission,,,,,,,,,,,,164067,,,,,Thu Sep 25 16:21:23 UTC 2008,,,,,,,0|i0gkfr:,94741,,,,,,,,,,"25/Sep/08 00:19;alangates;CombinerOptimizer is a visitor that walks the entire plan of MapReduceOpers.  It was not resetting state as it visited each operator, causing it to get confused on the key to set in the combiner in cases where there were multiple ops that could use the combiner.",25/Sep/08 00:25;olgan;+1,25/Sep/08 16:21;olgan;patch is in,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issues when non existent columns are projected,PIG-452,12405014,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,pkamath,pkamath,23/Sep/08 21:26,24/Mar/10 22:04,14/Mar/19 03:05,24/Sep/08 18:39,0.2.0,,,,,,0.2.0,,,,,0,,,,"Script:
{code}
-- columns x,y,z do not exist
a = load 'st10k' as (name, age, gpa, x, y, z);
b = load 'st10k' as (name, age:chararray, gpa);
c = join a by (name, y), b by (name, age);
dump c;

{code}

Error:
{noformat}
2008-09-23 14:22:20,237 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Job failed!
2008-09-23 14:22:20,253 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher - Error message from task (map) tip_200809051428_0112_m_000000java.io.IOException: Received Error while processing the map plan.
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:197)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:158)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:79)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2124)

2008-09-23 14:22:20,253 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher - Error message from task (map) tip_200809051428_0112_m_000000java.io.IOException: Received Error while processing the map plan.
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:197)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:158)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:79)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2124)

2008-09-23 14:22:20,253 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher - Error message from task (map) tip_200809051428_0112_m_000000java.io.IOException: Received Error while processing the map plan.
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:197)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:158)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:79)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2124)

2008-09-23 14:22:20,259 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher - Error message from task (map) tip_200809051428_0112_m_000000java.io.IOException: Received Error while processing the map plan.
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:197)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:158)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:79)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2124)

java.io.IOException: Unable to open iterator for alias: c [Job terminated with anomalous status FAILED]
        at org.apache.pig.PigServer.openIterator(PigServer.java:384)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:268)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:176)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:83)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:64)
        at org.apache.pig.Main.main(Main.java:306)
Caused by: java.io.IOException: Job terminated with anomalous status FAILED
        ... 6 more

{noformat}",,,,,,,,,,,,,,,,,,,24/Sep/08 18:02;alangates;PIG-452.patch;https://issues.apache.org/jira/secure/attachment/12390853/PIG-452.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-24 18:02:15.24,,,no_permission,,,,,,,,,,,,164066,,,,,Wed Sep 24 18:39:37 UTC 2008,,,,,,,0|i0gkev:,94737,,,,,,,,,,24/Sep/08 18:02;alangates;Changed POProject to return STATUS_OK instead of STATUS_NULL.,24/Sep/08 18:07;olgan;+1,24/Sep/08 18:39;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If an field is part of group followed by flatten, then referring to it causes a parse error",PIG-451,12405009,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,23/Sep/08 20:21,24/Mar/10 22:04,14/Mar/19 03:05,24/Sep/08 20:35,0.2.0,,,,,,0.2.0,,,,,0,,,,"Script which causes error:
{code}
a = load 'st10k' as (name, age, gpa);
b = group a by name;
c = foreach b generate flatten(a);
d = filter c by name != 'fred';
e = group d by name;
f = foreach e generate flatten(d);
g = foreach f generate name;
{code}

Error got:
{noformat}
java.io.IOException: Found more than one match: a::name, d::a::name
        at org.apache.pig.PigServer.parseQuery(PigServer.java:293)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:258)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:432)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:242)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:83)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:64)
        at org.apache.pig.Main.main(Main.java:306)
Caused by: org.apache.pig.impl.logicalLayer.parser.ParseException: Found more than one match: a::name, d::a::name
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.AliasFieldOrSpec(QueryParser.java:5854)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.ColOrSpec(QueryParser.java:5709)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseEvalSpec(QueryParser.java:4012)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.UnaryExpr(QueryParser.java:3909)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.CastExpr(QueryParser.java:3863)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.MultiplicativeExpr(QueryParser.java:3772)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.AdditiveExpr(QueryParser.java:3698)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.InfixExpr(QueryParser.java:3664)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItem(QueryParser.java:3590)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItemList(QueryParser.java:3500)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.GenerateStatement(QueryParser.java:3457)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.NestedBlock(QueryParser.java:2933)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.ForEachClause(QueryParser.java:2336)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:973)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:748)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:549)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:60)
        at org.apache.pig.PigServer.parseQuery(PigServer.java:290)
        ... 6 more

{noformat}",,,,,,,,,,,,,,,,,,,23/Sep/08 22:15;pkamath;PIG-451.patch;https://issues.apache.org/jira/secure/attachment/12390794/PIG-451.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-24 20:35:15.197,,,no_permission,,,,,,,,,,,,164065,,,,,Wed Sep 24 20:35:15 UTC 2008,,,,,,,0|i0gkef:,94735,,,,,,,,,,23/Sep/08 22:15;pkamath;Attached patch to resolve the issue,"24/Sep/08 20:35;olgan;patch committed; thanks, pradeep",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PERFORMANCE:  Distinct should make use of combiner to remove duplicate values from keys.,PIG-450,12405001,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,23/Sep/08 19:31,24/Mar/10 22:04,14/Mar/19 03:05,24/Sep/08 18:14,0.2.0,,,,,,0.2.0,,impl,,,0,,,,In 2.0 distinct was improved by removing values in the map and just passing an empty tuple along with the key.  This can be further improved by adding a combiner step that passes along only the first empty tuple instead of all of them.,,,,,,,,,,,,,,,,,,,23/Sep/08 21:18;alangates;PIG-450.patch;https://issues.apache.org/jira/secure/attachment/12390788/PIG-450.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-24 01:06:40.773,,,no_permission,,,,,,,,,,,,164064,,,,,Wed Sep 24 18:14:06 UTC 2008,,,,,,,0|i0gkdz:,94733,,,,,,,,,,"23/Sep/08 21:18;alangates;This patch adds a combiner step to distincts that just removes the duplicate values so that less data is carried across from map to reduce.  Here are the resulting time differences (all times in seconds):

||Num records||Num keys||Num reducers||1.4 || 2.0 || 2.0 with this patch ||
| 200M | 60 | 1 | 2547 | 1388 | 142 |
| 200M | 16M | 50 | 384 | 227 | 231 |

The main benefit is with a small number of keys, but there does not appear to be a penalty with a larger number of keys.

",24/Sep/08 01:06;olgan;+1,24/Sep/08 18:14;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schemas for bags should contain tuples all the time,PIG-449,12404925,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,sms,sms,23/Sep/08 02:46,24/Mar/10 22:04,14/Mar/19 03:05,09/Dec/08 02:02,0.2.0,,,,,,0.2.0,,,,,1,,,,"The front end treats relations as operators that return bags.  When the schema of a load statement is specified, the bag is associated with the schema specified by the user. Ideally, the schema corresponds to the tuple contained in the bag. 

With PIG-380, the schema for bag constants are computed by the front end. The schema for the bag contains the tuple which in turn contains the schema of the columns. This results in errors when columns are accessed directly just like the load statements.

The front end should then treat access to the columns as a double dereference, i.e., access the tuple inside the bag and then the column inside the tuple.

{code}
grunt> a = load '/user/sms/data/student.data' using PigStorage(' ') as (name, age, gpa);
grunt> b = foreach a generate {(16, 4.0e-2, 'hello')} as b:{t:(i: int, d: double, c: chararray)};

grunt> describe b;
b: {b: {t: (i: integer,d: double,c: chararray)}}

grunt> c = foreach b generate b.i;
111064 [main] ERROR org.apache.pig.tools.grunt.GruntParser  - java.io.IOException: Invalid alias: i in {t: (i: integer,d: double,c: chararray)}
        at org.apache.pig.PigServer.parseQuery(PigServer.java:293)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:258)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:432)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:242)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:93)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:58)
        at org.apache.pig.Main.main(Main.java:282)
Caused by: org.apache.pig.impl.logicalLayer.parser.ParseException: Invalid alias: i in {t: (i: integer,d: double,c: chararray)}
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.AliasFieldOrSpec(QueryParser.java:5851)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.ColOrSpec(QueryParser.java:5709)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BracketedSimpleProj(QueryParser.java:5242)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseEvalSpec(QueryParser.java:4040)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.UnaryExpr(QueryParser.java:3909)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.CastExpr(QueryParser.java:3863)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.MultiplicativeExpr(QueryParser.java:3772)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.AdditiveExpr(QueryParser.java:3698)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.InfixExpr(QueryParser.java:3664)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItem(QueryParser.java:3590)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItemList(QueryParser.java:3500)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.GenerateStatement(QueryParser.java:3457)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.NestedBlock(QueryParser.java:2933)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.ForEachClause(QueryParser.java:2336)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:973)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:748)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:549)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:60)
        at org.apache.pig.PigServer.parseQuery(PigServer.java:290)
        ... 6 more

111064 [main] ERROR org.apache.pig.tools.grunt.GruntParser  - Invalid alias: i in {t: (i: integer,d: double,c: chararray)}
111064 [main] ERROR org.apache.pig.tools.grunt.GruntParser  - java.io.IOException: Invalid alias: i in {t: (i: integer,d: double,c: chararray)}
grunt> c = foreach b generate b.t;
grunt> describe c;
c: {t: {i: integer,d: double,c: chararray}}

{code}",,,,,,,,,,,,,,,,,,,08/Dec/08 23:02;pkamath;PIG-449.patch;https://issues.apache.org/jira/secure/attachment/12395600/PIG-449.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-26 22:47:40.208,,,no_permission,,,,,,,,,,,,164063,,,,,Tue Dec 09 02:02:12 UTC 2008,,,,,,,0|i0gkdb:,94730,,,,,,,,,,"26/Sep/08 22:47;pkamath;Another script snippet which shows the above inconsistency:
{code}
grunt> a = load 'bla' as (b:bag{t:(x,y,z)}, t:tuple(a,b,c), m:[]);
grunt> describe a;
a: {b: {t: (x: bytearray,y: bytearray,z: bytearray)},t: (a: bytearray,b: bytearray,c: bytearray),m: map[ ]}
grunt> b = foreach a generate b.y;
2008-09-26 15:35:22,673 [main] ERROR org.apache.pig.tools.grunt.GruntParser - java.io.IOException: Invalid alias: y in {t: (x: bytearray,y: bytearray,z: bytearray)}
        at org.apache.pig.PigServer.parseQuery(PigServer.java:293)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:258)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:434)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:242)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:94)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:58)
        at org.apache.pig.Main.main(Main.java:282)
Caused by: org.apache.pig.impl.logicalLayer.parser.ParseException: Invalid alias: y in {t: (x: bytearray,y: bytearray,z: bytearray)}
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.AliasFieldOrSpec(QueryParser.java:5851)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.ColOrSpec(QueryParser.java:5709)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BracketedSimpleProj(QueryParser.java:5242)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseEvalSpec(QueryParser.java:4040)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.UnaryExpr(QueryParser.java:3909)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.CastExpr(QueryParser.java:3863)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.MultiplicativeExpr(QueryParser.java:3772)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.AdditiveExpr(QueryParser.java:3698)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.InfixExpr(QueryParser.java:3664)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItem(QueryParser.java:3590)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItemList(QueryParser.java:3500)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.GenerateStatement(QueryParser.java:3457)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.NestedBlock(QueryParser.java:2933)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.ForEachClause(QueryParser.java:2336)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:973)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:748)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:549)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:60)
        at org.apache.pig.PigServer.parseQuery(PigServer.java:290)
        ... 6 more

2008-09-26 15:35:22,674 [main] ERROR org.apache.pig.tools.grunt.GruntParser - Invalid alias: y in {t: (x: bytearray,y: bytearray,z: bytearray)}
2008-09-26 15:35:22,674 [main] ERROR org.apache.pig.tools.grunt.GruntParser - java.io.IOException: Invalid alias: y in {t: (x: bytearray,y: bytearray,z: bytearray)}
grunt> b = foreach a generate b.t.y;
grunt> describe b; 
b: {y: {y: bytearray}} --> THIS ALSO LOOKS WRONG

{code}","11/Nov/08 22:51;sms;Currently, bags in Pig are containers of tuples. Accessing elements inside a bag should translate to accessing elements inside the tuple contained in the bag. In addition, accessing tuples inside a bag should be restricted to the FLATTEN keyword in a FOREACH statement. A few examples shown below will demonstrate the point.

{code}
a = load '/user/pig/data/student.data' using PigStorage(' ') as (name, age, gpa);
b = foreach a generate {(16, 4.0e-2, 'hello')} as b:{t:(i: int, d: double, c: chararray)};
c = foreach b generate b.i; -- Here b.i should generate a bag of integers by accessing the column called 'i' inside each tuple
d = foeach b generate b.t; -- This should be outlawed as the tuple inside the bag does not have a column called 't' although the tuple inside the bag are named 't'
{code}

Summary:

1. The frontend should translate access to columns in a bag to columns inside the tuple in the bag
2. The frontend should prevent access to tuples inside the bag via projections and allow access only via the FLATTEN keyword

Thoughts/suggestions/comments are welcome.","08/Dec/08 23:02;pkamath;A new flag has been introduced in schema to distinguish bag schemas which have only one field schema of a tuple containing a list of field schemas for the elements in the bag (these kind of bag schemas occur in two cases explained in the code comment below). This flag is will be used to solve the problems reported in this issue by resolving access to fields in such bags as access to the fields present in the inner tuple schema. This is explained in the comment for this flag pasted here for reference:
{noformat}
    // In bags which have a schema with a tuple which contains
    // the fields present in it, if we access the second field (say)
    // we are actually trying to access the second field in the
    // tuple in the bag. This is currently true for two cases:
    // 1) bag constants - the schema of bag constant has a tuple
    // which internally has the actual elements
    // 2) When bags are loaded from input data, if the user 
    // specifies a schema with the ""bag"" type, he has to specify
    // the bag as containing a tuple with the actual elements in 
    // the schema declaration. However in both the cases above,
    // the user can still say b.i where b is the bag and i is 
    // an element in the bag's tuple schema. So in these cases,
    // the access should translate to a lookup for ""i"" in the 
    // tuple schema present in the bag. To indicate this, the
    // flag below is used. It is false by default because, 
    // currently we use bag as the type for relations. However 
    // the schema of a relation does NOT have a tuple fieldschema
    // with items in it. Instead, the schema directly has the 
    // field schema of the items. So for a relation ""b"", the 
    // above b.i access would be a direct single level access
    // of i in b's schema. This is treated as the ""default"" case
    private boolean twoLevelAccessRequired = false;
{noformat}

The changes are in getPosition() in Schema.java to use the above flag to do a two level access whenever an access to the above kind of bag is involved. Besides this there are changes in getSchema() of LOForEach and getFieldSchema() of LOProject to use the inner tuple schema in cases of these kinds of bags. A new unit test case, TestDataBagAccess has also been added to test out various access scenarios for the above cases of bag schemas which have a tuple field schema with a list of item field schemas.
","09/Dec/08 02:02;olgan;patch committed; thanks, pradeep",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
load with schema followed by explain gives and exception,PIG-448,12404920,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,22/Sep/08 23:42,24/Mar/10 22:04,14/Mar/19 03:05,23/Sep/08 00:57,0.2.0,,,,,,0.2.0,,,,,0,,,,"grunt> a = load 'st10k' as (name:chararray, age:int, gpa:float);
grunt> explain a;                                               
2008-09-22 16:40:55,559 [main] ERROR org.apache.pig.tools.grunt.GruntParser - java.io.IOException: Unable to explain alias a [null]
        at org.apache.pig.PigServer.explain(PigServer.java:476)
        at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:154)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:186)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:93)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:58)
        at org.apache.pig.Main.main(Main.java:282)
Caused by: java.lang.NullPointerException
        ... 6 more

2008-09-22 16:40:55,560 [main] ERROR org.apache.pig.tools.grunt.GruntParser - Unable to explain alias a [null]
2008-09-22 16:40:55,560 [main] ERROR org.apache.pig.tools.grunt.GruntParser - java.io.IOException: Unable to explain alias a [null]
grunt> 
",,,,,,,,,,,,,,,,,,,23/Sep/08 00:48;pkamath;PIG-448.patch;https://issues.apache.org/jira/secure/attachment/12390707/PIG-448.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-23 00:57:17.275,,,no_permission,,,,,,,,,,,,164062,,,,,Tue Sep 23 00:57:17 UTC 2008,,,,,,,0|i0gkcv:,94728,,,,,,,,,,23/Sep/08 00:48;pkamath;Attached patch,"23/Sep/08 00:57;olgan;patch committed. thanks, pradeep",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validation exception when comapring incomparable types in filter,PIG-447,12404919,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,22/Sep/08 23:40,24/Mar/10 22:04,14/Mar/19 03:05,23/Sep/08 00:52,0.2.0,,,,,,0.2.0,,,,,0,,,,"Script which gives error:
{noformat}
grunt> a = load 'st10k' as (name:chararray, age:int, gpa:float);
grunt> b = filter a by $0 == 1;                                 
grunt> explain b;                                               
2008-09-22 16:38:24,286 [main] ERROR org.apache.pig.PigServer - Cannot evaluate output type of Equal/NotEqual Operator
2008-09-22 16:38:24,289 [main] ERROR org.apache.pig.PigServer - Severe problem found during validation org.apache.pig.impl.plan.PlanValidationException: An unexpected exception caused the validation to stop
2008-09-22 16:38:24,296 [main] ERROR org.apache.pig.tools.grunt.GruntParser - java.io.IOException: Unable to explain alias b [Cannot evaluate output type of Equal/NotEqual OperatorSevere problem found during validation org.apache.pig.impl.plan.PlanValidationException: An unexpected exception caused the validation to stop]
        at org.apache.pig.PigServer.compileLp(PigServer.java:693)
        at org.apache.pig.PigServer.compileLp(PigServer.java:631)
        at org.apache.pig.PigServer.explain(PigServer.java:466)
        at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:154)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:186)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:93)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:58)
        at org.apache.pig.Main.main(Main.java:282)
Caused by: org.apache.pig.backend.executionengine.ExecException: Cannot evaluate output type of Equal/NotEqual OperatorSevere problem found during validation org.apache.pig.impl.plan.PlanValidationException: An unexpected exception caused the validation to stop
        ... 8 more
Caused by: org.apache.pig.impl.plan.PlanValidationException: An unexpected exception caused the validation to stop
        at org.apache.pig.impl.plan.PlanValidator.validateSkipCollectException(PlanValidator.java:104)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingValidator.validate(TypeCheckingValidator.java:40)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingValidator.validate(TypeCheckingValidator.java:30)
        at org.apache.pig.impl.logicalLayer.validators.LogicalPlanValidationExecutor.validate(LogicalPlanValidationExecutor.java:79)
        at org.apache.pig.PigServer.compileLp(PigServer.java:660)
        ... 7 more
Caused by: org.apache.pig.impl.plan.VisitorException: Cannot evaluate output type of Equal/NotEqual Operator
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.visit(TypeCheckingVisitor.java:855)
        at org.apache.pig.impl.logicalLayer.LOEqual.visit(LOEqual.java:66)
        at org.apache.pig.impl.logicalLayer.LOEqual.visit(LOEqual.java:29)
        at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:68)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.checkInnerPlan(TypeCheckingVisitor.java:2081)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.visit(TypeCheckingVisitor.java:1629)
        at org.apache.pig.impl.logicalLayer.LOFilter.visit(LOFilter.java:101)
        at org.apache.pig.impl.logicalLayer.LOFilter.visit(LOFilter.java:32)
        at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:68)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.impl.plan.PlanValidator.validateSkipCollectException(PlanValidator.java:101)
        ... 11 more
{noformat}",,,,,,,,,,,,,,,,,,,23/Sep/08 00:22;pkamath;PIG-447.patch;https://issues.apache.org/jira/secure/attachment/12390704/PIG-447.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-23 00:52:06.76,,,no_permission,,,,,,,,,,,,164061,,,,,Tue Sep 23 00:52:06 UTC 2008,,,,,,,0|i0gkcf:,94726,,,,,,,,,,23/Sep/08 00:22;pkamath;The issue was with the error message being misleading - corrected error message for incompatible operands in all operators; patch attached.,"23/Sep/08 00:52;olgan;patch committed. thanks, pradeep",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problem when schema contains more columns that actual data,PIG-446,12404918,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,olgan,olgan,22/Sep/08 23:40,24/Mar/10 22:04,14/Mar/19 03:05,23/Sep/08 19:28,0.2.0,,,,,,0.2.0,,,,,0,,,,"Script:

-- extra1 and extra2 are not present in the data
A = load 'data' as (name, age: int, gpa, extra1, extra2);
B = limit A 10;
dump B;

Error:

 Out of bounds access: Request for field number 3 exceeds tuple size of 3
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:223)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:121)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:391)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2124)

",,,,,,,,,,,,,,,,,,,23/Sep/08 05:17;alangates;PIG-446.patch;https://issues.apache.org/jira/secure/attachment/12390717/PIG-446.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-23 05:17:47.947,,,no_permission,,,,,,,,,,,,164060,,,,,Tue Sep 23 19:28:42 UTC 2008,,,,,,,0|i0gkbz:,94724,,,,,,,,,,23/Sep/08 05:17;alangates;Changed POProject to catch exceptions from Tuple.get(int) and insert a null instead of pass the exceptions along.,23/Sep/08 16:10;olgan;+1. My only concer is that if there is enough data like this we will be logging a lot. Buit we already doing this with UDFs so at least this is consistent. Once we address logging we could aggregate warnings.,23/Sep/08 19:28;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null Pointer Exceptions in the mappers leading to lot of retries,PIG-445,12404905,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,shravanmn,shravanmn,shravanmn,22/Sep/08 19:56,25/Mar/10 00:12,14/Mar/19 03:05,22/Sep/08 21:30,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"Even with successfully completed jobs, usually with a large data set, we see that there are NPE produced in the mappers which lead to task failure. However, this problem goes away on retries. The problem occurs at places where we access the reporter to report progress. 

From the analysis, this should happen with jobs that use combiner. The combiner is called whenever the mapper outputs a buffer full of data. So the combiner is called multiple times in between a map task. In the Combiner.close method we currently set the reporter to null as it was assumed that combiner is called only after the entire output of map is produced. 

The fix is to not set the reporter to null in the Combiner.close() method",,,,,,,,,,,,,,,,,,,22/Sep/08 20:25;shravanmn;mq.patch;https://issues.apache.org/jira/secure/attachment/12390680/mq.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-22 21:30:58.577,,,no_permission,,,,,,,,,,,,38498,,,,,Mon Sep 22 21:30:58 UTC 2008,,,Patch Available,,,,0|i0gkbj:,94722,,,,,,,,,,"22/Sep/08 21:30;olgan;patch committed. thanks, shravan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Config files left behind,PIG-444,12404892,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,olgan,olgan,22/Sep/08 17:15,24/Mar/10 22:04,14/Mar/19 03:05,22/Sep/08 22:24,0.2.0,,,,,,0.2.0,,,,,0,,,,Looks like files that we send to hadoop like Job.jar are not cleaned up.,,,,,,,,,,,,,,,,,,,22/Sep/08 21:36;pkamath;PIG-444.patch;https://issues.apache.org/jira/secure/attachment/12390686/PIG-444.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-22 21:36:04.099,,,no_permission,,,,,,,,,,,,164059,,,,,Mon Sep 22 22:24:53 UTC 2008,,,,,,,0|i0gkb3:,94720,,,,,,,,,,22/Sep/08 21:36;pkamath;Patch to fix cleanup of JobXXX.jar,22/Sep/08 22:24;olgan;patch committed. Thanks pradeep,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Order by desc does not work with more than one reducer,PIG-441,12404771,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,19/Sep/08 19:02,24/Mar/10 22:04,14/Mar/19 03:05,23/Sep/08 00:49,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"Order by with desc works if you use a single reducer.  But with more than one reducer the partitions are not correctly constructed.  The SortParitioner uses the object comparators, not the raw the comparators.  The object comparators have no concept of order by descending.  So they build the partitions for ascending order.  The end result is data sorted descending within the partition but not across partitions.",,,,,,,,,,,,,,,,,,,22/Sep/08 22:07;alangates;PIG-441.patch;https://issues.apache.org/jira/secure/attachment/12390692/PIG-441.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-22 22:31:59.955,,,no_permission,,,,,,,,,,,,164056,,,,,Tue Sep 23 00:49:44 UTC 2008,,,,,,,0|i0gk9r:,94714,,,,,,,,,,"22/Sep/08 22:07;alangates;This patch does a couple of things:

1) add object comparators to the PigXRawComparator classes so that in places where hadoop uses the object instead of null comparators the desc behavior can still be handled.

2) Change FindQuantiles UDF to take in its constructor an array that indicates ascending vs descending order so that the quantiles can be correclty computed.  ",22/Sep/08 22:31;olgan;+1,23/Sep/08 00:49;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exceptions from UDFs inside a foreach are not captured,PIG-440,12404770,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,19/Sep/08 18:53,24/Mar/10 22:04,14/Mar/19 03:05,20/Sep/08 00:45,0.2.0,,,,,,0.2.0,,,,,0,,,,"The following code runs to completion without an error though the UDF throws an exceptions:
{code}

register UdfThrowsException.jar;

a = load 'st10k';
b = load 'st10k';
c = cogroup a by $0, b by $0;
d = foreach c generate flatten(udfThrowsException((a,b));
dump d;

{code}",,,,,,,,,,,,,,,,,,,19/Sep/08 23:42;pkamath;PIG-440.patch;https://issues.apache.org/jira/secure/attachment/12390556/PIG-440.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-20 00:45:47.274,,,no_permission,,,,,,,,,,,,164055,,,,,Sat Sep 20 00:45:47 UTC 2008,,,,,,,0|i0gk9b:,94712,,,,,,,,,,19/Sep/08 23:42;pkamath;Patch for the issue,20/Sep/08 00:45;olgan;patch committed; thanks pradeep,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Currently we do not support A=B; correctly - for now capture this case and produce a meaningful message,PIG-439,12404686,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,18/Sep/08 21:50,24/Mar/10 22:04,14/Mar/19 03:05,19/Sep/08 22:43,0.2.0,,,,,,0.2.0,,,,,0,,,,Currently we do not support A=B; correctly - for now capture this case and produce a meaningful message - A separate JIRA-438 has been created to fix the main issue,,,,,,,,,,,,,,,,,,,19/Sep/08 00:09;pkamath;PIG-439.patch;https://issues.apache.org/jira/secure/attachment/12390439/PIG-439.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-19 22:43:09.74,,,no_permission,,,,,,,,,,,,164054,,,,,Fri Sep 19 22:43:09 UTC 2008,,,,,,,0|i0gk8n:,94709,,,,,,,,,,19/Sep/08 00:09;pkamath;Attached patch for the issue,19/Sep/08 22:43;olgan;patch committed. thanks pradeep,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle realiasing of existing Alias (A=B;) ,PIG-438,12404685,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,jcoveney,pkamath,pkamath,18/Sep/08 21:49,26/Apr/12 20:32,14/Mar/19 03:05,16/Mar/12 23:46,0.2.0,,,,,,0.10.0,0.11,,,,1,,,,"We do not handle re-aliasing of an existing alias - this should be handled correctly.

The following script should work:
{code}

a = load 'studenttab10k';
b = filter a by $1 > '25';
c = b;

-- use b
d = cogroup b by $0, a by $0;
e = foreach d generate flatten(b), flatten(a);
dump e

-- use c
f = cogroup c by $0, a by $0;
g = foreach f generate flatten(c), flatten(a);
dump g;

{code}",,,,,,,,,,,,,,,,,,,13/Mar/12 18:45;daijy;PIG-438-2.patch;https://issues.apache.org/jira/secure/attachment/12518213/PIG-438-2.patch,10/Feb/12 19:15;jcoveney;PIG-438.patch;https://issues.apache.org/jira/secure/attachment/12514143/PIG-438.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-02-10 19:14:24.131,,,no_permission,,,,,,,,,,,,46119,Reviewed,,,,Fri Mar 16 23:46:44 UTC 2012,,,,,,,0|i0gk87:,94707,,,,,,,,,,"10/Feb/12 19:14;jcoveney;I actually have a patch that does this pretty trivially. It just takes B = A; and converts it to B = FOREACH A GENERATE *; The only issue is that this affects some error messages and whatnot, and I wasn't able to fix that. Attaching.","10/Feb/12 19:15;jcoveney;It works to realias, but some tests fail and I'm not sure how to best fix them.","10/Feb/12 20:34;russell.jurney;This is pretty cool.  This would address some of my issues outlined in PIG-2511

","10/Feb/12 22:28;jcoveney;For clarification, the tests that fail have nothing to do with correctness, but rather that some error messages get changed by the changes in the parser.","15/Feb/12 01:08;jcoveney;The only test that fails is:

testMissingSemicolon in TestLogicalPlanBuilder

The issue is that now that new_relation = old_relation; is valid, if you leave off the semicolon, it sees the error there, instead of elsewhere. Is it ok to change this accordingly, or should I try and finagle it so that the error will be the same? (no clue how I'd do that)

I can add some tests as well, but that's the blocker for me.","21/Feb/12 21:56;dvryaboy;Jonathan, what's the error that is now being thrown in testMissingSemicolon? If it's a sensible message , we can change the test to that. If it's something that totally obscures the actual missing semicolon error, we should consider how to make errors for this kind of case more meaningful.","21/Feb/12 22:14;jcoveney;Ah, I thought I had included that.

The old:
{code}
mismatched input 'B' expecting SEMI_COLON
{code}

The new:
{code}
ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1200: <line 1, column 4>  Syntax error, unexpected symbol at or near 'load'
{code}",13/Mar/12 18:45;daijy;Attach a different fix.,"13/Mar/12 19:05;jcoveney;Daniel: in general, is it a better practice to add operators instead of expressing them in terms of other operators?","13/Mar/12 19:13;daijy;Not in general, but in this case, adding a foreach seems to be unnecessary, we only need to update the alias map.",15/Mar/12 22:03;thejas;+1,"16/Mar/12 23:46;daijy;test-patch:
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 9 new or modified tests.
     [exec] 
     [exec]     -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

javadoc warning is not related.

Patch committed to 0.10/trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"parser gets confused if ""as"" is used inside of expression",PIG-437,12404664,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,sms,olgan,olgan,18/Sep/08 17:14,24/Mar/10 22:04,14/Mar/19 03:05,30/Sep/08 20:56,0.2.0,,,,,,0.2.0,,,,,0,,,,"Script:

grunt> a = load 'foo' as (foo, fas);
grunt> b = group a by foo;
grunt> c = foreach b generate group, SUM(a.fas);
>>

Parser things that last line is incomplete.",,,,,,,,,,,,,,,,,,,30/Sep/08 00:55;sms;PIG-437.patch;https://issues.apache.org/jira/secure/attachment/12391179/PIG-437.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-30 00:55:32.142,,,no_permission,,,,,,,,,,,,38554,,,,,Tue Sep 30 20:56:31 UTC 2008,,,Patch Available,,,,0|i0gk7r:,94705,,,,,,,,,,30/Sep/08 00:55;sms;Attached patch that fixes the AS issue. It also fixes a similar problem with generate,"30/Sep/08 20:56;olgan;patch committed; thanks, santhosh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When a single column is flattened, the alias is lost in subsequent statements that refer to the alias",PIG-436,12404611,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,sms,sms,18/Sep/08 00:01,24/Mar/10 22:04,14/Mar/19 03:05,18/Sep/08 21:34,0.2.0,,,,,,0.2.0,,,,,0,,,,"When a single column is flattened, the front end generates aliases that disambiguate the columns in the flattened column. Subsequent statements that refer to this column always need to refer to the unambiguous alias even though there could be no ambiguity. A reproducible use case is given below:

{code}
a = load 'one' as (name, age, gpa);
b = group a by name;
c = foreach b generate flatten(a);
d = foreach c generate name;
e = foreach d generate name;

1971337 [main] ERROR org.apache.pig.tools.grunt.GruntParser  - java.io.IOException: Invalid alias: name in {a::name: bytearray}
        at org.apache.pig.PigServer.parseQuery(PigServer.java:293)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:258)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:432)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:242)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:93)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:58)
        at org.apache.pig.Main.main(Main.java:282)
Caused by: org.apache.pig.impl.logicalLayer.parser.ParseException: Invalid alias: name in {a::name: bytearray}
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.AliasFieldOrSpec(QueryParser.java:5818)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.ColOrSpec(QueryParser.java:5677)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseEvalSpec(QueryParser.java:3969)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.UnaryExpr(QueryParser.java:3866)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.CastExpr(QueryParser.java:3820)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.MultiplicativeExpr(QueryParser.java:3729)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.AdditiveExpr(QueryParser.java:3653)
{code}",,,,,,,,,,,,,,,,,,,18/Sep/08 18:32;pkamath;PIG-436.patch;https://issues.apache.org/jira/secure/attachment/12390399/PIG-436.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-18 18:32:03.121,,,no_permission,,,,,,,,,,,,164053,,,,,Thu Sep 18 21:34:48 UTC 2008,,,,,,,0|i0gk7b:,94703,,,,,,,,,,18/Sep/08 18:32;pkamath;Attached patch to resolve the issue - new unit tests included in the patch,"18/Sep/08 21:34;olgan;patch committed; thanks, pradeep!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AND and OR do not give right results with nulls,PIG-434,12404469,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,16/Sep/08 21:41,24/Mar/10 22:04,14/Mar/19 03:05,17/Sep/08 15:26,0.2.0,,,,,,0.2.0,,,,,0,,,,"Here are the truth tables for AND and OR - currently we do not short circuit and return a null if either operand is null (for both AND and OR)

{noformat}

        truth table for AND 
        t = true, n = null, f = false
        AND  t n f
              t    t n f
             n   n n f
             f    f f f


        truth table for OR 
        t = true, n = null, f = false
        OR   t n f
             t    t t t
            n    t n n
            f     t n f

{noformat}",,,,,,,,,,,,,,,,,,,16/Sep/08 22:35;pkamath;PIG-434.patch;https://issues.apache.org/jira/secure/attachment/12390224/PIG-434.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-17 15:26:37.422,,,no_permission,,,,,,,,,,,,38599,,,,,Wed Sep 17 15:26:37 UTC 2008,,,Patch Available,,,,0|i0gk6f:,94699,,,,,,,,,,16/Sep/08 22:35;pkamath;Attached patch to fix the issue,"17/Sep/08 15:26;olgan;patch committed, thanks, pradeep",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
aliasing flatten requires parenthesis around alias,PIG-433,12404454,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,olgan,olgan,16/Sep/08 16:23,24/Mar/10 22:04,14/Mar/19 03:05,18/Sep/08 00:40,0.2.0,,,,,,0.2.0,,,,,0,,,,"Script:

a = load 'data' as (name, age, gpa);
b = foreach a generate flatten(name) as foo;

Error:

08/09/16 09:21:19 ERROR grunt.GruntParser: java.io.IOException: Encountered ""foo"" at line 1, column 41.
Was expecting:
    ""("" ...
    
        at org.apache.pig.PigServer.parseQuery(PigServer.java:293)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:258)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:432)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:242)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:93)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:58)
        at org.apache.pig.Main.main(Main.java:282)
Caused by: org.apache.pig.impl.logicalLayer.parser.ParseException: Encountered ""foo"" at line 1, column 41.
Was expecting:
    ""("" ...
    
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.generateParseException(QueryParser.java:7656)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.jj_consume_token(QueryParser.java:7533)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItem(QueryParser.java:3522)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItemList(QueryParser.java:3456)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.GenerateStatement(QueryParser.java:3413)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.NestedBlock(QueryParser.java:2899)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.ForEachClause(QueryParser.java:2302)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:953)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:728)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:529)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:60)
        at org.apache.pig.PigServer.parseQuery(PigServer.java:290)
        ... 6 more

This a regression!",,,,,,,,,,,,,,,,,,,17/Sep/08 20:11;sms;PIG-433.patch;https://issues.apache.org/jira/secure/attachment/12390299/PIG-433.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-17 20:11:10.428,,,no_permission,,,,,,,,,,,,164052,,,,,Thu Sep 18 00:40:00 UTC 2008,,,Patch Available,,,,0|i0gk67:,94698,,,,,,,,,,"17/Sep/08 20:11;sms;Attached patch (PIG-433.patch) fixes the following:

1. Parser now handles the AS clause with flatten without requiring the additional ( ). Without the parenthesis, only the first column will be re-aliased
2. When bags or tuples without schemas are flattened and user provides an alias, the default type is set to bytearray. This part was omitted in an earlier fix.

All unit tests pass.","18/Sep/08 00:40;olgan;patch committed. thanks, santhosh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
When the specified load function cannot be found the error message is totally incomprehensible.,PIG-431,12404383,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,alangates,alangates,15/Sep/08 19:36,24/Mar/10 22:04,14/Mar/19 03:05,16/Sep/08 22:32,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"""a = load ':INPATH:/singlefile/studenttab10k' using NoSuchFunction(':');

In Pig 1.x the resulting error message was:

Could not resolve NoSuchFunction

In 2.0 instead the user gets

java.lang.ClassCastException: java.io.IOException
    at org.apache.pig.impl.logicalLayer.parser.QueryParser.LoadClause(QueryParser.java:1104)
    at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:869)
    at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:728)
    at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:529)
    at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:60)
    at org.apache.pig.PigServer.parseQuery(PigServer.java:290)
    at org.apache.pig.PigServer.registerQuery(PigServer.java:258)
    at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:432)
    at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:242)
    at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:83)
    at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:64)
    at org.apache.pig.Main.main(Main.java:306)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-09-15 21:59:47.051,,,no_permission,,,,,,,,,,,,164050,,,,,Tue Sep 16 22:32:48 UTC 2008,,,,,,,0|i0gk5b:,94694,,,,,,,,,,"15/Sep/08 21:59;pkamath;As part of my patch for PIG-269 (to merge PIG-183 from trunk to types branch) I have fixed this. I will submit the fix in that issue.

Here is what you will see now:
[pradeepk@chargesize:~/dev/pig-apache/pig/branches/types]java -cp pig.jar:$localc org.apache.pig.Main
2008-09-15 14:56:49,712 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: localhost:9000
2008-09-15 14:56:49,778 [main] WARN  org.apache.hadoop.fs.FileSystem - ""localhost:9000"" is a deprecated filesystem name. Use ""hdfs://localhost:9000/"" instead.
2008-09-15 14:56:50,184 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to map-reduce job tracker at: localhost:9001
2008-09-15 14:56:50,305 [main] WARN  org.apache.hadoop.fs.FileSystem - ""localhost:9000"" is a deprecated filesystem name. Use ""hdfs://localhost:9000/"" instead.
grunt> a = load 'st10k' using NoSuchFunction(':');                              
java.lang.Error: java.io.IOException: Cannot instantiate:NoSuchFunction [Cannot instantiate:NoSuchFunction]
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.LoadClause(QueryParser.java:1088)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:869)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:728)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:529)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:60)
        at org.apache.pig.PigServer.parseQuery(PigServer.java:290)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:258)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:432)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:242)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:93)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:58)
        at org.apache.pig.Main.main(Main.java:282)
Caused by: java.io.IOException: Cannot instantiate:NoSuchFunction [Cannot instantiate:NoSuchFunction]
        at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:405)
        at org.apache.pig.impl.logicalLayer.LOLoad.<init>(LOLoad.java:64)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.LoadClause(QueryParser.java:1086)
        ... 11 more
Caused by: java.lang.RuntimeException: Cannot instantiate:NoSuchFunction
        ... 14 more
Caused by: java.io.IOException: Could not resolve NoSuchFunction using imports: [, org.apache.pig.builtin., com.yahoo.pig.yst.sds.ULT., org.apache.pig.impl.builtin.] [Could not resolve NoSuchFunction using imports: [, org.apache.pig.builtin., com.yahoo.pig.yst.sds.ULT., org.apache.pig.impl.builtin.]]
        at org.apache.pig.impl.PigContext.resolveClassName(PigContext.java:389)
        at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:402)
        ... 13 more
Caused by: java.lang.ClassNotFoundException: Could not resolve NoSuchFunction using imports: [, org.apache.pig.builtin., com.yahoo.pig.yst.sds.ULT., org.apache.pig.impl.builtin.]
        ... 15 more


If this is fine, it can be close when that fix is committed",16/Sep/08 22:32;pkamath;Resolved as per previous comment,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Projections in nested filter and inside foreach do not work,PIG-430,12404283,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,sms,sms,12/Sep/08 23:08,24/Mar/10 22:04,14/Mar/19 03:05,22/Sep/08 20:20,0.2.0,,,,,,0.2.0,,,,,0,,,,"The following queries do not work:

Nested filter:

a = load 'studenttab10k' as (name, age, gpa);
b = filter a by age < 20;
c = group b by age;
d = foreach c { cf = filter b by gpa < 3.0; cp = cf.gpa; cd = distinct cp; co = order cd by $0; generate group, flatten(co); }
store d into 'output';

Nested Distinct:

a = load '/user/pig/tests/data/singlefile/studenttab10k' as (name, age, gpa);
b = group a by name;
c = foreach b { aa = distinct a.age; generate group, COUNT(aa); }
store c into 'output';",,,,,,,,,,,,,,,,,,,17/Sep/08 20:25;shravanmn;430-1.patch;https://issues.apache.org/jira/secure/attachment/12390300/430-1.patch,21/Sep/08 00:03;sms;PIG-430_2.patch;https://issues.apache.org/jira/secure/attachment/12390584/PIG-430_2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-09-17 20:25:05.602,,,no_permission,,,,,,,,,,,,38494,,,,,Mon Sep 22 20:20:09 UTC 2008,,,Patch Available,,,,0|i0gk4v:,94692,,,,,,,,,,"17/Sep/08 20:25;shravanmn;I have fixed part of the problem that addresses the project issue. The issue mentioned in distinct still remains. The problem here is that we see that projects are being introduced into the input of distinct which creates a unique case where the projection chaining will not work. The problem is similar to the one where you can assign a nested project to a variable inside a nested block. This has been solved by replacing the nested project with a foreach statement. The solution to the distinct problem should be something similar where the input to the distinct can also be a nested project. I made a local change by replacing BaseEvalSpec by NestedProject in my code for this and it works. However, I don't want to mess up something because I am not completely aware of the side-effects of changing this in the parser. Its better if someone more comfortable with the parser took a look at this one.

Also, I think there are some issues with the parsing of nested things. I tried the following and the parser just doesn't terminate the nested block waiting and keeps waiting for more input:

A = load 'file';
B = group A by $0;
C = foreach B { C1=distinct ""const""; generate C1;};

I was clueless as  to why this is happening but I tried this because I thought that the input to a nested distinct shouldn't be BaseEvalSpec which can FuncEvalSpecs and Constants. I think we need to change things a bit here.","17/Sep/08 20:34;sms;Replace the double quotes with the single quotes and it should work.

{code}
A = load 'file';
B = group A by $0;
C = foreach B { C1=distinct 'const'; generate C1;};
{code}","18/Sep/08 20:14;shravanmn;Hmm, I have hit a bug accidentally! Shouldn't it have terminated the nested block instead of waiting for more input when there is nothing to?","21/Sep/08 00:03;sms;New patch (PIG-430_2.patch), includes the rewrite of nested projects as foreach generates for the nested sort, nested distinct and nested filter inputs. Shravan's patch is included as part of this patch. His patch for fixing the BracketedProject projection was fine. Thanks Shravan!

Unit tests that still fail are:

    [junit] Running org.apache.pig.test.TestEvalPipeline
    [junit] Tests run: 8, Failures: 1, Errors: 0, Time elapsed: 175.711 sec
    [junit] Test org.apache.pig.test.TestEvalPipeline FAILED
",22/Sep/08 20:20;olgan;patch committed. thanks santhosh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Self join wth implicit split has the join output in wrong order,PIG-429,12404280,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,12/Sep/08 22:58,24/Mar/10 22:04,14/Mar/19 03:05,13/Sep/08 16:52,0.2.0,,,,,,0.2.0,,,,,0,,,,"Query:
{code}
A = load 'st10k' split by 'file';
B = filter A by $1 > 25;
D = join A by $0, B by $0;
dump D;
{code}

In the output the columns from B are projected out first and from A next. On closer examination of the code, the ImplicitSplitInserter class adds in the split and two splitoutput operators into the plan and tries the connect the successors of LOad to these. However it does this by iterating over its successors and disconnecting from them and connecting up the split-splitoutput to the successors. However the order in which it gets its successors is NOT the same as the order in which cogroup (join) expects its inputs. Hence the discrepancy. 
",,,,,,,,,,,,,,,,,,,13/Sep/08 05:13;pkamath;PIG-429.patch;https://issues.apache.org/jira/secure/attachment/12390046/PIG-429.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-13 16:52:24.834,,,no_permission,,,,,,,,,,,,164049,,,,,Sat Sep 13 16:52:24 UTC 2008,,,Patch Available,,,,0|i0gk4f:,94690,,,,,,,,,,13/Sep/08 05:13;pkamath;patch for issue - passes all unit tests,13/Sep/08 16:52;olgan;patch committed. thanks pradeep for contributing!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TypeCastInserter does not replace projects in inner plans correctly,PIG-428,12404221,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,12/Sep/08 06:40,24/Mar/10 22:04,14/Mar/19 03:05,12/Sep/08 19:30,0.2.0,,,,,,0.2.0,,,,,0,,,,"The TypeCastInserter tries to replace the Project's input operator in inner plans with the new foreach operator it adds. However it should replace only those Projects' input where the new Foreach has been added after the operator which was earlier the input to Project.

Here is a query which fails due to this:
{code}
a = load 'st10k' as (name:chararray,age:int, gpa:double);
another = load 'st10k';
c = foreach another generate $0, $1+ 10, $2 + 10;
d = join a by $0, c by $0;
dump d;

{code}

Here is the error:
{noformat}
2008-09-11 23:34:28,169 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher - Error message from task (map) tip_200809051428_0045_m_000000java.io.IOException: Type mismatch in key from map: expected org.apache.pig.impl.io.NullableText, recieved org.apache.pig.impl.io.NullableBytesWritable
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:419)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:83)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:172)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:158)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:75)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2124)

{noformat}",,,,,,,,,,,,,,,,,,,12/Sep/08 17:24;pkamath;PIG-428.patch;https://issues.apache.org/jira/secure/attachment/12390018/PIG-428.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-12 19:30:04.14,,,no_permission,,,,,,,,,,,,164048,,,,,Mon Feb 02 18:58:38 UTC 2009,,,,,,,0|i0gk3r:,94687,,,,,,,,,,12/Sep/08 17:23;pkamath;Attached patch for the issue ,"12/Sep/08 19:30;olgan;patch committed. thanks, pradeep","14/Jan/09 05:27;ankur;I am still seeing this issue. What I do is load my data using a custom loader. One of the fields returned by loader is of type Map.
When I retrieve a value from the map and group on that, I get this exception. Here is a snippet of  my script.

raw =  LOAD '/mydata/*' USING MyLoader() ;
entry = FILTER raw BY (CUSTOMARGMAP#'keyOfInterest' is not null);
listing = FOREACH entry GENERATE CUSTOMARGMAP#'keyOfInterest' as keyGroup;
myGroup = GROUP listing BY (keyGroup);
unordered_results = FOREACH myGroup GENERATE group, COUNT(*);
results = ORDER unordered_results by $1 DESC;
STORE results INTO 'Results' USING PigStorage();

 

","02/Feb/09 18:58;pkamath;Have you tried your query with top of trunk? The original issue fixed in this issue was when the TypeCastInserter was involved in the query. That is the case only when the load statement has a schema like "" a = load 'bla' as (x:int, y:float);"". In your query in the previous comment the load statement does not have a schema. I am wondering if the issues is somewhere else in that query but the error message is the same. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding result of two UDFs gives a syntax error,PIG-426,12404016,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,alangates,alangates,09/Sep/08 19:19,24/Mar/10 22:04,14/Mar/19 03:05,18/Sep/08 15:07,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"A script like:

{code}
a = load 'studenttab10k' using PigStorage() as (name, age, gpa);
b = load 'votertab10k' as (name, age, registration, contributions);
c = cogroup a by name, b by name;
d = foreach c generate flatten(group), COUNT(a) + COUNT(b);
store d into 'output';
{code}

gives

{code}
java.io.IOException: Atomic field expected but found non-atomic field
    at org.apache.pig.PigServer.registerQuery(PigServer.java:276)
    at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:432)
    at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:83)
    at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:64)
    at org.apache.pig.Main.main(Main.java:306)
Caused by: org.apache.pig.impl.logicalLayer.parser.ParseException: Atomic field expected but found non-atomic field
    at org.apache.pig.impl.logicalLayer.parser.QueryParser.assertAtomic(QueryParser.java:262)
    at org.apache.pig.impl.logicalLayer.parser.QueryParser.AdditiveExpr(QueryParser.java:3671)
    at org.apache.pig.impl.logicalLayer.parser.QueryParser.InfixExpr(QueryParser.java:3612)
    at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItem(QueryParser.java:3538)
    at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItemList(QueryParser.java:3472)
    at org.apache.pig.impl.logicalLayer.parser.QueryParser.GenerateStatement(QueryParser.java:3416)
    at org.apache.pig.impl.logicalLayer.parser.QueryParser.NestedBlock(QueryParser.java:2902)
    at org.apache.pig.impl.logicalLayer.parser.QueryParser.ForEachClause(QueryParser.java:2305)
    at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:956)
    at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:731)
    at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:532)
    at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:60)
    at org.apache.pig.PigServer.registerQuery(PigServer.java:273)
{code}
",,,,,,,,,,,,,,,,,,,18/Sep/08 03:20;sms;PIG-426.patch;https://issues.apache.org/jira/secure/attachment/12390335/PIG-426.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-18 03:20:28.466,,,no_permission,,,,,,,,,,,,38495,,,,,Thu Sep 18 15:07:34 UTC 2008,,,Patch Available,,,,0|i0gk33:,94684,,,,,,,,,,"18/Sep/08 03:20;sms;Patch (PIG-426.patch) addresses the following:

1. Parser fix to compute if a type is atomic or not.
2. Logical expression operators allow multiple outputs. This was preventing the graph construction inside the foreach where expression assignments were reused.

All unit test cases pass.",18/Sep/08 15:07;olgan;patch committed. Thanks Santhosh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split -> distinct or order -> cogroup fails,PIG-425,12404015,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,alangates,alangates,alangates,09/Sep/08 19:12,24/Mar/10 22:04,14/Mar/19 03:05,19/Sep/08 21:28,0.2.0,,,,,,0.2.0,,impl,,,1,,,,"A script like:

{code}
\a = load 'myfile' as (name:chararray, age:int, gpa:double);
split a into a1 if age > 50, a2 if name < 'm';
b2 = distinct a2;
b1 = order a1 by name;
c = cogroup b2 by name, b1 by name;
d = foreach c generate flatten(group), COUNT($1), COUNT($2);
store d into 'OUTPATH';
{code}

Will abort with the error:
{code}
08/09/09 11:46:50 ERROR mapReduceLayer.Launcher: Error message from task (map) tip_200809080906_0185_m_000000java.lang.ClassCastException: org.apache.pig.data.DefaultTuple cannot be cast to org.apache.pig.data.IndexedTuple
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:81)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:135)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:75)
    at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)
    at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2124)
{code}

The issue is that the RearrangeAdjuster in MRCompiler is not properly seeing this as a cogroup and moving the localrearrnge out of the reduce and into the
map.
",,,,,,,,,,,,,,,,,,,11/Sep/08 17:14;shravanmn;425.patch;https://issues.apache.org/jira/secure/attachment/12389946/425.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-11 17:14:22.65,,,no_permission,,,,,,,,,,,,164046,,,,,Fri Sep 19 21:28:38 UTC 2008,,,,,,,0|i0gk2n:,94682,,,,,,,,,,"11/Sep/08 17:14;shravanmn;The MRCompiler currently tries to pack as many operators possible into a single phase. So when we have two cogroups one after the other, the LR in the second cogroup gets pushed into the reducer. Since the store just stores away, LRs output, if we load it and pass it to GR we should be just fine.

However, since IndexedTuple isn't implemented as a new kind of Tuple with a Factory, the Load on the other side tries to load a DefaultTuple from an IndexedTuple and incidentally succeeds due to the way IndexedTuple is serialized. However, this can't be carried any further and when the mapper tries to collect the IndexedTuple, it fails.

The fix I have is three fold. I have modified IndexedTuple's serialization to suit the solution. Second, I have made IndexedTuple a type of tuple by writing a different byte to the marker byte indicating that this is an IndexedTuple(like we identify null and non-null tuples). Third, I have modfied DataReaderWriter's readDatum method to check if we have an IndexedTuple and process it according to IndexedTuple's serialization format.

With this, to try out, I have removed the RearrangeAdjuster from MRCompiler to see if my hypothesis is correct. The unit tests passed except MRCompiler due to GoldenPlan issues. We need to run all the end to end tests against this patch and confirm that it works.","11/Sep/08 20:40;olgan;I verified that all the tests run with the patch. However, we are not going to commit the patch since IndexTuple is going away and Alan will integrate this changes together with his NULL work on join.","11/Sep/08 20:40;olgan;Thanks, Shravan for figuring out what the problem was and providing the patch!","17/Sep/08 23:17;alangates;I found one significant downside to the approach of this patch.  Not moving the local rearrange into the map removes the possibility of running the combiner.  So if you have a query like:

C = cogroup A, B;
D = foreach C flatten(A), (B);
E = group D by $0;
F = foreach E generate group, COUNT(D)

that count will not be done in the combiner now.   That seems like a significant downside.",19/Sep/08 21:28;alangates;This issue was resolved by the patch that fixed PIG-361.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nested foreach with flatten and agg gives an error,PIG-424,12403944,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,olgan,olgan,09/Sep/08 00:29,24/Mar/10 22:04,14/Mar/19 03:05,22/Sep/08 19:51,0.2.0,,,,,,0.2.0,,,,,0,,,,"Script:

A = load 'data' as (name, age, gpa);
B = group A by name;
C = foreach B{
D = order A by gpa;
generate flatten(D), MAX(A.age);}
store C into 'ciemo_4';

Error:

08/09/08 17:25:48 ERROR pig.PigServer: Problem resolving LOForEach schema org.apache.pig.builtin.MAX does not work with inputs of type {age: bytearray}
08/09/08 17:25:48 ERROR pig.PigServer: Severe problem found during validation org.apache.pig.impl.plan.PlanValidationException: An unexpected exception caused the validation to stop
08/09/08 17:25:48 ERROR grunt.GruntParser: java.io.IOException: Unable to store for alias: 12 [Problem resolving LOForEach schema org.apache.pig.builtin.MAX does not work with inputs of type {age: bytearray}Severe problem found during validation org.apache.pig.impl.plan.PlanValidationException: An unexpected exception caused the validation to stop]
        at org.apache.pig.PigServer.compileLp(PigServer.java:606)
        at org.apache.pig.PigServer.compileLp(PigServer.java:547)
        at org.apache.pig.PigServer.execute(PigServer.java:533)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:283)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:432)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:242)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:93)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:58)
        at org.apache.pig.Main.main(Main.java:282)",,,,,,,,,,,,,,,,,,,19/Sep/08 04:12;sms;PIG-424.patch;https://issues.apache.org/jira/secure/attachment/12390453/PIG-424.patch,20/Sep/08 20:17;sms;PIG-424_1.patch;https://issues.apache.org/jira/secure/attachment/12390582/PIG-424_1.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-09-19 04:12:23.445,,,no_permission,,,,,,,,,,,,164045,,,,,Mon Sep 22 19:51:54 UTC 2008,,,Patch Available,,,,0|i0gk27:,94680,,,,,,,,,,"19/Sep/08 04:12;sms;Attached patch (PIG-424.patch) addresses the following:

1. The schema computation in LOProject used references from its predecessor instead of making a copy. The manipulation of the field schemas altered the schema of the predecessor

2. Added copy constructors for FieldSchema and Schema

All unit test cases pass.","20/Sep/08 20:17;sms;Modified the earlier patch to be consistent with the use of the copy constructor and made the patch current with SVN

Unit tests that still fail are:

    [junit] Running org.apache.pig.test.TestEvalPipeline
    [junit] Tests run: 8, Failures: 1, Errors: 0, Time elapsed: 176.18 sec
    [junit] Test org.apache.pig.test.TestEvalPipeline FAILED
","22/Sep/08 19:51;olgan;patch committed, thanks santhosh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cross is broken,PIG-422,12403939,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,shravanmn,olgan,olgan,08/Sep/08 22:51,24/Mar/10 22:04,14/Mar/19 03:05,12/Sep/08 18:19,0.2.0,,,,,,0.2.0,,,,,0,,,,"The following script fails:

a = load 'data1' as (name, age, gpa);
b = load 'data2' as (name, age, registration, contributions);
c = filter a by age < 19 and gpa < 1.0;
d = filter b by age < 19;
e = cross c, d;
store e into 'output';

produces the following stack:

0808261638_3210_r_000000java.lang.ClassCastException: org.apache.pig.data.DefaultDataBag cannot be cast to org.apache.pig.data.Tuple
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:264)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:220)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:231)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:220)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODistinct.getNext(PODistinct.java:76)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:270)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.processInputBag(POProject.java:351)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:158)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.processInput(POUserFunc.java:123)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:175)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:241)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:217)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:156)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.runPipeline(PigMapReduce.java:206)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:176)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:87)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:391)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2124)
/Cross
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:158)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.processInput(POUserFunc.java:123)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:175)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:241)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:217)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:156)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.runPipeline(PigMapReduce.java:206)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:176)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:87)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:391)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2124)",,,,,,,,,,,,,,,,,,,10/Sep/08 18:15;shravanmn;422.patch;https://issues.apache.org/jira/secure/attachment/12389848/422.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-10 18:14:02.281,,,no_permission,,,,,,,,,,,,164043,,,,,Fri Sep 12 18:19:24 UTC 2008,,,,,,,0|i0gk1b:,94676,,,,,,,,,,"10/Sep/08 18:14;shravanmn;This one got broken because of the fix to POUserFunc to adhere to trunk behavior. We removed the Tuple inside a Tuple check. The initial fix used a constant expression which was a Tuple and relied on POUserFunc to remove the nesting before sending it to GFCross. 

So now I split the list of objects inside the constant tuple into 2 constant expressions. However, it did not work because of our unordered plan structure. It was accessing the two constants in random order and GFCross would not work if we pass(1,2) instead of (2,1).

I think we need to be careful about this one. If a UDF is given constant expressions like UDF('2','1'), We create constant expressions and attach it to the UDF as inputs. However, I am not sure if there is guarantee that the two constant expressions will be pulled in the same order as our plan doesn't support order.

I was able to fix this one because, luckily the POUserFunc operator relies on its inputs and not on the ones got by using getPredecessors() on the plan. I think most of the operators that were created earlier did that since we did not have a handle to the plan the operator is a part of. So, I explicitly initialized the inputs of POUserFunc to the list of constanct expressions, created in the right order, after connecting all the operators in the plan. I think we need to take a look at the code and see if we can hit such problems elsewhere.",10/Sep/08 18:15;shravanmn;Included in this patch are the changes to visit(LOCross) in LogToPhyTranslationVisitor and a very simple new unit test for cross.,"12/Sep/08 18:19;olgan;patch committed. Thanks, Shravan!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
error with complex nested plan,PIG-421,12403932,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,olgan,olgan,08/Sep/08 21:32,24/Mar/10 22:04,14/Mar/19 03:05,13/Sep/08 16:13,0.2.0,,,,,,0.2.0,,,,,0,,,,"Even after applying patch for PIG-398, the following query still fails:

a = load 'studenttab10k' as (name, age, gpa);
b = filter a by age < 20;
c = group b by age;
d = foreach c {
    cf = filter b by gpa < 3.0;
    cp = cf.gpa;
        cd = distinct cp;
    co = order cd by $0;
    generate group, flatten(co);
    }
store d into 'output';",,,,,,,,,,,,,,,,,,,11/Sep/08 00:51;sms;PIG-421.patch;https://issues.apache.org/jira/secure/attachment/12389885/PIG-421.patch,12/Sep/08 22:56;sms;PIG-421_1.patch;https://issues.apache.org/jira/secure/attachment/12390041/PIG-421_1.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-09-08 22:54:34.078,,,no_permission,,,,,,,,,,,,38597,,,,,Sat Sep 13 16:13:38 UTC 2008,,,Patch Available,,,,0|i0gk0v:,94674,,,,,,,,,,"08/Sep/08 22:54;sms;With Pig-398, the issue with LOVisitor not walking all inner plans correctly was resolved. That has uncovered other issues. One of which is the following:

During type checking, schemas are computed for a collection of operators. Later, the optimizer, resets the schemas and calls the getSchema() method on the operators. Instead of returning the schemas computed by the type checker (which are now lost), the schemas computed by the individual operator are returned. In order to resolve this, we need to move the schema computation out of the type checker and into a common place (getSchema() method of the operators).

The query in the bug report generates the following stack trace:

{code}
384826 [main] ERROR org.apache.pig.tools.grunt.GruntParser  - java.io.IOException: Unable to explain alias d [Can't overwrite cause]
        at java.lang.Throwable.initCause(Throwable.java:320)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.resolveLOProjectType(TypeCheckingVisitor.java:212)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.checkInnerPlan(TypeCheckingVisitor.java:2298)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.visit(TypeCheckingVisitor.java:1848)
        at org.apache.pig.impl.logicalLayer.LOFilter.visit(LOFilter.java:102)
{code}","11/Sep/08 00:51;sms;The attached patch (PIG-421.patch) does the following:

1. Moved all the schema computation that existed within the type checker to the appropriate operators.
2. The getType() method for all expressions will now call getFieldSchema() and return the type appropriately
3. Renamed the resetSchema methods to unsetSchema which was an existing method in LogicalOperator

Unit tests that still fail are:

    [junit] Running org.apache.pig.test.TestMRCompiler
    [junit] Tests run: 16, Failures: 1, Errors: 0, Time elapsed: 0.34 sec
    [junit] Test org.apache.pig.test.TestMRCompiler FAILED

    [junit] Running org.apache.pig.test.TestSplitStore
    [junit] Tests run: 9, Failures: 0, Errors: 2, Time elapsed: 216.145 sec
    [junit] Test org.apache.pig.test.TestSplitStore FAILED

TestSplitStore has two test cases failing test6 and test7. These tests fail due to the issue reported in PIG-407.
","12/Sep/08 22:56;sms;Attaching a new patch (PIG-421_1.patch). In addition to the previous patch, this patch fixes:

1. Fixes the 2 failing unit test cases. Removed the 2 gold files for the Logical to Physical translation
2. Fixed a bug in LOVisitor where LOSplit was visiting the LOSplitOutputs. The walker was also visiting LOSplitOutputs

All unit test cases pass.","13/Sep/08 16:13;olgan;patch committed. thanks, santhosh!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
divide by 0 is not handled correctly,PIG-418,12403775,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,05/Sep/08 17:58,24/Mar/10 22:04,14/Mar/19 03:05,05/Sep/08 20:54,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"A = load 'data' as (name:chararray, age:int, gpa:double);
B = foreach A generate age, (int)(age/(age - 20));
store B into 'types_2_3';

Get the following exceptiopn

08/09/05 10:56:55 ERROR mapReduceLayer.Launcher: Error message from task (map) tip_200808261638_3046_m_000000java.lang.ArithmeticException: / by zero
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Divide.getNext(Divide.java:119)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(POCast.java:139)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:211)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:156)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:127)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.map(PigMapOnly.java:64)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2124)",,,,,,,,,,,,,,,,,,,05/Sep/08 19:24;olgan;PIG-418.patch;https://issues.apache.org/jira/secure/attachment/12389581/PIG-418.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,164040,,,,,Fri Sep 05 20:54:09 UTC 2008,,,,,,,0|i0gjzj:,94668,,,,,,,,,,05/Sep/08 20:54;olgan;patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
split with types causing failure during dump,PIG-416,12403728,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,alangates,olgan,olgan,04/Sep/08 23:35,24/Mar/10 22:04,14/Mar/19 03:05,09/Sep/08 19:33,0.2.0,,,,,,0.2.0,,,,,0,,,,"grunt> A = load 'data' as (name: chararray, age: int, gpa: float);
grunt> split A into A1 if name == 'foo', A2 if name != 'foo';
grunt> dump A1;

The following script causes runtime exception:

08/09/04 16:24:33 INFO mapReduceLayer.MapReduceLauncher: 0% complete
08/09/04 16:24:36 INFO mapReduceLayer.MapReduceLauncher: 50% complete
08/09/04 16:25:01 INFO mapReduceLayer.MapReduceLauncher: 100% complete
08/09/04 16:25:04 ERROR mapReduceLayer.MapReduceLauncher: Map reduce job failed
08/09/04 16:25:04 ERROR mapReduceLayer.MapReduceLauncher: Job failed!
08/09/04 16:25:04 ERROR mapReduceLayer.Launcher: Error message from task (map) tip_200808261638_3037_m_000000java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.pig.data.DataByteArray
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.ConstantExpression.getNext(ConstantExpression.java:111)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.EqualToExpr.getNext(EqualToExpr.java:68)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter.getNext(POFilter.java:147)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:127)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.map(PigMapOnly.java:64)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2124)",,,,,,,,,,,,,,,,,,,09/Sep/08 19:00;alangates;PIG-416.patch;https://issues.apache.org/jira/secure/attachment/12389768/PIG-416.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-09 19:00:04.745,,,no_permission,,,,,,,,,,,,164038,,,,,Tue Sep 09 19:33:02 UTC 2008,,,,,,,0|i0gjyn:,94664,,,,,,,,,,09/Sep/08 19:00;alangates;Fixes LogicalOptimizer to properly select the SplitOutputs to patch up projections for when inserting types.  Also fixes explain to work with splits.,09/Sep/08 19:33;alangates;Checked in patch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testSort test from TestEvalPipeline unit test is failing,PIG-415,12403723,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,shravanmn,olgan,olgan,04/Sep/08 22:29,24/Mar/10 22:04,14/Mar/19 03:05,10/Sep/08 18:56,0.2.0,,,,,,0.2.0,,,,,0,,,,"Testcase: testSort took 49.287 sec
    FAILED
null
junit.framework.AssertionFailedError: null
    at org.apache.pig.test.TestEvalPipeline.testSortDistinct(TestEvalPipeline.java:350)
    at org.apache.pig.test.TestEvalPipeline.testSort(TestEvalPipeline.java:309)
",,,,,,,,,,,,,,,,,,,10/Sep/08 17:36;shravanmn;415.patch;https://issues.apache.org/jira/secure/attachment/12389845/415.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-10 17:36:16.844,,,no_permission,,,,,,,,,,,,164037,,,,,Wed Sep 10 18:56:50 UTC 2008,,,,,,,0|i0gjy7:,94662,,,,,,,,,,10/Sep/08 17:36;shravanmn;The problem here was with the raw comparators. An extra byte was being read since the start index was incremented to skip the NULL byte but the length wasn't decremented. Fixed this in all the Raw Comparators.,"10/Sep/08 18:56;olgan;patch committed. thanks, shravan!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
explicit split causes name to be dropped from describe,PIG-414,12403716,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,pkamath,olgan,olgan,04/Sep/08 21:29,24/Mar/10 22:04,14/Mar/19 03:05,12/Sep/08 18:57,0.2.0,,,,,,0.2.0,,,,,0,,,,"A = load 'studenttab10k' as (name: chararray, age: int, gpa: float);
B = load 'studenttab10k' as (name: chararray, age: int, gpa: float);
split A into A1 if name eq 'foo', A2 if name eq 'bar';
C = cogroup A1 by name, B by name;
describe C;
C: {group: chararray,{name: chararray,age: integer,gpa: float},B: {name: chararray,age: integer,gpa: float}}

Note that name of A1 is missing from dewcribe",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-09-11 15:12:31.615,,,no_permission,,,,,,,,,,,,640,,,,,Fri Sep 12 18:57:25 UTC 2008,,,,,,,0|i0gjxr:,94660,,,,,,,,,,"11/Sep/08 15:12;alangates;This affects more than just describe.  If you have a query like:

{code}
a = load 'infile';
split a into a1 if $0 > 'm', a2 if $0 <= 'm';
b = cogroup a1 by $1, a2 by $1;
c = foreach b generate flatten(a1), flatten(a2);
store c into 'outfile';
{code}

it will fail saying that a1 and a2 are not valid aliases in b at the foreach line.
","12/Sep/08 18:00;sms;Assigning the bug to Pradeep, who has a fix.",12/Sep/08 18:57;olgan;This issue has been resolved by the patch to PIG-407,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestBuiltin has an error in testSumFinal ,PIG-413,12403714,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,04/Sep/08 21:16,24/Mar/10 22:04,14/Mar/19 03:05,04/Sep/08 22:27,0.2.0,,,,,,0.2.0,,,,,0,,,,"Here's the error:
{noformat}
Testcase: testSUMFinal took 0.005 sec
    Caused an ERROR
Caught exception in IntSum.Final [java.lang.Integer]
java.io.IOException: Caught exception in IntSum.Final [java.lang.Integer]
    at org.apache.pig.builtin.IntSum$Final.exec(IntSum.java:90)
    at org.apache.pig.builtin.IntSum$Final.exec(IntSum.java:71)
    at org.apache.pig.test.TestBuiltin.testSUMFinal(TestBuiltin.java:436)
Caused by: java.lang.ClassCastException: java.lang.Integer
    ... 18 more 

{noformat}",,,,,,,,,,,,,,,,,,,04/Sep/08 21:47;pkamath;PIG-413.patch;https://issues.apache.org/jira/secure/attachment/12389544/PIG-413.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-04 22:27:48.054,,,no_permission,,,,,,,,,,,,164036,,,,,Thu Sep 04 22:27:48 UTC 2008,,,,,,,0|i0gjxb:,94658,,,,,,,,,,04/Sep/08 21:47;pkamath;Patch for the issue,04/Sep/08 22:27;olgan;patch committed; thanks pradeep!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig leaves HOD processes behind if Ctrl-C is used before HOD connection is fully established,PIG-411,12403696,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,olgan,olgan,olgan,04/Sep/08 17:35,24/Mar/10 22:04,14/Mar/19 03:05,22/Sep/08 20:20,0.2.0,,,,,,0.2.0,,,,,0,,,,"The code in doHod in backend/hadoop/executionengine/HExecutionEngine.java waits for hod connection to be fully established before setting a flag that indicates need to discunnect. The flag should be set as soon as HOD process is created so that we destroy it. 

This change will not guarantee that process is not left behind but would make it much less likely",,,,,,,,,,,,,,,,,,,19/Sep/08 23:17;olgan;PIG-411.patch;https://issues.apache.org/jira/secure/attachment/12390551/PIG-411.patch,22/Sep/08 19:47;olgan;PIG-411_2.patch;https://issues.apache.org/jira/secure/attachment/12390674/PIG-411_2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-09-22 18:20:29.483,,,no_permission,,,,,,,,,,,,164034,,,,,Mon Sep 22 20:20:31 UTC 2008,,,,,,,0|i0gjwf:,94654,,,,,,,,,,"22/Sep/08 18:20;pkamath;Changes look good.

Couple of comments:
1)
{code}
    private synchronized void closeHod(String server){
              if (hodProcess == null){
                  // just cleanup the dir if it exists and return
                  if (hodConfDir != null)
                      deleteDir(server, hodConfDir);                                                                  
                  return;
              }                                                                                                       
              
              // hod deallocate format: hod deallocate -d <conf dir>                                                  
              String[] cmdarray = new String[4];                                                                      
              cmdarray[0] = ""hod"";
              cmdarray[1] = ""deallocate"";                                                                             
              cmdarray[2] = ""-d"";
             if (remoteHodConfDir != null)
                 cmdarray[3] = remoteHodConfDir;                                                                     
             else
                  cmdarray[3] = hodConfDir;                                                                           
              
              log.info(""Disconnecting from HOD..."");
              log.debug(""Disconnect command: "" + cmdToString(cmdarray));                                              
              
              try {
                  runCommand(server, cmdarray, false);                                                                
             } catch (Exception e) {
                  log.warn(""Failed to disconnect from HOD; error: "" + e.getMessage());                                
             } finally {
                 if (remoteHodConfDir != null)
                     deleteDir(server, remoteHodConfDir);                                                             
                 deleteDir(LOCAL, hodConfDir);                                                                        
            }                                                                                                        
            
            hodProcess = null;                                                                                       
     }

{code}

Should the code in 
{code}
if (hodProcess == null){
 ..
}
{code}

be same as the code which deletes and cleans up in 
{code}
 } finally {
                 if (remoteHodConfDir != null)
                     deleteDir(server, remoteHodConfDir);                                                             
                 deleteDir(LOCAL, hodConfDir);                                                                        
            }                    
{code}
to be consistent in the way cleanup is done?

2)
If hodProcess != null, and if we failed to deallocate hod cluster in the catch{}, should we try to destroy hodProcess -(hodProcess.destroy()) so that we don't leave the hodProcess as a zombie?",22/Sep/08 19:47;olgan;Thanks pradeep for the feedback. New patch attached.,22/Sep/08 20:07;pkamath;Reviewed the changes - look good.,22/Sep/08 20:20;olgan;patch committed. thanks pradeep for reviews,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Explain followed by Describe throws exception,PIG-407,12403410,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,sms,sms,30/Aug/08 01:08,24/Mar/10 22:04,14/Mar/19 03:05,12/Sep/08 18:56,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"{code}
A = load '/user/sms/data/student.data' using PigStorage(' ') as (name: chararray, age: int, gpa: float);
B = group A by $1;
C = foreach B{ D = order A by $0; generate D; }
describe C;
describe B;
explain C;
describe B;


56335 [main] ERROR org.apache.pig.tools.grunt.GruntParser  - java.lang.NullPointerException
        at org.apache.pig.impl.logicalLayer.LOVisitor.visit(LOVisitor.java:121)
        at org.apache.pig.impl.logicalLayer.PlanSetter.visit(PlanSetter.java:58)
        at org.apache.pig.impl.logicalLayer.LOCogroup.visit(LOCogroup.java:262)
        at org.apache.pig.impl.logicalLayer.LOCogroup.visit(LOCogroup.java:39)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:65)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:67)
        at org.apache.pig.impl.plan.DepthFirstWalker.walk(DepthFirstWalker.java:50)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.PigServer.compileLp(PigServer.java:549)
        at org.apache.pig.PigServer.dumpSchema(PigServer.java:285)
        at org.apache.pig.tools.grunt.GruntParser.processDescribe(GruntParser.java:149)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:181)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:92)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:58)
        at org.apache.pig.Main.main(Main.java:282)

{code}",,,,,,,,,,,,,,,,,,,12/Sep/08 14:30;pkamath;PIG-407.patch;https://issues.apache.org/jira/secure/attachment/12390008/PIG-407.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-12 14:30:55.298,,,no_permission,,,,,,,,,,,,671,,,,,Fri Sep 12 18:56:46 UTC 2008,,,Patch Available,,,,0|i0gjuv:,94647,,,,,,,,,,12/Sep/08 14:30;pkamath;Attached patch for resolving the issue. THe reported query now works. All unit tests passed with the patch,"12/Sep/08 18:56;olgan;patch committed, thanks, pradeep",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Porting of the slicer into types branch caused all scripts to run just one map.,PIG-403,12403280,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,alangates,alangates,alangates,28/Aug/08 19:58,24/Mar/10 22:04,14/Mar/19 03:05,28/Aug/08 21:07,0.2.0,,,,,,0.2.0,,,,,0,,,,"In PigSlicer.java, splittable is set to false by default.  This causes the system to scan an entire file in a single map.  It should be set to true by default, so that files are only scanned in a single map if the user explicitly requests it.",,,,,,,,,,,,,,,,,,,28/Aug/08 20:36;alangates;PIG-403.patch;https://issues.apache.org/jira/secure/attachment/12389125/PIG-403.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-08-28 20:55:11.924,,,no_permission,,,,,,,,,,,,164028,,,,,Thu Aug 28 21:07:37 UTC 2008,,,,,,,0|i0gjtb:,94640,,,,,,,,,,28/Aug/08 20:36;alangates;Changed PigSlicer.splittable default to true from false.,"28/Aug/08 20:55;pkamath;Review comments:
I think this one line change should be good - ""splittable"" field is used in the slice() method in PigSlicer and that looks at filename (to see if it ends with .gz) or if splittable is set to false. The only way we allow splittable to set is through the setSplittable() which is not being called currently. I will be changing PigInputFormat to call setSplittable(false) when the script has ""SPLIT by file"" as part of the patch for streaming",28/Aug/08 21:07;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
order by on single field with user defined comparator fails,PIG-402,12403205,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,shravanmn,olgan,olgan,28/Aug/08 01:00,24/Mar/10 22:04,14/Mar/19 03:05,10/Sep/08 18:56,0.2.0,,,,,,0.2.0,,,,,0,,,,"register udf.jar;
a = load 'data';
c = order a by $0 using MyOrderUDF();
store c into 'out',",,,,,,,,,,,,,,,,,,,10/Sep/08 17:31;shravanmn;402.patch;https://issues.apache.org/jira/secure/attachment/12389844/402.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-09 19:03:29.885,,,no_permission,,,,,,,,,,,,164027,,,,,Wed Sep 10 18:56:16 UTC 2008,,,,,,,0|i0gjsv:,94638,,,,,,,,,,"09/Sep/08 19:03;shravanmn;This is what I have understood - Comparators in Hadoop assume that you know the type of key(keyClass) beforehand and do not let you configure the type dynamically. So I feel, the only way out for us is to wrap the key inside a tuple whenever, a User Defined Comparison Func is used.

If any of you have other suggestions, please comment.","09/Sep/08 19:16;alangates;I think that's perfectly reasonable.  There is a performance penalty for wrapping in a tuple.  But user defined comparators are expected to be the exception, especially now that we provide numeric and descending order sort (the only two reasons we added the user defined comparators to begin with).","10/Sep/08 17:30;shravanmn;The solution I am providing is to wrap the key inside a tuple whenever a user defined comparison func is used. For that I have the following in the patch.

1) Created a new Mapper class MapWithComparator in PigMapReduce which will be used whenever a u.d. comparator is used. The assumuption is that keyType and keyClass will be appropriately set to Tuple and the collect here wraps the key in a Tuple. This was done to avoid an if branch in the earlier Mapper class.

2) JobControlCompiler: To meet the assumptions in 1 above, the changes to job control compiler ensures consistency

3) PigMapBase: Incidental. Introduced a tuple factory instance into the base class.

4) TestEvalPipeline: Added a new unit test to test Sort with UDF.","10/Sep/08 18:56;olgan;patch committed. shravan, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SplitIntroducer does not handle some cases correctly,PIG-401,12403192,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,alangates,pkamath,pkamath,27/Aug/08 22:59,24/Mar/10 22:04,14/Mar/19 03:05,05/Sep/08 23:33,0.2.0,,,,,,0.2.0,,,,,0,,,,"The splitIntroducer does not handle the following cases correctly:
   * As soon as SplitIntroducer sees an operator which does NOT have multiple outputs it stops - it should still recursively check the successors:
Current code which needs to change:
{code}
        List<LogicalOperator> sucs = mPlan.getSuccessors(root);
        if(sucs==null) return; 
        int size = sucs.size();
        if(size==0 || size==1) return;  --> THIS should change to recursively process the succesor if it exists(size == 1 case)
{code}

   * If there are more than one operators in the plan which need implicit splits to be introduced
   * If the new SPLIT operator is introduced before a cogroup, the cogroup's inner map for it's inputs and GroupBy expressions needs to be updated
   * The kind of changes need for cogroup maybe needed for some other operators 

I started looking at it some and have a patch for the first two issues - *UNTESTED* which I am attaching to this issue",,,,,,,,,,,,,,,,,,,27/Aug/08 23:01;pkamath;PIG-401-partial.patch;https://issues.apache.org/jira/secure/attachment/12389053/PIG-401-partial.patch,05/Sep/08 22:49;alangates;PIG401.patch;https://issues.apache.org/jira/secure/attachment/12389599/PIG401.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-09-05 22:49:50.367,,,no_permission,,,,,,,,,,,,164026,,,,,Fri Sep 05 23:33:27 UTC 2008,,,,,,,0|i0gjsn:,94637,,,,,,,,,,"05/Sep/08 22:49;alangates;This patch moves the SplitIntroducer into the optimizer (and renames it as ImplicitSplitInserter for whatever reason).  This addresses two issues:

1) catching splits past the initial node in the graph
2) patching up schemas and inner plans of other operators after a split is introduced.",05/Sep/08 23:33;alangates;PIG401.patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flatten causes schema naming problems,PIG-400,12403028,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,olgan,olgan,25/Aug/08 21:54,24/Mar/10 22:04,14/Mar/19 03:05,28/Aug/08 23:48,0.2.0,,,,,,0.2.0,,,,,0,,,,"Script:

A = load 'data' as (name: chararray, age: chararray, gpa: float);
B = group A by (name, age);
C = foreach B generate flatten(group) as res, COUNT(A);
D = foreach C generate res;
dump D;

Error:

java.io.IOException: Invalid alias: res in {res::name: chararray,res::age: chararray,long}
        at org.apache.pig.PigServer.registerQuery(PigServer.java:255)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:422)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:241)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:82)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:64)
        at org.apache.pig.Main.main(Main.java:302)
Caused by: org.apache.pig.impl.logicalLayer.parser.ParseException: Invalid alias: res in {res::name: chararray,res::age: chararray,long}

",,,,,,,,,,,,,,,,,,,28/Aug/08 23:16;sms;PIG_400.patch;https://issues.apache.org/jira/secure/attachment/12389130/PIG_400.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-08-28 23:16:02.169,,,no_permission,,,,,,,,,,,,38596,,,,,Thu Aug 28 23:48:56 UTC 2008,,,Patch Available,,,,0|i0gjs7:,94635,,,,,,,,,,"28/Aug/08 23:16;sms;The patch (PIG_400.patch) fixes the following:

1. Allows users to specify schema for flattened columns
2. Performs a prefix merge of schemas provided by users
3. Unit test cases to handle points 1 and 2
3. Removed debug statements from PODistinct

Unit tests that still fail are:

    [junit] Running org.apache.pig.test.TestBuiltin
    [junit] Tests run: 23, Failures: 1, Errors: 2, Time elapsed: 14.495 sec
    [junit] Test org.apache.pig.test.TestBuiltin FAILED

    [junit] Running org.apache.pig.test.TestEvalPipeline
    [junit] Tests run: 9, Failures: 1, Errors: 1, Time elapsed: 170.88 sec
    [junit] Test org.apache.pig.test.TestEvalPipeline FAILED

    [junit] Running org.apache.pig.test.TestFilterOpNumeric
    [junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 56.765 sec
    [junit] Test org.apache.pig.test.TestFilterOpNumeric FAILED

    [junit] Running org.apache.pig.test.TestMRCompiler
    [junit] Tests run: 16, Failures: 1, Errors: 0, Time elapsed: 0.401 sec
    [junit] Test org.apache.pig.test.TestMRCompiler FAILED",28/Aug/08 23:48;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
limit interfers with types in filter,PIG-399,12403026,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,olgan,olgan,25/Aug/08 20:35,25/Mar/10 00:12,14/Mar/19 03:05,28/Aug/08 00:10,,,,,,,0.2.0,,,,,0,,,,"The following script fails:

A = load 'data' as (name: chararray, age: chararray, gpa: float);
B = limit A 40;
C = filter B by age eq '40';
D = group C by name;
E = foreach D generate group, COUNT(C);
dump E;

If limit is removed, the script runs without any problems. The explain shows that filter looses type of the non-constant operand.

The errror is

8/08/25 13:34:15 INFO mapReduceLayer.MapReduceLauncher: Unsuccessful attempt. Completed 0.0% of the job
08/08/25 13:34:15 ERROR mapReduceLayer.Launcher: Error message from task (reduce) tip_200808131548_2163_r_000000java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.pig.data.DataByteArray
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.ConstantExpression.getNext(ConstantExpression.java:111)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.EqualToExpr.getNext(EqualToExpr.java:68)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter.getNext(POFilter.java:147)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:169)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:87)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:391)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2124)


",,,,,,,,,,,,,,,,,,,27/Aug/08 23:57;alangates;PIG-399.patch;https://issues.apache.org/jira/secure/attachment/12389057/PIG-399.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-08-27 23:57:42.061,,,no_permission,,,,,,,,,,,,164025,,,,,Thu Aug 28 00:10:59 UTC 2008,,,,,,,0|i0gjrb:,94631,,,,,,,,,,27/Aug/08 23:57;alangates;Added limit to visitors in the optimizer so that it would be properly handled when we're rearranging the tree.,28/Aug/08 00:10;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expressions not allowed inside foreach,PIG-398,12403020,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,olgan,olgan,25/Aug/08 19:41,24/Mar/10 22:04,14/Mar/19 03:05,04/Sep/08 23:33,0.2.0,,,,,,0.2.0,,,,,0,,,,"The following used to work but no longer does:

A = load 'data';
B = group A by $0;
C = foreach B{
   D = MAX(A.$0)
   generate D;
}",,,,,,,,,,,,,,,,,,,03/Sep/08 23:17;sms;Pig-398.patch;https://issues.apache.org/jira/secure/attachment/12389468/Pig-398.patch,04/Sep/08 22:50;sms;Pig-398_1.patch;https://issues.apache.org/jira/secure/attachment/12389547/Pig-398_1.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-09-03 23:17:44.795,,,no_permission,,,,,,,,,,,,38595,,,,,Thu Sep 04 23:33:56 UTC 2008,,,Patch Available,,,,0|i0gjqv:,94629,,,,,,,,,,"03/Sep/08 23:17;sms;Patch, Pig-398.patch fixes the following:

1. Expression assignment within foreach is now re-instated
2. Visit methods for operators that contain nested plans in LOVisitor are fixed. Previously only the roots of the nested plans were visited.
3. Cast field schemas will no longer be reset during optimization
4. History will now persist across multiple pig interactive sessions - thanks to Raghotham S Murthy from Facebook.

Unit tests that still fail are:

    [junit] Running org.apache.pig.test.TestBuiltin
    [junit] Tests run: 23, Failures: 1, Errors: 2, Time elapsed: 14.363 sec
    [junit] Test org.apache.pig.test.TestBuiltin FAILED

    [junit] Running org.apache.pig.test.TestEvalPipeline
    [junit] Tests run: 10, Failures: 1, Errors: 0, Time elapsed: 163.484 sec
    [junit] Test org.apache.pig.test.TestEvalPipeline FAILED

    [junit] Running org.apache.pig.test.TestMRCompiler
    [junit] Tests run: 16, Failures: 1, Errors: 0, Time elapsed: 0.24 sec
    [junit] Test org.apache.pig.test.TestMRCompiler FAILED","04/Sep/08 22:50;sms;Updated patch (Pig-398_1.patch) after streaming was fixed as part of Pig-269.

Unit tests that still fail are:

    [junit] Running org.apache.pig.test.TestBuiltin
    [junit] Tests run: 23, Failures: 1, Errors: 2, Time elapsed: 14.622 sec
    [junit] Test org.apache.pig.test.TestBuiltin FAILED

    [junit] Running org.apache.pig.test.TestEvalPipeline
    [junit] Tests run: 10, Failures: 1, Errors: 0, Time elapsed: 353.427 sec
    [junit] Test org.apache.pig.test.TestEvalPipeline FAILED

    [junit] Running org.apache.pig.test.TestMRCompiler
    [junit] Tests run: 16, Failures: 1, Errors: 0, Time elapsed: 0.422 sec
    [junit] Test org.apache.pig.test.TestMRCompiler FAILED",04/Sep/08 23:33;olgan;patch committed. thanks santhosh!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
by default reduce parallelism is set to 1,PIG-397,12403013,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,25/Aug/08 18:36,24/Mar/10 22:04,14/Mar/19 03:05,26/Aug/08 15:25,0.2.0,,,,,,0.2.0,,,,,0,,,,It should be left alone to that hod/hadoop can pick the best value,,,,,,,,,,,,,,,,,,,25/Aug/08 23:34;olgan;PIG-397.patch;https://issues.apache.org/jira/secure/attachment/12388882/PIG-397.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,164024,,,,,Tue Aug 26 15:25:44 UTC 2008,,,,,,,0|i0gjqf:,94627,,,,,,,,,,25/Aug/08 23:34;olgan;please review,26/Aug/08 15:25;olgan;patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dump (name); crashes grunt,PIG-395,12402959,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,johndetreville,johndetreville,24/Aug/08 19:48,25/Mar/10 00:12,14/Mar/19 03:05,27/Jan/09 02:26,,,,,,,0.2.0,,grunt,,,0,,,,"If I incorrectly type:

dump (name);

for some name, grunt prints a backgrace and exits. Other syntax errors don't cause grunt to exit, and ideally this one shouldn't either.",Pre-2.0.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-01-27 02:26:52.318,,,no_permission,,,,,,,,,,,,164022,,,,,Tue Jan 27 02:26:52 UTC 2009,,,,,,,0|i0gjpj:,94623,,,,,,,,,,"27/Jan/09 02:26;olgan;The latest code give the following error

2009-01-26 18:26:22,412 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Lexical error at line 1, column 5.  Encountered: ""("" (40), after : """"",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When a pig script creates multiple MR jobs, if any job but the last fails, pig report success and returns no results.",PIG-392,12402828,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,21/Aug/08 23:27,24/Mar/10 22:04,14/Mar/19 03:05,25/Aug/08 23:22,0.2.0,,,,,,0.2.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,22/Aug/08 15:28;alangates;PIG-392.patch;https://issues.apache.org/jira/secure/attachment/12388751/PIG-392.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-08-24 19:11:17.555,,,no_permission,,,,,,,,,,,,164020,,,,,Mon Aug 25 23:22:57 UTC 2008,,,,,,,0|i0gjo7:,94617,,,,,,,,,,"22/Aug/08 15:28;alangates;This patch changes the launcher classes to check for any failed jobs, instead of just checking that progress is 100% when hadoop declares all jobs finished.","24/Aug/08 19:11;olgan;Alan, how do I validate the patch?","25/Aug/08 10:57;shravanmn;But the calculateProgress does the same thing. It returns the progress of all the jobs as (sum of all successful jobs and running progress of running jobs). The launchers know how many jobs were launched. If JobControl works correctly, the completion check to see that the progress returned by calculateProgress equals num MR jobs submitted is right. Can you give me a sample script where it was failing?","25/Aug/08 23:22;olgan;patch committed. thanks, Alan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Union doesn't work,PIG-390,12402822,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,arthurz,arthurz,21/Aug/08 22:01,25/Mar/10 00:12,14/Mar/19 03:05,26/Jan/09 21:59,,,,,,,0.2.0,,,,,0,,,,"data files:

$ cat ~/tmp/data
1	1
2	1
3	10

$ cat ~/tmp/data-2
4	20
5	20

pig script:
data = load '/Users/arthur/tmp/data' as (x, y);
data2 = load '/Users/arthur/tmp/data-2' as (x, y);
both = union data, data2;
dump both;

result:
(4, 20)
(5, 20)
",Mac OS X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-10-06 00:32:30.724,,,no_permission,,,,,,,,,,,,164018,,,,,Mon Jan 26 21:59:01 UTC 2009,,,,,,,0|i0gjnb:,94613,,,,,,,,,,"31/Aug/08 00:18;arthurz;Here's a workaround I'm using:

package com.cooliris.analytics;

import java.io.IOException;

import org.apache.pig.EvalFunc;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.Tuple;

/**
 * Implements a UNIONALL Pig function. It accepts a tuple of the format <unused, {bag-1}, {bag-2}, {bag-3}, ...>
 * and outputs a set of tuples corresponding to UNION bag-1, bag-2, bag-3, ... . This is intended as a workaround
 * to bug PIG-390 — Union doesn't work.
 * 
 * Instead of:
 *   combined = UNION data1, data2, data3;
 * ...do the following:
 *   cg_combined = COGROUP data1 BY 1, data2 BY 1, data3 BY 1;
 *   combined = FOREACH cg_combined GENERATE FLATTEN(com.cooliris.analytics.UNIONALL(*));
 * 
 * @author arthur@cooliris.com
 */
public class UNIONALL extends EvalFunc<DataBag> {

    @Override
    public void exec(Tuple input, DataBag output) throws IOException {
        for (int i = 1; i < input.arity(); ++i) {
            for (Tuple nested : input.getBagField(i)) {
                output.add(nested);
            }
        }
    }
}
","06/Oct/08 00:32;kevinweil;An update to this bug: it appears to be fixed in the types branch, using hadoop 0.18.1 and the stable-1 svn tag.

Kevin",26/Jan/09 21:59;olgan;Works in the latest trunk (merge from types branch),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unary minus not supported in FILTER expressions,PIG-389,12402810,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,tdunning@veoh.com,tdunning@veoh.com,21/Aug/08 18:49,25/Mar/10 00:12,14/Mar/19 03:05,26/Jan/09 21:57,0.1.0,,,,,,0.2.0,,,,,0,,,,"This script fails due to the ""-cnt"" expression.  removing the negation allows it to succeed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-01-26 21:57:22.572,,,no_permission,,,,,,,,,,,,164017,,,,,Mon Jan 26 21:57:22 UTC 2009,,,,,,,0|i0gjmv:,94611,,,,,,,,,,"21/Aug/08 18:50;tdunning@veoh.com;Here is the script

log = LOAD 'tutorial/data/excite-small.log' USING PigStorage('\t') as (ip, date, query);

t1 = GROUP log by ip;
describe t1;

counts = FOREACH t1 {
   cnt = COUNT(log);
   GENERATE group as ip, cnt as cnt;
}
describe counts;

r = FILTER counts by -cnt > 2;
describe r;
dump r;
",26/Jan/09 21:57;olgan;works with the latest code,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FILTER doesn't allow string expressions,PIG-388,12402809,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,tdunning@veoh.com,tdunning@veoh.com,21/Aug/08 18:46,25/Mar/10 00:12,14/Mar/19 03:05,26/Jan/09 21:54,0.1.0,,,,,,0.2.0,,grunt,,,0,,,,"The following scripts fails for a string expression, but works for an arithmetic expression.

log = LOAD 'tutorial/data/excite-small.log' USING PigStorage('\t') as (ip, date, query);

t1 = GROUP log by ip;
describe t1;

counts = FOREACH t1 {
   cnt = COUNT(log);
   GENERATE group as ip, cnt as cnt;
}
describe counts;

r1 = FILTER counts by cnt > 2;
r2 = FILTER counts by ip ne 'BED75271605EBD0C';
describe r1;
describe r2
dump r2;
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-01-26 21:54:15.489,,,no_permission,,,,,,,,,,,,164016,,,,,Mon Jan 26 21:54:15 UTC 2009,,,,,,,0|i0gjmf:,94609,,,,,,,,,,26/Jan/09 21:54;olgan;ne seems to be broken in the latest code and I will open a separate bug on that but using != works,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unary minus not supported in FOREACH expressions,PIG-387,12402808,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,tdunning@veoh.com,tdunning@veoh.com,21/Aug/08 18:40,25/Mar/10 00:12,14/Mar/19 03:05,26/Jan/09 21:08,0.1.0,,,,,,0.2.0,,grunt,,,0,,,,"Grunt parser broken with respect to unary minus.

This script provides an example.


log = LOAD 'tutorial/data/excite-small.log' USING PigStorage('\t') as (ip, date, query);

t1 = GROUP log by ip;
describe t1;

counts = FOREACH t1 {
   cnt = COUNT(log);
   -- unary minus fails
   neg = -cnt;
   -- prefixing to make the negation a subtraction works
   -- neg = 0-cnt;
   GENERATE group as ip, cnt as cnt, neg as key;
}
describe counts;

sorted = ORDER counts by key;
describe sorted;
dump sorted;

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-01-26 21:08:32.486,,,no_permission,,,,,,,,,,,,164015,,,,,Mon Jan 26 21:08:32 UTC 2009,,,,,,,0|i0gjlz:,94607,,,,,,,,,,26/Jan/09 21:08;olgan;resolved in the latest code,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DESCRIBE does not display the table name,PIG-383,12402653,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,alangates,yhan,yhan,19/Aug/08 18:42,24/Mar/10 22:04,14/Mar/19 03:05,27/Aug/08 00:15,0.2.0,,,,,,0.2.0,,,,,1,,,,"In PIG 1.4 DESCRIBE shows this:

out_links: (sig, outlinks, dup, crawled, errorcode )

But in PIG 2.0 it shows this:

{sig: chararray,outlinks: chararray,dup: chararray,crawled: chararray,errorcode: integer}

No name for the table.",,,,,,,,,,,,,,,,,,,26/Aug/08 22:21;alangates;PIG-383.patch;https://issues.apache.org/jira/secure/attachment/12388957/PIG-383.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-08-26 22:21:58.153,,,no_permission,,,,,,,,,,,,627,,,,,Wed Aug 27 00:15:58 UTC 2008,,,,,,,0|i0gjk7:,94599,,,,,,,,,,26/Aug/08 22:21;alangates;Adds relation name to describe.,27/Aug/08 00:15;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bincond does not performa implicit cast of parameters,PIG-382,12402583,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,shravanmn,olgan,olgan,18/Aug/08 20:22,24/Mar/10 22:04,14/Mar/19 03:05,21/Aug/08 19:26,0.2.0,,,,,,0.2.0,,,,,0,,,,"The following script

grunt> A = load 'foo' as (name, age, gpa);
grunt> B = foreach A generate ((name matches 'bob') ? name : '');
grunt> dump B; 

produces error below. Also, I am not sure why the error is not produce after just the second line. You need dump to see the error

2008-08-18 13:18:37,442 [main] WARN  org.apache.pig.PigServer - bytearray is implicitly casted to chararray under LORegexp Operator
2008-08-18 13:18:37,442 [main] ERROR org.apache.pig.PigServer - Two inputs of BinCond do not have compatible types
2008-08-18 13:18:37,442 [main] ERROR org.apache.pig.PigServer - Problem resolving LOForEach schema Two inputs of BinCond do not have compatible types
2008-08-18 13:18:37,442 [main] ERROR org.apache.pig.PigServer - Severe problem found during validation org.apache.pig.impl.plan.PlanValidationException: An unexpected exception caused the validation to stop
2008-08-18 13:18:37,447 [main] ERROR org.apache.pig.tools.grunt.GruntParser - java.io.IOException: Unable to open iterator for alias: B [Two inputs of BinCond do not have compatible typesProblem resolving LOForEach schema Two inputs of BinCond do not have compatible typesSevere problem found during validation org.apache.pig.impl.plan.PlanValidationException: An unexpected exception caused the validation to stop]
        at org.apache.pig.PigServer.compileLp(PigServer.java:582)
        at org.apache.pig.PigServer.execute(PigServer.java:516)
        at org.apache.pig.PigServer.openIterator(PigServer.java:310)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:258)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:175)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:92)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:58)
        at org.apache.pig.Main.main(Main.java:278)
Caused by: org.apache.pig.backend.executionengine.ExecException: Two inputs of BinCond do not have compatible typesProblem resolving LOForEach schema Two inputs of BinCond do not have compatible typesSevere problem found during validation org.apache.pig.impl.plan.PlanValidationException: An unexpected exception caused the validation to stop
        ... 8 more
Caused by: org.apache.pig.impl.plan.PlanValidationException: An unexpected exception caused the validation to stop
        at org.apache.pig.impl.plan.PlanValidator.validateSkipCollectException(PlanValidator.java:104)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingValidator.validate(TypeCheckingValidator.java:40)
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingValidator.validate(TypeCheckingValidator.java:30)
        at org.apache.pig.impl.logicalLayer.validators.LogicalPlanValidationExecutor.validate(LogicalPlanValidationExecutor.java:79)
        at org.apache.pig.PigServer.compileLp(PigServer.java:549)
        ... 7 more
Caused by: org.apache.pig.impl.plan.VisitorException: Problem resolving LOForEach schema Two inputs of BinCond do not have compatible types
        at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.visit(TypeCheckingVisitor.java:2251)
        at org.apache.pig.impl.logicalLayer.LOForEach.visit(LOForEach.java:88)
        at org.apache.pig.impl.logicalLayer.LOForEach.visit(LOForEach.java:37)
        at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:68)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.impl.plan.PlanValidator.validateSkipCollectException(PlanValidator.java:101)",,,,,,,,,,,,,,,,,,,21/Aug/08 18:10;shravanmn;382.patch;https://issues.apache.org/jira/secure/attachment/12388687/382.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-08-20 11:49:30.386,,,no_permission,,,,,,,,,,,,164011,,,,,Thu Aug 21 19:26:48 UTC 2008,,,,,,,0|i0gjjr:,94597,,,,,,,,,,"20/Aug/08 11:49;shravanmn;Two parts:
1) Why did it not complain earlier?
     This is also related to the GruntParser using things separately. From GruntParser we only call registerQuery which does not call execute unless it hits a store. Execute gets called directly from grunt only from processDump(). We need to do type checking inside registerQuery and not in execute if we want to see the behavior we are expecting.

2) I still need to look at the binCond issue","21/Aug/08 18:10;shravanmn;Fixed the issue mentioned where implicit casts are not being inserted for BinCond. 

Also there was another bug in the parser where it did not set the type of Const in RegExp. That is also fixed in this patch. 

However, there is another issue where the typechecking happens only on dump, which is reported in the bug. I think this is a separate bug unrelated to BinCond. I will create a new bug for that. It needs changes to the execution codepaths in PigServer. Will track this with the new bug.",21/Aug/08 19:26;olgan;patch committed. thanks shravan!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BinCond does not seem to handle null values,PIG-381,12402582,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,shravanmn,olgan,olgan,18/Aug/08 19:27,24/Mar/10 22:04,14/Mar/19 03:05,21/Aug/08 19:27,0.2.0,,,,,,0.2.0,,,,,0,,,,"Users see the following exception:

java.lang.NullPointerException
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POBinCond.getNext(POBinCond.java:115)
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:222)
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:155)
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:216)
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter.getNext(POFilter.java:94)
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:216)
        at
org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNext(POLocalRearrange.java:156)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:127)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:75)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:157)
2008-08-17 02:45:13,048 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.LocalLauncher -
Unsuccessful attempt. Completed 0.0% of the job
",,,,,,,,,,,,,,,,,,,21/Aug/08 11:18;shravanmn;381.patch;https://issues.apache.org/jira/secure/attachment/12388665/381.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-08-20 11:50:32.203,,,no_permission,,,,,,,,,,,,164010,,,,,Thu Aug 21 19:27:08 UTC 2008,,,,,,,0|i0gjjb:,94595,,,,,,,,,,"20/Aug/08 11:50;shravanmn;Related to Pig 355. If the Result.returnstatus is set to null appropriately, null should not be a problem with BinCond.",21/Aug/08 11:18;shravanmn;Changed BinCond to check if res.result is null along with res.returnStatus.,"21/Aug/08 19:27;olgan;patch committed. thanks, shravan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
invalid schema for databag constant,PIG-380,12402580,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,olgan,olgan,18/Aug/08 19:13,24/Mar/10 22:04,14/Mar/19 03:05,23/Sep/08 03:34,0.2.0,,,,,,0.2.0,,,,,0,,,,"One = load 'foo' using PigStorage() as (one: int);
DataBag = foreach One generate
        {
                ( 'a', 3, 'z' ),
                ( 'b', 3, 'z' ),
                ( 'a', 2, 'y' ),
                ( 'b', 2, 'y' ),
                ( 'a', 1, 'x' ),
                ( 'b', 1, 'x' )
        };

describe DataBag;

The result is {bytearray}",,,,,,,,,,,,,,,,,,,23/Sep/08 02:35;sms;PIG-380.patch;https://issues.apache.org/jira/secure/attachment/12390709/PIG-380.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-23 02:35:33.732,,,no_permission,,,,,,,,,,,,164009,,,,,Tue Sep 23 03:34:19 UTC 2008,,,Patch Available,,,,0|i0gjj3:,94594,,,,,,,,,,"23/Sep/08 02:35;sms;Attached patch (PIG-380.patch) addresses the following:

1. Computes the field schema of constants
2. Fixes issues in the type checker for bincond, mod and user function type checking
3. New unit tests and changes to existing unit tests to accommodate the schema computation of constants

Note: The schema of a bag constant will contain the schema of a tuple which in turn contains the columns.

All unit tests pass.","23/Sep/08 03:34;olgan;patch committed. thanks, santhosh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
describe interfiers with name resolution,PIG-379,12402579,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,alangates,olgan,olgan,18/Aug/08 18:57,24/Mar/10 22:04,14/Mar/19 03:05,27/Aug/08 00:00,0.2.0,,,,,,0.2.0,,,,,1,,,,"If I ran the following script:

A = load 'studenttab10k' as (name: chararray, age: int, gpa: float);
B = foreach A generate name, age;
describe B;
C = filter B by age > 30;
describe C;
D = group C by name;
describe D;

I get the error below. Also notice that the schema of C no longer have names:

{name: chararray,age: integer}
{chararray,integer}
java.io.IOException: Invalid alias: name in {chararray,integer}
        at org.apache.pig.PigServer.registerQuery(PigServer.java:254)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:422)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:241)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:82)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:64)
        at org.apache.pig.Main.main(Main.java:302)
Caused by: org.apache.pig.impl.logicalLayer.parser.ParseException: Invalid alias: name in {chararray,integer}
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.AliasFieldOrSpec(QueryParser.java:5179)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.ColOrSpec(QueryParser.java:5048)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseEvalSpec(QueryParser.java:3357)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.UnaryExpr(QueryParser.java:3254)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.CastExpr(QueryParser.java:3208)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.MultiplicativeExpr(QueryParser.java:3117)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.AdditiveExpr(QueryParser.java:3043)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.InfixExpr(QueryParser.java:3009)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItem(QueryParser.java:2911)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.GroupItem(QueryParser.java:1548)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.CogroupClause(QueryParser.java:1468)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:751)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:569)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:378)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:60)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:251)

If I remove describe, I don't see any errors",,,,,,,,,,,,,,,,,,,26/Aug/08 22:53;alangates;PIG-379.patch;https://issues.apache.org/jira/secure/attachment/12388960/PIG-379.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-08-20 12:23:06.24,,,no_permission,,,,,,,,,,,,618,,,,,Wed Aug 27 00:00:42 UTC 2008,,,,,,,0|i0gjin:,94592,,,,,,,,,,18/Aug/08 23:35;olgan;Bumpimng priority since we see users running into this issue,"20/Aug/08 12:23;sms;The describe statement kicks of the logical plan -> type checker -> optimizer process. During the logical plan optimization the schema of each operator is reset. When the schema of each operator is recomputed, the computation uses the attributes of the operator along with the information about its inputs. User defined schemas specified with the as clause are not annotated as such in each operator. As a result, when the schema is reset in the logical optimizer, this information is lost resulting in incorrect schemas.

There are multiple items that we need to consider:

1. Annotate each relational operator and expression operator with an attributed to denote presence of user specified schemas

2. Checks to ensure compatibility of user specified schemas with the generated/inferred schemas, i.e.,
   a. if the user specifies incorrect types, then perform appropriate checks and type promotions
   b. if the schema is a mismatch then flag it as an error

3. For complex constants, the schema computation is a bit complex and involves type promotions, null introductions, etc.","26/Aug/08 22:53;alangates;Changed describe to not call the optimizer.  There's no good reason for it to, and all the moving around of operators in the optimizer was causing problems for describe.",27/Aug/08 00:00;alangates;patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FOREACH followed by LIMIT produces wrong results,PIG-378,12402410,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,daijy,olgan,olgan,14/Aug/08 22:03,02/May/13 02:29,14/Mar/19 03:05,15/Aug/08 00:14,0.2.0,,,,,,0.2.0,,,,,0,,,,"Script:

grunt> A = load '/user/pig/tests/data/singlefile/studenttab10k' as (name, age, gpa);
grunt> B = foreach A generate name;
grunt> C = limit B 10;
grunt> dump C;

Output:

((luke king,65,0.73))
((fred miller,55,3.77))
((holly white,43,0.24))
((calvin brown,56,0.72))
((katie carson,25,3.65))
((holly davidson,57,2.43))
((holly davidson,59,1.26))
((luke steinbeck,51,1.14))
((nick underhill,31,2.46))
((ulysses thompson,64,1.90))

Note that we have a tuple with 3 fields within tuple",,,,,,,,,,,,,,,,,,,14/Aug/08 22:27;daijy;PIG-387.patch;https://issues.apache.org/jira/secure/attachment/12388285/PIG-387.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,164008,,,,,2008-08-14 22:03:02.0,,,,,,,0|i0gji7:,94590,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SET job.name does not work,PIG-376,12402259,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,olgan,yhan,yhan,13/Aug/08 00:16,24/Mar/10 22:04,14/Mar/19 03:05,26/Sep/08 22:52,0.2.0,,,,,,0.2.0,,grunt,,,0,,,,Put SET job.name there but the job name not changed.,,,,,,,,,,,,,,,,,,,15/Sep/08 21:10;viraj;PIG-376-patch;https://issues.apache.org/jira/secure/attachment/12390134/PIG-376-patch,26/Sep/08 21:30;olgan;PIG-376_2.patch;https://issues.apache.org/jira/secure/attachment/12391058/PIG-376_2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-08-25 23:52:20.845,,,no_permission,,,,,,,,,,,,164007,,,,,Fri Sep 26 22:52:01 UTC 2008,,,,,,,0|i0gjhb:,94586,,,,,,,,,,"25/Aug/08 23:52;olgan;Just to clarify.

In the trunk code, running command

set job.name='foo'

 would result if the 'foo' being used as the name of MR job. apparently, we have lost this functionlity in types branch.","15/Sep/08 21:10;viraj;Set the JobConf object with the jobname in the JobControlCompiler in getJobConf method. 
Populate the PigContext.JOB_NAME property during the initialization of the PigServer to set the default jobname to ""PigLatin:DefaultJobName""","15/Sep/08 23:30;olgan;I don't think the patch is quite right. 

I think you need to do the following:

(1) Make sure that default name is set in PigServer constructors. I can see that it is set in one but not in the other. You can look at PigServer in the trunk for comparison.
(2) Retrieve the value from the properties in PigContext and set them in JobConf in the JobControlCompiler. (It should be always set, you should never have to set it there.) Look at MapReduceLauncher.java in the trunk.",15/Sep/08 23:30;olgan;patch needs some changes,"26/Sep/08 21:31;olgan;made a couple of changes to the original change:

(1) set default name for both PigServer constructors
(2) don't set default name when setting up JobConf - it should already be set

please, review","26/Sep/08 21:51;alangates;""DefaultJobName"" is a really unhelpful name.  Something like ""PigJob"" would be more useful on grids where people use pig plus other tools.  

The code looks fine.",26/Sep/08 22:52;olgan;patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception with implicit split,PIG-375,12402236,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,shravanmn,olgan,olgan,12/Aug/08 18:38,24/Mar/10 22:04,14/Mar/19 03:05,14/Aug/08 22:22,0.2.0,,,,,,0.2.0,,,,,0,,,,"Script:

grunt> A = load 'foo';
grunt> B = foreach A generate $0;
grunt> C = foreach A generate $1;
grunt> D = Union B, C;
grunt> dump D;

Error:

08/08/12 11:36:57 ERROR plan.OperatorPlan: Attempt to give operator of type org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad multiple outputs.  This operator does not support multiple outputs.
08/08/12 11:36:57 ERROR physicalLayer.LogToPhyTranslationVisitor: Invalid physical operators in the physical planAttempt to give operator of type org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad multiple outputs.  This operator does not support multiple outputs.
08/08/12 11:36:59 ERROR grunt.GruntParser: java.io.IOException: Unable to open iterator for alias: E [null]
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:268)
        at org.apache.pig.PigServer.execute(PigServer.java:519)
        at org.apache.pig.PigServer.openIterator(PigServer.java:307)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:258)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:175)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:92)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:58)
        at org.apache.pig.Main.main(Main.java:278)
Caused by: org.apache.pig.backend.executionengine.ExecException
        ... 8 more
Caused by: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobCreationException
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.compile(JobControlCompiler.java:162)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.compile(JobControlCompiler.java:107)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:68)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:258)
        ... 7 more
Caused by: java.lang.ClassCastException: org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLimit
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJobConf(JobControlCompiler.java:203)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.compile(JobControlCompiler.java:156)
        ... 10 more",,,,,,,,,,,,,,,,,,,14/Aug/08 13:46;shravanmn;375.patch;https://issues.apache.org/jira/secure/attachment/12388238/375.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-08-14 13:45:36.566,,,no_permission,,,,,,,,,,,,164006,,,,,Thu Aug 14 22:22:55 UTC 2008,,,,,,,0|i0gjgv:,94584,,,,,,,,,,"12/Aug/08 18:38;olgan;Shravan, could you take a look, thanks","14/Aug/08 13:45;shravanmn;Wrote a walker(SplitIntroducer) which identifies implicit split condition and introduces a split into the plan. This is done while compilation of logicalplan in PigServer.compileLp before type checking. The split has true conditions in the form of constant expressions with true as the value.

Changes:
Added SplitIntroducer. Modified PigServer's compileLp to incorporate the call to splitintroducer. Added a test case to check the same.

Also included in this is the patch for pig-369. Details of the included patch are in the comments of Pig-369. 

I have caneled the patch submitted for pig-369. Please verify and close pig-369 along with this.",14/Aug/08 22:22;olgan;patch committed. thanks shravan for such a quick turnaround!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
describe schema does not show correct types,PIG-374,12402235,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,olgan,olgan,12/Aug/08 18:30,24/Mar/10 22:04,14/Mar/19 03:05,27/Aug/08 00:01,0.2.0,,,,,,0.2.0,,,,,0,,,,"grunt> B = load 'foo' USING PigStorage() AS (s:chararray);
grunt> describe B;
{s: bytearray}

This is because of lazy casting of the data. We need to store the ""user percieved type"" in addition to actual type and use the first in describe",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-08-27 00:01:39.278,,,no_permission,,,,,,,,,,,,691,,,,,Wed Aug 27 00:01:39 UTC 2008,,,,,,,0|i0gjg7:,94581,,,,,,,,,,27/Aug/08 00:01;alangates;Fixed by same change that fixed PIG-379,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unconnected load causes exception,PIG-373,12402234,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,shravanmn,olgan,olgan,12/Aug/08 18:28,24/Mar/10 22:04,14/Mar/19 03:05,26/Aug/08 18:41,0.2.0,,,,,,0.2.0,,,,,0,,,,"Load expects to be connected to another operator and when it is not,an exception is raised. Two scripts that cause this behavior

grunt> B = load 'foo' USING PigStorage() AS (s:chararray);
grunt> describe B;

grunt> B = load 'foo' USING PigStorage() AS (s:chararray);
grunt> dump B;",,,,,,,,,,,,,,,,,,,22/Aug/08 14:22;shravanmn;373-1.patch;https://issues.apache.org/jira/secure/attachment/12388742/373-1.patch,21/Aug/08 10:57;shravanmn;373.patch;https://issues.apache.org/jira/secure/attachment/12388662/373.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-08-12 18:38:23.377,,,no_permission,,,,,,,,,,,,18572,,,,,Tue Aug 26 18:41:42 UTC 2008,,,,,,,0|i0gjfr:,94579,,,,,,,,,,"12/Aug/08 18:32;olgan;Stack:

And the error is:
java.io.IOException: Unable to open iterator for alias: a [Unable to insert type casts into plan]
        at org.apache.pig.impl.logicalLayer.optimizer.TypeCastInserter.transform(TypeCastInserter.java:144)
        at org.apache.pig.impl.plan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:65)
        at org.apache.pig.PigServer.compileLp(PigServer.java:583)
        at org.apache.pig.PigServer.execute(PigServer.java:514)
        at org.apache.pig.PigServer.openIterator(PigServer.java:307)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:258)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:175)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:82)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:64)
        at org.apache.pig.Main.main(Main.java:302)
Caused by: org.apache.pig.impl.plan.optimizer.OptimizerException: Unable to insert type casts into plan
        ... 10 more
Caused by: java.lang.NullPointerException
        at org.apache.pig.impl.logicalLayer.optimizer.LogicalTransformer.insertAfter(LogicalTransformer.java:203)
        at org.apache.pig.impl.logicalLayer.optimizer.TypeCastInserter.transform(TypeCastInserter.java:139)
        ... 9 more

","12/Aug/08 18:38;pkamath;Another use case for the same issue:
{noformat}
grunt> register my.jar                                                               
grunt> a = load 'testdata' using MyLoader() as (s:map[], m, l);
grunt> b = foreach a generate s#'name';
grunt> describe a;
2008-08-12 18:33:45,960 [main] ERROR org.apache.pig.tools.grunt.GruntParser - java.io.IOException: Unable to describe schema for alias a [Unable to insert type casts into plan]
        at org.apache.pig.impl.logicalLayer.optimizer.TypeCastInserter.transform(TypeCastInserter.java:144)
        at org.apache.pig.impl.plan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:65)
        at org.apache.pig.PigServer.compileLp(PigServer.java:583)
        at org.apache.pig.PigServer.dumpSchema(PigServer.java:281)
        at org.apache.pig.tools.grunt.GruntParser.processDescribe(GruntParser.java:149)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:180)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:92)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:58)
        at org.apache.pig.Main.main(Main.java:278)
Caused by: org.apache.pig.impl.plan.optimizer.OptimizerException: Unable to insert type casts into plan
        ... 9 more
Caused by: java.lang.NullPointerException
        at org.apache.pig.impl.logicalLayer.optimizer.LogicalTransformer.insertAfter(LogicalTransformer.java:203)
        at org.apache.pig.impl.logicalLayer.optimizer.TypeCastInserter.transform(TypeCastInserter.java:139)
        ... 8 more

{noformat}",16/Aug/08 00:11;olgan;Alan suggested that we should not require that load requires that another operator follows it.,20/Aug/08 11:54;shravanmn;You are right olga. I have a fix for this in the LogicalTransformer. But svn is not working and hence could not submit patch. Will do it once svn is up.,21/Aug/08 10:57;shravanmn;373.patch attached,"21/Aug/08 19:58;olgan;Shravan, with you patch, load followed by dump works. However, describe still dies with the following stack:

08/08/21 12:57:58 ERROR grunt.GruntParser: java.io.IOException: Unable to describe schema for alias A [Unable to insert type casts into plan]
        at org.apache.pig.impl.logicalLayer.optimizer.TypeCastInserter.transform(TypeCastInserter.java:144)
        at org.apache.pig.impl.plan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:65)
        at org.apache.pig.PigServer.compileLp(PigServer.java:587)
        at org.apache.pig.PigServer.dumpSchema(PigServer.java:284)
        at org.apache.pig.tools.grunt.GruntParser.processDescribe(GruntParser.java:149)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:180)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:92)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:58)
        at org.apache.pig.Main.main(Main.java:278)
Caused by: org.apache.pig.impl.plan.optimizer.OptimizerException: Unable to insert type casts into plan
        ... 9 more
Caused by: java.lang.NullPointerException
        at org.apache.pig.impl.logicalLayer.optimizer.LogicalTransformer.insertAfter(LogicalTransformer.java:212)
        at org.apache.pig.impl.logicalLayer.optimizer.TypeCastInserter.transform(TypeCastInserter.java:139)
        ... 8 more",22/Aug/08 14:22;shravanmn;In the old patch I did the null check but forgot to break. Fixed that.,"24/Aug/08 18:38;olgan;Shravan, now describe works but dump does not. Do you have a cluster on which you can test both?","26/Aug/08 10:33;shravanmn;Olga, I tried the patch on the trunk both locally and on a cluster. It seems to work for me. I can see that both describe and dump work just fine.","26/Aug/08 15:32;olgan;Shravan,

I just tried again and here is my grunt session:

grunt> B = load 'foo' USING PigStorage() AS (s:chararray);
grunt> describe B;
{s: chararray}
grunt> C = load 'foo' USING PigStorage() AS (s:chararray);
grunt> dump C;
08/08/26 08:32:01 INFO mapReduceLayer.MapReduceLauncher: 0% complete
08/08/26 08:32:01 ERROR mapReduceLayer.MapReduceLauncher: Map reduce job failed
08/08/26 08:32:01 ERROR mapReduceLayer.MapReduceLauncher: java.io.IOException: foo does not exist
        at org.apache.pig.backend.executionengine.PigSlicer.validate(PigSlicer.java:105)
        at org.apache.pig.impl.io.ValidatingInputFileSpec.validate(ValidatingInputFileSpec.java:59)
        at org.apache.pig.impl.io.ValidatingInputFileSpec.<init>(ValidatingInputFileSpec.java:44)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:199)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:712)
        at org.apache.hadoop.mapred.jobcontrol.Job.submit(Job.java:347)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs(JobControl.java:247)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.run(JobControl.java:279)
        at java.lang.Thread.run(Thread.java:595)

08/08/26 08:32:01 ERROR grunt.GruntParser: java.io.IOException: Unable to open iterator for alias: C [Job terminated with anomalous status FAILED]
        at org.apache.pig.PigServer.openIterator(PigServer.java:315)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:258)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:176)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:92)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:58)
        at org.apache.pig.Main.main(Main.java:278)
Caused by: java.io.IOException: Job terminated with anomalous status FAILED
        ... 6 more","26/Aug/08 17:27;shravanmn;Is the file foo available to your config because its saying that it can't find the file 'foo'.

Are you using Local mode with hadoop-site.xml?","26/Aug/08 18:29;olgan;Shravan, my bad . WIll be commiting the patch shortly",26/Aug/08 18:41;olgan;patch committed . thanks shravan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
describe fails if schema is not specified,PIG-372,12402231,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,olgan,olgan,12/Aug/08 18:07,24/Mar/10 22:04,14/Mar/19 03:05,27/Aug/08 00:10,0.2.0,,,,,,0.2.0,,,,,0,,,,"a = LOAD 'foo' USING PigStorage();
DESCRIBE a;

java.lang.NullPointerException
        at org.apache.pig.PigServer.dumpSchema(PigServer.java:286)
        at org.apache.pig.tools.grunt.GruntParser.processDescribe(GruntParser.java:149)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:180)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:82)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:64)
        at org.apache.pig.Main.main(Main.java:302)

",,,,,,,,,,,,,,,,,,,26/Aug/08 22:38;alangates;PIG-372.patch;https://issues.apache.org/jira/secure/attachment/12388959/PIG-372.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-08-26 22:38:24.728,,,no_permission,,,,,,,,,,,,648,,,,,Wed Aug 27 00:10:23 UTC 2008,,,,,,,0|i0gjfb:,94577,,,,,,,,,,26/Aug/08 22:38;alangates;Added check that a schema exists before trying to print it.,27/Aug/08 00:10;alangates;patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dump with split does not work,PIG-370,12401984,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,shravanmn,shravanmn,shravanmn,08/Aug/08 12:05,24/Mar/10 22:04,14/Mar/19 03:05,26/Aug/08 19:31,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"The following script doesn't work:

A = load 'file:/etc/passwd' using PigStorage(':');
split A into A1 if $2<25, A2 if $2>25;
dump A1;

The error dump is:
2008-08-08 17:03:06,888 [main] WARN  org.apache.pig.PigServer - bytearray is implicitly casted to integer under LOGreaterThan Operator
2008-08-08 17:03:06,889 [main] WARN  org.apache.pig.PigServer - bytearray is implicitly casted to integer under LOLesserThan Operator
2008-08-08 17:03:06,970 [main] ERROR org.apache.pig.impl.plan.OperatorPlan - Attempt to give operator of type org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore multiple inputs.  This operator does not support multiple inputs.
2008-08-08 17:03:06,973 [main] ERROR org.apache.pig.tools.grunt.GruntParser - java.io.IOException: Unable to open iterator for alias: A1 [Attempt to give operator of type org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore multiple inputs.  This operator does not support multiple inputs.]
	at org.apache.pig.backend.local.executionengine.LocalExecutionEngine.execute(LocalExecutionEngine.java:158)
	at org.apache.pig.PigServer.execute(PigServer.java:519)
	at org.apache.pig.PigServer.openIterator(PigServer.java:307)
	at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:258)
	at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:175)
	at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:92)
	at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:58)
	at org.apache.pig.Main.main(Main.java:278)
Caused by: org.apache.pig.backend.executionengine.ExecException: Attempt to give operator of type org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore multiple inputs.  This operator does not support multiple inputs.
	... 8 more
Caused by: org.apache.pig.impl.plan.PlanException: Attempt to give operator of type org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore multiple inputs.  This operator does not support multiple inputs.
	at org.apache.pig.impl.plan.OperatorPlan.connect(OperatorPlan.java:169)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan.connect(PhysicalPlan.java:81)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan.connect(PhysicalPlan.java:1)
	at org.apache.pig.impl.plan.OperatorPlan.addAsLeaf(OperatorPlan.java:395)
	at org.apache.pig.backend.local.executionengine.LocalExecutionEngine.execute(LocalExecutionEngine.java:142)
	... 7 more

2008-08-08 17:03:06,973 [main] ERROR org.apache.pig.tools.grunt.GruntParser - Unable to open iterator for alias: A1 [Attempt to give operator of type org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore multiple inputs.  This operator does not support multiple inputs.]
2008-08-08 17:03:06,973 [main] ERROR org.apache.pig.tools.grunt.GruntParser - java.io.IOException: Unable to open iterator for alias: A1 [Attempt to give operator of type org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore multiple inputs.  This operator does not support multiple inputs.]

The issue is that PigServer.OpenIterator doesn't send the logical plan corresponding to A1 but sends the plan with both splitOutputs. I have a fix for this and will attach it. We have two parallel branches which do nearly the same thing. OpenIterator & Store. However they have individual code paths and we see the same script with a store not raise a plan exception. For now I will just copy some code. But we need to fix this in a better way. Otherwise we will leave some bugs that we fix for store in dump.",,,,,,,,,,,,,,,,,,,22/Aug/08 13:24;shravanmn;370-1.patch;https://issues.apache.org/jira/secure/attachment/12388735/370-1.patch,08/Aug/08 13:48;shravanmn;370.patch;https://issues.apache.org/jira/secure/attachment/12387816/370.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-08-21 22:24:16.231,,,no_permission,,,,,,,,,,,,164004,,,,,Tue Aug 26 19:31:00 UTC 2008,,,Patch Available,,,,0|i0gjen:,94574,,,,,,,,,,08/Aug/08 13:48;shravanmn;Please apply this patch after applying patch for PIG-343,"21/Aug/08 22:24;olgan;I verified that with this patch the problem with dump goes away. However, if I try and store A2 into a file, I see the following exception:

08/08/21 15:22:45 ERROR grunt.GruntParser: java.lang.NullPointerException
        at org.apache.pig.impl.logicalLayer.LOSplitOutput.getSchema(LOSplitOutput.java:74)
        at org.apache.pig.impl.logicalLayer.LOStore.getSchema(LOStore.java:88)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:418)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:60)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:252)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:422)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:241)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:92)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:58)
        at org.apache.pig.Main.main(Main.java:278)

08/08/21 15:22:45 ERROR grunt.GruntParser: 
08/08/21 15:22:45 ERROR grunt.GruntParser: java.lang.NullPointerException","22/Aug/08 13:24;shravanmn;Fixed the issue of dump and store not working. Two parts:
1) Fixed the openIterator method to use the store() codepath instead of its own to execute.
2) Because of some debug calls using getSchema method in LOSplitoutput which used an object without checking for null, there was a NPE. Fixed it to return a null schema if there were no predecessors of LOSplitOutput.

Also included is a test case to test the various split + dump + registerQuery(Store) + PigServer.store combinations ","24/Aug/08 19:09;olgan;The end-to-end tests that I tried now passing put one of the unit tests started to fail:

[junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.006 sec
    [junit] Test org.apache.pig.test.TestSplitStore FAILED
","26/Aug/08 15:53;shravanmn;Olga, even this one seems to be working for me. If it does not work for you could you please attach the log?","26/Aug/08 19:31;olgan;The problem was that the test file was built for new version of junit. Updated the test file and committed the patch.

thanks shravan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filter does not allow udf as the filter operator and only allows ComparisonOperators,PIG-369,12401949,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,shravanmn,pkamath,pkamath,08/Aug/08 01:07,24/Mar/10 22:04,14/Mar/19 03:05,14/Aug/08 21:31,0.2.0,,,,,,0.2.0,,,,,0,,,,"The following pig script does not work:
{code}
register util.jar;
define MyFilterSet util.FilterUdf('filter.txt');
A = load 'simpletest' using PigStorage() as ( x, y );
B = filter A by MyFilterSet(x);
dump B;
{code}

The following error is seen:
{noformat}

java -cp pig.jar:$localc org.apache.pig.Main filter.pig 
2008-08-07 17:59:37,663 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: localhost:9000
2008-08-07 17:59:37,748 [main] WARN  org.apache.hadoop.fs.FileSystem - ""localhost:9000"" is a deprecated filesystem name. Use ""hdfs://localhost:9000/"" instead.
2008-08-07 17:59:38,035 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to map-reduce job tracker at: localhost:9001
2008-08-07 17:59:38,166 [main] WARN  org.apache.hadoop.fs.FileSystem - ""localhost:9000"" is a deprecated filesystem name. Use ""hdfs://localhost:9000/"" instead.
java.io.IOException: Unable to open iterator for alias: B [org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc]
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter.setPlan(POFilter.java:179)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.LogToPhyTranslationVisitor.visit(LogToPhyTranslationVisitor.java:592)
        at org.apache.pig.impl.logicalLayer.LOFilter.visit(LOFilter.java:102)
        at org.apache.pig.impl.logicalLayer.LOFilter.visit(LOFilter.java:31)
        at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:68)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:245)
        at org.apache.pig.PigServer.compilePp(PigServer.java:590)
        at org.apache.pig.PigServer.execute(PigServer.java:516)
        at org.apache.pig.PigServer.openIterator(PigServer.java:307)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:258)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:175)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:82)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:64)
        at org.apache.pig.Main.main(Main.java:302)
Caused by: java.lang.ClassCastException: org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc
        ... 15 more

{noformat}

I looked further and the issue seems to be in POFilter which only thinks of the filter operator as a ComparisonOperator and doesn't allow a UDF for filtering:
{code}
public void setPlan(PhysicalPlan plan) {
        this.plan = plan;
        comOp = (ComparisonOperator) (plan.getLeaves()).get(0);
        compOperandType = comOp.getOperandType();
    }
{code}",,,,,,,,,,,,,,,,,,,13/Aug/08 12:54;shravanmn;369.patch;https://issues.apache.org/jira/secure/attachment/12388140/369.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-08-13 12:53:46.991,,,no_permission,,,,,,,,,,,,164003,,,,,Thu Aug 14 21:31:09 UTC 2008,,,,,,,0|i0gje7:,94572,,,,,,,,,,"13/Aug/08 12:53;shravanmn;Changes to POFilter: Made the leaf operator to a PhysicalOperator from ComparisonOperator
Changes to TypeCheckingVisitor: Was failing in checkInnerPlan as LOUserFunc was not one of the supported roots. Added it since it can occur in the inner plan of a filter and used the visit(LOUserFunc) method to do the necessary type checking and schema propogation.
Changes to Translator: Minor. The addition was causing a NPE. Fixed it by putting a null check
Added a new unit test for FilterUDF",14/Aug/08 13:04;shravanmn;I am including this patch in the patch for Pig-375.,"14/Aug/08 21:31;olgan;patch committed; thanks, shravan!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
User defined Loader functions need a way to get jobconf without going through Slicer,PIG-368,12401937,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,07/Aug/08 21:47,24/Mar/10 22:04,14/Mar/19 03:05,07/Aug/08 22:12,0.2.0,,,,,,0.2.0,,,,,0,,,,Some user defined loader functions in the current pig release (without types) need the JobConf to build the appropriate RecordReader. Currently they do this in a round about way by using the Slicer. The jobConf should be available from PigInputFormat.,,,,,,,,,,,,,,,,,,,07/Aug/08 21:48;pkamath;PIG-368.patch;https://issues.apache.org/jira/secure/attachment/12387771/PIG-368.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-08-07 22:12:02.048,,,no_permission,,,,,,,,,,,,164002,,,,,Thu Aug 07 22:12:02 UTC 2008,,,,,,,0|i0gjdr:,94570,,,,,,,,,,07/Aug/08 21:47;pkamath;Attached  patch,07/Aug/08 22:12;olgan;patch committed. thanks pradeep for contributing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit return incorrect records when we use multiple reducer,PIG-364,12401845,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,07/Aug/08 00:35,24/Mar/10 22:04,14/Mar/19 03:05,19/Sep/08 19:10,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"Currently we put Limit(k) operator in the reducer plan. However, in the case of n reducer, we will get up to n*k output. ",,,,,,,,,,,,,,,,,,,14/Sep/08 04:37;daijy;PIG-364-2.patch;https://issues.apache.org/jira/secure/attachment/12390073/PIG-364-2.patch,19/Sep/08 06:03;daijy;PIG-364-3.patch;https://issues.apache.org/jira/secure/attachment/12390457/PIG-364-3.patch,11/Sep/08 05:31;daijy;PIG-364.patch;https://issues.apache.org/jira/secure/attachment/12389903/PIG-364.patch,18/Sep/08 20:06;shravanmn;limitsplit.png;https://issues.apache.org/jira/secure/attachment/12390412/limitsplit.png,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2008-09-11 19:47:37.845,,,no_permission,,,,,,,,,,,,163998,,,,,Fri Sep 19 19:10:03 UTC 2008,,,,,,,0|i0gjbz:,94562,,,,,,,,,,"08/Aug/08 00:58;daijy;Seems no perfect solution. Here are three possible treatments:
1. If there is a limit in reducer, and number of reducer > 1, add another map-reduce after that with only 1 reducer
    Cons: extra-overhead
2. Instead of map-reduce, manupilate output file directly, keep top k in output file
    Cons: not orthodox, extra-overhead (but not as much as 1)
3. If there is a limit in reducer, change the parallel degree of the reducer to 1
    Cons: can not take advantage of parallel processing for reducer","11/Sep/08 05:31;daijy;This patch takes approach 1. It will add one additional map-reduce operator with 1 reducer if the requested parallelism > 1. Now the behavior of limit is:

1. If the map plan is closed before POLimit operator, we put POLimit in reduce plan, grant requested parallelism, if requested parallelism > 1, close reduce plan, add one additional map-reduce operator with 1 reducer

2. If the map plan is open before POLimit operator, we put POLimit in map plan, close map plan, add another POLimit to reduce plan, and set parallelism of this map-reduce operator 1. Although in this case, POLimit create a map-reduce boundary, we do not associate a parallel option with limit keyword. I believe provide a parallel option with limit will arouse confusion to the user, because it is relatively hard to explain to the user whether this parallel option will be granted or not

3. In limited sort case, we will have POSort with limit<>-1. If the parallelism for POSort > 1, we add one additional map-reduce operator with 1 reducer
","11/Sep/08 19:47;olgan;Shravan, could you please review this patch, thanks","12/Sep/08 20:29;shravanmn;The patch seems to follow the approach mentioned, but I think there is a problem with this approach. I tested it both with and without the patch. The problem exists in both cases. Consider the following script:

A = load 'st10M';
B = limit A 10;
C = filter B by 2>1 parallel 10;
dump C;

I would expect to see only 10 tuples here. But I see 40 tuples here. The reason is that there were 4 mappers that produced 40 tuples in all and there were 10 reducers asked by filter since limit crosses map-reduce boundary. There was no limiting action done by the limit on the reduce side as there is capacity to pass 10*10=100 tuples.

The problem is that we cannot guarantee the parallelism by setting the requested parallelism to some value while visiting limit as it can get modified further down the line which is shown in the example. The same case happens even in the limter in the reducer case for the following script where I see 97 tuples instead of the expected 10. If everything went right (that is no tuples are clipped) this should have been 100 and the actual output is pretty close:

A = load 'st10M';
B1 = group A by $0 parallel 10;
B = limit B1 10;
C = filter B by 2>1 parallel 10;
dump C;

One way I could think of is to terminate the new reduce phase created with a store therby ensuring that the parallelism is ensured to be that set during limit. Then start a new MapReduceOper by loading this file. Another way is to disable further changes to the reduce parallelism by maintaining a flag which controls the changes to the parallelism of the map reduce operator. But this would mean that we disobey the user's request, which might be meaningless at some places. Also, the semantics of parallel will be distorted when the limit is in picture.","12/Sep/08 21:30;olgan;Would the following work:

If a limit is present, always add an MR state with single reducer unless the whole plan is already terminated by a single reducer?","13/Sep/08 02:24;daijy;I see the problem. The largest requestedParallelism determine the number of reducers. I thought it was determined by the operator creating the map-reduce boundary. Then I have to do this check after complete compiling a map-reduce operator, if it contains a limit and requestedParallelism>1, then add a singler reducer after that. Thank you for pointing out.","14/Sep/08 04:37;daijy;The new patch add a MRPlan visitor after compiling. For each MapReduceOper, if it contains limit and requestedParallelism>1, insert a MapReduceOper after it with 1 reducer.","16/Sep/08 21:15;shravanmn;I see a couple of problems with the patch.
1) When you insert the new limitAdjustMROp MapReduceOper into the plan, the case where multiple successors might exist is not handled. For instance if there is a split after the limit, then you will only insert the limitMROp for the first outgoing edge.

2) This is more of a semantic issue. By following the approach in the patch the semantics of limit do not hold. Consider the following:
A = load 'URLs' as (url:string, pagerank:double);
B = order A by pagerank parallel 100;
C = limit B 10;
D = foreach C generate url, CRAWL(url);
store D into 'crawledpages';

Here I would expect to crawl only the top 10 pages. However, with the current patch, I would probably crawl 1000 pages and trim my result to 10. This might not be what users want.

3) If at all we decide to go with this approach afterx fixing 1, it is probably a good idea to introduce a limit operator into the map of limitAdjustMROp.",17/Sep/08 19:00;olgan;Lets fix (1) and leave (2) alone for now,18/Sep/08 05:27;olgan;updated patch is needed,"18/Sep/08 07:12;daijy;For Shravan's comments
1) Can you give a script? I tested several split after limit cases, didn't get this error. 
2) As discussed in [PIG-171|https://issues.apache.org/jira/browse/PIG-171], since it seems to be no simple solution, so we just leave it as it is for now
3) No problem if this looks better

Thank you","18/Sep/08 20:10;shravanmn;Consider the following script:
{noformat}
a = load 'file:/etc/passwd';
b = limit a 10;
c = filter b by 2>1 parallel 10;
split c into c1 if 2>1, c2 if 2>1;
d = group c1 by $0;
e = group c2 by $0;
f = group d by $0, e by $0;
dump f;
{noformat}

This is a case where, multiple MROps are generated at the split as shown in the figure below, if what I understand from the code is right.

!https://issues.apache.org/jira/secure/attachment/12390412/limitsplit.png!

Now when the job controller sees this graph of MROps, it first schedules the LD MROp. To remind you, the limitadjuster has now changed the output of this to some temporary file. At this point, the controller has an option to schedule both the Lim Adj Op and the free 2-LRs Op whose dependency has been just resolved. If at all the choice is to execute the 2-LRs oP it tries to read the original output of the split which doesn't exist since the Lim Adj Op hasn't run yet and will fail. However if it decides to choose the Lim Adj Op, things will go fine.

In order to avoid this, we need to make sure to disconnect all the successors and make the Lim Adj Op their predecessor and connect Lim Adj Op to LD as indicated in the figure.

Let me know if I my understanding is wrong.","19/Sep/08 06:03;daijy;Hi, Shravan,
Thank you for your detailed explanation. Here is the modified patch addressing your comment 1 and 3. I tested using the following script:

a = load 'studenttab10k';
b = group a by $0 parallel 10;
c = limit b 10;
split c into c1 if $0 lt 'bob white', c2 if $0 gte 'bob white';
c12 = group c1 by $0;
c22 = group c2 by $0;
c4 = union c12, c22;
dump c4;
",19/Sep/08 15:40;shravanmn;Looks good to me. Thanks Daniel for incorporating all the comments!,"19/Sep/08 19:10;olgan;patch committed. thanks, daniel and shravan!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Describe does not produce correct schema for generate with UDF,PIG-363,12401844,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,pkamath,olgan,olgan,07/Aug/08 00:13,24/Mar/10 22:04,14/Mar/19 03:05,07/Aug/08 21:55,0.2.0,,,,,,0.2.0,,,,,0,,,,"B = foreach A generate string.CONCATSEP(' ', $0, 'foo');
describe B;
{chararray}

UDF has the following outputSchema function:

public Schema outputSchema(Schema input) {
                String name = ""concatsep_"";
                if (input.getAliases().size() > 0)
                        name += input.getAliases().iterator().next() + ""_"";
                return new Schema(new Schema.FieldSchema(name +  getNextSchemaId(), DataType.CHARARRAY));
        }",,,,,,,,,,,,,,,,,,,07/Aug/08 21:44;pkamath;desc.patch;https://issues.apache.org/jira/secure/attachment/12387769/desc.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-08-07 21:44:10.276,,,no_permission,,,,,,,,,,,,697,,,,,Thu Aug 07 21:55:47 UTC 2008,,,Patch Available,,,,0|i0gjbj:,94560,,,,,,,,,,"07/Aug/08 17:18;olgan;Alan says this is likely that generate code does not query the schema of UDF. He suggested we look at code in src/org/apache/pig/impl/logicalLayer/validators/TypeCheckingVisitor.java
",07/Aug/08 21:44;pkamath;Attached patch to fix this,07/Aug/08 21:55;olgan;patch verified and committed. thanks pradeep,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit can not push in front of ForEach with flatten,PIG-362,12401839,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,06/Aug/08 22:54,24/Mar/10 22:04,14/Mar/19 03:05,19/Aug/08 00:09,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"Currently logical optimizer will push Limit in front of ForEach with flatten. It is based on the assumption that ForEach with a flatten always increase the number of records. However, this is a false assumption. In the case that there is empty bags inside input tuple, the number of output records can be 0, thus less than input records. 

We have no way to know whether there is an empty bag in the input at optimization time. So the only solution is not to push Limit in front of ForEach with flatten",,,,,,,,,,,,,,,,,,,15/Aug/08 21:38;daijy;PIG-362-2.patch;https://issues.apache.org/jira/secure/attachment/12388336/PIG-362-2.patch,15/Aug/08 00:05;daijy;PIG-362.patch;https://issues.apache.org/jira/secure/attachment/12388289/PIG-362.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-08-15 16:46:08.71,,,no_permission,,,,,,,,,,,,163997,,,,,Tue Aug 19 00:09:39 UTC 2008,,,,,,,0|i0gjb3:,94558,,,,,,,,,,"15/Aug/08 16:46;olgan;Daniel, could you add a unit test for this to your patch. thanks",15/Aug/08 21:38;daijy;Include test case,19/Aug/08 00:09;olgan;patch committed. thanks daniel for contributing!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Semantics of generate * have changed,PIG-359,12401714,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,shravanmn,alangates,alangates,05/Aug/08 17:45,24/Mar/10 22:04,14/Mar/19 03:05,27/Aug/08 21:43,0.2.0,,,,,,0.2.0,,impl,,,1,,,,"In the main trunk, the script

A = load 'myfile';
B = foreach A generate *;

returns:

(x, y, z)

In the types branch, it returns:

((x, y, z))

There is an extra level of tuple in it.  In the main branch generate * seems to include an implicit flatten.",,,,,,,,,,,,,,,,,,,25/Aug/08 10:47;shravanmn;359-1.patch;https://issues.apache.org/jira/secure/attachment/12388839/359-1.patch,22/Aug/08 14:20;shravanmn;359.patch;https://issues.apache.org/jira/secure/attachment/12388741/359.patch,28/Aug/08 21:40;alangates;PIG-359-2.patch;https://issues.apache.org/jira/secure/attachment/12389126/PIG-359-2.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2008-08-18 23:32:33.05,,,no_permission,,,,,,,,,,,,163994,,,,,Thu Aug 28 22:52:49 UTC 2008,,,,,,,0|i0gj9r:,94552,,,,,,,,,,18/Aug/08 23:32;olgan;Bumping the priority since we see users who are trying this code running into this issue,"21/Aug/08 21:15;olgan;Shravan, could you take a look please",22/Aug/08 14:20;shravanmn;Added a check in CreateTuple to see if we have a Single Tuple inside a Tuple and added logic to return the inner tuple if so.,"22/Aug/08 14:54;olgan;Shravan, why is it always a good idea to do this? This is not * specific?","25/Aug/08 10:47;shravanmn;You are right olga. This is * specific. Changed the patch to include the following:
In foreach when the operator gets created it also creates a list of the leaves of its inner plans for optimization. Here I also check if the leaf of an innerplan is a project(*). If so I set flatten true for that plan. This causes the foreach logic to flatten tuples.

The same was the case in POUserFunc when you process * as an input. The semantics were different from the trunk. So changed it in a similar way to ensure the trunk behaviour. 

Because of the changes, needed to change a test case and a golden file.

All of them inculded in 359-1. Thanks Olga for reviewing.","26/Aug/08 21:45;alangates;I don't think we want the changes to POUserFunc.  In the cases of udf(*) the right thing will happen in the existing code because lines 159-161 handle making sure we don't double wrap tuples.  And removing these lines causes problems for scripts like this:

A = load 'myfile' as a:tuple (...);
B = foreach A generate udf(a);

Now 'a' will be double wrapped (that is, there will be a tuple containing just the tuple 'a').  This isn't what we want.

The changes to POForEach look good.","27/Aug/08 04:04;shravanmn;Alan, two things. 
1) The current code isn't enough because of the following:
A = load 'file:/etc/passwd' using PigStorage(':');
B = foreach A generate ARITY(*,*);
dump B;

Trunk emits 14(2 times the artiy of each tuple in A which is 7). The current code would emit two. Another example of what current code doesn't handle is

A = load 'file:/etc/passwd' using PigStorage(':');
B = foreach A generate ARITY($0, '---', *);
Trunk emits 9(2 + 7). Current code would emit 3.

2) You are right in saying that 'a' will be double wrapped. But thats how trunk works right now and I think its right because consider this script:

A = load 'myfile' as (a:tuple(...), b:tuple(...));
B = foreach A generate udf(a,b);

We want 'a', 'b' to be intact inside the tuple input that is being passed to the UDF. So we would expect the arity to be two instead of 2 times the arity of 'a' & 'b'. Generalizing this, I think double wrapping should be ok. The way I tested this behaviour in trunk is by writing a UDF that returns a Tuple say TupleOutputUDF, which just copies the input tuple to the output. I tried the following script in trunk:
A = load 'file:/etc/passwd' using PigStorage(':');
B = foreach A generate ARITY(TupleOutputUDF(*));
dump B;

with a return value of 1. The current code returns 7.","27/Aug/08 21:43;alangates;I still don't like the double wrapping.  But Shravan is correct that this matches the previous behavior, and there's no good reason to change it so we shouldn't.  The patch has been checked in.","28/Aug/08 16:58;sms;In POUserFunc.java, the following code makes an assumption that project( * ) always returns a tuple. In the foreach nested block, we could be projecting bags, at which point the code will fail with ClassCastException. E.g: testNestedPlan in TestEvalPipeline.java

{code}
                
+                if(op instanceof POProject){
+                    POProject projOp = (POProject)op;
+                    if(projOp.isStar()){
+                        Tuple trslt = (Tuple) temp.result;
+                        Tuple rslt = (Tuple) res.result;
+                        for(int i=0;i<trslt.size();i++)
+                            rslt.append(trslt.get(i));
+                        continue;
+                    }
+                }
{code}",28/Aug/08 21:40;alangates;This patch addresses the issues Santhosh identified with the patch 359-1.,28/Aug/08 22:52;sms;+1 for Pig-359-2.patch. Looks good.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change to default outputSchema for UDFs,PIG-354,12401514,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,,olgan,olgan,01/Aug/08 21:51,24/Mar/10 22:04,14/Mar/19 03:05,07/Aug/08 17:14,0.2.0,,,,,,0.2.0,,,,,0,,,,"Currently, if UDF writer does not specify outputSchema the default is bytearray which is not what you would want most of the time. Making chararray a default would make things backward compatible.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-08-04 22:07:44.879,,,no_permission,,,,,,,,,,,,163989,,,,,Thu Aug 07 17:14:15 UTC 2008,,,,,,,0|i0gj7r:,94543,,,,,,,,,,03/Aug/08 22:56;olgan;I think the same should be true for default input as well,"04/Aug/08 22:07;alangates;I don't think we want to be converting data to chararray by default for input to UDFs, for several reasons:

1 It's expensive
2 It mangles any data that isn't utf8
3 It is a fair amount of work for users to provide type specific implementations of their UDFs, and so I suspect most won't.

By contrast, on the outbound side I agree that chararray is the right default, for two reasons:

1 It's very easy to determine what type the UDF is returning, either by declaring a schema or by pig reflecting the return type.  Only in the case where they do not give a schema and their return type is tuple or bag (thus we have no idea what inside that tuple or bag) will we be forcing data to strings.

2 In general pig does not assume any particular representation of data in byte arrays.  That's why we make the load function provide casts.  So if we took this unknown data from UDFs to be byte arrays we'd have no idea how to convert it to anything else.  Conversions from strings on the other hand are well understood.","07/Aug/08 15:38;alangates;Here's how output type determination works now:  the type checker calls outputSchema on the UDF.  If it gets a schema it goes with that.  If not, it uses reflection to determine the return type of the UDF and then maps that to a data type.  The only case where we can't really tell what the UDF is returning is if it doesn't declare a schema and its return type is Tuple or Bag.  In that case we have no way to guess what's inside.  But all through the code we treat tuples and bags with unknown contents as containing byte arrays.  If we try to do otherwise just for UDFs, it will be difficult (we'll end up tracking lineage).  so I don't want to change that.

The one other area we could change is POUserFunc.  Currently, when it has a type of bytearray, it checks if the object passed back is really a bytearray or not.  If not, it calls toString().toBytes() on it and constructs a DataByteArray from that. We could do the same check when the type is charray.  Not sure how useful this would be.","07/Aug/08 16:48;olgan;The first part sounds reasonable. 

What would be the use case for the second one?",07/Aug/08 17:14;olgan;Looks like the right things is already happening.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Certain parse errors force exit from grunt (interactive mode),PIG-353,12401513,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,olgan,olgan,01/Aug/08 21:21,24/Mar/10 22:04,14/Mar/19 03:05,25/Aug/08 22:59,0.2.0,,,,,,0.2.0,,,,,0,,,,"I tried the following load:

A = load 'foo' as (B: bag{T: tuple(I: int)});

and it gave me a huge stack and through me out of grunt",,,,,,,,,,,,,,,,,,,20/Aug/08 12:15;sms;bag_schema_bag_constant_as_clause.patch;https://issues.apache.org/jira/secure/attachment/12388595/bag_schema_bag_constant_as_clause.patch,18/Aug/08 12:37;sms;bag_schema_in_pigscript_parser.patch;https://issues.apache.org/jira/secure/attachment/12388431/bag_schema_in_pigscript_parser.patch,25/Aug/08 21:53;sms;pig_353.patch;https://issues.apache.org/jira/secure/attachment/12388878/pig_353.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2008-08-18 12:37:48.111,,,no_permission,,,,,,,,,,,,38375,,,,,Mon Aug 25 22:59:46 UTC 2008,,,Patch Available,,,,0|i0gj7b:,94541,,,,,,,,,,"18/Aug/08 12:37;sms;The attached patch addresses the issue of declaring bag schemas using grunt. PigScriptParser.java was modified to have an additional lexical state for schemas. The opening curly brace '{' in the bag schema was being treated as the delimiter of a nested block. With the additional lexical state the schema definition is tokenized appropriately.

Unit test cases that still fail are:

    [junit] Running org.apache.pig.test.TestBuiltin
    [junit] Tests run: 23, Failures: 1, Errors: 2, Time elapsed: 14.305 sec
    [junit] Test org.apache.pig.test.TestBuiltin FAILED

    [junit] Running org.apache.pig.test.TestEvalPipeline
    [junit] Tests run: 9, Failures: 1, Errors: 0, Time elapsed: 195.352 sec
    [junit] Test org.apache.pig.test.TestEvalPipeline FAILED

    [junit] Running org.apache.pig.test.TestFilterOpNumeric
    [junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 57.163 sec
    [junit] Test org.apache.pig.test.TestFilterOpNumeric FAILED

    [junit] Running org.apache.pig.test.TestMRCompiler
    [junit] Tests run: 16, Failures: 1, Errors: 0, Time elapsed: 0.353 sec
    [junit] Test org.apache.pig.test.TestMRCompiler FAILED

    [junit] Running org.apache.pig.test.TestStoreOld
    [junit] Tests run: 3, Failures: 0, Errors: 2, Time elapsed: 39.973 sec
    [junit] Test org.apache.pig.test.TestStoreOld FAILED",18/Aug/08 12:39;sms;Pig-349 (https://issues.apache.org/jira/browse/PIG-349) is a duplicate of this issue and has been marked so.,"18/Aug/08 18:07;olgan;Santhosh, the original problem seems to be gone. But the following case does not work:

B = load 'foo' as (B: bag{(I: int)});

Is this a valid syntax?","19/Aug/08 05:43;sms;B = load 'foo' as (B: bag{(I: int)}); is not valid syntax. Each element in the schema has to have a name. The valid syntax would be one of:

B = load 'foo' as (B: bag{t (I: int)});

OR

B = load 'foo' as (B: bag{t: tuple(I: int)});","20/Aug/08 12:15;sms;Patch fixes the following issues:

1. Grunt parser can disambiguate '{' amongst the foreach nesting, bag constant and bag schema declaration

2. The AS clause in conjunction with generate works. Fully qualified schema specified with the AS clause is set correctly.


When a column is aliased in the generate statement, the schema should be fully qualified. The patch failed testSUM1 in TestTypeChecking.java. A column cast to type long was aliased as age. Since the type information for age was not specified, bytearray is inferred as the type. Casts to bytearray are not supported. As a result, the type checker flagged this as an error. In order to fix, the alias had to be fully qualified, i.e., age:long

{code}

//old code
LogicalPlan plan1 = planTester.buildPlan(""b = foreach a generate (long)age as age, (int)gpa as gpa;"") ;

//modified code
LogicalPlan plan1 = planTester.buildPlan(""b = foreach a generate (long)age as age:long, (int)gpa as gpa:int;"") ;
{code}

Unit tests that still fail are:

    [junit] Running org.apache.pig.test.TestBuiltin
    [junit] Tests run: 23, Failures: 1, Errors: 2, Time elapsed: 14.297 sec
    [junit] Test org.apache.pig.test.TestBuiltin FAILED

    [junit] Running org.apache.pig.test.TestEvalPipeline
    [junit] Tests run: 9, Failures: 1, Errors: 0, Time elapsed: 145.656 sec
    [junit] Test org.apache.pig.test.TestEvalPipeline FAILED

    [junit] Running org.apache.pig.test.TestFilterOpNumeric
    [junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 57.466 sec
    [junit] Test org.apache.pig.test.TestFilterOpNumeric FAILED

    [junit] Running org.apache.pig.test.TestMRCompiler
    [junit] Tests run: 16, Failures: 1, Errors: 0, Time elapsed: 0.37 sec
    [junit] Test org.apache.pig.test.TestMRCompiler FAILED

    [junit] Running org.apache.pig.test.TestStoreOld
    [junit] Tests run: 3, Failures: 0, Errors: 2, Time elapsed: 40.966 sec
    [junit] Test org.apache.pig.test.TestStoreOld FAILED","21/Aug/08 21:06;olgan;Santhosh, I applied the patch and it made several cases work but the following still gives errors

A = load 'data' using PigStorage() as (
        x: chararray,
        y: int,
        z: chararray
        );

B = group A by ( x );

C = foreach B generate
        FOO(A) as pair: (
                curr: ( user, seq, query ),
                next: ( user, seq, query )
        );

describe C;","22/Aug/08 14:07;sms;If colon (:) is used in the schema, the type has to be specified. The following should work (not sure about describe though). I have removed the ':' after the identifier. The other option is to specify the type - pair: tuple, curr: tuple, next: tuple

{code}

A = load 'data' using PigStorage() as (
x: chararray,
y: int,
z: chararray
);

B = group A by ( x );

C = foreach B generate
FOO(A) as pair (
curr ( user, seq, query ),
next ( user, seq, query )
);

describe C;

{code}",24/Aug/08 18:08;olgan;this seems like a regression. why did we change that?,24/Aug/08 18:09;olgan;seems like the patch still has issues that need to be resolved,"25/Aug/08 21:53;sms;Removed the restriction of specifying the type information after colon ':' for a bag or a tuple. 

Unit tests that still fail are:

    [junit] Running org.apache.pig.test.TestBuiltin
    [junit] Tests run: 23, Failures: 1, Errors: 2, Time elapsed: 14.824 sec
    [junit] Test org.apache.pig.test.TestBuiltin FAILED

    [junit] Running org.apache.pig.test.TestEvalPipeline
    [junit] Tests run: 9, Failures: 1, Errors: 0, Time elapsed: 150.556 sec
    [junit] Test org.apache.pig.test.TestEvalPipeline FAILED

    [junit] Running org.apache.pig.test.TestFilterOpNumeric
    [junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 57.101 sec
    [junit] Test org.apache.pig.test.TestFilterOpNumeric FAILED

    [junit] Running org.apache.pig.test.TestMRCompiler
    [junit] Tests run: 16, Failures: 1, Errors: 0, Time elapsed: 0.445 sec
    [junit] Test org.apache.pig.test.TestMRCompiler FAILED","25/Aug/08 22:59;olgan;Patch committed. Thanks, Santhosh for contributing!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.ClassCastException when invalid field is accessed,PIG-352,12401448,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,olgan,olgan,31/Jul/08 23:27,24/Mar/10 22:04,14/Mar/19 03:05,21/Aug/08 20:22,0.2.0,,,,,,0.2.0,,,,,0,,,,"grunt> A = load 'foo' as (a, b, c);
grunt> B = foreach A generate $5;
2008-07-31 16:25:13,847 [main] ERROR org.apache.pig.tools.grunt.GruntParser - java.lang.ClassCastException: org.apache.pig.impl.logicalLayer.FrontendException
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:454)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:60)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:248)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:425)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:241)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:92)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:58)
        at org.apache.pig.Main.main(Main.java:278)",,,,,,,,,,,,,,,,,,,21/Aug/08 09:09;sms;out_of_bound_schema_access.patch;https://issues.apache.org/jira/secure/attachment/12388658/out_of_bound_schema_access.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-08-21 09:09:36.879,,,no_permission,,,,,,,,,,,,163988,,,,,Thu Aug 21 20:22:17 UTC 2008,,,,,,,0|i0gj6v:,94539,,,,,,,,,,"21/Aug/08 09:09;sms;This patch (out_of_bound_schema_access.patch) fixes the problem with accessing column numbers that are out of bound. A parse time exception is thrown with the appropriate error message.

Unit tests that still fail are:

    [junit] Running org.apache.pig.test.TestBuiltin
    [junit] Tests run: 23, Failures: 1, Errors: 2, Time elapsed: 14.579 sec
    [junit] Test org.apache.pig.test.TestBuiltin FAILED

    [junit] Running org.apache.pig.test.TestEvalPipeline
    [junit] Tests run: 9, Failures: 1, Errors: 0, Time elapsed: 210.738 sec
    [junit] Test org.apache.pig.test.TestEvalPipeline FAILED

    [junit] Running org.apache.pig.test.TestFilterOpNumeric
    [junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 57.131 sec
    [junit] Test org.apache.pig.test.TestFilterOpNumeric FAILED

    [junit] Running org.apache.pig.test.TestMRCompiler
    [junit] Tests run: 16, Failures: 1, Errors: 0, Time elapsed: 0.44 sec
    [junit] Test org.apache.pig.test.TestMRCompiler FAILED

    [junit] Running org.apache.pig.test.TestStoreOld
    [junit] Tests run: 3, Failures: 0, Errors: 2, Time elapsed: 40.861 sec
    [junit] Test org.apache.pig.test.TestStoreOld FAILED","21/Aug/08 20:22;olgan;pathc committed. santhosh, thanks for contributing!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PERFORMANCE: Join optimization for pipeline rework,PIG-350,12401432,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,alangates,alangates,31/Jul/08 18:51,24/Mar/10 22:04,14/Mar/19 03:05,04/Nov/08 01:46,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"Currently, joins in pig are done as groupings where each input is grouped on the join key.  In the reduce phase, records from each input are collected into a bag for each key, and then a cross product done on these bags.  This can be optimized by selecting one (hopefully the largest) input and streaming through it rather than placing the results in a bag.  This will result in better memory usage, less spills to disk due to bag overflow, and better performance.  Ideally, the system would intelligently select which input to stream, based on a histogram of value distributions for the keys.  Pig does not have that kind of metadata.  So for now it is best to always pick the same input (first or last) so that the user can select which input to stream.

Similarly, order by in pig is done in this same way, with the grouping keys being the ordering keys, and only one input.  In this case pig still currently collects all the records for a key into a bag, and then flattens the bag.  This is a total waste, and in some cases causes significant performance degradation.  The same optimization listed above can address this case, where the last bag (in this case the only bag) is streamed rather than collected.

To do these operations, a new POJoinPackage will be needed.  It will replace POPackage and the following POForEach in these types of scripts, handling pulling the records from hadoop and streaming them into the pig pipeline.  A visitor will need to be added in the map reduce compilation phase that detects this case and combines the POPackage with POForeach into this new POJoinPackage.",,,,,,,,,,,,,,,,,,,31/Oct/08 17:52;pkamath;PIG-350-2.patch;https://issues.apache.org/jira/secure/attachment/12393168/PIG-350-2.patch,01/Nov/08 00:55;pkamath;PIG-350-3.patch;https://issues.apache.org/jira/secure/attachment/12393184/PIG-350-3.patch,21/Oct/08 17:42;pkamath;PIG-350.patch;https://issues.apache.org/jira/secure/attachment/12392591/PIG-350.patch,06/Aug/08 22:01;daijy;join.patch;https://issues.apache.org/jira/secure/attachment/12387686/join.patch,07/Aug/08 00:28;daijy;join2.patch;https://issues.apache.org/jira/secure/attachment/12387695/join2.patch,07/Aug/08 21:07;daijy;join3.patch;https://issues.apache.org/jira/secure/attachment/12387768/join3.patch,13/Aug/08 07:59;daijy;join4.patch;https://issues.apache.org/jira/secure/attachment/12388109/join4.patch,,,,,,,7.0,,,,,,,,,,,,,,,,,,,2008-08-06 22:01:41.407,,,no_permission,,,,,,,,,,,,163986,,,,,Tue Nov 04 01:46:23 UTC 2008,,,,,,,0|i0gj5z:,94535,,,,,,,,,,"06/Aug/08 22:01;daijy;Here are some notes for this patch:
# The criteria for join optimization
   a. POPackage->POForEach is the root of reduce plan
   b. POUnion is the leaf of map plan (so that we exclude distinct, sort...)
   c. No combiner plan
   d. POForEach nested plan only contains POProject in any depth
   e. Inside POForEach, all occurrences of the first input are flattened
# If the MR plan is not eligible for optimization, everything stay the same
# The first alias in cogroup statement is the one we are going to stream.
# In optimization case, we will change the output key format for map-reduce, originally it is (key, IndexedTuple), now it is ((old_key, index), IndexedTuple)
# Here is the layout for patitioner, comparator for new key
   #* Partition key: (old_key, index)
   #* OutputKeyComparator: by old_key + reverse index
   #* OutputValueGroupingComparator: by old_key
   With this layout, we get
   ## If two inputs share the same old_key, they go to the same reducer
   ## For every old_key, reduce function is called once
   ## Values iterator are sorted on reverse index order, so that we can materialize n-1 inputs, then stream the first input
# Implementation detail for POJoinPackage:
   a. For every tuple in the first input, combine it with materialized n-1 inputs, feed to delegated ForEach operator
   b. Call getNext on ForEach until nothing available, then feed the next tuple
   c. Maintain internal status, since we feed POJoinPackage once, and then call getNext multiple times
# For performance reason, I use a byte level comparator for OutputKeyComparator, there is no byte level comparator for partitioner and OutputValueGroupingComparator in hadoop probably because OutputKeyComparator is more critical than others. While OutputValueGroupingComparator and partitioner is called one tuple a time, OutputKeyComparator is called more frequently


Possible improvement:
# User is able to give a hint for which alias to use for stream, we can change OutputKeyComparator to implement it easily","06/Aug/08 22:56;alangates;Comments:

1. In PigTupleReverseOrderWritableComarator and PigTupleReverseOrderWritableGroupComarator it looks to me like you're assuming that tuples from a cogroup only have one field from each.  But that may not be the case if there are multiple columns in the cogroup statement, such as:

C = cogroup A by ($0, $1), B by ($1, $2)

2. In JoinOptimizationVisitor where you remove the POPackage and POForeach and replace it with POJoinPackage, you should use OperatorPlan.replace to remove the POForeach and add the POJoinPackage.  This will preserve any ordering in the graph that might be lost otherwise.","07/Aug/08 00:28;daijy;In response to Alan's comments:
1. Tested with multiple key cases, works fine
2. Changed the patch to use OperatorPlan.replace instead of remove and then add",07/Aug/08 16:51;olgan;the patch looks good to me,"07/Aug/08 19:44;alangates;Cogroups that use inner return wrong results.  Scripts like:

{code}
e = cogroup c by $0, d by $0;
f = foreach e generate flatten (c), flatten(d); 
{code}

do fine but scripts like:

{code}
c = filter a by age < 20;
e = cogroup c by (name, age), b by (name, age) inner;
{code}

return wrong results.",07/Aug/08 21:07;daijy;Try join3.patch,13/Aug/08 07:59;daijy;Make several changes for performance optimization. ,"18/Aug/08 23:24;olgan;Daniel run a large test but the optimized code is still significantly slower. The spill time was about 5 minutes on the old code. He did not measure the read/parse time.

Without patch: 6:36:0
With patch: 6:55:25
",21/Aug/08 18:06;olgan;reducing priority since so far we have not been able to figure a way to make this code run efficiently. will revisit later,24/Aug/08 18:05;olgan;the patch still has performance issues; can't be applied as is,"21/Oct/08 17:51;pkamath;Submitted patch with modification to Join4.patch
- The last input is now streamed instead of the first input. This is because we now have tuples arriving in package in the order of the index starting from input 0 upto input n. All tuples for a given index (input) arrive before any tuple for the next index.
- The last input is now sent as a tuple at a time to foreach rather than as a bag containing one tuple at a time
- A new optimized foreach has been introduced which assumes its input has already been ""attached"" (which will be the case in the join optimization)
- In POForeach all data structures (arrays) are now created statically and reused in each getNext() call rather than being re-created on each getNext() call. Likewise the datastructures in POJoinPackage are also created once up front and reused.
","23/Oct/08 20:12;daijy;Glad to see this issue is moving forward. This patch is logically good, and also include several optimization trick. Expect to see performance data. Thanks Pradeep!","31/Oct/08 17:52;pkamath;Attached new version of the patch with following changes:
- Added optimizations in POForeach to store isToBeFlattened arraylist of Booleans to a static array of booleans - this saves on get() calls and on conversions from Boolean to boolean in all the checks for flatten for an input
- POJoinPackage now sends the last inputs in chunk sizes (default of 1000) which can be changed by the system property ""join.biggest.input.chunksize""
- Rolled back changes in previous patch to reuse data structures (arrays) in POForeach - runs with big data showed that this caused GC overhead issues - the possible explanation for this is that these arrays on reuse go into the old generation of the heap causing the GC more problems to reclaim space
- tightened the code a little more in POJoinPackage","31/Oct/08 20:22;alangates;In general, looks very good.  One question.

In POForEach you added arrays isToBeFlattenedArray and planLeafOps, which look like copies of the lists isToBeFlattened and planLeaves respectively.  In the comment on isToBeFlattenedArray you say you're doing this for performance.  I think that's fine, but why do you still have the lists?  Having two copies of the data seems dangerous, as you're likely to get out of sync.",01/Nov/08 00:55;pkamath;New version with above review comment incorporated - now we only have array and no arraylist for the data structures mentioned in the comment.,04/Nov/08 01:46;alangates;PIG-350-3.patch checked in.  Thanks to Pradeep and Daniel for all your work on this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig (help) Commands,PIG-347,12401263,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,olgan,chandec,chandec,29/Jul/08 19:30,17/Dec/10 22:43,14/Mar/19 03:05,04/Aug/10 23:52,,,,,,,0.8.0,,,,,0,,,,"Pig help can be specified 2 ways: $pig -help and $pig -h

I. $pig -help (seen by external/internal users)

(1) fix
-c, -cluster clustername, kryptonite is default 
>> remove ""kryptonite is default""

(2) change 
-x, -exectype local|mapreduce, mapreduce is default 
>> change mapdreduce to hadoop (maintain backward compatibility)


II. $pig -h (seen by internal users users only)

(1) fix typos
-l, --latest   use latest, untested, unsupported version of pig.jar instaed of relased, tested, supported version.
>>  "" ....instead of released ....""

(2) fix
-c, -cluster clustername, kryptonite is default 
>> remove ""kryptonite is default"" 
(same as above)

(3) change:  -x, -exectype local|mapreduce, mapreduce is default ... 
>> change mapdreduce to hadoop (maintain backward compatibility)
(same as above)
 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-08-04 23:52:45.373,,,no_permission,,,,,,,,,,,,163983,,,,,Wed Aug 04 23:52:45 UTC 2010,,,,,,,0|i0gj4n:,94529,,,,,,,,,,"04/Aug/10 23:52;olgan;(1) has been done for a while
(2) we don't support hadoop. We use value mapred or mapreduce and I am not sure we should change it now
(3) Part || is internal to yahoo.

Closing this bug",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Grunt (help) commands ,PIG-346,12401208,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,chandec,chandec,29/Jul/08 01:11,17/Dec/10 22:43,14/Mar/19 03:05,05/Aug/10 20:28,,,,,,,0.8.0,,,,,0,,,,"I think there are 22 grunt commands .... and 2 different lists of the commands can be displayed.

I. Grunt commands displayed with ""grunt> help""

(1) put 22 grunt commands in alphabetical order
(2) fix double entry for cd ... cd <path> and cd <dir> .... keep cd <path>
(3) fix notation for set key value ... set <key> '<value>'
(4) add explain
(5) add illustrate
(6) add help

II. Grunt commands display with ""grunt> asdf"" 

The ""asdf"" is a mistake and generates msg ""Was expecting one of:"" and list of grunt commands

(1) put 22 grunt commands in alphabetical order
(2) add define
(3) add du

------------------------
22 Grunt commands in aphabetical order:

cat <src>
cd <path>
copyFromLocal <localsrc> <dst>
copyToLocal <src> <localdst>
cp <src> <dst>
define <functionAlias> <functionSpec>
describe <alias>
dump <alias>
du <path>
explain
help
illustrate
kill <job_id>
ls <path>
mkdir <path>
mv <src> <dst>
pwd
quit
register <udfJar>
rm <src>
set <key> '<value>'
store <alias> into <filename> [using <functionSpec>]













",,,,,,,,,,,,,,,,,,,05/Aug/10 00:13;olgan;PIG-346.patch;https://issues.apache.org/jira/secure/attachment/12451286/PIG-346.patch,05/Aug/10 20:26;olgan;PIG-346_2.patch;https://issues.apache.org/jira/secure/attachment/12451365/PIG-346_2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-08-04 21:05:12.12,,,no_permission,,,,,,,,,,,,163982,,,,,Thu Aug 05 20:28:06 UTC 2010,,,,,,,0|i0gj47:,94527,,,,,,,,,,04/Aug/10 21:05;olgan;We also need to make sure to cover commens that implemented in PigServer. This is from PIG-523 that I will close as duplicate of this bug,"05/Aug/10 00:14;olgan;I have made the following changes:

(1) Removed depricated file system commands
(2) Organized the information the same way it is organized in the documentation
(3) Added more detailed information (used info from docs)
(4) Made sure that all commands are covered","05/Aug/10 20:26;olgan;Changes based on review from Corinne, thanks!",05/Aug/10 20:28;olgan;patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In map reduce mode pig gives an error from every task, even when it succeeds.",PIG-345,12401186,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,alangates,alangates,alangates,28/Jul/08 20:13,25/Mar/10 00:12,14/Mar/19 03:05,04/Sep/08 23:33,,,,,,,0.2.0,,,,,0,,,,"A query that succeeds when run still returns:

2008-07-26 17:41:29,164 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher - Error message from task (reduce) tip_200807231111_0142_r_000000

for every map task.",,,,,,,,,,,,,,,,,,,28/Jul/08 21:34;alangates;boguserror.patch;https://issues.apache.org/jira/secure/attachment/12387055/boguserror.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,163981,,,,,Thu Sep 04 23:33:53 UTC 2008,,,,,,,0|i0gj3r:,94525,,,,,,,,,,28/Jul/08 21:34;alangates;Changed Launcher.java to only print the error message if there was actually a message from hadoop.,04/Sep/08 23:33;alangates;Should have been marked resolved a while ago.  Fix was checked in when the previous comment was logged.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ordering on types other than byte array fails.,PIG-344,12401181,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,alangates,alangates,alangates,28/Jul/08 19:02,24/Mar/10 22:04,14/Mar/19 03:05,29/Jul/08 01:27,0.2.0,,,,,,0.2.0,,,,,0,,,,"{code}
A = load '/Users/gates/test/data/studenttab10' as (name: chararray, age: long, gpa: float);
B = order A by gpa;
dump B;
{code}

java.io.IOException: Type mismatch in key from map: expected org.apache.hadoop.io.BytesWritable, recieved org.apache.hadoop.io.FloatWritable
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:419)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:79)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:119)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:71)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:157)

If the gpa type is not declared, then the sort passes.
",,,,,,,,,,,,,,,,,,,28/Jul/08 20:23;alangates;sortkey.patch;https://issues.apache.org/jira/secure/attachment/12387049/sortkey.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-28 20:40:23.593,,,no_permission,,,,,,,,,,,,163980,,,,,Tue Jul 29 01:27:30 UTC 2008,,,,,,,0|i0gj3b:,94523,,,,,,,,,,28/Jul/08 20:23;alangates;MRCompiler was assuming that all order bys were on bytearrays.  This fixes it to use the type provided in the physical plan instead of hardwire it to byte array.,28/Jul/08 20:40;sms;The getSortJob looks good. The getQuantileJob has similar code where the result type of the project operator is set to DataType.BYTEARRAY instead of the type of the projection operator. Maybe this code has to change to reflect the logic in the getSortJob method.,"28/Jul/08 22:19;daijy;Patch looks good. Tested ""Limited sort"", works fine with this patch. Thanks Alan!","29/Jul/08 00:14;alangates;In response to Santhosh's concern about the byte array in getQuantileJob:

getQuantileJob always uses a group all for the rearrange and POSort for sorting the sample bag.  The POSort seems to be doing the right things regardless of the data type on the project.  I tested it and it works fine, so I'm not going to mess with it.",29/Jul/08 01:27;alangates;Checked in sortkey.patch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simple script with SPLIT fails,PIG-343,12401180,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,shravanmn,olgan,olgan,28/Jul/08 18:57,24/Mar/10 22:04,14/Mar/19 03:05,12/Aug/08 04:35,0.2.0,,,,,,0.2.0,,,,,0,,,,"Script:

grunt> A = load '/user/pig/tests/data/singlefile/studenttab10k' as (name, age, gpa);
grunt> split A into X if age > 19, Y if age <= 19;
grunt> store X into 'X';

Stack:

08/07/28 11:46:28 WARN pig.PigServer: bytearray is implicitly casted to integer under LOGreaterThan Operator
08/07/28 11:46:29 ERROR grunt.GruntParser: java.io.IOException: Unable to store for alias: X [null]
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:285)
        at org.apache.pig.PigServer.execute(PigServer.java:494)
        at org.apache.pig.PigServer.store(PigServer.java:333)
        at org.apache.pig.PigServer.store(PigServer.java:319)
        at org.apache.pig.tools.grunt.GruntParser.processStore(GruntParser.java:189)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:342)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:92)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:58)
        at org.apache.pig.Main.main(Main.java:278)
Caused by: org.apache.pig.backend.executionengine.ExecException
        ... 9 more
Caused by: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobCreationException
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.compile(JobControlCompiler.java:159)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.compile(JobControlCompiler.java:104)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:53)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:275)
        ... 8 more
Caused by: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobCreationException
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.compile(JobControlCompiler.java:159)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.compile(JobControlCompiler.java:146)
        ... 11 more
Caused by: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobCreationException
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJobConf(JobControlCompiler.java:291)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.compile(JobControlCompiler.java:131)
        ... 12 more
Caused by: java.lang.NullPointerException
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJobConf(JobControlCompiler.java:243)
        ... 13 more",,,,,,,,,,,,,,,,,,,08/Aug/08 13:45;shravanmn;343.patch;https://issues.apache.org/jira/secure/attachment/12387815/343.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-08-08 13:44:23.233,,,no_permission,,,,,,,,,,,,163979,,,,,Tue Aug 12 04:35:28 UTC 2008,,,,,,,0|i0gj2v:,94521,,,,,,,,,,"08/Aug/08 13:44;shravanmn;This is a small problem in the logical to physical translator. POSplit assumes that the temporary storage file to which tuples will be dumped is specified. I have a fix and the above script works in my updated local version of the trunk. However, I see that the unit tests on the trunk fail. Should I be using a different set of unit tests? I have corrected some golden files and am attaching the patch. The main fix is only LogToPhyTranslationVisitor. Others are just to get some unit tests working.","08/Aug/08 18:05;olgan;Shravan, as long as the number of failures before and after the patch is the same, the patch is considered to be good","08/Aug/08 18:38;shravanmn;Seeing unit test failures in trunk now, I thought I must be missing something:) 

The patch satisfies the above definition for a good patch. Also I have included modified golden files to make the log to phy compiler tests pass. So now we have the LogToPhyCompiler test passing and hence lesser number of failures :)

Please verify.

Also related is Pig-370. Please check. Apply the current patch before applying Pig-370.",12/Aug/08 04:35;alangates;343.patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Size of DistinctDataBag is calculated incorrectly if spill occurs and non-distinct elements are inserted,PIG-342,12401075,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,bdimcheff,bdimcheff,bdimcheff,26/Jul/08 05:04,24/Mar/10 22:01,14/Mar/19 03:05,30/Jul/08 19:46,0.1.0,,,,,,0.1.0,,,,,0,,,,"If a spill occurs while elements are being inserted into a DistinctDataBag, it's possible that non-unique items will be added to the in-memory data structure, and the mSize counter will be incremented.  If the same elements also exist on disk, the count will be higher than it should be.

The following is copied from an email exchange I had with Alan Gates:

Alan,

Thanks for your help.  I've done a bit more experimentation and have discovered a couple more things.  I first looked at how COUNT was implemented.  It looks like COUNT calls size() on the bag, which will return mSize.  I thought that mSize might be calculated improperly so I added ""SUM(unique_ids) AS crazy_userid_sum"" to my GENERATE line and re-ran the pigfile:

GENERATE FLATTEN(group), SUM(nice_data.duration) AS total_duration, COUNT(nice_data) AS channel_switches, COUNT(unique_ids) AS unique_users, SUM(unique_ids) AS crazy_userid_sum;

It turns out that the SUM generates the correct result in all cases, while there are still occasional errors in the COUNT.  Since SUM requires an iteration over all the elements in the DistinctDataBag, this led me to believe that the uniqueness constraint is indeed operating correctly, but there is some error in the logic that calculates mSize.

Then I started poking around in DistinctDataBag looking for anything that changes mSize that might be incorrect.  I noticed that on line 87 in addAll(), the size of the DataBag that is passed into the method is added to the mSize instance variable, and then during the iteration a few lines later mSize is being incremented when an element is successfully added to mContents.  I thought this might be the problem, since it seems like elements would be double counted if addAll() was called.  I commented out line 87, recompiled Pig, and ran it again, but there are still errors (though I do think line 87 might be incorrect anyways).

Thanks to my coworker Marshall, I think we may have discovered what the actual problem is.  The scenario is as follows:  We're adding a bunch of stuff to the bag, and before we're finished a spill occurs.  mContents is cleared during the spill (line 157).  All add() does is check uniqueness against mContents.  So now we will get duplicates in mContents that are already on disk and an inflated mSize.  Now, the reason why SUM works is because the iterator is smart and enforces uniqueness as it reads the records back in. We think this occurs at the beginning of addToQueue, around line 363 - 369.  mMergeTree is a TreeSet, so it'll enforce uniqueness and the call to addToQueue is aborted if there's already a matching record in mMergeTree.

Do you think our assessment is correct?  If so, it seems that the calculation of mSize needs to be significantly more complex than it is now.  It looks to me like the entire bag will need to be iterated in order to reliably calculate the size.  Do you have any ideas about how to implement this in a less expensive way?  I'd be happy to take a stab at it, but I don't want to do anything particularly silly if you have a better idea.",,,,,,,,,,,,,,,,,,,26/Jul/08 05:05;bdimcheff;size.patch;https://issues.apache.org/jira/secure/attachment/12386939/size.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-30 15:19:14.704,,,no_permission,,,,,,,,,,,,163978,,,,,Wed Jul 30 19:46:42 UTC 2008,,,Patch Available,,,,0|i0gj2f:,94519,,,,,,,,,,26/Jul/08 05:05;bdimcheff;This is a patch that fixes the problem and adds a test to verify the correct behavior.,30/Jul/08 15:19;alangates;Patch looks good.  I'll run it through the tests and if all is well apply it.,30/Jul/08 19:46;alangates;Fix checked in at revision 681184.  Thanks Brandon for contributing.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Remove ""order by ... limit"" syntax",PIG-341,12401048,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,daijy,daijy,daijy,25/Jul/08 20:03,24/Mar/10 22:04,14/Mar/19 03:05,12/Aug/08 00:02,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"""order by ... limit"" syntax is only for test purpuse and should be removed.",,,,,,,,,,,,,,,,,,,08/Aug/08 01:06;daijy;PIG-341.patch;https://issues.apache.org/jira/secure/attachment/12387790/PIG-341.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-08-12 00:02:49.976,,,no_permission,,,,,,,,,,,,163977,,,,,Tue Aug 12 00:02:49 UTC 2008,,,,,,,0|i0gj1z:,94517,,,,,,,,,,12/Aug/08 00:02;alangates;PIG-341 patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Specifying 'using PigStorage('\t')' in a script causes pig to fail,PIG-340,12401038,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,alangates,alangates,alangates,25/Jul/08 18:24,24/Mar/10 22:04,14/Mar/19 03:05,29/Jul/08 23:44,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"The script 

{code}
A = load '/user/pig/tests/data/perf/studenttab200m' using PigStorage('\t') as (name, age, gpa);
B = filter A by gpa < 3.6;
store B into 'filter10pct2' using PigStorage();
{code}

fails with the error message:

        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.map(PigMapOnly.java:64)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2124)
 java.io.IOException: Request for field number 2 exceeds tuple size of 1
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:139)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.map(PigMapOnly.java:64)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2124)

while
{code}
A = load '/user/pig/tests/data/perf/studenttab200m' as (name, age, gpa);
B = filter A by gpa < 3.6;
store B into 'filter10pct2';
{code}

runs just fine over the same data.  It appears that the specification of the load function or the delimiter is causing issues.
",,,,,,,,,,,,,,,,,,,29/Jul/08 22:46;alangates;tabdelim.patch;https://issues.apache.org/jira/secure/attachment/12387144/tabdelim.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-29 22:56:02.457,,,no_permission,,,,,,,,,,,,163976,,,,,Tue Jul 29 23:44:59 UTC 2008,,,,,,,0|i0gj1j:,94515,,,,,,,,,,"29/Jul/08 22:46;alangates;Patch to fix tab delimiter issue.  This patch also makes it so that PigStorage can use unicode characters.  

This patch overlaps some with PIG-85.  But I looked and there appeared to be other changes in there, so it will still need to be ported.","29/Jul/08 22:56;olgan;+1, looks good.

",29/Jul/08 23:44;alangates;tabdelim.patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit follow cross/union return wrong number of records,PIG-339,12400986,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,25/Jul/08 07:54,02/May/13 02:29,14/Mar/19 03:05,14/Aug/08 23:33,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"The following script returns double records as expected:
a = load 'a';
b = load 'b';
c = union a, b;
d = cross a, b;
e = limit c 100;
f = limit d 100;
dump e;   // return double number of records
dump f;    // return double number of records

Seems to be the limit operator in reduce plan is not effective.",,,,,,,,,,,,,,,,,PIG-378,,25/Jul/08 18:57;daijy;PIG-339.patch;https://issues.apache.org/jira/secure/attachment/12386908/PIG-339.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-08-12 03:47:51.122,,,no_permission,,,,,,,,,,,,163975,,,,,Thu Aug 14 23:33:01 UTC 2008,,,,,,,0|i0gj13:,94513,,,,,,,,,,"12/Aug/08 03:47;alangates;When I run a script like the following:

{code}
a = load 'studenttab10k';
b = load 'votertab10k';
a1 = foreach a generate $0, $1;
b1 = foreach b generate $0, $1;
c = union a1, b1;
d = limit c 100;
store d into 'result';
{code}

I get:
java.io.IOException: Unable to store for alias: 12 [null]
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.LogToPhyTranslationVisitor.visit(LogToPhyTranslationVisitor.java:1010)
        at org.apache.pig.impl.logicalLayer.LOLimit.visit(LOLimit.java:76)
        at org.apache.pig.impl.logicalLayer.LOLimit.visit(LOLimit.java:10)
        at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:68)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:245)
        at org.apache.pig.PigServer.compilePp(PigServer.java:590)
        at org.apache.pig.PigServer.execute(PigServer.java:516)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:265)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:425)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:241)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:82)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:64)
        at org.apache.pig.Main.main(Main.java:302)
Caused by: java.lang.NullPointerException
",14/Aug/08 23:27;daijy;This error is gone after applying [PIG-378|https://issues.apache.org/jira/browse/PIG-378].,14/Aug/08 23:33;olgan;Verified that the issue is gone. Thanks Daniel ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
limit return uncorrect records following distinct,PIG-338,12400973,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,25/Jul/08 05:03,24/Mar/10 22:04,14/Mar/19 03:05,11/Aug/08 17:34,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"The following script return fewer records than expected:
a = load 'f';
b = distinct a;
c = limit b 10;
dump c;",,,,,,,,,,,,,,,,,,,29/Jul/08 02:41;daijy;PIG-338-2.patch;https://issues.apache.org/jira/secure/attachment/12387069/PIG-338-2.patch,25/Jul/08 05:46;daijy;PIG-338.patch;https://issues.apache.org/jira/secure/attachment/12386855/PIG-338.patch,29/Jul/08 00:32;alangates;TEST-org.apache.pig.test.TestLogicalOptimizer.txt;https://issues.apache.org/jira/secure/attachment/12387065/TEST-org.apache.pig.test.TestLogicalOptimizer.txt,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2008-07-29 00:31:40.59,,,no_permission,,,,,,,,,,,,163974,,,,,Mon Aug 11 17:34:17 UTC 2008,,,,,,,0|i0gj0n:,94511,,,,,,,,,,29/Jul/08 00:31;alangates;After applying the patch I get three failures in TestLogicalOptimizer.  I'll attach the test log.,29/Jul/08 02:41;daijy;Try PIG-338-2.patch. Thanks.,11/Aug/08 17:34;alangates;PIG-338-2.patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If limit size exceeds number of records in the file, a few records get dropped",PIG-337,12400971,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,alangates,alangates,25/Jul/08 03:22,24/Mar/10 22:04,14/Mar/19 03:05,11/Aug/08 21:25,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"Given a file with 10k records, the following script returned 9996 records:

a = load 'studenttab10k';
b = limit a 100000;
dump b;

It looks like maybe the limit operator isn't returning its last record or something.",,,,,,,,,,,,,,,,,,,25/Jul/08 07:32;daijy;PIG-337.patch;https://issues.apache.org/jira/secure/attachment/12386861/PIG-337.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-25 06:31:06.384,,,no_permission,,,,,,,,,,,,163973,,,,,Mon Aug 11 21:25:56 UTC 2008,,,,,,,0|i0gj0f:,94510,,,,,,,,,,"25/Jul/08 06:31;daijy;More general, all input file with duplicate records is potentially affected, even if the limit size is within the number of records in file. Will submit patch shortly. ","25/Jul/08 08:38;daijy;Please hold on for a while, there is still some issue in this patch",25/Jul/08 18:57;daijy;Should be fine now.,11/Aug/08 21:25;alangates;PIG-337.patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NULL checks are not in place in the types branch,PIG-336,12400950,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,pkamath,pkamath,24/Jul/08 20:48,25/Mar/10 00:12,14/Mar/19 03:05,15/Jan/10 06:18,0.2.0,,,,,,0.2.0,,,,,0,,,,"The following code currently does not work
{code}
B = filter A by $0 is null and $1 is null;
{code}

Some other things which don't work with nulls include POAND, POOR etc
",,,,,,,,,,,,,,,,,,,31/Jul/08 18:20;pkamath;PIG-336-part1.patch;https://issues.apache.org/jira/secure/attachment/12387302/PIG-336-part1.patch,01/Aug/08 21:02;pkamath;PIG-336-part1_v2.patch;https://issues.apache.org/jira/secure/attachment/12387372/PIG-336-part1_v2.patch,05/Aug/08 06:31;pkamath;PIG-336-part1_v3.patch;https://issues.apache.org/jira/secure/attachment/12387542/PIG-336-part1_v3.patch,06/Aug/08 17:32;pkamath;PIG-336-part1_v4.patch;https://issues.apache.org/jira/secure/attachment/12387660/PIG-336-part1_v4.patch,05/Sep/08 21:21;pkamath;PIG-336-part2.patch;https://issues.apache.org/jira/secure/attachment/12389592/PIG-336-part2.patch,31/Jul/08 06:36;pkamath;PIG-336.patch;https://issues.apache.org/jira/secure/attachment/12387259/PIG-336.patch,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2008-07-28 10:04:36.591,,,no_permission,,,,,,,,,,,,163972,,,,,Fri Jan 15 06:18:45 UTC 2010,,,,,,,0|i0gizz:,94508,,,,,,,,,,24/Jul/08 22:52;pkamath;Attached a patch to fix is null issues,25/Jul/08 18:59;pkamath;I am currently working on adding null in data for unit test cases so that testing with nulls is part of unit test cases. I am also trying to fix code issues along the way - will have a bigger patch once done.,27/Jul/08 08:15;pkamath;One thing I noticed while working on fixing null issues in arithmetic operations is currently the return type for Integer multiplication and addition is an Integer.This could give wrong results when the integers are big and the multiplication results in a value which cannot be stored in an integer - Should the result be a Long?,28/Jul/08 10:04;pi_song;Long can be overflown as well. Moving from int to long will just lower possibility. This is the same as our aggregate functions (SUM for example). I agree with anything that most people think is good.,"31/Jul/08 06:36;pkamath;New patch which addresses many issues with handling nulls and also adds many unit test cases dealing with null data.



",31/Jul/08 18:20;pkamath;Resubmitting patch with a minor delta - change in test/org/apache/pig/test/util/GenRandomData.java to make the random null data generation more correct.,"01/Aug/08 00:08;pkamath;I will be resubmitting the patch to work with latest revision in svn and also to fix a bug in the expressions ops - eq, lt, lte, neq, gt and gte",01/Aug/08 21:02;pkamath;Attached patch for changes noted in previous comment,05/Aug/08 06:31;pkamath;Attached patch compatible with current svn version. This patch does not handle nulls in joins/group/cogroup and sort,"06/Aug/08 01:19;alangates;This patch looks great.  What a pile of work!

I have one comment.  The KeyTypeDiscoverVisitor is used in the map and combine sections PigMapReduce and PigCombiner.  This should be done sooner.  Those classes are focussed on actual execution, not execution planning.  I think you're waiting until then because you only want to visit specific pieces of the physical plan in each map, combine, or reduce phase.  If that's the case, you should instead run this visitor in MapReduceLauncher and LocalLauncher, where the physical plan is compiled into a map reduce plan.  (Look for CombinerOptimizer, you should run the KeyTypeDiscoveryVisitor right after that.)  You can then record that information in the MapReduceOper itself, and read that in PigMapReduce and PigCombiner.","06/Aug/08 17:32;pkamath;Attached patch to address review comments.
Delta to previous patch:


- Removed TestOrderBy changes - we should revisit this after order desc is fixed

- NullableXXX classes have a change in the compareTo():
If both the objects being compared are nulls, it returns a 0 - this enables all nulls to collapse during a group by

-keyDiscoverKeyTypeVisitor has been changed to visit MROperPlan
 Supporting changes are in JobControlCompiler, PigMapBase, LocalLauncher,MapReduceLauncher, MapReduceOper, PigCombiner, ",06/Aug/08 18:53;alangates;part1_v4 patch checked in.  Thanks Pradeep for all your work on this.,18/Aug/08 23:19;olgan;lowering priority to major since most of this has already ,24/Aug/08 18:04;olgan;all existing patches has been applied,05/Sep/08 21:21;pkamath;Attached patch with modifications to some of the existing unit test cases to test with null data,"05/Sep/08 21:24;pkamath;Some more of the existing unit test cases which still need to be potentially modified to test for null input data are:
TestEvalPipeline
TestFilterOpNumeric
TestFilterUDF
TestPigSplit
TestPOGenerate
TestPOMapLookup
TestSplitStore
TestStore
TestStoreOld
TestUnion","08/Sep/08 18:12;olgan;PIG-336-part2.patch committed; thanks, pradeep.",15/Jan/10 06:18;alangates;Patch was committed a long time ago.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Casting does not work in certain cases with multiple loads,PIG-335,12400940,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,sms,alangates,alangates,24/Jul/08 18:27,24/Mar/10 22:04,14/Mar/19 03:05,02/Oct/08 20:37,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"Given a script like:

A = load 'bla' as (x, y) using Loader1();
B = load 'morebla' as (s, t) using Loader2();
C = cogroup A by x, B by s;
D = foreach C generate flatten(A), flatten(B);
E = foreach D generate x, y, t + 1;

In this case, in the last foreach, a cast will need to be added to t + 1 to allow t (a byte array) to be added to an integer.  We use load functions to handle this late casting.  The issue is that we do not currently have a way to know whether to use Loader1 or Loader2 to cast the data.  We need to track the lineage of fields so that the cast operator can select the correct loader.
",,,,,,,,,,,,,,,,,,,29/Sep/08 05:25;sms;PIG_335.patch;https://issues.apache.org/jira/secure/attachment/12391112/PIG_335.patch,29/Sep/08 18:05;sms;PIG_335_1.patch;https://issues.apache.org/jira/secure/attachment/12391150/PIG_335_1.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-08-21 11:19:41.157,,,no_permission,,,,,,,,,,,,163971,,,,,Thu Oct 02 20:37:17 UTC 2008,,,Patch Available,,,,0|i0gizj:,94506,,,,,,,,,,21/Aug/08 11:19;shravanmn;I think Santosh is the right person to fix this. I can swap something you have on the physical side.,"04/Sep/08 23:05;sms;The proposed design for computing the lineage for load functions:

The FieldSchema class will include an additional member variable that will contain a  list of parent/ancestral canonical names. The list of parent canonical names corresponds to the canonical names required by the operator to compute the field schema.

The parent list will be empty for canonical names that originate from the load statement and remain unchanged as they move from operator to operator. Only expressions (like arithmentic expressions, etc) will create new canonical names.

The load operator corresponding to the parent canonical name is required only to cast byte arrays into Pig types. Other than UDFs, there are no operators that generate byte arrays. CONCAT (also an UDF) can generate byte arrays. For now, its an UDF.

To compute the load function associated with field schema, each canonical name in the parent list of canonical names is matched against the operator responsible for the canonical name. If the operator is an UDF, then we throw an exception as we will not know how to convert a byte array generated by the UDF into a Pig type. The check bubbles up the graph until we hit the load operator corresponding to the canonical name under question.

Breakdown of the changes:

1. The logic mentioned in the previous paragraph will reside in the type checker. 
2. The changes to the FieldSchema will (of course) be in limited to the FieldSchema class. 
3. The computation of the list of the parent canonical names will happen in each logical operator.

Thoughts/comments on the proposed design are welcome.",04/Sep/08 23:37;olgan;+1. Also streaming would have the same impact as UDFs,"29/Sep/08 05:10;sms;The attached patch, PIG_335.patch adds the lineage feature to PIG. The patch adds more than 50 test cases to TestTypeCheckingValidator as the feature touches all the logical operators.

All unit tests pass. TestStreaming had 1 failure which was not reproducible when it was run on a standalone basis.",29/Sep/08 18:05;sms;New patch which fixes the logical to physical translator to use the load function interface only in the case of bytearrays to pig types during object construction and serialization.,29/Sep/08 19:07;sms;All unit tests pass. TestStreaming had 1 failure which was not reproducible when it was run on a standalone basis.,"30/Sep/08 22:51;alangates;Comments:

This new method of tracking lineage of data through the script is complex.  It would be good to add a couple of paragraphs to the class level comments in Schema.java describing how it works.

LOProject, around line 206, you added mFieldSchema.setParent(null, expressionOperator).  If I understand the code correctly this is the case where you are projecting star from a relational operator.  Why is the parent canonical name null in this case?  And what are the ramifications of that?

If the user writes a query like:

A = load 'Alpha' using MyLoadFunc;
B = load 'Beta' using TheirLoadFunc;
C = cogroup A by $0, B by $0;
D = foreach c generate group + 1;

they will get ""Found more than one load function interface to use: MyLoadFunc, TheirLoadFunc"" as an error message.  That doesn't make clear what the issue is (of course there's more than one load func interface, I gave you two load funcs!).  Something like:  ""Cannot resolve load function to use for casting $0 to integer, two possibilities:  MyLoadFunc, TheirLoadFunc"" would be much more helpful.

Same with some of the other error messages that just mention load func interface.  They should at the very least mention that they're trying to find the right cast to use.

In TypeCheckingVisitor.getLoadFunc(LogicalOperator, String) I see a list of relational operators (Filter, etc.).  But I don't see Cogroup, Union, or Cross in that list.  How are you tracing data that comes through those operators?  Those are the ones with the special case, where if the load functions match we know how to do the cast, and if they don't match we don't know.  But I don't see where they're tracing the lineage of their data.","02/Oct/08 18:48;sms;1. Added documentation to Schema.java describing the use of the canonical maps

2. This is a bug. It requires a change in the front end design to handle multi-column projects. I will open a new JIRA for this.

3. Changed the error message to ""Cannot resolve load function to use for casting from "" + DataType.findTypeName(inputType) + "" to  ...

4. The error message in the getLoadFunc message is appropriate, the correction I made here was to drop the word interface.

5. For Unions, Cross and Cogroup, depending on the column accessed, it will either result in the  appropriate traversal or a branch and unbound find.","02/Oct/08 20:37;olgan;patch committed; thanks, santhosh!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sorting on fields of type double does not work,PIG-334,12400938,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,alangates,alangates,alangates,24/Jul/08 18:04,24/Mar/10 22:04,14/Mar/19 03:05,28/Jul/08 22:11,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"In the new pipeline, when possible, pig uses hadoop writable comparable types for the hadoop key rather than tuple.  As of hadoop 0.17 there is no DoubleWritable type.  It has been added for hadoop 0.18.  But it appears that we will be ready to integrate the types branch back into trunk before hadoop 0.18 is released.  So we need to implement a DoubleWritable for ourselves until that time.

The code can be taken from HADOOP-3061.  The code where we convert to and from hadoop types (DataType.getWritableComparableTypes and convertToPigType) needs to be changed to use this type.",,,,,,,,,,,,,,,,,,,25/Jul/08 22:40;alangates;doublesort.patch;https://issues.apache.org/jira/secure/attachment/12386930/doublesort.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,163970,,,,,Mon Jul 28 22:11:02 UTC 2008,,,,,,,0|i0giz3:,94504,,,,,,,,,,"25/Jul/08 22:40;alangates;This takes the file from HADOOP-3061 and adds it to the pig data so we can use a double as a key.

I also, at Pi's request, moved the hadoop->pig data type translation functions from data.DataType to backend.hadoop.HDataType.

This does not however fully resolve the sorting issue.  Sorting on any type of declared type returns 

java.io.IOException: Type mismatch in key from map: expected org.apache.hadoop.io.BytesWritable, recieved org.apache.pig.backend.hadoop.DoubleWritable
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:419)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:83)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:122)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:75)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:157)

From looking at the explain plan, it looks like the project schema for the local rearrange is set to bytearray instead of the
correct type.
",28/Jul/08 18:52;alangates;I'll file a separate bug for sorting on types other than byte array.,28/Jul/08 22:11;alangates;Checked in doublesort.patch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Combiner needs to be used in the types branch,PIG-331,12400765,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,alangates,alangates,alangates,22/Jul/08 18:54,24/Mar/10 22:04,14/Mar/19 03:05,04/Aug/08 19:28,0.2.0,,,,,,0.2.0,,,,,0,,,,"The initial implementation in the types branch does not make use of the combiner.  For performance, it needs to make use of the combiner.",,,,,,,,,,,,,,,,,,,25/Jul/08 19:02;alangates;combiner.patch;https://issues.apache.org/jira/secure/attachment/12386911/combiner.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-23 11:00:10.007,,,no_permission,,,,,,,,,,,,163968,,,,,Tue Jul 29 21:53:04 UTC 2008,,,,,,,0|i0gixj:,94497,,,,,,,,,,23/Jul/08 11:00;pi_song;This has been blocking me from implementing OLAP operators.,"24/Jul/08 19:50;alangates;A first pass at using the combiner.  This patch contains a couple of things:

1) Foreachs that have only simple projection and algebraic functions make use of the combiner.
2) Distincts were optimized to not carry the bags of data along.  There is no actual need for them to use the combiner because we're only passing they keys.  hadoop takes care of collecting the keys and not passing multiple instances from map to reduce.

Things this patch does not contain:

1) Foreachs that have a combination of algebraic and non-algebraic.  These should be do-able, with the non-algebraic functions just being replaced by simple projections of the appropriate fields in the combiner plan.

2) Foreachs that include inner plans.  This is in particular useful for inner plans that use distinct, as this is the way to emulate count(distinct x) from SQL, a fairly common operation.  Some inner plans (such as those containing filters) cannot be split.  This is a little more challenging because it requiring duplicating parts of the inner plan in the combiner and reducer and not duplicating other parts.

These two should be added later. ","25/Jul/08 14:14;pi_song;I agree with the patch.

Filter can be done as Filter Pusher and we should even get a better result.",25/Jul/08 19:02;alangates;I attached the wrong file previously.  That was an earlier version of the patch.  Here's the right version.,"29/Jul/08 21:53;sms;   - File: src/org/apache/pig/backend/local/executionengine/LocalJob.java

      * In the anonymous Iterator that is returned, I noticed that the public member variables are not initialized but they are used in the hasNext() method.

{code}
return new Iterator<Tuple>() {

+            Tuple   t;
+            boolean atEnd;
+            public boolean hasNext() {
+                if (atEnd)
+                    return false;
+                try {
+                    if (t == null)
{code}


   - File: 

src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/CombinerOptimizer.java

      * Minor Comment: 

The log message indicates a leaf while the checks are being made for the roots.

{code}
 if (reduceRoots.size() != 1) {
+            log.warn(""Expected reduce to have single leaf"");
+            return;
+        }

{code}

      * Minor Comment: 

The condition to check for the existence of at least one algebraic and no instances of non-algebraic types is probably better written using && instead of || Method: +    private List<ExprType> algebraic(


{code}
//current code
if (!atLeastOneAlgebraic || !noNonAlgebraics) return null;

//more readable code?

if (atLeastOneAlgebraic && noNonAlgebraics) return null;

{code}


   - File: src/org/apache/pig/impl/plan/OperatorPlan.java

      * Major Comment:

The new replace(oldNode, newNode) method is similar to the private replaceNode(src, replacement, dst, multiMap) method added as part of  https://issues.apache.org/jira/browse/PIG-290. The difference lies in the method signature and the access specifier.


{code}
public void replace(E oldNode, E newNode) throws PlanException {
//Pig-290 change below
private boolean replaceNode(E src, E replacement, E dst, MultiMap<E, E> multiMap) 
{code}


   - File: src/org/apache/pig/impl/util/Pair.java

      * Minor Comment:

Do we need equals() and hashCode() methods to make the class usable in containers?

   - File: 

src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POUserComparisonFunc.java

      * Major Comment:

In the clone() method, the ComparisonFunc func is not cloned


   - File: 

src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POUserFunc.java

   * Major Comment:

In the clone method, the final two constructor arguments are in the wrong order. POUserFunc expects FuncSpec, EvalFunc while the clone method uses null, funcSpec.clone. This should not compile in the first place.

{code}
+        POUserFunc clone = new POUserFunc(new OperatorKey(mKey.scope, 
+            NodeIdGenerator.getGenerator().getNextNodeId(mKey.scope)),
+            requestedParallelism, null, funcSpec.clone());
{code}

   - File: 

src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/BinaryExpressionOperator.java

   * Major Comment:

In the cloneHelper method, the operator's resultType is not set. In the corresponding cloneHelper method in UnaryExpressionOperator, the restultType is set. The resultType is also set in PhysicalOperator's cloneHelper method.

{code}
//in UnaryExpressionOperator.java's cloneHelper

+        resultType = op.getResultType();
{code}

   - File: 

src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/PODistinct.java

   * Minor Comment:

The clone method is missing in PODistinct. Is this due to the re-write of distinct in LocalRearrange?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SpillableMemoryManager reads in gabage property values,PIG-330,12400763,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,pkamath,yhan,yhan,22/Jul/08 18:34,24/Mar/10 22:04,14/Mar/19 03:05,23/Jul/08 00:56,0.0.0,,,,,,0.2.0,,impl,,,0,,,,"I observed this problem. When properties is not correctly specified somewhere(in hadoop-site.xml or from command line), SpillableMemoryManager set properties to garbage numbers. It should be, if no correct properties passed in, the default values are kept for the properties.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-07-22 19:05:12.5,,,no_permission,,,,,,,,,,,,163967,,,,,Wed Jul 23 00:56:53 UTC 2008,,,,,,,0|i0gix3:,94495,,,,,,,,,,"22/Jul/08 19:05;pkamath;This will be fixed in the types branch for the next release and the fix will be checked in as part of https://issues.apache.org/jira/browse/PIG-269.

The current workaround is to supply the right properties with values in pig.properties. Users can even specify their own ""pig.properties"" file as explained in https://issues.apache.org/jira/browse/PIG-326?focusedCommentId=12615703#action_12615703",23/Jul/08 00:56;pkamath;Fixed as part of PIG-269,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Queries with multiple, unconnected loads fail",PIG-329,12400749,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,shravanmn,alangates,alangates,22/Jul/08 15:21,24/Mar/10 22:04,14/Mar/19 03:05,21/Aug/08 22:11,0.2.0,,,,,,0.2.0,,,,,0,,,,"testMultipleStore and testStoreWithMultipleMRJobs in TestStoreOld fail:

Testcase: testMultipleStore took 7.222 sec
    Caused an ERROR
Unable to store for alias: B [Problem resolving LOForEach schema org.apache.pig.builtin.SUM does not work with inputs of type {{A: {null}}}Severe problem found
java.io.IOException: Unable to store for alias: B [Problem resolving LOForEach schema org.apache.pig.builtin.SUM does not work with inputs of type {{A: {null}}}
    at org.apache.pig.PigServer.compileLp(PigServer.java:551)
    at org.apache.pig.PigServer.execute(PigServer.java:487)
    at org.apache.pig.PigServer.store(PigServer.java:331)
    at org.apache.pig.PigServer.store(PigServer.java:317)
    at org.apache.pig.PigServer.store(PigServer.java:302)
    at org.apache.pig.test.TestStoreOld.testMultipleStore(TestStoreOld.java:64)
Caused by: org.apache.pig.backend.executionengine.ExecException: Problem resolving LOForEach schema org.apache.pig.builtin.SUM does not work with inputs of type
    ... 21 more
Caused by: org.apache.pig.impl.plan.PlanValidationException: An unexpected exception caused the validation to stop
    at org.apache.pig.impl.plan.PlanValidator.validateSkipCollectException(PlanValidator.java:104)
    at org.apache.pig.impl.logicalLayer.validators.TypeCheckingValidator.validate(TypeCheckingValidator.java:40)
    at org.apache.pig.impl.logicalLayer.validators.TypeCheckingValidator.validate(TypeCheckingValidator.java:30)
    at org.apache.pig.impl.logicalLayer.validators.LogicalPlanValidationExecutor.validate(LogicalPlanValidationExecutor.java:79)
    at org.apache.pig.PigServer.compileLp(PigServer.java:518)
    ... 20 more
Caused by: org.apache.pig.impl.plan.VisitorException: Problem resolving LOForEach schema org.apache.pig.builtin.SUM does not work with inputs of type {{A: {null
    at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.visit(TypeCheckingVisitor.java:2205)
    at org.apache.pig.impl.logicalLayer.LOForEach.visit(LOForEach.java:87)
    at org.apache.pig.impl.logicalLayer.LOForEach.visit(LOForEach.java:36)
    at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:68)
    at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
    at org.apache.pig.impl.plan.PlanValidator.validateSkipCollectException(PlanValidator.java:101)
    ... 24 more

Testcase: testStoreWithMultipleMRJobs took 0.124 sec
    Caused an ERROR
Unable to store for alias: D [Problem resolving LOForEach schema org.apache.pig.builtin.SUM does not work with inputs of type {{A: {null}}}Severe problem found
java.io.IOException: Unable to store for alias: D [Problem resolving LOForEach schema org.apache.pig.builtin.SUM does not work with inputs of type {{A: {null}}}
    at org.apache.pig.PigServer.compileLp(PigServer.java:551)
    at org.apache.pig.PigServer.execute(PigServer.java:487)
    at org.apache.pig.PigServer.store(PigServer.java:331)
    at org.apache.pig.PigServer.store(PigServer.java:317)
    at org.apache.pig.PigServer.store(PigServer.java:302)
    at org.apache.pig.test.TestStoreOld.testStoreWithMultipleMRJobs(TestStoreOld.java:85)
Caused by: org.apache.pig.backend.executionengine.ExecException: Problem resolving LOForEach schema org.apache.pig.builtin.SUM does not work with inputs of type
    ... 21 more
Caused by: org.apache.pig.impl.plan.PlanValidationException: An unexpected exception caused the validation to stop
    at org.apache.pig.impl.plan.PlanValidator.validateSkipCollectException(PlanValidator.java:104)
    at org.apache.pig.impl.logicalLayer.validators.TypeCheckingValidator.validate(TypeCheckingValidator.java:40)
    at org.apache.pig.impl.logicalLayer.validators.TypeCheckingValidator.validate(TypeCheckingValidator.java:30)
    at org.apache.pig.impl.logicalLayer.validators.LogicalPlanValidationExecutor.validate(LogicalPlanValidationExecutor.java:79)
    at org.apache.pig.PigServer.compileLp(PigServer.java:518)
    ... 20 more
Caused by: org.apache.pig.impl.plan.VisitorException: Problem resolving LOForEach schema org.apache.pig.builtin.SUM does not work with inputs of type {{A: {null
    at org.apache.pig.impl.logicalLayer.validators.TypeCheckingVisitor.visit(TypeCheckingVisitor.java:2205)
    at org.apache.pig.impl.logicalLayer.LOForEach.visit(LOForEach.java:87)
    at org.apache.pig.impl.logicalLayer.LOForEach.visit(LOForEach.java:36)
    at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:68)
    at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
    at org.apache.pig.impl.plan.PlanValidator.validateSkipCollectException(PlanValidator.java:101)
    ... 24 more",,,,,,,,,,,,,,,,,,,20/Aug/08 11:53;shravanmn;329.patch;https://issues.apache.org/jira/secure/attachment/12388594/329.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-23 13:19:05.603,,,no_permission,,,,,,,,,,,,163966,,,,,Thu Aug 21 22:11:58 UTC 2008,,,,,,,0|i0giwn:,94493,,,,,,,,,,"23/Jul/08 13:19;pi_song;
The usage of SUM in the test is wrong. That's why the error message complains about type problem.

Here is the new error message.
{noformat}
Testcase: testMultipleStore took 30.544 sec
        Caused an ERROR
Unable to open iterator for alias: C [Job terminated with anomalous status FAILED]
java.io.IOException: Unable to open iterator for alias: C [Job terminated with anomalous status FAILED]
        at org.apache.pig.PigServer.openIterator(PigServer.java:281)
        at org.apache.pig.test.TestStoreOld.testMultipleStore(TestStoreOld.java:66)
Caused by: java.io.IOException: Job terminated with anomalous status FAILED

Testcase: testStoreWithMultipleMRJobs took 0.262 sec
        Caused an ERROR
Unable to store for alias: D [The output file(s): /tmp/temp1052178065/tmp-26634357 already exists]
java.io.IOException: Unable to store for alias: D [The output file(s): /tmp/temp1052178065/tmp-26634357 already exists]
        at org.apache.pig.PigServer.compileLp(PigServer.java:547)
        at org.apache.pig.PigServer.execute(PigServer.java:483)
        at org.apache.pig.PigServer.store(PigServer.java:327)
        at org.apache.pig.PigServer.store(PigServer.java:313)
        at org.apache.pig.PigServer.store(PigServer.java:298)
        at org.apache.pig.test.TestStoreOld.testStoreWithMultipleMRJobs(TestStoreOld.java:85)
Caused by: org.apache.pig.backend.executionengine.ExecException: The output file(s): /tmp/temp1052178065/tmp-26634357 already exists
{noformat}
",20/Aug/08 11:53;shravanmn;There was a problem with FileLocalizer's random number generator being initialized with the same seed. Fixed that by doing a randomize the FileLocalizer's RNG before every compile in both launchers.,21/Aug/08 22:11;olgan;thanks shravan for contributing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testNestedBinCond fails in TestFilterOpNumeric,PIG-328,12400748,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,shravanmn,alangates,alangates,22/Jul/08 15:18,24/Mar/10 22:04,14/Mar/19 03:05,02/Sep/08 19:55,0.2.0,,,,,,0.2.0,,grunt,,,0,,,,"Testcase: testNestedBinCond took 6.889 sec
    Caused an ERROR
java.lang.String
java.lang.ClassCastException: java.lang.String
    at org.apache.pig.test.TestFilterOpNumeric.testNestedBinCond(TestFilterOpNumeric.java:183)",,,,,,,,,,,,,,,,,,,29/Aug/08 12:29;shravanmn;328.patch;https://issues.apache.org/jira/secure/attachment/12389165/328.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-08-29 12:29:03.191,,,no_permission,,,,,,,,,,,,163965,,,,,Tue Sep 02 19:55:33 UTC 2008,,,,,,,0|i0giw7:,94491,,,,,,,,,,29/Aug/08 12:29;shravanmn;Fixed the test case to use numeric values instead of strings.,02/Sep/08 19:55;alangates;Applied the patch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Combiner Error,PIG-324,12400519,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,amirhyoussefi,amirhyoussefi,18/Jul/08 02:08,25/Mar/10 00:12,14/Mar/19 03:05,26/Jan/09 21:05,,,,,,,0.2.0,,,,,0,,,,"A = load '...' USING PigStorage('\t') AS (c1, c2, c3, n1);
B = group A by (c1,c2,c3);
C = foreach B generate flatten(group), SUM(A.n1);
store C into ...;

Runs with combiner and errors out. 

java.io.IOException: For input string: ""..."" additional info: iteration = 1bag size = 2 partial sum = 0.0
previous tupple = (...)
 at org.apache.pig.builtin.SUM.sum(SUM.java:95)
 at org.apache.pig.builtin.SUM$Final.exec(SUM.java:63)
 at org.apache.pig.builtin.SUM$Final.exec(SUM.java:60)
 at org.apache.pig.impl.eval.FuncEvalSpec$1.add(FuncEvalSpec.java:116)
 at org.apache.pig.impl.eval.GenerateSpec$CrossProductItem.<init>(GenerateSpec.java:159)
 at org.apache.pig.impl.eval.GenerateSpec$1.add(GenerateSpec.java:79)
 at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.reduce(PigMapReduce.java:165)
 at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.reduce(PigMapReduce.java:80)
 at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:391)
 at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2124)
 

Work-around was that I put out combiner: 

C = foreach B generate SUM(A.n1),flatten(group);

and it worked. Input data has some private information in it so I cannot post it. Let me know if it was not possible to solve it without having it. Then we compile a similar input. 

c1,c2,c3 are alphabetic, 
n1 is numeric.
",Pig + Hadoop 17 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-07-21 20:45:06.04,,,no_permission,,,,,,,,,,,,163961,,,,,Mon Jan 26 21:05:33 UTC 2009,,,,,,,0|i0giu7:,94482,,,,,,,,,,"18/Jul/08 21:00;amirhyoussefi;Workaround which disables combiner and makes script run successfully:

A = load '...' USING PigStorage('\t') AS (c1, c2, c3, n1);
B = group A by (c1,c2,c3);
C = foreach B generate SUM(A.n1) , flatten(group);",21/Jul/08 20:45;olgan;Is this data dependent? I know I ran similar queries in the past without any problem.,21/Jul/08 20:53;amirhyoussefi;Yes. ,21/Jul/08 21:00;olgan;we need a reproducible case to look into this. can you provide a fragment of data that is causing this problem?,"26/Jan/09 21:05;olgan;combiner code has been completely rewritten. Please, reopen if you case still fails",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove DEFINE from QueryParser,PIG-323,12400508,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,sms,sms,sms,17/Jul/08 19:22,24/Mar/10 22:04,14/Mar/19 03:05,18/Jul/08 21:07,0.2.0,,,,,,0.2.0,,impl,,,0,,,,Remove the keyword DEFINE and the associated methods from QueryParser. The syntax and semantics of define as proposed in the functional specification breaks backward compatibility. The UDFs will now provide the list of function arguments that are expected.,,,,,,,,,,,,,,,,,,,17/Jul/08 19:26;sms;remove_define_from_query_parser.patch;https://issues.apache.org/jira/secure/attachment/12386338/remove_define_from_query_parser.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-18 21:07:48.466,,,no_permission,,,,,,,,,,,,163960,,,,,Fri Jul 18 21:07:48 UTC 2008,,,Patch Available,,,,0|i0gitr:,94480,,,,,,,,,,"17/Jul/08 19:26;sms;This patch (remove_define_from_query_parser.patch) 

1. Removes the define keyword and associated methods from the QueryParser. 
2. Re-introduces the keyword and semantics of define in PigScriptParser and Grunt 
3. The test cases in TestLogicalPlanBuilder for define are removed
4. The test case in TestGrunt is modified to handle define
5. Other changes are required due to the constructor change in QueryParser

Unit test cases that still fail are:

    [junit] Running org.apache.pig.test.TestEvalPipeline
    [junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 182.018 sec
    [junit] Test org.apache.pig.test.TestEvalPipeline FAILED

    [junit] Running org.apache.pig.test.TestFilterOpNumeric
    [junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 56.336 sec
    [junit] Test org.apache.pig.test.TestFilterOpNumeric FAILED

    [junit] Running org.apache.pig.test.TestStoreOld
    [junit] Tests run: 3, Failures: 0, Errors: 2, Time elapsed: 21.436 sec
    [junit] Test org.apache.pig.test.TestStoreOld FAILED",18/Jul/08 21:07;alangates;remove_define_from_query_parser.patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect results from arithmetic expression,PIG-321,12400418,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pi_song,pkamath,pkamath,16/Jul/08 22:03,24/Mar/10 22:04,14/Mar/19 03:05,22/Jul/08 12:05,0.2.0,,,,,,0.2.0,,,,,0,,,,"Query:
{code}

a = load '/user/pig/tests/data/singlefile/studenttab10k' as (name:chararray, age:int, gpa:double);                    
b = foreach a generate 1 + 0.2f + 253645L, gpa+1;                                                                     
store b into '/tmp/arithtest';                                                                                        

{code}

Results
25365.2 2.9
25365.2 4.65
...

The first projection above has 253645 as a Long constant. The results have 25365.2 which is an order less",,,,,,,,,,,,,,,,,,,21/Jul/08 12:20;pi_song;Pig321_parser.patch;https://issues.apache.org/jira/secure/attachment/12386519/Pig321_parser.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-21 12:20:35.088,,,no_permission,,,,,,,,,,,,163958,,,,,Tue Jul 22 12:05:01 UTC 2008,,,,,,,0|i0gisf:,94474,,,,,,,,,,21/Jul/08 12:20;pi_song;Trivial fix in the parser,22/Jul/08 12:05;pi_song;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The parser/type checker should use the getSchema method of UDFs to deduce return type/schema,PIG-320,12400416,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,sms,sms,16/Jul/08 21:24,24/Mar/10 22:04,14/Mar/19 03:05,28/Jul/08 21:07,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"Currently, the parser/type checker uses the getReturnType to deduce the return type of the user defined function (UDF). This mechanism is satisfactory only for basic types (int, long, ...); for composite types (tuple, bag), the schema is also required.The abstract class EvalFunc interface exposes the outputSchema to deduce the return type/schema of the UDF. The parser/type checker should use this method to figure out the return type/schema of the UDF and use it appropriately. ",,,,,,,,,,,,,,,,,,,17/Jul/08 06:26;sms;udf_outputSchema.patch;https://issues.apache.org/jira/secure/attachment/12386266/udf_outputSchema.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-17 22:25:11.147,,,no_permission,,,,,,,,,,,,38592,,,,,Mon Jul 28 21:07:30 UTC 2008,,,Patch Available,,,,0|i0girz:,94472,,,,,,,,,,"17/Jul/08 06:26;sms;The patch udf_outputSchema.patch adds the following:

1. The type checker uses the outputSchema method in EvalFunc to deduce the output schema of the UDF

2. The TestEvalPipeline.java:testBagFunctionWithFlattening passes the unit test cases with the addition of outputSchema method to the UDFs used inside the test case

3. Adds resetSchema method to relational operators that rely on nested plans to compute schemas.

4. Resets schema of all the relational operators in the type checker to ensure correctness of the computed schema due to changes in the inputs of the operators. 

Unit test cases that still fail are:

    [junit] Running org.apache.pig.test.TestFilterOpNumeric
    [junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 56.735 sec
    [junit] Test org.apache.pig.test.TestFilterOpNumeric FAILED

    [junit] Running org.apache.pig.test.TestStoreOld
    [junit] Tests run: 3, Failures: 0, Errors: 2, Time elapsed: 21.46 sec
    [junit] Test org.apache.pig.test.TestStoreOld FAILED","17/Jul/08 22:25;alangates;Pi, since much of these changes are in the type checker, I'd like to get your feedback on them.

One comment from me.  Why do POForeach and POCogroup have resetSchema() in addition to unsetSchema()?  Shouldn't their unsetSchema methods do the work that's in resetSchema?  Is there ever a case you'll want to unset the foreach's schema with unsetting it's plan's schemas?","18/Jul/08 15:08;pi_song;First, let's define a concrete EvalFunc semantic:-

{noformat}
EvalFunc: U -> T
{noformat}

T is hardcoded in UDF implementation but in case that T is a complex type then its schema can be determined by U

getReturnType() should return the hardcoded type (e.g. Int, Double, Bag). outputSchema() should return schemas of complex types (always returning null for simple types). 

I think this should be simple enough for users to understand. The most common case would be users creating UDF: simpleType -> simpleType , they don't have to override outputSchema().

Some of the current built-in functions will be broken following this semantic but they can be easily fixed.

Agree?","18/Jul/08 16:12;sms;EvalFunc implements the outputSchema method and returns null. UDF writers need not implement the outputSchema method. In th typechecker code, we verify the return value of outputSchema method and if it is NULL, then the return type is set to the type returned by getReturnType().

{code}
//In private void setUdfSchema
if (null != udfSchema) {
...
} else {
    func.setType(DataType.findType(ef.getReturnType()));
}

{code}","21/Jul/08 14:00;pi_song;I'm curios about this.
Then if the outputSchema implementation is provided, why do extract only the first field? Isn't it more intuitive to use the whole schema directly? My suggestion might break some stuffs but it seems a bit better for users.

{noformat}
        Schema udfSchema = ef.outputSchema(inputSchema);
        if (null != udfSchema) {
            Schema.FieldSchema fs;
            try {
                fs = udfSchema.getField(0);
            } catch (ParseException pe) {
                throw new VisitorException(pe.getMessage());
            }
            func.setType(fs.type);
            try {
                func.setFieldSchema(fs);
            } catch (FrontendException fe) {
                throw new VisitorException(fe.getMessage());
            }
        } else {
            func.setType(DataType.findType(ef.getReturnType()));
        }

{noformat}","21/Jul/08 21:49;sms;UDFs are expression operators that are expected to return a FieldSchema. The schema returned by outputSchema should always have a single FieldSchema inside. Hence, only the first field is extracted.",22/Jul/08 11:56;pi_song;I think it's fine in terms of internal implementation but can't we make it simpler for users by exposing simpler interface?,"23/Jul/08 19:17;sms;The interface outputSchema in EvalFunc has remained unchanged from trunk. Prior to submitting the patch, I tried to change the interface and found that it broke a whole bunch of existing UDFs. There are two things that require a change:

1. The outputSchema method should not take any inputs. It should be outputSchema()
2. The return type of the method should be a FieldSchema and not a Schema

If we decide that these changes are required, I will go ahead and make these changes.","24/Jul/08 12:25;pi_song;EvalFunc interface is a bit confusing.
Let's wait for clarification from the mailing-list first.","28/Jul/08 10:22;pi_song;Since we don't know how EvalFunc is expected to be used. You choice of doing it at outputSchema() is one way to do and doesn't bad.
I agree with the patch.

Please note that EvalFunc should be consolidated at some stage.",28/Jul/08 21:07;alangates;udf_outputSchema.patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Union, Cross is not working",PIG-319,12400403,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pi_song,daijy,daijy,16/Jul/08 18:16,24/Mar/10 22:04,14/Mar/19 03:05,23/Jul/08 11:26,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"union and cross operator is not working in branches/types. For example:

a = load 'a';
b = load 'b';
c = union a, b;
d = cross a, b;
dump c;     // fail
dump d;     // fail

Error message: "" Attempt to give operator of type org.apache.pig.impl.physicalLayer.relationalOperators.POLoad multiple inputs.  This operator does not support multiple inputs.""",,,,,,,,,,,,,,,,,,,18/Jul/08 13:12;pi_song;fix_union.patch;https://issues.apache.org/jira/secure/attachment/12386390/fix_union.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-17 22:20:42.212,,,no_permission,,,,,,,,,,,,163957,,,,,Wed Jul 23 11:26:56 UTC 2008,,,,,,,0|i0girj:,94470,,,,,,,,,,"17/Jul/08 22:20;alangates;The fact that cross does not work is a duplicate of PIG-311.  So this bug will be used to track union not working.  

Strangely enough union appears to work in the following scenario:

a = load 'a';
b = load 'b';
a1 = foreach a generate $0, $1;
b1 = foreach b generate $0, $1;
c = union a1, b1;
dump c;","18/Jul/08 13:12;pi_song;Currently our UNION implementation doesn't handle injected inputs. This patch will add that functionality.

For CROSS, I don't see any implementation at all.","22/Jul/08 12:13;pi_song;Patch committed.

Now only the problem with Cross will have to be addressed.","22/Jul/08 14:08;alangates;I think we can close this bug now, as PIG-311 tracks the issue with Cross.",23/Jul/08 11:26;pi_song;Good idea,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issue with cast in foreach,PIG-315,12400263,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pi_song,pkamath,pkamath,15/Jul/08 00:28,24/Mar/10 22:04,14/Mar/19 03:05,16/Jul/08 12:22,0.2.0,,,,,,0.2.0,,,,,0,,,,"Query which causes error:
{code}
a = load ':INPATH:/singlefile/studenttab10k' as (name:chararray, age:int, gpa:double);
b = foreach a generate (long)age as age, (int)gpa as gpa;
c = foreach b generate SUM(age), SUM(gpa); 
store c into ':OUTPATH:';\,
{code}

Error:
{quote}
2008-07-14 16:34:42,130 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: mytesthost:8020
2008-07-14 16:34:42,187 [main] WARN  org.apache.hadoop.fs.FileSystem - ""mytesthost:8020"" is a deprecated filesystem name. Use ""hdfs://mytesthost:8020/"" instead.
2008-07-14 16:34:42,441 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to map-reduce job tracker at: mytesthost:50020
2008-07-14 16:34:42,696 [main] WARN  org.apache.hadoop.fs.FileSystem - ""mytesthost:8020"" is a deprecated filesystem name. Use ""hdfs://mytesthost:8020/"" instead.
2008-07-14 16:34:43,006 [main] ERROR org.apache.pig.PigServer - Problem resolving LOForEach schema
2008-07-14 16:34:43,006 [main] ERROR org.apache.pig.PigServer - Severe problem found during validation org.apache.pig.impl.plan.PlanValidationException: An unexpected exception caused the validation to stop
2008-07-14 16:34:43,007 [main] ERROR org.apache.pig.tools.grunt.Grunt - java.io.IOException: Unable to store for alias: c

{quote}",,,,,,,,,,,,,,,,,,,15/Jul/08 14:41;pi_song;PIG315.patch;https://issues.apache.org/jira/secure/attachment/12386057/PIG315.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-15 14:41:51.546,,,no_permission,,,,,,,,,,,,163953,,,,,Wed Jul 16 12:22:21 UTC 2008,,,,,,,0|i0gipr:,94462,,,,,,,,,,"15/Jul/08 14:41;pi_song;The patch!

1) In ""(long)age as age"" we treat ""as"" as type cast but in fact it is only used for altering the alias in this context
2) ""SUM(age)"" is aggregate of a flattened member which is a case we haven't handled yet in type checker (Normally we do like SUM(A.age) ).

I think how we treat aggregates still can be improved to be cleaner.",16/Jul/08 12:22;pi_song;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error handling aggregate of a computation,PIG-313,12400260,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,alangates,pkamath,pkamath,15/Jul/08 00:20,04/Aug/11 00:34,14/Mar/19 03:05,24/Jan/11 18:45,0.2.0,,,,,,0.9.0,,,,,0,,,,"Query which fails:

{code}
a = load ':INPATH:/singlefile/studenttab10k' as (name:chararray, age:int, gpa:double);
b = group a by name;
c = foreach b generate group, SUM(a.age*a.gpa);                            
store c into ':OUTPATH:';\,
{code}

Error output:
{quote}
2008-07-14 16:34:08,684 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: testhost.com:8020
2008-07-14 16:34:08,741 [main] WARN  org.apache.hadoop.fs.FileSystem - ""testhost.com:8020"" is a deprecated filesystem name. Use ""hdfs://testhost:8020/"" instead.
2008-07-14 16:34:08,995 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to map-reduce job tracker at: testhost.com:50020
2008-07-14 16:34:09,251 [main] WARN  org.apache.hadoop.fs.FileSystem - ""testhost.com:8020"" is a deprecated filesystem name. Use ""hdfs://testhost:8020/"" instead.
2008-07-14 16:34:09,559 [main] ERROR org.apache.pig.PigServer - Cannot evaluate output type of Mul/Div Operator
2008-07-14 16:34:09,559 [main] ERROR org.apache.pig.PigServer - Problem resolving LOForEach schema
2008-07-14 16:34:09,559 [main] ERROR org.apache.pig.PigServer - Severe problem found during validation org.apache.pig.impl.plan.PlanValidationException: An unexpected exception caused the validation to stop 
2008-07-14 16:34:09,560 [main] ERROR org.apache.pig.tools.grunt.Grunt - java.io.IOException: Unable to store for alias: c
2008-07-14 16:34:09,560 [main] ERROR org.apache.pig.Main - java.io.IOException: Unable to store for alias: c
{quote}",,,,,,,,,,,,,,,,,,,12/Jan/11 20:50;daijy;PIG-313-1.patch;https://issues.apache.org/jira/secure/attachment/12468163/PIG-313-1.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-16 12:25:36.795,,,no_permission,,,,,,,,,,,,66260,Reviewed,,,,Mon Jan 24 18:45:32 UTC 2011,,,,,,,0|i0giov:,94458,,,,,,,,,,"16/Jul/08 12:25;pi_song;I think we have discussed about this before and the conclusion is we don't support this.

Consider this query:-
{noformat}
b = cogroup a1 by name, a2 by name;
c = foreach b generate group, SUM(a1.age*a2.gpa);                            
store c into ':OUTPATH:';\,
{noformat}

This will make it difficult for us because a1.age gives a bag and a2.gpa also gives a bag.
What is the definition of bag multiplied by bag?","16/Jul/08 20:58;pkamath;Per http://wiki.apache.org/pig/PigTypesFunctionalSpec - in the last section: ""Argument Construction for Functions"" - it says that the computation will be done the fields per tuple in the group and the computed results will be stored into a bag and then supplied to SUM - Is this not going to be the case in this new Pig types release - if not the wiki should be updated.",21/Jul/08 13:13;pi_song;That sounds right. Then this is a problem in parser.,"23/Jul/08 18:14;pkamath;Another case of this issue is the following:

{code}
a = load 'singlefile/studenttab10k' as (name, age, gpa);
b = group a ALL;
c = foreach b generate SUM((int)(a.age)), MIN((int)(a.age)), MAX((int)(a.age)), AVG((int)(a.age)), MIN((chararray)(a.name)), MAX((chararray)(a.name)), SUM((double)(a.gpa)), MIN((double)(a.gpa)), MAX((double)(a.gpa)), AVG((double)(a.gpa));
store c into 'outdir';
{code}
In this case, the cast fails since it is trying to cast a bag of bytearray to int. However it should really cast each bytearray to int and then supply the bag of ints to SUM() etc.",04/Sep/08 23:39;olgan;Pi is correct - we do not support this right now. One idea we considered for future work is to define + operator on bags to match SQL semantics. Other approaches are also possible.,"12/Jan/11 20:50;daijy;Run it on trunk, I get a meaningful error message in the front end:
ERROR 1039: In alias c, incompatible types in Multiplication Operator left hand side:bag right hand side:bag

Attach a test case to make sure after new TypeChecker, this error message is still there.",15/Jan/11 00:25;rding;+1,"24/Jan/11 18:45;daijy;Review notes:
https://reviews.apache.org/r/276/

Patch committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Casting a byte array that contains a double value to an int results in a null pointer,PIG-312,12400257,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,14/Jul/08 21:42,24/Mar/10 22:04,14/Mar/19 03:05,17/Jul/08 17:05,0.2.0,,,,,,0.2.0,,,,,0,,,,"{code}
a = load 'myfile' as (name, age, gpa);                                                                        
c = foreach a generate age * 10, (int)gpa * 2;                                                                                                                  
store c into 'outfile';
{code}
The values in gpa are doubles.  The issue is that they are read as byte arrays and then when the user tries to cast them to an int, the system does a direct cast from byte array to int, which results in a null.  First of all, it should result in a zero, not a null (unless the underlying value is null).  Second, we have to clarify semantics here.  gpa was never officially declared to be a double, so trying to do a cast directly from bytearray to int is a reasonable thing to do.  But users may not see it that way.  Do we want to first cast numbers to double and then to anything subsequent to avoid this?  Or should we force users to write this as (int)(double)gpa * 2 so we know to first cast to double and then int?  In the interest of speed (especially considering the rarity of doubles in most data) I'd vote for the latter.",,,,,,,,,,,,,,,,,,,16/Jul/08 18:37;alangates;intcast.patch;https://issues.apache.org/jira/secure/attachment/12386211/intcast.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,163951,,,,,Thu Jul 17 17:05:30 UTC 2008,,,,,,,0|i0giof:,94456,,,,,,,,,,"15/Jul/08 15:39;alangates;Rather than letting (int)gpa return a 0, we could change it to take just the integer portion of the double.  This seems better.","16/Jul/08 18:37;alangates;Fix for this issue.  This fix does differ a bit from what I said in the initial posting.  At this point anything that cannot be cast to the requested numeric type is still returned as a null rather than 0.  After further thought, this seems like a better course, as putting a 0 in there implies we managed to cast the data rather than we didn't know what to do with the data.

It does fix the issue of casting double values to ints and longs.  The casts now first try to cast to int (or long) and if that fails they then cast to a double and then cast that to an int (or long) checking to make sure there isn't an overflow.",17/Jul/08 17:05;alangates;intcast.patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scripts using CROSS fail in logical to physical translator,PIG-311,12400255,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,shravanmn,alangates,alangates,14/Jul/08 21:35,24/Mar/10 22:04,14/Mar/19 03:05,13/Aug/08 23:47,0.2.0,,,,,,0.2.0,,,,,0,,,,"{code}
a = load 'myfile' as (name, age, gpa);                                                                        
b = load 'myotherfile' as (name, age, registration, contributions);                                                                         
c = filter a by age < 19 and gpa < 1.0;                                                                                                                         
d = filter b by age < 19;                                                                                                                                       
e = cross c, d;                                                                                                                                                 
store e into 'outfile';
{code}

fails:

java.io.IOException: Unable to store for alias: e [null]
    at org.apache.pig.impl.plan.OperatorPlan.checkInPlan(OperatorPlan.java:252)
    at org.apache.pig.impl.plan.OperatorPlan.connect(OperatorPlan.java:140)
    at org.apache.pig.impl.physicalLayer.plans.PhysicalPlan.connect(PhysicalPlan.java:77)
    at org.apache.pig.impl.logicalLayer.LogToPhyTranslationVisitor.visit(LogToPhyTranslationVisitor.java:859)
    at org.apache.pig.impl.logicalLayer.LOStore.visit(LOStore.java:101)
    at org.apache.pig.impl.logicalLayer.LOStore.visit(LOStore.java:36)
    at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:68)
    at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
    at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:229)
    at org.apache.pig.PigServer.compilePp(PigServer.java:556)
    at org.apache.pig.PigServer.execute(PigServer.java:482)
    at org.apache.pig.PigServer.store(PigServer.java:324)
    at org.apache.pig.PigServer.store(PigServer.java:310)
    at org.apache.pig.tools.grunt.GruntParser.processStore(GruntParser.java:173)
    at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:317)
    at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:77)
    at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:58)
    at org.apache.pig.Main.main(Main.java:311)
Caused by: java.lang.NullPointerException
    ... 18 more",,,,,,,,,,,,,,,,,,,13/Aug/08 12:48;shravanmn;311.patch;https://issues.apache.org/jira/secure/attachment/12388139/311.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-18 15:11:06.215,,,no_permission,,,,,,,,,,,,163950,,,,,Wed Aug 13 23:47:56 UTC 2008,,,,,,,0|i0ginz:,94454,,,,,,,,,,18/Jul/08 15:11;pi_song;It fails because there is no implementation.,"18/Jul/08 15:32;pi_song;Apparently, it can be done in replicate-and-merge fashion on MR.","13/Aug/08 12:47;shravanmn;Implemented the visit(LOCross) method in LogToPhyTranslationVisitor. This mimics what we were doing in Pig-1.0. To summarize, the following script with Cross will be converted as shown below:

{noformat}
A1 = load 'f1';
A2 = load 'f2';
.
.
.
An = load 'fn';
B = cross A1,A2,...,An;
{noformat}

{noformat}
A1 = load 'f1';
.
.
.
An = load 'fn';
B1 = foreach A1 generate flatten(GFCross('n','0')), flatten(*);
B2 = foreach A2 generate flatten(GFCross('n','1')), flatten(*);
.
.
.
Bn = foreach An generate flatten(GFCross('n','n-1')), flatten(*);
C = splgroup B1 by ($0,$1,..,$n-1) inner, B2 by ($0,$1,..,$n-1) inner, ..., Bn by ($0,$1,..,$n-1) inner;
D = foreach C generate flatten($1), flatten($2), ..., flatten($n);
{noformat}

GFCross outputs a bag with n-tuples and the foreach flattens the bag attaches them to the original tuples thus replicating each tuple.

The only difference from a normal pig script is the splgroup where the local-rearrange has a slight modification. When it is processing a cross, it removes the first n values from each value tuple which were attached to it by the foreach and passes the correct tuple as value while retaining the first n values as the key.

For ex, the foreach might produce (2,1,R,4) where (R,4) is the actual tuple & (2,1) is one of the tuples in the GFCross output. The localrearrange here arranges such tuples into keys and values by makeing (2,1) the key and (R,4) the value.

So the patch has two changes: one to translator & the other to localrearrange.","13/Aug/08 23:47;olgan;patch committed. thanks, shravan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some nested order by queries fail in logical to physical translator,PIG-310,12400254,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,sms,alangates,alangates,14/Jul/08 21:33,24/Mar/10 22:04,14/Mar/19 03:05,24/Jul/08 12:24,0.2.0,,,,,,0.2.0,,,,,0,,,,"The query:

{code}
a = load 'myfile';                                                                                            
b = group a by $0;                                                                                                                                              
c = foreach b {c1 = order $1 by $1; generate flatten(c1); };                                                                                                    
store c into 'outfile'
{code}

dies with the error message:

java.io.IOException: Unable to store for alias: c [null]
    at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:232)
    at org.apache.pig.PigServer.compilePp(PigServer.java:556)
    at org.apache.pig.PigServer.execute(PigServer.java:482)
    at org.apache.pig.PigServer.store(PigServer.java:324)
    at org.apache.pig.PigServer.store(PigServer.java:310)
    at org.apache.pig.tools.grunt.GruntParser.processStore(GruntParser.java:173)
    at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:317)
    at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:77)
    at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:58)
    at org.apache.pig.Main.main(Main.java:311)
Caused by: org.apache.pig.backend.executionengine.ExecException
    ... 10 more
Caused by: org.apache.pig.impl.plan.VisitorException
    at org.apache.pig.impl.logicalLayer.LogToPhyTranslationVisitor.visit(LogToPhyTranslationVisitor.java:726)
    at org.apache.pig.impl.logicalLayer.LOSort.visit(LOSort.java:141)
    at org.apache.pig.impl.logicalLayer.LOSort.visit(LOSort.java:35)
    at org.apache.pig.impl.plan.DependencyOrderWalkerWOSeenChk.walk(DependencyOrderWalkerWOSeenChk.java:68)
    at org.apache.pig.impl.logicalLayer.LogToPhyTranslationVisitor.visit(LogToPhyTranslationVisitor.java:651)
    at org.apache.pig.impl.logicalLayer.LOForEach.visit(LOForEach.java:87)
    at org.apache.pig.impl.logicalLayer.LOForEach.visit(LOForEach.java:36)
    at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:68)
    at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
    at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:229)
    ... 9 more
Caused by: org.apache.pig.impl.plan.PlanException: Attempt to connect operator Project[tuple][1] - gates-Sat Jul 12 17:57:09 PDT 2008-16 which is not in the pla
    at org.apache.pig.impl.plan.OperatorPlan.checkInPlan(OperatorPlan.java:254)
    at org.apache.pig.impl.plan.OperatorPlan.connect(OperatorPlan.java:140)
    at org.apache.pig.impl.physicalLayer.plans.PhysicalPlan.connect(PhysicalPlan.java:77)
    at org.apache.pig.impl.logicalLayer.LogToPhyTranslationVisitor.visit(LogToPhyTranslationVisitor.java:723)
    ... 18 more",,,,,,,,,,,,,,,,,,,23/Jul/08 19:04;sms;sort_with_nested_project.patch;https://issues.apache.org/jira/secure/attachment/12386749/sort_with_nested_project.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-23 18:34:49.187,,,no_permission,,,,,,,,,,,,38593,,,,,Thu Jul 24 12:24:13 UTC 2008,,,Patch Available,,,,0|i0ginj:,94452,,,,,,,,,,"18/Jul/08 17:08;alangates;It looks to me like the logical plan isn't being constructed quite correctly.  It appears that one project is being connected in two different places in the plan.

Here is the logical plan for the above script:

{code}
Logical Plan:
ForEach gates-Fri Jul 18 09:28:13 PDT 2008-8 Schema: {bytearray} Type: bag
|   |
|   Project gates-Fri Jul 18 09:28:13 PDT 2008-6 Projections:  [*]  Overloaded: false FieldSchema: CA: bag Type: bag
|   Input: SORT gates-Fri Jul 18 09:28:13 PDT 2008-5|
|   |---SORT gates-Fri Jul 18 09:28:13 PDT 2008-5 Schema: null Type: bag
|       |   |
|       |   Project gates-Fri Jul 18 09:28:13 PDT 2008-4 Projections: [0] Overloaded: false FieldSchema: bytearray Type: bytearray
|       |   Input: Project gates-Fri Jul 18 09:28:13 PDT 2008-3 Projections: [1] Overloaded: true|
|       |   |---Project gates-Fri Jul 18 09:28:13 PDT 2008-3 Projections: [1] Overloaded: true FieldSchema: A: tuple Type: tuple
|       |       Input: CoGroup gates-Fri Jul 18 09:28:13 PDT 2008-2
|       |
|       |---Project gates-Fri Jul 18 09:28:13 PDT 2008-3 Projections: [1] Overloaded: true FieldSchema: A: tuple Type: tuple
|           Input: CoGroup gates-Fri Jul 18 09:28:13 PDT 2008-2
|
|---CoGroup gates-Fri Jul 18 09:28:13 PDT 2008-2 Schema: {group: bytearray,A: (null)} Type: bag
    |   |
    |   Project gates-Fri Jul 18 09:28:13 PDT 2008-1 Projections: [0] Overloaded: false FieldSchema: bytearray Type: bytearray
    |   Input: Load gates-Fri Jul 18 09:28:13 PDT 2008-
    |
    |---Load gates-Fri Jul 18 09:28:13 PDT 2008-0 Schema: null Type: bag
{code}

Notice that the project named ""Project gates-Fri Jul 18 09:28:13 PDT 2008-3"" is connected both directly to the sort (that is, it's sorts input), and it is in the plan of sort.  This does not look correct.  And if you change the script from ""order by $1"" to ""order by *"", then the two projects are different, and the script works.

Assigning to Santhosh to take a look.  If this isn't an issue please assign it back to me.","23/Jul/08 18:34;sms;The issue has been fixed. The logical, type checked and physical plans for the query will now look like:

{noformat}
ForEach Test-Plan-Builder-8 Schema: {bytearray} Type: bag
|   |
|   Project Test-Plan-Builder-6 Projections:  [*]  Overloaded: false FieldSchema: c1: tuple Type: tuple
|   Input: SORT Test-Plan-Builder-5|
|   |---SORT Test-Plan-Builder-5 Schema: null Type: bag
|       |   |
|       |   Project Test-Plan-Builder-4 Projections: [1] Overloaded: false FieldSchema: bytearray Type: bytearray
|       |   Input: Project Test-Plan-Builder-3 Projections: [1] Overloaded: fals
|       |
|       |---Project Test-Plan-Builder-3 Projections: [1] Overloaded: false FieldSchema: a: bag Type: bag
|           Input: CoGroup Test-Plan-Builder-2
|
|---CoGroup Test-Plan-Builder-2 Schema: {group: bytearray,a: {null}} Type: Unknown
    |   |
    |   Project Test-Plan-Builder-1 Projections: [0] Overloaded: false FieldSchema: bytearray Type: bytearray
    |   Input: Load Test-Plan-Builder-
    |
    |---Load Test-Plan-Builder-0 Schema: null Type: bag
Printing the logical plan
ForEach Test-Plan-Builder-8 Schema: {bytearray} Type: bag
|   |
|   Project Test-Plan-Builder-6 Projections:  [*]  Overloaded: false FieldSchema: c1: bag Type: bag
|   Input: SORT Test-Plan-Builder-5|
|   |---SORT Test-Plan-Builder-5 Schema: null Type: bag
|       |   |
|       |   Project Test-Plan-Builder-4 Projections: [1] Overloaded: false FieldSchema: bytearray Type: bytearray
|       |   Input: Project Test-Plan-Builder-3 Projections: [1] Overloaded: tru
|       |
|       |---Project Test-Plan-Builder-3 Projections: [1] Overloaded: true FieldSchema: a: tuple Type: tuple
|           Input: CoGroup Test-Plan-Builder-2
|
|---CoGroup Test-Plan-Builder-2 Schema: {group: bytearray,a: (null)} Type: bag
    |   |
    |   Project Test-Plan-Builder-1 Projections: [0] Overloaded: false FieldSchema: bytearray Type: bytearray
    |   Input: Load Test-Plan-Builder-
    |
    |---Load Test-Plan-Builder-0 Schema: null Type: bag
Printing the physical  plan
New For Each(true)[bag] - Test-Plan-Builder-18
|   |
|   Project[bag][*] - Test-Plan-Builder-17
|   |
|   |---POSort[bag]() - Test-Plan-Builder-16
|       |   |
|       |   Project[bytearray][1] - Test-Plan-Builder-15
|       |
|       |---Project[tuple][1] - Test-Plan-Builder-14
|
|---Package[tuple]{bytearray} - Test-Plan-Builder-11
    |
    |---Global Rearrange[tuple] - Test-Plan-Builder-10
        |
        |---Local Rearrange[tuple]{bytearray} - Test-Plan-Builder-12
            |   |
            |   Project[bytearray][0] - Test-Plan-Builder-13
            |
            |---Load(myfile:org.apache.pig.builtin.PigStorage) - Test-Plan-Builder-9
{noformat}","23/Jul/08 19:04;sms;The patch fixes the following issue:

1. In LOSort's nested plans, the projections will not be connected to their inputs. Earlier, due to the connection, the same operator was playing dual roles: 
   i. Sort's input
   ii. The sort column's input

Unit test cases that still fail are:

 [junit] Running org.apache.pig.test.TestEvalPipeline
[junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 179.904 sec
[junit] Test org.apache.pig.test.TestEvalPipeline FAILED

[junit] Running org.apache.pig.test.TestFilterOpNumeric
[junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 56.124 sec
[junit] Test org.apache.pig.test.TestFilterOpNumeric FAILED

[junit] Running org.apache.pig.test.TestBuiltin
[junit] Tests run: 23, Failures: 1, Errors: 1, Time elapsed: 14.8 sec
[junit] Test org.apache.pig.test.TestBuiltin FAILED

[junit] Running org.apache.pig.test.TestStoreOld
[junit] Tests run: 3, Failures: 0, Errors: 2, Time elapsed: 21.453 sec
[junit] Test org.apache.pig.test.TestStoreOld FAILED","24/Jul/08 12:24;pi_song;Committed

Thanks Santhosh.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flatten is not being set to true in joins,PIG-308,12400252,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,14/Jul/08 21:25,24/Mar/10 22:04,14/Mar/19 03:05,18/Jul/08 15:58,0.2.0,,,,,,0.2.0,,impl,,,0,,,,Queries that use the JOIN keyword are returning incorrect results because the flatten values are not being set to true for the foreach that is put after the cogroup.,,,,,,,,,,,,,,,,,,,17/Jul/08 19:54;alangates;join.patch;https://issues.apache.org/jira/secure/attachment/12386340/join.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,163948,,,,,Fri Jul 18 15:58:35 UTC 2008,,,,,,,0|i0gimn:,94448,,,,,,,,,,"17/Jul/08 19:54;alangates;The issue was not flatten being set.  That was done properly.  The issue was that in rewriting the join to the cogroup the parser was incorrectly connecting the resulting foreach to the load instead of the cogroup, and it was incorrectly setting the projects to star instead of two the appropriate columns.",18/Jul/08 15:58;alangates;join.patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
count with multiple group by keys fails,PIG-306,12400250,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,alangates,alangates,14/Jul/08 21:21,24/Mar/10 22:04,14/Mar/19 03:05,23/Jul/08 10:43,0.2.0,,,,,,0.2.0,,,,,0,,,,"The query:

{code}
a = load 'myfile' as (name, age, gpa);                                                                        
b = group a by (name, age);                                                                                                                                     
c = foreach b generate group.name, group.age, COUNT(a.gpa);                                                                                                     
store c into 'outfile';
{code}

generates

07-12 16:55:54,348 [main] ERROR org.apache.pig.impl.mapReduceLayer.Launcher - Error message from task (reduce) tip_200807090821_0580_r_000000 java.lang.ClassCastException: org.apache.pig.data.DataByteArray cannot be cast to org.apache.pig.data.Tuple
at org.apache.pig.impl.physicalLayer.expressionOperators.POProject.getNext(POProject.java:262)
at org.apache.pig.impl.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:189)
at org.apache.pig.impl.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:148)
at org.apache.pig.impl.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:164)
at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:333)
at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2071)",,,,,,,,,,,,,,,,,,,23/Jul/08 00:57;sms;cogroup_schema.patch;https://issues.apache.org/jira/secure/attachment/12386675/cogroup_schema.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-23 00:55:12.737,,,no_permission,,,,,,,,,,,,163946,,,,,Wed Jul 23 10:43:33 UTC 2008,,,Patch Available,,,,0|i0gilr:,94444,,,,,,,,,,"18/Jul/08 17:23;alangates;The issue is that the type of one of the projects is not being set correctly.  The logical plan for this query looks like:

{code}
Logical Plan:
ForEach gates-Fri Jul 18 10:11:47 PDT 2008-12 Schema: {name: (null),age: (null),long} Type: bag
|   |
|   Project gates-Fri Jul 18 10:11:47 PDT 2008-5 Projections: [0] Overloaded: false FieldSchema: name: tuple Type: tuple
|   Input: Project gates-Fri Jul 18 10:11:47 PDT 2008-4 Projections: [0] Overloaded: false|
|   |---Project gates-Fri Jul 18 10:11:47 PDT 2008-4 Projections: [0] Overloaded: false FieldSchema: group: tuple({bytearray,bytearray}) Type: tuple
|       Input: CoGroup gates-Fri Jul 18 10:11:47 PDT 2008-3
|   |
|   Project gates-Fri Jul 18 10:11:47 PDT 2008-7 Projections: [1] Overloaded: false FieldSchema: age: tuple Type: tuple
|   Input: Project gates-Fri Jul 18 10:11:47 PDT 2008-6 Projections: [0] Overloaded: false|
|   |---Project gates-Fri Jul 18 10:11:47 PDT 2008-6 Projections: [0] Overloaded: false FieldSchema: group: tuple({bytearray,bytearray}) Type: tuple
|       Input: CoGroup gates-Fri Jul 18 10:11:47 PDT 2008-3
|   |
|   UserFunc gates-Fri Jul 18 10:11:47 PDT 2008-10 function: org.apache.pig.builtin.COUNT FieldSchema: long Type: long
|   |
|   |---Project gates-Fri Jul 18 10:11:47 PDT 2008-9 Projections: [2] Overloaded: false FieldSchema: gpa: bag({gpa: bytearray}) Type: bag
|       Input: Project gates-Fri Jul 18 10:11:47 PDT 2008-8 Projections: [1] Overloaded: false|
|       |---Project gates-Fri Jul 18 10:11:47 PDT 2008-8 Projections: [1] Overloaded: false FieldSchema: a: bag({name: bytearray,age: bytearray,gpa: bytearray}) Type: bag
|           Input: CoGroup gates-Fri Jul 18 10:11:47 PDT 2008-3
|
|---CoGroup gates-Fri Jul 18 10:11:47 PDT 2008-3 Schema: {group: (bytearray,bytearray),a: {name: bytearray,age: bytearray,gpa: bytearray}} Type: bag
    |   |
    |   Project gates-Fri Jul 18 10:11:47 PDT 2008-1 Projections: [0] Overloaded: false FieldSchema: name: bytearray cn: 0 Type: bytearray
    |   Input: Load gates-Fri Jul 18 10:11:47 PDT 2008-
    |   |
    |   Project gates-Fri Jul 18 10:11:47 PDT 2008-2 Projections: [1] Overloaded: false FieldSchema: age: bytearray cn: 1 Type: bytearray
    |   Input: Load gates-Fri Jul 18 10:11:47 PDT 2008-
    |
    |---Load gates-Fri Jul 18 10:11:47 PDT 2008-0 Schema: {name: bytearray,age: bytearray,gpa: bytearray} Type: bag
{code}

Projects ""Project gates-Fri Jul 18 10:11:47 PDT 2008-5"" and ""Project gates-Fri Jul 18 10:11:47 PDT 2008-7"" should have type bytearray, not type tuple.","23/Jul/08 00:55;sms;Fixed the issue with the cogroup schema computation. The logical plan for the query in the bug report will now look like:

{noformat}
ForEach Test-Plan-Builder-12 Schema: {name: bytearray,age: bytearray,long} Type: bag
|   |
|   Project Test-Plan-Builder-5 Projections: [0] Overloaded: false FieldSchema: name: bytearray Type: bytearray
|   Input: Project Test-Plan-Builder-4 Projections: [0] Overloaded: false|
|   |---Project Test-Plan-Builder-4 Projections: [0] Overloaded: false FieldSchema: group: tuple({name: bytearray,age: bytearray}) Type: tuple
|       Input: CoGroup Test-Plan-Builder-3
|   |
|   Project Test-Plan-Builder-7 Projections: [1] Overloaded: false FieldSchema: age: bytearray Type: bytearray
|   Input: Project Test-Plan-Builder-6 Projections: [0] Overloaded: false|
|   |---Project Test-Plan-Builder-6 Projections: [0] Overloaded: false FieldSchema: group: tuple({name: bytearray,age: bytearray}) Type: tuple
|       Input: CoGroup Test-Plan-Builder-3
|   |
|   UserFunc Test-Plan-Builder-10 function: org.apache.pig.builtin.COUNT FieldSchema: long Type: long
|   |
|   |---Project Test-Plan-Builder-9 Projections: [2] Overloaded: false FieldSchema: gpa: bytearray cn: 2 Type: bytearray
|       Input: Project Test-Plan-Builder-8 Projections: [1] Overloaded: false|
|       |---Project Test-Plan-Builder-8 Projections: [1] Overloaded: false FieldSchema: a: bag({name: bytearray,age: bytearray,gpa: bytearray}) Type: bag
|           Input: CoGroup Test-Plan-Builder-3
|
|---CoGroup Test-Plan-Builder-3 Schema: {group: (name: bytearray,age: bytearray),a: {name: bytearray,age: bytearray,gpa: bytearray}} Type: Unknown
    |   |
    |   Project Test-Plan-Builder-1 Projections: [0] Overloaded: false FieldSchema: name: bytearray cn: 0 Type: bytearray
    |   Input: Load Test-Plan-Builder-
    |   |
    |   Project Test-Plan-Builder-2 Projections: [1] Overloaded: false FieldSchema: age: bytearray cn: 1 Type: bytearray
    |   Input: Load Test-Plan-Builder-
    |
    |---Load Test-Plan-Builder-0 Schema: {name: bytearray,age: bytearray,gpa: bytearray} Type: bag

{noformat}","23/Jul/08 00:57;sms;The patch fixes the following:

1. Cogroup schema compuation
2. LOMapLookup fieldSchema computation


Unit test cases that still fail are:

 [junit] Running org.apache.pig.test.TestEvalPipeline
[junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 179.904 sec
[junit] Test org.apache.pig.test.TestEvalPipeline FAILED

[junit] Running org.apache.pig.test.TestFilterOpNumeric
[junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 56.124 sec
[junit] Test org.apache.pig.test.TestFilterOpNumeric FAILED

[junit] Running org.apache.pig.test.TestBuiltin
[junit] Tests run: 23, Failures: 1, Errors: 1, Time elapsed: 14.8 sec
[junit] Test org.apache.pig.test.TestBuiltin FAILED

[junit] Running org.apache.pig.test.TestStoreOld
[junit] Tests run: 3, Failures: 0, Errors: 2, Time elapsed: 21.453 sec
[junit] Test org.apache.pig.test.TestStoreOld FAILED","23/Jul/08 10:43;pi_song;Totally agree.

Committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nested order by queries return only one key instead of all keys,PIG-305,12400249,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,14/Jul/08 21:17,24/Mar/10 22:04,14/Mar/19 03:05,16/Jul/08 15:38,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"Queries like:

{code}
a = load 'myfile';                                                                                            
b = group a by $0;                                                                                                                                              
c = foreach b {
    c1 = order $1 by *; 
    generate flatten(c1); 
};                                                                                                     
store c into 'outfile';
{code}

return just one key of the data instead of returning all keys.
",,,,,,,,,,,,,,,,,,,16/Jul/08 00:15;alangates;innerorder.patch;https://issues.apache.org/jira/secure/attachment/12386117/innerorder.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-15 18:13:45.194,,,no_permission,,,,,,,,,,,,163945,,,,,Wed Jul 16 15:38:57 UTC 2008,,,,,,,0|i0gilb:,94442,,,,,,,,,,15/Jul/08 16:20;alangates;At least part of the issue is the logical plan in the above query is not correctly connected together.  This looks related to what Tyson reported in PIG-299.  Don't know whether it's a duplicate or if there are additional issues.  Assigning to Santhosh to at least deal with the parser issues.,"15/Jul/08 18:13;sms;A debug output for the above query without the store command is given below. The post-parse logical plan looks fine.

{noformat}
08/07/15 10:55:01 INFO parser.QueryParser: Generate Inputs: [(Name: SORT Test-Plan-Builder-5 Operator Key: Test-Plan-Builder-5)]
08/07/15 10:55:01 INFO parser.QueryParser: Generate Input required
08/07/15 10:55:01 INFO parser.QueryParser: Connected (Name: SORT Test-Plan-Builder-5 Operator Key: Test-Plan-Builder-5) to (Name: Project Test-Plan-Builder-6 Projections:  [*]  Overloaded: false Operator Key: Test-Plan-Builder-6)
Printing the logical plan
ForEach Test-Plan-Builder-8 Schema: {bytearray} Type: bag
|   |
|   Project Test-Plan-Builder-6 Projections:  [*]  Overloaded: false FieldSchema: c1: tuple Type: tuple
|   Input: SORT Test-Plan-Builder-5|
|   |---SORT Test-Plan-Builder-5 Schema: null Type: bag
|       |   |
|       |   Project Test-Plan-Builder-4 Projections:  [*]  Overloaded: false FieldSchema: a: bag Type: bag
|       |   Input: Project Test-Plan-Builder-3 Projections: [1] Overloaded: fals
|       |
|       |---Project Test-Plan-Builder-3 Projections: [1] Overloaded: false FieldSchema: a: bag Type: bag
|           Input: CoGroup Test-Plan-Builder-2
|
|---CoGroup Test-Plan-Builder-2 Schema: {group: bytearray,a: {null}} Type: Unknown
    |   |
    |   Project Test-Plan-Builder-1 Projections: [0] Overloaded: false FieldSchema: bytearray Type: bytearray
    |   Input: Load Test-Plan-Builder-
    |
    |---Load Test-Plan-Builder-0 Schema: null Type: bag
{noformat}",15/Jul/08 18:14;sms;Assigning it to Alan for further investigation.,"16/Jul/08 00:15;alangates;This has nothing to do with the logical plan, contrary to what I originally posited.  It was an error in POSort that wasn't resetting the done flag so that subsequent values were not getting processed.","16/Jul/08 15:38;pi_song;Added a test and committed.
Thanks Alan.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Distinct fail if previous map plan is closed,PIG-304,12400125,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,daijy,daijy,11/Jul/08 17:13,24/Mar/10 22:04,14/Mar/19 03:05,15/Jul/08 16:58,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"Distinct will fail when the previous map plan is closed. For example, the following script fail:

a = load 'a';
b = group a by $0;
c = foreach b generate $1;
d = distinct c;
dump d;",,,,,,,,,,,,,,,,,,,11/Jul/08 17:14;daijy;distinct.patch;https://issues.apache.org/jira/secure/attachment/12385886/distinct.patch,14/Jul/08 17:13;alangates;movelocalrearrange.patch;https://issues.apache.org/jira/secure/attachment/12385992/movelocalrearrange.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-07-11 17:46:34.146,,,no_permission,,,,,,,,,,,,163944,,,,,Tue Jul 15 16:58:14 UTC 2008,,,Patch Available,,,,0|i0gil3:,94441,,,,,,,,,,"11/Jul/08 17:46;alangates;The problem is more general in nature than just distinct.  Queries such as the following do not work either:

a = load '/Users/gates/test/data/studenttab10' as (name, age, gpa);
b = load '/Users/gates/test/data/votertab10' as (name, age, registration, contributions);
c = filter a by age < 50;   
d = filter b by age < 50;  
e = cogroup c by (name, age), d by (name, age) ;
f = foreach e generate flatten(c), flatten(d);                                                                                                                  
g = group f by registration;                                                                                                                                    
h = foreach g generate group, SUM(f.d::contributions);                                                                                                          
i = order h by $1;                                                                                                                                            
dump i;

The same erro is seen at the second grouping (alias g).","14/Jul/08 17:13;alangates;This patch adds a post MR compile visitor that moves local rearranges from the reducer to the next mapper.  This is not the best solution.  The best solution would be to make the maps somehow no-ops, so that we can avoid that stage.  Ways to do that need further investigation.  The next best solution would be to correct the logic in MRCompiler to place the local rearranges in the subsequent mapper instead of the reducer.  But Shravan is out until Aug 4 and I don't want to change that code without his input.  And seems to work.",15/Jul/08 16:58;alangates;movelocalrearrange.patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
POCast does not cast chararray to bytearray,PIG-303,12400069,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,sms,sms,10/Jul/08 22:00,24/Mar/10 22:04,14/Mar/19 03:05,31/Jul/08 17:59,0.2.0,,,,,,0.2.0,,,,,0,,,,"When chararray is cast to bytearray, the query execution fails due to ClassCastException. The problem is inside the getNext(DataByteArray) code in POCast.",,,,,,,,,,,,,,,,,,,21/Jul/08 22:41;sms;pig_type_to_bytearray.patch;https://issues.apache.org/jira/secure/attachment/12386582/pig_type_to_bytearray.patch,27/Jul/08 05:15;sms;remove_cast_to_bytearray.patch;https://issues.apache.org/jira/secure/attachment/12386968/remove_cast_to_bytearray.patch,29/Jul/08 23:30;sms;remove_cast_to_bytearray_reuse_parser.patch;https://issues.apache.org/jira/secure/attachment/12387149/remove_cast_to_bytearray_reuse_parser.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2008-07-24 14:38:26.33,,,no_permission,,,,,,,,,,,,38591,,,,,Thu Jul 31 17:59:49 UTC 2008,,,Patch Available,,,,0|i0gikv:,94440,,,,,,,,,,"11/Jul/08 17:31;sms;The conversion from any pig type to byte array is broken. 

The cast functionality is used in the following scenarios:

1. Cast bytes to appropriate pig types during load
2. Cast one pig type to another during execution
3. Cast pig types to appropriate storage representation during a store

Out of these three scenarios, POCast plays a role in the first two. The third scenario influences the behavior of POCast.

Currently, POCast uses the load function to convert bytes to the appropriate pig type (scenario 1). During the pipeline execution, after the load, users can use casts as they deem fit. This covers scenarios like converting a pig type (other than byte array) to byte array followed by a conversion of the byte array to the same or a different pig type (Scenario 2). Consider the hypothetical use of the cast below.

{code}

a = load 'myfile' as (t: tuple(i: int, f: float));

b = foreach a generate (bytearray) $0;

c = foreach b generate (tuple(int, int)) $0;
{code}

The tuple is first cast to a byte array and then cast back to a tuple. In order to facilitate these types of casts, the byte array representation should retain information about the original type it was cast from. This information is conceptually encapsulated in the load function, which supports the ability to convert bytes to pig types. The inverse mechanism of converting pig types to bytes will nicely fit in the context of the load function. This will enable pig to use the conversion and inversion hooks in the load function to convert bytes to pig types and vice versa in the context of the pipeline execution (Scenario 2).

The obvious benefit of this approach: Store functions which understand the byte representation of the data can now convert the bytes back in  the format of choice (Scenario 3).

Summary:

1. Load function interface supports  toBytes for each pig type in addition to bytesToInteger, bytesToLong, etc.
2. POCast uses the load function to convert bytes to pig types and vice versa
3. PigStorage will be extended to support complex types (tuples, bags, maps) and provide inverse functions, i.e., convert pig types to bytes representation","16/Jul/08 21:17;sms;This issue will be fixed a bit later, i.e., lowering the priority.","21/Jul/08 21:57;sms;The patch (pig_type_to_bytearray.patch) includes the following:

1. The load function interface is extended to include toBytes, a method that will convert Pig Types to the appropriate byte representation that can be used with the bytesToPigType routine

2. The methods are implemented in all the classes that implement the load function interface, i.e., PigStorage, BinStorage, TextLoader, private classes used in the unit test cases, etc.

3. A text data parser that converts text data to Pig Types

4. Unit test cases for the conversion routines and the casts.

5. Helper methods in DataType.java for checking equal byte arrays and converting Pig Map to strings.


TODO:

The byte arrays are parsed as strings as the parser cannot distinguish between the two.

Unit test cases that still fail are:

    [junit] Running org.apache.pig.test.TestEvalPipeline
    [junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 179.904 sec
    [junit] Test org.apache.pig.test.TestEvalPipeline FAILED

    [junit] Running org.apache.pig.test.TestFilterOpNumeric
    [junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 56.124 sec
    [junit] Test org.apache.pig.test.TestFilterOpNumeric FAILED

    [junit] Running org.apache.pig.test.TestBuiltin
    [junit] Tests run: 23, Failures: 1, Errors: 1, Time elapsed: 14.8 sec
    [junit] Test org.apache.pig.test.TestBuiltin FAILED

    [junit] Running org.apache.pig.test.TestStoreOld
    [junit] Tests run: 3, Failures: 0, Errors: 2, Time elapsed: 21.453 sec
    [junit] Test org.apache.pig.test.TestStoreOld FAILED
",21/Jul/08 22:21;sms;Ignore my previous patch. I will be uploading a new one shortly. One of the new files was not added to the patch.,21/Jul/08 22:41;sms;Replacing the old patch with the new one. The new patch includes the TextDataParser.jjt file.,"24/Jul/08 14:38;pi_song;Is there really a scenario where users might want to cast a known type to bytearray and cast it back? I think users should be more happy to always work with the known type.

According to the Jira title, I guess if a user, for example, wants UTF-8 representation of Chararray, then he should use a UDF which returns ByteArray, not casting.

My concern is that we shouldn't make LoadFunc more complex just to support an operation which doesn't actually have a real use.","24/Jul/08 14:40;pi_song;BTW, Sorry for late reply.","24/Jul/08 21:52;olgan;I reviewed the patch.

My feedback is that I don't think we need to support cast to bytearray. I can think of any reasonable use cases for it and makes a more common case of writing custom load function more complex.","27/Jul/08 05:15;sms;Casts to bytearray are no longer allowed. The patch includes the following:

1. Parser throws an exception when explicit casts to bytearray are seen

2. Type checker throws an exception when implicit casts to bytearray are used

3. The load interface does not have toBytes(PigType) methods

4. The toBytes method in Utf8StorageConverter.java, BinStorage.java and TextLoader.java are retained for future use

5. A text data parser that converts text data to Pig Types

6. Unit test cases for the conversion routines and the casts.

7. Helper methods in DataType.java for checking equal byte arrays and converting Pig Map to strings.

TODO:

The byte arrays are parsed as strings as the parser cannot distinguish between the two.

Unit tests that still fail are:

    [junit] Running org.apache.pig.test.TestBuiltin
    [junit] Tests run: 23, Failures: 1, Errors: 1, Time elapsed: 14.986 sec
    [junit] Test org.apache.pig.test.TestBuiltin FAILED

    [junit] Running org.apache.pig.test.TestEvalPipeline
    [junit] Tests run: 9, Failures: 0, Errors: 1, Time elapsed: 159.046 sec
    [junit] Test org.apache.pig.test.TestEvalPipeline FAILED

    [junit] Running org.apache.pig.test.TestFilterOpNumeric
    [junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 56.258 sec
    [junit] Test org.apache.pig.test.TestFilterOpNumeric FAILED

    [junit] Running org.apache.pig.test.TestStoreOld
    [junit] Tests run: 3, Failures: 0, Errors: 2, Time elapsed: 41.005 sec
    [junit] Test org.apache.pig.test.TestStoreOld FAILED","28/Jul/08 21:16;alangates;One question on parsing complex types in PigStorage.  In the function parseFromBytes, which is called on every complex field that is parsed, you construct a new TextDataParser to parse that field.  Is that necessary?  Can the same one not be used repeatedly?","29/Jul/08 23:30;sms;The text data parser is reused in the parseFromBytes() method in Utf8StorageConverter.

Unit tests that still fail are:

    [junit] Running org.apache.pig.test.TestBuiltin
    [junit] Tests run: 23, Failures: 1, Errors: 1, Time elapsed: 14.732 sec
    [junit] Test org.apache.pig.test.TestBuiltin FAILED

    [junit] Running org.apache.pig.test.TestFilterOpNumeric
    [junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 56.902 sec
    [junit] Test org.apache.pig.test.TestFilterOpNumeric FAILED

    [junit] Running org.apache.pig.test.TestStoreOld
    [junit] Tests run: 3, Failures: 0, Errors: 2, Time elapsed: 41.091 sec
    [junit] Test org.apache.pig.test.TestStoreOld FAILED
",31/Jul/08 17:59;alangates;remove_cast_to_bytearray_reuse_parser.patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cross products of two flattens is incorrectly including records with null values,PIG-302,12400051,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,10/Jul/08 19:20,24/Mar/10 22:04,14/Mar/19 03:05,12/Jul/08 19:29,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"Given two data sets:

studenttab10
alex garcia,39,3.81
bob jones,40,2.77
zach johnson,23,4.00
tony mendleson,87,2.10
todd wellington,55,3.32
melany smith,19,3.98
jane wesley,62,1.98
irene chan,34,3.14
laverne shirley,58,2.43
marcia tently,32,3.48

and

alex garcia,39,republican,1.50
bob jones,40,democrat,1000.30
zach johnson,23,independent,0.00
tony mendleson,87,socialist,101012.92
todd wellington,55,green,99.89
melany smith,29,republican,88787.29
john wesley,62,democrat,0.89
bob smith,18,independent,0.99
johnny appleseed,234,green,99.95
barak obama,47,democrat,3.48

and the script:

a = load '/Users/gates/test/data/studenttab10' using PigStorage(',') as (name, age, gpa);
b = load '/Users/gates/test/data/votertab10' using PigStorage(',') as (name, age, registration, contributions);
c = filter a by age < 40;
d = filter b by age < 40;
e = cogroup c by name, d by name;
f = foreach e generate flatten (c), flatten(d);
dump f;

The result is:

(NULL, bob smith, 18, independent, 0.99)
(alex garcia, 39, 3.81, alex garcia, 39, republican, 1.50)
(melany smith, 19, 3.98, melany smith, 29, republican, 88787.29)
(zach johnson, 23, 4.00, zach johnson, 23, independent, 0.00)

The first record should not be there.  Flatten is supposed to remove records without a match.",,,,,,,,,,,,,,,,,,,10/Jul/08 21:21;alangates;cogroup.patch;https://issues.apache.org/jira/secure/attachment/12385807/cogroup.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,163943,,,,,Sat Jul 12 19:29:57 UTC 2008,,,,,,,0|i0gikf:,94438,,,,,,,,,,"10/Jul/08 21:20;alangates;Flatten logic was not properly resetting a couple of variables, which resulted in extraneous rows being included in the results.",12/Jul/08 19:29;alangates;cogroup patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ORDER BY DESC doesn't work at all,PIG-301,12400026,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,alangates,pi_song,pi_song,10/Jul/08 14:53,24/Mar/10 22:04,14/Mar/19 03:05,14/Aug/08 23:21,0.2.0,,,,,,0.2.0,,,,,0,,,,I have committed TestOrderBy2.java. Please run and look at the log file.,,,,,,,,,,,,,,,,,,,14/Aug/08 19:33;alangates;orderdesc.patch;https://issues.apache.org/jira/secure/attachment/12388272/orderdesc.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-08-14 19:33:03.341,,,no_permission,,,,,,,,,,,,163942,,,,,Thu Aug 14 23:21:28 UTC 2008,,,,,,,0|i0gijz:,94436,,,,,,,,,,"24/Jul/08 12:41;pi_song;The current implementation also doesn't seem to handle different order directions.

{noformat}
B = ORDER A BY $0 DESC, $1 ASC, $2 DESC, $3 ASC
{noformat}

I think implementation-wise, we should somehow implement PigWriteableComparator  using composite design pattern.","14/Aug/08 19:33;alangates;This patch provides raw comparators for pig types so that we can handle order by descending.  It uses hadoop's RawComparator interface, because these raw comparators can also implement Configurable.  The ascending/descending array is passed in the JobConf and then deserialized in the setConf call.

Much of the work of this patch was actually done by Santhosh.","14/Aug/08 23:21;olgan;patch committed. thanks, alan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filter operator not included in the main predecessor plan structure,PIG-299,12399971,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,sms,tcondie,tcondie,09/Jul/08 23:33,24/Mar/10 22:04,14/Mar/19 03:05,16/Jul/08 13:58,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"Take the following query, which can be found in TestLogicalPlanBuilder.java method testQuery80();
a = load 'input1' as (name, age, gpa);
b = filter a by age < '20';"");
c = group b by (name,age);
d = foreach c {
            cf = filter b by gpa < '3.0';
            cp = cf.gpa;
            cd = distinct cp;
            co = order cd by gpa;
            generate group, flatten(co);
            };

The filter statement 'cf = filter b by gpa < '3.0'' is not accessible via the LogicalPlan::getPredecessor method. Here is the explan plan print out of the inner foreach plan:

|---SORT Test-Plan-Builder-17 Schema: {gpa: bytearray} Type: bag
    |   |
    |   Project Test-Plan-Builder-16 Projections: [0] Overloaded: false FieldSchema: gpa: bytearray cn: 2 Type: bytearray
    |   Input: Distinct Test-Plan-Builder-1
    |
    |---Distinct Test-Plan-Builder-15 Schema: {gpa: bytearray} Type: bag
        |
        |---Project Test-Plan-Builder-14 Projections: [2] Overloaded: false FieldSchema: gpa: bytearray cn: 2 Type: bytearray
            Input: Project Test-Plan-Builder-13 Projections:  [*]  Overloaded: false|
            |---Project Test-Plan-Builder-13 Projections:  [*]  Overloaded: false FieldSchema: cf: tuple({name: bytearray,age: bytearray,gpa: bytearray}) Type: tuple
                Input: Filter Test-Plan-Builder-12OPERATOR PROJECT SCHEMA {name: bytearray,age: bytearray,gpa: bytearray}

As you can see the filter is only accessible via the LOProject::getExpression() method. It is not showing up as an input operator. Focus on the projection immediately following the filter. If I remove this projection then I get a correct plan. For example, let the inner foreach plan be as follows:

d = foreach c {
            cf = filter b by gpa < '3.0';
            cd = distinct cf;
            co = order cd by gpa;
            generate group, flatten(co);
            };

Then I get the following (correct) explan plan output.


|---SORT Test-Plan-Builder-15 Schema: {name: bytearray,age: bytearray,gpa: bytearray} Type: bag
    |   |
    |   Project Test-Plan-Builder-14 Projections: [2] Overloaded: false FieldSchema: gpa: bytearray cn: 2 Type: bytearray
    |   Input: Distinct Test-Plan-Builder-1
    |
    |---Distinct Test-Plan-Builder-13 Schema: {name: bytearray,age: bytearray,gpa: bytearray} Type: bag
        |
        |---Filter Test-Plan-Builder-12 Schema: {name: bytearray,age: bytearray,gpa: bytearray} Type: bag
            |   |
            |   LesserThan Test-Plan-Builder-11 FieldSchema: null Type: Unknown
            |   |
            |   |---Project Test-Plan-Builder-9 Projections: [2] Overloaded: false FieldSchema:  Type: Unknown
            |   |   Input: CoGroup Test-Plan-Builder-7
            |   |
            |   |---Const Test-Plan-Builder-10 FieldSchema: chararray Type: chararray
            |
            |---Project Test-Plan-Builder-8 Projections: [1] Overloaded: false FieldSchema: b: bag({name: bytearray,age: bytearray,gpa: bytearray}) Type: bag
                Input: CoGroup Test-Plan-Builder-7OPERATOR PROJECT SCHEMA {name: bytearray,age: bytearray,gpa: bytearray}


Alan said that the problem is we don't generate a foreach operator for the 'cp = cf.gpa' statement. Please let me know if this can be resolved.

Thanks,
Tyson

",N/A,,,,,,,,,,,,,,,,,,15/Jul/08 23:02;sms;nested_project_as_foreach.patch;https://issues.apache.org/jira/secure/attachment/12386108/nested_project_as_foreach.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-15 17:18:33.913,,,no_permission,,,,,,,,,,,,163940,,,,,Wed Jul 16 13:58:05 UTC 2008,,,Patch Available,,,,0|i0gij3:,94432,,,,,,,,,,"15/Jul/08 17:18;sms;Just adding the noformat tags around the plans to make them readable.


{noformat}

|---SORT Test-Plan-Builder-17 Schema: {gpa: bytearray} Type: bag
    |   |
    |   Project Test-Plan-Builder-16 Projections: [0] Overloaded: false FieldSchema: gpa: bytearray cn: 2 Type: bytearray
    |   Input: Distinct Test-Plan-Builder-1
    |
    |---Distinct Test-Plan-Builder-15 Schema: {gpa: bytearray} Type: bag
        |
        |---Project Test-Plan-Builder-14 Projections: [2] Overloaded: false FieldSchema: gpa: bytearray cn: 2 Type: bytearray
            Input: Project Test-Plan-Builder-13 Projections:  [*]  Overloaded: false|
            |---Project Test-Plan-Builder-13 Projections:  [*]  Overloaded: false FieldSchema: cf: tuple({name: bytearray,age: bytearray,gpa: bytearray}) Type: tuple
                Input: Filter Test-Plan-Builder-12OPERATOR PROJECT SCHEMA {name: bytearray,age: bytearray,gpa: bytearray}

{noformat}

{noformat}

|---SORT Test-Plan-Builder-15 Schema: {name: bytearray,age: bytearray,gpa: bytearray} Type: bag
    |   |
    |   Project Test-Plan-Builder-14 Projections: [2] Overloaded: false FieldSchema: gpa: bytearray cn: 2 Type: bytearray
    |   Input: Distinct Test-Plan-Builder-1
    |
    |---Distinct Test-Plan-Builder-13 Schema: {name: bytearray,age: bytearray,gpa: bytearray} Type: bag
        |
        |---Filter Test-Plan-Builder-12 Schema: {name: bytearray,age: bytearray,gpa: bytearray} Type: bag
            |   |
            |   LesserThan Test-Plan-Builder-11 FieldSchema: null Type: Unknown
            |   |
            |   |---Project Test-Plan-Builder-9 Projections: [2] Overloaded: false FieldSchema:  Type: Unknown
            |   |   Input: CoGroup Test-Plan-Builder-7
            |   |
            |   |---Const Test-Plan-Builder-10 FieldSchema: chararray Type: chararray
            |
            |---Project Test-Plan-Builder-8 Projections: [1] Overloaded: false FieldSchema: b: bag({name: bytearray,age: bytearray,gpa: bytearray}) Type: bag
                Input: CoGroup Test-Plan-Builder-7OPERATOR PROJECT SCHEMA {name: bytearray,age: bytearray,gpa: bytearray}

{noformat}","15/Jul/08 23:02;sms;The nested_project_as_foreach.patch contains the following:

1. The project statements like A = $1.$0;  B = A.($1, $2); C = A.$1; etc. are rewritten as for each statements with nested plans that project the columns.
2. Unit test cases for testing the rewrite.

Unit test cases that still fail are:

    [junit] Running org.apache.pig.test.TestEvalPipeline
    [junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 142.518 sec
    [junit] Test org.apache.pig.test.TestEvalPipeline FAILED

    [junit] Running org.apache.pig.test.TestFilterOpNumeric
    [junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 246.872 sec
    [junit] Test org.apache.pig.test.TestFilterOpNumeric FAILED

    [junit] Running org.apache.pig.test.TestStoreOld
    [junit] Tests run: 3, Failures: 0, Errors: 2, Time elapsed: 21.584 sec
    [junit] Test org.apache.pig.test.TestStoreOld FAILED
","15/Jul/08 23:03;sms;The Logical Plan for the first query in the initial bug report will now look like:

{noformat}
ForEach Test-Plan-Builder-784 Schema: {group: (name: (null),age: (null),bytearray),co::gpa: bytearray} Type: bag
|   |
|   Project Test-Plan-Builder-781 Projections: [0] Overloaded: false FieldSchema: group: tuple({name: (null),age: (null),bytearray}) Type: tuple
|   Input: CoGroup Test-Plan-Builder-77
|   |
|   Project Test-Plan-Builder-782 Projections:  [*]  Overloaded: false FieldSchema: co: tuple({gpa: bytearray}) Type: tuple
|   Input: SORT Test-Plan-Builder-780|
|   |---SORT Test-Plan-Builder-780 Schema: {gpa: bytearray} Type: bag
|       |   |
|       |   Project Test-Plan-Builder-779 Projections: [0] Overloaded: false FieldSchema: gpa: bytearray cn: 122 Type: bytearray
|       |   Input: Distinct Test-Plan-Builder-77
|       |
|       |---Distinct Test-Plan-Builder-778 Schema: {gpa: bytearray} Type: bag
|           |
|           |---ForEach Test-Plan-Builder-777 Schema: {gpa: bytearray} Type: bag
|               |   |
|               |   Project Test-Plan-Builder-776 Projections: [2] Overloaded: false FieldSchema: gpa: bytearray cn: 122 Type: bytearray
|               |   Input: Filter Test-Plan-Builder-77
|               |
|               |---Filter Test-Plan-Builder-775 Schema: {name: bytearray,age: bytearray,gpa: bytearray} Type: bag
|                   |   |
|                   |   LesserThan Test-Plan-Builder-774 FieldSchema: null Type: Unknown
|                   |   |
|                   |   |---Project Test-Plan-Builder-772 Projections: [2] Overloaded: false FieldSchema:  Type: Unknown
|                   |   |   Input: CoGroup Test-Plan-Builder-770
|                   |   |
|                   |   |---Const Test-Plan-Builder-773 FieldSchema: chararray Type: chararray
|                   |
|                   |---Project Test-Plan-Builder-771 Projections: [1] Overloaded: false FieldSchema: b: bag({name: bytearray,age: bytearray,gpa: bytearray}) Type: bag
|                       Input: CoGroup Test-Plan-Builder-770
|
|---CoGroup Test-Plan-Builder-770 Schema: {group: (name: (null),age: (null),bytearray),b: {name: bytearray,age: bytearray,gpa: bytearray}} Type: Unknown
    |   |
    |   Project Test-Plan-Builder-768 Projections: [0] Overloaded: false FieldSchema: name: bytearray cn: 120 Type: bytearray
    |   Input: Filter Test-Plan-Builder-76
    |   |
    |   Project Test-Plan-Builder-769 Projections: [1] Overloaded: false FieldSchema: age: bytearray cn: 121 Type: bytearray
    |   Input: Filter Test-Plan-Builder-76
    |
    |---Filter Test-Plan-Builder-767 Schema: {name: bytearray,age: bytearray,gpa: bytearray} Type: bag
        |   |
        |   LesserThan Test-Plan-Builder-766 FieldSchema: null Type: Unknown
        |   |
        |   |---Project Test-Plan-Builder-764 Projections: [1] Overloaded: false FieldSchema: age: bytearray cn: 121 Type: bytearray
        |   |   Input: Load Test-Plan-Builder-763
        |   |
        |   |---Const Test-Plan-Builder-765 FieldSchema: chararray Type: chararray
        |
        |---Load Test-Plan-Builder-763 Schema: {name: bytearray,age: bytearray,gpa: bytearray} Type: bag
{noformat}","16/Jul/08 12:40;pi_song;That would be what I suggest to do as well.
Running tests now.","16/Jul/08 13:58;pi_song;Committed.
Thanks Santhosh!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong sort logic in POSort,PIG-298,12399909,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pi_song,pi_song,pi_song,09/Jul/08 14:23,25/Mar/10 00:12,14/Mar/19 03:05,10/Jul/08 13:03,0.2.0,,,,,,0.1.0,,,,,0,,,,"This might relate to PIG-292.

The current logic is obviously wrong as it only returns the comparison return of the last comparison only!!.

Patch + tests attached.",,,,,,,,,,,,,,,,,,,09/Jul/08 14:24;pi_song;Wrong_Sort_logic_in_POSort.patch;https://issues.apache.org/jira/secure/attachment/12385629/Wrong_Sort_logic_in_POSort.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-09 19:06:42.739,,,no_permission,,,,,,,,,,,,163939,,,,,Thu Jul 10 13:03:39 UTC 2008,,,Patch Available,,,,0|i0giin:,94430,,,,,,,,,,09/Jul/08 14:24;pi_song;patch,09/Jul/08 19:06;alangates;+1,10/Jul/08 13:03;pi_song;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The testSortDistinct function in TestEvalPipeline is buggy,PIG-295,12399753,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,shravanmn,shravanmn,shravanmn,07/Jul/08 18:23,24/Mar/10 22:04,14/Mar/19 03:05,11/Jul/08 12:17,0.0.0,0.2.0,,,,,0.2.0,,,,,0,,,,"The problem is that last is supposed to be holding the last read value from the tuple but clearly it does not right now as can be seen below:
              int last = -1;
		while (iter.hasNext()){
			Tuple t = iter.next();
			if (eliminateDuplicates){
				assertTrue(last < t.getAtomField(0).numval().intValue());
			}else{
				assertTrue(last <= t.getAtomField(0).numval().intValue());
				assertEquals(t.arity(), 2);
			}
		}

last is always -1 & all tests pass. This should be correct aptly for the non-types branch if we want to. But the fix I am attaching is just for the types branch. Here since we do not have types, there will be bytearray comparison leading to lexicographic sorting. So changed the tests to test that rather than numeric sorting.",,,,,,,,,,,,,,,,,,,07/Jul/08 18:25;shravanmn;testSort.patch;https://issues.apache.org/jira/secure/attachment/12385428/testSort.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-11 12:17:00.447,,,no_permission,,,,,,,,,,,,37818,,,,,Fri Jul 11 12:17:00 UTC 2008,,,Patch Available,,,,0|i0gih3:,94423,,,,,,,,,,"07/Jul/08 18:25;shravanmn;The fix I am attaching is just for the types branch. Here since the test is not typed, there will be bytearray comparison leading to lexicographic sorting. So changed the tests to test that rather than numeric sorting.","11/Jul/08 12:17;pi_song;Committed.
Thanks Shravan for your good eyes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parse errors for boolean conditions,PIG-294,12399744,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,sms,sms,07/Jul/08 17:05,25/Mar/10 00:12,14/Mar/19 03:05,08/Jul/08 11:19,0.2.0,,,,,,0.1.0,,impl,,,0,,,,"The parser throws exceptions for pig statements that contain boolean conditions with operands that use string comparators. A sample statement to reproduce the test is given below:

split a into b if name lt 'f', c if (name ge 'f' and name le 'h'), d if name gt 'h';",,,,,,,,,,,,,,,,,,,07/Jul/08 21:06;sms;boolean_test.patch;https://issues.apache.org/jira/secure/attachment/12385440/boolean_test.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-08 11:19:50.388,,,no_permission,,,,,,,,,,,,163936,,,,,Tue Jul 08 11:19:50 UTC 2008,,,Patch Available,,,,0|i0gign:,94421,,,,,,,,,,"07/Jul/08 17:17;sms;Replacing the string comparators with numerical comparators resolves the issue.

split a into b if name lt 'f', c if (name ge 'f' and name le 'h'), d if name gt 'h'; --parse errors
split a into b if name lt 'f', c if (name >= 'f' and name <= 'h'), d if name gt 'h'; -- works","07/Jul/08 21:06;sms;The parser works as expected. The test case was incorrect. Instead of using 'gte' for greater than or equal to, the script was using 'ge' ala perl. I have added a positive and  negative test case to check if the parser works.

Patch: boolean_test.patch

The unit tests that still fail are:

    [junit] Running org.apache.pig.test.TestEvalPipeline
    [junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 201.12 sec
    [junit] Test org.apache.pig.test.TestEvalPipeline FAILED

    [junit] Running org.apache.pig.test.TestFilterOpNumeric
    [junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 55.618 sec
    [junit] Test org.apache.pig.test.TestFilterOpNumeric FAILED

    [junit] Running org.apache.pig.test.TestStoreOld
    [junit] Tests run: 3, Failures: 0, Errors: 2, Time elapsed: 39.291 sec
    [junit] Test org.apache.pig.test.TestStoreOld FAILED
",08/Jul/08 11:19;pi_song;Unit tests checked in. Thanks Santhosh.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
order by * goes into infinite loop,PIG-293,12399740,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,alangates,alangates,07/Jul/08 16:45,24/Mar/10 22:04,14/Mar/19 03:05,10/Jul/08 14:50,0.2.0,,,,,,0.2.0,,,,,0,,,,"Scripts with order by * go into an infinite loop.  Worse yet, they appear to be reporting progress to hadoop in this loop, and thus are never terminated.",,,,,,,,,,,,,,,,,,,09/Jul/08 21:37;sms;sort_star_with_project.patch;https://issues.apache.org/jira/secure/attachment/12385678/sort_star_with_project.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-09 21:37:07.239,,,no_permission,,,,,,,,,,,,163935,,,,,Fri Jul 11 17:03:41 UTC 2008,,,,,,,0|i0gig7:,94419,,,,,,,,,,09/Jul/08 21:37;sms;The sort_star_with_project.patch inserts a project( * ) in the sort inner plan for order by * queries.,"10/Jul/08 13:38;pi_song;From my understanding, if LOSort.isStar() is true, it already indicates that we need all columns. 

My opinion:-
- If we keep setStar(), then we shouldn't have to add inner plans 
- If we add inner plans at parsing time, we shouldn't have setStar()

What do you think?","10/Jul/08 14:48;pi_song;BTW, it works now. Apparently much better than doesn't work.

I've added some more unit tests and committed.","11/Jul/08 17:03;sms;The issue is resolved. However, wrt Pi's comment about not having setStar(), I agree that having setStar and inner plans for sort is redundant. With the patch that was committed, project( * ) is the correct way of representing order by *. The member variable mIsStar and the associated methods setStar and isStar are no longer required in LOSort.java ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Order by on more than one field returns wrong order,PIG-292,12399739,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,07/Jul/08 16:44,24/Mar/10 22:04,14/Mar/19 03:05,09/Jul/08 22:21,0.2.0,,,,,,0.2.0,,,,,0,,,,Pig scripts with order by of a single column work fine.  But once a second column is added the data is returned in the wrong order.,,,,,,,,,,,,,,,,,,,08/Jul/08 16:20;pi_song;PIG-292_test.patch;https://issues.apache.org/jira/secure/attachment/12385516/PIG-292_test.patch,09/Jul/08 21:19;alangates;manysortfixes.patch;https://issues.apache.org/jira/secure/attachment/12385676/manysortfixes.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-07-08 16:20:01.671,,,no_permission,,,,,,,,,,,,163934,,,,,Wed Jul 09 22:21:47 UTC 2008,,,,,,,0|i0gifr:,94417,,,,,,,,,,"08/Jul/08 16:20;pi_song;It seems to be OK except when you use DESC keyword.

Unit tests attached.","09/Jul/08 21:19;alangates;This patch contains a lot. The relevant areas are:

1) Fixed DataByteArray.compareTo to first check byte to byte and only size when the bytes compare equal, instead of other way around (that is, it now matches String.compareTo behavior).

2) Fixed use of binary comparator for hadoop sorting. It was being used anytime we were sorting on a tuple. It should instead be used anytime we are not doing an order by.",09/Jul/08 22:21;alangates;manysort patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hod.param parameters not passed properly,PIG-291,12399554,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thatha,thatha,thatha,03/Jul/08 17:53,25/Mar/10 00:12,14/Mar/19 03:05,08/Jul/08 23:15,,,,,,,0.1.0,,impl,,,0,,,,"pig -Dhod.param='-N hodclustername' script.pig

fails with the following error:

2008-07-03 17:53:18,236 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to HOD...
org.apache.pig.backend.executionengine.ExecException: Could not connect to HOD
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.doHod(HExecutionEngine.java:428)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.init(HExecutionEngine.java:121)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.init(HExecutionEngine.java:108)
        at org.apache.pig.impl.PigContext.connect(PigContext.java:177)
        at org.apache.pig.PigServer.<init>(PigServer.java:149)
        at org.apache.pig.tools.grunt.Grunt.<init>(Grunt.java:43)
        at org.apache.pig.Main.main(Main.java:293)
Caused by: org.apache.pig.backend.executionengine.ExecException: org.apache.pig.backend.executionengine.ExecException: Failed to run command hod allocate -d /tmp/PigHod.hostname.thatha.304309240344558 -n 15 -N hodclustername   on server local; return code: 4; error: CRITICAL - qsub Failure : qsub: illegal -N value 
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.runCommand(HExecutionEngine.java:541)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.doHod(HExecutionEngine.java:373)
        ... 6 more
Caused by: org.apache.pig.backend.executionengine.ExecException: Failed to run command hod allocate -d /tmp/PigHod.hostname.thatha.304309240344558 -n 15 -N hodclustername   on server local; return code: 4; error: CRITICAL - qsub Failure : qsub: illegal -N value 
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.runCommand(HExecutionEngine.java:538)
        ... 7 more

It appears that the problem is in the parsing of hod.param, located in org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java, in doHod(...).","Linux hostname 2.6.9-55.ELsmp #1 SMP Fri Apr 20 16:36:54 EDT 2007 x86_64 x86_64 x86_64 GNU/Linux
Apache Pig version 0.1.0-dev (r8087) 
Hadoop 0.17.1 Subversion http://svn.apache.org/repos/asf/hadoop/core/branches/branch-0.17 -r 669344
hod --version: 0.17.1",7200,7200,,0%,7200,7200,,,,,,,,,,,,03/Jul/08 23:23;thatha;pig-291.patch;https://issues.apache.org/jira/secure/attachment/12385243/pig-291.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-08 23:15:39.4,,,no_permission,,,,,,,,,,,,163933,,,,,Tue Jul 08 23:15:39 UTC 2008,,,,,,,0|i0gifb:,94415,,,,,,,,,,08/Jul/08 23:15;olgan;I have tested and committed the patch. Thanks Ian for contributing!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LOCross output schema is not right,PIG-290,12399536,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,pi_song,pi_song,03/Jul/08 14:11,24/Mar/10 22:04,14/Mar/19 03:05,21/Jul/08 11:32,0.2.0,,,,,,0.2.0,,impl,,,0,,,,"From the schema generation code:-

{noformat}
        List<LogicalOperator> inputs = mPlan.getPredecessors(this);

            for (LogicalOperator op : inputs) {
                    // Create schema here
            }
{noformat}

The output schema is generated based on inputs determined in the logical plan. However,  mPlan.getPredecessors() doesn't always preserve the right order  (A x B and B x A result in different schemas). I suggest maintaining mInputs variable in LOCross (as it used to be) to resolve this issue.",,,,,,,,,,,,,,,,,,,09/Jul/08 00:21;sms;insert_between.patch;https://issues.apache.org/jira/secure/attachment/12385553/insert_between.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-09 00:21:37.351,,,no_permission,,,,,,,,,,,,163932,,,,,Mon Jul 21 11:32:05 UTC 2008,,,Patch Available,,,,0|i0giev:,94413,,,,,,,,,,"09/Jul/08 00:21;sms;The insert_between.patch fixes the following:

1. Changes the implementation of insertBetween in OperatorPlan.java so that the ordering of the predecessors is not changed

2. Changed log.info in LOForeach getSchema() to log.debug. One of my earlier patches had the log.info

3. Removed a printPlan use in TestLogicalPlanBuilder.java

The unit tests that still fail are:

    [junit] Running org.apache.pig.test.TestEvalPipeline
    [junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 141.926 sec
    [junit] Test org.apache.pig.test.TestEvalPipeline FAILED

    [junit] Running org.apache.pig.test.TestFilterOpNumeric
    [junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 56.446 sec
    [junit] Test org.apache.pig.test.TestFilterOpNumeric FAILED

    [junit] Running org.apache.pig.test.TestStoreOld
    [junit] Tests run: 3, Failures: 0, Errors: 2, Time elapsed: 39.782 sec
    [junit] Test org.apache.pig.test.TestStoreOld FAILED
","09/Jul/08 14:27;pi_song;The patch doesn't seem to relate to the bug.
Are you posting in the right Jira?","09/Jul/08 17:56;sms;I should have explained better on my previous comment. The ordering of the nodes in the getPredecessor and getSuccessor calls are preserved as long as the nodes are not moved around using the disconnect, connect and insertBetween calls. I have fixed the insertBetween calls to preserve the right order.

If the original order was A->C and A->D if a new node B was inserted between A and C, then previously you would end up with A->D and then A-B->C. Now you will have A->B->C and A->D, thus preserving the order for the getPredecessor and getSuccessor calls.","15/Jul/08 17:03;sms;Can we commit this patch, if the review and the explanation are fine?

Thanks,
Santhosh","16/Jul/08 11:30;pi_song;Sorry, I had a look at this a few days ago but forgot to reply.
I think instead of relying on  edge ordering in the graph (which is, in fact, not defined), we'd better maintain the order inside LO.

What is your opinion on this?","16/Jul/08 21:35;sms;The edge ordering in the implementation is the order in which the connections were made. This is a side effect of using a list to store the edges. While this does not appear to be a clean solution, the scope of changing the definition of the logical operators LOCross and LOCogroup is widespread. The changes will affect the optimizer which has to re-wire the inputs when operators are moved around in the graph.

For now, I would go with the proposed change. We can fix the issue as an enhancement when we merge from types branch to trunk.","17/Jul/08 14:33;pi_song;I wouldn't rely on a side effect instead of the correct model.

The rewiring problem seems to be universal. One solution is we can have a set of utility methods for inserting/cutting nodes in logical plans that always guarantees correct wiring.

Now our votes are 1 to 1. I'll ask Alan to give the third vote.","17/Jul/08 22:13;alangates;I don't think it's quite fair to characterize this is as relying a on side effect.  Santhosh's changes guarantee that side effect and then rely on it.  I agree that the resulting API is a little confusing (using disconnect/connect will have a different affect that using replaceNode).  That needs to be very clearly documented.  But I'm sure it would be any better if operators had inputs and anyone rearranging the tree had to remember to reconnect the inputs.  In that case the programmer would have to remember to reset the inputs, which is just as hard as remembering to use the correct API function.",18/Jul/08 14:38;pi_song;will commit this once I figure out why a lot more tests are failing today.,"21/Jul/08 11:32;pi_song;Checked in.
Thanks Santhosh!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataByteArray is too long,PIG-289,12399486,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,sms,sms,02/Jul/08 21:51,25/Mar/10 00:12,14/Mar/19 03:05,03/Jul/08 23:02,0.2.0,,,,,,0.1.0,,impl,,,0,,,,"A new test case (testNestedPlan) added to TestEvalPipeline has the following query:

        pig.registerQuery(""A = LOAD 'file:"" + tmpFile + ""';"");
        pig.registerQuery(""B = group A by $0;"");
        + ""C1 = filter A by $0 > -1;""
        + ""C2 = distinct C1;""
        + ""C3 = distinct A;""
        + ""generate group;""
        + ""};"";

The projected item group is of type byte array. The length of this byte array is 100 which appears to be to too big. Also, when the tuple containing group is printed out, you can see junk characters ^@

(0^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@)
DataByteArray length: 100
(1^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@)
DataByteArray length: 100
(2^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@)
DataByteArray length: 100
(3^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@)
DataByteArray length: 100
(4^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@)
DataByteArray length: 100
(5^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@)
DataByteArray length: 100
(6^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@)
DataByteArray length: 100
(7^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@)
DataByteArray length: 100
(8^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@)
DataByteArray length: 100
(9^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@)
DataByteArray length: 100
",,,,,,,,,,,,,,,,,,,03/Jul/08 23:00;alangates;PIG-289.patch;https://issues.apache.org/jira/secure/attachment/12385240/PIG-289.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-03 23:00:55.341,,,no_permission,,,,,,,,,,,,163931,,,,,Thu Jul 03 23:02:00 UTC 2008,,,,,,,0|i0gief:,94411,,,,,,,,,,03/Jul/08 23:00;alangates;Changed translation of BytesWritable to DataByteArray to use BytesWritable.size() instead of BytesWritable.get().length as the bytes to read for the DataByteArray.,03/Jul/08 23:02;alangates;Fix checked in at revision 673858.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null pointer exception with load as schema - Optimizer,PIG-288,12399482,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pi_song,sms,sms,02/Jul/08 21:19,25/Mar/10 00:12,14/Mar/19 03:05,08/Jul/08 11:58,0.2.0,,,,,,0.1.0,,impl,,,0,,,,"A new test case (testNestedPlan) added to TestEvalPipeline has the following query:

        pig.registerQuery(""A = LOAD 'file:"" + tmpFile + ""'as (a:int, b:int);"");
        pig.registerQuery(""B = group A by $0;"");
        + ""C1 = filter A by $0 > -1;""
        + ""C2 = distinct C1;""
        + ""C3 = distinct A;""
        + ""generate (int)group;""
        + ""};"";

Testcase: testNestedPlan took 0.913 sec
    Caused an ERROR
Unable to open iterator for alias: C
java.io.IOException: Unable to open iterator for alias: C
    at org.apache.pig.impl.util.WrappedIOException.wrap(WrappedIOException.java:34)
    at org.apache.pig.PigServer.openIterator(PigServer.java:268)
    at org.apache.pig.test.TestEvalPipeline.testNestedPlan(TestEvalPipeline.java:376)
Caused by: org.apache.pig.impl.plan.optimizer.OptimizerException: Unable to insert type casts into plan
    at org.apache.pig.impl.logicalLayer.optimizer.TypeCastInserter.transform(TypeCastInserter.java:144)
    at org.apache.pig.impl.plan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:63)
    at org.apache.pig.PigServer.compileLp(PigServer.java:551)
    at org.apache.pig.PigServer.execute(PigServer.java:477)
    at org.apache.pig.PigServer.openIterator(PigServer.java:259)
    ... 16 more
Caused by: java.lang.NullPointerException
    at org.apache.pig.impl.logicalLayer.LOVisitor.visit(LOVisitor.java:121)
    at org.apache.pig.impl.logicalLayer.optimizer.SchemaRemover.visit(SchemaRemover.java:65)
    at org.apache.pig.impl.logicalLayer.LOCogroup.visit(LOCogroup.java:273)
    at org.apache.pig.impl.logicalLayer.LOCogroup.visit(LOCogroup.java:37)
    at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:68)
    at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
    at org.apache.pig.impl.logicalLayer.optimizer.LogicalTransformer.rebuildSchemas(LogicalTransformer.java:57)
    at org.apache.pig.impl.logicalLayer.optimizer.TypeCastInserter.transform(TypeCastInserter.java:141)
    ... 20 more
",,,,,,,,,,,,,,,,,,,03/Jul/08 14:02;pi_song;PIG_288_OptimizerNPE.patch;https://issues.apache.org/jira/secure/attachment/12385207/PIG_288_OptimizerNPE.patch,06/Jul/08 13:39;pi_song;PIG_288_OptimizerNPE_2.patch;https://issues.apache.org/jira/secure/attachment/12385348/PIG_288_OptimizerNPE_2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-07-03 14:02:20.629,,,no_permission,,,,,,,,,,,,163930,,,,,Tue Jul 08 11:58:36 UTC 2008,,,,,,,0|i0gidz:,94409,,,,,,,,,,"03/Jul/08 14:02;pi_song;The patch!

This is due to the fact that LOCOGroup also maintains internal plan relations. The optimizer also has to fix up the downstream operator's internal.","04/Jul/08 14:15;pi_song;The similar issue can happen in type checker and plan optimizer (future) as well. In the meantime, this patch should work but we will have to come up with a guaranteed safe way to alter the plan.

For example, we may have utility methods for removing/inserting nodes in linear subgraph to start with:-

RemoveNode (a -> b -> c,  b)   -->  (a -> c) 

InsertNode(a -> c, b)   -->   a -> b -> c
","06/Jul/08 13:39;pi_song;Just found out that the thing I mentioned already exists so moved the COGroup fixing logic to the appropriate place.
The test is still failing though due to problem in JarManager instead now.",08/Jul/08 11:58;pi_song;Patch checked in. Thanks to myself.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
local mode is broken,PIG-286,12399478,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,shravanmn,olgan,olgan,02/Jul/08 20:41,24/Mar/10 22:04,14/Mar/19 03:05,05/Aug/08 00:09,0.2.0,,,,,,0.2.0,,,,,0,,,,"Any query I ran in the local mode fails with the following error(s):

2008-07-02 13:37:52,780 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Initializing JVM Metrics with processName=JobTracker, sessionId=
2008-07-02 13:37:54,119 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
2008-07-02 13:37:54,625 [main] INFO  org.apache.pig.impl.mapReduceLayer.Launcher - 0.0% complete
2008-07-02 13:37:54,637 [main] INFO  org.apache.pig.impl.mapReduceLayer.Launcher - Unsuccessful attempt. Completed 0.0% of the job",,,,,,,,,,,,,,,,,,,07/Jul/08 20:31;shravanmn;TestCombiner.java;https://issues.apache.org/jira/secure/attachment/12385437/TestCombiner.java,08/Jul/08 18:38;shravanmn;localmode.patch;https://issues.apache.org/jira/secure/attachment/12385528/localmode.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-07-07 17:50:20.755,,,no_permission,,,,,,,,,,,,163928,,,,,Tue Aug 05 00:09:30 UTC 2008,,,,,,,0|i0gid3:,94405,,,,,,,,,,07/Jul/08 17:50;shravanmn;There is already a test case TestCombiner which tests the local functionality and is passing. I tried doing some other tests & could not replicate the issue. Can I get a TestCase for this?,"07/Jul/08 18:27;olgan;I think you will see it if you run any end-to-end test in the local mode.

I ran the following:

a = load 'studenttab10k' as (name, age, gpa);
store a into 'bar';

and see the following error with the latest code:

2008-07-07 11:25:35,893 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Initializing JVM Metrics with processName=JobTracker, sessionId=
2008-07-07 11:25:37,242 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
2008-07-07 11:25:37,747 [main] INFO  org.apache.pig.impl.mapReduceLayer.Launcher - 0.0% complete
2008-07-07 11:25:37,747 [main] INFO  org.apache.pig.impl.mapReduceLayer.Launcher - Unsuccessful attempt. Completed 0.0% of the job",07/Jul/08 20:31;shravanmn;I think I am missing something here. I wrote the same thing in the attached file TestCombiner.java and it works.,07/Jul/08 20:36;olgan;did you try to run an end-to-end test on a real cluster? Maybe MiniCluster works somewhat different than real cluster,07/Jul/08 20:51;shravanmn;Okie... will try that and chk what the issue is,"08/Jul/08 18:37;shravanmn;The local launcher has to make sure the local filesystem is used. So the job config created should have the ""fs.default.name"" set to ""file:///""",10/Jul/08 00:40;olgan;I am still seeing exactly the same problem,05/Aug/08 00:09;alangates;Issue fixed by combiner.patch for issue PIG-331.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
custom compare functions is ignored,PIG-285,12399476,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sms,olgan,olgan,02/Jul/08 20:36,24/Mar/10 22:04,14/Mar/19 03:05,30/Jul/08 15:57,0.2.0,,,,,,0.2.0,,,,,0,,,,"The following query successfully runs but the results don't come in the correct order:

a = load 'studenttab10k';
c = order a by $0 using org.apache.pig.test.udf.orderby.OrdDesc;
store c into ;out';

results:

alice allen     27      1.950
alice allen     42      2.460
alice allen     38      0.810
alice allen     68      3.390
alice allen     77      2.520
alice allen     36      2.270
.....

expcted:

zach zipper     66      2.670
zach zipper     47      2.920
zach zipper     19      1.910
zach zipper     23      1.120
zach zipper     40      2.030
zach zipper     59      2.530
.....",,,,,,,,,,,,,,,,,,,29/Jul/08 20:56;sms;sort_using_udf.patch;https://issues.apache.org/jira/secure/attachment/12387133/sort_using_udf.patch,08/Jul/08 21:46;shravanmn;sortfin.patch;https://issues.apache.org/jira/secure/attachment/12385541/sortfin.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-07-07 20:48:42.491,,,no_permission,,,,,,,,,,,,38590,,,,,Wed Jul 30 15:57:51 UTC 2008,,,Patch Available,,,,0|i0gicv:,94404,,,,,,,,,,"07/Jul/08 20:48;shravanmn;Can you provide the comparison function, org.apache.pig.test.udf.orderby.OrdDesc please?","07/Jul/08 20:55;olgan;package org.apache.pig.test.udf.orderby;

import org.apache.pig.data.Tuple;
import org.apache.pig.ComparisonFunc;

public class OrdDesc extends ComparisonFunc {
    // this is a simple example - more complex comparison will require
    //   breakout of the individual values. I suggest you'll have
    //   to convert ""catch(IOException e) to RuntimeException('msg', e)""
    public int compare(Tuple t1, Tuple t2) {
        return t2.compareTo(t1);
    }
}","08/Jul/08 19:21;shravanmn;I have found the issue. Before describing it let me give some background so that fixing other related issues is simpler:

The Order by Clause is handled with Quantiles. So a job which has an order by occuring in the main plan is run as multiple jobs:
1. Store the output till the order by.
2. Run a quantile job to find the quantiles
3. Run the sort job.

The following should be the pig-script version of getQuantileJob in MRCompiler
{noformat}
A = load fSpec using RandomSampleLoader
B = foreach A generate flatten(col1), flatten(col2), ...
C = group all
D = foreach C {
	D1 = order $1 by *;
	generate requestedParallelism,D1;
}
E = foreach D generate FindQuantiles(*);
store E into quantFiles
{noformat}

The getSortJob should look something like this
{noformat}
A = load fSpec using BinStorage
B = group A by (col1,col2,...);
C = foreach B generate flatten(A);
{noformat}

C should have the output of ORDER BY

Also, the sort job should have some key things turned on in Hadoop for it to work:
1. Use the SortPartitioner as the key partitioner which internally uses the quantile file generated by the quantile job
2. Also supply any user defined comparator to hadoop as the output key comparator

That is the ideal thing to do. The issue was the following:
Since the quantile job physical plan was hand crafted, it had the plan for the following instead of what it should have been:
{noformat}
A = load fSpec using RandomSampleLoader
B = foreach A generate flatten(col1), flatten(col2), ...
C = group all
D = foreach C {
	generate requestedParallelism,$1;
}
E = foreach D generate FindQuantiles(*);
store E into quantFiles;
{noformat}

Hence instead of the sorted output, all we saw was the grouped output and probably some incorrect results as the quantiles might have got messed if a parallel statement was used along with the order by which is the cause for [Pig-292|https://issues.apache.org/jira/browse/PIG-292].

The other part was that the user defined comparator was not being passed as the output key comparator which is the cause of the current bug. Another thing that led to us not finding the bug early was an error in the testSort test case which I have corrected in [Pig-295|https://issues.apache.org/jira/browse/PIG-295].

To resolve this issue, first corrected the quantile job to include the order by in the nested plan. However this caused issues with deserializing POUserComparisonFunc which extended from POUserFunc. The issue was because when POUserComparisonFunc was deserialized POUserFunc got deserialized first and tried to instantiate EvalFunc from a ComparisonFunc spec. To resolve this, I had to make POUserComparisonFunc independent of POUserFunc and here I have made the assumption that ComparisonFunc is used only in ORDER BY and not elsewhere. This corresponds to all the extraneous things in the patch.

The next thing I did was to try to correct the missing supply of user defined comparator to Hadoop as the key comparator. However, this causes issues:
We assume that ComparisonFunc always compares Tuples. However, with the inclusion of types, we do not always wrap everything into a tuple and instead try to use the basic types wherever possible. The patch I am going to submit does not address this part. The patch will assume that issue with ComparisonFunc will be fixed and directly sets the user defined comparator as the output key comparator. This will for the time being cause all user defined comparisons to fail.

Some hints on the ComparisonFunc issue:
1. The soln should take into consideration that sometimes ComparisonFunc are generic and need not know the schema of the input. Ex. OrdDesc
2. Many a times however, if its not a generic ComparisonFunc, we can assume that schema is known.
3. The ComparisonFunc will have to work with hadoop types and not pig types as it would be used in the boundary between LR & Pkg

Currently, ComparisonFunc extends WritableComparator and gives a concrete implementation that delegates all compare(WritableComparable,WritableComparable) calls to compare(Tuple,Tuple). Instead if we leave the compare(WritableComparable,WritableComparable) abstract I feel it should solve the problem and users can provide an implementation of the compare for the type that they are expecting. Will attach a patch shortly.","09/Jul/08 17:59;alangates;Checked in the sortfin patch.  This addresses some of the sorting issues, but some order by queries still fail.",10/Jul/08 00:21;olgan;Has this issue been resolved? Can it be closed?,10/Jul/08 18:45;alangates;No.  Shravan's patch addressed some but not all of the issues.,"29/Jul/08 20:56;sms;The patch (sort_using_udf.patch) fixes the issue with passing user defined comparators to Hadoop by replacing the Class.forname() call with PigContext.resolveClassName()

Unit tests that still fail are:

    [junit] Running org.apache.pig.test.TestBuiltin
    [junit] Tests run: 23, Failures: 1, Errors: 1, Time elapsed: 14.66 sec
    [junit] Test org.apache.pig.test.TestBuiltin FAILED

    [junit] Running org.apache.pig.test.TestFilterOpNumeric
    [junit] Tests run: 8, Failures: 0, Errors: 1, Time elapsed: 57.662 sec
    [junit] Test org.apache.pig.test.TestFilterOpNumeric FAILED

    [junit] Running org.apache.pig.test.TestStoreOld
    [junit] Tests run: 3, Failures: 0, Errors: 2, Time elapsed: 39.853 sec
    [junit] Test org.apache.pig.test.TestStoreOld FAILED
","30/Jul/08 00:45;alangates;Committed sort_using_udf.patch.  This solved the issue of the script failing saying that it could not find the user provided comparator.  However, the hadoop sort still does not use the user provided comparator.  So I'm returning this bug to the open state until that issue is resolved as well.","30/Jul/08 01:04;sms;This is what I see when I run it on grunt 

{noformat}
grunt> ls
hdfs://wilbur20.labs.corp.sp1.yahoo.com:8020/user/sms/data      <dir>

grunt> a = load 'data' as (name, age, gpa);

grunt> dump a;
22438 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher  - 0% complete
25315 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher  - 100% complete
27836 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher  - Completed Successfully
(Joe 20 3.5)
(Harry 20 3.2)
(John 19 3.8)
(Jack 19 3.1)
(Govinda 20 4.0)

grunt> register testudf.jar;

grunt> b = order a by * using org.apache.pig.test.udf.orderby.OrdDesc;

grunt> dump b;
113532 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher  - 0% complete
117077 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher  - 33% complete
121938 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher  - 50% complete
126999 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher  - 66% complete
132339 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher  - 83% complete
137393 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher  - 100% complete
139418 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher  - Completed Successfully
(John 19 3.8)
(Joe 20 3.5)
(Jack 19 3.1)
(Harry 20 3.2)
(Govinda 20 4.0)

grunt>
{noformat}",30/Jul/08 15:57;alangates;My mistake.  The patch works.  I compared it against the incorrect benchmark.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow no alias in Dot schema definition in Dot LogicalPlanLoader,PIG-278,12398828,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pi_song,pi_song,pi_song,23/Jun/08 12:28,25/Mar/10 00:12,14/Mar/19 03:05,04/Jul/08 15:24,,,,,,,0.1.0,,,,,0,,,,"Our schema parser doesn't allow ""null"" alias but we have to be able to do that in Dot test files.
This is a work around by introducing ""[NoAlias]"" keyword in schema definition just for Dot LogicalPlanLoader.

Sample:-
{noformat}
foreach [  key=""20"", type=""LOForEach"" , schema=""[NoAlias] : long, [NoAlias] : byteArray""    ] ;
{noformat}

At runtime, [NoAlias] will be substituted by dummy column names before being sent to the parser. Subsequently those names will be replaced by ""null"". There is no changes in the actual query parser.",,,,,,,,,,,,,,,,,,,23/Jun/08 12:29;pi_song;AllowNoAliasSchemaInDot.patch;https://issues.apache.org/jira/secure/attachment/12384491/AllowNoAliasSchemaInDot.patch,03/Jul/08 14:44;pi_song;AllowNoAliasSchemaInDot2.patch;https://issues.apache.org/jira/secure/attachment/12385210/AllowNoAliasSchemaInDot2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-06-23 15:43:51.324,,,no_permission,,,,,,,,,,,,163922,,,,,Fri Jul 04 15:24:19 UTC 2008,,,Patch Available,,,,0|i0gi9z:,94391,,,,,,,,,,23/Jun/08 12:29;pi_song;Patch,23/Jun/08 15:43;alangates;+1 on the work around for having no alias.  I'd like to have Santhosh look at the changes to Schema.java.,24/Jun/08 12:25;pi_song;Basically it now handles null aliases which I had never expected before.,"26/Jun/08 16:40;sms;In the Schema.FieldSchema equals() method, the comparison of fschema.alias should be with fother.alias and not with fschema.alias itself. I have marked those lines with ""<-- should be fother.alias"" in the code below.

{code}
+            if (!relaxAlias) {
+                if ( (fschema.alias == null) &&
+                     (fschema.alias == null) ) { <-- should be fother.alias
+                    // good
+                }
+                else if ( (fschema.alias != null) &&
+                          (fschema.alias == null) ) { <-- should be fother.alias
+                    return false ;
+                }
+                else if ( (fschema.alias == null) &&
+                          (fschema.alias != null) ) { <-- should be fother.alias
+                    return false ;
+                }
+                else if (!fschema.alias.equals(fschema.alias)) { <-- should be fother.alias
+                    return false ;
+                }
{code}","03/Jul/08 14:44;pi_song;Unbelievably stupid mistake.

Thanks for review!

Here is the fixed patch.","04/Jul/08 15:24;pi_song;I ran the test and it didn't cause more tests to fail. 

Committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
changing combiner behavior past hadoop 18,PIG-274,12398464,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,olgan,olgan,17/Jun/08 23:01,25/Mar/10 00:12,14/Mar/19 03:05,26/Jan/09 20:25,,,,,,,0.2.0,,,,,0,,,,"In hadoop 18, the way commbiners are handled is changing. The hadoop team agreed to keep things backward compatible for now but will depricate the current behavior in the future (likely in hadoop 19) so pig needs to adjust to the new behavior. This should be done in the post 2.0 code base.

Old behavior: combiner is called once and only once per map task
New behavior: combiner can be run 0 or more times on both map and reduce sides. 0 times happens if only a single <K, V> fits into sort buffer. Multiple time can happen in case of a hierarchical merge.

The main issue that causes problem for pig is that we would not know in advance whether the combiner will run 0,1 or more times. This causes several issues:

(1) Lets assume that we compute count. If we enable combiner, reducer expects to get numbers not values as its input. Hadoop team suggested that we could annotate each tuple with a byte that tells if it want through combiner. This could be expensive computatinally as well as will use extra memory. One things to notice is that some algebraics (like SUM, MIN, MAX) don't care whether the data was precombined as they always to the same thing. Perhaps we can make algebaic functions declare if they care or not. Then we only anotate the ones that need it.
(2) Since combiner can be called 1 or more times, getInitial and getIntermediate have to do the same thing. So again, we need to change the interface to reflcat that.
(3) current combiner code assumes that it only works with 1 input. When it runs on the reduce side, it can be dealing with tuples from multiple inputs. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-06-24 13:33:59.215,,,no_permission,,,,,,,,,,,,163918,,,,,Mon Jan 26 20:25:36 UTC 2009,,,,,,,0|i0gi87:,94383,,,,,,,,,,"19/Jun/08 16:48;olgan;After some discussion with hadoop guys, I think what they are proposing makes sense.

Pig already treats algebraic the computation as having 3 stages: initial, intermediate final. The proposal is for stages to map to the MR framework as followos:

initial - done in the map 
intermediate - done in combiner; it is fine that the computation is done 0 or more times since it does not impact the correctness of the computation
final - done in the reducer.

We can make the computation quite efficient in the map for a common case by performing hash based aggregation. This way the data to be sorted by the map would significantly smaller than what it is now.

Pradeep will be running some performance tests to confirm this.

Please, comment","24/Jun/08 13:33;pi_song;Sounds reasonable. Please note that the contract when implementing ""Interim"" will have to allow multi-level from now on.
All the algebraic functions that we've got should be alright. The new covariance thing should be fine as well.","15/Jul/08 17:37;olgan;Some additional thoughts.

Hash based approach would work well for group by keys with very low cardinality since the data given to each map is usually pretty small. For other cases hashing would actually introduce a significant overhead. Since we don't currently have metadata, we will not be able to make an intelligent choice. Moreover, if the combiner is called after the aggregation, it is completely wasted since the data is already reduced.

Alan suggested that we could opportunistically reduce data if we see adjacent keys that are the same but I am wondering whether the overhead of compare is justified on this case.

Comments are welocome.","16/Jul/08 14:27;pi_song;Sounds like this hash thing should be implemented in Hadoop instead. The reason not being there already might be because of the risk.

If we want implement it solely in Pig then this feature should be optional.","16/Jul/08 17:56;olgan;According to Ben, that how combiner used to work in earlier versions of Hadoop :).

I agree that Hadoop seems to be pushing this functionality for users to implement.

Also, it seems thta it is not giving us enough control to tell when and how to execute code. Ideally I should be able to say Run/don't run combiner on the map side, run/don't run combiner on the reduce side.",26/Jan/09 20:25;olgan;This work has been done,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failure running complex script with streaming,PIG-272,12398384,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,acmurthy,olgan,olgan,17/Jun/08 00:30,24/Mar/10 22:01,14/Mar/19 03:05,25/Jun/08 23:19,,,,,,,0.1.0,,,,,0,,,,"The following script fails (stack is further down):

define CMD `perl identity.pl`;
define CMD1 `perl identity.pl`;
A = load '/user/pig/tests/data/singlefile/studenttab10k' as (name, age, gpa);
B = stream A through CMD;
store B into 'B1';
C = stream B through CMD1;
D = JOIN B by name, C by name;
store D into 'D1';

If I remove the intermediate store, the script works fine. Also if I replace streaming commands with other operators such as filter and foreach, it works even with the intermediate store.",,,,,,,,,,,,,,,,,,,23/Jun/08 06:30;acmurthy;PIG-272_0_20080621.patch;https://issues.apache.org/jira/secure/attachment/12384471/PIG-272_0_20080621.patch,23/Jun/08 06:30;acmurthy;PIG-272_test.pig;https://issues.apache.org/jira/secure/attachment/12384472/PIG-272_test.pig,23/Jun/08 06:30;acmurthy;split.pl;https://issues.apache.org/jira/secure/attachment/12384473/split.pl,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2008-06-19 20:29:52.15,,,no_permission,,,,,,,,,,,,163916,,,,,Wed Jun 25 23:19:41 UTC 2008,,,,,,,0|i0gi7b:,94379,,,,,,,,,,"18/Jun/08 00:02;olgan;Arun, helped to diagnose the problem. The issue is that the following sequence

B = stream A through CMD;
store B into 'B1';

kicks in the optimization and as the result store users BinaryStorage to write the results of the first job.

When the second job starts to run, it realizes that it can reuse the results and tries to load them also using BinaryStorage which is wrong and causes exceptions since the tuples don't have structure expected by the second script.

The solution is to attach the original store function to the materialized results; however, the code changes for it are quite ugly.","19/Jun/08 20:29;acmurthy;Sigh, attaching the original store function isn't enough.

The problem is that currently Pig re-executes the entire pipeline and doesn't use the existing results on HDFS for the JOIN in the above example. When that happens the StreamingCommand's output-spec is still setup as 'BinaryStorage' and results in this error.","20/Jun/08 08:02;acmurthy;To clarify the above comment: it seems like the 'materialized results' of the first of the two resulting Map-Reduce jobs isn't being used by the second. Rather, it goes ahead and re-executes the entire pipeline. Clearly, it is rather inefficient. Thus, it looks like the existing code for tracking/using previous job's results has a bug.","23/Jun/08 06:30;acmurthy;Attached fix. The patch ensures we deep-copy the StreamingCommand before optimizing it and reverts the optimization piece-meal (i.e for input and output separately).

The test cases are quite complex/convoluted and are pretty hard to convert to unit-tests, which I why I've attached them here and propose we integrate them into our end-to-end tests...","23/Jun/08 16:57;olgan;Thanks, Arun. I will be testing your patch today.","25/Jun/08 23:19;olgan;I committed the changes. I ran all existing unit and end-to-end tests as well as the end-to-end tests provided by Arun. They all passed.

Thanks, Arun for fixing this issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Describe Bug,PIG-268,12398327,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,,amirhyoussefi,amirhyoussefi,16/Jun/08 13:19,25/Mar/10 00:12,14/Mar/19 03:05,26/Jan/09 21:00,,,,,,,0.2.0,,,,,0,,,,"Priority: Trivial 

grunt> a = load 'test.txt' as (x,y) ;
grunt> b = filter a by  x eq 'abc';
grunt> describe a;
a: (x, y )
grunt> describe b;
b: (x, y )
grunt> describe a;
b: (x, y )

last one is describe a but gives b: (x, y )
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-01-26 21:00:11.988,,,no_permission,,,,,,,,,,,,631,,,,,Mon Jan 26 21:00:11 UTC 2009,,,,,,,0|i0gi5j:,94371,,,,,,,,,,26/Jan/09 21:00;olgan;This issue is resolved with latest code,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Warnings with hadoop 17,PIG-266,12398120,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,11/Jun/08 23:24,24/Mar/10 22:01,14/Mar/19 03:05,17/Jun/08 18:48,,,,,,,0.1.0,,,,,0,,,,"We now seeing the following warnings when running with HOD and hadoop 0.17 

""host:port"" is a deprecated filesystem name. Use
""hdfs://host:port/"" instead.
",,,,,,,,,,,,,,,,,,,17/Jun/08 00:11;olgan;PIG-266.patch;https://issues.apache.org/jira/secure/attachment/12384094/PIG-266.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-06-17 18:02:21.187,,,no_permission,,,,,,,,,,,,163911,,,,,Tue Jun 17 18:48:28 UTC 2008,,,,,,,0|i0gi4n:,94367,,,,,,,,,,"17/Jun/08 00:12;olgan;We no longer need to validate hosts. The modification we made to the names caused the warnings. Tested that with this change, I no longer see the warnings",17/Jun/08 18:02;alangates;+1,17/Jun/08 18:48;olgan;patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
explain exception,PIG-261,12397769,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,,haijun,haijun,08/Jun/08 21:17,25/Mar/10 00:12,14/Mar/19 03:05,26/Jan/09 20:57,,,,,,,0.2.0,,grunt,,,0,,,,"grunt> a = load 'whparsed_20071231';
grunt> b = filter a by $0 neq 'SE' AND ARITY (*) >= 6;
grunt> c = group b by ($4, $5);
grunt> d = foreach c {
>> e = distinct b.$1;
>> generate flatten (group), COUNT (b), COUNT (e);
>> }
grunt> explain d
Logical Plan:
|---LOEval ( GENERATE {[FLATTEN PROJECT $0],[COUNT(GENERATE {[PROJECT $1]})],[COUNT(GENERATE {[PROJECT $1]->[PROJECT $1]->[DISTINCT ]})]} )
      |---LOCogroup ( GENERATE {[PROJECT $4],[PROJECT $5],[*]} )
            |---LOEval ( [FILTER BY (([PROJECT $0] neq ['SE']) AND ([ARITY(GENERATE {[*]})] >= ['6']))] )
                  |---LOLoad ( file = whparsed_20071231 )
-----------------------------------------------
Physical Plan:
|---POMapreduce
    Map : Composite(*,Filter:  AND )
    Reduce : Generate(Project(0),FuncEval(COUNT(Generate(Project(1)))),FuncEval(COUNT(Generate(Composite(Project(1),Project(1),Sort(Distinct(148045 [main] ERROR org.apache.pig.tools.grunt.GruntParser  - java.lang.NullPointerException
        at org.apache.pig.impl.eval.EvalSpecTreePrinter.visitSortDistinct(EvalSpecTreePrinter.java:42)
        at org.apache.pig.impl.eval.SortDistinctSpec.visit(SortDistinctSpec.java:130)
        at org.apache.pig.impl.eval.EvalSpecTreePrinter.visitCompositeEval(EvalSpecTreePrinter.java:111)
        at org.apache.pig.impl.eval.CompositeEvalSpec.visit(CompositeEvalSpec.java:116)
        at org.apache.pig.impl.eval.EvalSpecTreePrinter.visitGenerate(EvalSpecTreePrinter.java:61)
        at org.apache.pig.impl.eval.GenerateSpec.visit(GenerateSpec.java:366)
        at org.apache.pig.impl.eval.EvalSpecTreePrinter.visitFuncEval(EvalSpecTreePrinter.java:95)
        at org.apache.pig.impl.eval.FuncEvalSpec.visit(FuncEvalSpec.java:249)
        at org.apache.pig.impl.eval.EvalSpecTreePrinter.visitGenerate(EvalSpecTreePrinter.java:61)
        at org.apache.pig.impl.eval.GenerateSpec.visit(GenerateSpec.java:366)
        at org.apache.pig.impl.physicalLayer.POTreePrinter.visitMapreduce(POTreePrinter.java:79)
        at org.apache.pig.backend.hadoop.executionengine.POMapreduce.visit(POMapreduce.java:281)
        at org.apache.pig.backend.hadoop.executionengine.MapRedPhysicalPlan.explain(MapRedPhysicalPlan.java:41)
        at org.apache.pig.PigServer.explain(PigServer.java:442)
        at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:136)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:177)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:73)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:54)
        at org.apache.pig.Main.main(Main.java:270)

148046 [main] ERROR org.apache.pig.tools.grunt.GruntParser  - java.lang.NullPointerException
","ubuntu 7.10
jdk1.6.0_06
ant 1.7.0
hadoop-0.17.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-01-26 20:57:20.16,,,no_permission,,,,,,,,,,,,163906,,,,,Mon Jan 26 20:57:20 UTC 2009,,,,,,,0|i0gi2n:,94358,,,,,,,,,,"08/Jun/08 21:21;haijun;I should add that the script runs fine when I store d, it just throws exception during explain.
I would like to use explain to see if distinct is in combiner phase.",26/Jan/09 20:57;olgan;works with latest code,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
All error output goes to the stderr,PIG-260,12397707,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,olgan,olgan,06/Jun/08 18:24,25/Mar/10 00:12,14/Mar/19 03:05,26/Jan/09 20:54,,,,,,,0.2.0,,,,,0,,,,Currently all error information is dumped onto the screen including error stacks from all tasks. A better approach would be to provide a single meaningful message to the user and store the rest of the information in a client side log file.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,163905,,,,,Mon Jan 26 20:54:30 UTC 2009,,,,,,,0|i0gi27:,94356,,,,,,,,,,26/Jan/09 20:54;olgan;This is done in latest code,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig should cleanup output directory of a failed query,PIG-258,12397705,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,daijy,olgan,olgan,06/Jun/08 18:15,01/Apr/10 04:06,14/Mar/19 03:05,10/Jul/08 00:08,,,,,,,0.1.0,,,,,0,,,,"Currently, after a failed store, the output directory is left behind and can't be re-used without manual cleanup
",,,,,,,,,,,,,,,,,,,01/Jul/08 17:38;daijy;clearoutput.patch;https://issues.apache.org/jira/secure/attachment/12385042/clearoutput.patch,09/Jul/08 22:47;daijy;clearoutput2.patch;https://issues.apache.org/jira/secure/attachment/12385683/clearoutput2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-06-26 22:18:30.159,,,no_permission,,,,,,,,,,,,163903,,,,,Thu Jul 10 00:08:17 UTC 2008,,,,,,,0|i0gi1b:,94352,,,,,,,,,,"26/Jun/08 22:18;daijy;Currently only one POStore in one physical plan, so if the execution of a physical plan fails, remove the associated output file.

Here is the plan:
1. Create an entry deleteOnFail in FileLocalizer
2. Capture POStore in physical plan. We can put this code in MRCompiler.compile
3. Clear deteleOnFail before execution of any physical plan
4. After an unsuccessful execution of a physical plan, remove output file if it exists. Add this in PigServer.execute 

I will create a patch shortly if no problem",01/Jul/08 17:38;daijy;Attached patch target branches/types,"02/Jul/08 00:17;olgan;Hi Daniel,

Looks good. I have a couple of questions/comments:

(1) I don't think we should through exception if we can't register the file to delete. We should just log a warning
(2) Also, if we can't delete, we should log a warning not an error
(3) I think you might be double deleting in case of dump. dump stores into temp file which would get cleaned up when we remove temp files. This might be ok if both check for file existance before trying to delete
(3) What kind of tests did you run?

I think we need to test the following:

(1) Pig script with a single successful store
(2) Pig script with multiple successful stores
(3) Pig script with one failed store 
(4) Pig script with multiple failures 
(5) Pig script with one successful and one failed store
(6) Pig script with a single successful dump
(7) Pig script with a single failed dump

","09/Jul/08 22:47;daijy;Some changes in the patch:
1. Use log.warn instead of throw exception/log.error
2. To prevent double deletion, check if output file is inside temp file list before putting into deleteOnFail
3. Add new testcase TestDeleteOnFail","10/Jul/08 00:08;olgan;changies committed into the types branch. Thanks, Daniel for contributing!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect message trying to store non-existent alias,PIG-251,12397513,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,olgan,olgan,04/Jun/08 21:09,24/Mar/10 22:01,14/Mar/19 03:05,04/Jun/08 21:12,,,,,,,0.1.0,,,,,0,,,,"the following script:  

store foo into 'test';

produces the error stack:

java.lang.NullPointerException
        at org.apache.pig.tools.grunt.Utils.getPermissionException(Utils.java:24)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:77)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:54)
        at org.apache.pig.Main.main(Main.java:253)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,163896,,,,,Wed Jun 04 21:12:46 UTC 2008,,,,,,,0|i0ghy7:,94338,,,,,,,,,,"04/Jun/08 21:12;olgan;Looks like this was an issues with earlier versions. With the latest code, I get the following error:

grunt> store foo into 'bar';
08/06/04 14:11:56 ERROR grunt.GruntParser: java.io.IOException: Undefined alias: foo used in STORE
        at org.apache.pig.PigServer.registerQuery(PigServer.java:278)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:457)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:233)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:73)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:54)
        at org.apache.pig.Main.main(Main.java:270)
Caused by: org.apache.pig.impl.logicalLayer.parser.ParseException: Undefined alias: foo used in STORE
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.StoreClause(QueryParser.java:3554)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:729)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:512)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:382)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:47)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:275)
        ... 5 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig is broken with speculative execution,PIG-250,12396943,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,28/May/08 00:44,24/Mar/10 22:01,14/Mar/19 03:05,29/May/08 18:04,,,,,,,0.1.0,,,,,0,,,,"If I have speculative execution turned on, the following script fails:

a = load 'studenttab20m' as (name, age, gpa);
b = load 'votertab10k' as (name, age, registration, contributions);
c = filter a by age < '50';
d = filter b by age < '50';
e = cogroup c by (name, age), d by (name, age) parallel 10;
f = foreach e generate flatten(c), flatten(d) parallel 10;
g = group f by registration parallel 10;
h = foreach g generate group, SUM(f.d::contributions) parallel 10;
i = order h by ($1, $0);
store i into 'out';

I traced this to the fact that the first MR job produces one or more empty outputs from the reducer. This happened on the reducers that happened to have second task running.

I am not sure what the issue is and I am working with hadoop guys to investigate. Until this issue is resolved, I would like to trun speculative execution off.",,,,,,,,,,,,,,,,,,,28/May/08 01:06;olgan;PIG-250.patch;https://issues.apache.org/jira/secure/attachment/12382893/PIG-250.patch,29/May/08 16:53;olgan;PIG-250_v2.patch;https://issues.apache.org/jira/secure/attachment/12383032/PIG-250_v2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-05-28 01:15:54.134,,,no_permission,,,,,,,,,,,,163895,,,,,Thu May 29 18:04:33 UTC 2008,,,,,,,0|i0ghxz:,94337,,,,,,,,,,28/May/08 01:15;acmurthy;+1,28/May/08 17:08;alangates;+1,"29/May/08 16:53;olgan;Arun helped to diagnose the problem. It turned out that we were not passing the right directory to the task to write to and as the result all tasks (primary + speculative) were writing to the same place overwriting each others output.

The attached patch fixes the issue and also reverses the prior change of disabling speculative execution. With this changes, by default the speculative execution will be consistent with hadoop default (currently on) and will be controlled by configuration (hadoop-site.xml)

I tested that speculative execution works now and that I can control it from hadoop-site.xml.

Thanks, Arun.

One of the committers - please review.",29/May/08 16:57;alangates;+1,29/May/08 18:04;olgan;Latest patch committed. Thanks Arung for figuring out this issue!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig Local give wrong results,PIG-248,12396745,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,amirhyoussefi,amirhyoussefi,24/May/08 02:36,02/May/13 02:29,14/Mar/19 03:05,26/Jan/09 20:52,,,,,,,0.2.0,,,,,0,,,,"Pig Local mode gives wrong results (but Pig + Hadoop give correct results) . This created issues for users who tried to debug in local mode...

Here's my pig code:

a = load '/homes/amiry/tmp/my_test.txt' as (c1,c2,c3);
b = group a by c1;
c = foreach b {
   d = order a by c3;
   generate d;
}
dump c;

Pig + Hadoop: 

({(abc, http://www.sss.net, a), (abc, http://www.rrr.com, b)})
({(def, http://local.yahoo.com/, d), (def, http://lmn.com/, f), (def, http://xyz.com/, g)})


Pig Local:
({(abc, http://www.sss.net, a), (abc, http://www.rrr.com, b)})
({})


Input: 

abc     http://www.rrr.com      b
abc     http://www.sss.net      a
def     http://local.yahoo.com/ d
def     http://lmn.com/ f
def     http://xyz.com/ g


",Pig Local,,,,,,,,,,,,,,,,PIG-157,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-05-30 17:27:25.261,,,no_permission,,,,,,,,,,,,163893,,,,,Mon Jan 26 20:52:03 UTC 2009,,,,,,,0|i0ghx3:,94333,,,,,,,,,,"30/May/08 17:27;pkamath;With a flatten in the generate, correct results are obtained:

cat ~/work/pig/localtest.pig
a = load '/homes/pradeepk/work/pig/localtest.txt' as (c1, c2, c3);
b = group a by c1;
c = foreach b { d = order a by c3; generate flatten(d); }
dump c;


(abc, http://www.sss.net, a)
(abc, http://www.rrr.com, b)
(def, http://local.yahoo.com/, d)
(def, http://lmn.com/, f)
(def, http://xyz.com/, g)

Could be an issue with dump in local mode?

The types implementation for the next release of PIG has changes in the local mode code - this bug should also be addressed in that re-write
","30/May/08 17:57;pkamath;One other observation is with larger dataset only the last ""group""'s bag is missing in the dump output",26/Jan/09 20:52;olgan;tested that we do the right thing in both local and hadoop mode with the latest code,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make pig work on Windows,PIG-243,12396335,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,olgan,olgan,19/May/08 17:58,07/Jul/12 00:18,14/Mar/19 03:05,26/Jun/08 00:16,,,,,,,0.1.0,,,,,0,,,,Currently a large number of unit tests is failing on Windows. We need to fix that.,,,,,,,,,,,,,,,,,,,30/May/08 18:24;daijy;PIG_243.patch;https://issues.apache.org/jira/secure/attachment/12383129/PIG_243.patch,24/Jun/08 16:56;daijy;cygpath.patch;https://issues.apache.org/jira/secure/attachment/12384599/cygpath.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-05-19 23:14:36.885,,,no_permission,,,,,,,,,,,,163888,,,,,Thu Jun 26 00:16:35 UTC 2008,,,,,,,0|i0ghuv:,94323,,,,,,,,,,"19/May/08 23:14;pi_song;Like Alan said, our developers don't use Windows so it's difficult to test. Is it possible to also run tests on windows as nightly build?","28/May/08 06:43;daijy;Attached patch fix -most- all problems under cygwin. -4 errors remaining in TestStreaming for map-reduce testing under cygwin need further work.- Patch is tested in both Linux and Cygwin. Here is a list of detected problem and solution:
||Problem||Affected Testcase||Solution||
|Windows path delimit ""\"" interpreted as escape in java|TestMapReduceResultRecycling, TestMapReduce, TestOrderBy, TestBuiltin, TestFilterOpNumeric, TestBinaryStorage, TestFilterOpString, TestEvalPipeline, TestPigFile, TestReversibleLoadStore, TestAlgebraicEval, TestLargeFile, TestCombiner, TestPi |replace ""\"" with ""\ \"", see [http://issues.apache.org/jira/browse/PIG-152]|
| Runtime.exec(String[]) fail for perl -e if perl statement include quote character for unknown reason |TestParamSubPreproc, TestStreaming|append a escape character \ to quote character "" under Windows|
| There is a tailing '\r' (0x0d) character for line read from text file under cygwin|TestBuiltin, TestEvalPipeline, TestPigSplit, TestStreaming|remove tailing '\r' |
| Bug: forget to close output stream, following deletion operation fail|TestBinaryStorage|Fix bug |
| Pig streaming put the current directory into PATH. Current directory is obtained using ""System.getProperty(""user.dir"")"", it is a Windows style path under cygwin. However, Cygwin only accept Unix style path in PATH environment variable. So put the Windows path obtained from ""System.getProperty"" directly into PATH will not work | TestStreaming | Change Windows path to Unix path under Cygwin. Using Cygwin utility ""cygpath""|
Only two change are in core code. One is PigStorage.java (to fix tailing '\r'). The other is ExecutableManager (Add cygpath). All other changes are in testcase. ","29/May/08 16:07;olgan;Thanks, Daniel. Changes look good. Once concern I have is with changes to PigStorage since it adds a little bit of overhead to each record processing. However, since I assume that String caches the length, the overhead should be minimal. Could you run a quick test that reads a million lines with and without the extra processing and see what the time difference is. Thanks.","29/May/08 20:10;daijy;I have tested reading one million records. Here is the result:
* On Linux, where trimming not taking place:
** Before patch: 10.53s
** After patch: 10.60s
* On Windows, where trimming branch taken:
** Before patch: 13.65s
** After patch: 13.69s

Seems that the overhead is neglectable. 

Another solution is to change the testcase rather than core code to fix this test failure. However, I think move it to core is better. Failing to read a file in dos format (or need extra processing) under cygwin sounds not reasonable. How do you think? ","29/May/08 21:10;olgan;Yes, I think this is fine. I will test and commit the patch sometimes tomorrow or Monday.",29/May/08 21:26;daijy;Thank you!,"30/May/08 11:39;pi_song;+1

BTW, there is another guy called TextLoader that needs '\r' removed as well.
",30/May/08 18:26;daijy;You are right. We should change TextLoader as well. I have updated patch to include this. Thank you!,30/May/08 20:31;olgan;Patch committed. Thanks Daniel for contributing!,"02/Jun/08 11:33;pi_song;So far, we have ensured the data internalize/externalize bit. Is there anything else in the engine that can cause trouble on Windows? 
I couldn't think of any.",24/Jun/08 16:56;daijy;Fix the remaining 4 unit test error under cygwin,25/Jun/08 23:30;olgan;Thanks Daniel. Running tests now.,26/Jun/08 00:16;olgan;Patch committed. Thanks Daniel for contributing.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig allows to overwrite existing files,PIG-237,12395757,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,olgan,olgan,09/May/08 22:58,24/Mar/10 22:01,14/Mar/19 03:05,12/May/08 18:01,,,,,,,0.1.0,,,,,0,,,,"You can run the following script multiple times and it will not produce the error second time saying that the directory already exists. If the existing file is a single file and not a directory, hadoop eventually produces an error but with directory it runs to completion.

A = load 'data';
store A into 'foo';

The validation code is missing from org/apache/pig/impl/logicalLayer/parser/QueryParser.jjt in  StoreClause() function.",,,,,,,,,,,,,,,,,,,12/May/08 14:46;pi_song;PIG_237_no_override.patch;https://issues.apache.org/jira/secure/attachment/12381879/PIG_237_no_override.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-05-11 12:19:29.923,,,no_permission,,,,,,,,,,,,163883,,,,,Mon May 12 18:01:34 UTC 2008,,,,,,,0|i0ghs7:,94311,,,,,,,,,,"11/May/08 12:19;pi_song;If this is not an urgent need, I would say it has already been addressed in the type branch.",12/May/08 12:39;olgan;I think we need a temp fix. This can cause data loss.,"12/May/08 13:24;pi_song;Sorry for Pradeep. In this case I will create a patch which is a piece of code copied from type branch.
It will be available in 20 if there is no problem (I'm running unit tests).","12/May/08 14:46;pi_song;The patch.
No test failed.","12/May/08 18:01;olgan;Patch committed. Thanks, Pi.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
properties specified on the command line are ignored,PIG-236,12395674,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,olgan,olgan,09/May/08 00:15,24/Mar/10 22:01,14/Mar/19 03:05,13/May/08 16:36,0.0.0,,,,,,0.1.0,,,,,0,,,,Looks like code in src/org/apache/pig/impl/util/PropertiesUtil.java does not take system properties into account.,,,,,,,,,,,,,,,,,,,12/May/08 22:34;pkamath;pig-236.patch;https://issues.apache.org/jira/secure/attachment/12381913/pig-236.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-05-09 00:24:00.389,,,no_permission,,,,,,,,,,,,163882,,,,,Tue May 13 16:36:26 UTC 2008,,,,,,,0|i0ghrr:,94309,,,,,,,,,,"09/May/08 00:24;pkamath;Currently the code is not looking at the ""System properties"" which include the command line ""-D"" overrides and hence the issue reported in this bug. Here is the plan to address this:

First the pig.properties (and currently .pigrc - from comments in the code, there seems to be a plan to not support .pigrc in the future) file(s) will be read into the ""properties"" object. Then the ""System properties"" (from System.getProperties()) will be added into the properties object overriding any existing key/values.

A repercussion of this is that any  default java or system properties (like java.io.tmpdir) that a user wants to override should be supplied in the commandline and NOT in pig.properties or .pigrc
","09/May/08 01:13;pi_song;+1 
This will increase usability","10/May/08 00:49;pkamath;In the code, there were several changes made:
1) In Main the earlier code was setting JOBTRACKER_LOCATION prematurely to ""local"" in the properties - this has been removed since this should be determined from hadoop-site.xml and then overriden by pig.properties - see the 3) below

2) Changed PropertiesUtil.java to also include properties from System including command line overrides

3) In HEexecutionEngine.java, for the non HOD case, the earlier code was only reading ""properties"" object (built from pig.properties/.pigrc and System.getProperties() which includes command line overrides). Now it first reads hadoop-site.xml and then adds on (over-writing existing keys) properties from the ""properties"" object. Towards this also added a utility function in ConfigurationUtil.java
",10/May/08 00:51;pkamath;Patch file - details in comments section,"11/May/08 01:17;pi_song;Overall patch looks good except that I don't understand why you look up hadoop-site.xml in this location.

{noformat}
configuration.addResource(""/hadoop-site.xml"");
{noformat}","11/May/08 01:31;pkamath;Hadoop looks for the named file in configuration.AddResource() in the classpath. So by specifying ""/hadoop-site.xml"" as the argument, we are letting the configuration.AddResource() pick it up from the first location in the classpath where such a file is present.","11/May/08 01:44;pkamath;Using ""hadoop-site.xml"" in configuration.addResource(); in HExecutionEngine.init(Properties) instead of ""/hadoop-site.xml""","11/May/08 01:45;pkamath;""/hadoop-site.xml"" was confusing - so I changed it to ""hadoop-site.xml"" as indicated in previous comment and uploaded a new patch with this change.","11/May/08 01:50;pi_song;+1

We have two semantics here.
1) Some configuration files have to be under {pig.home}/conf
2) Some configuration files have to be under classpath.

Possibly we should also fix the shell script to add conf in the classpath as well so that we can say ""put everything under conf""","11/May/08 22:30;pkamath;The Configuration constructor by default handles hadoop-default.xml and hadoop-site.xml. 

{code}

  public Configuration() {
    if (LOG.isDebugEnabled()) {
      LOG.debug(StringUtils.stringifyException(new IOException(""config()"")));
    }
    resources.add(""hadoop-default.xml"");
    resources.add(""hadoop-site.xml"");
  }

{code}


Attaching a new patch to make use of this. The logic is the same as before, only with fewer changes to use the above fact.","12/May/08 04:16;amirhyoussefi;I thinks it's time we have a Wiki page for different configuration files and how they override each other: 

http://wiki.apache.org/pig/

or 

http://wiki.apache.org/pig/FAQ",12/May/08 12:57;olgan;I don't think we need to pick up config from hadoop-default. That suppose to be used by the backend only.,"12/May/08 13:10;pi_song;Like Pradeep mentioned, trying to read both ""hadoop-default.xml"" and ""hadoop-site.xml"" from classpath is a default behavior of Hadoop Configuration so we just have to live with it.","12/May/08 16:00;alangates;These changes should be tested against a static cluster, against a cluster that uses HOD, and against a cluster that has been allocated with HOD.  Pradeep, have you tested all three of those scenarios?","12/May/08 16:05;olgan;yes, pradeep and I tested all 3 of these cases together.",12/May/08 17:11;alangates;Testing found that the fix did not work in the case where HOD was being used to allocate clusters.  Pradeep will take a look at fixing this.,"12/May/08 22:34;pkamath;The issue with the earlier patch was that the following code was moved up only into the non-HOD case in the ""if-else""
{code}
configuration = ConfigurationUtil.toConfiguration(properties);
{code}

Now this call is made again after the if-else in the snippet below - the comment
explains the code in the snippet:
{code}

        ds = new HDataStorage(properties);
                
        // The above HDataStorage constructor sets DEFAULT_REPLICATION_FACTOR_KEY in properties.
        // So we need to reconstruct the configuration object for the non HOD case
        // In the HOD case, this is the first time the configuration object will be created
        configuration = ConfigurationUtil.toConfiguration(properties);	

{code} 
","13/May/08 16:36;alangates;Fix checked in at revision 655940.  Tested this patch against static cluster, cluster using hod, and cluster allocated via hod.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance issues with memory spills.,PIG-235,12395667,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,utkarsh,amirhyoussefi,amirhyoussefi,08/May/08 22:30,24/Mar/10 22:01,14/Mar/19 03:05,26/Jun/08 19:03,,,,,,,0.1.0,,,,,0,,,,"We have are hitting low performance issue with memory spills.

A reducer gets stuck in following state for *tens* of hours while
thousands of small files are spilled. This is besides skewed keys issue.

Note that size of spills become smaller as times goes. We can use this
and try to address the issue by spilling in larger chunks.

I tried different sub-set of data. Then made it work with all kind of tricks/hacks but would like to have this working easily to say the least: 

cogroup large_data_set by $0, small_date_set by $0


small_date_set fits in memory.

Log:
2008-05-06 23:24:06,014 INFO
org.apache.pig.impl.util.SpillableMemoryManager: low memory handler
called init = 4194304(4096K) used = 251352480(245461K) committed =
437256192(427008K) max = 477233152(466048K)
2008-05-06 23:24:06,734 INFO
org.apache.pig.impl.util.SpillableMemoryManager: low memory handler
called init = 4194304(4096K) used = 251401304(245509K) committed =
437256192(427008K) max = 477233152(466048K)
2008-05-06 23:24:07,455 INFO
org.apache.pig.impl.util.SpillableMemoryManager: low memory handler
called init = 4194304(4096K) used = 251254912(245366K) committed =
437256192(427008K) max = 477233152(466048K)
2008-05-06 23:24:08,175 INFO
org.apache.pig.impl.util.SpillableMemoryManager: low memory handler
called init = 4194304(4096K) used = 251281808(245392K) committed =
437256192(427008K) max = 477233152(466048K)
2008-05-06 23:24:08,895 INFO
org.apache.pig.impl.util.SpillableMemoryManager: low memory handler
called init = 4194304(4096K) used = 251309400(245419K) committed =
437256192(427008K) max = 477233152(466048K)
2008-05-06 23:24:09,615 INFO
org.apache.pig.impl.util.SpillableMemoryManager: low memory handler
called init = 4194304(4096K) used = 251358232(245467K) committed =
437256192(427008K) max = 477233152(466048K)
2008-05-06 23:24:10,336 INFO
org.apache.pig.impl.util.SpillableMemoryManager: low memory handler
called init = 4194304(4096K) used = 251267696(245378K) committed =
437256192(427008K) max = 477233152(466048K)
2008-05-06 23:24:11,056 INFO
org.apache.pig.impl.util.SpillableMemoryManager: low memory handler
called init = 4194304(4096K) used = 251307352(245417K) committed =
437256192(427008K) max = 477233152(466048K)
2008-05-06 23:24:11,776 INFO
org.apache.pig.impl.util.SpillableMemoryManager: low memory handler
called init = 4194304(4096K) used = 251335344(245444K) committed =
437256192(427008K) max = 477233152(466048K)
(used column slowly increasing)
Actual spill example:
 8000 files 
Sorting by time (new to old) I see small spills:
-rw-------  1 amiry users     3675 May  6 23:44 pigbag635090.tmp
-rw-------  1 amiry users        4 May  6 23:44 pigbag635091.tmp
-rw-------  1 amiry users     3917 May  6 23:44 pigbag635086.tmp
-rw-------  1 amiry users     3949 May  6 23:44 pigbag635088.tmp
-rw-------  1 amiry users        4 May  6 23:44 pigbag635089.tmp
-rw-------  1 amiry users     3969 May  6 23:44 pigbag635084.tmp
-rw-------  1 amiry users     4073 May  6 23:44 pigbag635068.tmp
-rw-------  1 amiry users    47634 May  6 23:44 pigbag635070.tmp
-rw-------  1 amiry users     5101 May  6 23:44 pigbag635065.tmp
-rw-------  1 amiry users     5722 May  6 23:44 pigbag635059.tmp
-rw-------  1 amiry users     7570 May  6 23:44 pigbag635062.tmp
-rw-------  1 amiry users     7802 May  6 23:44 pigbag635056.tmp
-rw-------  1 amiry users     7514 May  6 23:44 pigbag635051.tmp
-rw-------  1 amiry users     3929 May  6 23:44 pigbag635054.tmp
-rw-------  1 amiry users     5342 May  6 23:44 pigbag635045.tmp
-rw-------  1 amiry users     7361 May  6 23:44 pigbag635048.tmp
-rw-------  1 amiry users     6663 May  6 23:44 pigbag635042.tmp
-rw-------  1 amiry users     7511 May  6 23:44 pigbag635036.tmp
-rw-------  1 amiry users     7520 May  6 23:44 pigbag635039.tmp
-rw-------  1 amiry users     3873 May  6 23:44 pigbag635034.tmp
-rw-------  1 amiry users     4029 May  6 23:44 pigbag635032.tmp
-rw-------  1 amiry users     3823 May  6 23:44 pigbag635028.tmp
-rw-------  1 amiry users     3726 May  6 23:44 pigbag635030.tmp
-rw-------  1 amiry users     3934 May  6 23:44 pigbag635024.tmp

Sorting by time (old to new) I see a few large spills then quickly (in
less than 15min) come small ones:

-rw-------  1 amiry users 45221453 May  6 21:23 pigbag59657.tmp
-rw-------  1 amiry users 56161613 May  6 21:23 pigbag59658.tmp
-rw-------  1 amiry users 70661942 May  6 21:23 pigbag59659.tmp
-rw-------  1 amiry users 75308107 May  6 21:23 pigbag59660.tmp
-rw-------  1 amiry users 76381091 May  6 21:24 pigbag59661.tmp
-rw-------  1 amiry users 74691366 May  6 21:24 pigbag81914.tmp
-rw-------  1 amiry users 73133098 May  6 21:24 pigbag103839.tmp
-rw-------  1 amiry users 72750330 May  6 21:24 pigbag125123.tmp
-rw-------  1 amiry users 71267460 May  6 21:25 pigbag146472.tmp
-rw-------  1 amiry users 69638363 May  6 21:25 pigbag167358.tmp
-rw-------  1 amiry users 68010250 May  6 21:25 pigbag187566.tmp
-rw-------  1 amiry users 66312739 May  6 21:25 pigbag207447.tmp
-rw-------  1 amiry users 64601422 May  6 21:26 pigbag226895.tmp
-rw-------  1 amiry users 62997501 May  6 21:26 pigbag245690.tmp
-rw-------  1 amiry users 62525926 May  6 21:26 pigbag264154.tmp
-rw-------  1 amiry users 60940107 May  6 21:26 pigbag282367.tmp
-rw-------  1 amiry users 59540198 May  6 21:26 pigbag300215.tmp
-rw-------  1 amiry users 57918140 May  6 21:27 pigbag317750.tmp
-rw-------  1 amiry users 57728845 May  6 21:27 pigbag334505.tmp
-rw-------  1 amiry users 55427771 May  6 21:27 pigbag351436.tmp
-rw-------  1 amiry users 55405942 May  6 21:27 pigbag367615.tmp
-rw-------  1 amiry users 54600778 May  6 21:28 pigbag383872.tmp
-rw-------  1 amiry users 52438311 May  6 21:28 pigbag399722.tmp
-rw-------  1 amiry users 51250459 May  6 21:28 pigbag415094.tmp
-rw-------  1 amiry users 50489324 May  6 21:28 pigbag430026.tmp
-rw-------  1 amiry users 48311361 May  6 21:28 pigbag444835.tmp
-rw-------  1 amiry users 47296555 May  6 21:28 pigbag458869.tmp
-rw-------  1 amiry users 45703372 May  6 21:29 pigbag472771.tmp
-rw-------  1 amiry users 46243949 May  6 21:29 pigbag486062.tmp
-rw-------  1 amiry users 46195603 May  6 21:29 pigbag499549.tmp
-rw-------  1 amiry users 43916731 May  6 21:29 pigbag513154.tmp
-rw-------  1 amiry users 42970027 May  6 21:29 pigbag525921.tmp
-rw-------  1 amiry users     5965 May  6 21:29 pigbag538555.tmp
-rw-------  1 amiry users 46288099 May  6 21:30 pigbag538558.tmp
-rw-------  1 amiry users     7735 May  6 21:30 pigbag539078.tmp
-rw-------  1 amiry users     8058 May  6 21:30 pigbag539075.tmp
-rw-------  1 amiry users 34034358 May  6 21:30 pigbag539079.tmp
-rw-------  1 amiry users     8800 May  6 21:30 pigbag549021.tmp
-rw-------  1 amiry users 45054789 May  6 21:31 pigbag549025.tmp
-rw-------  1 amiry users     5750 May  6 21:31 pigbag549950.tmp
-rw-------  1 amiry users     6794 May  6 21:31 pigbag549953.tmp
-rw-------  1 amiry users 35112392 May  6 21:31 pigbag549956.tmp
-rw-------  1 amiry users     7330 May  6 21:31 pigbag560146.tmp
-rw-------  1 amiry users     7856 May  6 21:31 pigbag560143.tmp
-rw-------  1 amiry users 44039882 May  6 21:32 pigbag560149.tmp
-rw-------  1 amiry users     7508 May  6 21:32 pigbag561917.tmp
-rw-------  1 amiry users     4567 May  6 21:32 pigbag561921.tmp
-rw-------  1 amiry users 33031655 May  6 21:32 pigbag561924.tmp
-rw-------  1 amiry users     7744 May  6 21:32 pigbag571510.tmp
-rw-------  1 amiry users     7709 May  6 21:32 pigbag571507.tmp
-rw-------  1 amiry users 42771860 May  6 21:33 pigbag571513.tmp
-rw-------  1 amiry users 29912726 May  6 21:33 pigbag572784.tmp
-rw-------  1 amiry users 42563305 May  6 21:34 pigbag581561.tmp
-rw-------  1 amiry users 29961190 May  6 21:34 pigbag583024.tmp
-rw-------  1 amiry users     7704 May  6 21:34 pigbag591783.tmp
-rw-------  1 amiry users 41448656 May  6 21:35 pigbag591786.tmp
-rw-------  1 amiry users     7679 May  6 21:35 pigbag592482.tmp
-rw-------  1 amiry users     7328 May  6 21:35 pigbag592488.tmp
-rw-------  1 amiry users     7809 May  6 21:35 pigbag592485.tmp
-rw-------  1 amiry users   128909 May  6 21:35 pigbag592491.tmp
-rw-------  1 amiry users 33069409 May  6 21:35 pigbag592530.tmp
-rw-------  1 amiry users     7004 May  6 21:35 pigbag602114.tmp
-rw-------  1 amiry users     4083 May  6 21:35 pigbag602121.tmp
-rw-------  1 amiry users    11533 May  6 21:35 pigbag602117.tmp
-rw-------  1 amiry users     7603 May  6 21:35 pigbag602124.tmp
-rw-------  1 amiry users     7611 May  6 21:35 pigbag602130.tmp
-rw-------  1 amiry users     7692 May  6 21:35 pigbag602127.tmp
-rw-------  1 amiry users     7821 May  6 21:35 pigbag602136.tmp
-rw-------  1 amiry users     7467 May  6 21:35 pigbag602133.tmp
-rw-------  1 amiry users     1147 May  6 21:35 pigbag602139.tmp
-rw-------  1 amiry users     8543 May  6 21:35 pigbag602144.tmp
-rw-------  1 amiry users     8517 May  6 21:35 pigbag602141.tmp
-rw-------  1 amiry users     3893 May  6 21:35 pigbag602150.tmp
-rw-------  1 amiry users     6029 May  6 21:35 pigbag602147.tmp
-rw-------  1 amiry users     7687 May  6 21:35 pigbag602152.tmp
-rw-------  1 amiry users     7411 May  6 21:35 pigbag602158.tmp
-rw-------  1 amiry users     6079 May  6 21:35 pigbag602155.tmp
-rw-------  1 amiry users     6964 May  6 21:35 pigbag602161.tmp
-rw-------  1 amiry users     5404 May  6 21:35 pigbag602168.tmp
-rw-------  1 amiry users     7915 May  6 21:35 pigbag602164.tmp
-rw-------  1 amiry users     4132 May  6 21:35 pigbag602174.tmp
-rw-------  1 amiry users     7708 May  6 21:35 pigbag602171.tmp
-rw-------  1 amiry users     7802 May  6 21:35 pigbag602176.tmp
-rw-------  1 amiry users     6089 May  6 21:35 pigbag602182.tmp
-rw-------  1 amiry users     8161 May  6 21:35 pigbag602179.tmp
-rw-------  1 amiry users     7356 May  6 21:35 pigbag602188.tmp
-rw-------  1 amiry users     8231 May  6 21:35 pigbag602185.tmp
-rw-------  1 amiry users     8340 May  6 21:35 pigbag602191.tmp
-rw-------  1 amiry users     3870 May  6 21:35 pigbag602198.tmp
-rw-------  1 amiry users     7587 May  6 21:35 pigbag602195.tmp
-rw-------  1 amiry users     5884 May  6 21:35 pigbag602200.tmp
-rw-------  1 amiry users     7644 May  6 21:35 pigbag602206.tmp
-rw-------  1 amiry users     7756 May  6 21:35 pigbag602203.tmp
-rw-------  1 amiry users     8054 May  6 21:35 pigbag602209.tmp
-rw-------  1 amiry users     6182 May  6 21:35 pigbag602215.tmp
-rw-------  1 amiry users     7740 May  6 21:35 pigbag602212.tmp
-rw-------  1 amiry users     6264 May  6 21:35 pigbag602222.tmp
-rw-------  1 amiry users     7709 May  6 21:35 pigbag602218.tmp
-rw-------  1 amiry users    10195 May  6 21:35 pigbag602225.tmp
-rw-------  1 amiry users     7914 May  6 21:35 pigbag602232.tmp
-rw-------  1 amiry users     4170 May  6 21:35 pigbag602229.tmp
-rw-------  1 amiry users     7670 May  6 21:35 pigbag602235.tmp
-rw-------  1 amiry users     2823 May  6 21:35 pigbag602241.tmp
-rw-------  1 amiry users     6947 May  6 21:35 pigbag602238.tmp
-rw-------  1 amiry users     7311 May  6 21:35 pigbag602246.tmp
-rw-------  1 amiry users     7521 May  6 21:35 pigbag602243.tmp
-rw-------  1 amiry users     7772 May  6 21:35 pigbag602249.tmp
-rw-------  1 amiry users     5842 May  6 21:35 pigbag602255.tmp
-rw-------  1 amiry users     7689 May  6 21:35 pigbag602252.tmp
-rw-------  1 amiry users     5399 May  6 21:35 pigbag602261.tmp
-rw-------  1 amiry users     5951 May  6 21:35 pigbag602258.tmp
-rw-------  1 amiry users     7717 May  6 21:35 pigbag602264.tmp
-rw-------  1 amiry users     4911 May  6 21:35 pigbag602271.tmp
-rw-------  1 amiry users    11384 May  6 21:35 pigbag602267.tmp
-rw-------  1 amiry users     6714 May  6 21:35 pigbag602277.tmp
-rw-------  1 amiry users     6639 May  6 21:35 pigbag602274.tmp
-rw-------  1 amiry users     7807 May  6 21:35 pigbag602280.tmp
-rw-------  1 amiry users     6754 May  6 21:35 pigbag602286.tmp
-rw-------  1 amiry users     6159 May  6 21:35 pigbag602283.tmp
-rw-------  1 amiry users     7643 May  6 21:35 pigbag602292.tmp
-rw-------  1 amiry users     7636 May  6 21:35 pigbag602289.tmp
-rw-------  1 amiry users     6007 May  6 21:35 pigbag602295.tmp
-rw-------  1 amiry users     7245 May  6 21:35 pigbag602301.tmp
-rw-------  1 amiry users     7742 May  6 21:35 pigbag602298.tmp
-rw-------  1 amiry users     1994 May  6 21:35 pigbag602304.tmp
-rw-------  1 amiry users     8080 May  6 21:35 pigbag602309.tmp
-rw-------  1 amiry users     8030 May  6 21:35 pigbag602306.tmp
-rw-------  1 amiry users     6009 May  6 21:35 pigbag602315.tmp
-rw-------  1 amiry users     5019 May  6 21:35 pigbag602312.tmp
-rw-------  1 amiry users     3746 May  6 21:35 pigbag602318.tmp
-rw-------  1 amiry users     5977 May  6 21:35 pigbag602323.tmp
",Pig + Hadoop,,,,,,,,,,,,,,,,,,11/Jun/08 21:50;pkamath;gcoverhead.patch;https://issues.apache.org/jira/secure/attachment/12383871/gcoverhead.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-05-08 22:34:28.748,,,no_permission,,,,,,,,,,,,163881,,,,,Thu Jun 26 19:03:53 UTC 2008,,,,,,,0|i0ghrb:,94307,,,,,,,,,,08/May/08 22:33;amirhyoussefi;Utkarash and I had a chat on it and will try to release Bags which are not needed. Theoretically all we need for specific case of  L + S (memory) is having S in mem and streaming L through and  abandoning rows not needed anymore.,08/May/08 22:34;alangates;https://issues.apache.org/jira/browse/PIG-176 (already committed to trunk) should address this issue.  Have you tried it with a version of pig that includes that patch?,08/May/08 23:09;pi_song;That feature is optional. You also have to turn it on. Have a look in pig.properties,09/May/08 00:49;olgan;The feature that Alan is referring to has not been released to gateways yet. We are planning to put it on latest in the next couple of days.,"15/May/08 14:31;pi_song;Amir,

If it works, there is another experimental option called ""pig.spill.gc.activation.size"" . Could you please also try that out? If it is good, we may make it a default setting.",11/Jun/08 20:41;olgan;I think this issue will be addressed by the changes Pradeep is making,"11/Jun/08 21:50;pkamath;Proposal for memory allocation overhead issue:
==============================================

Currently in org.apache.pig.impl.util.SpillableMemoryManger:

1) We use MemoryManagement interface to get notified when the ""collection threshold"" exceeds a limit (we set this to biggest_heap*0.5). With this in place we are still seeing ""GC overhead limit"" issues when trying large dataset operations. Observing some runs, it looks like the notification is not frequent enough and early enough to prevent memory issues possibly because this notification only occurs after GC.

2) We only attempt to free upto :
long toFree = info.getUsage().getUsed() - (long)(info.getUsage().getMax()*.5);
This is only the excess amount over the threshold which caused the notification and is not sufficient to not be called again soon.

3) While iterating over spillables, if current spillable's memory size is > gcActivationSize, we try to invoke System.gc

4) We *always* invoke System.gc() after iterating over spillables


Proposed changes are:
=====================
1) In addition to ""collection threshold"" of biggest_heap*0.5, a ""usage threshold"" of biggest_heap*0.7 will be used so we get notified early and often irrespective of whether garbage collection has occured.

2) We will attempt to free 
toFree = info.getUsage().getUsed() - threshold + (long)(threshold * 0.5); where threshold is (info.getUsage().getMax() * 0.7) if the handleNotification() method is handling a ""usage threshold exceeded"" notification and (info.getUsage().getMax() * 0.5) otherwise (""collection threshold exceeded"" case)

3) While iterating over spillables, if the *accumulated memory freed thus far* is > gcActivationSize OR if we have freed sufficient memory (based on 2) above), then we set a flag to invoke System.gc when we exit the loop.  Acummulated Free memory is the memory freed across spills and between GC invocations.

4) We will invoke System.gc() only if the flag is set in 3) above",11/Jun/08 21:51;pkamath;Please review the attached patch and give comments/thoughts. This is currently being tested and at this point will not be committed till further testing/validation.,"12/Jun/08 11:33;pi_song;I buy your idea.
Please give us some impressive test results","12/Jun/08 16:53;pkamath;Test Results:

|| Number of node || Heap size (-Xmx) || Number of rows in data || Script used || New code time || Old Code time ||
| 9 | 512m | 200 million | a = load '/user/pig/tests/data/singlefile/studenttab200m'; b = group a all; c = foreach b generate COUNT(a.$0); store c into '/tmp/pig/bigdata_out'; | 1hrs, 18mins, 23sec | 4hrs, 45mins, 17sec (In another run this took 8 hrs, 26mins, 28 secs [3 reduce attemtpts - 1st attempt had GC overhead limit exceeded error., 2nd attempt had hadoop issues (""Lost task tracker""), 3rd attempt succeeded ] |
| 9 | 512m | 20 million | a = load '/user/pig/tests/data/singlefile/studenttab20m'; b = group a all; c = foreach b generate COUNT(a.$0); store c into '/tmp/pig/meddata_out'; | 6mins, 43sec | 37mins, 43sec with GC overhead limit exception in 1st attempt |
| 9 | 512m | 200 million | a = load '/user/pig/tests/data/singlefile/studenttab200m'; b = group a by $0; c = foreach b generate COUNT(a.$0), group; store c into '/tmp/pig/bigdata_complex_out'; | 1hrs, 8mins, 29sec | 1hrs, 8mins, 51sec |
",12/Jun/08 17:50;alangates;These results look good.  Massive speedup in the case we were targeting (very large groups) because of no failures.  No real change in cases we weren't targeting (smaller groups).,"12/Jun/08 23:21;pi_song;There are two parts:-
- In normal condition, it's faster because we call System.gc() less often.
- In low memory condition, we manage it better.

Looks really good.",26/Jun/08 19:03;olgan;I have reviewed the patch and ran tests. All looks good. Patch committed. Thanks Pradeep for contributing!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix synchronization around staleCount in DataCollector,PIG-234,12395560,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,whipkey,whipkey,whipkey,07/May/08 21:01,24/Mar/10 22:01,14/Mar/19 03:05,08/May/08 23:04,,,,,,,0.1.0,,impl,,,0,,,,"
DataCollector uses synchronized statements on staleCount, but the staleCount reference changes!  I'm proposing it switch to use the concurrent package Lock and condition to manage staleness.",,,,,,,,,,,,,,,,,,,07/May/08 21:11;whipkey;Change_synchronization_on_DataCollector.patch;https://issues.apache.org/jira/secure/attachment/12381631/Change_synchronization_on_DataCollector.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-05-08 04:50:38.69,,,no_permission,,,,,,,,,,,,38546,,,,,Thu May 08 23:04:06 UTC 2008,,,Patch Available,,,,0|i0ghqv:,94305,,,,,,,,,,07/May/08 21:11;whipkey;Please code review.  This is my first patch ever so let me know if I should do things differently.,"08/May/08 04:50;pi_song;Good eyes!!! Another bug from auto-boxing!!!

However:-
1. Currently we only run local execution engine in single-threaded mode so this buggy synchronization code doesn't cause any problem.
2. We are removing all of those spec classes as we are moving to the new pipeline design. Please have a look at PigType trunk. We need more people like you to help!!!","08/May/08 22:29;whipkey;Okay, if you are not going to be using this code then I think it's okay to close this bug.  I will look at the PigType trunk in the future before making submissions.  Thanks!",08/May/08 23:04;olgan;This code will no longer be used after execution pipeline rework is completed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Number of input/output rows in the logs is invalid with BinaryStorage,PIG-232,12395466,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,acmurthy,olgan,olgan,07/May/08 00:33,24/Mar/10 22:01,14/Mar/19 03:05,09/May/08 16:19,,,,,,,0.1.0,,,,,0,,,,"My pig script:

define CMD `perl PigStreamingBad.pl end` ship('PigStreamingBad.pl') stderr('CMD' limit 1);
A = load 'studenttab10k';
B = stream A through CMD;
store B into 'out';

My perl script:

use strict;

# This script is used to test streaming error cases in pig.
# Usage: PigStreaming.pl <start|middle|end>
# the parameter tells the application when to exit with error

if ($#ARGV < 0)
{
        print STDERR ""Usage PigStreaming.pl <start|middle|end>\n"";
        exit (-1);
}

my $pos = $ARGV[0];

if ($pos eq ""start"")
{
        print STDERR ""Failed in the beginning of the processing\n"";
        exit(1);
}


print STDERR ""PigStreamingBad.pl: starting processing\n"";

my $cnt = 0;
while (<STDIN>)
{
        print ""$_"";
        $cnt++;
        print STDERR ""PigStreaming.pl: processing $_\n"";
        if (($cnt > 100) && ($pos eq ""middle""))
        {
                print STDERR ""Failed in the middle of processing\n"";
                exit(2);
        }
}

print STDERR ""Failed at the end of processing\n"";
exit(3);",,,,,,,,,,,,,,,,,,,07/May/08 23:22;acmurthy;PIG-232_0_20080507.patch;https://issues.apache.org/jira/secure/attachment/12381644/PIG-232_0_20080507.patch,08/May/08 00:45;acmurthy;PIG-232_1_20080507.patch;https://issues.apache.org/jira/secure/attachment/12381649/PIG-232_1_20080507.patch,09/May/08 00:31;acmurthy;PIG-232_2_20080508.patch;https://issues.apache.org/jira/secure/attachment/12381729/PIG-232_2_20080508.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2008-05-07 08:24:32.821,,,no_permission,,,,,,,,,,,,163879,,,,,Fri May 09 16:19:10 UTC 2008,,,,,,,0|i0ghpz:,94301,,,,,,,,,,07/May/08 00:41;olgan;I see the same behavior with valid queries as well,"07/May/08 08:24;acmurthy;Olga, this is due to the fact that the stream/store optimization is kicking in and hence only the 'binary tuples' are being reported... could you please try by switching off the optimization?

/pig/studenttab10k has 10,000 records. 

Now:
{noformat}
IP = load '/pig/studenttab10k';
OP = stream IP through `perl -ne 'print $_;'`; 
store OP into '/pig/out' using PigStorage(',');
{noformat}

correctly shows 10,000 as the no. of output-records while:

{noformat}
IP = load '/pig/studenttab10k';
OP = stream IP through `perl -ne 'print $_;'`; 
store OP into '/pig/out';
{noformat}

shows the no. of output-records as 4 due to the stream/store optimization.

Could you please re-check? Thanks!","07/May/08 17:40;olgan;Ok, when this happen, can we report ""not known"" or some such thing rather than giving an invalid value?",07/May/08 20:50;acmurthy;Patch to not show the #output-records when BinaryStorage is being used...,"07/May/08 23:21;acmurthy;Patch isn't correct, needs to be fixed.","07/May/08 23:22;acmurthy;Better, simpler, smaller patch...",08/May/08 00:37;olgan;I committed the patch. It fixes the number of the output rows. The same issue needs to be fixed for input rows as well,08/May/08 00:45;acmurthy;Patch to fix input-records as well...,08/May/08 01:04;olgan;second patch committed as well,08/May/08 23:08;olgan;This patch broke cache statement - it know always claims that it is invalid. This is because #name is not stripped.,"09/May/08 00:31;acmurthy;Patch to take care of ""#"" in the cache-spec; also added a test-case for cache specs.","09/May/08 16:19;olgan;patch committed. thanks, arun",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
validation of files in ship/cache specs,PIG-231,12395454,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,acmurthy,olgan,olgan,06/May/08 20:00,24/Mar/10 22:01,14/Mar/19 03:05,07/May/08 20:00,,,,,,,0.1.0,,,,,0,,,,"Currently the code fails after map reduce job starts if files for ship/cache don't exist.

We should be able to detect that on the client side.

For ship, make sure that the file(s) to be shipped exist on the client
For cache, make sure it exists on the server.",,,,,,,,,,,,,,,,,,,07/May/08 08:16;acmurthy;PIG-231_0_20080507.patch;https://issues.apache.org/jira/secure/attachment/12381571/PIG-231_0_20080507.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-05-06 22:38:48.379,,,no_permission,,,,,,,,,,,,163878,,,,,Wed May 07 20:00:28 UTC 2008,,,,,,,0|i0ghpj:,94299,,,,,,,,,,06/May/08 22:30;olgan;Also skippath is not validated.,"06/May/08 22:38;pi_song;I guess Arun is gonna implement this.

Could you please separate this new validation logic into a new class?

We have a place to keep all of these post-validation logics in post-parsing stage in PigType branch. That is we have to migrate this to the new structure when we start merging two branches.","07/May/08 08:16;acmurthy;Patch to validate ship/cache specs and stream.skippath.

Pi, I've had to split the validation between StreamingCommand and GruntParser and can't reuse any existing class since ValidatingInputSpec was the only option, but ship/cache specs aren't really FileSpecs... I'd be happy to fix this once your branch becomes mainstream. Does that work? Thanks!","07/May/08 15:08;pi_song;OK.

Patch also looks good.","07/May/08 17:39;olgan;Arun, a couple of questions/comments:

(1) For skip path, you need to also check that it is a directory. If this is the only change needed, I can make it.
(2) It was not clear to me why you could not validate everything in the grunt parser. You should be able to find if file exists on dfs. We already do it for dfs operations such as ls.","07/May/08 18:07;olgan;Ok, I think I figured it out. The patch is fine.","07/May/08 20:00;olgan;changes committed, thanks arun",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
auto ship broken is presence of define for another streaming operator.,PIG-230,12395344,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,acmurthy,olgan,olgan,05/May/08 18:46,24/Mar/10 22:01,14/Mar/19 03:05,09/May/08 16:19,,,,,,,0.1.0,,,,,0,,,,"The following works:

A = load 'studenttab10k';
B = foreach A generate $0, $1, $2;
C = stream B through `perl PigStreaming.pl`;
store C into 'out';

But this one causes an error: Can't open perl script ""PigStreaming.pl"": No such file or directory.

define CMD `perl PigStreamingDep.pl` input(stdin using PigDump) ship(':SCRIPTHOMEPATH:/PigStreamingDep.pl', ':SCRIPTHOMEPATH:/PigStreamingModule.pm');
A = load 'studenttab10k';
B = stream A through `perl PigStreaming.pl`;
C = stream B through CMD as (name, age, gpa);
D = foreach C generate name, age;
store D into 'out';#,",,,,,,,,,,,,,,,,,,,06/May/08 03:10;acmurthy;PIG-230_0_20080505.patch;https://issues.apache.org/jira/secure/attachment/12381474/PIG-230_0_20080505.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-05-05 19:14:18.583,,,no_permission,,,,,,,,,,,,163877,,,,,Fri May 09 16:19:42 UTC 2008,,,,,,,0|i0ghp3:,94297,,,,,,,,,,"05/May/08 19:14;acmurthy;Olga, was PigStreaming.pl on the PATH? Thanks.","05/May/08 20:52;olgan;yes, it was. Both scripts ran in identical environment.",06/May/08 03:10;acmurthy;Fixed POMapreduce to handle multiple ship/cache specs; updated TestStreaming to add this as a test-case ...,"06/May/08 18:19;olgan;Arun, I have committed the patch, thanks. All my tests that used to fail now passing. If you could add a unit test for multiple cache options in addtion to multiple ship options, it would be great.

You could just attache it to this bug. (I will keep it open till then.)",09/May/08 16:19;olgan;latest patch for PIG-232 adds the test case,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig Streaming isn't resilient to errors in the streaming process's stdout/stderr or in LoadFuncs used to deserialize them.,PIG-229,12395241,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,acmurthy,acmurthy,acmurthy,03/May/08 01:34,24/Mar/10 22:01,14/Mar/19 03:05,03/May/08 21:47,,,,,,,0.1.0,,impl,,,0,,,,Jobs do not fail correctly when there are errors in the streaming process's stdout/stderr or bugs in the LoadFuncs being used to deserialize them.,,,,,,,,,,,,,,,,,,,03/May/08 01:56;acmurthy;PIG-229_0_20080502.patch;https://issues.apache.org/jira/secure/attachment/12381363/PIG-229_0_20080502.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-05-03 21:47:20.631,,,no_permission,,,,,,,,,,,,163876,,,,,Sat May 03 21:47:20 UTC 2008,,,,,,,0|i0ghon:,94295,,,,,,,,,,03/May/08 01:56;acmurthy;Candidate patch which fixes the problem by communicating the exception to the 'Main' thread; also contains necessary fixes to DefaultOutputHandler.close and BufferedPostionedInputStream.close to ensure the correctly close the underlying streams.,"03/May/08 02:11;acmurthy;Submitting patch, passes all unit tests locally.","03/May/08 21:47;olgan;patched tested and committed. Thanks, Arun!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
names for secondary streaming outputs don't match the spec,PIG-228,12395206,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,acmurthy,olgan,olgan,02/May/08 16:37,24/Mar/10 22:01,14/Mar/19 03:05,02/May/08 20:06,,,,,,,0.1.0,,,,,0,,,,"we currently prepend the task id to the name which can be confusing to the users especially in the presenese of multiple outputs.

the proposal is to create a directory that matches the output name and have part files within that (similarly to how we treat regular outputs.)",,,,,,,,,,,,,,,,,,,02/May/08 19:21;acmurthy;PIG-228_1_20080502.patch;https://issues.apache.org/jira/secure/attachment/12381337/PIG-228_1_20080502.patch,02/May/08 16:37;olgan;secondary_outputs.patch;https://issues.apache.org/jira/secure/attachment/12381321/secondary_outputs.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-05-02 19:21:01.326,,,no_permission,,,,,,,,,,,,163875,,,,,Fri May 02 20:06:52 UTC 2008,,,,,,,0|i0gho7:,94293,,,,,,,,,,02/May/08 16:37;olgan;Patch supplied by arun,02/May/08 17:02;olgan;verified the patch. will commit once svn is back,02/May/08 19:21;acmurthy;Patch to fix the secondary outputs.,"02/May/08 20:06;olgan;patch committed, thanks arun.",02/May/08 20:06;olgan;second patch also committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minor irritants in parsing input/output specs for streaming command in the define clause,PIG-227,12395173,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,acmurthy,acmurthy,acmurthy,02/May/08 01:51,24/Mar/10 22:01,14/Mar/19 03:05,02/May/08 21:45,,,,,,,0.1.0,,impl,,,0,,,,"Currently the following throw syntax errors:

1. define CMD `blah` input(stdin using PigStorage)
 -> Expects *PigStorage()*

2. define CMD `blah` input(stdin)
 -> Expects a 'using' clause.


Should be straight-forward fix to the parser...",,,,,,,,,,,,,,,,,,,02/May/08 04:49;acmurthy;PIG-227_0_20080501.patch;https://issues.apache.org/jira/secure/attachment/12381299/PIG-227_0_20080501.patch,02/May/08 21:02;acmurthy;PIG-227_1_20080502.patch;https://issues.apache.org/jira/secure/attachment/12381342/PIG-227_1_20080502.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-05-02 21:45:47.563,,,no_permission,,,,,,,,,,,,163874,,,,,Fri May 02 21:45:47 UTC 2008,,,,,,,0|i0ghnr:,94291,,,,,,,,,,02/May/08 04:49;acmurthy;Straight-forward fix to the QueryParser to handle the aforementioned cases...,"02/May/08 06:30;acmurthy;Marking it PA, passes all unit tests locally...",02/May/08 21:02;acmurthy;Updated patch to reflect recent changes to trunk ...,"02/May/08 21:45;olgan;committed the fix, thanks Arun",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problem with streaming optimization,PIG-226,12395161,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,acmurthy,olgan,olgan,01/May/08 20:53,24/Mar/10 22:01,14/Mar/19 03:05,02/May/08 04:07,,,,,,,0.1.0,,,,,0,,,,"Current optimization code assumes that every storeage function implements both LoadFunc and StoreFunc interfaces and causes an exception if it does not.

2008-05-01 13:31:23,662 [main] ERROR org.apache.pig.tools.grunt.Grunt - java.lang.RuntimeException: could not instantiate 'org.apache.pig.test.udf.storefunc.DumpLoader' with arguments '[]'
        at org.apache.pig.impl.PigContext.instantiateFunc(PigContext.java:515)
        at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:521)
        at org.apache.pig.impl.logicalLayer.optimizer.streaming.StoreOptimizer.visitStore(StoreOptimizer.java:103)
        at org.apache.pig.impl.logicalLayer.LOStore.visit(LOStore.java:121)
        at org.apache.pig.impl.logicalLayer.optimizer.streaming.StoreOptimizer.optimize(StoreOptimizer.java:145)
        at org.apache.pig.PigServer.optimizeAndRunQuery(PigServer.java:411)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:297)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:450)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:233)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:62)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:60)
        at org.apache.pig.Main.main(Main.java:296)

The code that needs to change is in impl/logicalLayer/optimizer/streaming/LoadOptimizer.java and impl/logicalLayer/optimizer/streaming/StoreOptimizer.java.

The logic that needs to be added is if the load/store interface is not implemented then disable optimization.",,,,,,,,,,,,,,,,,,,02/May/08 01:05;acmurthy;PIG-226_0_20080501.patch;https://issues.apache.org/jira/secure/attachment/12381291/PIG-226_0_20080501.patch,02/May/08 18:57;acmurthy;PIG-226_1_20080502.patch;https://issues.apache.org/jira/secure/attachment/12381333/PIG-226_1_20080502.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-05-02 01:05:51.363,,,no_permission,,,,,,,,,,,,163873,,,,,Fri May 02 18:57:19 UTC 2008,,,,,,,0|i0ghnb:,94289,,,,,,,,,,02/May/08 01:05;acmurthy;I've fixed the {Load|Store}Optimizer to check if the StreamingCommand's {in|out}Specs implement {Load|Store}Func before casting them. The patch also contains a test case for the same. I have also checked that the test fails without the fix to the {Load|Store}Optimizer.,02/May/08 04:07;olgan;patch committed. thanks Arun,"02/May/08 04:21;pi_song;Actually we have an interface called ""ReversibleLoadStoreFunc"" that enforces the load/store operations to be reversible.

Only  checking  ""if (streamStorer instanceof LoadFunc)"" is not enough because the load/store operations might not be symmetric.


The ReversibleLoadStoreFunc interface guarantees the property that:-
{noformat}
LoadFunc (StoreFunc(x)) = x
{noformat}","02/May/08 16:26;olgan;Thanks, Pi. I did not realize that the change already ent in. Arun, could you make the change please.",02/May/08 18:56;acmurthy;Fixed the check to use ReversibleLoadStoreFunc as suggested by Pi...,02/May/08 18:57;acmurthy;The 'right' patch ...,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming returns -127 on regardless of the error code,PIG-224,12395002,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,acmurthy,olgan,olgan,29/Apr/08 18:20,24/Mar/10 22:01,14/Mar/19 03:05,02/May/08 20:47,,,,,,,0.1.0,,,,,0,,,,"If streaming application fails with any error code, the return seen via UI is always -127.",,,,,,,,,,,,,,,,,,,29/Apr/08 18:57;acmurthy;PIG-224_0_20080429.patch;https://issues.apache.org/jira/secure/attachment/12381130/PIG-224_0_20080429.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-04-29 18:57:08.761,,,no_permission,,,,,,,,,,,,163871,,,,,Fri May 02 20:47:21 UTC 2008,,,,,,,0|i0ghm7:,94284,,,,,,,,,,"29/Apr/08 18:22;olgan;Sample python script:

#!/usr/bin/python

import sys
import optparse

def main():
    p = optparse.OptionParser()
    p.add_option('--upper', '-u', action=""store_true"")
    options, arguments = p.parse_args()

    sys.exit(-3)

    line = sys.stdin.readline()
    while line:
        if options.upper == True:
            line = line.upper()
        sys.stdout.write(line)
        line = sys.stdin.readline()

if __name__ == '__main__':
    main()


Pig script: 
define X `MyStreamAppExitBefore.py` ship('MyStreamAppExitBefore.py') stderr('error_handling_1');
A = load ':studenttab10k';
B = stream A through X;
store B into 'error_handling_1';",29/Apr/08 18:57;acmurthy;Straight-forward to fix to get the exit code in the 'finally' block in ExecutableManager.close().,29/Apr/08 20:41;olgan;+1 on the code change. Running tests now. WIll commit once testing is done,"02/May/08 20:47;olgan;patch committed, thanks arun",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestParamSubPreproc.testCmdlineParamWithInlineCmd failing since it was committed,PIG-222,12394919,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,nidaley,nidaley,28/Apr/08 15:26,24/Mar/10 22:01,14/Mar/19 03:05,29/Apr/08 20:26,,,,,,,0.1.0,,impl,,,0,,,,"TestParamSubPreproc.testCmdlineParamWithInlineCmd  was committed as part of PIG-218.  You can see here that it has been failing since then: 
http://hudson.zones.apache.org/hudson/view/Pig/job/Pig-trunk/163/


",,,,,,,,,,,,,,,,,,,28/Apr/08 19:46;olgan;PIG-222.patch;https://issues.apache.org/jira/secure/attachment/12381049/PIG-222.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-04-28 18:42:54.838,,,no_permission,,,,,,,,,,,,163869,,,,,Tue Apr 29 20:26:28 UTC 2008,,,,,,,0|i0ghlb:,94280,,,,,,,,,,"28/Apr/08 18:42;olgan;The test runs fine in my environment. Also, looks like the automated test thinks there are 29 tests while when I run it locally, it shows only 28. Will investigate.",28/Apr/08 19:44;olgan;perl is in a different location on the build mashine. ,"29/Apr/08 20:26;olgan;patch committed
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incorrect variable definition in parameter substitution,PIG-220,12394730,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,24/Apr/08 20:22,24/Mar/10 22:01,14/Mar/19 03:05,24/Apr/08 22:58,,,,,,,0.1.0,,,,,0,,,,Current definitions allows variables of the form $_ which causes problems for embeded perl scripts and is not a valid var name.,,,,,,,,,,,,,,,,,,,24/Apr/08 20:25;olgan;PIG-220.patch;https://issues.apache.org/jira/secure/attachment/12380867/PIG-220.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-04-24 22:52:14.088,,,no_permission,,,,,,,,,,,,163867,,,,,Thu Apr 24 22:58:09 UTC 2008,,,,,,,0|i0ghkf:,94276,,,,,,,,,,24/Apr/08 22:52;alangates;+1,24/Apr/08 22:58;olgan;patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig tests must cover local and mapreduce execution types,PIG-219,12394706,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kali,kali,kali,24/Apr/08 14:50,24/Mar/10 22:01,14/Mar/19 03:05,05/May/08 16:22,,,,,,,0.1.0,,,,,0,,,,Followup of Local and MapReduce Test modes in pig-dev.,,,,,,,,,,,,,,,,,,,30/Apr/08 13:54;kali;Test.all.v1.patch;https://issues.apache.org/jira/secure/attachment/12381183/Test.all.v1.patch,24/Apr/08 14:52;kali;Test.v1.patch;https://issues.apache.org/jira/secure/attachment/12380848/Test.v1.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-04-25 22:46:04.763,,,no_permission,,,,,,,,,,,,163866,,,,,Mon May 05 16:22:53 UTC 2008,,,Patch Available,,,,0|i0ghjz:,94274,,,,,,,,,,"24/Apr/08 14:51;kali;ant test runs twice the junit target, with two different 
values for the property ${test.exectype}. ant test-local runs 
all tests locally, ant-test-mapreduce runs them on the minicluster.

logs are produced in two different subdirectories of 
build/test : logs- mapreduce and logs-local to avoid confusion.

PigTestCase implements junit's TestCase, and is responsible 
for PigServer setup. All current TestCase will have to extend 
PigTestCase instead of TestCase. They can test this.execType 
to skip some test method.

This proposal hardcodes the fact that pig supports two 
backends in PigTestCase and build.xml, but adding a new 
backend does not require to change the actual tests. Good 
enough for now in my opinion.

The patch contains PigTestCase, changes to build.xml and 
TestAlgebraicEval as an example of what has to be done to the 
actual TestCases.
","25/Apr/08 22:46;alangates;The general approach looks good.  One comment on the name PigTestCase.  This class is specific to tests that need to execute pig scripts.  Other tests should instead just extend TestCase.  It think that's fine, we should just rename PigTestCase to make it clear that it is only needed for tests that execute pig scripts.  Maybe something like PigExecTestCase.

Other than that, I think it looks fine to go ahead and convert the other tests that execute pig scripts to this so they run in local mode as well.

",28/Apr/08 11:55;pi_song;I think tests that have nothing to do with backend should be run only once.,"28/Apr/08 12:13;kali;I renamed PigTestCase to PigExecTestCase, I have converted most test scripts, and I am trying to get all tests working (or skipping the cases that do not work).

I tend to agree with Pi. That would require the ant script to know about what test case is not backend dependent. I don't think there is a straightforward way to hint ant to run only the test that extend PigExecTestCase. Some options
 - move all ""PigExec"" test in another directory.
 - change their name so that they all contains something we can match in build.xml
 - list ""backend independent"" test scripts in the <exclude> section of build.xml

On the other hand, most of the time spent in running pig tests is actualy spent instantiating the MiniClusters. So it may not be that big an issue to run the backend-independent test twice.","30/Apr/08 13:54;kali;Test.all.v1.patch converts all the patches executing pig commands to the proposed schema.

Some tests had to be skipped in one mode in the following testcases, they are marked by a comment starting by ""FIXME"".

TestCompressedFiles.java
TestCustomSlicer.java
TestExGenCogroup.java
TestExGenEval.java
TestParser.java
TestStreaming.java

Some tests have not been changed by the patch, and are run twice by build.xml. Only two of them (TestMapReduce and TestInfixArithmetic) are taking more than a few seconds to run in local mode. I think we can live with that for now, but we probably need to choose a way to identify these tests are manage them differently.",30/Apr/08 14:00;kali;ASF license is obviously granted on the previous patch. (Test.all.v1.patch),"30/Apr/08 15:12;pi_song;Your patch looks good. Moving all the running minicluster boilerplate to PigExecTestCase also makes all the tests cleaner.
I don't have problem with what you suggested about testing twice but let's see what other people think. 

FYI. At the moment, things will move a bit slowly because most people are focusing on the new branch.","05/May/08 16:22;alangates;Patch checked in at revision 653524.  Mathieu, thanks for working on this, it was badly needed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
parameter substitution broken with some commands,PIG-218,12394656,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,23/Apr/08 23:03,24/Mar/10 22:01,14/Mar/19 03:05,24/Apr/08 16:29,,,,,,,0.1.0,,,,,0,,,,"If parameter value is generated via a command, it would fail for some commands. Example:

%declare cmd `/usr/local/bin/perl -e 'print ""studenttab10k""'`

This command fails because currently the command is executed via call to Runtime.exec(cmd). In this case, default tokenizer is used to split the command and it does not respect the quotes.",,,,,,,,,,,,,,,,,,,23/Apr/08 23:23;olgan;PIG-218.patch;https://issues.apache.org/jira/secure/attachment/12380803/PIG-218.patch,24/Apr/08 15:02;olgan;PIG-218_v2.patch;https://issues.apache.org/jira/secure/attachment/12380851/PIG-218_v2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-04-24 12:15:11.016,,,no_permission,,,,,,,,,,,,163865,,,,,Thu Apr 24 16:29:35 UTC 2008,,,,,,,0|i0ghjj:,94272,,,,,,,,,,"24/Apr/08 12:15;alangates;Will this change handle commands that have a unix pipe in them?  If so, can we add a test for that?",24/Apr/08 15:02;olgan;The new patch has a modified test that includes pipe,24/Apr/08 16:04;alangates;New tests look good.,24/Apr/08 16:29;olgan;patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig Streaming doesn't handle commands which use Unix pipes,PIG-216,12394487,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,acmurthy,acmurthy,acmurthy,22/Apr/08 00:01,24/Mar/10 22:01,14/Mar/19 03:05,22/Apr/08 18:58,,,,,,,0.1.0,,impl,,,0,,,,"Currently ExecutableManager quotes each component of the the streaming command before exec'ing it via 'bash -c exec ...', this breaks commands which use Unix pipes:
OP = stream IP through `cut -f 1 | sort | uniq -c`

The straight-forward fix is to remove quoting of the command's components...",,,,,,,,,,,,,,,,,,,22/Apr/08 00:44;acmurthy;PIG-216_0_20080421.patch;https://issues.apache.org/jira/secure/attachment/12380656/PIG-216_0_20080421.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-04-22 18:58:39.549,,,no_permission,,,,,,,,,,,,163864,,,,,Tue Apr 22 18:58:39 UTC 2008,,,,,,,0|i0ghin:,94268,,,,,,,,,,"22/Apr/08 00:44;acmurthy;Straight-forward patch, with additional test-cases...","22/Apr/08 18:42;acmurthy;Patch passes all e2e tests, submittng for review...",22/Apr/08 18:58;alangates;Patch checked in at revision 650606.  Thanks Arun.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Miscellaneous cleanups after PIG-111 Configuration,PIG-215,12394371,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,pi_song,pi_song,pi_song,19/Apr/08 13:37,24/Mar/10 22:01,14/Mar/19 03:05,02/May/08 22:39,,,,,,,0.1.0,,,,,0,,,,"- Set default execution mode to MapReduce (This is a surprise as it should have been fixed even before PIG-111 got checked-in)
- When using local hadoop, changed message ""Connecting to HDFS at null"" to ""Connecting to HDFS at local""
- Added the missing conf/log4j.properties
- Removed some dead code.",,,,,,,,,,,,,,,,,,,19/Apr/08 13:38;pi_song;CleanPig111.patch;https://issues.apache.org/jira/secure/attachment/12380565/CleanPig111.patch,19/Apr/08 13:52;pi_song;CleanPig111_2.patch;https://issues.apache.org/jira/secure/attachment/12380566/CleanPig111_2.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-05-02 22:39:05.492,,,no_permission,,,,,,,,,,,,38589,,,,,Fri May 02 22:39:05 UTC 2008,,,Patch Available,,,,0|i0ghi7:,94266,,,,,,,,,,19/Apr/08 13:38;pi_song;The patch.,19/Apr/08 13:52;pi_song;Sorry mistake in attachment. Use this one instead.,02/May/08 22:39;alangates;Fix checked in at revision 652935.  Thanks Pi.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Non-static Log objects in org.apache.pig.data.* classes are inefficient,PIG-213,12394333,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,vgeshel,vgeshel,vgeshel,18/Apr/08 18:06,24/Mar/10 22:01,14/Mar/19 03:05,18/Apr/08 22:09,0.1.0,,,,,,0.1.0,,impl,,,0,,,,LogFactory.getLog called from the constructor of Tuple accounts for significant percentage of my job's running time. The proposed fix is to make the Log fields static (which is generally standard practice).,,,,,,,,,,,,,,,,,,,18/Apr/08 18:08;vgeshel;logging.patch;https://issues.apache.org/jira/secure/attachment/12380527/logging.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-04-18 18:22:49.664,,,no_permission,,,,,,,,,,,,38587,,,,,Fri Apr 18 22:09:58 UTC 2008,,,Patch Available,,,,0|i0ghhb:,94262,,,,,,,,,,"18/Apr/08 18:22;breed;+1 Good catch.
","18/Apr/08 18:41;kali;Yes, I actualy had a similar patch ready. I measured a 2 figures percentage of time spent on the logger instantiation on some processes...",18/Apr/08 22:09;alangates;Patch checked in at revision 649710..  Thank Vadim for contributing.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New illustrate command does not work in mapreduce mode.,PIG-207,12394220,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,shubhamc,alangates,alangates,17/Apr/08 17:21,24/Mar/10 22:01,14/Mar/19 03:05,22/Apr/08 22:57,0.1.0,,,,,,0.1.0,,impl,,,0,,,,"In local mode, illustrate will work.  But if exectype is set to mapreduce, then:

{noformat}
grunt> a = load 'data/test.txt';
grunt> b = filter a by $0 eq 'f2';
grunt> illustrate b;
2008-04-16 00:03:06,512 [main] ERROR org.apache.pig.tools.grunt.GruntParser - java.lang.ClassCastException: org.apache.pig.backend.hadoop.executionengine.HExecutionEngine cannot be cast to org.apache.pig.backend.local.executionengine.LocalExecutionEngine
        at org.apache.pig.pen.ExGen.GenerateExamples(ExGen.java:61)
        at org.apache.pig.PigServer.showExamples(PigServer.java:573)
        at org.apache.pig.PigServer.showExamples(PigServer.java:569)
        at org.apache.pig.tools.grunt.GruntParser.processIllustrate(GruntParser.java:131)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:172)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:72)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:54)
        at org.apache.pig.Main.main(Main.java:272)
{noformat}

dump a and dump b work.",,,,,,,,,,,,,,,,,,,22/Apr/08 16:05;shubhamc;exgen.patch;https://issues.apache.org/jira/secure/attachment/12380701/exgen.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-04-22 16:05:07.267,,,no_permission,,,,,,,,,,,,163857,,,,,Tue Apr 22 22:57:19 UTC 2008,,,,,,,0|i0ghev:,94251,,,,,,,,,,22/Apr/08 16:05;shubhamc;Patch fixing the bug. The test cases are also modified accordingly to ensure the HDFS connectivity is established using the MiniCluster,22/Apr/08 22:56;alangates;Patch previously submitted by Shubham,22/Apr/08 22:57;alangates;Patch checked in at revision 650688.  Thanks Shubham.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revision 643583 for PIG-94 broke input splits,PIG-204,12393705,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,acmurthy,alangates,alangates,10/Apr/08 21:49,24/Mar/10 22:01,14/Mar/19 03:05,14/Apr/08 18:23,0.0.0,,,,,,0.1.0,,impl,,,0,,,,"For revisions of the code after 643583, all pig jobs run in 1 map, instead of multiple maps.  The jobs still produce correct results, but they do not run in parallel.",,,,,,,,,,,,,,,,,,,10/Apr/08 22:30;acmurthy;PIG-204_0_20080410.patch;https://issues.apache.org/jira/secure/attachment/12379895/PIG-204_0_20080410.patch,11/Apr/08 21:07;acmurthy;PIG-204_1_20080411.patch;https://issues.apache.org/jira/secure/attachment/12379955/PIG-204_1_20080411.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-04-10 22:30:47.688,,,no_permission,,,,,,,,,,,,163855,,,,,Mon Apr 14 18:23:47 UTC 2008,,,,,,,0|i0ghdj:,94245,,,,,,,,,,10/Apr/08 22:30;acmurthy;Fix to PigInputFormat...,"11/Apr/08 21:07;acmurthy;Updated patch, I had to fix {{MapreducePlanCompile.connectInputs}} to correctly propagate the properties, also fixed {{EvalSpec.copy}} to do the same. Finally {{PigInputFormat.getSplits}} now uses the correct default value for {{pig.input.splittable}}.","11/Apr/08 21:28;acmurthy;Submitting patch for review, passes all unit-tests locally.",14/Apr/08 18:23;alangates;Fix checked in at revision 647356.  Thanks Arun for your quick response.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig parser hangs on input script bigger ~1kb,PIG-203,12393652,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kali,kali,kali,10/Apr/08 12:26,24/Mar/10 22:01,14/Mar/19 03:05,11/Apr/08 22:21,,,,,,,0.1.0,,,,,0,,,,"When the command line interpreter is run on a file bigger than 1kb or so, it overflows the PipeReader/PipeWriter internal buffers and freezes.",,,,,,,,,,,,,,,,,,,10/Apr/08 12:28;kali;Main.patch;https://issues.apache.org/jira/secure/attachment/12379834/Main.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-04-11 22:21:09.26,,,no_permission,,,,,,,,,,,,163854,,,,,Fri Apr 11 22:21:09 UTC 2008,,,,,,,0|i0ghd3:,94243,,,,,,,,,,10/Apr/08 12:28;kali;Main.patch uses a StringWriter as an intermediate buffer to the store the output of the parameter substitution.,11/Apr/08 22:21;alangates;Checked in patch at revision 647335.  Thanks Mathieu for finding and fixing this issue.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ComparatorFunc provided to ORDER clause is not always honoured,PIG-202,12393636,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kali,kali,kali,10/Apr/08 10:04,24/Mar/10 22:01,14/Mar/19 03:05,07/May/08 17:13,,,,,,,0.1.0,,,,,0,,,,"Specifying a comparator function is acknowledge neither by local implementation, nor by quartile lookup job.

Patch coming soon.",,,,,,,,,,,,,,,,,,,10/Apr/08 12:09;kali;EvalSpec.patch;https://issues.apache.org/jira/secure/attachment/12379830/EvalSpec.patch,15/Apr/08 10:57;kali;InstantiateFunc.patch;https://issues.apache.org/jira/secure/attachment/12380166/InstantiateFunc.patch,10/Apr/08 12:13;kali;MapreducePlanCompiler.patch;https://issues.apache.org/jira/secure/attachment/12379831/MapreducePlanCompiler.patch,22/Apr/08 14:01;kali;Sort.patch;https://issues.apache.org/jira/secure/attachment/12380696/Sort.patch,25/Apr/08 09:56;kali;Sort.v2.patch;https://issues.apache.org/jira/secure/attachment/12380916/Sort.v2.patch,10/Apr/08 10:31;kali;TestOderBy.patch;https://issues.apache.org/jira/secure/attachment/12379821/TestOderBy.patch,26/Apr/08 08:01;kali;quantiles.in;https://issues.apache.org/jira/secure/attachment/12380976/quantiles.in,26/Apr/08 08:01;kali;quantiles.pig;https://issues.apache.org/jira/secure/attachment/12380977/quantiles.pig,,,,,,8.0,,,,,,,,,,,,,,,,,,,2008-04-16 12:09:22.397,,,no_permission,,,,,,,,,,,,163853,,,,,Wed May 07 17:13:38 UTC 2008,,,,,,,0|i0ghcn:,94241,,,,,,,,,,"10/Apr/08 10:31;kali;This patch makes testOrderBy test both LOCAL and MAPREDUCE mode.

Local attempt on testTopLevelOrderBy_*_Using are failing.

","10/Apr/08 12:09;kali;EvalSpec.patch is fix a first issue where comparatorFunction is not created and default implementation is used where it should not.

But I'm struggling with the fact that in local mode, the comparator is given the whole tuple instead of the relevant datum ($2 for instance) for the tuple.

I guess I won't be able to do much more by myself on the localmode issue.","10/Apr/08 12:13;kali;MapreduceCompiler patch puts the same compartor in the quantile job than in the sort job. I just feel like it makes more sense, but I might be totally wrong here.","14/Apr/08 16:44;kali;After struggling a bit with the local mode evaluator, I found the read issue. The code I was looking for was actualy in instantiateFunc() method, and my previous patch (1) a useless and incomplete duplication of this code.

Instead I tried to make sure instantiateFunc() is called on all EvalSpec nodes prior to execution attempts. I now run a POVisitor in LocalExecutionEngine.execute() before pp.open(). It looks much better as the 16 tests from testOrderBy are now working in both local and mapreduce mode.

The patch is not ready yet (the visitor only visit what my tests needs it to visit) and there is still come cleanup and commenting to do.

Any comment welcome.

By the way, the MapReducePlanCompiler patch is a piece of junk, but I'm still convinced there is a severe performance issue here.",15/Apr/08 10:57;kali;Patch 4 introduces a POVisitor that call instantiateFunc on all EvalSpec of PO nodes. It is called by the LogicalExecutionEngine just before execution.,15/Apr/08 10:59;kali;Please review InstantiateFunc patch. TestOrderBy may be worth a look too. The rest is junk.,"16/Apr/08 12:09;pi_song;Mathieu,

Welcome in Pig community. I appreciate your attempt to help us. My real work is killing me and  seems other people are busy too. Please don't give up. We will have a look at your patches as soon as possible (Latest will be on the weekend).

Cheers,
Pi ","20/Apr/08 14:22;pi_song;Mathieu,

Ensuring all the specs instantiating what needed before use using a visitor pattern is a good idea but just to make the code clean we should remove code where those stuffs were previously instantiated as well.

For the MapreducePlanCompiler thing, you've got very good eyes. Now I also tempted to add that comparator instantiation. Can you prove that at the moment the quantilejob doesn't really sort based on the given function? Possibly a small unit test would be enough.

In regard to the test, currently we only test either MapReduce or Local at a time. That's why a bug such like this still occurs. In the mean time, I think you should send an email to the mailing list so we can find a global solution for all the tests together.

PS. Could you please combine all the needed patches? Too many fragments will make it difficult for our already busy committers. ","22/Apr/08 14:01;kali;As requested, a all-in-one patch (Sort.patch) that:
 - call instantiateFunc on PO before the actual execution (fix using clause in local context)
 - discard the only one ""late"" comparator instantiation I could found (made redundant, dead code)
 - correct a marginal biais in the findQuantile builtin function (one of the two extremum quantile was bigger or smaller depending on truncation)
 - fix quantile job.

The quantile job issue is tricky. It is not easy to show how it misbehaves with a pig unit test, as the result is correct... FindQuantiles is responsible for defining a partition of the intermediary keyspace. Hadoop uses this partition through a SortPartitioner instance to split the reduce half of the Sort job among several reduce tasks. Now the FindQuartiles were using a StarSpec as a comparator, whereas SortPartitioner were using the UDF comparator to perform a Arrays.binarySearch. The binary search can not work correctly in these conditions, and this leads to widely unbalanced reduce tasks as most of the keys fall in the same partition. 

""Prooving"" this point actualy required counting how many items go to which partition in SortPartitioner (some printf-like debugging). But honestly, I think the patch just makes a lot of sense.

The fix just provides the UDF compartor to the sort used internaly by the findQuartile job.","25/Apr/08 09:56;kali;Well, well, well...

New delivery of the all-in-one sort patch (Sort.v2.patch). There was still a glitch in my findQuantileJob version, but I think I got it right this time (at least as much as I did before...).

By the way, I'm starting to worry about the absence of attention or interest from the commiters on this issue. Am I struggling with some piece of code about to be superseded by physical and/or logical layers refactoring ? Or is it the fact that efficient sort operations are critical in what i'm trying to do with pig that make me perceive this as more critical than it is ?","25/Apr/08 22:08;alangates;A couple of comments/questions:

# I don't understand your changes to FindQuantiles.  If I read the code correctly, its now taking the first x values out of the bag, instead of sampling at regular intervals from the whole bag.  This has the advantage of not needing to read the whole bag (though the code isn't taking advantage of that), but it will give a much worse sample, at least if the bag is ordered.  Am I missing something?  Should it be that we take the first n values and then break if the bag is unordered, and every 1/n values if the bag is ordered?
# Have you done any performance testing to get an idea of the speed up this gives?  Obviously that will depend on the data set, but it would be interesting to see.

FWIW, I haven't been ignoring your work on this.  It seemed you were making good progress and getting feedback from Pi, so I hadn't jumped in yet.","26/Apr/08 07:59;kali;Thanks for having confirmed I was not wasting my time.

1. This is not what I'm trying to do. The current implementation, when asked for 9 quantiles among 100 elements (0..99) returns this:
(all, {(0), (12), (24), (36), (48), (60), (72), (84), (96)})
This does not lead to a good partition. The first part is empty or so, the last part is smaller than the ""central"" parts.

Sort.patch and Sort.v2.patch change FindQuantiles to make it return:
(all, {(11), (21), (31), (41), (51), (61), (71), (81), (91)})

It looks better. Actualy, it looks even nicer with a <= instead of < in the big if in the loop...
(all, {(10), (20), (30), (40), (50), (60), (70), (80), (90)})

The impact on sort performance of this fix in FindQuantiles is probably marginal. it just avoids some empty or smaller reduce jobs.  But it gives better quantiles to a end user trying to use the function.

2. I will try to run and time my test job over the weekend. The performance killer was not the small glitch in FindQuantiles, but the fact that the SortPartitioner's and the quantiles' comparator were not consistent. I'll try to give you some figures.

3. I will also generate a Sort.v3.patch (with the <= in FindQuantiles) using svn diff as eclipse tends to generates ugly patches with absolute paths.",26/Apr/08 08:01;kali;The test files I used to generates the quantiles in my previous comment.,"28/Apr/08 12:17;pi_song;1) The first quantile always being zero is something really has to be fixed.
2) The <= to < thing also helps a (very tiny) bit more to make the first bucket the same size as others. 

Theoretically, there should be some small performance gain due to (1)

I'm looking forward to seeing the result!!!","29/Apr/08 15:52;kali;Just a short update to say I'm alive, but still crunching numbers on my cluster. And trying to make sense of what is hapenning : it looks like I never get balanced partitions, whether or not I specify a UDF comparator, with or without my patches...

Anyway, still digging.",03/May/08 23:56;pi_song;+1 on Sort.v2.patch. This fixes an obvious bug in quantile bucketing. It would be good to see the benchmark but not necessary.,"07/May/08 17:13;alangates;Fixed checked in at revision 654172.  I made one small change, which was to change the name of InsantiateFuncCallerPOVisitor to InstantiateFuncCallerPOVisitor.  Thanks Mathieu for your work on this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BufferedPositionedInputStream is not buffered,PIG-201,12393632,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kali,kali,kali,10/Apr/08 09:43,25/Mar/10 00:12,14/Mar/19 03:05,26/Jan/09 20:23,,,,,,,0.2.0,,,,,0,,,,"BufferedPositionedInputStream is actualy not buffered, leading (I guess) to constant round trip to dfs as byte are read one by one. I just wrapped the provided input stream in the constructor in a good old BufferedInputStream.

I measured a 40% performance boost on a script that reads and writes 3.7GB in dfs through PigStorage on one node. I guess the impact may be greater on a real hdfs cluster with actual network roundtrips.

FYI, the issue was found while profiling with Yourkit java profiler. Usefull toy...",,,,,,,,,,,,,,,,,,,10/Apr/08 09:45;kali;BufferedPositionedInputStream.patch;https://issues.apache.org/jira/secure/attachment/12379817/BufferedPositionedInputStream.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-04-10 14:44:27.307,,,no_permission,,,,,,,,,,,,163852,,,,,Mon Jan 26 20:23:34 UTC 2009,,,,,,,0|i0ghc7:,94239,,,,,,,,,,"10/Apr/08 09:50;kali;Ho ! The 8Kb buffer size is just plain guess. I honestly have no idea about the ""right"" value.","10/Apr/08 14:44;breed;The InputStream we get from Hadoop DFS should be buffered, so we don't do extra buffering in BufferedPositionedInputStream again. This is important because the buffering needs to be done before the compression codecs so that the positioning works out properly. Doing it after, like this patch does, will cause premature detection of end of split.

Having said all that, there obviously is a performance gain to be had. Perhaps we need to figure out why the buffering done by Hadoop DFS InputStream isn't helping us. If we do need to buffer, it should go into PigSlice.init() to buffer fsis.",22/Apr/08 17:51;olgan;clearing the patch based on Ben's comments,26/Jan/09 20:23;olgan;we have added additional buffer and seeing much better performance now,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bz2 doesn't work,PIG-197,12393544,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,breed,amirhyoussefi,amirhyoussefi,09/Apr/08 16:24,24/Mar/10 22:01,14/Mar/19 03:05,28/Apr/08 17:34,,,,,,,0.1.0,,,,,0,,,,"Storing a file in bzip2 doesn't work. It stores output in regular plain text in following examples:

a = load 'any_input'; -- use any sample input
store a into 'any_output.bz2';

or  to have reducer/combiner

a = load 'any_input';
b = group a all parallel 1;
c = foreach b generate group, COUNT(a);
store c into 'any_output.bz2';

--

pig -version
Apache Pig version 0.1.0-dev (r6455)
compiled Apr 03 2008, 16:10:08
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-04-24 14:39:29.28,,,no_permission,,,,,,,,,,,,163849,,,,,Mon Apr 28 17:34:33 UTC 2008,,,,,,,0|i0ghan:,94232,,,,,,,,,,"24/Apr/08 11:41;amirhyoussefi;loading a bz2 doesn't work either.

grunt> a = load 'test.txt.bz2';
grunt> dump a;
(BZh91AY&SY???
              ?0r?8 ""??=Lj?E??}!RW??N$1?)


bzcat test.txt.bz2 shows correct results:
a1      1       5700
b1      2       2001
c2      2
",24/Apr/08 14:39;olgan;Ben will be looking at this issue today and will provide update,"25/Apr/08 07:32;breed;I track the problem down to a runtime change in Hadoop, which made our code to figure out the extension of the output directory break. The fix and test cases for it are in PIG-151.",28/Apr/08 17:34;olgan;This issue is resolved by PIG-151. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig should use '-reducer NONE' for map-only jobs,PIG-196,12393427,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,acmurthy,acmurthy,acmurthy,08/Apr/08 16:53,25/Mar/10 00:12,14/Mar/19 03:05,30/Sep/08 23:07,,,,,,,0.2.0,,,,,0,,,,"Currently, for map-only jobs, Pig writes map-outputs directly to HDFS and then sends zero data to reducers. The problem with this is two fold:
 * Reduce slots are unnecessarily wasted on the cluster
 * Reduces write empty files to HDFS putting pressure on the Namenode

Both these can we very easily avoided by just calling:
{noformat}
job.setNumReduces(0);
{noformat}
and letting Hadoop Map-Reduce take care of writing map-outputs directly to HDFS.",,,,,,,,,,,,,,,,,,,12/Apr/08 23:47;acmurthy;PIG-196_0_20080412.patch;https://issues.apache.org/jira/secure/attachment/12379988/PIG-196_0_20080412.patch,22/Apr/08 21:40;acmurthy;PIG-196_1_20080422.patch;https://issues.apache.org/jira/secure/attachment/12380726/PIG-196_1_20080422.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-04-18 18:40:08.664,,,no_permission,,,,,,,,,,,,163848,,,,,Tue Sep 30 23:07:19 UTC 2008,,,,,,,0|i0gha7:,94230,,,,,,,,,,"12/Apr/08 23:47;acmurthy;Straight-forward fix, it sets number of reduces to zero.

The one change is that map-only jobs will have outputs created by the Map-Reduce framework and hence they will be named part-[0-9] and not map-[0-9] as previously...",12/Apr/08 23:48;acmurthy;Passes tests locally... I'm hoping the change to file names is acceptable.,18/Apr/08 18:40;breed;This patch looks like it assumes that splitSpec is not null if there isn't a groupSpec. It would be nice to add a comment stating that assumption and justifying it. Looking back at MapReduceLauncher doesn't hint that this is the case. I think it is important so that later changes do not break the justification for this assumption. (Or at least it will give the person debugging a hint at where to look.),"22/Apr/08 21:40;acmurthy;Thanks for the review Ben.

Here is an updated patch with comments stating and justifying the assumptions...",30/Sep/08 23:07;olgan;This issue is resolved on the types branch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException if storing a non-existing alias,PIG-193,12393219,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,acmurthy,xuzh,xuzh,05/Apr/08 05:50,24/Mar/10 22:01,14/Mar/19 03:05,07/Apr/08 22:56,,,,,,,0.1.0,,,,,0,,,,"If a non-existing alias (such as E in the following example script) is stored, a NullPointerException is generated.

{code}
A = load '../../singlefile/studenttab10k' as (name, age, gpa);
store E into 'results_n_30';
{code}

{noformat}
2008-04-04 22:41:12,686 [main] ERROR org.apache.pig.tools.grunt.Grunt - java.lang.NullPointerException
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.StoreClause(QueryParser.java:3523)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:706)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:489)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:359)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:47)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:262)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:446)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:226)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:62)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:60)
        at org.apache.pig.Main.main(Main.java:265)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-04-07 22:56:43.571,,,no_permission,,,,,,,,,,,,163845,,,,,Mon Apr 07 22:56:43 UTC 2008,,,,,,,0|i0gh8v:,94224,,,,,,,,,,07/Apr/08 22:56;olgan;Fixed with patch to PIG-182,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Got ClassCastException when using BinaryStorage on the load statement,PIG-190,12393214,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,acmurthy,xuzh,xuzh,05/Apr/08 04:39,24/Mar/10 22:01,14/Mar/19 03:05,07/Apr/08 22:56,,,,,,,0.1.0,,,,,0,,,,"With the following Pig script, I got the exception below:

{code}
A = load '/user/xu/test/PigLoggingTest' using BinaryStorage() split by 'file';
B = stream A through `BinaryReadWrite.pl -o f`;
store B into 'results_38';
{code}

{noformat}
2008-04-04 21:34:08,619 [main] ERROR org.apache.pig.tools.grunt.Grunt - java.lang.ClassCastException: org.apache.pig.builtin.BinaryStorage
        at org.apache.pig.builtin.PigStorage.equals(PigStorage.java:93)
        at org.apache.pig.impl.logicalLayer.optimizer.streaming.LoadOptimizer.visitEval(LoadOptimizer.java:85)
        at org.apache.pig.impl.logicalLayer.LOEval.visit(LOEval.java:98)
        at org.apache.pig.impl.logicalLayer.LOVisitor.basicVisit(LOVisitor.java:115)
        at org.apache.pig.impl.logicalLayer.LOVisitor.visitStore(LOVisitor.java:106)
        at org.apache.pig.impl.logicalLayer.optimizer.streaming.LoadOptimizer.visitStore(LoadOptimizer.java:123)
        at org.apache.pig.impl.logicalLayer.LOStore.visit(LOStore.java:121)
        at org.apache.pig.impl.logicalLayer.optimizer.streaming.LoadOptimizer.optimize(LoadOptimizer.java:134)
        at org.apache.pig.PigServer.optimizeAndRunQuery(PigServer.java:391)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:280)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:446)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:226)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:62)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:60)
        at org.apache.pig.Main.main(Main.java:265)

2008-04-04 21:34:08,621 [main] ERROR org.apache.pig.tools.grunt.Grunt - org.apache.pig.builtin.BinaryStorage
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-04-07 20:19:20.11,,,no_permission,,,,,,,,,,,,163842,,,,,Mon Apr 07 22:56:24 UTC 2008,,,,,,,0|i0gh7j:,94218,,,,,,,,,,"07/Apr/08 19:06;xuzh;I did not see the ClassCastException exception anymore, but the output of the streaming (ie, the contents of the results_38 is not what I expected.","07/Apr/08 20:19;acmurthy;Xu, what was the expected results and what did you see?",07/Apr/08 22:56;olgan;fixed with patch for PIG-182,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
There seems to be some mismatches between the actual stderr log and what I expected,PIG-188,12393211,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,acmurthy,xuzh,xuzh,05/Apr/08 03:15,24/Mar/10 22:01,14/Mar/19 03:05,15/Apr/08 20:31,,,,,,,0.1.0,,,,,0,,,,"With the following Pig script, I got streaming logs as shown below.  The job for running this script is job_200804041056_0182.  What PigLoggingTest does in this case is simply take tab delimited lines from STDIN and then output them to SDTOUT as tab delimited lines (so the same line comes in and out of PigLogginTest) after spitting out 10 STDERR messages.  Also as shown in the UI of job_200804041056_0181, there were a total of 21 tasks (1 map and 20 reduces).

From all these, I would expect the number of input records and output records to match in the log.  Also, I would expect there to be 26 logs.  In addition, since there was no error when running the script, all exit code should 0.

However, there are actually only  6 logs.  The number of input records and output records does not match.  The logs show that some of the tasks exit with -127.

In addition, the Input-split *** values in the logs do not make much sense to me:

{quote}
Input-split file: null
Input-split start-offset: -1
Input-split length: -1
{quote}

Here is Pig script:

{code}
define X `PigLoggingTest 10 t` ship('./cplusplus/PigLoggingTest') stderr('logging_test_1');
A = load '/user/pig/tests/data/singlefile/studenttab10k' as (name, age, gpa);
B = stream A through X;
store B into 'logging_test_1';
C = load 'logging_test_1/_logs/logging_test_1';
store C into 'results_26';
{code}

Here are the logs:

{noformat}
===== Task Information Header =====
Command: PigLoggingTest 10 t 
Start time: Fri Apr 04 19:18:44 PDT 2008
Input-split file: null
Input-split start-offset: -1
Input-split length: -1
=====          * * *          =====
This is stderr message number 1
This is stderr message number 2
This is stderr message number 3
This is stderr message number 4
This is stderr message number 5
This is stderr message number 6
This is stderr message number 7
This is stderr message number 8
This is stderr message number 9
This is stderr message number 10
===== Task Information Footer =====
End time: Fri Apr 04 19:18:45 PDT 2008
Exit code: 0
Input records: 10000
Input bytes: 1898380 bytes 
Output records: 4
Output bytes: 219446 bytes (stdout using org.apache.pig.builtin.BinaryStorage)
=====          * * *          =====
===== Task Information Header =====
Command: PigLoggingTest 10 t 
Start time: Fri Apr 04 19:31:34 PDT 2008
Input-split file: null
Input-split start-offset: -1
Input-split length: -1
=====          * * *          =====
This is stderr message number 1
This is stderr message number 2
This is stderr message number 3
This is stderr message number 4
This is stderr message number 5
This is stderr message number 6
This is stderr message number 7
This is stderr message number 8
This is stderr message number 9
This is stderr message number 10
===== Task Information Footer =====
End time: Fri Apr 04 19:31:36 PDT 2008
Exit code: 0
Input records: 10000
Input bytes: 1898380 bytes 
Output records: 4
Output bytes: 219446 bytes (stdout using org.apache.pig.builtin.BinaryStorage)
=====          * * *          =====
===== Task Information Header =====
Command: ./cplusplus/PigLoggingTest 10 t 
Start time: Fri Apr 04 10:11:22 PDT 2008
Input-split file: null
Input-split start-offset: -1
Input-split length: -1
=====          * * *          =====
===== Task Information Footer =====
End time: Fri Apr 04 10:11:22 PDT 2008
Exit code: -127
Input records: 747
Input bytes: 141796 bytes 
Output records: 0
Output bytes: 0 bytes (stdout using org.apache.pig.builtin.BinaryStorage)
=====          * * *          =====
===== Task Information Header =====
Command: ./cplusplus/PigLoggingTest 10 t 
Start time: Fri Apr 04 10:11:28 PDT 2008
Input-split file: null
Input-split start-offset: -1
Input-split length: -1
=====          * * *          =====
===== Task Information Footer =====
End time: Fri Apr 04 10:11:28 PDT 2008
Exit code: -127
Input records: 747
Input bytes: 141796 bytes 
Output records: 0
Output bytes: 0 bytes (stdout using org.apache.pig.builtin.BinaryStorage)
=====          * * *          =====
===== Task Information Header =====
Command: ./cplusplus/PigLoggingTest 10 t 
Start time: Fri Apr 04 10:11:32 PDT 2008
Input-split file: null
Input-split start-offset: -1
Input-split length: -1
=====          * * *          =====
===== Task Information Footer =====
End time: Fri Apr 04 10:11:33 PDT 2008
Exit code: -127
Input records: 747
Input bytes: 141796 bytes 
Output records: 0
Output bytes: 0 bytes (stdout using org.apache.pig.builtin.BinaryStorage)
=====          * * *          =====
===== Task Information Header =====
Command: ./cplusplus/PigLoggingTest 10 t 
Start time: Fri Apr 04 10:11:37 PDT 2008
Input-split file: null
Input-split start-offset: -1
Input-split length: -1
=====          * * *          =====
===== Task Information Footer =====
End time: Fri Apr 04 10:11:37 PDT 2008
Exit code: -127
Input records: 747
Input bytes: 141796 bytes 
Output records: 0
Output bytes: 0 bytes (stdout using org.apache.pig.builtin.BinaryStorage)
=====          * * *          =====
{noformat}",,,,,,,,,,,,,,,,,,,08/Apr/08 01:46;acmurthy;PIG-188_0_20080407.patch;https://issues.apache.org/jira/secure/attachment/12379619/PIG-188_0_20080407.patch,08/Apr/08 06:59;acmurthy;PIG-188_1_20080407.patch;https://issues.apache.org/jira/secure/attachment/12379631/PIG-188_1_20080407.patch,08/Apr/08 16:25;acmurthy;PIG-188_2_20080408.patch;https://issues.apache.org/jira/secure/attachment/12379667/PIG-188_2_20080408.patch,05/Apr/08 03:15;xuzh;PigLoggingTest.cpp;https://issues.apache.org/jira/secure/attachment/12379461/PigLoggingTest.cpp,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2008-04-07 08:04:12.361,,,no_permission,,,,,,,,,,,,163840,,,,,Tue Apr 15 20:31:11 UTC 2008,,,,,,,0|i0gh6n:,94214,,,,,,,,,,"07/Apr/08 08:04;acmurthy;Xu,

1. How did u get 20 reduces and 1 map for your first job?
2. You should expect 21 logs (20maps and 1reduces) only on HDFS.
3. The null/-1/-1 data for input-splits is due to the fact that reduces work on map-outputs and not on HDFS data.",08/Apr/08 01:46;acmurthy;Looks like PIG-55 broke the feature where the InputSplit was displayed correctly in the logs... fixed now.,"08/Apr/08 06:59;acmurthy;Updated patch:
 * Fix the break caused by PIG-55
 * Ensures that we do not output information about input-splits for reduces in the stderr logs since it could confuse users... ","08/Apr/08 16:07;olgan;Changes look good; however, the test for CustomSlicer is failing after I applied the patch:

java.lang.ArrayIndexOutOfBoundsException: 0
    at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.SliceWrapper.makeReader(SliceWrapper.java:96)
    at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigInputFormat.getRecordReader(PigInputFormat.java:113)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:200)
    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:150)
08/04/08 16:00:29 INFO mapreduceExec.MapReduceLauncher: Pig progress = 0%
",08/Apr/08 16:25;acmurthy;Fixed for TestCustomSlicer too... my bad.,"14/Apr/08 15:26;alangates;Changes look fine to me and all the tests pass.

I'd like to get input from Ben or Charlie Groves, both of whom worked on the split stuff this modifies, to make sure the changes fit well with that interface.",14/Apr/08 20:53;breed;+1 looks good to me.,15/Apr/08 20:31;alangates;Fix checked in revision 647997.  Thanks Arun.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig appears to hang with this Pig script,PIG-186,12393084,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,acmurthy,xuzh,xuzh,04/Apr/08 01:25,24/Mar/10 22:01,14/Mar/19 03:05,07/Apr/08 22:55,,,,,,,0.1.0,,,,,0,,,,"Pig stoped at progress 56%.   It seemed there had been exceptions on the the reduce task trackers (see below).  But waiting for 20 reduce tasks to time out themselves is excruciating and blocking my other tests.

Here is my Pig script:

{code}
define X `DataGuaranteeTest.pl -n 1` ship('/home/xu/streamingscript/DataGuaranteeTest.pl');
A = load '/user/pig/tests/data/singlefile/studenttab10k' as (name, age, gpa);
B = group A by name;
C = foreach B generate flatten(A);
D = stream C through X;
store D into 'results_24';
{code}

Here is the exception on the reduce task trackers:

{noformat}
java.lang.RuntimeException: java.io.IOException: Cannot run program ""./home/xu/streamingscript/DataGuaranteeTest.pl"": java.io.IOException: error=2, No such file or directory
	at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.(StreamSpec.java:132)
	at org.apache.pig.impl.eval.StreamSpec.setupDefaultPipe(StreamSpec.java:91)
	at org.apache.pig.impl.eval.CompositeEvalSpec.setupDefaultPipe(CompositeEvalSpec.java:51)
	at org.apache.pig.impl.eval.EvalSpec.setupPipe(EvalSpec.java:123)
	at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.setupReducePipe(PigMapReduce.java:303)
	at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.reduce(PigMapReduce.java:140)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:333)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2071)
{noformat}

I will attach DataGuaranteeTest.pl to the report

",,,,,,,,,,,,,,,,,,,04/Apr/08 01:26;xuzh;DataGuaranteeTest.pl;https://issues.apache.org/jira/secure/attachment/12379343/DataGuaranteeTest.pl,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-04-04 09:00:14.43,,,no_permission,,,,,,,,,,,,163838,,,,,Mon Apr 07 22:55:58 UTC 2008,,,,,,,0|i0gh5z:,94211,,,,,,,,,,"04/Apr/08 09:00;acmurthy;Xu, PIG-182 has a patch which fixes this - could you try and let me know if it solves this problem? Thanks!","07/Apr/08 18:20;xuzh;The ""No such file or directory"" exception is gone, but I got ""broken pipe"" exception instead.  See the tasks UI for job_200804041056_0346 .","07/Apr/08 20:21;acmurthy;Xu, you are getting broken pipe since your command specification 
{noformat}
define X `./home/xu/streamingscript/DataGuaranteeTest.pl -n 1` ship('/home/xu/streamingscript/DataGuaranteeTest.pl');
{noformat}
is in-correct as per the new *ship* specifications (outlined in the wiki) ... 

Your command should be:
{noformat}
define X `DataGuaranteeTest.pl -n 1` ship('/home/xu/streamingscript/DataGuaranteeTest.pl');
{noformat}

Could you please check against that? Thanks!
",07/Apr/08 20:33;xuzh;The second form of command is exactly what I used.  I just updated the description of the bug.  Sorry for the confusion.,07/Apr/08 22:55;olgan;Fixed with patch to PIG-182.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parsing issue on the stream statement?,PIG-184,12393072,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,acmurthy,xuzh,xuzh,03/Apr/08 22:05,24/Mar/10 22:01,14/Mar/19 03:05,08/Apr/08 00:43,,,,,,,0.1.0,,,,,0,,,,"When I ran the following Pig script on the command line (pig -c cluster -latest p_11), Pig returns immediately without doing any processing.  When I entered the script line by line into GRUNT, after I entered the second line, I had to type one extra "";"" in addition to the original one in order for Pig to accept the line.  In the end, Pig gave me the correct results.

Here is the script, p_11:

{code}
A = load '/user/pig/tests/data/singlefile/studenttab10k';
B = stream A through `python ./python/MySimpleStreamApp.py --upper` as (name, age, gpa);
store B into 'results_11';
{code}

Here is the python streaming script, MySimpleStreamApp.py:

{code}
#!/usr/bin/python

import sys
import optparse

def main():
    p = optparse.OptionParser()
    p.add_option('--upper', '-u', action=""store_true"")
    options, arguments = p.parse_args()

    line = sys.stdin.readline()
    while line:
        if options.upper == True:
            line = line.upper()
        sys.stdout.write(line)
        line = sys.stdin.readline()

if __name__ == '__main__':
    main()
{code}",,,,,,,,,,,,,,,,,,,07/Apr/08 21:41;acmurthy;PIG-184_0_20080407.patch;https://issues.apache.org/jira/secure/attachment/12379605/PIG-184_0_20080407.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-04-04 09:23:19.651,,,no_permission,,,,,,,,,,,,163836,,,,,Tue Apr 08 00:43:44 UTC 2008,,,,,,,0|i0gh53:,94207,,,,,,,,,,"04/Apr/08 09:23;acmurthy;This is a very interesting bug:

A = stream IP through `--upper`;

also exhibits this problem.

Looks like anything with ""--"" causes the parser to freeze up, maybe it needs a LOOKAHEAD - I'll take a closer look in the morning.","07/Apr/08 21:41;acmurthy;Fixes the grunt parser to correctly handle -- inside backtick-quoted strings ... thanks for noticing this Xu, it's a very interesting corner case! *smile*",07/Apr/08 23:43;xuzh;It works now with the latest Pig stuff. Thanks Arun.,08/Apr/08 00:43;olgan;patch committed. thanks arun for contributing!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig could not resolve my udf class,PIG-183,12393069,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pi_song,xuzh,xuzh,03/Apr/08 21:42,24/Mar/10 22:01,14/Mar/19 03:05,16/Apr/08 18:30,,,,,,,0.1.0,,,,,0,,,,"When I ran the following Pig script with the latest Pig stuff, I got an exception for unresolved class. I noticed that org.apache.pig.test.udf.storefunc is not among the packages that Pig searches ([, org.apache.pig.builtin., com.yahoo.pig.yst.sds.ULT., org.apache.pig.impl.builtin.]). 

I am sure that the package path for StringStore (org.apache.pig.test.udf.storefunc) is correct in testudf.jar.  I attached testudf.jar here with this bug report. 

{code}
register /path/to/my/jar/testudf.jar;
A = load '/user/pig/tests/data/singlefile/textdoc' using org.apache.pig.test.udf.storefunc.StringStore();
store A into 'results_4_1';
{code}

{noformat}
2008-04-03 13:20:25,154 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: wilbur11.labs.corp.sp1.yahoo.com:8020
2008-04-03 13:20:25,781 [main] ERROR org.apache.pig.tools.grunt.Grunt - java.lang.RuntimeException: could not instantiate 'org.apache.pig.test.udf.storefunc.StringStore' with arguments '[]'
        at org.apache.pig.impl.PigContext.instantiateFunc(PigContext.java:504)
        at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:510)
        at org.apache.pig.impl.io.ValidatingInputFileSpec.getSlicer(ValidatingInputFileSpec.java:50)
        at org.apache.pig.impl.io.ValidatingInputFileSpec.validate(ValidatingInputFileSpec.java:42)
        at org.apache.pig.impl.io.ValidatingInputFileSpec.<init>(ValidatingInputFileSpec.java:37)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.LoadClause(QueryParser.java:795)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:628)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:484)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:334)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:47)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:262)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:446)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:226)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:62)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:60)
        at org.apache.pig.Main.main(Main.java:265)
Caused by: java.io.IOException: Could not resolve org.apache.pig.test.udf.storefunc.StringStore using imports: [, org.apache.pig.builtin., com.yahoo.pig.yst.sds.ULT., org.apache.pig.impl.builtin.]
        at org.apache.pig.impl.util.WrappedIOException.wrap(WrappedIOException.java:16)
        at org.apache.pig.impl.PigContext.resolveClassName(PigContext.java:456)
        at org.apache.pig.impl.PigContext.instantiateFunc(PigContext.java:486)
        ... 15 more
Caused by: java.lang.ClassNotFoundException: Could not resolve org.apache.pig.test.udf.storefunc.StringStore using imports: [, org.apache.pig.builtin., com.yahoo.pig.yst.sds.ULT., org.apache.pig.impl.builtin.]
        at org.apache.pig.impl.PigContext.resolveClassName(PigContext.java:455)
        ... 16 more

2008-04-03 13:20:25,784 [main] ERROR org.apache.pig.tools.grunt.Grunt - could not instantiate 'org.apache.pig.test.udf.storefunc.StringStore' with arguments '[]'
{noformat}",,,,,,,,,,,,,,,,,,,09/Apr/08 12:49;pi_song;pig_183_v1.patch;https://issues.apache.org/jira/secure/attachment/12379726/pig_183_v1.patch,03/Apr/08 21:43;xuzh;testudf.jar;https://issues.apache.org/jira/secure/attachment/12379319/testudf.jar,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-04-04 18:01:05.138,,,no_permission,,,,,,,,,,,,38582,,,,,Wed Apr 16 18:30:27 UTC 2008,,,Patch Available,,,,0|i0gh4n:,94205,,,,,,,,,,04/Apr/08 18:01;olgan;My guess would be that your UDF.jar is not properly built.,"04/Apr/08 21:26;xuzh;How so? As you can see from the attached testudf.jar, the StringStore class can be correctly found in the org.apache.pig.test.udf.storefunc package.","09/Apr/08 12:43;pi_song;Xu,

You compiled your class using Java 6 but your Pig runs on Java 5 JVM. 

{noformat}
pi@pidev2008:~/dump/org/apache/pig/test/udf/storefunc$ javap -verbose StringStore
Compiled from ""StringStore.java""
public class org.apache.pig.test.udf.storefunc.StringStore extends java.lang.Object implements org.apache.pig.StoreFunc,org.apache.pig.LoadFunc
  SourceFile: ""StringStore.java""
  minor version: 0
  major version: 50     <=== This indicates Java 6
  Constant pool:   

{noformat}","09/Apr/08 12:49;pi_song;Mismatch java versions is something I also come across very often. I think the real problem is that our code does swallow exception. Here is the patch that exposes it as RuntimeException.

The new error message will look like this:-
{noformat}
08/04/09 22:29:44 ERROR grunt.Grunt: java.lang.RuntimeException: could not instantiate 'org.apache.pig.test.udf.storefunc.StringStore' with arguments '[]'
	at org.apache.pig.impl.PigContext.instantiateFunc(PigContext.java:509)
	at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:515)
	at org.apache.pig.impl.logicalLayer.LOLoad.<init>(LOLoad.java:54)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.LoadClause(QueryParser.java:802)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:635)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:491)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:341)
	at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:47)
	at org.apache.pig.PigServer.registerQuery(PigServer.java:262)
	at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:446)
	at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:226)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:62)
	at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:60)
	at org.apache.pig.test.TestGrunt.testUDF(TestGrunt.java:47)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:164)
	at junit.framework.TestCase.runBare(TestCase.java:130)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:120)
	at junit.framework.TestSuite.runTest(TestSuite.java:228)
	at junit.framework.TestSuite.run(TestSuite.java:223)
	at org.junit.internal.runners.OldTestClassRunner.run(OldTestClassRunner.java:35)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:38)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: java.lang.RuntimeException: java.lang.UnsupportedClassVersionError: Bad version number in .class file
	at org.apache.pig.impl.PigContext.resolveClassName(PigContext.java:449)
	at org.apache.pig.impl.PigContext.instantiateFunc(PigContext.java:491)
	... 32 more
Caused by: java.lang.UnsupportedClassVersionError: Bad version number in .class file
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:620)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:124)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:260)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:56)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:195)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:251)
	at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:319)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:242)
	at org.apache.pig.impl.PigContext.resolveClassName(PigContext.java:442)
	... 33 more
{noformat}",16/Apr/08 18:30;alangates;Fixed checked in revision 648789.  Thanks Pi.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken pipe if excuting the streaming script via the stream command directory,PIG-182,12392989,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,acmurthy,xuzh,xuzh,03/Apr/08 01:44,24/Mar/10 22:01,14/Mar/19 03:05,07/Apr/08 22:55,,,,,,,0.1.0,,,,,0,,,,"I got ""broken pipe"" exception with the following Pig script.  I also attached the Pig script and the perl script to this bug report.

{code}
A = load '/user/pig/tests/data/singlefile/studenttab10k';
B = stream A through `perl /home/xu/streamingscript/MySimpleStreamApp.pl` as (name, age, gpa); 
store B into 'results_9';
{code}

Here is Pig's console output

{noformat}
I can't find HOD configuration for piglet, hopefully you weren't planning on using HOD.
2008-04-02 18:37:29,214 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: wilbur11.labs.corp.sp1.yahoo.com:8020
2008-04-02 18:37:30,030 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - ----- MapReduce Job -----
2008-04-02 18:37:30,030 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Input: [/user/pig/tests/data/singlefile/studenttab10k:org.apache.pig.builtin.PigStorage()]
2008-04-02 18:37:30,031 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Map: [[*]->org.apache.pig.impl.eval.StreamSpec@121f1d]
2008-04-02 18:37:30,031 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Group: null
2008-04-02 18:37:30,032 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Combine: null
2008-04-02 18:37:30,032 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Reduce: null
2008-04-02 18:37:30,032 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Output: results_9:org.apache.pig.builtin.BinaryStorage
2008-04-02 18:37:30,032 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Split: null
2008-04-02 18:37:30,032 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Map parallelism: -1
2008-04-02 18:37:30,033 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Reduce parallelism: -1
219190 hdfs://wilbur11.labs.corp.sp1.yahoo.com:8020/user/pig/tests/data/singlefile/studenttab10k
2008-04-02 18:37:32,889 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Pig progress = 0%
2008-04-02 18:37:53,985 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (map) tip_200803281454_0803_m_000000 java.lang.RuntimeException: java.io.IOException: Broken pipe
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.finish(StreamSpec.java:152)
        at org.apache.pig.impl.eval.collector.DataCollector.finishPipe(DataCollector.java:131)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.run(PigMapReduce.java:119)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:208)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2071)
Caused by: java.io.IOException: Broken pipe
        at java.io.FileOutputStream.writeBytes(Native Method)
        at java.io.FileOutputStream.write(FileOutputStream.java:260)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at org.apache.pig.impl.streaming.DefaultInputHandler.close(DefaultInputHandler.java:56)
        at org.apache.pig.impl.streaming.ExecutableManager.close(ExecutableManager.java:128)
        at org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager.close(HadoopExecutableManager.java:115)
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.finish(StreamSpec.java:148)
        ... 4 more
 java.lang.RuntimeException: java.io.IOException: Broken pipe
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.finish(StreamSpec.java:152)
        at org.apache.pig.impl.eval.collector.DataCollector.finishPipe(DataCollector.java:131)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.run(PigMapReduce.java:119)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:208)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2071)
Caused by: java.io.IOException: Broken pipe
        at java.io.FileOutputStream.writeBytes(Native Method)
        at java.io.FileOutputStream.write(FileOutputStream.java:260)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at org.apache.pig.impl.streaming.DefaultInputHandler.close(DefaultInputHandler.java:56)
        at org.apache.pig.impl.streaming.ExecutableManager.close(ExecutableManager.java:128)
        at org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager.close(HadoopExecutableManager.java:115)
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.finish(StreamSpec.java:148)
        ... 4 more
 java.lang.RuntimeException: java.io.IOException: Broken pipe
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.finish(StreamSpec.java:152)
        at org.apache.pig.impl.eval.collector.DataCollector.finishPipe(DataCollector.java:131)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.run(PigMapReduce.java:119)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:208)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2071)
Caused by: java.io.IOException: Broken pipe
        at java.io.FileOutputStream.writeBytes(Native Method)
        at java.io.FileOutputStream.write(FileOutputStream.java:260)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at org.apache.pig.impl.streaming.DefaultInputHandler.close(DefaultInputHandler.java:56)
        at org.apache.pig.impl.streaming.ExecutableManager.close(ExecutableManager.java:128)
        at org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager.close(HadoopExecutableManager.java:115)
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.finish(StreamSpec.java:148)
        ... 4 more
 java.lang.RuntimeException: java.io.IOException: Broken pipe
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.finish(StreamSpec.java:152)
        at org.apache.pig.impl.eval.collector.DataCollector.finishPipe(DataCollector.java:131)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.run(PigMapReduce.java:119)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:208)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2071)
Caused by: java.io.IOException: Broken pipe
        at java.io.FileOutputStream.writeBytes(Native Method)
        at java.io.FileOutputStream.write(FileOutputStream.java:260)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at org.apache.pig.impl.streaming.DefaultInputHandler.close(DefaultInputHandler.java:56)
        at org.apache.pig.impl.streaming.ExecutableManager.close(ExecutableManager.java:128)
        at org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager.close(HadoopExecutableManager.java:115)
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.finish(StreamSpec.java:148)
        ... 4 more

2008-04-02 18:37:53,998 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0803_r_000000
2008-04-02 18:37:53,998 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0803_r_000001
2008-04-02 18:37:53,998 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0803_r_000002
2008-04-02 18:37:53,998 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0803_r_000003
2008-04-02 18:37:53,998 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0803_r_000004
2008-04-02 18:37:53,999 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0803_r_000005
2008-04-02 18:37:53,999 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0803_r_000006
2008-04-02 18:37:53,999 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0803_r_000007
2008-04-02 18:37:53,999 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0803_r_000008
2008-04-02 18:37:53,999 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0803_r_000009
2008-04-02 18:37:53,999 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0803_r_000010
2008-04-02 18:37:53,999 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0803_r_000011
2008-04-02 18:37:53,999 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0803_r_000012
2008-04-02 18:37:53,999 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0803_r_000013
2008-04-02 18:37:53,999 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0803_r_000014
2008-04-02 18:37:54,000 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0803_r_000015
2008-04-02 18:37:54,001 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0803_r_000016
2008-04-02 18:37:54,001 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0803_r_000017
2008-04-02 18:37:54,001 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0803_r_000018
2008-04-02 18:37:54,001 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0803_r_000019
2008-04-02 18:37:54,005 [main] ERROR org.apache.pig.tools.grunt.Grunt - java.io.IOException: Unable to store alias null
        at org.apache.pig.impl.util.WrappedIOException.wrap(WrappedIOException.java:16)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:283)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:446)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:226)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:62)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:60)
        at org.apache.pig.Main.main(Main.java:265)
Caused by: org.apache.pig.backend.executionengine.ExecException: java.io.IOException: Job failed
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:288)
        at org.apache.pig.PigServer.optimizeAndRunQuery(PigServer.java:400)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:280)
        ... 5 more
Caused by: java.io.IOException: Job failed
        at org.apache.pig.backend.hadoop.executionengine.POMapreduce.open(POMapreduce.java:179)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:274)
        ... 7 more

2008-04-02 18:37:54,005 [main] ERROR org.apache.pig.tools.grunt.Grunt - Unable to store alias null
{noformat}
",,,,,,,,,,,,,,,,,,,03/Apr/08 01:44;xuzh;MySimpleStreamApp.pl;https://issues.apache.org/jira/secure/attachment/12379221/MySimpleStreamApp.pl,04/Apr/08 08:59;acmurthy;PIG-182_0_20080404.patch;https://issues.apache.org/jira/secure/attachment/12379369/PIG-182_0_20080404.patch,04/Apr/08 18:32;acmurthy;PIG-182_1_20080404.patch;https://issues.apache.org/jira/secure/attachment/12379420/PIG-182_1_20080404.patch,07/Apr/08 07:32;acmurthy;PIG-182_2_20080407.patch;https://issues.apache.org/jira/secure/attachment/12379527/PIG-182_2_20080407.patch,03/Apr/08 01:44;xuzh;script.pig;https://issues.apache.org/jira/secure/attachment/12379220/script.pig,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2008-04-04 08:59:28.744,,,no_permission,,,,,,,,,,,,163835,,,,,Mon Apr 07 22:55:33 UTC 2008,,,,,,,0|i0gh3z:,94202,,,,,,,,,,"03/Apr/08 21:31;xuzh;I got the following exceptions with this Pig script.  Arun thought he could fix this issue along with this bug (PIG-182).  

{code}
A = load '/user/pig/tests/data/singlefile/studenttab10k';
B = stream A through `./streamingscript/MySimpleStreamApp.pl` as (name, age, gpa);
store B into 'results_10';
{code}

{noformat}
2008-04-03 14:19:26,682 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: wilbur11.labs.corp.sp1.yahoo.com:8020
2008-04-03 14:19:27,847 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - ----- MapReduce Job -----
2008-04-03 14:19:27,847 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Input: [/user/pig/tests/data/singlefile/studenttab10k:org.apache.pig.builtin.PigStorage()]
2008-04-03 14:19:27,847 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Map: [[*]->org.apache.pig.impl.eval.StreamSpec@121f1d]
2008-04-03 14:19:27,848 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Group: null
2008-04-03 14:19:27,848 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Combine: null
2008-04-03 14:19:27,848 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Reduce: null
2008-04-03 14:19:27,848 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Output: results_10:org.apache.pig.builtin.BinaryStorage
2008-04-03 14:19:27,848 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Split: null
2008-04-03 14:19:27,849 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Map parallelism: -1
2008-04-03 14:19:27,849 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Reduce parallelism: -1
219190 hdfs://wilbur11.labs.corp.sp1.yahoo.com:8020/user/pig/tests/data/singlefile/studenttab10k
2008-04-03 14:19:31,468 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Pig progress = 0%
2008-04-03 14:19:50,560 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (map) tip_200803281454_1076_m_000000 java.lang.RuntimeException: java.io.IOException: Cannot run program ""/home/xu/streamingscript/MySimpleStreamApp.pl"": java.io.IOException: error=2, No such file or directory
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.<init>(StreamSpec.java:132)
        at org.apache.pig.impl.eval.StreamSpec.setupDefaultPipe(StreamSpec.java:91)
        at org.apache.pig.impl.eval.CompositeEvalSpec.setupDefaultPipe(CompositeEvalSpec.java:51)
        at org.apache.pig.impl.eval.EvalSpec.setupPipe(EvalSpec.java:123)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.setupMapPipe(PigMapReduce.java:247)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.run(PigMapReduce.java:108)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:208)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2071)
Caused by: java.io.IOException: Cannot run program ""/home/xu/streamingscript/MySimpleStreamApp.pl"": java.io.IOException: error=2, No such file or directory
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:459)
        at org.apache.pig.impl.streaming.ExecutableManager.exec(ExecutableManager.java:208)
        at org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager.exec(HadoopExecutableManager.java:110)
        at org.apache.pig.impl.streaming.ExecutableManager.run(ExecutableManager.java:246)
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.<init>(StreamSpec.java:127)
        ... 7 more
Caused by: java.io.IOException: java.io.IOException: error=2, No such file or directory
        at java.lang.UNIXProcess.<init>(UNIXProcess.java:148)
        at java.lang.ProcessImpl.start(ProcessImpl.java:65)
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:452)
        ... 11 more
 java.lang.RuntimeException: java.io.IOException: Cannot run program ""/home/xu/streamingscript/MySimpleStreamApp.pl"": java.io.IOException: error=2, No such file or directory
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.<init>(StreamSpec.java:132)
        at org.apache.pig.impl.eval.StreamSpec.setupDefaultPipe(StreamSpec.java:91)
        at org.apache.pig.impl.eval.CompositeEvalSpec.setupDefaultPipe(CompositeEvalSpec.java:51)
        at org.apache.pig.impl.eval.EvalSpec.setupPipe(EvalSpec.java:123)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.setupMapPipe(PigMapReduce.java:247)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.run(PigMapReduce.java:108)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:208)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2071)
Caused by: java.io.IOException: Cannot run program ""/home/xu/streamingscript/MySimpleStreamApp.pl"": java.io.IOException: error=2, No such file or directory
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:459)
        at org.apache.pig.impl.streaming.ExecutableManager.exec(ExecutableManager.java:208)
        at org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager.exec(HadoopExecutableManager.java:110)
        at org.apache.pig.impl.streaming.ExecutableManager.run(ExecutableManager.java:246)
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.<init>(StreamSpec.java:127)
        ... 7 more
Caused by: java.io.IOException: java.io.IOException: error=2, No such file or directory
        at java.lang.UNIXProcess.<init>(UNIXProcess.java:148)
        at java.lang.ProcessImpl.start(ProcessImpl.java:65)
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:452)
        ... 11 more
 java.lang.RuntimeException: java.io.IOException: Cannot run program ""/home/xu/streamingscript/MySimpleStreamApp.pl"": java.io.IOException: error=2, No such file or directory
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.<init>(StreamSpec.java:132)
        at org.apache.pig.impl.eval.StreamSpec.setupDefaultPipe(StreamSpec.java:91)
        at org.apache.pig.impl.eval.CompositeEvalSpec.setupDefaultPipe(CompositeEvalSpec.java:51)
        at org.apache.pig.impl.eval.EvalSpec.setupPipe(EvalSpec.java:123)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.setupMapPipe(PigMapReduce.java:247)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.run(PigMapReduce.java:108)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:208)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2071)
Caused by: java.io.IOException: Cannot run program ""/home/xu/streamingscript/MySimpleStreamApp.pl"": java.io.IOException: error=2, No such file or directory
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:459)
        at org.apache.pig.impl.streaming.ExecutableManager.exec(ExecutableManager.java:208)
        at org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager.exec(HadoopExecutableManager.java:110)
        at org.apache.pig.impl.streaming.ExecutableManager.run(ExecutableManager.java:246)
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.<init>(StreamSpec.java:127)
        ... 7 more
Caused by: java.io.IOException: java.io.IOException: error=2, No such file or directory
        at java.lang.UNIXProcess.<init>(UNIXProcess.java:148)
        at java.lang.ProcessImpl.start(ProcessImpl.java:65)
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:452)
        ... 11 more
 java.lang.RuntimeException: java.io.IOException: Cannot run program ""/home/xu/streamingscript/MySimpleStreamApp.pl"": java.io.IOException: error=2, No such file or directory
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.<init>(StreamSpec.java:132)
        at org.apache.pig.impl.eval.StreamSpec.setupDefaultPipe(StreamSpec.java:91)
        at org.apache.pig.impl.eval.CompositeEvalSpec.setupDefaultPipe(CompositeEvalSpec.java:51)
        at org.apache.pig.impl.eval.EvalSpec.setupPipe(EvalSpec.java:123)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.setupMapPipe(PigMapReduce.java:247)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.run(PigMapReduce.java:108)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:208)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2071)
Caused by: java.io.IOException: Cannot run program ""/home/xu/streamingscript/MySimpleStreamApp.pl"": java.io.IOException: error=2, No such file or directory
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:459)
        at org.apache.pig.impl.streaming.ExecutableManager.exec(ExecutableManager.java:208)
        at org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager.exec(HadoopExecutableManager.java:110)
        at org.apache.pig.impl.streaming.ExecutableManager.run(ExecutableManager.java:246)
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.<init>(StreamSpec.java:127)
        ... 7 more
Caused by: java.io.IOException: java.io.IOException: error=2, No such file or directory
        at java.lang.UNIXProcess.<init>(UNIXProcess.java:148)
        at java.lang.ProcessImpl.start(ProcessImpl.java:65)
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:452)
        ... 11 more

2008-04-03 14:19:50,574 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_1076_r_000000
2008-04-03 14:19:50,574 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_1076_r_000001
2008-04-03 14:19:50,574 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_1076_r_000002
2008-04-03 14:19:50,574 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_1076_r_000003
2008-04-03 14:19:50,574 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_1076_r_000004
2008-04-03 14:19:50,574 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_1076_r_000005
2008-04-03 14:19:50,574 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_1076_r_000006
2008-04-03 14:19:50,574 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_1076_r_000007
2008-04-03 14:19:50,575 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_1076_r_000008
2008-04-03 14:19:50,575 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_1076_r_000009
2008-04-03 14:19:50,575 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_1076_r_000010
2008-04-03 14:19:50,575 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_1076_r_000011
2008-04-03 14:19:50,575 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_1076_r_000012
2008-04-03 14:19:50,575 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_1076_r_000013
2008-04-03 14:19:50,575 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_1076_r_000014
2008-04-03 14:19:50,575 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_1076_r_000015
2008-04-03 14:19:50,575 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_1076_r_000016
2008-04-03 14:19:50,575 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_1076_r_000017
2008-04-03 14:19:50,575 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_1076_r_000018
2008-04-03 14:19:50,576 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_1076_r_000019
2008-04-03 14:19:50,582 [main] ERROR org.apache.pig.tools.grunt.Grunt - java.io.IOException: Unable to store alias null
        at org.apache.pig.impl.util.WrappedIOException.wrap(WrappedIOException.java:16)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:283)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:446)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:226)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:62)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:60)
        at org.apache.pig.Main.main(Main.java:265)
Caused by: org.apache.pig.backend.executionengine.ExecException: java.io.IOException: Job failed
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:288)
        at org.apache.pig.PigServer.optimizeAndRunQuery(PigServer.java:400)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:280)
        ... 5 more
Caused by: java.io.IOException: Job failed
        at org.apache.pig.backend.hadoop.executionengine.POMapreduce.open(POMapreduce.java:180)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:274)
        ... 7 more

2008-04-03 14:19:50,582 [main] ERROR org.apache.pig.tools.grunt.Grunt - Unable to store alias null
{noformat}","04/Apr/08 00:48;xuzh;I am promoting this bug to ""blocker"" priority since I wa unable to perform many of my test cases due to this bug. ","04/Apr/08 08:59;acmurthy;Xu, after discussions with Olga we both concluded that we need to tweak semantics of ship().

Now we do not auto-ship if the file is an absolute path. Also we use DistributedCache instead of jar and hence the files are available in cwd of the task itself and with changes implemented by this patch 'myscript' will work and you don't have to use './myscript'.

So, could u please try this again without any abs path?

This patch also fixes ExecutableManager to use bash for launching the streaming command so that PATH and other env. variables can work properly ... it also has a fix to DataCollector.finishPipe to fix a error-handling bug.",04/Apr/08 09:21;acmurthy;I've updated http://wiki.apache.org/pig/PigStreamingFunctionalSpec to further refine on the ship and cache specs.,"04/Apr/08 15:49;olgan;+1 on the patch. Also looks like it includes changes for PIG-181 as well. I am working on getting PIG-181 to commit and once that's in it would be great if you regenrate the patch, thanks.",04/Apr/08 18:32;acmurthy;Fixes a typo in ExecutableManager...,"07/Apr/08 07:32;acmurthy;Patch featuring:
 * DIstributedCache now used for shipping files also
 * Better error handling in DataCollector
 * Usage of bash for better control of the streaming process' environment
 * Fixes PigMapReduce to ensure it translates all exceptions to IOException so that Hadoop can handle it correctly.
 * Includes fix for PIG-186, PIG-190 & PIG-193.",07/Apr/08 22:55;olgan;patch committed. thanks arun for fixing and xu for testing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shipping appears not working properly in this case,PIG-181,12392988,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,acmurthy,xuzh,xuzh,03/Apr/08 00:49,24/Mar/10 22:01,14/Mar/19 03:05,04/Apr/08 19:42,,,,,,,0.1.0,,,,,0,,,,"With the attached Pig script and the Perl script, I got IOException errors for being unable to find Perl script in the specified relative path.

{noformat}
I can't find HOD configuration for piglet, hopefully you weren't planning on using HOD.
2008-04-02 17:35:38,686 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: wilbur11.labs.corp.sp1.yahoo.com:8020
2008-04-02 17:35:39,556 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - ----- MapReduce Job -----
2008-04-02 17:35:39,557 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Input: [/user/pig/tests/data/singlefile/studenttab10k:org.apache.pig.builtin.PigStorage()]
2008-04-02 17:35:39,557 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Map: [[*]->org.apache.pig.impl.eval.StreamSpec@121ab80]
2008-04-02 17:35:39,557 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Group: [GENERATE {[PROJECT $0],[*]}]
2008-04-02 17:35:39,558 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Combine: null
2008-04-02 17:35:39,558 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Reduce: GENERATE {[COUNT(GENERATE {[PROJECT $1]->[PROJECT $0]})]}
2008-04-02 17:35:39,558 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Output: results_8:org.apache.pig.builtin.PigStorage
2008-04-02 17:35:39,558 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Split: null
2008-04-02 17:35:39,559 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Map parallelism: -1
2008-04-02 17:35:39,559 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Reduce parallelism: -1
219190 hdfs://wilbur11.labs.corp.sp1.yahoo.com:8020/user/pig/tests/data/singlefile/studenttab10k
2008-04-02 17:35:42,414 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Pig progress = 0%
2008-04-02 17:36:05,527 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (map) tip_200803281454_0795_m_000000 java.lang.RuntimeException: java.io.IOException: Cannot run program ""./streamingscript/MySimpleStreamApp.pl"": java.io.IOException: error=2, No such file or directory
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.<init>(StreamSpec.java:132)
        at org.apache.pig.impl.eval.StreamSpec.setupDefaultPipe(StreamSpec.java:91)
        at org.apache.pig.impl.eval.CompositeEvalSpec.setupDefaultPipe(CompositeEvalSpec.java:50)
        at org.apache.pig.impl.eval.EvalSpec.setupPipe(EvalSpec.java:123)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.setupMapPipe(PigMapReduce.java:250)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.run(PigMapReduce.java:107)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:208)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2071)
Caused by: java.io.IOException: Cannot run program ""./streamingscript/MySimpleStreamApp.pl"": java.io.IOException: error=2, No such file or directory
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:459)
        at org.apache.pig.impl.streaming.ExecutableManager.exec(ExecutableManager.java:208)
        at org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager.exec(HadoopExecutableManager.java:110)
        at org.apache.pig.impl.streaming.ExecutableManager.run(ExecutableManager.java:246)
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.<init>(StreamSpec.java:127)
        ... 7 more
Caused by: java.io.IOException: java.io.IOException: error=2, No such file or directory
        at java.lang.UNIXProcess.<init>(UNIXProcess.java:148)
        at java.lang.ProcessImpl.start(ProcessImpl.java:65)
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:452)
        ... 11 more
 java.lang.RuntimeException: java.io.IOException: Cannot run program ""./streamingscript/MySimpleStreamApp.pl"": java.io.IOException: error=2, No such file or directory
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.<init>(StreamSpec.java:132)
        at org.apache.pig.impl.eval.StreamSpec.setupDefaultPipe(StreamSpec.java:91)
        at org.apache.pig.impl.eval.CompositeEvalSpec.setupDefaultPipe(CompositeEvalSpec.java:50)
        at org.apache.pig.impl.eval.EvalSpec.setupPipe(EvalSpec.java:123)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.setupMapPipe(PigMapReduce.java:250)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.run(PigMapReduce.java:107)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:208)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2071)
Caused by: java.io.IOException: Cannot run program ""./streamingscript/MySimpleStreamApp.pl"": java.io.IOException: error=2, No such file or directory
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:459)
        at org.apache.pig.impl.streaming.ExecutableManager.exec(ExecutableManager.java:208)
        at org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager.exec(HadoopExecutableManager.java:110)
        at org.apache.pig.impl.streaming.ExecutableManager.run(ExecutableManager.java:246)
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.<init>(StreamSpec.java:127)
        ... 7 more
Caused by: java.io.IOException: java.io.IOException: error=2, No such file or directory
        at java.lang.UNIXProcess.<init>(UNIXProcess.java:148)
        at java.lang.ProcessImpl.start(ProcessImpl.java:65)
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:452)
        ... 11 more
 java.lang.RuntimeException: java.io.IOException: Cannot run program ""./streamingscript/MySimpleStreamApp.pl"": java.io.IOException: error=2, No such file or directory
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.<init>(StreamSpec.java:132)
        at org.apache.pig.impl.eval.StreamSpec.setupDefaultPipe(StreamSpec.java:91)
        at org.apache.pig.impl.eval.CompositeEvalSpec.setupDefaultPipe(CompositeEvalSpec.java:50)
        at org.apache.pig.impl.eval.EvalSpec.setupPipe(EvalSpec.java:123)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.setupMapPipe(PigMapReduce.java:250)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.run(PigMapReduce.java:107)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:208)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2071)
Caused by: java.io.IOException: Cannot run program ""./streamingscript/MySimpleStreamApp.pl"": java.io.IOException: error=2, No such file or directory
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:459)
        at org.apache.pig.impl.streaming.ExecutableManager.exec(ExecutableManager.java:208)
        at org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager.exec(HadoopExecutableManager.java:110)
        at org.apache.pig.impl.streaming.ExecutableManager.run(ExecutableManager.java:246)
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.<init>(StreamSpec.java:127)
        ... 7 more
Caused by: java.io.IOException: java.io.IOException: error=2, No such file or directory
        at java.lang.UNIXProcess.<init>(UNIXProcess.java:148)
        at java.lang.ProcessImpl.start(ProcessImpl.java:65)
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:452)
        ... 11 more
 java.lang.RuntimeException: java.io.IOException: Cannot run program ""./streamingscript/MySimpleStreamApp.pl"": java.io.IOException: error=2, No such file or directory
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.<init>(StreamSpec.java:132)
        at org.apache.pig.impl.eval.StreamSpec.setupDefaultPipe(StreamSpec.java:91)
        at org.apache.pig.impl.eval.CompositeEvalSpec.setupDefaultPipe(CompositeEvalSpec.java:50)
        at org.apache.pig.impl.eval.EvalSpec.setupPipe(EvalSpec.java:123)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.setupMapPipe(PigMapReduce.java:250)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.run(PigMapReduce.java:107)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:208)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2071)
Caused by: java.io.IOException: Cannot run program ""./streamingscript/MySimpleStreamApp.pl"": java.io.IOException: error=2, No such file or directory
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:459)
        at org.apache.pig.impl.streaming.ExecutableManager.exec(ExecutableManager.java:208)
        at org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager.exec(HadoopExecutableManager.java:110)
        at org.apache.pig.impl.streaming.ExecutableManager.run(ExecutableManager.java:246)
        at org.apache.pig.impl.eval.StreamSpec$StreamDataCollector.<init>(StreamSpec.java:127)
        ... 7 more
Caused by: java.io.IOException: java.io.IOException: error=2, No such file or directory
        at java.lang.UNIXProcess.<init>(UNIXProcess.java:148)
        at java.lang.ProcessImpl.start(ProcessImpl.java:65)
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:452)
        ... 11 more

2008-04-02 17:36:05,533 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0795_r_000000
2008-04-02 17:36:05,533 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0795_r_000001
2008-04-02 17:36:05,533 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0795_r_000002
2008-04-02 17:36:05,533 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0795_r_000003
2008-04-02 17:36:05,533 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0795_r_000004
2008-04-02 17:36:05,534 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0795_r_000005
2008-04-02 17:36:05,534 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0795_r_000006
2008-04-02 17:36:05,534 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0795_r_000007
2008-04-02 17:36:05,534 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0795_r_000008
2008-04-02 17:36:05,534 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0795_r_000009
2008-04-02 17:36:05,534 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0795_r_000010
2008-04-02 17:36:05,534 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0795_r_000011
2008-04-02 17:36:05,535 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0795_r_000012
2008-04-02 17:36:05,535 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0795_r_000013
2008-04-02 17:36:05,535 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0795_r_000014
2008-04-02 17:36:05,535 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0795_r_000015
2008-04-02 17:36:05,535 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0795_r_000016
2008-04-02 17:36:05,535 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0795_r_000017
2008-04-02 17:36:05,535 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0795_r_000018
2008-04-02 17:36:05,536 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200803281454_0795_r_000019
2008-04-02 17:36:05,539 [main] ERROR org.apache.pig.tools.grunt.Grunt - java.io.IOException: Unable to store alias null
        at org.apache.pig.impl.util.WrappedIOException.wrap(WrappedIOException.java:16)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:283)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:446)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:226)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:62)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:60)
        at org.apache.pig.Main.main(Main.java:265)
Caused by: org.apache.pig.backend.executionengine.ExecException: java.io.IOException: Job failed
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:288)
        at org.apache.pig.PigServer.optimizeAndRunQuery(PigServer.java:400)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:280)
        ... 5 more
Caused by: java.io.IOException: Job failed
        at org.apache.pig.backend.hadoop.executionengine.POMapreduce.open(POMapreduce.java:179)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:274)
        ... 7 more

2008-04-02 17:36:05,539 [main] ERROR org.apache.pig.tools.grunt.Grunt - Unable to store alias null
{noformat}",,,,,,,,,,,,,,,,,,,03/Apr/08 01:00;xuzh;MySimpleStreamApp.pl;https://issues.apache.org/jira/secure/attachment/12379216/MySimpleStreamApp.pl,03/Apr/08 18:39;acmurthy;PIG-181_0_20080403.patch;https://issues.apache.org/jira/secure/attachment/12379302/PIG-181_0_20080403.patch,03/Apr/08 00:49;xuzh;script.pig;https://issues.apache.org/jira/secure/attachment/12379214/script.pig,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2008-04-03 18:39:16.06,,,no_permission,,,,,,,,,,,,163834,,,,,Fri Apr 04 19:42:26 UTC 2008,,,,,,,0|i0gh3j:,94200,,,,,,,,,,"03/Apr/08 18:39;acmurthy;Attached fix: 
1. POMapreduce fixed to ensure it correctly saves 'properties' right in a couple of cases (addInputFile & copy)
2. CompositeEvalSpec fixed to ensure it keeps properties of its components.",03/Apr/08 21:24;xuzh;+1 as I saw it work with Arun's new jar file.,04/Apr/08 19:42;olgan;patch committed. Thanks Arun for fixing this but and thanks Xu for finding and verifying it,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BinaryStorage is implicitly used where it should not be,PIG-180,12392983,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,acmurthy,xuzh,xuzh,02/Apr/08 22:54,24/Mar/10 22:01,14/Mar/19 03:05,03/Apr/08 19:13,,,,,,,0.1.0,,,,,0,,,,"When I ran the attached Pig script, I got unexpected resulting output data.  After a closer at Pig's console output, it appeared BinaryStorage is incorrectly used:

{noformat}
I can't find HOD configuration for piglet, hopefully you weren't planning on using HOD.
2008-04-02 15:39:11,668 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: wilbur11.labs.corp.sp1.yahoo.com:8020
2008-04-02 15:39:12,478 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - ----- MapReduce Job -----
2008-04-02 15:39:12,478 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Input: [/user/pig/tests/data/singlefile/studenttab10k:org.apache.pig.builtin.PigStorage()]
2008-04-02 15:39:12,478 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Map: [[*]->org.apache.pig.impl.eval.StreamSpec@181edf4]
2008-04-02 15:39:12,479 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Group: null
2008-04-02 15:39:12,479 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Combine: null
2008-04-02 15:39:12,479 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Reduce: null
2008-04-02 15:39:12,479 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Output: results_7:org.apache.pig.builtin.BinaryStorage
2008-04-02 15:39:12,479 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Split: null
2008-04-02 15:39:12,480 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Map parallelism: -1
2008-04-02 15:39:12,480 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Reduce parallelism: -1
219190 hdfs://wilbur11.labs.corp.sp1.yahoo.com:8020/user/pig/tests/data/singlefile/studenttab10k
2008-04-02 15:39:15,308 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Pig progress = 0%
2008-04-02 15:39:20,329 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Pig progress = 50%
2008-04-02 15:39:26,355 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Pig progress = 60%
2008-04-02 15:39:28,395 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Pig progress = 80%
2008-04-02 15:39:30,407 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Pig progress = 90%
2008-04-02 15:39:32,415 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Pig progress = 100%
{noformat}",,,,,,,,,,,,,,,,,,,03/Apr/08 01:23;acmurthy;PIG-180_0_20080402.patch;https://issues.apache.org/jira/secure/attachment/12379217/PIG-180_0_20080402.patch,02/Apr/08 22:58;xuzh;script.pig;https://issues.apache.org/jira/secure/attachment/12379198/script.pig,02/Apr/08 22:59;xuzh;streaming.pl;https://issues.apache.org/jira/secure/attachment/12379199/streaming.pl,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2008-04-03 01:23:53.478,,,no_permission,,,,,,,,,,,,163833,,,,,Thu Apr 03 19:13:39 UTC 2008,,,,,,,0|i0gh33:,94198,,,,,,,,,,"03/Apr/08 01:23;acmurthy;This patch fixes a bug in the {Load|Store}Optimizer where the input/output {Load|Store}Func of the StreamingCommand wasn't being set correctly in some cases when it got flipped to BinaryStorage.
",03/Apr/08 01:33;xuzh;+1 as I saw Arun's new pig.jar work.,03/Apr/08 19:13;olgan;all tests passed. Patch committed. Thanks Arun for contributing and Xu for testing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"On hadoop 0.16, some jobs using combiner fail with an NPE",PIG-179,12392979,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,02/Apr/08 21:28,24/Mar/10 22:01,14/Mar/19 03:05,11/Apr/08 13:58,0.0.0,,,,,,0.1.0,,,,,0,,,,"Some jobs (it appears to only be larger jobs) now fail with an NPE in the combiner code on this line:

{code}
PigSplit split = PigInputFormat.PigRecordReader.getPigRecordReader().getPigFileSplit();
{code}

Looking into the PigRecordReader a comment in the class indicates that, as implemented, it depends on the mapper and splitter (and in this case the combiner as well) running in the same thread.  It seems that in some cases in hadoop 0.16 this is no longer the case.",Hadoop 0.16,,,,,,,,,,,,,,,,,,02/Apr/08 22:06;alangates;PIG-179.patch;https://issues.apache.org/jira/secure/attachment/12379191/PIG-179.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-04-02 22:15:35.983,,,no_permission,,,,,,,,,,,,163832,,,,,Fri Apr 11 13:58:37 UTC 2008,,,,,,,0|i0gh2n:,94196,,,,,,,,,,"02/Apr/08 22:08;alangates;A patch that removes the ThreadLocal modifier for the PigRecordReader.  According to Ben Reed (who wrote this) he originally made it thread local because he was concerned that hadoop might change to run multiple maps in the same JVM.  As that does not now seem likely converting this ThreadLocal to static will be safe and not cause and NPE in cases where the RecordReader, Mapper, and Combiner aren't all running in the same thread.",02/Apr/08 22:15;olgan;Looks good. +1,02/Apr/08 22:19;alangates;Fix checked in.,02/Apr/08 23:32;alangates;This patch conflicts with the fix for PIG-55.  I'll regenerate it and attach the new patch.,"11/Apr/08 13:58;alangates;Patch regenerated, tested, and checked in at revision 647166.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use of schema on a secondary output of SPLIT throws IndexOutOfBoundsException,PIG-178,12392918,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kali,kali,kali,02/Apr/08 09:50,24/Mar/10 22:01,14/Mar/19 03:05,11/Apr/08 16:12,,,,,,,0.1.0,,impl,,,0,,,,outputSchema for LOSplitOutput is trivialy broken. patch including testcase and fix are coming.,not relevant,,,,,,,,,,,,,,,,,,02/Apr/08 09:54;kali;PigSplit.patch;https://issues.apache.org/jira/secure/attachment/12379115/PigSplit.patch,02/Apr/08 09:54;kali;TestPigSplit.patch;https://issues.apache.org/jira/secure/attachment/12379114/TestPigSplit.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-04-02 15:12:56.426,,,no_permission,,,,,,,,,,,,38588,,,,,Fri Apr 11 16:12:42 UTC 2008,,,Patch Available,,,,0|i0gh27:,94194,,,,,,,,,,"02/Apr/08 09:54;kali;Fix and TestCase.

The test case actualy covers more than required for this fix.",02/Apr/08 15:12;alangates;Marking as patch available.,03/Apr/08 13:40;pi_song;+1 Trivial bug. Another end-to-end test but I'm fine with it.,"10/Apr/08 09:16;kali;Could we please get the patch commited ? I'm quite confident it won't break anything...

Thanks...",11/Apr/08 16:12;alangates;Fix checked in revision 647208.  Thanks Mathieu.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig creates many small files when it spills,PIG-176,12392793,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,olgan,olgan,01/Apr/08 00:38,24/Mar/10 22:01,14/Mar/19 03:05,02/May/08 20:55,,,,,,,0.1.0,,,,,0,,,,"Currently, on spill pig can generate millions of small (under 128K) files. Partially this is due to PIG-170 but even with that patch, you can still try and spill small bags.

The proposal is to not spill small files. Alan told me that the logic is already there but we just need to bump the size limit.",,,,,,,,,,,,,,,,,,,18/Apr/08 14:28;pi_song;pig176_v2.patch;https://issues.apache.org/jira/secure/attachment/12380505/pig176_v2.patch,09/Apr/08 14:29;pi_song;pig_176_smallbags_v1.patch;https://issues.apache.org/jira/secure/attachment/12379732/pig_176_smallbags_v1.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-04-03 11:59:04.878,,,no_permission,,,,,,,,,,,,163830,,,,,Fri May 02 20:55:00 UTC 2008,,,,,,,0|i0gh1b:,94190,,,,,,,,,,"03/Apr/08 11:59;pi_song;So let's say if the size is smaller than something, don't spill right? This is very easy to fix but we will be able to reclaim a bit less memory than before therefore causing some tasks to fail more often in exchange for some tasks running faster. Is this acceptable?

Probably the best way to go is to make it configurable but Pig-111 isn't in yet. Sighhh..... I want to have more time.","03/Apr/08 16:08;olgan;Pi,

Running faster is part of it. The other part is not to fill up disks with tiny files which causes disk frgamentation and also takes forever to cleanup at the end of processing though you suggestion of cleaning as we go might help that a bit.","07/Apr/08 13:32;pi_song;Based on the fact that now we spill big bags first, my observation is that there are still cases where a big container bag is spilled and therefore its mContent becomes empty but most of its inner bags' WeakReferences aren't clean-up by GC yet. In such cases, if we haven't freed up enough memory, those inner bags will be unnecessarily spilled (however all their contents were already spilled in the big bag spill). Possibly that are 2 simple ways to solve this:- 

1) In SpillableMemoryManager, we try putting Thread.yield() in between each spill. This should allow some more time for GC to do more clean-up without degrading performance too much. However, if the main execution thread doesn't produce any bag (e.g. a map task where all keys and values are tuples and atomic data), this will give more time to the main execution thread to use up more memory more quickly.

2) Check the size of the current spillable being spilled. If it is larger than constant X, do a System.GC(). This is safer than (1) but due to the fact that we explicitly call GC more often, it may have some impact on performance. However, by considering the fact that spilling small files is much slower than doing System.GC(), this approach should then generally give a better performance.

I don't really have a processing task that incurs spilling that much. Can anyone please try (2) out?","09/Apr/08 14:29;pi_song;This patch implements (1) Spill file size threshold  (2)My idea in the last comment

""spill.size.threshold"" and ""spill.gc.activation.size"" are to be set as JVM parameters or .pigrc in order to use this new feature. Default values are 0 and Long.MAX_VALUE respectively.

There is a bit of problem in (1) that Bag.getMemorySize() sometimes doesn't return accurate value so even the threshold is set, it's still possible that files smaller than the threshold are created.

The configuration code is still messy in MapReduceLauncher. This needs a clean-up after the configuration patch gets in.","17/Apr/08 15:47;alangates;Pi,

Did you want to rework this patch now since PIG-111 is in and you can read the properties from pig's Properties object rather than System.getProperties()?","17/Apr/08 23:10;pi_song;OK, will do that.",18/Apr/08 14:28;pi_song;Updated with the latest trunk + make use of the new configuration structure,02/May/08 20:55;alangates;Patch checked in at revision 652906.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig hangs at progress 0% in this case,PIG-174,12392613,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,acmurthy,xuzh,xuzh,28/Mar/08 23:53,24/Mar/10 22:01,14/Mar/19 03:05,03/Apr/08 19:14,,,,,,,0.1.0,,,,,0,,,,"Pig appears to hang at progress 0% with the following console output:

{noformat}
2008-03-28 13:58:50,398 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: wilbur11.labs.corp.sp1.yahoo.com:8020
2008-03-28 13:58:51,342 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - ----- MapReduce Job -----
2008-03-28 13:58:51,343 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Input: [/user/pig/tests/data/singlefile/studenttab10k:org.apache.pig.builtin.PigStorage()]
2008-03-28 13:58:51,343 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Map: [[*]->org.apache.pig.impl.eval.StreamSpec@16fa474]
2008-03-28 13:58:51,343 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Group: null
2008-03-28 13:58:51,343 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Combine: null
2008-03-28 13:58:51,344 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Reduce: null
2008-03-28 13:58:51,344 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Output: results_7:org.apache.pig.builtin.PigStorage
2008-03-28 13:58:51,344 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Split: null
2008-03-28 13:58:51,344 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Map parallelism: -1
2008-03-28 13:58:51,344 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Reduce parallelism: -1
2008-03-28 13:58:54,245 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Pig progress = 0%
{noformat}

The contents of the pig script used is as follows:

{code}
define X `./streamingscript/MySimpleStreamApp.pl` ship('./streamingscript/MySimpleStreamApp.pl');
A = load '/user/pig/tests/data/singlefile/studenttab10k';
B = stream A through X;
store B into 'results_7';
{code}

The streaming script ""MySimpleStreamApp.pl"" is as follows and it is located under the streamingscript directory in my home directory.  I issued the pig command from my home directory.

{code}
#!/home/y/bin/perl

while (<>) {
    chomp;
    print ""$_\n"";
}
{code}",,,,,,,,,,,,,,,,,,,03/Apr/08 01:25;acmurthy;PIG-174_0_20080402.patch;https://issues.apache.org/jira/secure/attachment/12379218/PIG-174_0_20080402.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-03-29 00:49:42.313,,,no_permission,,,,,,,,,,,,163828,,,,,Thu Apr 03 19:14:19 UTC 2008,,,,,,,0|i0gh0f:,94186,,,,,,,,,,"29/Mar/08 00:49;acmurthy;Xu, thanks for the detailed report.

Could you check if /home/y/bin/perl is actually present on your cluster?

I ran the same script on my machine (with the same data) with /usr/bin/perl and seems to work without any issues...","29/Mar/08 16:33;olgan;Arun, couple of comments:

(1) I will check on Monday but my guess is that perl is there but not in /home/y/bin
(2) The main problem here is that we hang and almost bring the cluster down. This is the one we need to figure out and resolve
(3) Xu was running on piglet so you can try it there to reproduce the problem. Please, don't do it till Monday though as I am running some tests and this pretty much kills the cluster",31/Mar/08 07:31;acmurthy;I've update the milestone3 patch in PIG-94 with better error handling which fixes the observed bad behaviour...,"31/Mar/08 21:02;xuzh;I just took a look and there was no /home/y/ on that cluster.  Arun, please let me know when you are done with cluster.  ","01/Apr/08 02:56;pi_song;Arun,

Remember what we've discussed before? If you keep polling the child process status, this should not happen.","01/Apr/08 14:52;acmurthy;Pi, this was more of a error handling issue - the script could not be exec'ed and the exception didn't lead to Map-Reduce task failure due to a bug in hadoop-0.16, I've worked around that to ensure that it failed the task.",02/Apr/08 06:23;acmurthy;Fixed as a part of PIG-94.,"02/Apr/08 21:33;xuzh;It still hangs if a non-existing location for perl is specified in the perl script.  For example, at the very beginning of the script if you have something like this:

#!/nonexistingpath/bin/perl

However, this should be a negative test case so I will lower the priority of the bug.  On a related note, if an existing is specified, the test case passes successfully. 

Also, I might have brought down the cluster.  Can someone restart it again?","03/Apr/08 01:25;acmurthy;This patch fixes the error handling of the failure to setup the eval pipeline, the absence of which led to the observed my-bad-ness.",03/Apr/08 01:33;xuzh;+1 as I saw it work with Arun's new pig.jar,03/Apr/08 12:45;pi_song;+1 setupMapPipe is supposed to be in there since the beginning.,03/Apr/08 19:14;olgan;all tests passed; patch committed. Thanks Arun for contributing and Xu for testing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException when running my Pig script with streaming,PIG-173,12392607,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,acmurthy,xuzh,xuzh,28/Mar/08 22:50,24/Mar/10 22:01,14/Mar/19 03:05,02/Apr/08 06:25,,,,,,,0.1.0,,,,,0,,,,"Execute the following Pig statements in Grunt or with a Pig script, and a NullPointerException is generated.

{noformat}
grunt> A = load '/user/pig/tests/data/singlefile/studentcolon10k' using PigStorage(':');
grunt> B = foreach A generate $2, $1, $0;
grunt> define X `awk 'BEGIN {FS = "",""; OFS = ""\t""} {print $3, $2, $1}'` input (stdin using
PigStorage(','));
grunt> C = stream B through X;
grunt> D = foreach C generate $0, $1;
2008-03-28 14:17:20,978 [main] ERROR org.apache.pig.tools.grunt.GruntParser - java.lang.NullPointerException
        at org.apache.pig.impl.logicalLayer.LOEval.outputSchema(LOEval.java:69)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.ForEachClause(QueryParser.java:1508)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:691)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:492)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:342)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:47)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:261)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:462)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:226)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:73)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:54)
        at org.apache.pig.Main.main(Main.java:253)

2008-03-28 14:17:20,979 [main] ERROR org.apache.pig.tools.grunt.GruntParser - java.lang.NullPointerException
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-04-02 06:25:26.325,,,no_permission,,,,,,,,,,,,163827,,,,,Wed Apr 02 06:25:26 UTC 2008,,,,,,,0|i0gh07:,94185,,,,,,,,,,02/Apr/08 06:25;acmurthy;Fixed as a part of PIG-94.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException thrown if we catch exception with null message,PIG-172,12392590,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,28/Mar/08 19:49,24/Mar/10 22:01,14/Mar/19 03:05,28/Mar/08 21:16,,,,,,,0.1.0,,,,,0,,,,"grunt> A = load '/user/pig/tests/data/singlefile/studentcolon10k' using PigStorage(':');
grunt> B = foreach A generate $2, $1, $0;
grunt> define X `awk 'BEGIN {FS = "",""; OFS = ""\t""} {print $3, $2, $1}'` input (stdin using PigStorage(','));
grunt> C = stream B through X;
grunt> D = foreach C generate a, b;
java.lang.NullPointerException
        at org.apache.pig.tools.grunt.Utils.getPermissionException(Utils.java:24)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:77)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:54)
        at org.apache.pig.Main.main(Main.java:253)
",,,,,,,,,,,,,,,,,,,28/Mar/08 19:56;olgan;PIG-172.patch;https://issues.apache.org/jira/secure/attachment/12378811/PIG-172.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-03-28 20:40:38.78,,,no_permission,,,,,,,,,,,,163826,,,,,Fri Mar 28 21:16:20 UTC 2008,,,,,,,0|i0ggzr:,94183,,,,,,,,,,28/Mar/08 20:40;alangates;+1 on the patch,28/Mar/08 21:16;olgan;patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory manager spills bags in the wrong order,PIG-170,12392464,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,amirhyoussefi,olgan,olgan,27/Mar/08 17:02,24/Mar/10 22:01,14/Mar/19 03:05,29/Mar/08 15:55,,,,,,,0.1.0,,,,,0,,,,"For optimal performance, we want to spill the largest bags first. This is not what is happening right now and could be causing some of our memory issues.",,,,,,,,,,,,,,,,,,,27/Mar/08 18:26;amirhyoussefi;PIG-170_0_20080327.patch;https://issues.apache.org/jira/secure/attachment/12378736/PIG-170_0_20080327.patch,28/Mar/08 15:16;pi_song;compareMemUsage.gif;https://issues.apache.org/jira/secure/attachment/12378794/compareMemUsage.gif,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-03-27 18:25:06.403,,,no_permission,,,,,,,,,,,,163824,,,,,Sat Mar 29 16:35:15 UTC 2008,,,,,,,0|i0ggyv:,94179,,,,,,,,,,27/Mar/08 18:25;amirhyoussefi;Ben and I had a closer look at spilling and identified this issue but assessing the impact of this patch is not complete yet. I am conducting a few long-running tests in comming days. Attaching patch per request.,28/Mar/08 02:25;alangates;+1,"28/Mar/08 13:42;pi_song;In certain scenarios, for example, a lot of small intermediate bags are used to calculate something and outputs are stored in a big bag. The big bag will become the first node after
{noformat}
Collections.sort(spillables, new Comparator<WeakReference<Spillable>>()
{noformat}

 and the below code will no longer work.
{noformat}
    public void registerSpillable(Spillable s) {
        synchronized(spillables) {
            // Cleaing the entire list is too expensive.  Just trim off the front while
            // we can.
            WeakReference<Spillable> first = spillables.peek();
            while (first != null && first.get() == null) {
                spillables.remove();
                first = spillables.peek();
            }
            spillables.add(new WeakReference<Spillable>(s));
        }
    }
{noformat}
I","28/Mar/08 15:16;pi_song;By profiling memory usage, there is no significant difference. My system might be too small.

   A gap between horizonal grid lines = 50MB
   Input size = 540MB    
   ChildTask process Max Heap = 200MB (Only 1 process)

   a1 = group (load 'file:/tmp/bigfile1') ALL 
   store a1 into '/tmp/out3' USING BinStorage() ;
","28/Mar/08 17:21;alangates;Pi is correct that there are scenarios where a big bag could be sorted ahead of smaller bags that will empty faster.  But to get into this circumstance, a very specific set of conditions have to occur:

1) many small bags are created
2) small bags are moved into large bag, without yet being released
3) spill happens, forcing a sort of the linked list
4) small bags go out of scope

This set of events seems fairly rare.  And even when it does occur, the worst that happens is we are not as aggressive as we could be about cleaning the list.  In the very worst case it will cause an early spill.

We cannot clean the entire list on every register call, as that is far too expensive (I tried it, it slowed performance by an order of magnitude on large scripts).  We want to spill large bags first so that we spill as few bags as possible.  We could change the code to copy the list and sort that copy, thus avoiding reordering the existing list.  However, once we're in the spill code, we are in a low memory situation.  Copying a potentially large list to sort it is a bad idea in that case.  So I don't see a better solution.","28/Mar/08 17:25;olgan;Given, what alan described I feel that we should commit the patch since it will improve common case. Objections?","28/Mar/08 18:37;breed;+1 I agree it should be committed. I should also point out that this doesn't invalidate the cleanup in the registerSpillable. That code will still clean up weak references of bags that have been freed since the last GC, which is exactly what it is supposed to do. We do a global cleanup after a GC when our threshold callback is invoked.","28/Mar/08 18:52;olgan;ok, will commit it shortly","28/Mar/08 22:46;pi_song;+1, no objection.",29/Mar/08 07:48;amirhyoussefi;Initial results of tests showed that application of PIG-170 makes a job fail (it was not failing before application).,29/Mar/08 07:57;amirhyoussefi;Now I am investigating whether this failure is directly related to PIG-170 or it's cluster-wide issue happening because we spill more data to Disks.,29/Mar/08 15:55;olgan;patch committed. Thanks Amir for contributing!,"29/Mar/08 16:35;olgan;Amir, I did not see your comments and I committed the patch. Please, update your bug with the results of the investigation so that we can decide whether to roll it back.

Alan, please, help amir to figure out the disk full issue, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
In scripts that create large groups pig runs out of memory,PIG-164,12391973,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,20/Mar/08 16:24,24/Mar/10 22:01,14/Mar/19 03:05,20/Mar/08 19:01,0.0.0,,,,,,0.1.0,,impl,,,0,,,,"Scripts that need to group large amounts of data, such as a group all with 20m records, often die with errors indicating that no more memory can be allocated.  PIG-40 addressed this somewhat, but not completely.  In fact, it appears that in some situations it made it worse.  If a script creates many data bags it can now run out of memory tracking all those data bags that it may need to spill even if none of those bags gets very large.

The issue is that the fix to PIG-40 introduced a memory manager that has a LinkedList of WeakReferences that it uses to track these data bags.  When it is told by the memory manager to dump memory, it walks this LinkedList, cleaning any entries that have gone stale and dumping any that are still valid.  The problem is that in a script that processes many rows, the LinkedList itself grows very large, and becomes the cause of needing to dump memory.",,,,,,,,,,,,,,,,,,,20/Mar/08 16:33;alangates;PIG-164.patch;https://issues.apache.org/jira/secure/attachment/12378322/PIG-164.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-03-20 16:53:16.117,,,no_permission,,,,,,,,,,,,163818,,,,,Thu Mar 20 19:01:26 UTC 2008,,,,,,,0|i0ggw7:,94167,,,,,,,,,,"20/Mar/08 16:33;alangates;The attached patch addresses the issue by changing the memory manager to do some cleaning of the list whenever a databag is registered.  

I tried two previous approaches that did not work.

First, I had the memory manager spin a separate thread that woke up every five seconds and cleaned the list.  For reasons that are entirely unclear to me this solution ran out of memory faster than before.

Second, I had the register call clean the entire list.  This proved to be far too expensive, and slowed down performance by about 10x.  

So, in this final register begins searching at the head of list, cleaning any weak references it can.  As soon as it encounters an entry in the list that is valid, it quits looking.  This avoids long searches through the list when most of the entries are valid.  It rests on the assumption that data bags generally live about the same amount of time, thus there won't be a long lived databag at the head of the list blocking the cleaning of many stale references later in the list.",20/Mar/08 16:53;olgan;Looks good. +1,20/Mar/08 17:11;breed;+1 excellent,20/Mar/08 19:01;alangates;Fix checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
All 4 unit test cases for Pig streaming fails on Windows,PIG-156,12391810,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,,xuzh,xuzh,18/Mar/08 23:51,24/Mar/10 22:01,14/Mar/19 03:05,19/May/08 17:59,,,,,,,0.1.0,,,,,0,,,,"All 4 test cases in TestStreaming.java fails on Windows. Since now Windows is an officially supported platform by Pig, which means builds and unit tests should succeed on it, I gave this bug a priority of ""Blocker"". 

Here is a sample output of TestStreaming on Windows:

{noformat}
Testcase: testSimpleMapSideStreaming took 27.691 sec
        Caused an ERROR
expected:<(A, 5)> but was:<null>
Java.lang.AssertionError: expected:<(A, 5)> but was:<null>
        at org.junit.Assert.fail(Assert.java:69)
        at org.junit.Assert.failNotEquals(Assert.java:314)
        at org.junit.Assert.assertEquals(Assert.java:94)
        at org.junit.Assert.assertEquals(Assert.java:104)
        at org.apache.pig.test.Util.checkQueryOutputs(Util.java:111)
        at org.apache.pig.test.TestStreaming.testSimpleMapSideStreaming(TestStreaming.java:75)

Testcase: testSimpleMapSideStreamingWithOutputSchema took 23.63 sec
        Caused an ERROR
expected:<(C, 8)> but was:<null>
java.lang.AssertionError: expected:<(C, 8)> but was:<null>
        at org.junit.Assert.fail(Assert.java:69)
        at org.junit.Assert.failNotEquals(Assert.java:314)
        at org.junit.Assert.assertEquals(Assert.java:94)
        at org.junit.Assert.assertEquals(Assert.java:104)
        at org.apache.pig.test.Util.checkQueryOutputs(Util.java:111)
        at org.apache.pig.test.TestStreaming.testSimpleMapSideStreamingWithOutputSchema(TestStreaming.java:102)

Testcase: testSimpleReduceSideStreamingAfterFlatten took 13.494 sec
        Caused an ERROR
expected:<(A, 5)> but was:<null>
java.lang.AssertionError: expected:<(A, 5)> but was:<null>
        at org.junit.Assert.fail(Assert.java:69)
        at org.junit.Assert.failNotEquals(Assert.java:314)
        at org.junit.Assert.assertEquals(Assert.java:94)
        at org.junit.Assert.assertEquals(Assert.java:104)
        at org.apache.pig.test.Util.checkQueryOutputs(Util.java:111)
        at org.apache.pig.test.TestStreaming.testSimpleReduceSideStreamingAfterFlatten(TestStreaming.java:131)

Testcase: testSimpleOrderedReduceSideStreamingAfterFlatten took 19.866 sec
        Caused an ERROR
expected:<(A, 1, 2, 3)> but was:<null>
java.lang.AssertionError: expected:<(A, 1, 2, 3)> but was:<null>
        at org.junit.Assert.fail(Assert.java:69)
        at org.junit.Assert.failNotEquals(Assert.java:314)
        at org.junit.Assert.assertEquals(Assert.java:94)
        at org.junit.Assert.assertEquals(Assert.java:104)
        at org.apache.pig.test.Util.checkQueryOutputs(Util.java:111)
        at org.apache.pig.test.TestStreaming.testSimpleOrderedReduceSideStreamingAfterFlatten(TestStreaming.java:175)
{noformat}",Windows XP,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-05-19 17:59:27.714,,,no_permission,,,,,,,,,,,,163810,,,,,Mon May 19 17:59:27 UTC 2008,,,,,,,0|i0ggsn:,94151,,,,,,,,,,19/May/08 17:59;olgan;Opened generic windows issue: https://issues.apache.org/jira/browse/PIG-243,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect results when there is a dump in between statements. ,PIG-153,12391711,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pi_song,amirhyoussefi,amirhyoussefi,18/Mar/08 07:05,24/Mar/10 22:01,14/Mar/19 03:05,09/Apr/08 04:37,,,,,,,0.1.0,,,,,0,,,,"Following scenario is with Pig + Hadoop. 

A similar run with Local Pig showed correct results.


Here is test file data/test/test2.txt: 

a1	1	5700
b1	2	2001
c2	2	

I run the following script step by step:

grunt> a = load 'data/test/test2.txt';
grunt> dump a;
2008-03-18 06:41:55,163 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - ----- MapReduce Job ---     --
2008-03-18 06:41:55,163 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Input: [/user/amiry/dat     a/test/test2.txt:org.apache.pig.builtin.PigStorage()]
2008-03-18 06:41:55,163 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Map: [[*]]
2008-03-18 06:41:55,163 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Group: null
2008-03-18 06:41:55,163 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Combine: null
2008-03-18 06:41:55,163 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Reduce: null
2008-03-18 06:41:55,163 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Output: /tmp/temp135967     7959/tmp-246846292:org.apache.pig.builtin.BinStorage
2008-03-18 06:41:55,163 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Split: null
2008-03-18 06:41:55,163 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Map parallelism: -1
2008-03-18 06:41:55,163 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Reduce parallelism: -1
2008-03-18 06:41:57,472 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Pig      progress = 0%
2008-03-18 06:41:58,477 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Pig      progress = 50%
2008-03-18 06:42:04,495 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Pig      progress = 100%
(a1, 1, 5700)
(b1, 2, 2001)
(c2, 2, )
grunt> b = filter a by $0 eq 'a1';
grunt> dump b;
2008-03-18 06:42:23,881 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - ----- MapReduce Job ---     --
2008-03-18 06:42:23,881 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Input: [/tmp/temp135967     7959/tmp-246846292:org.apache.pig.builtin.BinStorage]
2008-03-18 06:42:23,881 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Map: [[*]->[FILTER BY (     [PROJECT $0] eq ['a1'])]]
2008-03-18 06:42:23,882 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Group: null
2008-03-18 06:42:23,882 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Combine: null
2008-03-18 06:42:23,882 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Reduce: null
2008-03-18 06:42:23,882 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Output: /tmp/temp135967     7959/tmp1851797397:org.apache.pig.builtin.BinStorage
2008-03-18 06:42:23,882 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Split: null
2008-03-18 06:42:23,882 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Map parallelism: -1
2008-03-18 06:42:23,882 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Reduce parallelism: -1
2008-03-18 06:42:25,938 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Pig      progress = 0%
2008-03-18 06:42:28,946 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Pig      progress = 50%
2008-03-18 06:42:34,963 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Pig      progress = 100%
(a1, 1, 5700)
grunt> c = filter a by $0 eq 'b1';
grunt> dump c;
2008-03-18 06:42:59,884 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - ----- MapReduce Job ---     --
2008-03-18 06:42:59,884 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Input: [/tmp/temp135967     7959/tmp1851797397:org.apache.pig.builtin.BinStorage]
2008-03-18 06:42:59,885 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Map: [[*]->[FILTER BY (     [PROJECT $0] eq ['b1'])]]
2008-03-18 06:42:59,885 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Group: null
2008-03-18 06:42:59,885 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Combine: null
2008-03-18 06:42:59,885 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Reduce: null
2008-03-18 06:42:59,885 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Output: /tmp/temp135967     7959/tmp-1157182212:org.apache.pig.builtin.BinStorage
2008-03-18 06:42:59,885 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Split: null
2008-03-18 06:42:59,885 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Map parallelism: -1
2008-03-18 06:42:59,885 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Reduce parallelism: -1
2008-03-18 06:43:01,964 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Pig      progress = 0%
2008-03-18 06:43:04,974 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Pig      progress = 50%
2008-03-18 06:43:06,980 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Pig      progress = 100%
grunt>


Meaning c is empty.


",Pig + Hadoop,,,,,,,,,,,,,,,,,,18/Mar/08 13:27;pi_song;PIG_153_fix_optimization.patch;https://issues.apache.org/jira/secure/attachment/12378124/PIG_153_fix_optimization.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-03-18 13:27:36.884,,,no_permission,,,,,,,,,,,,38586,,,,,Wed Apr 09 04:37:30 UTC 2008,,,Patch Available,,,,0|i0ggrb:,94145,,,,,,,,,,"18/Mar/08 07:09;amirhyoussefi;This may be a clue: 

 Comparing Input and Output paths above we see that input of ""c"" calculation is using output of ""b"" even though c is dependent on a only.","18/Mar/08 13:27;pi_song;This one-line fix took me an hour to hunt!!! The problem that causes this is in MapReduce  optimization where already-executed sub-plans are catched and reused as input of new plans.

Unit test only does test the reported scenario. Other than that I couldn't find a non-intrusive way to do it. ","20/Mar/08 21:20;pi_song;Explain a bit more.

This fix just changes a line in caching logic which seems to be obviously wrong.

{noformat}
-            this.materializedResults.put(pom.sourceLogicalKey,
+            this.materializedResults.put(plan.getRoot(),
                                          new MapRedResult(pom.outputFileSpec,
                                                            pom.reduceParallelism));
{noformat}

Before we save logicalkey of the source data to the executed output.
The right logic should be linking the output to the executed output.",04/Apr/08 15:17;pi_song;This is a trivial bug. Please review it.,09/Apr/08 04:37;alangates;Fix checked in revision 646189.   Thanks Pi.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Solution for PIG-123 might have introduced parsing errors on Windows with the Pig unit tests,PIG-152,12391681,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuzh,xuzh,xuzh,17/Mar/08 20:33,24/Mar/10 22:01,14/Mar/19 03:05,19/May/08 18:01,,,,,,,0.1.0,,impl,,,0,,,,"*Since the changes for PIG-123 were checked in, we got lots of parsing errors when running the unit tests on Windows.*

It seems all these errors are generated when a query string that contains a single quote for the path of the data file is passed into the PigServer::registerQuery() method.  

Here is an example of such an error and the location where it is generated in the unit test code (on the line ""pig.registerQuery(query);"" in the following code):

{noformat}
org.apache.pig.impl.logicalLayer.parser.TokenMgrError: Lexical error at line 1, column 39.  Encountered: ""W"" (87), after : ""\'file:c:\\""
	at org.apache.pig.impl.logicalLayer.parser.QueryParserTokenManager.getNextToken(QueryParserTokenManager.java:1599)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.jj_consume_token(QueryParser.java:4069)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.FileName(QueryParser.java:717)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.LoadClause(QueryParser.java:615)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:497)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.NestedExpr(QueryParser.java:425)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.GroupItem(QueryParser.java:1043)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.CogroupClause(QueryParser.java:996)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:523)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.NestedExpr(QueryParser.java:425)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.ForEachClause(QueryParser.java:1364)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:552)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:373)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:227)
	at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:47)
	at org.apache.pig.PigServer.registerQuery(PigServer.java:237)
	at org.apache.pig.test.TestAlgebraicEval.testSimpleCount(TestAlgebraicEval.java:50)

Standard Output

Starting DataNode 0 with dfs.data.dir: dfs\data\data1,dfs\data\data2
Starting DataNode 1 with dfs.data.dir: dfs\data\data3,dfs\data\data4
Starting DataNode 2 with dfs.data.dir: dfs\data\data5,dfs\data\data6
Starting DataNode 3 with dfs.data.dir: dfs\data\data7,dfs\data\data8
myid =  foreach (group (load 'file:c:\WINDOWS\TEMP\test39837txt') all) generate COUNT($1);
myid = foreach (group (load 'file:c:\WINDOWS\TEMP\test39838txt') all) generate group, COUNT($1) ;
myid = foreach (group (load 'file:c:\WINDOWS\TEMP\test39839txt') all) generate COUNT($1), group ;
myid = foreach (group (load 'file:c:\WINDOWS\TEMP\test39840txt' using org.apache.pig.builtin.PigStorage(':')) by $0) generate group, COUNT($1.$1) ;
myid = foreach (group (load 'file:c:\WINDOWS\TEMP\test39841txt' using org.apache.pig.builtin.PigStorage(':')) by $0) generate group, COUNT($1.$1), COUNT($1.$0) ;
{noformat}

Here is the corresponding code:

{code}
public class TestAlgebraicEval extends TestCase {
    
	MiniCluster cluster = MiniCluster.buildCluster();
    @Test
    public void testSimpleCount() throws Throwable {
        int LOOP_COUNT = 1024;
        PigServer pig = new PigServer(MAPREDUCE);
        File tmpFile = File.createTempFile(""test"", ""txt"");
        PrintStream ps = new PrintStream(new FileOutputStream(tmpFile));
        for(int i = 0; i < LOOP_COUNT; i++) {
            ps.println(i);
        }
        ps.close();
        String query = ""myid =  foreach (group (load 'file:"" + tmpFile + ""') all) generate COUNT($1);"";
        System.out.println(query);
        pig.registerQuery(query);
        Iterator it = pig.openIterator(""myid"");
        tmpFile.delete();
        Tuple t = (Tuple)it.next();
        Double count = t.getAtomField(0).numval();
        assertEquals(count, (double)LOOP_COUNT);
    }

    .
    .
    .
{code}",,,,,,,,,,,,,,,,,,,18/Mar/08 22:43;xuzh;PIG_152.patch;https://issues.apache.org/jira/secure/attachment/12378186/PIG_152.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-03-17 21:10:36.979,,,no_permission,,,,,,,,,,,,163807,,,,,Mon May 19 18:01:08 UTC 2008,,,Patch Available,,,,0|i0ggqv:,94143,,,,,,,,,,"17/Mar/08 21:10;pi_song;Xu, I'll be looking at this as soon as possible. In the mean time, can you tell me if there is any trick to get Pig running on Windows?","17/Mar/08 21:20;pi_song;A minute, I think I know what the problem is. *This is due to the change in string input semantic*.

Look at these lines:-
{noformat}
myid =  foreach (group (load 'file:c:\WINDOWS\TEMP\test39837txt') all) generate COUNT($1);
myid = foreach (group (load 'file:c:\WINDOWS\TEMP\test39838txt') all) generate group, COUNT($1) ;
myid = foreach (group (load 'file:c:\WINDOWS\TEMP\test39839txt') all) generate COUNT($1), group ;
{noformat}

In order to input ""\"", from now on you have to do ""\ \"".
So
{noformat}
myid =  foreach (group (load 'file:c:\WINDOWS\TEMP\test39837txt') all) generate COUNT($1);
{noformat}

should become
{noformat}
myid =  foreach (group (load 'file:c:\\WINDOWS\\TEMP\\test39837txt') all) generate COUNT($1);   (Like in C++/Java)
{noformat}

In Linux we don't use ""\"" to separate path components so we don't have this problem.
","17/Mar/08 21:46;xuzh;Thanks Pi.  I do not think there is any special tricks besides those with
Linux (such as you will need ant, junit, java, svn, etc).  If you want, you
can also send me your pig.jar and then I will try it out for you.

Best regards,
Xu


org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:373>>
)

","17/Mar/08 21:46;xuzh;Thanks Pi.  I do not think there is any special tricks besides the same ones with Linux (such as you will need ant, junit, java, svn, etc).  If you want, you can also send me your pig.jar and then I will try it out for you.","17/Mar/08 21:48;pi_song;Xu,

Please try changing all ""\"" to ""\ \"" like what I said.","17/Mar/08 22:21;xuzh;Continued from my previous comment:  You might also need Cygwin for running Pig on Windows.  This is because Hadoop needs Cygwin to run on Windows.  

For Pi's latest comment, I will try hard coding the temp file path to see if that fixes the issue.  However, we will need to find a permanent solution for this.  How does fixing by scanning the path string and replacing all instances of ""\"" with ""\\"" in the unit tests code sound to everyone?","17/Mar/08 23:30;pi_song;Sounds right to me.
I forgot to change other unit tests to use the new semantic. Sorry for that.","18/Mar/08 02:51;xuzh;Ok, I have found the solution (or a workaround?).  It took me almost whole day to get the thing to work on a remote Windows machine with the dreaded Strng::replaceAll() method, where ""\"" is also used for escape.  

However, before I go ahead adding code to every test method of all the test case classes where the query string contains ""\"", I would like to pose the following questions:

*Is this an issue in the unit test code, or in Pig itself?*  What if the user issues the same commands via grunt on a Windows machine?  In that case, should they be required to escape ""\"" as well?","18/Mar/08 13:34;pi_song;Xu,

Please read PIG-123 again.
We've changed the string input semantic from ""\"" being a normal char to ""\"" being an escape char. Therefore from now on if you want to refer to ""\"" itself you will always have to use double ""\"". This will affect both Grunt and all unit tests.

In fact, this kind of breaking changes are supposed to be highlighted in the changelog but I don't even see it!!

Regarding the tests, I don't have a windows machine to test. Could you please encode all queries in unit tests to use double ""\"" for me?
Let's create TestUtil.EncodeEscape method to do that. One common scenario that you will have to encode is when we generate a temp file to use as an input.","18/Mar/08 13:39;pi_song;Xu Zhang:
{quote}
should they be required to escape ""\"" as well?
{quote}

To answer this question, I regard Pig as a developer tool like Java or SQL so adding a bit more complexity to support more use cases is acceptable. In both Java and SQL you always have to know about escape characters.

Example from javac
{noformat}
Test1.java:3: illegal escape character
                System.out.println(""This is Mr.Pi \M hohoho"");
                                                   ^
1 error
{noformat}",18/Mar/08 15:30;xuzh;Pi:  What you said makes sense to me.  I will work on the unit test code as what you said.   ,"18/Mar/08 22:43;xuzh;Attach the patch.  Changes include adding the static method encodeEscape() to Util.java and calling this method in the test classes to replace single ""\"" with double ""\"".","18/Mar/08 23:10;pi_song;I already reviewed. It is ready to be committed.

Please make sure all the unit tests work.",18/Mar/08 23:19;xuzh;Tested already before submitting the patch. ,19/May/08 18:01;olgan;Opened generic windows issue: https://issues.apache.org/jira/browse/PIG-243,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Zero length BZip files are not generated correctly,PIG-151,12391499,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,breed,breed,breed,14/Mar/08 22:44,24/Mar/10 22:01,14/Mar/19 03:05,29/Apr/08 20:23,,,,,,,0.1.0,,impl,,,0,,,,"If a zero length BZIp file is created as a result of a store, the resulting BZIp file will be invalid: it will have part of the BZip header, but it will be missing everything else. The BZip library does not behave correctly when a BZip file is created and nothing is written before the close.",,,,,,,,,,,,,,,,,,,25/Apr/08 07:30;breed;bz.patch;https://issues.apache.org/jira/secure/attachment/12380900/bz.patch,14/Mar/08 22:45;breed;bz.patch;https://issues.apache.org/jira/secure/attachment/12377947/bz.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-03-19 14:08:56.821,,,no_permission,,,,,,,,,,,,38234,,,,,Thu Jun 26 21:21:20 UTC 2008,,,Patch Available,,,,0|i0ggqf:,94141,,,,,,,,,,"19/Mar/08 14:08;pi_song;The solution looks good.

- would be better if the unit test only does test CBZip2OutputStream/CBZip2InputStream.",19/Mar/08 14:38;breed;Agreed. I made it end to end in part because we don't have an end to end test of the bzip code. I will add another test that just tests CBZip2OutputStream/CBZip2InputStream.,"25/Apr/08 07:30;breed;This patch adds a unit test specifically for reading zero length bzip files and well as another end-to-end test.

The patch also fixes an error related to figuring out the name of the output directory when determining whether this is a bzip file. Note, the source of this error was Hadoop-16, so the fix is specific to Hadoop-16. This will not work with Hadoop-15.","28/Apr/08 19:09;olgan;Ben, thanks for figuring this out.

I have reviewed and tested the patch and will commit it once the SVN is back to normal.

I agree that this is pretty hacky. I spoke with Owen and in Hadoop 0.17 things change again so we need to make more more changes. Fortunately, Hadoop is providing us a way to find the original name provided by the user via a static method in FileOutputFormat.

Ben, how do we handle gzip files. Do we need to make any changes there?

","29/Apr/08 20:23;olgan;patch committed. Thanks, Ben","26/Jun/08 21:21;amirhyoussefi;Ben, 

 Using Pig tutorial's excite.log.bz2 doesn't work but excite.log (uncompressed version runs). Here is URL and stack trace:

http://wiki.apache.org/pig/PigTutorial 


java -cp pig_latest.jar org.apache.pig.Main -x local script1-local.pig
2008-06-26 20:27:09,708 [main] ERROR org.apache.pig.tools.grunt.Grunt - java.io.IOException: Unable to store alias null
        at org.apache.pig.impl.util.WrappedIOException.wrap(WrappedIOException.java:16)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:296)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:457)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:233)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:63)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:60)
        at org.apache.pig.Main.main(Main.java:294)
?Pt???4??fd???@Q(/??C?!Ap7??;?+???w]?<=v}k?m??w??[3?{=?Z????????u??????????6r??v????l??8???????Y??vlwR??P??;??P 8p\?b?????;??}??+??|?[t??}?v>?????y?z?^h=?]??;j>w???<?Z??}?????{?c?{?n>?????wh>?@(????W????
???????m?n?????;ol????|p'{?}?t{???[???>??>???^??
                  ?oxf?)
        at org.apache.pig.backend.local.executionengine.LocalExecutionEngine.execute(LocalExecutionEngine.java:136)
        at org.apache.pig.backend.local.executionengine.LocalExecutionEngine.execute(LocalExecutionEngine.java:27)
        at org.apache.pig.PigServer.optimizeAndRunQuery(PigServer.java:413)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:293)
        ... 5 more
?Pt???4??fd???@Q(/??C?!Ap7??;?+???w]?<=v}k?m??w??[3?{=?Z????????u??????????6r??v????l??8???????Y??vlwR??P??;??P 8p\?b?????;??}??+??|?[t??}?v>?????y?z?^h=?]??;j>w???<?Z??}?????{?c?{?n>?????wh>?@(????W????
???????m?n?????;ol????|p'{?}?t{???[???>??>???^??
                  ?oxf?)
        at org.apache.pig.data.Tuple.getField(Tuple.java:176)
        at org.apache.pig.impl.eval.ProjectSpec.eval(ProjectSpec.java:84)
        at org.apache.pig.impl.eval.SimpleEvalSpec$1.add(SimpleEvalSpec.java:38)
        at org.apache.pig.impl.eval.GenerateSpec$CrossProductItem.exec(GenerateSpec.java:264)
        at org.apache.pig.impl.eval.GenerateSpec$1.add(GenerateSpec.java:88)
        at org.apache.pig.impl.eval.EvalSpec.simpleEval(EvalSpec.java:223)
        at org.apache.pig.impl.eval.cond.FuncCond.eval(FuncCond.java:72)
        at org.apache.pig.impl.eval.FilterSpec$1.add(FilterSpec.java:60)
        at org.apache.pig.backend.local.executionengine.POEval.getNext(POEval.java:113)
        at org.apache.pig.backend.local.executionengine.POEval.getNext(POEval.java:107)
        at org.apache.pig.backend.local.executionengine.POEval.getNext(POEval.java:107)
        at org.apache.pig.backend.local.executionengine.POEval.getNext(POEval.java:107)
        at org.apache.pig.backend.local.executionengine.POCogroup.open(POCogroup.java:88)
        at org.apache.pig.impl.physicalLayer.PhysicalOperator.open(PhysicalOperator.java:68)
        at org.apache.pig.backend.local.executionengine.POEval.open(POEval.java:74)
        at org.apache.pig.impl.physicalLayer.PhysicalOperator.open(PhysicalOperator.java:68)
        at org.apache.pig.backend.local.executionengine.POCogroup.open(POCogroup.java:66)
        at org.apache.pig.impl.physicalLayer.PhysicalOperator.open(PhysicalOperator.java:68)
        at org.apache.pig.backend.local.executionengine.POEval.open(POEval.java:74)
        at org.apache.pig.impl.physicalLayer.PhysicalOperator.open(PhysicalOperator.java:68)
        at org.apache.pig.backend.local.executionengine.POCogroup.open(POCogroup.java:66)
        at org.apache.pig.impl.physicalLayer.PhysicalOperator.open(PhysicalOperator.java:68)
        at org.apache.pig.backend.local.executionengine.POEval.open(POEval.java:74)
        at org.apache.pig.impl.physicalLayer.PhysicalOperator.open(PhysicalOperator.java:68)
        at org.apache.pig.backend.local.executionengine.POEval.open(POEval.java:74)
        at org.apache.pig.impl.physicalLayer.PhysicalOperator.open(PhysicalOperator.java:68)
        at org.apache.pig.backend.local.executionengine.POEval.open(POEval.java:74)
        at org.apache.pig.impl.physicalLayer.PhysicalOperator.open(PhysicalOperator.java:68)
        at org.apache.pig.backend.local.executionengine.POSort.open(POSort.java:54)
        at org.apache.pig.impl.physicalLayer.PhysicalOperator.open(PhysicalOperator.java:68)
        at org.apache.pig.backend.local.executionengine.LocalExecutionEngine.execute(LocalExecutionEngine.java:126)
        ... 8 more

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the errors from building the doc target,PIG-150,12391034,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,xuzh,xuzh,xuzh,13/Mar/08 23:02,24/Mar/10 22:01,14/Mar/19 03:05,16/Apr/08 17:44,,,,,,,0.1.0,,,,,0,,,,"There are 2 errors when building the doc target, in addition to the dependency issues as described in PIG-149.

Here are the 2 errors:

{noformat}
javadoc: warning - Error reading file: /homes/xu/workspace/pig-trunk/build/docs/api/${javadoc.link.java}/package-list

javadoc: error - Error while reading file /homes/xu/workspace/pig-trunk/src/overview.html
{noformat}

To rid of these errors, I will add an overview.html file with some overview (duh!) information about Pig and also assign a valid value to ${javadoc.link.java} in build.xml.",,,,,,,,,,,,,,,,,,,13/Mar/08 23:15;xuzh;PIG_150.patch;https://issues.apache.org/jira/secure/attachment/12377848/PIG_150.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-04-16 17:44:06.782,,,no_permission,,,,,,,,,,,,38585,,,,,Wed Apr 16 17:44:06 UTC 2008,,,Patch Available,,,,0|i0ggpz:,94139,,,,,,,,,,16/Apr/08 17:44;alangates;Fix checked in at revision 648768.  Thanks Xu.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the depency for the doc target,PIG-149,12391000,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,xuzh,xuzh,xuzh,13/Mar/08 19:28,24/Mar/10 22:01,14/Mar/19 03:05,16/Apr/08 17:44,,,,,,,0.1.0,,,,,0,,,,"There was dependency issue with the doc target which resulted in the following errors and warnings:

{code}
  [javadoc] Constructing Javadoc information...
  [javadoc] javadoc: warning - Multiple sources of package comments found for package ""org.apache.commons.logging""
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/PigServer.java:52: cannot find symbol
  [javadoc] symbol  : class ParseException
  [javadoc] location: package org.apache.pig.impl.logicalLayer.parser
  [javadoc] import org.apache.pig.impl.logicalLayer.parser.ParseException;
  [javadoc]                                               ^
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/PigServer.java:53: cannot find symbol
  [javadoc] symbol  : class QueryParser
  [javadoc] location: package org.apache.pig.impl.logicalLayer.parser
  [javadoc] import org.apache.pig.impl.logicalLayer.parser.QueryParser;
  [javadoc]                                               ^
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/impl/logicalLayer/LOLoad.java:29: cannot find symbol
  [javadoc] symbol  : class ParseException
  [javadoc] location: package org.apache.pig.impl.logicalLayer.parser
  [javadoc] import org.apache.pig.impl.logicalLayer.parser.ParseException;
  [javadoc]                                               ^
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/impl/logicalLayer/LOLoad.java:47: cannot find symbol
  [javadoc] symbol  : class ParseException
  [javadoc] location: class org.apache.pig.impl.logicalLayer.LOLoad
  [javadoc]                   FileSpec inputFileSpec) throws IOException, ParseException {
  [javadoc]                                                               ^
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/impl/io/BufferedPositionedInputStream.java:30: package org.apache.tools.bzip2r does not exist
  [javadoc] import org.apache.tools.bzip2r.CBZip2InputStream;
  [javadoc]                               ^
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java:40: package org.apache.pig.shock does not exist
  [javadoc] import org.apache.pig.shock.SSHSocketImplFactory;
  [javadoc]                            ^
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/backend/hadoop/executionengine/mapreduceExec/PigOutputFormat.java:37: package org.apache.tools.bzip2r does not exist
  [javadoc] import org.apache.tools.bzip2r.BZip2Constants;
  [javadoc]                               ^
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/backend/hadoop/executionengine/mapreduceExec/PigOutputFormat.java:38: package org.apache.tools.bzip2r does not exist
  [javadoc] import org.apache.tools.bzip2r.CBZip2OutputStream;
  [javadoc]                               ^
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/backend/hadoop/executionengine/mapreduceExec/PigInputFormat.java:46: package org.apache.tools.bzip2r does not exist
  [javadoc] import org.apache.tools.bzip2r.CBZip2InputStream;
  [javadoc]                               ^
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/impl/streaming/PigExecutableManager.java:40: cannot find symbol
  [javadoc] symbol  : class ParseException
  [javadoc] location: package org.apache.pig.impl.logicalLayer.parser
  [javadoc] import org.apache.pig.impl.logicalLayer.parser.ParseException;
  [javadoc]                                               ^
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/impl/logicalLayer/LogicalPlanBuilder.java:25: cannot find symbol
  [javadoc] symbol  : class ParseException
  [javadoc] location: package org.apache.pig.impl.logicalLayer.parser
  [javadoc] import org.apache.pig.impl.logicalLayer.parser.ParseException;
  [javadoc]                                               ^
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/impl/logicalLayer/LogicalPlanBuilder.java:26: cannot find symbol
  [javadoc] symbol  : class QueryParser
  [javadoc] location: package org.apache.pig.impl.logicalLayer.parser
  [javadoc] import org.apache.pig.impl.logicalLayer.parser.QueryParser;
  [javadoc]                                               ^
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/impl/logicalLayer/LogicalPlanBuilder.java:44: cannot find symbol
  [javadoc] symbol  : class ParseException
  [javadoc] location: class org.apache.pig.impl.logicalLayer.LogicalPlanBuilder
  [javadoc]         throws IOException, ParseException {
  [javadoc]                             ^
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/tools/grunt/GruntParser.java:23: package org.apache.pig.tools.pigscript.parser does not exist
  [javadoc] import org.apache.pig.tools.pigscript.parser.ParseException;
  [javadoc]                                             ^
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/tools/grunt/GruntParser.java:24: package org.apache.pig.tools.pigscript.parser does not exist
  [javadoc] import org.apache.pig.tools.pigscript.parser.PigScriptParser;
  [javadoc]                                             ^
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/tools/grunt/GruntParser.java:25: package org.apache.pig.tools.pigscript.parser does not exist
  [javadoc] import org.apache.pig.tools.pigscript.parser.PigScriptParserTokenManager;
  [javadoc]                                             ^
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/tools/grunt/GruntParser.java:27: cannot find symbol
  [javadoc] symbol: class PigScriptParser
  [javadoc] public class GruntParser extends PigScriptParser {
  [javadoc]                                  ^
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/tools/grunt/GruntParser.java:46: cannot find symbol
  [javadoc] symbol  : class PigScriptParserTokenManager
  [javadoc] location: class org.apache.pig.tools.grunt.GruntParser
  [javadoc]     public GruntParser(PigScriptParserTokenManager tm) {
  [javadoc]                        ^
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/tools/grunt/GruntParser.java:55: cannot find symbol
  [javadoc] symbol  : class ParseException
  [javadoc] location: class org.apache.pig.tools.grunt.GruntParser
  [javadoc]     public void parseStopOnError() throws IOException, ParseException
  [javadoc]                                                        ^
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/tools/grunt/GruntParser.java:131: cannot find symbol
  [javadoc] symbol  : class ParseException
  [javadoc] location: class org.apache.pig.tools.grunt.GruntParser
  [javadoc]     protected void processSet(String key, String value) throws IOException, ParseException {
  [javadoc]                                                                             ^
  [javadoc] javadoc: warning - Error reading file: /homes/xu/workspace/pig-trunk/build/docs/api/${javadoc.link.java}/package-list
  [javadoc] Standard Doclet version 1.6.0_04
  [javadoc] Building tree for all the packages and classes...
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/EvalFunc.java:151: warning - @return tag has no arguments.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/PigServer.java:383: warning - @return tag has no arguments.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/PigServer.java:412: warning - @return tag has no arguments.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/PigServer.java:156: warning - @param argument ""aliases"" is not a parameter name.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/PigServer.java:224: warning - @return tag cannot be used in method with void return type.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/PigServer.java:301: warning - @param argument ""id:"" is not a parameter name.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/PigServer.java:301: warning - @param argument ""filename:"" is not a parameter name.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/backend/datastorage/ElementDescriptor.java:94: warning - @param argument ""name"" is not a parameter name.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/backend/executionengine/ExecJob.java:54: warning - @return tag has no arguments.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/backend/executionengine/ExecJob.java:47: warning - @return tag has no arguments.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/backend/executionengine/ExecJob.java:63: warning - @return tag has no arguments.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/backend/executionengine/ExecJob.java:38: warning - @return tag has no arguments.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/backend/executionengine/ExecutionEngine.java:108: warning - @return tag has no arguments.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/backend/executionengine/ExecutionEngine.java:85: warning - @throws tag has no arguments.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/backend/executionengine/ExecutionEngine.java:100: warning - @return tag has no arguments.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/backend/executionengine/ExecutionEngine.java:72: warning - @param argument ""logical"" is not a parameter name.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/backend/hadoop/executionengine/mapreduceExec/MapReduceLauncher.java:117: warning - @param argument ""mapFuncs"" is not a parameter name.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/backend/hadoop/executionengine/mapreduceExec/MapReduceLauncher.java:117: warning - @param argument ""groupFuncs"" is not a parameter name.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/backend/hadoop/executionengine/mapreduceExec/MapReduceLauncher.java:117: warning - @param argument ""reduceFunc"" is not a parameter name.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/backend/hadoop/executionengine/mapreduceExec/MapReduceLauncher.java:117: warning - @param argument ""mapTasks"" is not a parameter name.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/backend/hadoop/executionengine/mapreduceExec/MapReduceLauncher.java:117: warning - @param argument ""reduceTasks"" is not a parameter name.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/backend/hadoop/executionengine/mapreduceExec/MapReduceLauncher.java:117: warning - @param argument ""input"" is not a parameter name.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/backend/hadoop/executionengine/mapreduceExec/MapReduceLauncher.java:117: warning - @param argument ""output"" is not a parameter name.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/data/DataMap.java:95: warning - @return tag has no arguments.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/impl/PigContext.java:320: warning - @return tag has no arguments.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/impl/PigContext.java:307: warning - @param argument ""aliases"" is not a parameter name.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/impl/eval/EvalSpec.java:136: warning - @return tag has no arguments.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/impl/eval/EvalSpec.java:152: warning - @return tag has no arguments.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/impl/eval/EvalSpec.java:126: warning - @return tag has no arguments.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/impl/eval/EvalSpec.java:184: warning - @return tag has no arguments.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/impl/eval/EvalSpec.java:213: warning - @return tag has no arguments.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/impl/eval/EvalSpec.java:152: warning - @param argument ""input"" is not a parameter name.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/impl/eval/FuncEvalSpec.java:231: warning - @param argument ""in"" is not a parameter name.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/impl/eval/FuncEvalSpec.java:291: warning - @param argument ""finalTuplePos"" is not a parameter name.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/impl/io/FileLocalizer.java:150: warning - @return tag has no arguments.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/impl/util/JarManager.java:97: warning - @return tag cannot be used in method with void return type.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/impl/util/JarManager.java:256: warning - Tag @author cannot be used in method documentation.  It can only be used in the following types of documentation: overview, package, class/interface.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/impl/util/Spillable.java:28: warning - @returns is an unknown tag.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/impl/util/Spillable.java:33: warning - @returns is an unknown tag.
  [javadoc] /homes/xu/workspace/pig-trunk/src/org/apache/pig/tools/cmdline/CmdLineParser.java:64: warning - @param argument ""valueExpected"" is not a parameter name.
  [javadoc] Building index for all the packages and classes...
  [javadoc] Building index for all classes...
  [javadoc] javadoc: error - Error while reading file /homes/xu/workspace/pig-trunk/src/overview.html
  [javadoc] Generating /homes/xu/workspace/pig-trunk/build/docs/api/stylesheet.css...
  [javadoc] 1 error
  [javadoc] 62 warnings
{code}",,,,,,,,,,,,,,,,,,,13/Mar/08 19:32;xuzh;PIG_149.patch;https://issues.apache.org/jira/secure/attachment/12377827/PIG_149.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-04-16 17:44:26.428,,,no_permission,,,,,,,,,,,,38404,,,,,Wed Apr 16 17:44:26 UTC 2008,,,Patch Available,,,,0|i0ggpj:,94137,,,,,,,,,,13/Mar/08 23:45;xuzh;Note that there is also another bug related to the doc target - PIG-150,16/Apr/08 17:44;alangates;Fix checked in at revision 648768.  Thanks Xu.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More meaningful error message could be used when a non-existing column is accessed,PIG-146,12390806,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,,xuzh,xuzh,12/Mar/08 02:19,25/Mar/10 00:12,14/Mar/19 03:05,26/Jan/09 20:15,,,,,,,0.2.0,,,,,0,,,,"When accessing a non-existing column after getting the columns with a streaming command, I got the following error which is not quite meaningfule:
{noformat}
[main] ERROR org.apache.pig.tools.grunt.Grunt -
{noformat}

Here is the sample Pig script that I used.  The data file has only 3 tab seperate fields so the streaming command on line 3 generates tuples with 3 columns.  On line 4 the script tries to access the 4th column which does not exist and thus the error above occurs.

{code}
A = load 'data;
B = foreach A generate $2, $1, $0;
C = stream B through `awk 'BEGIN {FS = ""\t""; OFS = ""\t""} {print $3, $2, $1}'`;
D = foreach C generate $4;
store D into 'results';
{code}

Here is what happens on my machine:

{code}
grunt> A = load 'data';
grunt> B = foreach A generate $2, $1, $0;
grunt> stream B through `awk 'BEGIN {FS = ""\t""; OFS = ""\t""} {print $3, $2, $1}'`;
grunt> D = foreach C generate $4;
2008-03-11 18:53:00,376 [main] ERROR org.apache.pig.tools.grunt.GruntParser - 
grunt>
{code}

A related note is that Pig behaves differently if there is no streaming command in the Pig script.  In this case, an IndexOutOfBoundsException exception is generated at runtime.

{code}
grunt> A = load 'data';
grunt> B = foreach A generate $2, $1, $0;
grunt>  D = foreach B generate $4;                    
grunt> store D into 'results';
2008-03-11 18:57:40,107 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - ----- MapReduce Job -----
2008-03-11 18:57:40,108 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Input: [data:org.apache.pig.builtin.PigStorage()]
2008-03-11 18:57:40,109 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Map: [[*]->GENERATE {[PROJECT $2],[PROJECT $1],[PROJECT $0]}->GENERATE {[PROJECT $4]}]
2008-03-11 18:57:40,109 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Group: null
2008-03-11 18:57:40,110 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Combine: null
2008-03-11 18:57:40,110 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Reduce: null
2008-03-11 18:57:40,111 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Output: results:org.apache.pig.builtin.PigStorage
2008-03-11 18:57:40,111 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Split: null
2008-03-11 18:57:40,112 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Map parallelism: -1
2008-03-11 18:57:40,112 [main] INFO  org.apache.pig.backend.hadoop.executionengine.POMapreduce - Reduce parallelism: -1
2008-03-11 18:57:42,391 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Pig progress = 0%
2008-03-11 18:57:59,466 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (map) tip_200802211201_1494_m_000000 java.lang.IndexOutOfBoundsException: Requested index 4 from tuple (2.93, 21, rachel ovid)
        at org.apache.pig.data.Tuple.getField(Tuple.java:153)
        at org.apache.pig.impl.eval.ProjectSpec.eval(ProjectSpec.java:84)
        at org.apache.pig.impl.eval.SimpleEvalSpec$1.add(SimpleEvalSpec.java:35)
        at org.apache.pig.impl.eval.GenerateSpec$CrossProductItem.exec(GenerateSpec.java:261)
        at org.apache.pig.impl.eval.GenerateSpec$1.add(GenerateSpec.java:86)
        at org.apache.pig.impl.eval.GenerateSpec$CrossProductItem.add(GenerateSpec.java:230)
        at org.apache.pig.impl.eval.collector.UnflattenCollector.add(UnflattenCollector.java:52)
        at org.apache.pig.impl.eval.collector.DataCollector.addToSuccessor(DataCollector.java:93)
        at org.apache.pig.impl.eval.SimpleEvalSpec$1.add(SimpleEvalSpec.java:35)
        at org.apache.pig.impl.eval.GenerateSpec$CrossProductItem.exec(GenerateSpec.java:261)
        at org.apache.pig.impl.eval.GenerateSpec$1.add(GenerateSpec.java:86)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.run(PigMapReduce.java:113)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:192)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1760)
 java.lang.IndexOutOfBoundsException: Requested index 4 from tuple (2.93, 21, rachel ovid)
        at org.apache.pig.data.Tuple.getField(Tuple.java:153)
        at org.apache.pig.impl.eval.ProjectSpec.eval(ProjectSpec.java:84)
        at org.apache.pig.impl.eval.SimpleEvalSpec$1.add(SimpleEvalSpec.java:35)
        at org.apache.pig.impl.eval.GenerateSpec$CrossProductItem.exec(GenerateSpec.java:261)
        at org.apache.pig.impl.eval.GenerateSpec$1.add(GenerateSpec.java:86)
        at org.apache.pig.impl.eval.GenerateSpec$CrossProductItem.add(GenerateSpec.java:230)
        at org.apache.pig.impl.eval.collector.UnflattenCollector.add(UnflattenCollector.java:52)
        at org.apache.pig.impl.eval.collector.DataCollector.addToSuccessor(DataCollector.java:93)
        at org.apache.pig.impl.eval.SimpleEvalSpec$1.add(SimpleEvalSpec.java:35)
        at org.apache.pig.impl.eval.GenerateSpec$CrossProductItem.exec(GenerateSpec.java:261)
        at org.apache.pig.impl.eval.GenerateSpec$1.add(GenerateSpec.java:86)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.run(PigMapReduce.java:113)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:192)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1760)
 java.lang.IndexOutOfBoundsException: Requested index 4 from tuple (2.93, 21, rachel ovid)
        at org.apache.pig.data.Tuple.getField(Tuple.java:153)
        at org.apache.pig.impl.eval.ProjectSpec.eval(ProjectSpec.java:84)
        at org.apache.pig.impl.eval.SimpleEvalSpec$1.add(SimpleEvalSpec.java:35)
        at org.apache.pig.impl.eval.GenerateSpec$CrossProductItem.exec(GenerateSpec.java:261)
        at org.apache.pig.impl.eval.GenerateSpec$1.add(GenerateSpec.java:86)
        at org.apache.pig.impl.eval.GenerateSpec$CrossProductItem.add(GenerateSpec.java:230)
        at org.apache.pig.impl.eval.collector.UnflattenCollector.add(UnflattenCollector.java:52)
        at org.apache.pig.impl.eval.collector.DataCollector.addToSuccessor(DataCollector.java:93)
        at org.apache.pig.impl.eval.SimpleEvalSpec$1.add(SimpleEvalSpec.java:35)
        at org.apache.pig.impl.eval.GenerateSpec$CrossProductItem.exec(GenerateSpec.java:261)
        at org.apache.pig.impl.eval.GenerateSpec$1.add(GenerateSpec.java:86)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.run(PigMapReduce.java:113)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:192)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1760)
 java.lang.IndexOutOfBoundsException: Requested index 4 from tuple (2.93, 21, rachel ovid)
        at org.apache.pig.data.Tuple.getField(Tuple.java:153)
        at org.apache.pig.impl.eval.ProjectSpec.eval(ProjectSpec.java:84)
        at org.apache.pig.impl.eval.SimpleEvalSpec$1.add(SimpleEvalSpec.java:35)
        at org.apache.pig.impl.eval.GenerateSpec$CrossProductItem.exec(GenerateSpec.java:261)
        at org.apache.pig.impl.eval.GenerateSpec$1.add(GenerateSpec.java:86)
        at org.apache.pig.impl.eval.GenerateSpec$CrossProductItem.add(GenerateSpec.java:230)
        at org.apache.pig.impl.eval.collector.UnflattenCollector.add(UnflattenCollector.java:52)
        at org.apache.pig.impl.eval.collector.DataCollector.addToSuccessor(DataCollector.java:93)
        at org.apache.pig.impl.eval.SimpleEvalSpec$1.add(SimpleEvalSpec.java:35)
        at org.apache.pig.impl.eval.GenerateSpec$CrossProductItem.exec(GenerateSpec.java:261)
        at org.apache.pig.impl.eval.GenerateSpec$1.add(GenerateSpec.java:86)
        at org.apache.pig.backend.hadoop.executionengine.mapreduceExec.PigMapReduce.run(PigMapReduce.java:113)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:192)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1760)

2008-03-11 18:57:59,469 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapreduceExec.MapReduceLauncher - Error message from task (reduce) tip_200802211201_1494_r_000000
2008-03-11 18:57:59,470 [main] ERROR org.apache.pig.tools.grunt.GruntParser - Unable to store alias D
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-01-26 20:15:38.552,,,no_permission,,,,,,,,,,,,163804,,,,,Mon Jan 26 20:15:38 UTC 2009,,,,,,,0|i0ggnz:,94130,,,,,,,,,,26/Jan/09 20:15;olgan;This is addressed with the latest code.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The error message should be more meaningful when there is a typo in PIg script,PIG-144,12390511,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,xuefuz,xuzh,xuzh,08/Mar/08 01:03,04/Aug/11 00:34,14/Mar/19 03:05,14/Apr/11 17:06,,,,,,,0.9.0,,,,,0,,,,"When I ran the following Pig script on the command line {{""pig -c mycluster myscript.pig""}}, I got the error: 

2008-03-07 16:31:45,992 [main] ERROR org.apache.pig.tools.grunt.Grunt -
  
{code}
A = load '/user/pig/tests/data/singlefile/fileexists';
B = foreach A generate $2, $1, $0;
C = strean B through `awk '{print $3 "" "" $4 ""\t"" $2 ""\t"" $1}'`;
store C into '/user/pig/tests/data/singlefile/results1';
{code}

The error message is not quite meaningful, and it took me a while to find out what was wrong - the word ""strean"" should have been ""stream"".



",,,,,,,,,,,,,,,,,PIG-1618,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-07-09 17:55:04.019,,,no_permission,,,,,,,,,,,,66252,,,,,Thu Apr 14 17:06:03 UTC 2011,,,,,,,0|i0ggn3:,94126,,,,,,,,,,09/Jul/10 17:55;olgan;Quality of error messages should be addressed as part of parser change,"11/Mar/11 18:10;xuefuz;It's hard to give a good error message in case of typo. In this case, tream will be treated as an IDENTIFIER and Pig doesn't have a syntax (in the new parser) for Alias = IDENTIFIER. Thus, it actually complains at B where it gets no way out. Here is what you will see with the new parser.

grunt> A = load 'x';
grunt> B = tream A through `cat`;
2011-03-11 10:02:32,431 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 0: line 2:0 [query, statement]  no viable alt; token = [@9,14:14='B',<77>,2:0] (decision=2 state 6) decision=<<>>

It will not be easy to give a really useful error message such as ""Hey, 'tream' is misspelled. do you mean 'stream'?"" without significant effort (if possible at all).

I assume for now the error message is okay for such a case, unless anyone disagree or has an easy way out.","11/Mar/11 18:16;alangates;I don't think we have to suggest alternate spellings.  But surely we can do better than exposing the internal state of the parser (decision=2 state 6).  We could at least give a message like:  ""expected keyword found identifier"" or ""malformed line at _linnum_ 'B = tream ...'""","11/Mar/11 19:03;xuefuz;As to the internal state, I happened to turn on debug mode. Without that, the error msg looks like:

2011-03-11 10:28:44,651 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 0: line 2:0 no viable alternative at input [@9,14:14='B',<77>,2:0]

Better but possibly we can remove all things in [] except 'B'. This is the default msg returned from Antlr.

As to the error message itself, Antlr desn't know it's expected an operator based on the current way that the grammar is written. The grammar is rule is written to take care the foreach statement, mostly.

I suggest we keep this bug open, and address it at a low priority.","14/Apr/11 17:06;xuefuz;Now the error we get is:

grunt> A = load '/user/pig/tests/data/singlefile/fileexists';
grunt> B = foreach A generate $2, $1, $0;
grunt> C = strean B through `awk '{print $3 "" "" $4 ""\t"" $2 ""\t"" $1}'`;
2011-04-14 10:02:56,903 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1200: <line 3, column 11>  mismatched input 'B' expecting LEFT_PAREN
Details at logfile: /home/xuefuz/dev/pig11/pig_1302734768140.log

Here pig is matching the rule for macro inline. I think the error msg now is good enough, though not perfect. Case is closed as fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Early detection of issues (by Parser),PIG-142,12390485,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,amirhyoussefi,amirhyoussefi,07/Mar/08 19:17,25/Mar/10 00:12,14/Mar/19 03:05,26/Jan/09 20:14,,,,,,,0.2.0,,,,,0,,,,"If we can detect parse/lexical/syntax/etc. issues before running a script we can save user/cluster time. Occasionally this results in a long script failing in the middle because of a simple detectable typo and wasting time/processing power. This issue can serve as holder of several examples and levels we can tackle this. 

There were some discussions regarding having save-points/check-points so script can resume from a failed point. That goes in a separate issue.

Please go ahead and add your examples. 

 
 ",Pig and Grunt,,,,,,,,,,,PIG-143,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-03-07 22:16:34.876,,,no_permission,,,,,,,,,,,,163801,,,,,Mon Jan 26 20:14:17 UTC 2009,,,,,,,0|i0ggm7:,94122,,,,,,,,,,07/Mar/08 19:20;amirhyoussefi;type check can be added when Pig types are implemented. ,"07/Mar/08 22:16;pi_song;To make this simpler, I would like to propose splitting validation logic from parser and make the validation extensible.
I will come up with a proper proposal as soon as possible.","07/Mar/08 22:20;amirhyoussefi;Pi, 

 You are welcome to create Sub-Tasks for this JIRA .

",17/Mar/08 15:06;francisoud;May be we could take advantage of [apache commons validator|http://commons.apache.org/validator/] for this ?,"17/Mar/08 22:14;pi_song;Ben,

I've been trying to push the same concept to the new type branch to make it easy to plug-in new validators.

We will have a type checking validator, and possibly an optional mock execution test.",26/Jan/09 20:14;olgan;I believe this has been addressed with the latest code.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test instantiation of StoreFunc in LOStore swallows (cause) exceptions,PIG-137,12390380,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,oae,oae,oae,06/Mar/08 18:43,25/Mar/10 00:12,14/Mar/19 03:05,26/Jan/09 20:13,,,,,,,0.2.0,,impl,,,0,,,,"The current handling
{noformat}
IOException ioe = new IOException(e.getMessage());
ioe.setStackTrace(e.getStackTrace());
throw ioe;
{noformat}
passes the exception message and the stacktrace of the exception, but not the stacktraces of the exceptions wich caused the exception.
",,,,,,,,,,,,,,,,,,,06/Mar/08 19:23;oae;PIG-137-633746.patch;https://issues.apache.org/jira/secure/attachment/12377275/PIG-137-633746.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-03-06 18:50:12.92,,,no_permission,,,,,,,,,,,,163796,,,,,Mon Jan 26 20:13:21 UTC 2009,,,,,,,0|i0ggk7:,94113,,,,,,,,,,"06/Mar/08 18:50;olgan;I ran into the same issue yesterday. I traced it down to the fact that commons error command only prints the top exception.

I replaced log.error(e) with e.printStackTrace() in Main and it worked fine.

Experts in commons logging, could you, please, let us know if there is a way to resolve this issue.

If it can't be resolved, I think it is bad enough to consider rolling back commons stuff.","06/Mar/08 19:13;joa23;I guess log4j is simply not yet correct configured.
If I remind correctly there was a log4j configuration within the shell script patch. I will work on that today to fix remaining issues.

","06/Mar/08 19:23;oae;Oh, i think this case it has nothing to do with the logging. Its just about the exception conversion, where informations simply getting lost.
I've uploaded a patch for this case (simply removed the try-catch, since there can be only RuntimeException, which did not need to be wrapped or converted to IOExceptions). 
The stacktrace without the patch:
{noformat}
java.io.IOException: could not instantiate 'com.my.DatabaseStoreFunc' with arguments 'aDb, table'
	at org.apache.pig.impl.PigContext.instantiateFunc(PigContext.java:422)
	at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:429)
	at org.apache.pig.impl.logicalLayer.LOStore.<init>(LOStore.java:50)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.generateStorePlan(QueryParser.java:74)
	at org.apache.pig.PigServer.store(PigServer.java:321)
	at org.apache.pig.PigServer.store(PigServer.java:317)
	...
{noformat}

The stacktrace with the patch:	
{noformat}
java.lang.RuntimeException: could not instantiate 'com.my.DatabaseStoreFunc' with arguments 'aDb, table'
	at org.apache.pig.impl.PigContext.instantiateFunc(PigContext.java:422)
	at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:429)
	at org.apache.pig.impl.logicalLayer.LOStore.<init>(LOStore.java:49)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.generateStorePlan(QueryParser.java:74)
	at org.apache.pig.PigServer.store(PigServer.java:321)
	at org.apache.pig.PigServer.store(PigServer.java:317)
	...
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:494)
	at org.apache.pig.impl.PigContext.instantiateFunc(PigContext.java:417)
	... 24 more
Caused by: java.lang.RuntimeException: could not load properties file from classpath '/aDb.properties'
	... 
{noformat}


 ","02/Jul/08 19:21;olgan;Johannes, this code is being almost completely rewritten in the types branch. I think it would make sense to review the code their and suubmit patch of needed for that code. Would that work?","03/Jul/08 09:00;oae;Hi Olga, sounds reasonable!","26/Jan/09 20:13;olgan;I think this issue has been addressed with the latest code. Please, reopen if this is still a problem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit test TestEvalPipeline fails on line 109 in test case testJoin on Windows,PIG-132,12389940,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,,xuzh,xuzh,29/Feb/08 23:54,24/Mar/10 22:01,14/Mar/19 03:05,19/May/08 17:59,,,,,,,0.1.0,,,,,0,,,,"Here are the results of running TestEvalPipeline on a Windows XP machine.  

{noformat}
junit.framework.AssertionFailedError: expected:<0> but was:<4>
	at org.apache.pig.test.TestEvalPipeline.testJoin(TestEvalPipeline.java:109)

Standard Output

Starting DataNode 0 with dfs.data.dir: dfs\data\data1,dfs\data\data2
Starting DataNode 1 with dfs.data.dir: dfs\data\data3,dfs\data\data4
Starting DataNode 2 with dfs.data.dir: dfs\data\data5,dfs\data\data6
Starting DataNode 3 with dfs.data.dir: dfs\data\data7,dfs\data\data8

Standard Error

08/02/29 13:50:17 INFO dfs.Storage: Storage directory dfs\name1 has been successfully formatted.
08/02/29 13:50:18 INFO dfs.Storage: Storage directory dfs\name2 has been successfully formatted.
08/02/29 13:50:18 INFO dfs.NameNode: Namenode up at: localhost/127.0.0.1:3917
08/02/29 13:50:18 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=NameNode, sessionId=null
08/02/29 13:50:18 INFO dfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
08/02/29 13:50:18 INFO dfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
.
.
.
{noformat}",Windows XP,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-05-19 17:59:46.659,,,no_permission,,,,,,,,,,,,163791,,,,,Mon May 19 17:59:46 UTC 2008,,,,,,,0|i0gghz:,94103,,,,,,,,,,"18/Mar/08 23:56;xuzh;Since Windows is now an officially supported platform by Pig, meaning builds and unit tests should succeed on Windows, I gave this bug a priority of ""Blocker"".",19/May/08 17:59;olgan;Opened generic windows issue: https://issues.apache.org/jira/browse/PIG-243,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit test TestBuiltin fails on line 507 in test case testShellFuncMultiple on Windows,PIG-131,12389938,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,,xuzh,xuzh,29/Feb/08 23:36,24/Mar/10 22:01,14/Mar/19 03:05,19/May/08 18:00,,,,,,,0.1.0,,,,,0,,,,"Here is the results of running TestBuiltin on a Windows XP machine.  Is it because a new line is interpreted differently on Linux and Windows?  

{noformat}
junit.framework.ComparisonFailure: null expected:<0AA[]> but was:<0AA[
]>
	at org.apache.pig.test.TestBuiltin.testShellFuncMultiple(TestBuiltin.java:507)

Standard Output

f00

b00

0BB

.
.
.




{noformat}",Windows XP,,,,,,,,,,PIG-130,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-05-19 18:00:12.988,,,no_permission,,,,,,,,,,,,163790,,,,,Mon May 19 18:00:12 UTC 2008,,,,,,,0|i0gghj:,94101,,,,,,,,,,"18/Mar/08 23:56;xuzh;Since Windows is now an officially supported platform by Pig, meaning builds and unit tests should succeed on Windows, I gave this bug a priority of ""Blocker"".",19/May/08 18:00;olgan;Opened generic windows issue: https://issues.apache.org/jira/browse/PIG-243,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit test TestBuiltin fails on line 477 in test case testShellFuncSingle on Windows,PIG-130,12389937,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,,xuzh,xuzh,29/Feb/08 23:33,24/Mar/10 22:01,14/Mar/19 03:05,19/May/08 18:00,,,,,,,0.1.0,,,,,0,,,,"Here is the results of running TestBuiltin on a Windows XP machine.  Is it because a new line is interpreted differently on Linux and Windows?  

{noformat}
org.apache.pig.test.TestBuiltin.testShellFuncSingle


junit.framework.ComparisonFailure: null expected:<f00[]> but was:<f00[
]>
	at org.apache.pig.test.TestBuiltin.testShellFuncSingle(TestBuiltin.java:477)

Standard Output

f00

b00

0BB

1BB

2BB

3BB

4BB

5BB

6BB

7BB

8BB

9BB

10BB

11BB

12BB

13BB

14BB

15BB

16BB

17BB

18BB

.
.
.


{noformat}",Windows XP,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-05-19 18:00:40.157,,,no_permission,,,,,,,,,,,,163789,,,,,Mon May 19 18:00:40 UTC 2008,,,,,,,0|i0ggh3:,94099,,,,,,,,,,"18/Mar/08 23:57;xuzh;Since Windows is now an officially supported platform by Pig, meaning builds and unit tests should succeed on Windows, I gave this bug a priority of ""Blocker"".",19/May/08 18:00;olgan;Opened generic windows issue: https://issues.apache.org/jira/browse/PIG-243,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
need to create temp files in the task's working directory,PIG-129,12389916,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,amirhyoussefi,olgan,olgan,29/Feb/08 20:08,24/Mar/10 22:01,14/Mar/19 03:05,07/Mar/08 01:09,,,,,,,0.1.0,,,,,0,,,,"Currently, pig creates temp data such is spilled bags in the directory specified by java.io.tmpdir. The problem is that this directory is usually shared by all tasks and can easily run out of space.

A better approach would be to create this files in the temp dir inside of the taks working directory as these locations usually have much mor space and also they can be hosted on different disks so the performance could be better.

There are 2 parts to this fix:

(1) in org.apache.pig.data.DataBag to check if the temp directory exists and create it if not before trying to create the temp file. This is somewhere around line 390 in the code.
(2) Change the mapred.child.java.opts in hadoop-site.xml to include new value for tmpdir property to point to ./tmp. For instance: 
<property>
        <name>mapred.child.java.opts</name>
        <value>-Xmx1024M -Djava.io.tmpdir=""./tmp""</value>
        <description>arguments passed to child jvms</description>
</property>
",,,,,,,,,,,,,,,,,,,06/Mar/08 23:44;amirhyoussefi;PIG-129.patch;https://issues.apache.org/jira/secure/attachment/12377297/PIG-129.patch,04/Mar/08 09:57;pi_song;TempAllocator0.patch;https://issues.apache.org/jira/secure/attachment/12377051/TempAllocator0.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-03-01 00:05:32.282,,,no_permission,,,,,,,,,,,,163788,,,,,Fri Mar 07 01:09:18 UTC 2008,,,,,,,0|i0gggn:,94097,,,,,,,,,,"01/Mar/08 00:05;pi_song;I think the concept of multi-dir temp file creator (LocalDirAllocator in Hadoop) should be adopted to Pig.  What it does is:-
- You can set up a set of tmp file dirs in configuration (They can be on different physical drives so you can utilize more disk space)
- When a temp file is being created, the system will probe the given temp dirs in round-robin fashion
- For a selected temp dir, if it exists and you have permission to write, temp file will be created
- For a selected temp dir, it it doesn't exist or you don't have permission to write, the temp dir will be kept in the black list, thus not being used later on.
- For the next temp file, move on to the next temp dir","02/Mar/08 12:59;pi_song;Olga,
I want to clarify a bit more about what I think and *I really need you opinion* on this bit. Regarding temp file creation due to DataBag spill,  this can happen in 2 places:-
- In Hadoop Map Reduce execution engine
- In Local execution engine

I agree with you that the working dir mechanism in hadoop is already good and you're trying to adopt it *BUT* what about local execution engine? 

I think even most people pay more attention on Hadoop backend and that's where Pig started, but the local engine still has its use.

A sample use case would be if I have a big data file on my harddisk(thus cannot be too big) and what I do is I just download Pig and then quickly write a pig script to perform processing in my local machine using local execution engine (without running Hadoop)

A good local engine implementation will help improve usability of Pig!!!

Can we handle this issue in 2 different ways? One for hadoop backend, one for local engine. I'm willing to implement what I've proposed in the last comment for the local engine.","02/Mar/08 13:37;amirhyoussefi;I think it's a good idea to have multiple tmp dirs. Having several physical drives is common these days. I brought up the same idea earlier this week as next logical step.  

A new feature in Hadoop 0.16.1 will partially address the tmp dir issue. But it takes a while for it to go through pipeline and reach users. Currently tmp directory is a hot issue for us so we plan to address this in Pig.

I will probably do this in two stages. 

1) ./tmp directory under working directory. This automatically gets cleaned.
2) open discussion on details of using multiple tmp directories (possibly over multiple physical drives). We need to take into account cleaning scenarios as well.

-Amir","03/Mar/08 21:07;olgan;Hi Pi,

On hadoop side, I am hoping that by using tasks directory we can get the multi-disk distribution and cleaning for free.

For local pig, I think that the idea is good but I question the priority of this feature. The use case is fairly limitted. Local pig is mostly for coming up to speed on the system not to run large scale processes.","04/Mar/08 09:56;pi_song;Olga,

I agree with you. But don't forget we're in an open source project. What you can also do for a low priority task is to give a good direction and leave until someone will do it.

Go ahead on Hadoop side and please don't forget to keep implementation generic.

Here I will leave a simple implementation that will give an idea what I expect as a guideline. If I have time on the weekend, I may come back to complete it.
",04/Mar/08 09:57;pi_song;The mentioned sample code,06/Mar/08 23:44;amirhyoussefi;Patch to create temporary directory. ,"07/Mar/08 00:07;amirhyoussefi;Here is summary of decisions made with Olga:

Goal of this JIRA is to have a means to create a temporary directory under Hadoop Task Dir. I will open a new JIRA  so others (Pi Song) can continue work on local mode and multiple directories.

 - We address the case in which Hadoop Platform is used.
 - We rely on Hadoop to clean up the directory.
 - We tested this on a cluster and observed logs showing creation of directory and actual directory/file being generated.
 - Added Code Block is actually called by a synchronized block of code. Second checking of directory creation is because of an observed case on a cluster. 

 ","07/Mar/08 00:25;amirhyoussefi;Moved Pi Song's additional improvement requests to PIG-138.
",07/Mar/08 01:09;olgan;I committed the patch. Thanks Amir for contributing.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cannot escape single quotes in single quoted strings when using the eq or match operator,PIG-123,12389557,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pi_song,andreib,andreib,26/Feb/08 02:50,24/Mar/10 22:01,14/Mar/19 03:05,14/Mar/08 22:46,,,,,,,0.1.0,,,,,0,,,,"a query like this :


filter c by ( c.string eq 'hell\'s angels' OR c.string eq 'blue\'s clues')  

generates an error with the pig parser, because the parser does not allow single quotes (') within single quotes, even if they are escaped with a backslash. ",,,,,,,,,,,,,,,,,,,11/Mar/08 12:58;pi_song;Pig123_EscapeCharSupport_11Mar2008.patch;https://issues.apache.org/jira/secure/attachment/12377610/Pig123_EscapeCharSupport_11Mar2008.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-02-26 11:29:54.122,,,no_permission,,,,,,,,,,,,163782,,,,,Fri Mar 14 22:23:57 UTC 2008,,,Patch Available,,,,0|i0ggdz:,94085,,,,,,,,,,"26/Feb/08 11:29;pi_song;After having a look, currently there is no escape character implementation in our grammar.

By changing the definition of quoted string in grammar file from

{noformat}
TOKEN : { <QUOTEDSTRING : ""'"" (~[""'""])* ""'""> }
{noformat}
to 
{noformat}
TOKEN : { <QUOTEDSTRING : ""'"" ( (~[""'"",""\\""]) | (""\\"" ([""\\"",""'""] ) )* ""'""> }
{noformat}

and changing the state manipulation in Grunt not to switch out of IN_STRING after  *""\'""* (backslash and a quote) is encountered, this should work.

****However**** the real problem of doing this is the semantic change. As we introduce an escape character, we no longer treat the character as itself. People using ""\"" in their code will get a weird error message.

Here are what will be affected from the above grammar change (Similar to Java) :-

{noformat}
""\\""  will be interpreted as ""\""
""\'""  will be interpreted as ""'""
""\""+(char) will no longer allowed. ***An error will be raised***.
{noformat}

****Another concern**** is I don't know how having a single quote in the expression evaluation engine (which sometimes uses encoded text notation internally) will be affected.

Need to listen to others!!!","26/Feb/08 19:07;olgan;+1. Pi, your proposal makes sense to me.","26/Feb/08 20:03;breed;It would be nice to fix this properly and allow support for specifying unicode characters, \uXXXX , and supporting standard escapes, \r \n \t. As you mention,  we have to introduce a semantic change, so we should do it right.

BTW, make sure to test for '\\' and '\\\''.","27/Feb/08 14:52;pi_song;This patch adds escape character to pig script and grunt.

{noformat}
-  ""\'"",""\r"",""\f"",""\t"",""\n"",""\b"" are supported
- ""\\"" will be interpreted to \
- ""\uxxxx"" for unicode are supported
- ""\x"" where x doesn't match anything above will be interpreted to x (skipping ""\"" )
{noformat}

This patch is only for review. Unit test is coming after I sort out the my miniMR problem.","28/Feb/08 10:55;pi_song;This patch includes implementation of the above spec.
A new test case also included (TestPigScriptParser) which tests 3 special cases

- single quote escape
- \n escape
- \uxxxx unicode escape

All unit tests passed.

Ready to be committed

**Please mark this as patch available**",28/Feb/08 17:12;breed;Excellent work! Have you done a microbenchmark to see the overhead?  I'm wondering if it would be worth it to scan the string first to see if there are any slashes to avoid processing (in the common case?).,"29/Feb/08 02:43;pi_song;Ben,

Pig is a processing-oriented system. You spend milliseconds to compile the query but spend hours to do processing work. The real point to be optimized should be in the execution engine where the code gets called over and over millions of time. This change in frontend encoding should have extremely small impact on the whole execution.

In terms of performance analysis, this is an O ( n ) operation on a short in-memory string (normally no longer than 1K) which gets called only once in an execution.

PS. If you do string scan first, it's still O ( n ) operation","03/Mar/08 23:52;breed;You are correct. I was more worried about the strain on the memory manager, but as you point out this is done in the parser, not in the execution engine.",03/Mar/08 23:53;breed;+1,"04/Mar/08 21:48;olgan;+1 on code changes.

I think tests should be true unit tests not end-to-end runs of pig scripts. We really want to minimize the time it takes to run unit tests and not to go to full tests when possible.","05/Mar/08 11:46;pi_song;Here it is.

The test is done at Parser level and subsequently Consts in LogicalOps are checked.
This test took 0.283 seconds on my machine.

In fact, I see this patch as an improvement rather than a bug fix.

Please mark as patch available.","05/Mar/08 11:54;pi_song;Surprisingly, there are so many of our tests that do end-to-end runs instead of a specific point test.

In the long run, we will need to consolidate all these tests. I don't want to bloat this Jira. Let's discuss later.","05/Mar/08 15:06;francisoud;Setting ""Patch Available"" as request by [Pi Song|https://issues.apache.org/jira/browse/PIG-123?focusedCommentId=12575314#action_12575314]","11/Mar/08 12:58;pi_song;In sync with the latest trunk (11Mar2008)

Unit tests passed

This patch has been around for a while. Somebody please commit.",13/Mar/08 17:11;breed;I'm checking this in now.,14/Mar/08 22:23;breed;Committed revision 637293.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support hadoop map reduce in loal mode,PIG-120,12389449,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,joa23,joa23,joa23,25/Feb/08 04:00,24/Mar/10 22:01,14/Mar/19 03:05,04/Mar/08 21:20,,,,,,,0.1.0,,,,,0,,,,"Currently pig support mapreduce and local as execution modes. LocalExecutionEngine is used for local and HExecutionEngine for map reduce. HExecutionEngine always expect that hadoop runs as cluster with a name node and jobtracker listing on a port. 
Though, hadoop can also run in a local mode (LocalJobRunner) this would give several advantages. 
First it would speed up the test suite significant. Second it would be possible to debug map reduce plans easily.
For example we was able to debug and reproduce PIG-110 with this method.

",,,,,,,,,,,,,,,,,,,25/Feb/08 05:18;joa23;PIG-120_v_1.patch;https://issues.apache.org/jira/secure/attachment/12376385/PIG-120_v_1.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-02-28 16:08:57.147,,,no_permission,,,,,,,,,,,,38584,,,,,Tue Mar 04 21:20:33 UTC 2008,,,Patch Available,,,,0|i0ggcn:,94079,,,,,,,,,,"25/Feb/08 05:18;joa23;A patch that allows to run pig in mapreduce mode but uses the hadoop localjobrunner. This is for sure not the most elegant solution but a starting point. As mentioned in PIG-121 I guess HExecutionEngine and Co need a cleanup anyhow.
This patch would be very useful for phase 1 of pig-119. It would be great if we can get this into trunk as a starting, since I guess PIG-121 will require some more discussion and work.

","28/Feb/08 16:08;alangates;Based on your comments in https://issues.apache.org/jira/browse/PIG-119, my understanding is that you want to be able to run the existing pig tests in hadoop's local mode.  But this patch provides a different test that runs in local mode.  Is this not a step in the wrong direction?","28/Feb/08 18:29;joa23;Alan, sorry I'm not sure if I can follow you. In general I see 3 kind of how we can run pig, LocalExecutionEngine, HadoopExecutionEngine - map reduce using a hadoop cluster and HadoopExecutionEngine - map reduce using hadoops localJobRunner.
This is very very helpful for debugging and profiling since the HadoopExecutionEngine is used but all runs in the same jvm. 
This patch makes it possible by not using a port in case the name node and job tracker are ""local"" and also not opening a remote proxy to the jobtracker in that case.

Makes that sense? ","28/Feb/08 19:32;alangates;If someone says:

ant -Dtest.mode=local

what happens?  Does it run *all the same tests* as usual, only using local mode hadoop?  Or does it run only tests that are specific to local mode hadoop?  I was envisioning the former, but your inclusion in patch of a test (TestLocalMapReduce) that was specific to local mode made me think you were suggesting the latter.

Your changes to HExecutionEngine would support either I think.","28/Feb/08 20:04;joa23;I'm very sorry for the confusion.
In general *all the same tests run in every case* we just switch execution engines and execution engine configurations. 
ant -Dtest.mode=excelLocal -> runs the pig local execution engine
ant -Dtest.mode=mapredLocal -> runs the hadoop execution engine but using the hadoops LocalJobRunner -- this should be default, since the test suite would run in the less than 50 % of the time
ant -Dtest.mode=mapredCluster -> runs the hadoop execution egine with the minicluster.

My testcase only test if it is possible to set ""local"" as nameNode and jobtracker - nothing else.

I guess we can find better names for the test modes. ","29/Feb/08 09:52;pi_song;+1  with the concept. Allowing Hadoop local execution mode will be very beneficial for testing with a subset of data before going into production. Theoretically outputs from Pig local and Hadoop Mapreduce should be exactly the same but sometimes I found that they are different. In such case, I would trust local hadoop more than Pig local for my development.

So, from the patch, if I want to run local hadoop Pig, I just have to set  cluster and nameNode properties to ""local"" right?","02/Mar/08 00:35;joa23;yeah just set the cluster name to local. 
This patch is a beginning - i would love to more explizit support that in the future. 
A related proplem for example is that we can not define a jobtracker and namenode on different host by today. 

The configuration patch will solve some basic problems here - please vote it for better hadoop local mode support in the future. :)

",04/Mar/08 21:20;alangates;Fix checked in as revision 633652.  Thanks Stefan.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UNION/CROSS/JOIN operations should not allow 1 operand,PIG-118,12389401,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pi_song,pi_song,pi_song,24/Feb/08 00:38,24/Mar/10 22:01,14/Mar/19 03:05,05/Mar/08 02:01,0.0.0,,,,,,0.1.0,,impl,,,0,,,,"At the moment UNION/CROSS/JOIN allow 1 operand.

You can write:-
{noformat}
b = UNION a ;
c = CROSS b ;
d = JOIN c BY $0 ;
{noformat}
Possibly UNION with 1 operand might be needed for implementing Sigma-styled union (Ui=1..n An)  but for CROSS/JOIN I think nobody would do such operation.

By simply replacing ""*"" with ""+"" in the parser tree should fix this problem. Should this be fixed?

{noformat}
LogicalOperator CrossClause() : {LogicalOperator op; ArrayList<OperatorKey> inputs = new ArrayList<OperatorKey>();}
{
	(
	op = NestedExpr() { inputs.add(op.getOperatorKey()); }
	("","" op = NestedExpr() { inputs.add(op.getOperatorKey()); })*
	)
	{return rewriteCross(inputs);}
}

LogicalOperator JoinClause() : {CogroupInput gi; ArrayList<CogroupInput> gis = new ArrayList<CogroupInput>();}
{
	(gi = GroupItem() { gis.add(gi); }
	("","" gi = GroupItem() { gis.add(gi); })*)
	{return rewriteJoin(gis);}
}

LogicalOperator UnionClause() : {LogicalOperator op; ArrayList<OperatorKey> inputs = new ArrayList<OperatorKey>();}
{
	(op = NestedExpr() { inputs.add(op.getOperatorKey()); }
	("","" op = NestedExpr() { inputs.add(op.getOperatorKey()); })*)
	{return new LOUnion(opTable, scope, getNextId(), inputs);}
}
{noformat}",,,,,,,,,,,,,,,,,,,28/Feb/08 12:58;pi_song;pig_1operand.patch;https://issues.apache.org/jira/secure/attachment/12376717/pig_1operand.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-03-05 02:01:41.757,,,no_permission,,,,,,,,,,,,38583,,,,,Wed Mar 05 02:01:41 UTC 2008,,,Patch Available,,,,0|i0ggbr:,94075,,,,,,,,,,"28/Feb/08 12:58;pi_song;This patch solves the above issue.

All unit tests passed.

I assume Sigma-styled Union/Cross/Join are not in the plan.","28/Feb/08 13:00;pi_song;After applying this patch when only one operand is found for these operators, you will get  ""   ,  expected""  message which I think is good enough.","05/Mar/08 02:01;olgan;fixes committed. Thanks, Pi.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
store one alias/logicalPlan twice leads to instantiation of StoreFunc as LoadFunc,PIG-114,12389173,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pi_song,oae,oae,20/Feb/08 23:57,24/Mar/10 22:01,14/Mar/19 03:05,17/Apr/08 15:44,0.0.0,,,,,,0.1.0,,impl,,,0,,,,"Calling PigServer#store() twice for an alias results in following exception :
{noformat}
java.lang.RuntimeException: java.lang.ClassCastException: org.apache.pig.test.DummyStoreFunc cannot be cast to org.apache.pig.LoadFunc
	at org.apache.pig.backend.local.executionengine.POLoad.<init>(POLoad.java:59)
	at org.apache.pig.backend.local.executionengine.LocalExecutionEngine.doCompile(LocalExecutionEngine.java:167)
	at org.apache.pig.backend.local.executionengine.LocalExecutionEngine.doCompile(LocalExecutionEngine.java:184)
	at org.apache.pig.backend.local.executionengine.LocalExecutionEngine.doCompile(LocalExecutionEngine.java:184)
	at org.apache.pig.backend.local.executionengine.LocalExecutionEngine.compile(LocalExecutionEngine.java:111)
	at org.apache.pig.backend.local.executionengine.LocalExecutionEngine.compile(LocalExecutionEngine.java:90)
	at org.apache.pig.backend.local.executionengine.LocalExecutionEngine.compile(LocalExecutionEngine.java:1)
	at org.apache.pig.PigServer.store(PigServer.java:330)
	at org.apache.pig.PigServer.store(PigServer.java:317)
	at org.apache.pig.test.StoreTwiceTest.testIt(StoreTwiceTest.java:31)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:589)
	at junit.framework.TestCase.runTest(TestCase.java:164)
	at junit.framework.TestCase.runBare(TestCase.java:130)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:120)
	at junit.framework.TestSuite.runTest(TestSuite.java:228)
	at junit.framework.TestSuite.run(TestSuite.java:223)
	at org.junit.internal.runners.OldTestClassRunner.run(OldTestClassRunner.java:35)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:45)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: java.lang.ClassCastException: org.apache.pig.test.DummyStoreFunc cannot be cast to org.apache.pig.LoadFunc
	at org.apache.pig.backend.local.executionengine.POLoad.<init>(POLoad.java:57)
	... 28 more
{noformat}

I will attach a patch with a test scenario for this. Basically the code is as follow:
{noformat}PigServer pig = new PigServer(ExecType.LOCAL);
        pig
                .registerQuery(""A = LOAD 'test/org/apache/pig/test/StoreTwiceTest.java' USING ""
                        + DummyLoadFunc.class.getName() + ""();"");
        pig.registerQuery(""B = FOREACH A GENERATE * ;"");
        File outputFile = new File(""/tmp/testPigOutput"");
        outputFile.delete();
        pig.store(""A"", outputFile.getAbsolutePath(), DummyStoreFunc.class
                .getName()
                + ""()"");
        outputFile.delete();
        pig.store(""B"", outputFile.getAbsolutePath(), DummyStoreFunc.class
                .getName()
                + ""()"");
        outputFile.delete();
        assertEquals(2, _storedTuples.size());
{noformat}",,,,,,,,,,,,,,,,,,,07/Mar/08 12:49;pi_song;PIG114_FixOptimize1.patch;https://issues.apache.org/jira/secure/attachment/12377342/PIG114_FixOptimize1.patch,06/Mar/08 14:24;pi_song;PIG114_FixOptimize_Sample.patch;https://issues.apache.org/jira/secure/attachment/12377258/PIG114_FixOptimize_Sample.patch,20/Feb/08 23:58;oae;pigPatch-storeTwice-620665.patch;https://issues.apache.org/jira/secure/attachment/12376073/pigPatch-storeTwice-620665.patch,09/Apr/08 12:36;pi_song;pig_114_optimize_fix_v2.patch;https://issues.apache.org/jira/secure/attachment/12379723/pig_114_optimize_fix_v2.patch,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2008-02-21 18:55:29.231,,,no_permission,,,,,,,,,,,,163775,,,,,Thu Apr 17 15:44:42 UTC 2008,,,Patch Available,,,,0|i0gg9z:,94067,,,,,,,,,,"21/Feb/08 18:55;joa23;for reference:
From a frist look into the code I guess this is what happens.
The data are stored successfully on disk the first time you call store. So PoStore adds an entry to materializedResults.
What is basically a hashmap that holds OperatorKey - just a name and LocalResult - a pointer to the file you just wrote.

If you now trigger store again for the same alias, pig tries to optimize performance bt reusing the output file you just stored.
This happens by first check if there is already materializedResults entry.
What is the case - so in theory this could be reused just read and writte again to a new path.
Now there are a couple of problems. First in your testcase you delete the output file (/tmp/testPigOutput) but pig tries to read in this file again to write it out again. What means you read and write at the same time into the same file. Another problem in your test you delete this file between the store calls , so it can't be read back.

Now a pig come in.  Pig tries to read back in this file with the same object you used for storing this file.
So the object need to implement LoadFunc und StoreFunc, what is not the case in your test you only implement storefunc, what makes sense from my pov. See POLoad, line 57,
lf = (LoadFunc) PigContext.instantiateFuncFromSpec(fileSpec.getFuncSpec()); // the return value can be a StoreFunc only as well.

This worked so far since most of the StoreFunc and LoadFunc are implemented in one class, but not a good idea.

So now the question to the pig developers, how we can solve that problem?
Only cache materialized files in case we do have a load and a store func available?
Re process all required plans in case we can not load a materialized result?

","24/Feb/08 01:56;pi_song;Even both StoreFunc and LoadFunc exist in the custom storage class, it doesn't guarantee that 
{noformat}
LoadFunc (StoreFunc(x)) = x
{noformat}
as this is left open to users to implement.

As the definition of optimization (in this case where we are only interested in output) , the output regardless of doing optimization or not should be the same.

Reading the output of ""Store Operator"" is therefore considered ""unsafe"" for optimization.

My suggestions would be :-
1. By default go back to get intermediate result before ""Store"" as this will rely on StoreFunc and LoadFunc of PigStorage (Supposing that this is not merely load-and-then-store execution plan). Though this will incur some performance hit as the output of MapReduce run associated with ""Store operator"" cannot be reused.
2. Provide a way for users implementing  storage to tell the execution engine that  LoadFunc  is truly inverse of StoreFunc in the implementation so that the execution engine can take advantage of that and doesn't have to go to the intermediate result before ""Store"" .
3. All the built-in storage implementation should be truly reversible","25/Feb/08 22:01;joa23;Hi Pi, 
from my point of view 2. would be the cleanest and best solution. We should add a interface named something like ReversibleStorage (please give me a better name). This interface extends from Load and StoreFunc - java doc clearly mentioned that this interface implementation has to guarantee  ruly reversibility. 
Then we change all internal store and load function to implement this interface. Finally we check during materialization if we can reuse output based if this interface is implemented. 
If people agree I would be happy to work on a patch asap - since this is kind of a blocker issue for our project.

Thoughts?","28/Feb/08 12:46;pi_song;Stefan,
Sorry I should have used bullets instead of numbers. What I really meant was all have to go together. (2) and (3) will be easy to do but (1) will require a lot more work. In order to do that I want to listen to other people a bit more.","29/Feb/08 18:24;alangates;I'm working on making changes to the load function interface as part of the types.  Based on your discussion, I was thinking of adding a method isReversable to the interface.  I think this would meet requirement two.  Sound reasonable?","01/Mar/08 00:23;pi_song;Alan, 

Personally, I prefer a new interface.

So we can have something like

{noformat}
LoadFunc
StoreFunc
ReversibleLoadStoreFunc extends LoadFunc, StoreFunc
{noformat}

This gives better control as someone to implement Load and Store in a reversible way will be enforced to implement both Load and Store. If you just add isReversable  in existing interfaces, you cannot enforce this.

Should we create a sub-issue to deal with (2), (3) first? I think (1) will take a lot more time for testing.","04/Mar/08 14:07;pi_song;Alan,

Are you still working on this? If you don't, I may submit an initial patch for discussion tomorrow.","06/Mar/08 14:24;pi_song;Here is the implementation of what we've discussed above.

This patch is only for preview. Open for discussion about the approach.(*Not to be committed*)
All the current unit tests passed.

Known issues
- Unit test still have to be refactored plus more tests needed (Thanks Johannes for initial test code)
- PigContext seems to know too much about class instantiation. It may require a refactor. Though let's create a new issue to do it later.

I also want to mention the concept of using marker interfaces for optimization hints if operators have special properties. For example, in this issue, we might want to do instead:-
{noformat}
LoadStoreFunc extends LoadFunc, StoreFunc
PigStorage implements LoadStoreFunc, Reversible
{noformat}
Reversible is a marker interface telling that the applied two-way function is reversible.","06/Mar/08 14:40;breed;I'm not a big fan of marker interfaces. (That doesn't mean we shouldn't do it of course :) If we do use a Reversible interface it should extend LoadFunc and StoreFunc since it would be silly to use a Reversible marker and not implement both interfaces.

Alternatively we could add a method to StoreFunc:

LoadFunc getReloader();

that returns a LoadFunc to use to reload the data or null if it cannot be reloaded. This would remove the requirement that a reloadable StoreFunc also implement LoadFunc. ","07/Mar/08 12:46;pi_song;Ben,

Sorry that I led to a long story again. I think whether to do marker interfaces or not can be addressed when we implement optimization.
At this stage I just want to fix this bug which as discussed above will be fixed by
(1) Add a structure to identify reversible property of StoreFunc
(2) Fix optimization engine not to do reverse if it's not safe
(3) Unit testing

I have implemented (1) using what I've proposed before
{noformat}
LoadFunc
StoreFunc
ReversibleLoadStoreFunc extends LoadFunc, StoreFunc
{noformat}

As you've said
{quote}
Alternatively we could add a method to StoreFunc:
LoadFunc getReloader();
{quote}
We still cannot conclude your way or my way which way is better. But since I've already implemented using my way. Let's use my one. ( Meritocrazy :D )

For (2) I've fixed optimization in both local and mapreduce engines

For (3) This time I do an end-to-end test. I'm sorry for that. I find it much more complex to do only plan compiler test (This involves both local and mapreduce engines and physical ops are different). Also a usual way to do optimization test is to compare actual output structure with expected output structure (deterministic optimization engine). We still don't have that framework yet. This should be done as a part of optimization work.


","07/Mar/08 12:48;pi_song;Here's the complete patch.

Unit test included.
All tests passed.

Ready to be committed",07/Mar/08 12:49;pi_song;The flow doesn't work. I have to do manual attach.,"07/Mar/08 19:55;breed;+1 Good job. My main concern was that Reversible not just be a marker otherwise programmers could mark classes that could not possibly be reversible (by not implementing LoadFunc for example), so I'm happy with extending LoadFunc and StoreFunc.","07/Mar/08 22:08;pi_song;That's right. This implementation doesn't rely on marker interface concept.

However you still cannot make it 100%. There's still risk if someone implements ReversibleLoadStoreFunc without really having ""LoadFunc (StoreFunc( x )) = x"". Therefore what we can do is just enforce the contract in documentation. In order to do optimization based on UDF, I think there is no 100% safe. (For example, in the current LoadFunc, you intention is to have the class reading data from filename supplied in bind() right? but people can still read from somewhere else. Someone might just open /etc/passwd and send it out )","07/Mar/08 22:21;breed;I completely agree, and you already have the contract documented in the ReversibleLoadStoreFunc interface.","17/Mar/08 21:33;olgan;Ben, is this patch ready to be committed?",17/Mar/08 22:50;breed;yes,"08/Apr/08 17:52;alangates;Sorry for the way long delay, but I have a question on this code.  If I understand it correctly, it forces all load and store functions to be reversible since the engine currently (wrongly) makes that assumption.  Is that correct?  If so, that doesn't seem good.  We have loader functions that users are using that are not store functions.  We can't break them now just because internally we try to do the wrong thing.  We should fix the underlying problem.  I'm not saying the Reversible interface is a bad idea, because that can allow the system to optimize better.  But we can't force all load functions to be reversible immediately.","08/Apr/08 23:10;pi_song;Alan,

This code changes the semantic to not load from previous result by default unless the previous result was stored by a reversible load/store function. It has also changed internal storages (e.g. PigStorage) to implement reversible interface.","08/Apr/08 23:31;alangates;In MapReducePlanCompiler.java, 

{code}
if (PigContext.instantiateFuncFromSpec(materializedResult.outFileSpec.getFuncSpec()) 
        instanceof ReversibleLoadStoreFunc) {
    POMapreduce pom = new POMapreduce(logicalKey.getScope(),
                                      nodeIdGenerator.getNextNodeId(logicalKey.getScope()),
                                      execEngine.getPhysicalOpTable(),
                                      logicalKey,
                                      pigContext);
    pom.addInputFile(fileSpec);
    pom.mapParallelism = Math.max(pom.mapParallelism, materializedResult.parallelismRequest);
                
    return pom.getOperatorKey();      
}
{code}

That looks to me like it won't allow map reduce to start unless the store func
is reversible.  Am I missing something?

","09/Apr/08 12:34;pi_song;Alan, 

That block is basically just a caching hook-up, if the function is not reversible then it will fall through to below which is actually compiling the operator and use it without reading the cached output. My opinion toward the new plan compilation is to move something like this to the optimization stage as an optional filter.

Here is the block including its context:-
{code}
    public OperatorKey compile(OperatorKey logicalKey, 
                               Map<OperatorKey, LogicalOperator> logicalOpTable, 
                               HExecutionEngine execEngine) throws IOException {
        
        // check to see if we have materialized results for the logical tree to
        // compile, if so, re-use them...
        //
        Map<OperatorKey, MapRedResult> materializedResults = execEngine.getMaterializedResults();
        
        MapRedResult materializedResult = materializedResults.get(logicalKey);
        
        if ( (materializedResult != null) && 
             (PigContext.instantiateFuncFromSpec(materializedResult.outFileSpec.getFuncSpec()) 
                                                           instanceof ReversibleLoadStoreFunc)  )  {
            	POMapreduce pom = new POMapreduce(logicalKey.getScope(),
                             	                 nodeIdGenerator.getNextNodeId(logicalKey.getScope()),
                                	              execEngine.getPhysicalOpTable(),
                                	              logicalKey,
                                	              pigContext);

           		pom.addInputFile(materializedResult.outFileSpec);
            	pom.mapParallelism = Math.max(pom.mapParallelism, materializedResult.parallelismRequest);

            	return pom.getOperatorKey();           
        }
        
        // first, compile inputs into MapReduce operators
        OperatorKey[] compiledInputs = new OperatorKey[logicalOpTable.get(logicalKey).getInputs().size()];
        
        for (int i = 0; i < logicalOpTable.get(logicalKey).getInputs().size(); i++)
            compiledInputs[i] = compile(logicalOpTable.get(logicalKey).getInputs().get(i),
                                        logicalOpTable,
                                        execEngine);
        
        // then, compile this operator; if possible, merge with previous MapReduce
        // operator rather than introducing a new one
        
        LogicalOperator lo = logicalOpTable.get(logicalKey);
        
        if (lo instanceof LOEval) {
            POMapreduce pom = ((POMapreduce)execEngine.getPhysicalOpTable().get(compiledInputs[0]))
                                .copy(nodeIdGenerator.getNextNodeId(logicalKey.getScope())); // make a copy of the previous

        // More and more and more plan compilation here
{code}","09/Apr/08 12:36;pi_song;BTW, this new patch is in sync with the current trunk.",09/Apr/08 15:35;olgan;Are we adding test cases with both reversable and non-reversable storage functions? I think this has to be part of this patch.,"09/Apr/08 21:59;pi_song;Of course, I've included the test cases.",17/Apr/08 15:44;alangates;Patch checked in at revision 649154.  Thanks Pi.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jobs using the combiner and flatten of group keys produce wrong results,PIG-110,12388789,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,antmagna,alangates,alangates,15/Feb/08 17:30,24/Mar/10 22:01,14/Mar/19 03:05,25/Feb/08 21:17,0.0.0,,,,,,0.1.0,,impl,,,0,,,,"A job such as the following:

a = load 'mydata';
b = group a by ($0, $1);
c = foreach b generate flatten(group), COUNT($1)

currently produces wrong results.  Instead of returning the count in the 3rd column, it returns a repeat of the value in the second column.  If the combiner is forced off, this does not occur, so I assume this is in the combiner logic somewhere.

This bug was introduced in revision 617338 (the checkin for PIG-32).",,,,,,,,,,,,,,,,,,,25/Feb/08 19:58;alangates;PIG-110.patch;https://issues.apache.org/jira/secure/attachment/12376430/PIG-110.patch,24/Feb/08 05:34;oae;PIG-110_test.patch;https://issues.apache.org/jira/secure/attachment/12376331/PIG-110_test.patch,25/Feb/08 01:33;oae;PIG-110_test_v2.patch;https://issues.apache.org/jira/secure/attachment/12376381/PIG-110_test_v2.patch,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2008-02-20 01:02:22.264,,,no_permission,,,,,,,,,,,,38376,,,,,Mon Feb 25 21:17:12 UTC 2008,,,Patch Available,,,,0|i0gg8f:,94060,,,,,,,,,,"20/Feb/08 01:02;olgan;Antonio, what is the status of this bug?","24/Feb/08 05:42;oae;I attached a patch with a junit test which reproduces this bug.
The test class execute pig in local mode and in mapreduce mode.
Local pass, mapreduce fails.",25/Feb/08 01:33;oae;Attached test in version 2 with corrected assertions.,"25/Feb/08 19:58;alangates;The issue was that several lines of code were inadvertently dropped in the PIG-32 patch.  This patch restores those lines.  It also includes a somewhat reworked version of the previous test patch (renamed the test, added @Test labels).",25/Feb/08 20:08;olgan;+1,25/Feb/08 20:17;joa23;+1 ,25/Feb/08 21:17;alangates;Fix checked in as revision 630997.   Thanks Johannes for contributing the tests.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"POPrinter does not implement functions for several local physical operators, resulting in incomplete printings of local plans.",PIG-105,12388392,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,alangates,alangates,alangates,11/Feb/08 23:23,25/Mar/10 00:12,14/Mar/19 03:05,26/Jan/09 20:10,0.0.0,,,,,,0.2.0,,impl,,,0,,,,"POVisitor does not implement visitCogroup, visitEval, visitSplit, or visitUnion.  This means that local plans with any of those operators will not be completely printed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-01-26 20:10:04.292,,,no_permission,,,,,,,,,,,,163767,,,,,Mon Jan 26 20:10:04 UTC 2009,,,,,,,0|i0gg67:,94050,,,,,,,,,,26/Jan/09 20:10;olgan;POVisitor no longer exists,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tests: NullPointerException parser.QueryParser.Alias(QueryParser.java:471),PIG-100,12388207,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,francisoud,francisoud,francisoud,08/Feb/08 16:33,24/Mar/10 22:01,14/Mar/19 03:05,26/Mar/08 00:06,0.1.0,,,,,,0.1.0,,impl,,,0,,,,"I think the root problem was that I forget to specify the configuration using -Djunit.hadoop.conf=hadoop-site.xml while running the tests.
But the error could be clearer...

The logs are big so I will provide them in a separate file...

But the core problem is:
{noformat}
    [junit] java.lang.NullPointerException
    [junit] 	at org.apache.pig.impl.logicalLayer.parser.QueryParser.Alias(QueryParser.java:471)
    [junit] 	at org.apache.pig.impl.logicalLayer.parser.QueryParser.NestedExpr(QueryParser.java:411)
    [junit] 	at org.apache.pig.impl.logicalLayer.parser.QueryParser.NestedExpr(QueryParser.java:417)
    [junit] 	at org.apache.pig.impl.logicalLayer.parser.QueryParser.GroupItem(QueryParser.java:1027)
...
    [junit] org.apache.pig.impl.logicalLayer.parser.ParseException: Encountered ""group"" at line 1, column 9.
    [junit] Was expecting one of:
    [junit]     <IDENTIFIER> ...
    [junit]     ""("" ...
    [junit]     
    [junit] 	at org.apache.pig.impl.logicalLayer.parser.QueryParser.generateParseException(QueryParser.java:4142)
...
    [junit] org.apache.pig.impl.logicalLayer.parser.ParseException: Encountered ""generate"" at line 1, column 1.
    [junit] Was expecting one of:
    [junit]     ""load"" ...
    [junit]     ""filter"" ...
{noformat}",,,,,,,,,,,,,,,,,,,08/Feb/08 16:35;francisoud;PIG-100-tests.log;https://issues.apache.org/jira/secure/attachment/12375087/PIG-100-tests.log,13/Feb/08 16:54;francisoud;PIG-100-v01.patch;https://issues.apache.org/jira/secure/attachment/12375497/PIG-100-v01.patch,28/Feb/08 10:36;francisoud;PIG-100-v02.patch;https://issues.apache.org/jira/secure/attachment/12376705/PIG-100-v02.patch,05/Mar/08 13:16;francisoud;PIG-100-v03-spaces.patch;https://issues.apache.org/jira/secure/attachment/12377167/PIG-100-v03-spaces.patch,05/Mar/08 13:15;francisoud;PIG-100-v03-tabs.patch;https://issues.apache.org/jira/secure/attachment/12377166/PIG-100-v03-tabs.patch,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2008-02-21 18:48:33.564,,,no_permission,,,,,,,,,,,,163762,,,,,Wed Mar 26 00:06:42 UTC 2008,,,Patch Available,,,,0|i0gg47:,94041,,,,,,,,,,"13/Feb/08 16:54;francisoud;This wasn't related to the fact that I didn't provide the hadoop-site.xml
The test is checking how the parser handle pasing exception (which is great).
But the error message (a null pointer) can be improve to provide feedback to the user why the parsing failed.

This patch add a better error handling and more important a better error message when an alias is not find in the aliases list.

Modified QueryParser.jjt

Now the error message looks like:
{noformat}
org.apache.pig.impl.logicalLayer.parser.ParseException: Unable to find alias: 'A' - aliases: ''
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.Alias(QueryParser.java:484)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.NestedExpr(QueryParser.java:411)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.NestedExpr(QueryParser.java:417)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.GroupItem(QueryParser.java:1043)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.CogroupClause(QueryParser.java:996)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:537)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.NestedExpr(QueryParser.java:423)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.ForEachClause(QueryParser.java:1364)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:566)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:371)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:244)
	at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:47)
	at org.apache.pig.test.TestLogicalPlanBuilder.buildPlan(TestLogicalPlanBuilder.java:544)
	at org.apache.pig.test.TestLogicalPlanBuilder.buildPlan(TestLogicalPlanBuilder.java:535)
	at org.apache.pig.test.TestLogicalPlanBuilder.testQueryFail1(TestLogicalPlanBuilder.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at junit.framework.TestCase.runTest(TestCase.java:164)
	at junit.framework.TestCase.runBare(TestCase.java:130)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:120)
	at junit.framework.TestSuite.runTest(TestSuite.java:228)
	at junit.framework.TestSuite.run(TestSuite.java:223)
	at org.junit.internal.runners.OldTestClassRunner.run(OldTestClassRunner.java:35)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:38)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
{noformat}","21/Feb/08 18:48;olgan;Benjamin, thanks for fixing this issue. Is there any value in actually throwing the exception? Seems like the error message should be sufficient in this case.","21/Feb/08 19:11;joa23;Olga, 
there is always value in throwing an exception especially for such a young project as pig. 
Pig does in many cases a bad job in giving the user an idea of what is going on/ wrong. 
On the one hand side I can understand that there is an interestes  of not bordering users with stack traces. On the other hand side, users of pig are not end users but mostly developers. Showing stack traces will help to understand faster the problem - it will invite more developer to read the code - what will help to grow the contributor base of a open source project. What finally will help to iron out more issues faster.

There are endless debates about exception handling but from my experience throwing an exception and handle it as high as possible always help to improve code stability.
Fail early...
Just my 2 cents.","21/Feb/08 22:10;olgan;There are two kinds of errors:

- problems caused by users - where providing a meaningful error message should be sufficient
- internal system errors which should have stack trace written (at least into log file)

This case is clearly a user problem and I think having stack trace only confuses users rather than helps.
","25/Feb/08 21:00;olgan;cleared ""patch available"" flag since I would like to see just an error message and no exception for user syntax error","28/Feb/08 10:36;francisoud;Stefan, Olga, I tried to match  both your requirements...

I added a special exception handling in LogicalPlanBuilder:
{code:java}
try {
    plan = parser.Parse();
} catch (ParseException e) {
    log.error(e.getMessage());
    log.debug(e);
}
{code}

Users should only see the error message: ""Unable to find alias: 'A' - aliases: ''""
Developers can set their logging level to debug to get the full stacktrace...

What do you think of it ?

Could you test my patch and see if it fix the problem ?

(I can't run the tests locally at the moment because of my OutOfMemory errors PIG-99)

thanks",28/Feb/08 11:54;pi_song;Should we somehow verify as many as possible the needed configurations before starting Pig? (Main configuration + the selected backend),"28/Feb/08 19:15;olgan;Pi, this issue is about missing alias. You might want to open a separate on for configuration verification. Thanks","02/Mar/08 20:15;oae;{noformat}
try {
    plan = parser.Parse();
} catch (ParseException e) {
    log.error(e.getMessage());
    log.debug(e);
}
{noformat}
Hi Benjamin, i'm very much aginst this kind of error handling.
First of all, i think the user of pig (or the caller of PigServer#registerQuery()) should be in control wether or not he want to stop the execution of his process when the registration of a query failed. I think in a lot of cases the user wants to fail fast. Grunt represents the other side. So if this error handling is appropriate for grunt, it should move to grunt.

Secondly the 2 log statments would lead to duplicate logging, would'nt they ?","03/Mar/08 11:18;francisoud;Johannes would you prefer this ? (added throw)
{code:java}
try {
    plan = parser.Parse();
} catch (ParseException e) {
    log.error(e.getMessage());
    log.debug(e);
    throw e;
}
{code}
About the double logging, I proposed this because:
* Stefan (and I ;) )wanted the full stacktrace
* Olga wanted a small user friendly message

The message will be logged twice and will look like (if DEBUG level is use for logs): 
{noformat}
[time] [class] Unable to find alias: 'A' - aliases: ''
[time] [class] org.apache.pig.impl.logicalLayer.parser.ParseException: Unable to find alias: 'A' - aliases: ''
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.Alias(QueryParser.java:484)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.NestedExpr(QueryParser.java:411)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.NestedExpr(QueryParser.java:417)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.GroupItem(QueryParser.java:1043)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.CogroupClause(QueryParser.java:996)
	at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:537)
...
{noformat}

It will look like this (if log level is ERROR, usually in a production environment):
{noformat}
[time] [class] Unable to find alias: 'A' - aliases: ''
{noformat}

Johannes, do you prefer this ? Do you have something else in mind ?","03/Mar/08 12:22;pi_song;Benjamin's proposed solution sounds good to me. If you don't like the full stack trace, you can turn it off.","03/Mar/08 17:29;oae;Hi Benjamin, i think doing
{noformat}
try {
    plan = parser.Parse();
} catch (ParseException e) {
    log.error(e.getMessage());
    log.debug(e);
}
{noformat}
is fine. But not in LogicalPlanBuilder, better somewhere in Grunt.
I think the duplicate logging could be avoided with something like:
{noformat}
if(log.isDebugEnabled()){
    log.error(e.getMessage(), e);
}{
   log.error(e.getMessage());
}
{noformat}",03/Mar/08 22:49;olgan;I agree with Johanes. This should be done outside of PigServer class so that the caller can make the choices of how to deal with the problem.,"03/Mar/08 22:50;olgan;Cleared ""patch available"" flag as this would need to be reworked once we agree on general error handling for pig","05/Mar/08 06:13;oae;I would like to vote for the half of the current patch!
The changes in LogicalPlanBuilder i would omit, but he changes in QueryParser.jjt are very useful i think. 
WDYT ?","05/Mar/08 13:03;francisoud;Johannes> it's funny because it's exactly PIG-100-v01.patch ;)
I suppose PIG-100-v01.patch will conflict with latest svn revision so I will regenerate it.","05/Mar/08 13:15;francisoud;Same patch as v1 (contains only QueryParser.jjt modification) against latest trunk revision.
I made 2 versions of patch v03
* 'PIG-100-v03-tabs.patch' use tabs for indentation (like the rest of the file)
* 'PIG-100-v03-spaces.patch' use spaces like pig code convention but is not consistent with the rest of the QueryParser.jjt

pick your favorite patch ;)","26/Mar/08 00:06;olgan;Patch committed. Thanks Benjamin for contributing! 

Also added some error hamdling to get more clear error message",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jobs produce wrong results when a cogroup is in the script and the compiler chooses to use the combiner feature of hadoop.,PIG-97,12388024,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,06/Feb/08 22:29,24/Mar/10 22:01,14/Mar/19 03:05,11/Feb/08 23:20,0.0.0,,,,,,0.1.0,,impl,,,0,,,,"The following script will produce 0 output records, even when it should produce records:

a = load 'file1';
b = load 'file2';
c = cogroup a by $0, b by $0;
d = foreach c generate $0, COUNT($1), COUNT($2);
dump d;

In this case pig chooses to use the combiner in order to be more efficient.  However, the following code in PigCombiner.java causes a problem:

for (int i = 0; i < inputCount; i++) {  // XXX: shouldn't we only do this if INNER flag is set?
    if (t.getBagField(1 + i).size() == 0) return;
}

In this case a map is often running on a machine where it has access to only one of the two files and thus there is nothing in one of the bags, so the above lines of code cause the combiner to bailout without pushing any tuples to the OutputCollector.

The proposed solution for the short term is to disable use of the combiner in cases where more than one file are grouped together.
",,,,,,,,,,,,,,,,,,,07/Feb/08 17:49;alangates;cogroupcombiner.patch;https://issues.apache.org/jira/secure/attachment/12375004/cogroupcombiner.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-02-11 21:43:41.09,,,no_permission,,,,,,,,,,,,38581,,,,,Mon Feb 11 23:20:28 UTC 2008,,,Patch Available,,,,0|i0gg33:,94036,,,,,,,,,,"07/Feb/08 17:49;alangates;The attached patch turns off combiner in the case of cogrouping being used.  It also restores the POVisitor to work the way it did before the front-end back-end split was introduced (PIG-32).  I needed this to make explain work again, so I could see when the combiner was and wasn't being invoked.

Antonio, please take a look at my changes for the POVisitor and make sure it will work within the new split framework.","11/Feb/08 21:43;olgan;+1

Antonio, could you also post your comments if any, thanks.","11/Feb/08 21:48;antmagna;The patch looks good for the POVisitor pattern. I just have comment/question:

it looks like the POPrinter is currently not overriding some of the visitX methods in the super class (POVisitor), such as visitCogroup, visitSplit, visitUnion. If the local physical query plan contains such operators no info would be printed out for those operators",11/Feb/08 23:20;alangates;Fix checked in as revision 620665.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Impossible to set jobconf parameters,PIG-93,12387878,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,francisoud,francisoud,francisoud,05/Feb/08 16:19,24/Mar/10 22:01,14/Mar/19 03:05,20/May/08 09:03,0.1.0,,,,,,0.1.0,,impl,,,1,,,,"I'm trying to set jobconf parameter before launching a pig job using pig api.

I tried 2 different ways but with no success:
{code:java}
PigContext pigContext = new PigContext(ExecType.MAPREDUCE);
pigContext.getExecutionEngine().getConfiguration().putAll(properties);
PigServer pigServer = new PigServer(pigContext);
....
{code}

Throw a NPE because the internal executionEngine var is initialize only when calling connect().

So I tried:
{code:java}
PigContext pigContext = new PigContext(ExecType.MAPREDUCE);
pigContext.connect();
pigContext.getExecutionEngine().getConfiguration().putAll(properties);
PigServer pigServer = new PigServer(pigContext);
...
{code}

My properties have been replace with a ""new JobConf()""
{noformat}
java.lang.RuntimeException: Bad mapred.job.tracker: local
at org.apache.hadoop.mapred.JobTracker.getAddress(JobTracker.java:711)
at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.init(HExecutionEngine.java:149)
at org.apache.pig.impl.PigContext.connect(PigContext.java:180)
{noformat}

""properties"" contains ""mapred.job.tracker"" and ""hadoop.tmp.dir values""

Before PIG-32 I use to do (and it was working): 
{code:java}
PigContext pigContext = new PigContext(ExecType.MAPREDUCE);
pigContext.setConf(myJobConf);
PigServer pigServer = new PigServer(pigContext);
...
{code}

Any idea before I start to work on a patch ?",,,,,,,,,,,,,,,,,,,06/Feb/08 17:13;francisoud;PIG93Main.java;https://issues.apache.org/jira/secure/attachment/12374883/PIG93Main.java,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-02-25 20:48:43.961,,,no_permission,,,,,,,,,,,,163756,,,,,Tue May 20 09:03:26 UTC 2008,,,,,,,0|i0gg1b:,94028,,,,,,,,,,"05/Feb/08 16:26;francisoud;2 other ways that don't work:

no connect():

{code:java}
final PigContext pigContext = new PigContext(ExecType.MAPREDUCE);
pigContext.setJobtrackerLocation(properties.get(""mapred.job.tracker""));
pigContext.setJobName(properties.get(""fs.default.name""));
final PigServer pigServer = new PigServer(pigContext);
{code}

{noformat}
java.lang.NullPointerException
at org.apache.pig.impl.PigContext.setJobtrackerLocation(PigContext.java:213)
...
{noformat}

using connect():

{code:java}
final PigContext pigContext = new PigContext(ExecType.MAPREDUCE);
pigContext.connect();
pigContext.setJobtrackerLocation(properties.get(""mapred.job.tracker""));
pigContext.setJobName(properties.get(""fs.default.name""));
final PigServer pigServer = new PigServer(pigContext);
{code}

{noformat}
java.lang.RuntimeException: Bad mapred.job.tracker: local
at org.apache.hadoop.mapred.JobTracker.getAddress(JobTracker.java:711)
at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.init(HExecutionEngine.java:149)
at org.apache.pig.impl.PigContext.connect(PigContext.java:180)
{noformat}
","06/Feb/08 17:13;francisoud;Java Main to reproduce this bug.
You need to change FS_NAME, JOB_TRACKER, INPUT_PATH and OUTPUT_PATH values.

The stacktrace:
{noformat}
Running main...
08/02/06 18:11:04 INFO apache.pig: Connecting to hadoop file system at: file:///
08/02/06 18:11:05 INFO apache.pig: Connecting to map-reduce job tracker at: local
Exception in thread ""main"" java.lang.RuntimeException: Bad mapred.job.tracker: local
	at org.apache.hadoop.mapred.JobTracker.getAddress(JobTracker.java:711)
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.init(HExecutionEngine.java:151)
	at org.apache.pig.impl.PigContext.connect(PigContext.java:182)
	at org.apache.pig.PigServer.<init>(PigServer.java:123)
	at org.apache.pig.PIG93Main.main(PIG93Main.java:22)
{noformat}",25/Feb/08 20:48;olgan;Cleared patch available flag since this is not a patch that needs to be committed,"27/Feb/08 10:32;francisoud;Yes you are right, I got caught by something else and didn't provide the patch rapidly...
Started to work on that now ;)","28/Feb/08 16:25;francisoud;This issue is currently being fix in PIG-111.
Will close it once PIG-111 patch gets commited in trunk.","29/Feb/08 10:23;jkff;I am having a similar problem while trying to get started with Pig.
I've successfully installed Hadoop on 2 machines and hadoop's examples finally worked.
Now, I built Pig and that's what I get when launching it:

<code>
jkff@*****  trunk$ scripts/pig.pl -v -cp pig.jar:~/hadoop/hadoop-0.16.0/conf
I can't find HOD configuration for , hopefully you weren't planning on using HOD.
2008-02-29 13:19:22,102 [main] DEBUG org.apache.hadoop.conf.Configuration - java.io.IOException: config()
        at org.apache.hadoop.conf.Configuration.<init>(Configuration.java:144)
        at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:112)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.<init>(HExecutionEngine.java:88)
        at org.apache.pig.impl.PigContext.connect(PigContext.java:179)
        at org.apache.pig.PigServer.<init>(PigServer.java:132)
        at org.apache.pig.tools.grunt.Grunt.<init>(Grunt.java:41)
        at org.apache.pig.Main.main(Main.java:247)

2008-02-29 13:19:22,259 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: file:///
2008-02-29 13:19:22,263 [main] DEBUG org.apache.hadoop.conf.Configuration - java.io.IOException: config()
        at org.apache.hadoop.conf.Configuration.<init>(Configuration.java:144)
        at org.apache.pig.backend.hadoop.datastorage.HConfiguration.getConfiguration(HConfiguration.java:38)
        at org.apache.pig.backend.hadoop.datastorage.HDataStorage.<init>(HDataStorage.java:36)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.init(HExecutionEngine.java:140)
        at org.apache.pig.impl.PigContext.connect(PigContext.java:181)
        at org.apache.pig.PigServer.<init>(PigServer.java:132)
        at org.apache.pig.tools.grunt.Grunt.<init>(Grunt.java:41)
        at org.apache.pig.Main.main(Main.java:247)

2008-02-29 13:19:22,418 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to map-reduce job tracker at: local
2008-02-29 13:19:22,428 [main] DEBUG org.apache.hadoop.conf.Configuration - java.io.IOException: config()
        at org.apache.hadoop.conf.Configuration.<init>(Configuration.java:144)
        at org.apache.pig.backend.hadoop.datastorage.HConfiguration.getConfiguration(HConfiguration.java:38)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.init(HExecutionEngine.java:149)
        at org.apache.pig.impl.PigContext.connect(PigContext.java:181)
        at org.apache.pig.PigServer.<init>(PigServer.java:132)
        at org.apache.pig.tools.grunt.Grunt.<init>(Grunt.java:41)
        at org.apache.pig.Main.main(Main.java:247)

2008-02-29 13:19:22,610 [main] ERROR org.apache.pig.Main - java.lang.RuntimeException: Bad mapred.job.tracker: local
</code>

I've been grepping the sources for a couple of hours already to find out where these properties really should have been set to something senseful, but I found that the HExecutionEngine constructor is actually invoked in its 1-argument version, where it creates an 'empty' JobConf() and does not seem to be going to read my hadoop-site.conf.

I am pretty sure I made a mistake somewhere, because it can't be that pig simply doesn't work at all from command line, and I'd be much pleased to be pointed at this mistake :)

It would be even better if you had a from-scratch description like 'How to install pig and hadoop and launch a simple example on two machines' in the wiki.","29/Feb/08 10:51;francisoud;Eugene you should watch (and vote for) PIG-111, I tested the latest patch and it fixes PIG-93.

I like the idea of making an example page in the wiki for that, I will take care of it once the new PigServer and PigContext api is commited in svn...","29/Feb/08 11:36;jkff;Thanks, I'll have a look there.",19/May/08 18:06;olgan;PIG-111 has been committed a while back. Can this issue be closed?,20/May/08 09:03;pi_song;Already fixed as a part of Pig-111,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigContext NullPointerException because of uninitialize conf,PIG-92,12387877,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,francisoud,francisoud,francisoud,05/Feb/08 15:23,24/Mar/10 22:01,14/Mar/19 03:05,12/Feb/08 17:38,0.1.0,,,,,,0.1.0,,impl,,,0,,,,"This simple code throw an NPE
{code:java}
final PigContext pigContext = new PigContext(ExecType.MAPREDUCE);
pigContext.getConf().putAll(properties);
{code}

Because in PigContext.java:

{code:java}
transient private Properties conf = null;
public void connect() throws ExecException {
    ... 
    conf = new Properties();
    ....
}
{code}

Simple patch:

{code:java}
transient private Properties conf = new Properties();
public void connect() throws ExecException {
    ... 
}
{code}

This is regression already fix in PIG-69.
Introduce with PIG-32
",,,,,,,,,,,,,,,,,,,05/Feb/08 15:25;francisoud;PIG-92-v01.patch;https://issues.apache.org/jira/secure/attachment/12374778/PIG-92-v01.patch,11/Feb/08 15:53;francisoud;PIG-92-v02.patch;https://issues.apache.org/jira/secure/attachment/12375245/PIG-92-v02.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-02-06 21:08:26.311,,,no_permission,,,,,,,,,,,,38580,,,,,Tue Feb 12 17:38:58 UTC 2008,,,Patch Available,,,,0|i0gg0n:,94025,,,,,,,,,,05/Feb/08 15:25;francisoud;Patch as explain in the jira description.,"06/Feb/08 21:08;olgan;Does this patch still needs to be applied?

From the comment above, it seems that it has already been adressed in another bug.","07/Feb/08 14:06;francisoud;> Does this patch still needs to be applied?

Unless I missed something, I don't think the null pointer has been fix...

","07/Feb/08 18:46;olgan;ok, could you then regenerate your patch from the latest svn, thanks.",11/Feb/08 15:53;francisoud;new patch against trunk r620508,12/Feb/08 17:38;alangates;Patch contributed by Benjamin Francisoud committed as revision 620878.  Thanks Benjamin.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
outdated @Override tags,PIG-91,12387818,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,oae,oae,oae,04/Feb/08 20:40,24/Mar/10 22:01,14/Mar/19 03:05,06/Feb/08 16:53,,,,,,,0.1.0,,impl,,,0,,,,"There are a bunch of @Override tags which are not correct anymore (i guess since PIG-32).
In my ide (eclipse) this results in compiling errors.

See for example
HDataStorage.java",,,,,,,,,,,,,,,,,,,04/Feb/08 20:42;oae;pig-overirde.patch;https://issues.apache.org/jira/secure/attachment/12374704/pig-overirde.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-02-06 16:52:53.518,,,no_permission,,,,,,,,,,,,163755,,,,,Wed Feb 06 16:52:53 UTC 2008,,,Patch Available,,,,0|i0gg0f:,94024,,,,,,,,,,04/Feb/08 20:42;oae;I've uploaded a patch wich removes the @Override tags from these location which caused the compile errors.The patch is taken from the src folder.,"06/Feb/08 16:52;alangates;Antonio has uploaded a new patch to PIG-32 (patch.2008.02.04) that removes all the same @Overrides as your patch.  That patch has already been committed to trunk, so I'll close this bug as resolved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigServer#store does swallow exception,PIG-90,12387815,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,francisoud,joa23,joa23,04/Feb/08 20:09,24/Mar/10 22:01,14/Mar/19 03:05,12/Feb/08 18:53,0.0.0,,,,,,0.1.0,,,,,0,,,,"My custom DatabaseStoreFunction throws an (runtime or ioException) exception in putNext.
Instead throwing this exception all the way up  (the exceptions contains a nice error message text) however in pigServer 326 a java.lang.NoSuchMethodError: java.io.IOException: method <init>(Ljava/lang/String;Ljava/lang/Throwable;)V not found Exception will be thrown. 


",,,,,,,,,,,,,,,,,,,12/Feb/08 13:37;francisoud;PIG-90-v01.patch;https://issues.apache.org/jira/secure/attachment/12375355/PIG-90-v01.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-02-12 13:37:11.139,,,no_permission,,,,,,,,,,,,163754,,,,,Tue Feb 12 18:53:13 UTC 2008,,,,,,,0|i0gfzz:,94022,,,,,,,,,,"12/Feb/08 13:37;francisoud;I tried but couldn't reproduce it...

I created a Unit Test to simulate this error with a fake StoreFunc throwing a RuntimeException in putNext() and here's the stacktrace I got:
{noformat}
java.io.IOException: Unable to store alias counts
	at org.apache.pig.PigServer.store(PigServer.java:328)
	at org.apache.pig.PigServer.store(PigServer.java:310)
	at org.apache.pig.PigServerTest.testStoreException(PigServerTest.java:34)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.junit.internal.runners.TestMethodRunner.executeMethodBody(TestMethodRunner.java:99)
	at org.junit.internal.runners.TestMethodRunner.runUnprotected(TestMethodRunner.java:81)
	at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)
	at org.junit.internal.runners.TestMethodRunner.runMethod(TestMethodRunner.java:75)
	at org.junit.internal.runners.TestMethodRunner.run(TestMethodRunner.java:45)
	at org.junit.internal.runners.TestClassMethodsRunner.invokeTestMethod(TestClassMethodsRunner.java:71)
	at org.junit.internal.runners.TestClassMethodsRunner.run(TestClassMethodsRunner.java:35)
	at org.junit.internal.runners.TestClassRunner$1.runUnprotected(TestClassRunner.java:42)
	at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)
	at org.junit.internal.runners.TestClassRunner.run(TestClassRunner.java:52)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:38)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: org.apache.pig.backend.executionengine.ExecException
	at org.apache.pig.backend.local.executionengine.LocalExecutionEngine.execute(LocalExecutionEngine.java:137)
	at org.apache.pig.backend.local.executionengine.LocalExecutionEngine.execute(LocalExecutionEngine.java:1)
	at org.apache.pig.PigServer.store(PigServer.java:325)
	... 22 more
Caused by: java.io.IOException: java.lang.RuntimeException: putNext
	at org.apache.pig.builtin.BogusStoreFunc.putNext(BogusStoreFunc.java:17)
	at org.apache.pig.impl.io.PigFile.store(PigFile.java:64)
	at org.apache.pig.backend.local.executionengine.POStore.getNext(POStore.java:103)
	at org.apache.pig.backend.local.executionengine.LocalExecutionEngine.execute(LocalExecutionEngine.java:130)
	... 24 more
{noformat}

You can see the correct error message down the stacktrace.

According to the error message at runtime you pasted in the jirra description; the jvm couldn't find:
[http://java.sun.com/javase/6/docs/api/java/io/IOException.html|IOException(String message, Throwable cause)]  which is java 6.

I think you tested pig just when the [PIG-32|https://issues.apache.org/jira/browse/PIG-32?focusedCommentId=12564725#action_12564725] had introduce java 5/6 incompatibilities, Antonio Magnaghi and Olga Natkovich  fixed it.
So you should test again and see if you still et this error...","12/Feb/08 13:39;francisoud;Unless Stefan Groschupf says no, I think this one can be close as ""already fix"" or ""see else where > PIG-32""","12/Feb/08 18:53;joa23;Sure, lets close. Thanks Benjamin.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Too many spills to files causes ArrayIndexOutOfBoundsException if new temp file cant be created,PIG-89,12387807,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,francisoud,craigm,craigm,04/Feb/08 18:27,24/Mar/10 22:01,14/Mar/19 03:05,06/Feb/08 20:57,,,,,,,0.1.0,,data,,,0,,,,"Hello,

I am experimenting, trying to perform a DISTINCT on a medium sized set of URLs - about 3million (same set as I discussed previously - Utkarsh has a copy), this time in local execution mode.

Pig script:
{{
A = LOAD 'all_13122007.txt';
B = DISTINCT A;
store B into 'bla;
}}


Bring these errors (two lines swapped in DefaultDatabag) to find real error.
{{
2008-02-04 18:09:44,756 [Low Memory Detector] INFO  org.apache.pig - low memory handler called init = 29491200(28800K) used = 269834064(263509K) committed = 307036160(299840K) max = 471662592(460608K)
2008-02-04 18:09:45,355 [Low Memory Detector] ERROR org.apache.pig - Unable to spill contents to disk
java.io.IOException: Too many open files
        at java.io.UnixFileSystem.createFileExclusively(Native Method)
        at java.io.File.checkAndCreate(File.java:1704)
        at java.io.File.createTempFile(File.java:1793)
        at java.io.File.createTempFile(File.java:1830)
        at org.apache.pig.data.DataBag.getSpillFile(DataBag.java:367)
        at org.apache.pig.data.DefaultDataBag.spill(DefaultDataBag.java:69)
        at org.apache.pig.impl.util.SpillableMemoryManager.handleNotification(SpillableMemoryManager.java:123)
        at sun.management.NotificationEmitterSupport.sendNotification(NotificationEmitterSupport.java:138)
        at sun.management.MemoryImpl.createNotification(MemoryImpl.java:171)
        at sun.management.MemoryPoolImpl$CollectionSensor.triggerAction(MemoryPoolImpl.java:300)
        at sun.management.Sensor.trigger(Sensor.java:120)
java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.ArrayList.remove(ArrayList.java:390)
        at org.apache.pig.data.DefaultDataBag.spill(DefaultDataBag.java:84)
        at org.apache.pig.impl.util.SpillableMemoryManager.handleNotification(SpillableMemoryManager.java:123)
        at sun.management.NotificationEmitterSupport.sendNotification(NotificationEmitterSupport.java:138)
        at sun.management.MemoryImpl.createNotification(MemoryImpl.java:171)
        at sun.management.MemoryPoolImpl$CollectionSensor.triggerAction(MemoryPoolImpl.java:300)
        at sun.management.Sensor.trigger(Sensor.java:120)
Exception in thread ""Low Memory Detector"" java.lang.InternalError: Error in invoking listener
        at sun.management.NotificationEmitterSupport.sendNotification(NotificationEmitterSupport.java:141)
        at sun.management.MemoryImpl.createNotification(MemoryImpl.java:171)
        at sun.management.MemoryPoolImpl$CollectionSensor.triggerAction(MemoryPoolImpl.java:300)
        at sun.management.Sensor.trigger(Sensor.java:120)

}}

There are a two sub-issues here: 

1. Pig spills too much using a default JVM (64MB) size - expected?
Perhaps pig.pl should set a default JVM size of more than 64MB?


2. the line DefaultDataBag.java:84
{{{
mSpillFiles.remove(mSpillFiles.size() - 1);
}}}
line should check that mSpillFiles.size() > 0,  because if File.createTempFile( ) in Databag.getSpillFile() fails, the mSpillFiles will not yet have been updated. My preference would be to split try{ } catch (IOException ioe) { } within DefaultDatabag.spill() into two exception handlers - one for getSpillFile() errors, and one for actual writing errors (when we know mSpillFiles has been added to).

If this latter point isnt coherent, I can create patch.

Ta muchly.

C","Linux, Local execution Mode, JDK 1.6",,,,,,,,,,,,,,,,,,06/Feb/08 20:35;alangates;databag-89-v3.patch;https://issues.apache.org/jira/secure/attachment/12374907/databag-89-v3.patch,04/Feb/08 23:29;breed;patch-v2.defaultdatabag;https://issues.apache.org/jira/secure/attachment/12374724/patch-v2.defaultdatabag,04/Feb/08 22:42;craigm;patch.defaultdatabag;https://issues.apache.org/jira/secure/attachment/12374719/patch.defaultdatabag,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2008-02-04 23:29:31.879,,,no_permission,,,,,,,,,,,,163753,,,,,Wed Feb 06 20:57:35 UTC 2008,,,,,,,0|i0gfzj:,94020,,,,,,,,,,04/Feb/08 18:28;craigm;formatting,04/Feb/08 22:42;craigm;Patch attached to fix the IOException/ArrayIndexOutOfBoundsException issue in DefaultDatabag.,04/Feb/08 23:29;breed;It looks like we are missing a close. I've amended your patch to add the missing close. Could you let me know if this fixes your problem?,"05/Feb/08 12:46;craigm;Thanks Benjamin. I can no longer produce the exception using the patch. I agree that the close() was missing and this is the likely cause of the Too many open files, as they were not being closed until the finalizer ran on those output streams. Explicitly closing them as soon as they are written will resolve the issue.

To increase the memory available to Pig, add
push @javaArgs, ""-Xmx512M"";
to your pigclient.conf

Cheers

C","05/Feb/08 13:31;craigm;Hi Ben,

On examination, similar changes should be made to DistinctDataBag and SortedDataBag.

C",06/Feb/08 20:35;alangates;Here's a patch that applies Ben's changes to SortedDataBag and DistinctDataBag in addition to DefaultDataBag.,06/Feb/08 20:45;olgan;+ 1,06/Feb/08 20:57;alangates;Last patch committed (revision 619151).  Thanks to Ben and Craig for your work on this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
the project does not compile because of reference to HadoopExe class in Main.java,PIG-88,12387704,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,pi_song,pi_song,pi_song,02/Feb/08 08:21,24/Mar/10 22:01,14/Mar/19 03:05,20/Feb/08 17:01,0.1.0,,,,,,0.1.0,,,,,0,,,,"The project does not compile because of this line in Main.java

import org.apache.hadoop.util.HadoopExe;

From HADOOP-435, the patch to introduce this class has been canceled plus the class itself is not being used at all in Main.java.

The simple patch removes that particular line and now the project compiles successfully.",Win XP,300,300,,0%,300,300,,,,,,,,,,,,02/Feb/08 08:41;pi_song;PIG-88.HadoopExe.patch;https://issues.apache.org/jira/secure/attachment/12374601/PIG-88.HadoopExe.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-02-20 17:01:14.73,,,no_permission,,,,,,,,,,,,38578,,,,,Wed Feb 20 17:01:14 UTC 2008,,,Patch Available,,,,0|i0gfz3:,94018,,,,,,,,,,02/Feb/08 08:41;pi_song;A very simple patch to remove reference to HadoopExe,20/Feb/08 17:01;alangates;Fixed checked in as revision 629534.  Thanks Pi Song.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to specify CTRL-A as a delimiter for the PigStorage function,PIG-85,12387572,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pi_song,anandm,anandm,31/Jan/08 20:01,24/Mar/10 22:01,14/Mar/19 03:05,03/Jun/08 21:47,,,,,,,0.1.0,,,,,0,,,,"A PIG command like - 

store abc into 'abc' using PigStorage('\x01');

 does not recognize hat the user is requesting the data to by ^A separated. Instead the data that is stored is literally separated by the string '\x01'. 

Neither does punching in ^A directly through the editor, nor do any other strings like \u0001 help. 
Using a ^A directly through the editor complains about it being an invalid XML character and bails out. ",,,,,,,,,,,,,,,,,,,29/May/08 18:18;olgan;PIG-85_v4.patch;https://issues.apache.org/jira/secure/attachment/12383039/PIG-85_v4.patch,16/May/08 13:11;pi_song;PIG_85_escaping_parameters.patch;https://issues.apache.org/jira/secure/attachment/12382168/PIG_85_escaping_parameters.patch,16/May/08 16:54;pi_song;PIG_85_v2.patch;https://issues.apache.org/jira/secure/attachment/12382187/PIG_85_v2.patch,18/May/08 14:57;pi_song;PIG_85_v3.patch;https://issues.apache.org/jira/secure/attachment/12382267/PIG_85_v3.patch,16/May/08 22:31;olgan;TEST-org.apache.pig.test.TestStore.txt;https://issues.apache.org/jira/secure/attachment/12382223/TEST-org.apache.pig.test.TestStore.txt,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2008-01-31 21:07:18.812,,,no_permission,,,,,,,,,,,,163750,,,,,Tue Jun 03 21:47:55 UTC 2008,,,,,,,0|i0gfxr:,94012,,,,,,,,,,"31/Jan/08 21:07;spullara;This is the right format for this: PigStorage('\01')
","31/Jan/08 21:58;anandm;I am not sure if that helps. 

Here is the query:
entries = LOAD '/user/anandm/testData/testTableA.txt' AS (key,value);
STORE entries INTO '/user/anandm/tmp/testTableACtrl01.tsv' USING PigStorage('\01');

The output contains this:
a\011
b\012
c\013
d\014
e\015

We do not want to see \01 but instead see '^A' character as the separator.","31/Jan/08 22:11;spullara;Ah, this may be a bug in store vs. load.  I know that load works with that format.","29/Apr/08 20:29;amirhyoussefi;Yes, store has a problem. Users should be able to have: 

STORE ... USING PigStorage('\u0001') 

for Ctrl+A
","30/Apr/08 13:26;pi_song;At the moment, string escaping only works with standalone literals. To add support for LOAD/STORE function parameter is not difficult but it will cause trouble in the parser change (which is close to finish) in type branch. Please wait a bit more and I'll fix this.

In the mean time one way to do is inheriting the PigStorage and set the default delimitor to anything you want in Java. Then use it as a UDF LOAD/STORE.","12/May/08 22:46;breed;StringList() in QueryParser.jjt should be using unquote() on QUOTEDSTRING. It looks like it is the only place that was missed when we did the unquote patch. It's simple enough to fix (a two line patch). Is that going to mess up your parser change too much Pi? Here are the lines in StringList that need to be changed:

	t = <QUOTEDSTRING> {sb.append(t.image);}
        t = <QUOTEDSTRING> {sb.append(t.image);}

they should both become
	t = <QUOTEDSTRING> {sb.append(unquote(t.image));}","12/May/08 23:12;pi_song;Thanks Ben.

At the beginning, I thought I wouldn't want to touch the grammar file for a while but so many people (including myself) already touched it. I'll fix this.","13/May/08 14:40;olgan;Pi, when do you think this might be resolved. We have several users seeing this problem. Thanks.",14/May/08 20:47;olgan;I integrated and tested the solution proposed by Ben. It does solve the store issue but it breaks the load :).,14/May/08 21:26;olgan;I am guessing this is due to the fact that we are using regular expression for load. I will take a stab at integrating code I got from Alan to get rid of regular expression and then retest the changes,"15/May/08 14:17;pi_song;In Load we use regex to split:-
{noformat}
String[] splitString = textLine.split(delimiter, -1);

JavaDoc: String[] 	split(String regex)      Splits this string around matches of the given regular expression
{noformat}
But in Store we use normal string concatenation:-
{noformat}
if (it.hasNext()) buf.append(delim);
{noformat} 
  
This can be very confusing for users.
For example, if I do
{noformat}
a = LOAD '/tmp/datatest1.txt' USING PigStorage('\\d') ;
{noformat}
 ""\ \d"" will get unescaped as in Ben's solution to '\d' which is seperating by digits.

Then I  do
{noformat}
STORE a INTO '/tmp/dataout' USING PigStorage('\\d')
{noformat}
the output will have the actual string ""\d"" as separators 

Are there many users relying on this Regex stuff? If not +1 for removing it. We can introduce a new built-in storage that supports Regex.
",15/May/08 14:53;alangates;AFAIK people are not relying heavily on the regex in the load.  And using a regex for parsing lines in our default load function is very heavy.  I was planning on removing that from PigStorage as part of the rewrite of that function in the pipeline work.  So I vote for removing it too.,"15/May/08 16:09;breed;I also vote to remove the Regex. To be honest, I only used it in our initial implementation because it was easy not because I wanted regular expressions. Apart from causing the problems listed here it also has a performance impact.","16/May/08 13:11;pi_song;Just in case nobody has got the solution yet.
- Regex thing removed
- Tested","16/May/08 16:08;olgan;Hi Pi,

Thanks for submitting the patch. Couple of comments:

(1) To keep QueryParser.jjt code consistent I think it would make sense to use unquote rather than StringUtils.unescapeInputString since that's what the rest of the code does
(2) I think we should keep track of where we are at in the initial textLine rather than reallocating it every time we find the delimiter to keep the code more efficient.","16/May/08 16:22;pi_song;Olga,

1) In this case we have to use StringUtils.unescapeInputString() because LoadFunc/StoreFunc expects quoted strings as input. This is different from string literals where we want to get rid of quotes.

2) I was thinking about that too but I was not quite sure whether you have already got the solution yet so just did a quick one.

will change (2) quickly.","16/May/08 16:54;pi_song;The patch optimizes string splitting.

I've run all tests in local mode. No problem. 

Gotta go outside now. Enjoy your weekend!!!",16/May/08 21:32;olgan;Patch looks good. Running tests now,"16/May/08 22:29;olgan;I added the following unit test to TestStore:

public void testDelimiter() throws IOException{
        System.out.println(""Temp files: "" + tmpFile1 + "", "" + tmpFile2);
        pigServer.registerQuery(""A = load "" + fileName + "";"");
        pigServer.store(""A"", tmpFile1, ""PigStorage('\u0001')"");
        pigServer.registerQuery(""B = load "" + tmpFile1 + ""using PigStorage('\u0001');"");
        pigServer.registerQuery(""C = foreach B generate $0, $1;"");
        pigServer.store(""C"", tmpFile2);
        pigServer.registerQuery(""E = load "" + tmpFile2 + "";"");
        Iterator<Tuple> iter  = pigServer.openIterator(""E"");
        int i =0;
        while (iter.hasNext()){
            Tuple t = iter.next();
            assertEquals(t.getAtomField(0).numval().intValue(),i);
            assertEquals(t.getAtomField(1).numval().intValue(),i);
            i++;
        }
    }

With the latest patch, it works in local mode but fails in M-R mode. Will attach test outut in a second.","17/May/08 00:48;pi_song;Seems like hadoop jobconf doesn't like special char. I may have to work around this.



08/05/16 22:19:29 FATAL conf.Configuration: error parsing conf file: org.xml.sax.SAXParseException: Character reference ""&#1"" is an invalid XML character.

08/05/16 22:19:29 INFO ipc.Server: IPC Server handler 5 on 39413, call submitJob(job_200805162217_0010) from 127.0.0.1:39792: error: java.io.IOException: java.lang.RuntimeException: org.xml.sax.SAXParseException: Character reference ""&#1"" is an invalid XML character.

java.io.IOException: java.lang.RuntimeException: org.xml.sax.SAXParseException: Character reference ""&#1"" is an invalid XML character.

","18/May/08 14:57;pi_song;This patch includes all above plus fixes the backend job submission issue.
The test provided by Olga incorporated.
All tests passed in Local & MapReduce.

@Olga
This line:-
{noformat}
 pigServer.registerQuery(""B = load "" + tmpFile1 + ""using PigStorage('\\u0001') ;"");
{noformat}
needs double \ because it has to get through query parser as ""\u0001"" whereas 
{noformat}
pigServer.store(""A"", tmpFile1, ""PigStorage('\u0001')"");
{noformat}
doesn't need because this one doesn't go through query parser.","19/May/08 17:49;olgan;Pi,

Thanks for reworking the patch. 

Looking at the unit test, the store does not require escaping special character while load does. This will be confusing for the users. Why is this necessary?","20/May/08 00:10;pi_song;The current semantic is ""do escape when you do Pig query"" and ""don't do escape when you work directly with API""

We may have to address this but I would suggest we open a new Jira for discussion.
","20/May/08 00:42;olgan;In the unit test that you provided, the delimiter is escaped in load but not in store. Is that intentional?

Also, what would be the work involved in not requiring escaping at all?","20/May/08 00:49;pi_song;Yes, that's intentional. We have introduced escaping in Query Parser to allow user to do escape like in Java. Basically users can use escaping in string literals. This bug is that our escaping doesn't work after USING.  We have fixed it to be consistent in the patch.","21/May/08 19:16;olgan;Pi, sounds good. I will be testing your patch today and will commit it after hadoop 0.17 patch sometimes tomorrow or Friday.","21/May/08 22:48;olgan;I was able to successfully run unit as well as end-to-end tests. Will commit as soon is Hadoop 17 patch is on. Thanks, Pi!",23/May/08 22:19;olgan;Patch committed. Thanks Pi for fixing this,"27/May/08 22:16;olgan;Pi, I think the patch still has an issue. With this changes, there is potential of using much more memory then needed. This is caused by the changes to the parsing code in the tuple. Looks like when weallow array list to grow dinamically instead of specifying fixed size, it causes large memory overhead. (Reading Java documentation, I did not see what is the reallocation algorithm is but if it is like STL - doubling every time - this can get expensive.)

After I applied this patch, I have a group all query that used to run but now is failing.

I made quick fix - just for testing - of reusing the size of the previous tuple since most of the time tuples have the same number of fields, and that solved the issue for this particular case. 

That might be a resonable approach but I am open for other suggestions as well.","28/May/08 00:32;pi_song;I think that's a good solution. 

I will do performance tests on other different ways e.g. use trimToSize()  or count the exact number before instantiate the ArrayList to be able to tolerate variable tuple size.","28/May/08 07:05;pi_song;Test result: Splitting strings of 100 tokens 1 million times

||Approach||time in ms||
|String.Split() |87407|
|StringTokenizer|25969|
|Count  #tokens before create ArrayList|22094|
|Manual Split with trimToSize() |15031|
|Manual Split without trimToSize() |14860|

I think we have already gained a lot from switching to manual splitting. It shouldn't be too bad to also call ArrayList.trimToSize() before returning the result. Memory is a hard constrain so very important to save.","28/May/08 17:48;olgan;Pi, 

Thanks for the numbers. This is very useful!

I agree that we are making huge improvement which is great to see but I also think that allowing extra reallocation and copy of the references is not desirable and seem like it can be avoided with remembering the last size.","29/May/08 18:18;olgan;This patch reuses the size from the previous tuple.

The next thing I want to try is to walking the data counting number of delimiters upfront and then using String[] rather than ArrayList.",29/May/08 21:51;olgan;Ran tests: the current code is about 30% faster than double-walking line and using String[]. So I belive that the current patch should be a reasonable solution.,29/May/08 23:06;pi_song;+1,03/Jun/08 21:47;olgan;Patch has been committed. issue resolved. Thanks Pi for contributing.,,,,,,,,,,,,,,,,,,,,,,,,,
Stacktrace information is lost at MapReduceLauncher.java:289,PIG-80,12387469,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,francisoud,francisoud,francisoud,30/Jan/08 14:11,24/Mar/10 22:01,14/Mar/19 03:05,14/Feb/08 17:55,0.1.0,,,,,,0.1.0,,impl,,,0,,,,"{code:java}
...
}catch (Exception e) {
    // Do we need different handling for different exceptions
    e.printStackTrace();
    throw new IOException(e.getMessage());
}finally{ ...
{code}

in my case the sandard output is redirtected to /dev/null so ""e.printStackTrace();"" is lost.

it should be :
{code:java}throw new IOException(e);{code} 
no getMessage() because we loose the rest of the stacktrace",,,,,,,,,,,,,,,PIG-84,,,,13/Feb/08 11:33;francisoud;PIG-80-generics.patch;https://issues.apache.org/jira/secure/attachment/12375474/PIG-80-generics.patch,30/Jan/08 14:13;francisoud;PIG-80-v01.patch;https://issues.apache.org/jira/secure/attachment/12374372/PIG-80-v01.patch,31/Jan/08 10:21;francisoud;PIG-80-v02.patch;https://issues.apache.org/jira/secure/attachment/12374451/PIG-80-v02.patch,31/Jan/08 10:25;francisoud;PIG-80-v03.patch;https://issues.apache.org/jira/secure/attachment/12374452/PIG-80-v03.patch,12/Feb/08 10:05;francisoud;PIG-80-v04.patch;https://issues.apache.org/jira/secure/attachment/12375338/PIG-80-v04.patch,12/Feb/08 17:18;breed;PIG-80-v05.patch;https://issues.apache.org/jira/secure/attachment/12375395/PIG-80-v05.patch,15/Feb/08 17:03;francisoud;PIG-80-v06-unit-test-only.patch;https://issues.apache.org/jira/secure/attachment/12375684/PIG-80-v06-unit-test-only.patch,,,,,,,7.0,,,,,,,,,,,,,,,,,,,2008-01-30 17:03:42.153,,,no_permission,,,,,,,,,,,,38579,,,,,Fri Feb 15 17:03:03 UTC 2008,,,Patch Available,,,,0|i0gfvr:,94003,,,,,,,,,,"30/Jan/08 14:13;francisoud;this patch remove the ""getMessage()""","30/Jan/08 17:03;breed;Alright! Someone else is using 1.6! Unfortunately, I got nixed on requiring 1.6, so this needs to use initCause(Throwable) rather than using the constructor.

If I may, I'd like to expand this bug to cover all instances of new XXXException(e.getMessage()) or e.fillStackTrace(), there have been cases where having the root cause would have GREATLY!!! reduced the debug time. At the same time we should also remove all e.printStackTrace() calls.","31/Jan/08 10:21;francisoud;Yes your right my patch is not compliant with java 5.

Here's a 'PIG-80-v02.patch' using (I hope) only java 5 api.
It uses the initCause to keep track of the previous exception and add a FIXME to keep track of that issue.","31/Jan/08 10:25;francisoud;'PIG-80-v03.patch' An other cleaner solution and still using java 5 api is to throw new RuntimeException(e).

Choose your favorite patch ;)","31/Jan/08 19:35;breed;We don't want to use RuntimeException because we want to keep it a checked exception, so I think v02 is the correct way. Would you be willing to fix all instances of this pattern?","12/Feb/08 10:05;francisoud;This patch does the same PIG-80-v02.patch using initCause() method but on all cases with the same pattern: an exception transformed into an IOException where the stacktrace is lost.

There are still e.printStracktrace() in the code but that's a different issue see PIG-84.

To avoid duplicating this ugly code everywhere and keep it DRY, I create a class: org/apache/pig/impl/util/WrappedIOException.java
The client code look like this:
{code:java}throw WrappedIOException.wrap(e);{code}

I also created a unit test: WrappedIOExceptionTest.java
You will need to create folder: test/org/apache/pig/impl/util to apply this patch","12/Feb/08 17:18;breed;Excellent work Benjamin! I enhanced the patch a bit:

1) Made WrappedIOException take throwable
2) Found more places where it could be used
3) Made DataStorageException extend IOException so we can skip wrapping in a few cases

Alan or Olga can you review so that we can get this committed?","13/Feb/08 01:20;alangates;In general, the patch looks fine.  But I was wondering if we could make it more general.  Instead of having a class specifically for wrapping IOExceptions, could we have something like this:

public class ExceptionWrapper {

    public static <T extends Throwable> wrap(T toThrow Throwable thrown) {
           toThrow.initCause(t);
           return toThrow;
     }
}

Callers would then do something like:  

throw ExceptionWrapper.wrap(new MyException(""whoa, that didn't work""), e);

This way all types of exception could be wrapped, instead of just IOExceptions.","13/Feb/08 11:33;francisoud;PIG-80-generics.patch contains only 2 classes ExceptionWrapper and ExceptionWrapperTest as a proof of concept.

I tried to use generics as explain by alan.
I improved it by using Class<Throwable> instead of a real instance of the class (T toThrow)
But I couldn't use the static methods anymore because using a generic in class signature prevent it.

The resulting code would look like this:
{code:java}
throw (new ExceptionWrapper<IOException>()).wrap(""Deserialization error: "" + e.getMessage(), e, IOException.class);
{code}

or a more verbose version:

{code:java}
ExceptionWrapper<IOException> wrapper = new ExceptionWrapper<IOException>();
IOException ioe = wrapper.wrap(""Deserialization error: "" + e.getMessage(), e, IOException.class);
throw ioe;
{code}

I'm not sure the gain we get from using generics balance the lost of readability...

May be there is a better way to use generics but I couldn't find one :(","13/Feb/08 14:36;francisoud;> Benjamin Reed - 12/Feb/08 09:18 AM
> 2) Found more places where it could be used

Yes I only replaced the case where the stracktrace was lost but that even better to replace this pattern everywhere, I like DRY code ;)
Good catch.
","14/Feb/08 17:54;alangates;Last patch (PIG-80-v05.patch) without changes to use generics checked in.  Ben Reed convinced me that there were very few cases where we were throwing anything other than an IOException, so making the wrapping mechanism generic wasn't worth the extra effort.  Thank Benjamin for the patch.","15/Feb/08 17:03;francisoud;I think the unit test got lost between v04 and v05 because you need to create ""test/org/apache/pig/impl/util"" prior to apply the patch...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
src/org/apache/pig/builtin/PigStorage.java doesn't compile,PIG-78,12387212,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,acmurthy,acmurthy,acmurthy,26/Jan/08 03:05,24/Mar/10 22:01,14/Mar/19 03:05,31/Jan/08 00:53,,,,,,,0.1.0,,,,,0,,,,"{noformat}
compile:
     [echo] *** Building Main Sources ***
    [javac] Compiling 6 source files to /Users/arunc/dev/java/pig/trunk/dist
    [javac] /Users/arunc/dev/java/pig/trunk/src/org/apache/pig/builtin/PigStorage.java:85: cannot find symbol
    [javac] symbol  : method getBytes(java.nio.charset.Charset)
    [javac] location: class java.lang.String
    [javac]         os.write((f.toDelimitedString(this.fieldDel) + (char)this.recordDel).getBytes(utf8));
    [javac]                  ^
{noformat}
",,,,,,,,,,,,,,,,,,,26/Jan/08 03:08;acmurthy;PIG-78_0_20080125.patch;https://issues.apache.org/jira/secure/attachment/12374114/PIG-78_0_20080125.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-01-26 03:23:40.085,,,no_permission,,,,,,,,,,,,163744,,,,,Thu Jan 31 00:53:05 UTC 2008,,,,,,,0|i0gfvb:,94001,,,,,,,,,,"26/Jan/08 03:06;acmurthy;This is in trunk:

{noformat}
$ svn up
At revision 615427.
{noformat}
","26/Jan/08 03:08;acmurthy;Straight-forward fix to a typo (... I believe):

{noformat}
-        os.write((f.toDelimitedString(this.fieldDel) + (char)this.recordDel).getBytes(utf8));
+        os.write((f.toDelimitedString(this.fieldDel) + (char)this.recordDel).getBytes(""utf8""));
{noformat}","26/Jan/08 03:23;spullara;I've been automatically building if anyone wants to track it as well:

http://bamboo.javarants.com/browse/PIG-DEF
","28/Jan/08 13:39;francisoud;+1 for the fix.

The compilation was ok for me ?! but I my job were failing with:
{noformat}
Error message from task (reduce) tip_xxxx java.lang.RuntimeException: getBytes
    at org.apache.pig.builtin.PigStorage.putNext(PigStorage:85)
    at org.apache.pig.impl.mapreduceExec.PigOutputFormat$PigRecordWriter.write(PigOutputFormat:103)
    at org.apache.hadoop.mapred.ReduceTask$2.collect(ReduceTask:309)
    at org.apache.pig.impl.mapreduceExec.PigMapReduce$ReduceDataOutputCollector.add(PigMapReduce:338)
{noformat}

after the fix no such problem anymore.
","28/Jan/08 15:50;breed;String.getBytes(Charset) is a 1.6 API. If we use String.getBytes(String), there is going to be some extra overhead on each Tuple read.

Do we require 1.6? And if we don't, do we know when we will?","28/Jan/08 16:47;acmurthy;I can't speak for Pig, but we decided to punt on 1.6 for Hadoop since it wasn't available for Mac yet. 

This was a month or two ago... YMMV.","28/Jan/08 17:18;groves;It's not builtin, but I've been using [SoyLatte|http://landonf.bikemonkey.org/static/soylatte/] for command line Java 6 stuff on the Mac for a couple months.  It's worked perfectly for all of the stuff I've done.  I don't know if the getBytes(Charset) method is enough to require Java 6, but it isn't impossible to run it on the Mac.",28/Jan/08 18:06;olgan;The main reason we went back to Java 1.5 was the fact that 1.6 was not available for mac. if this issie is resolved I think we should move to Java 6 especially that it promises better performance for code on critical path.,"28/Jan/08 18:12;tdunning@veoh.com;
I checked last week and 1.6 was not generally available.

If you compile with 1.6 with a 1.5 target and then run on 1.6 you should get all the performance you would have had if you compiled with a 1.6 target.  The only difference is that I will be able to compile the code on my mac.

The 1.6 features that were at issue were trivial and had 1.5 equivalents.  There is no penalty for staying 1.5 compatible and some localized benefit. ","28/Jan/08 18:25;acmurthy;My apologies for starting a (religious?) debate, but I have to agree with Ted.

I'm happy to switch over my Mac whenever I can, but I'm not sure I want to try SoyLatte and such others since this is an 'official' matter... I'd appreciate if I could keep it so and develop on it.","29/Jan/08 01:57;olgan;I ran performance tests on the modified code and the number look very similar to the current version. I am +1 on the change proposed by Arun. If I don't hear any objections by tomorrow, I am planning to commit the patch.",31/Jan/08 00:53;olgan;patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SPLIT clause needs end-to-end/regression tests,PIG-75,12387050,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,antmagna,antmagna,24/Jan/08 15:28,25/Mar/10 00:12,14/Mar/19 03:05,26/Jan/09 19:55,,,,,,,0.2.0,,,,,0,,,,"I believe the end-to-end tests currently do not perform any validation on the correctness of the SPLIT clause.
As part of the abstraction layer I have added unit tests for that, but would be nice to have something in the regression test suite as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-01-26 19:55:42.527,,,no_permission,,,,,,,,,,,,163741,,,,,Mon Jan 26 19:55:42 UTC 2009,,,,,,,0|i0gfun:,93998,,,,,,,,,,26/Jan/09 19:55;olgan;tests have been added in latest code,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in setJobtrackerLocation() in PigContext.java:68,PIG-69,12386942,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,francisoud,francisoud,francisoud,23/Jan/08 13:51,24/Mar/10 22:01,14/Mar/19 03:05,29/Jan/08 01:27,0.1.0,,,,,,0.1.0,,impl,,,0,,,,"{noformat}
java.lang.NullPointerException
    at org.apache.pig.impl.PigContext.setJobtrackerLocation(PigContext.java:425)
    ... (the rest of the stacktrace is my own servlet code)
{noformat}

The code:
{code:java}
final PigContext pigContext = new PigContext(ExecType.MAPREDUCE);
pigContext.setJobtrackerLocation(configuration.get(""mapred.job.tracker""));
pigContext.setFilesystemLocation(configuration.get(""fs.default.name""));
        
final PigServer pigServer = new PigServer(pigContext);
{code}
Where configuration is a org.apache.hadoop.conf.Configuration object initialized with spring framework.",,,,,,,,,,,,,,,PIG-92,,,,23/Jan/08 15:07;francisoud;PigContext-PIG-69-v01.patch;https://issues.apache.org/jira/secure/attachment/12373835/PigContext-PIG-69-v01.patch,28/Jan/08 13:21;francisoud;PigContext-PIG-69-v02.patch;https://issues.apache.org/jira/secure/attachment/12374179/PigContext-PIG-69-v02.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-01-25 17:15:38.361,,,no_permission,,,,,,,,,,,,163735,,,,,Tue Jan 29 01:27:12 UTC 2008,,,Patch Available,,,,0|i0gftb:,93992,,,,,,,,,,"23/Jan/08 15:07;francisoud;This patch fix the null pointer and allows at the same time to set more configuration parameters using IoC approach.

The problem with setting conf param with methods like ""setJobtrackerLocation()"" and ""setFilesystemLocation()"" is that you will end up adding lots of those methods filling the class with boiler plate code for each parameter :(

The client code will look like:
{code:java}
PigContext pigContext = new PigContext(ExecType.MAPREDUCE);
JobConf conf = new JobConf(configuration);
pigContext.setConf(conf);

PigServer pigServer = new PigServer(pigContext);
{code}

This patch makes setJobtrackerLocation() and setFilesystemLocation() deprecated.","25/Jan/08 17:15;alangates;One question.  Why did you remove the line:

conf = new JobConf()

at PigContext line 179?  As conf is used in the code subsequent to this it isn't clear to me that it's ok.  Did you test your change in local mode (which is the only codepath that would exercise this change)?","28/Jan/08 13:21;francisoud;Good catch :)

I have uploaded the wrong patch :(

This one contains the missing line initializing the instance var ""conf"".",29/Jan/08 01:27;alangates;Fix provided by francisoud checked in (thanks).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileLocalizer doesn't work on reduce sise,PIG-67,12386556,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,utkarsh,utkarsh,18/Jan/08 00:19,25/Mar/10 00:12,14/Mar/19 03:05,03/Aug/09 20:52,,,,,,,0.4.0,,,,,1,,,,"FileLocalizer.openDFSFile() does not work on the reduce side. This is probably because FileLocalizer uses PigRecordReader which exists only on the map task.

The correct solution will be for FileLocalizer to have a hadoop conf that is initialized by the reduce task on the reduce side, and the pig record reader on the map side.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-01-18 00:24:30.462,,,no_permission,,,,,,,,,,,,163733,,,,,Mon Aug 03 20:52:48 UTC 2009,,,,,,,0|i0gfsv:,93990,,,,,,,,,,18/Jan/08 00:24;breed;It would probably be best if the FileLocalizer uses the Configuration that will be exposed using PIG-66 rather than using PigRecordReader.,22/May/09 21:01;yinghe;get JobConf from PigMapReduce class  so that  reducers can operate on files as well.,03/Aug/09 20:52;olgan;This issues has been resolved by PIG https://issues.apache.org/jira/browse/PIG-792,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
convert tabs to spaces,PIG-65,12386553,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,groves,groves,groves,17/Jan/08 23:23,24/Mar/10 22:01,14/Mar/19 03:05,06/Feb/08 23:09,,,,,,,0.1.0,,,,,0,,,,"Many of the pig source files mix tabs and 4 spaces for indentation.  This is particularly painful for me when reading the code as I've set up my editor to indent tabs 8 spaces so I can catch if I actually use them anywhere, and the source jumps back and forth in indentation level, sometimes from line to line.

The patch replaces all tabs with 4 spaces in java code since that's what's mentioned as the standard in the wiki.",,,,,,,,,,,,,,,,,,,17/Jan/08 23:24;groves;tabs_to_spaces.diff;https://issues.apache.org/jira/secure/attachment/12373454/tabs_to_spaces.diff,06/Feb/08 21:30;groves;tabs_to_spaces_post_PIG-32.diff;https://issues.apache.org/jira/secure/attachment/12374912/tabs_to_spaces_post_PIG-32.diff,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-01-18 00:04:30.984,,,no_permission,,,,,,,,,,,,163731,,,,,Wed Feb 06 23:09:58 UTC 2008,,,Patch Available,,,,0|i0gfrz:,93986,,,,,,,,,,18/Jan/08 00:04;breed;+1 Thank you!!!,"06/Feb/08 21:11;olgan;Charlie, we have committed changes for PIG-32 which were blocking your patch. Could you generate a new patch, thanks!",06/Feb/08 21:30;groves;Here's a version of the patch for trunk as of r619153.,06/Feb/08 22:20;olgan;The current revision that I see is  619182. ,06/Feb/08 22:39;groves;That revision is across the entire Apache repository.  It's 619193 now.  Pig still hasn't changed since 619153.,06/Feb/08 23:09;olgan;patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigStorage does not properly handle UTF8 data,PIG-63,12386235,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,breed,olgan,olgan,15/Jan/08 00:36,03/Jul/08 18:12,14/Mar/19 03:05,24/Jan/08 22:03,,,,,,,0.1.0,,,,,0,,,,"From Ben:

I just checked the code and the problem seems to be PigStorage. getNext() uses
readLine() which does not handle UTF8 correctly. putNext() also uses default encoder rather than UTF8 explicitly.

Internally and in BinStorage UTF8 appears to be handled correctly.
",,,,,,,,,,,,,,,,,,,23/Jan/08 00:20;breed;utf8.patch;https://issues.apache.org/jira/secure/attachment/12373795/utf8.patch,18/Jan/08 19:43;breed;utf8.patch;https://issues.apache.org/jira/secure/attachment/12373555/utf8.patch,15/Jan/08 16:12;breed;utf8.patch;https://issues.apache.org/jira/secure/attachment/12373175/utf8.patch,24/Jan/08 02:08;olgan;utf8_v4.patch;https://issues.apache.org/jira/secure/attachment/12373900/utf8_v4.patch,24/Jan/08 21:51;olgan;utf8_v5.patch;https://issues.apache.org/jira/secure/attachment/12373978/utf8_v5.patch,24/Jan/08 23:58;breed;utf8test.patch;https://issues.apache.org/jira/secure/attachment/12373991/utf8test.patch,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2008-01-15 16:12:16.637,,,no_permission,,,,,,,,,,,,163729,,,,,Mon Jan 28 09:03:01 UTC 2008,,,,,,,0|i0gfr3:,93982,,,,,,,,,,15/Jan/08 16:12;breed;Here is a patch to fix the problem in PigStorage as well as TextLoader. Test cases still need to be written.,16/Jan/08 17:31;utkarsh;Buffering will create problems for us since in.getPos() will be unreliable.,16/Jan/08 19:55;alangates;Unit and end to end tests pass.  Hand testing of UTF8/non-ascii data works.  I'm working on adding a test for this case to the end to end tests.,"18/Jan/08 19:43;breed;The previous patch did not handle buffering properly. Since this is a rather common need, I moved the readLine() to BufferedPositionInputStream. I would like to read multiple bytes at a time in readLine(), but since that can adversely affect the positioning of the compressed stream I read a byte at a time so that I can stop at the delimiter.","18/Jan/08 21:06;alangates;When I apply this patch and run the tests, they fail with the following error message:

08/01/18 11:50:16 ERROR apache.pig: Error message from task (map) tip_200801071337_2639_m_000000 java.lang.NullPointerException
	at org.apache.pig.impl.io.BufferedPositionedInputStream.readLine(BufferedPositionedInputStream.java:146)
	at org.apache.pig.builtin.PigStorage.getNext(PigStorage.java:62)
	at org.apache.pig.impl.mapreduceExec.PigInputFormat$PigRecordReader.next(PigInputFormat.java:188)
	at org.apache.pig.impl.mapreduceExec.PigInputFormat$PigRecordReader.next(PigInputFormat.java:131)
	at org.apache.hadoop.mapred.MapTask$1.next(MapTask.java:174)
	at org.apache.pig.impl.mapreduceExec.PigMapReduce.run(PigMapReduce.java:114)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:192)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1760)
",23/Jan/08 00:20;breed;This makes the test cases run and adds a builtin test case that isolated the problem that Alan found.,"24/Jan/08 02:08;olgan;I have made to more changes to the code to make end-to-end tests pass:

(1) Made sure that we write utf8 in the store function
(2) Make sure we properly maintained position in readLine.

We still need code review and performance numbers before this can be committed","24/Jan/08 20:00;olgan;Ran a simple performance test:

A = load '/user/pig/tests/data/singlefile/studenttab20m' using PigStorage('\t');
store A into utf8;

The input file contains 20M records and is 419 MB. The test ran on 10 machine cluster.

Run tests 3 times with old and new code and avergaged the results.

The results:

old code = 47.5 s
new code = 50 s

the query slowed down about 5%.

I think it is an acceptable slowdown of the correctness we get from it. Also, the impact for more computationally intensive queries would be significantly less.
","24/Jan/08 20:45;olgan;Code review:

Overall looks good. One question that I have is about readLine function in BufferedPositionedInputStream.java.

I assume that you have a switch statement for performance reason there. Is that correct? Also, seems like you would need a break for the default case. ","24/Jan/08 21:23;breed;The switch statement is there for performance. You are correct. There is a break missing in the default case. (The default case should never really happen, but I put it there just in case some implementation of the decoder does something weird.)",24/Jan/08 21:50;olgan;Fixed problem uncovered in the code review,24/Jan/08 21:51;olgan;fixed problem uncovered during code review,24/Jan/08 22:03;olgan;Patch committed,24/Jan/08 23:58;breed;The utf8 test case in LFTest that was checked in has a bug.,"25/Jan/08 00:08;olgan;+1 

The patch makes the tests run. please, commit","28/Jan/08 09:03;francisoud;The ""Affects Version"" and ""Fix Version"" are 0.1.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
User comparator does not work if the comapre function is not in pig.jar,PIG-61,12386092,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,olgan,olgan,11/Jan/08 23:22,24/Mar/10 22:01,14/Mar/19 03:05,15/Jan/08 00:52,,,,,,,0.1.0,,,,,0,,,,"From what I can tell, comparison function lookup does not interact with register command. Instead, it expects to find the function on the class path. The problem with that is that we don't ship jar that are not registered and the registered jars don't get placed in classpath on the server path. The solution should be to use the same way to instanciate the object as used for other udfs.

Comparator: Class.forName
Other UDFs: PigContext.resolveClassName",,,,,,,,,,,,,,,,,,,14/Jan/08 23:45;alangates;pig61.patch;https://issues.apache.org/jira/secure/attachment/12373132/pig61.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-01-12 03:57:33.287,,,no_permission,,,,,,,,,,,,163727,,,,,Tue Jan 15 00:52:21 UTC 2008,,,,,,,0|i0gfq7:,93978,,,,,,,,,,"12/Jan/08 03:57;spullara;Yeah, I ran into this when I was testing my patch -- you need to change Class.forName to PigContext.instantiateFuncFromSpec.",14/Jan/08 23:45;alangates;Patch to address this issue.  I tested this with a comparison function in a jar other than pig.jar and it worked fine.,15/Jan/08 00:01;olgan;+1,15/Jan/08 00:52;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Occasional NullPointerException in PigContext.fixUpDomain method,PIG-57,12385820,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,francisoud,xuzh,xuzh,09/Jan/08 04:52,24/Mar/10 22:01,14/Mar/19 03:05,28/Jan/08 23:52,,,,,,,0.1.0,,impl,,,0,,,,"I occasionally see the following NPE when running a Pig job with HOD:

2008-01-08 06:14:24,558 [main] INFO  org.apache.pig - Connecting to HOD...
2008-01-08 06:14:29,732 [main] INFO  org.apache.pig - HDFS Web UI: nn-host:50070
2008-01-08 06:14:29,732 [main] INFO  org.apache.pig - JobTracker Web UI: jt-host:54597
2008-01-08 06:14:29,846 [main] FATAL org.apache.pig - Could not connect to HOD
java.lang.NullPointerException
	at org.apache.pig.impl.PigContext.fixUpDomain(PigContext.java:350)
	at org.apache.pig.impl.PigContext.doHod(PigContext.java:324)
	at org.apache.pig.impl.PigContext.connect(PigContext.java:175)
	at org.apache.pig.PigServer.<init>(PigServer.java:128)
	at org.apache.pig.tools.grunt.Grunt.<init>(Grunt.java:37)
	at org.apache.pig.Main.main(Main.java:212)
",,,,,,,,,,,,,,,,,,,24/Jan/08 09:00;francisoud;PIG-57-v01.patch;https://issues.apache.org/jira/secure/attachment/12373911/PIG-57-v01.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-01-24 09:00:23.593,,,no_permission,,,,,,,,,,,,163724,,,,,Mon Jan 28 23:52:13 UTC 2008,,,,,,,0|i0gfon:,93971,,,,,,,,,,"24/Jan/08 09:00;francisoud;This patch:
* fix the NPE by returning null if parameter is null (does throwing a IllegalArgumentException seems better to you ?)
* throws an IllegalArgumentException if parameter doesn't match host:port pattern providing a better error message
* add javadoc to the method
* add a FIXME about not hardcoding "".inktomisearch.com"" in the code (not sure anyone else than yahoo could be interested in this 'feature' ;) )",28/Jan/08 23:52;alangates;Issue fixed with patch provided by francisoud (thank you).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
built in aggregate functions fail on invalid input,PIG-54,12385000,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pkamath,olgan,olgan,19/Dec/07 23:50,02/May/13 02:29,14/Mar/19 03:05,30/Sep/08 23:37,0.2.0,,,,,,0.2.0,,,,,0,,,,"This issue is true for all built-in aggregate functions. If they get type different from what they expect, they through an exception. Instead, they should treat the value as null. We still need to define what that means in Pig but if we stick with SQL definition, all but count(*) computations would ignore them.",,,,,,,,,,,,,,,,,PIG-157,,30/Sep/08 20:37;pkamath;PIG-54.patch;https://issues.apache.org/jira/secure/attachment/12391234/PIG-54.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-30 20:36:29.316,,,no_permission,,,,,,,,,,,,163721,,,,,Tue Sep 30 23:37:00 UTC 2008,,,,,,,0|i0gfnb:,93965,,,,,,,,,,30/Sep/08 20:36;pkamath;Attached patch,"30/Sep/08 20:38;pkamath;SUM handles this, we count NULLs in COUNT - AVG works using these semantics.

The patch fixed MIN and MAX","30/Sep/08 23:37;olgan;patch committed; thaks, pradeep",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Local Plan Compiler does not handle SPLIT clauses,PIG-52,12384780,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,antmagna,antmagna,antmagna,17/Dec/07 16:11,25/Mar/10 00:12,14/Mar/19 03:05,20/Jan/09 22:45,,,,,,,0.2.0,,impl,,,0,,,,"I believe the current implementation of the Local Plan Compiler does not properly support SPLIT clauses. The code seems not to properly handle instances of LOSplit logical operators.

I am opening this issue to track the problem and I am assigning it to me because, as part of the work I am doing on the abstraction layer (see PIG-32), I am currently working on the logical plan compiler.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-01-20 22:45:19.018,,,no_permission,,,,,,,,,,,,163719,,,,,Tue Jan 20 22:45:19 UTC 2009,,,,,,,0|i0gfmn:,93962,,,,,,,,,,20/Jan/09 22:45;alangates;Split in local now works.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Combiner gives wrong result in the presence of flattening,PIG-51,12384550,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,utkarsh,utkarsh,utkarsh,13/Dec/07 05:17,24/Mar/10 22:01,14/Mar/19 03:05,04/Feb/08 21:21,,,,,,,0.1.0,,,,,0,,,,"If you do something like

a = load ... as (f1,f2,f3);
b = group a by (f1,f2);
c = foreach b generate flatten(group), SUM(a.f3);

The reduce side refers to field number expecting data will not have been flattened yet. But if the combiner kicks in, it already flattens the group, leading to column references being wrong.
 ",,,,,,,,,,,,,,,,,,,13/Dec/07 05:45;utkarsh;combiner-flatten.patch;https://issues.apache.org/jira/secure/attachment/12371580/combiner-flatten.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-12-13 17:56:58.648,,,no_permission,,,,,,,,,,,,163718,,,,,Mon Feb 04 21:21:58 UTC 2008,,,Patch Available,,,,0|i0gfm7:,93960,,,,,,,,,,"13/Dec/07 17:56;alangates;+1, patch looks good.","14/Dec/07 20:34;tdunning@veoh.com;
I think I am still seeing this issue or a cousin even after applying this patch.  I don't know enough to be sure, however.

grunt> ls /logs/search/2007/12/10
/logs/search/2007/12/10/part-00000<r 3>	1313515859
/logs/search/2007/12/10/part-00001<r 3>	1313535390
/logs/search/2007/12/10/part-00002<r 3>	1313485045
/logs/search/2007/12/10/part-00003<r 3>	1313536061
grunt>  a = load '/logs/search/2007/12/10' as (eventType, date, month,
week, day, hour, id, videoId, VisitorUID, engineName, query, offset);
 b = filter a by (id neq '-');

grunt>  b = filter a by (id neq '-');
grunt>  c = group b by id;
grunt>  describe c
c: (group, b: (eventType, date, month, week, day, hour, id, videoId, VisitorUID, engineName, query, offset ) )
grunt>  d = foreach c {
 click = filter b by eventType eq '/search/click';
 generate COUNT(click);
 }
>> >> >> grunt>  describe d
d: (count1 )
grunt>  e = group d by 1;
grunt>  describe e
e: (group: ( ), d: (count1 ) )
grunt>  f = foreach e generate COUNT(*), SUM(d.count1);
grunt> dump f

----- MapReduce Job -----
Input: [/logs/search/2007/12/10:org.apache.pig.builtin.PigStorage()]
Map: [[*]->[FILTER BY ([PROJECT $6] neq ['-'])]]
Group: [GENERATE {[PROJECT $6],[*]}]
Combine: null
Reduce: GENERATE {[COUNT(GENERATE {[PROJECT $1]->[FILTER BY ([PROJECT $0] eq ['/search/click'])]})]}
Output: /tmp/temp1435257199/tmp1109313480:org.apache.pig.builtin.BinStorage
Split: null
Map parallelism: -1
Reduce parallelism: -1
Job jar size = 482135
2007-12-14 12:04:44,776 [main] INFO  org.apache.pig - Pig progress = 0%
2007-12-14 12:04:57,832 [main] INFO  org.apache.pig - Pig progress = 0%
2007-12-14 12:04:59,841 [main] INFO  org.apache.pig - Pig progress = 0%
2007-12-14 12:05:01,849 [main] INFO  org.apache.pig - Pig progress = 0%
2007-12-14 12:05:03,857 [main] INFO  org.apache.pig - Pig progress = 0%
2007-12-14 12:05:05,865 [main] INFO  org.apache.pig - Pig progress = 1%
2007-12-14 12:05:07,873 [main] INFO  org.apache.pig - Pig progress = 1%
2007-12-14 12:05:09,881 [main] INFO  org.apache.pig - Pig progress = 2%
2007-12-14 12:05:11,889 [main] INFO  org.apache.pig - Pig progress = 2%
2007-12-14 12:05:13,897 [main] INFO  org.apache.pig - Pig progress = 2%
2007-12-14 12:05:15,905 [main] INFO  org.apache.pig - Pig progress = 3%
2007-12-14 12:05:17,913 [main] INFO  org.apache.pig - Pig progress = 3%
2007-12-14 12:05:21,929 [main] INFO  org.apache.pig - Pig progress = 3%
2007-12-14 12:05:23,937 [main] INFO  org.apache.pig - Pig progress = 4%
2007-12-14 12:05:25,945 [main] INFO  org.apache.pig - Pig progress = 4%
2007-12-14 12:05:27,953 [main] INFO  org.apache.pig - Pig progress = 4%
2007-12-14 12:05:29,961 [main] INFO  org.apache.pig - Pig progress = 5%
2007-12-14 12:05:31,969 [main] INFO  org.apache.pig - Pig progress = 5%
2007-12-14 12:05:33,977 [main] INFO  org.apache.pig - Pig progress = 5%
2007-12-14 12:05:37,993 [main] INFO  org.apache.pig - Pig progress = 5%
2007-12-14 12:05:40,001 [main] INFO  org.apache.pig - Pig progress = 6%
2007-12-14 12:05:42,009 [main] INFO  org.apache.pig - Pig progress = 6%
2007-12-14 12:05:44,016 [main] INFO  org.apache.pig - Pig progress = 6%
2007-12-14 12:05:46,024 [main] INFO  org.apache.pig - Pig progress = 7%
2007-12-14 12:05:48,032 [main] INFO  org.apache.pig - Pig progress = 7%
2007-12-14 12:05:50,040 [main] INFO  org.apache.pig - Pig progress = 8%
2007-12-14 12:05:52,051 [main] INFO  org.apache.pig - Pig progress = 8%
2007-12-14 12:05:54,060 [main] INFO  org.apache.pig - Pig progress = 8%
2007-12-14 12:05:56,068 [main] INFO  org.apache.pig - Pig progress = 8%
2007-12-14 12:05:58,077 [main] INFO  org.apache.pig - Pig progress = 8%
2007-12-14 12:06:00,085 [main] INFO  org.apache.pig - Pig progress = 9%
2007-12-14 12:06:02,092 [main] INFO  org.apache.pig - Pig progress = 9%
2007-12-14 12:06:04,100 [main] INFO  org.apache.pig - Pig progress = 9%
2007-12-14 12:06:08,116 [main] INFO  org.apache.pig - Pig progress = 10%
2007-12-14 12:06:10,124 [main] INFO  org.apache.pig - Pig progress = 10%
2007-12-14 12:06:12,133 [main] INFO  org.apache.pig - Pig progress = 10%
2007-12-14 12:06:18,160 [main] INFO  org.apache.pig - Pig progress = 10%
2007-12-14 12:06:20,168 [main] INFO  org.apache.pig - Pig progress = 10%
2007-12-14 12:06:22,176 [main] INFO  org.apache.pig - Pig progress = 11%
2007-12-14 12:06:24,184 [main] INFO  org.apache.pig - Pig progress = 11%
2007-12-14 12:06:26,192 [main] INFO  org.apache.pig - Pig progress = 11%
2007-12-14 12:06:28,201 [main] INFO  org.apache.pig - Pig progress = 12%
2007-12-14 12:06:30,208 [main] INFO  org.apache.pig - Pig progress = 12%
2007-12-14 12:06:32,216 [main] INFO  org.apache.pig - Pig progress = 12%
2007-12-14 12:06:34,224 [main] INFO  org.apache.pig - Pig progress = 13%
2007-12-14 12:06:36,232 [main] INFO  org.apache.pig - Pig progress = 13%
2007-12-14 12:06:38,240 [main] INFO  org.apache.pig - Pig progress = 13%
2007-12-14 12:06:40,251 [main] INFO  org.apache.pig - Pig progress = 14%
2007-12-14 12:06:42,260 [main] INFO  org.apache.pig - Pig progress = 14%
2007-12-14 12:06:44,268 [main] INFO  org.apache.pig - Pig progress = 15%
2007-12-14 12:06:46,276 [main] INFO  org.apache.pig - Pig progress = 15%
2007-12-14 12:06:48,285 [main] INFO  org.apache.pig - Pig progress = 15%
2007-12-14 12:06:50,292 [main] INFO  org.apache.pig - Pig progress = 15%
2007-12-14 12:06:52,300 [main] INFO  org.apache.pig - Pig progress = 16%
2007-12-14 12:06:56,316 [main] INFO  org.apache.pig - Pig progress = 16%
2007-12-14 12:06:58,324 [main] INFO  org.apache.pig - Pig progress = 17%
2007-12-14 12:07:00,332 [main] INFO  org.apache.pig - Pig progress = 17%
2007-12-14 12:07:02,340 [main] INFO  org.apache.pig - Pig progress = 17%
2007-12-14 12:07:04,348 [main] INFO  org.apache.pig - Pig progress = 17%
...
2007-12-14 12:07:06,374 [main] ERROR org.apache.pig - Error message from task (map) tip_200712121227_0004_m_000071 java.lang.RuntimeException: java.io.IOException: Column number out of range: 6 -- (                 )
	at org.apache.pig.impl.eval.ProjectSpec.eval(ProjectSpec.java:95)
	at org.apache.pig.impl.eval.SimpleEvalSpec$1.add(SimpleEvalSpec.java:35)
	at org.apache.pig.impl.eval.EvalSpec.simpleEval(EvalSpec.java:216)
	at org.apache.pig.impl.eval.cond.CompCond.eval(CompCond.java:58)
	at org.apache.pig.impl.eval.FilterSpec$1.add(FilterSpec.java:58)
	at org.apache.pig.impl.mapreduceExec.PigMapReduce.run(PigMapReduce.java:113)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:192)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1760)
Caused by: java.io.IOException: Column number out of range: 6 -- (                 )
	at org.apache.pig.data.Tuple.getField(Tuple.java:147)
	at org.apache.pig.impl.eval.ProjectSpec.eval(ProjectSpec.java:85)
	... 7 more

2007-12-14 12:07:06,374 [main] ERROR org.apache.pig - Error message from task (map) tip_200712121227_0004_m_000072
2007-12-14 12:07:06,374 [main] ERROR org.apache.pig - Error message from task (map) tip_200712121227_0004_m_000073
2007-12-14 12:07:06,374 [main] ERROR org.apache.pig - Error message from task (map) tip_200712121227_0004_m_000074
2007-12-14 12:07:06,374 [main] ERROR org.apache.pig - Error message from task (map) tip_200712121227_0004_m_000075 java.lang.RuntimeException: java.io.IOException: Column number out of range: 6 -- (full, 50)
	at org.apache.pig.impl.eval.ProjectSpec.eval(ProjectSpec.java:95)
	at org.apache.pig.impl.eval.SimpleEvalSpec$1.add(SimpleEvalSpec.java:35)
	at org.apache.pig.impl.eval.EvalSpec.simpleEval(EvalSpec.java:216)
	at org.apache.pig.impl.eval.cond.CompCond.eval(CompCond.java:58)
	at org.apache.pig.impl.eval.FilterSpec$1.add(FilterSpec.java:58)
	at org.apache.pig.impl.mapreduceExec.PigMapReduce.run(PigMapReduce.java:113)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:192)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1760)
Caused by: java.io.IOException: Column number out of range: 6 -- (full, 50)
	at org.apache.pig.data.Tuple.getField(Tuple.java:147)
	at org.apache.pig.impl.eval.ProjectSpec.eval(ProjectSpec.java:85)
	... 7 more

2007-12-14 12:07:06,374 [main] ERROR org.apache.pig - Error message from task (map) tip_200712121227_0004_m_000076
2007-12-14 12:07:06,374 [main] ERROR org.apache.pig - Error message from task (map) tip_200712121227_0004_m_000079
2007-12-14 12:07:06,375 [main] ERROR org.apache.pig - Error message from task (reduce) tip_200712121227_0004_r_000000
2007-12-14 12:07:06,375 [main] ERROR org.apache.pig - Error message from task (reduce) tip_200712121227_0004_r_000001
2007-12-14 12:07:06,375 [main] ERROR org.apache.pig - Error message from task (reduce) tip_200712121227_0004_r_000002
2007-12-14 12:07:06,375 [main] ERROR org.apache.pig - Error message from task (reduce) tip_200712121227_0004_r_000003
Job failed
grunt> ","15/Dec/07 02:31;utkarsh;Seems there is an empty tuple in your data set (due to which id  
cannot be resolved). Thats what is throwing the exception. In fact  
the combiner doesn't trigger in your query.

Is there a reason why you do one bit of filter outside, and one  
inside the foreach. Couldn't both the filters be done before  
grouping. It would be more efficient that way, plus the combiner will  
probably kick in.

Utkarsh



",04/Feb/08 21:21;alangates;Fix for this was checked in some time ago (12/13/07).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig.pl assumes that the name of the hodrc file for a given cluster matches the name of the cluster.,PIG-45,12384117,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,alangates,alangates,alangates,06/Dec/07 22:00,24/Mar/10 22:01,14/Mar/19 03:05,07/Dec/07 00:53,0.0.0,,,,,,0.1.0,,,,,0,,,,"pig.pl currently assumes that the hodrc for a given hadoop cluster matches the name of the cluster, rather than allowing it to be hodrc (the default).",,,,,,,,,,,,,,,,,,,06/Dec/07 23:18;alangates;pigpl.patch;https://issues.apache.org/jira/secure/attachment/12371189/pigpl.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-12-06 23:59:13.259,,,no_permission,,,,,,,,,,,,163712,,,,,Fri Dec 07 00:53:58 UTC 2007,,,,,,,0|i0gfk7:,93951,,,,,,,,,,06/Dec/07 23:18;alangates;Changes pig.pl to always look for the hod config file as hodrc.,06/Dec/07 23:59;olgan;Looks good. +1,07/Dec/07 00:53;alangates;Fix checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problem with spilling BigBags,PIG-44,12383905,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,utkarsh,olgan,olgan,04/Dec/07 19:03,24/Mar/10 22:01,14/Mar/19 03:05,08/Dec/07 00:56,,,,,,,0.1.0,,,,,0,,,,"Currently, once we spill the bag, if no additional memory becomes available, we would be spilling 1 record at a time because of the problem with the logic. Short term, we will make a change to spill 100 records at a time. Longer term, we need to try and drain the memory before doing so.",,,,,,,,,,,,,,,,,,,06/Dec/07 20:13;utkarsh;spilling.patch;https://issues.apache.org/jira/secure/attachment/12371174/spilling.patch,07/Dec/07 06:07;utkarsh;spilling1.patch;https://issues.apache.org/jira/secure/attachment/12371203/spilling1.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2007-12-06 20:13:55.339,,,no_permission,,,,,,,,,,,,163711,,,,,Sat Dec 08 00:56:47 UTC 2007,,,,,,,0|i0gfjr:,93949,,,,,,,,,,"06/Dec/07 20:13;utkarsh;This is not a particularly attractive solution, but may be adequate for the short-term. Please review",06/Dec/07 20:36;olgan;isn't 50000 too high? I am concerned that we can blow memory if we set it that high.,07/Dec/07 00:00;olgan;Looks good. +1,"07/Dec/07 00:05;olgan;please, ignore my last post - wrong issue","07/Dec/07 06:07;utkarsh;I now *adaptively* make the choice of the number of records to hold in memory. I start with holding 1000 records in memory. Once a low-memory condition is hit, I aim that the bag should not become more than 1% of jvm heap size (TARGET_IN_MEMORY_SIZE). When the bag spills to disk, I measure how many bytes were actually written. If the bytes written were  < TARGET_IN_MEMORY_SIZE, I accordingly increase the number of records to hold in memory, otherwise accordingly decrease it. 

Most of the patch is some refactoring that I did in the big data bag unit test. I tested with up to 10 million records, and it seems to work great. My heap size was the default 64M.

",07/Dec/07 17:36;alangates;+1.,08/Dec/07 00:56;utkarsh;Fixed with revision 602287,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Combiner queries are not repeatable using the same alias,PIG-43,12383900,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,alangates,alangates,alangates,04/Dec/07 18:24,24/Mar/10 22:01,14/Mar/19 03:05,06/Dec/07 19:10,0.0.0,,,,,,0.1.0,,impl,,,0,,,,"In general, it is possible to define an alias in pig and use it multiple times:

a = load 'file';
b = group a by $0;
c = foreach b generate group, COUNT($1);
dump c;
dump c;

Queries that choose to use the combiner give the following error on the second dump:
java.lang.AssertionError: Can't convert non-algebraic function to final.
        at org.apache.pig.impl.eval.FuncEvalSpec.resetFuncToFinal(FuncEvalSpec.java:285)
        at org.apache.pig.impl.physicalLayer.MapreducePlanCompiler$ReduceAdjuster.visitFuncEval(MapreducePlanCompiler.java:393)
        at org.apache.pig.impl.eval.FuncEvalSpec.visit(FuncEvalSpec.java:238)
        at org.apache.pig.impl.physicalLayer.MapreducePlanCompiler$ReduceAdjuster.visitGenerate(MapreducePlanCompiler.java:367)
        at org.apache.pig.impl.eval.GenerateSpec.visit(GenerateSpec.java:374)
        at org.apache.pig.impl.physicalLayer.MapreducePlanCompiler.pushInto(MapreducePlanCompiler.java:223)
        at org.apache.pig.impl.physicalLayer.MapreducePlanCompiler.compile(MapreducePlanCompiler.java:79)
        at org.apache.pig.impl.physicalLayer.PhysicalPlan.<init>(PhysicalPlan.java:56)
        at org.apache.pig.impl.physicalLayer.IntermedResult.compile(IntermedResult.java:95)
        at org.apache.pig.PigServer.openIterator(PigServer.java:319)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:120)
        at org.apache.pig.tools.grunt.GruntParser.parse(GruntParser.java:334)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:38)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:48)
        at org.apache.pig.Main.main(Main.java:211)

This is because the logic that chooses the combiner modifies the EvalSpecs in the logical plan.  Pig does not preserve the physical plan built for the first run of the query, but tries to reconstruct it. When it does so the modified EvalSpecs cause it a problem.  The combiner logic needs to be changed to make a copy of the EvalSpecs rather than modify them.",,,,,,,,,,,,,,,,,,,05/Dec/07 01:01;alangates;pig43.patch;https://issues.apache.org/jira/secure/attachment/12370988/pig43.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-12-06 19:02:18.165,,,no_permission,,,,,,,,,,,,163710,,,,,Thu Dec 06 19:10:49 UTC 2007,,,,,,,0|i0gfjb:,93947,,,,,,,,,,05/Dec/07 01:01;alangates;Changed  MapReducePlanCompiler to make a copy of the eval spec before altering it and pushing it into the combiner.  This allow subsequent constructions of physical plans to use the original eval spec again.,06/Dec/07 19:02;olgan;looks good. +1,06/Dec/07 19:10;alangates;Fixed checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a few patterns to svn:ignore,PIG-41,12383646,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,,phunt,phunt,30/Nov/07 18:07,24/Mar/10 22:01,14/Mar/19 03:05,13/Dec/07 00:49,,,,,,,0.1.0,,,,,0,,,,"Could you add a few exclusions to svn:ignore to cleanup svn display? Esp annoying if you are using eclipse.

Add the following:
dist (the build directory and all subdirs)
the generated .java files for QueryParser.jjt/GruntParser.jj

might be others but these are the ones I notice most.
",svn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2007-12-13 00:49:24.978,,,no_permission,,,,,,,,,,,,163708,,,,,Mon Jan 28 09:11:36 UTC 2008,,,,,,,0|i0gfif:,93943,,,,,,,,,,13/Dec/07 00:49;utkarsh;Fixed with revision 603786,"28/Jan/08 09:11;francisoud;Linking the same kind of bug (PIG-77) to find informations more easily ...

I suppose Fix Version is 0.1.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory management in BigDataBag is probably wrong,PIG-40,12383575,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,breed,spullara,spullara,30/Nov/07 00:10,24/Mar/10 22:01,14/Mar/19 03:05,10/Jan/08 21:16,,,,,,,0.1.0,,impl,,,0,,,,"src/org/apache/pig/data/BigDataBag.java

1) You should not use finalizers for things other than external resources -- using them here is very dangerous and could inadvertantly lead to deadlocks and object resurrection and just decreases performance without any advantage.
2) Using .freeMemory() the way it is used in this class is broken.  freeMemory() is going to return a mostly random number between 0 and the real amount.  Adding gc() in here is a terrible performance burden.  If you really want to do something like this you should using softreferences and finalization queues.
",,,,,,,,,,,,PIG-30,,,,,,,01/Dec/07 21:42;spullara;BigDataBag.java;https://issues.apache.org/jira/secure/attachment/12370764/BigDataBag.java,01/Dec/07 21:03;spullara;MemoryUsage.java;https://issues.apache.org/jira/secure/attachment/12370761/MemoryUsage.java,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2007-11-30 02:41:23.664,,,no_permission,,,,,,,,,,,,163707,,,,,Thu Jan 10 21:25:30 UTC 2008,,,,,,,0|i0gfhz:,93941,,,,,,,,,,"30/Nov/07 02:41;utkarsh;Are you looking at the latest version of the code?

Regarding

1) we have patched finalize() such that it now only deletes the files on disk.

2) I couldn't find references to gc() any more.

 ","30/Nov/07 08:29;tdunning@veoh.com;
> 1) we have patched finalize() such that it now only deletes the files on disk.

Shouldn't this be done with File.deleteOnExit rather than a finalizer?
","30/Nov/07 15:37;breed;No, we need to delete in finalize() since we will no longer be using those files once the BigDataBag is GCed. If we relied on File.deleteOnExit we may end up exhausting disk space in tasks with lots of bags that spill to disk.

As you say, finalizers are for freeing up external resources associated with an object. The temp spill files are such external resources.",30/Nov/07 16:42;olgan;Can we  close this bug or are there other concerns?,30/Nov/07 19:53;spullara; I think deleting the files is a legitimate use of a finalizer.  Doesn't really answer the 2nd concern though where this class attempts to do memory management in a way that I don't believe is accurate.,30/Nov/07 20:09;olgan;I don't think the latest version does any memory management. Is your tree up to date?,"30/Nov/07 20:21;spullara;In BigDataBag.java:

    private boolean isMemoryAvailable(long memLimit){
	long freeMemory = Runtime.getRuntime().freeMemory();
        long usedMemory = Runtime.getRuntime().totalMemory() - freeMemory;
        return MAX_MEMORY-usedMemory > memLimit;
    }

Then it is used here to make a decision about whether to write to disk based on that call:

                    if (!isMemoryAvailable(FREE_MEMORY_TO_MAINTAIN) && trueCount > 10) {
	                writeContentToDisk();
                    }

isMemoryAvailable isn't quite a random boolean but it is close in all the JVM implementation I am aware of.","30/Nov/07 20:52;breed;We aren't really doing memory management. We just need to decide when to spill a bag to disk. We can't just count the elements of a bag since elements can be of different size. We also need to spill earlier if memory is constrained.

Using freeMemory may cause us to spill before we need to, but the important thing is that we make sure to spill when memory is constrained. There doesn't seem to be a better way to do it.","01/Dec/07 19:10;spullara;I'm not sure that its true that there isn't a better way to do it.  I'm investigating using this mechanism to get what you are looking for:

http://java.sun.com/javase/6/docs/api/java/lang/management/MemoryPoolMXBean.html
",01/Dec/07 21:03;spullara;I've tested the attached program in a few VMs/OSs.  It appears to do something more like what you want and it will increase performance as well because memory will only be checked after automatic garbage collections instead of before every add to the bag.  I haven't yet integrated it in to PIG as I would like to get any feedback about it. ,01/Dec/07 21:03;spullara;Better memory management example.,01/Dec/07 21:42;spullara;I suggest something like the attached implementation rather than the current one.,"01/Dec/07 21:50;spullara;One more thing on the finalize() call.  Both close() and delete() can throw runtime exceptions which would cause files to be left on the disk. Similarly they will be left on disk if the VM exits abnormally (and sometimes normally, VM dependent).  I would probably add a call to store.deleteOnExit() when you create the temp files to ensure that they are not left over no matter what happens.","03/Dec/07 17:20;breed;I like the MemoryPoolMXBean approach. The main advantage is doing a simple boolean check rather than a native method call on every add. It's not clear that it will be more accurate than getFreeMemory though. (Do you have reason to believe otherwise?) The there are two things that bother me:

1) It is an optional mechanism. There seem to be quite a few caveats to it being there and working, so we probably need a fallback mechanism. (Always spilling seems harsh.)

2) The threshold event only triggers one way. (Seems odd doesn't it.) The low memory situation will usually be a transient condition. We should probably poll periodically to reset the condition.

Since low memory state may be used in other places, perhaps we should pull out the memory tracker into separate class.","03/Dec/07 17:47;spullara;The particular notification that I use in my example is evaluated right after a garbage collection occurs, it should be very accurate.

1) If you want to keep in the old code for a case where you find a VM that doesn't support that seems fine to me.  That said, it worked on Mac (Sun & soylatte) and Linux (Sun & Jrockit).  Haven't tested it on Windows but I can't imagine Sun failed to implement it there.
2) Rechecking periodically doesn't seem that bad to me.  In fact there is a polling version of this in the javadocs: https://java.sun.com/j2se/1.5.0/docs/api/java/lang/management/MemoryPoolMXBean.html though I would always use the 'Collected' thresholds so you don't get false positives.

I was actually pleasantly surprised to find such great support for doing this stuff in the VM -- somehow I had missed it in the release notes or at least forgot to check it out.","12/Dec/07 22:01;olgan;Ben, could you add a write on how you are planning to address memory management for bags.",12/Dec/07 22:44;breed;The plan is to use Sam's idea to monitor memory usage and then invoke spill() on big bags when memory gets low. See PIG-30 for documentation of the spill hook being added by Alan.,12/Dec/07 23:03;alangates;We also need to agree on a way that the DataBags can register themselves with the memory manager so it can later tell them to spill.,"12/Dec/07 23:31;breed;How about 

class MemoryManager {
    static MemoryManager getMemoryManager();
    void add(Spillable spillable);
}

add() will put Spillable on a WeakList so that the Spillable object will get GCed when strong references go away.",04/Jan/08 23:03;alangates;The fix for PIG-30 addresses the issues brought up in this bug as well.,10/Jan/08 21:16;olgan;This is addressed by PIG-30,"10/Jan/08 21:25;spullara;Looked through the solution to this... looks great!
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"BufferedPositionedInputStream drastically reduces read performance because it doesn't override read([], o, l) in InputStream",PIG-39,12383574,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,spullara,spullara,30/Nov/07 00:03,03/Jul/08 18:12,14/Mar/19 03:05,21/Dec/07 00:42,,,,,,,0.1.0,,impl,,,0,,,,"Simple fix can have a huge effect on performance of certain kinds of PIG programs:

Index: src/org/apache/pig/impl/io/BufferedPositionedInputStream.java
===================================================================
--- src/org/apache/pig/impl/io/BufferedPositionedInputStream.java	(revision 597597)
+++ src/org/apache/pig/impl/io/BufferedPositionedInputStream.java	(working copy)
@@ -49,7 +49,14 @@
         pos += rc;
         return rc;
     }
-    
+
+    @Override
+    public int read(byte b[], int off, int len) throws IOException {
+        int read = in.read(b, off, len);
+        pos += read;
+        return read;
+    }
+
     /**
      * Returns the current position in the tracked InputStream.
      */
","Java 1.6, Mac OS X 10.5",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2007-11-30 02:45:35.672,,,no_permission,,,,,,,,,,,,163706,,,,,Mon Dec 10 21:56:17 UTC 2007,,,,,,,0|i0gfhj:,93939,,,,,,,,,,"30/Nov/07 02:45;utkarsh;+1 to this patch. 

(pending of course, all tests passing).",30/Nov/07 15:53;breed;+1 Great catch Sam!,30/Nov/07 18:29;olgan;Great catch! I am going to make the change and run our functional as well as performance tests.,"10/Dec/07 21:56;olgan;I incorporated the change and ran performance tests. Unfortunately, I did not see any change in performance. By looking at Hadoop, code, I think they already buffering the data, so our code just going against data cached in memory.

I am still going to commit the patch since this is a bug.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FindBugs: Possible null pointer dereference of esp in PigCombine.reduce,PIG-37,12383427,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,phunt,phunt,28/Nov/07 18:09,25/Mar/10 00:12,14/Mar/19 03:05,26/Jan/09 19:50,,,,,,,0.2.0,,impl,,,0,,,,"deserialize can return null if evalSpec is empty, need to extend the check on the previous line to include setting evalPipe

                String evalSpec = job.get(""pig.combineFunc"", """");
                EvalSpec esp = (EvalSpec)ObjectSerializer.deserialize(evalSpec);
                if(esp != null) esp.instantiateFunc(pigContext);
                evalPipe = esp.setupPipe(finalout);

Severity and Description	Path	Resource	Location	Creation Time	Id
M C NP: Possible null pointer dereference of esp in org.apache.pig.impl.mapreduceExec.PigCombine.reduce(WritableComparable, Iterator, OutputCollector, Reporter)	pig-apache/src/org/apache/pig/impl/mapreduceExec	PigCombine.java	line 69	1196272508421	23812
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2007-11-29 00:31:50.523,,,no_permission,,,,,,,,,,,,163704,,,,,Mon Jan 26 19:50:10 UTC 2009,,,,,,,0|i0gfgn:,93935,,,,,,,,,,29/Nov/07 00:31;utkarsh;PigCombine is not currently used and should be in the process of being rewritten from scratch for supporting algebraic functions,29/Nov/07 00:37;alangates;PigCombine is being used now that I'm in the process of adding the use of the combiner into pig.  And the issue pointed out by Patrick hasn't been fixed.,26/Jan/09 19:50;olgan;This code is no longer used.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FindBugs: Method ignores results of InputStream.skip(),PIG-36,12383372,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,phunt,phunt,28/Nov/07 01:48,25/Mar/10 00:12,14/Mar/19 03:05,26/Jan/09 19:48,,,,,,,0.2.0,,impl,,,0,,,,"InputStreams don't always skip as much as they are asked to skip, need to do this in a loop:

		if (toSkip > 0)
			in.skip(toSkip);
		return t;

Severity and Description	Path	Resource	Location	Creation Time	Id
M B RR: org.apache.pig.impl.builtin.RandomSampleLoader.getNext() ignores result of org.apache.pig.impl.io.BufferedPositionedInputStream.skip(long)	pig-apache/src/org/apache/pig/impl/builtin	RandomSampleLoader.java	line 49	1196213971062	22891
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2007-11-28 01:55:34.647,,,no_permission,,,,,,,,,,,,163703,,,,,Mon Jan 26 19:48:28 UTC 2009,,,,,,,0|i0gfg7:,93933,,,,,,,,,,"28/Nov/07 01:55;tdunning@veoh.com;
It will probably be obvious to whomever does this fix that the code above should be more like:

    while (toSkip > 0) {
			toSkip -= in.skip(toSkip);
    }
    return t;

But it doesn't hurt to mention the desired state as well as the current state.","28/Nov/07 04:42;utkarsh;RandomSampleLoader is just meant to load a few random rows of the input. So it does not care whether we were actually able to skip the amount requested or not. In fact, we want to make every disk seek count to give us a sample (the bigger our sample size, the better our quantile accuracy). So if we called skip, and that didnt skip the requested amount, we would still want to get a sample from the current position before moving on.","28/Nov/07 17:46;phunt;Er, ok. So the desired state in this case should be: since this diverges from the std ""stream skip"" idiom add these comments to the source so the next guy to run findbugs will be less likely to flag it. (I'd do it but I'm not a commiter)
","28/Nov/07 18:19;utkarsh;Good idea, will do. I am assuming, I can go ahead and commit comment-only patches without going through the review and approval process. If someone feels otherwise, please object.","28/Nov/07 18:27;olgan;Yes, trivial modifications can go in and comments fall into this category.",26/Jan/09 19:48;olgan;This code is no longer used.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FindBugs: Possible null pointer dereference in TimeWindowSpec,PIG-35,12383371,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,phunt,phunt,28/Nov/07 01:33,25/Mar/10 00:12,14/Mar/19 03:05,26/Jan/09 19:45,,,,,,,0.2.0,,impl,,,0,,,,"(looks like typo & vs &&)

            if (tail != null & tail.getTimeStamp()<= expireTime) {

Severity and Description	Path	Resource	Location	Creation Time	Id
H C NP: Possible null pointer dereference of tail in org.apache.pig.impl.eval.window.TimeWindowSpec$1.add(Datum)	pig-apache/src/org/apache/pig/impl/eval/window	TimeWindowSpec.java	line 59	1196213052841	22410
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2007-11-28 04:30:03.974,,,no_permission,,,,,,,,,,,,163702,,,,,Mon Jan 26 19:45:52 UTC 2009,,,,,,,0|i0gffr:,93931,,,,,,,,,,28/Nov/07 04:30;utkarsh;This class is no longer used and should be removed from the source tree.,"30/Nov/07 18:48;olgan;Utkarsh, can you go ahead and remove it and then close the bug, thanks.",26/Jan/09 19:45;olgan;This code is no longer used,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
help in grunt shows single line of help,PIG-33,12383357,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,olgan,phunt,phunt,27/Nov/07 23:05,24/Mar/10 22:01,14/Mar/19 03:05,28/Nov/07 19:04,,,,,,,0.1.0,,grunt,,,0,,,,"grunt shows only a single line for ""help"" command. this is with the current Pig trunk code.

------------------
$ java -cp ""pig.jar;."" org.apache.pig.Main
Connecting to hadoop file system at: localhost:54310
Connecting to map-reduce job tracker at: localhost:54311
grunt> help
Commands:
store <alias> into <filename> [using <functionSpec>]
grunt> quit
","windows xp, hadoop local",,,,,,,,,,,,,,,,,,28/Nov/07 01:48;olgan;help.patch;https://issues.apache.org/jira/secure/attachment/12370368/help.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-11-28 01:31:42.713,,,no_permission,,,,,,,,,,,,163700,,,,,Wed Nov 28 19:04:21 UTC 2007,,,,,,,0|i0gfev:,93927,,,,,,,,,,28/Nov/07 01:31;olgan;This is because all other lines happen to be commented out. Patch is coming shortly.,"28/Nov/07 01:48;olgan;Please, review the patch","28/Nov/07 18:25;alangates;+1

Looks good.",28/Nov/07 19:04;olgan;fix committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
concurrent modification error,PIG-31,12383337,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,olgan,olgan,olgan,27/Nov/07 18:10,24/Mar/10 22:01,14/Mar/19 03:05,30/Nov/07 18:47,,,,,,,0.1.0,,impl,,,0,,,,"Many users encountered this problem. The stack looks like:

Error:java.lang.RuntimeException
	at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
	at java.util.AbstractList$Itr.next(AbstractList.java:343)
	at com.yahoo.pig.builtin.SUM.sum(SUM.java:58)
	at com.yahoo.pig.builtin.SUM.exec(SUM.java:25)
	at com.yahoo.pig.builtin.SUM.exec(SUM.java:21)
	at com.yahoo.pig.impl.eval.FuncEvalSpec$1.add(FuncEvalSpec.java:101)
	at com.yahoo.pig.impl.eval.GenerateSpec$CrossProductItem.(GenerateSpec.java:140)
	at com.yahoo.pig.impl.eval.GenerateSpec$1.add(GenerateSpec.java:52)
	at com.yahoo.pig.impl.eval.GenerateSpec$CrossProductItem.add(GenerateSpec.java:217)
	at com.yahoo.pig.impl.eval.collector.UnflattenCollector.add(UnflattenCollector.java:42)
	at com.yahoo.pig.impl.eval.collector.DataCollector.addToSuccessor(DataCollector.java:79)
	at com.yahoo.pig.impl.eval.SimpleEvalSpec$1.add(SimpleEvalSpec.java:21)
	at com.yahoo.pig.impl.eval.GenerateSpec$CrossProductItem.exec(GenerateSpec.java:248)
	at com.yahoo.pig.impl.eval.GenerateSpec$1.add(GenerateSpec.java:61)
	at com.yahoo.pig.impl.mapreduceExec.PigMapReduce.reduce(PigMapReduce.java:144)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:355)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1707)

We traced the issue to the same bag being shared between multiple operations and clearing it in one place caused problem in another. Will attach Ben's fix shortly. All unit and end-to-end tests passed.",,,,,,,,,,,,,,,,,,,27/Nov/07 18:12;olgan;concurrentmod.patch;https://issues.apache.org/jira/secure/attachment/12370333/concurrentmod.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-11-28 00:43:48.798,,,no_permission,,,,,,,,,,,,163698,,,,,Fri Nov 30 18:47:15 UTC 2007,,,,,,,0|i0gfdz:,93923,,,,,,,,,,"27/Nov/07 18:12;olgan;please, review","28/Nov/07 00:43;alangates;As I understand the problem, the issue is that DataBag.clear()  (which is eventually called by BigDataBag.finalize()) calls content.clear().  But we have no guarantee that we haven't handed out a reference to DataBag.content via DataBag.content().  This seems like a bigger issue than just BigDataBag.finalize(), as there are other places that DataBag.clear() is called.  Isn't the correct solution to have DataBag.clear() not call content.clear()?  Do we ever need to reuse the content of the bag?
","28/Nov/07 15:52;breed;The point of the CoModificationException is to catch places where the bag is modified while an iterator is outstanding. By reuse the bag I assume you mean clear the bag and readd tuples. You certainly can do such a thing using the API. It may be useful to reexamine the DataBag API. Perhaps it would be good to do in the context of PIG-30.

DataBag.clear() should call content.clear() since that is the semantics of clear(). The exception will also arise if someone calls DataBag.add().

There is no reason to call clear() in finalize. The GC will collect everything when it is not being used. The only thing we need finalize() for is to clean up any temporary files we have created.","28/Nov/07 18:17;utkarsh;+1 to Ben's patch.

The point of the finalize method is to collect the temp files left around by the big bag. It should not be calling content.clear().

If the users decide to call databag.clear() while they have an iterator open on the bag, its only right that they get a CoMod exception.

Internally we don't call clear except at one place, where also, it is not strictly necessary.

 ","28/Nov/07 18:23;alangates;I am fine with BigDataBag.finalize() not calling DataBag.clear().  The question I still have is should DataBag.clear() call content.clear().  Shouldn't it instead allocate a new content object, so that if someone still has a reference to the old content object they aren't left to die with a comodification error?  If we think that once someone calls DataBag.clear() then any other instances of content should be declared invalid, we at least need to give a way to check that, so that we don't encounter this error again and again have no idea how we got it.",28/Nov/07 19:35;breed;I think if one thread calls clear() on the DataBag the other with the iterator should get a CoModificationException just like if one thread does an add.,"28/Nov/07 21:12;alangates;Ok, let's check it in then and plan on reworking the bag interface as part of types to help avoid these kinds of issues.",30/Nov/07 18:47;olgan;patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Get rid of DataBag and always use BigDataBag,PIG-30,12383329,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,breed,breed,27/Nov/07 15:47,24/Mar/10 22:01,14/Mar/19 03:05,04/Jan/08 23:05,,,,,,,0.1.0,,data,,,0,,,,"We should never use DataBag directly; instead, we should always use BigDataBag. I think we already do this. The problem is that the logic in BigDataBag is hard to follow and it is made more complicated because it subclasses DataBag. We should merge these two classes together.",,,,,,,,,,,,,,,,,,,07/Jan/08 20:52;alangates;addhashcode.patch;https://issues.apache.org/jira/secure/attachment/12372659/addhashcode.patch,03/Jan/08 17:45;alangates;bagrewrite.patch;https://issues.apache.org/jira/secure/attachment/12372456/bagrewrite.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2007-12-12 17:15:57.345,,,no_permission,,,,,,,,,,,,163697,,,,,Wed Jan 09 22:26:32 UTC 2008,,,,,,,0|i0gfdj:,93921,,,,,,,,,,"12/Dec/07 17:15;alangates;Based on bugs and complaints we are seeing from users, problems in the data bag implementation are causing a number of different issues.  I propose fixing several
issues:
1) Whether or not a bag needs to be sorted or distinct is known at bag creation time.  However, we always create the bag the same way and only sort or apply distinct
to the bag either when it is time to store it to disk or read from it.  It will be more efficient to subclass bag into three separate types, default, sorted, and
distinct and modify the bag factory to allow callers to create the correct type of bag up front.  Each type can then optimize their memory and disk storage.

2) The algorithm bags use to determine when to dump data to disk is not adequate.  This will be addressed in the bags by making use of the changes being done to fix
http://issues.apache.org/jira/browse/PIG-40

3) When merging back files from disk, the merge algorithm does not open enough files.  Performance testing done by the hadoop team found that 100 files was about an
optimal number.  We currently use 25.  As part of this fix we need to do our own performance testing and assure ourselves that 100 is at or near that inflection
point for us as well.

4) During the merge phase, when tuples are read off of disk, then placed in a HeapEntry, a new HeapEntry is created for each tuple.  A large number of object
creations could be saved by pooling these HeapEntry objects and reusing them.  Also, HeapEntry contains a reference to an Iterator<Tuple>.  This does not appear to
be used and should be removable.

To address these changes, BagFactory, BigDataBag, and DataBag will be significanly reworked.  BigDataBag will go away, with the understanding that all bags can spill
to disk as necessary.  DataBag will become an abstract class.  Three new classes will be introduced:  DefaultDataBag, SortedDataBag, and DistinctDataBag, all of
which will extend DataBag.

For the memory management changes related to PIG-40, it is assumed that something like the following interface will be introduced:

interface Spillable {
	/**
	 * Instructs an object to spill whatever it can to disk and release
	 * references to any data structures it spills.
	 */
	void spill();

	/**
	 * Requests that an object return an estimate of its in memory size.
	 * @returns estimated in memory size.
	 */
	long getMemorySize();
}

BagFactory's interface will change to be:

public class BagFactory {
	private static BagFactory self;

	/**
	 * Get a reference to the singleton factory.
	 */
	public static BagFactory getFactory();

	/**
	 * Get a default (unordered, not distinct) data bag.
	 */
	public DataBag newDefaultBag();

	/**
	 * Get a sorted data bag.
	 * @param spec EvalSpec that controls how the data is sorted.
	 * If null, default comparator will be used.
	 */
	public DataBag newSortedBag(EvalSpec spec);

	/**
	 * Get a distinct data bag.
	 */
	public DataBag newDistinctBag();
}

DataBag's interface will be:

public abstract class DataBag implements Writable, Spillable {
	// Containers that holds the tuples.  Actual object instantiated by subclasses.
	protected Collection<Tuple> contents;

	/**
	 * Get the number of elements in the bag, both in memory and on disk.
	 */
	public abstract long size();

	/**
	 * Find out if the bag is sorted.
	 */
	public abstract boolean isSorted();

	/**
	 * Find out if the bag is distinct.
	 */
	public abstract boolean isDistinct();

	/**
	 * Get an iterator to the bag.  For default and distinct bags,
	 * no particular order is guaranteed.  For sorted bags the order
	 * is guaranteed to be sorted according
	 * to the provided comparator.
	 */
	public abstract Iterator<Tuple> content();

	/**
	 * Add a tuple to the bag.
	 * @param t tuple to add.
	 */
	public void add(Tuple t);

	/**
	 * Add contents of a bag to the bag.
	 * @param b bag to add contents of.
	 */
	public void addAll(DataBag b);

	// Do I need remove, I couldn't find it used anywhere.

	/**
	 * Return the size of memory usage.
	 */
	public long getMemorySize();

	/**
	 * Write a bags contents to disk.  This won't change significantly
	 * from the current implementation, except that it will need to record
	 * the type of bag begin written.
	 * @param out DataOutput to write data to.
	 * @throws IOException (passes it on from underlying calls).
	 */
	public void write(DataOutput out) throws IOException;

	/**
	 * Read a bag from disk.  This won't change significantly from
	 * the current implementation, except that it will need to read the
	 * bag type and use the BagFactory to create the correct type of bag.
	 * @param in DataInput to read data from.
	 * @throws IOException (passes it on from underlying calls).
	 */
	static DataBag(DataInput in) throws IOException;

	// The old databag had a markStale() call here, but it's a NOP.
	// Does it need to be preserved?

	/**
	 * Write the bag into a string.  This will not change significantly
	 * from the current implementation.
	 */
	@Override
	public String toString();
}

public class DefaultDataBag extends AbstractDataBag {
	// Will set contents to be an ArrayList.

	// A custom iterator to handle getting data from memory and/or disk
	private class AbstractDataBagIterator implements Iterator<Tuple> { ... }

	// See above for comments on these
	public abstract long size();
	public abstract boolean isSorted();
	public abstract boolean isDistinct();
	public abstract Iterator<Tuple> content();

	/**
	 * Spill contents to disk.
	 */
	public void spill();
}
	
public class SortedDataBag extends AbstractDataBag {
	// Will set contents to be a PriorityQueue.  Experimentation found it to
	// to be faster to store this in a PriorityQueue up front rather than
	// store it in a List and then call Collections.sort() on it.

	// A custom iterator to handle getting data from memory and/or disk
	private class SortedtDataBagIterator implements Iterator<Tuple> { ... }

	// See above for comments on these
	public abstract long size();
	public abstract boolean isSorted();
	public abstract boolean isDistinct();
	public abstract Iterator<Tuple> content();

	/**
	 * Spill contents to disk.
	 */
	public void spill();
}
	
public class DistinctDataBag extends AbstractDataBag {
	// Will set contents to be a HashSet.  A little experimentation 
	// found that it was significantly faster to store distinct 
	// values in a hash set and sort them before the spill rather 
	// than store them in a TreeSet so that no sort is needed at spill
	// time.  This is also good because if the bag never spills we don't
	// waste time sorting it.

	// A custom iterator to handle getting data from memory and/or disk
	private class DistincttDataBagIterator implements Iterator<Tuple> { ... }

	// See above for comments on these
	public abstract long size();
	public abstract boolean isSorted();
	public abstract boolean isDistinct();
	public abstract Iterator<Tuple> content();

	/**
	 * Spill contents to disk.
	 */
	public void spill();
}


A getMemorySize() will need to be added to each of the other data types to allow the bag to make a guess at its memory usage.

","12/Dec/07 18:26;olgan;A couple of other issues I observed with BigDataBag:

- Should check memory availability periodically, not on every add
- Try to buffer in memory first. Currently we always write to disk after the first spill
","12/Dec/07 19:15;breed;With the Spill interface, the memory is never checked on add. Instead a memory manager will call spill() explicitly on the databag to make it spill when it determines that memory is low.","03/Jan/08 17:45;alangates;The attached patch file contains a rewrite of DataBag in line with the proposal given in previous comments.  Highlights include:

    * DataBag has been entirely rewritten.  As part of this the interface has been brought into line with standard java container interface (size() instead of cardinality() and iterator() instead of content()).  cardinality() and content() have been kept for backward compatibility but marked as deprecated.  Also as part of this change, DataBag has become an abstract class.  Also, functionality to sort and apply distinct to a bag have been removed.  This functionality is now provided by subclasses instead.

    * BigDataBag has been removed.  All data bags can now spill to disk when necessary.

    * DefaultDataBag, SortedDataBag, and DistinctDataBag have been added.  Each of these extends DataBag.

    * BagFactory has been entirely rewritten.  As part of this its interface has been changed in a non-backward compatible way.  Now the caller must specify up front what type of bag (default, sorted, distinct) is desired, and the appropriate type of bag will be provided.  In making these changes I assumed that users never directly call BagFactory, and thus changing the interface won't break any UDFs.  If this assumption is wrong, please let me know.

    * Spillable interface has been added.  This interface says that an implementing class can be asked by the system to spill its contents to the disk.  DataBag implements Spillable.

    * SpillableMemoryManager has been added (courtesy of Ben).  This memory manager registers with the JVM to be called when the largest memory pool becomes more than 50% full.  It then goes through its list of Spillable objects and asks them to spill.","03/Jan/08 18:12;breed;Excellent job Alan! That was a lot of work! Just a couple of small comments:

*  Do we need to expose DefaultDataBag, SortedDataBag, and DistinctDataBag? We don't want people constructing them directly right? Maybe we should make them package protected.

* One reason to expose SortedDataBag would be to get the sort spec. Do we want to expose that?",04/Jan/08 23:01;olgan;+1,"04/Jan/08 23:05;alangates;Fix checked in, revision 609048","05/Jan/08 04:43;utkarsh;Great job! This was a fairly large chunk of work.

It will be nice to have a few more comments. Specifically, one part that is implicit is that bag behavior is undefined if you add() to Databag after opening an iterator(). Alan and I talked about this.

Other issues:

0. TreeSet used in DistinctBag while merging files. But TContainer compares only based on tuple equality. Once you add a tuple equal to the one already in the treeset but from another input, one of the inputs will get eliminated from the treeset and never be read again. Am I missing something?

1. HashSet<> in DistinctBag. For hash set to work properly we need hashcode() methods to work properly. Since Tuple.hashcode() calls hashcode() on all its fields, all Datums should have a hash code. Databag doesn't have one which implies that DistinctBag wont work with nested data.

2. Spill() code in DistinctBag and sortedbag() is the same except that the former always uses the default comparator whereas sortedBag might use a specified comparator. Can we reuse code instead of duplicating?

","07/Jan/08 16:30;alangates;Responses to Utkarsh's comments:

0.  TreeSet.add() only adds an element if it is not already present (see http://java.sun.com/j2se/1.5.0/docs/api/java/util/TreeSet.html#add(E)).  This guarantees that the element already in the tree will not be obliterated.  That's why if that call returns false, the code goes back and rereads from the file it read the last element from.  This guarantees that we read from that file until either the file is empty or we find a new unique element to put in the TreeSet.

1.  Good catch, I'll add a hashcode() implementation for DataBag.

2.  They aren't quite as combinable as they first appear.  The code in next() is identical, and could be combined.  DistinctDataBag.readFromTree() and SortedDataBag.readFromPriorityQ() create different containers and access them differently.  I could put just the create and access methods in each and combine the rest of the logic.  The addToQueue() functions in each are different and have different logic about how to add an element to the queue.   I can work on this, but it may be a bit before I get to it.","07/Jan/08 20:52;alangates;In response to Utkarsh's comments, added more comments to the code and dealt with the hash code issue.  I also fixed an issue in DataBag.compareTo that I found as a result of thinking about the need for overriding hashCode().  compareTo() wasn't properly handling equals, which would have meant errors in distinct data bags.",07/Jan/08 21:20;olgan;+1,"09/Jan/08 22:26;alangates;Some performance numbers based on the code before and after these changes.  I tested default bags (that is, no sorting, no distinct), distinct bags, and sorted bags.  Each test was run on the code pre- and post-patch.  Each test was run on data with 100k rows, 1m rows, and and 5m rows.

Default:

pig script:

a = load './studenttab5m';

b = group a all;

c = foreach b generate group, COUNT(a.$0);

dump c;

Results:

pre patch, 100k rows:  13.539

post 100k:  15.489

pre 1m:  43.002

post 1m:  48.191

pre 5m: 111.158

post 5m:  117.112

Notes:  I'm assuming the slight slowdown here is do to the introduction of locking into add() and next() in the data bags.

Distinct

pig script:

a = load './studenttab10m';

b = group a all;

c = foreach b { c1 = distinct $1; generate group, COUNT(c1); }

dump c;

pre-patch 100k rows:  14.927

post 100k:  14.134

pre 1m:  83.190

post 1m: 52.320

pre 5m:  744.834

post 5m:  216.043

Notes:  Data had about 90% distinct values, so 100k had about 90k distinct rows, etc.

Sorted

pig script:

a = load './studenttab5m';

b = group a all;

c = foreach b { c1 = order $1 by $0; generate group, COUNT(c1); }

dump c;

pre-patch 100k rows:  16.964

post 100k: 12.895

pre 1m:  51.351

post 1m:  51.598

pre 5m:  236.669

post 5m:  225.688",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Big bags not created when working in local mode,PIG-29,12383278,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,utkarsh,utkarsh,utkarsh,27/Nov/07 00:52,24/Mar/10 22:01,14/Mar/19 03:05,03/Dec/07 21:17,,,,,,,0.1.0,,impl,,,0,,,,"When executing in local mode, the BigDataBag class is not initialized. As a result big data bags are never created, leading to sorting and distinct not working. Could someone review the attached simple patch?

",,,,,,,,,,,,,,,,,,,27/Nov/07 00:53;utkarsh;bigdatabag.patch;https://issues.apache.org/jira/secure/attachment/12370253/bigdatabag.patch,29/Nov/07 00:51;utkarsh;bigdatabag1.patch;https://issues.apache.org/jira/secure/attachment/12370489/bigdatabag1.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2007-11-27 01:05:28.363,,,no_permission,,,,,,,,,,,,38389,,,,,Mon Dec 03 21:17:58 UTC 2007,,,Patch Available,,,,0|i0gfdb:,93920,,,,,,,,,,"27/Nov/07 01:05;olgan;does this mean that we now always create a BigDataBag? Are there any cases we still want to use DataBag?

Also, did all the unit tests pass?",27/Nov/07 15:41;breed;You should use the system property java.io.tmpdir rather than hardcoding /tmp. (See javadoc for System.getProperties and File.createTmpFile),"29/Nov/07 00:51;utkarsh;Updated patch to include java.io.tmpdir instead of hardcoded ""/tmp""",29/Nov/07 00:52;utkarsh;Please review the updated patch. All unit tests pass.,"30/Nov/07 02:53;utkarsh;Could I get a review on this very simple patch? It ""un""breaks two operators: sort and distinct in local mode. If only our tests were running in both local and mapreduce mode, it would be caught.

",30/Nov/07 15:46;breed;+1 looks great.,03/Dec/07 21:17;utkarsh;Fix committed in revision 600691,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Independence of Logical Plan from Physical Plan,PIG-28,12382931,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,antmagna,antmagna,21/Nov/07 00:47,24/Mar/10 22:01,14/Mar/19 03:05,19/May/08 18:09,,,,,,,0.1.0,,impl,,,0,,,,"The implementation of a Pig Logical Plan is currently closely tied with implementation details of the Physical Plan. 

For instance, the construction of the tree of Logical Operators puts in place the data pipes in conjunction with the construction of Eval Spec and Cond nodes. Data pipes are utilized later on during the execution of the query. Conceptually, this appears to obfuscate the code.

It would be nice to remove or reduce this tight coupling. This aspect may also be relevant to the proposal to provide an abstraction layer for Pig, where front-end activities are clearly separated from back-end tasks and responsibilities (see wiki at:  http://wiki.apache.org/pig/PigAbstractionLayer)

Two possible alternatives initially discussed could be:

1.) to decouple logical from physical part of plan construction by having two distinct sets of Eval Specs: a Logical Eval Spec/Cond set without physical plan details and a Physical Eval Spec/Cond set that addresses the aspects of the physical plan.

2.) to merge the logical operators with the local physical operators (used by the LocalPlanCompiler) and preserve the MapReduce physical operator. 

The second option may be preferable because less extensive changes are involved. Additionally, choosing the second option would integrate the Local Plan data structures into the core code packages: long term this could simplify the maintenance and support of the Local Plan construction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-05-19 18:09:26.759,,,no_permission,,,,,,,,,,,,163696,,,,,Mon May 19 18:09:26 UTC 2008,,,,,,,0|i0gfcv:,93918,,,,,,,,,,19/May/08 18:09;olgan;closing since this is being addressed by pipeline rewrite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
distinct does not work on Bags that have spilled to disk.,PIG-26,12382226,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,breed,breed,breed,09/Nov/07 19:03,24/Mar/10 22:01,14/Mar/19 03:05,12/Nov/07 23:48,0.0.0,0.1.0,site,,,,0.1.0,,data,,09/Nov/07 00:00,0,,,,"If you call distinct on a bag that has spilled to disk, you get the following error:

java.lang.NullPointerException
        at org.apache.pig.data.BigDataBag$FileMerger$1.compare(BigDataBag.java:288)
        at org.apache.pig.data.BigDataBag$FileMerger$1.compare(BigDataBag.java:280)
        at java.util.PriorityQueue.siftUpUsingComparator(PriorityQueue.java:594)
        at java.util.PriorityQueue.siftUp(PriorityQueue.java:572)
        at java.util.PriorityQueue.offer(PriorityQueue.java:274)
        at java.util.PriorityQueue.add(PriorityQueue.java:251)
        at org.apache.pig.data.BigDataBag$FileMerger.<init>(BigDataBag.java:304)
        at org.apache.pig.data.BigDataBag.doSorting(BigDataBag.java:167)
        at org.apache.pig.data.BigDataBag.content(BigDataBag.java:211)
        at org.apache.pig.test.TestDataModel.testBigDataBag(TestDataModel.java:343)
        at org.apache.pig.test.TestDataModel.testBigDataBagOnDisk(TestDataModel.java:210)",,,,,,,,,,,,,,,,,,,09/Nov/07 19:05;breed;distinct-test.patch;https://issues.apache.org/jira/secure/attachment/12369252/distinct-test.patch,09/Nov/07 19:12;breed;distinct.patch;https://issues.apache.org/jira/secure/attachment/12369255/distinct.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2007-11-09 19:23:36.622,,,no_permission,,,,,,,,,,,,163694,,,,,Mon Nov 12 23:48:14 UTC 2007,,,,,,,0|i0gfbr:,93913,,,,,,,,,,09/Nov/07 19:05;breed;The patch tweaks the bag test to manifest the bug.,"09/Nov/07 19:12;breed;All of the changes except one are formatting. (Tabs->spaces)

The fix is actually the null -> new StarSpec() in distinct()",09/Nov/07 19:13;breed;Patch is ready for commit. Can I get a review?,"09/Nov/07 19:23;cutting;> All of the changes except one are formatting. (Tabs->spaces)

That's a red flag.
",09/Nov/07 19:26;olgan;How much testing was done on this patch?,"09/Nov/07 19:27;cutting;Perhaps I should have been more verbose.  The HowToContribute page in the wiki advises against making formatting changes unless the patch is exclusively dedicated to formatting.  The point is to make patches maximally easy to review, so that reviewers can easily focus on the changes of function.

If there are lots of tabs in the codebase, then someone should file an issue for that and then fix it in one independent sweep.","09/Nov/07 20:03;breed;Yes, we definitely need a independent sweep. We should probably open another ticket for that.

As far as testing goes, the first patch to the unit tests causes a failure in the current code base and after the distinct.patch is applied the test succeeds.

Here is the fix without the formatting changes:

     @Override
public void distinct() {
-   	sort(null,true);
+        sort(new StarSpec(), true);
       isSorted = true;
   }","09/Nov/07 23:31;cutting;Can you please attach that to this issue as a patch file, per the wiki's HowToContribute instructions?  Thanks!
 ","10/Nov/07 01:45;olgan;I successfully ran all unit tests and a 60+ end-to-end tests. The code changes also look good.

+1",12/Nov/07 23:48;breed;Fix committed in revision 594348,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
com.yahoo.pig dir left under pig/test,PIG-25,12382126,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,olgan,olgan,olgan,08/Nov/07 20:32,24/Mar/10 22:01,14/Mar/19 03:05,08/Nov/07 22:01,,,,,,,0.1.0,,,,,0,,,,An empty directory was left there during conversion from yahoo to apache. I am planning to just remove it unless I hear otherwise.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-01-28 09:12:32.081,,,no_permission,,,,,,,,,,,,163693,,,,,Mon Jan 28 09:12:32 UTC 2008,,,,,,,0|i0gfbj:,93912,,,,,,,,,,08/Nov/07 22:01;olgan;fixed,28/Jan/08 09:12;francisoud;I think it was fixed in Fix Version: 0.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ant clean does not cleanup test/reports,PIG-24,12382034,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,milindb,milindb,milindb,07/Nov/07 19:20,24/Mar/10 22:01,14/Mar/19 03:05,08/Nov/07 02:11,0.0.0,,,,,,0.1.0,,,,,0,,,,"There are two issues.

1. There are some files in test/report that are under subversion. test/report is used to generate test reports for unit tests. They should be removed from version control.
2. The second issue is that test/report is not cleaned up when one does ant clean.",All,,,,,,,,,,,,,,,,,,07/Nov/07 19:30;milindb;pig.patch;https://issues.apache.org/jira/secure/attachment/12369134/pig.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-11-08 02:11:39.479,,,no_permission,,,,,,,,,,,,38576,,,,,Thu Nov 08 02:11:39 UTC 2007,,,Patch Available,,,,0|i0gfbb:,93911,,,,,,,,,,"07/Nov/07 19:21;milindb;Files in test/reports that are under subversion:

!      test/reports/TEST-org.apache.pig.test.TestBuiltin.txt
!      test/reports/TEST-org.apache.pig.test.TestCompressedFiles.txt
!      test/reports/TEST-org.apache.pig.test.TestCmdLineParser.txt
!      test/reports/TEST-org.apache.pig.test.TestDataModel.txt
!      test/reports/TEST-org.apache.pig.test.TestEvalPipeline.txt
!      test/reports/TEST-org.apache.pig.test.TestAlgebraicEval.txt
","07/Nov/07 19:30;milindb;In fact, since test/reports is created when ""ant test"" is invoked, the entire test/report should be moved out of svn. Attaching patch soon.",08/Nov/07 02:11;alangates;Fix checked in.  Thanks Milind.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need basic type checking before query execution,PIG-22,12381865,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,olgan,olgan,05/Nov/07 21:50,25/Mar/10 00:12,14/Mar/19 03:05,20/Jan/09 22:46,,,,,,,0.2.0,,impl,,,0,,,,This will be addressed as part of adding types to pig. See http://wiki.apache.org/pig/PigTypesFunctionalSpec,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2007-11-05 22:07:15.206,,,no_permission,,,,,,,,,,,,163691,,,,,Tue Jan 20 22:46:43 UTC 2009,,,,,,,0|i0gfaf:,93907,,,,,,,,,,"05/Nov/07 21:52;olgan;Brief problem description: since pig does not have types, expressions are not validated till runtime which is very frustrating to users. For instance, if you try to add a string and a float, the parser would not detect a problem.","05/Nov/07 22:07;tdunning@veoh.com;
Even more frustrating are problems that are detected even later in the reduce process such as trying to output a bag.",10/Jan/08 21:13;olgan;This will be done as part of types,"20/Jan/09 22:46;alangates;The types functionality, which includes up front semantic checking, is now in trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Making Pig work with HOD 0.4 and hadoop 0.16,PIG-18,12381735,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,olgan,olgan,olgan,02/Nov/07 20:24,24/Mar/10 22:01,14/Mar/19 03:05,20/Mar/08 21:37,,,,,,,0.1.0,,impl,,,0,,,,Pig needs to work with the soon to be released hadoop on demand (HOD) version.,,,,,,,,,,,,,,,,,,,20/Mar/08 04:21;olgan;PIG-18.patch;https://issues.apache.org/jira/secure/attachment/12378294/PIG-18.patch,07/Mar/08 00:58;olgan;PIG-18.patch;https://issues.apache.org/jira/secure/attachment/12377304/PIG-18.patch,07/Mar/08 18:48;craigm;PIG-18.patch-v2craigm-07032008;https://issues.apache.org/jira/secure/attachment/12377377/PIG-18.patch-v2craigm-07032008,10/Mar/08 13:37;craigm;PIG-18.patch-v3craigm-10032008;https://issues.apache.org/jira/secure/attachment/12377526/PIG-18.patch-v3craigm-10032008,20/Mar/08 20:24;olgan;PIG-18_20080320.patch;https://issues.apache.org/jira/secure/attachment/12378341/PIG-18_20080320.patch,07/Mar/08 22:10;olgan;hadoop16.jar;https://issues.apache.org/jira/secure/attachment/12377395/hadoop16.jar,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2007-11-02 21:06:57.553,,,no_permission,,,,,,,,,,,,38577,,,,,Thu Mar 20 21:41:44 UTC 2008,,,Patch Available,,,,0|i0gf8v:,93900,,,,,,,,,,02/Nov/07 21:06;cutting;Where is HOD 0.4 described?  HADOOP-1301?  It's hard to be compatible with something that's unpublished...,02/Nov/07 21:22;olgan;This is question for hadoop team. I have not seen the spec for it yet but I know that it is coming.,"25/Feb/08 15:53;craigm;I had a think on this issue. It's fairly easy to integrate HOD, if the ssh gateway support is *not* required. This is because, essentially, we do create a local temporary directory, then call HOD. 

For the ssh gateway support, there needs to be a way to get the Hadoop configuration across the connection. How important a use-case is this for Yahoo? 
Feasible solutions:
1. the current expect script parsing HOD output.
2. Using a shell script wrapper emulating HOD? This would call hod on the remote machine, and then scp the resulting configuration back
3. Another option would be to add an option to HOD that allows the Hadoop XML config file to be displayed once nodes have been allocated.
4. You install qsub on all machines that run Pig.
5. You write a shell script to emulate qsub using ssh and use HOD locally.
{noformat}
#!/bin/bash
GATEWAY=headnode
exec ssh headnode qsub ""$@""
{noformat}

Craig","25/Feb/08 16:47;breed;Perhaps we should rip out the ssh gateway. There is currently a bug such that any data transfers of non-trivial size could cause a deadlock. I've tried repeatedly to get help from the jsch mailing list, but to no avail. So, given that it isn't working properly at the moment (and hasn't ever worked really), we should probably rip it out.","25/Feb/08 19:25;olgan;I already have the patch for HOD. I have not tested the ssh part yet.

Ben, I was under impression that people actually use ssh tunnel?

Comitting this change requires fixing issue in https://issues.apache.org/jira/browse/PIG-116 which I am working on now. Because I added a shutdown hook to cleanup hod connections in case of kill, we are leaving temp files behind more frequently.",25/Feb/08 23:10;breed;I'm not sure if anyone uses it. It works great as long as you don't try to tranfer files :),"25/Feb/08 23:45;joa23;Is HOD a requirement for y!? I guess so? If not, I would be very much in favor of removing all the HOD related code. 
It adds a lot of code into the system that makes things more complex and fragil. Implementing sshSockets is just something pig should not do. 
As far I understand we could do all of that also with a shell script.
At least from the perspective of the project we working on there are more important bridges to cross than supporting HOD. 
So I would prefer a pig-hod.sh script.
Sorry, just my 2 cents. :)",26/Feb/08 00:42;olgan;HOD is now part of hadoop so supporting it is not Yahoo specific. But we do use it inside of Yahoo so we can't remove it. The need for expect script would go away and hopefully the code will be a bit cleaner once I am done with this patch.,"07/Mar/08 01:06;olgan;Just apploaded the patch that makes pig work with hadoop 0.16 and HOD 0.4. 

This patch contains several things:

(1) New hadoop16.jar and I made it the default in build.xml since this is the current hadoop release
(2) Changes to integrate with HOD 0.4
   - complete API rewrite; we no longer need expect script and all the strange inetraction we had to do. Now we just call hod allocate in the beginning and hod deallocate at the end.
   - added shutdown hook to make sure that we disconnect even during abnormal termination
(3) Moved temp file cleanup from shutdown hook into Main.java. The problem with shutdown hook is that it might be called after DFS shutdown hook in which case cleanup does not happen. So at this point we cleanup under normal conditions and don't cleanup when client process is killed via signal. (I opened a bug with hadoop team to help is deal with this issue.)
","07/Mar/08 01:25;joa23;Does hadoop16.jar is a self build jar that contains all jars hadoop comes with?
That would be critica, since we would run in version conflicts in case we have one of those libraries different on the cluster. 
Embedding jars into jars is very uncommon in the java world and critical. PIG-136 shows how we can easily deal with hadoop dependencies. ","07/Mar/08 01:40;craigm;Hi Olga,

Some comments/questions on this patch:

1. fixUpDomain() still patches non FQ hostnames to be in the domain .inktomisearch.com. Can this patch resolve this? Perhaps this can become a property that PIG-111 uses?

2. 
{noformat}
DataInputStream err = new DataInputStream(p.getErrorStream());
String errMsg = new String();
try {
errMsg = err.readUTF();
{noformat}

This doesnt look quite right. DataInput/DataOutputStreams are a Java specific form of Data IO, not generally compatible with other platforms. Indeed readUTF expects first a number telling it how long the String is.

Perhaps this is more appropriate?

{noformat}
String errMsg;
try{
BufferedReader br = new BufferedReader(new InputStreamReader(p.getErrorStream()));
errMsg = br.readLine();
br.close();
{noformat}


3. How doe hod work over an SSH connection. You ssh to connect to HOD, but your passing a directory that is on the local node. How does HOD read/write to that?

4. hod is assumed to be on the user's path?

Overall, looks much better :-)

C","07/Mar/08 16:59;craigm;Olga,

Can you attach a hadoop16.jar so I can test this please?

Thanks

Craig","07/Mar/08 17:52;olgan;In response to Stefan's comment.

The hadoop 0.16 jar is the same format as for hadoop 0.15. Why would it make the things worse for your project than with hadoop-0.15.

I would like to separate migration to hadoop 0.16 from changes to the library structure as I think we need to have a discussion before hand on how we see integration with hadoop and other libraries. There are pros and cons with bundling and splitting things and I would like for that discussion to happen prior to making changes","07/Mar/08 17:56;olgan;Craig, thanks for detailed feedback.

(1) I agree that this code should be moved out. Once PIG-111 is resolved, I would be happy to make the transition.
(2) Thanks for the proposed change; will make the change and resubmitt the patch.
(3) You are right. The plan is to actually get rid of that code, I just did not have time to do the cleanup.
(4) Yes, we assume hod is on the patch

I will provide hadoop16.jar shortly. Need to get over to my other box :)","07/Mar/08 18:48;craigm;Here's a completely uncompiled and untested patch to resolve issues (1), (2) & (3). (1) is obviously temporary until PIG-111 lands. 

For (4) we could simply introduce a hod.path property. With all the flakey NFS servers around here, I try to minimise the number of places on my path.

C","10/Mar/08 13:37;craigm;Here's an updated version of my earlier patch. This even compiles, and I tested it ;-)

All ssh code has been removed. Consequently the hod.server property has been removed.

Olga, I couldn't follow your HOD commands versus the HOD documentation http://hadoop.apache.org/core/docs/r0.16.0/hod.html, so I recoded it to follow that document. As related issue is them that the command line should be passed as an array, to ensure that spaces within the double quoted region are not tokenised by Runtime.exec(String).

I added a System property simply named ""hod"", which takes the value of $hodRoot from pig.pl. This is what defines if HOD should be used, and also allows HOD to be explicitly specified, not just in the user's path. 

The ExecException ""Failed to connect to HOD. Return code"" was never actually thrown (the throw was missing). doHod() now propagates any ExecException thrown in doHod(), rather than wrapping it in another one.

A future ""feature""  for debugging log level would be to capture the HOD standard output and log that.

Craig","14/Mar/08 15:53;olgan;Craig,

We have some internal users that might still need the ssh stuff. I am meeting with them on Monday to see what we can do.

Also, the hod documentation is not up to date so the interface there is not correct - they have changed it last week.

I will update the patch today ignoring ssh part and then follow up with ssh changes once I figure out what needs to be done. I will also take a look at your exception changes.

Thanks for your help and review!","14/Mar/08 21:15;olgan;Craig,

After further thinking about this, I would like to split this work into two separate bugs and separate patches:

(1) Done within this bug, just makes the changes needed to work with hadoop 0.16 and HOD 0.4 in the same way that it does now. It will include my original patch with the following fixes based on your comments:

- cleaned up error handling 
- properly throwing the exception
- removing or making work ssh tunnel depending on my discussion with the internal user on Monday.

(2) A separate issue to address hod configurability, improving error propagation etc.

The are a couple of reasons for the proposed split:

- I feel that those changes are quite distinct and should be discussed separately. Also some of the config changes winght be breaking backward compatibility.
- I would like to get the code for hadoop 0.16 and hod 0.4 committed as soon as possible. Combining the two would mean more testing and more time.

I am planning to submit the patch for (1) on Monday after I figure out if we need to keep the ssh tunnel in place.","16/Mar/08 15:01;craigm;Olga,

Apologies on the HOD confusion. I think you are privy to more information than me!

For SSH tunneling, i understand your Hadoop cluster is behind a SSH gateway. Is it also SOCKS accessible? 

An easier option would be: 
Run HOD locally, use ssh wrappers to run the Torque tools, and tell Java to use the SOCKS server for all socket connections, i.e. the Hadoop connections.

For the SSH tunneling of the Torque commands, create this executable script in your path:
{noformat}
#!/bin/bash
REMOTEPBSPATH=/usr/local/bin
ssh sshgateway $REMOTEPBSPATH/$0 ""$@""
{noformat}

and symlink qsub and qstat to it.

See http://java.sun.com/javase/6/docs/technotes/guides/net/proxies.html for how to configure Java to use SOCKS.
A simple SOCKS server is http://ss5.sourceforge.net

Overall, this would keep the Pig codebase more lightweight.

C","17/Mar/08 20:45;olgan;Our scenerion is that we have hod installed on a set of machines from which the cluster can be accessed. However, some users choose to run from their own machines via ssh tunnel that connects them one of the machines with HOD.

For most cases this is not that much different than running pig command via ssh except that the would need to copy pig script over. The useful case is when the input or output data is local to their machine - that saves them the need to copy things back and force. However, in that case, there is a bug in ssh library that we use - it sometimes hangs the system during large data transfers.

What you are suggesting sound like something that HOD would need to do since pig does not have direct access to torque and maui.

My plan is to put ssh tunneling code back for now since we have some people here using it. I will change create and remove temp file code to do it via command that can run locally or remotely depending on the mode.","20/Mar/08 04:21;olgan;Updated patch:

- makes ssh tunnel work
- makes changes suggested by Craig","20/Mar/08 17:11;craigm;Olga, 

1. This patch does not resolve the hard-coding of .inktomisearch.com, which prevents us using Pig without patching in our own setting. 

I don't see a need why this hack should remain any longer. Indeed it will break compatability for internal users, but you have control over your internal users. If suffixing "".inktomisearch.com"" to any non FQDN remains, then any some percentage of new users trying to use Pig through HOD will fail. It can be simply replaced with a property, and given a good configuration implementation, this will be a one-time only change for the users. Perhaps you should only move your users when you reach a release status, and this prevents them being interrupted while we try to determine the best way to configure Pig.

(If the above reads overly harshly, please accept my apologies, I'm not hear to stir things up).

2. I would recommend change the spec of runCommand to (String, String[]). This would be less likely to break in settings where the command line contains paths with spaces. (Often on Windows ;-). In this case, you'd be using runtime.exec(String[]) instead of runtime.exec(String). This is better as the Java tokeniser is not as good as Unix shells at tokenising spaces, escapes, double quotes etc [e.g Perl uses the Unix shell to tokenise for system("""") calls, while Java just splits on space]. Using the array method prevents the tokenisation, and means arguments containing spaces wont be incorrectly tokenised.

Other than that I'm happy to see this patch committed.

Craig",20/Mar/08 17:25;breed;Excellent comments Craig. I second both of them.,"20/Mar/08 18:34;alangates;A couple of questions:

1) This patch reworks how we disconnect from HOD in both regular and interrupt situations.  Were both those situations tested to make sure that pig didn't leave the hod session live?

2) Could bin/startHOD.expect be removed as part of this patch?  Is it still used anywhere?

Regarding Craig's comment 1 in https://issues.apache.org/jira/browse/PIG-18?focusedCommentId=12580854#action_12580854, do we need to hold this patch for that fix?  I fully agree with him that we need to get all Yahoo specific references out of the code.  But these changes aren't making that any worse, they just aren't addressing it yet.  Can that be done in a separate patch?","20/Mar/08 18:38;olgan;Craig, thanks for your comments. Will make both changes and submit new patch in a bit.","20/Mar/08 18:52;olgan;Alan, thanks for the review

Yes, I did test all disconnect case both normal and with Ctrl-C for both gateway and ssh tunnle access. I made sure what we don't leave things in the torque queue as well as temp files both on local host and on the gateway.

And yes, I will remove the expect script.",20/Mar/08 20:24;olgan;Updated patch with issues brought up by Craig and Alan resolved.,20/Mar/08 20:42;alangates;+1.  Looks good.,"20/Mar/08 21:31;pi_song;A trivial thing that I spotted
{noformat}
+        // this is a hack to see full error till we resolve commons logging config
+        e.printStackTrace();
{noformat}
TODO: should be used so static code analyzer will report that if you forget.",20/Mar/08 21:36;olgan;we still need it since the log.error as of now does not print stack trace of the cause which we need to diagnose problems.,"20/Mar/08 21:37;olgan;Patch committed. Please, note that

(1) hadoop 0.16 is now set as default; can be changes vi proprty
(2) I removed hadoop 0.14
(3) I removed hod expect script","20/Mar/08 21:41;craigm;Super Olga. I was about to +1 your latest patch.
Thanks :-)

C",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Pig work with Hadoop 0.15,PIG-17,12381720,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,olgan,olgan,olgan,02/Nov/07 17:59,24/Mar/10 22:01,14/Mar/19 03:05,30/Nov/07 16:47,,,,,,,0.1.0,,impl,,,0,,,,"Currently the code does not compile with hadoop 0.15. Also we might have to allign with configuration changes.

This is due to the changes to hadoop interface that introduced templates:

> Nightly/ws/pig/src/com/yahoo/pig/impl/mapreduceExec/
> PigInputFormat.java :113:  
> com.yahoo.pig.impl.mapreduceExec.PigInputFormat.PigRecordReader is not 
> abstract and does not override abstract method next 
> (org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable
> ) in org.apache.hadoop.mapred.RecordReader
>     [javac]     public static class PigRecordReader implements  
> RecordReader {
>     [javac]                   ^
>     [javac] Note: Some input files use or override a deprecated API.
>     [javac] Note: Recompile with -Xlint:deprecation for details.
>     [javac] Note: Some input files use unchecked or unsafe operations.
>     [javac] Note: Recompile with -Xlint:unchecked for details.
>     [javac] 1 error
",,,,,,,,,,,,,,,,,,,30/Nov/07 01:50;olgan;hadoop15.jar;https://issues.apache.org/jira/secure/attachment/12370595/hadoop15.jar,19/Nov/07 19:20;olgan;hadoop15.patch;https://issues.apache.org/jira/secure/attachment/12369806/hadoop15.patch,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2007-11-19 19:55:09.704,,,no_permission,,,,,,,,,,,,163687,,,,,Fri Nov 30 16:47:09 UTC 2007,,,,,,,0|i0gf8f:,93898,,,,,,,,,,"09/Nov/07 00:51;olgan;HADOOP-1621.  FileStatus is now a concrete class and FileSystem.listPaths is deprecated and replaced with listStatus. (Chris Douglas via omalley)

We use this in a a few places and need to move to listStatus:

ucdev3:~/src/pig/src/org/apache/pig> grep -r listPaths *
PigServer.java:    public String[] listPaths(String dir) throws IOException {
PigServer.java:        Path paths[] = pigContext.getDfs().listPaths(new Path(dir));
impl/io/.svn/text-base/FileLocalizer.java.svn-base:             paths = fs.listPaths(path);
impl/io/FileLocalizer.java:             paths = fs.listPaths(path);
impl/mapreduceExec/.svn/text-base/PigInputFormat.java.svn-base:                 Path children[] = fs.listPaths(fullPath);
impl/mapreduceExec/PigInputFormat.java:                 Path children[] = fs.listPaths(fullPath);
impl/mapreduceExec/PigInputFormat15.java.sav:                   Path children[] = fs.listPaths(fullPath);
tools/grunt/.svn/text-base/GruntParser.jj.svn-base:                     Path paths[] = mDfs.listPaths(dfsPath);
tools/grunt/.svn/text-base/GruntParser.jj.svn-base:             Path paths[] = mDfs.listPaths(dir);
tools/grunt/GruntParser.jj:                     Path paths[] = mDfs.listPaths(dfsPath);
tools/grunt/GruntParser.jj:             Path paths[] = mDfs.listPaths(dir);
tools/grunt/GruntParser.java:                        Path paths[] = mDfs.listPaths(dfsPath);
tools/grunt/GruntParser.java:                Path paths[] = mDfs.listPaths(dir);

","19/Nov/07 19:22;olgan;The attachement contains all the changes needed to transition to hadoop 0.15. Note that the changes are not backward compatible - once applied, the trunk will no longer work with earlier versions of hadoop.

All unit tests and about 75 end-to-end tests ran successfully.",19/Nov/07 19:55;breed;+1 patch looks good,"19/Nov/07 20:06;olgan;If I don't see any objections, I will commit the patch tomorrow moring.",20/Nov/07 18:28;olgan;Changes committed for hadoop 0.15. Testing for hadoop0.15.1. Will need to commit new hadoop15.jar once the testing is completed,"20/Nov/07 18:41;ab;Is there any particular reason why old versions of hadoop*.jar are kept in the lib/ directory? Also, IMHO it would be better to leave the original name of the jar so that it's easier to recognize it ...","20/Nov/07 21:09;olgan;The idea was that we would have an easy way to support multiple versions. However, up till now we had to make non-backward compatible changes to support new versions of hadoop. In general, we need a better way to integrate with hadoop rather than including it in SVN.",30/Nov/07 01:50;olgan;Need to upload jar compatible with hadoop 0.15.1. Will do that tomorrow morning unless I hear any objections. Will aslo remove hadoop13.jar since it is 2 versions behind.,30/Nov/07 01:51;olgan;Forgot to mention that all unit and end-to-end tests passed,30/Nov/07 16:47;olgan;Changes committed. Note that you will need to upgrade your cluster to hadoop 0.15.1 once you update your tree since 0.15 and 0.15.1 are not backward compatible.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filenames in the logical plan are not absolute,PIG-15,12381678,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,olgan,olgan,02/Nov/07 03:11,25/Mar/10 00:12,14/Mar/19 03:05,26/Jan/09 19:43,,,,,,,0.2.0,,impl,,,0,,,,"Lets say my current working dir (CWD) is /user/x and there exists a file 'a';


The following results in an error:

a = load 'a';

cd /

dump a;


The problem is that the logical plan does not have fully qualified name for 'a' and it is converted only when the
physical plan compilation is done. So in between if someone changes the directory as above we have a problem.

As a user I do not worry/know about the side effects of pig's choice to execute queries lazily. So when I asked it to
load 'a' I made sure that I was in the right directory. I think that the file is loaded and I change the directory. Now
when the dump is asked for, the compilation starts and all it does to qualify the filename is to get the CWD and append
it with the filename if its not absolute already. Hence the problem.

One solution is to store the FQFN while parsing the query. Since there might users of just the logical plan like
pigbody, I think the logical plan should support this.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,163685,,,,,Mon Jan 26 19:43:53 UTC 2009,,,,,,,0|i0gf7j:,93894,,,,,,,,,,26/Jan/09 19:43;olgan;tested that this works with latest code.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
large key cause pig reduce jobs to die,PIG-14,12381677,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,02/Nov/07 03:09,24/Mar/10 22:01,14/Mar/19 03:05,30/Nov/07 18:47,,,,,,,0.1.0,,impl,,,0,,,,"The reducer sends a heartbeat to the task tracker every time it starts processing new key. The task tracker expects to
get a message every 10 minutes. If processing of an individual key takes longer, which could be the case for your job,
the task tracker would not get a heartbeat in time and would kill the task.

The current patch is to add <property>
	<name>mapred.task.timeout</name>
	<value>0</value>
	<description>timeout value</description>
</property>

to the cluster's hadoop-site.xml. This results in disabling heartbeat functionality which might not be what we want
long term.

A more flexible approach is to periodically report from map and reduce job via
http://lucene.apache.org/hadoop/api/org/apache/hadoop/mapred/Reporter.html#setStatus(java.lang.String)

As a workaround for a UDF, call: PigMapReduce.reporter.progress() every 1000th time


",,,,,,,,,,,,,,,,,,,28/Nov/07 20:14;olgan;heartbeat.patch;https://issues.apache.org/jira/secure/attachment/12370437/heartbeat.patch,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-11-28 20:51:11.65,,,no_permission,,,,,,,,,,,,163684,,,,,Fri Nov 30 18:47:48 UTC 2007,,,,,,,0|i0gf7b:,93893,,,,,,,,,,"19/Nov/07 19:24;olgan;Based on discussion with Ben, we need to make both DataBag and BigDataBag to send periodic heartbeats.",28/Nov/07 20:14;olgan;Please review the patch. All unit tests including the new ones are passing. about 80 end-to-end tests are passing. Finally and large data test that used to have this problem now does now show it.,28/Nov/07 20:51;breed;+1 code looks good. My only suggestion is that you comment numNotifies to indicate that it is only used by the unit tests. (It would be nice if we could test without that variable.),"28/Nov/07 21:00;olgan;Ben, thanks for the review. I will add the comments.

I will wait till this evening to commit in case there are any objections.","29/Nov/07 00:28;utkarsh;Looks good, except for the slightly longer-term issue that we seem to be pushing more and more hadoop specific stuff into our core codebase. It would be nice if we could maintain a clean separation between our data model, and the underlying execution platform. 

Thus, my comment relates more to https://issues.apache.org/jira/browse/PIG-32

For example, what happens if we are executing in the local mode? The patch happens to work alright because PigMapReduce.reporter is null. In general, the correct approach seems to be that progress() is an abstract method provided by the execution engine.
","29/Nov/07 15:58;breed;I second Utkarsh's comment. I think the patch is fine for now, but longer term there should be a more generic solution that should be part of PIG-32.",30/Nov/07 18:47;olgan;patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
need description of pig on website,PIG-4,12381567,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,olgan,cutting,cutting,31/Oct/07 17:08,24/Mar/10 22:01,14/Mar/19 03:05,01/Nov/07 00:01,site,,,,,,0.1.0,,site,,,0,,,,The website lacks a description of the project.  We should add that ASAP.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2007-10-31 18:40:39.155,,,no_permission,,,,,,,,,,,,163674,,,,,Thu Nov 01 00:01:38 UTC 2007,,,,,,,0|i0gf2v:,93873,,,,,,,,,,31/Oct/07 18:40;olgaln;I will take this bug. I could not find a way to assign it to myself. The option to change bug assignment does not show when I click on Edit. Any ideas why?,"31/Oct/07 19:08;cutting;You have two jira accounts.  One (ogaln) that you're logged in with, and have reported some issues with, and another (olgan) that I gave permissions to, since it appeared to be spelled correctly.  Do you prefer 'olgaln'?  If so, I will give permissions to that account and remove 'olgan'.  Otherwise, please login as 'olgan' and I will remove 'olgaln'.",01/Nov/07 00:01;olgan;I added a brief description and info to About Pig page. The changes have been committed but are not showing up yet on the page.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig should not crash when it encounters an unknown function during interactive use,PIG-3,12381509,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,tdunning@veoh.com,tdunning@veoh.com,30/Oct/07 23:48,24/Mar/10 22:01,14/Mar/19 03:05,28/Feb/08 07:05,0.0.0,,,,,,0.1.0,,grunt,,,0,,,,"I get this.  The crash is unfortunate when all that happened is that I should have capitalized count.

grunt> z = foreach y generate group, count(y);                                                   
java.lang.RuntimeException: Function count not found.
        at com.yahoo.pig.impl.PigContext.getUDF(PigContext.java:155)
        at com.yahoo.pig.impl.logicalLayer.parser.QueryParser.EvalFunction(QueryParser.java:3135)
        at com.yahoo.pig.impl.logicalLayer.parser.QueryParser.FuncEvalItem(QueryParser.java:1915)
        at com.yahoo.pig.impl.logicalLayer.parser.QueryParser.BaseEvalItem(QueryParser.java:1811)
        at com.yahoo.pig.impl.logicalLayer.parser.QueryParser.UnaryExpr(QueryParser.java:1757)
        at com.yahoo.pig.impl.logicalLayer.parser.QueryParser.MultiplicativeExpr(QueryParser.java:1689)
        at com.yahoo.pig.impl.logicalLayer.parser.QueryParser.AdditiveExpr(QueryParser.java:1622)
        at com.yahoo.pig.impl.logicalLayer.parser.QueryParser.InfixExpr(QueryParser.java:1590)
        at com.yahoo.pig.impl.logicalLayer.parser.QueryParser.GenerateItem(QueryParser.java:1549)
        at com.yahoo.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItem(QueryParser.java:1495)
        at com.yahoo.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItemList(QueryParser.java:1440)
        at com.yahoo.pig.impl.logicalLayer.parser.QueryParser.GenerateStatement(QueryParser.java:1394)
        at com.yahoo.pig.impl.logicalLayer.parser.QueryParser.NestedBlock(QueryParser.java:1164)
        at com.yahoo.pig.impl.logicalLayer.parser.QueryParser.ForEachClause(QueryParser.java:1128)
        at com.yahoo.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:411)
        at com.yahoo.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:253)
        at com.yahoo.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:177)
        at com.yahoo.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:31)
        at com.yahoo.pig.PigServer.registerQuery(PigServer.java:153)
","hadoop 13.1, hacked pig",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-02-28 07:05:59.166,,,no_permission,,,,,,,,,,,,163673,,,,,Thu Feb 28 07:05:59 UTC 2008,,,,,,,0|i0gf2f:,93871,,,,,,,,,,"28/Feb/08 07:05;joa23;Ted, I think this was fixed with PIG-95. I tried with the latest svn revision and could not reproduce the problem. Please reopen with a detailed and up to date stack trace if  this still occurs for you. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
need non-Y! icon image,PIG-2,12381496,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,utkarsh,cutting,cutting,30/Oct/07 22:30,24/Mar/10 22:01,14/Mar/19 03:05,01/Nov/07 00:02,site,,,,,,0.1.0,,site,,,0,,,,"Pig needs an icon image for the website.  The existing image has a Y! logo on it, which is inappropriate.  For a start, someone could simply photoshop the Y! out.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2007-10-30 23:00:09.356,,,no_permission,,,,,,,,,,,,163672,,,,,Thu Nov 01 00:02:46 UTC 2007,,,,,,,0|i0gf1r:,93868,,,,,,,,,,30/Oct/07 23:00;utkarsh;Forwarded request to George who did the original image,"31/Oct/07 17:06;cutting;Assigning to Utkarsh, since he spoke first.","31/Oct/07 17:07;cutting;I'm raising the priority of this, since we should really fix it ASAP.",01/Nov/07 00:02;olgan;I pushed new logo on Behalf of Utkarsh. It is not yet showing up on the site.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Initial Pig code contribution from Yahoo! ,PIG-1,12380118,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgaln,olgaln,10/Oct/07 22:29,24/Mar/10 22:01,14/Mar/19 03:05,31/Oct/07 16:57,,,,,,,0.1.0,,,,,0,,,,The submission includes pig source code and unit tests.,,,,,,,,,,,,,,,,,,,10/Oct/07 22:36;olgaln;pig-src.tar.gz;https://issues.apache.org/jira/secure/attachment/12367531/pig-src.tar.gz,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-10-31 16:57:41.32,,,no_permission,,,,,,,,,,,,163671,,,,,Wed Oct 31 16:57:41 UTC 2007,,,,,,,0|i0gf1b:,93866,,,,,,,,,,31/Oct/07 16:57;cutting;Olga committed this in http://svn.apache.org/viewvc?view=rev&revision=589866.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
